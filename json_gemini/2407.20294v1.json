{"title": "A Bayesian Flow Network Framework for Chemistry Tasks", "authors": ["Nianze TAO", "Minori ABE"], "abstract": "In this work, we introduce ChemBFN, a language model that handles chemistry tasks based on\nBayesian flow networks working on discrete data. A new accuracy schedule is proposed to improve\nthe sampling quality by significantly reducing the reconstruction loss. We show evidence that\nour method is appropriate for generating molecules with satisfied diversity even when a smaller\nnumber of sampling steps is used. A classifier-free guidance method is adapted for conditional\ngeneration. It is also worthwhile to point out that after generative training, our model can be\nfine-tuned on regression and classification tasks with the state-of-the-art performance, which opens\nthe gate of building all-in-one models in a single module style. Our model has been open sourced\nat https://github.com/Augus1999/bayesian-flow-network-for-chemistry.", "sections": [{"title": "Introduction", "content": "Autoregressive models (ARs) including SMILES-based or fragment-based models 1\u20134 that leverage the\npower of language models (LMs) and graph-based models5-7 coupled with advanced techniques such\nas Monte Carlo tree search 6,7 have been proved their success in several de novo design benchmarks 4,8\nconsisted of drug-like molecules. The constraint of ARs, i.e., the number of sampling steps is the size of\ngenerated object, however, limits the potential of generating large molecules. Conversely, the recently\nemerging denoising-diffusion models (DMs) offer a way to generate objects of any size within a fixed\nsequence of sampling process. However, it has been pointed out in the research of C. Vignac et al 10\nthat SMILES-based models generally worked better than graph DMs even when a dedicatedly designed\ndiscrete diffusion method was applied.\nBayesian flow networks 11 (BFNs) are in a different category of generative models that decouple\nthe sampling process with the size of generated objects as well. Different from DMs, BFNs directly\nwork on the parameters of data distributions which naturally enable them to handle both continuous\n(including discretised) and discrete data without any data preprocessing or change of (mathematical)\nframework. Although the authors of BFN showed evidence in the origin paper 11 that BFN advantaged\nover discrete DMs on discrete data generating, e.g., text generation, the recent researches considering\nde novo molecule design only successfully employed it on continuous and discretised data, e.g., 3D\nmolecular conformation generation12 rather than language-like representations such as SMILES13 or\nSELFIES. 14 One potential reason discouraging the application to text generation is the lack of exact\nanalytical expression for the accuracy schedule \u1e9e(t), one critical component of BFNs, in the discrete case,\nwhile the speculated quadratic \u03b2(t) in the original paper is, as admitted by the authors, 11 suboptimal.\nIn this paper, we introduce ChemBFN, a Bayesian Flow Network framework for Chemistry tasks,\nthat leverages our newly proposed accuracy schedule and transformer 15 encoder model to generate 1D\nlanguage-like molecular representations e.g., SMILES and SELFIES. The experiments demonstrated\nthat models with our accuracy schedule outperform those with the quadratic accuracy schedule. Besides,\nthe generative training of BFN method can be a powerful pretraining strategy for downstream tasks in\nmolecular property predictions, including regressions and classifications, and reaction yield predictions."}, {"title": "Methods", "content": "2.1 Model Architecture\nOur model is an adaptation of DiT 16 model. The differences in our implementation include (1) the use\nof categorical distributions rather than image embeddings for input tokens because we are not dealing\nwith images; (2) logits output that are then transformed into probabilities by softmax function; (3) the\nreplacement of activation function with SELU17 function; (4) the use of a 2-layer MLP to form the time\nembedding since \"time\" in BFN is continuous from 0 to 1; (5) the employment of XPOS 18 variation of\nrotary positional embedding. 19\nFollowing the notations of the BFN paper, 11 the parameter of categorical distributions inputted\ninto the neural network is denoted by \u03b8 = (\u03b8(1), \u03b8(2), ..., \u03b8(D)) \u2208 [0, 1]KD (K is the number of categories,\nD is the number of input data, and \u03b8(i) is the ith data) and the output distribution at time step t is\ndenoted by po(0;t) \u2208 [0,1]KD. We denote the sum of time embedding vector and conditioning vector\nas c. A null conditioning & is equivalent to a zero vector 0.\nIn each experiment described in the later text, we employed the same hyperparameters of the model\nexcept category number K that depends on molecular representations. The 2-layer MLP with SELU\nactivation has the shape of [1, 256, 512]. We employed 12 Transformer layers, of which had 8 attention\nheads each, with the attention temperature 22 \u03c4 = \u221a2dh (dh is the feature number of each attention\nhead). The dropout rate was 0.01 and the hidden feature number was 512. These settings lead to a\ntotal learnable parameters of the model of the magnitude of 54M.\n2.2 A New Accuracy Schedule\nIdeally, an accuracy schedule function \u1e9e(t) drives the expectation of entropy of the input distribution\nEpF (0|x;t) H [p1(x|\u03b8)] to decrease linearly with t, where x stands for the clear data, pF(0|x; t) represents\nBayesian flow distribution, and p\u2081(x0) is the input distribution as denoted in the origin paper. 11 The\nmathematical difficulty of deriving the expectation analytically in the discrete case compels us to specu-\nlate from intuition. The authors of BFN claimed that \"\u1e9e(t) = t\u00b2\u1e9e(1) was a reasonable approximation\",\nbut disclosed later that finding a suitable value for the hyperparameter \u03b2(1) was not an easy job. 11\nHere, we give our estimation of \u1e9e(t). If we estimate the expected entropy of the input distribution\n(denoted as E for short) as E ~ f(K)e-\u1e9e(t), then the relationship E(t) = (1 \u2013 t)E(0) + tE(1) that"}, {"title": null, "content": "eliminates the unknown function f(K) gives us\n\u03b2(t) = -ln(1 - t + te-\u03b2(1))\n(1)\nK\nand the corresponding\nd\u03b2\n4\n1 - e-\u03b2(1)\n\u03b1(t) =\ndt=\nK 1 - t + te-\u03b2(1)\n(2)\nwhere \u1e9e(1) is still a hyperparameter. Equation (2) changes the continuous time loss L\u221e to\nK\nL\u221e(x) = Et~U(0,1),pp (0|x;t) (\u03b1(t)||ex \u2013 e(0;t)||2),\n(3)\nwhere ex is the one-hot representation of data \u00e6 while e(0;t) is the predicted categorical distribution\nof data x at time t. Note that when \u1e9e(1) is large, a(1) goes to extremely large. Therefore, we limit\n\u03b1(1) \u2264 32\u03b2(1), from which\n\u03b2(1)max \u2248 20.4054/K\n(4)\nis obtained. An example of how our accuracy schedule looks different from original one is plotted in\nFigure 2. We shall show in the later experiments that our \u1e9e(t) in Equation (1) works better than\nquadratic ones."}, {"title": "Datasets and Benchmarks", "content": "Two benchmarks \u2013 MOSES and GuacaMol 4\nwere used to evaluate the generative performance, e.g.,\nthe similarity between generated molecules and training molecules, of ChemBFN. We reported the\ndistribution-learning metrics of these benchmarks in Section 3.\nThe QM923 dataset was employed to study the capability of conditional generation of our method.\nWe randomly selected 110k molecules, before which 3054 invalid data were removed, with the triple\n(EHOMO, ELUMO, \u2206EHOMO-LUMO) as the conditioning label to form the training set.\nIn order to evaluate the downstream performance, 40M unique SMILES and 190M unique SMILES\nstrings were randomly selected from ZINC1524 database that formed two pretraining sets. The model\ntrained on the 40M set was finetuned on several regression (ESOL, FreeSolv, and Lipo) and classification\n(\u0412\u0412\u0412\u0420, \u0412\u0410\u0421\u0415, ClinTox, and HIV) subsets of widely used MoleculeNet 25 benchmark. Each dataset was\nsplit into training/validation/testing sets in the ratio of 80/10/10 following the scaffold splitting method\nproposed in DeepChem 26 project. We reported ROC-AUC (area under receiver operating characteristic\ncurve) for classification tasks and RMSE (root-mean squared error) for regression tasks in Section 3.\nThe USPTO-50k27 dataset, Buchwald-Hartwig and Suzuki-Miyaura reaction yield datasets from\nhigh-throughput experiments (HTE) cleaned by P. Schwaller et al 28 were employed to train the model\nto predict reaction yield. We report coefficient of determination (R2 score) on testing sets in Section 3.\nAqSolDB, 29 a more challenging solubility dataset containing more species than ESOL, was used\nto investigate the effect of the size of pretraining data. A training/validation/testing (80/10/10) split\nwas generated using scaffold splitting method. Testing MAE (mean absolute error) and RMSE were\nreported in Section 3."}, {"title": null, "content": "For SMILES representation, we developed a universal tokeniser that generates a fixed number (specif-\nically K = 246) of unique vocabulary for any collection of molecules. The similar strategy was not ap-\nplicable to SELFIES strings, which were translated from SMILES via official selfies 14 package, hereby\nthe vocabulary should be computed separately for each dataset and the category number K varies.\nNote that we include three special tokens start , end , and pad in the vocabulary.\n2.4 Fine-tuning Strategy\nSimilar to the strategy of ChemBERTa models, 30,31 the embedding, denoted as \u03a8 start , of start\ntoken at time t = 1 was used as a fingerprint for downstream tasks. A 2-layer MLP absorbing a\ndropout layer is used as the prediction head. We replace the input distribution in generative mode with\nthe one-hot representation of data (token), i.e., 0 \u2190 ex = (e start ,..., end ) \u2208 {0, 1}KD in this stage.\n3 Experiments and Results\n3.1 Unconditional Generation"}, {"title": "Conditional Generation of Small Molecules", "content": "The classifier-free guidance 32 method is easily adapted into BFN, where only the computing of output\ndistribution needs changing during sampling process. The pseudocode of computing discrete output\ndistribution is presented in Algorithm 1. In the experiment, we jointly trained a model conditional and\nunconditional on QM9 dataset with an unconditional rate Puncond = 0.2. In the sampling stage, w was\nset to 4. We sampled 10 molecules using the label [-0.249, 0.0615, 0.3105] that was transformed to y\nvia a trained 2-layer MLP. 10 unconditioned samples were generated as a control group. RDKit 33 was\nemployed to generate the 3D conformations, then the geometry optimisations and energy calculations\nwere performed via PySCF34 at B3LYP/6-31G(2df,p) level of accuracy. The results of MAE between\ncalculated values and labels are presented in Table 6. The conditioned samples are displayed in Figure 5."}, {"title": null, "content": "Algorithm 1 Invoking classifier-free guidance into output distribution\nRequire: w \u2208 R, conditioning vector y\nfunction DISCRETE_OUTPUT_DISTRIBUTION(\u03b8 \u2208 [0, 1]KD, t \u2208 [0, 1], y \u2208 R\u0192)\nInput (\u03b8, t, y) to network, receive \u03a8(\u03b8,t,y) as output\nif in training stage or y is & then\npo(\u03b8; t) \u2190 softmax(\u03a8(\u03b8, t, y))dim=-1\nelse\nInput (\u03b8, t, \u03c6) to network, receive \u03a8(\u03b8,t, \u03c6) as output\nPo(\u03b8; t) \u2190 softmax((1 + w)\u03a8(\u03b8,t,y) \u2013 w\u03a8(0, t, $))dim=\u22121\nend if\nreturn po(\u03b8;t)\nend function"}, {"title": "Molecular Scaffold Extension", "content": "Here, we show a simple inpaint strategy can extend molecular scaffolds by using ChemBFN. In every\nsampling steps, parameters of input distributions are modified as 0 \u2190 M \u00a9 ex + (1 \u2212 M)\nbeing inputted into the network, where M is the mask and ex is the one-hot representation of scaffold.\nFigure 6 shows an example of extending scaffold \u2018Cc1cc(OC5)cc(C6)c1.\u2019 by a model trained on MOSES\nSAFE 35 version, a variation of SMILES. We found that inpainting sampling for 10 to 100 steps was\nsufficient to generate complex molecules."}, {"title": "Finetuning on Prediction Tasks", "content": "In this section, we compare our model with SOTA models, 30,31,36\u201341 including graph-based and language-\nbased which could be further classified as smaller scale natural language processing models (NLPs) and\nlarge language models (LLMs), on subsets of MoleculeNet benchmark. As shown in Table 7, our method\noutperformed SOTA language models on several tasks, especially ClinTox and BBBP. It is notable that\nChemBERTa 30 and ChemBERTa-2,31 which had a similar model size with ours, were pretrained on\n77M molecules but had worse scores on 3 out of 5 tasks than ours. This indicated that BFN-style\ngenerative pretraining is a better strategy than masked language modeling and multitask regression\npretraining. A similar observation applied to CaRROBERTa model that coupled the knowledge of Chat-\nGPT42 (which is far larger in scale than ours and is believed to have seen more chemical texts) and\nthe distillation capability of RoBERTa21 method: our model outperformed CaRROBERTa on 4 out of 5\ntasks. However, when comparing with graph neural networks (GNNs) our model performed averagely\n1.7% worse, especially on regression tasks."}, {"title": "Reaction Yield Prediction", "content": "In order to predict the reaction yield, we first trained the generative model to understand chemical\nreaction by learning to predict the products. We developed an in-context style guidance that during\ntraining stage only the parameters of product in reaction SMILES were predicted by always masking\nthe input distribution of reactant/reagent and >> token to the one-hot representation, i.e., 0 \u2190\nMrrex + (1 \u2013 Mrr) 0, where Mrr is the mask for reactant, reagent, and >> token."}, {"title": null, "content": "The generative model was first pre-trained on USPTO-50k dataset then post-trained on Buchwald-\nHartwig and Suzuki-Miyaura coupling datasets before the whole prediction model was fine-tuned. The\ntesting scores compared with previous researches 28,43,44 were reported in Table 8. It is notable that the\nYield-BERT series 28,44 were based on a pre-trained RXNFP45 model which had been pre-trained on\nover 2M reactions while our model was pre-trained on 50k reactions. Despite the disadvantage of limited\naccess of pretraining data, the performance of our method was still close to that of largely pretrained\nmodel on random-split sets and significantly better on out-of-sample predictions."}, {"title": "Is Larger Pretrain Dataset Better?", "content": "We have seen that our model, although was pretrained on 40M molecules, outperformed models pre-\ntrained on larger dataset on several prediction tasks. Here rises a question: whether larger pretrain\ndataset benefits our method? To answer this, 3 models were trained on AqSolDB dataset, of which one\nwas trained from scratch, one was pretrained on 40M molecules from ZINC15 database, and one was\npretrained on 190M molecules from ZINC15. We summarised the testing results in Table 9. Interest-\ningly, the errors did not shrink when the pretrain data grew from 40M to 190M. However, compared\nwith zero pretraining, an improvement on performance of \u226512.5% can be confirmed."}, {"title": "Training Details", "content": "For all generative tasks, the models were trained for 100 epochs with the batch-size of 120 molecule/batch.\nThe learning rate (lr) was 5.0 \u00d7 10-5 that was linearly increased (warm-up) from 10-8 during the first\n1,000 training steps.\nWe pre-trained one model on 40M SMILES for 15 epochs with the batch-size of 512 on single A100\nGPU and one model on 190M SMILES for 5 epochs with the effective batch-size of 1,024 (2 \u00d7 512) on\n2xA100 GPUs. The warm-up strategy and Ir were the same as mentioned above.\nDuring fine-tuning stages, models were trained for 100 epochs on labelled datasets. The batch-size,\nboth for training and validation, was 32 on MoleculeNet benchmark; the training batch-size was 16 for\nreaction yield prediction. Irmax was 10-4 that was warmed up from 10-7 during the first 1,000 steps\nfor regression tasks and 100 steps for classification tasks. After warm-up stage, Ir decreased by 0.2\nafter the validation metrics stopped improving for 20 epochs unless the learning rate had reached 10-6.\nThe dropout rate of prediction MLP head was fine-tuned for each case and we recommend to try from\n{0.0,0.5,0.7}. The validation metrics for regression and classification tasks were MAE and accuracy,\nrespectively.\nWe employed AdamW46 with default hyperparameters implemented in PyTorch 47 as the optimizer\nfor all tasks."}, {"title": "Conclusion", "content": "ChemBFN, a Bayesian flow network framework for chemistry tasks of both generation and prediction,\nwas developed in this work. The new accuracy schedule helped ChemBFN achieve competitive per-\nformance of discrete diffusion models and autoregressive models on generating large molecules. We\nproposed a BFN-style generative pretraining strategy that surpassed existing language-based trans-\nformer models on several classification and regression tasks. We believe this work provides a tool that\ncan accelerate researches of both drug designing and filtering and give in helpful information for syn-\nthesis planning. However, we still leave gaps between graph-based models in prediction tasks, which we\nshall keep for the future research."}]}