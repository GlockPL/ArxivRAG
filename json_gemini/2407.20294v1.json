{"title": "A Bayesian Flow Network Framework for Chemistry Tasks", "authors": ["Nianze TAO", "Minori ABE"], "abstract": "In this work, we introduce ChemBFN, a language model that handles chemistry tasks based on Bayesian flow networks working on discrete data. A new accuracy schedule is proposed to improve the sampling quality by significantly reducing the reconstruction loss. We show evidence that our method is appropriate for generating molecules with satisfied diversity even when a smaller number of sampling steps is used. A classifier-free guidance method is adapted for conditional generation. It is also worthwhile to point out that after generative training, our model can be fine-tuned on regression and classification tasks with the state-of-the-art performance, which opens the gate of building all-in-one models in a single module style. Our model has been open sourced at https://github.com/Augus1999/bayesian-flow-network-for-chemistry.", "sections": [{"title": "Introduction", "content": "Autoregressive models (ARs) including SMILES-based or fragment-based models 1\u20134 that leverage the power of language models (LMs) and graph-based models5-7 coupled with advanced techniques such as Monte Carlo tree search 6,7 have been proved their success in several de novo design benchmarks 4,8 consisted of drug-like molecules. The constraint of ARs, i.e., the number of sampling steps is the size of generated object, however, limits the potential of generating large molecules. Conversely, the recently emerging denoising-diffusion models (DMs) offer a way to generate objects of any size within a fixed sequence of sampling process. However, it has been pointed out in the research of C. Vignac et al 10 that SMILES-based models generally worked better than graph DMs even when a dedicatedly designed discrete diffusion method was applied.\nBayesian flow networks 11 (BFNs) are in a different category of generative models that decouple the sampling process with the size of generated objects as well. Different from DMs, BFNs directly work on the parameters of data distributions which naturally enable them to handle both continuous (including discretised) and discrete data without any data preprocessing or change of (mathematical) framework. Although the authors of BFN showed evidence in the origin paper 11 that BFN advantaged over discrete DMs on discrete data generating, e.g., text generation, the recent researches considering de novo molecule design only successfully employed it on continuous and discretised data, e.g., 3D molecular conformation generation12 rather than language-like representations such as SMILES13 or SELFIES. 14 One potential reason discouraging the application to text generation is the lack of exact analytical expression for the accuracy schedule \u03b2(t), one critical component of BFNs, in the discrete case, while the speculated quadratic \u03b2(t) in the original paper is, as admitted by the authors, 11 suboptimal.\nIn this paper, we introduce ChemBFN, a Bayesian Flow Network framework for Chemistry tasks, that leverages our newly proposed accuracy schedule and transformer 15 encoder model to generate 1D language-like molecular representations e.g., SMILES and SELFIES. The experiments demonstrated that models with our accuracy schedule outperform those with the quadratic accuracy schedule. Besides,"}, {"title": "Methods", "content": null}, {"title": "Model Architecture", "content": "Our model is an adaptation of DiT 16 model. The differences in our implementation include (1) the use of categorical distributions rather than image embeddings for input tokens because we are not dealing with images; (2) logits output that are then transformed into probabilities by softmax function; (3) the replacement of activation function with SELU17 function; (4) the use of a 2-layer MLP to form the time embedding since \"time\" in BFN is continuous from 0 to 1; (5) the employment of XPOS 18 variation of rotary positional embedding. 19 The architecture is shown in Figure 1.\nFollowing the notations of the BFN paper, 11 the parameter of categorical distributions inputted into the neural network is denoted by \u03b8 = (\u03b8(1), \u03b8(2), ..., \u03b8(D)) \u2208 [0, 1]KD (K is the number of categories, D is the number of input data, and \u03b8(i) is the ith data) and the output distribution at time step t is denoted by po(\u03b8; t) \u2208 [0, 1]KD. We denote the sum of time embedding vector and conditioning vector as c. A null conditioning \u03c6 is equivalent to a zero vector 0.\nIn each experiment described in the later text, we employed the same hyperparameters of the model except category number K that depends on molecular representations. The 2-layer MLP with SELU activation has the shape of [1, 256, 512]. We employed 12 Transformer layers, of which had 8 attention heads each, with the attention temperature 22 \u03c4 = \u221a2dh (dh is the feature number of each attention head). The dropout rate was 0.01 and the hidden feature number was 512. These settings lead to a total learnable parameters of the model of the magnitude of 54M."}, {"title": "A New Accuracy Schedule", "content": "Ideally, an accuracy schedule function \u03b2(t) drives the expectation of entropy of the input distribution EpF (\u03b8|x;t) H [p1(x|\u03b8)] to decrease linearly with t, where x stands for the clear data, pF(\u03b8|x; t) represents Bayesian flow distribution, and p1(x|\u03b8) is the input distribution as denoted in the origin paper. 11 The mathematical difficulty of deriving the expectation analytically in the discrete case compels us to speculate from intuition. The authors of BFN claimed that \"\u03b2(t) = t2\u03b2(1) was a reasonable approximation\", but disclosed later that finding a suitable value for the hyperparameter \u03b2(1) was not an easy job. 11\nHere, we give our estimation of \u03b2(t). If we estimate the expected entropy of the input distribution (denoted as E for short) as E ~ f(K)e\u2212\u03b2(t), then the relationship E(t) = (1 \u2013 t)E(0) + tE(1) that"}, {"title": null, "content": "eliminates the unknown function f(K) gives us\n\u03b2(t) = \u2212ln (1 \u2212 t + te\u2212\u03b2(1)\n)\nK\n(1)\nand the corresponding\n\u03b1(t) = \u2212\nd\u03b2\ndt\n4\n1 \u2212 e\u2212\u03b2(1)\nK 1 \u2212 t + te\u2212\u03b2(1),\n(2)\nwhere \u03b2(1) is still a hyperparameter. Equation (2) changes the continuous time loss L\u221e to\nL(x) = Et\u223cU(0,1),pp (\u03b8|x;t)\n(\u03b1(t)||ex \u2212 e(\u03b8; t)||2),\nK\n(3)\nwhere ex is the one-hot representation of data x while e(\u03b8; t) is the predicted categorical distribution of data x at time t. Note that when \u03b2(1) is large, \u03b1(1) goes to extremely large. Therefore, we limit \u03b1(1) \u2264 32\u03b2(1), from which\n\u03b2(1)max \u2248 20.4054/K\n(4)\nis obtained. An example of how our accuracy schedule looks different from original one is plotted in Figure 2. We shall show in the later experiments that our \u03b2(t) in Equation (1) works better than quadratic ones."}, {"title": "Datasets and Benchmarks", "content": "Two benchmarks \u2013 MOSES and GuacaMol 4 were used to evaluate the generative performance, e.g., the similarity between generated molecules and training molecules, of ChemBFN. We reported the distribution-learning metrics of these benchmarks in Section 3.\nThe QM923 dataset was employed to study the capability of conditional generation of our method. We randomly selected 110k molecules, before which 3054 invalid data were removed, with the triple (EHOMO, ELUMO, \u2206EHOMO\u2212LUMO) as the conditioning label to form the training set.\nIn order to evaluate the downstream performance, 40M unique SMILES and 190M unique SMILES strings were randomly selected from ZINC1524 database that formed two pretraining sets. The model trained on the 40M set was finetuned on several regression (ESOL, FreeSolv, and Lipo) and classification (BBBP, BACE, ClinTox, and HIV) subsets of widely used MoleculeNet 25 benchmark. Each dataset was split into training/validation/testing sets in the ratio of 80/10/10 following the scaffold splitting method proposed in DeepChem 26 project. We reported ROC-AUC (area under receiver operating characteristic curve) for classification tasks and RMSE (root-mean squared error) for regression tasks in Section 3.\nThe USPTO-50k27 dataset, Buchwald-Hartwig and Suzuki-Miyaura reaction yield datasets from high-throughput experiments (HTE) cleaned by P. Schwaller et al 28 were employed to train the model to predict reaction yield. We report coefficient of determination (R2 score) on testing sets in Section 3.\nAqSolDB, 29 a more challenging solubility dataset containing more species than ESOL, was used to investigate the effect of the size of pretraining data. A training/validation/testing (80/10/10) split was generated using scaffold splitting method. Testing MAE (mean absolute error) and RMSE were reported in Section 3."}, {"title": "Fine-tuning Strategy", "content": "Similar to the strategy of ChemBERTa models, 30,31 the embedding, denoted as \u03a8 (start), of \u27e8start\u27e9 token at time t = 1 was used as a fingerprint for downstream tasks. A 2-layer MLP absorbing a dropout layer is used as the prediction head. We replace the input distribution in generative mode with the one-hot representation of data (token), i.e., \u03b8 \u2190 ex = (e\u27e8start\u27e9, ..., e\u27e8end\u27e9) \u2208 {0, 1}KD in this stage."}, {"title": "Experiments and Results", "content": null}, {"title": "Unconditional Generation", "content": "We first evaluate the effect of different \u03b2(t) with different values of \u03b2(1) using MOSES dataset. We reported the validity, FCD on scaffold set, SNN on scaffold set, Frag on scaffold set, Scaf on scaffold set, Filters, and Novelty scores computed by MOSES program in Table 1 together with reconstruction loss L' = \u2212EpF (\u03b8|x;t) ln p\u03b8(x|\u03b8; t) and continuous time loss L\u221e in Figure 4. It is clear that raising"}, {"title": "Conditional Generation of Small Molecules", "content": "The classifier-free guidance 32 method is easily adapted into BFN, where only the computing of output distribution needs changing during sampling process. The pseudocode of computing discrete output distribution is presented in Algorithm 1. In the experiment, we jointly trained a model conditional and unconditional on QM9 dataset with an unconditional rate Puncond = 0.2. In the sampling stage, w was set to 4. We sampled 10 molecules using the label [-0.249, 0.0615, 0.3105] that was transformed to y via a trained 2-layer MLP. 10 unconditioned samples were generated as a control group. RDKit 33 was employed to generate the 3D conformations, then the geometry optimisations and energy calculations were performed via PySCF34 at B3LYP/6-31G(2df,p) level of accuracy. The results of MAE between calculated values and labels are presented in Table 6. The conditioned samples are displayed in Figure 5."}, {"title": "Molecular Scaffold Extension", "content": "Here, we show a simple inpaint strategy can extend molecular scaffolds by using ChemBFN. In every sampling steps, parameters of input distributions are modified as \u03b8 \u2190 M \u2299 ex + (1 \u2212 M) \u2299 \u03b8 before being inputted into the network, where M is the mask and ex is the one-hot representation of scaffold. Figure 6 shows an example of extending scaffold \u2018Cc1cc(OC5)cc(C6)c1.\u2019 by a model trained on MOSES SAFE 35 version, a variation of SMILES. We found that inpainting sampling for 10 to 100 steps was sufficient to generate complex molecules."}, {"title": "Finetuning on Prediction Tasks", "content": "In this section, we compare our model with SOTA models, 30,31,36\u201341 including graph-based and language-based which could be further classified as smaller scale natural language processing models (NLPs) and large language models (LLMs), on subsets of MoleculeNet benchmark. As shown in Table 7, our method outperformed SOTA language models on several tasks, especially ClinTox and BBBP. It is notable that ChemBERTa 30 and ChemBERTa-2,31 which had a similar model size with ours, were pretrained on 77M molecules but had worse scores on 3 out of 5 tasks than ours. This indicated that BFN-style generative pretraining is a better strategy than masked language modeling and multitask regression pretraining. A similar observation applied to CaRROBERTa model that coupled the knowledge of ChatGPT 42 (which is far larger in scale than ours and is believed to have seen more chemical texts) and the distillation capability of RoBERTa 21 method: our model outperformed CaRROBERTa on 4 out of 5 tasks. However, when comparing with graph neural networks (GNNs) our model performed averagely 1.7% worse, especially on regression tasks."}, {"title": "Reaction Yield Prediction", "content": "In order to predict the reaction yield, we first trained the generative model to understand chemical reaction by learning to predict the products. We developed an in-context style guidance that during training stage only the parameters of product in reaction SMILES were predicted by always masking the input distribution of reactant/reagent and >> token to the one-hot representation, i.e., \u03b8 \u2190 Mrrex + (1 \u2212 Mrr) \u2299 \u03b8, where Mrr is the mask for reactant, reagent, and >> token."}, {"title": "Is Larger Pretrain Dataset Better?", "content": "We have seen that our model, although was pretrained on 40M molecules, outperformed models pre-trained on larger dataset on several prediction tasks. Here rises a question: whether larger pretrain dataset benefits our method? To answer this, 3 models were trained on AqSolDB dataset, of which one was trained from scratch, one was pretrained on 40M molecules from ZINC15 database, and one was pretrained on 190M molecules from ZINC15. We summarised the testing results in Table 9. Interestingly, the errors did not shrink when the pretrain data grew from 40M to 190M. However, compared with zero pretraining, an improvement on performance of \u226512.5% can be confirmed."}, {"title": "Training Details", "content": "For all generative tasks, the models were trained for 100 epochs with the batch-size of 120 molecule/batch. The learning rate (lr) was 5.0 \u00d7 10\u22125 that was linearly increased (warm-up) from 10\u22128 during the first 1,000 training steps.\nWe pre-trained one model on 40M SMILES for 15 epochs with the batch-size of 512 on single A100 GPU and one model on 190M SMILES for 5 epochs with the effective batch-size of 1,024 (2 \u00d7 512) on 2xA100 GPUs. The warm-up strategy and lr were the same as mentioned above.\nDuring fine-tuning stages, models were trained for 100 epochs on labelled datasets. The batch-size, both for training and validation, was 32 on MoleculeNet benchmark; the training batch-size was 16 for reaction yield prediction. lrmax was 10\u22124 that was warmed up from 10\u22127 during the first 1,000 steps for regression tasks and 100 steps for classification tasks. After warm-up stage, lr decreased by 0.2"}, {"title": "Conclusion", "content": "ChemBFN, a Bayesian flow network framework for chemistry tasks of both generation and prediction, was developed in this work. The new accuracy schedule helped ChemBFN achieve competitive performance of discrete diffusion models and autoregressive models on generating large molecules. We proposed a BFN-style generative pretraining strategy that surpassed existing language-based transformer models on several classification and regression tasks. We believe this work provides a tool that can accelerate researches of both drug designing and filtering and give in helpful information for synthesis planning. However, we still leave gaps between graph-based models in prediction tasks, which we shall keep for the future research."}]}