{"title": "Transforming Game Play: A Comparative Study of DCQN and DTQN Architectures in Reinforcement Learning", "authors": ["William A. Stigall"], "abstract": "In this study we investigate the performance of Deep Q-Networks utilizing Convolutional Neural Networks (CNNs) and Transformer architectures across 3 different Atari Games. The advent of DQNs have significantly advanced Reinforcement Learning, enabling agents to directly learn optimal policy from high dimensional sensory inputs from pixel or RAM data. While CNN based DQNs have been extensively studied and deployed in various domains Transformer based DQNs are relatively unexplored. Our research aims to fill this gap by benchmarking the performance of both DCQNs and DTQNs across the Atari games' Asteroids, SpaceInvaders and Centipede. We find that in the 35-40 million parameter range, the DCQN outperforms the DTQN in speed across both ViT and Projection Architectures, We also find the DCQN outperforms the DTQN in all games except for centipede.", "sections": [{"title": "I. INTRODUCTION", "content": "The field of Reinforcement Learning (RL) has witnessed substantial progress with the integration of deep learning techniques, particularly through the development of Deep Q-Networks (DQNs), which have revolutionized the way that Agents learn from and interact with their environments. Google Deepmind introduced the Deep Q-Network in 2013 as a means to learn control policies from high-dimensional sensory input in the form of Atari games [3]. This Deep Q-Network achieved human performance across 49 Atari games using a CNN architecture introduced in AlexNet in 2012 [4]. Our research utilizes the Arcade Learning Environment (ALE) framework and OpenAI Gym to access the ROMS for Atari 2600 games. OpenAI Gym provides a standardized easy-to-use interface for interacting with these game environments [5], [6]. Although Transformers have been introduced to RL, they typically have not been used in isolation and are often combined with convolutional or recurrent structures [7], [8]. Our study aims to extend this body of work by specifically evaluating the performance of DQNs that employ a Vision Transformer architecture as well as ones in the context of Atari games, thus contributing to the understanding of Transformer-based DQNs without reliance on convolutions or recurrence. We find it difficult to mitigate information loss in Transformers"}, {"title": "II. RELATED WORK", "content": "In this section, we will provide an overview of the existing literature on Deep RL and how it's been used to train agents."}, {"title": "A. Deep Q-Networks (DQNs)", "content": "The beginnings of Deep RL can be largely attributed to the introduction of DQNs in the seminal work by Google Deepmind. The DQN by leveraging convolutional neural networks allowed to learn control policies directly from high-dimensional sensory input and the output of the network being the estimate of future rewards. The preprocessing for the algorithm utilizes frame stacking to provide temporal information for the network. Deepmind's DQN was able to surpass human experts in Beam Rider, Breakout, and Space Invaders [3]. This was built upon in \"Human-level control through deep reinforcement learning\", The DQN was evaluated largely, outperforming both the best linear agents, as well as professional human games testers across 49 games, marking it as the first artificial agent capable of learning and excelling at a diverse set of challenging tasks [9]."}, {"title": "B. Advancements in DQN", "content": "There have been numerous improvements tot he original DQN architecture such as Double Q-Learning, a technique designed to mitigate overestimation bias by employing two separate estimators for action values [10]. Dueling, an architecture that decouples the value and advantage functions improving the efficiency of learning and improving training stability [11]. Additionally, Rainbow networks combining elements such as prioritized experience replay, distributional reinforcement learning, multi-step learning and noisy networks [12]."}, {"title": "C. Transformers in Reinforcement Learning", "content": "Transformers introcued in the seminal work \"Attention is All You Need\", revolutionized the field of natural language processing by offering an architecture that exclusively relies on self-attention mechanisms, omitting the need for recurrent layers. This architecture captures dependencies regardless of distance in the input sequence [13]. While Transformers where first found to make significant gains in natural language processing, they then had exceptional applications in computer vision demonstrating that Transformers could compete with CNNs in image recognition tasks. In reinforcement learning, Transformers were found to be difficult to optimize in RL objectives, the solution to this was introducing a Gated Transformer which could surpass LSTMs on the DMLab-30 benchmarks [2]. The Decision Transformer (Chen, R.T.Q., et al. 2019). Decision Transformers offer a unique perspective on RL tasks by framing them as conditional sequence modeling problems. By conditioning an autoregressive model on desired rewards, past states, and actions, the Decision Transformer effectively generates future actions that optimize returns [8]."}, {"title": "D. Vision Transformers and Patch Embedding", "content": "Vision Transformers (ViTs) represent a significant shift in how images are processed for recognition tasks. Unlike CNNs, which extract local features through convolutional layers, ViTs treat an image as a sequence of patches. These patches are embedded into vectors and processed by the Transformer architecture, enabling the model to capture global dependencies across the image. This approach has shown promising results, with ViTs achieving competitive performance against traditional CNN architectures when trained on large datasets. The key to ViT's effectiveness lies in its patch embedding technique, which converts image patches into a format that the Transformer can process, allowing for a more flexible and efficient way to understand both local and global image features [14]."}, {"title": "E. Reinforcement Learning Techniques", "content": "Experience Replay is a critical technique in the training of Deep Q-Networks (DQNs), enhancing both learning stability and efficiency. Initially introduced by Lin in 1992, the concept involves storing the agent's experiences at each timestep in a replay buffer and then randomly sampling mini-batches from this buffer to perform learning updates. This method addresses two main issues in reinforcement learning: the correlation between consecutive samples and the non-stationary distribution of data[reference]. The use of separate target and policy networks is a technique introduced to stabilize the training of DQNs. This approach, detailed in the work by Mnih et al. in 2015, involves maintaining two neural networks: the policy network, which is updated at every learning step, and the target network, which is updated less frequently The policy network is used to select actions, while the target network is used to generate the target Q-values for the learning updates. This separation helps to mitigate the issue of moving targets in the learning process, where updates to the Q-values could otherwise lead to significant oscillations or divergence in the learning process. By keeping the target Q-values relatively stable between updates, the training process becomes more stable and converges more reliably[reference]. Huber loss is a loss function introduced by Huber in 1964, which has found application in DQNs due to its advantages over other loss functions like the mean squared error (MSE) Huber loss is less sensitive to outliers in data than MSE, as it behaves like MSE for small errors and like mean absolute error (MAE) for large errors. This property makes it particularly useful in the context of DQNs, where the distribution of the temporal-difference (TD) error can have large outliers. By using Huber loss, DQNs can achieve more robust learning, as the loss function prevents large errors from disproportionately affecting the update of the network weights. This contributes to the overall stability and efficiency of the learning process in DQNs[reference]."}, {"title": "III. METHODOLOGY", "content": "Describe your research methodology. Detail the procedures for data collection and analysis."}, {"title": "A. Enviornments", "content": "To streamline the learning process and enhance the efficiency of our RL agents we reduce the action space using the specifications provided by OpenAI's gym library for each game. This mitigates the complexity of the decision-making process. To ensure consistency and fairness across the comparative analysis between the DCQN and DTQN models, we use a fixed seed of 42 for the first environment of the training run, ensuring that all agents are exposed to an identical"}, {"title": "B. DCQN Architecture", "content": "Input Representation: Let $I \\in \\mathbb{R}^{B\\times F\\times H\\times W}$ represent the input batch, where $B$ is the batch size, $F$ is the number of stacked frames (acting as channels), and $H$ and $W$ are the height and width of the frames, respectively. Each frame is an 84x84 grayscale image, so $H = 84$ and $W = 84$. The DCQN model processes $I$ through a series of operations as follows:\nConvolutional Layers: The input is passed through three convolutional layers. Each convolutional operation, denoted by $C_i$, includes a convolution with ReLU activation followed by batch normalization:\n$X\u2081 = ReLU (BN (C\u2081(I)))$\n$X2 = ReLU (BN (C2(X1)))$\n$X3 = ReLU (BN (C3(X2)))$\nwhere $C_i$ represents the i-th convolutional layer. The kernel sizes and strides are implicit in $C_i$ but are not specified here to keep the representation general.\nFlattening: The output of the last convolutional layer $X_3$ is flattened to form a vector $Xflat$.\nFully Connected Layers: The flattened vector $Xflat$ is then passed through three fully connected layers with ReLU activations for the first two layers:\n$X4 = ReLU (F1(Xflat))$\n$X5 = ReLU (F2(X4))$\n$X6 = F3(X5)$\nwhere $Fi$ denotes the i-th fully connected layer operation. The final output $X6$ represents the Q-values for each action, thus $X6 \\in \\mathbb{R}^{Nactions}$, where $Nactions$ is the environment-dependent number of possible actions."}, {"title": "C. DTQN Architecture", "content": "The DTQN model is designed to leverage the Transformer architecture for processing sequences of stacked frames from the environment. It consists of several key components: dimensionality reduction, positional embeddings, a Transformer encoder, and fully connected layers for action value prediction. The model can be formalized as follows:\nInput Representation: Let $X \\in \\mathbb{R}^{B\\times(F.84.84)}$ represent the input batch, where B is the batch size and F is the number of stacked frames, each flattened from an 84x84 grayscale image.\nPatch Embedding: Given our input tensor $X \\in \\mathbb{R}^{B\\times(F.84.84)}$, where:\nThe input is transformed into a sequence of embedded patches $Xp \\in \\mathbb{R}^{B\\times(F\\cdot N)\\times E}$, where $N = (\\frac{84}{16})(\\frac{84}{16})$ is the number of patches per frame. This transformation involves unfolding the input into patches, linearly projecting each patch to an embedding space, and reshaping to match the Transformer's input format.\nwhere $Wproj \\in \\mathbb{R}^{(p^2)\\times E}$ and $bproj$ represent the weights and bias of the linear projection layer, respectively. The resulting tensor, $Xfinal$, with dimensions [B, F, N, E], effectively encodes each patch into an embedding vector of size E, ready for processing by the Transformer encoder; this process is heavily inspired by Vision Transformers (Doistevsky et al., 2021) [14].\nTransformer Encoder and Action Value Prediction: The Transformer encoder with gating mechanisms processes $Xpos$, yielding $Xenc$, which is then flattened and passed through fully connected layers to predict the Q-values for each action. The inclusion of gating mechanisms allows the DTQN model to effectively leverage the Transformer's capabilities for sequence processing while maintaining stability and enhancing learning, enabling it to learn complex policies based on sequences of environmental frames.\nThe gated Transformer encoder processes the sequence $Xpos$, applying self-attention with gating and feed-forward with gating at each layer, yielding $Xenc$:\n$Xenc = GatedTransformerEncoder(Xpos)$\nwhere the GatedTransformerEncoder function represents the application of multiple layers of the GatedTransformerXL-Layer, each consisting of a self-attention mechanism with a gating mechanism followed by a feed-forward network with another gating mechanism. Gating mechanisms are implemented using linear layers that combine the inputs and outputs of the self-attention and feed-forward sublayers, respectively, allowing for controlled information flow and stabilization of the learning process [2].\nOur\nFully Connected Layers: Finally, $Xenc$ is flattened and passed through fully connected layers to predict the Q-values for each action:\n$Xflat = Xenc.view(B, \u22121)$\n$Xfc1 = ReLU(Wfc1Xflat + bfc1)$\n$Xfc2 = ReLU(Wfc2Xfc1 + bfc2)$\n$Xfc3 = ReLU(Wfc3Xfc2 +bfc3)$\n$Q = WoutXfc3 + bout$\nwhere $Wfc1, Wfc2, Wfc3, Wout$ and $bfc1, bfc2, bfc3, bout$ are the weights and biases of the respective fully connected layers, and $Q \\in \\mathbb{R}^{B\\times A}$ represents the predicted Q-values for A actions.\n1) Efficient Attention Model: The Efficient Attention Model processes the input using linear projections, positional embeddings, a Transformer encoder, and attention pooling to output action values. The model architecture is formalized as follows:\nInput Projection: Let $X \\in \\mathbb{R}^{B\\times D}$ be the input batch, where B is the batch size and D is the input dimension. The input is first projected to a higher-dimensional embedding space:\n$Xproj = XWproj + bproj$"}, {"title": "D. Training", "content": "Let M represent the memory buffer of size N, where an experience et = (St,at,rt+1, St+1) is stored at time t. Sampling a minibatch of size k from M can be mathematically represented by $B = {e1, e2, ..., ek} \\subseteq M$.\nFor the e-greedy strategy, the action at at time t is chosen as follows:\n$at =\n{\nrandom action from A',\nwith probability \u0454\narg maxa\u2208 A' Q(st, a),\nwith probability 1 \u2013 \u0454$\nwhere e is the exploration rate, and $Q(st, a)$ represents the action-value function estimate for state st and action a.\nThe epsilon decay process is mathematically represented as:\n$\u20ac \u2190 max(fend, e decay_rate)$\nwhere decay_rate is a factor less than 1 used to decrease e after each episode.\n1) Q-value Update: Following the selection of an action at, the Q-values are updated using the Bellman equation:\n$Q(St, at) Q(St, at) + a (rt+1 + y max Q(st+1, a')\n$\\alpha$ (rt+1+ymax Q(st+1, a')\n= Q(st, at) + \u03b1\u2206Q\nwhere a is the learning rate, y is the discount factor, a' represents all possible actions from the state st+1, and AQ is the temporal-difference error."}, {"title": "E. Experimental Setup", "content": "The training of both DCQN and DTQN models follows a similar protocol, with specific adaptations to each model's architecture This section will cover all the different configurations for each of the 6 individually trained Agents."}, {"title": "IV. RESULTS", "content": "In this section, we will discuss the results of our RL Agents after being trained over 10000 episodes, in this section we address not only the results of the evaluations but also the observations from training.\nWe see that the Convolutional architecture outperforms the Transformer architectures across all games but Centipede in evaluation. However, as evidenced from 3 This may be an outlier since during training the DCQN trended higher towards the later episodes. All version of the DTQN were slower than the DCQN except for Asteroids, in evaluation this is because the Agent is quickly dying and is not related to computational efficiency. Without pretraining the Transformer is able to detect objects directly overhead the player, this results in high performance playing centipede, as well as shooting straight upwards in Space Invaders. In Centipede, in which both models utilize Huber Loss, we see that the Transformer is flat across all benchmarks for the entire duration of training, and we notice significant increase in the reward of the DCQN after the loss function spikes and then decreases, we can assume that these models have reached a plateau for traditional Q-Learning based on the behavior and these measures."}, {"title": "A. Model Behavior", "content": "1) Centipede: The DTQN Agent learned quickly in Centipede that an easy way to maximize points was to sit in one area the whole time and continuously shoot. Since the Centipede is guaranteed to move side to side across the screen shooting quickly upwards allowed for large sections and points to be accumulated at one time. Not moving is also an advantage because the Spider is more likely to cross the firing path than it is for the Agent to collide with it, resulting in more chances to reap rewards. This allows it to outperform the DCQN over 10000 episodes in this game. The DCQN Agent learns to dodge occasionally, performing best when the Centipede consists of a larger number of pixels (less of it has been destroyed). However, the DCQN Agent also shows signs of the same camping strategy.\n2) Space Invaders: There is a significant difference in performance between the two agents visually in Space Invaders, the DTQN does not move at any point of time, this is problematic because the Agent is stuck at 285 points, perhaps using a very low epsilon would allow for the Agent to perform better during the deterministic stages. The DCQN learns slightly how to dodge towards the end of the episodes which allows for it to perform on average much better than the DTQN.\n3) Asteroids: The DCQN learns the process normally, learning to shoot the moving objectts, the DTQN has not learned anything over 10000 episodes even with the loss significantly increasing towards the end. We believe we can attribute this to the feature loss during linear projection. We also believe that this Agent could perform better if we established a penalty on repeated actions."}, {"title": "V. DISCUSSION", "content": "This study presents a comparative analysis of Deep Convolutional Q-Networks (DCQN) and Deep Transformer Q-Networks (DTQN) across three different Atari games, highlighting the strengths and limitations of each model in the context of reinforcement learning. The findings reveal significant differences in performance and learning behaviors between the two architectures, offering insights into their respective efficiencies and strategic capabilities.\nThe results indicate that DCQN generally outperforms DTQN in terms of both speed and average reward across most games, with the notable exception of Centipede, where DTQN demonstrates superior performance. This exception can be attributed to DTQN's ability to capitalize on specific game dynamics, such as the predictable movement patterns of the Centipede, which align well with the model's architectural strengths in handling sequential data.\nHowever, the DTQN's performance in other games, particularly Asteroids, was underwhelming. The lack of learning progress in Asteroids suggests that DTQN may suffer from significant feature loss during the linear projection phase, which inhibits its ability to effectively process and respond to dynamic game environments. This feature loss might be mitigated by adjusting the model's architecture or training protocol, such as introducing penalties for repeated actions to encourage more diverse strategic exploration.\nMoreover, the DCQN's ability to adapt and improve performance over time, as seen in Space Invaders, underscores its utility in scenarios where agents must learn to navigate and respond to evolving threats dynamically. The contrast in performance between DCQN and DTQN in this game also highlights the potential challenges DTQN faces in environments requiring rapid and complex spatial decision-making.\nThese observations suggest that while Transformer-based models like DTQN hold promise for enhancing sequential decision-making in games, their current implementations may need further refinement to fully exploit their capabilities across a broader range of scenarios. Meanwhile, DCQN remains a robust and reliable choice for many typical game environments, balancing performance with computational efficiency. For the most part this refinement involves pre-training or simply using larger models."}, {"title": "A. Limitations", "content": "1) Patch Embedding Size: Our Transformer implementation uses the same 16x16 patch size as ViT[]. However, in the original paper, the 16x16 patch was designed for 224x224 images; in this study, we use a 16x16 patch on an 84x84 image which is not proportional segmentation of information. Additionally, most implementations like Deit and other lightweight models rely on ViT Feature Extractor to prepossess the images since it would be computationally intensive to do so inside of the model the reason we do not use a 6x6 patch size, which would line up more with ViT in terms of proportionality is that it increases the param size by approximately 700 percent.\n2) Model Size: In our study we only tested the DQN and DTQN models in the 36M-39M parameter range, it should be considered that one model might perform differently with a different level of complexity even with the same architecture.In other studies\n3) Model Architecture: Our proposed DTQN is one of many possible implementations of Transformers in RL, based on the existing literature, different implementations lead to different findings. For our specific architecture replacing the patch embedding and using Convolutions to extract features from the model before applying the positional embedding leads to a 300 percent reduction in the number of parameters. As a result of this the Embed Size of the Transformer can be doubled to get back to our 30 million parameter class model."}, {"title": "APPENDIX A SEQUENTIAL REPLAY BUFFER", "content": "The Sequential Replay Buffer is a modified version of the standard experience replay buffer, designed to handle sequences of observations, actions, rewards, and terminal states. This buffer is particularly useful for training agents that rely on temporal information, such as recurrent neural networks or agents that operate on sequences of observations.\nThe key components of the Sequential Replay Buffer are as follows:\nA. Initialization\nThe Sequential Replay Buffer is initialized with the following parameters:\nC = capacity of the replay buffer\nS = shape of the state/observation\nBbatch size\nL = sequence length\nThe buffer is implemented using a LazyMemmapStorage to efficiently store the experiences, and a TensorDictReplayBuffer to manage the sampling and retrieval of experiences.\nB. Adding Experiences\nTo add a sequence of experiences to the buffer, the add method is used:\nadd(s, a, r, s', d)\nwhere:\ns = sequence of states/observations \u2208 RCXS\na = sequence of actions \u2208 ZC\u00d71\nr = sequence of rewards \u2208 RC\u00d71\ns' = sequence of next states/observations \u2208 RCXS\nd = sequence of terminal states \u2208 {0,1}\u00a3\u00d71\nThe method ensures that the input tensors have the correct shape and stores the sequence in the replay buffer.\nC. Sampling Experiences\nTo sample a batch of sequences from the replay buffer, the sample method is used:\nsample() \u2192 {s, a, r, s', d}\nThe method returns a dictionary containing the sampled sequences of states, actions, rewards, next states, and terminal states.\nD. Implementation Details\nThe Sequential Replay Buffer is implemented using the LazyMemmapStorage and TensorDictReplayBuffer from the torchrl library. The storage keys are initialized with the appropriate shapes to accommodate the sequence-level data. The add and sample methods are implemented to handle the sequence-level data accordingly."}, {"title": "APPENDIX B CONVOLUTIONAL TRANSFORMER MODEL", "content": "The Convolutional Transformer Model integrates convolutional features into a Transformer architecture. The model can be expressed as follows:\na) Convolutional Feature Extraction:: Let $I \\in \\mathbb{R}^{B\\times C\\times H\\times W}$ represent the input image batch. The convolutional layers extract features:\n$F = ConvLayers(I)$\nwhere $F \\in \\mathbb{R}^{B\\times Fdim\\times H'\\times W'}$ and $Fdim$ is the number of feature dimensions after the convolutional layers.\nb) Feature Flattening and Positional Embedding:: The features are flattened and combined with positional embeddings:\n$Fflat = F.reshape(B, Fdim \\times H' \\times W') + P$\nwhere $P \\in \\mathbb{R}^{1\\times(Fdim\\times H'\\times W')}$ are the positional embeddings.\nc) Transformer Encoder:: The flattened features are then processed by a Transformer encoder:\n$Fenc = Gated Transformer XL(Fflat)$\nsimilar to the Efficient Attention Model."}, {"title": "APPENDIX C GATED TRANSFORMER-XL LAYER MATHEMATICAL DESCRIPTION", "content": "The Gated Transformer-XL architecture enhances the standard Transformer by integrating gating mechanisms at each layer to control the flow of information and improve learning dynamics. The detailed operations within a Gated Transformer-XL layer are as follows:\nA. Layer Components\nEach GatedTransformerXLLayer comprises the following components:\n\u2022\n\u2022\n\u2022\n\u2022\nA multi-head self-attention mechanism.\nTwo feed-forward neural networks.\nLayer normalization applied before each sub-layer.\nDropout for regularization.\nGating mechanisms to combine inputs from different sub-layers.\nB. Mathematical Formulation\nGiven an input sequence $X \\in \\mathbb{R}^{n\\times d}$, where n is the sequence length and d is the model dimension, the operations are defined as follows:\n1) Self-Attention with Gating: The self-attention mechanism computes the output as:\n$Attn(Q, K, V) = softmax (\\frac{QK^T}{\\sqrt{dk}})V$\nwhere Q, K, and V are the queries, keys, and values respectively, all derived from the layer input X. The output of the attention layer is then gated as follows:\n$G\u2081 = \u03c3(Wg [X, Attn(Q, K, V)] \u2013 + bg)$\nwhere $Wg \\in \\mathbb{R}^{2d\\times d}$ and $bg \\in \\mathbb{R}^{d}$ are the gating parameters, and o is the sigmoid activation function. The final output after the gating and residual connection is:\n$Y = X + Dropout(G1)$\n2) Feed-Forward Network with Gating: The feed-forward network consists of two linear transformations with a ReLU activation in between:\n$F(X) = ReLU(W1X + b1)W2 + b2$\nwhere $W\u2081 \\in \\mathbb{R}^{d\\times f}$, $b\u2081 \\in \\mathbb{R}^{f}$, $W2 \\in \\mathbb{R}^{f\\times d}$, and $b\u2082 \\in \\mathbb{R}^{d}$ are the network parameters, and f is the dimensionality of the feed-forward layer. The output of the feed-forward layer is gated:\n$G2 = \u03c3(W[Y, F(Y)] + b)$"}]}