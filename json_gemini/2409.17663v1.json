{"title": "Explanation Bottleneck Models", "authors": ["Shin'ya Yamaguchi", "Kosuke Nishida"], "abstract": "Recent concept-based interpretable models have succeeded in providing meaningful explanations by pre-defined concept sets. However, the dependency on the pre-defined concepts restricts the application because of the limited number of concepts for explanations. This paper proposes a novel interpretable deep neural network called explanation bottleneck models (XBMs). XBMs generate a text explanation from the input without pre-defined concepts and then predict a final task prediction based on the generated explanation by leveraging pre-trained vision-language encoder-decoder models. To achieve both the target task performance and the explanation quality, we train XBMs through the target task loss with the regularization penalizing the explanation decoder via the distillation from the frozen pre-trained decoder. Our experiments, including a comparison to state-of-the-art concept bottleneck models, confirm that XBMs provide accurate and fluent natural language explanations without pre-defined concept sets. Code will be available at https://github.com/yshinya6/xbm/.", "sections": [{"title": "Introduction", "content": "Although deep learning models can achieve remarkable performance on many applications, they are black-box, i.e., their output predictions are not interpretable for humans. Introducing concept bottleneck models (CBMs, Koh et al. (2020)) is a promising approach to interpreting the output of deep models. In contrast to black-box models that directly predict output labels from input in an end-to-end fashion, CBMs first predict concept labels from input and then predict final target class labels from the predicted concepts. Since the predicted concepts represent semantic input ingredients, this two-staged prediction enables users to know the reasons for the final target label predictions and interactively intervene in the decision-making process for critical applications such as healthcare (Chauhan et al. 2023).\nHowever, the existing CBMs depend on the fixed pre-defined concept sets to predict final labels. In other words, they can not provide interpretability to any other than the pre-defined concepts. We argue that this limitation presents a fundamental challenge for CBMs in achieving interpretable deep models. Although recent CBM variants leveraging pre-trained large language models (Yuksekgonul, Wang, and Zou 2023; Oikarinen et al. 2023) enable to express concepts of arbitrary target classes, the interpretability is still restricted to a fixed and small number of concepts. This is because a large number of concept labels are difficult to learn due to their long-tail distribution and are less interpretable by the limitation of human perception (Ramaswamy et al. 2023). In fact, the prior works restrict the number of concepts by filtering with the similarity between concepts and training images to maintain the performance and interpretability (Oikarinen et al. 2023; Yang et al. 2023). Therefore, as long as they depend on pre-defined concepts, CBMs are restricted in the number of interpretable concepts and are insufficient to explain the output of deep models.\nThis paper tackles a research problem where we do not assume pre-defined concept sets for constructing interpretable deep neural networks. To this end, we propose a novel family of interpretable models called explanation bottleneck models (XBMs), which leverage pre-trained multi-modal encoder-decoder models that can generate text descriptions from input data (e.g., BLIP (Li et al. 2022, 2023)). Leveraging pre-trained multi-modal encoder-decoder enables capturing concepts that actually appeared in the input beyond pre-defined concept sets. Our key idea is to decode concepts as text explanations from input and then predict the final label with a classifier that takes the decoded explanations (Fig. 1). In contrast to CBMs, which make predictions based on pre-defined concepts, XBMs make predictions based on concepts actually appeared in the input data through the decoded explanations and can provide an intuitive interpretation of the final prediction tied to the input. Through end-to-end training, XBMs aim to generate explanations focusing on the textual features for solving the target task.\nA major challenge for XBMs is forgetting the text generation capability during training on target tasks. Since target datasets usually lack ground-truth text labels, it is challeng-"}, {"title": "Explanation Bottleneck Models", "content": "This section introduces the principle of explanation bottleneck models (XBMs). XBMs are interpretable deep learning models that predict a final label from the generated explanation text from XBMs themselves. Since the predicted final labels are based on the generated explanation of input images, we can naturally interpret the explanation as the reason for the prediction of XBMs. Figure 2 illustrates the overview of training an XBM. An XBM consists of a visual encoder \\(h_\\psi\\), an explanation decoder \\(g_\\phi\\), and a classifier \\(f_\\theta\\) for predicting final target labels. Among them, \\(h_\\psi\\) and \\(g_\\phi\\) are initialized by an arbitrary pre-trained multi-modal encoder-decoder like BLIP (Li et al. 2022). \\(f_\\theta\\) is a multi-modal classifier built on a transformer that takes the generated explanations as input and conditions the cross-attention layers with image embeddings; this design is inspired by hybrid post-hoc CBMs (Yuksekgonul, Wang, and Zou 2023) that uses input embeddings to complement missing concepts not in the predicted concepts. We also confirm the practicality when using a text classifier in Section 3.4. In this section, we mainly describe XBMs with a multi-modal classifier. XBMs are trained by the target classification loss in an end-to-end manner. Since na\u00efve training leads to collapse in generated text explanation, we avoid the collapse by explanation distillation. Explanation distillation penalizes the explanation decoder with a reference text generated from a frozen pre-trained text decoder \\(g_\\rho\\) to prevent the decoders from forgetting the text generation capability."}, {"content": "We consider a \\(K\\)-class image classification task as the target task. We train neural network models \\(h_\\psi : \\mathcal{X} \\rightarrow \\mathbb{R}^{d_\\mathcal{X}}\\, g_\\phi : \\mathbb{R}^{d_\\mathcal{X}} \\rightarrow \\mathcal{E}\\), and \\(f_\\theta : (\\mathbb{R}^{d_\\mathcal{X}},\\mathcal{E}) \\rightarrow \\mathcal{Y}\\) on a labeled target dataset \\(\\mathcal{D} = \\{(x^i, y^i) \\in \\mathcal{X} \\times \\mathcal{Y}\\}_{i=1}^N\\), where \\(\\mathcal{X}, \\mathcal{E}\\), and \\(\\mathcal{Y}\\) are the input, text explanation, and output label spaces, respectively. The text explanation space consists of token sequences of the length \\(L\\) with token vocabulary \\(\\mathcal{V}\\), i.e., \\(\\mathcal{E} = \\mathcal{V}^L\\). \\(h_\\psi\\) is a vision encoder, which embeds an input \\(x\\) into \\(d_\\mathcal{X}\\)"}, {"title": "Problem Setting", "content": "dimensional space, \\(g_\\phi\\) is an auto-regressive text decoder that generates a text explanation \\(e \\in \\mathcal{E}\\) from an input embedding \\(h_\\psi(x)\\), and \\(f_\\theta\\) is a classifier that predicts a final target task label \\(y\\). We assume that \\(h_\\psi\\) and \\(g_\\phi\\) are initialized by pre-trained multi-modal model's parameters \\(\\Theta_\\rho\\) and \\(\\Phi_\\rho\\), which are pre-trained on large-scale text-image paired datasets with an existing method such as BLIP (Li et al. 2022) and LLaVA (Liu et al. 2023). Note that we do not assume ground truth text explanation set \\(\\{e^i\\}_{i=1}^N\\) in \\(\\mathcal{D}\\) for training \\(g_\\phi\\).\nThis setting is similar to that of concept bottleneck models (CBMs, Koh et al. (2020)), where a model predicts a final label \\(y\\) from a set of concepts \\(\\{c_i \\in \\mathcal{C}\\}_{i=1}^I\\) decoded from input \\(x\\) instead of using \\(e\\). The major difference is in the assumption of pre-defined concept sets: our setting does not explicitly specify the words and phrases for the explanations, whereas CBMs explain the model's output based on the words and phrases in a pre-defined concept set \\(\\{c_i\\}_{i=1}^I\\)."}, {"title": "Objective Function", "content": "XBMs aim to achieve high target classification accuracy while providing interpretable explanations of the predictions. To this end, XBMs solve an optimization problem with a regularization term defined by the following objective function.\n\\[\\min_\\{\\theta,\\phi,\\psi\\} \\mathcal{L}_{cls}(\\theta, \\phi, \\psi) + \\lambda \\mathcal{R}_{int}(\\phi, \\psi),\\tag{1}\\]\n\\[\\mathcal{L}_{cls}(\\theta, \\phi, \\psi) = \\mathbb{E}_{(x,y) \\in \\mathcal{D}} \\ell_{CE}(f_\\theta \\circ g_\\phi \\circ h_\\psi(x), y),\\tag{2}\\]\nwhere \\(\\mathcal{R}_{int}(\\cdot)\\) is a regularization term that guarantees the fluency of the explanations generated from \\(g_\\phi, \\lambda\\) is a hyperparameter for balancing \\(\\mathcal{L}_{cls}\\) and \\(\\mathcal{R}_{int}\\), and \\(\\ell_{ce}\\) is cross-entropy loss. Through this objective, the text decoder \\(g_\\phi\\) is trained to focus on the textual features that are useful for minimizing \\(\\mathcal{L}_{cls}\\) while keeping the interpretability by \\(\\mathcal{R}_{int}\\). We found that \\(g_\\phi\\) easily collapses their output without \\(\\mathcal{R}_{int}\\). Thus, the design of \\(\\mathcal{R}_{int}\\) is crucial for training XBMs. However, since we often do not have the ground truth explanation sets in a real-world target dataset \\(\\mathcal{D}\\), we can not directly penalize \\(g_\\phi\\) with supervised losses as \\(\\mathcal{R}_{int}\\). To"}, {"title": "Explanation Distillation", "content": "XBMs utilize pre-trained multi-modal models as the initial parameters of the text (explanation) decoder \\(g_\\phi\\). As an auto-regressive sequence model, the pre-trained text decoder \\(g_\\rho\\) can learn a conditional distribution \\(q(e|x)\\) as\n\\[q(e|x) = \\prod_{l=1}^L q(e_l | x, e_{<l}),\\tag{3}\\]\nwhere \\(L\\) is the maximum token length, \\(e_l\\) is the \\(l\\)-th token, and \\(e_{<l}\\) is the text sequence before \\(e_l\\). Since \\(g_\\rho\\) is trained on large-scale text-image pairs, \\(q(e|x)\\) is expected to be able to generate a token sequence describing important information of various inputs \\(x\\).\nOur key idea is to leverage \\(q(e|x)\\) as the reference distribution for maintaining the interpretability of the generated explanation \\(\\hat{e} \\sim p_\\phi(e|x)\\), where \\(p_\\phi(e|x)\\) is the model distribution of \\(g_\\phi\\). If \\(p_\\phi(e|x)\\) and \\(q(e|x)\\) are sufficiently close, it can be guaranteed that the interpretability of the sequence generated by \\(p(e|x)\\) approximate to that by \\(q(e|x)\\). Concretely, we compute the KL divergence between \\(p_\\phi(e|x)\\) and \\(q(e|x)\\) as the regularization term \\(\\mathcal{R}_{int}\\) in Eq. (1).\n\\[\\begin{aligned}\\mathcal{R}_{int}(\\phi, \\psi) &= D_{KL}(q || p_\\phi) = \\mathbb{E}_{e \\sim q(e|x)} \\log \\frac{q(e|x)}{p_\\phi(e|x)}\\\n&= \\mathbb{E}_{e \\sim q(e|x)} \\log q(e|x) - \\log p_\\phi(e|x).\\tag{4}\\end{aligned}\\]\nHowever, \\(D_{KL}(q || p_\\phi)\\) is computationally intractable because it requires multiple sequential sampling over \\(\\mathcal{E} = \\mathcal{V}^L\\) from \\(q(e|x)\\) and the back-propagation through all sampling processes of \\(p_\\phi(e|x, e_{<l})\\). To approximate Eq. (4), we focus on the connection to knowledge distillation (Hinton, Vinyals, and Dean 2015). That is, minimizing Eq. (4) can be seen as a knowledge distillation from \\(g_\\rho\\) to \\(g_\\phi\\). In such a sense, the approximation is\n\\[\\mathcal{R}_{int}(\\phi, \\psi) \\approx - \\sum_{e \\in \\mathcal{E}} I_{e=e_p} \\log p_\\phi(e|x) = - \\log p_\\phi(e = e_p | x),\\tag{5}\\]\nwhere \\(e_p\\) is the sample from \\(q(e|x)\\) and \\(I\\) is the indicator function returning one when \\(e\\) equals to \\(e_p\\) or returning zero otherwise; we omit the constant terms from the approximation for the simplicity. As a concrete procedure, we first generate \\(e_p\\) from \\(g_\\rho\\) and then penalize the output logits of \\(g_\\phi\\) through the cross-entropy loss for each output token in a next token prediction task. This approximation technique is well-known as sequence-level knowledge distillation (Kim and Rush 2016) in the field of neural machine translation, and it works well in the knowledge distillation of auto-regressive sequence models. Sequence-level knowledge distillation corresponds to matching the modes of \\(p\\) and \\(q\\) and omits to transfer the uncertainty represented by the entropy \\(H(q)\\) (Kim and Rush 2016). Nevertheless, we consider that this is sufficient for XBMs because the goal of XBMs is to provide interpretable explanations for target task predictions, not to replicate the pre-trained models perfectly. We call the regularization with Eq. (5) explanation distillation, and introduce it in training XBMs to maintain the text generation capability."}, {"title": "Algorithm", "content": "Training We show the training procedure in Algorithm 1. In the training loop, we first generate the reference and predicted explanations \\(e_p\\) and \\(\\hat{e}\\) by generate(.) and g_sampling(.), respectively (line 4 and 5). To approximate the mode of \\(q(e|x)\\) and ensure the quality as the reference, we generate \\(e_p\\) from frozen \\(g_\\rho\\) by beam search following the previous work (Kim and Rush 2016). For sampling \\(\\hat{e}\\), we introduce the Gumbel-softmax trick (Jang, Gu, and Poole 2017) to retain the computation graph for the end-to-end training with back-propagation. The \\(l\\)-th token can be approximately sampled by\n\\[e_l = softmax((log(g_\\phi(h_\\psi(x))) + g) / \\tau),\\tag{6}\\]\nwhere \\(g = \\{g_1, \\dots, g_{|\\mathcal{V}|}\\} \\) is a vector of length \\(|\\mathcal{V}|\\) where each element is sampled from Gumbel(0, 1) and \\(\\tau\\) is the temperature parameter. Intuitively, the temperature \\(\\tau\\) controls the diversity of the token outputs from \\(g_\\phi\\); larger \\(\\tau\\) stimulates more diverse outputs. To obtain diverse and accurate tokens for describing input, we apply exponential annealing to the temperature values according to the training steps, i.e., \\(\\tau^{(i+1)} = \\tau^{(0)} \\exp (-r_a i)\\), where \\(i\\) and \\(r_a\\) are training step and annealing rate. This allows XBMs to focus on the diversity of the output tokens in the early training steps and on the quality in the later steps. We evaluate this design choice in Appendix E.1. After sampling \\(e_p\\) and \\(\\hat{e}\\), we update all trainable parameters according to the objective function Eq. (1).\nInference For the inference of test input \\(x\\), we generate \\(\\hat{e}\\) by beam search instead of the Gumbel-softmax trick, i.e., \\(\\hat{e} \\leftarrow generate(g_\\phi, h_\\psi(x))\\). Finally, we return the target label prediction \\(\\hat{y} \\leftarrow f_\\theta(h_\\psi(x), \\hat{e})\\) and the explanation \\(\\hat{e}\\) to users. Optionally, XBMs provide the other styles of explanation in addition to \\(\\hat{e}\\) (Fig. 3). A concept phrase \\(c\\) is a noun phrase that compose \\(\\hat{e}\\), which can be extracted by natural language parser automatically (Feng et al. 2022). Similar to the concept outputs of CBMs, \\(c\\) provides contributions of noun phrases in text explanations for the prediction. For example, if the"}, {"title": "Experiment", "content": "We evaluate XBMs on multiple visual classification tasks and pre-training models. We conduct qualitative and quantitative experiments on the explanation outputs of XBMs to evaluate the target performance and the interpretability. We also provide a more detailed analysis, including varying hyperparameters \\(\\lambda, \\tau\\) and comparing explanation distillation with an alternative regularization loss in Appendix E."}, {"title": "Setting", "content": "Implementation Our basic implementation of XBMs is based on BLIP (Li et al. 2022) because of its simplicity; we denote this model as XBM-BLIP. That is, as the visual encoder \\(h_\\psi\\), we used the ViT-B/32 (Dosovitskiy et al. 2021). For the classifier \\(f_\\theta\\), we used a BERT-base transformer (Devlin et al. 2019); we input \\(h(x)\\) into the cross-attention layers when using a multi-modal classifier inspired by BLIP (Li et al. 2022). We initialized \\(h_\\psi\\) and \\(f_\\theta\\) by the BLIP model pre-trained on image captioning tasks in the official repository\u00b9. We also report the results using larger pre-trained multi-modal models of LLaVA (Liu et al. 2023). We used v1.5 and v1.6 of LLaVA with multiple language model backbones (LLaMA2-7B (Touvron et al. 2023), Vicuna-7B (Chiang et al. 2023), and Mistral-7B (Jiang et al. 2023)); we denote these models as XBM-LLaVA. We provide detailed training settings in Appendix A.\nBaselines We compare XBMs to black-box and interpretable baselines in performance and interpretability. Fine-tuned BLIP-ViT is the black-box baseline, which directly optimizes the visual encoder of BLIP via fine-tuning. Label-free CBM (Oikarinen et al. 2023) is a state-of-the-art concept"}, {"title": "Design Evaluation of XBMs", "content": "Quantitative Evaluation Table 1 demonstrates the quantitative performance and interpretability of XBM-BLIP on the four target datasets. For the target performance, our XBMs outperformed the Label-free CBM baselines and achieved competitive performance with the black-box baseline in the test accuracy. In particular, XBM achieved high performance on datasets where label-free CBM did not perform well (i.e., Aircraft and Car). This can be caused by insufficient pre-defined concepts due to the limited vocabulary in ConceptNet and GPT-3 about describing objects in these datasets, whereas XBMs promote multi-modal understanding by training the explanation decoder to describe arbitrary objects useful for the target dataset with unlimited vocabulary. For the interpretability, XBMs outperformed CBMs in CLIP-Score. This indicates that the explanations from XBMs are more factual to the input images than the concept outputs of CBMs, which are in pre-defined concept sets.\nFurthermore, the ablation study in the bottom rows of Table 1 shows that the objective function in Eq. (1) works effectively as we expected. Compared to the frozen BLIP baselines, which simply apply fixed pre-trained BLIP to generate text captions, our XBM significantly improved all of the test accuracy, CLIP-Score, and GPT-2 Perplexity. This suggests that optimizing text decoders with respect to target tasks guides the generated explanation to be informative and target-related for solving the task. We also confirm that the regularization term \\(\\mathcal{R}_{int}\\) by explanation distillation (Eq. (5)) is crucial to generate meaningful explanation; XBM w/o \\(\\mathcal{R}_{int}\\) catastrophically degraded CLIP-Score and GPT-2 Perplexity."}, {"title": "Evaluations of Cross-Attention Heatmap", "content": "The cross-attention heatmap explanation of XBMs visualizes the local input space regions correlated to the text explanation in the classifier. To assess the validity of XBMs on improving multi-modal understanding, we evaluate the generated heatmaps on the ImageNet segmentation task by following Chefer, Gur, and Wolf (2021) and Gandelsman, Efros, and Steinhardt (2024). That is, we generate the heatmaps on the test set of ImageNet Segmentation (Guillaumin, K\u00fcttel, and Ferrari 2014) and compute the pixel accuracy, mean IoU (mIoU), and mean average precision (mAP) with the ground truth segmentation masks. Through this evaluation, we can evaluate how heatmaps cover the object of target classes in the pixel spaces. Table 4 shows the results. Compared to the frozen BLIP, XBM-BLIP improved all of the segmentation metrics. This means that the training objective of XBMs encourages the multi-modal understanding of target class objects on the models. In Appendix D, we further compare the XBM's heat maps with existing attribution methods, such as GradCAM (Selvaraju et al. 2017)."}, {"title": "Reliability Evaluation via Human Intervention", "content": "CBMs allow the debugging of the model behavior through human intervention in the predicted concepts (Koh et al. 2020). Similarly, we can debug the behavior of XBMs by intervening in the generated explanations. Here, we show examples of an intervention in which all explanations are replaced to check the effect of the explanation quality on the final classification results. At inference, we replace the generated explanations from the explanation decoder with modified explanations. We tested two types of interventions: (i) ran-"}, {"title": "XBMs with Large Vision-Language Models", "content": "Here, we evaluate the scalability and practicality of XBMs by combining them with larger vision-language models than BLIP. Instead of BLIP, we used the LLaVA models with various language model backbones (Liu et al. 2023). Table 3 shows that leveraging the high-performance vision-language model in XBMs yields better performance and interpretability scores, suggesting that the XBM's objective function can enhance the multi-modal understanding ability even if using the large vision-language models pre-trained on massive image-text pairs. This emphasizes the flexibility of XBM, consisting of arbitrary vision-language models."}, {"title": "XBMs with Text Classifier", "content": "Table 3 also evaluates XBMs with a text classifier \\(f_\\theta(\\hat{e})\\), which relies only on text information for the final predictions. Although XBM-BLIP with \\(f_\\theta(\\hat{e})\\) drops the performance from one with a multi-modal classifier \\(f_\\theta(h_\\psi(x), \\hat{e})\\), switching the backbone from BLIP to LLaVA (Liu et al. 2023) resolves the performance gap. This indicates that more sophisticated vision-language models make XBMs generate informative text explanations, and they can achieve practical performance even when not using input features \\(h_\\psi(x)\\). Appendix C further shows the results on the other datasets."}, {"title": "Related Work", "content": "The main research directions of the interpretability of black-box deep neural networks are briefly divided into attribution-based and concept-based methods. Attribution-based methods such as CAM (Zhou et al. 2016) and GradCAM (Selvaraju et al. 2017) generate a localization map representing important regions for the model predictions for specific classes. However, since the maps generated by attribution-based methods do not have information other than that they responded to the predictions, they are less interpretable regarding what semantic input features contribute to the output. In contrast to these methods, our XBMs can generate semantically interpretable heatmaps via cross-attention between image and text explanations, which can be decomposed at the level of noun phrases.\nOn the other hand, concept-based methods such as TCAV (Kim et al. 2018) and CBMs (Koh et al. 2020) compute contribution scores for pre-defined concepts on intermediate outputs of models. Among them, CBMs are highly relevant to our XBMs since both have interpretable intermediate layers in models. CBMs predict concept labels and then predict final class labels from the predicted concepts. The original CBMS have the challenge of requiring human annotations of concept labels (Zarlenga et al. 2022; Moayeri et al. 2023; Xu et al. 2024). Post-hoc CBMs (Yuksekgonul, Wang, and Zou 2023) and Label-free CBMs (Oikarinen et al. 2023) addressed this challenge by automatically collecting concepts corresponding to target task labels by querying large language models (e.g., GPT-3 (Brown et al. 2020b)) or existing concept banks (e.g., ConceptNet (Speer, Chin, and Havasi 2017)). However, CBMs' explanations are still restricted to pre-defined concepts, and they are not necessarily reliable because CBMs often predict the concepts without mapping to corresponding input regions (Huang et al. 2024). On the contrary, our XBMS directly generate natural language explanations to interpret the model outputs without pre-defined concepts.\nSimilar to our work, a few works attempted to generate linguistic explanations for target classification models (Hendricks et al. 2016; Nishida, Nishida, and Nishioka 2022). However, these methods require ground truth text explanations for training models, which are expensive and restrict applications. Our XBMs address this limitation by learning explanation generation by the classification loss and explanation distillation using a pre-trained text decoder."}, {"title": "Conclusion", "content": "In this paper, we presented a novel interpretable deep neural networks called explanation bottleneck models (XBMs). By leveraging pre-trained vision-language models, XBMs generate explanations corresponding to input and output in the forms of natural language description, concept phrases with contribution scores, and cross-attention heatmaps on input spaces. To ensure both the target task performance and the explanation quality, XBMs are optimized by the target task loss with explanation distillation, which penalizes the divergence between the distributions of the training and pre-trained text decoders. Experiments show that XBMs can achieve both high target task performance and accurate and fluent explanations; they achieve competitive performance to black-box baselines and largely outperform CBMs in target test accuracy. Furthermore, we found that training of XBMs can enhance the multi-modal understanding capability of backbone vision-language models even when using large vision-language models pre-trained on massive image-text pairs. We believe that this work introduces a new perspective on natural language explanations and advances the study of interpretable deep models to the next paradigm."}, {"title": "Training", "content": "We trained the models by the AdamW (Loshchilov and Hutter 2019) optimizer with the initial learning rate of 3.0\u00d710-5 that decayed by cosine annealing. The training epochs were 100 on the Aircraft/Bird/Car datasets and 5 on the ImageNet dataset. We used mini-batch sizes of 32. The input samples were resized into resolutions of 384 \u00d7 384 for XBM-BLIP and 336 \u00d7 336 for XBM-LLaVA according to the setting of vision encoders. We used \\(\\lambda\\) of 0.1 and \\(\\tau\\) of 10 with exponential annealing by \\(r_a\\) = 1.0 \u00d7 10\u20134 if not otherwise noted; we discuss the effect of \\(\\lambda\\) and \\(\\tau\\) in Section E. For the experiments on XBM-LLaVA, we fine-tuned the LoRA adapter parameters (Hu et al. 2022) of backbone language models instead of the entire parameters. We selected the final model by checking the validation accuracy for each epoch. We implemented the training and evaluation with PyTorch-1.13. We ran the experiments three times on a 24-core Intel Xeon CPU with eight NVIDIA A100 GPUs with 80GB VRAM and recorded the average evaluated on the final models; we omit the standard deviations for saving spaces, but we have confirmed the statistical significance of our method with a p-value < 0.05 toward baselines."}, {"title": "Datasets", "content": "ImageNet (Russakovsky et al. 2015): We downloaded ImageNet from the official site https://www.image-net.org/. ImageNet is released under a license that allows it to be used for non-commercial research/educational purposes (see https://image-net.org/download.php).\nAircraft (FGVC Aircraft) (Maji et al. 2013): We downloaded FGVC Aircraft from the official site https://www. robots.ox.ac.uk/~vgg/data/fgvc-aircraft/. FGVC Aircraft is released under a license that allows it to be used for non-commercial research/educational purposes (see https://www. robots.ox.ac.uk/~vgg/data/fgvc-aircraft/).\nBird (CUB-200-2011) (Welinder et al. 2010): We downloaded CUB-200-2011 from the official site http://www. vision.caltech.edu/datasets/cub_200_2011/. CUB-200-2011 is released under a license that allows it to be used for non-commercial purposes (see https://authors.library.caltech.edu/ 27452/).\nCar (Stanford Cars) (Krause et al. 2013): We downloaded Stanford Cars from the official sitehttps://ai.stanford.edu/ ~jkrause/cars/car_dataset.html. StanfordCars is released under a license that allows it to be used for non-commercial research purposes (see https://ai.stanford.edu/~jkrause/cars/ car_dataset.html)."}, {"title": "Additional Qualitative Experiments", "content": "Table 6 shows the qualitative evaluation results on the Aircraft and Car datasets, which are omitted in the main paper due to the page constraint. The evaluation protocol is the same as Section 3.2. Similar to Table 2, our method succeeded in capturing the semantic concepts of input images in the text explanation. Also, the concept phrases and cross-attention heatmaps show that the captured semantic concepts contribute to the final output and the main focus of models is on the target objects."}, {"title": "Additional Results with Large Vision-Language Models", "content": "In Sections 3.3 and 3.4, we confirm the results of XBMs combined with LLaVA (XBM-LLaVA) on ImageNet. Here, we show the additional XBM-LLaVA results on the other datasets. Table 7 demonstrates the results with the same tendency as Table 3, i.e., combining XBMs with larger vision-language backbones significantly improves the target task performance and interoperability. This indicates that our methods can be extendable even if a new and powerful vision-language model emerges."}, {"title": "Additional Results of ImageNet Segmentation", "content": "We additionally compare our method with existing attribution methods on BLIP-ViT. Note that we omit this result from the main paper because, strictly speaking, BLIP-ViT and XBM-BLIP are different models and thus this evaluation is not a direct comparison. By following Chefer et al. (Chefer, Gur, and Wolf 2021), we tried LRP (Binder et al. 2016), partial-LRP (Voita et al. 2019), rollout (Abnar and Zuidema 2020), raw attention output from BLIP-ViT, GradCAM (Selvaraju et al. 2017), and the method of (Chefer, Gur, and Wolf 2021). Table 8 shows the results of ImageNet Segmentation with the same setting of Table 4. Surprisingly, the cross-attention output of the classifier fo (i.e., Pre-trained BLIP and XBM-BLIP) significantly outperformed the conventional visualization methods in the segmentation metric. This indicates that visualization explanation outputs of XBMs are quite accurate and reliable as the interpretation of model outputs."}, {"title": "Detailed Analysis", "content": "In this section, we provide detailed analyses of XBMs. In particular, we assess temperature annealing in the Gumbel softmax sampling (Eq. (6)), the hyperparameter \\(\\lambda\\) in Eq. (1), and the localization ability of the cross-attention heatmaps introduced in Section 2.4."}, {"title": "Evaluations of Temperature Annealing", "content": "We introduce the temperature annealing strategy for determining \\(\\tau\\) in Eq. (6). Here, we evaluate the effects by varying the initial temperature \\(\\tau^{(0)}\\) in \\(\\{1, 10, 100\\}\\). Table 9 shows the test performance and interpretability scores. We tested the cases leveraging a constant temperature \\(\\tau^{(0)}\\) and applying exponential temperature annealing, i.e., + Annealing. In the cases of constant temperatures, we confirm that the larger temperatures tend to achieve better target performance but degrade perplexity scores. This is because using a larger temperature increases the entropy of the generative distribution of tokens in the Gumbel softmax sampling, and thus, it slightly loses the naturalness of the generated sentences. On the other"}, {"title": "Effects of Hyperparameter \\(\\lambda\\"}]}