{"title": "Prism: Mining Task-aware Domains in Non-i.i.d. IMU Data for Flexible User Perception", "authors": ["Yunzhe Li", "Facheng Hu", "Hongzi Zhu", "Quan Liu", "Xiaoke Zhao", "Jiangang Shen", "Shan Chang", "Minyi Guo"], "abstract": "A wide range of user perception applications leverage inertial measurement unit (IMU) data for online prediction. However, restricted by the non-i.i.d. nature of IMU data collected from mobile devices, most systems work well only in a controlled setting (e.g., for a specific user in particular postures), limiting application scenarios. To achieve uncontrolled online prediction on mobile devices, referred to as the flexible user perception (FUP) problem, is attractive but hard. In this paper, we propose a novel scheme, called Prism, which can obtain high FUP accuracy on mobile devices. The core of Prism is to discover task-aware domains embedded in IMU dataset, and to train a domain-aware model on each identified domain. To this end, we design an expectation-maximization (EM) algorithm to estimate latent domains with respect to the specific downstream perception task. Finally, the best-fit model can be automatically selected for use by comparing the test sample and all identified domains in the feature space. We implement Prism on various mobile devices and conduct extensive experiments. Results demonstrate that Prism can achieve the best FUP performance with a low latency.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent years have witnessed the soaring development of appealing user perception applications on smart mobile devices, such as user authentication [1]\u2013[3], activity recognition [4]\u2013[6], and health monitoring [7], [8], where machine learning models trained on collected inertial measurement unit (IMU) data are leveraged for online prediction. In general, the successes of these user perception applications rely on the superior performance of deep neural networks (DNNs), trained on independent and identically distributed (i.i.d.) datasets [9], largely limiting the application scenarios in a controlled setting (e.g., for a specific user in particular postures). However, datasets flexibly collected from mobile devices are often the case non-i.i.d. because of different device types and usage habits [10]. Can we achieve flexible user perception (FUP) by training DNNs on IMU data flexibly collected from different types of devices and distinct users without requiring how they operate their devices?\nAn attractive scheme to the FUP problem is demanding due to the following reasons. First, it should be able to deal with IMU data collected from multiple non-i.i.d. sources (e.g., a device held in different postures or a device of a different brand carried by a different user). Such a dataset contains multiple hidden distributions (or domains) [11], [12], making it hard to train an effective DNN. Second, it should achieve satisfactory prediction accuracy with few constraints on how devices are operated. Third, such a scheme should be lightweight and can be easily deployed to a wide variety of mobile devices with limited computational capacity.\nIn the literature, much effort has been made to improve the accuracy of FUP on mobile devices. One main direction aims to develop one single prediction model that can generalize on all potential domains via domain generalization methods [13], [14], meta-learning [15] and pre-training [16]\u2013[18]. How-ever, their performance improvements are marginal because the essential non-i.i.d. issue still exists [19], [20]. Another direction is to divide training data into subsets and train an individual model on each subset, respectively. One or a few of those models best fitting the current scenario are selected for online prediction. One class of methods divides training dataset manually based on some prior knowledge [21]\u2013[23] (e.g., user intention in recommendation system [23] or image quality in computer vision [21]) or associated attributes of data samples (i.e., metadata) such as where a device is carried or the user ID [24], [25]. However, to obtain meaningful metadata is of intensive manpower and how to select effective metadata to use is not straightforward. Another class of methods clusters similar data samples either in raw data space [26]\u2013[29] or in some high-dimensional feature space [23], [30], [31]. How-ever, the derived subsets may not match the latent distributions with respect to a particular user perception task. As a result, to the best of our knowledge, there is no existing scheme that successfully addresses the FUP problem.\nIn this paper, we propose an effective data partition scheme, called Prism, which measures the inconsistence extent of a non-i.i.d. dataset and wisely divides the dataset into domains friendly to a downstream user perception task. Given a non-i.i.d. dataset, we have an insight that different tasks (or corre-sponding DNN models) may have distinct domain partitions. The core idea of Prism, therefore, is to automatically find a feature space where similar samples form i.i.d. domains for a particular downstream task. Then, individual models can be well-trained on each task-specific domain. As illustrated in"}, {"title": "II. PROBLEM DEFINITION", "content": "Given a dataset of IMU samples collected from different users, denoted as $D$, there exists a data partition scheme $P$, which separates $D$ into $n$ subsets, denoted as $\\{\\Gamma_1, \\Gamma_2,\\cdots, \\Gamma_n\\}$. For each $\\Gamma_i$, for $i \\in [1,n]$, a task model $M_i$ can be trained on the training set of $\\Gamma_i$, denoted as $\\Gamma_i^{trn}$. The FUP problem can be defined as follows:\nDefinition 1: The FUP problem is to find an optimal data partition scheme, denoted as $P^*$, so that the prediction errors of testing each obtained task model $M_i$ on the corresponding testing set of $\\Gamma_i$, denoted as $\\Gamma_i^{tst}$, i.e., $\\sum_{i=1}^{n}E(M_i,\\Gamma_i^{tst})$, is minimized, where $E(M_i,\\Gamma_i^{tst})$ denotes the prediction error of testing $M_i$ on $\\Gamma_i^{tst}$.\nThe FUP problem is hard when dataset $D$ contains non-i.i.d. distributions (e.g., $D$ is collected from multiple subjects with different devices) as data distributions captured by DNN models are latent. We have the following theorem:\nTheorem 1: The FUP problem is NP-hard.\nProof: The FUP problem can be reduced from the weighted set cover problem [36], a classic NP problem. Specifically, let $U$ denote a set of $N$ elements, i.e., $U = \\{u_1, u_2,\\dots, u_N\\}$ and $C(S_i)$ denote the cost of $S_i$ in the power set of $U$, i.e., $\\wp(U) = \\{S_1, S_2, \\dots, S_{2^N}\\}$, for $i \\in [1,2^N]$. Given $U$ and $\\wp(U)$, the objective of the weighted set cover problem is to infer a subset of $\\wp(U)$, denoted as $K$, where $\\cup_{S_i \\in K} S_i = \\cup_{S_i \\in \\wp(U)} S_i$ so that the sum of the cost of $U_i$ for $S_i \\in K$, i.e., $\\sum_{S_i \\in K} C(S_i)$, is minimized. We regard data samples $\\{x_1,x_2,\\dots, x_N\\}$ in dataset $D$ as the elements $\\{u_1, u_2,\\dots, u_N\\}$ in $U$. Similarly, the power set of $D$, $\\wp(D) = \\{\\Gamma_1, \\Gamma_2, \\dots, \\Gamma_{2^N}\\}$, can be regarded as $\\wp(U) = \\{S_1, S_2,\\dots, S_{2^N}\\}$. The prediction error $E(M_i, \\Gamma_i^{tst})$ can be regarded as $C(S_i)$. Therefore, our objective $\\sum_{i=1}^{n}E(M_i, \\Gamma_i^{tst})$ is equivalent to $\\sum_{S_i \\in K} C(S_i)$ and thus is NP-hard."}, {"title": "III. DESIGN OVERVIEW", "content": "The core idea of Prism is to effectively estimate latent domains regarding a specific perception task, embedded in"}, {"title": "IV. DISTRIBUTION INCONSISTENCE DETECTION", "content": "A. Initial Model Training\nGiven the available dataset $D$, we first train a model $M_0$ for a specific perception task using all training samples. Specifically, $M_0$ comprises a feature extraction backbone network $M_{enco}$ and a task classifier $M_{task}$. We train $M_0$ with all training samples using the cross-entropy loss:\n$\\mathcal{L}_{init} = -\\sum_{i=1}^{N} \\sum_{j=1}^{C} Y_{i,j} log(M_{task}(M_{enco}(x_i)))_j,$, where $x_i$ denote the i-th data sample in dataset $D$; $C$ denotes the number of classes; $Y_{i,j}$ denotes a binary label indicating whether i-th sample belongs to j-th label; and $N$ is the number of samples in dataset $D$.\nB. Non-i.i.d. Test\nGiven the dataset $D$ and the initial model $M_0$, we examine the non-i.i.d. level of $D$ regarding a particular perception task to determine whether latent domains should be identified. Specifically, we first partition $D$ randomly into $2k$ data clips, denoted as $c_i$, for $i \\in [1,2k]$, where $k$ denotes the pre-set epochs of non-i.i.d. tests and $i$ denotes the index of rounds. Then, we group all clips into two sets, each with $k$ clips, denoted as $D_a^i$ and $D_b^i$, respectively. For example, $D_a^1 \\leftarrow \\{c_1, c_2,\\dots, c_k\\}$ and $D_b^1 \\leftarrow \\{c_{k+1}, c_{k+2},\\dots,c_{2k}\\}$.\nThe non-i.i.d. index (NI) between two sub-datasets $D_a^i$ and $D_b^i$ can be calculated as the norm of their features [37]:\n$NI_i = \\frac{1}{C} \\sum_{cls=1}^{C} \\frac{|| \\mu(M_{enco}([D_a^i])^{cls}) - \\mu(M_{enco}([D_b^i])^{cls}) ||_2}{\\sigma(M_{enco}([D^i])^{cls})}$         (1)\nwhere $[D_a^i]^{cls}, [D_b^i]^{cls}$ and $[D^i]^{cls}$ denotes the set of data samples in $D_a^i$, $D_b^i$ and $D^i$ with class label $cls$, respectively; $\\mu(\\cdot)$ denotes the first order moment; $\\sigma(\\cdot)$ denotes the standard deviation used to normalize the scale of features; $|| \\cdot ||_2$ denotes the L2-norm; $C$ denotes the number of classes to classify in a specific perception task.\nAs illustrated in Figure 3, we repeat the NI calculation between different pairs of $D_a^i$ and $D_b^i$, constructed by exchanging $c_{i-1}$ in $D_a^{i-1}$ and $c_{k+i-1}$ in $D_b^{i-1}$, for $i \\in [2, k]$. For example, $D_a^2 \\leftarrow \\{c_2,\\dots,c_k,c_{k+1}\\}$ and $D_b^2 \\leftarrow \\{c_{k+2},\\dots,c_{2k},c_1\\}$, where $c_1$ in $D_a^1$ and $c_{k+1}$ in $D_b^1$ are exchanged. After $k - 1$ rounds of exchanges, $D_a^k$ and $D_b^k$ are totally swapped, i.e., $D_a^k = D_b^1$ and $D_b^k = D_a^1$, which"}, {"title": "V. TASK-SPECIFIC DOMAIN ESTIMATION", "content": "A. Model Initialization\nWe first initialize all task models. Specifically, each $M_i$ comprises a common backbone $M_{enco}$ and $n$ domain-specific task classifiers $M_{task}^i$ for $i \\in [1,n]$, where $n$ represents the number of estimated domains. Each task classifier is trained for a particular domain to produce a prediction $\\hat{y}$. The parameters of the pre-trained $M_0$ are used to initialize all $M_i$ for $i \\in [1, n]:$\n$M_{enco}^i \\leftarrow M_{enco}, M_{task}^i \\leftarrow M_{task} \\quad for \\quad i \\in [1, n].$          (3)\nB. E-step: Latent Distribution Estimation\nThe E-step of Prism aims to estimate the task-specific domains in $D$. Specifically, we first utilize the encoder $M_{enco}$ to encode $\\{x_1,x_2,\\cdots, x_N\\}$ into $\\{h_1, h_2,\\dots, h_N\\}$ in a feature space. Then, we group $\\{h_1,h_2,\\dots,h_N\\}$ into $n$ domains $\\{\\mathcal{H}_1, \\mathcal{H}_2,\\dots, \\mathcal{H}_n\\}$ by clustering in the feature space through k-means. Each $\\mathcal{H}_i$ for $i \\in [1,n]$ in the feature space corresponds to a latent domain $\\Gamma_i$ in the data space, leading to a partition scheme $P^*$.\nC. M-step: Task-oriented Domain Likelihood Maximization\nThe parameters of backbone encoder $M_{enco}^i$ are further optimized in a manner of gradient descent in M-step. In Prism, two assessments are designed to obtain the loss for optimization.\n1) Clustering Result Assessment: The clustering result as-sessment aims to assess whether the hidden feature space is well embedded so that the domains $\\{\\Gamma_1, \\Gamma_2,\\cdots, \\Gamma_n\\}$ are well divided in the feature space. The contrastive loss [38] is used for clustering result assessment, which encourages similar pairs to be closer and dissimilar pairs to be farther apart in the feature space, which can be computed as follows:\n$\\mathcal{L}_c = \\frac{1}{2N^2} \\sum_{n=1}^{N} [u \\cdot d^2 + (1 - u) \\cdot max(M - d, 0)^2]$, (4)\nwhere $u$ denotes a binary label indicating whether two input samples belong to the same class ($u = 1$) or not ($u = 0$); $d$ denotes the distance between two samples in the feature space; $M$ denotes the contrastive margin, which is a hyper-parameter that determines the minimum distance for different-class samples.\n2) Trial Task Assessment: To obtain the task-oriented loss, denoted as $\\mathcal{L}_T$, the task classifiers $M_{task}^i$ for $i \\in [1,n]$ are jointly trained to access the quality of current partition scheme $P^*$. Specifically, for each training sample $x_j$ and its corresponding domain index $m_j \\in [1,n]$, we forward the features $h_j$ of $x_j$ with the $m_j$-th classifier. Then, the cross-entropy loss is used for the optimization of $M_{enco}^i$ and $M_{task}^i$ for $i \\in [1, n]:$\n$\\mathcal{L}_T = -\\frac{1}{N_c} \\sum_{j=1}^{N} \\sum_{k=1}^{C} Y_{j,k} log(M_{task}^{m_j}(M_{enco}(x_j))_k)$.          (5)\nFinally, the total loss (denoted as $\\mathcal{L}_{tde}$) for domain estima-tion is defined as the weighted sum of the contrastive loss $\\mathcal{L}_c$ and the task-specific cross-entropy loss $\\mathcal{L}_T$:\n$\\mathcal{L}_{tde} = \\alpha \\cdot \\mathcal{L}_c + \\mathcal{L}_T$,          (6)\nwhere $\\alpha$ denotes a hyper-parameter of contrastive loss weight for the balance of $\\mathcal{L}_c$ and $\\mathcal{L}_T$.\nD. Theoretical Analyses\nConvergence Analysis of Prism. Prism can be proven to converge as follows.\nConvergence of Prism: Denote the set of all parameters in $M_i$ for $i \\in [1,n]$ to be $\\theta$. In Prism, we first estimate the domains $\\mathcal{H}_1, \\mathcal{H}_2,\\dots, \\mathcal{H}_n$ in E-step and then update the current parameters, denoted as $\\theta^{(t)}$, to $\\theta^{(t+1)}$ by minimizing the loss function $\\mathcal{L}_{tde}$ shown in Equation 6. Therefore, to prove the convergence of Prism is to prove the convergence of $\\mathcal{L}_{tde}$. To this end, we first prove the monotonicity of $\\mathcal{L}_{tde}$ during iteration and then prove the boundedness of $\\mathcal{L}_{tde}$.\nMonotonicity. The monotonicity of $\\mathcal{L}_{tde}$ during iterations, i.e., $\\mathcal{L}_{tde}(\\theta^{(t+1)}) < \\mathcal{L}_{tde}(\\theta^{(t)})$ for each $t$, can be guaranteed in the M-step. Specifically, in M-step, we obtain $\\theta^{(t+1)}$ by minimizing $\\mathcal{L}_{tde}$, i.e., $\\theta^{(t+1)} = arg \\min_{\\theta^{(t)}} \\mathcal{L}_{tde} (\\theta^{(t)})$. As a result, we have $\\mathcal{L}_{tde}(\\theta^{(t+1)}) < \\mathcal{L}_{tde}(\\theta^{(t)})$.\nBoundedness. We consider the custom loss function $\\mathcal{L}_{tde} = \\alpha \\cdot \\mathcal{L}_c + \\mathcal{L}_T$, where $\\mathcal{L}_c$ and $\\mathcal{L}_T$ are shown in Equation 4 and Equation 5, respectively. $\\mathcal{L}_{tde}$ has a lower bound of 0 because both components of $\\mathcal{L}_{tde}$, $\\mathcal{L}_c$ and $\\mathcal{L}_T$, have a lower bound of 0. For $\\mathcal{L}_c = \\frac{1}{2N^2} \\sum_{n=1}^{N} [u \\cdot d^2 + (1 - u) \\cdot max(M - d, 0)^2]$, since both $d^2$ and $max(M - d, 0)^2$ are non-negative, $\\mathcal{L}_c$ is non-negative, and its minimum value is 0 when $d = 0$. For $\\mathcal{L}_T = -\\sum_{j=1}^{N} \\sum_{k=1}^{C} Y_{j,k} log(M_{task}^{m_j}(M_{enco}(x_j))_k)$, since $0 < M_{task}^{m_j}(M_{enco}(x_j))_k \\leq 1$ and $log(M_{task}^{m_j}(M_{enco}(x_j))_k) \\leq"}, {"title": "VII. DISCUSSION", "content": "Differences between Prism and MoE. MoE, i.e., Mixture of Experts, is a popular technology to extend the model parameters and is similar with Prism. Prism differs from MoE in two aspects. First, experts in MoE are diversified just by constraints of losses, but they themselves cannot be related to the domains in the dataset. Second, models based on MoE architecture can only be deployed with the entire model, which is unacceptable for mobile applications. On the contrary, the models in Prism can be partially deployed [29], i.e., only models related to testing scenarios to be deployed, making Prism more lightweight and suitable for mobile devices."}, {"title": "VIII. RELATED WORK", "content": "A. Flexible User Perception for IMU Data.\nFlexible user perception for IMU data has been widely explored with transfer-learning-based solutions [11], [12]. However, these methods based on transfer learning do not consider the mobile setting, where the test domains are un-known. The domain partition is therefore proposed to solve the FUP problem [22], [24], [25]. TeamNet [22] explores and trains multiple small NNs through competitive and selective learning. UniHAR [10] adapts to all seen domains offline to ensure inference performance. All of these methods require accurate apriori information for data partition, which is hard to obtain in the real-world setting.\nB. Automatic Domain Estimation.\nAs for automatic domain estimation, a natural idea is to per-form clustering before training, e.g., Clustered partition [46]."}, {"title": "IX. CONCLUSION", "content": "In this paper, we have proposed a flexible user perception scheme, called Prism, for flexible user perception on mobile devices. Prism can automatically discover latent domains in a dataset with respect to a specific perception task, resulting in a set of domain-specific reliable task models for use. As a result, Prism can obtain state-of-the-art prediction accuracy while having no particular requirements on how users operate their devices. Prism is lightweight and can be easily implemented on various mobile devices at a low cost. Extensive experiment results demonstrate that Prism can achieve the best flexible user perception performance at low latency."}]}