{"title": "Hint-AD: Holistically Aligned Interpretability in\nEnd-to-End Autonomous Driving", "authors": ["Kairui Ding", "Boyuan Chen", "Yuchen Su", "Huan-ang Gao", "Bu Jin", "Chonghao Sima", "Wuqiang Zhang", "Xiaohui Li", "Paul Barsch", "Hongyang Li", "Hao Zhao"], "abstract": "End-to-end architectures in autonomous driving (AD) face a significant challenge\nin interpretability, impeding human-AI trust. Human-friendly natural language has\nbeen explored for tasks such as driving explanation and 3D captioning. However,\nprevious works primarily focused on the paradigm of declarative interpretability,\nwhere the natural language interpretations are not grounded in the intermediate\noutputs of AD systems, making the interpretations only declarative. In contrast,\naligned interpretability establishes a connection between language and the in-\ntermediate outputs of AD systems. Here we introduce Hint-AD, an integrated\nAD-language system that generates language aligned with the holistic perception-\nprediction-planning outputs of the AD model. By incorporating the intermediate\noutputs and a holistic token mixer sub-network for effective feature adaptation,\nHint-AD achieves desirable accuracy, achieving state-of-the-art results in driving\nlanguage tasks including driving explanation, 3D dense captioning, and command\nprediction. To facilitate further study on driving explanation task on nuScenes,\nwe also introduce a human-labeled dataset, Nu-X. Codes, dataset, and models are\npublicly available at https://air-discover.github.io/Hint-AD/.", "sections": [{"title": "1 Introduction", "content": "End-to-end perception-planning architecture is critical in autonomous driving (AD) [1, 2] and general\nembodied intelligence [3, 4] due to its potential for self-supervised training with extensive data.\nHowever, these systems face significant interpretability challenges [5, 6].\nInterpretability issue [7, 8, 9, 10, 11] is particularly pronounced in embodied intelligence problems\nsuch as AD. When an AD system directly outputs control signals, it becomes difficult for human"}, {"title": "2 Related Works", "content": "End-to-end autonomous driving systems aim to create an architecture that processes sensor data\nand directly outputs vehicle control signals [23]. These systems have gained research attention due\nto their ability to address error accumulation problems found in traditional modular designs, where\nperception and planning are separated into distinct modules [24, 25, 26, 27, 28, 29, 30, 31, 32].\nProminent examples include UniAD [2] and VAD [21] integrate modular perception tasks such as\nobject tracking, map building, motion forecasting, and trajectory planning within a unified framework.\nOffline datasets for end-to-end autonomous driving have also been developed [22, 33].\nInterpretability of AD [5, 6, 7, 8], the ability to provide a comprehensive explanation for AD\nplanning, is crucial for user trust and system transparency in AD systems [5, 6]. Natural language, as a"}, {"title": "3 Methodology", "content": "To explore holistic alignment between natural language and intermediate results in end-to-end AD\nframeworks, we propose a novel framework named Hint-AD, which consists of three modules: a\nholistic token mixer, a language decoder, and a traditional AD framework. An overview of Hint-AD\nis shown in Fig. 2. The existing AD pipeline in Fig. 2 can be any end-to-end AD system that\ndecomposes AD into perception, prediction and planning. Without loss of generality, we implement\nour method on top of both UniAD [2] (as Hint-UniAD) and VAD [21] (as Hint-VAD), which use\nrasterized and vectorized representations, respectively."}, {"title": "3.1 Overall framework of Hint-AD", "content": "Firstly, we extract intermediate query tokens from the existing AD model of a perception-prediction-\nplanning architecture, yielding track tokens, motion tokens, and planning tokens. Secondly, a holistic\ntoken mixer module will adapt the tokens for language decoder input, in which we design an instance\nmixer to merge instance-level track and motion information of each detected instance. We also\nintroduce BEV blocks and instance blocks for further feature extraction and converting length-variable\ninstance tokens to a fixed length. All the processed tokens are concatenated as context tokens for\ntext generation (see Sec. 3.2). Finally, the context tokens are formulated as prompt tokens and put\ninto the language decoder together with text prompts. We adopt a barbell adaptation paradigm for\nefficient context understanding of the language decoder (see Sec. 3.3).\nFor aligning language and the intermediate results of the AD pipeline in training, we incorporate\nextra training data called alignment task, which is constructed online during training (see Sec. 3.4).\nAdditionally, the training procedures are described in Sec.3.5."}, {"title": "3.2 Holistic token mixer", "content": "The query tokens extracted from the AD pipeline are not directly understandable to the language\ndecoder. Addressing this, we propose a holistic token mixer architecture. The implementation is\nslightly different between Hint-UniAD and Hint-VAD in detail. We primarily follow the design of\nHint-UniAD, while small adjustments for Hint-VAD are provided in Appendix B.3.\nWe start by giving denotations for query tokens extracted from the AD pipeline. For a typical\nperception-prediction-planning AD pipeline, we can extract the following components: BEV tokens\n$F_{bev} \\in R^{H \\times W \\times C}$, where $H_b$, $W_b$, and C are the height, width, and channels of the BEV field.\nTrack tokens ${F_{track}^{idet}}_{i=1}^{N_{det}} \\subset R^{D}$ contain position and past trajectory information of each detected\nobject, where $N_{det}$ is the number of detected objects, and D is the dimension of the token vector.\nMotion tokens ${F_{motion}^{i}}_{i=1}^{N_{det}} \\subset R^{D}$ contain predicted future trajectories of each detected object.\nPlanning steps $F_{plan} \\in R^{T_p \\times 2}$ would be the future trajectories predicted by the model.\nTo effectively merge tokens into an instance level, we design a novel instance mixer that integrates\nthe track token $F_{track}$ and the motion token $F_{motion}$ of each detected instance into an instance\ntoken $F_{instance}$. This is accomplished through tensor concatenation followed by a Multi-Layer\nPerceptron (MLP) projector $P_{instance}$ to project tokens of $N_{det}$ detected instances into embeddings\nwith dimension $D_{embed}$:\n$F_{instance}= P_{instance}(Concat(F_{track}, F_{motion})),  \\text{ for } i = 1, 2, ..., N_{det}.$  (1)"}, {"title": "3.3 Language decoder with barbell adaptation", "content": "To incorporate high-level reasoning and context understanding ability of Multimodal Large Language\nModels (MLLMs) in AD relevant language tasks [36, 37, 17, 18, 38], we employ a LLaMa-Adapter-\nV2 [39] as our language generator with a pretrained LLaMA-2-7B [40]. For language fine-tuning,\nlearnable adapters ${F_{adapter}^{i}}_{i=1}^{N_{adapter}} \\subset R^{D_{dec}}$ are employed, which serve as extra keys and values\nin the inserted layers with zero-initialized attention [39], where $N_{adapter}$ is the number of layers to\ninsert, and $D_{dec}$ is the dimension of tokens in the language decoder.\nIn the original LLaMa-Adapter-V2 strategy, context tokens $F_{context}$ would be inserted in the first\nlayer, and learnable adapters would be inserted in all other N \u2013 1 layers, where N is the total number\nof layers of LLaMA-2-7B. We observed that the adapters, which are for language tuning, tend to\ndominate the adaptation process and reduce the context-language alignment. This is crucial for AD\ntasks that requires high-level context understanding ability. Thus we propose a barbell adaptation"}, {"title": "3.4 Aligning language and intermediate outputs", "content": "To align language with intermediate outputs from AD model, the language decoder needs grounded\ncontext understanding of the information contained in each token (i.e. object's position in track\ntokens) generated in AD model's inference steps. We implemented this by adding an online alignment\ntask dataset in the training process.\nDuring the alignment task, given the AD model's intermediate inputs, a set of prompt-answer pairs\nare generated (see Fig. 3). This task includes four types of alignments: (a) Counting alignment,\nrequiring the language decoder to interpret the number of instances of each type detected in the frame\naccording to track tokens; (b) Position alignment, necessitating the model to provide the position\nof a tracked instance based on a specific instance token; (c) Motion alignment, involving decoding\nthe velocity information contained in an instance token; and (d) Planning alignment, requiring the\nlanguage decoder to output the future trajectory points contained in a planning token.\nAll the question-answer pairs for alignment tasks are generated online during training. The alignment\ntasks greatly improve the language decoder's context understanding of the intermediate tokens, thus\nimproving the accuracy of AD captioning by a large margin (see Sec. 4.4)."}, {"title": "3.5 Training pipeline", "content": "The whole training pipeline of Hint-AD includes two stages. In the first stage, the end-to-end AD\nmodel is trained independently. In the second stage, we freeze all parameters of the AD model and\nthe MLLM parameters, updating only the parameters of the holistic token mixer and adapters. The\ntotal trainable parameters at the second stage are 87M, more training details are stated in Appendix A."}, {"title": "4 Experiments", "content": "4.1 Datasets and baselines\nDatasets. Explanation serves as a guide for human learning and understanding [41, 42]. Particularly\nin the context of end-to-end autonomous driving (AD) systems, human users often seek explanations to\nbridge the gap between sensor inputs and AD behaviors [14]. Currently, there is no dataset providing\nsuch explanations for nuScenes [22], a widely utilized dataset in AD research. To address this gap\nand facilitate interpretability-focused research on nuScenes, we introduce Nu-X, a comprehensive,\nlarge-scale, human-labeled explanation dataset. Nu-X offers detailed contextual information and\ndiverse linguistic expressions for each of the 34,000 key frames in nuScenes.\nA sentence of explanation typically comprises narration and reasoning [14], for instance: \u201c The car is merging into the right lane.  To pass the red car in front.\" In our dataset, each\ncaption encompasses these two components. Detailed examples and key data statistics are presented\nin Fig. 10. For an in-depth description of the labelling process and comprehensive data statistics,\nplease refer to Appendix D.\nAll Hint-AD architectures and baselines were trained and evaluated on the following datasets to\nprovide a comprehensive analysis: (1) Alignment task dataset (see Sec. 3.4), designed to align lan-\nguage with intermediate outputs of the AD model by requiring the language decoder to interpret each\nintermediate token, with ground truth answers generated online during training; (2) TOD\u00b3Cap [15],\na 3D dense captioning dataset offering object descriptions for 64.3K outdoor objects in nuScenes,"}, {"title": "4.2 Comparing with baseline models", "content": "Quantitative results. We present results separately for different input types and backbone modules\non four datasets. For Nu-X and TOD\u00b3Cap datasets, we adopt four standard image captioning metrics,\nCIDEr (C) [47], BLEU (B) [48], METEOR (M) [49] and Rouge (R) [50]. GPT-3.5 scoring (G) is\nalso used for Nu-X' evaluation due to the comprehensive expression in driving explanation (See"}, {"title": "4.3 Analysis on alignment between language and AD model", "content": "To quantify the alignment between language and the intermediate outputs of the AD model, we\nevaluate the language decoders' output against the predictions of the AD perception modules, which\nis generated online on the validation set. Four kinds of disalignment protocols are designed: (a)\nCounting Disalignment (CD) which measures the discrepancy between the number of instances in\neach category given by the decoding head and the tracking model, (b) Position Disalignment (PD)\nwhich measures the positional difference of a specific instance, (c) Motion Disalignment (MD) which\nmeasures the velocity difference, calculated as the mean distance between the velocity in the caption\nand the velocity predicted by the perception system, and (d) Planning Disalignment (PLD) which\nmeasures the discrepancy in trajectory points. For detailed definition, please refer to Appendix C.1.\nWe tested Hint-AD with both aligned interpretability (original design) and declarative interpretability,\nresults are detailed in Appendix C.1. The aligned language decoder performs significantly better than\nthe models operating under the declarative interpretability paradigm, indicating the effectiveness of\nalignment designs including holistic token mixer and alignment tasks."}, {"title": "4.4 Ablation study", "content": "Effectiveness of holistic alignment. To evaluate the effectiveness of holistic language-AD alignment\non language task accuracy, we conducted an ablation study by removing track, motion, and planning\ntokens from the language decoder's inputs. Results in Tab. 2 show that using all tokens achieves the\nhighest performance. Track tokens enhance 3D dense captioning with positional information, while\nplanning tokens improve command prediction by providing future trajectory data."}, {"title": "5 Conclusions and Limitations", "content": "We present Hint-AD, an integrated AD-language framework that aligns language generation with\nthe holistic perception-prediction-planning process of AD models, achieving SOTA performance\nin multiple AD captioning tasks. Meanwhile, as an exploratory research on the implementation of\naligned interpretability, the following limitations are waiting to be resolved by further research:\n\u2022 Due to its pipeline-specific nature, any changes in the intermediate output format necessitate\nmodifications in the design of the token mixer. For purely end-to-end models, such as\nblack-box models, adjustments are required to handle latent outputs effectively.\n\u2022 The LLaMA-based language decoder is relatively time-consuming. Further investigation into\nsmaller model alternatives, such as MiniChat-1.5-3B and StableLM-3B-4E1T, is warranted.\nAs LLM's potential to comprehend AD models' intermediate outputs becomes evident, future research\ncan delve deeper into this area and enhance user trust in AD models through aligned interpretability."}, {"title": "A Training and inference", "content": "The graphics memory usage for the language decoder varies with batch size, ranging from 31GB\n(batch size = 1) to 78GB (batch size = 24). For Hint-VAD, the memory usage ranges from 26GB\n(batch size = 1) to 45GB (batch size = 20). Training Hint-UniAD takes 13 hours for 3 epochs on 8\nA100 GPUs, while training Hint-VAD takes 7 hours on 8 A100 GPUs. The inference times for the\nAD module and language decoder are detailed in Tab. 5.\nIn Tab. 6, we present the wall-clock time of the entire system with a batch size of 1, as applied in a\nreal car scenario. This time encompasses data acquisition, pre-processing, and post-processing, all\nmeasured in seconds.\nData acquisition time includes capturing images from six HIKROBOT MV-CU013-A0UC cameras\nand transmitting these images from the computer to the online server if using online inference.\nPre-processing time involves preparing image sequences before inputting them into the model, which\nis conducted online if using an online server.\nAutonomous driving inference time refers to the time taken by UniAD and VAD to perform inferences.\nLLM inference time is calculated as the time required to generate 26.5 tokens, which is the average\ngeneration length observed in our research.\nPost-processing time includes the duration needed for data transmission if the inference is conducted\nonline.\nSince users can access language output as soon as the first token is generated, we define interactive\nlatency as the sum of data acquisition, pre-processing, autonomous driving inference, and post-\nprocessing times."}, {"title": "B More on Model Designs", "content": "B.1 Parameters in Model Design\nHere we provide Tab. 7 to specify parameters used in our architecture and additional explanation.\nBEV blocks consist of a sequence of 8 transformer encoder blocks. Each block includes a normaliza-\ntion layer, a multi-head self-attention layer with 16 attention heads, a second normalization layer, and\na multi-layer perceptron (MLP) with a hidden dimension of 3072."}, {"title": "B.2 Comparison with Other Adaptation Methods", "content": "Here we present analysis on why adapter is chosen as tuning method for Hint-AD.\nWe selected adapters due to considerations of training stability and parameter efficiency. As high-\nlighted by LLaMA-Adapter [59], parameter-efficient fine-tuning methods like LoRA exhibit instability\nduring training, particularly when addressing entirely new modalities. For instance, LoRA experi-\nences gradient explosion within approximately 2.3K training steps. The LLaMA-Adapter addresses\nthis issue by introducing zero-initialized cross-attention weights between adapter tokens and text\nqueries.\nWe show experiment results with LoRA and DoRA in Tab. 8, demonstrating that the adapter is\nthe only method that ensures stable training for this application while delivering superior overall\nperformance."}, {"title": "B.3 Variations on Hint-VAD framework", "content": "VAD [21] uses the same BEV encoder as UniAD [2], but vectorizes scene representations for\nplanning and getting rid of dense maps. So the BEV tokens are the same: $F_{bev} \\in R^{H \\times W_b \\times C'}$, where\n$H_b$, $W_b$, and C are the height, width, and channels of the BEV field. But for VAD has different\nmotion prediction and planning phases, so the input tokens are adjusted as follows: In Track tokens\n${F_{track}}_i^{i=1} \\subset R^{D}$ and Motion tokens ${F_{motion}}_i^{i=1} \\subset R^{D}$, N is the number of agent queries, not\nthe number of detected objects. We select valid tokens based on the query's classification score, using\na threshold of 0.5, thereby only considering queries with a high probability of corresponding to actual\nobjects. Additionally, VAD considers both ego-agent interaction and ego-map interaction. We use\nupdated queries as Ego token $F_{ego} \\subseteq R^D$, which contain both dynamic and static information of the\ndriving scene. Finally, we adopt Planning steps $F_{plan} \\in R^{T_p \\times 2}$, the future trajectories predicted by\nthe model. We adjust the token mixer to accommodate the shape of each token while maintaining the\nsame overall architecture."}, {"title": "C More on experiments", "content": "C.1 More analysis on language-AD model alignment\nTo quantify the alignment between language and the intermediate outputs of the AD model, we\nevaluate the output of language decoders against the results predicted by the AD perception modules,\nwhich is generated online on the validation set. We define four kinds of disalignment as follows:\nCounting Disalignment (CD) measures the discrepancy between the number of instances of each\ncategory given by the text and the number detected by the AD model. It is defined as: $CD =\\frac{1}{N_{cat}} \\sum_{i=1}^{N_{cat}} | c_{text}^{(i)} - c_{AD}^{(i)} |$ , where $N_{cat}$ is the number of object categories, $c_{text}^{(i)}$ and $c_{AD}^{(i)}$ are the\ncounts of the ith category given by the text and the AD model, respectively.\nPosition Disalignment (PD) measures the difference in the position of a specific instance given by the\ntext and the AD model. It is quantified by calculating the mean distance between the position in the\ncaption $r_{text}$ and the position predicted by the AD model $r_{AD}$: PD = Mean($r_{text} - r_{AD}$).\nMotion Disalignment (MD) measures the difference in velocity, calculated as the mean distance\nbetween the velocity in the caption $v_{text}$ and the velocity predicted by the AD model $v_{AD}$: MD =\nMean($v_{text} - v_{AD}$).\nPlanning Disalignment (PLD) measures the discrepancy in trajectory points. A trajectory is expressed\nas four future positions of the ego car with 0.5s time steps between each $[r_1, r_2, r_3, r_4]$, and\nPLD is defined as the mean distance from the original AD prediction at 2s (L2 loss): PLD =\nMean($|r_{text} - r_{AD}|_2$).\nHere we offer a comparison of alignment between Hint-UniAD and various declarative alignment\nmethods, as shown in Tab. 9.\n\u2022 Among the declarative methods, the Hint-UniAD architecture achieved the highest perfor-\nmance, except in the counting disalignment task, demonstrating its overall efficacy.\n\u2022 Methods utilizing a pre-trained LLM as a language decoder performed poorly on the\ncounting task. This poor performance arises from a bias towards certain numbers: for\nexample, LLaMA tends to output specific integers like 10 and 20."}, {"title": "C.2 More Qualitative results", "content": "Here we demonstrate more detailed Qualitative results on all four datasets. All predictions are\ngenerated by Hint-UniAD model, and we choose TOD\u00b3Cap as baseline while evaluating results on\nNu-X and Command datasets.\nOur model excels in generating accurate and contextually appropriate predictions across multiple tasks.\nFor driving explanation and command prediction, our model outperforms the baseline by providing\nmore accurate narration, reasoning, and driving commands. In VQA tasks, it excels in accurately\ndescribing the surroundings, including the type and number of objects. In 3D dense captioning, our"}, {"title": "C.3 Analysis on testing difference between Hint-UniAD and Hint-VAD", "content": "According to Tab. 1, VAD performs much worse than UniAD on TOD\u00b3Cap dataset but slightly better\non the other three datasets. This can be explained by VAD's restricted perception range of 60m by\n30m, whereas objects in TOD\u00b3Cap often fall outside this range. Since all questions in TOD\u00b3Cap are\nobject-centered, missed detections significantly lower the scores on this dataset."}, {"title": "C.4 Baseline implementations", "content": "ADAPT. [12] The original work [12] takes a sequence of raw video frames as inputs, and outputs\nnatural language narration and reasoning. It employs Video Swin Transformer [51] as the video\nencoder. Two tasks, Driving Caption Generation (DCG) and Control Signal Prediction (CSP), are\njointly trained based on the same video tokens.\nHere we adopt its text generation head, where the sentences are generated by vision-language\ntransformer in an auto-regressive manner. The text inputs are tokenized and padded to a fixed length,\nand then are embedded with a segment embedding method. While training, we use cross attention\nmask within text tokens and sparse attention mask between text tokens and BEV features. The\nvision-language transformer encoder starts with a \"[CLS]\u201d token and generates one word token at a\ntime, consuming previously generated tokens as the inputs until it outputs the ending token \"[SEP]\"\nor reaches the maximum length.\nTOD\u00b3Cap. [15] The original TOD\u00b3Cap framework is implemented using a BEV detector module\nthat processes both 2D and 2D+3D inputs. We adapted TOD\u00b3Cap for UniAD and VAD by adding an\nextra captioning head. The inputs for Image BEV features and object proposals are derived from the\nBEV tokens and tracking tokens of these two AD models, respectively.\nVote2Cap-DETR++. [45] Unlike the traditional \"detect-then-describe\" methods that first detect\nobjects and then describe them, Vote2Cap-DETR++ uses a transformer-based framework to parallelly\ndecode object localization and caption generation tasks. It uses 3D positional encoding is added to\nenhance the context-awareness of the generated captions: Absolute Position Token is injected into the\ncaption prefix to indicate the spatial location of the object in the 3D scene and Rank-Based Position\nEncoding is applied to local context tokens."}, {"title": "C.5 More Ablation Experiments", "content": "We have expanded the ablation studies to include multi-module ablation and cross-model consistency,\nencompassing both Hint-UniAD and Hint-VAD. The ablation results for Hint-UniAD are presented\nin Tab. 10 and Tab. 11. These results indicate a consistent performance trend between Hint-UniAD\nand Hint-VAD. Notably, during multi-module ablation, the performance of the model experiences a\nfurther decline."}, {"title": "C.6 Caption evaluation", "content": "GPT-3.5 for Logicality and Accuracy Assessment The evaluation of visual language models for\nautomated driving presented in this paper focuses on two main components: the logicality assessment\nof driving descriptions and the accuracy evaluation of driving behavior predictions.\nFor the logicality assessment of driving descriptions, we utilized GPT-3.5 to determine the correlation\nbetween driving behavior descriptions and the triggers for those behaviors in the model's predictions.\nInitially, we prompted GPT-3.5 to extract key information from each predicted description, including\nthe car's driving behavior (movement and geographic location) and the causative factors (subject and\nform). We evaluated the associative relationship between the preceding and following events based\non three criteria:\n\u2022 The consistency between the car action implied by the causative factors and the predicted\ndriving behavior.\n\u2022 The logical coherence between the causative factors and the predicted driving behavior.\n\u2022 The consistency between the details in the causative factors and those in the driving behavior.\nEach metric was scored and summarized for each component.\nAdditionally, we conducted a comparative analysis of the similarity between predicted and manually\nlabeled driving behaviors. Using GPT-3.5, we extracted key information, such as the car's action,\npurpose, and reason, from both the predicted and manually labeled driving behaviors for comparison.\nEach pair was scored based on the degree of match (complete, partial, or missing). The primary\nmetrics for this comparison included:\n\u2022 The similarity between driving behavior and geographic location.\n\u2022 The objects and reasons leading to such driving behavior.\n\u2022 Other relevant details."}, {"title": "D.1 Annotation procedure", "content": "To balance accuracy and effectiveness of annotation, we conducted a process incorporating both\nhuman labelling and LLM diversification (see Fig. 11). The training / validation branches of nuScenes\ncontain 850 videos, each spanning about 20 seconds, with approximately 40 key frames per video\nannotated. labelling all frames is inefficient as driving behaviors and road conditions in adjacent\nframes are often similar. Each video typically features only 3-4 distinct driving behaviors. However,\nidentical annotations across frames risk over-fitting, reducing effectiveness. Therefore, we used\nMLLM to diversify expressions and add scene-description details to each frame. The annotation\nprocess includes the following procedures:\nHuman labelling We employed five professional annotators for a total of 126 hours. All annotators\nare familiar with US driving rules and have driving experience. They were instructed to describe\nspecific driving behaviors and the potential reasons for these decisions. Annotations for each video\ninclude the time intervals of certain driving behaviors, along with narration and reasoning."}, {"title": "D.2 More on data statistics", "content": "We show some key results of data statistics in Fig. 11(b)-(d). The average length of a narration is\n8.29 words for a sentence, and the average length of reasoning is 11.25 words."}, {"title": "E More on command dataset", "content": "The ground truth of the nuScenes dataset provides only three types of directional commands:  which lack the necessary velocity information. To\naddress this, we labeled a command dataset using programmed protocols based on the ground truth\nfuture steps, incorporating additional velocity commands such as , .\nDirectional commands were generated by analyzing the trajectory slope, applying a criterion where\ndriving 5 meters with only 1 meter of lateral drift was considered.\nFor velocity commands, polynomial fitting was used to smooth the speed curve derived from the\naverage speed between predicted points. The average acceleration over the next 4 seconds was then\nused to determine the velocity commands. Given the vehicle's increased sensitivity to speed changes\nwhen approaching zero, different thresholds for speed changes were set: 0.36 m/s\u00b2, 0.12 m/s\u00b2, and\n0.05 m/s\u00b2 for initial speeds of >2 m/s, 1-2 m/s, and <1 m/s, respectively."}]}