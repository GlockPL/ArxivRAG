{"title": "EaTVul: ChatGPT-based Evasion Attack Against Software Vulnerability Detection", "authors": ["Shigang Liu", "Di Cao", "Junae Kim", "Tamas Abraham", "Paul Montague", "Seyit Camtepe", "Jun Zhang", "Yang Xiang"], "abstract": "Recently, deep learning has demonstrated promising results in enhancing the accuracy of vulnerability detection and identifying vulnerabilities in software. However, these techniques are still vulnerable to attacks. Adversarial examples can exploit vulnerabilities within deep neural networks, posing a significant threat to system security. This study showcases the susceptibility of deep learning models to adversarial attacks, which can achieve 100% attack success rate (refer to Table 5). The proposed method, EaTVul, encompasses six stages: identification of important samples using support vector machines, identification of important features using the attention mechanism, generation of adversarial data based on these features using ChatGPT, preparation of an adversarial attack pool, selection of seed data using a fuzzy genetic algorithm, and the execution of an evasion attack. Extensive experiments demonstrate the effectiveness of EaTVul, achieving an attack success rate of more than 83% when the snippet size is greater than 2. Furthermore, in most cases with a snippet size of 4, EaTVul achieves a 100% attack success rate. The findings of this research emphasize the necessity of robust defenses against adversarial attacks in software vulnerability detection.", "sections": [{"title": "1 Introduction", "content": "Software vulnerability detection systems play a crucial role in safeguarding computer systems and networks. Deep neural networks have made significant advancements in this field, as demonstrated by recent algorithms [6], [13]. For instance, Lin et al. [23] extract high-level function representations from the abstract syntax tree (AST) to detect function-level vulnerabilities across projects. Feng et al. [3] propose a method utilizing the AST to extract syntax features and minimize data redundancy. Yang et al. [50] leverage a deep learning-based method using the Tree-LSTM network to assess the semantic equivalence of functions across platforms. Fu and Tantithamthavorn [4] present LineVul, a Transformer-based approach for line-level vulnerability prediction. However, it is crucial to acknowledge that these systems can be susceptible to attacks, which can compromise overall security [37].\nRecent studies have revealed that adversarial attacks can exploit vulnerabilities in software vulnerability detection techniques that use machine learning, particularly deep learning [38, 52]. Zhang et al. [55] proposed the Metropolis-Hastings Modifier algorithm to generate adversarial samples for attacking machine learning-based software vulnerability detection systems. Ramakrishnan and Albarghoutthi [36] investigated the feasibility of backdoor attacks on deep learning-based techniques used in software vulnerability detection systems. Henkel et al. [7] assessed the current architectures of machine learning-based software vulnerability detection. However, these studies are still in the early stages of exploring adversarial attacks in machine learning-based techniques, and the security issues of these techniques have not been thoroughly evaluated. For example, the defense against the scenario of adversarial attacks has not been considered in almost all machine/deep learning-based software vulnerability detection systems [2, 13, 21, 27, 31, 33, 39]. The adversarial attacks hold significant importance as they allow attackers to modify their samples (e.g., vulnerable samples) to bypass the prediction model. By manipulating the input data, adversaries can deceive the model into making incorrect predictions (e.g., vulnerable samples predicted as non-vulnerable), compromising the overall security of the system. As the number of hackers has grown [3], there is a strong demand to evaluate the security of software analysis techniques. Therefore, this motivates us to conduct fundamental research on evasion attacks to have a thorough understanding of the security issues of machine learning-based software vulnerability detection techniques.\nIn this paper, we propose EaTVul (Evasion Attack Against Software Vulnerability Detection), an automatic attack strategy from the perspective of an attacker. We assume no knowledge of the target model and cannot manipulate the training data. Our experiments demonstrate that EaTVul achieves an attack success rate of more than 83% when the snippet size is greater than 2 and 100% for most cases with a snippet"}, {"title": "2 Related Work", "content": "In this section, we will only review works closely related to this study. For more comprehensive information about adversarial machine learning in other research areas such as computer vision, natural language processing, and cybersecurity, please refer to [16, 28, 29, 37].\nRecent research has highlighted the vulnerability of machine learning, particularly deep learning, techniques to adversarial attacks in the field of software vulnerability detection [38, 52]. Zhang et al. [55] introduced the Metropolis-Hastings Modifier algorithm to generate adversarial samples specifically for attacking machine learning-based software vulnerability detection systems. Zeng et al. [54] developed OpenAttack, an open-source toolkit for textual adversarial attacking with unique strengths in supporting all attack types, multilinguality, and parallel processing. Yang et al. [51] further improved the strategy using a greedy and genetic algorithm with a focus on semantic preservation. Srikant et al. [43] further combined site-selection and perturbation-choice into a"}, {"title": "3 Overview of the EaTVul", "content": "This section introduces EaTVul, an automated system designed to attack machine learning-based software vulnerability detection systems. Figure 2 provides an overview of the proposed EaTVul, which consists of two main phases: adversarial data generation (1) and adversarial learning (\u2461).\nIn the first place, we train a surrogate model based on BiLSTM with an attention mechanism. To generate the adversarial data, several stages are involved in Phase 1. First, we identify important non-vulnerable samples using SVM. Then, we retrieve the averaged attention scores from the attention layer to identify the key features that contribute significantly to the prediction. These important features serve as inputs to ChatGPT, which generates adversarial data. The generated adversarial data will then be further reviewed and optimized, and ChatGPT is used again to regenerate the adversarial data. The optimized adversarial data is then added to the preserved attack pool. In Phase 2, our goal is to bypass a machine learning-based software vulnerability detection system using a vulnerable sample. To achieve this, we utilize a fuzzy genetic algorithm to select the best seed data, which is added to the vulnerable test case. The expectation is that the modified vulnerable test case will be predicted as non-vulnerable with a high probability. Details regarding the input/output of each step will be discussed in Section 3.1.\nIn this study, the attacker's capability, knowledge, and goal are as follows: Attacker's capability. The attacker is capable of perturbing the test queries given as input to pre-trained vulnerability detectors to generate adversarial samples. We follow the existing paradigm for generating adversarial examples in programming languages [51] and allow for two types of perturbations for the input code sequence: (i) token-level perturbations (for instance, variable renaming) and (ii) statement-level perturbations (for example, dead code insertion). To maintain the stealthiness and functionality of the perturbated code samples, we choose statement-level modification in this work. Specifically, the attacker is allowed to insert a certain number of non-functional statements in arbitrary locations. Attacker's Knowledge. Attacker's Knowledge. In the context of this study, we employ a conventional black-box framework for deep-learning-based vulnerability detection methods. Here, we presume that attackers do not have access to the architecture and parameters of target models; they are restricted to querying the deployed vulnerability detection model solely with input code sequences, receiving corresponding output probabilities or predictions. However, attackers have the capability to collect all open-source resources, including vulnerability information from NVD, to train their surrogate prediction model. Since practitioners typically utilize public vulnerability repositories to construct their training datasets, real-world vulnerable samples are limited with respect to vulnerability types. Therefore, we assume there will be overlap between the training data collected by"}, {"title": "3.1 Adversarial Data Generation", "content": "In this study of software vulnerability detection, adversarial data generation involves adding adversarial data in the vulnerable samples that are intentionally designed to deceive machine learning algorithms into making incorrect predictions or decisions. The goal of generating adversarial data is to identify weaknesses in the machine learning models used to detect vulnerabilities in software. Adversarial data should meet the following requirements: 1) It should include all the important features identified by the attention mechanism; 2) The adversarial data must maintain the code's functionality and should not introduce any syntactic errors or alter its operation; 3) The size of the adversarial data should be limited to less than 8 lines in this study to enhance its concealment and make it challenging to detect. To meet these requirements, we employ ChatGPT to generate adversarial data while considering all the important features (requirement 1). Subsequently, the raw adversarial data generated will undergo further optimization through prompts optimization (requirement 2) and re-generation using ChatGPT (requirement 3). The following context will provide a detailed explanation of these steps."}, {"title": "3.1.1 Important Samples Identification using SVM", "content": "A well-known fact is that not every sample contributes equally to the prediction. In this paper, our objective is to select the most important non-vulnerable samples and then identify the features that contribute the most to the prediction. Hence, a layered approach is essential to pick the most critical code samples and identify the most important features within those code samples. Therefore, we employ SVM, which is a low cost way to identify the important code segments (i.e., important non-vulnerable samples, which are the data points that lie closest to the decision boundary of the machine learning"}, {"title": "3.1.2 Important Feature Identification", "content": "Identifying the most important features is crucial for launching effective adversarial attacks. Adversaries aim to manipulate a system's decision-making process by introducing carefully crafted adversarial examples that resemble normal samples but are misclassified by the system. In this paper, we employ BiLSTM (bidirectional long short-term memory) with the attention mechanism [25] as the surrogate model and identify the most important features using the average attention scores. These features will be further utilized to generate adversarial data. In this step, we adopt the indices set from the previous stage and retrieve the weights from the attention layer. Further, the averaged attention score will be projected to the tokens in the statements. To maintain the stealthiness, we exclude low-frequency or unusual user-defined terms unique to a singular project. The outcome of this stage comprises a corpus of candidate features deemed significant."}, {"title": "3.1.3 Adversarial Data Generation using ChatGPT", "content": "Generative AI has the potential to revolutionize various aspects of our lives, with chatbots being one of the most popular implementations. ChatGPT has been extensively used and tested in different domains, showcasing its remarkable capabilities. In our work, we digest the candidate set of significant features and leverage ChatGPT as the code generation tool to generate adversarial data. In comparison to the predefined templates, the code snippets exhibit a higher level of fluency. To generate effective code snippets, we propose query templates as prompts to ChatGPT, incorporating the important features extracted using attention mechanisms. Especially, to better fulfill the requirement of stealthiness, we incorporate the partial codes preceding and succeeding to the inserting locations as context, following the template: <Context> <Query><Context>. And the <Context> yields one of the following prompt variations: \"Given the partial"}, {"title": "3.1.4 Preserved Attack Pool Generation", "content": "We have prepared a preserved attack pool that includes meticulously crafted adversarial data generated by ChatGPT. In detail, we generate plenty of samples from ChatGPT. To guarantee the compilability and functionality preservation, we employ the public program analysis tool (i.e., Comex) to remove these code snippets that are uncompilable or possess data dependency with the original programs and preserve the remaining adversarial code snippets. These adversarial data is specifically designed for studying adversarial attacks and assessing the robustness of machine learning-based software vulnerability detection systems models.\nThe preserved attack pool contains all the samples generated based on five categories of important features as discussed in subsection 3.1.3. These important features are data type, control statement, storage classes, input-output, and miscellaneous. These samples will be used as seed input for the fuzzy genetic algorithm to optimize the attack strategy. In other words, by using the preserved attack pool as a source of seed samples, fuzzy genetic algorithms can leverage the knowledge and characteristics of previously crafted adversarial examples. These seed data will provide desirable properties, such as effective attack strategies or high success rates"}, {"title": "3.2 Adversarial Learning", "content": "This subsection will discuss Step 2, which involves adversarial learning, including seed data selection and evasion attacks."}, {"title": "3.2.1 Seed Data Selection using FGA", "content": "To discover the optimal combination of preserved templates and enhance the success rate of evasion attacks, we employ optimized Fuzzy Genetic Algorithm (FGA) [47] (Algorithm 1). The FGA method employs a fuzzy clustering approach to ensure that all reserved members in the population have an opportunity to pass on to the next generation. Additionally, a fuzzy selection method is utilized to mitigate the drawbacks of a greedy strategy. The novel genetic algorithm consists of four major steps: initialization, clustering, selection, and crossover. Algorithm 1 provides detailed information regarding the seed data generation.\nInitialization. The genetic algorithm begins by randomizing the initial population, which serves as the first step in our proposed optimization algorithm. Each sample within the randomly generated population is filled with the predefined code snippets based on the templates. The population size remains constant throughout the method. Additionally, we initialize the score set of the population by calculating the fitness scores for each member. Furthermore, we randomly sample a centroid set from a uniform distribution within the range of [0,1), denoted as $C = (C_1, C_2, ...,c_k)$, where k represents the number of clusters.\nFitness Function. The design of the fitness function is a crucial step in genetic algorithms as it greatly affects the inheritance and success rate of the algorithm. In our proposed approach, we incorporate the attack success rate and the length of inserted code snippets into the fitness function. Intuitively, an effective adversarial sample should have a higher attack success rate and a lower length of inserted code snippets. Such samples are more likely to be selected as mating candidates or as the desired outcome. Based on this idea, the calculation of the fitness score for each member of the population is formulated as follows:\n$Score(s_j) = ASR(D_i \\otimes s_j) - \\lambda* len(s_j)$\nwhere $ASR(D_i \\otimes s_j)$ is the averaged attack success rate of entities generated by inserting the statement snippets into test vulnerable programs and $len(s_j)$ is the number of lines of the sample in the population. \u03bb controls the significance of the lengths of code snippets.\nFuzzy clustering approach. Fuzzy clustering [32] is a type of clustering algorithm that assigns each data point to multiple clusters with corresponding probabilities instead of a single cluster. In our work, we employ fuzzy clustering for further selection of the mating pool, aiming to avoid sub-optimal"}, {"title": "3.2.2 Evasion Attack", "content": "When launching an attack on a vulnerable test case, the FGA randomly chooses a seed sample. It then produces an optimized adversarial data snippet, which is added to the vulnerable test case, which mean we add the code snippet into the vulnerable samples before feed it to the prediction model. The objective is to modify the test case in such a way that it can evade detection by the machine learning-based software vulnerability detection system. It should be emphasized that EaTVul is specifically designed for evasion attacks targeting vulnerable test cases. The consolidation of all the keywords from Figure 5a into a single function is illustrated in Figure 5, as shown in Figure 5b, while maintaining the core logic. The code depicted in Figure 5b will be incorporated into the non-vulnerable sample to initiate the attack.\nIn addition, it is worth noting that, for the input and output of each step in the proposed EaTVul, the generated results are consistent with expectations at each stage. Specifically, given"}, {"title": "3.3 Visualization of EaTVul evasion attack", "content": "In this subsection, we present the visualization of the data distribution using t-SNE [44] given vulnerable and non-vulnerable samples. Figure 6 showcases the application of t-SNE for visualizing evasion attacks, specifically focusing on the vulnerable and non-vulnerable features. The visualization presented in Figure 6a reveals interesting insights regarding the separability of vulnerable and non-vulnerable samples based on the decision boundary, with only a few vulnerable samples overlapping. By leveraging t-SNE, we can gain a deeper understanding of the effectiveness of evasion attacks and the distinguishability of vulnerable and non-vulnerable instances. The clear separation observed in the visualization indicates that the decision boundary is generally successful in classifying samples as vulnerable or non-vulnerable.\nHowever, it is worth noting that despite the initial separation"}, {"title": "4 Experimental Setup", "content": "We will fundamentally evaluate the proposed EaTVul by answering the following research questions (RQ):\n\u2022 How effective is fuzzy genetic algorithm in selecting the seed adversarial data compared with randomization?\n\u2022 How effective is EaTVul based on adversarial data generated by ChatGPT originally and after optimization? To answer this question, we conducted experiments by presenting the results based on the scenario using the original adversarial data directly and optimized data.\n\u2022 How effective is EaTVul with recently developed machine learning-based software vulnerability detection systems?\n\u2022 How effective is EaTVul when compared with state-of-the-art large language models (LLM) and other machine learning tools that using BiLSTM for software vulnerability detection? To answer this question, we compare EaTVul with four recent large program generation models/algorithms: CodeBERT [8], CodeGen-2B [1], Poster-Lin [23], and MDVD [22].\n\u2022 How EaTVul behaves/performs regarding obfuscation/diversification methods? To answer this question,"}, {"title": "4.1 Datasets", "content": "In this study, we consider using multiple datasets in the C/C++ programming language including the real-world Asterisk project and OpenSSL project. These two projects include multiple types of vulnerabilities and are widely used in the community [18, 19]. For the scenario of a single type of vulnerability, we reuse the datasets from [20, 31], which are CWE119 and CWE399 datasets. CWE119 is a buffer error dataset, CWE399 is a resource management errors dataset, and CWE416 is a use-after-free vulnerability dataset. Considering the SARD dataset [3], [50], [4] has been widely used in the area of software vulnerability detection, experiments based on this dataset has been conducted as well. To demonstrate the generalizability of our proposed method, we systematically assess its performance across Java code samples sourced from the National Vulnerability Database (NVD) and open-source projects on GitHub, specifically targeting those classified within the top 5 to top 30 most perilous categories as defined in the Common Weakness Enumeration (CWE) [46].\nTable 2 presents the dataset information used in this paper. The first column shows the name of the dataset, the second column shows the number of vulnerable samples, followed by the number of non-vulnerable samples. In the experimental settings of training the surrogate model, we split the dataset into training (70%), evaluation (15%), and test data (15%). All the vulnerable test cases come from the test data. All the vulnerable test cases have been tested and confirmed that they are classified as vulnerable by the target model before launching adversarial attacks. The Abstract Syntax Trees (AST) feature has been considered in this paper since all the baselines use AST in their study."}, {"title": "4.2 Evaluation Metrics", "content": "This work only considers the attack success rate, which is defined as follows:\n$\\frac{\\text{#bypass}}{\\text{#total test cases}} \\times 100\\%$\nIn this study, the success rate typically refers to the percentage of attempts or instances where an adversarial attack is successful in bypassing the target model's (i.e., the vulnerable test cases misclassified as non-vulnerable). The success"}, {"title": "4.3 Baselines", "content": "In this work, we choose the recently developed systems as the target models to show the effectiveness of the proposed"}, {"title": "4.4 Results and Discussion", "content": "We structure our evaluation by stepping through each of our three research questions (RQ1-6).\nRQ1: How effective is fuzzy genetic algorithm in selecting the seed adversarial data compared with randomization? In order to show that fuzzy genetic algorithm outperforms randomization selection, we conducted experiments on the four"}, {"title": "4.5 Limitations", "content": "EaTVul system has several limitations. We plan to address the following issues in the future.\nThis study focuses on evasion attacks. From a defence perspective, it would be great to automatically recognize an adversarial sample and tell if it is actually an adversarial sample or not. However, this is a research topic in itself, and this topic is outside the scope of this work at the current stage. We believe the effectiveness of evasion attacks can be influenced by defense mechanisms. If the vulnerability detection system implements robust defense techniques such as input sanitization, anomaly detection, or ensemble models, the success rate of evasion attacks may significantly decrease. We leave this as future work.\nThe proposed EaTVul relies on the important samples (i.e., support vectors) identified by SVM. In this case, the number of important samples is limited to the number of support vectors. We would like to explore other methods, such as information retrieval [24] method, to further identify important samples based on the probability of being non-vulnerable and"}, {"title": "5 Conclusion", "content": "In conclusion, the rapid advancement of technology and the increasing complexity of cyber threats have made cybersecurity a critical concern in today's digital world. While deep learning techniques have shown promise in detecting vulnerabilities and improving the accuracy of software vulnerability detection systems, they are not immune to attacks themselves. Adversarial examples can exploit vulnerabilities in deep neural networks, compromising the security of the entire system.\nThis paper has addressed the issue of adversarial attacks on machine learning-based software vulnerability detection systems. It has introduced a novel attack strategy called EaTVul, which successfully generates adversarial examples to evade detection. The proposed strategy utilizes support vector machines, attention mechanisms, chatGPT, and a fuzzy genetic"}]}