{"title": "Rethinking Word Similarity: Semantic Similarity through Classification Confusion", "authors": ["Kaitlyn Zhou", "Haishan Gao", "Sarah Chen", "Dan Edelstein", "Dan Jurafsky", "Chen Shani"], "abstract": "Word similarity has many applications to social science and cultural analytics tasks like measuring meaning change over time and making sense of contested terms. Yet traditional similarity methods based on cosine similarity between word embeddings cannot capture the context-dependent, asymmetrical, polysemous nature of semantic similarity. We propose a new measure of similarity, Word Confusion, that reframes semantic similarity in terms of feature-based classification confusion. Word Confusion is inspired by Tversky (1977)'s suggestion that similarity features be chosen dynamically. Here we train a classifier to map contextual embeddings to word identities and use the classifier confusion (the probability of choosing a confounding word c instead of the correct target word t) as a measure of the similarity of c and t. The set of potential confounding words acts as the chosen features. Our method is comparable to cosine similarity in matching human similarity judgments across several datasets (MEN, WirdSim353, and Sim-Lex), and can measure similarity using predetermined features of interest. We demonstrate our model's ability to make use of dynamic features by applying it to test a hypothesis about changes in the 18th C. meaning of the French word \"r\u00e9volution\" from popular to state action during the French Revolution. We hope this reimagining of semantic similarity will inspire the development of new tools that better capture the multi-faceted and dynamic nature of language, advancing the fields of computational social science and cultural analytics and beyond.", "sections": [{"title": "1 Introduction", "content": "Semantic similarity measures allow computational social scientists, digital humanists, and NLP practitioners to perform fine-grained synchronic and diachronic analysis on word meaning, with important applications to areas like cultural analytics and legal and historical document analysis (Bhattacharya et al., 2020; R\u00edos et al., 2012).\nThe cosine between two embedding vectors is the most commonly used similarity metric for textual analysis across a variety of fields (Johri et al., 2011; Caliskan et al., 2017; Manzini et al., 2019; Martinc et al., 2020). However, it neither accounts for the multi-faceted nature of similarity (Tversky, 1977; Ettinger and Linzen, 2016; Zhou et al., 2022a, inter alia) nor does it align exactly with how humans perceive similarity (Nematzadeh et al., 2017). Cosine similarity is dominated by a small number of rogue dimensions due to the anisotropy of contextual embedding spaces (Timkey and Van Schijndel, 2021; Ethayarajh, 2019), underestimates the semantic similarity of high-frequency words (Zhou et al., 2022a), is a symmetric metric that cannot capture the asymmetry of semantic relationships (Vilnis and McCallum, 2014), and often fails in capturing human interpretation (Sitikhu et al., 2019).\nHere, we propose to think about concept similarity metrics differently. We are inspired by Tversky (1977)'s seminal work on similarity, presuming that humans have a rich mental representation of concepts. When faced with a particular task, like similarity assessment, we extract and compile from this rich representation only the relevant features for the required task. This formulation highlights the multi-faceted and context-dependent nature of similarity judgments (Evers and Lakens, 2014).\nTo demonstrate the potential of this new framing, we introduce a proof-of-concept: Word Confusion, a self-supervised method that defines the semantic similarity between words according to a classifier's confusion between them. In a nutshell, we first train a classifier to map from a word embedding to the word itself, distinguishing it from a set of distractors. At inference time, given a new"}, {"title": "2 Introducing Word Confusion", "content": "Our method begins by defining a set of words, or features. For example, we might choose the set W = {red, green, blue} if we wanted to study similarity related to colors. These words will act as features that can be selected by the analyst to focus on a particular dimension or question.\nOur process then has two phases: training and inference. In training (illustrated in part (a) of Figure 1)) we extract from a corpus a set of sentences containing each of these words, such as \u201cThe sunset painted the sky a brilliant shade of red\" for the word \"red\". We then use BERT to extract the contextual embeddings of these feature-words, and train a classifier to map from an embedding to its corresponding word identity. The classifier's training objective is to correctly classify the embedding to the word that corresponds to it.\nMore formally, given a chosen set of word W and embeddings {e1, e2....ei} \u2208 E that correspond to word identities {W1, W2, ..., Wi} \u2208 W, we train a logistic regression classifier on all pairs of {ei, wi}.\nAt inference (part (b) in Figure 1), we wish to define the semantic similarity of a word in terms of the classifier's classes, which can be thought of as features. Now suppose we would like to compute the similarity of the new word \"burgundy\" to various colors. We extract the contextual embedding of \"burgundy\" given the sentence \u201cBurgundy is a deep reddish-brown shade inspired by wine\u201d, and use the trained classifier to compute the probability that the \"burgundy\u201d-embedding corresponds to each class W = {red, green, blue}. We then use the classifier's confusion matrix to understand which primary colors burgundy is similar to. For example, the similarity of \u201cburgundy\" to \"red\" is the probability our classifier assigns to the class \"red\".\nMore formally, the probability distribution predicted by the model, p; \u2208 R|W|, is used to quantify the semantic similarity between wj (Burgundy) and each wi, wi \u2208 W (in this case W ={red, green, blue}). Thus:\nsimwc(wi, wj) det p(wilej)   (1)\nThe set of distractor words chosen to train the initial classifier thus act as features that can be selected by the analyst to focus on a particular dimension or question.\nNote that as with the example \"burgundy\", the input word at inference can be out-of-vocabulary with respect to the classifier, or the target word can be one of the classifier's classes (in which case we ignore the probability it assigns to that word and use the other N 1 features.)"}, {"title": "2.1 Benchmarking Word Confusion", "content": "The intuition behind Word Confusion is that if it struggles to distinguish between contextual embeddings of burgundy and red, this could indicate they are similar. To test this hypothesis, we use Word Confusion on three semantic similarity benchmarks. For each task, we trained a model using sentences from English Wikipedia. Our classes contained all the words from the benchmark. We then built word embeddings by averaging the last four hidden layers of BERT-base-cased (Details in Appendix B)."}, {"title": "3 Theoretical Intuitions", "content": "In this section, we discuss the importance of word identifiability and how it enables the core mechanics of Word Confusion, and discuss some theoretical differences between Word Confusion and cosine similarity."}, {"title": "3.1 The Identifiability of Contextualized Word Embeddings", "content": "Word Confusion depends on the ability of a classifier to identify a word based on its contextual embedding; here we confirm that this classification task is indeed solvable, and examine some error cases to better understand it.\nWhile contextualized word embeddings vary in their representation based on context, prior work showed that tokens of the same word still cluster together in geometric space (Zhou et al., 2022b).\nTo test whether these boundaries are indeed learnable, we test how well a model can identify a contextualized word embedding after seeing one other example of the same word's contextualized embedding. We randomly sampled 26,000 words from English Wikipedia, trained 1000-class one-shot classifiers, and tested them on 10,000 examples (ten examples per class). Indeed, we found that the average test set accuracy on all our classifiers is 90%, suggesting that the contextualized word embeddings are highly identifiable. Thus, given an embedding, it is possible to identify its symbolic representation. See A for experimental details."}, {"title": "3.2 Differences Between Word Confusion and Cosine Similarity", "content": "Word Confusion and cosine similarity give different kinds of distances. We can see one way to visualize this in Figure 2. Note the differences in the decision surface between Word Confusion and cosine similarity: cosine boundaries emerge from the origin, whereas boundaries from Word Confusion are not restricted in the same way.\nUsing a linear classifier in Word Confusion also introduces new parameters that transform the input vectors into a different space, effectively redefining the notion of distance compared to the raw embeddings. To see this, consider two normalized 2-dimensional vectors, x and y, and a real linear transformation, A applied to each. Using the singular value decomposition (SVD) of A = UDVI,"}, {"title": "3.3 Advantageous Properties of Word Confusion as a Similarity Measure", "content": "Using a trainable linear classifier and analyzing its error signal for word-similarity purposes introduces a few advantages for measuring similarity:\nAsymmetry. Human perceived similarity is not symmetric (Tversky, 1977). Yet cosine, like many distance functions commonly used to calculate semantic similarity, is symmetric. One of the advantages of using a model's confusion matrix for measuring semantic similarity is that these scores are asymmetric; i.e., pij \u2260 pji. For example, Word Confusion assigns lower probabilities for animal being predicted as cat than for cat being predicted as animal. The ability to measure asymmet-"}, {"title": "4 Real-World Data", "content": "Word Confusion is a new similarity measuring tool that could assist in understanding real-world data and trends. In this section, we focus on two applications of Word Confusion \u2013 its ability to serve as a feature extractor and to detect temporal trends in word meaning."}, {"title": "4.1 Word Confusion for Feature Classification", "content": "Word Confusion can be used to define out-of-domain word classes, i.e. when wj \u2209 W. Using our earlier example, if the classes of Word Confusion are the features {positive, negative}, given an out-of-domain word like school, we can use the confusion matrix to represent the embedding for school as a mixture of the classes the model is familiar with, i.e., {positive, negative}. Following this intuition, we test whether Word Confusion can use features as classes to identify objects' membership to these classes accurately. We used the following tasks:\nSentiment classification using the NRC corpus (Pang et al., 2002; Mohammad et al., 2013). The goal is to classify words according to their sentiment (either positive or negative). The words were"}, {"title": "4.2 What Is A Revolution?", "content": "We now examine whether Word Confusion could be used as a tool for studying concepts in a way that aids humanistic or social science investigations. By collaborating with the fourth author, a scholar of French literature and history, we investigate historical changes in the meaning of the French word \u201cr\u00e9volution\". Together, we used Word Confusion to test a prominent hypothesis of how the meaning of the word and concept of revolution changed (Baker, 1990): that the meaning of \"r\u00e9volution\u201d in the early years of the French Revolution was more associated with popular action, but later become identified with state actions.\nWe constructed a set of French words associated with the people ({peuple, populaire, ...}) and the state ({conseil, gouvernement, ...}). These seed words were used as classes for our classifier, which we trained on different temporal segments to observe the temporal change in our concept of interest. Our corpora are the Archives Parlementaires, transcripts of parliamentary speeches during a time that contains moments of both emancipation and elite control of political processes. The corpus contains 9,628 speeches and 54,460,150 words from the years 1789-1793. Within this corpus, the"}, {"title": "5 Related Work on Cultural Change", "content": "Both static and contextualized embedding spaces contain semantically meaning dimensions that align with high-level linguistic and cultural features (Bolukbasi et al., 2016; Coenen et al., 2019). These embeddings have enabled a large number of quantitative analyses of temporal shifts in meaning and links to cultural or social scientific variables. For example early on, using static embeddings, Hamilton et al. (2016) measured linguistic drifts in global semantic space as well as cultural shifts in particular local semantic neighborhoods. Garg et al. (2018) demonstrated that changes in word embeddings correlated with demographic and occupation shifts through the 1900s."}, {"title": "6 Discussion and Conclusion", "content": "In this paper, we reframe the task of semantic similarity from one of measuring distances to one of classification confusion. This formulation highlights the context-dependency of similarity judgments, meanwhile avoiding the pitfalls of geometric similarity measures (Evers and Lakens, 2014).\nThis new framing of semantic similarity in terms of classification confusion introduces new properties that are inspired by cognitive models of similarity (Tversky, 1977) and accounts for the asymmetric nature of semantic similarity, captures different aspects of both similarity and multi-faceted words and ofter a measure that has interpretability benefits.\nOur proof-of-concept method, Word Confusion, demonstrates the practical applicability and effectiveness of this reframing. Empirical results show that it outperforms cosine similarity on standard datasets. For computational social science or cultural analytics applications, Word Confusion can serve as a way to learn to represent words using target features (e.g., \"school\u201d in terms of {positive, negative}, and can be used to trace the meaning of a word as a function of time (like the word \u201cr\u00e9volution\").\nThe theoretical underpinnings of Word Confusion allow it to learn complex word identity boundaries and capture the directional nature of similarity, offering a richer and more flexible framework for understanding word meanings.\nWhile we implemented Word Confusion as a linear classifier, the method naturally extends to capturing non-linear relationships among embedding components by replacing the linear projection with neural networks. Investigating whether the error function preserves its useful properties in non-linear settings remains an open question for future work.\nWhile our experiments are preliminary and the space of possible similarity measures is enormous, we hope this reimagining of semantic similarity will inspire the development of new tools that better capture the multi-faceted and dynamic nature of language, advancing the fields of computational social science and cultural analytics and beyond.\""}, {"title": "Limitations", "content": "Our proof-of-concept suggests a promising path where cosine similarity can be replaced by a more sophisticated method that involves self-supervision. However, the boost in performance also comes with some caveats. Because Word Confusion is a supervised classifier, it requires an extra training step that simple cosine doesn't require. Furthermore, potential users will need a basic understanding of model training and the pitfalls of over-fitting data.\nAs mentioned earlier, while we implemented Word Confusion as a linear classifier, the method naturally extends to non-linear models. Additionally, various transformations commonly applied to embeddings before measuring distances (Mu et al., 2018) can also be incorporated prior to using Word Confusion, as our method relies on the resulting error signal to assess word similarity. Although non-linear models offer a promising direction, we have not yet examined whether the error function preserves its useful properties in such settings-an important avenue for future work. Introducing non-linearity into the classifier is known to alter its behavior in various ways, but its impact on confusion-based similarity remains uncertain. Further research is needed to evaluate its potential advantages and limitations.\nAnother key limitation of our approach is that we used three simple implementations of cosine similarity without exploring many possible augmentations to cosine, like normalizing it across the dataset (as was shown to be effective by (Timkey"}, {"title": "Ethics Statement", "content": "As with all language technologies, there are a number of ethical concerns surrounding their usage and societal impact. It is likely that with this method, the biases known in contextualized embeddings can continue to propagate through downstream tasks, leading to representation or allocation harms. Additionally, the use of large language models for building contextualized embeddings is expensive and requires time and energy resources. To our knowledge, the method we have developed does not exacerbate any of these pre-existing ethical concerns but we recognize our work here also does not mitigate or avoid them."}, {"title": "A Additional Details", "content": "Here, we provide additional details about the experimental set-up of Word Confusion.\nWe used the logistic regression model from the scikit-learn library using a one-vs-rest (OvR) scheme.\nDid you try other ways of creating embeddings? We explored alternative methods of creating word embeddings, such as various ways of concatenating layers, but they produced almost identical results.\nDid you perform any preprocessing? We filtered out short (<20 characters) and long (>512 characters) sentences, and matched keywords on token IDs to ensure punctuation and casing are consistent across examples.\nWhich hyperparameters did you use? Our task is also trained without any use of hyperparameters or special pre-processing steps to help address the concerns pointed out by Liu et al. (2019); Hewitt and Liang (2019).\nHow does this differ from BERT's training task and other works? The identity retrieval task differs from the masked LM training task: in masked LM training, the word identity must be predicted from its surrounding context rather than the embedding itself. Our task is also related to but different from the \"word identity\" classifier of Zhang and"}, {"title": "A.1 Error Analysis", "content": "Although Word Confusion is relatively accurate (> 90% accuracy), it can still makes mistakes, particularly with highly frequent or polysemous words.\nFrequency We find that a word's training data frequency correlates negatively with identifiability."}, {"title": "B Details and Full Results from Section 4.1", "content": "Implementation Out-of-vocabulary words here are represented as the average of the words' tokens, following Pilehvar and Camacho-Collados (2019) and Blevins and Zettlemoyer (2020). We experiment with a variety of embedding methods, taking the last layer and taking the first subtoken of out-of-vocabulary words and find comparable results.\nSimilarity Experiments For cosine, we took 30 samples of each word and we took the average"}, {"title": "B.1 Seed and Target Words Used", "content": "Sentiment Classification\nTask: Classifying concepts based on sentiment by using the NRC corpus (Mohammad et al., 2013). Target words: 98 positive and 98 negative words. Seed words: \u201cpositive\u201d and \u201cnegative\u201d.\nCorpus: wikitext-103-v1 from HuggingFace. We remove sentences that are shorter than 15 tokens and longer than 200 tokens.\nSampling: We sample 1000 occurrences of \"positive\" and 1000 occurrences of \u201cnegative\u201d. For each target word, we sample 30 occurrences.\nGrammatical Gender in French and Italian Experiment 1:\nTask: Classifying concepts by the grammatical gender of nouns.\nCorpus: Latest Italian Wikipedia abstracts from DBPedia. We removed sentences shorter than 20 tokens and longer than 100 tokens.\nSampling: Target words: 140 Italian nouns. Seed words: 59 Italian masculine and feminine adjectives. For each target word, we sample 30 occurrences. For each seed word, we sample 20 occurrences. Seed and target words have been filtered with respect to frequency. Data comes from Flex-IT (Pescuma et al., 2021).\nExperiment 2:\nTask: Classifying concepts by the grammatical gender of nouns.\nCorpus: Latest French Wikipedia abstracts from DBPedia. We removed sentences shorter than 20 tokens and longer than 100 tokens.\nSampling: Target words: 201 French nouns. Seed words: 65 French masculine and feminine adjectives. Seed and target words have been filtered with respect to frequency. Data comes form Lexique383 (New et al., 2004).\nBERT Concept Net Classification Land-Sea\nTask: Classifying concepts by classes based on the ConceptNet dataset (Dalvi et al., 2022), predicting if an animal is a sea or land animal.\nCorpus: wikitext-103-v1 from HuggingFace. We remove sentences that are shorter than 15 tokens and longer than 200 tokens.\nSampling: Target words: 64 land or sea animals. Seed words: category names: \"land\" and \"sea\". We sample 1000 occurrences of each seed word. For each target word, we sample 30 occurrences.\nBERT Concept Net Classification Fashion-Gaming\nTask: Classifying concepts by classes based on the ConceptNet dataset (Dalvi et al., 2022), predicting if a concept comes from the fashion domain or the design domain.\nCorpus: wikitext-103-v1 from HuggingFace. We remove sentences that are shorter than 15 tokens and longer than 200 tokens.\nSampling: Target words: 29 terms related to fashion or gaming. Seed words: category names: \"fashion, clothes\" and \"gaming, games\". We sample 500 occurrences of each seed word. For each target word, we sample 30 occurrences."}, {"title": "C Capturing Trends in Inflation", "content": "In a very preliminary experiment, we also apply Word Confusion to a novel social science domain: representation of economical value or financial meaning. Here we test whether we can recover"}, {"title": "D Details and Full Results from Section C", "content": "Data Segmentation We segment the temporal data based on the Dow Jones Index trend12 and aggregate intervals with the same fluctuation directions (see Table 3).\nData Pre-processing We use California Digital Newspaper Collection (Center for Bibliographic Studies and Research, University of California, Riverside, 2024) spanning from 1915 to 2008. The data is pre-processed in the following manner for model continual training:\n\u2022 Convert all text to lowercase."}]}