{"title": "Analyzing Fine-tuning Representation Shift for Multimodal LLMs Steering", "authors": ["Pegah Khayatan", "Mustafa Shukor", "Jayneel Parekh", "Matthieu Cord"], "abstract": "Multimodal LLMs have reached remarkable levels of proficiency in understanding multimodal inputs, driving extensive research to develop increasingly powerful models. However, much less attention has been paid to understanding and explaining the underlying mechanisms of these models. Most existing explainability research examines these models only in their final states, overlooking the dynamic representational shifts that occur during training. In this work, we systematically analyze the evolution of hidden state representations to reveal how fine-tuning alters the internal structure of a model to specialize in new multimodal tasks. Using a concept-based approach, we map hidden states to interpretable visual and textual concepts, enabling us to trace changes in encoded concepts across modalities as training progresses. We also demonstrate the use of shift vectors to capture these concepts changes. These shift vectors allow us to recover fine-tuned concepts by shifting those in the original model. Finally, we explore the practical impact of our findings on model steering, showing that we can adjust multimodal LLMs behaviors without any training, such as modifying answer types, captions style, or biasing the model toward specific responses. Our work sheds light on how multimodal representations evolve through fine-tuning and offers a new perspective for interpreting model adaptation in multimodal tasks. The code for this project is publicly available at https://github.com/mshukor/xl-vlms.", "sections": [{"title": "1. Introduction", "content": "With the rapid progress in Large Language Models (LLMs) [7, 10, 22, 34, 52], Multimodal LLMs (MLLMs) [3, 11, 27, 30] have recently demonstrated remarkable capabilities in addressing complex multimodal tasks such as image caption-ing and visual question-answering.\nMLLMs are typically composed of a visual encoder, an LLM, and an intermediary connector. Following initial uni-modal pretraining\u2014and, in many cases, multimodal pre-training on large datasets-these models can be further spe-"}, {"title": "2. Related Work", "content": "Concept-based explainability. Concept-based explainabil-ity methods have emerged as an alternative to traditional fea-ture attribution based methods, that are capable of extracting key semantic features from the model internal representa-tions. Most post-hoc concept explainability approaches are based on the idea of concept activation vectors (CAV) [23], which represent concepts as vectors in the activation space. Instead or relying on human annotations, recent works have proposed methods to automatically discover concepts via clustering [17, 57] or matrix decomposition [14], which can be viewed as instances of a dictionary learning problem [13]. Initially focusing on understanding vision models, dictionary learning for concept extraction has been extended to LLMs, for instance, in the form of sparse autoencoders [21, 41]. However, none of the prior approaches have been applied to understand multimodal models, with the exception of recently proposed CoX-LMM [37].\nMLLMs and Explainability. Multimodal LLMs [3, 27, 30, 54] have recently garnered significant interest. These models generally adopt a late fusion architecture, which includes an image encoder [15, 40, 58], a connector mod-ule, and an LLM [22, 50, 52]. This family of models has inspired extensive research to better understand them and explain their behavior. For example, studies like [20, 35, 44] seek to identify multimodal neurons within LLMs or analyze modality-specific sub networks [45]. Given that these mod-els are text-generative, some methods leverage this property to simply generate textual explanations for model outputs [9, 16, 48, 56]. Based on LLMs, multimodal models ben-efit from in-context learning capabilities, which have been examined for limitations, including biases [4] and links to"}, {"title": "3. Notations and Background", "content": "This section covers the background and the notations.\nModel architecture. A generic MLLM consists of a visual encoder \\(f_v\\), a trainable connector \\(C\\), and a language model \\(f_{LM}\\). We assume that the model is pretrained on a multi-modal dataset \\(S = \\{(x_i, Y_i)\\}\\_i\\), such as captioning, where \\(X_i \\in X\\) represents images and \\(y_i \\subset Y\\) are the associated captions specified as sequence of tokens from token vocab-ulary space \\(Y\\). The model is trained to generate the next text tokens, conditioned on text and images. The input to \\(f_{LM}\\) is a sequence of tokens that includes the concatenation of: (1) \\(N_v\\) visual tokens extracted from the image \\(x\\) via the visual encoder followed by the connector (\\(C(f_v(x))\\)), and (2) linearly embedded textual tokens corresponding to the text instruction and previously predicted tokens. This can be expressed as:\n\\(\u0177P = f_{LM}(h^1,...,h^{N_v}....,h^P),\\)\nwhere \\(h^1,...,h^{N_v} = C(f_v(x))\\), and \\(h_p = Emb(\u0177_{p-1})\\), with Emb representing the token embedding layer. During generation, the output token \\(\u00fb^p\\) is derived by normalizing the last layer (\\(L\\)) tokens \\(h^L\\), then applying the unembed-ding layer \\(W_U\\) and a softmax operation. The model keeps"}, {"title": "4. Fine-tuning and evolution of concept repre-sentations", "content": "We aim to study how fine-tuning process affects the concepts learned by the model. Then we try to recover the fine-tuned model concepts using shift vectors computed in the feature space.\nImplementation details. We focus on MLLMs with the architecture described in Section 3. The image encoder \\(f_v\\) is a ViT-L/14 CLIP [40], followed by a transformer-based connector that reduce the number of encoded visual tokens. The LLM \\(f_{LM}\\) is an OPT-6.7B model [59] with 32 layers. We conduct our study in a controlled setups that consists of specializing the model on a target dataset. Specifically, we fine-tune on three different subsets of the Visual Genome dataset [24], related to places, colors, and sentiments (see App. A for more details)."}, {"title": "4.1. Impact of fine-tuning on learned concepts", "content": "The fine-tuning process introduces changes in the overall structure of the learned concepts. This change can be ob-served through the concepts groundings in both the text and image space (Fig. 3)."}, {"title": "Matched concepts.", "content": "Matched concepts. To analyze how each concept changes after fine-tuning, we focus on each concept and its match"}, {"title": "Concept evolution.", "content": "Concept evolution. To quantify how much a concept \\(u \\in U^a\\) is changed after fine-tuning, we compute the overlap between its grounding words and those of its closest matching concept from the fine-tuned model \\(U^b\\). Specifi-cally, we compute \\(T\\text{-}Overlap(u, u_{m(i)})\\) (Eq. (3)) for all the concepts \\(i \\in \\{1, . . ., K\\}\\), and visualize them for different fine-tunings in Fig. 5. We observe varying rates of change across different concepts and fine-tunings. This might be due to the difference in the fine-tuning dataset size, complexity, or similarity to the original dataset. It also highlights 2 main behaviors, detailed as follows:\n\u2022 Concepts that are refined. This group contains the con-cepts that slightly change to be more specialized towards the fine-tuning task (Fig. 6a). These concepts exhibit a relatively high (\\(T\\text{-}Overlap(u, u_{m(i)})\\)).\n\u2022 Concepts that change completely. This group contains the concepts that emerge (Fig. 6b) or disappear (Fig. 6c) in the fine-tuned model. New concepts emerge in the fine-tuned model, likely due to the introduction of novel patterns or relationships not present in the original model. These concepts exhibit a relatively low (\\(T\\text{-}Overlap(u, u_{m(i)})\\)).\nWe also notice that the T-Overlap decreases with the num-ber of training iterations, indicating that fine-tuning leads to deviation from original model concepts (see App. A for more details)."}, {"title": "4.2. Recovering concepts in fine-tuned model via shift vectors", "content": "In this section, we study if the learned concepts in the fine-tuned model can be recovered from the original model con-cepts, without any training or decomposition. In particular, we propose to characterize the change in original concepts due to fine-tuning as linear directions in the feature space, referred to these as concept shift vectors.\nShifting original model concepts. To compute the con-cept shift vectors, we first associate each concept \\(u^a_k\\) in the original model with a subset of samples common to the analysis of both \\(f^a\\) and \\(f^b\\). Specifically, projecting the repre-sentation of each sample \\(x_m \\in X^{toi}_a \\cap X^{toi}_b\\) on \\(U^a\\) and selecting those that activate \\(u^a_k\\) the most:\n\\(A_k = \\{m \\mid k = arg max |v_i^a(x_m)|\\}.\\)\nFor each sample \\(x_m\\), \\(m \\in A_k\\) we define \\(d_m^{ab} = b_m - a_m\\) as the change in its representation from \\(f^a\\) to \\(f^b\\). To compute the concept shift vector \\(\\Delta \\vec(u^a_k)\\) associated with \\(u^a_k\\), we aggregate shifts of its associated samples specified by \\(A_k\\):\n\\(\\Delta \\vec(u_k^a) = \\frac{1}{|A_k|} \\Sigma_{m \\in A_k} (b_m-a_m). \\)\nThe concept shift vector is used to shift each concept in the original model \\(u^a_i\\) to obtain the shifted concept \\(u^\\ast_i\\):\n\\(u_i^{\\ast} = u_i + \\alpha \\Delta_{ab}(u_i),\\)"}, {"title": "Evaluating fine-tuned concept recovery.", "content": "Evaluating fine-tuned concept recovery. To study if the fine-tuned concepts \\(U^b\\) can be recovered from the original ones \\(U^a\\), we first establish a matching \\((m: i \\rightarrow j)\\) be-tween the set of original \\(\\{u_i^a\\}\\_{i=1}^K\\) and fine-tuned concepts \\(\\{u_i^b\\}\\_{i=1}^K\\). For systematic evaluation of recovery of all fine-tuned concepts, we constrain \\(m\\) to be bijective (see App. A for more details). Finally, we evaluate how well a shifted concept \\(u_i^{\\ast}\\) is similar to its match \\(u_{m(k)}^b\\) using the overlap metrics. Fig. 7 shows the results of recovering the fine-tuned concepts for models fine-tuned on different subsets of the VG dataset (place, color, sentiment). We compare the text grounding overlap between the shifted \\(u_i^{\\ast}\\) and fine-tuned concepts \\(u_{m(k)}^b\\). We use the overlap between the original concepts \\(u_i^a\\) with the fine-tuned ones as a baseline. As we can see, most shifted concepts exhibit higher overlap than the original ones. Many of them have very high overlap close to 100% (full recovery).\nShift magnitude (\\(\\alpha\\)) and concepts recovery. We also study the amount of recovery for shifted concepts, obtained with different shift magnitudes \\(\\alpha\\). We report the average recovery over \\(K = 20\\) concepts for each fine-tuning task for different \\(\\alpha\\) values in Fig. 8. Note that \\(\\alpha = 0\\) corresponds to original concepts. \\(\\alpha = 1\\) generally corresponds to the most optimal value of shift magnitude (color, sentiment fine-tuning) or very close to the optimal value (place fine-tuning). This indicates that simply adding the mean shift vector to the original concept without scaling, generally offers the best"}, {"title": "Which concepts are recovered?", "content": "Which concepts are recovered? We examine the poten-tial correlation between the computed concept shift vectors and the recovery of concepts in the fine-tuned model. We hypothesize that if samples related to an original concept consistently and significantly shift toward the corresponding fine-tuned model concepts, the shift vector, computed based on these samples, should be more effective at recovering this concept. Assuming that individual shifts \\(\\Delta\\vec_m\\) associated with each original concept tend to have similar magnitudes, their aggregation \\(\\Delta\\vec_k\\) should have a higher magnitude when they are aligned. We validate that this assumption is reasonable for most concepts in our analysis (App. A).\nFor discovered concepts across all three fine-tuning tasks, we measure the correlation between the magnitude of the concept shift vectors \\(||\\Delta\\vec_k||\\) and concepts recovery, here, measured as the cosine similarity between the shifted and matched fine-tuned concept. For a given shifted concept \\(u_k^{\\ast}\\), this recovery (CR) is calculated as:\n\\(CR_k = \\frac{cos(u_{m(k)}^b, u_i^{\\ast}) - cos(u_{m(k)}^b, u_i^a)}{cos(u_{m(k)}^b, u_i^a)}\\)(6)\nThis is illustrated in Fig. 9, where we observe a notice-able correlation between the two quantities, which further strengthens our hypothesis."}, {"title": "5. Fine-grained multimodal LLM steering", "content": "Model steering (see Fig. 10) refers to guiding the model outputs towards desired outcomes by modifying the features without altering the model weights. Here, we investigate this technique, focusing on large multimodal models for visual question-answering and image captioning tasks. We provide additional qualitative results and ablation study in App. B.\nMotivation. In the previous sections, we demonstrate the feasibility of recovering target concepts in fine-tuned models by shifting the original model features along shift vectors. In addition, we find that the features related to different concepts are almost linearly separable (Fig. 11), and this becomes more prominent in deeper layers. This validates the linear representation hypothesis for MLLMs, previously studied for LLMs [32, 38]. Finally, model steering might be an alternative method to avoid costly fine-tuning."}, {"title": "5.1. Multimodal LLMs steering framework.", "content": "Setup. For a text-generative multimodal model \\(f : T \\times I \\rightarrow T\\), we aim to modify the model's output \\(\u0177\\) to a desired"}, {"title": "5.2. Coarse-grained model steering", "content": "In coarse-grained or global steering, the objective is to adjust the model outputs \\(\u0177\\) to generally align with a set of target samples (e.g., changing answers type). Given input-output samples, we first extract the answer representations \\(B = \\{b_1,..., b_N\\}\\) at a specified layer \\(l\\) (we drop the layer index for simplicity) from the target sample set. Similarly, we obtain representations for a set of original samples \\(A = \\{a_1, ..., a_M\\}\\), for example, randomly drawn from the training set. We then compute the coarse steering vector \\(s_c\\) as follows:\n\\(s_c = \\frac{\\Sigma_i^N b_i}{\\Sigma_i^M a_i},\\)(7)\n\\(s_c\\) is applied to all the samples in the validation set. For instance, the activations \\(z_i\\) of a sample \\(x_i\\) (at the same layer \\(l\\)), are changed as follows:\n\\(\\tilde z_i = z_i + \\alpha s_c, z_i = f_l(x_i),\\)(8)\nwhere \\(\\alpha\\) controls the steering strength and it is set to 1 (we study \\(\\alpha\\) in App. B). \\(\\tilde z_i\\) replaces \\(z_i\\) and becomes the input to the next layer \\(l + 1\\).\nExperiments. We steer the model answers towards a par-ticular type among; yes/no, numbers or other (e.g. colors, objects). For each target answer type, we compute a steering vector. Table 1 shows that the number of answer types in-creases significantly when applying the corresponding steer-ing vector, this validates the efficacy of globally steering the model."}, {"title": "5.3. Discovering meaningful and fine-grained steer-ing directions.", "content": "Unlike global steering, fine-grained steering focuses on ad-justing outputs at the concept level. Specifically, we seek editing directions that adjust only certain concepts. To do this, we decompose the hidden states of a set of samples into a set of concepts \\(U\\). We then compute a series of fine-grained steering vectors \\(s_{ij}^f = s_{11},..., s_{NN}\\), where each \\(s_{ij}^f\\) represents the steering vector from concept \\(u_i\\) to \\(u_j\\):\n\\(s_{ij}^f = u_j - u_i.\\)(9)\nHowever, not all computed vectors are necessarily mean-ingful steering vectors. We identify these vectors based on the strongest impact on guiding the model towards generat-ing specific answers or concepts (e.g. producing significantly more target answers). This is more detailed in App. B.\nExperiments. Fig. 12 illustrates some steering vectors. We try to find the steering vectors between concepts decomposed from 3 sample sets corresponding to: \"yes/no\", \"number\""}, {"title": "5.4. Steering towards a specific target answer", "content": "Motivated by the previous section, the objective here is to steer the model towards a specific answer specified by the user. For each pair of original/target answers (e.g., yes/no), we collect few hundred samples and compute the steering vector (as in Section 5.2). Then we apply the vectors on all samples in the validation set. In Table 2, we report the evaluation metrics when targeting the last layer. We can"}, {"title": "5.5. Steering image captions", "content": "In our earlier experiments, we applied steering on relatively brief answers from the VQAv2 dataset. Here, we extend this approach to longer, descriptive outputs using the COCO image captioning dataset [29]. Given that multiple captions can effectively describe an image by emphasizing various aspects such as the main object, surroundings, actions, or events, our focus is modifying captions to align with a spe-cific target style (see Fig. 13). We presents the results in Table 3, demonstrating that captions can be steered towards a target style so that they focus more on places, colors, or sentiments. These findings validate the feasibility of steering MLLMs on tasks with longer responses."}, {"title": "6. Discussion", "content": "Limitations. Ideally, steering should be targeted; shifting from a \"yes\" to a \"no\" should affect only questions initially answered with \"yes\". While this is often achieved, it is not perfect. We also notice a tradeoff between the steer-ing strength and the quality of generated responses. This approach focuses on changing the model answers or style, however, it can be extended to other applications, such as addressing biases or mitigating safety concerns. It is also interesting to extend our study to larger and more recent MLLMs, eventually with different architectures."}]}