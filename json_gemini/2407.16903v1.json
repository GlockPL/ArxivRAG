{"title": "US-China perspectives on extreme AI risks and global governance", "authors": ["Akash R. Wasil", "Tim Durgin"], "abstract": "The United States and China will play an important role in navigating safety and security\nchallenges relating to advanced artificial intelligence. We sought to better understand how\nexperts in each country describe safety and security threats from advanced artificial intelli-\ngence, extreme risks from AI, and the potential for international cooperation. Specifically,\nwe compiled publicly-available statements from major technical and policy leaders in both\nthe United States and China. We focused our analysis on advanced forms of artificial in-\ntelligence, such as artificial general intelligence (AGI), that may have the most significant\nimpacts on national and global security. Experts in both countries expressed concern about\nrisks from AGI, risks from intelligence explosions, and risks from AI systems that escape\nhuman control. Both countries have also launched initial efforts designed to promote in-\nternational cooperation around safety standards and risk management practices. Notably,\nour findings only reflect information from publicly available sources. Nonetheless, our find-\nings can inform policymakers and researchers about the state of AI discourse in the US and\nChina. We hope such work can contribute to policy discussions around advanced AI, its\nglobal security threats, and potential international dialogues or agreements to mitigate such\nthreats.", "sections": [{"title": "Introduction", "content": "Artificial intelligence is a transformative technology with major implications for national\nand global security. Many AI experts have expressed concerns about AI-related national and\nglobal security threats. Examples include risks from AI-enabled biological weapons, risks from\nautonomous AI systems that escape human control, and risks from AI applied in military oper-\nations (Bengio et al., 2024; Hendrycks et al., 2023).\n\nThe United States and China are the world's leaders in AI development. In 2022, the size\nof the AI market in the US amounted to USD 103.7B (Statista-1, 2024), while the size of the\nChinese AI market was approximately USD 40B (Statista-2, 2024). As of 2022, 26 percent of\nthe world's top AI researchers came from China, while 28 percent came from the United States\n(Yang 2024). US companies dominate the top of the large language model (LLM) rankings:\nexamples include Open AI's Chat-GPT, Anthropic's Claude, and Google's Gemini (Guinness,\n2024). While most Chinese LLMs lag quite a bit behind, there has been progress in recent years.\nSpecifically, Moonshot AI's Kimi model, under certain conditions, can achieve performance\ncomparable to Chat GPT-4 (Zhang, 2023).\n\nWe aimed to acquire a better understanding of AI policy discourse in China and the United\nStates. Discourse about US-China relations is often partisan, rather than grounded in objective\ndata. Discussions about US-China AI policy and potential international agreements should be\nrooted in a concrete understanding of attitudes toward AI, safety and security, and international\ncooperation in both countries. To improve our understanding of these topics, we compiled pub-\nlicly available statements from Chinese and US officials regarding artificial intelligence, safety\nand security risks, and international cooperation and global governance. We focus our analysis\non statements regarding the most powerful forms of artificial intelligence (e.g., artificial gen-\neral intelligence or\u201cAGI\") and the most extreme risks posed by AI (e.g., developing biological\nweapons, developing novel kinds of weapons of mass destruction, or escaping human control.)\""}, {"title": "Methodology", "content": "We included publicly available material about extreme AI risks, advanced AI, safety and security\nchallenges, and international governance. Our approach was semi-structured and not meant to\nbe systematic or comprehensive. For both searches, we focused on statements that acknowledged\nextreme risks from AI or AI safety and security threats\u2013 notably, our aim was not to characterize\ndebates about the likelihood or plausibility of such risks. Below, we offer more details about our\napproach for both the China search and the US search.\n\nWhen searching for statements by Chinese leaders, we first identified commonly used terms for\nextreme AI risks. We started with \u901a\u7528\u4eba\u5de5\u667a\u80fd, the most commonly used word in Chinese for\nGeneral Artificial Intelligence. Using searches with this keyword, we identified additional key-\nwords such as \u751f\u5b58\u98ce\u9669 (existential risk) and \u5bf9\u9f50 (alignment). We prioritized the inclusion of\nstatements from government officials and officials from high-ranking academic institutions that\nregularly advise government officials. When we identified a relevant statement with an English\ntranslation, we used the English translation. Otherwise, the 2nd author (who is fluent in Chi-\nnese) translated the statements. Since our analysis was focused on Mainland China, we searched\nfor the keyword terms in Simplified Chinese. When searching for statements by US leaders, we\nfollowed a similar procedure. We focused on statements from policymakers or statements given"}, {"title": "Results", "content": ""}, {"title": "China", "content": ""}, {"title": "Safety and security", "content": "When examining the discourse among Chinese scholars and policymakers, there is a clear ac-\nknowledgment of long-term security risks from advanced AI development. The discourse in-\ncludes descriptions of specific safety and security concerns- such as intelligence explosions,\nthe potential for self-replication and deception, and the importance of AGI security. Chinese\ngovernment officials are concerned about AI's potential to generate \u201cunhealthy\" or \"illegal", "added)": "n\n\u2022 \"If there is an intelligence explosion once AI has evolved to a certain level, the default\nresult will inevitably be catastrophic.", "[Referring to AI] Every technology is like nuclear technology-if humankind had a choice,\nperhaps it would be best not to discover dangerous materials like uranium and ra-\ndium.": "Ya-Qin Zhang, director of the Tsinghua Institute for AI Industry Research (Zhang,\nYa-Qin, 2023)\n\n\u2022 \u201cService providers must pay close attention to the generative AI's potential long-term\nrisks. They must treat with caution any artificial intelligence (models) with the ability\nto deceive humans, self-replicate, or self-modify.\u201d \u2013 Gen. AI Security Requirements\ninstituted by the National Information Security Standardization Technical Committee of\nChina (TC260)2 (TC260, 2024)\n\n\u2022 \u201c(AI systems) must use keywords, classification models, and other methods to monitor\nuser input. If the user makes 3 inputs in a row that contain illegal, unhealthy informatio\nand/or are obviously designed to induce the AI system to generate unhealthy information,\nor if the user makes a total of 5 such inputs in one day, approved, legal methods shall\nbe used to suspend said user's (AI) services.\u201d\u2013 TC260 Gen. AI Security Requirements\n(TC260, 2024)"}, {"title": "International cooperation and global governance", "content": "When examining discourse on international cooperation and global governance in China, we\nobserved several expressions of interest in global cooperation on AI and risk management. More\nspecifically, the Chinese government has expressed a desire to promote tiered testing systems\nwith different requirements based on AI risk levels. Additionally, China is very concerned about\nensuring equal access to AI technology across global borders.\n\nExample quotes:\n\n\u2022 \"We should promote the establishment of a testing and assessment system based on AI\nrisk levels, implement agile governance, and carry out tiered and category-based manage-\nment for rapid and effective response.\u201d \u2013 China's AI Global Governance Initiative (Em-\nbassy of the People's Republic of China, 2023)\n\n\u2022 \"We support discussions within the United Nations framework to establish an interna-\ntional institution to govern AI.\u201d \u2013 China's AI Global Governance Initiative (Embassy of\nthe People's Republic of China, 2023)\n\n\u2022 \"We call for global collaboration to foster the sound development of AI, share A\u0399\nknowledge, and make AI technologies available to the public under open-source terms.\u201d \u2013\nChina's AI Global Governance Initiative (Embassy of the People's Republic of China,\n2023)\n\n\u2022 \"AI must be fair. All countries should be able to participate in AI governance and share\nin its benefits; no country should be left behind.\" \u2013 China's AI Global Governance\nInitiative (Embassy of the People's Republic of China, 2023)\n\n\u2022 \"AI research and governance need the participation and cooperation of people in differ-\nent fields all over the world. In addition to those engaged in AI R&D and use, they also\""}, {"title": "United States", "content": ""}, {"title": "Safety and Security", "content": "Discourse among American scholars and policymakers also includes clear acknowledgment of\nlong-term security risks from advanced AI development. AI experts in the US have expressed\nstrong concern about potential catastrophic outcomes from advanced AI. More recently, policy-\nmakers have begun to discuss artificial general intelligence and its risks. Many US policymakers\nand AI experts recognize the potential for extreme risks, both from malicious use of advanced\nAI and from potential loss of control scenarios. US policymakers have also expressed an interest\nin learning more about artificial general intelligence, standardizing the definition of AGI, under-\nstanding the likelihood and magnitude of AGI risks, and implementing policies to prepare for\nAGI risks.\n\nExample quotes:\n\n\u2022 \"There is a spectrum of problems we could face related to this, at the extreme end of which\n[are] concerns about whether a sufficiently powerful AI, without appropriate safeguards,\ncould be a threat to humanity as a whole-referred to as existential risk. Left unchecked,\nhighly autonomous, intelligent systems could also be misused or simply make catas-\ntrophic mistakes.\u201d \u2013 Dario Amodei, testifying before the Senate Judiciary Committee\n(Amodei, 2023).\n\n\u2022 \"The third possibility, which could emerge in as little as a few years, is that of loss of con-\ntrol, when an Al is given a goal that includes or implies maintenance of its own agency,\nwhich is equivalent to a survival objective... If the AI is misspecified, powerful enough,\nand exploits a loophole in its goals, the consequences could be unforeseen and severe.\nTherefore, a reactive approach to mitigating misspecified goals could be extremely costly\nfor society, and we may only have a few chances of getting the alignment right for super-\nhuman AI.\u201d \u2013 Yoshua Bengio, testifying before the Senate Judiciary Committee (Bengio,\n2023).\n\n\u2022 \"As long as there are really thoughtful people, like Dr. Hinton or others, who worry about\nthe existential risks of artificial intelligence\u2014the end of humanity\u2014I don't think we\ncan afford to ignore that... Even if there's just a one in a 1000 chance, one in a 1000 hap-\npens. We see it with hurricanes and storms all the time.\u201d \u2013 Representative Beyer (Henshall,\n2024).\n\n\u2022 \u201cThe AI Working Group recognizes that there is not widespread agreement on the defini-\ntion of AGI or threshold by which it will officially be achieved. Therefore, we encourage\nthe relevant committees to better define AGI in consultation with experts, characterize"}, {"title": "International cooperation and global governance", "content": "When examining discourse in the United States around international cooperation and global gov-\nernance, we observed an interest in international cooperation on AI standards and risk manage-\nment practices. For example, a recent bill has outlined specific ways that the US can cooperate\nwith international partners on safety standards, and the recently established US AI Safety Insti-\ntute has highlighted international cooperation in its strategic vision.\n\nExample quotes:\n\n\u2022 \u201c[This bill] directs the heads of Commerce, State and the Office of Science and Technology\nPolicy (\u201cOSTP\u201d) to jointly seek to form alliances or coalition with like-minded coun-\ntries to cooperate on approaches to innovation in AI and to coordinate and promote\nthe development and adoption of common AI standards... [the bill] ensures that par-\nticipating countries maintain adequate research security measures, intellectual property\nprotections, safety standards, and risk management approaches.\u201d \u2013 Bill summary of the\nFuture of AI Innovation Act (Senate Committee on Commerce, Science, and Transporta-\ntion, 2024).\n\n\u2022 \u201c[The US AI Safety Institute aims to] lead an inclusive, international network on the\nscience of AI safety. AI safety practices must be globally adopted to the greatest extent\npossible. We intend to serve as a partner for other AI Safety Institutes, national research\norganizations, and multilateral entities like the OECD and G7. We intend to work with\nour partners to foster commonly accepted scientific methodologies with the intention of\ndeveloping a shared and interoperable suite of AI safety evaluations and agreed-upon risk\nmitigations. In doing so, we hope to help develop the science and practices that underpin\nfuture arrangements for international AI governance.\" US AI Safety Institute (NIST,\n2024)\""}, {"title": "Joint statements", "content": ""}, {"title": "Safety and security", "content": "Joint statements including representatives from the United States and China have acknowledged\nsafety and security threats from AI. Several of these statements were made in the context of\ninternational summits or track II dialogues.\n\nExample quotes:\n\n\u2022 \u201cParticular safety risks arise at the frontier of AI. Substantial risks may arise from\npotential intentional misuse or unintended issues of control relating to alignment with\nhuman intent.\u201d (Bletchley Declaration, 2023)4\n\n\u2022 \u201cUnsafe development, deployment, or use of AI systems may pose catastrophic or\neven existential risks to humanity within our lifetimes. These risks from misuse and\nloss of control could increase greatly as digital intelligence approaches or even surpasses\nhuman intelligence.\u201d (International Dialogues on AI Safety, 2024)5\n\n\u2022 \u201cCoordinated global action on AI safety research and governance is critical to prevent\nuncontrolled frontier AI development from posing unacceptable risks to humanity.\u201d (In-\nternational Dialogues on AI Safety, 2023)\n\n\u2022 \"We call on leading AI developers to make a minimum spending commitment of one-\nthird of their AI R&D on AI safety and for government agencies to fund academic and non-\nprofit AI safety and governance research in at least the same proportion.\u201d (International\nDialogues on AI Safety, 2023).\n\n\u2022 \"Safe, secure and trustworthy artificial intelligence systems... are such that they are\nhuman-centric, reliable, explainable, ethical, inclusive, in full respect, promotion and pro-\ntection of human rights and international law, privacy-preserving, sustainable development-\noriented, and responsible.\" (United Nations General Assembly, 2024).\""}, {"title": "International cooperation and global governance", "content": "Joint statements including representatives from the United States and China have also recog-\nnized the importance of global cooperation to avoid some of the most catastrophic outcomes of\nadvanced AI development. Such statements have emphasized that risks arising from AI are \u201cin-\nherently international in nature\u201d and suggested that governments should work together to define\n\"clear red lines that, if crossed, mandate immediate termination of an AI system.\"\n\nExample quotes:"}, {"title": "Discussion", "content": "We compiled statements about advanced Al discourse in China and the United States, with a\nfocus on statements about safety and security risks and international cooperation. Overall, major\ntechnical officials and policymakers in both countries have acknowledged extreme risks from\nadvanced AI development. More specifically, experts in both China and the United States seem to\nhave a shared understanding of some of the concrete safety and security challenges. For example,\ntop Chinese scientists discuss concepts like \u201cintelligence explosions\u201d, risks from systems that\ncan \"self-replicate or self-modify\u201d, and risks from systems that \u201cdo not hesitate to harm human\nbeings", "loss of control": "cenarios and calling for more work on \u201cthe\nrisks that AGI development would pose.", "content\nthat violates Socialist values": "such as content that can \u201cincite the subversion of state power", "illegal, unhealthy information.": "o highlight another differ-\nence, the United States discourse included details about the role of the US AI Safety Institute in\nadvancing the science of AI safety and setting international standards. Several countries-includ-\ning the United States and United Kingdom-recently agreed to establish an international network\nof AI safety institutes to \u201cforge a common understanding of AI safety"}]}