{"title": "US-China perspectives on extreme AI risks and global governance", "authors": ["Akash R. Wasil", "Tim Durgin"], "abstract": "The United States and China will play an important role in navigating safety and security challenges relating to advanced artificial intelligence. We sought to better understand how experts in each country describe safety and security threats from advanced artificial intelligence, extreme risks from AI, and the potential for international cooperation. Specifically, we compiled publicly-available statements from major technical and policy leaders in both the United States and China. We focused our analysis on advanced forms of artificial intelligence, such as artificial general intelligence (AGI), that may have the most significant impacts on national and global security. Experts in both countries expressed concern about risks from AGI, risks from intelligence explosions, and risks from AI systems that escape human control. Both countries have also launched initial efforts designed to promote international cooperation around safety standards and risk management practices. Notably, our findings only reflect information from publicly available sources. Nonetheless, our findings can inform policymakers and researchers about the state of AI discourse in the US and China. We hope such work can contribute to policy discussions around advanced AI, its global security threats, and potential international dialogues or agreements to mitigate such threats.", "sections": [{"title": "Introduction", "content": "Artificial intelligence is a transformative technology with major implications for national and global security. Many AI experts have expressed concerns about AI-related national and global security threats. Examples include risks from AI-enabled biological weapons, risks from autonomous AI systems that escape human control, and risks from AI applied in military operations (Bengio et al., 2024; Hendrycks et al., 2023).\n\nThe United States and China are the world's leaders in AI development. In 2022, the size of the AI market in the US amounted to USD 103.7B (Statista-1, 2024), while the size of the Chinese AI market was approximately USD 40B (Statista-2, 2024). As of 2022, 26 percent of the world's top AI researchers came from China, while 28 percent came from the United States (Yang 2024). US companies dominate the top of the large language model (LLM) rankings: examples include Open AI's Chat-GPT, Anthropic's Claude, and Google's Gemini (Guinness, 2024). While most Chinese LLMs lag quite a bit behind, there has been progress in recent years. Specifically, Moonshot AI's Kimi model, under certain conditions, can achieve performance comparable to Chat GPT-4 (Zhang, 2023).\n\nWe aimed to acquire a better understanding of AI policy discourse in China and the United States. Discourse about US-China relations is often partisan, rather than grounded in objective data. Discussions about US-China AI policy and potential international agreements should be rooted in a concrete understanding of attitudes toward AI, safety and security, and international cooperation in both countries. To improve our understanding of these topics, we compiled publicly available statements from Chinese and US officials regarding artificial intelligence, safety and security risks, and international cooperation and global governance. We focus our analysis on statements regarding the most powerful forms of artificial intelligence (e.g., artificial general intelligence or\u201cAGI\") and the most extreme risks posed by AI (e.g., developing biological weapons, developing novel kinds of weapons of mass destruction, or escaping human control.)\""}, {"title": "Methodology", "content": "We included publicly available material about extreme AI risks, advanced AI, safety and security challenges, and international governance. Our approach was semi-structured and not meant to be systematic or comprehensive. For both searches, we focused on statements that acknowledged extreme risks from AI or AI safety and security threats\u2013 notably, our aim was not to characterize debates about the likelihood or plausibility of such risks. Below, we offer more details about our approach for both the China search and the US search.\n\nWhen searching for statements by Chinese leaders, we first identified commonly used terms for extreme AI risks. We started with \u901a\u7528\u4eba\u5de5\u667a\u80fd, the most commonly used word in Chinese for General Artificial Intelligence. Using searches with this keyword, we identified additional keywords such as \u751f\u5b58\u98ce\u9669 (existential risk) and \u5bf9\u9f50 (alignment). We prioritized the inclusion of statements from government officials and officials from high-ranking academic institutions that regularly advise government officials. When we identified a relevant statement with an English translation, we used the English translation. Otherwise, the 2nd author (who is fluent in Chinese) translated the statements. Since our analysis was focused on Mainland China, we searched for the keyword terms in Simplified Chinese. When searching for statements by US leaders, we followed a similar procedure. We focused on statements from policymakers or statements given"}, {"title": "Results", "content": "to government bodies (e.g., testimony to Congressional hearings).\n\nWhen determining which statements to include, we prioritized statements by senior faculty at top universities, experts working at frontier AI companies, policymakers and government officials, and officials who would likely advise policymakers. We were also most interested in characterizing statements relating to extreme risks related to advanced AI, safety and security, and the potential for international cooperation or global governance."}, {"title": "China", "content": ""}, {"title": "Safety and security", "content": "When examining the discourse among Chinese scholars and policymakers, there is a clear acknowledgment of long-term security risks from advanced AI development. The discourse includes descriptions of specific safety and security concerns- such as intelligence explosions, the potential for self-replication and deception, and the importance of AGI security. Chinese government officials are concerned about AI's potential to generate \u201cunhealthy\" or \"illegal\" information. Finally, some influential minority of Chinese scholars have expressed concerns about potential existential risks created by advanced AI.\n\nExample quotes (bolding added):\n\n\u2022 \"If there is an intelligence explosion once AI has evolved to a certain level, the default result will inevitably be catastrophic.\" \u2013 Paper by Scientists from Peking University's Dept. of Computer Science\u00b9\n\n\u2022 \"[Referring to AI] Every technology is like nuclear technology-if humankind had a choice, perhaps it would be best not to discover dangerous materials like uranium and radium.\" - Ya-Qin Zhang, director of the Tsinghua Institute for AI Industry Research (Zhang, Ya-Qin, 2023)\n\n\u2022 \u201cService providers must pay close attention to the generative AI's potential long-term risks. They must treat with caution any artificial intelligence (models) with the ability to deceive humans, self-replicate, or self-modify.\u201d \u2013 Gen. AI Security Requirements instituted by the National Information Security Standardization Technical Committee of China (TC260)\u00b2 (TC260, 2024)\n\n\u2022 \u201c(AI systems) must use keywords, classification models, and other methods to monitor user input. If the user makes 3 inputs in a row that contain illegal, unhealthy informatio and/or are obviously designed to induce the AI system to generate unhealthy information, or if the user makes a total of 5 such inputs in one day, approved, legal methods shall be used to suspend said user's (AI) services.\u201d\u2013 TC260 Gen. AI Security Requirements (TC260, 2024)"}, {"title": "International cooperation and global governance", "content": "When examining discourse on international cooperation and global governance in China, we observed several expressions of interest in global cooperation on AI and risk management. More specifically, the Chinese government has expressed a desire to promote tiered testing systems with different requirements based on AI risk levels. Additionally, China is very concerned about ensuring equal access to AI technology across global borders.\n\nExample quotes:\n\n\u2022 \"We should promote the establishment of a testing and assessment system based on AI risk levels, implement agile governance, and carry out tiered and category-based management for rapid and effective response.\u201d \u2013 China's AI Global Governance Initiative (Embassy of the People's Republic of China, 2023)\n\n\u2022 \"We support discussions within the United Nations framework to establish an international institution to govern AI.\u201d \u2013 China's AI Global Governance Initiative (Embassy of the People's Republic of China, 2023)\n\n\u2022 \"We call for global collaboration to foster the sound development of AI, share AI knowledge, and make AI technologies available to the public under open-source terms.\u201d \u2013 China's AI Global Governance Initiative (Embassy of the People's Republic of China, 2023)\n\n\u2022 \"AI must be fair. All countries should be able to participate in AI governance and share in its benefits; no country should be left behind.\" \u2013 China's AI Global Governance Initiative (Embassy of the People's Republic of China, 2023)\n\n\u2022 \"AI research and governance need the participation and cooperation of people in different fields all over the world. In addition to those engaged in AI R&D and use, they also need the participation of people in different fields such as law, morality and ethics. We need to clarify the standards of ethics and morality and what it means to be 'moral' and 'ethical'.\" \u2013 Bo Zhang Professor, Tsinghua University's Department of Computer Science\u00b3 (Zhang, 2022)"}, {"title": "United States", "content": ""}, {"title": "Safety and Security", "content": "Discourse among American scholars and policymakers also includes clear acknowledgment of long-term security risks from advanced AI development. AI experts in the US have expressed strong concern about potential catastrophic outcomes from advanced AI. More recently, policymakers have begun to discuss artificial general intelligence and its risks. Many US policymakers and AI experts recognize the potential for extreme risks, both from malicious use of advanced AI and from potential loss of control scenarios. US policymakers have also expressed an interest in learning more about artificial general intelligence, standardizing the definition of AGI, understanding the likelihood and magnitude of AGI risks, and implementing policies to prepare for AGI risks.\n\nExample quotes:\n\n\u2022 \"There is a spectrum of problems we could face related to this, at the extreme end of which [are] concerns about whether a sufficiently powerful AI, without appropriate safeguards, could be a threat to humanity as a whole-referred to as existential risk. Left unchecked, highly autonomous, intelligent systems could also be misused or simply make catastrophic mistakes.\u201d \u2013 Dario Amodei, testifying before the Senate Judiciary Committee (Amodei, 2023).\n\n\u2022 \"The third possibility, which could emerge in as little as a few years, is that of loss of control, when an Al is given a goal that includes or implies maintenance of its own agency, which is equivalent to a survival objective... If the AI is misspecified, powerful enough, and exploits a loophole in its goals, the consequences could be unforeseen and severe. Therefore, a reactive approach to mitigating misspecified goals could be extremely costly for society, and we may only have a few chances of getting the alignment right for superhuman AI.\u201d \u2013 Yoshua Bengio, testifying before the Senate Judiciary Committee (Bengio, 2023).\n\n\u2022 \"As long as there are really thoughtful people, like Dr. Hinton or others, who worry about the existential risks of artificial intelligence\u2014the end of humanity\u2014I don't think we can afford to ignore that... Even if there's just a one in a 1000 chance, one in a 1000 happens. We see it with hurricanes and storms all the time.\u201d \u2013 Representative Beyer (Henshall, 2024).\n\n\u2022 \u201cThe AI Working Group recognizes that there is not widespread agreement on the definition of AGI or threshold by which it will officially be achieved. Therefore, we encourage the relevant committees to better define AGI in consultation with experts, characterize"}, {"title": "International cooperation and global governance", "content": "both the likelihood of AGI development and the magnitude of the risks that AGI development would pose, and develop an appropriate policy framework based on that analysis.\" \u2013 Senators Schumer, Rounds, Heinrich, and Young (Bipartisan Senate AI Working Group, 2024).\n\n\u2022 \u201cArtificial intelligence (AI) has the potential to dramatically improve and transform our way of life, but also presents a spectrum of risks that could be harmful to the American public, some of which could have catastrophic effects. Extremely powerful frontier AI could be misused by foreign adversaries, terrorists, and less sophisticated bad actors to cause widespread harm and threaten U.S. national security. Experts from across the U.S. government, industry, and academia believe that advanced AI could one day enable or assist in the development of biological, chemical, cyber, or nuclear weapons.\u201d \u2013 Senators Romney, Reed, Moran, and King in the Framework for Mitigating Extreme AI Risks (Romney et al., 2024).\n\nWhen examining discourse in the United States around international cooperation and global governance, we observed an interest in international cooperation on AI standards and risk management practices. For example, a recent bill has outlined specific ways that the US can cooperate with international partners on safety standards, and the recently established US AI Safety Institute has highlighted international cooperation in its strategic vision.\n\nExample quotes:\n\n\u2022 \u201c[This bill] directs the heads of Commerce, State and the Office of Science and Technology Policy (\u201cOSTP\u201d) to jointly seek to form alliances or coalition with like-minded countries to cooperate on approaches to innovation in AI and to coordinate and promote the development and adoption of common AI standards... [the bill] ensures that participating countries maintain adequate research security measures, intellectual property protections, safety standards, and risk management approaches.\u201d \u2013 Bill summary of the Future of AI Innovation Act (Senate Committee on Commerce, Science, and Transportation, 2024).\n\n\u2022 \u201c[The US AI Safety Institute aims to] lead an inclusive, international network on the science of AI safety. AI safety practices must be globally adopted to the greatest extent possible. We intend to serve as a partner for other AI Safety Institutes, national research organizations, and multilateral entities like the OECD and G7. We intend to work with our partners to foster commonly accepted scientific methodologies with the intention of developing a shared and interoperable suite of AI safety evaluations and agreed-upon risk mitigations. In doing so, we hope to help develop the science and practices that underpin future arrangements for international AI governance.\" US AI Safety Institute (NIST, 2024)"}, {"title": "Joint statements", "content": ""}, {"title": "Safety and security", "content": "Joint statements including representatives from the United States and China have acknowledged safety and security threats from AI. Several of these statements were made in the context of international summits or track II dialogues.\n\nExample quotes:\n\n\u2022 \u201cParticular safety risks arise at the frontier of AI. Substantial risks may arise from potential intentional misuse or unintended issues of control relating to alignment with human intent.\u201d (Bletchley Declaration, 2023)\u2074\n\n\u2022 \u201cUnsafe development, deployment, or use of AI systems may pose catastrophic or even existential risks to humanity within our lifetimes. These risks from misuse and loss of control could increase greatly as digital intelligence approaches or even surpasses human intelligence.\u201d (International Dialogues on AI Safety, 2024)\u2075\n\n\u2022 \u201cCoordinated global action on AI safety research and governance is critical to prevent uncontrolled frontier AI development from posing unacceptable risks to humanity.\u201d (International Dialogues on AI Safety, 2023)\n\n\u2022 \"We call on leading AI developers to make a minimum spending commitment of one-third of their AI R&D on AI safety and for government agencies to fund academic and non-profit AI safety and governance research in at least the same proportion.\u201d (International Dialogues on AI Safety, 2023).\n\n\u2022 \"Safe, secure and trustworthy artificial intelligence systems... are such that they are human-centric, reliable, explainable, ethical, inclusive, in full respect, promotion and protection of human rights and international law, privacy-preserving, sustainable development-oriented, and responsible.\" (United Nations General Assembly, 2024)."}, {"title": "International cooperation and global governance", "content": "Joint statements including representatives from the United States and China have also recognized the importance of global cooperation to avoid some of the most catastrophic outcomes of advanced AI development. Such statements have emphasized that risks arising from AI are \u201cinherently international in nature\u201d and suggested that governments should work together to define \"clear red lines that, if crossed, mandate immediate termination of an AI system.\"\n\nExample quotes:\n\n\u2022 \"In the depths of the Cold War, international scientific and governmental coordination helped avert thermonuclear catastrophe. Humanity again needs to coordinate to avert a catastrophe that could arise from unprecedented technology.\u201d (International Dialogues on AI Safety, 2024).\n\n\u2022 \u201cMany risks arising from AI are inherently international in nature, and so are best addressed through international cooperation.\u201d (Bletchley Declaration, 2023).\n\n\u2022 \u201cWe also recommend defining clear red lines that, if crossed, mandate immediate termination of an AI system \u2014 including all copies \u2014 through rapid and safe shut-down procedures. Governments should cooperate to instantiate and preserve this capacity. Moreover, prior to deployment as well as during training for the most advanced models, developers should demonstrate to regulators' satisfaction that their system(s) will not cross these red lines.\u201d (International Dialogues on AI Safety, 2023)"}, {"title": "Discussion", "content": "We compiled statements about advanced Al discourse in China and the United States, with a focus on statements about safety and security risks and international cooperation. Overall, major technical officials and policymakers in both countries have acknowledged extreme risks from advanced AI development. More specifically, experts in both China and the United States seem to have a shared understanding of some of the concrete safety and security challenges. For example, top Chinese scientists discuss concepts like \u201cintelligence explosions\u201d, risks from systems that can \"self-replicate or self-modify\u201d, and risks from systems that \u201cdo not hesitate to harm human beings\" in order to survive. American scientists and policymakers have also acknowledged these risks, noting that AI could produce \u201closs of control\u201d scenarios and calling for more work on \u201cthe risks that AGI development would pose.\"\n\nThere were also some noteworthy differences. For example, the Chinese discourse includes discussions about how AI may produce content that does not conform to the Chinese Communist Party's restrictions on speech. Experts have discussed how AI might help users generate \u201ccontent that violates Socialist values\u201d, such as content that can \u201cincite the subversion of state power\" or \u201coverthrow the Socialist System.\u201d Chinese standards for AI security also note that users should be suspended if they try to generate \u201cillegal, unhealthy information.\u201d To highlight another difference, the United States discourse included details about the role of the US AI Safety Institute in advancing the science of AI safety and setting international standards. Several countries-including the United States and United Kingdom-recently agreed to establish an international network of AI safety institutes to \u201cforge a common understanding of AI safety\" and share information about model capabilities and risks (UK Government, 2024). So far, China is not part of this agreement and has not yet announced plans for a formal AI safety institute.\n\nOur work should be interpreted in light of a few important limitations. First, our work did not aim to be a comprehensive summary of AI discourse. We focused on a particular subset of the discourse (discourse related to safety and security concerns around advanced AI development) among a particular group of stakeholders (policymakers and technical experts who inform policymakers). Future work could examine discourse around other topics (e.g., bias and fairness, US-China competition, military applications of AI) among a wider array of stakeholders (e.g., members of the general public, investors, civil society groups). Second, our methodology was subjective- we did not aim to perform a comprehensive literature review or statistically examine trends in the entire corpus of discourse. Rather, we aimed to provide brief and illustrative findings that can offer readers a general understanding of the discourse. Future work could aim to be more comprehensive or quantitative. Third, our work only included publicly available statements\u2013 we do not provide insights into private discussions among policymakers. Finally, it should be noted that AI discourse can change rapidly in response to new events. As AI progress continues and policymakers' understanding of AI increases, we should suspect important changes in how policymakers in each country view the safety and security risks.\n\nOverall, we hope our work provides some useful insights into the state of advanced AI discourse in China and the United States. Such work could contribute to a broader body of literature that helps policymakers better prepare for the global security implications of advanced AI and inform discussions about global governance strategies."}]}