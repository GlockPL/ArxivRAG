{"title": "ADJOINT SHARDING FOR VERY LONG CONTEXT TRAINING OF STATE SPACE MODELS", "authors": ["Xingzi Xu", "Amir Tavanaei", "Kavosh Asadi", "Karim Bouyarmane"], "abstract": "Despite very fast progress, efficiently training large language models (LLMs) in very long contexts remains challenging. Existing methods fall back to training LLMs with short contexts (a maximum of a few thousands tokens in training) and use inference time techniques when evaluating on long contexts (above 1M tokens context window at inference). As opposed to long-context-inference, training on very long context input prompts is quickly limited by GPU memory availability and by the prohibitively long training times it requires on state-of-the-art hardware. Meanwhile, many real-life applications require not only inference but also training/fine-tuning with long context on specific tasks. Such applications include, for example, augmenting the context with various sources of raw reference information for fact extraction, fact summarization, or fact reconciliation tasks. We propose adjoint sharding, a novel technique that comprises sharding gradient calculation during training to reduce memory requirements by orders of magnitude, making training on very long context computationally tractable. Adjoint sharding is based on the adjoint method and computes equivalent gradients to backpropagation. We also propose truncated adjoint sharding to speed up the algorithm while maintaining performance. We provide a distributed version, and a paralleled version of adjoint sharding to further speed up training. Empirical results show the proposed adjoint sharding algorithm reduces memory usage by up to 3X with a 1.27B parameter large language model on 1M context length training. This allows to increase the maximum context length during training or fine-tuning of a 1.27B parameter model from 35K tokens to above 100K tokens on a training infrastructure composed of five AWS P4 instances.", "sections": [{"title": "Introduction", "content": "Foundation models are a new paradigm in artificial intelligence research focused on building large, general-purpose models that adapt to different tasks [44, 40, 7, 51]. Extensive training on large datasets equips foundation models with broad capabilities, which are then fine-tuned on smaller datasets for specific applications. Foundation models commonly employ the transformer architecture [60]. Despite the immense success, training transformer-based models requires memory growing quadratically with the context length L, limiting their applications on long context tasks [36]. Researchers developed various techniques to conquer this problem, ranging from inference time context window expansion [19, 18], IO-aware algorithms [16, 13, 55], and various linearly scaling language model architectures [23, 15, 49, 6]. On another note, distributed learning enables training large models with a big number of GPUs, and efficient"}, {"title": "Related works", "content": "Linear LLMS [17, 5, 49] proposed LLM architectures with a linear inference time complexity. Each of them is formed by stacking K residual layers together, where each layer has a recurrent relation. However, their temporal relationships are nonlinear, which limits the application of adjoint sharding to dissemble the gradients into independent vector-Jacobian products.\nBackpropagation through time Applying the adjoint method for recurrent models leads to backpropagation through time (BPTT) [64]. BPTT is a training algorithm developed for recurrent neural networks (RNNs). RNN models suffer from the exploding and vanishing gradient because of the $\\prod_{I=i+1} \\frac{df(x_i, h_{i-1}, W_h)}{dh_{i-1}}$ term [46]. SSMs provide remedies with careful parameterization of the recurrent dynamics inspired by classical SSM theory [21, 24, 25, 27, 45, 33]. Linear temporal relations allow efficient evaluations of the model, while preserving universal approximation capabilities [63]. By a similar token, truncated adjoint sharding can be seen as a more general version of the truncated backpropagation through time [31, 57].\nNeural ordinary differential equations The adjoint method has also been applied to the optimization of continuous systems, especially the ordinary differential equations (ODEs) [9, 20]. Optimizing neural ODEs with autograd requires backpropagating through numerical solvers along every step, using an unrealistic amount of memory. The adjoint method does not backpropagate through the operations of the solver and uses a constant amount of memory. However, applying the adjoint method for continuous systems requires solving a costly ODE initial value problem with dimensionality of the number of parameters.\nLow memory training methods Researchers proposed various low memory training techniques to train big models in long contexts. ZERO provides data- and model-parallel training while retaining low communication volume, while eliminating memory redundancies [53]. PyTorch FSDP provides a streamline for model, gradient, and data parallelization [69]. Activation checkpointing discards intermediate values during the forward step, and recompute on the fly during the training phase [56]. CPU offloading scales large model training by offloading data and computations to the CPU, trading computing time for memory reduction [54]. Ring attention leverages the blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention, enabling very-long context training of attention-based methods [38, 39]. The proposed adjoint sharding distributes state-space model computations across multiple devices as well as multiple multi-GPU-instances (MIG) to enable very-long context training of state-space models.\nContext length extension methods Existing context length extension method separate into two classes. The first type is fine-tuning free methods, including Positional Interpolation (PI) [10], the NTKAware Scale ROPE (NTK) [59], and StreamingLLM [65]. The second type is fine-tuning methods, including LongChat [35], LongAlpaca [11], YaRN [50], and LongLlama [11]. Additional methods such as activation beacon do tune a network seperate from the LLM [68]. As shown in Figure 3, fine-tuning methods achieve better performances than that of fine-tuning free methods at lengths that they have been fine-tuned on. However, fine-tuning methods suffer from a high computational cost and require a potentially intractable amount of GPU memory during fine-tuning."}, {"title": "Background", "content": "We first give a concise introduction to the state-space models, the residual networks, and the adjoint method."}, {"title": "State-space models", "content": "While our method generally applies to all recurrent models, we illustrate the idea using state-space models (SSMs), which have shown performances at least on par with transformers at small to medium scale [14]. Given an input token sequence {xt}=1, the SSMs first calculate the corresponding matrices At, Bt, and Ct to evolve the dynamics as follows:\n$A_t = A(x_t); B_t = B(x_t); C_t = C(x_t)$.\nThe SSMs evolve a latent dynamics ht, whose initial condition h\u00ba is often assumed to be zero. With h\u00ba and At, Bt defined, the dynamics evolves as:\n$h_t = A_th_{t-1} + B_tx_t$.\nThe matrices Ct then maps the latent dynamics ht back to token space as yt = Ctht, with yt being the predicted token at t. For a sequence of T tokens, we denote:\nA = (A1, A2, . . ., AT), B = (B1, B2, ...,BT), C = (C1, C2, ..., CT),\nH = (h1, h2, ..., h7), X = (x1, x2,...,x7), Y = (y1, y2, ..., y).\nIn the most general case, we have H \u2208 RT\u00d7N, A \u2208 RT\u00d7N\u00d7N, B\u2208 RT\u00d7N\u00d7P, C \u2208 RT\u00d7P\u00d7N,X \u2208 RT\u00d7P,Y \u2208 RT\u00d7P, where N is the hidden state dimension, and P is the input/output dimension. We evolve the dynamics for t = 1, . . ., T, and assume that h\u00ba is a fixed and predefined constant.\nThe input to an SSM is X and h\u00ba, and the output is Y. We define SSM(\u00b7) as performing the following five steps:\n1. {At}T_{t=1} = {A(xt)}T_{t=1},\n2. {Bt}T_{t=1} = {B(xt)}T_{t=1},\n3. {Ct}T_{t=1} = {C(xt)}T_{t=1},\n4. {ht}_{t=1} = {A_th_{t-1} + B_tx_t}T_{t=1};\n5. {yt}T_{t=1} = {C_th_t}T_{t=1}\nThe input to the five steps is X, and the output is Y. We can then write SSM(X) = Y. SSMs decrease the quadratic computational complexity with sequence length on transformers to linear and decrease the large inference-time memory requirements from the key-value cache. SSM-based models at a small to medium scale have shown performances on par with or better than transformer-based models. For instance, [51, 1] shows that SSM-based mixture-of-experts"}, {"title": "Residual Networks", "content": "In practice, we have K SSMs stacked together, and we have a large language head (LLH) $\u03a9 \\in R^{T\u00d7P}$, where T is the number of all possible tokens. To predict a token, we have ot = N\u0177k. Define (yk,\u2026\u2026\u2026,yk) = Y\u043a, \u0430 ResNet computes YK as follows:\n$(\\hat{y_k} \\dots , \\hat{y_k}) = Y_{K-1} + SSM_K(\\hat{Y}_{K-1})$\n$= Y_0 + SSM_1(Y_0) + \\dots + SSM_K(Y_{K-1})$\n$= Y_0 + \\sum_{k=1}^{K} SSM_k(Y_{k-1}) = Y_0 + \\hat{Y}_K$,\nwhere \u0176k = (y,...,y) = (Norm(y),..., Norm(y)) and SSMk(k-1) = \u0176k. Therefore, for a latent state at time t we have $y^k_t = y^0_t + \\sum_{k=1}^{K} \\hat{Y}_k$\nResNet has been the foundation of numerous modern networks, including the transformers, diffusion models, segmentation models, SSMs, and more [29, 26, 34, 48]. ResNet's residual structure allows for a separation between gradients of each layer by applying differentiation on summations."}, {"title": "Adjoint method", "content": "The adjoint method is concerned with optimizing y(h(\u03b8), \u03b8) with respect to \u03b8, where h(0) \u2208 RP is the solution to f(h(0), 0) = 0 [8]. To employ gradient based algorithms like the stochastic gradient descent (SGD) or the Adam, we compute the derivative of y regarding 0 \u2208 R||:\n$\\frac{dy}{d \\theta} = \\frac{\\partial y}{\\partial \\theta} + \\frac{\\partial y}{\\partial h} \\frac{\\partial h}{\\partial \\theta}$,\nwith d being the total derivative, and \u2202 being the partial derivative. The adjoint method converts computing dy/d0 to solving an adjoint equation. In our case, we need the adjoint method for recurrence relations, where y is given by y = y = y(ht(0), 0), and h is given by\n$\\begin{cases}h^0 = b(\\theta), \\\\ h^t = f(t, h^{t-1}, \\theta).\\end{cases}$\nWe have\n$\\frac{df(t, h^{t-1},\\theta)}{d \\theta} = \\frac{\\partial f(t, h^{t-1},\\theta)}{\\partial \\theta} + \\frac{\\partial f(t, h^{t-1},\\theta)}{\\partial h^{t-1}} \\frac{d h^{t-1}}{d \\theta}$.\nProposition 1 [8] When the states h are defined as Equation 2, the gradient of y with respect to 0 is given as:\n$\\begin{cases}\\frac{dy}{d \\theta} = \\frac{dy}{d \\theta} + \\lambda^T_0b(\\theta) + \\sum_{i=1}^{t} \\lambda^t (\\frac{\\partial f(i, h^{i-1},\\theta)}{\\partial \\theta}), \\\\  \\lambda^t_i = \\frac{dy}{d h^t}, \\\\  \\lambda^{t-1}_i = \\lambda^t (\\frac{\\partial f(i, h^{i-1},\\theta)}{\\partial h^{i-1}}).\\end{cases}$\nEquivalently, we have $\\lambda^{t-1} = (\\frac{dy}{d h^t}) (\\frac{d (\\sum_{j=t+1}^{t} (\\frac{\\partial f(j, h^{j-1},\\theta)}{\\partial h^{j-1}}))}{ \\partial h^{j-1}})) [32]$\nAfter computing adjoint states {\u03bbt}t=0, the computation of the elements of $A^t(\\partial f(i, h^{i-1}, \u03b8)/\u2202\u03b8)$ are independent, allowing parallelism. This computation is a vector-Jacobian product (vjp), with \u03bbt as the vector and \u2202f(i, hi\u22121, \u03b8)/\u2202\u03b8 as the Jacobian. vjps can be evaluated with the reverse-mode automatic differentiation and initializing the reverse phase with \u03bbt [3]. As each vjp only requires saving their corresponding computation graph, and can be disposed"}, {"title": "Adjoint sharding", "content": "We now introduce the adjoint sharding technique. We first illustrate the method assuming only one layer of SSM, and generalize to K layers."}, {"title": "Adjoint sharding for one SSM", "content": "Large scale neural networks are usually trained with the autograd framework [4, 47]. However, this framework suffers from a high memory cost when used with networks of recurrent nature [4]. Although activation checkpointing has been developed, which discards part of the intermediate values and recomputes them later on the fly, the memory cost is still high [30]. We employ the adjoint method for recurrence relations to further reduce the memory cost, and more importantly, to break the temporal dependencies of activations and parallelize their computations.\nDefine 0 = (0, \u03b8\u03b2, \u03b8c) as A's, B's, and C\u2019s parameters, for loss lt = l(yt), in the context of a single-layer SSM, we prove:\nProposition 2 The gradient $dl_t/d\\theta$ is given as\n$\\frac{dl_t}{d \\theta} = \\sum_{i=1}^{t} VJP_A(\\frac{dl_t}{dy_t} \\otimes h^{i-1}) +  \\sum_{i=1}^{t} VJP_B(\\frac{dl_t}{dy_t} \\otimes x^i) + vjPct(\\frac{dl_t}{dy_t} \\otimes h^t)$,\nwhere the adjoint state = C(\u220fit+1\u2212i), vjPNeti(v) = v\u00b7 Neto(Input), with 0 being Net's parameters and i being the index of Input, & is the vector outer product, and \u2295 is vector concatenation.\nThe proof of proposition 2 is in section A.1. The gradient for parameters of A, and B are each separated into {vjp(hti\u22121)}, {vjp(x)}t=1, and the gradient for parameters of C only depend on inputs at time t. After computing the adjoint states, these vjp computations are separate from each other on both the network and the temporal level."}, {"title": "Adjoint sharding for multiple SSMs", "content": "We now generalize the results from subsection 4.1 to the general case of K SSMs concatenated together. As introduced in subsection 3.2, the outputs of each SSM layer are added to the results of the last layer and normalized before it is fed into the next layer. Define the loss over all token predictions $L = \\sum_{t=1}^{T} l_t$, using the residual structure we have\n$\\frac{dL}{d \\theta} = \\sum_{t=1}^{T} \\frac{dl_t}{dy} \\frac{dy_k}{d \\theta} = \\sum_{t=1}^{T} \\frac{dl_t}{d(y+1)} \\frac{d(\\sum_{j=1}^{K} k)}{de}$\nCombining with proposition 2, we have"}, {"title": "Truncated adjoint sharding", "content": "One limitation of adjoint sharding is that the number of vjps performed increases polynomially regarding the number of tokens T. In particular, adjoint sharding computes the vjp for Ak and Bk (1 + T)T/2 times, and for Ck T times. When training large networks with many layers and long context length T, applying adjoint sharding becomes computationally expensive. We propose truncated adjoint sharding, with which we argue that we can get similar results by computing a linearly growing number of vjps, and empirically showcase its performance.\nAttention mechanisms have suffered from the O(T2) complexities arising from the self-attention structure [60]. To enable training with longer context lengths, global-local attention has been proposed, where we divide the contexts into sections, and compute the attention between sections rather than tokens [67]. [57] proposed truncated backpropagation through time (T-BPTT) to avoid gradient explosion/vanishing when training with long contexts by only counting a fixed number of state transitions. Here, inspired by global-local attention and T-BPTT, instead of computing the full gradient given in Equation 11, we propose to train the SSMs to depend on up to T states:\n$\\frac{dL}{de} = \\sum_{t=1}^{T} \\frac{dl_t}{dyk}vjp_C(\\frac{dl_t}{dy_t} \\otimes h) \\\\  \\bigoplus \\sum_{t=1}^{T} \\sum_{i=1}^{t} \\sum_{i=1}^{K} VJPA (\\frac{dl_t}{dy^k_{t,i}} \\otimes h_{i-1}) +  \\sum_{t=T+1}^{T} \\sum_{k=1}^{K} \\sum_{i=t+1-T}^{t} VJPA (\\frac{dl_t}{dy^k_{t,i}} \\otimes h_{i-1}) \\\\  \\bigoplus \\sum_{t=1}^{T} \\sum_{i=1}^{t} \\sum_{i=1}^{K} VJPB (\\frac{dl_t}{dy^k_{t,i}}) +  \\sum_{t=T+1}^{T} \\sum_{k=1}^{K} \\sum_{i=t+1-T}^{t} ViPB (\\frac{dl_t}{dy^k_{t,i}})$"}, {"title": "Distributed training", "content": "We now discuss how to distribute the storage and compute of the adjoint sharding method, assuming that we have Y GPUs. Given the networks {Ak, Bk, Ck}K_1, initial tokens {y}T_{t=1} = {Norm(x)}T_{t=1}, and initial conditions {h}K_1 (usually set to 0), we can call algorithm 1 to get all necessary vectors for computing the gradient with adjoint sharding."}, {"title": "Parallel computing", "content": "Adjoint sharding converts the sequential process of backpropagation gradient computation into individual independent vjps, allowing for parallel computation. We analyze the time and memory cost of $vjp_A ((\\frac{dl_t}{dy})\\otimes h^{i-1})$, $vjp_B ((\\frac{dl_t}{dy^k})x^i)$, and $vjp_C ((\\frac{dl_t}{dy}) \\otimes h^t)$.\nvjp has a similar time complexity as a forward pass, and a memory complexity of bs(|O| + |0|) + |0|, where bs is the batch size, O is the number of elements in the network output, and |0| is the number of parameters [42]. We provide the memory and FLOPs required to compute the vjps in Table 1 [43].\nWe analyze training with a dataset containing contexts of lengths T, with Y NVIDIA H100 GPUs, and performing computations in FP16. We use a selective diagonal SSM with K layers, and each Ak, Bk, and Ck network is a single-layer multi-layer perceptron (MLP).\nFor each data point {x}T_{t=1}, we store A, C, hyk} (t,k) = (1,1) and {dl(ot)/dyk}{t=1, which is TK(2N+P)+TP FP16 numbers. We also save \u04e9\u0434, \u04e9\u0432, and \u04e9\u0441c, each taking PN + N FP16 numbers. We need to store T(2NK + PK + P) + 3N(P + 1) FP16 numbers before computing the vjp."}, {"title": "Limitation", "content": "The adjoint sharding method provides an alternative method of computing gradients to backpropagation. While we analytically proved that the gradients computed from adjoint sharding equals to that from backpropagation, adjoint sharding suffer from a time complexity polynomial regarding the training context length when computing equivalent gradients. We provided the truncated adjoint sharding as a linear time complexity alternative, and leave the analysis of its convergence and further improvements on it for future works. We also provided a distributed and parallel computing algorithm for performing adjoint sharding. However, the overhead of na\u00efve implementation of such algorithm with multi-threading or multiprocessing overweights the speedups when the training context length is small. We leave efficient implementation of the parallel algorithm on a CUDA kernel for future work."}, {"title": "Conclusion", "content": "We introduced adjoint sharding, a distributed and parallel computing algorithm, to facilitate training of LLMs on long contexts. Unlike the sequential backpropagation, the adjoint sharding computes gradients of each LLM layer against each token independently through vector-Jacobian product, allowing for parallel computation. To avoid the limitation of vjps increasing polynomially regarding context length, we propose truncated adjoint sharding to focus on important gradients. We analyzed the memory and FLOP cost of each computation block in adjoint sharding and proposed a method to accelerate it through parallel computing. Empirical results suggest orders of magnitude of memory reduction in training while maintaining the same training results as backpropagation."}, {"title": "Proof for proposition 2", "content": "Proof 1 Define $d\u1ef9/dh_t = \u1ef9\u0127^t$, $dh_t/dh_{t-1} = ht-1$, and $d\u1ef9/d\u03b8 = \u1ef9$, $d\u0125/d\u03b8 = h$, by plugging in the expression for \u1ef9t from subsection 3.2, proposition 1 states that\n$\\frac{d\u1ef9_t}{d \\theta} = yht + \\prod_{i=1}^{t-1}(\\frac{d \\lambda^2h^{t-i}}{d \\theta}) +  \\prod_{i=1}^{t-2}(\\frac{d \\lambda^2h^{t-i}}{d \\theta}) +...++h_0\u00af\u00b9 + h_0 + \\hat{y}$"}, {"title": "Proof of concept for VJP", "content": "As a proof of concept of why (dlt/dyt)Coh can computed with vjp, we present an explicit and simple example. We have y = [Y1, Y2], h = [h1, h2, h3], 0 = 0. We then have\n$\\frac{dl}{dy} = [ly1 ly2] \u2208 R^{1\u00d7P}$\n$Ce = \\begin{bmatrix}\nC_{11}&C_{12}&C_{13}\\\\ \nC_{21}&C_{22}&C_{23}\n\\end{bmatrix} \u2208 R^{PxNxO} \\|0|$\nh=$h=\\begin{bmatrix}\nh_{1}\\\\ \nh_{2}\\\\ \nh_{3}\n\\end{bmatrix} \\in R^{Nx1}$\nWith each Co = [Cij/901,..., Cij/20|0|] \u2208 R|0|. We have"}]}