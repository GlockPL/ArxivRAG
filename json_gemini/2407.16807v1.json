{"title": "In Search for Architectures and Loss Functions in Multi-Objective Reinforcement Learning", "authors": ["Mikhail Terekhov", "Caglar Gulcehre"], "abstract": "Multi-objective reinforcement learning (MORL) is essential for addressing the intricacies of real-world RL problems, which often require trade-offs between multiple utility functions. However, MORL is challenging due to unstable learning dynamics with deep learning-based function approximators. The research path most taken has been to explore different value-based loss functions for MORL to overcome this issue. Our work empirically explores model-free policy learning loss functions and the impact of different architectural choices. We introduce two different approaches: Multi-objective Proximal Policy Optimization (MOPPO), which extends PPO to MORL, and Multi-objective Advantage Actor Critic (MOA2C), which acts as a simple baseline in our ablations. Our proposed approach is straightforward to implement, requiring only small modifications at the level of function approximator. We conduct comprehensive evaluations on the MORL Deep Sea Treasure, Minecart, and Reacher environments and show that MOPPO effectively captures the Pareto front. Our extensive ablation studies and empirical analyses reveal the impact of different architectural choices, underscoring the robustness and versatility of MOPPO compared to popular MORL approaches like Pareto Conditioned Networks (PCN) and Envelope Q-learning in terms of MORL metrics, including hypervolume and expected utility.", "sections": [{"title": "1 Introduction", "content": "Many optimization problems in the real world require consideration of multiple conflicting objectives. Liu and Vicente [2022] provide examples of accuracy versus fairness trade-offs in credit scoring and criminal justice, and Vamplew et al. [2021] show how to trade performance for safety in intelligent agents. Multi-objective optimization is the field that studies these problems formally. It is known as multi-objective reinforcement learning (MORL) in the sequential decision-making setting. In MORL, we seek policies that maximize the respective objectives. A single policy mapping states to actions is insufficient to satisfy all possible trade-offs between objectives; hence, in MORL, we usually discuss sets of policies covering these trade-offs.\nThe performance of modern MORL approaches is often measured on toy grid-world or 2D locomotion problems, such as those in MO-Gym by Alegre et al. [2022]. At the same time, single-objective RL is already used in many practical applications, such as language modeling [Ouyang et al., 2022], real-world robot locomotion [Fu et al., 2023], and control of scientific equipment [Degrave et al., 2022]. One plausible explanation of this gap is that MORL approaches often explicitly store Pareto-optimal policies or rely on Q-learning. For toy problems, this provides optimal coverage, but this is not scalable, and we noticed that the training can suffer from unstable learning dynamics, especially when different rewards interfere with each other.\nRather than maintaining a set of weights for each trade-off, we implicitly model the optimal set of policies by conditioning the learned policy on each objective's vector of relative weights. We call this approach Dynamic MORL (DMORL). This allows us to learn a single model that encompasses all possible solutions"}, {"title": "2 Related work", "content": "Multi-objective RL has been primarily studied from an off-policy perspective. Multiple MORL approaches have been developed as extensions of Q-learning [Abels et al., 2019, Lu et al., 2022], some of which condition the Q-network on the scalarization weights as we do with policies. Among these methods, we use Envelope Q-learning [Yang et al., 2019] as a baseline, since its implementation is publicly available and includes the case of discrete actions, which is the focus of our work. Another notable example is the recent work by Hung et al. [2022], which also proposes a way to perform policy updates for policies conditioned on relative weights along with learning a Q-function, but the implementation only considers continuous control. Although Q-learning thrives in toy problems and is efficient in some more complex domains [Mnih et al., 2013], it has many failure modes, including the so-called \u201cdeadly triad\" [Van Hasselt et al., 2018]. Off-policy methods also struggle with capacity loss [Lyle et al., 2022] and, more generally, with generalizable feature learning [Lan et al., 2022]. Our work instead focuses on on-policy methods.\""}, {"title": "3 Dynamic multi-objective reinforcement learning", "content": "Multi-objective reinforcement learning is the search for optimal policies for multi-objective Markov decision processes (MOMDP). A MOMDP is a tuple (S, A, r, P, \u03ba, \u03b3), where sets S and A are state and action spaces respectively, r : S \u00d7 A \u2192 \u211d\u1d37 is a K-dimensional reward function, and P(s' | s, a) is the transition probability. Finally, \u03ba is the distribution of initial states, and \u03b3 \u2208 [0, 1) is the discount factor. The only difference between MOMDP and MDP is the range of the reward - \u211d\u1d37 instead of \u211d. In this work, we optimize so-called linear scalarizations with relative weights given by \u03b1 \u2208 \u0394k from the (K - 1)-dimensional simplex\n$\\Delta_K = \\{\\alpha \\in \\mathbb{R}^K | \\sum_{i=1}^K \\alpha_i = 1, \\alpha_i \\geq 0 \\forall i\\}$ (1)\nFor \u03b1 \u2208 \u0394\u03ba, a scalarized reward is r(s, a, \u03b1) = \u03b1\u1d40r(s, a). Since we would like to cover all scalarizations with a single model, we have to generalize our policy definition so that it is also conditioned on \u03b1. Hence, our parameterized policies are of the form \u03c0(\u03b1 | s, a). For a policy \u03c0, the vector-value function also has to depend on \u03b1:\nV^\\pi(s, \\alpha) = \\mathbb{E}_{\\tau \\sim \\pi(\\alpha)}[\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) | s_0 = s] \\in \\mathbb{R}^K. (2)\nHere, \u03c4 = (s\u2080, a\u2080, s\u2081, ...) is a trajectory sampled from the transition dynamics \u2119 and the policy \u03c0(\u03b1 | s, a). We can also define the Q-function and the advantage in the usual way:\nQ^\\pi(s, a, \\alpha) = r(s, a) + \\gamma \\mathbb{E}_{s'\\sim P(s'|s,a)}[V^\\pi(s', \\alpha)], (3)\nA^\\pi(s, a, \\alpha) = Q^\\pi(s, a, \\alpha) - V^\\pi(s, \\alpha). (4)\nThe vector-objective associated with \u03c0 is then given by\nJ(\\pi, \\alpha) = \\mathbb{E}_{s\\sim \\kappa}[V^\\pi(s, \\alpha)] \\in \\mathbb{R}^K. (5)\nIn practice, our policies will be represented by a neural architecture with parameters \u03b8 \u2208 \u211d\u1d30\u03b8. In this case, we write the policy, a.k.a. the actor, as \u03c0\u03b8(a|s, \u03b1) and use shortcuts V\u03b8 = V\u03b8 and J(\u03b8, \u03b1) = J(\u03c0\u03b8, \u03b1). MOPPO and MOA2C will also require a critic, i.e., a neural approximation to the value function. It is parameterized by \u03c8 \u2208 \u211d\u1d30c, and we denote it as V\u03c8(s, \u03b1). Some of the parameters might be shared between the actor and the critic. Our architecture's overall set of parameters will be called v.\nTo define a Pareto Front, we need the notion of strict dominance. We say that a vector p \u2208 \u211d\u1d37 strictly dominates q \u2208 \u211d\u1d37 (denoted as p > q) iff pi > qi for all i \u2208 {1, ... K}. Next, individual parameters \u03b1 give rise to partially specified policies \u03c0(\u00b7 | \u00b7, \u03b1). We say that a policy \u03c0(\u00b7 | \u00b7, \u03b1) strictly dominates \u03c0(\u00b7 | \u00b7, \u03b1') iff \ud835\udd3cs~\u03ba [V\u03c0 (s, \u03b1)] > \ud835\udd3cs~\u03ba [V\u03c0 (s, \u03b1')]. A partially specified policy \u03c0(\u00b7 | \u00b7, \u03b1) is said to be weakly Pareto-optimal if no other policy dominates it. The set of all such policies is called a weak Pareto set, and the image of the Pareto set in the objective space is called a weak Pareto front (PF).\nIn a nutshell, to identify the points on the PF, we are optimizing the expected utility metric, introduced by Zintgraf et al. [2015] as\n$\\mathcal{J}(\\theta) = \\mathbb{E}_{\\alpha \\sim D(\\alpha)}[\\alpha^T J(\\theta, \\alpha)]$ (6)"}, {"title": "4 Architectures", "content": "This work focuses on continuous state spaces and discrete actions, such that \ud835\udd4a \u2282 \u211d\u1d48s, \ud835\udd38 = {1, ..., K}. Generalization to small discrete state spaces is straightforward with one-hot encoding, and generalization to continuous actions can be performed analogously to scalar PPO and A2C variants through parameterized action distributions. We consider three architectures for the actor-critic network, presented in Figure 2.\nWhen both actor and critic are learned, an often-used practice is to share weights between them. A popular approach is to share the trunk fc : \ud835\udd4a \u2192 \u211d\u1da0 of a neural network that takes the state s in and produces intermediate features fc(s). The trunk is parameterized by \u03b6\u2208 \u211d\u1d30\u266d. We hope the features are general enough to convey information about both the optimal action and the value of the current state. In this case, we can use a shared trunk architecture with two separate linear layers to extract both the action distribution and the predicted value:\n$\\pi_\\theta(\\cdot | s, \\alpha) = softmax(W_a f_t(s, \\alpha) + b_a), $ (8)\nV_\\psi(s, \\alpha) = W_c f_t(s, \\alpha) + b_c. (9)\nThe learnable parameters of the actor are therefore \u03b8 = (\u03b6, Wa, ba). For the critic, the parameters are \u03c8 = (\u03b6, Wc, bc). We refer to (Wa, ba) as the actor head and to (Wc, bc) as the critic head. An alternative that we consider is that the critic and the actor do not share the trunk's parameters, even though its architecture is the same for both. We will refer to this as a non-shared trunk architecture.\nWhen the policy is conditioned on states s \u2208 \ud835\udd4a and relative weights \u03b1 \u2208 \u0394\u03ba, it is a priori unclear what woul d be the best architecture that combines these inputs in ft. Deep learning architectures provide a"}, {"title": "5 Algorithms", "content": "We chose to focus on two instantiations of our framework for DMORL. However, our actor-critic architectures are more general and could be applied to other policy iteration or policy gradient methods."}, {"title": "5.1 Actor and critic losses", "content": "Multi-objective policy gradient Scalar A2C relies on the Policy Gradient theorem to update the actor. This theorem can be generalized to the vector-valued reward case:\n$\\nabla_\\theta (\\alpha J(\\theta, \\alpha)) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta(\\tau, \\alpha)} [\\sum_{t=0}^{\\infty} \\gamma^t A^\\theta (s_t, a_t, \\alpha) \\nabla_\\theta log \\pi_\\theta(a_t | s_t, \\alpha)]$ (15)\nTo maximize the expected utility, we approximate the above expression by first sampling random relative weight vectors \u03b1 ~ D(\u0394\u1d37), and then performing a rollout of the current policy \u03c0\u03b8 using \u03b1. In practice,"}, {"title": "Critic loss", "content": "To optimize the critic over a minibatch of {(sk, Qk)}k, we use the least-squares loss:\n$L_c(\\psi) = \\sum_k || Q_k - V_\\psi(s_k) ||^2. $ (17)\nThe algorithm dynamically adjusts the weight \u03b2c of the critic loss. Depending on whether the critic shares parameters with the actor, it makes sense to use different strategies for setting \u03b2c. For the generality of our methods, we set \u03b2c dynamically to ensure an approximately constant ratio of norms of the actor and critic gradients. This technique was not as important in our environments, so we defer a discussion to Appendix A.2."}, {"title": "PopArt", "content": "PopArt is an approach to learning value functions across different orders of magnitude, especially in situations where the value scales are not known in advance or change depending on the performance of the policy. Hessel et al. [2019] successfully adapted it to the multi-task RL setup, similar to MORL. In MORL, the differences in scale between rewards also pose a problem. If we have to design trade-offs between objectives of varying magnitude, then the optimization target (6) will unfairly favor the objectives of larger scales. Multi-task PopArt maintains an approximate mean \u00b5\u03c2 and variance \u03c3\u03c2 of the target values for each task. We chose to use a combine the normalized advantages. \u0302\u03b1\u1d57 (Q\u1d57,j \u2212 V(st, j) ) we use a scalarization of the form\n$\\alpha^T \\frac{(Q_t, j - V(s_t,j))}{\\sigma_j}, $ (18)\nwhere \u03c3 = (\u03c3\u2081, ..., \u03c3\u1d37) and / denotes component-wise division."}, {"title": "Multi-objective PPO", "content": "The original PPO is formulated by Schulman et al. [2017] using a surrogate objective for policy iteration. This objective is optimized for multiple epochs over the same sampled trajectories to achieve higher sample efficiency. The actor loss for MOPPO differs from the standard PPO formulation only by using the scalarized advantage \u00c2, defined in Algorithm 1. Given a minibatch of {(sk, ak, \u00c2k)}k, we employ the following loss of the actor:\n$L_a(\\theta) = \\sum_k min \\left( r_k(\\theta) \\hat{A}_k, clip(r_k(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_k \\right), r_k(\\theta) = \\frac{\\pi_\\theta(a_k | s_k, \\alpha)}{\\pi_{tref}(a_k | s_k, \\alpha)},$ (19)\nwhere \u03c0tref is the policy directly after sampling the trajectory."}, {"title": "5.2 Entropy control during training", "content": "Our experiments show that entropy regularization is necessary to avoid collapse in more challenging environments. The entropy of the current policy embodies the exploration-exploitation trade-off: if the entropy is low, the agent fails to explore, but if it is too high, the agent cannot operate efficiently. Intuitively, a good training run keeps entropy high initially to explore sufficiently and then \u201canneals\u201d the entropy to lower values as training progresses. This behavior is hard to achieve in practice since training is sensitive to the entropy regularization coefficient. At this point, we can employ a key insight that we do not need to maximize the entropy. Rather, we would like to keep it at a pre-defined level. Hence, the framework of constrained optimization is applicable.\nWe use an algorithm by Platt and Barr [1987] called the Modified Differential Method of Multipliers (MDMM), which allows us to maximize a function subject to approximate equality constraints. Let the expected entropy of the policy and its empirical estimate be\n$\\hat{H}(\\theta) = \\mathbb{E}_{\\alpha \\sim D(\\Delta_K), s \\sim p^\\pi_\\theta(s; \\alpha)} [H(\\pi_\\theta(\\cdot | s, \\alpha))], \\quad \\hat{H}(\\theta) = \\frac{1}{T+1} \\sum_{t=0}^T H(\\pi_\\theta(\\cdot | s_t, \\alpha)), $ (20)\nwhere p\u03c0\u03b8 is the state visitation distribution. Assume that we would like to satisfy the constraint \u0124(\u03b8) \u2248 Htarget, where Htarget is a desired entropy value that can also depend on our progress in training. To this end, MDMM introduces the Lagrange multiplier \u03bb\u1d62 \u2208 \u211d, which is dynamically updated at each step i. Vanilla MDMM dictates that we use an update of the form:\ng = (\u03bb\u1d62 + c(Htarget \u2212 H))\u2207\u03b8H, \u03b8\u1d62\u208a\u2081 = \u03b8\u1d62 + \u03b7(g + \u2207\u03b8La), \u03bb\u1d62\u208a\u2081 = \u03bb\u1d62 + \u1fc6(Htarget \u2212 H). (21)\nHere, \u03b7 is the learning rate, and \u1fc6, c are new hyperparameters. In practice, we don't directly use the update vector g + \u2207\u03b8La and instead provide it to Adam. Note that the conventional method of entropy regularization keeps \u03bb constant and positive, while here it can also become negative, thus forcing the entropy to decrease to reach Htarget. Figure 3 demonstrates how entropy oscillates around the target value during training in the Minecart environment. We discuss the entropy schedules we used in more detail in Appendix A.4. In our experiments, entropy control did not perform well with A2C, so we only enabled it for PPO, which provides benefits as our ablation study demonstrates."}, {"title": "6 Experiments", "content": "We implement all architectures described above using the TorchRL library [Bou et al., 2023]. As a source of MOMDP environments, we use MO-Gymnasium [Alegre et al., 2022], the standard testbed in MORL. We run an ablation study to compare various architectures and algorithm details against each other and perform a comparison against two baselines, Pareto Conditioned Networks [Reymond et al., 2022] and Envelope Q-learning [Yang et al., 2019]. We chose these two approaches to MORL because, to the best of our knowledge, these are the most recent model-free MORL methods that condition a single policy (or value function) to generate the entire Pareto front and that have a public implementation supporting the case of continuous observations and discrete actions. We use the implementation provided in MORL-baselines [Felten et al., 2023] for both methods."}, {"title": "6.1 Preliminary results on deep-sea-treasure", "content": "All our methods can solve the standard grid-world environment called \"Deep Sea Treasure\". In this environment, the agent controls a submarine on a 2D grid. The two objectives are the cost of fuel (-1 for every step) and the reward of a treasure that the agent can discover in pre-defined locations. When the agent is willing to spend more fuel, it can find a bigger reward. Precise control over the placement of treasures and the rewards from each one allows us to shape the 2D PF. We use the version of Deep Sea Treasure with a convex PF. We discovered that the results in this simple environment are not sensitive to hyperparameter tuning. All runs used \u2248 10\u2075 environment steps and converged to Pareto fronts presented in Figure 1."}, {"title": "6.2 Ablation studies on Minecart", "content": "\"Minecart\" is a harder environment than deep-sea-treasure, also included in MO-Gym. This environment has an agent that operates a cart in a 2D continuous space. There are three rewards: two for bringing different types of ores and one for the consumed fuel. Depending on the agent's location, the probabilities for getting one or the other change, thus leading to a nontrivial multi-objective problem. This environment also has a deterministic version, where each mining action near a mine is guaranteed to produce a fixed amount of ore. We use the stochastic version for ablations on different architectural choices discussed here. For each architecture, we run a hyperparameter search detailed in Appendix B.1. Then, we run training for each method with five seeds to estimate the average and standard deviation. The resulting hypervolume for selecting methods is presented in Figure 4. We want to focus the attention of the reader on multiple conclusions that we can draw if we focus on parts of it:\n\u2022 PPO significantly outperforms A2C, likely because of its sample-efficient data reuse across multiple epochs. This effect mirrors the corresponding knowledge from the scalar RL community.\n\u2022 Non-shared trunk architectures consistently perform slightly worse than their shared counterparts.\n\u2022 Architecture choice seems to influence the performance. Multi-body networks perform the best, followed by hypernetworks and merge networks. This ordering also depends on the environment since it comes out differently for another environment in the following section."}, {"title": "6.3 Comparison with the baseline methods", "content": "We use three environments for comparison: two versions of Minecart (deterministic and non-deterministic) and MO-reacher. The non-deterministic version of Minecart checks that our methods can handle stochasticity, but it cannot be used to compare with PCN, which requires deterministic transitions. MO-reacher is a MuJoCo-based [Todorov et al., 2012] environment where an agent controls a two-jointed robot arm. There are four rewards, each corresponding to the 12-distance of the tip of the arm from one of four targets on the 2D-space. The action space consists of nine actions corresponding to one of three torques (positive, negative, and zero) in each joint.\nWe tune the hyperparameters of each method on each environment separately; the details are presented in Appendix B. Results from all grids for this experiment are also presented in Figures 7-10 in the Appendix. The best hyperparameters are used to run the method again with 5 different seeds. We found that in some cases, envelope Q-learning showed good results early but collapsed later in training, likely due to catastrophic forgetting. Because of this, we demonstrate the results corresponding to the best hypervolume for envelope Q-learning. Our methods or PCN did not suffer from this issue, so we reported the metrics after the training was finished. We present the evaluation results in Table 1. Overall, our methods can outperform the baselines in terms of hypervolume. This is partly because we use non-deterministic policies for evaluation, while PCN and Envelope Q-learning rely on deterministic ones. Learning non-deterministic policies allows us to cover the Pareto front more densely. As one can see, on Minecart, whether our methods outperform Envelope Q-learning is ambiguous. Mirroring the situation in the scalar RL literature, however, our methods perform better on the robotic control task. Note that the authors of PCN also provide hypervolume measurements on deterministic Minecart in [Reymond et al., 2022, Table 1], but they do not mention the reference point po they use, nor the discount factor \u03b3. Based on the true Pareto front that they provide and their measurements, they most likely use \u03b3 = 1, while we use \u03b3 = 0.99. Therefore, the results from their paper are not directly comparable with the ones provided in our Table 1."}, {"title": "7 Conclusion", "content": "We proposed several actor-critic architectures and two algorithms for dynamic multi-objective reinforcement learning. All of our approaches can solve a simple MORL environment and provide a continuous parametrization of the space of policies covering the Pareto front. We then performed an extensive comparison study on two more complicated environments, demonstrating the usefulness of the proposed improvements over the naive implementation. We showed that our implementation of multi-objective PPO can outperform the baselines on a robotic control task and perform competitively when delayed rewards and/or stochasticity are present in the environment."}, {"title": "8 Limitations", "content": "Our contributions are empirical in nature. We compare architectures and technical details of the implementation, but we do not perform a theoretical analysis of our algorithms. Such analysis could also be beneficial for the community: for example, under which conditions does policy gradient or policy iteration converge to a policy that approximately covers the Pareto front, and what would the notion of \u201cmisspecification\u201d be? Another important limitation of this work is that we only consider linear scalarizations of the vector"}, {"title": "A Implementation details", "content": ""}, {"title": "A.1 Other variants of our learning algorithms", "content": "Here we describe our versions of non-shared trunk PPO (Algorithm 2) and of A2C (shared trunk version in Algorithm 3, and non-shared in Algorithm 4). We note that non-shared versions can perform multiple updates on the critic per single actor update, but this would be harder to justify conceptually for a shared trunk architecture.\nSince MOA2C diverges when used together with our entropy regularization scheme, for it we have to rely on the standard entropy regularizer loss. Given a trajectory {(st, at, rt)}t sampled with reward weights \u03b1, it is given by\ng_{a2c} = \\frac{1}{T+1} \\sum_{t=0}^T H(\\pi_\\theta(\\cdot | s_t, \\alpha)). (22)"}, {"title": "A.2 Gradients for shared and non-shared trunk architectures", "content": "Suppose the actor and critic do not share parameters. In that case, updating them in PPO or A2C is straightforward: the gradient of the value loss is separated from the gradient of the policy with entropy regularization. We can even perform multiple \u201cinner\" optimization steps on the critic. Since the critic is aiming at a moving target (the value function depends on the policy, which keeps changing), there should be a clear sweet spot between overfitting the critic to the current policy and making it unable to catch up. Although more extensive experiments would be helpful, for now, we just set the number of inner updates to\""}, {"title": "A.3 Step discarding heuristics", "content": "During the preliminary experiments, we identified multiple scenarios that occur rather rarely but lead to the collapse of the learning process. Unstable training on toy environments was especially observed with the hypernetwork architecture. In part, it can be remedied by clipping the gradient norms (which we do) or even just by setting a lower learning rate, but this comes at the cost of lower sample efficiency. The collapse of learning that we observed mostly happened rapidly and led to trivial policies and zero entropy. To prevent the missteps that lead to collapse, we employ the following heuristics:\n1. If the entropy dropped significantly (the change in entropy is negative, and its absolute value is three standard deviations above the average absolute changes in entropy) and the average reward did not increase from the previous step, we discard the step."}, {"title": "A.4 Entropy control", "content": "The method described in Section 5.2 to control entropy during training allows us to shape the entropy according to any schedule. An investigation into principled ways of selecting such schedules would be quite interesting, but it is out of the scope of this paper. We have selected three schedules based on our intuition about exploration-exploitation trade-offs. All of these schedules start from the maximal possible entropy Htarget(0) = Hmax = log |A| and progress to Htarget(1) = Hmin. For Minecart and resource gathering experiments, we set Hmin = 0.4; for deep sea treasure, we set it to 0.1. The schedules are defined as Htarget(u), where u \u2208 [0, 1] denotes the proportion of environment interactions that were used so far from the total budget allocated for training.\nThe first schedule is a simple linear function going from the maximal entropy Hmax = log | A to a desired minimal value Hmin:\nH_{target,lin}(u) = H_{max} - (H_{max} - H_{min}) u. (24)\nThe second schedule is normalized cosine, used to provide extra high-entropy exploration time in the beginning:\nH_{target,cos}(u) = (H_{max} - H_{min}) cos(\\pi u/2) + H_{min}. (25)\nFinally, we also designed a \"custom\" schedule with a flat start to provide exploration time and, unlike the previous schedule, has an extended flat end around the lower entropy to provide time for exploitation. We used the expression\nH_{target,custom}(u) = (H_{max} - H_{min}) (0.5 - \\cos (\\pi (1 - u)^{1.3}) / 2 + H_{min}. (26)"}]}