{"title": "LoLaFL: Low-Latency Federated Learning via Forward-only Propagation", "authors": ["Jierui Zhang", "Jianhao Huang", "Kaibin Huang"], "abstract": "Federated learning (FL) has emerged as a widely adopted paradigm for enabling edge learning with distributed data while ensuring data privacy. However, the traditional FL with deep neural networks trained via backpropagation can hardly meet the low-latency learning requirements in the sixth generation (6G) mobile networks. This challenge mainly arises from the high-dimensional model parameters to be transmitted and the numerous rounds of communication required for con-vergence due to the inherent randomness of the training process. To address this issue, we adopt the state-of-the-art principle of maximal coding rate reduction to learn linear discriminative fea-tures and extend the resultant white-box neural network into FL, yielding the novel framework of Low-Latency Federated Learn-ing (LoLaFL) via forward-only propagation. LoLaFL enables layer-wise transmissions and aggregation with significantly fewer communication rounds, thereby considerably reducing latency. Additionally, we propose two nonlinear aggregation schemes for LoLaFL. The first scheme is based on the proof that the optimal NN parameter aggregation in LoLaFL should be harmonic-mean-like. The second scheme further exploits the low-rank structures of the features and transmits the low-rank-approximated covari-ance matrices of features to achieve additional latency reduction. Theoretic analysis and experiments are conducted to evaluate the performance of LoLaFL. In comparison with traditional FL, the two nonlinear aggregation schemes for LoLaFL can achieve reductions in latency of over 91% and 98%, respectively, while maintaining comparable accuracies.", "sections": [{"title": "I. INTRODUCTION", "content": "With the growing volume of data and the increasing number of edge devices, the sixth generation (6G) mobile networks are envisioned to support a wide range of AI-based applications at the network edge, including augmented/mixed/virtual reality, connected robotics and autonomous systems, and smart cities and homes, among others [1], [2]. To realize this vision, researchers have been motivated to develop technologies to deploy AI models at the network edge. These technologies, collectively called edge learning, leverage the mobile-edge-computing platform to train edge-AI models among edge servers and devices [3], [4]. For its preservation of data privacy, federated learning (FL) emerges as a widely adopted solution for distributed edge learning, where local models are trained using local devices' data and sent to the server for updating the global model [5]\u2013[7]. This collaborative training approach enables multiple devices and a server to train a global model without sharing raw data. However, FL faces its own challenges. First, in scenarios where edge devices exhibit high mobility (e.g., autonomous cars and drones), they may move out of the range of an edge server before the learning process is completed. Second, in contexts with dynamic environments and evolving user behaviors, timely model retraining is crucial. These challenges necessitate the development of low-latency FL techniques to achieve faster response times [8]-[11].\nHowever, achieving low-latency FL is challenging due to limited communication resources, which hinder the wireless exchange of high-dimensional stochastic gradients or models between devices and edge servers [12], [13]. Researchers have explored various approaches to allocate network resources and schedule participating devices such as wireless power transfer [14], resource allocation [15]\u2013[18], and client scheduling [19], [20], to improve task performance. For example, in [20], a problem of joint learning, resource allocation, and user selec-tion for FL over wireless networks was formulated and solved, improving the inference accuracy. Besides, the popular over-the-air computation (AirComp) technology is widely adopted to leverage the property of waveform superposition over a multi-access channel to realize simultaneous model uploading and over-the-air aggregation, thereby accelerating FL [21]\u2013[23].\nDespite these efforts to optimize the resource allocation for latency reduction, the bottleneck of low-latency FL lies in the high-dimensional gradients or model parameters to be transmitted and the numerous rounds for convergence [24], [25]. For the first problem, approaches are considered to reduce the number of parameters to be transmitted. For example, model splitting introduces a method where the global model can be partitioned and distributed between server and devices for collaborative training, thereby reducing latency by transmitting only a portion of the gradients [26], [27]. Some lossy compression techniques can also be utilized. In particular, sparsification helps to drop insignificant model parameters [28], and quantization enables the use of fewer bits to represent an element for transmission [29]. For the second problem, techniques like federated transfer learning can be utilized for model initialization and speed up the convergence [30]. In essence, these two problems arise from the fundamental nature of deep neural networks (DNNs), such as Convolutional Neural Networks (CNNs) [31]. Specifically, their architectures are typically designed using a heuristic ap-proach, and the training process involves random initialization and multiple rounds of weight updates via backpropagation (BP). This design principle, training method, and numerous heuristic techniques involved in DNNs, collectively earn them the label of black-box [32]-[36]. The bottleneck cannot be easily overcome without challenging the current paradigm of FL, which necessitates the adoption of novel NN architectures and training approaches along with the design of a compatible FL framework.\nRecently, the new approach of white-box has emerged, which focuses on providing rigorous mathematical principles to understand the underlying mechanisms of both the archi-tecture and parameters of DNNs [37]. One notable example is the recent work in [38], which proposes a forward-only algorithm to directly construct an AI model from the intrinsic structures of data, without the need for multiple rounds of BP. Taking the classification task as an example, many real-world datasets exhibit specific structures and distributions in high-dimensional spaces. Then the objective for white-box DNNS is to learn the intrinsic structures underlying the data, namely the linear discriminative features in order to achieve effective classification [37], [38]. The principle of maximal coding rate reduction (MCR2) was proposed in [39], [40] to obtain these kinds of features from data. Therein, the so-called coding rate was introduced to quantify the volume of feature space spanned by the features up to a specific precision [41], as inspired by the classic rate-distortion theory [42]. MCR2 calls for maximizing the volume of the entire feature space while minimizing that of the summed feature sub-spaces, which can be achieved through step-by-step feature transformation; the gradient information from each step forms the layer parameters of a novel white-box neural network constructed in a forward-only manner [38]. More surprisingly, it has been demonstrated that the white-box neural network has a similar architecture and comparable task accuracy to its black-box counterpart (e.g., the well-known ResNet [43]).\nThese white-box neural networks have two distinct charac-teristics. First, their parameters can be calculated from features directly and deterministically using formulae, eliminating the need of BP to update parameters. Second, such a model is constructed only forwardly, with each layer obtained based on the information from the previous layer. We advocate the de-sign of low-latency FL by adopting the new training approach of white-box neural network to leverage its above character-istics. The first characteristic facilitates rapid convergence in model training. On the other hand, the second characteristic presents a new opportunity to advance low-latency FL: in each communication round, only the parameters of the latest layer instead of the whole model need to be transmitted. However, how to apply this white-box approach to FL in order to achieve low-latency edge learning remains an open problem. Solving it requires designing unique and compatible techniques for parameter transmissions and aggregation.\nTo this end, this paper presents a novel low-latency feder-ated learning (LoLaFL) framework via forward-only propaga-tion. LoLaFL features layer-wise transmissions and aggrega-tion with much fewer communication rounds than traditional FL, thereby reducing communication latency dramatically. Specifically, in each communication round, only the latest layer targeted for updating in the round, rather than the entire NN, is uploaded for aggregation and subsequently updated with the received aggregated one."}, {"title": "II. PRINCIPLE COMPARISONS: BLACK-BOX VERSUS WHITE-BOX", "content": "DNNs can be seen as a nonlinear function that maps inputs to their corresponding outputs. In practice, the common approach is to design a heuristic architecture, and choose a loss function to measure the discrepancy between the network outputs and expected outputs for a specific learning task. The process to minimize the loss function, known as training, typically involves initializing the network parameters and then updating them via BP [32]. For classification, the global loss function is given by\n$$F(w) = \\frac{1}{D} \\sum_{(x_i,Y_i) \\in D} f(w, x_i, Y_i),$$\nwhere $D$ is the dataset, and $f(w,x,y)$ is the cross entropy (CE) to measure the sample-wise error over the model, $w$, w.r.t. sample $x$ and its true class, $y$ [44]. Then, the SGD can be used to minimize the global loss function as follows\n$$w(l + 1) = w(l) \u2013 \u03b7 \\frac{\\partial F(w)}{\\partial w}|_{w=w(l)},$$\nwhere \u03b7 is the learning rate and w(l) is the model in training round l. Despite their impressive performance in implementing various learning tasks, DNNs have long been regarded as black-boxes [32], [36]. It is challenging to interpret how the data is transformed as it passes through the DNNs and what the underlying mechanisms are.\nFL has been adopted to deal with data privacy concerns associated with training the black-box DNNs at the edge. Instead of uploading the original dataset directly, FL focuses on transmitting model updates to renew the global model through multiple rounds of communication [11]. Specifically, in round $l \\in L = \\{1, 2, ..., L\\}$, the edge server broadcasts the global model, $w(l)$, to edge devices. Let $F_k(w)$ be the local loss function over a local dataset, $D_k$, (assuming uniform sizes) at device $k \\in K = \\{1, 2, ..., K\\}$. Each device $k$ calculates the gradient of the $F_k(w)$ w.r.t. $w(l)$ based on $D_k$, and the local model is updated as\n$$wk(l + 1) = wk(l) \u2013 \u03b7 \\frac{\\partial F_k(w)}{\\partial w}|_{w=w(l)}.$$\nSubsequently, each edge device uploads the updated model $wk(l + 1)$ to the edge server, and the edge server aggregates the models using the arithmetic mean as\n$$w(l + 1) = \\frac{1}{K} \\sum_{k=1}^{K} wk(l + 1).$$\nThe procedures of (3)-(4) are iteratively repeated until convergence or the maximal round number $L$ is reached. However, significant communication latency is incurred for two reasons. First, the entire model needs to be transmitted by every device in each communication round. Second, numerous rounds are generally required to achieve convergence, due to the randomness in parameter initialization and SGD."}, {"title": "B. White-box NNs via Forward-only Propagation", "content": "The training of DNNs has been believed to follow the parsimony principle, whose goal is to learn a mapping $\\phi(\\cdot, \\theta_1)$ with parameters $\\theta_1$ to transform data $x$ to a more compact and structured feature $z$, facilitating downstream tasks [37]. Taking classification as an example, after the feature $z$ is obtained, a classifier $\\psi(\\cdot, \\theta_2)$ with parameters $\\theta_2$ (see, e.g, [43]) is then used to predict its class $y$ [38]. The entire pipeline is given as $x \\xrightarrow{\\phi(x,\\theta_1)} z \\xrightarrow{\\psi(z,\\theta_2)} y$. However, the mapping, $\\phi(\\cdot, \\theta_1)$, and the classifier, $\\psi(\\cdot, \\theta_2)$, are jointly optimized in black-box learning, without considering the features' distribution and characteristics. In contrast, white-box learning aims to find a mapping, $\\phi(\\cdot, \\theta_1)$, that produces $Z$ with the following linear discriminative properties. Features $Z$ belonging to different classes exhibit low correlation, indicating that they occupy distinct sub-spaces (ideally orthogonal) and collectively span a large feature space. Conversely, features $Z$ from the same class $j\\in I = \\{1, 2, ..., J\\}$ exhibit high correlation and span a small feature sub-space [38], [39]. This necessitates addressing three issues: 1) measuring the volume of the feature space, 2) finding the mapping $(\\cdot, \\theta_1)$ to transform the data into linear discriminative features, and 3) designing a classifier to classify unlabeled samples.\n1) Measuring the Feature Space with Coding Rate: The rate-distortion was introduced in [42] to measure the compact-ness of a random distribution, defined as the minimal binary bits to encode a random variable up to a specific distortion. Fig. 1(a) illustrates a feature space packed with small balls with diameter 2$\\\\epsilon$, where the ball number represents the rate-distortion up to distortion $\\epsilon$. With unknown distribution and limited samples, computing the rate-distortion is typically intractable. Fortunately, distributions with linear discriminative properties allow closed-form expressions for the total bits to encode the samples [41]. With enough samples, the average coding length per sample, a.k.a. the coding rate, can approx-imate the rate-distortion, serving as a natural measure of a feature space's volume.\nIn particular, given data $X = [x^{(1)}, x^{(2)}, ..., x^{(m)}] \\in \\mathbb{R}^{d\\times m}$ with $m$ samples and $d$ dimensions, and their latent features $Z = [z^{(1)}, z^{(2)}, ..., z^{(m)}]$ with the same shape, the coding rate of features $Z$ is\n$$R(Z, \\epsilon) = \\frac{1}{2} \\log \\det (I + aZZ^*),$$\nw.r.t. a certain distortion $\\epsilon$, where $(\\cdot)^*$ denotes the transpose of a matrix or vector and $a = d/(m\\epsilon^2)$ [38], [41]. Similarly, the coding rate of the union of feature sub-spaces belonging to different classes is given by\n$$R_c(Z, \\Pi) = \\sum_{j=1}^{J} \\frac{\\gamma_j}{2} \\log \\det (I + a^\\prime Z \\Pi^j Z^*),$$\nwhere $a^\\prime = d/(tr(\\Pi^j)^2 \\epsilon^2)$, $\\gamma_j = tr(\\Pi^j)/m$. And $I \\triangleq {\\Pi^j \\in \\mathbb{R}^{m \\times m}}_{j=1}^{J}$ is a set of diagonal membership matrices to characterize the associated classes of data samples. For example, if sample $i$ belongs to class $j$, then $\\Pi^j (i, i) = 1$, otherwise, $\\Pi^j (i, i) = 0$.\n2) Constructing ReduNet via MCR2: The linear discriminative properties call for a large vol-ume of the whole feature space, $R(Z, \\epsilon)$, and a small vol-ume of the individual feature spaces, $R_c(Z, \\epsilon|\\Pi)$, which necessities maximizing $\\Delta R(Z, \\epsilon|\\Pi)$ w.r.t. normalized features $Z$. Meanwhile, a mapping $(\\cdot, \\theta_1)$ is needed to transform original data $X$ to features $Z(\\theta_1) = \\phi(X, \\theta_1)$. This is called maximal coding rate reduction (MCR2), formulated as: $\\max_{\\theta_1} \\Delta R(Z, \\Pi)$, s.t. $||Z_j (\\theta_1)||_2 = tr(\\Pi^j)$. The projected gradient ascent scheme [38] works for it:\n$$Z^{l+1} = \\mathcal{P}_{S^{d-1}}(Z^l + \\eta \\frac{\\partial \\Delta R}{\\partial Z}|_{Z=Z^l}), l = 1, 2, ..., L,$$\nwhere \u03b7 is the learning rate, L is the number of transfor-mations, $Z^l$ are the features after $(l - 1)$ transformations, and $\\mathcal{P}_{S^{d-1}}(\\cdot)$ denotes the projection operation which projects vectors to the unit sphere $S^{d-1}$ for normalization. Specifically, $Z^1 = \\mathcal{P}_{S^{d-1}}(X) \\in \\mathbb{R}^{d\\times m}$.\nTo better understand the gradient, with (5)-(7), the gradient in (8) is calculated as\n$$\\frac{\\partial \\Delta R}{\\partial Z}|_{Z=Z^l} = a (I + a Z Z^*)^{-1} Z - \\sum_{j=1}^{J} a^\\prime (I + a^\\prime Z \\Pi^j Z^*)^{-1} Z \\Pi^j = \\alpha (E^l Z - \\sum_{j=1}^{J} C^{lj} Z \\Pi^j),$$\nThe matrices $E_l$ and $C^{lj}$ form the layer parameters of a neural network, called ReduNet, which is constructed via forward-only propagation. As $E_l$ and $C^{lj}$ are from (5) and (6) respectively, $E_l$ forces $Z_l$ from different classes to diverge while $C^{lj}$ compresses $Z_l$ from the same class $j$, as shown"}, {"title": "III. SYSTEM MODEL", "content": "Consider a general FL system in which K edge devices with their local datasets aim to learn the optimal NN parameters as coordinated by the edge server over a total of L commu-nication rounds. Similar to the traditional FL procedure, in each communication round, local models are updated based on local datasets, the updated parameters are uploaded to the edge server for aggregation, and the aggregated parameters are subsequently broadcast to edge devices for updating local models (elaborated in Section IV-A). The transmission process in each communication round is described as follows.\nThe orthogonal frequency-division multiple access (OFDMA) is adopted, where the available bandwidth B is divided into M orthogonal subchannels, and each edge device is assigned M/K sub-channels to avoid the interference [21], [45]. At edge device k, local parameters $g_k \\in \\mathbb{R}^{Q}$ are to be uploaded. Each parameter is quantized into Q bits by uniform quantization as in [23] which are then modulated into symbols. The i-th symbol received at the server is given by\n$$y_{i,k} = h_k\\sqrt{p_k}x_{i,k} + n_{i,k},$$\nwhere $x_{i,k}$ is the i-th symbol from edge device k, $h_k$ is the channel coefficient between device k and server, $p_k$ is the associated power control policy, and $n_{i,k} \\sim \\mathcal{CN}(0, \\sigma^2)$ is the independent and identically distributed (IID) additive white Gaussian noise (AWGN). We assume a slow fading channel where $h_k$ remains constant over a single uploading round and is assumed to be known to both sides. We model $h_k$ as Rayleigh fading with $h_k \\sim \\mathcal{CN}(0,1)$, where the coefficients are IID across different devices and different rounds [18], [23]. In FL, model aggregation is implemented after all devices have completed uploading their local models. Consequently, poor channel conditions can impede the local model uploading process on some devices, thereby increasing overall latency. To mitigate fading, we adopt the truncated power control policy, as in [21]:\n$$p_k = \\begin{cases} p_0 / |h_k|^2, & |h_k|^2 \\geq \\tau, \\\\ 0, & |h_k|^2 < \\tau, \\end{cases}$$\nwhere p0 is a scaling factor to meet the power constraint in the sequel, and \u03c4 is the power cut-off threshold to avoid deep fading. The power constraint for each device is $E[p_k] \\leq P_0$, with P0 being the power budget per device. By analyzing this expectation, we can derive the exact value of $p_0 \\triangleq P_0/E_i(\\tau)$, where $E_i(x) = \\int_x^\\infty \\frac{e^{-s}}{s} ds$ [46]. This policy can result in an outage probability of $\\epsilon \\approx Pr(|h_k|^2 < \\tau) = 1-\\exp(-\\tau)$. According to above settings, when $|h_k|^2 > \\tau$, the transmission rate of device k is given by\n$$T_k = \\frac{B}{K} \\log_2 (1 + \\frac{M P_0 \\gamma}{K \\sigma \\nu E_i(\\tau)}),$$\nThen, the uploading communication latency (in seconds) for device k in round l is given by\n$$T_{comm,l,k} = \\frac{K q Q}{B \\log_2 (1 + \\frac{M P_0 \\gamma}{K \\sigma \\nu E_i(\\tau)})}$$\nFor devices whose channels fail to meet the threshold \u03c4, they give up transmission without repeated attempts. Thus, no additional retransmission latency is incurred. The resultant loss could degrade the learning performance, which will be investigated in experiments."}, {"title": "IV. LOLAFL VIA FORWARD-ONLY PROPAGATION", "content": "In this section, we propose a novel FL framework for achieving low-latency edge learning based on the white-box NN introduced in Section II-B. First, the model uploading and aggregation processes of the proposed framework based on the forward-only propagation algorithm are introduced. Then, two novel nonlinear aggregation methods are presented.\nUnlike traditional FL where the whole model is exchanged between the edge devices and the server, LoLaFL enables the white-box NNs to be constructed and updated in a layer-wise manner."}, {"title": "B. Harmonic-mean-Like Aggregation", "content": "In this subsection, the parameters to be exchanged between edge devices and the edge server are the parameters of the white-box NN, i.e., $g_{l,k} = \\{E_{l,k}\\} \\cup \\{C_{l,k}^j\\}_{j=1}^J$. However, the FedAvg in traditional FL is not optimal for the aggregation in this scenario, because the NN parameters are derived from the features with nonlinear mappings. Hence, a novel compatible aggregation scheme is designed for LoLaFL as follows.\nReferring to (16) and (17), the local NN parameters, $\\{E_{l,k}\\} \\cup \\{C_{l,k}^j\\}_{j=1}^J$, are determined by the CMs of local features, i.e., $R_{l,k} \\triangleq Z_{l,k}Z_{l,k}^*$ and $R_{l,k}^j \\triangleq Z_{l,k} \\Pi^j Z_{l,k}^*$. And referring to (9), the global NN parameters, $\\{\\bar{E}_{l}\\} \\cup \\{\\bar{C}_{l}^j\\}_{j=1}^J$, are determined by CMs of global features, i.e., $R \\triangleq Z Z^*$ and $R^j \\triangleq Z \\Pi^j Z^*$. Therefore, aggregation of $\\{E_{l,k}\\} \\cup \\{C_{l,k}^j\\}_{j=1}^J$ fundamentally requires aggregation of the CMs of local features. To this end, we obtain the following results.\nLemma 1 In each communication round l, the CMs of global features can be decomposed as the summation of the CMs of local features. In other words,\n$$\\bar{R}_l = \\sum_{k=1}^{K} R_{l,k} \\quad \\text{and} \\quad \\bar{R}_l^j = \\sum_{k=1}^{K} R_{l,k}^j$$\nProposition 1 (HM-like aggregation): In each communi-cation round l, the global NN parameters, $g_l \\triangleq \\{\\bar{E}_{l}\\} \\cup \\{\\bar{C}_{l}^j\\}_{j=1}^J$, can be calculated directly with local NN parameters, $\\{g_{l,k}\\}_{k=1}^K$, as\n$$\\bar{E}_{l} = \\left( \\sum_{k=1}^{K} (E_{l,k})^{-1} \\right)^{-1}$$\n$$\\bar{C}_l^j = \\left( \\sum_{k=1}^{K} (C_{l,k}^j)^{-1} \\right)^{-1}$$\nAlgorithm 1"}, {"title": "C. Covariance-matrix-Based Aggregation", "content": "In this subsection, the parameters to be exchanged between edge devices and the edge server are the collection of the low-rank versions of local CMs, i.e., $g_{l,k} = \\{\\tilde{R}_{l,k}\\} \\cup \\{\\tilde{R}_{l,k}^j\\}_{j=1}^J$, the details of which are given in the sequel. We propose this approach because the NN parameters in the HM-like scheme have very high dimensionality and may be difficult to compress. In contrast, these CMs have low-rank structures, resulting from the low-rank structures of the features. This is because ReduNet is making features sparse, so the intrinsic dimensionality of the features is small. Therefore, these CMs can be further compressed, which moti-vates the design of CM-based aggregation as follows. For ease of notation, in the following exposition, the index l is omitted whenever no confusion arises.\nThe procedure of each communication round is described as follows. Firstly, the local CMs $R_k$ and $R^j$ at each edge device are calculated. Then, the local CMs at each edge device are decomposed with singular value decomposition (SVD) [47] and approximated to some degree as follows:\n$$R_k \\approx \\tilde{R}_k = \\sum_{i=1}^{s_k} \\sigma_{i,k} u_{i,k} v_{i,k}^*,$$\n$$R^j \\approx \\tilde{R}^j = \\sum_{i=1}^{s_k^j} \\sigma_{i,k}^j u_{i,k}^j v_{i,k}^{j*},$$\nIn the preceding formulae, $s_k$ and $s_k^j$ are the minimal possible s to remain desired information: $\\beta \\triangleq \\frac{\\sum_{i=1}^{s_k} \\sigma_i}{\\sum_i \\sigma_i} \\geq \\beta_0$, where $\\beta$ is the information remaining rate and $\\beta_0$ is the threshold. We define the compression rate \u03b4 as the expected ratio of the number of chosen singular values to the total number of singular values. Then the singular values and vectors are uploaded as $\\tilde{\\sigma}_{i,k} = \\sigma_{i,k} + n_{\\sigma,i,k}$, $\\tilde{u}_{i,k} = u_{i,k} + n_{u,i,k}$, $\\tilde{v}_{i,k} = v_{i,k} + n_{v,i,k}$, $k = \\sigma_{i,k}^j + n_{\\sigma, i, k}^j$, $\\tilde{u}_{i,k}^j = u_{i,k}^j + n_{u,i,k}^j$, and $\\tilde{v}_{i,k}^j = v_{i,k}^j + n_{v,i,k}^j$, where the distortions are specified in the system model. Thus the low-rank-approximated CMs can be reconstructed at the edge server as\n$$\\tilde{R}_k = \\sum_{i=1}^{s_k} \\tilde{\\sigma}_{i,k} \\tilde{u}_{i,k} \\tilde{v}_{i,k}^*,$$\n$$\\tilde{R}^j = \\sum_{i=1}^{s_k^j} \\tilde{\\sigma}_{i,k}^j \\tilde{u}_{i,k}^j \\tilde{v}_{i,k}^{j*},$$\nThen we can calculate the CMs of global features, $\\bar{R}$ and $\\bar{R}^j$, using (18) by replacing $R_{e,k}$ with $\\tilde{R}_{e,k}$ and $R^j$ with $\\tilde{R}^j$. If needed (when the edge server also needs the entire model), the global NN parameters can be calculated using (9)"}, {"title": "V. PERFORMANCE ANALYSIS", "content": "In this section, we first analyze the communication latency and computational complexity of the LoLaFL with a com-parison with traditional FL. Next, we provide a proof of the privacy guarantee in LoLaFL."}, {"title": "A. Latency Analysis", "content": "For brevity, we only consider the number of parameters uploaded from each device k, from which the communication latency can be easily obtained. For LoLaFL with HM-like ag-gregation, in each round, uploading of local parameters yields $(J+1)d^2$. So, the total number of parameters transmitted over L rounds is $L(J+1)d^2$. For LoLaFL with CM-based aggrega-tion, since SVD is used to reduce the latency, in each round, the upload of compressed CMs yields $(J+1)(2\\delta d^2+\\delta d)$. Thus, the total number of parameters transmitted over L rounds is $L(J + 1)(2\\delta d^2 + \\delta d)$. For traditional FL, let W denote the parameter number of the utilized DNN model. In each round, uploading the local parameters yields W. And the total number of parameters transmitted over L rounds is LW.\nAs summarized in Table II, considering the number of parameters to be transmitted and focusing on the dominant part (i.e., terms with $d^2$) in the expressions, the CM-based scheme outperforms the HM-like scheme, as long as $\\delta < 1/2$. The latency of LoLaFL is proportional to $d^2$ and J while that of traditional FL does not depend on d and J. This means that for datasets with high dimensionality and a large number of classes, LoLaFL may not outperform traditional FL."}, {"title": "B. Complexity Analysis", "content": "For computational complexity, we only consider matrix multiplication, matrix inversion, and SVD (if any), as these operations dominate the complexity. Generally, the multiplica-tion of two matrices with shapes $(m\\times n)$ and $(n\\times k)$ takes mnk operations. For an invertible $n\\times n$ matrix, the computational complexity of calculating its inversion is $O(n^3)$. For an $m \\times n$ matrix, the computational complexity of calculating its SVD is $O(mn \\min(m, n))$ [48].\nFor LoLaFL with HM-like aggregation, in each round, according to (16) and (17), the parameter calculation at edge devices requires $\\sum_{k=1}^K O(2m_kd^2 + (J + 1)d^3) = O((J + 1)Kd^3 + 2md^2)$. Based on (19) and (20), the aggregation at edge server requires $O((J + 1)(K + 1)d^3)$. According to (8), feature transformation requires $\\sum_{k=1}^K O((J+1)m_kd^2) = O((J+1)md^2)$. Combining these operations yields a computational complexity of $O((J + 1)(2K + 1)d^3 + (J+3)md^2)$.\nFor LoLaFL with CM-based aggregation, in each round, the local CM calculation at edge devices requires $\\sum_{k=1}^K O(2m_kd^2) = O(2md^2)$. According to (21a) and (21b), the SVD for the local CMs requires $\\sum_{k=1}^K O((J + 1)d^3) = O((J+1)Kd^3)$. The reconstruction process at the edge server requires $\\sum_{k=1}^K O(2\\delta d d^2) = O(2\\delta Kd^2)$. The aggregation at the edge server can be omitted because only addition is used. According to (23a) and (23b), the SVD for the global CMs requires $O((J+1)d^3)$, and the reconstruction process at the edge devices requires $\\sum_{k=1}^K O(2\\delta d d^2) = O(2\\delta Kd^2)$. The parameter calculation and feature transformation require $O((J+1)Kd^3)$ and $\\sum_{k=1}^K O((J + 1)m_kd^2) = O((J + 1)md^2)$ respectively. Combining these operations yields $O((J + 1)(2K + 1)d^3 + [4\\delta K + (J +3)m]d^2)$.\nFor traditional FL, we analyze a fully-connected NN with N layers, each containing n nodes. During forward propagation, passing $m_k$ samples from the input layer to the first hidden layer incurs $O(m_kdn)$. Passing them through the subsequent (N - 1) hidden layers yields $O((N - 1)m_kn^2)$, and passing them from the last hidden layers to the output layer yields $O(m_kJn)$. The low complexity associated with adding the bias term and calculating the activation function is omitted. Combining these components results in the complexity of forward propagation for device k as $O(m_k(dn+(N-1)n^2+Jn))$, which is equivalent to that of the backpropagation. Therefore, the overall complexity of forward propagation and backpropagation in all edge devices is given by $\\sum_{k=1}^K O(2m_k(dn + (N - 1)n^2 + Jn)) = O(2m((N - 1)n^2 + (J + d)n))$ [49].\nAs summarized in Table II, for LoLaFL, if we only focus on the dominant part (i.e., terms with $d^3$) in the expres-sions, the HM-like and CM-based schemes have comparable computational complexity. The computational complexity of LoLaFL is proportional to $d^3$ and J, while for traditional FL, the dominant part is proportional to $n^2$ and N. This indicates that the bottleneck of LoLaFL is primarily related to the complexity of the datasets, while that of traditional FL is associated with the width and depth of the neural network.\nAs demonstrated by the experiments in the sequel, the CM-based scheme achieves over 98% reduction in total latency (communication latency and computation latency) compared with traditional FL."}, {"title": "C. Privacy Guarantee", "content": "In traditional FL, the original data are kept locally and are not sent to the server, thereby ensuring data privacy. In Lo-LaFL, although the original data remain local, the transmitted parameters are related to features that are transformed from the original data. We will demonstrate that, for both the HM-like and CM-based schemes in LoLaFL, it is not possible to derive the features from the transmitted parameters, let alone recovering the original data. The details are as follows.\nLet $Z_k^j"}]}