{"title": "DM-CODEC: DISTILLING MULTIMODAL REPRESENTATIONS FOR SPEECH TOKENIZATION", "authors": ["Md Mubtasim Ahasan", "Md Fahim", "Tasnim Mohiuddin", "A KM Mahbubur Rahman", "Aman Chadha", "Tariq Iqbal", "M Ashraful Amin", "Md Mofijul Islam", "Amin Ahsan Ali"], "abstract": "Recent advancements in speech-language models have yielded significant im- provements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual infor- mation for precise speech representations. Existing speech representations gen- erally fall into two categories: acoustic tokens from audio codecs and seman- tic tokens from speech self-supervised learning models. Although recent efforts have unified acoustic and semantic tokens for improved performance, they over- look the crucial role of contextual representation in comprehensive speech mod- eling. Our empirical investigations reveal that the absence of contextual repre- sentations results in elevated Word Error Rate (WER) and Word Information Lost (WIL) scores in speech transcriptions. To address these limitations, we propose two novel distillation approaches: (1) a language model (LM)-guided distilla- tion method that incorporates contextual information, and (2) a combined LM and self-supervised speech model (SM)-guided distillation technique that effec- tively distills multimodal representations (acoustic, semantic, and contextual) into a comprehensive speech tokenizer, termed DM-Codec. The DM-Codec archi- tecture adopts a streamlined encoder-decoder framework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM during the training process. Experiments show DM-Codec significantly outperforms state-of-the-art speech tokenization models, reducing WER by up to 13.46%, WIL by 9.82%, and im- proving speech quality by 5.84% and intelligibility by 1.85% on the LibriSpeech benchmark dataset. The code, samples, and model checkpoints are available at https://github.com/mubtasimahasan/DM-Codec.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, the advent of Large Language Models (LLMs) has revolutionized various domains, offering unprecedented advancements across a wide array of tasks (OpenAI, 2024). A critical com- ponent of this success has been the tokenization of input data, enabling vast amounts of information processing (Du et al., 2024; Rust et al., 2021). Inspired by these breakthroughs, significant attention has shifted towards replicating similar successes in the realm of speech understanding and genera- tion (D\u00e9fossez et al., 2022; Hsu et al., 2021). However, tokenizing speech into discrete units presents unique challenges compared to text, as speech is inherently continuous and multidimensional, re- quiring various speech attributes such as acoustic properties, semantic meaning, and contextual clues (Ju et al., 2024). Traditional approaches using feature representations such as Mel-Spectrograms (Sheng et al., 2019), Mel-frequency cepstral coefficients (MFCCs) (Juvela et al., 2018), and Wave-"}, {"title": "2 PROPOSED METHOD", "content": "In this section, we present DM-Codec, a novel speech tokenizer designed to encapsulate a compre- hensive fusion of multimodal (acoustic, semantic, and contextual) representations. As illustrated in Figure 2, we propose two distinct training approaches to incorporate these representations: (i) a lan- guage model (LM)-guided distillation method, and (ii) a combined LM and self-supervised speech model (SM)-guided distillation method. The first approach distills contextual representations from the LM and integrates them with learned acoustic representations. The second approach combines SM and LM to further incorporate semantic representations with contextual and acoustic represen- tations. It ensures that DM-Codec captures the essential elements of speech by harmonizing the acoustic features with contextual and semantic information. The following subsections detail our proposed distillation methods (\u00a72.1), model details (\u00a72.2), and components (\u00a72.3)."}, {"title": "2.1 SPEECH AND LANGUAGE MODEL GUIDED DISTILLATION", "content": "Our approach first transcribes the raw speech x into its corresponding text x' using a Speech-to- Text (STT) model $M_{STT}$, such that $x' = M_{STT}(x)$. For simplicity, we omit any post-processing techniques on the x'. Subsequently, we pass the text x' through a pretrained language model $M_{LM}$ to obtain contextual representations of x', tokenized into a set of tokens, $T = {t_i}_{i=1}^I$. For each token $t_i$, we extract its corresponding layer-wise hidden representations ${h_i^l}_{l=1}^L$, where L denotes the total number of layers in $M_{LM}$. We utilize all layer representations to derive the representations for each token, as each layer of a pre-trained language model captures hierarchical and contextually distinct information (Niu et al., 2022; Kovaleva et al., 2019; Hao et al., 2019). To obtain the contextual representation $S_i$ for token $t_i$, we average the hidden representations across all layers, yielding $S_i = \\frac{1}{L}\\sum_{l=1}^L h_i^l$, where $S_i \\in \\mathbb{R}^D$ where D is hidden dimension. Consequently, we obtain the contextual representations $S = {S_i}_{i=1}^I$ for the speech input x, which captures the contextually diverse information from $M_{LM}$.\nSimultaneously, we process the raw speech x through an Encoder E(x) to obtain the latent feature v. We then pass v through a Residual Vector Quantizer (RVQ) to obtain quantized features $Q = {Q^k}_{k=1}^K$, where K represents the number of quantization layers in the RVQ, and $Q^k \\in \\mathbb{R}^{D'}$ where D' is hidden dimension of $k^{th}$ RVQ layer. These quantized features are subsequently used to reconstruct the audio y via a decoder. To align the quantized feature $Q^k$ with the LM distilled features $S_i$, we apply a linear transformation $Q^k = WQ^k$, where $W \\in \\mathbb{R}^{D' \\times D}$, ensuring the dimensional consistency for the distillation process.\nLM Guided Distillation: In this approach, we distil the LM representations S. To calculate the LM-guided distillation loss, we adopt a continuous representation distillation technique, similar to the one employed by SpeechTokenizer (Zhang et al., 2024a), which maximizes the cosine similarity at the dimension level across all time steps. In our case, we calculate the continuous representa- tion distillation of the transformed quantized features $Q_i^k$ and the LM representation features S as follows:"}, {"title": "2.2 MODEL DETAILS", "content": "Our framework builds upon the Residual Vector Quantizer with Generative Adversarial Networks (RVQ-GAN) architecture, incorporating state-of-the-art components and novel distillation tech- niques. The core of our model consists of an Encoder E and Decoder D with an RVQ architecture, inspired by Encodec (D\u00e9fossez et al., 2022) and SpeechTokenizer (Zhang et al., 2024a). Moreover, we employ a multi-discriminator framework, comprising: Multi-Scale Discriminator (MSD), Multi- Period Discriminator (MPD), and Multi-Scale Short-Time Fourier Transform (MS-STFT) Discrimi- nator, adopted from HiFi-Codec (Yang et al., 2023) and HiFi-GAN (Kong et al., 2020). This founda- tion provides a robust basis for speech quantization. To further enhance the quantizer with distilled multimodal representations, we use wav2vec 2.0 (wav2vec2-base-960h) as $M_{STT}$ (Baevski et al., 2020), BERT (bert-base-uncased) as $M_{LM}$ (Devlin et al., 2019), and HuBERT (hubert-base-1s960) as $M_{SM}$ (Hsu et al., 2021). We extract the quantized output from the first layer of the RVQ (RVQ-1) for LM-guided distillation and the average of the quantized features across all eight layers (RVQ- 1:8) for SM-guided distillation to calculate the distillation loss."}, {"title": "2.3 MODEL COMPONENTS", "content": "Encoder Decoder. The encoder-decoder architecture in DM-Codec is based on SEANet (Tagliasac- chi et al., 2020), leveraging the successful design employed in recent speech tokenization models (Zhang et al., 2024a; D\u00e9fossez et al., 2022; Zeghidour et al., 2021). The architecture is designed to efficiently process and reconstruct speech signals while maintaining high fidelity. The Encoder E consists of a 1D convolution layer with C channels and a kernel size of 7, followed by B residual convolutional blocks. Each block contains a strided convolutional downsampling layer with kernel size K (where K = 2S, and S represents the stride), paired with a residual unit. The residual unit comprises two convolutional layers with a kernel size of 3 and a skip connection, while the number of channels is doubled at each downsampling stage. This is followed by a two-layer BiLSTM and a final ID convolutional layer with D output channels and a kernel size of 7. The Decoder D mir- rors the encoder's structure but replaces BiLSTM with LSTM, strided convolutions with transposed convolutions, and employs reversed strides for up-sampling. The final audio output is reconstructed from D. For the experiments, we use the following configuration: C = 32, B = 4, and S = (2, 4, 5, 8).\nResidual Vector Quantizers. The Residual Vector Quantizer (RVQ) plays a central role in our tokenization process, quantizing the encoder's outputs. Our implementation is inspired by the train- ing procedures described in Encodec (D\u00e9fossez et al., 2022) and SpeechTokenizer (Zhang et al., 2024a). The RVQ projects input vectors to the most similar entry in a codebook, and the residual is calculated and processed in subsequent quantization steps, each utilizing a different codebook. The codebook entries are updated using an exponential moving average (EMA) with a decay rate of 0.99 for the matched item, while unmatched entries are replaced by candidates from the current batch. To ensure proper gradient flow during training, we employ a straight-through estimator. A commitment loss is also computed and added to the total training loss to promote stability. In our experiments, we utilize a codebook size of 1024 and 8 quantization levels.\nDiscriminators. We incorporate a trio of discriminators to enhance the quality and realism of the generated speech: the Multi-Scale Discriminator (MSD), the Multi-Period Discriminator (MPD), and the Multi-Scale Short-Time Fourier Transform (MS-STFT) discriminator. The MS-STFT dis- criminator follows the implementation outlined in (D\u00e9fossez et al., 2022), operating on the real and imaginary components of multi-scale complex-valued STFTs. It begins with a 2D convolutional layer, followed by 2D convolutions with increasing dilation rates in the time dimension (1, 2, and 4) and a stride of 2 across the frequency axis in each sub-network. A final 2D convolution with a kernel size of 3 \u00d7 3 and a stride of (1, 1) is applied to produce the prediction. The MSD and MPD discriminators follow the architectures introduced in (Kong et al., 2020), with adjustments to the channel numbers to align the parameter count more closely with the MS-STFT discriminator. This ensemble of discriminators works in concert to provide comprehensive feedback on various aspects of the generated speech, contributing to the overall quality and naturalness of the output."}, {"title": "2.4 TRAINING OBJECTIVE", "content": "Our training strategy employs a GAN-guided framework, following methodologies established in recent work (Zhang et al., 2024a; Yang et al., 2023). In addition to the distillation loss described in Section 2.1, we utilize reconstruction losses, adversarial and feature matching losses, and a commit- ment loss to guide the learning process. For the original speech x and the reconstructed speech x, we calculate the losses as described below.\nReconstruction Loss. To ensure that the model preserves the key attributes of speech, we employ both time-domain and frequency-domain reconstruction losses. The time-domain loss $L_t$ is com- puted as the L1 distance between x and y. For the frequency-domain loss $L_f$, we combine L1 and L2 losses over 64-bin Mel-spectrograms $Mel_i$, with varying window sizes of $2^2$, hop lengths of $2^{2/4}$, and scales $e = {5, ..., 11}$.\n$L_t = ||x - \\hat{x}||_1$ (4)\n$L_f = \\sum_{i \\in e}(||Mel_i(x) - Mel_i(\\hat{x})||_1 + ||Mel_i(x) - Mel_i(\\hat{x})||_2)$ (5)\nAdversarial Loss. The adversarial loss promotes the generator to produce realistic and indistin- guishable speech. We apply a hinge loss formulation to compute the adversarial loss for both the generator $L_g$ and the discriminator $L_d$. These losses are computed across all three discriminators: the multi-scale discriminator (MSD), multi-period discriminator (MPD), and the multi-scale STFT guided (MS-STFT) discriminator.\n$L_g = \\frac{1}{N}\\sum_{n=1}^N max(1 - R_n(x),0)$ (6)\n$L_d = \\frac{1}{N}\\sum_{n=1}^N (max(1 - R_n(x),0) + max(1 + R_n(\\hat{x}), 0))$ (7)\nwhere N is the number of discriminators and $R_n$ represents the $n^{th}$ discriminator.\nFeature Matching Loss. To prevent the generator from overfitting to the discriminator's decisions, we apply a feature matching loss $L_{fm}$. This loss compares features from each discriminator $R_n$'s internal layers M across all dimensions, promoting stability and better generalization.\n$L_{fm} = \\frac{1}{NM}\\sum_{n=1}^N \\sum_{m=1}^M \\frac{||R_m^n(x) - R_m^n(\\hat{x})||_1}{mean(||R_m^n(x)||_1)}$ (8)\nRVQ Commitment Loss. To guide the encoder to produce outputs that closely match their corre- sponding quantized values in the residual vector quantization (RVQ) process, we introduce a com- mitment loss $L_w$. For $N_q$ quantization vectors, where $q_i$ represents the current residual and $q_c$ is the closest entry in the corresponding codebook for the $i^{th}$ entry, the $L_w$ is computed as:\n$L_w = \\sum_{i=1}^{N_q} ||q_i - q_c||_2$ (9)\nOverall Generator Loss. The total generator loss $L_G$ is a weighted sum of the individual loss components, including the distillation loss $L_{L/LS}$ (which is either $L_L$ or $L_{LS}$ depending on the chosen distillation method). We use the corresponding weighting factors $\\lambda_{L/LS}$, $\\lambda_t$, $\\lambda_f$, $\\lambda_g$, $\\lambda_{fm}$, and $\\lambda_w$ to control the influence of each loss component on the overall training objective as:\n$L_G = \\lambda_{L/LS}L_{L/LS} + \\lambda_tL_t + \\lambda_fL_f + \\lambda_gL_g + \\lambda_{fm}L_{fm} + \\lambda_wL_w$ (10)\nThis comprehensive training objective ensures DM-Codec learns acoustic speech representations while incorporating semantic and contextual representation through novel distillation approaches."}, {"title": "4 EXPERIMENTAL RESULTS AND DISCUSSION", "content": "We conducted extensive experiments to evaluate DM-Codec's reconstructed speech using WER and WIL for contextual information retention, and ViSQOL and STOI for semantic-acoustic information preservation. To demonstrate the effectiveness of our two distillation approaches, we present results for DM-Codec (LM-guided Distillation) and DM-Codec (LM and SM-guided Distillation)."}, {"title": "4.1 COMPARISON OF SPEECH TOKENIZATION MODELS", "content": "We compared the quality of DM-Codec's discrete speech representations by reconstructing speech from quantized vector features and comparing it with state-of-the-art (SOTA) speech tokenization models: EnCodec, SpeechTokenizer, and FACodec. For LM-guided distillation, we utilize quantized features from the first Residual Vector Quantizer layer (RVQ-1), and for the combined LM and SM- guided distillation, we average all layers (RVQ-1:8).\nResults: The results in Table 1 show that DM-Codec outperformed all evaluated SOTA speech to- kenization models across most metrics. Specifically, DM-Codec with only LM-guided distillation exceeds the SOTA models, achieving improved scores: WER 4.36, WIL 7.06, and ViSQOL 3.18. Furthermore, DM-Codec's with combined LM and SM-guided distillation outscore LM-guided dis- tillation and all previous scores with 4.05 WER, 6.61 WIL, 3.26 ViSQOL, and achieved highly compatible 0.937 STOI scores compared to SOTA models.\nDiscussion: The observed performance gains stem from the proposed LM-guided distillation, which enhances the quantized features by leveraging LM's contextual representations. This process aligns the speech with its overall context and word relation, resulting in more accurate reconstructions, as reflected in the reduced WER and WIL scores. By embedding contextual cues, the method effectively grounds isolated phonetic units within their overall context, reconstructing speech that aligns with human expectations, as demonstrated by the higher ViSQOL and STOI scores.\nMoreover, the integration of LM and SM-based distillation further amplifies these improvements. The addition of SM distillation contributes to enhanced semantic-acoustic fidelity, as SM mod- els capture phonetic nuances alongside prosodic and tonal characteristics. This dual representa- tion-context from LM and phonetic detail from SM-produces a more coherent and natural speech reconstruction, yielding superior results across all metrics."}, {"title": "4.2 SIGNIFICANCE ANALYSIS OF SPEECH TOKENIZER PERFORMANCE", "content": "We conducted a significance analysis at a = 0.05, following the approach of Dror et al. (2019), to measure the stochastic dominance of DM-Codec over the baselines: EnCodec, SpeechTokenizer, and FACodec. Specifically, we computed inverse cumulative distribution functions (CDFs) for all reconstructed speech samples' individual WER, WIL, ViSQOL, and STOI scores. Notably, the average WER and WIL are calculated from each sentence individually, while the Table 1 scores are calculated by concatenating all sentences into one. Significance was evaluated using the e value and categorized as: significantly better when $0.0 < \\epsilon < 0.5$, significantly dominant when $\\epsilon = 0.0$, and not significantly better when $\\epsilon > 0.5$. For this analysis, we selected DM-Codec, trained with combined LM and SM-guided distillation. To the best of our knowledge, we are the first to conduct significance analysis to measure the effectiveness of different speech tokenizers.\nResults and Discussion: The results in Table 2 show that DM-Codec significantly outperforms the baselines in WER, WIL, ViSQOL, and STOI scores. The improved average values (0.053 WER, 0.082 WIL, 3.258 ViSQOL, 0.937 STOI) and consistent standard deviations (0.113 WER, 0.157 WIL, 0.193 ViSQOL, 0.019 STOI) further demonstrate the statistical significance. Notably, DM- Codec's performance in WER and WIL underscores the importance of contextual representation distillation for enhanced speech reconstruction. Additionally, its dominance in ViSQOL and STOI, especially over EnCodec, highlights the benefits of combining LM and SM distillation for retaining semantic-acoustic fidelity. While DM-Codec does not achieve significant dominance over FACodec in terms of STOI, it significantly outperforms the baselines across all other metrics. Among the base- line, however, FACodec achieves improved results over EnCodec and SpeechTokenizer, whereas SpeechTokenizer surpasses EnCodec in performance."}, {"title": "4.3 ABLATION STUDIES", "content": "We conducted a thorough analysis of DM-Codec's performance and the impact of each methodolog- ical choice in LM-guided and combined LM and SM-guided distillation. Unless otherwise stated, we use distillation for both LM and SM from the first Residual Vector Quantizer layer (RVQ-1) for comparison consistency and simplicity."}, {"title": "4.3.1 ABLATION STUDY: IMPACT OF COMBINED SEMANTIC DISTILLATION", "content": "We conducted experiments with different weighted combina- tions of LM and SM distillation loss to evaluate their impact on reducing WER. The combined distillation loss from Equation 3 was updated using SM and LM weights (ASM and ALM), ranging from 0.0 to 1.0, with the constraint $A_{SM} + X_{LM} = 1$.\n$L_{LS} = \\frac{1}{2}(A_{SM} \\cdot L_{SM} + A_{LM} \\cdot L_{L})$ (11)\nResults and Discussion: The experimental results are pre- sented in Table 3, showing the speech reconstruction results with WER scores for different weighted combinations. From the values, we notice a trend showing that incorporating LM representations significantly improves WER, especially when LM distillation is dominant. The lowest WER score of 4.07 occurs with a weight of $X_{LM} = 0.8$ for LM, while $A_{SM} = 0.2$ for SM, highlighting the strong influence of LM distillation on capturing contextual information. A balanced weighting of $A_{SM} = 0.5$ and $X_{LM} = 0.5$ produces a WER of 4.18, confirming that distillation from both LM and SM is benefi- cial. However, as the weighting shifts more in favor of SM ($A_{SM} > 0.7$), WER deteriorates, reaching 4.83 when relying entirely on SM. This underscores that over-reliance on SM distillation compromises contextual accuracy in favor of raw speech features. Thus, an LM-dominant approach yields optimal results, while using SM alone is less effective in preserving content."}, {"title": "4.3.2 ABLATION STUDY: IMPACT OF DISTILLATION ON DIFFERENT RVQ LAYERS", "content": "We evaluated the effect of applying distillation at various Residual Vector Quantizer (RVQ) layers, including the first layer (RVQ-1), the average of eight layers (RVQ-1:8), and the last layer (RVQ-8). Table 4 shows the full results.\nResults and Discussion: In LM-guided distillation, RVQ-1:8 achieves the best WER and WIL scores (4.23 and 6.94), though with lower ViSQOL and STOI scores (3.12 and 0.929) compared to RVQ-8 (3.28 and 0.935). The RVQ-1 layer provides the best overall balance between content preser- vation and perceptual quality, with WER, WIL, ViSQOL, and STOI scores of 4.36, 7.06, 3.18, and 0.935. This demonstrates RVQ-1:8 prioritizes contextual integrity, while RVQ-8 favors perceptual quality. Thus, we select RVQ-1 for LM-guided distillation due to its balanced performance.\nFor LM and SM-based distillation, the RVQ-1 and RVQ-1:8 combination achieves the best WER and WIL scores (4.05 and 6.61), with RVQ-1 and RVQ-1 as the second-best (4.18 and 6.84). In contrast, the RVQ-1 and RVQ-8 combination yields the highest ViSQOL and STOI scores (3.33 and 0.939), followed by RVQ-8 and RVQ-1 (3.30 and 0.938). RVQ-1 captures contextual representation more effectively due to its simpler quantized vector, while RVQ-1:8 incorporates more nuanced semantic and acoustic aspects. Overall, this ablation shows that selecting RVQ layers for LM and SM-based distillation greatly affects the balance between contextual accuracy and semantic-acoustic fidelity, allowing layer combinations to be tailored to task requirements."}, {"title": "4.3.3 ABLATION STUDY: IMPACT OF DIFFERENT MODELS ON DISTILLATION", "content": "We experimented with different LM and SM distillations to analyze performance variations based on different model selections. In addition to our selected BERT (Devlin et al., 2019) and HuBERT (Hsu et al., 2021), we experiment with ELECTRA (electra-base-discriminator) (Clark et al., 2020) as the LM and wav2vec 2.0 (wav2vec2-base-960h) (Baevski et al., 2020) as the SM. Table 5 shows the full results.\nResults and Discussion: In LM-guided distillation, the ELECTRA model significantly enhances performance, achieving WER and WIL scores of 4.12 and 6.63, respectively, compared to BERT's scores of 4.36 and 7.06. This indicates the architecture of ELECTRA's effectiveness for the pro- posed LM-guided distillation, demonstrating its superior contextual representation. These results are consistent with ELECTRA's better performance in general natural language processing tasks. However, we select BERT for its simplicity and established performance.\nIn LM and SM-guided distillation, the combination of BERT and wav2vec 2.0 achieves the highest overall performance, with scores of WER 4.13, WIL 6.77, ViSQOL 3.15, and STOI 0.942. However, the combination of BERT and HuBERT closely follows with second-best scores of WER 4.18, WIL 6.84, and ViSQOL 0.933. These findings demonstrate that different speech models can be effectively integrated with the BERT model."}, {"title": "4.3.4 ABLATION STUDY: IMPACT OF DIFFERENT DISTILLATION LAYER(S)", "content": "We evaluated speech reconstruction using different distillation layers of the LM and SM, examining which combination of layers yields the most relevant representations of semantic and contextual information. For this ablation, we considered the average of all layer representations, the $9^{th}$ layer representations, and the last layer representations. Table 6 shows the full results.\nResults and Discussion: In LM-guided distillation, the use of the average layer achieves superior overall performance, with a WER of 4.36, WIL of 7.06, ViSQOL of 3.18, and STOI of 0.935, com- pared to the variants utilizing the last and $9^{th}$ layers. Similarly, in LM and SM-guided distillation, the average layer yields superior results compared to the last and $9^{th}$ layer variants.\nThe results indicate that averaging all layers leads to more comprehensive representations of seman- tic or contextual information. In the case of LM, the averaging process provides greater contextual representation and synergizes syntactic information from earlier layers and abstract word relations from higher layers. In combined LM and SM-guided distillation, averaging all SM layers provides a more nuanced understanding of the earlier layer's phonetic information and the higher layers' richer semantic information. Conversely, relying solely on the last layer or the $9^{th}$ layer fails to capture the overall context and semantic information, yielding less relevant representation distillation."}, {"title": "5 RELATED WORK", "content": "Tokenization Techniques in Speech. Tokenization in speech processing can be broadly categorized into two main approaches: (i) speech encoder-based and (ii) language-based. In the speech encoder- based tokenization approach, a pre-trained speech encoder provides audio representations. These representations are then used to guide the training model, either through an alignment network (Messica & Adi, 2024) or by optimizing specific losses (Zhang et al., 2024a; Liu et al., 2024). The language-based tokenization approach involves processing audio through a speech encoder to obtain discrete representations, or using the corresponding text to feed into a language model. This has been explored by (Hassid et al., 2024), (Wang et al., 2024), (Zhang et al., 2023), and (Zhang et al., 2024b). Recently, LAST (Turetzky & Adi, 2024), explored a language model to tokenize speech toward improved sequential modeling, using the LLM to perform the next token prediction of quantized vectors. However, these approaches significantly differ from our language representations distillation method and do not focus on combining multimodal representations.\nDiscrete Speech Representation. There are two well-known methods for discrete speech represen- tation: semantic tokens and acoustic tokens. Semantic tokens are derived through self-supervised learning (SSL) techniques for speech (Baevski et al., 2019; Hsu et al., 2021; Chung et al., 2021) and capture abstract, high-level features that relate to general, symbolic aspects of speech, while omitting details related to speaker identity and acoustic characteristics. In contrast, acoustic tokens are obtained using neural audio codecs (Zeghidour et al., 2021; D\u00e9fossez et al., 2022; Yang et al., 2023) and focus on delivering precise reconstructions of acoustic features. However, recent models (Turetzky & Adi, 2024; Liu et al., 2024; Shi et al., 2024) have shown that speech models based on self-supervised learning (SSL) are effective at extracting acoustic representations where LMs be"}, {"title": "6 LIMITATIONS AND BROADER IMPACT", "content": "Limitations. In this work, we present the effectiveness of our proposed method, DM-Codec, based on the LibriSpeech dataset. Future research could investigate its performance across a variety of datasets and domains. Additionally, exploring the capabilities of DM-Codec in multilingual contexts would be valuable. Another limitation of our work is the absence of experiments with emerging LLMs. Currently, we focus solely on masked language models to derive representations. Further investigation into these decoder-based LLMs' impact on DM-Codec can be studied and addressed.\nBroader Impact. The integration of language models in speech processing has traditionally focused on model-specific implementations or specific training objectives. In this work, we propose a novel approach by leveraging a language model during the tokenization phase through our model, DM- Codec. By incorporating language-specific representations from the corresponding text, DM-Codec enhances the quality of discrete speech representations. This method bridges the gap between lan- guage and speech models, offering a more unified approach to multimodal representation learning. DM-Codec provides a robust framework for generating high-quality audio representations, with potential applications in various domains, including multilingual speech processing, low-resource languages, and other audio-related tasks. Our findings pave the way for more effective and contex- tually aware speech processing models, contributing to advancements in the broader field of speech and language technologies."}, {"title": "7 CONCLUSION", "content": "In this work, we introduced a speech tokenizer DM-Codec, with two novel distillation methods to leverage multimodal (acoustic, semantic, and contextual) representations from a language model and speech self-supervised learning model. Our extensive experimental results and ablation studies suggest that distilling multimodal representations enables DM-Codec to introduce salient speech information in discrete speech tokens. Our significance analysis further revealed that DM-Codec with comprehensive multimodal representations consistently outperforms existing speech tokeniz- ers. This approach highlights the potential of multimodal representations to enhance speech tok- enization in various domains, including multilingual and code-switched speech processing."}]}