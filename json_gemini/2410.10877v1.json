{"title": "IMPROVING DATA EFFICIENCY VIA CURATING LLM-DRIVEN RATING SYSTEMS", "authors": ["Jinlong Pang", "Jiaheng Wei", "Ankit Parag Shah", "Zhaowei Zhu", "Yaxuan Wang", "Chen Qian", "Yang Liu", "Yujia Bao", "Wei Wei"], "abstract": "Instruction tuning is critical for adapting large language models (LLMs) to downstream tasks, and recent studies have demonstrated that small amounts of human-curated data can outperform larger datasets, challenging traditional data scaling laws. While LLM-based data quality rating systems offer a cost-effective alternative to human annotation, they often suffer from inaccuracies and biases, even in powerful models like GPT-4. In this work, we introduce DS2, a Diversity-aware Score curation method for Data Selection. By systematically modeling error patterns through a score transition matrix, DS2 corrects LLM-based scores and promotes diversity in the selected data samples. Our approach shows that a curated subset (just 3.3% of the original dataset) outperforms full-scale datasets (300k samples) across various machine-alignment benchmarks, and matches or surpasses human-aligned datasets such as LIMA with the same sample size (1k samples). These findings challenge conventional data scaling assumptions, highlighting that redundant, low-quality samples can degrade performance and reaffirming that \u201cmore can be less.\"", "sections": [{"title": "Introduction", "content": "In recent years, large language models (LLMs) [Floridi and Chiriatti, 2020, Touvron et al., 2023, Achiam et al., 2023, Jiang et al., 2023, Bao et al., 2024] have shown remarkable success across various downstream tasks, from natural language understanding to generative AI applications. One critical step in advancing LLMs is aligning them with human expectations, ensuring the generated responses align with human values and preferences. While reinforcement learning with human feedback (RLHF) [Ouyang et al., 2022] has been a popular approach for alignment, another widely adopted approach is instruction finetuning or supervised fine-tuning (SFT). This method uses annotated instructional data to fine-tune pre-trained models [Touvron et al., 2023, Guo et al., 2024]. In line with general data scaling laws [Zhang et al., 2024], substantial efforts have been made to collect instructional data containing millions of examples [Wang et al., 2022, Longpre et al., 2023, Liu et al., 2024, Chung et al., 2024].\nHowever, recent studies suggest that most of the knowledge in LLM is acquired during pre-training, and a small, high-quality dataset curated through human annotations may suffice for effective alignment [Zhou et al., 2024], challenging traditional data scaling laws. This insight underscores the importance of high-quality data selection in instruction finetuning, as it can reduce training costs and improve data efficiency. Historically, data selection methods have relied"}, {"title": "Related Work", "content": "Data selection and filtering are essential for improving LLM performance in instruction tuning. Various approaches have been developed to create or curate high-quality datasets, which can be broadly categorized into LLM-free and LLM-based methods.\nLLM-free data selection Cao et al. [2023] investigate and integrate various common metrics, such as k-NN embedding distance, input length, and output length, to assess data quality. He et al. [2024] propose a Shapley-value-based metric for data selection. Xie et al. [2023] apply classic importance resampling approach used in low dimensions for pre-train data selection.\nLLM-based data selection Many recent studies leverage LLMs themselves as data selectors, filtering and identnti-fying high-quality data samples [Chen et al., 2023, Liu et al., 2023b, Lu et al., 2023, Li et al., 2023a]. For example,"}, {"title": "Understanding the Error Pattern of LLM Scores", "content": "We consider the standard prompt-based LLM rating system, where we use pre-trained LLMs to generate scores for each data sample tuple (Instruction, Input, Response). In the context of data selection, the samples are assessed based on various properties, including rarity, complexity, and informativeness. High-rated samples can then be utilized to fine-tune pre-trained models, following the established instruction tuning pipeline [Chen et al., 2023, Liu et al., 2023b]. The prompt template used in this process is detailed in Table A.2."}, {"title": "Prompt-based LLM Rating", "content": "We utilize three popular LLMs for rating: GPT-40-mini [Achiam et al., 2023], LLAMA-3.1-8B-Instruct [Dubey et al., 2024], and Mistral-7B-Instruct-v0.3 [Jiang et al., 2023]. The data pool consists of five instruct-finetuning datasets: Flan_v2 [Longpre et al., 2023], Open Assistant 1 [K\u00f6pf et al., 2024], WizardLM [Xu et al., 2023b], Dolly [Databricks, 2023], and Stanford Alpaca [Taori et al., 2023]. Detailed statistics of our data pool are provided in Table 2."}, {"title": "Data pool & Rating models", "content": "We rate the data samples on an integer scale from 0 to 5. The distributions of these rated scores across various models are summarized in Figure 1. We observe that the score distributions differ among models: GPT-40-mini has a more spread-out distribution over the median range, whereas LlaMA-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3 focus heavily on the score of 3. Moreover, these models often do not agree on their ratings. For instance, the overlap between GPT-40-mini and LlaMA-3.1-8B-Instruct is minimal, with only 229 samples (5.5% of LLaMA's) rated at 5 by both models. In contrast, Mistral-7B-Instruct-v0.3, despite having fewer samples rated at 5, shares 118 samples (24.2% of Mistral's highest score) with GPT-40-mini. This suggests that the pre-trained knowledge of LLaMA-3.1-8B-Instruct diverges significantly from that of GPT-40-mini, while Mistral-7B-Instruct-v0.3 is more aligned with GPT-40-mini. Our subsequent experimental results further support this observation."}, {"title": "Score distribution analysis", "content": "The differences in LLM-generated scores produced by various models raise a few questions: How reliable are these scores? Are there inherent errors or inaccuracies? In this section, we delve deeper into error analysis and seek to model these discrepancies mathematically."}, {"title": "Score Transition Matrix", "content": "We consider a data pool comprising N samples, denoted as $\\mathcal{D} := \\{x_n, y_n\\}_{n=1}^N$. Here, \u00e6 represents the embedding vector of the data sample (Instruction, Input, Response)\u00b3, \u1ef9 denotes the rated score generated by a LLM. We use y to represent the unobserved ground-truth score. We assume that both the ground-truth score y and the rated score \u1ef9 are in the same discretized K-class classification space Y. In our case, we have K = 6 as the scores range from 0 to 5.\nZhu et al. [2021] has demonstrated that, based on a clusterability condition, we can identify noisy labels using a score transition matrix without requiring access to ground truth labels. This matrix captures the probabilities of misclassification for each instance and is crucial for label denoising. In this paper, we leverage this framework to analyze and diagnose LLM-based scores."}, {"title": "Definition 3.1 (Score Transition Matrix)", "content": "The transition matrix T(x) is defined as a K \u00d7 K square matrix, where x is the embedding feature vector. Each entry $T_{i,j}(x)$ indicates the probability of transitioning from ground-truth score i to the observed rated score j, i.e.,"}, {"title": "3.2 Score Transition Matrix", "content": "$T_{i,j}(x) = P(\\tilde{y} = j | y = i, x), \\forall i, j \\in [K]$.\nIn this paper, we assume that the transition matrix is independent of sample-level features \u00e6, i.e., T(x) = T. Ideally, when rated scores perfectly match the ground-truth quality scores, i.e., \u0177n = yn, Vn, then the transition matrix would be equivalent to the identity matrix, i,e, T(x) = I. In this case, no error would occur. Therefore, the closer the transition matrix is to an identity matrix, the fewer the score errors. Although we do not have access to the ground-truth scores to directly compute T, we can still estimate it automatically using the LLM-generated scores under the following clusterability condition [Zhu et al., 2021]."}, {"title": "Definition 3.2 (k-NN score clusterability)", "content": "The data pool D satisfies k-NN score clusterability if, for all n, the feature xn and its k-Nearest Neighbors $X_{n1},..., x_{nk}$ belong to the same ground-truth class.\nThe k-NN clusterability characteristic is commonly observed in various tasks, especially when cross-attention layers are used for feature extraction, with each feature corresponding to a specific ground-truth class. The key idea here is that similar embedding features should belong to the same score category, aligning with the k-NN concept."}, {"title": "Deriving the score transition matrix", "content": "For a K-class classification problem, we define the ground-truth score probability distribution as p := [P(y = i), i \u2208 [K]]T, and the score transition matrix as T := T \u2022 As,\u2200s \u2208 [K], where As: [es+1, es+2,\u00b7\u00b7 , \u0435K, e1, e2,\u2026, es] is a cyclic permutation matrix, and es is the K \u00d7 1 column vector with 1 at the s-th position and 0 elsewhere. The permutation matrix As cyclically shifts each column of T to its left side by s units. We define $(i + s)_K := [(i + s \u2212 1) \\mod K] + 1$ to be the index after performing the cyclic shift within the range of K.\nNext, we introduce consensus vectors to measure the agreement between neighboring scores. Let $Y_1, Y_2, Y_3$ be the scores for three neighboring embedding features. We define:"}, {"title": "3.1 Prompt-based LLM Rating", "content": "$v^{[1]} := [P (\\tilde{y}_1 = i), i \\in [K]] = \\mathbb{T}^{-1} p$\n$v^{[2]} := [P (\\tilde{y}_1 = i, \\tilde{y}_2 = (i + 1)_K), i \\in [K]] = (\\mathbb{T} \\circ \\mathbb{T}_1)^{-1} p$\n$v^{[3]} := [P (\\tilde{y}_1 = i, \\tilde{y}_2 = (i + 1)_K), \\tilde{y}_3 = (i + s)_K), i \\in [K]] = (\\mathbb{T} \\circ \\mathbb{T}_1 \\circ \\mathbb{T}_s)^{-1} p$                                                        (1)\nwhere o denotes the Hadamard product. These consensus vectors quantify how likely neighboring embedding features share the same scores, and score transition probability information is directly encoded into this score agreement. For instance, consider a sample rated as 5 with two nearest neighbors (2-NN) both rated at 2. Then, the agreement between 2-NN scores and disagreement between high rating of 5 and low rating of 2 are controlled by certain probabilities, i.e., T and p, shown in Eq. (1). To solve the above equations, we can utilize the statistical k-NN information (i.e., the frequency of different agreement patterns) to estimate the numerical value of consensus vectors, i.e., LHS of Eq. (1). Given the available estimated values of consensus vectors, Eq. (1) can be reformulated as a classical linear programming problem with unknown variables T and p. Liu et al. [2023c], Zhu et al. [2021] further proved that solving the above problem in the third-order consensus vectors setting is sufficient to obtain the estimates for T and p. For more details, please refer to the Appendix B."}, {"title": "DS2: Diversity-aware Score curation method for Data Selection", "content": "Our data selection pipeline, DS2, consists of four key steps:\n1. Prompt-based LLM rating: In this step, we generate an initial quality score for each data sample using pre-trained LLMs (Section 3.1).\n2. Curated quality score generation: This step corrects potential rating score errors from the previous step by leveraging the Score Transition Matrix (Section 3.2) to derive a curated quality score (Section 4.1).\n3. Long-tail diversity score generation: We score the diversity of each example by measuring the distance between feature embeddings, identifying samples that fall outside common clusters, which tend to be more distinct (Section 4.2).\n4. Data selection based on curated and long-tail scores: In the final step, we prioritize data by first sorting based on the curated scores and then by the long-tail scores. This dual sorting strategy helps with removing poor-quality outliers while ensuring a diverse, high-quality dataset."}, {"title": "4 DS2: Diversity-aware Score curation method for Data Selection", "content": "The score transition matrix characterizes the transition probabilities of labeling errors; however, it operates at the dataset level. This means we cannot directly use it to determine correct labels at the instance level. Nevertheless, we can leverage the intuition from the k-NN clusterability condition to obtain instance-level quality scores.\nThe score curation process starts by evaluating and ranking samples based on the agreement of rated scores among k-NN similar samples. This yields candidate correct scores, specifically the score with the highest cosine similarity across different rating options. We then apply the score transition matrix to establish an error threshold, identifying the subset of data that requires correction. Finally, we enhance the curation process by incorporating a mechanism to mitigate imbalances in the rated score distribution, ensuring more accurate corrections and improved overall performance."}, {"title": "Curated Quality Score", "content": "We adopt the cosine similarity measure to evaluate each instance:"}, {"title": "4.1 Curated Quality Score", "content": "$SIMILARITYSCORE (v_1, v_2) = \\frac{v_1^\\top v_2}{||v_1||_2 ||v_2||_2}$\nwhere v1 and v2 represent general vectors, which could either be an embedding features an or one-hot encoding rated score vector \u0177n. To calculate the score agreement using Eq. (1), one can directly input the one-hot encoding of the original sample score \u0177n and the soft k-NN score of the n-th sample \u1ef9k-NN, which can be calculated by counting the score agreement among the k neighbor examples when the k-NN clusterability hypothesis holds."}, {"title": "k-NN agreement score", "content": "Given the k-NN agreement score, we need to determine the threshold for classifying examples as misrated and correcting them with candidate scores. This threshold can be estimated from the score transition matrix T and the ground-truth score probability distribution p using the Bayes rule:"}, {"title": "Error threshold", "content": "$THRESHOLD: N_i \\approx N_i \\times P(y \\ne i | \\tilde{y} = i) = N_i \\times (1 - \\frac{P(\\tilde{y} = i | y = i) \\cdot P(y = i)}{P(\\tilde{y} = i)})$\nwhere Ni is the sample size for i-th rated score, P(\u1ef9 = i | y = i) is the score transition probability from T and P(y = i) denote the ground-truth score probability from p. The rated score probability P(\u1ef9 = i) is estmiated by counting the frequency of the original scores.\nIntuitively, a lower cosine similarity score indicates a higher likelihood of a rating error. Therefore, the lowest-ranking Ni samples are deemed misrated and should be corrected using the candidate scores suggested by the k-NN agreement, specifically those with the highest cosine similarity among the different rating options."}, {"title": "Mitigating imbalances in LLM-based scores", "content": "The rated score distribution is often not uniform across all scores, as illustrated in Figure 1. Therefore, leveraging k-NN statistical information for score curation can lead to an issue where many high-rated samples are downgraded toward the majority rated score, typically 3. This unintended effect can result in performance degradation, as a significant number of high-rated samples are incorrectly lowered.\nTo alleviate this tendency, we introduce the confidence probability to regulate the size of the misrated samples. This is defined as $P(\\tilde{y}_n = j) := P(\\tilde{y}_n = j) \\times P_n$ where \u0177n represents the curated score of sample n, P(\u0177n = j) is the average probability of assigning sample n to the j-th score, and $P_n$ denotes the average likelihood of identifying the sample n as misrated over multiple epochs. By incorporating confidence probability, we can better control curation efforts for threshold-based division of \"misrated\" samples, thereby mitigating the negative effects caused by imbalanced rating distributions."}, {"title": "Long-tail Diversity Score", "content": "Ensuring diversity in data samples is critical, particularly when selecting a high-quality subset for instruction fine-tuning [Wang et al., 2023]. Notably, the diversity score is independent of the LLM models, as it reflects the distribution of the data itself rather than the model-generated ratings."}, {"title": "4.2 Long-tail Diversity Score", "content": "To measure this sample-level diversity, we utilize the feature embeddings of the samples. Specifically, we compute the average cosine similarity between a sample embedding and its k-Nearest Neighbors, defining this as the diversity-aware long-tail score. Intuitively, a higher long-tail score indicates greater diversity among the samples. In Figure 4, we illustrate two examples: one with a high diversity score (blue), where neighbors are far from the sample, and another with a low diversity score (red), where neighbors are clustered closely around the sample."}, {"title": "Experiments", "content": "In this paper, we select three popular and well-known open-source LLMs as our base models, including LLaMA-2-7B [Touvron et al., 2023], LLaMA-3.1-8B [Dubey et al., 2024] and Mistral-7B-v0.3 [Jiang et al., 2023]. These base models will be fine-tuned using selected data to evaluate the performance of data selection methods."}, {"title": "Experimental Setup", "content": "Several recent methods are adopted as our baselines for performance comparisons: (1) Random Selection selects examples randomly; in all experiments, we present the average result of three trials using different random seeds for data selection. (2) Completion Length uses the length of the whole conversation as a metric to estimate the data quality. Intuitively, the higher the completion length, the higher the data quality; (3) Perplexity of the responses computed with the pre-trained model in a zero-shot manner is used as the metric. We collect the perplexity scores from LLaMA-3.1-8B-Instruct model. A large perplexity score measures the difficulty or rarity of the data sample; (4) k-NN; uses the average distance to i nearest neighbors in SentenceBERT [Reimers, 2019] embedding space as the metric. Generally, a greater distance indicates that the data sample is rarer; (5) AlpaGasus [Chen et al., 2023] utilizes ChatGPT to rate data samples and solely select high-rated samples; (6) DEITA [Liu et al., 2023b] jointly uses ChatGPT to rate data samples based on complexity and quality. Considering the substantial increase in dataset size\u2014six times larger\u2014resulting from Evol Instruct [Xu et al., 2023b] and the associated costs, we take our scores as an alternative. For enhancing diversity, it iteratively selects data samples by setting a threshold to the embedding distance to filter out outliers; (7) LESS [Xia et al., 2024] rates data samples according to the influence score calculated from the gradient of the data sample and a specific validation dataset. (8) Full Data utilizes the entire data pool to finetune the pre-trained models."}, {"title": "Base models", "content": "We adopt five OpenLLM Leaderboard tasks as our benchmark for evaluation, including MMLU [Hendrycks et al., 2020], TruthfulQA [Lin et al., 2021], GSM [Cobbe et al., 2021], BBH [Suzgun et al., 2022], TydiQA [Clark et al., 2020]. For MMLU, TruthfulQA, GSM, and BBH datasets, we use Exact Match (EM) as the criteria. For TydiQA dataset, we consider using the 1-shot F1 score."}, {"title": "Baselines", "content": "Table 3 demonstrates the performance of DS2 as well as nine baselines. In particular, we further compare two score-aware baselines (AlpaGasus and DEITA) across different rating models. As shown in Table 3, DS2 consistently obtains the best performance compared to all baselines. Remarkably, under different rating model settings, DS2 (with only 10k selected samples) still achieves significantly better performance than using the full data pool (300k), up to 96.7% data reduction. More experimental results on various base models are provided in the Appendix (Tables 8 and 9)."}, {"title": "OpenLLM Leaderboard Evaluation Results", "content": "Intuitively, without score curation, we observe in Tables 3 that different rating models can affect overall performance for all score-aware methods including ours. The experimental results match their detected score errors. For instance, as shown in Figure 2, the LLaMA-3.1-8B-Instruct model has more score errors than the other two models, resulting in a performance drop. Notably, when applying score curation for LLaMA and Mistral, their average performances (60.2 for LLaMA and 61.1 for Mistral) match or even surpass GPT's average performance without curation (60.2). This shows that once combined with score curation, the scores generated by weaker rating models can be a cost-effective alternative to commercial LLMs such as GPT-40."}, {"title": "3% of the data outperforms the full data pool", "content": "Table 3 also highlights the performance gap of DS2 with and without score curation. It is evident that score curation can consistently improve the average performance of DS2 across different rating models, even for the GPT-40-mini (60.2 \u2192 61.4). Additional results on various base models, provided in the Appendix (Table 12), consistently support this claim."}, {"title": "Weaker models rating w. score curation \u2248 GPT-40's rating", "content": "To assess the overall quality of the dataset generated by DS2, we finetune two base models using human-annotated dataset LIMA (1k samples) [Zhou et al., 2024]. To match this data size, we generate a 1k-sample dataset using DS2. We then compare the performance of models fine-tuned on 1k version selected datasets with those models fine-tuned on LIMA. In particular, Table 4 demonstrates the openLLM leaderboard performance for LIMA and ours across various rating models."}, {"title": "Score curation works for all rating models", "content": "In this section, we challenge traditional data scaling laws in instruction tuning by introducing DS2, a novel data selection pipeline that curates LLM-rated quality scores to improve data efficiency. Through the systematic exploration of error patterns in LLM-rated data quality scores, we developed a score curation mechanism to correct inaccuracies and enhance the effectiveness of selected data. Empirically, DS2\u2013 using only 3.3% of the original data \u2013 outperforms training on the full dataset (300k samples) and even exceeds the performance of the human-aligned dataset \u201cLIMA\u201d with the same sample size (1k samples). This demonstrates that smaller, high-quality datasets can achieve superior results by avoiding performance drops caused by low-rated or redundant data, revising the traditional scaling laws that suggest more data is always better. By curating LLM-driven rating scores, DS2 not only improves data efficiency, but also offers a cost-effective alternative to large-scale datasets and human annotations. Our results highlight the importance of data quality over quantity in instruction tuning and show how score curation can mitigate LLM biases, leading to improved model alignment and downstream performance. In conclusion, this work underscores the need to rethink data scaling laws in light of more efficient, curated data selection methods."}, {"title": "Human Alignment v.s. Machine Alignment", "content": "In this section, we also utilize the original LIMA test set (300 samples) to compare the performance between LIMA (human annotation) and DS2 (machine annotations). Similarly, we finetune two base models (LLaMA-3.1-8B and Mistral-7B-v0.3) on 1k LIMA samples. The finetuned models are then directly compared with finetuned models using DS2 selected examples at both 1k and 10k sample sizes. The experimental results for 1k and 10k settings are shown in Figure 12 and 13, respectively. While DS2 performs worse than LIMA in the 1k sample setting, it totally surpasses LIMA in the 10k setting, consistently demonstrating the superiority of DS2. This lower performance at the 1k setting is expected, as LIMA has a natural advantage in a limited sample size scenario due to the IID nature of its training and test sets."}, {"title": "Conclusion", "content": "Our data selection pipeline, DS2, consists of four key steps:\n1. Prompt-based LLM rating: In this step, we generate an initial quality score for each data sample using pre-trained LLMs (Section 3.1).\n2. Curated quality score generation: This step corrects potential rating score errors from the previous step by leveraging the Score Transition Matrix (Section 3.2) to derive a curated quality score (Section 4.1).\n3. Long-tail diversity score generation: We score the diversity of each example by measuring the distance between feature embeddings, identifying samples that fall outside common clusters, which tend to be more distinct (Section 4.2).\n4. Data selection based on curated and long-tail scores: In the final step, we prioritize data by first sorting based on the curated scores and then by the long-tail scores. This dual sorting strategy helps with removing poor-quality outliers while ensuring a diverse, high-quality dataset."}, {"title": "DS2 can be an alternative to LIMA", "content": "In this section, we are also interested in the performance of concatenating samples from three rating models. We combined all high-rated samples with a score of 5, resulting in a subset of 8K samples. To reach a"}, {"title": "Exploring the Impact of Concatenating High-Rated Examples Across Rating Models", "content": "In this section, we challenge traditional data scaling laws in instruction tuning by introducing DS2, a novel data selection pipeline that curates LLM-rated quality scores to improve data efficiency. Through the systematic exploration of error patterns in LLM-rated data quality scores, we developed a score curation mechanism to correct inaccuracies and enhance the effectiveness of selected data. Empirically, DS2 \u2013 using only 3.3% of the original data \u2013 outperforms training on the full dataset (300k samples) and even exceeds the performance of the human-aligned dataset \u201cLIMA\u201d with the same sample size (1k samples). This demonstrates that smaller, high-quality datasets can achieve superior results by avoiding performance drops caused by low-rated or redundant data, revising the traditional scaling laws that suggest more data is always better. By curating LLM-driven rating scores, DS2 not only improves data efficiency, but also offers a cost-effective alternative to large-scale datasets and human annotations. Our results highlight the importance of data quality over quantity in instruction tuning and show how score curation can mitigate LLM biases, leading to improved model alignment and downstream performance. In conclusion, this work underscores the need to rethink data scaling laws in light of more efficient, curated data selection methods."}]}