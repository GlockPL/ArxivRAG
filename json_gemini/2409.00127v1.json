{"title": "LATENT-ENSF: A LATENT ENSEMBLE SCORE FILTER FOR HIGH-DIMENSIONAL DATA ASSIMILATION WITH SPARSE OBSERVATION DATA", "authors": ["Phillip Si", "Peng Chen"], "abstract": "Accurate modeling and prediction of complex physical systems often rely on data assimilation techniques to correct errors inherent in model simulations. Traditional methods like the Ensemble Kalman Filter (EnKF) and its variants as well as the recently developed Ensemble Score Filters (EnSF) face significant challenges when dealing with high-dimensional and nonlinear Bayesian filtering problems with sparse observations, which are ubiquitous in real-world applications. In this paper, we propose a novel data assimilation method, Latent-EnSF, which leverages EnSF with efficient and consistent latent representations of the full states and sparse observations to address the joint challenges of high dimensionlity in states and high sparsity in observations for nonlinear Bayesian filtering. We introduce a coupled Variational Autoencoder (VAE) with two encoders to encode the full states and sparse observations in a consistent way guaranteed by a latent distribution matching and regularization as well as a consistent state reconstruction. With comparison to several methods, we demonstrate the higher accuracy, faster convergence, and higher efficiency of Latent-EnSF for two challenging applications with complex models in shallow water wave propagation and medium-range weather forecasting, for highly sparse observations in both space and time.", "sections": [{"title": "1 INTRODUCTION", "content": "Many complex physical systems are traditionally modeled by partial differential equations (PDEs). First-principle PDE-based modeling and simulation have been demonstrated to be powerful in making predictions of complex systems in various scientific and engineering fields. However, in practical applications, significant discrepancy of the simulation-based prediction and the reality may arise from different sources, e.g., model inadequacy, uncertainties in model parameters, boundary and/or initial conditions, exteral forcing/loading terms, numerical approximation errors, etc. Meanwhile, data-driven machine learning (ML) models have been significantly developed and employed to make system predictions in the last few years, which have been shown to be extremely efficient in the case of large dynamical systems such as weather forecasting, a problem that requires the supercomputer of hours to run for the European Centre for Medium-Range Weather Forecasts (ECMWF), but one that can be solved by neural network surrogates such as FourCastNet (Pathak et al., 2022) in a matter of seconds. These properties also make them particularly suitable for ensemble forecasting and uncertainty quantification. However, these models are also often subject to the key limitation of divergence from reality in the long run, resulting in significant accumulated errors for long-term predictions because of various uncertainties and the autoregressive structure of the ML models.\nTo mitigate the discrepancy and accumulated errors and make more accurate predictions, data assimilation plays an essential role in incorporating additional observation data of the reality into the current PDE/ML-based prediction models. By adapting the PDE/ML state to match the observation data, the accuracy of future state predictions can be markedly improved. Data assimilation (Sanz-Alonso et al., 2023; Asch et al., 2016) approaches such as Kalman filter (KF) (Kalman, 1960a), particle filters (K\u00fcnsch, 2013), and their variants have been widely applied to problems such as weather prediction, geophysical modeling, robotics, and many more areas."}, {"title": "2 DYNAMICAL SYSTEMS AND DATA ASSIMILATION", "content": "To model a dynamical physical system, we denote the state at a certain time t as $x_t \\in \\mathbb{R}^d$, where d is typically very high for complex models. For a system with discrete time steps $t = 1, 2, ...,$ given initial condition $x_0$, we then model the evolution of the state from the time t 1 to time t as\n$x_t = M(x_{t-1},\\varepsilon_t),$\nwhere $\\varepsilon_t$ is a process noise term coming from a known distribution; this term accounts for hidden interactions and numerical errors. The corresponding observation $y_t \\in \\mathbb{R}^m$ at time t is given by\n$y_t = H(x_t) + \\gamma_t,$\nwhere $\\gamma_t$ (also coming from a known distribution, typically Gaussian) represents the observation noise because of instrumental inaccuracies, etc. Both the model map $M : \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ and the observation map $H : \\mathbb{R}^d \\rightarrow \\mathbb{R}^m$ can be nonlinear.\nNow, the model of the dynamical system in Equation 1 may lead to inaccurate representation of the ground truth with increasing discrepancy in time because of, e.g., model inadequacy or input uncertainty. The goal of data assimilation is to assimilate the observation data of the true state to the model of the dynamical system and recover an accurate representation of the true state. Note that in practical applications the observation data can be sparsely distributed in both space and time, which makes the recovery of the true state very challenging, especially for high dimensional problems with the state dimension $d\\gg 1$, and the observation dimension m much smaller than d."}, {"title": "2.1 BAYESIAN FILTERING", "content": "From a Bayesian filtering perspective (Dore et al., 2009), a data assimilation problem at each time t can be divided into two steps: a prediction step that advances the dynamical system and an update step that assimilates the data. Assuming that the posterior density of the state $x_{t-1}$ given observation data $Y_{1:t-1} = (y_1,..., y_{t-1})$, denoted as $P(x_{t-1}|Y_{1:t-1})$, is available at time $t - 1$, with $P(x_0|Y_{1:0}) = P(x_0)$ given for time t = 0, then the prediction step provides the density of $x_t$ as\nPrediction: $P(x_t|Y_{1:t-1}) = \\int P(x_t|x_{t-1})P(x_{t-1}|Y_{1:t-1})dx_{t-1},$\nwhere $P(x_t|x_{t-1})$ represents the transition probability governed by the dynamical system in Equation 1. Note that $x_t$ depends on $Y_{1:t-1}$ only through $x_{t-1}$. Let $P(y_t|x_t)$ denote the likelihood function of the data $y_t$ given state $x_t$ from Equation 2. Then, the update step provides the updated posterior density $P(x_t|y_{1:t})$ of the state $x_t$ given the new observation data $y_t$ by Bayes' rule as\nUpdate: $P(x_t|Y_{1:t}) = \\frac{P(y_t|x_t)P(x_t|Y_{1:t-1})}{P(y_t|Y_{1:t-1})},$\nwhere the normalization constant or model evidence term $P(y_t|Y_{1:t-1})$ is given by $P(y_t|Y_{1:t-1}) = \\int P(y_t|x_t)P(x_t|Y_{1:t-1})dx_t$, which is typically intractable to compute."}, {"title": "2.2 DIFFUSION MODELS AND THE ENSEMBLE SCORE FILTER", "content": "A recent development for a new particle filter is the EnSF, where the ensemble of samples can be drawn from the posterior distribution through a diffusion process. It builds on the recent advancement in score-based generative modeling (Ho et al., 2020) in generating high-fidelity samples from a distribution by first computing a score function $\\nabla_x\\log P(x)$ and then sampling with it by solving stochastic differential equations (SDE) (Song et al., 2021). This circumvents knowing the true density as it only needs to generate samples from the posterior distribution. Notably, the diffusion model takes the form of a noisy forward stochastic differential equation process\n$dx = f(x,t)d\\tau + g(\\tau)dw,$\nwhich transforms data from an arbitrary distribution to an isotropic Gaussian for $\\tau \\in T = [0,T]$. Here, f(x, t) is a drift term, g(t) is a diffusion term, and w is a d-dimensional Wiener process. It has the corresponding reverse-time SDE that runs backward in time from $T = T$ to 0 as\n$dx = [f(x,\\tau) - g^2(\\tau)\\nabla_x\\log P_\\tau(x)]d\\tau + g(\\tau)dw,$\nwhere w is another Wiener process independent of w. This reverse-time SDE can be solved to sample from the distribution P(x) if the score function $\\nabla_x \\log P_\\tau(x_\\tau)$ is known, e.g., by the Euler-Maruyama scheme at the discrete time steps $0 = T_0 < T_2 < ... < T_k = T$.\nIn the following part, we use $x_{t,\\tau}$ to indicate the state at physical time t and diffusion time $\\tau$. The EnSF method (Bao et al., 2024b) uses T = 1 and the state-independent drift and diffusion terms\n$f(x_{t,\\tau},\\tau) = \\frac{d\\log \\alpha_\\tau}{d\\tau}x_{t,\\tau} \\text{ and } g^2(\\tau) = \\beta_\\tau^2 - 2 \\frac{d\\log \\alpha_\\tau}{d\\tau},\\$\nwith $\\alpha_\\tau = 1 - \\tau(1 - \\epsilon_\\alpha)$ and $\\beta_\\tau^2 = \\epsilon_\\beta + \\tau(1 - \\epsilon_\\beta)$ for two small positive hyperparameters $\\epsilon_\\alpha$ and $\\epsilon_\\beta$ which aid in avoiding the collapse of the generated samples. This choice leads to the distribution $x_{t,\\tau} \\sim N(\\alpha_\\tau x_{t,0}, \\beta_\\tau^2I)$ conditioned on $x_{t,0} = x_t$, which results in the prior score function\n$\\nabla_x\\log P(x_{t,\\tau}, Y_{1:t-1}) = \\int \\nabla_x\\log P(x_{t,\\tau}|x_t)P(x_t|Y_{1:t-1})dx_t$\n$= \\frac{\\beta_\\tau}{\\alpha_\\tau} \\int \\frac{x_{t,\\tau} - \\alpha_\\tau x_t}{\\beta_\\tau^2} \\omega(x_{t,\\tau}, x_t)P(x_t|Y_{1:t-1})dx_t,$\nwhere the gradient $\\nabla_x$ is taken with respect to $x_{t,\\tau}$, and the weight $\\omega(x_{t,\\tau}, x_t)$ is given by\n$\\omega(x_{t,\\tau}, x_t) = \\frac{P(x_{t,\\tau}|x_t)}{\\int P(x_{t,\\tau}|x_t)P(x_t|Y_{1:t-1})dx_t}.$\nBoth the score function in Equation 8 and the weight in Equation 9 can be evaluated by sample average approximation with random samples from the distribution $P(x_t|Y_{1:t-1})$ obtained in the prediction step in Equation 3. In the update step in Equation 4, the posterior score function in the EnSF method (Bao et al., 2024b) is formulated as\n$\\nabla_x \\log P(x_t, Y_{1:t}) = \\nabla_x \\log P(x_{t,\\tau}|Y_{1:t-1}) + h(\\tau)\\nabla_x \\log P(y_t|x_{t,\\tau}),$\nwith the prior score function in the first term given in Equation 8, the likelihood function $P(y_t|x_{t,\\tau})$ in the second term given explicitly from the observation map in Equation 2, and a damping function h(\\tau) that is monotonically decreasing with h(0) = 1 and h(1) = 0, e.g., h(\\tau) = 1-\\tau. Note that the posterior score function is consistent with the Bayes' rule for the posterior in Equation 4 at $\\tau = 0$.\nTo this end, a brief description of one step of the EnSF algorithm is presented in Algorithm 1."}, {"title": "2.3 SPARSE OBSERVATIONS", "content": "A key limitation of EnSF in high-dimensional nonlinear filtering problems occurs when the observation data are sparse, which is the case for most practical applications. For example, suppose that the observation map $H(x_t) = x_t[S]$, which only make observations of $x_t$ in the dimensions from the subset $S \\subset \\{1, ..., d\\}$, where S may have a much smaller cardinality $|S| < d$ than the full dimension. In this case, the gradient of the log-likelihood $\\nabla_x \\log P(y_t|x_t)$ vanishes in the dimensions in $\\{1, ..., d\\}/S$, as shown in Figure 2, where we calculate $\\nabla_xP(y_t|x_t)$ for one of our experiments in Section 4.1 with 1/16 of the total dimensions used for the observations. Note that, the gradient is nonzero only at the points which are observed as seen in Figure 2. Though the ramifications are minimal for sufficiently dense observations, the vanishing gradients for higher sparsity scenarios significantly limits the effectiveness of the standard EnSF, which we demonstrate empirically in section 4.1.1.\nTo mitigate this weakness of the EnSF, rather than conducting data assimilation in the full state space with sparse observations, we propose to transform the data assimilation into a learned latent space with sufficiently preserved information and significantly reduced dimension."}, {"title": "3 LATENT ENSEMBLE SCORE FILTER", "content": "To address the key limitation of EnSF in the case of sparse observations due to the vanishing gradient of the log-likelihood function, we propose a latent representation of the sparse observations by a variational autoencoder (VAE) and match this latent representation to the encoded full state for the sake of consistent data assimilation in the latent space, for which we employ EnSF with the additional advantage of VAE regularization of the latent variables.\nThe advantages are twofold: it allows us to not only make use of the benefits of standard latent score-based generative models (Vahdat et al., 2021; Rombach et al., 2022) which offer optimized sampling speed and expressivity, but also create a expressive map from a sparse observation space to a full-dimensional latent space. Additionally, the successful application of latent-diffusion models to videos (Blattmann et al., 2023) offers a potential application of exploring latent-assimilation coupled with latent dynamics."}, {"title": "3.1 COMPRESSION BY VARIATIONAL AUTOENCODER", "content": "The Variational Autoencoder (VAE) (Kingma & Welling, 2014) provides a compressed representation of a high-dimensional distribution by means of bottlenecking. Note that since this is a family of models, it admits many specific instances such as VQ-VAE (Van Den Oord et al., 2017), InfoVAE (Zhao et al., 2019), etc., which can promote desirable properties such as independence and adaptable priors in the latent space. VAEs have two components, an encoder $\\mathcal{E}$ and a decoder $\\mathcal{D}$. The encoder casts the original state $x \\in \\mathbb{R}^d$ into a low dimensional representation $\\mathcal{E}(x) \\in \\mathbb{R}^{2r}$ such that $2r < d$. Then, the model splits $\\mathcal{E}(x)$ into mean $\\mu\\in \\mathbb{R}^r$ and variance $\\sigma^2 \\in \\mathbb{R}$, and samples from the normal distribution $\\mathcal{N}(\\mu, \\Sigma)$ with $\\Sigma = diag(\\sigma_1^2, ..., \\sigma_0^2)$ by applying a reparameterization trick $z = \\mu + \\sigma \\cdot \\epsilon$, where $\\epsilon \\in \\mathcal{N}(0, I_r)$. The decoder then constructs an approximation of the state as $\\hat{x} = \\mathcal{D}(z)$."}, {"title": "3.2 COUPLED VAES WITH LATENT-SPACE MATCHING", "content": "We use VAEs to compress both the sparse observations and the full states to the latent space. To achieve consistent latent representations of sparse observations and full states, we couple the VAEs in the latent space as shown in Figure 3.\nSpecifically, we formulate a coupled VAE with two encoders, one encoder $\\mathcal{E} : \\mathbb{R}^d \\rightarrow \\mathbb{R}^{2r}$ that encodes the full state $x_t$ to a latent variable $z_t$ with distribution $\\mathcal{N}(\\mu_t, \\Sigma_t)$, where $(\\mu_t, \\sigma_t^2) = \\mathcal{E}(x_t)$, and $z_t = \\mu_t + \\sigma_t \\cdot \\epsilon_t$, and the other encoder $\\mathcal{E}_{obs} : \\mathbb{R}^m \\rightarrow \\mathbb{R}^{2r}$ that encodes the sparse observation $y_t$ to another latent variable $z_{obs}$ with distribution $\\mathcal{N}(\\mu_{obs}, \\Sigma_{obs})$, where $(\\mu_{obs}, \\sigma_{obs}^2) = \\mathcal{E}_{obs}(y_t)$ and $z_{obs} = \\mu_{obs} + \\sigma_{obs} \\cdot \\epsilon_t$. To assimilate the encoded latent data to the encoded latent state, we seek to match the two latent representations for their means and variances. To build a consistent reconstruction of the full state from the latent representation, we use the same decoder $\\mathcal{D} : \\mathbb{R}^r \\rightarrow \\mathbb{R}^d$. To this end, we formulate the following loss function to account for all three contributions:\n$\\ell_t(\\theta) = ||x_t - \\mathcal{D}(z_t)||^2 + ||x_t - \\mathcal{D}(z_{obs})||^2$ (Reconstruction Term)\n$+ KLD(\\mu_t, \\Sigma_t) + KLD(\\mu_{obs}, \\Sigma_{obs})$ (Regularization Term)\n$+ ||\\mu_t - \\mu_{obs}||^2 + ||\\Sigma_t - \\Sigma_{obs}||^2$ (Latent Matching Term),\nwhere $\\theta$ represents the parameters of the encoders and decoder, for which we use convolutional neural networks (CNNs) in this work. Then for the training of the parameter $\\theta$ in the coupled VAE, we minimize an empirical loss function as a sum of the above loss function over the training trajectories of the dynamical system, and over all the time steps t in each trajectory.\nNote that both of the latent representations use the same decoder for reconstruction of the full state. By training a coupled VAE which minimizes the error for the reconstruction of the true state conditioned on the observations, we can strive to obtain a latent representation of the sparse observations consistent to that of the full states. This approach is similar to the Generalized Latent Assimilation (GLA) representation proposed by Cheng et al. (2023), but with the addition of matching the means and variances of the state and observation encoders, as illustrated in Figure 3. Experimentally, training the encoders without matching the latent representations can lead to a solution where the latent observations are segregated from the latent states, yet result in sufficient reconstructions. Using these two separate encoders, we can match the latent representations such that we are able to apply EnSF in a full-dimensional manner, even with extremely sparse observations.\nSome early papers have utilized autoencoder architectures to conduct data assimilation in various ways. GLA proposed in Cheng et al. (2023) uses the EnKF to assimilate observations in the latent space of the VAE. However, for latent spaces with high dimensions, EnKFs are computationally expensive and also subject to the curse of dimensionality. Meanwhile, overly constricting the latent"}, {"title": "3.3 LATENT ENSEMBLE SCORE FILTER", "content": "For simplicity, let the latent state $\\xi_t$ and latent data $\\zeta_t$ be defined as the encoded state and data as\n$\\xi_t = (\\mu_t, \\sigma_t^2) = \\mathcal{E}(x_t)$ and $\\zeta_t = (\\mu_{obs}, \\sigma_{obs}^2) = \\mathcal{E}_{obs}(y_t),$\nrespectively. We conduct data assimilation in the latent space by assimilating the latent data $\\zeta_t$ to the latent state $\\xi_t$. Mathematically, the latent state $\\xi_t$ follows approximately (up to the VAE reconstruction error) the latent dynamical system\n$\\xi_t \\approx \\mathcal{E}(M(\\mathcal{D}(z_{t-1}(\\xi_{t-1})), \\epsilon_t))$\nas a result of the full dynamical system in Equation 1 and the VAE with the reparametrization $z_{t-1}(\\xi_{t-1}) = \\mu_{t-1} + \\sigma_{t-1} \\cdot \\epsilon_{t-1}$ at time t 1. The latent data $\\zeta_t$ approximately satisfies\n$\\zeta_t = \\mathcal{E}_{obs}(H(\\mathcal{D}(z_t(\\xi_t)) + \\epsilon_t)$\nas a result of the observation map in Equation 2 and the VAE with the reparametrization $z_t(\\xi_t) = \\mu_t + \\sigma_t \\cdot \\epsilon_t$ at time t. When calculating the likelihood of the observation in the latent space, we can simply Equation 17 and use an additive latent observation noise $\\hat{\\gamma}_t$ estimated from the full space observation noise $\\gamma_t$ through the observation encoder. As the coupled encoders seek to match the full states and the sparse observations in their latent representations, we can further approximate the latent observation map in Equation 17 as an identity map, i.e., $\\zeta_t \\approx \\xi_t + \\hat{\\gamma}_t$, which simplifies and accelerates the computation of the gradient of the log-likelihood function by avoiding automatic differentiation through the map in Equation 17 with respect to the latent state.\nWith the latent state following the latent dynamics, and the approximate latent observations, we consider the Bayesian filtering problem in the latent space by formulating the prediction step as\nPrediction: $P(\\xi_t|\\zeta_{1:t-1}) = \\int P(\\xi_t|\\xi_{t-1})P(\\xi_{t-1}|\\zeta_{1:t-1})d\\xi_{t-1},$\nwith the transition probability $P(\\xi_t|\\xi_{t-1})$ detemined by the latent dynamical system in Equation 16. The update step for the posterior of the latent state $\\xi_t$ given the latent data $\\zeta_t$ is defined as\nUpdate: $P(\\xi_t|\\zeta_{1:t}) = \\frac{P(\\zeta_t|\\xi_t)P(\\xi_t|\\zeta_{1:t-1})}{P(\\zeta_t|\\zeta_{1:t-1})},$\nwhere $P(\\zeta_t|\\xi_t)$ is the likelihood of the latent data given latent state $\\xi_t$, which can be computed by the latent observation map in Equation 17. The latent normalization term $P(\\zeta_t|\\zeta_{1:t-1}) = \\int P(\\zeta_t|\\xi_t)P(\\xi_t|\\zeta_{1:t-1})d\\xi_t$ is also generally intractable.\nWe employ the diffusion-based ensemble score filter in Section 2.2 for the above Bayesian filtering problem in the latent space by replacing the state $x_t$ with the latent state $\\xi_t$, and the observation $y_t$ with the latent observation $\\zeta_t$. Note that the latent state $\\xi_t$ and the latent observation $\\zeta_t$ have the same dimension $2r < d$. We present one step of the Latent-EnSF in Algorithm 2.\nBecause the sparse observations have been mapped to the same dimension as the state in the latent space, we are able to circumvent the weakness of the EnSF with vanishing gradient of the log-likelihood function when dealing with the sparse observation problems. In addition, since the encoder is a flexible neural-network representation, the model is able to restrict the latent observation map $H_{latent}$ in the latent space to be the identity function $H_{latent}(\\xi_t) = \\xi_t$, allowing for analytical solutions for the score function without the need for automatic differentiation. This means that the model can leverage the full computational speed-up of the EnSF in the latent space. Finally, the regularization done by the VAE such that the latent vector tends to follow a $\\mathcal{N}(0,I)$ distribution simplifies hyperparameter search of the appropriate noise $\\gamma_t$ for real world problems."}, {"title": "3.4 ADDRESSING ENSF'S NUMERICAL INSTABILITY ON SMALL SCALES", "content": "In practice, when applying the EnSF to problems where the values of the states and their variances are very small, specifying the corresponding observation noise such that $\\gamma_t < 0.05$ can result in numerical instability issues when sampling the posterior distribution by reverse time SDE with the state-independent drift and diffusion terms given in Equation 7.\nThis numerical instability issue is also present in the VAE latent space where the components are regularized such that $\\mu_t \\approx 0$ and $\\sigma_t \\approx 1$. We address this issue by multiplying the latent representations with a scalar constant $\\Psi_{latent}$, conducting the EnSF for the rescaled latent representation, and subsequently scaling the posterior samples back to the original latent representation. We conducted a small grid search experiment when working with the shallow water wave propogation problem in Section 4.1, where $\\Psi_{latent}$ = 500 seemed to work reasonably.\nRemarkably, this $\\Psi_{latent}$ applies well in all our experiments, which implies that this scaling hyperparameter stays fairly steady across all problems. Intuitively, this is consistent; the latent regularization conducted by the VAE constrains the latent space to be in a similar range. In contrast, when EnSF is conducted on the full state space, such advantages are lost and a different scaling parameter $\\xi$ is needed for different problems with different scales of the state and observation noise."}, {"title": "4 EXPERIMENTS", "content": "In this section, we illustrate the limitation of using the EnSF for sparse observations, and demonstrate the improved accuracy and fast convergence of the Latent-EnSF compared to several data assimilation methods for a synthetic complex physical system modeled by shallow water equations with sparse observations in both space and time. We also demonstrate the merits of Latent-EnSF for medium-range weather forecasting with real-world data and a ML-based dynamical system."}, {"title": "4.1 SHALLOW WATER WAVE PROPAGATION", "content": "We consider the propagation of shallow water waves described by shallow-water equations (Vreugdenhil, 1994), which are a system of hyperbolic PDEs that describe fluid flow of free surface with the conservation of momentum and mass, with three state variables including the water height and two components of the water velocity field. Such models find practical applications in hydrology for predicting flood waves and in oceanography for modeling tsunami wave propagation.\nIn the experiment, we consider water wave propagation with the initial water displacement modeled by a local Gaussian bump perturbation from the flat surface in a square domain of size L \u00d7 L with L = $10^6$ m in each direction and a constant depth h = 100 m for the topography of the floor. For the simulation, we use a finite difference code adapted from jostbr (2019), with the domain discretized into a uniform grid of 150 \u00d7 150. The simulation is then run for 2000 time steps using a upwind scheme with a time step $\\Delta t = 0.1s$ with $\\Delta x = L/150$ to satisfy the Courant-Friedrichs-Lewy condition (Courant et al., 1967), see Figure 4 for the evolution of the height and its sparse observations with 4x (75 \u00d7 75) and 225x (10 \u00d7 10) data reduction."}, {"title": "4.1.1 ENSF AND SPARSITY", "content": "In Section 2.3, we illustrated the vanishing gradient of the log-likelihood function for the dimensions in which observations are not available. Here we empirically demonstrate this effect on EnSF for data assimilation with sparse observations at different levels of sparsity."}, {"title": "4.1.2 LATENT-ENSF RESULTS", "content": "We train the coupled VAE as shown in Figure 3 using convolutional layers with input dimension 3 \u00d7 150 \u00d7 150 and latent dimension 4 \u00d7 10 \u00d7 10 = 400 with 4 channels for the experiments other than that in Figure 6, which is for the evaluation of the performance with respect to the latent dimension size. The following experiments use a 10 \u00d7 10 observation grid, corresponding to the 225x sparse entry in Figure 5. We compare the Latent-EnSF and the Latent-EnKF, which shares the same VAE weights as the Latent-EnSF. For the approximate $\\gamma_t$ observation noise terms in the latent space, we adopt a heuristic by taking the mean standard deviation values given by encoded latent states for all three approaches. In Figure 6, we can see that the Latent-EnKF struggles with higher dimensional latent spaces, whereas the Latent-EnSF gets more accurate as a result of a decrease in reconstruction error for the VAE, even though they perform similarly for the 100-dimensional case.\nIn addition to the Latent-EnKF, we test against the Latent-LETKF in high-dimensional latent space, as shown in the right part of Figure 6. This utilizes the LETKF which in practice handles high-dimensional data assimilation much better than the EnKF. However, we can see that though it indeed beats the Latent-EnKF benchmark, especially with higher latent dimensions, our approach is still much better in terms of assimilation speed, accuracy, and efficiency (see Table 1) for all cases."}, {"title": "4.2 MEDIUM-RANGE WEATHER FORECASTING", "content": "Sparse data assimilation is particularly important for accurate medium-range weather forecasting due to the complexity of the problem which can skew results of machine learning models. The commonly used dataset for conducting these weather forecasting experiments is the ERA5 (Hersbach et al., 2020), coming from the European Centre for Medium-Range Weather Forecasts (EMCWF). The main variables at play in this dataset include the wind speeds, temperature, geopotential, and humidity, just to name a few, all of which are available at any of the 137 different pressure levels. This dataset comes with 40 years of reanalyzed data, and can support various degrees of fidelity. Currently, ML-based prediction approaches such as FourCastNet (Pathak et al., 2022), Pangu Weather (Bi et al., 2023), and GraphCast (Lam et al., 2023) have shown to be extremely efficient when applied to weather forecasting compared to standard PDE-based approaches. We adopt the 21 variables used in the FourCastNet paper which include wind velocities and geopotential at 3 different pressure levels, along with relative humidity and temperature at a few key pressure levels and the total column water vapor at the surface. For forecasting, we train a FourCastNet model on our coarse subset of the dataset. When training the VAE for data assimilation, we create a ResNet-style architecture similar to what we constructed in the shallow-water experiments. The encoder downsamples by a factor of 2 twice between convolution layers, resulting in a latent dimension size of 32 x 36 x 18 = 20736. In between the layers we use Leaky-ReLU layers with a coefficient of 0.2. We use this as a demonstrative task to show the feasibility of applying Latent-EnSF to complex systems, so we train and test on 10 years of data with 2.5\u00b0 resolution, with a grid of size 144 x 72, as a proof of concept. Evaluations are done for the year 2015, where we average the metrics across nine 41-day windows. Data assimilation is done once a day, incorporating observations at a 64x sparsity rate, with 8x reduction in each dimension, or a grid of 18 \u00d7 9 observations.\nAs shown in Figure 9, the forecasting by the FourCastNet-based ML model without data assimilation becomes increasingly inaccurate because of the accumulated errors, which limits its relatively reliable prediction for the first 14 days. The data assimilation errors by EnSF follow closely those"}, {"title": "5 CONCLUSION", "content": "In this paper, we introduced Latent-EnSF, a novel data assimilation method in addressing the joint challenges of high dimensionality in the state and high sparsity in the observation for nonlinear"}]}