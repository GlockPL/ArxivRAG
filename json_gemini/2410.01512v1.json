{"title": "InstaTrans: An Instruction-Aware Translation Framework for Non-English Instruction Datasets", "authors": ["Yungi Kim", "Chanjun Park"], "abstract": "It is challenging to generate high-quality instruction datasets for non-English languages due to tail phenomena, which limit performance on less frequently observed data. To mitigate this issue, we propose translating existing high-quality English instruction datasets as a solution, emphasizing the need for complete and instruction-aware translations to maintain the inherent attributes of these datasets. We claim that fine-tuning LLMs with datasets translated in this way can improve their performance in the target language. To this end, we introduces a new translation framework tailored for instruction datasets, named INSTATRANS (INSTruction-Aware TRANSlation). Through extensive experiments, we demonstrate the superiority of INSTATRANS over other competitors in terms of completeness and instruction-awareness of translation, highlighting its potential to broaden the accessibility of LLMs across diverse languages at a relatively low cost. Furthermore, we have validated that fine-tuning LLMs with datasets translated by INSTATRANS can effectively improve their performance in the target language.", "sections": [{"title": "Introduction", "content": "Recently, instruction tuning has emerged as the predominant and effective methodology for fine-tuning large language models (LLMs) to align with user intentions (Peng et al., 2023; Touvron et al., 2023a; Kim et al., 2023; Wu et al., 2023). The efficacy of instruction tuning largely depends on the quality of the instruction dataset, which is composed of instruction and response pairs (Longpre et al., 2023; Zhou et al., 2023; Luo et al., 2023). However, scaling these datasets poses significant challenges, as it typically necessitates the involvement of human experts to generate high-quality instruction datasets (Wang et al., 2022b; Therefore, translating high-quality English instruction datasets that are publicly available might be a promising direction for the development of non-English high-quality instruction datasets (Muennighoff et al., 2022; Lai et al., 2023; Li et al., 2023a). To successfully translate an instruction dataset, we claim that the following two critical factors should be considered: i) completeness of translation, and ii) instruction-aware translation. The completeness of translation measures the extent to which translators fully translate the instruction data. It is essential for instruction datasets, since its incompleteness (i.e., the absence of either instruction or response) makes it difficult to perform instruction tuning (Mukherjee et al., 2023; Wang et al., 2023; Touvron et al., 2023b). In addition, instruction-aware translation measures the extent of informativeness of the translated instruction data. Since the informativeness of an instruction dataset determines the success or failure of instruction tuning, translators must precisely consider the inherent attributes of the instructions. For example, as demonstrated in Figure 1, a sentence that is grammatically incorrect in the instruction and its corrected counterpart in the response should not be translated. This is because their translations could result in identical sentences, thereby diluting the original intent of the instruction and leading to meaningless data.\nTo effectively consider the aforementioned critical aspects, we introduce a new translation framework tailored for instruction datasets, named INSTATRANS (INSTruction-Aware TRANSlation). First, to generate paired datasets translated from English to the target language, we translate a small subset of a high-quality English instruction dataset using GPT-4 (Achiam et al., 2023) and our specially designed prompt for instruction dataset translation. We then use these paired datasets to fine-tune LLMs for instruction dataset translation, resulting in INSTATRANS. This approach ensures high translation quality while simultaneously minimizing the API costs associated with the translation. Our empirical findings demonstrate that (1) instruction datasets translated by INSTATRANS ensure completeness and instruction-awareness, and (2) fine-tuning LLMs with datasets translated by INSTATRANS can effectively improve their performance in the target language."}, {"title": "Related Work", "content": "Large language models. The field of natural language processing (NLP) has been significantly revolutionized in various tasks through the advent of Large Language Models (LLMs) (Zhao et al., 2023). Especially, 'scaling law' (Kaplan et al., 2020; Hernandez et al., 2021; Anil et al., 2023b), which shows a positive correlation between the size of the model and training data and the model's performance, has led to the substantial advancements on LLMs. Based on this scaling law, LLMs are typically trained via \u201cpre-training and fine-tuning\" learning paradigm where they are pre-trained with large amounts of unsupervised data and fine-tuned with a relatively small amount of supervised data. Meanwhile, the way prompts are input into the model can elicit various capabilities of the LLM, presenting a paradigm where outputs are generated according to the user's intentions (Zhang et al., 2023a; Wei et al., 2022).\nMachine Translation. Machine translation (MT) refers to a system where a computer translates a source text into a target text (Hutchins, 1995). It has been a fundamental component of NLP, progressing from rule-based systems (Shiwen and Xiaojing, 2014) to the present era, which is dominated by LLMs (Zhu et al., 2023a).\nMT systems face difficulties with syntactic and grammatical differences across languages (Xu et al., 2022). This differences make it difficult to translate from one syntax to another without losing meaning (Holman et al., 2007). Inconsistency in terminology, especially in technical or specialized texts, poses another challenge. MT systems can sometimes vary in their translation of specific terms within the same document, leading to confusion and a lack of coherence in the translated text (Alam et al., 2021). Lastly, MT systems face significant challenges due to omission errors, which result in translations often lacking crucial information from the source text (Lotz and Van Rensburg, 2016). This issue becomes especially pronounced when the source text extends beyond the sentence-level (Voita et al., 2019; Maruf et al., 2021).\nMeanwhile, numerous studies are being undertaken that employ LLMs in order to advance the field of machine translation (Jiao et al., 2023; Zeng et al., 2023; Xu et al., 2023). They are mainly focused on how to fine-tune LLMs to boost translation performance. However, none of these studies have addressed the translation of instruction datasets."}, {"title": "Methodology", "content": "We assess the quality of instruction datasets translated by existing machine translators in terms of completeness and informativeness. To create an evaluation dataset, we manually categorized the widely-used Alpaca instruction dataset (Peng et al., 2023) into various tasks, such as Correction, Rephrase, Code, and Others. In these tasks, instruction-aware translation is critical since the failure to consider the intrinsic attributes of these tasks can yield a meaningless instruction dataset. We then selected 30 samples from each task and translate them into Korean using both LLM-based translators (Achiam et al., 2023; OpenAI, 2023) and commercial translators (Johnson et al., 2017; Lee et al., 2016). Finally, we measured the quality of the translated instruction datasets through an automated assessment using GPT-4 (Achiam et al., 2023). To this end, we used the adjusted prompt of GEMBA (Kocmi and Federmann, 2023), as shown in Figure 2, to assess the quality of instruction datasets in terms of completeness and informativeness.\nAs shown in Table 1, LLM-based translators struggle with completeness of translation. Because they are fine-tuned to generate responses to any instruction, they often produce new responses rather than translations. This incompleteness of translation leads to lower informativeness score. However, they exhibited a high informativeness-to-completeness score ratio. In other words, they performed instruction-aware translation at a high rate as long as they translated it completely.\nWhile commercial translators (Johnson et al., 2017; Lee et al., 2016) scored higher on completeness, they did not sufficiently capture the instruction's inherent attributes due to their limited capabilities, resulting in inappropriate translations (i.e., a low informativeness-to-completeness score ratio). These findings suggest that neither approach fully meets the requirements for translating instruction datasets successfully."}, {"title": "INSTATRANS: Instruction-aware Translation Framework", "content": "In this paper, we aim to improve the completeness of the LLM translation for the instruction dataset, thereby enhancing its informativeness. To this end, we propose a new translation framework tailored for instruction datasets, named INSTATRANS (INSTruction-Aware TRANSlation), as shown in Figure 3.\nStep 1: Translate a small subset of instruction data. In order to enhance completeness of translation, we use function calling, which is choosing the appropriate function and generating appropriate arguments for the instruction, given a list of candidate functions (Srinivasan et al., 2023). Specifically, we split the (instruction, response) pair into the multiple sentences using newline character. Then, we input this array of sentences along with the function definition in Figure 4. This enables LLMs to translate individual sentences, ensuring no omissions occur, while simultaneously comprehending the context of the given input. In addition, we conduct extensive prompt engineering to successfully perform instruction-aware translation, as illustrated in Figure 5. It can easily be used for translation between any two languages by changing the source and target languages within the prompt.\nStep 2: Further fine-tune the LLMs that have already been fine-tuned. Currently, achieving high performance in function calling requires using proprietary models such as GPT-3.5 (OpenAI, 2023) and GPT-4 (Achiam et al., 2023), which incur API costs. To reduce ongoing API costs, we fine-tune open-source LLMs to develop INSTATRANS. Motivated by (Xu et al., 2023), we further fine-tune open-source LLMs that have already been fine-tuned, utilizing instruction datasets translated in Step 1. Specifically, we minimize the following loss function L:\n$L = - \\sum_{X \\in X} \\sum_{i} log p_{\\theta} (y_i|x),$ (1)\nwhere $X$ represents the English instruction dataset, $x$ denotes a sample from $X$ such as the one shown in (a) of Figure 1, $y_i$ denotes the i-th token of the text where x has been translated into the target language during Step 1, and $\\theta$ denotes the trainable parameters of LLMs. Note that we did not use any instructions for translation during the fine-tuning process, which showed superior performance in our experiments. Consequently, INSTATRANS can perform complete and instruction-aware translations without the need for function calling, as it is specifically optimized for translating instruction datasets."}, {"title": "Experiments", "content": "We designed our experiments, aiming at answering the following key research questions (RQs):\n\u2022 RQ1: Does INSTATRANS show superior performance in existing translation metrics when translating instruction datasets?\n\u2022 RQ2: Does INSTATRANS successfully perform complete and instruction-aware translations?\n\u2022 RQ3: Does the instruction dataset translated by INSTATRANS enhance the performance of LLMs in the target language?"}, {"title": "Experimental Settings", "content": "Dataset and model details. In Step 1, we translated segments from OpenOrca (Mukherjee et al., 2023) and Flan v2 (Longpre et al., 2023) into Korean. Then, in Step 2, given the outstanding performance of the fine-tuned version of SOLAR 10.7B (Kim et al., 2023) as evidenced on the HuggingFace Open LLM Leaderboard (Beeching et al., 2023), we selected this model as our base model for further fine-tuning with instruction datasets translated in Step 1, resulting in INSTATRANS.\nCompetitors. We compare our INSTATRANS with competitors in the following three categories:\n\u2022 Proprietary LLMs: To validate the effectiveness of our prompt specifically designed to translate instruction datasets, we compare GPT-4 (our prompt) and GPT-3.5 (our prompt), which utilize our prompt along with function calling, with proprietary LLMs (GPT-4, GPT-3.5, and Gemini-Pro (Anil et al., 2023a));\n\u2022 LLM-based translators: We compare INSTATRANS with other LLM-based translators (TowerBase (Unbabel, 2024a) and TowerInstruct (Unbabel, 2024b)) that support the Korean language. Also, we present the performance of INSTATRANSpre, which is fine-tuned from the pre-trained version of SOLAR 10.7B, to verify the effectiveness of further fine-tuning strategy;\n\u2022 Commercial translators: To demonstrate the limitations of commercial translators (DeepL (DeepL Pro, 2018), Google Translator (Johnson et al., 2017), and Papago (Lee et al., 2016)), we also present their performance.\nEvaluation details. To evaluate the performance and quality of machine translation for instruction datasets, we used the sampled Ko-H3 datasets. These datasets were obtained for research purposes through a request to the Open Ko-LLM leaderboard (Park et al., 2023). They include the sampled Ko-arc, Ko-mmlu, and Ko-truthfulqa, which are composed of segments from the ARC (Clark et al., 2018), MMLU (Hendrycks et al., 2020), and TruthfulQA (Lin et al., 2022) datasets translated into Korean, respectively. For the machine translation evaluation metrics, we utilize BLEU (Papineni et al., 2002), COMET (Rei et al., 2020), and GEMBA (Kocmi and Federmann, 2023)."}, {"title": "Main Results", "content": "We highlight the best results in bold and the second-best results with an underline in the following tables. Also, ours are shown in purple color.\nRQ1: Comparison of machine translation performance. As shown in Table 2, within the category of proprietary LLMs, GPT-4 (our prompt), GPT-4, and Gemini-Pro demonstrated a superior performance. In the realm of LLM-based translators, INSTATRANS and INSTATRANSpre outperform TowerBase and TowerInstruct. In addition, TowerInstruct exhibits lower machine translation performance than TowerBase because it is fine-tuned to generate responses to any instruction, often providing answers without translation. Meanwhile, INSTATRANSpre outperforms INSTATRANS in terms of machine translation performance. Finally, DeepL achieved the state-of-the-art among commercial translators, even showing superior machine translation performance among all competitors.\nRQ2: Quality comparison of translated instruction datasets. Since machine translation metrics have limitations in assessing the quality of translated instruction datasets, we assessed the quality of instruction datasets translated by the top performers in each category using the same method outlined in Section 3.1. Table 3 shows that i) the quality of instruction datasets translated by GPT-4 (our prompt) surpassed others in terms of both completeness and informativeness; ii) the quality of instruction datasets translated by INSTATRANS was comparable to that of GPT-4 (our prompt); iii) LLM-based methods demonstrated a high ratio exceeding 90%, in contrast to commercial translators, i.e., Should they accomplish a complete translation, they successfully perform instruction-aware translation with a high ratio. Note that our INSTATRANS successfully translates instruction datasets without requiring API costs, which means that LLMs can be extended to diverse languages at a relatively low cost.\nRQ3: Effectiveness of translated instruction datasets. To validate the effectiveness of translated instruction datasets in fine-tuning LLMs to align with user intentions, we measure the performance of LLMs that have been fine-tuned with instruction datasets translated by the top performers in each category. Specifically, we randomly sampled 5,000 examples from the Alpaca dataset and translated them using each translator. Then, we fine-tuned the pre-trained SOLAR 10.7B model with those translations. To evaluate the performance of fine-tuned models, we utilize Ko MT-bench, an automated evaluation method based on GPT-4 that employs the Ko MT-bench dataset. This dataset is a translation of the MT-bench (Zheng et al., 2023) by engaging experts in English-to-Korean translation.\nAs illustrated in Figure 6, the models fine-tuned with instruction datasets translated by INSTATRANS achieved the highest score. This indicates that INSTATRANS can generate high-quality non-English instruction datasets. These results also indicate that existing translation metrics do not sufficiently measure the performance of translation for instruction datasets."}, {"title": "Conclusion", "content": "In this paper, we have identified the successful translation of high-quality English instruction datasets as a promising direction for generating high-quality non-English instruction datasets, due to tail phenomena. To successfully translate instruction datasets, we claim the critical importance of completeness of translation and instruction-aware translation. In light of this, we proposed a new translation framework, named INSTATRANS (INSTruction-Aware TRANSlation), which specializes in efficiently translating instruction datasets using LLMs. We have empirically demonstrated that (1) INSTATRANS successfully performs complete and instruction-aware translations, and (2) fine-tuning LLMs with datasets translated by INSTATRANS can effectively improve their performance in the target language."}, {"title": "Ethics Statement", "content": "In the development and evaluation of the INSTATRANS (INSTruction-Aware TRANSlation) framework, as detailed in our study, we have adhered strictly to ethical guidelines and principles to ensure fairness and integrity throughout the research process. Our experimental setup was meticulously designed to provide an equitable and unbiased evaluation, taking into account the unique characteristics and challenges associated with translating instruction datasets for non-English languages. We have ensured that the translation process, facilitated by fine-tuned large language models (LLMs), was conducted with the utmost respect for the original content's integrity, aiming for completeness and instruction-awareness in translation to preserve the datasets' inherent attributes. We confirmed that all the data used in our experiments were free of licensing issues.\nFurthermore, our approach to leveraging LLMs for enhancing translation quality was guided by ethical considerations, particularly in minimizing potential biases and ensuring the translations' fidelity to their source materials. The use of LLMs was conducted in a controlled and transparent manner, with careful examination of the models' outputs to detect and correct any errors or biases, thus maintaining the high-quality and reliability of the translated datasets.\nIn conclusion, our research on INSTATRANS has been carried out with a strong commitment to ethical principles, ensuring that the experimental procedures were fair, the data handling was secure, and the translations produced were both accurate and sensitive to the nuances of instruction. We believe that our ethical approach not only underscores the validity of our research findings but also contributes positively to the broader field of language model development and application."}, {"title": "Limitations", "content": "In this study on the INSTATRANS (INSTruction-Aware TRANSlation) framework, we acknowledge crucial limitations about the scope of research. Although we demonstrated the effectiveness of INSTATRANS, our experiments were not conducted across a wide range of language pairs due to the absence of datasets to evaluate translation of instruction datasets. Generally, generating evaluation datasets necessitates substantial expenditure, as the engagement of human experts is essential. Due to these limitations, the scope of our evaluation was limited to English-to-Korean translations, which could potentially pose a challenge in ensuring the applicability of our findings across diverse languages.\nRecently, numerous studies have been focusing on how to fine-tune the large language models (LLMs) for translation. However, due to the constraints of our computational resources, our study has not yet thoroughly investigated the methodology for fine-tuning to enhance their translation performance of instruction datasets. We proposed further fine-tuning LLMs that have already been fine-tuned, focusing soley on efficiency rather than the effectiveness in translating instruction datasets. Therefore, there is still room for improvement in the quality of the instruction dataset.\nDespite these challenges, we demonstrated the critical importance of completeness of translation and instruction-aware translation in translating instruction datasets successfully. Furthermore, the INSTATRANS framework marks a significant advancement in the field of translation instruction datasets, offering a scalable approach. Our research has established a foundation for future research in this critical area."}]}