{"title": "Vortex under Ripplet: An Empirical Study of RAG-enabled Applications", "authors": ["Yuchen Shao", "Yuheng Huang", "Jiawei Shen", "Lei Ma", "Ting Su", "Chengcheng Wan"], "abstract": "Large language models (LLMs) enhanced by retrieval-augmented generation (RAG) provide effective solutions in various application scenarios. However, developers face challenges in integrating RAG-enhanced LLMs into software systems, due to lack of interface specification, requirements from software context, and complicated system management. In this paper, we manually studied 100 open-source applications that incorporate RAG-enhanced LLMs, and their issue reports. We have found that more than 98% of applications contain multiple integration defects that harm software functionality, efficiency, and security. We have also generalized 19 defect patterns and proposed guidelines to tackle them. We hope this work could aid LLM-enabled software development and motivate future research.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models (LLMs) offer effective solutions for a spectrum of language-processing tasks. Retrieval-augmented generation (RAG) techniques further enhance their capability in concrete application scenarios, by providing relevant information from external data sources. Cloud services and various frameworks [1]\u2013[6] also relieve developers from implementing and hosting their own LLM models and RAG solutions. Therefore, an increasing number of software systems have integrated RAG-enhanced LLMs to realize intelligence features, which this paper refers to as LLM-enabled software. Indeed, more than 36,000 open-source LLM-enabled software applications have been created on GitHub in recent 6 months, to solve a variety of real-world problems.\nCloud services and various frameworks have greatly eased developers' burden of incorporating RAG-enhanced LLMs. However, there still remain many challenges that must be addressed to build correct, efficient, and reliable LLM-enabled software. In fact, some non-expert developers may not even notice these failures, due to insufficient testing and lack of LLM and RAG knowledge. While much work has been conducted to improve LLM and RAG algorithms [7]\u2013[9], this paper focuses on the unique challenges raised by system integration of RAG-enhanced LLMs.\nChallenge-1: Lacking interface specification. Performing cognitive and generation tasks, LLM agents and general AI components typically lack a specification detailing their behavior [10]. Given a particular input, LLM agents cannot specify whether it is able to provide a correct answer in an expected format.\nIn addition, as LLMs are instructed through text prompts and enhanced by RAG, it is also impractical to define the capability boundary of a certain LLM. Therefore, LLM-enabled software systems cannot formally describe the interface between the RAG-enabled LLM and the remaining software components. Thus, the developers have to tackle the under-specific interface and resolve potential failures.\nChallenge-2: Requirements from software context. As a generative model, an RAG-enabled LLM inherently tends to provide different responses for the same question in multiple runs. While these responses may all seem feasible, not all of them will match the software context and trigger correct software behavior. For example, a user has different expected responses of \"how to rename a file\" when using a code editor and a system helper. Furthermore, conventional software components typically have strict format requirements, while the data-driven LLM agents support text of various formats."}, {"title": "II. BACKGROUND", "content": "LLMs enable a wide range of cognitive features of the software, including conversation, document comprehension, and question-answering [19]. To further assist LLMs in resolving knowledge-intensive tasks, retrieval-augmented generation (RAG) techniques [7], [8] are proposed to provide external knowledge through prompt engineering. They equip LLMs with timely, trusted, and highly related knowledge that is unseen in its training procedure, without fine-tuning requirements. Therefore, LLM could be easily extended to various application scenarios and updated to the latest knowledge.\nSeveral vector databases are proposed to manage external knowledge and provide RAG solutions, including MongoDB [20], ChromaDB [5], and Faiss [6].\nAs shown in Figure 1, the RAG algorithm contains two phases. In the storage phase, text is extracted from source files and sliced into multiple chunks, which is the knowledge entry. With the embedding module from an LLM, each knowledge entry is embedded into a semantic vector, which is a high-dimensional float vector that represents semantic features. These semantic vectors serve as the index of knowledge entries when stored in the vector database. In the query stage, the RAG algorithm embeds the query question using the same embedding module and retrieves relevant knowledge entries by the distance between semantic vectors. The retrieved knowledge then constructs the LLM context, simplifying the original knowledge-intensive task to a comprehension task.\nTo further ease developers' burden, LangChain [3], LlamaIndex [4], and other frameworks provide unified interfaces for developers to integrate various LLMs and vector databases into software systems. This leads to the emergence of LLM-enabled software - more than 36,000 LLM-enabled software applications are created on GitHub in recent 6 months.\nFigure 1 illustrates a common workflow of LLM-enabled software. Note that, some LLM-enabled software may have different workflows, but still follow a similar structure. Before the deployment, a vector database is initialized with carefully sliced text from various files. During the execution, a software component collects and converts user inputs. It then extracts key phrases to construct the query question and retrieve relevant knowledge from the vector database. An LLM agent then takes the retrieved knowledge and original user input to construct a prompt. It also manages the execution history and maintains the LLM context. Finally, the software component processes the LLM response and answers the user. Sometimes, the software component also invokes third-party libraries to assist data processing, user interaction, and other features."}, {"title": "III. STUDY METHODOLOGY", "content": "We collect a suite of 100 open-source LLM-enabled software applications for GitHub (all latest versions as of May 22nd, 2024). As GitHub contains many toy applications, we manually checked around 500 randomly selected open-source applications that incorporate both LLMs and vector databases for their intelligence features, to obtain these 100 non-trivial ones. We confirmed that they each targets a concrete real-world problem, tightly integrates LLMs and vector databases in their workflow (i.e. not a simple UI wrapper), and maintains an active user community.\nOur application suite covers different programming languages, including Python(71%), TypeScript(24%), and others(5%). More than 90% applications utilize LangChain framework, due to its early release, while remaining create their own framework or other existing ones. Around 85% applications incorporate OpenAI services [1] as their LLM module, 10% use LLaMA [22], and the remaining use ChatGLM [23]. These applications incorporate various vector databases, including ChromaDB(45%), MongoDB(29%), Faiss(19%) and others(7%), through local deployment and cloud services. Due to the young age of RAG-enhanced LLM techniques, 40% applications are created within 12 months at the time of our study. Despite their young age, these applications are widely-used and actively updated. Half of the applications have received more than 135 stars, with the maximum value of 163,000. In recent 6 months, the developers of each application have made 500(374) commits on average(median).\nIn general, these applications support five major functionality categories, reflecting the common application scenarios of LLM and vector databases. Around 39% applications support context-based question-answering(QA), including document comprehension and knowledge search; 23% applications support task management, assisting users to achieve their goal; 19% applications serve as chat robots, keeping track of user-specific histories; 13% applications are in the form of central platforms, scheduling multiple correlated AI tasks; and the remaining 6% applications perform various text-related tasks, including automated fact-checking and plagiarism detection.\nSince no prior work studies integration failures in LLM-enabled applications. Therefore, we cannot rely on an existing list of defect patterns. Instead, our team, including LLM experts, carefully studied around 10,000 GitHub issue reports reported of the studied applications and obtained more than 3,000 bug-related ones. We manually confirm whether each of them is caused by software defects instead of end-user misuses: closed issues are confirmed by the developer's patch, and open issues are confirmed by issue reproduction through tests. We finally obtain 320 confirmed bug-related issues. We then generalize and cluster these issues, according to their root causes and impacts. For each cluster, we manually examine if there are similar defects in other applications and refine the clustering. We repeat this process for several rounds until converge to the findings in this paper.\nIn total, 19 defect patterns and 495 defects are identified. All issues and defects are examined by three of the authors, and their results are discussed and confirmed by all the co-authors.\nTo understand how defects impact software performance, we profile the end-to-end latency of applications both before and after fixing. We use real-world data that reflect application scenarios, including text/voice queries and files of different formats, referring to application manuals and issue reports. By default, we run each test 10 times and report the average latency.\nAll experiments were conducted on the same machine, which has a 16-core Apple M3 Max CPU (4.05GHz), 32MB L2 Cache, 64GB RAM, 2TB SSD, and 1000Mbps network connection. Note that most applications perform LLM tasks through cloud services."}, {"title": "IV. IDENTIFIED INTEGRATION FAILURES", "content": "Through empirical study, we identified 495 defects from 100 LLM-enabled applications and summarized 19 defect patterns, as listed in Table I. They are typically caused by developers' unsystematic prompt/query construction, misunderstanding of interface specification, unaware of software context, and lacking system management. They are widespread and harm software quality in various aspects: (1) functionality problems, including unexpected fail-stops, incorrect software behavior and unfriendly user interface; (2) efficiency problems, including slow execution and increased token cost; and (3) security problems.\nWe further explore the mechanism of these defects and divide LLM-enabled software into 4 major components that tightly work together: (1) LLM agent constructs prompt and generates LLM response; (2) vector database supports RAG algorithm and enhances the LLM agent; (3) software component is the remaining software that interacts with the first two components to perform certain tasks; and (4) system that manages resources and privileges to carry out the execution. Although we localize each defect pattern in a specific component, these patterns are actually related to the integration failure between multiple components, while that specific component is believed to be responsible for eliminating such failure."}, {"title": "B. Defects Located in LLM Agent", "content": "While LLMs have outstanding performance on various tasks, an incorrect integration would degrade the overall correctness and efficiency of software systems or even lead to fail-stop failures. In our benchmark suite, all applications suffer problems when integrating LLM agents.\n1) Unclear context in prompt: Large language models (LLMs) suffer hallucination problems, especially when their prompts do not include enough information [24]. Due to the nature of generative models, LLMs are likely to produce grammatically coherent, contextually relevant, but semantically incorrect text outputs, e.g., non-existent quotes, false historical events, or even spurious scientific facts. In LLM-enabled applications, the unreliability of LLM agents could easily transmit to the tightly-integrated software components and even downstream tasks [14], [25]. Therefore, the LLM agent should construct clear and informative prompts to alleviate hallucinations. However, around a quarter of the benchmark applications failed.\nTake ChatIQ [26], a chatbot for Slack platform, as an example. It is expected to answer questions according to the chat history and uploaded text files. Unfortunately, it is very likely to provide fictive responses when asked about the specialty foods of certain cities, i.e., claiming that an inland city produces seafood. In another case, when the user asks about the meeting after uploading an invitation mail of work plan discussion, it responds with topics that are not mentioned in the mail: \"In addition to the work plan, we will also discuss arrangements for our annual gathering and a new employee training plan.\u201d\nDevelopers may easily blame the inner flaw of LLM itself. While LLM hallucination is an unresolved problem, the software's incorrect behavior is not just caused by LLM itself. Actually, such hallucination could be alleviated through improving the prompt design [27]. The former failure could be resolved by enabling the RAG framework or online search module to provide external knowledge. For the latter, it would generate less incorrect responses by including clear instructions in the prompt template, i.e., \u201cPlease precisely answer according to the given file.\u201d\n2) Lacking restrictions in prompt: The LLM agents control LLM behavior through prompts. Besides guiding LLM to complete certain tasks, the prompts also include instructions that restrict LLM not acting in a certain way. Similar to conventional software, developers tend to spend most efforts to ensure the core functionality (enabling LLM to perform the intended actions), but ignore to handle corner cases (preventing LLM from performing unexpected actions). Due to the infinite input space of LLMs, it is hard for developers to design test cases that have high semantic coverage. Therefore, developers are unlikely to discover all unexpected behaviors and add restrictions in the prompt. Around 19% applications in our benchmark suffer such a problem.\nRealChar [21] is an application designed to simulate certain characters and chat with users (Figure 2). While expected to keep role-playing, it generates out-of-character responses from time to time. For example, when asked \"Are you an AI?\", it will admit that it is an LLM without hesitation. Similarly, when invoking OpenAI GPT-3 model without proper restrictions, it will answer the question of \"How can I hack into someone's social media account?\u201d frankly, leading to serious ethical and legal problems.\nThe most critical step in tackling this problem is to explore the output space of LLM agents and identify the unexpected responses, which require thorough in-lab testing (alpha testing) and also large-scale user testing (beta testing). Once identified, one may avoid them with a combination of fine-grained prompt instructions and output validation to restrict LLM agents' behavior.\n3) Insufficient history management: For applications that involve multi-turn human-machine interactions (i.e. chat robots, text editors), in order to generate responses within the dialogue context, the LLM agent manages the recent history, including inputs and responses. When lost track of the history conversation, it is likely to (a) provide contextually incorrect answers when the user refers to the history; or (b) perform operations that conflict with earlier ones. In our benchmark suite, 27% applications have such problems, degrading the software correctness. Note that, the long-term history and RAG data are typically managed by the vector database, which will be discussed in Section IV-C.\nAs LLM's transformer structure naturally remembers recent activities, it is more likely to forget when the history accumulates. For example, PDF-chatbot [28] incorporates GPT-4 to comprehend PDF documents and answer user questions. Its conversation module is expected, but unfortunately fails, to retain chat history across interactions. As a consequence, after the user informs the author of a PDF document, it correctly answers the question of \"Who is the author?\". However, after 4 to 5 rounds of conversation, it replies \"I do not have access to such information\" instead, indicating its forgetting. As another example, the task management application babyAGI [29] suggests a list of tasks of a certain topic given by the user. However, it simply displays the response of LLM, without examining these generated tasks. Therefore, its suggestions are increasingly repetitive when the user continues asking for more tasks. Similarly, the repetitive response of Godmode-GPT [30] leads to repeated execution on failed task commands, wasting computation resources.\nSuch a forgetting problem greatly harms the functionality and usability of the user interface of multi-turn conversation. Although end-users could resolve the forgetting problem by repeating their earlier inputs, it actually defeats the purpose of using the application \u2013 reducing human efforts. One potential solution is maintaining a short summary of key information of past conversations, and always appending it when constructing prompts. Another other solution is validate LLM responses using execution history.\n4) Missing LLM input format validation: Integrated in software, LLM agents take the textual output of the upstream tasks to construct its prompt. While LLMs' interface accepts all text strings within a certain length, they are only capable of handling a subset of text format (i.e. markdown syntax, JSON, CSV, etc). If the software fails to validate the input format, the LLM agent is likely to provide incorrect responses or even lead to software crashes. Among all the applications in our benchmark suite, 24% lack input format validation.\nFor example, IncarnaMind [31] supports comprehending text and PDF documents. While the interface allows users to upload Markdown files as text documents, the LLM agent actually fails to recognize the Markdown syntax from time to time. A potential fix is removing these syntax and converting them to plain text through rule-based approaches. Even for the supported PDF documents, if the software does not recognize text inside it, an \"list index out of range\" exception will be thrown out. As another example, AppifyAi [32], a code generator, allows users to upload CSV and Excel files, but lacks a module that converts them to the format that LLM could recognize.\nMany developers are unaware of LLM's requirement of clear and standardized text, as its specification mainly focuses on the length of input text and suffixes of uploaded documents. Therefore, almost all context-based QA applications in our benchmark suite do not examine surface-level pattern of the input text. To tackle this problem, we strongly recommend developers carefully design input validation algorithms and maintain a whitelist of handleable file formats.\n5) Incompatible LLM output format: Besides input validation, integrating LLMs into the software systems also requires converting their output to be compatible to the downstream tasks. LLMs support a wide spectrum of output formats, including structured/semi-structured text and code. However, their downstream tasks typically only support a subset of them. Sometimes, the downstream tasks perform rule-based string operations and have explicit format requirements: following certain syntax, the existence of certain keywords, etc. Sometimes, the responses have implicit requirements when displayed: the ordering of content, text style, etc. If the LLM agent fails to provide compatible output, it would lead to bad user experience, software misbehaviors, or even fail-stop failures. In our benchmark suite, around 13% of applications suffer such problems.\nWhile the semantic content of LLM responses is usually reliable, it is quite hard to restrict a generative model to output in a strict format. An example is h2oGPT [33], a document processing application that supports various file formats. When extracting text snippets from a longer article, the LLM agent does not retain the original line breaks and other text formats, which harms the readability. As another example, babyAGI is expected to remain the list order after updated, but often wrongly re-order them.\nSome applications also apply rule-based processing (i.e. decoding, string operations), where the incompatible output is likely to cause software crashes. The finance module of h2oGPT requires a JSON-format string from the LLM agent, as shown in Figure 3. However, the LLM agent constructs tuple-format strings from time to time, leading to unexpected decoding failures. Even when the LLM output passes JSON decoding, the software will misbehave if it misses a certain key or contains an incompatible value (i.e., a timestamp of wrong format).\nThere is no silver bullet for the LLM agent to tackle this problem, due to the different requirements from various downstream tasks. Instead, developers should design task-specific solutions to re-order and re-structure the LLM output format. The developers of h2oGPT could match the text before and after processing, and align them with rule-based approaches, and the developers of babyAGI could keep track of each list element.\n6) Unnecessary LLM output: LLMs tend to provide long responses when not restricted by instructions or computation resources. Therefore, the LLM agent is likely to output unnecessary content. Some developers may accept it as all the required information is provided. However, considering the additional effort of retrieving useful information, these unnecessary outputs greatly harm the service quality of the entire application. There are two major sources of unnecessary outputs: (1) the LLM over-generalizes a question and provides extra information that is not required; (2) the LLM repeats or rephrases its earlier responses and provides redundant information. Among all the applications in our benchmark suite, 24.0% suffer such problems.\nprivateGPT [34] utilizes Vicuna-7B [35] to generate document ingestion and answer user questions. Sometimes, it would appends meaningless text after a short answer, i.e. \u201cI don't know.]]>### Jack ### Sally ### Charlie ### David\u201d. As another example, LLMChat [36] incorporates GPT-3.5 to realize the similar feature. It is expected to answer \u201cJ.\u041a. Rowling\u201d when asked \u201cWho is the author of the Harry Potter series?\u201d. However, besides the author name, it also outputs a long paragraph introducing the main characters, plots, writing styles, and other unrelated information. Code-Review-GPT [37] also tends to generate overly polite and verbose code reviews, while users typically only care about bug reports.\nTo alleviate unnecessary outputs, the LLM prompts should clearly specify the required information and instruct the LLM to respond briefly. Another solution is to explicitly set the number of generated tokens, either through prompt instructions or API parameters (i.e. -n flag of OpenAI APIs).\n7) Exceeding LLM context limit: Facing constrained memory and limited computation resources, most cloud service providers set a maximum token length for their LLM services [1], [38]. Even for local LLMs running on a powerful machine, extra-long inputs would lead to accuracy issues, due to the limitations of the attention mechanism adopted by LLM architectures. In practice, the LLM agent limits the context length, which counts all the tokens inside the input and the corresponding output. If one LLM invocation exceeds the context length limits, the LLM agent will truncate the output and provide an incomplete response. This problem is particularly severe when (1) the vector database is involved in prompt construction, or (2) the software maintains a long dialogue history. Among our benchmark suite, 32% of the applications contain such defects.\nIn many applications, the token quota is likely exhausted by detailed instructions, long text to analyze, and uncompressed history. Every time invokes LLM, Quivr [39] sends the entire chat history to the LLM to ensure an in-context response, and thus quickly reaches the token limit. Similarly, ChatDocs [40] constructs the prompt with all text extracted from PDF documents, which easily exceeds the token limits and triggers CUDA out-of-memory error. Clearly, developers could 1) compress instructions through prompt engineering, 2) limit user input length through UI design and data chunking; and 3) abridge history through NLP (natural language processing) and RAG techniques. Note that, even executed on a powerful server, the application could easily exhaust the enlarged token quota if the LLM agent does not carefully construct the prompt.\n8) Improper LLM agent management: In current software framework, the applications have to initialize an agent instance before invoking the LLM. Sometimes, an application would initialize multiple agents for different tasks or users. These agents records and maintains the configurations, execution history, and prompt constructions of LLMs. If LLM agents are not properly managed, the application will face problems of data loss, broken data pipeline, and performance degradation. This defect eixsts in 14% of the applications that manages multiple LLM agents.\nEven using LangChain and other frameworks that warp LLMs and provide unify APIs, developers still need to systematically manage the LLM agents. Take DB-GPT [41], a multi-source knowledge management application, as an example. While expected, it does not solidify the LLM agent to store its configurations and history persistently. Therefore, all data will be flashed out when software terminates, greatly harming user experience. As another example, LlamaChat [2] fails to initialize LLM agent with Vicuna-7b and other models, failing to provide real-time conversation with users. Given these observations, developers should carefully design data storage and error handling mechanism, preventing data losses and unexpected fail-stops. When an LLM agent instance fails, The application should be able to restart it or switch to another agent without data loss."}, {"title": "C. Defects Located in Vector Database", "content": "In LLM-enabled applications, vector databases provide important support for intelligence features, serving as the long-term memory of language models. The incorrect usage of vector databases is likely to result in software misbehaviors, harming service quality and user experience. Around 35% of our benchmark applications does not correctly use the vector database. While developers are likely to criticize the RAG algorithm and neglect the coordination with software [42], there is a chance to eliminate the incorrect behavior through a better integration of vector databases.\n1) Knowledge misalignment: Vector databases store and manage knowledge entries. To create these entries, the application first extracts text from documents of various formats, and splits text into several chunks, each containing a piece of cohesive knowledge unit. Each chunk is then tokenized and embedded into a semantic vector, which forms a knowledge entry together with the original text chunk. If the data chunking and tokenization are not performed in an accurate and robust way, the LLM-enabled software application would misbehave due to the low-quality knowledge base, or suffer memory overflow due to inefficient memory management. In our benchmark suite, around 28% of applications have knowledge misalignment problems.\nSometimes, an application fails to extract data chunks from files of various sizes. For example, AutoGPT [43] encounters failure when processing both large and small files. It triggers out-of-memory error when tokenizing data chunks from large JSON files, due to the large chunk size. Meanwhile, it also ignores small files (i.e. less than 150 characters), wrongly regarding them do not contain any data.\nIn other cases, an application fails to extract intact knowledge units due to bad chunking positions. FastGPT [44] simply splits data chunks according to character counts. Therefore, when given structured data (i.e. tables), it is likely to break data integrity and fail to obtain cohesive knowledge units. If the chunking position is moved to the end of a table or sentence, the knowledge entries will become much more feasible.\n2) Conflict knowledge entries: In the vector databases, the semantic vectors serves as both identifiers and indices of an knowledge entry. Similar as relational databases, developers have to carefully design these identifiers to ensure software correctness and reliability. Particularly, embedding different text data into same feature vector would lead to severe correctness problem: the earlier data would be overwritten and suffer data loss! In our benchmark suite, we observe that 20% applications contain such defect.\nTake Webui [23] as an example. When updating its knowledge base with a new document, due to a semantic vector collision, it overwrites the knowledge entry obtained from a previous document with similar topics. Similarly, Godmode-GPT [30] wrongly overwrites a knowledge entry when expected to append new content to it. Clearly, developers should carefully design the embedding mechanism and manage the potential conflict knowledge entries, instead of simply relying on the existing vector database design.\n3) Improper text embedding: The knowledge entries are indexed by their semantic vectors when stored in vector databases. Their correct retrieval highly relies on the correct embedding the entries containing similar knowledge topics should also have similar semantic vectors, and vice versa. An inaccurate embedding would cause RAG techniques to decrease LLM accuracy and efficiency, and thus harm service quality of the entire software. In general, the developers have to carefully deal with text characteristics in three different levels: 1) encoding format, i.e. encodings and natural languages; 2) surface-level pattern, i.e. writing styles and structures; and 3) deep-level pattern, i.e. semantics and topics. In our benchmark suite, around 34% applications improperly manage text embedding.\nQnA [45] is an in-context QA application. While its LLM agent supports a wide range of natural languages, it uses ISO-8859-1 encoding when embedding the text, which only supports languages from Western Europe. Therefore, the software will respond garbled text when the user upload documents in Asian languages. As another example, Anything-LLM [46] improperly interprets text structures and truncates words in the middle, which the embedding model could not understand. However, the software correctness could be greatly improved by a simple format conversion.\n4) Improper similarity search: When accessing a vector database, the similarity search algorithm identifies the most pertinent data that matches the query. It plays a critical role in constructing context for the LLM agent such that serves the software downstream tasks. If the similarity search fails, the LLM agent will provide inaccurate or out-of-context responses, resulting in software misbehaviors. We observe such defects in 19% applications that conduct similarity searches in their core function modules.\nGenerally, the query to the vector database should concentrate on the topic of targeted knowledge, otherwise, the relevant entries are unlikely to be retrieved. For example, the task management application babyAGI queries the vector database with a general task (i.e. prepare a farewell) from the user, but fails to retrieve relevant data with such a vague description. In fact, babyAGI also generates several concrete steps (i.e. invite guests, order food, and decorate house) for the user to accomplish this general task. If query with these concrete sub-tasks individually, the vector database is able to find the knowledge entries that have similar semantic vectors to the query.\nBesides failing to retrieve relevant data entries, retrieving irrelevant ones also harms software correctness. Sometimes, the vector database provides more information than required: it returns the correct knowledge entry together with others that have close feature vectors. If the application does not validate the output of vector databases, these irrelevant data entries would end up constructing misleading context for the LLM agents. For example, when ChatDocs [40] queries vector databases for documents of a certain topic (i.e. climate change), it obtains a list of unrelated references (i.e. cooking recipes), due to mentioning a same entity (i.e. temperature). This failure could be alleviated by a relevance validation before appending them to the LLM prompt, either through similarity scores from the vector database or utilizing a small NLP model."}, {"title": "D. Defects Located in Software Components", "content": "In an intelligence software system, the AI-related components (i.e. LLMs and vector databases) typically play central characters. Meanwhile, the software components around them also provide important support to ensure the functionality and performance of entire software systems, including data/control flow logic, UI components, and coordination across different modules. Without the reliable integration of software components, the software is likely to misbehave. For instance, a defect in a UI component could result in an unresponsive button, while a defect in data flow is likely to cause data loss or even corruption.\nIn this section, we focus on the defects that emerge from general software modules that interact with LLMs and vector databases.\n1) Without final output: The LLM agents support multi-turn interaction, allowing users to instruct it in a step-by-step or feedback-based way. This is typically visible in in-context QA and chat robot applications. However, the users of LLM-enabled software applications usually expect an \"on-exit\" conclusion of the entire conversation, which serves as the final output. Otherwise, although finishes all the tasks, the latest output of the LLM agent would remain intermediate or incomplete answer, making a normal software termination seem like an unexpected crash. We observe this problem in 22 of 24 applications that have multi-turn conversations.\nTake LocalAGI [47] as an example. After accomplishing an article according to user requirements, it allows the user to request revision in a conversation-like manner. During the revision process, it regenerates a certain part of the original article according to user requirements. However, it simply stops without outputting the final version of the entire article when the user finishes the revision process. Making things worse, due to its infinite loop design, LocalAGI, from time to time, gets stuck in the revision step and endlessly regenerates its response until the entire software is manually terminated. This further prevents the application from providing a final output.\nActually, missing final output is a common problem for conversation-based applications, as applications have no idea whether the user has finished his/her tasks until explicitly noticed. To tackle this problem, we recommend developers design an \u201con-exit\u201d behavior to provide a final output that summarizes all the intermediate results in the history conversation.\n2) Improper error handling: Error handling is a classic problem of software engineering. The integration of LLM agents and vector databases further challenges this task. These two components are data-centered and have loose interface format requirements. However, the conventional software components are logic-centered and have strict interface specifications. Therefore, LLM-enabled applications are more likely to trigger errors and exceptions during execution. While most the benchmark applications have implemented exception handlers, not all of these handlers are feasible. In fact, we have triggered fail-stop failures in around 10% of application.\nDB-GPT [41] is a central platform that translates users' natural language requests to SQL queries and fetches data from SQL databases. It will receive an error code from the SQL server if the LLM-generated SQL query contains syntax error and fails parsing. While expected to automatically fix the syntax error, or at least to guide the user to resolve it, DB-GPT simply output \u201cApplication error: a client-side exception has occurred.\u201d without any detailed information or fixing attempts. We have conducted an experiment on the all the 1,534 natural language requests from BIRD development dataset [48], and received syntax error reports for 24% of the generated SQL query. Developers may easily blame the inability of LLM or insufficient schema information from RAG. However, 70% of these syntax errors could be fixed by rule-based solutions or re-generation by additionally proving the error message to the LLM agent.\nWorse still, some applications do not even realize the existence of these errors. Langchain-Chatchat [49] wrongly requests an internet connection when accessing a local LLM. It crashes with Internal Server Error and Connection Refused Error when unconnected to the internet, which is unlikely to be exposed during testing. Therefore, it lacks an exception handler for these errors.\n3) Low-Frequency interactivity: When the LLM agent or vector database is deployed on a remote server (including cloud services), the server typically sets a timeout limit for the connection from clients. If the client sends requests at a low frequency, it is likely to lose connection with the server and suspend operations, leading the application to shut down unexpectedly. Moreover, since LLM agents are stateful and require query history, the re-connection would also harm the core intelligence functionality of the application and user experiences. We observe such low-frequency interactivity problems in around 90% applications.\nAutoGPT [43] supports a conversation-based user interface and allows users to enter requests at any time. It incorporates urllib3 library [50] to open a session and access LLMs on the Open AI cloud server. However, due to the time-out mechanism, if the user requests at a low frequency, the application will lose connection with the cloud server and crash. To tackle this problem, the developer could either change the timeout settings of the timeout settings, or send a 'heartbeat\" request periodically to maintain the connection.\n4) Privacy Violation: Expanding the scope from RAG-enhanced LLM algorithms to the entire software system, the privacy issue arises. Here, we focus on the privacy violation that is unique to LLM-enabled applications. In traditional software, users access data through the operating system (OS) which validates user identity and grants access. In LLM-enable applications, the LLM agent acts as a super-user and requests data to OS on behalf of the end-users. Meanwhile, the LLM agent itself lacks an identity authentication mechanism. If the application does not properly isolate the data of different users, the LLM agent will access unauthorized data when it makes incorrect inferences or encounters malicious requests. In our benchmark suite, 11 of 19 applications that invoke system calls violate user privacy.\nAs a multi-user platform, AutoGPT [43] is expected to manage file ownership and prevent unauthorized access from users. However, it allows the LLM agent to read and write all files within its workspace, without examining the user identity. One could easily read or damage the file belonging to others, through injection attacks [51] on the LLM agent. While convenient, developers actually should not grant all system privileges to the LLM agent, as system root access should not be granted to an untrustworthy object. Instead, they should keep track of the original user request of each LLM agent behavior, and grant system privileges according to the user identity.\""}, {"title": "E. Defects Located in System"}]}