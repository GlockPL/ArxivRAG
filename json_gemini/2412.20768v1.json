{"title": "Sample Correlation for Fingerprinting Deep Face Recognition", "authors": ["Jiyang Guan", "Jian Liang", "Yanbo Wang", "Ran He"], "abstract": "Face recognition has witnessed remarkable advancements in recent years, thanks to the development of deep learning techniques. However, an off-the-shelf face recognition model as a commercial service could be stolen by model stealing attacks, posing great threats to the rights of the model owner. Model fingerprinting, as a model stealing detection method, aims to verify whether a suspect model is stolen from the victim model, gaining more and more attention nowadays. Previous methods always utilize transferable adversarial examples as the model fingerprint, but this method is known to be sensitive to adversarial defense and transfer learning techniques. To address this issue, we consider the pairwise relationship between samples instead and propose a novel yet simple model stealing detection method based on SAmple Correlation (SAC). Specifically, we present SAC-JC that selects JPEG compressed samples as model inputs and calculates the correlation matrix among their model outputs. Extensive results validate that SAC successfully defends against various model stealing attacks in deep face recognition, encompassing face verification and face emotion recognition, exhibiting the highest performance in terms of AUC, p-value and F1 score. Furthermore, we extend our evaluation of SAC-JC to object recognition datasets including Tiny-ImageNet and CIFAR10, which also demonstrates the superior performance of SAC-JC to previous methods. The code will be available at https://github.com/guanjiyang/SAC_JC.", "sections": [{"title": "1 Introduction", "content": "In recent years, remarkable advancements in face recognition have been largely attributable to the development of deep learning techniques [1]. A common practice for model owners is to offer their models to clients through either cloud-based services or client-side software. Generally, training deep neural networks, especially deep face recognition models, is both resource-intensive and financially burdensome, requiring extensive data collection and significant computational resources. Therefore, well-trained models possess valuable intellectual property and necessitate protection [2; 3]. Nonetheless, model stealing attacks can steal these well-trained models and evade the model owners' detection with only API access to the models [4], posing serious threats to the model owner's Intellectual Property (IP).\nModel stealing attacks are carried out with the goal of illegally obtaining functionally equivalent copies of the well-trained model owners' source model, with the white-box or even the black-box access to the source models. In the case of white-box access, the attacker can gain access to all the internal parameters of the source model. To avoid detection by the model owner, the attacker is able to employ source model modification, including pruning [5], fine-tuning [6], adversarial training [7], and knowledge distillation [8]. Furthermore, attackers are also able to leverage the model extraction attack [9; 10] to steal the function of the source model, with only the black-box access to the source model. In such a paradigm of model stealing attack, the attacker can steal the function of the source model using only the model's outputs, without the need for access to the inner parameters, and thus is considered more general and threatening. Regarding deep face recognition models, we observe that model extraction attacks achieve an accuracy of up to 95.0% of the original accuracy of the source model on KDEF [11] in face emotion recognition with only output labels of the source model. Moreover, in face verification, we also observe attackers can evade most of the stealing detection easily because these models only output the verification results rather than labels. In total, deep face recognition is confronted with a pressing challenge posed by model stealing attacks.\nIn recent years, the growing concerns over model stealing attacks have led to the development of various methods aiming at protecting the intellectual property (IP) of the deep models. Generally, these methods can be categorized into two categories: the watermarking methods [12; 13; 14; 15; 16; 17; 18; 2; 19] and the fingerprinting methods [4; 20; 21; 22; 23]. Watermarking techniques typically incorporate either weight regularization methods [12; 13; 14] or backdoor insertion strategies [16; 17; 2] during the model training phase to embed a distinct watermark into the model. However, these approaches need to manipulate the model's training process, often resulting in a trade-off where the model's performance on its main task is compromised. On the contrary, fingerprinting methods leverage the transferability of adversarial examples and identify stolen models by calculating the attack success rate on the suspected model. These methods do not interfere with the model's training procedure, which means they do not sacrifice the model's accuracy on its main task. However, it's important to note that adversarial-example-based fingerprinting methods can still be vulnerable to adversarial training [24] or transfer learning [25] and are resource-intensive as well as time-consuming for the model owner [4]. Furthermore, when it comes to the threat of model stealing attacks on well-trained deep face recognition models, unfortunately, it is regrettable to note that no model fingerprinting methods have been proposed so far. Faced with these stealing threats within the field of deep face recognition, we propose a model fingerprinting method tailored specifically to this domain.\nTo overcome the weaknesses of existing methods and solve the model fingerprinting problem in deep face recognition, we propose a correlation-based model fingerprinting method called SAC. As mentioned above, existing model fingerprinting methods rely on the suspect model's output as a point-wise indicator to detect the stolen models, which neglects the information hidden behind pair-wise correlation. Intuitively, samples with similar outputs in the source model are more likely to have similar outputs in the stolen models [26]. Specifically, we utilize the correlation difference between the source model and the suspect model as an indicator for detecting the stolen model. Nevertheless, calculating correlation among clean samples from the defender's dataset may be affected by the common knowledge shared by most models trained for the same task, on which most of the models produce identical labels. To get rid of the influence of common knowledge shared by most models, we leverage data augmentation to magnify the difference between models and have studied the influence of different augmentation methods used in Hendrycks and Dietterich [27] on SAC. Results demonstrate that SAC with JPEG Compression (SAC-JC) achieves the best results on different tasks and model architectures. Furthermore, on the task of face verification, because the model owner can only know whether two images are from the same identity and cannot get access to the exact label or probability of the images, we propose Feature from Reference Images (FRI). FRI gathers a batch of n + 1 images from the same identity and chooses one sample as the target for augmentation and the other samples as reference, and we leverage the results of the face verification model (whether 0 or 1) to get an n dimension vector to replace the model outputs used in SAC. To assess the effectiveness of SAC on face recognition, we conduct experiments involving five distinct types of attacks: fine-tuning, pruning, transfer learning, model extraction, and adversarial training on two common face recognition tasks face verification [28] and face emotion recognition [11] across different model architectures. Furthermore, we extend our evaluation of SAC-JC to object recognition datasets including Tiny-ImageNet and CIFAR10, also demonstrating that SAC-JC outperforms the previous methods.\nOur main contributions are summarized as follows:\nWe introduce sample correlation into model IP protection, and propose utilizing the correlation difference as a robust indicator to identify model stealing attacks, which provides a new insight into model IP protection.\nWe disclose the model stealing risk in deep face recognition and are the first to propose the model fingerprinting method on deep face recognition tasks using SAC-JC with feature generation method FRI.\nWe study different augmented methods' influence on SAC and propose SAC-JC which leverages JPEG compression to augment data and magnify the difference between models.\nComprehensive results substantiate that SAC-JC not only successfully detects different model stealing attacks in face recognition tasks but also succeeds in object classification tasks, across different model architectures and datasets. Furthermore, without training surrogate models, SAC-JC significantly reduces the computational burden, being around 34393 times faster than CAE.\nCompared to the preliminary conference version [26], this manuscript has made significant improvements and extensions. The main differences can be summarized into five aspects: 1) We disclose the vulnerability of deep face recognition models to model stealing attacks and design a feature generation method FRI to fingerprint the face verification task in Section 3.4. 2) We study the influence of augmented samples on SAC and propose JPEG compression based SAC-JC in Section 3.3, which is more effective and converted. 3) We extend SAC-JC to two more face-related tasks including the face verification task and face emotion recognition task KDEF, and evaluate SAC-JC on two more model stealing attacks including adversarial training and knowledge distillation in Section 4.2. 4) We provide additional experimental results and in-depth analysis in Section 4. 5) We add analysis and related work about fingerprinting on deep face recognition in Section 1 and Section 2.2."}, {"title": "2 Related Work", "content": "2.1 Deep IP Protection\nModel stealing attacks present a significant risk to the proprietary rights of the model's owner. These attacks can be categorized into several distinct methods: 1. Fine-tuning [29]: In this method, the attacker modifies the parameters of the source model with labeled training data for multiple epochs. The attackers can fine-tune the source model in all layers or only the last layer. 2. Pruning [5; 6; 30]: Attackers employing this technique selectively prune less significant weights in the source model based on certain indicators, often involving activation values. 3. Transfer learning [25]: In this setting, the attacker adapts the source model for similar tasks and utilizes the knowledge embedded in the source model to advance their own goals. 4. Model extraction [10; 9]: Given the substantial expenses and time required for data labeling, attackers opt for this technique. It involves replicating the functionality of the source model using unlabeled data from the same distribution. Remarkably, this attack can be executed without access to the source model's internal parameters, relying solely on the model's outputs. 5. Adversarial training [24]: Attackers employ a blend of normal and adversarial examples to train models, which helps circumvent most fingerprinting detection methods. To counter the threat of model stealing attacks, numerous methods for protecting model intellectual property (IP) have been proposed. These methods can generally be categorized into two main categories: watermarking methods and fingerprinting methods.\nWatermarking Methods Watermarking methods mainly focus on the training phase of the source model. They usually rely on weight regularization [12; 13; 14; 15] to add weight-related watermark into models, or train models on triggered set to leave the backdoor in them [16; 17]. Nevertheless, methods mentioned above can not detect newer attacks, for example, model extraction which trains a surrogate model from scratch [10; 4]. Even though some watermark methods such as VEF [31] or EWE [2] could handle model extraction, they unavoidably interfere in the training process, sacrificing model utility [18; 2; 19] for IP protection. For VEF, it also requires white-box access to the suspect model, which greatly limits the functionality, let alone some special circumstances under which the accuracy drop for IP protection is absolutely unacceptable.\nFingerprinting Methods Fingerprinting, in contrast, capitalizes on the transferability of adversarial examples and therefore authenticates model ownership without the need to manipulate the model training process. This approach ensures zero compromise in model accuracy. Lukas et al. [4] proposes conferrable adversarial examples, with the aim of optimizing their transferability to stolen models while minimizing the transferability to irrelevant models trained independently. Additionally, ModelDiff [21], FUAP [22], and DFA [23] employ various types of adversarial examples, such as DeepFool [32] and UAP [33], to fingerprint the source model. Except for this, DeepJudge [34] introduces a unified framework, utilizing various indicators to detect model stealing under both white-box and black-box settings. However, these techniques are vulnerable to adversarial defenses like adversarial training [24] or transfer learning, making them less effective in preserving model ownership. Moreover, these methods often necessitate the training of numerous surrogate and irrelevant models with diverse architectures to create robust fingerprints, which imposes a significant computational burden on the model owner. Unlike previous approaches, our method relies on the correlation between samples rather than simple instance-level differences, enabling faster and more robust model fingerprinting with data-augmented samples instead of adversarial examples."}, {"title": "3 Method", "content": "3.1 Problem Definition\nIn the context of the model IP protection scenario, there are two primary parties involved: the defender and the attacker. The defender is the entity that owns the well-performing machine learning model by using a proprietary training dataset and a specific training algorithm [20]. In a cloud service setting, the defender deploys their well-trained models as a cloud service or client-sided software [20] so that attackers could only get access to the model output. Another setting is called the client-sided software setting, where the attacker can get white-box access to all the inner parameters as well as the model structure.\nThe attacker's objective is to use their own dataset, which follows the same distribution as the defender's data, to reverse engineer a model that closely mimics the accuracy of the original model, while the defender aims at identifying the ownership of the suspect model. Generally speaking, cloud service deployment is the prevailing choice in the market. As such, our primary focus is on black-box IP protection, where the defender is constrained to accessing only the output of the suspect model while remaining oblivious to the architecture of the suspect model.\n3.2 Sample Correlation for Neural Network Fingerprinting\nPrevious fingerprinting methods have traditionally focused solely on point-wise consistency between the source model and the suspect model, identifying model stealing behavior by determining whether the suspect model classifies the same adversarial examples into the same incorrect classes as the source model. However, these methods often prove to be less robust when it comes to scenarios involving adversarial training or transfer learning.\nTo enhance the robustness of model stealing detection, we shift our approach away from point-wise criteria to the pairwise relationship between model outputs. The intuition behind this is easy to understand: when two samples yield similar outputs in the source model, they are more likely to produce similar outputs in the stolen models as well. Therefore, we introduce SAC, a correlation-based fingerprinting method that leverages the previously mentioned correlation consistency to identify model stealing effectively. Additionally, to mitigate the impact of shared common knowledge among irrelevant models, we investigate the process of searching for suitable samples, as elaborated in Section 3.3. An overview of our framework is illustrated in Figure 1.\nHere we elaborate on the details of model fingerprinting with sample correlation [37; 38]. Assign $O^{source} = {o^{source}_1, o^{source}_2, ..., o^{source}_n}$ and $O^{stolen} = {o^{stolen}_1, o^{stolen}_2, ..., o^{stolen}_n}$ as the set of the outputs for the source and stolen models, where $o^{source}_k$ and $o^{stolen}_k$ are the output of the k-th input sample from the source model and stolen model, we could calculate the correlation matrix among all n input samples so as to get a model-specific correlation matrix C as follows:\n$\u0421 = \\Phi(O), C \\in R^{n \\times n}$ (1)\nwhere $C_{i,j} = corr(o_i, o_j) \\quad i,j = 1,...,n,$ where $C_{i,j}$ denotes the i, j entry of the model's correlation matrix C, which is computed by assessing the correlation between the i-th and j-th outputs of the model. $\\Phi$ is the function for calculating the correlation matrix based on the output set. To precisely measure the correlation among the model's outputs, we introduce several functions to model the relationship of outputs [38]. First is cosine similarity [39], which denotes the cosine of the angle between two vectors:\n$C_{i,j} = Cos(O_i, O_j) = \\frac{o_i^T o_j}{||o_i|| ||o_j||}, \\quad i,j = 1,..., n.$ (2)\nAnother method for measuring the correlation of model outputs is Gaussian RBF [40]. Gaussian RBF is a popular kernel function, computing the distance between two instances based on their Euclidean distances:\n$C_{i,j} = RBF(O_i, O_j) = exp(-\\frac{||O_i - O_j||^2}{2\\sigma^2}), \\quad i,j = 1,..., n.$ (3)\nOnce the correlation matrices have been computed for both the source model and the suspect model, we proceed to calculate the L1 distance between these matrices, which serves as our fingerprinting indicator. Any model whose distance to the source model falls below a specified threshold value, denoted as d, will be identified as a stolen model:\n$Distance = \\frac{||C^{stolen} - C^{source}||_1}{n^2} \\leq d,$ (4)\nwhere we denote $C^{stolen}$ and $C^{source}$ as the correlation matrix of the stolen model and the source model. In scenarios where defenders seek to determine the optimal threshold value d, the use of a validation set can be instrumental. For instance, defenders can utilize the average of the means of the correlation indicators obtained from the irrelevant models and the models created through adversarial extraction on the validation set as the threshold d.\n3.3 How to Find Suitable Samples?\nUtilizing the correlation matrix stated above, SAC calculates the distance between the source models and the suspect models. In addition, it is crucial to have suitable samples as model inputs to support this fingerprinting process. Because models trained for the same task will output the same ground-truth labels on most of the clean samples, SAC's performance is affected by the common knowledge shared by these models.\nFingerprinting with JPEG Compressed Samples\nTo get rid of the influence of common knowledge and amplify the difference between models, we leverage data augmentation on the randomly selected normal samples as the input for SAC. We first study the influence of 14 different types of corrupted methods from Hendrycks and Dietterich [27] as the augmented methods on SAC. Figure 2 demonstrates the result of SAC with different augmented methods in terms of the average AUC on different kinds of model stealing attacks. Through the experiments, JPEG corruption (JPEG compression) has the best detection result. JPEG corruption is a common data corruption method and it compresses a clean image in the JPEG format using different quality levels [41]. It includes operations including color mode conversion, downsampling, discrete cosine transform, and quantization to lower the size of images. Different from adversarial noise, which is maliciously crafted by the attackers and is easily detected by the model provider using adversarial detection [42], JPEG compression is a daily-used image compression method, and therefore, cannot be detected by the model provider.\nAnother advantage of leveraging JPEG compression is that JPEG corruption is not related to the models' adversarial robustness and can detect the stolen models accurately after adversarial training or adversarial extraction. As far as we know, all existing model fingerprinting methods rely on transferable adversarial examples to identify stolen models. Because the success rate of adversarial examples is related to model robustness, attackers can utilize adversarial training [43] to evade these fingerprinting methods' detection. Our experiments demonstrate that attackers can successfully evade detection by fine-tuning the extracted model for just a few epochs with unlabeled data and the predicted label from the source model in an adversarial training way, expressed as follows:\n$\\min_{\\Theta_{stolen}} \\sum_x \\max_{\\|\\delta\\|_\\epsilon} l(f_{stolen}(x + \\delta), f_{source}(x)),$ (5)\nwhere $f_{stolen}$ and $f_{source}$ denotes the stolen and the source model, $\\delta$ denotes the adversarial noise smaller than the bound e, which is generated by the attacker using adversarial attack methods such as FGSM [44] or PGD [24] and $\\Theta_{stolen}$ denotes the parameters of the stolen model.\n3.4 Calculating Image Feature in Face Verification\nIn the context of face verification tasks, the model does not produce output labels. Instead, it only provides a binary result, typically denoting whether two images belong to the same identity. The lack of model output in label space from images causes problems in calculating the correlation matrix in SAC. To handle this issue as well as get the feature of the target image, we propose a method called Feature from Reference Images (FRI). Aiming at forming the specific feature of the target image, FRI first gathers n reference images with the same identity as the target image, and then, makes use of JPEG compression to augment the target image to amplify the difference between the target image and the reference images. Finally, FRI forms n target-reference pairs as the input of the suspect models and gets an n dimension vector as the model-specific feature of the target image:\n$F_t = [V(I_t, I_{r(1)}), V (I_t, I_{r(2)}) \\cdots V (I_t, I_{r(n)})]$ (6)\nwhere $F_t$ represents the feature of the target image, V represents the verification model, $I_t$ represents the target image, and $I_{r(n)}$ represents the n th reference image. To be specific, we set n = 50 in our experiments and $F_t$ is a 50 dimension 0-1 feature for the target image. We then replace the output of the model in SAC with the feature $F_t$ generated from FRI. The algorithm for employing SAC-JC with the feature generation method FRI in face verification is illustrated in Algorithm 1 The experiment results illustrated in Section 4.2 demonstrate the effectiveness of SAC-JC with FRI in face verification."}, {"title": "4 Experiment", "content": "4.1 Setup\nIn this section, we evaluate various methods for safeguarding model intellectual property (IP) against diverse model stealing attacks on a range of datasets and model structures, confirming the efficacy of SAC-JC. To be specific, we design our evaluation of different IP protection methods against five categories of stealing attacks as listed below:\nFine-tuning. Typically, there exist two prevalent fine-tuning techniques: fine-tuning the last layer (Finetune-L) or fine-tuning all the layers (Finetune-A). As the names stated, Finetune-L indicates keeping the majority of the model's layers frozen and only training the final layers, while Finetune-A means fine-tuning the entire model, including all of its layers. In our experiments, we assume that the attacker fine-tunes the source model with an SGD optimizer on the attacker's dataset.\nPruning. In our settings, we adopt Fine Pruning [5] as the pruning method. Fine Pruning, as a commonly used method that involves pruning neurons based on their activation values, removes less significant neurons from a neural network. As a common backdoor defense method, it could typically remove neurons that contribute to backdoor or malicious behaviors. Here it could serve as an attack to threat IP protection.\nModel Extraction. In the realm of model extraction attacks, there are generally two primary categories: probability-based model extraction and label-based model extraction. Label-based model extraction attacks [9; 10] focus on exploiting the defender's predicted labels to steal knowledge from the source model. The loss function could be expressed as $L = CE(f_{stolen}(x), I_{source})$, where $I_{source}$ indicates predicted labels from the source model and $CE(\u00b7)$ is the cross-entropy loss. As for the probability-based model extraction [45; 9; 46], the attacker possesses detailed output probability to train their stolen model:\n$L = \\alpha \\cdot KL(f_{stolen}(x), f_{source}(x)) + (1-\\alpha) \\cdot CE(f_{stolen}(x), I_{source}),$ (7)\nwhere $f_{stolen}(x)$ and $f_{source}(x)$ are the soft probability from the stolen and source models: $f^T (x) = softmax(f(x))$, in which T indicates the temperature, and KL(\u00b7) refers to the KL divergence. In the following experiments, we fix T = 20 as the temperature.\nIn face verification, because no label is available, we leverage the white-box knowledge distillation to replace the model extraction in our experiments. Knowledge distillation [46] is one of the model compression methods, which utilizes the knowledge from teachers to train the student models.\nAdversarial Model Extraction. Sharing the similar logic as adaptive model extraction from CAE [4], the attacker could evade fingerprint detection with adversarial training after the label-based model extraction. The slight difference between adaptive model extraction and this thread is that the attacker achieves the extraction by the predicted label from the source model in Equation 5, rather than the ground-truth label. Therefore, after adversarial training, the attacker could evade adversarial-example-based fingerprinting methods with negligible accuracy sacrifice. Additionally, in face verification, we leverage adversarial training as one of the attackers' methods to evade the attackers' detection.\nTransfer Learning. The attacker may also utilize the transfer learning technique to repurpose the source model for other related tasks, taking advantage of the model's knowledge while escaping from potential fingerprint detection. To simulate this, we transfer the CIFAR10 model to CIFAR10-C [27] and CIFAR100 dataset, from which we choose the first 10 labels. In addition, we perform transfer learning on the Tiny-ImageNet model, which is originally trained on the first 100 labels in the Tiny-ImageNet dataset, to the remaining 100 labels in the same dataset.\nModel Architecture. We evaluate different IP protection methods across a range of commonly used model architectures. All extraction models and irrelevant models are trained on VGG [47], ResNet [48], DenseNet [49] and MobileNet [50] on the multi-classification tasks, including KDEF, Tiny-ImageNet, and CIFAR10. Additionally, on the face verification task, we follow the experiment in a famous project Insightface 1 and leverage the commonly used model architecture in face recognition as irrelevant models and distillation models, including ResNet18, ResNet50, and MobileFace. Furthermore, to ensure the robustness of our results, for each attack and irrelevant model architecture, we train five models in case of randomness, in other words, 20 models for irrelevant models, extraction models, and fine-tuning models.\nModel IP Protection Methods. In order to validate the effectiveness of our method, we conduct a comparative analysis against several existing approaches, including IPGuard [20], CAE [4], and EWE [2]. IPGuard and CAE leverage the transferability of adversarial examples by testing the success rate of these adversarial examples when applied to the suspected models. If the attack success rate for any model exceeds a predefined threshold, it is identified as a stolen model. In the face verification task, we utilize adversarial attacks designed for face recognition [51] and calculate the attack success rate in pairs to adapt IPGuard and CAE to face verification tasks for a fair comparison. In contrast, EWE takes a different approach by training the source model using backdoor data [52] and embedding a watermark within the model. By employing a soft nearest neighbor loss to intertwine the watermark data with the training data, EWE aims to enhance the transferability of the watermark against model extraction. One thing to note is that we did not include the results of EWE on the face verification task and face emotion recognition task. In face verification, there is no output label from the verification model, and thus EWE fails. Additionally, in face emotion recognition, even if we try our best and use the official code of EWE, EWE causes the model to collapse in main tasks with accuracy dropping lower than 20%.\nDatasets. To assess the effectiveness and robustness of various fingerprinting methods, we perform experiments on different datasets and multiple tasks. Following previous works, there are two datasets used in our experiments, $D_{defender}$ and $D_{attacker}$, which belong to the defender and the attacker respectively. In the face verification task, we leverage MS1MV2 [53] as the dataset of the defender, and CASIA-Webface [54] as the dataset of the attacker. Additionally, we leverage ArcFace [55] as our model training protocol. As for the multi-classification tasks, e.g. KDEF [11], Tiny-ImageNet [56], and CIFAR10 [57], according to previous methods, we split the training dataset into two equal-sized subsets: $D_{defender}$ and $D_{attacker}$, which are owned by the attacker and defender, respectively. One point worth noting is that given the limitation of only 250 samples per label in Tiny-ImageNet, which leads to a source model accuracy drop to approximately 40%, we opt to curate a smaller dataset by selecting the initial 100 labels. This choice allows for a higher source model accuracy.\nEvaluation Metrics. To assess the effectiveness of different fingerprinting methods, similar to CAE [4], we employ the AUC-ROC curve [58] and calculate the AUC value, which quantifies the separation between the fingerprinting scores of the irrelevant models and the stolen models, serving as a measure of fingerprinting effectiveness. The ROC curve is a graphical representation of the True Positive Rate and False Positive Rate. AUC, the area under the ROC curve, ranges from 0 to 1, with a higher AUC indicating a superior fingerprinting method. For further evaluating the performance of different fingerprinting methods, we follow [31; 22] and introduce p-value as another evaluation metric. We leverage an independent two-sample T-test to calculate the p-value with the null hypothesis $H_0: p_{suspect} = p_{irrelevant}$, where $p_{suspect}$ and $p_{irrelevant}$ represent the average of the fingerprinting scores of the suspect and irrelevant models. To be specific, the fingerprinting score represents the correlation distance in SAC-JC, and attack success rate in IPGuard, CAE, and EWE. A smaller p-value indicates a higher level of confidence and a better distinction between the suspect models and the irrelevant models.\nAUC and p-value are both metrics not related to the threshold. To evaluate the performance of the fingerprinting methods with a specific threshold, we introduce the F1 score as another metric. The F1 score, which is the harmonic mean of precision and recall, can be calculated using the formula: $F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$. Additionally, we have provided the details of the specific threshold selection in the following paragraph.\nThreshold Selection. In general, thresholds can be determined using a small validation set. We follow the threshold decision method used in [21] and select the worst value found in the irrelevant models. To be specific, we choose the smallest correlation distance found in irrelevant models in SAC-JC and the highest attack success rate found in irrelevant models in IPGuard, CAE, and EWE as the threshold. To reduce the need for collecting irrelevant models, we only use four irrelevant models in our experiments across different datasets and source model architectures. Additionally, since model fingerprinting is a black-box detection method and we do not know the category of the model stealing attacks, we use the same threshold for all suspect models within one task for detection."}, {"title": "4.2 Fingerprinting on Face Recognition", "content": "In this section, we evaluate SAC-JC on two common face recognition tasks: face verification and face emotion recognition. Tables 1 and 2 demonstrate different fingerprinting methods against different model stealing attacks on face emotion recognition KDEF and face verification. Finetune-A and Finetune-L represent fine-tuning the source model on all the layers and the last layer, and Extract-L, Extract-P and Extract-Adv represent the three settings of the model extraction, label-based model extraction, probability-based model extraction, and adversarial model extraction. Besides, KD stands for knowledge distillation, where the source model serves as the teacher, transferring its knowledge to another model. On the other hand, Adv-Train involves utilizing adversarial training with the source model. In addition, to facilitate a more comprehensive comparison, we calculate the average AUC of different intellectual property protection methods applied to different model stealing attacks. Experiments demonstrate the effectiveness and superior performance of our method SAC-JC in terms of AUC, p-value, and F1. To be specific, SAC-JC achieves AUC = 0.97 on KDEF and AUC = 0.98 on face verification on average. Additionally, our method SAC-JC detects model stealing attacks successfully with black-box access to both the source model and the suspect models, while the other fingerprinting methods such as IPGuard and CAE need white-box access to the source model to generate adversarial examples. SAC-JC offers a broader range of applicability, allowing a third party to detect model stealing attacks without access to the inner parameters of the source model.\nFrom our experiments, we observe that the average attack success rate of CAE is higher than that of IPGuard, suggesting that CAE exhibits superior transferability compared to IPGuard. Additionally, in Table 1, our findings indicate that CAE outperforms IPGuard in identifying model extraction attacks, primarily owing to the introduction of conferrable scores. However, it's worth noting that the success rates of these methods still exhibit significant fluctuations across various model architectures, causing the low AUC in our multi-model architecture scenario. The success rate of attacks involving adversarial examples can be influenced by the robustness of the target model. Models designed with greater robustness or those that have undergone adversarial training may exhibit lower attack success rates when compared to irrelevant models. In contrast, the correlation difference metric is independent of model robustness and excels in its ability to detect stolen models consistently across various model architectures. Thus, our proposed method, SAC-JC, demonstrates a higher AUC compared to the other two fingerprinting methods. Furthermore, SAC-JC eliminates the need for the defender to train any surrogate models, which saves the defender's time on a large scale. We will discuss it in detail in Section 4.6.\nMoreover, we also compare the images used for fingerprinting in Figure 3. Adversarial examples used in CAE or IPGuard are not only easy to attract the attention of attackers, but also easy to detect and remove by attackers [4]. On the contrary, SAC-JC only leverages JPEG-compressed images as the input of the suspect models and can be hardly detected or observed by the owner of the suspect model. Additionally, in Table 3, we present the average accuracy of both the source and stolen models. This table illustrates that the majority of model stealing attacks have the capability to successfully replicate the source model with only a minimal decrease in accuracy. To be specific, the attackers achieve 95.0% of the accuracy of the source model only by utilizing the unlabeled data and the hard label from the source model."}, {"title": "4.3 Fingerprinting on Object Classification", "content": "To better assess the performance of SAC-JC in object classification tasks, we conduct experiments on Tiny-ImageNet [56] and CIFAR10 [57] in Tables 4 and 5. In these two tables, Finetune-A and Finetune-L represent fine-tuning the source model on all the layers and the last layer respectively, while Extract-L, Extract-P and Extract-Adv represent the three settings of the model extraction, label-based model extraction, probability-based model extraction and adversarial model extraction. Besides, Transfer-A and Transfer-L represents transferring the source model to a new dataset by fine-tuning all the layers or the last layer, and Transfer-10C represents transferring the source model to CIFAR10C. Similar to the results on face emotion recognition or face verification, SAC-JC performs better than IPGuard and CAE, especially when facing model stealing attacks such as adversarial training or adversarial model extraction. In addition, our experiments also reveal that another compared method EWE exhibits sensitivity to pruning, although its decline in AUC is less pronounced compared to other watermarking methods based on normal backdoors. One thing to note is that all three IP protection methods, IPGuard, CAE, and EWE, cannot detect the transfer-based model stealing attacks due to the label space change in transfer learning. These methods rely solely on the attack success rate as their identification criterion and consequently, they struggle to identify stolen models when dealing with transfer learning or label changes. Conversely, our methods utilize correlation differences as fingerprints, and this correlation consistency remains intact even when models are transferred to different tasks."}, {"title": "4.4 Sensitivity Analysis", "content": "To assess the efficacy of SAC-JC across varying numbers of augmented images, we conducted experiments employing SAC-JC on KDEF, Tiny-ImageNet, and CIFAR10. The results are depicted in Figure 4. In the figures, AUC-P, AUC-L, and AUC-Adv represent the performance of SAC-JC in terms of AUC on probability-based model extraction, label-based model extraction, and adversarial model extraction respectively. In addition, AUC-Finetune, AUC-Prune, and AUC-Transfer represent AUC of SAC-JC on fine-tuning, pruning, and transfer learning respectively. The outcomes of this experiment highlight the robustness of our SAC method to the number of samples as model fingerprints, even in a few-shot setting. With just"}]}