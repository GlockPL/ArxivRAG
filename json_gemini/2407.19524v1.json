{"title": "BIGbench: A Unified Benchmark for Social Bias in Text-to-Image Generative Models Based on Multi-modal LLM", "authors": ["Hanjun Luo", "Ziye Deng", "Haoyu Huang", "Xuecheng Liu", "Ruizhe Chen", "Zuozhu Liu"], "abstract": "With the rapid development of Text-to-Image models, biases in human image generation against demographic groups social attract more and more concerns. Existing methods are designed based on certain models with fixed prompts, unable to accommodate the trend of high-speed updating of Text-to-Image (T2I) models and variable prompts in practical scenes. Additionally, they fail to consider the possibility of hallucinations, leading to deviations between expected and actual results. To address this issue, we introduce VersusDebias, a novel and universal debiasing framework for biases in T2I models, consisting of one generative adversarial mechanism (GAM) and one debiasing generation mechanism using a small language model (SLM). The self-adaptive GAM generates specialized attribute arrays for each prompts for diminishing the influence of hallucinations from T2I models. The SLM uses prompt engineering to generate debiased prompts for the T2I model, providing zero-shot debiasing ability and custom optimization for different models. Extensive experiments demonstrate VersusDebias's capability to rectify biases on arbitrary models across multiple protected attributes simultaneously, including gender, race, and age. Furthermore, VersusDebias outperforms existing methods in both zero-shot and few-shot situations, illustrating its extraordinary utility. Our work is openly accessible to the research community to ensure the reproducibility.", "sections": [{"title": "1 Introduction", "content": "Text-to-Image (T2I) models, as a crucial part of Artificial Intelligence Generated Content (AIGC) technology, are evolving at an extraordinary speed due to the low barriers to entry and open-source initiatives by companies such as Stability AI (Rombach et al. 2022; Luo et al. 2023; Betker et al. 2023). Many models are able to generate high-quality and photo-realistic images from simple textual prompts. Nevertheless, similar to other generative models (Ross, Katz, and Barbu 2020; Mehrabi et al. 2021; Zhou et al. 2024), the growing adoption also raised concerns about fairness and biases in generated images, especially in images depicting human (Bansal et al. 2022; Cho, Zala, and Bansal 2023; Luccioni et al. 2023). Some research shows that the models have obvious biases to specific social groups (Perera and Patel 2023), especially women and colored races.\nSome researchers have proposed their solutions to address the issue (Gandikota et al. 2024; Schramowski et al. 2023; Shrestha et al. 2024). However, these methods face similar problems. Neither of them solves the two most important problems that stop debiasing methods to practical application. First, all existing methods are only applicable to a certain T2I model, usually SD 1.5 (Rombach et al. 2022). However, with the widely application of distillation (Meng et al. 2023) and the participation of communities like Hugging-Face, T2I models evolve rapidly. For example, the mainstream base model in the community evolves from SD1.5 to SDXL to SDXL Lightning (Lin, Wang, and Yang 2024) in only one year, and cutting-edge developers proposes models with new architectures such as Stable Cascade (Pernias et al. 2023) and Pixart (Chen et al. 2023), but recent research about debiasing still develops their models based on early models, limiting the application prospects. Second, most methods considers only specific debiasing metrics and are unable to zero-shot debias. For example, Fair Diffusion (Friedrich et al. 2023) uses a look-up table to add protected attributes to certain prompts about occupations in the table to ensuring demographic fairness. However, this method fails to tackle with any prompts that are not in the table. Debiasing techniques require good robustness and the ability of zero-shot generation to be adaptive for various inputs."}, {"title": "2 Related Works", "content": "Biases in T2I Models Over the last few years, T2I models like Stable Diffusion and DALL-E have seen increasing adoption (Ramesh et al. 2022; Esser et al. 2024). Many T2I models can generate high-quality images efficiently. Nonetheless, several studies prove that these models inherit biases from their training da tasets (Chinchure et al. 2023; Wan et al. 2024). Recent research classifies these biases into implicit generative bias and explicit generative bias (Luo et al. 2024a), corresponding to the sociological concepts of implicit bias (Pritlove et al. 2019) and explicit bias (Fridell 2013). Implicit generative bias refers to the phenomenon where, without specific instructions on protected attributes, T2I models tend to generate images that do not consist of demographic realities. For instance, when asked to generate \"a photo of a surgeon\u201d, models tend to produce images featuring male surgeons. Explicit generative bias is a specific type of hallucination, referring to the phenomenon where T2I models tend to generate images that do not consist with prompts containing specific protected attributes. For example, when asked to generate \u201ca photo of a rich black person\u201d models may sometimes fail to correctly generate an image featuring a black person. In our work, we primarily address the issue of implicit generative bias and avoid the influence of explicit generative bias which disturbs conventional methods.\nDebiasing Based on Prompt Engineering Recent works attempts to debias T2I models at different levels, such as diffusion-level (Shrestha et al. 2024) and prompt-level (Clemmer, Ding, and Feng 2024). Diffusion-level methods interfere with the image generation process, which can lead to uncontrollable side effects on the quality and style of the final image. Moreover, these methods can only be applied to diffusion-based models and are not suitable for transformer-based models like DALL-E3 (Betker et al. 2023). In contrast, debiasing based on prompt engineering, as a post-processing method, is more adjustable and more broadly applicable with less impact on image quality. However, these methods also face serious challenges. First, traditional prompt engineering methods rely on the assumption that the model correctly generate images based on the prompt, which cannot avoid hallucinations, especially those caused by explicit generative bias. Second, prompt engineering might have unintended side effects on unrelated attributes. For example, using SDXL-Turbo (Sauer et al. 2023), the proportion of women generated with the prompt \u201ca photo of a South Asian tennis player\u201d is significantly lower than that generated with the prompt \u201ca photo of a tennis player\u201d (Luo et al. 2024b). Lastly, the severity of these issues varies across different models, meaning that traditional methods, even if they address the first two issues, can only be applied to specific models, greatly limiting their practicality. In contrast, our method employs an innovative GAM structure that effectively addresses these problems, providing unprecedented practicality.\nDevelopment of SLM and MLLM Due to the tremendous success of ChatGPT-4 (Achiam et al. 2023), LLMS, especially MLLMs, have garnered significant attention and"}, {"title": "3 Method", "content": "In this section, we introduce the top-level pipeline of Versus-Debias. A brief overview of the pipeline is shown in Figure 2. As illustrated, VersusDebias consists of a GAM, a DGM, and the corresponding dataset.\nTo debias a model, VersusDebias first uses prompts without protected attributes from the dataset to drive the generator to generate images, which are then conveyed into the discriminator. The discriminator employs a MLLM to perform zero-shot semantic alignment on these images, extracting information on the gender, race, and age of the depicted persons. The alignment results are statistically averaged to determine the generative demographic proportions of three protected attributes for each main prompt, the occupation. The discriminator compares the proportions of each gender, race, and age group to the ground truth and modifies the prompts by adding the underrepresented protected attributes, which ensures that the generator produces more images featuring these ignored social groups in the next iteration. Each main prompt corresponds to a protected attribute array. The newly generated images are then conveyed into the discriminator again, and the process is repeated until the specified number of epochs.\nThe discriminator evaluates the generation results for each main prompt at each epoch, calculating the cosine similarity between the generative proportions and the ground truth to quantify the conformity between them. The protected attribute array with the highest cosine similarity for each main prompt across different epochs is then combined into a single dictionary, referred to as the best results, and conveyed to the DGM as shown in Figure 2.\nNext, the executor uses the fine-tuned SLM to accurately extract the potentially bias-inducing main prompt from the user prompt. It then selects an appropriate protected attribute based on the best results and adds it to the original prompt. Finally, the generator of the DGM uses the processed prompt to generate debiased images."}, {"title": "3.2 Generator", "content": "As a universal debiasing framework based on prompt engineering, VersusDebias can use any model in its generator, including diffusion-based models (Nichol et al. 2021; Sauer et al. 2023; Esser et al. 2024; Lin, Wang, and Yang 2024; Song, Sun, and Yin 2024) and transformer-based models (Dayma et al. 2021; Ding et al. 2022; Ramesh et al. 2022; Chen et al. 2023). To lower the barrier of VersusDebias and attract more researchers to debiasing studies, we develop an interface compatible with ComfyUI, which is a widely-used GUI for thousands of T2I models based on Stable Diffusion (Rombach et al. 2022). ComfyUI only requires changes to the input workflow file to adapt to various models and is equipped with a comprehensive API for external program"}, {"title": "3.3 Discriminator", "content": "As the core part of the GAM, the discriminator consists of three components: the aligner, the array editor, and the evaluator. In the following sections, we provide a detailed explanation of the implementation of these three components.\nAligner The function of the aligner is to extract information about the individuals in the images and compute the average of the extraction results for all prompts of the same main prompt. We used the fine-tuned InternVL-4B-1.5 (Chen et al. 2024c) from BIGbench in the aligner, which is trained via the dataset from FairFace (Karkkainen and Joo 2021) and reportedly achieves an alignment accuracy of 97.93%. We do a test at a sample of 50 images, whose result is shown in the supplementary material, indicating that this model exhibits high accuracy, meeting the requirements of VersusDebias.\nArray Editor The function of the array editor is comparing the generative proportions to the ground truth and adding the underrepresented protected attributes to the prompt. To achieve this, the array editor generates a new dictionary by subtracting the ground truth values from the actual generative proportions. The keys in this dictionary are main prompts, with negative values indicating that the generated data is below the actual data and positive values indicating the opposite. For all protected attributes with negative differences, the array editor adds these underrepresented attributes to the attribute array. The length of the attribute array $L$ can be adjusted based on the requirements for speed and accuracy, but it should generally be maintained at 4 times the number of epochs. This is because there are a total of nine protected attributes, and assuming an equal distribution of positive and negative differences, approximately 4.5 attributes will be added each time. Allowing for a buffer, the multiplier is set to 4 as a floor value, and $L$ is at least 5 as a"}, {"title": "Algorithm 1: Array Editing Algorithm", "content": "ceiling value. The array editor performs this operation once per epoch until the specified limit is reached. A detailed algorithm is shown in Algorithm 1."}, {"title": "Evaluator", "content": "The evaluator provides a comprehensive metric for evaluating the overlap between the generated results and the ground truth when selecting the best-performing debiased attribute arrays. Since gender, race, and age each have 2-4 protected attributes, we treat them as vectors and use cosine similarity to determine the similarity between the generated results and the ground truth:\n$S_{k} = \\frac{\\sum_{i=1}^{n_{k}} p_{k i} q_{k i}}{\\sqrt{\\sum_{i=1}^{n_{k}} p_{k i}^{2}} \\sqrt{\\sum_{i=1}^{n_{k}} q_{k i}^{2}}}$\nwhere $S_{k}$ is the cosine similarity for the protected attribute $k$ of a main prompt, $p_{k i}$ and $q_{k i}$ are the generative proportion and real demographic proportion of the $i$ attribute, and $n_{k}$ is the total number of the attributes for this protected attribute. After calculating the cosine similarity for these three attributes separately, the evaluator computes a weighted average to obtain the cumulative cosine similarity. We set the"}, {"title": "3.4 Executor", "content": "The pipeline of the executor is briefly shown in Figure 3. The main component of the executor is a fine-tuned SLM. We use the SLM to accurately perform the following tasks in sequence: first, analyze complex prompts from practical application scenarios and identify the potentially biased element; second, determine whether the prompt already includes protected attributes; Third, identify a occupation in the element list that has the closest meaning to response 1. The executor locates the debiased attribute arrays corresponding to response 3 from the best results, randomly selects a protected attribute from the generated debiased attribute array, and adds the selected protected attribute to the prompt.\nDue to the relatively fixed and simple nature of the tasks performed, we use a SLM instead of traditional LLMs to accomplish the aforementioned three tasks. SLMs are characterized by their faster operational speed and less resource consumption. However, they are subject to limitations in scale, which can lead to hallucinatory issues and less precise knowledge memory. To ensure accuracy, we develop a specialized dataset to fine-tune the model, thereby enhancing its capability to execute these tasks, particularly in the retrieval of synonyms. The dataset $D_{S L M}$ we used to train the SLM consists of 500 dialogue sets, with each set containing three rounds of queries $q$ and expected responses $r$, which is formulated as follows:\n$D_{S L M} = \\{(q_{i, k}, r_{i, k}) | i \\in [1, 500], k \\in [1, 3]\\}$\nIn the first round, $q_{1}$ asks the model to identify the potentially biased element in a given prompt. $r_{1}$ is \"none\" or the identified element, thus training the model's ability of extracting. To closely simulate practical application scenarios, we use Promptomania (Promptomania 2024) to generate one prompt for each of the 103 occupations in the prompt set. Additionally, we use the official US occupational classification method (United States and Budget 2018) and thesaurus (Merriam-Webster 2023) to collect synonyms and variations for the occupations, resulting in a total of 380 queries. We test these prompts with SDXL (Podell et al. 2023) and manually revise prompts that produced poor image results to improve their quality. To ensure the SLM does not misidentify potentially biased elements, we also include 120 queries containing prompts that solely describe landscapes and objects to train the model's judgment capabilities. In the second round, $q_{2}$ asks the model to determine whether the prompt originally contains a protected attribute, with $r_{2}$ being either \"yes\" or \"no\". We included 260 sets without protected attributes and 120 sets with protected attributes to comprehensively train the model's judgment capabilities. In the third round, $q_{3}$ asks the model to select a synonym from the element list for $r_{1}$ if it's not \"none\" while $r_{3}$ is the chosen synonym."}, {"title": "4 Experiment", "content": "In this section, we initially present the comparative experiments conducted in the selection of SLMs. Subsequently, we summarize the performance of VersusDebias in debiasing various T2I models for both few-shot and zero-shot scenarios and investigate the impact of the attribute array length $L$. Finally, we compare VersusDebias with several baselines."}, {"title": "4.1 SLM Comparison", "content": "We select three state-of-the-art SLMs, namely Qwen2-1.5B (Yang et al. 2024), MiniCPM-2B (Hu et al. 2024), and Gemma-2B (Team et al. 2024), for comparative analysis with representative LLMs deployable on a single GPU, Llama2-7B (Touvron et al. 2023) and Qwen2-7B, and the representative commercial large model, ChatGPT-4o (Achiam et al. 2023). All models except ChatGPT-4o are fine-tuned using our dataset $D_{S L M}$.\nWe conduct the analysis by developing a test set comprising 100 dialogue sets, consistent with the format of the training set. The test set consists of 52 dialogues describing humans without protected attributes, 24 describing humans with protected attributes, and 24 not involving human descriptions."}, {"title": "4.2 Debiasing Performance", "content": "Few-shot For few-shot scenarios, we utilize ChatGPT-4o to generate 5 prompts for each of the 103 professions in the dataset and each prompt is used to generate 8 images. The format of these prompts imitates the pratical prompts. We employ these prompts to generate images and calculated the cosine similarity against the ground truth in the dataset. We calculate the cosine similarity between the results and ground truth of three T2I models, both with and without VersusDebias at a $L$ of 20. The two diffusion-based models are Stable Diffusion 1.5 (Rombach et al. 2022) and Stable Diffusion XL (Podell et al. 2023), while the transformer-based model is Pixart-$\\Sigma$ (Chen et al. 2024a). These models are abbreviated as SDv1, SDXL, and PixArt in the following part.\nThe result is shown in Table 2. The results fully demonstrate VersusDebias's capability to effectively debias models of various principles in few-shot scenarios, especially in race and age where the models originally performed poorly. It is notable that PixArt, which initially performed worse, outperforms the originally better-performing SDv1 after debiasing. Upon reviewing the generated images, we find that this is because PixArt has a better capability to follow the guidance of the prompt. This highlights the importance of this capability for prompt-based debiasing."}, {"title": "Impact of Array Length L", "content": "Due to VersusDebias's reliance on randomly selecting protected attributes from debiased attribute arrays to add to the prompt for debiasing, the probability of selecting a protected attribute is quantized. Therefore, a larger attribute array can result in smoother variations in generative demographic proportions, thus improving accuracy. As explained in Section 3.3, the length of the debiased attribute array $L$ is greater than or equal to 5. We conducted comparative experiments with $L$ set to 5, 20, 50, and 100, corresponding to the number of epochs being 2, 5, 13, and 25, respectively. The T2I model used in the experiment is SDv1. The result is shown in Figure 4."}, {"title": "4.3 Comparison with Baselines", "content": "As no prior universal methods have been developed for debiasing, we select two existing methods based on the SDv1 model, FairDiffusion (Friedrich et al. 2023) and PreciseDebias (Clemmer, Ding, and Feng 2024). We also use SDv1 as the T2I model for VersusDebias in this part of the experiment to control for variables. FairDiffusion uses a lookup table to find occupations in the user's input prompt and adds gender descriptions based on predefined probabilities for different occupations. PreciseDebias employs a fine-tuned Llama2-7B model to identify the position of identity-setting words in the prompt and adds gender and racial descriptions according to fixed general probabilities. To ensure the fairness of the experiment, we replace the original racial proportions in PreciseDebias with the proportions from the ground truth and include additional test based on the dataset of PreciseDebias. Table 4 shows the results and Figure 5 provides a direct view with a example."}, {"title": "5 Limitation and Future Work", "content": "Although VersusDebias significantly outperforms existing debiasing methods, our framework still need to be improve in some aspects. First, although the fine-tuned InternVL-4B-1.5 demonstrates excellent alignment, particularly in racial alignment, it still cannot achieve perfect alignment. This limitation reduces the accuracy of the discriminator, which in turn affects the performance of the GAM. A better model that is specifically designed for human classification is worth investigating. Secondly, as a post-processing method, VersusDebias fails to handle extreme cases of hallucinations, such as when the model completely fails to generate images that match the prompt \u201can Asian husband and a white wife\u201d (Vincent 2024). This issue needs model-level improvement. Lastly, since VersusDebias only solves the problem of implicit bias, explicit bias remains to be addressed in future work. We hope our GAM and usage of LLM can be helpful for future research about explicit bias."}, {"title": "6 Conclusion", "content": "This paper introduces VersusDebias, a universal debiasing framework for T2I models, which enables precise and efficient debiasing for any T2I models by prompt engineering. To achieve this, we design a novel GAM to generate corresponding debiased attribute arrays for specific models and apply a advanced SLM to edit prompts which is fine-tuned by the arrays. T2I models use edited prompts to generate photos with less biases. Our GAM addresses the issues of model hallucinations and side effects caused by prompt engineering, which could not be handled by previous prompt-level debiasing methods, significantly improving the practicality of our method. Extensive experiments validate that VersusDebias surpasses existing methods in both zero-shot and pre-learned situations, while maintaining the original model's image generation style and quality. We hope that VersusDebias can promote the application of debiasing methods in T2I models, for a fairer AIGC community."}]}