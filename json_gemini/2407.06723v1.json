{"title": "Graph-Based Captioning: Enhancing Visual\nDescriptions by Interconnecting Region Captions", "authors": ["Yu-Guan Hsieh", "Louis B\u00e9thune", "Chun-Liang Li", "Cheng-Yu Hsieh", "Hadi Pour Ansari", "Ranjay Krishna", "Shih-Ying Yeh", "Pavan Kumar Anasosalu Vasu", "Oncel Tuzel", "Marco Cuturi"], "abstract": "Humans describe complex scenes with compositionality, using simple text descrip-\ntions enriched with links and relationships. While vision-language research has\naimed to develop models with compositional understanding capabilities, this is\nnot reflected yet in existing datasets which, for the most part, still use plain text to\ndescribe images. In this work, we propose a new annotation strategy, graph-based\ncaptioning (GBC) that describes an image using a labelled graph structure, with\nnodes of various types. The nodes in GBC are created using, in a first stage, object\ndetection and dense captioning tools nested recursively to uncover and describe\nentity nodes, further linked together in a second stage by highlighting, using new\ntypes of nodes, compositions and relations among entities. Since all GBC nodes\nhold plain text descriptions, GBC retains the flexibility found in natural language,\nbut can also encode hierarchical information in its edges. We demonstrate that\nGBC can be produced automatically, using off-the-shelf multimodal LLMs and\nopen-vocabulary detection models, by building a new dataset, GBC10M, gathering\nGBC annotations for about 10M images of the CC12M dataset. We use GBC10M\nto showcase the wealth of node captions uncovered by GBC, as measured with\nCLIP training. We show that using GBC nodes' annotations-notably those stored\nin composition and relation nodes-results in significant performance boost on\ndownstream models when compared to other dataset formats. To further explore\nthe opportunities provided by GBC, we also propose a new attention mechanism\nthat can leverage the entire GBC graph, with encouraging experimental results\nthat show the extra benefits of incorporating the graph structure. Our datasets are\nreleased at https://huggingface.co/graph-based-captions.", "sections": [{"title": "1 Introduction", "content": "The availability of huge paired image/caption datasets has revolutionized our ability to produce joint\nvision-language embedding, paving the way for tasks like efficient caption-guided image generation\nwithin powerful multimodal foundation models [2, 36, 38, 50]. The quality and granularity of\nthese datasets plays, therefore, a crucial role. While quality can be addressed by filtering out\ndata [17, 22, 53] or, inversely, by improving caption quality through recaptioning [14, 16, 35, 45],"}, {"title": "2 Related works", "content": "In this section, we discuss related works on vision-language datasets. We refer the readers to\nAppendix A for works that are specific to CLIP [50] training.\nVision-language datasets. First vision-language datasets were manually built using human anno-\ntations, such as Flickr30k [69], COCO [41] and Visual Genome [33]. This yielded annotations of\nhigh quality, but unfortunately of short length, and in limited amounts (with no dataset containing\nmore than 130k images). Several studies have then demonstrated the benefits of using larger scale\ndatasets obtained by crawling the web, such as YFCC100M [59], RedCaps [13], or Wikipedia-based\nimage-text dataset (WIT) [57]. The quality of these data became a concern when it was noticed that in\nsome situations the caption was only loosely related (or not related at all) with the image, which can\nbe detrimental to the overall performance [52]. This motivated researchers to use automatic filtering\nprocedures to select higher-quality data samples, like in Localized Narratives [48] or Conceptual\nCaptions (CC3M) [56], and its successor CC12M [6]. These efforts have reached billion scale with\nLAION-5B [53], and LAION-CAT [49]. In a similar vein, Meta-CLIP [68] reproduces the processing\nof the seminal CLIP paper [50] on a subset of the Common Crawl dataset, SemDeDup [1] relies on"}, {"title": "3 Improving image annotations with graph-based captioning", "content": "We introduce in this section our new captioning format to represent an image, explain how we can\nuse any off-the-shelf multimodal large language model (MLLM) and open-vocabulary detection\nmodel to obtain such captions, and briefly describe the two datasets GBC1M, and GBC10M that we\nconstruct following the proposed workflow. Additional details about the data preparation process and\nthe datasets can be found in the Appendices B and C."}, {"title": "3.1 Representing an image with graph-based captions", "content": "To encode the structured information contained in an image, we propose to represent each image\nas a directed acyclic graph (DAG), denoted as G = (V,E). Each node of the graph v \u2208 Vis\nassociated with a bounding box. Starting with the root node, which corresponds to the entire image\n(image node), other nodes can either hold a set of objects (composition node and relation node),\nor a single object in the image (entity node). Moreover, to benefit from the expressive power of\nnatural language descriptions and to ensure smooth integration of our annotations into the existing\necosystems of methods that rely primarily on image-text pairs, we label each node v with a set of\ncaptions Cu = {C1,..., Cnv}."}, {"title": "3.2 GBC dataset construction workflow", "content": "We show how to produce GBC annotations automatically, using any pre-trained MLLM and open-\nvocabulary detection model. This results in a workflow that is comparable, in compute time and\ncomplexity, to that of other widespread recaptioning approaches. At a high level, we use a MLLM\nmodel to provide captions and identify potential entity nodes, followed by a detection model to\nprovide bounding box coordinates for these entities.\nData annotation. Our overall process to annotate a single image is shown in Figure 2. To account\nfor the different types of nodes, we design four query templates as listed below:\n\u2022 Image query: We ask the model to provide detailed caption for the image, identify prominent\nelements, and summarize the long caption with a concise one that contains all these elements. The\nidentified elements are then passed to the detection model to obtain the bounding boxes.\n\u2022 Entity query: For each bounding box, we crop out the region and ask the model whether a\nspecific object appears in the cropped image. Moreover, we also ask the model to describe the\nobject and identify prominent elements of the object when it is present. The identified elements\nare again passed to detection models for detection.\n\u2022 Composition query: In the case where multiple bounding boxes are returned for a single type of\nobject, we ask the model to describe the composition of these objects with an annotated image.\n\u2022 Relation query: For image or entity nodes with more than two children, we ask the model to\ndescribe the relations between its children."}, {"title": "3.3 GBC1M and GBC10M", "content": "Following the process outlined in Section 3.2, we annotate the CC12M dataset [6] with graph-\nbased captions using LLaVA-1.6 [42, 43] as the MLLM and Yolo-World [9] as the open-vocabulary\ndetection model. Specifically, we construct two sets of annotations: GBC1M for a subset of around\n1M of images, with all the queries performed with the Yi-34B version of LLaVA-1.6, and GBC10M\nfor a subset of around 10M of images, with LLaVA-1.6 Yi-34B for image and composition queries,\nand LLaVA-1.6 Mistral-7B for entity and relation queries.\nWe provide statistics of the above two datasets in Table 1. We note that these two datasets have\nvery similar per-image statistics, with the number of words being the only exception, as LLaVA-1.6\nYi-34B tends to provide longer descriptions than LLaVA-1.6 Mistral-7B. Moreover, our datasets use\nan average number of around 500 words to describe each image. This is comparable to other dataset\nwith rich annotations such as DCI (1111 words/img) [61] and DOCCI (136 words/img) [46]. We also\ncompute the CLIP scores between the captions and their corresponding regions using the DFN-5B\nCLIP model [17], and we report their distribution for the GBC10M dataset in Figure 3. We note\nthat the original CC12M caption achieves the highest CLIP scores, followed by the short synthetic\ncaption for the entire image. This can be explained by the fact that in these two cases, the involved\nimage-caption pairs more closely align with the training data of standard CLIP models."}, {"title": "4 Encoding GBC via structure-aware hierarchical attention", "content": "Alongside many ways to leverage GBC annotations, as we shall present in Section 5, we propose a\nsimple text encoder architecture to incorporate structural information encoded in GBC graph along\nwith node captions. Specifically, we present structure-aware hierarchical attention (SAHA) block\nwhich treats each caption as an individual sample, and introduces an additional cross-attention layer\nthat enforces the captions to attend to their children.\nFormally, we consider a caption graph GC = (C,EC) with vertices C = \u22c3v\u2208V Cu and edges\nECCC \u00d7 C such that (C', C') \u2208 & if and only if C\u2208 C\u00b2, C' \u2208 C\u00ba, e = (u, v) \u2208 E, and the label\nLe is included within the caption C. In words, each vertex in the graph represents a caption from a\nnode of the original graph and there is an edge from one caption to another only if the second caption\ndescribes part of the first caption. After tokenization of the captions, we can map the edge labels to a"}, {"title": "5 Experiments", "content": "We present in this section a comprehensive set of experiments to benchmark different image annotation\nschemes. Specifically, using CLIP model training as the main task, we show that GBC annotations can\nbring improvements on a range of benchmarks across classification, retrieval, and dense prediction\ntasks, compared to existing annotation schemes (Section 5.3). On retrieval tasks, we demonstrate how\nGBC allows one to encode denser, more descriptive textual information to better represent images\nas shown by the performance gain compared to existing annotation formats (Section 5.4). Missing\nexperimental details are provided in Appendix E."}, {"title": "5.1 Annotation formats", "content": "We outline below the different types of image annotations that are considered in our experiments,\neach providing different opportunities to leverage information from the image.\nShort caption. Each image is paired with a short caption, as in common image-text datasets.\nLong caption. One can improve image description using a longer caption. We use long captions in\nour dataset. They are of 110 words on average, as compared to short captions, of only 28 words on\naverage. We extend the context length of text encoders in CLIP models from 77 to 512 for this setup."}, {"title": "6 Conclusion", "content": "We propose graph-based captioning (GBC) as a new image-text annotation format, and curated\nGBC1M and GBC10M datasets. Grounding on CLIP model training, we propose various baseline\nmethods to utilize the GBC datasets. Via our experiments, we demonstrate that training with GBC\nleads to improvements in various benchmarks compared to models derived from traditional annotation\nformats. This suggests that GBC provides richer textual information than existing annotation schemes\nand could hence be a valuable foundation for developing more advanced vision-langauge models\nacross various applications."}]}