{"title": "Graph-Based Captioning: Enhancing Visual\nDescriptions by Interconnecting Region Captions", "authors": ["Yu-Guan Hsieh", "Louis B\u00e9thune", "Chun-Liang Li", "Cheng-Yu Hsieh", "Hadi Pour Ansari", "Ranjay Krishna", "Shih-Ying Yeh", "Pavan Kumar Anasosalu Vasu", "Oncel Tuzel", "Marco Cuturi"], "abstract": "Humans describe complex scenes with compositionality, using simple text descrip-\ntions enriched with links and relationships. While vision-language research has\naimed to develop models with compositional understanding capabilities, this is\nnot reflected yet in existing datasets which, for the most part, still use plain text to\ndescribe images. In this work, we propose a new annotation strategy, graph-based\ncaptioning (GBC) that describes an image using a labelled graph structure, with\nnodes of various types. The nodes in GBC are created using, in a first stage, object\ndetection and dense captioning tools nested recursively to uncover and describe\nentity nodes, further linked together in a second stage by highlighting, using new\ntypes of nodes, compositions and relations among entities. Since all GBC nodes\nhold plain text descriptions, GBC retains the flexibility found in natural language,\nbut can also encode hierarchical information in its edges. We demonstrate that\nGBC can be produced automatically, using off-the-shelf multimodal LLMs and\nopen-vocabulary detection models, by building a new dataset, GBC10M, gathering\nGBC annotations for about 10M images of the CC12M dataset. We use GBC10M\nto showcase the wealth of node captions uncovered by GBC, as measured with\nCLIP training. We show that using GBC nodes' annotations-notably those stored\nin composition and relation nodes-results in significant performance boost on\ndownstream models when compared to other dataset formats. To further explore\nthe opportunities provided by GBC, we also propose a new attention mechanism\nthat can leverage the entire GBC graph, with encouraging experimental results\nthat show the extra benefits of incorporating the graph structure. Our datasets are\nreleased at https://huggingface.co/graph-based-captions.", "sections": [{"title": "1 Introduction", "content": "The availability of huge paired image/caption datasets has revolutionized our ability to produce joint\nvision-language embedding, paving the way for tasks like efficient caption-guided image generation\nwithin powerful multimodal foundation models [2, 36, 38, 50]. The quality and granularity of\nthese datasets plays, therefore, a crucial role. While quality can be addressed by filtering out\ndata [17, 22, 53] or, inversely, by improving caption quality through recaptioning [14, 16, 35, 45],"}, {"title": "2 Related works", "content": "In this section, we discuss related works on vision-language datasets. We refer the readers to\nAppendix A for works that are specific to CLIP [50] training.\nVision-language datasets. First vision-language datasets were manually built using human anno-\ntations, such as Flickr30k [69], COCO [41] and Visual Genome [33]. This yielded annotations of\nhigh quality, but unfortunately of short length, and in limited amounts (with no dataset containing\nmore than 130k images). Several studies have then demonstrated the benefits of using larger scale\ndatasets obtained by crawling the web, such as YFCC100M [59], RedCaps [13], or Wikipedia-based\nimage-text dataset (WIT) [57]. The quality of these data became a concern when it was noticed that in\nsome situations the caption was only loosely related (or not related at all) with the image, which can\nbe detrimental to the overall performance [52]. This motivated researchers to use automatic filtering\nprocedures to select higher-quality data samples, like in Localized Narratives [48] or Conceptual\nCaptions (CC3M) [56], and its successor CC12M [6]. These efforts have reached billion scale with\nLAION-5B [53], and LAION-CAT [49]. In a similar vein, Meta-CLIP [68] reproduces the processing\nof the seminal CLIP paper [50] on a subset of the Common Crawl dataset, SemDeDup [1] relies on"}, {"title": "3 Improving image annotations with graph-based captioning", "content": "We introduce in this section our new captioning format to represent an image, explain how we can\nuse any off-the-shelf multimodal large language model (MLLM) and open-vocabulary detection\nmodel to obtain such captions, and briefly describe the two datasets GBC1M, and GBC10M that we\nconstruct following the proposed workflow. Additional details about the data preparation process and\nthe datasets can be found in the Appendices B and C."}, {"title": "3.1 Representing an image with graph-based captions", "content": "To encode the structured information contained in an image, we propose to represent each image\nas a directed acyclic graph (DAG), denoted as G = (V,E). Each node of the graph v \u2208 V is\nassociated with a bounding box. Starting with the root node, which corresponds to the entire image\n(image node), other nodes can either hold a set of objects (composition node and relation node),\nor a single object in the image (entity node). Moreover, to benefit from the expressive power of\nnatural language descriptions and to ensure smooth integration of our annotations into the existing\necosystems of methods that rely primarily on image-text pairs, we label each node v with a set of\ncaptions C_v = {C_1,..., C_{n_v}}."}, {"title": "3.2 GBC dataset construction workflow", "content": "We show how to produce GBC annotations automatically, using any pre-trained MLLM and open-\nvocabulary detection model. This results in a workflow that is comparable, in compute time and\ncomplexity, to that of other widespread recaptioning approaches. At a high level, we use a MLLM\nmodel to provide captions and identify potential entity nodes, followed by a detection model to\nprovide bounding box coordinates for these entities.\nData annotation. Our overall process to annotate a single image is shown in Figure 2. To account\nfor the different types of nodes, we design four query templates as listed below:\n\u2022 Image query: We ask the model to provide detailed caption for the image, identify prominent\nelements, and summarize the long caption with a concise one that contains all these elements. The\nidentified elements are then passed to the detection model to obtain the bounding boxes.\n\u2022 Entity query: For each bounding box, we crop out the region and ask the model whether a\nspecific object appears in the cropped image. Moreover, we also ask the model to describe the\nobject and identify prominent elements of the object when it is present. The identified elements\nare again passed to detection models for detection.\n\u2022 Composition query: In the case where multiple bounding boxes are returned for a single type of\nobject, we ask the model to describe the composition of these objects with an annotated image.\n\u2022 Relation query: For image or entity nodes with more than two children, we ask the model to\ndescribe the relations between its children.\nProvided that there is no guarantee that all the detected objects would end up as a node in the\ngraph-consider the case where the MLLM says that the object is not present or just fails to reply in"}, {"title": "3.3 GBC1M and GBC10M", "content": "Following the process outlined in Section 3.2, we annotate the CC12M dataset [6] with graph-\nbased captions using LLaVA-1.6 [42, 43] as the MLLM and Yolo-World [9] as the open-vocabulary\ndetection model. Specifically, we construct two sets of annotations: GBC1M for a subset of around\n1M of images, with all the queries performed with the Yi-34B version of LLaVA-1.6, and GBC10M\nfor a subset of around 10M of images, with LLaVA-1.6 Yi-34B for image and composition queries,\nand LLaVA-1.6 Mistral-7B for entity and relation queries."}, {"title": "4 Encoding GBC via structure-aware hierarchical attention", "content": "Alongside many ways to leverage GBC annotations, as we shall present in Section 5, we propose a\nsimple text encoder architecture to incorporate structural information encoded in GBC graph along\nwith node captions. Specifically, we present structure-aware hierarchical attention (SAHA) block\nwhich treats each caption as an individual sample, and introduces an additional cross-attention layer\nthat enforces the captions to attend to their children.\nFormally, we consider a caption graph G_C = (C,E_C) with vertices C = \\bigcup_{v \\in V} C_v and edges\nE_C \\subset C \\times C such that (C,C') \\in E_C if and only if C \\in C^u, C' \\in C^v, e = (u, v) \\in E, and the label\nL_e is included within the caption C'. In words, each vertex in the graph represents a caption from a\nnode of the original graph and there is an edge from one caption to another only if the second caption\ndescribes part of the first caption. After tokenization of the captions, we can map the edge labels to a"}, {"title": "5 Experiments", "content": "We present in this section a comprehensive set of experiments to benchmark different image annotation\nschemes. Specifically, using CLIP model training as the main task, we show that GBC annotations can\nbring improvements on a range of benchmarks across classification, retrieval, and dense prediction\ntasks, compared to existing annotation schemes (Section 5.3). On retrieval tasks, we demonstrate how\nGBC allows one to encode denser, more descriptive textual information to better represent images\nas shown by the performance gain compared to existing annotation formats (Section 5.4). Missing\nexperimental details are provided in Appendix E."}, {"title": "5.1 Annotation formats", "content": "We outline below the different types of image annotations that are considered in our experiments,\neach providing different opportunities to leverage information from the image.\nShort caption. Each image is paired with a short caption, as in common image-text datasets.\nLong caption. One can improve image description using a longer caption. We use long captions in\nour dataset. They are of 110 words on average, as compared to short captions, of only 28 words on\naverage. We extend the context length of text encoders in CLIP models from 77 to 512 for this setup."}, {"title": "5.2 Experimental setup", "content": "We perform CLIP training on our GBC10M dataset, while leaving out 10,151 samples as the test set.\nFollowing common practice, we use the CLIP score computed by a pre-trained CLIP model [17] to\nfilter our training set, discarding the 5% of captions with the lowest scores for each type. In addition,\nwe retain the original CC12M captions associated with each image. Specifically, in all setups, both\nthe original caption and the short synthetic caption are consistently used as positive texts for the\nimage during training. This prevents the severe distribution shifts that could occur from using only\nlong or region captions when evaluating on standard benchmarks.\nObjective. To pair an image with multiple captions in training CLIP models, we adopt a multiple-\npositive contrastive loss in the spirit of LaCLIP [16] and DAC [14]. Briefly speaking, compared to\nstandard CLIP objective, the multiple-positive loss sums over the loss on each positive captions of an\nimage while all the captions from the images in the same batch are used in the normalization term.\nModel and hyperparmeters. We use the standard CLIP ViT-B/16 model, with the only difference\nof longer context length of text encoder for long caption and GBC-concat, and a replacement of the\nvanilla transformer block by the SAHA block in text encoder for GBC-graph. We fix the global batch\nsize (i.e., number of images in each batch) to 4,096 for all the methods. The models are trained for\n45,000 steps with AdamW and cosine scheduler at a learning rate of 10^{-3}. This roughly correspond\nto 20 epochs of training. We evaluate at the EMA checkpoint at epoch 10, as we observe that further\ntraining provides little to no improvement in performance across the benchmarks."}, {"title": "5.3 Evaluations on existing benchmarks", "content": "We compare the CLIP models derived from different annotation schemes on an array of evaluation\nbenchmarks, including: Flickr-1k [47] and MSCOCO-5k [41] for zero-shot retrieval, ImageNet [51]\nfor zero-shot classification, SugarCrepe [27] for compositional understanding evaluation, and\nADE20k [76] for semantic segmentation that measures models' dense prediction performances.\nTable 2 illustrates our results, from which we draw the following two key insights.\nGBC annotation leads to clear performance gains by encoding relational information. Table 2\ndemonstrates that training models with more detailed textual information, such as long captions or\nregion captions, consistently enhances downstream performance, particularly in retrieval tasks and\ndense prediction. However, the most significant improvements are seen with GBC-captions, which\naugment traditional region captions with relational and compositional descriptions. Given that the\nGBC workflow is uniquely positioned to provide these, this result demonstrates the soundness of\nGBC, capturing valuable insights not present in conventional captions.\nHow the captions are used matters. Compared to GBC-captions, the improvements achieved by\nGBC-concat and GBC-graph on these benchmarks are of a smaller margin. This indicates that the\nway GBC annotations is used significantly impacts performance. Specifically, this worse performance\nis likely due to a mismatch between training and evaluation. For instance, the graph information\nthat would benefit GBC-graph most is not provided in any of these benchmarks. We address this\ndiscrepancy below."}, {"title": "5.4 Evaluation on GBC test set", "content": "To assess the effectiveness of different annotation formats, we provide the model with these annota-\ntions at test time. Annotations that better describe the images should, ideally, result in better retrieval\nperformance when they are used. Specifically, we use our own test set and consider performing\nretrieval with the various types of annotations presented in Section 5.1. Note, however, that when\nusing region captions or GBC-captions, no single text embedding can naturally encompass all the\nrelevant information. To address this limitation, we perform retrieval based on the average CLIP\nscore between the image embedding and the text embeddings of the provided captions in this setup.\nWe report our results in Table 3, where the rows correspond to the annotations at training time and\nthe columns correspond to the annotation at test time. Unsurprisingly, we see a strong tendency that\nwhen a model is trained with a certain annotation format, it performs the best when we use the same\nformat for retrieval. Among the few exceptions, we note that models trained to pair with shorter\ncaptions may have better performance when concatenation of short captions is provided at test time.\nThis leads us to the following two observations.\nDenser textual information improves retrieval performances. The table clearly shows that\ntraining with richer annotations such as long captions, GBC-concat, or GBC-graph-enhances\nretrieval performance. This improvement suggests that these methods provide a more effective\nrepresentation of the images. Specifically, GBC-graph yields the best performance, indicating that\nthe proposed GBC format consists in a viable alternative to the commonly used detailed captions.\nSimple augmentation during training does not allow to exploit additional information when\navailable. Our observations from Section 5.3 show that treating all captions as independent positives"}, {"title": "5.5 Ablation studies", "content": "In addition to our main experiments, we have conducted extensive ablation studies on both the training\nand evaluation of our models. We present two of them here and defer the remaining to Appendix F.\nRetrieval with multiple captions using maximum CLIP score. An alternative to the mean CLIP\nscore we considered in Table 3 is to take the maximum, for which we report the results in Table 4.\nCompared to taking the average, using the maximum is more robust to low CLIP scores, and thus\ngives better results when the model is not trained to match the image with all local captions, as with\nShort, Long caption, and GBC-concat. Nonetheless, despite these differences, the overall retrieval\nperformance still significantly lags behind that achieved using a single caption.\nRetrieval with long captions. To complement the results presented in Table 2, we evaluate the\nretrieval performance of our extended context models on datasets with dense annotations. We focus\nspecifically on ShareGPT4V [8], which offers GPT-style detailed captions closely resembling those\nobtained from LLaVA, and DCI [61], containing human-annotated detailed and region captions. The\nlatter allows us to perform retrieval using either detailed captions or concatenated short captions, as\nwe did in Section 5.4. Our results shown in Table 5 demonstrates that the close caption distribution\nwith ShareGPT4V effectively enables strong retrieval results for models trained on our long captions.\nHowever, potentially due to the distribution shift, all the models perform badly on DCI retrieval\nwith long captions. In this setup, using concatenated captions for training and retrieval significantly\noutperformed other baselines, indicating the broader benefit of the concatenation approach."}, {"title": "6 Conclusion", "content": "We propose graph-based captioning (GBC) as a new image-text annotation format, and curated\nGBC1M and GBC10M datasets. Grounding on CLIP model training, we propose various baseline\nmethods to utilize the GBC datasets. Via our experiments, we demonstrate that training with GBC\nleads to improvements in various benchmarks compared to models derived from traditional annotation\nformats. This suggests that GBC provides richer textual information than existing annotation schemes\nand could hence be a valuable foundation for developing more advanced vision-langauge models\nacross various applications."}, {"title": "A Related works, limitations, and societal impact", "content": "This appendix delves deeper into the broader context of our study, examines additional related works,\ndiscusses the limitations of our methodologies, and explores its potential societal impacts."}, {"title": "A.1 Additional related works", "content": "In this section, we include works related to CLIP training.\nCLIP with recaptioning. CLIP [50] is a seminal vision-language model that utilizes text and image\nencoders to generate joint latent representations. While there is an extensive body of literature on CLIP\ntraining-ranging from modifications in the objective [37, 70], data augmentation techniques [19, 40],\nto training procedures [58, 72]\u2014it is impossible to cover all developments comprehensively here.\nAmong these, particularly relevant to our work is the recent trend that highlights the benefits of\nenhancing caption quality through dedicated models. For instance, VeCLIP [35] enriches image\nalt-text with outputs from LLaVA, while similar recaptioning strategies have also been explored\nby Doveh et al. [14], Nguyen et al. [45], and Vasu et al. [64]. On the other hand, LaCLIP [16]\nemploys LLaMA [60] to rewrite captions. Going further, SynthCLIP [24] leverages a dataset with\nentirely generated captions and images for CLIP training.\nCLIP with additional annotations. There has been a plethora of research on training CLIP models\nwith diverse annotations such as long captions, region captions, and scene graphs. As for long\ncaptions, DreamLIP [74] proposes to sample sub-captions from the long description to construct\nmultiple positive pairs, while Long-CLIP [73] addresses CLIP's 77-token limitation by modifying\nthe positional encoding to accommodate longer text sequences during fine-tuning. Meanwhile,\nregion annotations with varying granularity have been considered by works including GLIP [39],\nX-VLM [71], and RegionCLIP [75]. Their objectives match features of image crops to their specific\ndescriptions. Efforts that aim to improve CLIP training with the help of scene graphs include\nCLIP-SGVL [26] and Structure-CLIP [28]. The former integrates scene graphs to define additional\nobjective for image encoder, while the later uses scene graphs to guide the generation of negative\ncaptions, and to enrich the text encoder with additional contextual information."}, {"title": "A.2 Limitations and perspectives", "content": "We discuss below the limitations of our works from three different perspectives, the procedure and\nformat, the datasets, and the experiments. These limitations also naturally point to several future\ndirections that are to be explored."}, {"title": "A.2.1 Limitation concerning the GBC procedure and format", "content": "While GBC remains a versatile high-level annotation format that in principle applies to any image,\nits design is inherently tied to the coarse-to-fine and compositional nature of natural images. This\ndesign orientation means that GBC is not necessarily the most suitable for certain types of images\nsuch as scientific imagery, homogeneous patterns, or abstract art. Specifically, scientific imagery\noften requires annotations that convey precise, quantifiable data rather than relational or descriptive\ntext. This limitation highlights the need for tailored approaches to different visual content categories\nto address their unique characteristics."}, {"title": "A.2.2 Limitation concerning the GBC datasets", "content": "Our datasets are curated with the help of LLaVA and Yolo-World, and hence inherit their limitations.\nThis includes but is not limited to, the bias and hallucination from LLaVA captioning, incorrectly\nidentified objects from Yolo-World, and the inability of Yolo-World to recognize certain object\ncategory (see Appendix C.3 for concrete examples). Moreover, our approach mainly distinguishes\nbetween objects of the same type via composition nodes. Yet, we believe that there is a more effective\nstrategy than merely assigning numbers to these objects."}, {"title": "A.2.3 Limitation concerning our experiments", "content": "Our experiments, which focus on CLIP training and retrieval tasks, demonstrate the benefits of our\nmethod and dataset from several perspectives. Nonetheless, we believe this only represents a small\npart of what this new dataset and annotation method can offer. Moving forward, we commit to"}, {"title": "A.3 Societal impact", "content": "Our paper introduces the GBC datasets and procedure, both aimed at advancing the development\nof multimodal models. Specifically, the structured approach of GBC, designed to provide detailed\ndescriptions, may help overcome representational biases inherent in existing captioning pipelines,\noffering more accurate descriptions of images. The potential benefits of these advancements extend\nacross a range of applications, such as assistive technologies and scientific research. However,\nalongside these benefits, there are challenges including the potential spread of misinformation and\nconcerns about privacy. A comprehensive discussion of these broader societal impacts, both positive\nand negative, extends beyond the immediate focus of our methodological study."}, {"title": "B Dataset construction", "content": "In this appendix, we provide all the missing details about our dataset construction process that are not\nmentioned in Sections 3.2 and 3.3."}, {"title": "B.1 Query templates", "content": "To make the MLLM models fulfill the tasks described in Section 3.2, we perform COT prompting [66]\nwith few-shot examples. The four templates for our queries are shown in Figure 5 to 11. We make\nthe following remarks concerning the design of our prompts.\nPrompt structure. We craft these prompts with the help of ChatGPT, which results in prompts that\nmight be more complicated than necessary. Meanwhile, we did notice that the inclusion of few-shot\nexamples is crucial for the model to adhere to the required output formats. Given that using always\nthe same few-shot examples might significantly bias the model's output, it could be beneficial to\nrandomly retrieve examples from a diverse pool for each query, but we did not pursue this exploration.\n[Single] and [Multiple] annotations. Since a detection model could output multiple candidate\nbounding boxes for an input text, we ask the MLLM to annotate each identified element with\neither [single] or [multiple]. We then proceed with slightly different algorithms in the two cases,\nto encourage the selection of either only one, or multiple bounding boxes. In particular, we use\nrespectively an NMS threshold of 0.05 and 0.2 for objects labeled with [single] and [multiple].\nHowever, these labels do not necessarily dictate the final count of bounding boxes; multiple boxes\nmay still be selected for items labeled [single], and vice versa.\nDynamically filled-in elements. To ensure that the response of the MLLM is relevant, the prompts\nare dynamic and reflect the content of the current image (the image query being the only exception).\nSuch information comes from previous queries and can be naturally retrieved for different queries.\nThe only nonobvious part is the hard coded hints for composition queries, which we explain below.\nHard coded hints for composition queries. After numerous attempts, we observe that LLaVA-1.6\nstruggles with accurately describing the composition of multiple objects in a scene, even when\nthese objects are annotated with bounding boxes. To overcome this limitation, we guide the models\nwith hints generated programmatically using a set of predefined rules. Specifically, we begin by\nconstructing a Euclidean minimum spanning tree based on the centers of the bounding boxes. We\nthen select a random node as the root and perform a Depth-First Search (DFS) on the tree. During\nthis search, we interleave descriptions of the edges, which detail the geometric relations between\ntwo objects based on the positions of their bounding boxes, with node descriptions. These node\ndescriptions are added when an object is located at a particular extremity of the composition, such as\nthe rightmost or top-left position."}, {"title": "B.2 Text classifiers", "content": "Both of our text classifiers are trained for binary classification using logistic loss. To determine\nwhether a piece of text is suitable for object detection, we utilize a single linear layer added on top"}, {"title": "B.3 Other details for data annotation", "content": "We incorporate LLaVA-1.6 into our pipeline using 11ama . cpp. Moreover, to speed up the annotation\nprocess, we utilize models quantized at different precision levels: the vision encoders at 6-bit\nprecision, the LLM component of LLaVA-1.6 Mistral-7B at 5-bit precision, and the LLM component\nof LLaVA-1.6 Yi-34B at 3-bit precision. We use the default hyperparameters for inference except\nfor a temperature of 0.1 and context window of size of 5952 (note that LLaVA-1.6 can use up to 2880\nimage tokens). We discard any responses that do not comply with our required format.\nAs for the object detection model, we use YOLO-Worldv2-X trained with input resolution of\n640 \u00d7 640. We set the confidence threshold to 0.05 and retain a maximum of six bounding boxes for\neach input text, selecting those with the highest confidence scores. We exclude any region whose size\nis smaller than 5,000. To prevent repetitive descriptions of the same element, we keep only those\nbounding boxes that occupy less than 80% of the current image region for detections arising from\nentity queries. Regarding node merging, we consider two bounding boxes to be overlapping if their\nintersection occupies more than 85% of the area of each bounding box involved."}, {"title": "B.4 Computation cost", "content": "We list below the major computation cost of our data preparation process.\n\u2022 GBC1M: With our processing pipeline, it takes an average of around 3 minutes to annotate each\nimage on an A100 80G when all the queries are performed with LLaVA-1.6 Yi-34B. As a result,\nannotating 1 million images took us around 6 days with 300 A100 80Gs.\n\u2022 GBC12M: The average annotation time per image on an A100 80G is improved to 1 minute when\nrelation and entity queries are performed with LLaVA-1.6 Mistral-7B. This process is about twice\nslower on a V100 32G. In this regard, our GBC12M dataset was compiled in roughly 6 days using\n500 A100 80Gs and 1,000 V100 32Gs."}, {"title": "C Dataset information", "content": "In this appendix, we provide information about dataset release, dataset statistics, and visualizations\nof a few examples from our GBC10M dataset."}, {"title": "C.1 Data release and licensing", "content": "Our datasets are available at https://huggingface.co/graph-based-captions, released under\nthe Apple Sample Code License. Following CC12M, we include URLs to images along with\ncaptions generated through our GBC procedure, all stored in JSON lines format. Comprehensive\ndocumentation including a dataset card and croissant metadata is provided in the data repository. We\nare committed to maintaining the dataset to ensure its long-term public accessibility.\nPersonal identifiable information and offensive content. Our dataset comprises only captions\ngenerated by MLLM models (LLaVA 1.6 Yi-34B and LLaVA 1.6 Mistral-7B), which were trained on\ncarefully curated data. The images, sourced from CC12M, are generally free from offensive content.\nIn particular, CC12M is the result of a filtering operation involving adult content detection on images\nand their captions. While CC12M images may include human faces, we do not host the images\ndirectly; only the URLs are provided. Additionally, we conduct toxicity check with Detoxify [25]\non a subset of examples in GBC dataset and find no harmful contents. While it was not possible to\nmanually examine all the samples produced by GBC pipeline, we believe that the protective measures\nof the source dataset and model are sufficient to avoid both harmful content, and privacy leakages."}, {"title": "C.2 Dataset statistics", "content": "In this section, we provide statistical insights into the GBC1M and GBC10M datasets. In particular,\nwe zoom in on the statistics at image, vertex, edge, and caption levels, and present distributions of\nseveral key metrics including for example caption length, region size, and CLIP score. Since most of\nthese metrics exhibit long-tailed distributions, we often group excessively large values into a single\nhistogram bin for better visualization."}, {"title": "C.2.1 Image and graph statistics", "content": "We first look at the sizes of the images and of the annotation graphs, i.e., the numbers of vertices and\nedges in these graphs and their diameters (which is measured as the length of the longest path in a\ndirected graph). The distributions of these metrics are shown in Figures 12 and 13. We see that the\nimage size has a very long-tailed distribution, with the majority of images having around 786 \u00d7 786\npixels. Conversely, the distributions of graph diameters are more similar to that of a Poisson or a\nbinomial distribution, with most of the graphs having a diameter between 3 and 6. Finally, as one\ncould expect, the numbers of vertices and edges share quite similar distributions.\nWhile we expect the size of a graph to reflect the inherent complexity of an image, we acknowledge\nthat our annotations are influenced by the biases of the used models. In particular, we observe that our\nannotation process tends to yield larger graph for natural images compared to other types of images\nsuch as artworks or graphic designs."}, {"title": "C.2.2 Vertex statistics", "content": "We have shown previously that our datasets contain an average of 12 vertices per graph. This\ntranslates to 11 regions per image after excluding the root node that represents the entire image. We\ncompare this number with several other vision-language datasets with region-based annotations in\nTable 6. As one can see, this number aligns well with many of these datasets, particularly those used\nfor detection, such as COCO and Object365. However, it lags behind compared to Visual Genome\nand more recent datasets with dense annotations, such as AS-1B and DCI. We believe this discrepancy\ncan be attributed to both the top-down design of our annotation process, which tends to overlook less\nsignificant components of the images, and the limitations of the detection model used. Notably, both\nAS-1B and DCI utilize Segment Anything [32] to identify regions of interest. Segment Anything is"}, {"title": "C.2.3 Edge statistics", "content": "Our datasets feature an average of 22 edges per graph. We analyze the origins of these edges in\nFigure 18, which shows their distributions across different types of source vertices. The figure\nindicates that the image node is responsible for a large proportion of these edges, suggesting that\nmany of the entities that we identify directly come from the image caption. This is natural provided\nthat an image often contains many objects, while it is less common to need further decomposition of\na single object for detailed description. Besides this, these figures also indicate the number of entities\nthat are involved in our composition and relation descriptions. Notably, we see that most of these\ndescriptions only contain 2 or 3 objects, with few of them involving more than 4 objects. In contrast,\nwe observe a relatively large number of entity nodes with 4 outgoing edges, and we believe this can\nbe attributed to the bias caused by the few-shot examples provided in our query template.\nWe also provide analysis for the edge labels. These edge labels should represent the objects that are\nassociated to their respective target vertices. In particular, during our annotation process, we use these\nlabels as input of the detection model to obtain the bounding boxes of the entity nodes. In Figures 21\nand 23, we plot the distributions of the numbers of words and tokens contained in the edge labels. As\nexpected, most of the time we use only 1 or 2 words to represent the entities.\nWe next study the content of these labels. To this end, we plot the distribution of (i) the 20 most\ncommon edge labels at the in-edges of the entity nodes, reflecting the content of these entity nodes,"}, {"title": "C.2.4 Caption statistics", "content": "For statistics at the caption level, we first complete Table 1 and Figure3 by providing distribution of\nCLIP scores on the two datasets in Figure 19, and distribution of number of captions, words, and\ntokens per image in Figures 24 and 25. In particular, the significant variation in CLIP score distribu-\ntions across different caption types motivates our decision to perform CLIP-filtering independently\nfor each type, as mentioned in Section 5.2.\nGoing further, we report the average number of words and tokens per caption across different types of\ncaptions in Figure 27, 29, and Table 7. We can see that except for the detailed image captions, most\ncaptions indeed contain fewer than 77 tokens. Table 7 additionally reveals that we have near 2.5 times\nmore region captions (i.e., entity and multi-entity captions) than the total of relation and composition\ncaptions. However, as we have seen in Section 5.3 and will further ablate in Appendix F.3, these\nrelation and composition captions, unique to our dataset, are crucial for the performance improvement\nthat we observe across different evaluations.\nWe conclude this part by showing the distribution of the 20 most common words and trigrams that\nappear in our captions, with stop words removed when considering the word distributions. The\nfrequent appearances of colors among the top words again align with the distribution reported in\nVisual Genome [33, Fig. 24]. In addition, phrases like \u201cappears to be\u201d, \u201cpossibly\u201d, and \u201cthe image\ncaptures\" that commonly appear in our data, reflect LLaVA's use of GPT-generated data during\ninstruction tuning."}, {"title": "D Algorithm details", "content": "This appendix provides missing details about our architecture and training objective."}, {"title": "D.1 Structure-aware cross attention", "content": "We define below mathematically the SACA layer. For this, we denote by $\\mathcal{N}_C$ the children of\ncaption $C$ in caption graph $G_C$ and write the features of $C$ in the input of our SACA layer as\n$X_C = [x_1,...,x_n]$. Recall also that $P_e$ with $e = (C, C')$ represents the set of token positions in\nthe source caption $C$ that we map the edge label $L_e$ to. Then, the SACA layer maps each feature\nvector $x^i$ to\n$\\text{SACA}(x^i) = \\frac{\\sum_{C' \\in \\mathcal{N}_C} \\mathbb{1}_{i \\in P(C,C')} \\text{MHA}(x^i, X_C, X_{C'})}{\\text{min}(1, \\sum_{C' \\in \\mathcal{N}_C} \\mathbb{1}_{i \\in P(C,C')})},$ (1)\nwhere MHA implements the standard multi-head attention mechanism. Note that we average across\nthe results from all the relevant captions that describe this token, as we show in Figure 4.\nAs a side note, we highlight that with SAHA, information is only propagated from each node to its\ndirect parent within a block. Consequently, the number of blocks must exceed the depth of the $G_C$ to\nensure that information reaches the root node from all levels of the graph."}, {"title": "D.2 Multi-positive contrastive loss", "content": "To pair multiple positive captions to an image, we extend standard contrastive loss [50] into multiple-\npositive contrastive loss, as also considered in prior studies [14, 16]. Specifically, consider a batch\nof $N$ images $\\{I_i\\}_{i=1}^N$, where each image $I_i$ is associated with $M_i$ captions $\\{T_{i,j}\\}_{j=1}^{M_i}$, we utilize the\nfollowing loss function to account for multiple positive texts per image:\n$\\mathcal{L}_I = - \\frac{1}{Z} \\sum_{i=1}^N \\sum_{j=1}^{M_i} \\text{log} \\frac{S(I_i, T_{i,j})}{\\sum_{l=1}^N \\sum_{k=1,k\\neq i}^{M_i} S(I_i, T_{i,j}) + \\sum_{k=1,k\\neq i} \\sum_{l=1}^{M_i} S(I_l, T_{k,l})},$ (2)\nwhere $S(I, T) = \\text{exp}(\\text{cos}(I, T)/\\tau)$, $\\tau$ is a learnable temperature parameter, and $Z = \\sum_{i=1}^N M_i$ is a\nnormalizer. On the other hand, each caption still only has one paired image. Therefore, we use the\nstandard contrastive loss on for text-to-image alignment:\n$\\mathcal{L}_T =  \\sum_{i=1}^N \\sum_{j=1}^{M_i} \\text{log} \\frac{S(I_i, T_{i,j})}{\\sum_{k=1}^N S(I_k, T_{i,j})}.$ (3)"}, {"title": "E Experimental details", "content": "This appendix presents further details about our experiments that are omitted in Section 5."}, {"title": "E.1 Data filtering", "content": "For the computation of CLIP score, we split any caption that contains more than 77 tokens into\nindividual sentences, compute the score for each of these sentences, and compute the average of these\nscores. Then, we start by filtering out images whose short synthetic captions have CLIP scores that\nare lower than the 5% quantile. After this, we consider three filtering strategies depending on the\nannotation formats.\nLong caption. In this case, we just further filter out a portion of original captions and long captions\nwith the lowest CLIP scores (by considering the 5% quantiles from the non-filtered dataset).\nGBC. Naive CLIP filtering and tokenizer truncation could break the graph structure as some of\nthe edge labels would not appear in the captions of its source node anymore after these operations.\nWe address this issue by filtering out the captions following the reverse of a topological ordering of\nthe graph, drop a node along with its in edges when all its captions and children get filtered, and\notherwise, if necessary, add bag-of-words captions that collects edge labels from the remaining out\nedges of a node to ensure all these labels still appear in some captions of this node. Moreover, we\nsplit the captions whose length are longer than 77 tokens into concatenations of sentences that fit\nwithin this limit, and drop any caption which contains sentences that are of more than 77 tokens."}, {"title": "E.2 Dynamic batch size", "content": "Given the varying sizes of our graph, setting a fixed number of images per batch could result in\nout-of-memory errors unless we opt for a conservatively small batch size. To overcome this challenge,\nwe implement a dynamic batching strategy for the setups where the number of captions per image is\nin principle unbounded. This encompasses notably region, GBC-captions, and GBC-graph. With this\nstrategy, we ensure that the number of captions, and, in the case of GBC-graph, the number of edges,\nthat are included in each batch do not exceed a certain limit. In this regard, the batch size that we\nreport in Section 5 is actually just an upper bound on the number of images included in each batch.\nMore specifically, we set this limit based on the number of average captions/edges per image in the\nfiltered dataset. For example, for GBC-captions and GBC-graph we have in average 17.61 captions\nper graph. We thus set the limit on caption number to 18 \u00d7 64 = 1152 on each GPU (as mentioned\nin Appx. E.4, we use 64 GPUs for most of our experiments, which gives a batch size of 64 per GPU)."}, {"title": "E.3 Hyperparameters for CLIP training", "content": "We used a consistent set of hyperparameters for all model training runs, as detailed in Table 10. The\nsole exception is training with original CC12M captions, where we used a larger batch size of 8,192\nto ensure the model sees a comparable number of texts as during training with both short synthetic\nand original captions. For this specific setup with the larger batch size, we reported evaluation results\nfrom the EMA checkpoint at the end of epoch 15, for it achieving the best performance among the\nevaluated checkpoints. For GBC-graph, we drop the edges with probability 0.5 so that the model also\nlearns how to match images with short captions."}, {"title": "E.4 Computation cost", "content": "We train all CLIP models on A100-80G GPUs. As training with different annotation formats requires\nvarying size of GPU memory, we use different total numbers of GPUs to ensure the same batch size.\nSpecifically, we utilize 16 GPUs for training with Short captions, and utilize 64 GPUs for training\nwith all other annotation formats. We list the corresponding time required for training with different"}, {"title": "E.5 Evaluation details", "content": "Our evaluation uses the validation set of ImageNet-1k [51] and the test sets of Flickr30k [47] and\nMS-COCO [41]. For SugarCrepe [27] we report the average performance across all variants. As for\nShareGPT4V [76], we use a subset of size 15, 295 from ShareGPT4V-cap100k. These images were\nalso used for LLaVA training. When each image is paired with multiple captions, we only select\none of them. The evaluation setups with ADE20K and DCI are more involved, as we explain below.\nEvaluation on ADE20K. We evaluate the quality of CLIP models' image encoder for dense\nprediction tasks like image segmentation by performing full finetuning on ADE20k [76] dataset. We\nfollow the same setup as described in [62, 63] where we use a ViTDet style feature pyramid network\nwith UperNet [67] head. All models were trained using the MMSegmentation library [11]. We sweep\nthrough peak learning rate for all the results reported in the paper and the ranges are listed in Table 11.\nEvaluation on DCI. We perform text-to-image and image-to-text evaluations on DCI [61] using\neither long captions or concatenated captions. The long captions are marked as extra_caption in\nthe released DCI dataset. We filter out samples with empty long captions, resulting in a subset of\n7,602 images for evaluation with long captions. Regarding evaluation with concatenated captions, we\nleverage the full set of 7,805 images. We retain masks containing summary captions (these are masks\nwith bounding boxes larger than 224 \u00d7 224). If the human-annotated caption contains fewer than 77\ntokens and is longer than the first summary caption, we use it. Otherwise, we use the first summary\ncaption. For concatenation, we follow the Breadth-First Search (BFS) order based on the provided\ntree structure between the masks."}, {"title": "F Additional results and experiments", "content": "In this appendix, we present additional ablations that we have performed but were not presented in\nthe main paper due to space constraints."}, {"title": "F.1 Matching compute resource for training with short captions", "content": "All our models presented in Section 5 used 8 nodes for training, except for the models trained on\nshort captions, which only used 2 nodes. This raises the question of whether the performance gap\ncould be bridged by providing more computational resources to this setup. To address this, we\nspecifically considered two modifications that would naturally necessitate using more nodes for\ntraining with short captions: (i) extending the context length to 512, as done for training with Long\nand GBC-concat captions, and (ii) using a batch size that is four times larger, i.e., a batch size of"}, {"title": "F.2 The importance of multi-positive contrastive loss", "content": "We next look into the influence of the objective function when an image is paired with multiple\ncaptions. Instead of employing the multi-positive contrastive loss introduced in Appendix D.2, we\ncan use a standard contrastive loss with a single randomly sampled caption paired with each image.\nTable 14 presents the evaluation results for both the models trained with the original objective (left\nside of the arrow), and this new, sampled, objective (right side of the arrow).\nThe table clearly shows a performance decline across all the considered annotation formats and\nbenchmarks when sampling is applied, as also observed by Doveh et al. [14] and Fan et al. [16].\nThe performance drop is particularly important when the captions vary significantly (e.g., long\nversus short captions, or image versus region captions), and when many captions are involved. More\nsurprisingly, this alternative loss does not lead to improvement but rather to performance degradation\nwhen we increase the number of captions paired with each image. We conjecture this is because the\nadditional captions that we consider here are less relevant for these specific benchmarks, leading to a\nworse performance when they are forced to be treated as positive in the sampled objective.\nOverall, these results confirm the importance of our multi-positive contrastive loss in leveraging\nthe presence of multiple captions for an image."}, {"title": "F.3 Impact of caption type on CLIP training", "content": "To further highlight the value of relation and composition captions from GBC, we trained a CLIP\nmodel using only these captions alongside short image captions. As shown in the third row of\nTable 15, these captions, despite being more than twice as scarce as region captions, not only provided\na larger performance gain than using only region captions, but sometimes even enabled the model to\nachieve comparable or better performance than using all captions combined. This underscores the\nsignificant benefit of the relational captions from GBC datasets.\nLooking closely, we note that region captions primarily benefit retrieval and dense prediction tasks,\nwhile relation and composition captions improve performance across the board. While using all\ncaptions remains the best approach for most benchmarks, the marginal improvement from region\ncaptions hints at the potential for more efficient training with these captions through alternative\ntraining objectives."}, {"title": "F.4 Impact of the underlying graph on retrieval", "content": "In this part, we investigate how much GBC-graph relies on the underlying graph structure for retrieval.\nFor this, we probe the performance of our model when the graph is modified either in the mapped\ntokens or in the connectivity patterns. In terms of the mapped tokens, we consider\n\u2022 Last token: For any edge from a caption C to another caption C', we mapped the information of\nC' to the last token before the summary token in C.\n\u2022 Random token: For each edge, we randomly map the information to one token in the source\ncaption.\nAs for the connectivity pattern, we investigate\n\u2022 Star graph: All the captions are mapped to the short image synthetic caption.\n\u2022 Line graph: We map each caption to its next caption in a list (ordered as in GBC-concat following\nthe BFS order), with the short image synthetic caption being the first in the list.\nThe results are shown in Table 16. Since random-token mapping consistently leads to better result than\nlast-token mapping, we only report results for this in the case of star graph and line graph. First of all,\nwe observe that no matter which graph is given, we always achieve better performance than retrieval\nwith only short caption, suggesting that the model is always able to exploit the additional captions to\nsome extent. Furthermore, employing random-token mapping, whether with the groundtruth graph\ntopology or the star graph, yields performance that closely matches that of using the groundtruth\ngraph with correct mapping (interestingly, when using star graph the performance is also very close\nto that obtained with GBC-concat, see Table 3). This suggests that the specific retrieval task we are"}, {"title": "F.5 Evaluating at non-EMA checkpoints", "content": "For the sake of completeness, we also perform evaluation on the non-EMA checkpoints, with results\nshown in Table 17 and Figure 33. Comparing Figure 32 with Figure 33, we see that while EMA\ncheckpoints may experience a drop in performance during later training stages, non-EMA checkpoints\ntypically exhibit best performance at the final training checkpoint. Consequently, our evaluations in\nTable 17 are based on these last checkpoints. From the evaluation results, we observe a similar trend\nin the performance comparison of annotation formats with non-EMA checkpoints as with EMA ones,\nconfirming the validity of our previous claims. Finally, we also note that the use of larger batch size\nwhen training with short captions is only beneficial when we consider EMA checkpoints."}, {"title": "G Image attributions", "content": "All the images that we show in this paper come from Wikimedia Commons. We provide in Table 18\nthe exact source urls and license for each of the images. The urls to the CC BY-SA 3.0 and\nGFDL 1.2 licenses are respectively https://creativecommons.org/licenses/by-sa/3.0/\nand https://www.gnu.org/licenses/old-licenses/fdl-1.2.txt."}, {"title": "D.1 Structure-aware cross attention", "content": "We define below mathematically the SACA layer. For this, we denote by $N_C$ the children of caption C in caption graph $G_C$ and write the features of C in the input of our SACA layer as $X_C=[x_1,...,x_n]$. Recall also that $P_e$ with $e=(C,C')$ represents the set of token positions in the source caption C that we map the edge label $L_e$ to. Then, the SACA layer maps each feature vector $x_i$ to \\begin{equation}\\text{SACA}(x^i) = \\frac{\\sum_{C' \\in N_C} 1\\{i \\in P(C,C')\\}\\text{MHA}(x^i, X_C, X_{C'})}{\\text{min}(1, \\sum_{C' \\in N_C} 1\\{i \\in P(C,C')\\})}, \\end{equation} where MHA implements the standard multi-head attention mechanism. Note that we average across the results from all the relevant captions that describe this token, as we show in Figure 4. As a side note, we highlight that with SAHA, information is only propagated from each node to its direct parent within a block. Consequently, the number of blocks must exceed the depth of the $G_C$ to ensure that information reaches the root node from all levels of the graph."}, {"title": "D.2 Multi-positive contrastive loss", "content": "To pair multiple positive captions to an image, we extend standard contrastive loss [50] into multiple-positive contrastive loss, as also considered in prior studies [14, 16]. Specifically, consider a batch of N images $ \\{I_i\\}_{i=1}^{N} $, where each image $I_i$ is associated with $M_i$ captions $ \\{T_{i,j}\\}_{j=1}^{M_i} $, we utilize the following loss function to account for multiple positive texts per image: \\begin{equation} L_I = - \\frac{1}{Z} \\sum_{i=1}^{N} \\sum_{j=1}^{M_i} \\log \\frac{S(I_i, T_{i,j})}{\\sum_{l=1}^{N} \\sum_{k=1,k \\neq i}^{M_i} S(I_i, T_{i,j}) + \\sum_{k=1,k \\neq i} \\sum_{l=1}^{M_i} S(I_l, T_{k,l})}, \\end{equation} where $S(I,T) = \\exp(\\text{cos}(I,T)/\\tau)$, $ \\tau $ is a learnable temperature parameter, and $Z = \\sum_{i=1}^{N} M_i$ is a normalizer. On the other hand, each caption still only has one paired image. Therefore, we use the standard contrastive loss on for text-to-image alignment: \\begin{equation} L_T =  \\sum_{i=1}^{N} \\sum_{j=1}^{M_i} \\log \\frac{S(I_i, T_{i,j})}{\\sum_{k=1}^{N} S(I_k, T_{i,j})}. \\end{equation}"}]}