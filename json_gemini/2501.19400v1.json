{"title": "Vintix: Action Model via In-Context Reinforcement Learning", "authors": ["Andrey Polubarov", "Nikita Lyubaykin", "Alexander Derevyagin", "Ilya Zisman", "Denis Tarasov", "Alexander Nikulin", "Vladislav Kurenkov"], "abstract": "In-Context Reinforcement Learning (ICRL) represents a promising paradigm for developing generalist agents that learn at inference time through trial-and-error interactions, analogous to how large language models adapt contextually, but with a focus on reward maximization. However, the scalability of ICRL beyond toy tasks and single-domain settings remains an open challenge. In this work, we present the first steps toward scaling ICRL by introducing a fixed, cross-domain model capable of learning behaviors through in-context reinforcement learning. Our results demonstrate that Algorithm Distillation, a framework designed to facilitate ICRL, offers a compelling and competitive alternative to expert distillation to construct versatile action models. These findings highlight the potential of ICRL as a scalable approach for generalist decision-making systems. Code to be released at dunnolab/vintix.", "sections": [{"title": "1. Introduction", "content": "The pursuit of generalist control and decision-making agents has long been a major target of the reinforcement learning (RL) research community (Sutton & Barto, 1998). These agents are envisioned to handle a diverse set of tasks while exhibiting adaptation properties such as self-correction and self-improvement based on reward functions. Notably, traditional online RL algorithms demonstrate these properties in narrow domains and can achieve exceptional performance (Berner et al., 2019; Badia et al., 2020; Schrittwieser et al., 2020; Baker et al., 2022; Team et al., 2023). However, their online nature of learning and frequent reliance on environment-specific training is still considered a challenge to overcome limiting the scalability (Dulac-Arnold et al., 2019; Levine et al., 2020).\nIn contrast to online reinforcement learning, a recent boom"}, {"title": "2. Approach", "content": "At the core of our approach (Figure 2) is Algorithm Distillation (Laskin et al., 2022), a two-step Offline Meta-RL algorithm. The first step involves collecting ordered training histories from base reinforcement learning (RL) algorithms, while the second step involves a decoder-only transformer trained solely for the next-action prediction. This approach facilitates in-context learning by effectively distilling the policy improvement operator into a causal sequence model. We further propose two augmentations to this technique: (1) democratizing the data collection process by introducing a continuous extension of the noise-distillation procedure by Zisman et al. (2024a); (2) conducting generalist agent-style cross-domain training on the acquired dataset."}, {"title": "2.1. Continuous Noise Distillation", "content": "Collecting learning histories can be time-consuming and computationally expensive, as it requires curated learning histories of RL algorithms for each task individually. The resulting trajectories may be excessively long due to poor sample efficiency and may exhibit noise due to training instabilities. Recently, Zisman et al. (2024a) demonstrated"}, {"title": "2.2. Cross-Domain Dataset", "content": "Building upon Continuous Noise Distillation method, we then collect a cross-domain dataset. Our dataset consists"}, {"title": "2.3. Training and Inference Pipeline", "content": "The input data consists of a set of multi-episodic subsequences sampled from the original noise-distilled trajectories, formalized as follows:\n\\(h_{t:t+L}^{(n)} := {\\{o_t,a_t,r_t \\},..., \\{o_{t+i}, a_{t+i}^{j}, r_{t+i}^{j} \\},...,\\{o_{t+L-1}^{M-1}, a_{t+L-1}^{M-1}, r_{t+L-1}^{M-1}\\}}^{(n)}\\).\nWhere t is the index of the sub-sequence's starting point within the full noise-distilled trajectory, L is the length of the sub-sequence, M denotes the total number of episodes within the sub-sequence, which varies due to differing episode lengths across tasks, i \u2208 [0, L \u2212 1] is a lower subscript timestamp index within the sub-sequence, j\u2208 [0, M \u2212 1] is an upper subscript episode index within the sub-sequence. The global subscript n identifies a unique task, which is uniformly sampled from a multi-domain dataset, defined as: Mn = \u222ad=1D Mnd, where d \u2208 [1, D] represents the domain identifier, and nd denotes the task belonging to the respective domain. The overall data pipeline closely resembles that of Laskin et al. (2022), with the only difference being its cross-domain coverage."}, {"title": "2.3.1. MODEL ARCHITECTURE", "content": "At a high level, Vintix consists of three main components: an encoder, which maps raw input sequences \\(h_{t:t+L}^{(n)}\\) into a fixed-size embedding space; a transformer backbone, which processes the encoded inputs; and a decoder, which maps"}, {"title": "2.3.2. TRAINING", "content": "The input batches are created by collating together multiple input sequences \\(h_{t:t+L}^{(n)}\\). To standardize the data across tasks, rewards are scaled by task-specific factors (see Appendix F), and observations are normalized. This procedure significantly enhances model performance by reducing task distinguishability based on raw input values, thereby encouraging the model to rely on contextual information to infer the task.\nAfterward, sequences are processed by the corresponding encoders to obtain fixed-size representations. The sequence of representations is then passed into a transformer, whose outputs are subsequently decoded into predicted actions apred. The training objective is to minimize the Mean Squared Error loss between the predicted actions apred and the ground-truth trajectory actions atrue. Training is conducted on 8 H100 GPUs with a batch size of 64 and 2 gradient accumulation steps. The input sequence length L is set to 8192. For more detailed hyperparameter information, refer to Appendix C."}, {"title": "2.3.3. INFERENCE", "content": "Model inference is performed iteratively, starting with an empty context. The model generates the first action based on the initial observation and subsequently receives the next ob-"}, {"title": "3. Results", "content": "First, we aim to verify whether the Vintix model has the capability for context-based inference-time adaptation through self-correction. To achieve this, we deploy the model on training tasks (ML45 split for Meta-World, ML20 split for Bi-DexHands and setpoints p\u2208 [0,75] for Industrial-Benchmark) by iteratively unrolling its actions in a cold-start manner, beginning with an empty initial context. Figure 4 illustrates that the model progressively improves its policy in each domain as the number of shots (episodes played) increases. The agent starts with a suboptimal performance and gradually self-corrects by inferring task-related information from the accumulated context, ultimately reaching near-demonstrator-level performance.\nNotably, this improvement is both progressive and consistent along the shots-made axis and is observed across multiple domains with varying dynamics and morphology. For task-level graphs depicting inference-time performance with an empty context, refer to Appendix E.\nThis behavior suggests that, despite being task-agnostic, the model can infer environment's implicit structure and utilize it to enable self-corrective adaptation across training tasks."}, {"title": "3.1. Inference-Time Self-Correction on Training Tasks", "content": "First, we aim to verify whether the Vintix model has the capability for context-based inference-time adaptation through self-correction. To achieve this, we deploy the model on training tasks (ML45 split for Meta-World, ML20 split for Bi-DexHands and setpoints p\u2208 [0,75] for Industrial-Benchmark) by iteratively unrolling its actions in a cold-start manner, beginning with an empty initial context. Figure 4 illustrates that the model progressively improves its policy in each domain as the number of shots (episodes played) increases. The agent starts with a suboptimal performance and gradually self-corrects by inferring task-related information from the accumulated context, ultimately reaching near-demonstrator-level performance.\nNotably, this improvement is both progressive and consistent along the shots-made axis and is observed across multiple domains with varying dynamics and morphology. For task-level graphs depicting inference-time performance with an empty context, refer to Appendix E.\nThis behavior suggests that, despite being task-agnostic, the model can infer environment's implicit structure and utilize it to enable self-corrective adaptation across training tasks."}, {"title": "3.2. Comparison to Related Action Models", "content": "Next, we aim to assess the performance of Vintix compared to other generalist agents trained across multiple domains and determine whether its self-corrective inference provides an advantage in matching demonstrator performance levels.\nTo verify this, we compare the average demonstrator-normalized performance of Vintix with that of Gallou\u00e9dec et al. (2024) across training tasks in overlapping domains (MuJoCo and Meta-World). JAT scores are computed by aggregating demonstrator-normalized returns over 100 trials per task, as originally reported by the authors. For Vintix, we extract the average performance over 100 episodes after reaching inference-time convergence (i.e., achieving the best k-shot performance per task)."}, {"title": "3.3. Generalization Analysis", "content": "In this subsection, we take a closer look at the models performance in regards to both unseen parametric variations and tasks."}, {"title": "Generalization to Parametric Variations", "content": "Further experiments aim to assess whether the Vintix model is capable for context-based inference-time adaptation to task variations that were not encountered during training.\nTo evaluate this, we performed the cold-start inference procedure described in previous sections on a set of MuJoCo environments with unobservable variations in viscosity and gravity, which were also unseen during training (for more details on MuJoCo parameter variations, refer to Appendix A.2.1). Additionally, we applied the same procedure to unseen environments in the Industrial-Benchmark domain (setpoint p \u2208 [80, 100], see Appendix A.2.4).\nAs shown, Vintix is still able to achieve near-demonstrator performance in modified environments across both domains. However, a slight decline in convergence quality is observed slower convergence speed in MuJoCo and subtly diminished asymptotic performance in Industrial-Benchmark. The slower convergence rate suggests that the model requires more iterations of self-correction to reach demonstrator-level policy in environments with moderate parametric variations."}, {"title": "Generalization to New Tasks", "content": "Finally, we evaluate Vintix's performance on entirely new tasks that were not seen during training to determine whether it can perform in-context reinforcement learning by inferring task structure in this challenging setting.\nAs in the previous experiments, we unroll actions from Vintix with no initial context on test tasks from the Meta-World ML45 split and the Bi-DexHands ML20 split (Appendix A.2). Overall, we observed that Vintix is not yet capable to handle significantly new tasks. presents one successful rollout and one failure case for each domain"}, {"title": "4. Related Work", "content": "In-Context Learning. In-Context learning is often used to describe the capacity of large language models to adapt to new tasks after the training phase (Brown et al., 2020; Liu et al., 2021). In essence, in-context learning refers to an approach where the algorithm is provided with a set of demonstrations at test time, enabling it to infer task-related information (Min et al., 2022). In contrast, the in-weights paradigm typically relies on fine-tuning the model on downstream tasks (Finn et al., 2017; Wang et al., 2024; Ying et al., 2024). Compared to in-weights learning, in-context learning is gradient-free at deployment, which theoretically allows for a significant reduction in computational costs and facilitates the development of foundational models as a service, applicable to a broad range of real-world tasks (Sun et al., 2022; Dong et al., 2024). In our work, we use the many-shot in-context learning setup (Agarwal et al., 2024)"}, {"title": "Offline Memory-Based Meta-RL.", "content": "Meta-reinforcement learning (Meta-RL) focuses on enabling agents to adapt to new tasks, environments, or dynamics through interaction experience. Numerous diverse approaches exist within the field of Meta-RL (Beck et al., 2024). At a high level, Meta-RL algorithms can be broadly categorized into two main segments: those explicitly conditioned on a task representation (Espeholt et al., 2018; Rakelly et al., 2019; Zhao et al., 2020; Sodhani et al., 2021) and those that infer task dynamics and reward functions from past experience often referred to as \"In-Context\". Implicit memory-based Meta-RL can itself be divided into two major branches: a set of approaches inheriting from RL2 (Duan et al., 2016) which encodes task-related information using the RNN's hidden state and directly leverages RL off-policy updates, including more recent transformer-based variants like AMAGO-{1,2} (Grigsby et al., 2024a;b) and RELIC (Elawady et al., 2024). Another perspective on in-context reinforcement learning formalizes the training of a Meta-RL agent as an imitation learning problem. This can involve cloning optimal actions, as seen in methods like DPT (Lee et al., 2023), leveraging demonstrator's trajectories, as in ICRT (Fu et al., 2024), or utilizing the entire RL algorithm's learning history or its approximations derived via noise distillation (Zisman et al., 2024a)."}, {"title": "Multi-Task Learning.", "content": "Multi-Task Learning (MTL) can be formalized as a paradigm of joint multi-task optimization, aiming to maximize positive knowledge transfer (synergy) between tasks while minimizing detrimental task interference. Numerous studies explore Multi-Task Learning (MTL) beyond the naive minimization of the sum of individual task losses, a method commonly referred to as unitary scalarization. Significant efforts have been made in the fields of massively multilingual translation, where each language pair is treated as a separate task (McCann et al., 2018; Radford et al., 2019; Wang et al., 2020; Li & Gong, 2021), as well as in reinforcement learning (Espeholt et al., 2018; Hessel et al., 2018; Sodhani et al., 2021; Kumar et al., 2023) and robotics (Wulfmeier et al., 2020; Wang et al., 2024). The most common solutions include various types of gradient surgeries to minimize negative interactions between different tasks (Wang et al., 2020; Yu et al., 2020), adaptive tuning of task weights (Sener & Koltun, 2019; Li & Gong, 2021), scaling of model size, and temperature tuning (Shaham et al., 2023). It is also worth noting that several studies argue there is no substantial evidence that specialized multi-task optimizers consistently outperform unitary scalarization, while also introducing significant complexity and computational overhead (Xin et al., 2022; Kurin et al., 2023). It is important to note that the aforementioned works on Multi-Task RL rely on various forms of explicit task conditioning, while our approach follows a task-agnostic paradigm that implicitly infers task-related information from interaction experience."}, {"title": "Generalist Agents and Large Action Models.", "content": "Although most experiments in the field of meta-reinforcement learning (Meta-RL) are typically confined to a single domain of tasks (Duan et al., 2016; Anand et al., 2021; Laskin et al., 2022), generalist agents aim to perform cross-domain training, often integrating multiple data modalities (Gallou\u00e9dec et al., 2024; Reed et al., 2022). Primary advances in this area of research have been made in the field of robotic locomotion and manipulation, where researchers aim to develop generalizable policies and facilitate cross-domain knowledge transfer. This approach seeks to reduce the computational complexity of training policies for robotic applications. The renaissance of generalist robot policies has been notably driven by the availability of large open-source multi-domain robotic datasets like Open X-Embodiment (Collaboration et al., 2024). A variety of models build upon the foundations established by the Open X datasets, including RT-X"}, {"title": "Learned Optimizers.", "content": "In contrast to traditional optimizers that follow hand-crafted update rules, learned optimizers employ a parameterized update rule that is meta-trained to optimize various objective functions (Li & Malik, 2016; Andrychowicz et al., 2016; Metz et al., 2020; Almeida et al., 2021). Thus, learned optimization can be seen as an alternative perspective on meta-learning (learning-to-learn), with recent approaches scaling to a wide range of tasks and requiring thousands of TPU-months for training (Metz et al., 2022). However, due to the non-stationarity and high stochasticity of the temporal difference (TD) objective, learned optimizers fail in reinforcement learning setting (Metz et al., 2022). To address these challenges, Optim4RL introduces RL-specific inductive bias (Lan et al., 2024), while OPEN enhances exploration by leveraging learnable stochasticity (Goldie et al., 2024). Both works can be considered optimization-centric approaches to Meta-RL, whereas we adopt a context-based approach."}, {"title": "5. Conclusion and Future Work", "content": "The development of generalist reinforcement learning agents that adapt across domains remains a critical challenge, as traditional online RL methods\u2014despite their success in narrow settings-face scalability limitations due to their reliance on environment-specific, interactive training (Dulac-Arnold et al., 2019; Levine et al., 2020). While recent advances in offline RL and generative action models have expanded the potential for data-driven learning, these approaches often prioritize expert demonstrations or language conditioning over reward-centric adaptation (Reed et al., 2022; Gallou\u00e9dec et al., 2024; Haldar et al., 2024; Collaboration et al., 2024; Schmied et al., 2024; Sridhar et al., 2024). In this work, we explored an alternative paradigm rooted in In-Context Reinforcement Learning (ICRL), building on Algorithm Distillation (Laskin et al., 2022) to create agents that learn adaptive behaviors by a straightforward next-action prediction of learning histories or their proxies.\nOur proposed approach and model, Vintix, demonstrates that ICRL can extend beyond prior single-domain, grid-based benchmarks. By introducing Continuous Noise Distillation, we ease the data collection process inherent to Algorithm Distillation and release a suite of datasets and tools for 87 tasks across four domains, which we hope would be helpful in community efforts toward scalable, cross-domain action models capable of in-context reinforcement learning. Empirically, we show that Vintix exhibits self-correction on training tasks and adapt to moderate controlled parametric variations without requiring gradient updates at inference time. These results, though preliminary and confined to structured settings, suggest that reward-guided ICRL provides a viable pathway for agents to autonomously refine their policies in response to environmental feedback.\nWhile we believe the obtained results are promising, there is a large room for improvement as challenges remain in increasing the number of domains, developing more task-agnostic architectures that would allow not only for robust self-correction but vital generalization to unseen tasks and self-improvement beyond demonstrators' performance. We hope this work encourages further investigation into data-centric, reward-driven frameworks for cross-domain RL agentic systems."}, {"title": "Impact Statement", "content": "This proposed model is strictly limited to the simulated environments used for evaluation due to its architectural limitations, and therefore does not present any safety concerns. However, in case one would like to finetune it for real-world purposes, it does not guarantee any sort of safe behavior and cautions must be made on the part of the user."}, {"title": "A. Dataset Details", "content": "A.1. General Information\nMuJoCo. MuJoCo (Todorov et al., 2012) is a physics engine designed for multi-joint continuous control. For our purposes, we selected 11 classic continuous control environments from OpenAI Gym (Brockman et al., 2016) and Gymnasium (Towers et al., 2024).\nMeta-World. Meta-World (Yu et al., 2021) is an open-source simulated benchmark for meta-reinforcement learning and multitask learning consisting of 50 distinct robotic manipulation tasks. It is important to note that, similar to JAT (Gallou\u00e9dec et al., 2024), we limit the episode length to 100 transitions to reduce dataset size and increase the number of episodes that fit within the model's context. This truncation is feasible, as such a horizon is often sufficient to solve the task. However, this modification complicates direct performance comparisons with models such as AMAGO-2 (Grigsby et al., 2024b), which utilize full 500-step rollouts and reports total returns over three consecutive episodes.\nBi-DexHands. Bi-DexHands (Chen et al., 2022) is the first suite of bi-manual manipulation environments designed for practitioners in common RL, MARL, offline RL, multi-task RL, and Meta-RL. It provides 20 tasks that feature complex, high-dimensional continuous action and observation spaces. Some task subgroups share state and action spaces, making this benchmark applicable to the Meta-RL framework.\nIndustrial-Benchmark. Industrial-Benchmark (Hein et al., 2017) is a suite of synthetic continuous control problems designed to model various aspects that are crucial in industrial applications, such as the optimization and control of gas and wind turbines. The dynamic behavior of the environment, as well as its stochasticity, can be controlled through the setpoint parameter p. By varying this parameter from 0 to 100 in increments of 5, we generated 21 tasks that share a common state and action structure. Classic reward function was selected and subsequently down-scaled by a factor of 100."}, {"title": "A.2. Train vs. Test Tasks Split", "content": "To validate the inference-time optimization capability of our model, we divided the overall set of 102 tasks into two disjoint subsets. The validation subset was excluded from the training dataset. Below, we provide details of the split for each domain.\nA.2.1. MUJOCO\nValidation tasks for the MuJoCo domain were created by modifying the physical parameters of the environments using the provided XML API. For each embodiment, the default viscosity parameter was adjusted from 0 to 0.05, and 0.1. Additionally, the gravity parameter was varied by \u00b110 percent.\n\u0391.2.2. \u039c\u0395TA-WORLD\nThe standard ML45 split was selected, with 45 tasks assigned to the training set and 5 tasks reserved for validation: bin-picking, box-close, door-lock, door-unlock, and hand-insert.\nA.2.3. BI-DEXHANDS\nWe adopted the ML20 benchmark setting proposed by the original authors (Chen et al., 2022), in which 15 tasks are assigned to the training set, while 5 tasks are reserved for validation, including: door-close-outward, door-open-inward, door-open-outward, hand-kettle, and hand-over. It is important to note that the hand-over task does not share the same state-action space dimensionality with any tasks in the training set, making it incompatible with the current encoder architecture. As a result, we report a performance of 0 (random) for this task.\nA.2.4. INDUSTRIAL-BENCHMARK\nFor this domain, a global split based on the setpoint parameter was made, with setpoints ranging from 0 to 75 assigned to the training set, and setpoints from 80 to 100 assigned to the validation set."}, {"title": "A.3. Epsilon Decay Functions", "content": "The epsilon decay function defines how the noise proportion e depends on the transition number ns within trajectory. The primary purpose of utilizing such function is to ensure in smooth increase in the rewards through generated trajectories. A linear decay function may not yield this behavior across all environments, as some tasks exhibit rewards that increase rapidly either at the beginning or the end of trajectories when linear function is utilized. To address this variability, we employ the following generalized decay function:\n\u03b5(\u03b7\u03c2) =\n[/1 - (ns/((1 \u2013 f)N\uff61))p\n{0\n- f)Ns))pns <= (1 \u2212 f)Ns\nns > (1 - f)N\uff61\nwhere Ns represents the maximum number of transitions in a trajectory, f the fraction of the trajectory with zero noise, and p is a parameter that controls the curvature of the decay function. By adjusting p, it is possible to modulate the smoothness of the reward progression along the trajectory in order to better suit different tasks. Specifically, when rewards increase sharply at the beginning of the trajectory, setting p > 1 flattens the epsilon curve at the start and steepens it at the end. Conversely, when rewards increase in a sharp manner towards the end of the trajectory, choosing 0 < p < 1 results in the epsilon curve being steepened at the start and flattened at the end."}, {"title": "B. Demonstrators", "content": "B.1. Training\nMuJoCo. The dataset provided by JAT (Gallou\u00e9dec et al., 2024) consists of demonstrations from expert RL agents on MuJoCo (Todorov et al., 2012) tasks. It was used to train demonstrators through the imitation learning paradigm. The resulting models achieve performance similar to that of the original RL agents.\nMeta-World. For the Meta-World (Yu et al., 2021) benchmark, Gallou\u00e9dec et al. (2024) open-sourced trained agents for each task in the benchmark. These models served as demonstrators; however, a detailed analysis revealed that certain agents exhibited suboptimal performance. Specifically, models trained on the disassemble, coffee-pull, coffee-push, soccer, push-back, peg-insert-side, and pick-out-of-hole tasks were either too noisy or performed unsatisfactorily. As a result, we retrained agents for these tasks.\nJAT (Gallou\u00e9dec et al., 2024) also provided open-source training scripts, which we utilized for hyperparameter tuning and retraining new agents. This process led to improved performance for the selected tasks. However, many demonstrators across other tasks still exhibit noisy performance.\nBi-DexHands. Demonstrators were trained using the provided PPO (Schulman et al., 2017) implementation. To enhance convergence, the number of parallel environments in IsaacGym (Liang et al., 2018) was increased from 128 to 2048. In certain environments, such as the Re-Orientation and Swing-Cup tasks, we were able to significantly surpass the expert performance reported in the original Bi-DexHands paper (Chen et al., 2022). However, even after extensive training for over 1.5 billion timesteps, some policies continued to exhibit stochastic performance.\nIndustrial-Benchmark. Demonstrators were trained using the PPO implementation from Stable-Baselines3 (Raffin et al., 2021). For all tasks, PPO was trained with advantage normalization, a KL-divergence limit of 0.2, 2500 environment steps, and a batch size of 50. All agents were trained for 1 million timesteps. Similarly to original Industrial Benchmark setup (Hein et al., 2017), discount factor was set to 0.97. To ensure better score comparability, we limited the episode length to 250 interactions. PPO agents were trained using both classic and delta rewards; however, we observed that agents trained on delta rewards achieved higher classic reward scores compared to those trained directly on scaled classic returns. Consequently, for our experiments, we utilize demonstrators trained with delta rewards."}, {"title": "C. Hyperparameters", "content": "Hyperparameter\nValue\nLearning Rate\n0.0003\nOptimizer\nAdam\nBetta 1\n0.9\nBetta 2\n0.99\nBatch Size\n64\nGradient Accumulation Steps\n2\nTransformer Layers\n20\nTransformer Heads\n16\nContext Length\n8192\nTransformer Hidden Dim\n1024\nFF Hidden Size\n4096\nMLP Type\nGptNeoxMLP\nNormalization Type\nLayerNorm\nTraining Precision\nbf16\nParameters\n332100768"}, {"title": "G. Task-Level Performance", "content": ""}]}