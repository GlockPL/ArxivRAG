{"title": "Semantic Communication based on Large Language Model for Underwater Image Transmission", "authors": ["Weilong Chen", "Wenxuan Xu", "Haoran Chen", "Xinran Zhang", "Zhijin Qin", "Yanru Zhang", "Zhu Han"], "abstract": "Underwater communication is essential for environmental monitoring, marine biology research, and underwater exploration. Traditional underwater communication faces limitations like low bandwidth, high latency, and susceptibility to noise, while semantic communication (SC) offers a promising solution by focusing on the exchange of semantics rather than symbols or bits. However, SC encounters challenges in underwater environments, including information loss and difficulties in accurately identifying and transmitting critical information that aligns with the diverse requirements of underwater applications. To address these challenges, we propose a novel Semantic Communication (SC) framework based on Large Language Models (LLMs). Our framework leverages visual LLMs to perform semantic compression and prioritization of underwater image data according to the query from users. By identifying and encoding key semantic elements within the images, the system selectively transmits high-priority information while applying higher compression rates to less critical regions. On the receiver side, an LLM-based recovery mechanism, along with Global Vision ControlNet and Key Region ControlNet networks, aids in reconstructing the images, thereby enhancing communication efficiency and robustness. Our framework reduces the overall data size to 0.8% of the original. Experimental results demonstrate that our method significantly outperforms existing approaches, ensuring high-quality, semantically accurate image reconstruction.", "sections": [{"title": "I. INTRODUCTION", "content": "Underwater communication is a critical technology with widespread applications in various domains, such as environmental monitoring, marine biology research, and underwater exploration [1]. The ability to transmit multimodal data efficiently and reliably in underwater environments is essential for collecting and sharing vital information. For instance, environmental monitoring relies on underwater sensors and vehicles to track changes in marine ecosystems, detect pollution, and study climate change impacts. Marine biologists use underwater communication systems to observe and understand marine life behaviors, migrations, and interactions [2]. Additionally, underwater exploration missions, including resource extraction and archaeological studies, depend heavily on effective communication systems to ensure the safety and success of operations. The integration and transmission of multimodal data, such as images, videos, and sensory data, are crucial to providing comprehensive insights.\nTraditional underwater communication techniques primarily include acoustic communications, optical communications, and radio frequency (RF) communications [3]. Acoustic communication is the most widely used and practical underwater communication method, capable of providing wide coverage up up to tens of kilometers [4]. However, the acoustic communication system is facing several issues. First, limited bandwidth results in low data transmission rates on the order of kbps, limiting the amount of information that can be sent in a given time. Secondly, high latency is another issue, as the speed of sound in water is much slower compared to electromagnetic waves in air [5], thus leading to delays in data transmission. Moreover, underwater acoustic signals are highly susceptible to noise from marine life, human activities, environmental factors such as temperature gradients and salinity variations [6], signal attenuation and multipath effects [7], which could degrade signal quality and lead to data distortion. Therefore, in the complex underwater physical environment, traditional acoustic communication technologies face challenges such as low bandwidth, high latency, significant attenuation, and low robustness.\nCurrently, some researchers utilize Semantic Communication (SC) based on artificial intelligence to address communication problems under low bandwidth and significant attenuation [8]\u2013[12]. By introducing a semantic channel to the communication system [13], SC extracts and encodes the semantic information of data at the transmitter end, transmits it through the semantic channel, and successfully decodes the semantic information at the receiver end [14]. Traditional communication systems focus solely on the accuracy of bitstream transmission, ignoring the implicit semantic information behind the data. In contrast, SC can extract semantic information, achieving accurate semantic transmission [15] and reducing the amount of transmitted data, thereby optimizing the use of limited bandwidth.\nHowever, existing research in SC faces significant challenges in the underwater environment. Firstly, the delay spread from multipath propagation and Doppler spread from relative motion lead to information loss in underwater communication. Multipath effects, caused by reflections off the sea surface and seabed, and refractions from varying water densities, distort signals. Frequency-selective fading causes different frequencies to attenuate unevenly, while the Doppler effect from relative motion between transmitters (e.g., underwater vehicles) and receivers (e.g., stationary sensors) introduces frequency shifts. This challenge is particularly problematic for semantic communication, as it hinders the accurate extraction and representation of semantic content, making reliable data transmission and the delivery of meaningful information difficult in underwater environments.\nSecondly, SC faces the challenge of accurately identifying and transmitting different critical information in alignment with the diverse requirements of underwater applications. The limitations in extracting and representing semantic content complicate the process of aligning communication with specific application needs. Underwater applications, such as environmental monitoring, marine biology, and exploration, require tailored communication strategies that prioritize different types of data. For instance, environmental monitoring may focus on transmitting images of coral reef health, while marine biology might prioritize detecting specific marine species. Non-essential semantic information can interfere with the transmission of important data, potentially leading to misinterpretations or delays that negatively impact underwater operations and decision-making processes."}, {"title": "B. Related Work", "content": "Underwater communication technique plays an important role in marine biology observation, oil and gas drilling exploration, and natural disaster monitoring. Traditional underwater wireless communication technologies primarily include acoustic communication, optical communication, and radio frequency communication. Acoustic communication utilizes sound signals for transmission, allowing for the greatest propagation distances, which is currently the dominant technology in underwater communication. In the unique physical environment underwater, acoustic signals are significantly affected by multipath interference and ambient noise. Equalization techniques are employed to overcome inter symbol interference (ISI) caused by multipath propagation [16], [17]. Acoustic communication is characterized by limited bandwidth and slow data transmission rates. Modems are employed to improve transmission rates, mitigate multipath interference, and enhance noise resistance [18], [19]. To reduce data redundancy, data compression techniques are widely used in acoustic communication, including compressed sampling [20] and compressed sensing technologies [21]. Moreover, acoustic signal propagation is easily affected by underwater physical conditions such as temperature, pressure, salinity, and water density, which could lead to signal attenuation and data distortion. Optical and RF communication can provide higher data rates compared to acoustic communication, but both methods are limited in terms of transmission distance [22]. Optical signals exhibit exponential attenuation in water, moreover, optical communication requires a line-of-sight connection between the transmitter and receiver [23], which can result in high error rates and data loss in water with a high concentration of particulates. RF technology can provide medium data transmission rates, but electromagnetic signals experience significant attenuation in water [24], limiting the long-distance propagation of RF technology. Therefore, existing traditional underwater communication technologies face several significant challenges, including severe attenuation, multipath effects, dispersion, limited bandwidth, low transmission rates, and low robustness.\nShannon and Weaver's [25], [26] work reveals three levels of communication: the lowest level is the symbol transmission level, which focuses on transmitting symbols from the transmitter to the receiver; the intermediate level is the semantic level, where semantic information is extracted at the transmitter, transmitted through a semantic channel, and interpreted at the receiver; and the highest level is the effectiveness level, which ensures the communication efficiency of the other two levels. Traditional communication is measured by the correct bit transmission rate. Due to channel noise and interference, the transmitted symbols can be distorted, leading to the reception of garbled information. On the other hand, SC can extract the meaning of data in the semantic domain, filter out irrelevant and unimportant information, and significantly reduce the data volume by further compressing the semantic information while preserving the meaning. Therefore, when bandwidth is limited or the signal-to-noise ratio (SNR) is relatively low, SC systems may still perform well, maintaining robustness and reliability. Thanks to the development of artificial intelligence and communication technologies, related research has developed end-to-end SC systems based on deep learning to achieve the aforementioned second level of communication. Xie et al. proposed a Transformer-based SC framework to maximize system capacity and eliminate semantic errors [9]. Compared to text, image data is semantically richer and more bandwidth-sensitive, making the transmission of image data more challenging. Huang et al. proposed an image semantic encoding method based on Generative Adversarial Networks (GANs) [27]. Wu et al. proposed a semantic transmission system for segmenting regions of interest in images [28]. Irina et al. proposed an image transmission method based on joint source-channel coding [29]. However, these methods cannot identify the different critical information that aligns with the diverse requirements of underwater applications, which can lead to the loss of critical information and reduced transmission efficiency. Additionally, multipath propagation and Doppler spread from relative motion in underwater environments further intensify the difficulties in maintaining robust data transmission.\nTherefore, it is imperative to adequately consider identifying the varying important information of different tasks when designing SC systems. Moreover, we need to further consider multipath propagation and Doppler spread from relative motion, ensuring that critical information is preserved and accurately reconstructed even under challenging underwater conditions."}, {"title": "C. Our Contributions", "content": "To address these challenges, we propose a novel SC framework based on Large Language Models (LLMs). The advent of LLMs, particularly those designed for visual data, has opened new avenues for enhancing communication systems. Our framework leverages visual LLMs [30] to comprehend queries from individuals above the surface and perform semantic compression and prioritization of image data, aligning with the diverse requirements of underwater applications. This method allows for the identification and encoding of key semantic elements within images and selectively transmits high-priority information while applying higher compression rates to less critical regions, enhancing the effectiveness and relevance of data transmission. On the receiver side, we employ a text LLM recovery mechanism to reconstruct the textual data, and two new ControlNet networks to assist the diffusion model in recovering the image to the original size, thereby mitigating information loss, enhancing the system's ability to maintain the integrity and detail of the transmitted data. This approach not only reduces the overall data size to 0.8% of the original, but also significantly improves the resilience of the communication system against noise and signal loss.\nThe main contributions of this paper are summarized as follows:\nWe introduce an innovative LLM-based SC framework to enhance underwater communication. This framework can perform semantic compression and prioritization, enhancing data transmission by adapting dynamically to the different requirements of underwater applications.\nThe proposed framework integrates two newly designed ControlNet networks and a text-based LLM recovery mechanism with a diffusion model, ensuring the preservation of key semantic elements while effectively mitigating information loss.\nOur experimental results show that our framework can compress data to just 0.8% of its original size and still maintain high-quality transmission effects even under significant noise conditions.\nThe remainder of this paper is organized as follows. Section 2 provides a review of related work in underwater communication and semantic compression techniques. Section 3 describes the proposed framework in detail, including its architecture and technical components. Section 4 presents the experimental setup and results, highlighting the performance improvements achieved by our method. Finally, Section 5 concludes the paper and discusses potential directions for future research."}, {"title": "II. SYSTEM MODEL", "content": "The proposed comprehensive framework is tailored specifically for underwater image transmission, which is shown in Fig. 1. This framework initiates with a query generated by individuals located above the water surface, which is subsequently transmitted to the underwater environment. Upon receiving the query, the underwater transmitter employs a semantic encoder integrated with a LLM-based information prioritization mechanism. This mechanism is designed to identify and prioritize critical parts of the visual information based on the query's context. The essential information is then compressed into a format suitable for transmission through the underwater communication channel. Once transmitted, the information is received and decoded by a semantic decoder with a diffusion model and LLM recovery at the receiver's end. The decoded visual information is subsequently reconstructed and presented to the individuals above the water. This approach ensures efficient and effective transmission of visual information in underwater communication scenarios."}, {"title": "A. Semantic Encoder", "content": "Initially, an individual on the surface provides a query specifying the desired visual information. This query is then transmitted to the semantic encoder by the wireless channel. The semantic encoder is composed of two primary components: LLM-based Information Prioritization and Image Highly Compression.\nTo efficiently transmit images underwater and extract their semantic information, we employ visual LLM (vLLM) for information prioritization. The process begins with the reception of a query from individuals above the water surface. These queries, typically articulated in natural language, must be accurately understood and interpreted to extract the relevant contextual information.\nFirstly, given the query q and image $i \\in R^{C\\times H \\times W}$ taken underwater, the vLLM $F_i$ generates an answer and identifies the key regions of the original image. This process can be described as:\n$a = F_i(q, i)$,                                                                                                          (1)\nwhere a represents the answer provided by the vLLM. Subsequently, a is inputted into $F_i$ along with $i$ to determine the key informational regions of the image, as follows:\n$bbox_k = F_i(a, i)$,                                                                                                            (2)\nwhere $bbox$ denotes the bounding box encompassing the key regions. This bounding box is then applied to the original image $i$, followed by image compression techniques to resample and compress the image, yielding $i_k \\in R^{C^* \\times H^* \\times W^*}$. In comparison to $i$, $i_k$ contains only the essential information. Specifically, $C_k < C$, $H_k < H$, and $W_k < W$, which helps reduce the image size. Additionally, since only the key regions are transmitted, the data size can be further minimized within the original $R^{C^{'*}\\times H^{'*}\\times W^*}$ dimensions, ensuring an even lower transmission overhead. This selective extraction results in a representation that, while retaining the full dimensionality of the original image, significantly reduces informational entropy by excluding extraneous data and preserving only the semantically relevant content.\nTo efficiently transmit the entire image with minimal communication bandwidth, we employ image compression techniques to resample and compress the complete image. Resampling involves altering the image resolution, effectively reducing the number of pixels and, consequently, the overall data size. By decreasing the spatial dimensions, we minimize the amount of information that needs to be transmitted, thus reducing bandwidth requirements. Compared to compressing only the non-key regions, this holistic approach offers superior benefits because the image is uniformly compressed. Therefore, it is more efficient to apply compression directly to the entire image rather than separately compressing non-key regions. This strategy ensures that the overall data size is minimized while maintaining an acceptable level of visual quality throughout the image. The highly compressed image $i_{hc}$ can be expressed as:\n$i_{hc} = RESAMPLE(i)$,                                                                                                               (3)"}, {"title": "B. Underwater Acoustic Communication Channel", "content": "In this section, we simulate a realistic physical communication environment. In a shallow water acoustic propagation environment, to consider the multipath propagation and Doppler spread, we first define the path loss between surface and underwater robot, which can be defined as:\n$A (d, f) = d^o a(f)^d$,                                                                                                                  (4)\nwhere o is the spread factor, d is the distance and a(f) is the absorption coefficient, as calculated using the Thorp formula [31], measured by dB/km with f in kHz, which is expressed as:\n$10log(a(f)) =0.11 \\frac{f^2}{1 + f^2} +44 \\frac{f^2}{4100 + f^2} + 2.75 \\times 10^{-4}f^2 + 0.003$.                                                                  (5)\nSecondly, according to [32], we consider the underwater environmental noise N(f) which is composed of turbulence, ship, wind and thermal noises [33], denoted as $N_1(f), N_2(f), N_3(f),N_4(f)$, respectively. This can be defined as:\n$N(f) = N_1(f) + N_2(f) + N_3(f) + N_4(f)$,                                                                                       (6)\nand those components can be described as:\n$10 log N_1(f) = 17 - 30 log f$,\n$10 log N_2(f) = 30 + 20s + log (f^26/(f + 0.03)^60)$,\n$10 log N_3(f) = 60 + 7.5v^{1/2} + 20 log (f/(f + 0.4)^2)$,\n$10 log N_4(f) = -15 + 20 log f$,                                                                                                    (7)\nwhere s is the shipping activity factor between [0, 1], and v is the wind speed measured in m/s. The channel conditions are characterized by the signal-to-noise ratio (SNR), defined as:\n$SNR = \\frac{P_{signal}}{P_{noise}}$,                                                                                                                                 (8)\nwhere $P_{signal}$ and $P_{noise}$ represent the average received signal and noise powers, respectively. The $P_{noise}$ is decided by the $N(f) \\cdot A (d, f)$. Notably, the aggregate size of $i_k$, $a'$, and $i_{hc}$ constitutes only 0.8% of the original image $i$ in our experiments, underscoring the bandwidth efficiency of our proposed method."}, {"title": "C. Semantic Decoder", "content": "Upon receiving $i_k$, $i_{hc}$, and $a'$, the receiver employs a diffusion model $F_{\\theta}$ to reconstruct the original image. The semantic decoder in the receiver leverages two ControlNets [34] and is guided by textual information to ensure accurate and detailed image recovery.\nThe diffusion model $F_{\\theta}$ belongs to a class of generative models that define a forward and reverse process to generate data. The forward process incrementally adds noise to the data, while the reverse process denoises the data to reconstruct the original input. Formally, the forward process is represented as:\n$q(X_{1:T}|X_0) = \\prod_{t=1}^{T} q(x_t|x_{t-1})$,                                                                                                          (9)\nwhere $x_0$ is the original data and $x_t$ represents the noisy data at time step t. The reverse process aims to recover $x_0$ from $x_T$ by progressively denoising the data:\n$p_{\\theta} (x_{0:T}) = p(x_T) \\prod_{t=1}^{T} P_{\\theta}(x_{t-1}|x_t)$,                                                                                     (10)\nwhere $p_{\\theta}$ represents the learned reverse process parameterized by $\\theta$.\nTo better recover the key regions, we proposed the Key Region ControlNet $C_r^k$, which is dedicated to processing $i_k$. This network is specifically designed to enhance and accurately generate the critical regions identified during the LLM-based Information Prioritization phase, which can be referred to as:\n$f_k = C_k(i_k)$,                                                                                                               (11)\nwhere $f_k$ is the output of the $C_k$ and will be integrated into the denoising process in the diffusion model. By focusing on these key areas, the Key Region ControlNet ensures that the significant details, such as semantic segmentation and human pose estimation, are preserved and reconstructed with high fidelity. This targeted enhancement allows for a more refined generation of the most important parts of the image, thereby maintaining the integrity of the prioritized information.\nTo recover the rest restoration of the image without emphasizing fine details, we proposed Global Vision ControlNet, which is responsible for handling $i_{hc}$. The Global Vision ControlNet aims to reconstruct the broader spatial layout and general color distribution of the image, which can be described as:\n$f_{ci} = C_{ci}(i_{hc})$,                                                                                                                 (12)\nwhere $f_{ci}$ is the output of the $C_{ci}$ and will be also integrated into the denoising process in the diffusion model. The Global Vision ControlNet ensures that the reconstructed image retains the correct positioning and overall appearance of the objects within it. This broader approach helps in maintaining the coherence of the entire image while ensuring that the critical regions, processed by the Key Region ControlNet, are accurately integrated into the final output.\nIn addition to the visual information, we also employ an LLM-based recovery process for the textual guide $a'$. We also utilize the $F_i$ as the base model of the LLM recovery process. To recover the text, we use the prompt:\nHelp me recover the sentence from the noise sentence.\nThe sentence is a'.\nThis process can be defined as:\n$\\bar{a} = F_i(prompt, a')$,                                                                                                               (13)\nwhere $\\bar{a}$ is the final text utilized by the diffusion model. This step is crucial for preserving the semantic context and ensuring that the textual information is accurately reconstructed. The LLM-based textual recovery leverages the advanced capabilities of large language models to interpret and restore the original textual content, thereby enhancing the overall quality and relevance of the reconstructed information.\nTo provide fine-grained control over the generative process of diffusion models, we incorporate three guidance signals to enable the generation of more precise and contextually relevant outputs by conditioning the diffusion model on additional information. These guidance signals can be integrated into the reverse diffusion process as follows:\n$p_{\\theta}(x_{t-1} x_t, c) = N(x_{t-1}; \\mu_{\\theta}(x_t, c, t), \\sigma_{\\theta}(x_t, c, t))$                                                  (14)\nwhere c represents the control signal provided by the three guidance components, and $\\mu_{\\theta}$ and $\\sigma_{\\theta}$ are the mean and covariance functions parameterized by $\\theta$. c is defined as:\n$c = {\\bar{a}, [\\alpha f_k + (1 - \\alpha) f_{ci}]}$,                                                                                                     (15)\nwhere the hyperparameter $\\alpha$ controls the balance between the contributions of the Key Region ControlNet and the Global Vision ControlNet. The complete process of the diffusion model $F_{\\theta}$, which integrates the outputs from the two ControlNets and the LLM-based textual recovery to generate the final image, can be defined as:\n$i_r = F_{\\theta}(x_t, c)$,                                                                                                               (16)\nwhere $i_r \\in R^{C\\times H \\times W}$ is the recovered image. By combining these three guidance signals, the diffusion model effectively synthesizes the visual content, ensuring that the reconstructed image is both semantically accurate and visually coherent. The integration of these components allows the model to leverage the strengths of each guidance signal, resulting in a comprehensive and high-quality reconstruction of the original image. The whole process can be found in Alg. 1."}, {"title": "D. Training Process", "content": "To train the semantic encoder, there are question-answering loss and key information prioritization loss. The question answering loss is designed to ensure that the vLLM's answer a accurately reflects the text label y. This is achieved by using a cross-entropy loss between the predicted answer and the true label:\n$L_a = \\frac{1}{N} \\sum_{n=1}^{N} y^{(n)} log a^{(n)}$,                                                                                           (17)\nwhere N is the size of the training data. This loss ensures that the semantic information provided by the LLM is accurately captured and maintained during the reconstruction process.\nThe key information prioritization loss is designed to ensure that the extracted key regions match the original key regions as closely as possible. To achieve this, we use the bounding box-based loss to ensure that the spatial alignment and dimensions of the key regions are preserved. The bounding box loss is defined as the sum of the Intersection over Union (IoU) loss for the bounding boxes in the original and reconstructed images:\n$L_{key, bbox} = \\frac{1}{N} \\sum_{n=1}^{N} (1 - IoU (bbox_k^n, \\hat{bbox}_k^n))$,                                                                               (18)\nwhere $bbox_k^n$ and $\\hat{bbox}_k^n$ are the bounding boxes of the key regions in the original and extracted images, respectively.\nTo train the semantic decoder, two reconstruction controlnet losses are utilized. For the Key Region ControlNet, Given a set of conditions including time step t, answer $\\bar{a}$, as well as a task-specific condition $f_k$, Key Region ControlNet learn a network $\\theta_k$ to predict the noise added to the noisy image $x_t$ with:\n$L_k = E_{i,t,\\bar{a}, f_k, \\epsilon \\sim N(0,1)} [||\\epsilon - \\epsilon_{\\theta_k} (x_t, t, \\bar{a}, f_k)) ||^2]$,                                                                   (19)\nwhere $L_k$ is the overall learning objective of the diffusion model. This learning objective is directly used in fine-tuning diffusion models with Key Region ControlNet. The loss of Global Vision ControlNet can be also written as:\n$L_{ci} = E_{i,t,\\bar{a}, f_{ci}, \\epsilon \\sim N(0,1)} [||\\epsilon - \\epsilon_{\\theta_{ci}} (x_t, t, \\bar{a}, f_{ci})) ||^2]$,                                                                     (20)\nwhere $\\theta_{ci}$ is the network parameters of Global Vision ControlNet."}, {"title": "III. CASE STUDY", "content": "Here, we present a comprehensive analysis of the performance of the proposed method on the real-world underwater dataset named SUIM Dataset [35]. Specifically, we aim to perform 3 evaluations: (E1) How does the proposed method outperform other approaches? (E2) How does each part contribute to the performance? (E3) How do critical hyperparameters and components impact the proposed method?"}, {"title": "A. Dataset Description and Evaluation Metrics", "content": "To evaluate the performance of our proposed method, we utilize the SUIM Dataset [35], which comprises over 1,500 images annotated for eight object categories: fish (vertebrates), reefs (invertebrates), aquatic plants, wrecks/ruins, human divers, robots, and seafloor. Based on these categories, we formulate various questions pertaining to the object categories within the images. For the key region identification, we convert the segmentation annotations to bounding boxes corresponding to the object categories. The whole dataset is divided into 80% for training and 20% for testing.\nTo rigorously evaluate the performance of our proposed method, we employ several metrics: Fr\u00e9chet Inception Distance (FID) [36], Structural Similarity Index (SSIM) [37], Contrastive Language-Image Pretraining (CLIP) [38], and Learned Perceptual Image Patch Similarity (LPIPS) [39]. FID measures the similarity between the generated images and real images by comparing the distributions of their feature representations extracted from a pretrained Inception network. FID can be defined as:\n$d(X,Y) = ||\\mu_X - \\mu_Y ||^2 + Tr(\\Sigma_X + \\Sigma_Y - 2(\\Sigma_X \\Sigma_Y)^{1/2})$,                                              (21)\nwhere X and Y are the distributions of real image and generated image, respectively, $\\mu_X$ and $\\mu_Y$ are the means, $\\Sigma_X$ and $\\Sigma_Y$ are the covariance of X and Y, a lower FID score indicates higher quality and more realistic images.\nSSIM assesses the structural similarity between the original and reconstructed images. It evaluates the perceived quality based on luminance, contrast, and structure. SSIM can be defined as:\n$SSIM(x,y) = [l(x, y)]^{\\alpha}[c(x, y)]^{\\beta}[s(x, y)]^{\\gamma}$,                                                                             (22)\nwhere l is the luminance, c is the contrast and s is the structure comparison between the original image x and reconstructed image y. The exponent $\\alpha, \\beta, \\gamma$ are positive constants. SSIM values range from -1 to 1, with higher values indicating better similarity.\nThe luminance, contrast, and structure comparison of two images can be expressed respectively as:\n$l(x,y) = \\frac{2\\mu_x\\mu_y + C_1}{\\mu_x^2 + \\mu_y^2 + C_1}$,                                                                                                     (23)\n$c(x, y) = \\frac{2\\sigma_{xy} + C_2}{\\sigma_x^2 + \\sigma_y^2 + C_2}$,                                                                                                     (24)\n$s(x, y) = \\frac{\\sigma_{xy} + C_3}{\\sigma_x\\sigma_y + C_3}$,                                                                                                        (25)\nwhere $\\mu_x$ and $\\mu_y$ are the mean values, $\\sigma_x$ and $\\sigma_y$ are the standard deviations, and $\\sigma_{xy}$ is the cross-covariance for two images x and y.\nCLIP measures the alignment between images and textual descriptions. CLIP can be defined as:\n$CLIPScore(x,t) = max(cos(x,t), 0)$                                                                                                                 (26)\nwhere x is the visual CLIP embedding for an image, and t is the textual CLIP embedding for a description. By evaluating the cosine similarity between the image and text embeddings,"}, {"title": "B. Experimental Setups", "content": "During our experiments, for the training, we employed AdamW [40] as the optimizer, with the learning rate and weight decay set to 1e-4 and 1e-6, respectively. We utilize the SPHINX [41] as our LLM-based model for fine-tuning, and we initialize the two ControlNet model parameters from [34]. Both batch sizes are set to 4 and the model is trained until it converges. The original image sizes C, H, K are all set to 512, The key region image sizes $C_k$, $H_k$, $K_k$ are all set to 128, and the highly compressed image sizes $C_{ci}$, $H_{ci}$, $K_{ci}$ are all set to 32. When inference, the $\\alpha$ is set to 0.5. In the underwater acoustic communication channel, o, is set at 1.5, s at 0.5, and v at 2 m/s. Depending on the SNR, the frequency f and the distance d are adjusted to maintain the desired ratio with respect to the original signal.\nOur experiments are implemented based on the Py-Torch [42]. All experiments are conducted on a server running Ubuntu 20.04, equipped with a 64-core 2.60GHz Xeon(R) CPU, 256 GB of RAM, and 4 NVIDIA 4090 GPUs, each with 24 GB of memory."}, {"title": "C. Evaluation on different approaches (E1)", "content": "To simulate real-world underwater scenarios, the experiments are conducted under 0, 3, 6, 9, 12, 15, 18 SNR settings.\nOur proposed method is compared with the following Three different approaches:\nTransmit only text data and recover with text.\nHighly compressed picture: Transmit only highly compressed picture and recover with picture resampling method.\nLatent and text: a novel language-oriented SC framework that communicates both text and a compressed image embedding [43].\nAs shown in Table I, the proposed method achieves a substantial reduction in payload size, decreasing it by 99.2% compared to the original image. On average, key regions constitute 28% of the total image size, and the vLLM generates an average of 83 characters. To show the performance under different noise scenarios, we evaluate the different metrics under different SNR settings, as shown in Fig. 4. As depicted in Fig. 2a, the proposed method consistently outperforms the baseline approaches in terms of FID scores, demonstrating its ability to generate high-quality, realistic images. Lower FID scores indicate better visual quality, and our method maintains a significant margin over the others, particularly at lower SNR levels. The high visual quality is due to the effective combination of key region emphasis and holistic image reconstruction. The Highly compressed picture method performs the worst because heavy compression degrades image quality, and it cannot be recovered without diffusion methods.\nFig. 2b illustrates that the proposed method attains the highest SSIM scores when SNR is less than 12, signifying excellent structural similarity with the original images in the highly noisy environment. The high SSIM scores are due to the method's focus on preserving both the key regions and the overall image structure. the SSIM for only transmitting the text do not significantly increase with higher SNR values because transmitting only textual information lacks the direct structural cues present in the visual data, making it inherently challenging to reconstruct the original image's structural details accurately. As a result, even as the noise decreases (higher SNR), the structural similarity cannot substantially improve because the text alone does not provide sufficient information to fully restore the image's spatial and structural attributes. The Highly compressed picture method outperforms the proposed method at higher SNR values due to its simpler approach, which benefits more from reduced noise and better preserves certain structural details.\nFig. 2c shows that our proposed method achieves the highest CLIP scores under a more noisy environment, indicating superior semantic alignment between the reconstructed images and the textual descriptions. The high CLIP scores for the proposed method are attributed to the effective integration of textual guidance and image reconstruction, ensuring that the reconstructed images closely match the input queries. The Highly compressed picture transmission does not show a steep increase in CLIP scores like the other methods because CLIP primarily measures the alignment between text and images. Since this method does not utilize textual guidance, it cannot leverage the reduction in textual noise to improve semantic alignment as effectively as the other methods. Additionally, the Latent and text transmission surpasses the proposed method at SNR = 18 because it transmits more information, with the transmission size being 1.3 times that of the proposed method.\nThe LPIPS scores presented in Fig. 2d reveal that the"}, {"title": "D. Ablation study (E2)", "content": "In this subsection, we systematically evaluate the impact of different components of our proposed Semantic Communication (SC) framework. Understanding the contribution of each component provides valuable insights into the robustness and adaptability of the framework. For this evaluation, we individually remove the Key Region ControlNet and the LLM recovery mechanism in the semantic decoder to assess their respective contributions. Specifically, removing the Global Vision ControlNet poses a unique challenge as the Key Region ControlNet alone is designed to recover only key regions. Thus, it would be inappropriate to measure the overall performance using standard metrics without the Global Vision ControlNet. To address this, we analyze the hyperparameters in the next section to demonstrate the impact of the Global Vision ControlNet comprehensively.\nFig. 4a presents the FID scores for the different configurations. The absence of the Key Region ControlNet and the LLM recovery part results in significantly higher FID scores under high noise conditions, indicating that these components are crucial for maintaining image quality in noisy environments. As the SNR values increase, the FID scores for all methods converge and exhibit less variation because the reduction in noise diminishes the adverse effects of missing components.\nFig. 4b shows that the removal of key components leads to a noticeable decrease in SSIM scores, underscoring their importance in preserving structural details in the reconstructed images. Notably, the SSIM scores for the configuration without LLM recovery do not increase as rapidly as those for the proposed method and the configuration without the Key Region ControlNet when the noise decreases. This phenomenon can be attributed to the fact that more accurate text information significantly aids in recovering structural similarity under low noise conditions.\nFig. 4c shows that the absence of the LLM recovery part leads to a significant drop in CLIP scores under high noise conditions, underscoring the crucial role of LLM recovery in restoring semantically relevant information. In contrast, removing the Key Region ControlNet results in a less pronounced, yet still noticeable, decline in CLIP scores. This indicates that while the reconstruction of key regions is less critical than LLM recovery for maintaining semantic alignment, it still provides considerable support in preserving language-related coherence.\nFig. 4d presents the LPIPS scores for the different configurations. Similar to the SSIM scores, the removal of key components results in higher LPIPS scores under high noise conditions, whereas under low noise conditions, the absence of the LLM recovery has a more pronounced negative impact. In high noise environments, the"}]}