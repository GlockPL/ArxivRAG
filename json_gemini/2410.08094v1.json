{"title": "SAKA: An Intelligent Platform for Semi-automated Knowledge Graph Construction and Application", "authors": ["Hanrong Zhang", "Xinyue Wang", "Jiabao Pan", "Hongwei Wang"], "abstract": "Knowledge graph (KG) technology is extensively utilized in many areas, and many companies offer applications based on KG. Nonetheless, the majority of KG platforms necessitate expertise and tremendous time and effort of users to construct KG records manually, which poses great difficulties for ordinary people to use. Additionally, audio data is abundant and holds valuable information, but it is challenging to transform it into a KG. What's more, the platforms usually do not leverage the full potential of the KGs constructed by users. In this paper, we propose an intelligent and user-friendly platform for Semi-automated KG Construction and Application (SAKA) to address the problems aforementioned. Primarily, users can semi-automatically construct KGs from structured data of numerous areas by interacting with the platform, based on which multi-versions of KG can be stored, viewed, managed, and updated. Moreover, we propose an Audio-based KG Information Extraction (AGIE) method to establish KGs from audio data. Lastly, the platform creates a semantic parsing-based knowledge base question answering (KBQA) system based on the user-created KGs. We prove the feasibility of the semi-automatic KG construction method on the SAKA platform.", "sections": [{"title": "1 Introduction", "content": "The rise of big data in recent years have posed significant difficulties in managing, processing and understanding vast amounts of data. Knowledge graph (KG), as a graph-based storing utility, encodes facts amongst various entities (nodes or ontologies), which offers a novel method to better arrange vast data. Based on the rapid development of machine learning technologies [6, 10, 14, 15, 22], KG is increasingly prevalent and has been applied to numerous fields, such as media and geography.\nDespite the potential benefits of KG, most KG platforms are complex and demand specialized expertise to use correctly. Constructing KGs manually requires significant time and effort, and this process is usually beyond the capabilities of the average user. This poses a significant challenge for many individuals who are interested in"}, {"title": "2 Related work", "content": "2.1 KG construction platform\nCurrently, there are numerous KG construction applications to build KGs. Neo4j is one of the popular KG construction and graph database platforms, which constructs KGs by structured data and Cypher statements. It requires users to"}, {"title": "2.2 Construction of KG", "content": "To construct a KG, entity extraction is performed first, followed by extracting relationships among entities to realize the construction of a KG. For entity extraction, Huang et al. used Bi-Long Short-Term Memory (Bi-LSTM) model combined with Conditional Random Field (CRF) model based on the word and phrase chunk-ing annotation method to achieve good results in entity recognition tasks [7]. Later, the emergence of the Bidirectional Encoder Representations from Transformers (BERT) model greatly influenced subsequent research on entity extraction tasks, verifying the feasibility of pre-trained models for entity extraction tasks and providing a new idea for entity extraction techniques [2]. For relationship extraction, the relationship extraction model proposed in [19] introduces an attention mechanism based on a convolutional neural network (CNN) for better extraction results. To reduce manual labeling costs, Mintz et al. proposed remote supervision for automatic labeling [11] After the BERT model was proposed, the model was widely used in the relational extraction task and was found to be better than CNN and Attention-CNN in the relational extraction task after experiments of Wu et al.[21]."}, {"title": "2.3 KBQA systems", "content": "Early work on KBQA concentrates on addressing a basic question with a single fact [1]. In recent years, researchers have begun to focus more on addressing complicated queries about KBs, i.e., the complex KBQA task [5]. There are two mainstream approaches proposed to address the simple KBQA: SP-based methods and information"}, {"title": "3 Methodology", "content": "3.1 KG Construction Based on Structured Data\nThe KG construction process by structured data is split into the following four procedures:\nUpload a JSON file containing the structured data in a certain format\nDefine the user-desired KG, which consists of entity types, relationships, entity attributes, and relationship attributes\nKG automatic construction based on the uploaded data and KG definition.\nDisplay the constructed KG, which can be searched, modified, and stored in the database.\nThe KG construction procedures flow is shown in Fig. 2. Users are required to operate the module under certain rules to construct the KG in the first two steps, which we will elaborate on in the next subsections. After that, the construction and the display of the KG are done automatically by the backend server."}, {"title": "3.1.1 Definition of Uploaded Data", "content": "Initially, the user should upload a JSON file containing all the entities belonging to the topic entity type Oo by numerous data entries. The format of each data entry can be defined as follows:\n{Oo: e, Ri: Ei, Aj : Attrj}, i = 1\u2026 n, j = 1\u2026 m\nwhere Oo denotes the topic entity type, and e denotes the entity of the topic entity type of the data entry. Ri denotes the relationships between the topic entity type and other entity types. Aj denotes the attributes of Oo, of which Attri denotes the attribute content. Ei denotes the entity set of other entity types, which can be defined as:\nEi = {Cio, Ci\u2081,\u2026\u2026, Cik}\nwhere k\u2265 0 and k are not fixed."}, {"title": "3.1.2 KG Definition", "content": "After uploading the data file to the system, the user should manually define the entity types, relationships, entity attributes, and relation attributes contained in the KG, which are also corresponding to the data file format defined above. Initially, the entity types and relationships should be defined. The entity types consist of the topic entity type Oo and other entity types 01 - On. This part should also define the relationships of Ri between the topic entity type and other entity types. Next, entity attributes and relation attributes need to be defined. The attributes here must have appeared as keys in the JSON data, i.e., one of Aj, so that the corresponding attribute values can be acquired when constructing the KG."}, {"title": "3.1.3 KG Construction", "content": "After the KG definition, the definition information and KG data file will be uploaded to the back-end server and the subsequent KG construction starts. Initially, the defined entity types, relationships, and attributes are mapped to the uploaded data file in JSON format to facilitate KG construction. A mapping process example is illustrated in Fig. 3. The topic entity type is Oo, whose attributes A1 and A2 are mapped to the entity e of the topic entity type. e is in relation 1 to e\u2081\u2081 and e\u2081\u2082 of entity type 1 and in relation 2 to e2\u2081 of entity type 2."}, {"title": "3.2 KG Construction Based on Audio", "content": "3.2.1 The AGIE method\nDespite the structured data-based method, we propose the AGIE method to establish KGs based on audio on the KG platform. The AGIE method implements audio-preprocessing algorithms to distinguish the speakers in the audio and convert the audio segments into text. Then, the proposed method uses the MIE model [25] to extract entities and relations from the dialogue to generate the KG."}, {"title": "3.2.2 Audio Preprocessing", "content": "There are two steps of audio preprocessing. First, imply the VAD model removes the non-speech parts of the audio, and then the SD model is used to find the speaker segmentation points. The VAD model uses the ResNet network to train MFCC features of the audio data and classify speech and non-speech segments. After eliminating the non-speech section with the VAD model, the method uses the SD model to identify speakers in the dialogue. Based on the GE2E model [18], the proposed SD method can generate the d-vector of the audio [16], which represents the feature map of the audio data. The GE2E model is structured with multi-layer LSTM. During training, the model obtains 40-mel Filterbank feature from the audio to learn the d-vector feature map. The structures of the proposed models are presented in Fig. 4."}, {"title": "3.2.3 Relationship Extraction Model", "content": "With the preprocessed-audio clip, the method can convert audio to text. Then, the relation extraction model is applied to extract information from the converted dialogue. In this paper, we applied the Medical Information Extractor (MIE) model [25] as the information extractor. The MIE model is trained to capture critical information in dialogues, with Bi-LSTM layers [4] and attention mechanism [17] to learn the time-series-based dialogue information and emphasize the keyword in the conversation.\nThe MIE model has four modules: encoder module, matching module, aggregation module, and scoring module.\nThe encoder module obtains bi-directional features based on Bi-LSTM layers. Then, the module"}, {"title": "3.3 KBQA module Based on User-constructed KB", "content": "We accomplish a KBQA module that can answer users' natural language questions. It utilizes the user-constructed KG as a KB, which is stored in the graph database Neo4j. Neo4j supports the KBQA service with Cypher query statements as the search SQL to search answers in the database. Next, we will elaborate on the technical architecture, the data collection method, and the KBQA implementation details."}, {"title": "3.3.1 Technical Architecture", "content": "The technical architecture of the KBQA module is illustrated in Fig. 5. First, the question should be input by the user. Next, the question can be classified to obtain the question types and entities involved in it based on the region words and interrogative words KB. After that, according to the question types and entities, the question can be parsed to acquire the corresponding Cypher statements of the question. They can be used to query the Neo4j database, which stores the KG constructed by the crawled structured data, to obtain the answers. Finally, the answer beautifier can embellish the results returned by the KB according to pre-defined answer templates to obtain the final answers."}, {"title": "3.3.2 Data Crawling", "content": "Initially, the Urllib library in Python is utilized to request the HTML file of a certain website based on the URL of the main webpage. Then we parse the URLs of the HTML file to obtain more URLS of the classified information, which we can continue to crawl further information. Next, we parse the crawled files to acquire the basic information of the medical fields [23], which are divided into numerous types of knowledge afterward."}, {"title": "3.3.3 Classification of Question", "content": "In the classification stage, we need to obtain the question types and the entities by utilizing traditional rule-based matching algorithms and string-matching methods from the question input by a user. The whole process is illustrated in Fig. 6. First, we need to utilize interrogative words in"}, {"title": "3.3.4 Parse and Search Question", "content": "In this section, the KBQA module produces the appropriate Cypher query statements based on the classified question types. Each question type corresponds to one Cypher query template. Notably, each question may be converted to several Cypher statements as it may involve several entities. Then Cypher statements are executed in the Neo4j database storing the KB constructed previously. After that, the KB will return the raw results corresponding to the question. Finally, the answer beautifier module of the KBQA will call the related reply template to embellish the raw answer according to the related question type, and then return the final answer of the question to the user."}, {"title": "4 Results", "content": "To better demonstrate the functions of our system, we utilize our method and model to construct the KG and the KBQA module based on the structured data crawled from the medical field. In this section, primarily, we present the KG scale constructed by the user. After that, we evaluate the performance of our model to construct KG by medical audio data. Finally, we illustrate the supported QA types of the KBQA module."}, {"title": "4.1 Scale of KG Constructed by Structured Data", "content": "We crawled from the medical website (jib.xywy.com) to collect structured data in the medical domain used to construct a medical KG. The entity types of the KG consist of check items, department, disease, drug, food, producer, and symptom, with altogether about 33,000 entities. The relation scale of KG is altogether about 230,000 relations."}, {"title": "4.2 Evaluation of KG Constructed by Audio Data", "content": "4.2.1 The Results of VAD Model\nWe applied the Librispeech [13] dataset to evaluate the results in the VAD task. LibriSpeech is a corpus containing 1000 hours of English speech in 16 kHz. The audio is derived from people reading books from the LibriVox project and has been carefully preprocessed. Trained with the Librispeech dataset, the final accuracy of the validation set is 97.42%, which indicates that our trained VAD model can distinguish non-speech and speech sections effectively."}, {"title": "4.2.2 The Evaluation of SD Model", "content": "To prove the effectiveness of the SD model, we used LibriSpeech and VoxCeleb [12] datasets to train and validate the GE2E model. The VoxCeleb is a large-scale dataset for speaker identification. This data is collected from over 1,200 speakers of different accents, ages, and ethnicities. After training, the final EER (Equal Error Rate) is"}, {"title": "4.2.3 The Results of MIE Model", "content": "The doctor-patient dialogue dataset generated by Zhang et al. [25] is used to train and test the MIE model. This dataset uses the dialogues between patients and doctors from medical websites, and the labels are manually annotated.\nTo verify the effectiveness of our method, we compare the MIE method with baselines from [25]. Plain-Classifier The classifier extracts features based on Bi-LSTM and self-attention mechanism to generate vectors. Then, the vectors are used to train the classifier.\nMIE-Classifier This classifier utilized the MIE model architecture. The MIE-Classifier treats cutt c and cutt s directly as qe and qs, which is different from the situation in the MIE method."}, {"title": "4.3 Supported QA Types of KBQA", "content": "The KBQA module constructed by crawled data supports altogether 18 types of questions, which are shown in Table 4. The QA examples are shown in Table 5."}, {"title": "5 System Presentation", "content": "In this section, first, we will introduce the technical route of the system. Next, we will illustrate the KG construction procedures and the ways to retrieve information in the system. After that, we will introduce the management of different KG versions. Finally, we will illustrate the KBQA module."}, {"title": "5.1 Technical Route", "content": "This whole system is developed using a front-end and back-end isolation strategy, with data being transferred between them using the HTTP protocol and the Restful API. The front-end frameworks used are Vue and ECharts, and the back-end framework used is Django. Additionally, we utilize graph database Neo4j to store KG data, and relational database SQLite to store the metadata of each KG."}, {"title": "5.2 KG Construction Based on Structured Data", "content": "5.2.1 Construction Procedures\nThe KG construction flow is illustrated in Fig. 8. Primarily, you need to create an ontology by defining its name, which is used to distinguish different versions of KGs. If the name does not exist in the database used to store the meta information of ontologies, it will be stored in the database. Otherwise, the system will prompt the duplicate name and you should change a different name. Next, you can upload the structured data file of the KG in JSON format. After that, you need to define the entity types and relations of the ontology by dragging entity type boxes and relation lines on the webpage. Additionally, you can delete unwanted entity types and relations. What's more, you can add a nickname and entity properties to the ontology. Finally, the system will complete KG construction in the next step. Then the system will display some of the entities and relations randomly of the KG. As illustrated in subfigure (d) in Fig. 8, different colors represent different entity types, and the arrows represent the relations between the"}, {"title": "5.2.2 Information Retrieval", "content": "We can retrieve information on nodes and relations after constructing the KG. First, we can query a single entity by searching its name in the right panel, as depicted in subfigure (a) in Fig. 9. What's more, we can retrieve all entities in an entity type or all graphs containing a relation by clicking the entity type and relation directly in the right panel, as shown in subfigures (b) and (c) in Fig. 9. Moreover, we can retrieve the attributes of each node and relation by clicking it directly, which will be shown in the right panel directly. Finally, undesirable nodes and relations can be deleted forever by clicking the trash button on the right panel."}, {"title": "5.3 KG Construction Based on Unstructured Data", "content": "We can construct a KG from unstructured data by either uploading a recorded MP3 conversation audio or recording it in real time."}, {"title": "5.3.1 Upload the recorded audio", "content": "Initially, enter the user's basic information and submit the conversation audio, of which the format is MP3 and the sample rate is 16K. The system will analyze the audio after uploading and convert the voice into text, as illustrated in Fig. 10.\nAfter clicking the next step button, the information will be extracted from the text by the MIE model. Next, the system will return the analyzed outcomes and present the results based on the KG once the analysis is accomplished, as depicted in Fig. 11."}, {"title": "5.3.2 Record dialogues in real time", "content": "Initially, enter the basic information of the users and begin to record the conversation in real time. At any point throughout the recording process, customers have the option to stop and restart the recording. They may also download the audio in the WAV file or play it again. The recording webpage is shown in Fig. 12.\nAfter uploading the audio, the system will analyze it and use the MIE model to extract crucial information from the conversation. The system then displays the results as demonstrated in Fig. 13.\nNext, the system mainly presents two types of information. First, on the left side, it will demonstrate the information of patients. Moreover, the extraction results of the dialogue will also be visually illustrated in the KG format. Secondly, the analysis of the data extracted from the patient based on KG is shown on the right side. It displays some crucial details on other patients who have the same symptoms as the present user. For instance, in Fig. 13, it is determined that the user \"Maddy\" exhibits symptoms of arrhythmia. As a result, the right section lists statistical data about other individuals who have arrhythmia. It may aid in a more thorough analysis of the patient by the doctor."}, {"title": "5.4 KG Version Management", "content": "We can store the constructed KGs in a graph database, i.e. Neo4j, which can be viewed whenever needed. Additionally, we can add new data in the same format to the constructed KGs whenever we want. To achieve the function, we store the metadata of each KG in a relational database, i.e., SQLite, of which the database schema is shown in Fig. 14. The database model consists of three tables: KG, Lable, and Relation. The primary key of the KG table is \"name\", which represents the name of a KG. That's why the name of each KG cannot be the same. It also serves as the foreign key of the Label and Relation tables, which are used to store the entity types and relations in an ontology respectively. The KG table is in a one-to-many relationship with the Label and Relation tables.\nAs illustrated in Fig. 15, the system can present all the KGs and their specific information, such as their creation time and data type, according to the metadata of the KGs stored in the database. Moreover, we can choose a KG to view its entity types and relations. You can manage the KGs by deleting undesirable KGs, of which all entities and relations will be deleted in the Neo4j database and all metadata will be deleted in the SQLite."}, {"title": "5.5 KBQA Module", "content": "As depicted in Fig. 16, this KBQA system can receive the questions of users and then return accurate and concise answers to users. It is conducive to promoting access to health knowledge for ordinary people by applying the KBQA system to the medical area. Moreover, it can assist doctors to diagnose efficiently, which considerably alleviates the medical pressure on society."}, {"title": "6 Conclusion", "content": "In this article, we propose an intelligent KG construction and application platform SAKA and prove the automatic KG construction method feasible on the SAKA. It offers a user-friendly and intuitive KG construction technique, which only requires data upload and button operation to achieve semi-automatic KG construction, application, and management, in contrast to other platforms requiring expert knowledge and computing ability. Moreover, we propose the AGIE method to construct KGs by extracting semantic information from structured and audio data. We also evaluate the effectiveness of AGIE on serval datasets. Moreover, we also develop a KBQA module based on the KGs constructed by users.\nNevertheless, several potential limitations of the SAKA platform still exist. Primarily, the scalability of SAKA might be a problem when encountering large-scale KGs. This is an aspect we aim to address in our future work to ensure optimal performance even under heavy data loads. The handling of noisy data was also identified as a critical challenge. We are also planning on implementing more sophisticated error-handling mechanisms to tackle this issue. Finally, the need for handling domain-specific knowledge more efficiently was brought to our attention. Although our current platform permits users to customize entity types and relations, it is necessary to accommodate complex domain-specific scenarios more effectively. Future developments will aim to extend the platform's capabilities by integrating domain-specific models and rules."}]}