{"title": "SyncSpeech: Low-Latency and Efficient Dual-Stream Text-to-Speech based on Temporal Masked Transformer", "authors": ["Zhengyan Sheng", "Zhihao Du", "Shiliang Zhang", "Zhijie Yan", "Yexin Yang", "Zhenhua Ling"], "abstract": "This paper presents a dual-stream text-to-speech (TTS) model, SyncSpeech, capable of receiving streaming text input from upstream models while simultaneously generating streaming speech, facilitating seamless interaction with large language models. SyncSpeech has the following advantages: Low latency, as it begins generating streaming speech upon receiving the second text token; High efficiency, as it decodes all speech tokens corresponding to the each arrived text token in one step. To achieve this, we propose a temporal masked transformer as the backbone of SyncSpeech, combined with token-level duration prediction to predict speech tokens and the duration for the next step. Additionally, we design a two-stage training strategy to improve training efficiency and the quality of generated speech. We evaluated the SyncSpeech on both English and Mandarin datasets. Compared to the recent dual-stream TTS models, SyncSpeech significantly reduces the first packet delay of speech tokens and accelerates the real-time factor. Moreover, with the same data scale, SyncSpeech achieves performance comparable to that of traditional autoregressive-based TTS models in terms of both speech quality and robustness. Speech samples are available at https://SyncSpeech.github.io/.", "sections": [{"title": "1 Introduction", "content": "In recent years, with advancements in generative models and the expansion of training datasets, text-to-speech (TTS) models (Wang et al., 2023; Le et al., 2023; Ju et al., 2024) have made breakthrough progress in naturalness and quality, gradually approaching the level of real recordings. However, low-latency and efficient dual-stream TTS, which involves processing streaming text inputs while simultaneously generating speech in real time, remains a challenging problem (Dang et al., 2024). These models are ideal for integration with upstream tasks, such as large language models (LLMs) (OpenAI, 2023) and streaming translation models (Barrault et al., 2023), which can generate text in a streaming manner. Addressing these challenges can improve live human-computer interaction, paving the way for various applications, such as speech-to-speech translation and personal voice assistants.\nRecently, inspired by advances in image generation, denoising diffusion (Ho et al., 2020; Song et al., 2021), flow matching (Lipman et al., 2023), and masked generative models (Chang et al., 2022) have been introduced into non-autoregressive (NAR) TTS (Anastassiou et al., 2024; Chen et al., 2024b; Kim et al., 2023; Wang et al., 2024), demonstrating impressive performance in offline inference. During this process, these offline TTS models first add noise or apply masking guided by the predicted duration. Subsequently, context from the entire sentence is leveraged to perform temporally-unordered denoising or mask prediction for speech generation. However, this temporally-unordered process hinders their application to streaming speech generation\u00b9.\nWhen it comes to streaming speech generation, autoregressive (AR) TTS models (Wang et al., 2023; Song et al., 2024) hold a distinct advantage because of their ability to deliver outputs in a temporally-ordered manner. However, compared to recently proposed NAR TTS models, AR TTS models have a distinct disadvantage in terms of generation efficiency (Li et al., 2024). Specifically, the autoregressive steps are tied to the frame rate of speech tokens, resulting in slower inference speeds. While advancements like VALL-E 2 (Chen et al., 2024a) have boosted generation efficiency through"}, {"title": "2 Related Work", "content": "Text-to-Speech, the transformation of text into audible signals understandable by humans, is pivotal for human-computer interaction. TTS systems can be mainly divided into AR-based and NAR-based categories. For AR-based systems, VALL-E (Wang et al., 2023) predicts the first layer of acoustic tokens extracted by EnCodec (D\u00e9fossez et al., 2023) using an AR codec language model, while a NAR model is used to predict the remaining layers. CosyVoice (Du et al., 2024a) employs an AR model to predict supervised semantic representations and combines flow matching to predict acoustic representations. AR-based TTS models, with their in-context learning capability, can generate natural, prosody-diverse speech in a streaming manner. However, AR-based TTS models exhibit shortcomings in generation efficiency. Besides the previously mentioned VALL-E 2 (Chen et al., 2024a), MEDUSA (Li et al., 2024) and VALL-E R (Han et al., 2024) introduce speculative decoding (Leviathan et al., 2023) and a codec-merging method, respectively, to accelerate autoregressive generation. Nonetheless, the efficiency gains achieved by these approaches remain limited, unable to perform synchronized decoding steps with text tokens.\nFor NAR-based TTS models, most previous approaches require speech duration prediction conditioned on the input text, followed by upsampling the text representations to match the acoustic feature length before feeding them into the generation model. Following FastSpeech (Ren et al., 2021), VoiceBox (Le et al., 2023) and Natural-Speech 2 (Shen et al., 2024) predict phone-level durations using a regression-based approach. NaturalSpeech 3 (Ju et al., 2024) adopts a discrete diffusion model, combining classification loss and duration prompts for duration prediction, which outperforms text-dependent regression-based duration prediction in terms of speech robustness and quality. However, NaturalSpeech 3 requires an additional"}, {"title": "2.2 Speech Large Language Models", "content": "Speech Large Language Models (SLLMs) empower LLMs to interact with users through speech, responding to user's instruction with low latency (Ji et al., 2024a). A basic approach (Huang et al., 2024) to achieve this speech interaction involves a cascade of automatic speech recognition (ASR), LLM and TTS models, where the ASR transcribes the users' speech instruction into text, and the TTS model converts the LLM's textual response into speech. However, most current AR TTS models cannot process streaming text input, resulting in significant latency in the aforementioned cascaded systems. In contrast, some end-to-end speech-language models have been proposed that can generate speech tokens directly, thereby achieving extremely low response latency. LLaMA-Omni (Fang et al., 2024) aligns the hidden states of LLMs with discrete HuBERT (Hsu et al., 2021) representa-"}, {"title": "3 Method", "content": "A dual-stream TTS model simultaneously processes streaming text input and generates speech in a streaming manner. Upon receiving newly generated text tokens $y_{arr}$ from the upstream LLMs, the objective of the dual-streaming TTS it to estimate $p(x_{arr}|y_{arr},x_{pre},y_{pre})$. In this context, $x_{arr}$ represents the speech waveform segment corresponding to $y_{arr}$, while $y_{pre}$ and $x_{pre}$ denote the preceding text tokens and its corresponding speech waveform, respectively.\nSyncSpeech is a two-stage TTS system, consisting of the text-to-token and token-to-speech stages. The estimation of $p(x_{arr}|y_{arr},x_{pre},y_{pre})$ is decomposed into a text-to-token model $p(s_{arr}|y_{arr},x_{pre},y_{pre})$ and a token-to-speech model $p(x_{arr}|s_{arr})$, where $s_{arr}$ is the speech tokens corresponding to the speech waveform segment $x_{arr}$. Specifically, the proposed TMT is adopted as the backbone of text-to-token model. Then, an off-"}, {"title": "3.1 Training", "content": "Given a dataset of transcribed speech $(x,\\tilde{y})$, where $x$ and $\\tilde{y}$ denote an audio sample and its transcript, respectively, the transcript $\\tilde{y}$ is tokenized into a BPE token sequence $y = [y_1,y_2,y_3, \\dots,y_L]$, where L is the number of BPE tokens. An off-the-shelf speech tokenizer is used to encode the speech sample $x$ into T frame discrete speech tokens $s = [s_1,s_2,s_3, \\dots,s_T]$. We further define duration tokens $a = [a_1,a_2,a_3, \\dots,a_L]$ as the positions indicating the end time of each corresponding BPE token within the speech token sequence, with $a_L = T$. For a pair of $(x,\\tilde{y})$, a can be obtained through an open-source alignment tool.\nAs shown in Figure 1, to maintain consistency with the inference process (see Section 3.4), the sequence input is then constructed as follows. We select a random number $n \\in [1,L]$, which indicates that when receiving streaming text input, SyncSpeech needs to generate the speech tokens corresponding to the n-th BPE token at this moment. To avoid unnatural pauses, SyncSpeech allows look ahead q text tokens, obtaining a truncated text token sequence $y' = [y_1,y_2,y_3, \\dots,y_{L'}]$, where $L' = min(L,n + q)$. Based on the duration tokens a, the truncated speech token sequence $s_{1:a_n} = [s_1,s_2, \\dots,s_{a_n}]$ is obtained. Then, we define the masked speech token sequence $s'$ and and corresponding binary mask m as follows,\n$s' = \\begin{cases}\ns_{1:a_n}m,\n\\end{cases}$\n(1)\n$m = [m_i]_{i=1}^{a_n}, m_{1:a_{n-1}} = 0, m_{a_{n-1}:a_n} = 1$.\n(2)\nThat is all speech tokens corresponding to $a_n$ are replaced with the specific mask token, while the rest remain unchanged. Then, the truncated text token sequence $y'$, along with the masked speech token sequence $s'$ and duration tokens a, are used to construct the input sequence as follows,\n$f = [y', E, D, s'_{1:a_1}, \\dots, D, s'_{a_{n-1}:a_n}, D]$.\n(3)"}, {"title": "3.2 Pretraining", "content": "While the aforementioned method aligns with the prediction process, it suffers from low training efficiency. This training inefficiency arises because, during each training step, only the gradients of speech tokens $s_{a_{n-1}:a_n}$ and durations for $y_{n+1}$ are backpropagated. To further improve the training efficiency, we first perform masked pre-training on the TMT.\nGiven speech tokens s of a speech sample, we obtain the masked speech tokens $\\hat{s} = s \\odot m$, where $m = [m_i]_{i=1}^{T}$ is a binary mask of speech tokens. We design the masking rules primarily from two"}, {"title": "3.3 Other Modules", "content": "In this subsection, we introduce the other modules in SyncSpeech besides TMT. 1) Text BPE tokenizer: To facilitate interaction with upstream LLMs, we utilize the Qwen tokenizer (Yang et al., 2024a) directly. 2) Speech tokenizer: the open-source supervised speech semantic (S3) tokenizer (Du et al., 2024b) is selected, which operates at 25 Hz. The S3 tokenizer is developed by integrating finite scalar quantization (FSQ) (Mentzer et al., 2024) into the intermediate representations of an ASR model trained on large-scale data, and then fine-tuning it for the ASR task. 3) The off-the-shelf speech decoder (Du et al., 2024b) is based on the conditional flow matching (CFM) decoder"}, {"title": "3.4 Inference", "content": "During the inference process, SyncSpeech processes text in a streaming manner and synchronously generates speech, with the general algorithm flow shown in Algorithm 1. Specifically, when the number of input text BPE tokens y exceeds the look-ahead number q, the input sequence $f = [y, D]$ is built, which is fed into TMT to predict the duration of speech tokens corresponding to $y_1$. Then, based on the predicted duration, we perform sequence padding by inserting the mask tokens and a duration prediction placeholder. Subsequently, the sequence is fed back into TMT for synchronous mask prediction of $y_1$ and the duration prediction of $y_2$, followed by the input sequence s update and padding. For subsequent BPE token input, the above prediction step, update step, and padding step are repeated to generate speech tokens"}, {"title": "4 Experiments", "content": "Datasets We trained SyncSpeech on datasets in both English and Mandarin, including the 585-hour LibriTTS (Zen et al., 2019) dataset and 600 hours of internal Mandarin datasets. The internal Mandarin dataset was further expanded to approximately 2000 hours, employing techniques such as speed alteration and pitch shifting. The Montreal Forced Aligner (MFA) (McAuliffe et al., 2017) aligned transcripts according to its phone set, after which the alignment was transformed into text BPE-level format. We evaluated SyncSpeech using three benchmarks: (1) LibriSpeech text-clean (Panayotov et al., 2015), a standard English TTS evaluation set; (2) SeedTTS test-zh (Anastassiou et al., 2024), with 2,000 samples from the out-of-domain Mandarin DiDiSpeech dataset (Guo et al., 2021); and (3) SeedTTS test-hard, containing approximately 400 difficult cases to evaluate TTS model robustness with repeated text, tongue twisters, and other complex synthesis scenarios.\nSettings We set the number of text tokens to look ahead q = 1. The chunk size of speech decoder is 15. TMT has 16 layers, 16 attention heads, 1024-dimensional embeddings, and 2048-dimensional feed-forward layers. SyncSpeech was trained on 4 NVIDIA A800 80G GPUs. The pre-training stage lasts for 70K steps, and the second stage lasts for 20K steps.\nBaseline Models This paper focuses on low-latency and efficient TTS in dual-stream scenarios. Under the same data scale, we reproduced the following baseline models for comparison: CosyVoice (Du et al., 2024a) and recently pro-"}, {"title": "4.2 Main Results", "content": "The evaluation results for SyncSpeech and the baseline models are presented in Table 1.\nSpeech Robustness We found that SyncSpeech exhibits different performance compared to the baselines across the three benchmarks. Specifically, on the LibriSpeech test-clean benchmark, the performance of SyncSpeech was very close to that of CosyVoice2 based on the WER metric, with only a minor difference of 0.07%. SyncSpeech achieved a lower WER score on the Seed test-zh set compared to CosyVoice and CosyVoice2, with improvements of 0.65% and 0.93%, respectively. A key difference between the English and Mandarin datasets is the higher compression rate of the LLM tokenizer for Mandarin. In English, one word typically equals one token, while in Mandarin, a common phrase often corresponds to a single token. This means that, compared to the baseline model, SyncSpeech is better suited to the high compression rate tokenizer of the upstream large model. Furthermore, on the Seed test-hard set, the robustness advantage of SyncSpeech was even more pronounced, with the improvements 9.05% and 4.40%, respectively. In handling complex text, the explicit duration modeling in SyncSpeech helped the model learn the alignment between text and speech.\nSpeaker Similarity Due to the same speech decoder and the excellent voice disentanglement capability of the speech tokens, SyncSpeech, CosyVoice, and CosyVoice2 exhibited similar per-\nformance in terms of speaker similarity.\nSpeech Naturalness The MOS-N scores for SyncSpeech and CosyVoice2 were quite similar on the LibriSpeech text-clean, indicating that the naturalness of the generated speech was generally comparable. On the Seed test-zh benchmark, SyncSpeech outperformed CosyVoice2 by 0.08. In the Seed test-hard benchmark, high WER and uncommon text led to unnatural prosody and generally low MOS-N scores in the generated speech.\nLatency SyncSpeech has made a breakthrough in terms of latency, as shown in Table 1. Specifically, on the LibriSpeech test-clean benchmark, SyncSpeech was approximately 4 times faster than traditional AR models and over 20 times faster than the SOTA offline models in terms of FPL-A. On the Seed test-zh benchmark, SyncSpeech achieved speed improvements of over 5 times and 30 times, respectively. When receiving streaming text from the upstream large model (FPL-L), SyncSpeech can begin generating speech with just two text tokens. In contrast, CosyVoice2 requires five tokens, while CosyVoice and other baseline models need the entire text input. This highlights the distinct advantage of SyncSpeech in practical applications.\nEfficiency In terms of RTF, SyncSpeech is about 6.4 times faster on the LibriSpeech test-clean benchmark and about 8.6 times faster on the Seed test-zh benchmark compared to previous AR models. On the Seed test-hard set, due to the increased number of text tokens caused by the uncommon"}, {"title": "5 Analysis", "content": "Sampling Strategy In the LibriSpeech validation set, we provided the ground-truth durations and applied greedy search along with different Top-k thresholds for duration prediction, as shown in Table 2. We found that, in terms of speech robustness, both Top-k 3 and greedy search outperformed the use of ground-truth durations in terms of the WER metric. This is because the model struggled to effectively generalize to anomalies in the ground-truth durations. We employed UTMOSv24 as a surrogate objective metric of MOS-N. In terms of speech naturalness, the results of Top-k 3 sampling are slightly better than those with the given ground-truth durations. Additionally, we applied different Top-k thresholds for speech token prediction. SyncSpeech exhibited superior performance during greedy search, which is different from the previous AR TTS models or offline models. This is because the speech tokens obtained through single-step decoding have the temporal dependency, which cannot be compensated by subsequent generation.\nNumber of Look-ahead Tokens We evaluated how varying the number of tokens to look ahead affects speech robustness and speech naturalness on two validation sets, with the results presented in Table 3. We discovered that the optimal number of look-ahead text tokens varies across different"}, {"title": "Ablation Study", "content": "We conducted an ablation study on the pre-training strategy by directly training the randomly initialized model in a manner consistent with the prediction process. The WER results on the two validation sets are shown in Table 4. We found that pre-training significantly improved the speech robustness of the model, improving the WER metric by 1.17% and 1.06% on the two languages, respectively. This indicated that masked pre-training not only improved training efficiency but also enhanced the robustness of the synthesized speech. Additionally, a standard causal attention mask was applied to replace the designed attention mask, as shown in Table 4. If the mask token sequence of the same text token cannot attend to each other during inference, the robustness of the generated speech significantly decreased. This further demonstrated the effectiveness of the designed attention mask."}, {"title": "6 Conclusion", "content": "This paper presents SyncSpeech, a dual-stream speech generation model built on a temporal masked transformer. SyncSpeech can efficiently generate low-latency streaming speech from the"}, {"title": "7 Limitations", "content": "In this section, we will analyze the limitations of SyncSpeech and discuss potential future work. SyncSpeech requires token-level alignment information, which is challenging to achieve for sentences with mixed languages, and preprocessing becomes time-consuming on large-scale datasets. In the future, we will explore semi-supervised duration prediction, which only requires the duration of a complete sentence without strict token-level alignment information, and integrate SyncSpeech into SLLM as a speech generation module. In addition, since the off-and-shelf streaming speech decoder relies on flow matching, it limits the off-the-shelf RTF and the FPL. Moreover, current single-codebook acoustic tokens, such as WavTokenizer (Ji et al., 2024b), do not support streaming decoding. In the future, we will investigate efficient and low-latency streaming speech decoders."}, {"title": "A Details of Baselines", "content": "Cosy Voice A two-stage large-scale TTS system. The first stage is an autoregressive model similar to VALL-E (Wang et al., 2023), and the second stage"}, {"title": "C Duration Control", "content": "Since we have implemented duration prediction and control, we can multiply the predicted durations by a modulation factor to adjust speech rate. The results, shown in Table 5, indicate that the robustness of synthesized speech is optimal when the modulation factor is 1.1. However, when the modulation factor is too small or too large, the WER of the synthesized speech by SyncSpeech increases significantly. This is because when we multiply the predicted duration of each text token by a fixed modulation factor of less than 1, SyncSpeech's contextual learning capability causes the subsequent"}, {"title": "D Other Strategies for Sequence Construction", "content": "We also experimented with other sequence construction strategies. (1) One approach is to separate duration prediction and speech tokens prediction into two steps. This method reduces efficiency by half but achieves better speech robustness, with a WER of around 2.75 on the LibriSpeech test-clean dataset. (2) We also tried removing the duration placeholder and using the last speech token of the previous text token to predict the number of speech tokens corresponding to the current text token. However, we found that this sequence construction made the corresponding pre-training less effective than it is now. (3) We also attempted a method similar to ELLA-V (Song et al., 2024), where the corresponding text token is placed before each placeholder. However, we found that this sequence generated speech that was unnatural, with a noticeable disconnection between words."}, {"title": "B Details of Latency and Efficiency Evaluation Metrics", "content": "The first-package latency (FPL) and real-time factor (RTF) are two import metrics for streaming TTS models. We define $d_{LLM}$ as the average time required by the upstream LLM to generate one text token and $d_{TTS}$ as the the time for the corresponding AR TTS models to forward one step and for the NAR TTS models to perform one sampling. The FPL-L of baseline models and SyncSpeech are as follows,\n$L_{FPL-L}^{CosyVoice} = Ld_{LLM} + 15 \\cdot d_{TTS}$ (9)\n$L_{FPL-L}^{VALL-E} = Ld_{LLM} + 15 \\cdot d_{TTS}$ (10)\n$L_{FPL-L}^{CosyVoice2} = 5d_{LLM} + 15 \\cdot d_{TTS}$ (11)\n$L_{FPL-L}^{MaskGCT} = Ld_{LLM} + b \\cdot d_{TTS}$ (12)\n$L_{FPL-L}^{F5-TTS} = Ld_{LLM} + b \\cdot d_{TTS}$ (13)\n$L_{FPL-L}^{SyncSpeech} = (k + 1) \\cdot d_{LLM} + c \\cdot d_{TTS}$ (14)\nwhere b represents the number of sampling iterations for the NAR model, and c denotes the number of BPE text tokens when the generated speech tokens surpass the decoder's chunk size, typically ranging from 1 to 3. Here, we assume the upstream LLM model is Qwen-7B, and when running on a single NVIDIA A800 GPU, we obtain an average token generation time $d_{LLM} = 25ms$. When the first term in FPL-L is omitted, it becomes FPL-A. It is important to note that when calculating above metrics, we did not apply any engineering optimizations, such as KV cache.\nWe also conducted a brief theoretical analysis of RTF for SyncSpeech. The RTF for SyncSpeech is calculated as follows,\n$L_{RTF} = \\frac{(L + 1) \\cdot d_{TTS}}{T \\cdot F}$ (15)\nwhere L and T represent the number of BPE tokens and speech tokens, respectively F refers to the frame length of the speech tokens. The time complexity for SyncSpeech to generate an entire sentence can be simplified to $O(L)$, whereas the time complexity for concurrent approaches, such as CosyVoice2 and IST-LM, is $O(T)$. As a result, SyncSpeech can significantly expedite speech generation."}]}