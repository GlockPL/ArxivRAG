{"title": "GPT-4V Cannot Generate Radiology Reports Yet", "authors": ["Yuyang Jiang", "Chacha Chen", "Benjamin M. Mervak", "Dang Nguyen", "Chenhao Tan"], "abstract": "GPT-4V's purported strong multimodal abilities raise interests in using it to auto-\nmate radiology report writing, but there lacks thorough evaluations. In this work,\nwe perform a systematic evaluation of GPT-4V in generating radiology reports\non two chest X-ray report datasets: MIMIC-CXR and IU X-RAY. We attempt\nto directly generate reports using GPT-4V through different prompting strategies\nand find that it fails terribly in both lexical metrics and clinical efficacy metrics.\nTo understand the low performance, we decompose the task into two steps: 1)\nthe medical image reasoning step of predicting medical condition labels from\nimages; and 2) the report synthesis step of generating reports from (groundtruth)\nconditions. We show that GPT-4V's performance in image reasoning is consistently\nlow across different prompts. In fact, the distributions of model-predicted labels\nremain constant regardless of which groundtruth conditions are present on the\nimage, suggesting that the model is not interpreting chest X-rays meaningfully.\nEven when given groundtruth conditions in report synthesis, its generated reports\nare less correct and less natural-sounding than a finetuned LLaMA-2. Altogether,\nour findings cast doubt on the viability of using GPT-4V in a radiology workflow.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) are becoming multimodal, and GPT-4V represents the state-of-the-\nart [1]. Similar to the claimed general-purpose capabilities in LLMs [2, 3], large multimodal models\n(LMMs) are supposed to possess advanced skills across a wide range of domains, including high-\nstakes scenarios such as medicine [4]. However, in the field of radiology report generation, where\nrelatively rich datasets are available, there has been inconclusive evidence regarding the performance\nof LMMs. Some studies [4, 5] claimed that GPT-4V performs well to some extent based on case\nstudies and qualitative analysis. In contrast, [6] found that the model is not yet a reliable tool for\nradiological image interpretation on a small private dataset. [7] observed that GPT-4V can generate\nstructured reports with incorrect content, as evidenced by case studies and qualitative analysis. To\nmake sense of these results, we aim to perform a systematic and in-depth evaluation of GPT-4V\nbeyond simply providing performance numbers.2\nTo do that, we perform three experiments as shown in Fig. 1 on two popular radiology report\ngeneration benchmarks, MIMIC-CXR and IU X-RAY. Our evaluation starts with Experiment 1:\ndirect report generation. Different from previous works [4, 5], we conduct a thorough evaluation\nof GPT-4V's capability to directly generate reports from chest X-rays, utilizing different prompting\nstrategies and assessing both lexical metrics, which measure how textually similar a generated report\nis to a reference report, and clinical efficacy metrics, which measure how clinically accurate it is.\nWe experiment with various prompting strategies, including zero-shot, contextual enhancement,\nchain-of-thought (CoT) [8], and few-shot in-context learning. Despite our various attempts, the\nperformance of GPT-4V is consistently low in both metrics.\nTo further investigate the reason for GPT-4V's poor performance, we break down report generation\ninto two steps, medical image reasoning and report synthesis given medical conditions. For Ex-\nperiment 2 (medical image reasoning), we first test whether GPT-4V can identify medical conditions\nfrom X-rays. Our findings indicate that GPT-4V's performance in identifying medical conditions from\nimages is unsatisfactory across different prompts. Based on limited capability results, we further\ncompare the difference between distributions of predicted medical condition labels conditioned on dif-\nferent groundtruth image labels. We find that GPT-4V cannot interpret medical images meaningfully\nas the distribution of predicted labels does not vary depend on the groundtruth label.\nFinally, in Experiment 3 (report synthesis), we explore whether bypassing the image reasoning\nbottleneck by providing groundtruth conditions enables GPT-4V to generate clinically usable reports.\nAs expected, reports generated by GPT-4V achieve higher clinical efficacy; however, the limited\nimprovement in lexical metrics suggests that GPT-4V-generated reports remain dissimilar to human-\nwritten reports in style. Most importantly, GPT-4V underperforms a finetuned LLaMA-2 in both\nlexical metrics and clinical efficacy metrics, calling into question its utility. We further validate our\nfindings by conducting an additional human reader study with a board-certified radiologist to assess\nthe clinical viability of GPT-4V-generated reports.\nIn summary, our key contributions and conclusions are as follows:\n\u2022 We perform the first systematic and in-depth evaluation to benchmark GPT-4V in radiology report\ngeneration. Our main conclusion is that GPT-4V cannot generate radiology reports yet.\n\u2022 By decomposing the task into medical image reasoning and report synthesis, we demonstrate that\nGPT-4V cannot interpret chest X-ray images meaningfully in the image reasoning step, and further\nvalidate this finding through rigorous hypothesis testing.\n\u2022 During report synthesis, we address the image reasoning bottleneck by providing groundtruth\nconditions. Nonetheless, both experimental results and human evaluations consistently show that\nGPT-4V performs worse than a finetuned LLaMA-2 baseline.\nWe include our code in the supplementary material."}, {"title": "Related Work", "content": "While there is an emerging line of work in investigating the direct application of GPT-4 in radiology\nreport generation, there lacks a systematic evaluation. [4, 5, 7] tested capabilities for general medical\napplications through case studies, including selected examples of chest X-ray reports with qualitative\nanalysis. [6] provided quantitative results on GPT-4V's accuracy in interpreting medical images, using\na small private dataset that includes chest X-rays. But their evaluation only focused on identifying the\nimaging modality (e.g., CT, ultrasound, or MRI) and the anatomical region of the pathology, rather\nthan assessing the overall quality of generated radiology reports. [9] evaluated GPT-4V on the public"}, {"title": "Experiment Setup", "content": "In this section, we provide an overview of our methods, datasets, and evaluation metrics.\nMethod. In Experiment 1 (Section 4.1), we evaluate GPT-4V's ability to directly generate radiology\nreport given chest X-ray images. We consider five variations of prompts as outlined in Table 1. Prompt\n1.1 (Basic generation) is a prompt to test the out-of-the-box capability of GPT-4V. We implement\nthree additional prompting strategies leveraging insights in prompt engineering: (1) inspired by [18],\nwe add relevant contextual information (i.e., the INDICATION) to derive Prompt 1.2 as \"Indication\nenhancement\", and add instructions on medical condition labels to Prompt 1.3 as \"+instruction\"\nenhancement; (2) we use a chain-of-thought (CoT) strategy in Prompt 1.4, eliciting the model with\ntwo steps: medical condition label prediction based on images followed by report synthesis based on\nthe predicted labels; (3) We adopt few-shot in-context learning by adding a few example image-report\npairs in Prompt 1.5. We compare these results with the state-of-the-art (SOTA) models.\nIn addition to evaluation of the end-to-end radiology report generation capability, we further evaluate\non the decomposed tasks: Experiment 2 (Section 4.2): chest X-ray image reasoning; and Experiment\n3 (Section 4.2): synthesizing a radiology report from given conditions. This decomposition allows\nus to look into the bottlenecks in the current generation performance. In Experiment 2, we prompt\nthe model to directly output medical condition labels from images (Prompt 2.1). In Experiment\n3, we bypass image reasoning to test GPT-4V's textual synthesis ability and provide groundtruth\nconditions to evaluate the model's report composition capability independently (Prompt 3.1). To\ncontextualize the performance of GPT-4V, we also report the performance of a finetuned LLaMA-2\n7B on groundtruth labels and groundtruth impressions following Alpaca [19].\nDataset and pre-processing. We use two chest X-ray datasets: MIMIC-CXR and IU X-RAY. The\nMIMIC-CXR dataset [20] contains chest X-ray images and their corresponding free-text radiology\nreports. The dataset includes 377,110 images from 227,835 studies. Each study has one radiology\nreport and one or more chest X-rays. The IU X-RAYdataset [21] (also known as \u201cOpen-i\") includes\n3996 de-identified radiology reports and 8121 associated images from the Indiana University hospital\nnetwork. For our evaluation, we randomly sample 300 studies from the MIMIC-CXR and IU\""}, {"title": "Results", "content": "We first evaluate the out-of-the-box capability of GPT-4V in generating radiology reports from chest\nX-ray images using basic generation (Prompt 1.1). Table 2 shows the results compared with existing\nstate-of-the-art (SOTA) models. Overall, GPT-4V significantly underperforms the state-of-the-art\nmodels on both lexical and clinical efficacy metrics, with the exception of the METEOR score on\nthe IU X-RAY dataset. The relatively better METEOR performance is due to its comprehensive\nevaluation criteria, which include synonymy and paraphrasing, not just exact word matches like\nBLEU and ROUGE. This allows METEOR to recognize semantic equivalents, even if the word choice\ndiffers. In other words, the generated report somewhat resembles a radiology report, although it fails\nat the exact word-level matching. For clinical efficacy metrics, the gaps to SOTA are consistently\nlarge. This suggests that GPT-4V struggles to accurately identify conditions in its generated reports\nfrom images alone.\nOur results are consistent across prompting strategies. Our prompting strategies include adding\ncontextual information, chain-of-thought reasoning, and few-shot prompting. While indication\nenhancement (Prompt 1.2) provides indication section as input in addition to chest X-rays and\nimproves many metrics for both datasets, it remains within the same range and does not significantly\nreduce the gap compared to SOTA. Instruction enhancement (Prompt 1.3) provides medical condition\ndescriptions and improves the Positive F1-5 by 11.2% in MIMIC-CXR, the most effective so far, but\nthere is still a significant gap to SOTA (54.26%). Chain-of-Thought (Prompt 1.4) performs similarly\nto instruction enhancement, as both follow the same labeling instructions. Few-Shot (Prompt 1.5)\nprovides image-report pairs as context and generally improves only lexical metrics, RadGraph F1,\nand Hallucination, while clinical correctness remains consistently low across both datasets. This\nindicates that while few-shot prompting might help GPT-4V mimic the format of groundtruth reports,\nit still falls short in generating accurate reports."}, {"title": "Experiment 2: Can GPT-4V interpret chest X-rays meaningfully?", "content": "In this section, we probe GPT-4V's ability to reason about chest X-ray images alone. Specifically,\nwe evaluate whether the model can meaningfully interpret chest X-ray images by measuring how\naccurately GPT-4V can label medical conditions present (positive F1). Table 3 provides an overview\nof GPT-4V's labeling performance under different prompting strategies.\nWe can see that GPT-4V cannot accurately specify positive conditions from given chest X-rays. This\ncan be highlighted by consistently poor Positive F1 scores observed for both datasets under various\nprompting strategies. Furthermore, this inability to accurately interpret images may directly contribute\nto GPT-4V's failure in generating high-quality reports, as confirmed by similar Positive F1 score of\n0.151 (MIMIC-CXR) and 0.072 (IU X-RAY) from the report synthesis phase of Chain-of-Thought\n(see Table 2), compared to 0.166 (MIMIC-CXR) and 0.072 (IU X-RAY) from the initial label\ngeneration phase of Chain-of-Thought.\nOverall, these results indicate GPT-4V's limited ability in identifying medical conditions from chest\nX-ray images, regardless of whether labels are derived from CoT 1st step or direct prompting.\nTesting whether GPT-4V generates labels based on given chest X-rays. Considering the failure\nof GPT-4V to accurately label medical conditions, we would like to investigate to what extent can\nGPT-4V predict meaningful labels given a specific chest X-ray image. To test this, we group chest X-\nrays by their groundtruth conditions and then analyze the generated label distribution for each group."}, {"title": "Experiment 3: Given groundtruth conditions, can GPT-4V generate reports?", "content": "Given that GPT-4V cannot perform image reasoning, we next investigate whether GPT-4V can\nproduce high-quality radiology reports when provided with accurate medical conditions. We conduct\nan experiment on report synthesis (Prompt 3.1) on GPT-4V and use a finetuned LLaMA-2 model as a\nbaseline for comparison.\nTable 5 shows that while using groundtruth conditions significantly enhances GPT-4V's clinical\naccuracy, it still does not perform as well as the finetuned LLaMA-2, particularly in matching the\ncontent of groundtruth reports. Progress in clinical accuracy is evidenced by large improvements\nin F1 scores for both datasets compared to basic generation (Prompt 1.1). However, there are\nonly minor changes in lexical metrics and RadGraph F1, which focus on entity relation matching\nin groundtruth reports, along with consistently large gaps with finetuned LLaMA-2, suggest that\ngroundtruth conditions are insufficient to align GPT-4V's writing closely with that of groundtruth\nreports. The higher scores of the finetuned LLaMA-2 in lexical metrics also indicate that finetuning\nopen models is an effective way to leverage existing datasets.\nHuman Evaluation To further evaluate the quality of GPT-4V-generated reports beyond automatic\nmetrics, we collaborate with a board-certified radiologist to conduct a human evaluation. From our\ntesting set of 300 studies, we randomly select 50 cases for blind human evaluation. The radiologist is\nprovided with anonymized chest X-ray images and randomly ordered IMPRESSION sections from\ngroundtruth reports, as well as reports generated by LLaMA-2 and GPT-4V. Both LLaMA-2 and\nGPT-4V are prompted with groundtruth medical conditions. The evaluation involves a detailed review\nof three reports per study case, assessing each report's clinical usability with a binary label as the first"}, {"title": "Limitations", "content": "In this paper, we use GPT-4V, one of the most capable LMMs across various domains, to conduct\na systematic evaluation of its capabilities in generating radiology reports. Comparisons with other\ngeneral-domain LMMs, including Google's Gemini and OpenAI's newer GPT-40, are reserved for\nfuture research. Note that at the time of our submission, GPT-40 API was not available via Microsoft\nAzure platform.\nAdditionally, we employ four common prompting strategies in our study and encourage future\nresearch to explore additional techniques, such as Self-Critique [32], to verify the robustness of our\nfindings. Due to resource constraints, we randomly select a 300-sample subset for overall evaluation\nand choose 50 samples for a human study. Besides, the human study is limited to a single radiologist's\nsubjective assessment, potentially influenced by their personal style and preferences. While our\nhuman evaluation could be improved by recruiting more radiologists, we believe that GPT-4V's poor\nperformance may not justify a significantly larger human evaluation. That said, our results suggest\nthat finetuned open models may hold the potential of fitting into the current radiologist workflow if\nwe can leverage medical image reasoning abilities of other models.\nDespite these limitations, we believe the findings from this paper are well-supported by our com-\nprehensive and detailed evaluation framework. Results from our work raise serious concerns about\nhow to safely integrate general-domain LMMs into real-world radiology workflows. It is worth\nnoting that OpenAI itself restricts the medical use of GPT-4V. In our experiments, especially with\nthe few-shot prompt, GPT-4V tends to return \"I'm sorry, but I cannot provide a diagnostic report\nor interpretation for medical images. If you have any medical concerns, please consult a qualified\nhealthcare professional who can provide a proper examination and diagnosis.\""}, {"title": "Conclusions", "content": "We perform a systematic evaluation of GPT-4V in radiology report generation on two chest X-ray\nbenchamarks. We find that GPT-4V cannot generate radiology reports, even across different prompting\nstrategies. To understand the low performance, we decompose the main task into image reasoning\nand report synthesis. The results demonstrate that GPT-4V struggles significantly with interpreting\nchest X-rays meaningfully, which directly impacts its ability to generate reports. Furthermore, even\nwhen we bypass this problem by providing groundtruth conditions, GPT-4V still underperforms a\nfinetuned LLaMA-2 baseline and consistently fails to replicate the writing style of groundtruth reports\nor meet the preferences of radiologists. Overall, our study highlights substantial concerns regarding\nthe feasibility of integrating GPT-4V into real radiology workflows."}, {"title": "Hypothesis Test", "content": "For the second test, we define null hypothesis Ho as Pm = Pn \u2200 group m, n. For the\noverall pool, we can construct a 13\u00d713 contingency table with each entry equal to Y and then\ncalculate expected count E for each entry. Finally, report x2 = \u03a3\u03a3j- E, Considering\nthe sparsity of original study pool, we report results of two different tables: (1) A modified table that\nreplaces zero elements with 0.001; (2) A reduced table with only six most frequent conditions in\nsubsample."}, {"title": "Pearson Correlation Coefficient", "content": "We approximate P using Pr(X = 1) to obtain an estimator\nPk of Pk for each group k. Furthermore, we illustrate the correlation Corr(Pm, Pn) for all groups m\nand n in Figure 3. It is noted that the condition \"Pleural Other\" doesn't seem to be highly correlated\nwith other groups. However, considering that \"Pleural Other\" only has one positive mention in\ngroundtruth conditions and this can be treated as an outlier."}]}