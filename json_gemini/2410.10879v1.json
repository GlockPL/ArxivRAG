{"title": "ENHANCING VISION-LANGUAGE MODEL PRE-TRAINING WITH IMAGE-TEXT PAIR PRUNING BASED ON WORD FREQUENCY", "authors": ["Mingliang Liang", "Martha Larson"], "abstract": "We propose Word-Frequency-based Image-Text Pair Pruning (WFPP), a novel data pruning method that improves the efficiency of VLMs. Unlike MetaCLIP, our method does not need metadata for pruning, but selects text-image pairs to prune based on the content of the text. Specifically, WFPP prunes text-image pairs containing high-frequency words across the entire training dataset. The effect of WFPP is to reduce the dominance of frequent words. The result a better balanced word-frequency distribution in the dataset, which is known to improve the training of word embedding models. After pre-training on the pruned subset, we fine-tuned the model on the entire dataset for one additional epoch to achieve better performance. Our experiments demonstrate that applying WFPP when training a CLIP model improves performance on a wide range of downstream tasks. WFPP also provides the advantage of speeding up pre-training by using fewer samples. Additionally, we analyze the training data before and after pruning to visualize how WFPP changes the balance of word frequencies. We hope our work encourages researchers to consider the distribution of words in the training data when pre-training VLMs, not limited to CLIP.\nKeywords Vision-Language Model Multimodal Data Data Pruning.", "sections": [{"title": "Introduction", "content": "Large-scale pre-trained Vision-Language Models (VLMs) are gaining popularity, because of their remarkable zero-shot transferability [1, 2, 3, 4, 5]. This makes the VLM a foundational model with wide applicability in a variety of downstream tasks [6, 7]. The success of VLMs rests on two key points: (1) Large-scale image-text pair datasets crawled from the Internet [8, 9, 10, 11]. (2) Large-scale transformers used as image and text encoder [12, 13].\nDespite the importance of the scale of large-scale datasets, previous work has shown that pruning the training dataset can lead to improvements. MetaCLIP [14] employs a strategy that leverages metadata associated with text-image pairs in order to create a subset of CLIP training data. MetaCLIP pruning results in a new dataset that is balanced at the level of metadata categories (called \"entries\"), such that the number of texts associated with any given category does not exceed a threshold. The improvements of pruning are accompanied by the computational speed-up that results when the size of the training dataset is reduced.\nIn this paper, we propose that the decision to prune an image-text pair should be based directly on information about the frequency of words in that pair. Our method, Word-Frequency-based Image-Text Pair Pruning (WFPP) is inspired by observations of the importance of balancing word frequencies for training word embedding models. Specifically, [15] introduced a technique that subsamples frequent words in text data, in order to speed up the training and enhance the quality of word representations. Like [15], we consider a dataset to be better balanced in terms of word-frequency when the frequencies of frequent words are less dramatically higher than the frequencies of infrequent words. Our idea is also consistent with work that has demonstrated that neural networks tend to learn more from the majority class due to the higher number of examples available [16, 17].\nWFPP uses a simple yet effective text-level score based on word probabilities to prune image-text pairs from the data set in which the text contains frequent words. Word balance could also be improved by removing individual words from the"}, {"title": "Related work", "content": "Due to its remarkable zero-shot transferability, the visual-semantic embedding model as a foundational model has received sustained attention from researchers.\nIn this section, we describe the pre-training methods for vision-language models and discuss related works to accelerate their pre-training.\nDeViSE [19] learns visual-semantic embedding from labeled embeddings which are generated from pre-trained skip-gram on 5.7 million documents (5.4 billion words) extracted from wikipedia.org. The semantic knowledge learned from language provides the zero-shot prediction capability of a visual model, which improves performance on unseen data. To enhance visual-semantic embeddings and achieve good zero-shot performance, CLIP and ALIGN scale the data to 400M to learn the better visual-semantic embedding that achieves remarkable zero-shot performance across 27 datasets [1, 4]. These models are pre-trained by contrastive learning, which pushes positive image-text pairs closer to each other and separates negative image-text pairs, aligning the vision and language by acquiring a visual-semantic embedding from the natural language supervision. However, pre-training VLMs on large-scale data are quite expensive, demanding thousands of GPU days [1, 20].\nIn order to train VLMs effectively, Fast Language-Image Pre-training (FLIP) [2] removes a large portion of image patches to speedup pre-training VLMs. FLIP uses ViT as an image encoder, reducing computation by 2-4\u00d7 by removing 50%-75% patches of the image while obtaining better accuracy than the unmasked model [2]. In addition, they randomly masked 50% of the text to pre-train the VLMs, However, this approach does not work well in text encoders, and the performance of zero-shot classification on ImageNet is decreased. Resource-efficient CLIP (RECLIP) [21] employs a smaller version of images for the initial pre-training of CLIP and subsequently fine-tunes the models using larger versions of the images. The pre-training RECLIP with an image size of 64 \u00d7 64 reduces compute resource usage by approximately 80% while still outperforming CLIP on image-text retrieval tasks [21]. Subsampling of Frequent Words for Contrastive Language-Image Pre-training (SW-CLIP) [18] proposed a frequency-based word subsampling technique to reduce text length by half for pre-training VLMs, but does not remove image-text pairs.\nMetadata-Curated Language-Image Pre-training (MetaCLIP) [14] creates a balanced subset based on the metadata distribution to pre-train VLMs. To balance the training data, MetaCLIP selects image-text pairs from the data pool where the text contains a metadata entry. These metadata entries consist of four components: WordNet synsets, Wiki unigram, Wiki bigram, and Wiki titles. MetaCLIP utilizes a rich metadata source with 500k entries covering a wide range of concepts. However, it only seeks a balance at the level of the entries (metadata categories), which could lead to certain words being under- or over-represented in the sampled training data. In this paper, we prune image-text pairs based on word frequency to create a more balanced subset for pre-training VLMs. Our approach also is easier to implement, as it does not require collecting and filtering thousands of entries or complex curation processes."}, {"title": "Method", "content": "In this section, we present WFPP, a method designed to enhance the pre-training of Vision-Language Models (VLMs) by strategically selecting image-text data from the dataset based on word frequency. Following the data pruning process, we can effectively pre-train the VLM using a reduced portion of the dataset without compromising its performance.\nFollowing our proposed principles for building more balanced data, we remove as much text as possible from the dataset that contains higher-frequency words. The removal probability of a text is defined by the joint probability of the words in the text. This approach maintains the diversity of the data when filtering the information, as well as maintaining a balance between frequent and infrequent words.\nTo achieve our goal, we first compute the frequency of words using the equation:\n$f(w_i) = \\frac{c(w_i)}{\\sum_{i=1}^{n} c(W_i)}$\nIn this equation, $f(w_i)$ represents the frequency of the word $w_i$, and $c(w_i)$ stands for the word count for $w_i$. Next, we determine the probability of a word being discarded according to Eq. 2:\nP(wi) = \n\\begin{cases}\n1 - \\sqrt{\\frac{t}{f(w_i)}} & f(w_i) > t\\\\\notherwise\n\\end{cases}"}, {"title": "Experiments", "content": "4.1 Implementation Details\nDataset In our experiments, we utilize CC3M and CC12M to pre-train our model which includes about 3M and 12M image-text pairs [8, 9]. These datasets were chosen because they collect a large number of different image-text pairs, providing diverse content for pre-training effective VLMs. We employed various methods for subsampling the dataset, including random selection, and frequency-based sampling. This comparative analysis aims to illustrate the advantage of more diverse data over less diverse data in the context of pre-training models. As a result, we have successfully downloaded 2.72 million data items for CC3M, and 9.30 million data items for CC12M [8, 9]. In these datasets, each image has an associated text. We also use the COCO [22] and Flick30K [23] to evaluate the zero-shot retrieval performance, and in these datasets, each image has five associated texts to describe the context of the image.\nArchitecture For the image encoder, we used ViT-B-16 [12] to encode the image and the input size of the image is 224. We use a Transformer-based model [13] as the text encoder and the text length is 32 [2]. Following CLIP and OpenCLIP [1, 20], we compute the similarity score based on the cosine similarity between image and text embeddings. The model is pre-trained by InfoNCE loss [24], and the similarity scores are scaled by a learnable temperature parameter [1].\nTraining and Fine-tuning We first pre-trained the model on the entire dataset, as well as on random and WFPP subsets, for 30 epochs. Then, we fine-tuned the models pre-trained on the subsets using the entire dataset for an additional epoch. This additional epoch of fine-tuning aims to bridge the distribution gap between the pre-training and inference stages and to account for any unknown concepts that may have been present in the initially removed data. The value of t in Eq. 2 is set to 10-7, and the details of pre-training and fine-tuning configuration are shown in Table 2."}, {"title": "Evaluation", "content": "To evaluate the zero-shot classification performance on ImageNet, where the model correctly classifies data into never-before-seen categories during training. We follow the prompt engineering of CLIP and OpenCLIP [1, 20], utilizing their codebase, which includes a set of 80 templates. We then calculated the cosine similarity score between the image and text embeddings to evaluate the correspondence.\nZero-shot ImageNet Classification. First of all, as shown in Figure 1, we pre-train the model on different size subsets of CC12M. When we evaluate our method on zero-shot accuracy on ImageNet-1K [28] validation, we only need 80% of the computation to achieve a better performance as the CLIP counterpart. Specifically, as detailed in the first (100%) and sixth rows (80%) of Table 3, our model, utilizing just 83.3% of the computational resources compared to the model trained on the full dataset, achieves better performance in the zero-shot ImageNet-1K classification task, with scores of 35.0% versus 34.8%.\nAs shown in the second and third rows of Table 3, the model pre-trained on a subset pruned using a word frequency-based method performs significantly better in the zero-shot image classification task than the model trained with a randomly pruned subset (29.8% vs. 28.2%). The WFPP method outperforms the random method by 1.7% before fine-tuning. After fine-tuning for an additional epoch on the entire dataset, our method continues to outperform the random method by 1.1%. Notably, the differences between the random and frequency-based methods become smaller after fine-tuning the model on the entire dataset.\nSubsampling more data. As demonstrated in Table 3, subsampling 90% of the image-text pairs from the CC12M dataset allows us to attain comparable performance without needing to fine-tune the model on the entire dataset. This observation suggests that excluding the 10% of image-text pairs containing frequent words results in only a slight performance degradation of 0.1%. After fine-tuning the model on the entire dataset, the model trained on WFPP-pruned data outperforms the original CLIP by 0.7% on ImageNet-1K. Moreover, increasing the subsampling percentage from 50% to 60%, 70%, 80%, and 90% results in performance improvements of 2.5%, 1.1%, 0.9%, and 0.6%, respectively. This indicates that data efficiency decreases as the amount of data increases. The latter part of the data contains more high-frequency words than the former part of the data. Blindly adding data becomes increasingly inefficient. As a result, the efficiency of adding more data indiscriminately decreases over time. For the reason, we do not recommend to use the latter part of the data when using WFPP data pruning.\nZero-shot Robustness Evaluation Following the methodology of CLIP [1], we evaluate robustness in Table 4. Using 80% of the image-text pairs, we achieve a comparable average performance to CLIP (27.44% vs. 27.42%) on these 6"}, {"title": "Impact of Word Frequency Distribution", "content": "To investigate the benefits of pre-training with a better-balanced word distribution, we experiment with a badly-balanced word distribution, as a contrast. Specifically, we sort CC12M by Eq. 3 and pre-train on the second half, which is more likely to contain high-frequency words compared to the first half, usually used by WFPP. As shown in Table 9, the model pre-trained on the first half of the subset outperforms the model pre-trained on the second half by 8.7%. After fine-tuning, WFPP-First still maintains a 7.0% advantage. In addition, WFPP-Second performs 6.9% worse than a model pre-trained on a randomly selected 50% subset of CC12M. This result confirms the importance of selecting texts in a way that balances word frequency by reducing the frequency of high-frequency words."}, {"title": "Impact of Text Length and Text Length Normalization", "content": "Equation 3 is normalized by text length, indicating that length also affects our method. To examine the impact of text length, we pruned the dataset based on text length, retaining only the longer texts. As shown in Table 10, pruning based on text length resulted in poorer performance compared to both the random method (22.5 vs. 28.2) and the frequency-based method (22.5 vs. 29.8).\nAdditionally, Equation 3 without length normalization tends to retain longer texts. Therefore, we removed text length normalization in Equation 3 to evaluate its impact on model performance. The revised equation used to sort the text in the dataset is shown below:"}, {"title": "Data Analysis", "content": "To gain insight into the nature of the impact of WFPP on word-frequency distribution, we visualize word-frequency distribution before and after pruning in Figure 2. The figure reveals that high-frequency words are removed at a higher rate compared to low-frequency words. Notably, the likelihood of retaining a word increases as its frequency decreases. For example, the retention rate for the high-frequency word \u201cperson\" was 48.9%, while the retention rate for the less frequent word \"bag\" was 64.4%."}, {"title": "Vocabulary size analysis", "content": "To demonstrate that WFPP maintains the vocabulary diversity, while improving the word-frequency balance, we calculated the vocabulary size before and after applying WFPP. As shown in Table 12, removing 50% of the image-text pairs, the number of vocabulary words with more than 100 occurrences has not decreased substantially. However, the number of words with frequencies between 5 and 100 decreases. Future work should investigate the impact of these words, which might be low, given their low frequencies."}, {"title": "Conclusion and Outlook", "content": "In this paper, we introduce WFPP, a novel data-pruning method to enhance vision-language pre-training. By pruning image-text pairs based on word frequencies in the corpus, we reduce the size of the training dataset reducing the necessary computation to pre-train the model. We demonstrate that pruning improves the word-frequency balance and we claim that this is the reason it is possible to prune data without impacting performance. In fact, across a wide range of tasks and datasets, our experiments demonstrate that after data pruning with WFPP, CLIP is actually able to achieve better performance that CLIP trained on unpruned data. WFPP also outperforms MetaCLIP pruning, which similarly aims to yield a balanced subset over the metadata distribution.\nMoving forward, using WFPP to prune very large-scale datasets such as LAION-400M [30] would be an interesting direction to explore. As shown in Figure 1, the improvement in zero-shot classification accuracy on ImageNet-1K increases substantially when 60% instead of 50% of CC12M data is retained after pruning. When more than 60% is retained, the rate of improvement falls off. We believe that between 50% to 60%, we are observing the linear scaling law demonstrated in CLIP [1, 20], but after that CC12M offers insufficient image-text pairs containing low-frequency words in order to maintain this rate of improvement. If we are right, it means that starting with a larger data set like LAION-400M, we could improve zero-shot classification accuracy would already exceed 40% with 280 million. The broader implication is that WFPP would maintain and possibly enhance its ability to improve performance as size of the dataset before pruning is increased."}, {"title": "APPENDIX", "content": "A.1 More Evaluation\nFor comparison with MetaCLIP, we pre-trained our model by pruning 50% of the data in CC3M using the same metadata as MetaCLIP, with the same pre-training and fine-tuning settings as WFPP. As shown in Figure 3, WFPP pre-trained on CC3M only requires 70% of the sample size required by CLIP.\nZero-shot Robustness Evaluation: The performance of the pre-trained model on the CC3M [8] dataset is consistent with the pre-trained model on the CC12M dataset, as seen in Table 13. When using an 80% subset of CC3M for pre-training, the model achieves the best average performance on these datasets. Additionally, when comparing our method with MetaCLIP, our method outperforms MetaCLIP on all datasets and exceeds MetaCLIP by an average of 0.29% in zero-shot robustness evaluation."}]}