{"title": "Hybrid X-Linker: Automated Data Generation and Extreme Multi-label Ranking for Biomedical Entity Linking", "authors": ["Pedro Ruas", "Fernando Gallego", "Francisco J. Veredas", "Francisco M. Couto"], "abstract": "State-of-the-art deep learning entity linking methods rely on extensive human-labelled data, which is costly to acquire. Current datasets are limited in size, leading to inadequate coverage of biomedical concepts and diminished performance when applied to new data. In this work, we propose to automatically generate data to create large-scale training datasets, which allows the exploration of approaches originally developed for the task of extreme multi-label ranking in the biomedical entity linking task. We propose the hybrid X-Linker pipeline that includes different modules to link disease and chemical entity mentions to concepts in the MEDIC and the CTD-Chemical vocabularies, respectively. X-Linker was evaluated on several biomedical datasets: BC5CDR-Disease, BioRED-Disease, NCBI-Disease, BC5CDR-Chemical, BioRED-Chemical, and NLM-Chem, achieving top-1 accuracies of 0.8307, 0.7969, 0.8271, 0.9511, 0.9248, and 0.7895, respectively. X-Linker demonstrated superior performance in three datasets: BC5CDR-Disease, NCBI-Disease, and BioRED-Chemical. In contrast, SapBERT outperformed X-Linker in the remaining three datasets. Both models rely only on the mention string for their operations. The source code of X-Linker and its associated data are publicly available for performing biomedical entity linking without requiring pre-labelled entities with identifiers from specific knowledge organization systems.", "sections": [{"title": "I. INTRODUCTION", "content": "ENTITY LINKING (EL) is the task of linking an entity mention in a given piece of text to an entry in a target Knowledge organization system (KOS), such as an ontology, a knowledge base or graph, a terminology, etc. The entry must accurately represent the meaning of the linked entity. EL is essential in text mining and natural language processing pipelines since it connects text expressed in natural language to semantic, computer-friendly representations. In the biomedical field, substantial amounts of information are captured in clinical notes written in natural language. These notes include entities that require standardization using ontologies like SNOMED-CT or UMLS.\nChallenges in the biomedical EL task include name variations (synonyms, acronyms), ambiguity (where the same name can denote different entities) [1], and the highly specialised language, which hinders the use of complex resources typically available for general EL approaches, such as Wikipedia. The challenge of ambiguity is illustrated by the entity mention \"iris\", which can have several possible meanings: an eye-related anatomical structure, an insect or a plant taxonomic genus, a disease's acronym (immune re-constitution inflammatory syndrome) or a gene. Searching for \"iris\" in NCBI-Gene returns multiple homonymous results: \"[Drosophila melanogaster (fruit fly)]\" Gene ID: 33290), \u201cIris iris [Tribolium castaneum (red flour beetle)]\" Gene ID: 103314968, and \"Iris iris [Dalotia coriaria]\" Gene ID: 1357899981. Besides, the insufficient coverage of the target KOS results in outdated information and unlink-able entity mentions [2].\nAnother challenge is that current state-of-the-art approaches resort to supervised, deep-learning-based approaches that require abundant quality annotated data [1]. Large human-labelled datasets are expensive and hard to build since their creation requires biomedical expertise [3], [4]. The perfor-mance of the deep learning models is bounded by the in-formation accessed during their training. The applicability will be similarly restricted if most of the datasets are small-scale. To unlock the vast amount of biomedical text available and improve the performance of the task, it is necessary to go beyond supervised approaches trained on limited human-annotated data.\nTo evaluate the true generalization capability of EL ap-proaches, in [5] the authors underscored the significance of assessing these methods, in particular supervised ones, on refined test sets. These test sets exclude annotations that are concurrently present in both the training and development sets. The performance of a supervised approach is heavily reliant on the dataset, which does not align with a real setting where these approaches are employed for inference without artificial partitions of the available data into training and test sets.\nTo mitigate the requirement for costly human-labelled train-ing data, emphasis should be placed on domain-independent approaches trained on domains with large amounts of labelled data and then applied in domains with limited labelled data available [4]. To achieve this objective, distant supervision [6]- [8] and zero-shot methods [9], [10] have been investigated in the context of the EL task."}, {"title": "II. RELATED WORK", "content": "Distant supervision consists of generating training data us-ing only a limited amount of human-labelled data, for example, information such as concept names, relations, etc present in a curated KOS [8], [11]. The entity names and synonyms described in the target KOS can be matched with unlabelled text to label annotations instances [8]. Zero-shot methods focus on generalising an approach to new domains and entities, which were not accessed during the training stage. The key idea is to develop an approach able to link the entities by having only access to the descriptions of the entities belonging to the target domain [4].\nBiomedical KOS typically represent a large number of concepts, however not every concept is represented in the datasets used to evaluate the EL task. The BC5CDR dataset [12] includes 4,424 disease annotations, associated with 674 MEDIC vocabulary concepts, and 5,385 chemical annota-tions associated with 676 CTD-Chemical vocabulary concepts (check Table II) [13], which corresponds to a KOS concept coverage of 5.1 % and 0.38% in the dataset, respectively\u00b2. To develop EL approaches capable of handling a large number of concepts in the target KOS, relying solely on evaluation datasets is insufficient.\nIn our work, we expand on the idea of distant supervision and zero-shot to build a large-scale training dataset and an EL approach that can link disease and chemical entities without the need for retraining with human-labelled data. To effectively train a deep-learning-based model in the generated large-scale dataset, we frame the EL task as an extreme multi-label ranking (XMR) problem [14], where there are a large number of source texts to label as well a large set of target labels and adapt them to the EL task. We explore the hypothesis of applying XMR approaches to the biomedical domain by training models designated by PECOS-EL, jointly with several types of EL approaches that have proven effective in the task. The contributions of this work include:\n\u2022 Development of the PECOS-EL Model and X-Linker pipeline, comprising modular components designed for biomedical KOS to link disease and chemical entities.\n\u2022 Creation of large-scale training datasets featuring auto-mated entity annotations of chemical and disease entities.\n\u2022 Source code publicly available to allow experiment repro-ducibility and further improvements: https://github.com/ lasigeBioTM/X-Linker\nIn the past decade, varied types of approaches have been proposed to address the problem of EL in the biomedical domain, ranging from heuristics [15] to the most recent deep-learning-based architectures [9], [10], [16].\nRule-based approaches offer the advantage of bypassing the requirement for a large volume of labelled data, albeit at the cost of performance. For example, [15] proposed a multi-pass sieve approach for EL in clinical records and scientific articles. This work shares some similarities to our work in the sense that it outlines a rule-based pipeline that includes multiple entity processing steps, including string matching the input mentions to the target KOS, abbreviation expansion, identification of composite mentions and several syntactic transformations, such as stemming, hyphenation or dehyphenation, suffixation and the replacement of numbers by their extended form.\nMachine learning-based supervised approaches improve the performance in the task, but require human-labelled data. For instance, TaggerOne [17] is a machine learning-based approach that models jointly NER and EL. The EL component is a supervised semantic indexer that generates vectorized rep-resentations for both input token and candidate KOS entities and then it assesses the correlation between tokens and target KOS entities using a semi-Markov model.\nSupervised approaches work better for domains with plenty of labelled data available, such as the general domain that has Wikipedia. Therefore, more recent approaches to the biomedical EL task focus on deep-learning architectures that require less annotated data.\nFor instance, BioSyn [18] focus on learning sparse and dense representations for entities using the synonym marginal-ization technique. The approach applies an iterative candidate retrieval to maximise the marginal likelihood of the synonyms being present in the top candidates.\nThe recently proposed BELHD [19] expands on BioSyn by focusing on homonym entities. The approach replaces homonym entities with a disambiguated version included in the target KOS and then introduces candidate sharing and a new objective function to train the BioSyn model.\nBERN2 [20] shares similarities with our work as it adopts a hybrid approach: initially employing a rule-based module to link entities, then applying the deep-learning BioSYN model for more challenging cases. However, BERN2 is a supervised approach that uses annotations from the training sets of evaluation datasets to fine-tune BioSYN.\nSeveral zero-shot approaches have been proposed to tackle the EL task but these mostly focus on the general domain [21]-[24]. In the biomedical domain, two zero-shot approaches have achieved state-of-the-art performance: SapBERT [9] and KRISSBERT [10]. SapBERT [9] represents an unsupervised approach with a focus on learning representations for enti-ties within the target KOS. The method involves pretraining a Transformer-based model on UMLS data using a self-alignment objective. Initially, it clusters synonyms of UMLS entries, after which a BERT-based model learns a mapping function between names and their corresponding Concept Unique Identifiers (CUIs). KRISSBERT [10] introduces a self-supervised method for EL aimed at mitigating the scarcity of annotated data for model training. The approach generates entity annotations by matching UMLS entity names with unlabelled PubMed documents. Subsequently, it employs con-trastive learning to train a contextual encoder. This involves creating positive pairs, where two entity mentions are associ-ated with the same UMLS CUI, and negative pairs, where two entity mentions are linked to different CUIs. The encoder is trained to map mentions of the same entity closer together and mentions of different entities further apart, thereby generating distinct representations for UMLS entities.\nRecently, in [25] the authors introduced an XMR-based"}, {"title": "III. METHODS", "content": "Let T be a text document containing a set of entity mentions M = {m1, m2,...,mn}, and KOS a knowledge organization system including a set of entities or concepts E = {e1,e2,...,ek}. The goal of EL is to map each mention mi \u2208 M recognized in a given document to its corresponding entity ej \u2208 E. Ideally, input entity mentions are linked to entities that accurately represent their semantic meaning.\nThere are two essential phases in the EL task:\n\u2022 Candidate Generation: the goal is, for each mention mi, to generate a set of candidate entities Ci \u2286 E.\n\u2022 Candidate ranking and disambiguation: the goal is to rank the candidate entities Ci based on their sim-ilarity scores sim(mi, ej). Depending on the approach, the similarity can be calculated based on the features of the individual mentions (local approach), the features of other mentions within the same document (global approach), or a combination of both. The entity et with the highest similarity score in the candidates set is selected:\n\\(e_t = \\arg \\max_{e_j \\in C_i} \\text{sim}(m_i, e_j)\\)\nThe final mapping M from mentions M to entities E is given by:\n\\(M(m_i) = e_t  \\forall m_i \\in M\\)\nThere are various methods for candidate generation and ranking in the EL task. We demonstrate in this work that no single approach is universally optimal for all entities. Instead, a combination of different approaches yields better performance.\nThe candidate generation is achieved using a string sim-ilarity function. One commonly employed string similarity function is the edit distance also designated by Levenshtein Distance. The Levenshtein distance between two given strings represents the minimum number of single-character edits (insertions, deletions, or substitutions) necessary to convert one string into the other. Defining the distance as d, the distance between a given mention mi \u2208 M and an entity ei \u2208 E, the goal is to compute the distance between a mention and every entity and then choose the entity the smallest distance to link the mention to. The limitation is that it relies solely on individual features of the input entity mention, and these features are strictly string-based, lacking consideration of contextual information.\nIn this work, we investigate framing of the biomedical EL task as an XMR problem, which to the best of our knowledge, has been only recently attempted in the general domain by [25]. Given an input entity mention, the goal is to return the most relevant labels or identifiers from a large set of labels included in the target KOS. We used PECOS [26], a framework originally designed for Information Retrieval approaches. In the EL task, the input mention serves as the text and the set of entities E in the KOS represents the target labels. The PECOS framework encompasses three stages:\n1) Semantic Label Indexing: the set of KOS entities E is partitioned into K clusters.\n2) Matching: an entity mention is mapped into relevant clusters through a learned scoring function.\n3) Ranking: a ranker assigns scores to the candidate enti-ties present in the matched clusters.\nIn semantic label indexing, labels/entities from a target KOS are grouped into clusters reducing the search space. Represen-tations for each entity ze : e \u2208 E are obtained by aggregating the feature vectors of the training instances associated with the entity. The clustering algorithm maps each entity to a cluster: ce \u2208 CIE, where ce denotes the index of the cluster containing the entity e. The clustering is represented by the clustering matrix CIE \u2208 {0,1}E\u00d7K with E representing the entities in the target KOS and K representing the number of entity clusters.\nDuring the matching stage, a general matcher function g(x, k) determines the relevance between an instance x (an entity mention) and the k-th entity cluster. The top-b clusters in Cl are identified through g(x):\n\\(\\Delta_b(x) = \\arg \\max_{S \\subseteq C_I: |S|=b} \\sum_{k \\in S} g(x, k)\\)\nThe function gr(x) attempts to find the subset S of size b included in Cl that maximises the function g evaluated at each S for a given x. The deep text vectorizer is a pre-trained Transformer model, specifically BioBERT. We briefly explored other BERT-based models (BERT, SciBERT, BioBERT, Pub-MedBERT), but we found the differences to be minimal.\nAfter the matching stage, the ranker h(x, e) models the relevance between x and each candidate entity belonging to the clusters previously identified by the matcher function g(x).\nWe trained two PECOS models for two entity types, 'Disease' and 'Chemical. We further describe the generated training data.\nDeep-learning-based approaches that focus on specific tasks usually require a vast amount of human-labelled data, which is scarce in the biomedical domain. The annotation process is a bottleneck in the development of such approaches since it is"}, {"title": "F. X-Linker: pipeline for named entity linking", "content": "To deal with different entities, we explore the combination of the previous approaches into a single pipeline, designated by X-Linker. X-Linker is a heuristic that resorts to abbreviation detection, string matching, to the PECOS-EL model and the PPR-based model according to the entity being linked. The overview of the X-Linker pipeline is shown in Fig. 1 and the pseudo-code is shown in Algorithm 1.\nThe algorithm starts by taking a set of entity mentions M and initializing a score threshold for filtering matches out-putted by the respective PECOS-EL model. For each mention m, it applies an abbreviation detector to convert m to its long form long_m. Then, it retrieves candidate matches using a string matcher and the PECOS-EL model, storing results in string_matches and pecos_matches, respectively. If the top candidate from string_matches or pecos_matches has a perfect score (1.0), it is added to the candidate list C. If the top candidate from pecos_matches has a score above the threshold, it is also added to C. If the score is below the threshold, both the top candidate from pecos_matches and the top candidate from string_matches are added to C. Once candidate lists for all mentions are completed, a disambiguation graph G is built based on these candidates. The PPR algorithm is then applied to G to compute scores for each candidate. Finally, for each mention m, the highest-scoring candidate from the PPR results is selected to disambiguate the mention."}, {"title": "IV. EXPERIMENTS", "content": "The datasets used for the evaluation of the several ap-proaches are described in Table II. We selected datasets including annotations of the type \"Disease\u201d or \u201cChemical\" that are commonly used7: BC5CDR (819 citations), BioRED (74 citations), NCBI-Disease (843 citations), NLM-Chem (52 citations). From each dataset, we removed the NIL annota-tions, including annotations associated with no KOS identifiers (whose identifier is '-1' or '-'), but also obsolete annotations, i.e., annotations with KOS identifiers that are not present in the KOS version used in our experiments. The performance of an approach in the target evaluation dataset is assessed through the calculation of the top-k accuracy, which is defined as:\n\\(Top-k Accuracy = \\frac{1}{N} \\sum_{i=1}^{N} 1 \\{Y_i \\in \\{Y_{i,1}, Y_{i,2},..., Y_{i,k} \\}\\}\nwhere:\n\u2022 N is the total number of evaluation instances.\n\u2022 Yi is the true KOS identifier for the i-th instance.\n\u2022 \u0176i,1, \u0176i,2,..., \u0176i,k are the top k predicted identifiers (ranked by confidence) for the i-th instance.\n\u2022 1{} is the indicator function, which returns 1 if the true identifier yi is among the top k predicted identifiers {\u0176i,1, \u0176i,2,..., \u0176i,k}, and 0 otherwise.\nBesides the baselines defined in our work, we used the state-of-the-art approach SapBERT [9] for a relative comparison of the performance."}, {"title": "V. RESULTS AND DISCUSSION", "content": "To assess the impact of the size of the training data and of the addition of Pubtator3 annotations, we evaluated the performance of the model PECOS-EL-Disease trained on different versions of the training data as described in Table I. Since the training dataset \"Disease-All\u201d is large (9,497,985), we were not able to train the PECOS-EL-Disease model in this dataset due to the out-of-memory error. The performance of the Disease PECOS-EL model when applied to the different dataset versions is shown in Table III. For the PECOS-EL-Disease model, incorporating Pubtator annotations into the training data enhances performance in the EL task. Moreover, as the number of Pubtator annotations increases, performance improves accordingly. However, there are some caveats. In the NCBI-Disease dataset, the addition of Pubtator annotations to the \"Disease-100\" training dataset decreases the top-1-accuracy to 0.6519 from 0.5961, which was obtained training PECOS-EL-Disease in the \u201cDisease-KOS\" dataset. Training the model in the dataset \"Disease-200\u201d increases the top 1 accuracy to 0.6394, still below the performance of the model trained in the dataset in the \u201cDisease-KOS\" dataset. It's only when PECOS-EL-Disease is trained in the dataset \"Disease-300\" that the top-1 accuracy surpasses the baseline (0.6837). The highest top-1 accuracy is obtained when the model is trained in the dataset \"Disease-400\": 0.7292. In the BioRED evaluation dataset, the performance of the PECOS-EL-Disease increases with the number of Pubtator annotations in the training data, reaching a maximum of 0.7380. In the BC5CDR-Disease dataset, the performance of the model PECOS-EL-Disease also increases with the number of Pubtator annotations in the training data, peaking when the model is trained in the dataset \"Disease-300\u201d with a top-1-accuracy of 0.7870 and decreasing with the model training in \u201cDisease-400\". This contradictory result may be explained by the nature of the Pubtator annotations present in the training data, more concretely, it can be attributable to the fact that there are Pubtator annotations sharing the same string, but associated with different KOS identifiers. For an explanation of this, check the next subsection V-B. Observing the top-5 accuracy, the performance increases with the higher number of instances in the dataset. Also, the annotation performance of Pubtator3 is not 100%, so we can safely assume that there will be errors present in the training data which further decrease the downstream performance in the evaluation of the EL task in the selected datasets.\""}, {"title": "B. Is PECOS-EL a zero-shot entity linker?", "content": "We analysed for each evaluation dataset the percentage of annotations with strings that are also present in the data used to train the PECOS-EL model, as seen in Table IV. Following the strict definition for zero-shot evaluation, i.e., an EL approach must be able to link entities that were not seen during training using only the entity descriptions, the PECOS-EL models are not zero-shot entity linkers [4]. However, we followed the refined evaluation method recommended by the authors in [5]. Specifically, we removed all documents from the Pubtator set that were also present in the test sets of the evaluation datasets. The training data was gathered from the natural distribution of entities in biomedical literature. Therefore, we assume that the performance of X-Linker is robust since it is not dependent on a specific evaluation dataset. The only drawback is that the training data is biased towards the past, in the sense that is based on text already existing. There is no assurance that the same entities will continue to appear in biomedical literature in the future. However, the X-Linker approach can be updated with new training data and, in the event of new entities emerging, there is the potential to employ an approach that specifically handles NIL or unlinkable entities. This type of approach helps prevent the loss of semantic information and mitigates decreases in performance by EL approaches [2], [37]."}, {"title": "C. Impact of abbreviation detection", "content": "As shown in Table V, adding an abbreviation detection mod-ule greatly improves the performance of PECOS-EL. PECOS-EL relies solely on variations in the text of an entity for training and does not consider its context, thus its performance is highly dependent on the input mention text. [25] showed that considering the mention's context makes the approach more robust to text variations, but the resources required to train such a model leave that work for future exploration."}, {"title": "D. Impact of string matching and the rule-based filter", "content": "Table V shows the impact of adding the string matcher module to the X-Linker pipeline, which showed advantages in all evaluation datasets. Analysing the data shown in VI, the overlap of string between training and evaluation data ranges from 96.99% in the BC5CDR-Chemical dataset to 87.07% in the NLM-Chem dataset. However, some of the strings in the training data are associated with more than one KOS identifier. Moreover, in some cases the identifier for a given string in the training dataset is not the same identifier associated with the same string in the evaluation dataset (check column \"Incorrect\" in Table VI). For example, 12.0% of the strings present in the \"Disease\u201d training dataset that are also present in the BioRED-Disease dataset are associated with different identifiers. Even if the KOS identifier that appears associated with a given string in the evaluation dataset is also associated with the same string in the training dataset, there is a relevant part of ambiguity (check column \u201cAmbiguous\" in Table VI), i.e., there are more than one identifier for the string. For example, in the BioRED-Disease dataset, 89.29% of the strings in the training dataset are associated with the correct identifier as defined in the evaluation dataset, but only 49.16% of those strings have only one identifier. The remaining 50.84% strings have more than one associated identifier.\nAs shown in Table VI, there are annotations in the evalu-ation datasets associated with entity names/strings that have an exact match in the respective target KOS. However, not always the identifier associated with the exact matching is the same as the identifier chosen to annotate the entities in the evaluation datasets. This highlights the inherent ambiguity of the annotation process, but also that the task EL does not have a universal definition. The annotation criteria are strictly associated with the scope of the motivation. For example, in the context of an annotation project centred on rare diseases, the annotation guidelines will instruct annotators to prioritise selecting more specific diseases. However, if the project encompasses various entity types simultaneously, such as chemicals, anatomical parts, cell types, etc., the annotation guidelines may not necessitate the same level of specificity as in the case of rare diseases. In such instances, a broader categorization may be sufficient to fulfil the project's objectives. Evaluation datasets are useful to straightforwardly assess the performance of EL approaches, which can be then complemented by more extensive and realistic evaluations, for example, user testing. A rule-based pipeline such as X-Linker helps diminish the impact of these disparities.\""}, {"title": "E. Document context improves the performance", "content": "Table V shows the impact of adding the PPR algorithm-based module to the X-Linker pipeline. With the previously mentioned modules, the PECOS-EL module jointly with the abbreviation detector and the string matcher can deal with a large part of the entities present in the evaluation datasets. However, context in the EL task is relevant, since the same entity string can have multiple meanings according to the sur-rounding entities. For that, establishing a measure of coherence between a given entity and the other entities present in the same document can help to disambiguate decisions, as shown in the literature [32], [38].\nFigure 2 shows an example of how the X-Linker pipeline links two entity mentions present in the document with PubMed ID 19263707 from the BC5CDR-Disease dataset to entries in the MEDIC vocabulary: \u201cvasculitic\u201d and \u201cvasculitis.\" As a first step, X-Linker applies abbreviation detection to each mention. Then, the model PECOS-EL-Disease predicts the candidates \"Congenital Disorder\" with identifier D009358 and \"vasculitis\" with identifier D014657 for the mentions \"vasculitic\" and \"vasculitis\" respectively. Concurrently, the string matcher retrieves the candidate \"vasculitis\" (D014657) from MEDIC for both mentions. In the disambiguation pro-cess, for \"vasculitic\" PECOS-EL-Disease scores low (0.0964), and the string matcher finds a close candidate (score 0.9). Both are added to the candidate list due to the low PECOS-EL-Disease score. For \"vasculitis\" PECOS-EL-Disease scores 1.0, and the string matcher confirms an exact match (MEDIC). Only \"vasculitis\" (D014657) is listed. Both lists feature \"vasculitis\" linked in the disambiguation graph by MEDIC relations. The PPR selects \u201cvasculitis\" (D014657) as the top candidate, resolving ambiguity.\""}, {"title": "F. Comparison with SapBERT", "content": "The state-of-the-art EL approach SapBERT exhibits a high top-1 accuracy across all evaluated datasets, particularly for \"Chemical\" entities. Like the PECOS-EL model, SapBERT re-lies on the mention string. Therefore, we also present its results after applying a pre-processing step of abbreviation detection for a fairer comparison. X-Linker achieves higher performance in three of the evaluation datasets: BC5CDR-Disease, NCBI-Disease and BioRED-Chemical. SapBERT's performance is higher in the remaining three evaluation datasets: BioRED-Disease, BC5CDR-Chemical and NLM-Chem. For entities of type \"Disease\", SapBERT's performance is higher in a smaller dataset (BioRED-Disease), whereas for entities of type \"Chemical\" SapBERT's performance is higher in the larger datasets (BC5CDR-Chemical and NLM-Chem). X-Linker's performance is higher than the performance of PECOS-EL, which highlights the importance of combining different types of EL approaches."}, {"title": "G. Error analysis", "content": "One type of error is related to the specificity of the annota-tions. For instance, the entity \"liver neoplasms\" (document 26033014 in the BC5CDR dataset) is annotated with the MEDIC concept \u201cLiver neoplasms\u201d (identifier D008113) and X-Linker correctly links the entity mention to the referred con-cept. However, in the same document, the entity mention 'liver cancer\" has the candidates \"\u201cLiver neoplasms\u201d (D008113) and ", "hepatocellular": "D006528) and X-Linker links the entity mention to the child concept (D006528) instead of the correct one, the parent concept D008113. The same happens with the entity mention \u201ccognitive impairment", "Cognitive dysfunction": "D060825) instead of the correct parent concept", "disorders": "D003072). This relates to the imple-mentation of the PPR algorithm, which considers the IC of each concept to score the candidates. As a result, more specific terms are preferred over more general ones. Nevertheless, the opposite also happens: the entity mention \u201cDeterioration of vision", "vision disorders": "D014786) instead of the correct child concept \u201cVision, Low", "AL": "document 24040781 of the BC5CDR dataset) is an abbreviation of", "Amyloidosis": "so it should be linked to the concept", "Mousa Al din Al Nassar syndrome": "C536989), \u201cPallor\u201d (D010167) and Abetalipoproteinemia (D000012). The abbreviation detector fails to identify the abbreviation, and X-Linker generates wrong candidates. In another case, the entity mention \u201cmania\u201d (document 19447152 of the BC5CDR dataset) is linked to the concept instead of the concept", "Mania": "D000087122) instead of the concept", "Disorder": "D001714). In the Disease dataset used to train the PECOS-EL-Disease model the string \u201cmania\u201d as annotated with the identifier (D000087122) so the model outputted this identifier.\nAnother type of error is related to composite mentions, since X-Linker fails to deal with these mentions. The entity mention", "strokes": "document 19293073 of the BC5CDR dataset) is annotated with the identifiers D020300 (", ") and D020521 (\u201cStroke\u201d), but X-Linker links the mention to the concept ": "emorrhagic Stroke"}, {"title": "H. Limitations", "content": "There are several limitations associated with X-Linker. First, the performance of the PECOS-EL models is influenced by the accuracy of the automatic annotations provided by PubTator3 used for training. Any discrepancies arising from the automatic annotation will lead to downstream lower performance in the evaluation process using datasets. The matcher component of the PECOS-EL model uses BioBERT as the encoder model, meaning any biases associated with BioBERT may affect the results. Due to memory constraints, we did not train the PECOS-EL-Disease model on the entire training dataset and both PECOS-EL models were trained solely on the entity text, without incorporating the respective context. Training the PECOS-EL models requires significant GPU resources, so we did not perform extensive hyperparameter optimization, which may have resulted in suboptimal performance compared to fully optimized models."}, {"title": "VI. CONCLUSION", "content": "We generated large-scale training datasets including auto-matic annotations to train a deep-learning-based XMR ap-proach adapted to the biomedical EL designated by PECOS-EL. This module was integrated into the hybrid pipeline X-Linker, an EL approach including different modules to link disease and chemical entities to the MEDIC and CTD-Chemical vocabularies without the need for human-labelled data. We carried out an extensive evaluation of the X-Linker approach, resulting in top-1 accuracy values of 0.8307, 0.7969, 0.8271, 0.9511, 0.9248, 0.7895 in the datasets BC5CDR-Disease, BioRED-Disease, NCBI-Disease, BC5CDR-Chemical, BioRED-Chemical and NLM-Chem, re-spectively. X-Linker demonstrated superior performance com-pared to SapBERT in three datasets: BC5CDR-Disease, NCBI-Disease, and BioRED-Chemical. The source code is publicly available: https://github.com/lasigeBioTM/X-Linker.\nIn future work, we plan to enhance entity linking using X-Linker to connect mentions to the UMLS. While our current study focused on smaller KOS due to computational limits, future directions include adapting PECOS-EL to utilize the UMLS with lightweight BERT-based matchers. Additionally, we'll explore integrating NCBI Gene and Taxonomy data from Pubtator3 for generating training datasets. Currently, PECOS-EL employs a modified K-means algorithm based on string representations of KOS entities, so we aim to boost model performance by exploring different clustering approaches that incorporate KOS information and metadata."}, {"title": "APPENDIX", "content": "Our approach includes as a first step the rule-based abbrevi-ation detector Ab3P created by [39]. To implement X-Linker we used the PECOS framework [26], with the code available at https://github.com/amzn/pecos. Model training was done in two setups: (1) a server including 2 Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz and 8 Tesla M10 GPUs; Total CPU memory: \u2248 64 GB. Total GPU memory: \u2248 64 GB; (2) an HPC cluster with 8 nodes with each 2 x AMD EPYC 7742 processors/node x 64 cores 128 cores, 1024 GB RAM, 40 GB VRAM each GPU, 4 GPU NVIDIA A100 (only 80GB RAM were used for training). Training time varied according to the entity type and the number of instances: Disease-400 (the large file with Disease entities) \u2248 8 hours in the HPC cluster; Chemical \u2248 16 hours. In the X-Linker pipeline, the threshold for candidate filtering is set to 0.1 as the default."}]}