{"title": "Better Embeddings with Coupled Adam", "authors": ["Felix Stollenwerk", "Tobias Stollenwerk"], "abstract": "Despite their remarkable capabilities, LLMs learn word representations that exhibit the undesirable yet poorly understood feature of anisotropy. In this paper, we argue that the second moment in Adam is a cause of anisotropic embeddings, and suggest a modified optimizer called Coupled Adam to mitigate the problem. Our experiments demonstrate that Coupled Adam significantly improves the quality of embeddings, while also leading to better upstream and downstream performance on large enough datasets.", "sections": [{"title": "Introduction", "content": "Anisotropic Embeddings Large Language Models (LLMs) take a sequence of tokens as input and predict the next token. An embedding matrix is used to map the input tokens to the hidden space of the model, while an unembedding matrix provides the inverse mapping to the output token space. Although the two matrices can in principle be different, it is common practice to apply weight tying (Press and Wolf, 2017) and use the transpose of the embedding matrix for unembedding. During training, the model learns an embedding vector in hidden space for each token in the vocabulary. However, it is observed that those embedding vectors are clustered in a small subspace away from the origin (Gao et al., 2019). This anisotropy limits the semantic usefulness of the embeddings and, in turn, the expressiveness and generalizability of the model. Multiple attempts have been made to both explain the root cause of the problem and alleviate it (more on this in Sec. 7). In particular, Bi\u015b et al. (2021) have shown that the problem can be traced back to a mere shift of the mean embedding vector away from the origin. With the mean embedding vector as reference point, the embeddings feature near-perfect isotropy. However, the role of the employed optimization algorithm has, to the best of our knowledge, not yet been investigated.\nOptimization Algorithms Optimization algorithms are an indispensable ingredient in the training of neural networks generally and LLMs in particular. While SGD is the foundational optimization technique, Adam (Kingma and Ba, 2014) is the most widely used optimization techniques for LLMs due to its superior performance and robustness. While it provides multiple conceptional advantages over SGD, see e.g. Ruder (2017) for a detailed discussion, the one that is particularly striking with regard to word embeddings is that Adam is well-suited for sparse data. More concretely, this means that using Adam, the embedding update vectors for rare words are scaled up in comparison to those of more frequent words. This is relevant in the context of LLMs as word frequencies in the training data are typically very skewed and may differ by several orders of magnitude. Formally, this is captured by the unigram probability distribution $p \\in [0, 1]^V$, which for a given dataset d and tokenizer t is defined by\n$p_i = p_i(d, t) = \\frac{n_i}{\\sum_j n_j}$"}, {"title": "On the Root Cause of Anisotropic Embeddings", "content": "We study the collective shift of the embeddings (that underlies the anisotropy problem), by analyzing their vector updates based on the optimization algorithms SGD and Adam. Weight tying is assumed, but only contributions from the output layer are considered, following Bi\u015b et al. (2021). Our results apply to all model architectures with a standard language modeling head."}, {"title": "Language Modeling Head", "content": "The equations for the standard language modeling head read\n$L = -\\log (p_t)$\n$p_t = \\frac{\\exp (l_t)}{\\sum_{j=1}^V \\exp (l_j)}$\n$l_i = e_i^T h,$\nwhere $L \\in \\mathbb{R}_{>0}$ is the loss for next token prediction, and $p_t \\in [0, 1]$ is the predicted probability of the true token $t \\in V$. $l_i \\in \\mathbb{R}$ and $e_i \\in \\mathbb{R}^H$ denote the logits and embeddings for each token $i \\in V$, respectively. $h \\in \\mathbb{R}^H$ is the final hidden state provided by the model for a single token. Note that the operation in Eq. (4) is the dot product of two vectors in $\\mathbb{R}^H$. Backward propagation yields the following gradients with respect to the input vectors $e_i$ and $h$ of Eq. (4):\n$g_i := \\frac{\\partial L}{\\partial e_i} = - (\\delta_{it} - p_i) \\cdot h$"}, {"title": "Vanishing Sum of Embedding Gradients", "content": "Optimization algorithms for neural networks usually update the model parameters iteratively, using an additive update vector that points in direction opposite to the gradient of the loss with respect to the parameters. In the case of embedding vectors, this can be expressed by\n$e_i^{(\\tau)} = e_i^{(\\tau-1)} + u_i^{(\\tau)}$\n$u_i^{(\\tau)} = - \\eta^{(\\tau)} \\cdot g_i^{(\\tau)},$\nwhere $u_i^{(\\tau)}$ is the update vector for $e_i^{(\\tau)}$ at time step $\\tau$. Eq. (5) implies that the embedding vector $e_t$ of the true token is updated in direction $+h$, while the update vectors $u_i$ for all the other embedding vectors $e_i$ with $i \\neq t$ are proportional to $-h$. This circumstance is referred to in the literature as the \"common enemy effect\" (Bi\u015b et al., 2021), and regarded as the cause of the representation degeneration problem. However, as we will see in the following sections, this explanation is incomplete, as it does not take into account the scaling of the gradients with the predicted probabilities $p_i$, see Eq. (5). The basis for our argumentation is the observation that the sum of embedding gradients vanishes, as the following simple calculation shows:\n$\\sum_{i=1}^V g_i^{(\\tau)} = (\\sum_{i=1}^V (\\delta_{it} - p_i) ) \\cdot h^{(\\tau)} = (1 - \\sum_{i=1}^V p_t ) \\cdot h^{(\\tau)} = 0$"}, {"title": "Invariant Mean Embedding with SGD", "content": "We consider the application of the SGD optimization algorithm on the embedding vectors\u00b9. At each training step, an embedding vector is simply updated by adding the associated negative gradient $-g_i$, multiplied by a global learning rate $\\eta$. Hence, Eq. (7) becomes\n$u_i^{(\\tau)} = - \\eta g_i^{(\\tau)}$\nTogether with Eq. (8), this implies that the sum of embedding update vectors vanishes at any time step $\\tau$:\n$\\sum_{i=1}^V u_i^{(\\tau)} = - \\eta \\sum_{i=1}^V g_i^{(\\tau)} = 0$\nConsequently, the mean embedding vector will stay invariant during the training process:\n$\\mu^{(\\tau)} - \\mu^{(\\tau-1)} = \\frac{1}{V} \\sum_{i=1}^V u_i^{(\\tau)} = 0$\nThis holds even though the different embeddings $e_i$ will be individually updated in different directions with different magnitudes. Moreover, all of the above is true also in the case of SGD with momentum, which follows from linearity and mathematical induction. Eq. (12) has far-reaching implications with regard to the anisotropy problem. It entails that the embedding vectors do not collectively shift away from the origin if SGD (with or without momentum) is used."}, {"title": "Shifted Mean Embedding with Adam", "content": "In this section, we analyze the behavior of the mean embedding during optimization with Adam (Kingma and Ba, 2014), see Algorithm 1. The update vector Eq. (7) for the Adam algorithm is given by\n$u_i^{(\\tau)} = - \\eta^{(\\tau)} \\cdot m_i^{(\\tau)},$\nwhere we have introduced an $i$-dependent effective learning rate\n$\\eta_i^{(\\tau)} := \\frac{\\eta}{\\sqrt{\\hat{v}_i^{(\\tau)}} + \\epsilon}$"}, {"title": "Coupled Adam", "content": "In the previous section, we have identified the individual scales of the second moments $v_i$ for different embedding vectors $e_i$ as the root cause of the anisotropy problem. This implies that a solution to the problem is to enforce that the second moments are the same for every i. The question arises whether and how this can be done in the best way, without harming the performance of the model. To answer this, we note that the normalization of the embedding update vector by the Adam second moment can be split into two parts:\n$\\frac{E[v_i]}{\\sqrt{V_i}} = \\sqrt{\\frac{A \\cdot p_i}{V_i}}$\nThe first factor introduces a global scale to all update vectors simultaneously:\n$\\frac{A}{\\sqrt{V}} = \\frac{10^{-4}}{5 \\cdot 10^{4}} = 2 \\cdot 10^{-9},$\nwhere the numbers correspond to our experiments from the previous section with $V \\approx 50000$. The second factor scales the update vectors individually. It is one on average:\n$\\frac{1}{V} \\sum_{i=1}^V \\sqrt{\\frac{p_i}{V_i}} = 1$\nOur goal is to retain the first, global factor and get rid of the second, individual factor. The canonical way to do this is to simply take the average of the second moment over the vocabulary items $i$:\n$\\frac{1}{V} \\sum_{i=1}^V \\frac{E[v_i]}{\\sqrt{p_i \\cdot V}} = A$\nIn practice, the exponentially averaged second moments $\\hat{v}_i^{(\\tau)}$ as they appear in Eq. (14) are replaced by their average:\n$\\hat{v}^{(\\tau)} := \\frac{1}{V} \\sum_{i=1}^V \\hat{v}_i^{(\\tau)}$\nWe call the resulting algorithm Coupled Adam, as it couples the second moments of the embedding vectors via Eq. (24). It is displayed in Algorithm 1. Evidently, with Coupled Adam, the effective learning rate in Eq. (14) that enters the update vector in Eq. (13) becomes independent of i. Hence, like SGD but unlike standard Adam, the sum of embedding updates vanishes. However, like standard Adam but unlike SGD, Coupled Adam uses a second moment to normalize the embedding update vectors."}, {"title": "Experiments", "content": "Two types of experiments are conducted to study the impact of coupling the second moments of the embedding update vectors. First, a set of small-scale experiments (Sec. 4.1) with models and datasets of varying sizes up to 1B parameters and 20B tokens, respectively. Afterwards, we perform a few large-scale experiments (Sec. 4.2) to verify that the usefulness of our method extrapolates to the realm of large language models with more than 1B parameters trained on at least the corresponding compute-optimal (Hoffmann et al., 2022) amount of data. In order to verify the generalizability of our method, the small- and large-scale experiments involve different datasets, training frameworks and dense transformer model architectures. An overview of the model and dataset sizes employed in our experiments is given in App. E.1. For each combination, two models are trained: one using standard Adam and one using Coupled Adam for the embeddings, see Eq. (24). Both variants use standard Adam for all non-embedding parameters. The various metrics we employ to assess both the general model performance and the quality of the model embeddings will be discussed in Sec. 4.3."}, {"title": "Small-scale Experiments", "content": "Our small-scale experiments use the OpenWebText Corpus (Gokaslan and Cohen, 2019) and the GPT-2 tokenizer (Radford et al., 2019). The model architecture also follows GPT-2, while the hyperparameter setup is taken from GPT-3 (Brown et al., 2020), see App. E.2 for further details. An implementation based on nanoGPT (Karpathy, 2022) is used. We define a grid (D, N) with dataset sizes D \u2208 {5B, 10B, 20B} and model sizes N \u2208 {125M, 355M, 760M}, and repeat each experiment S = 3 times with different seeds in order to estimate uncertainties and assess statistical significance."}, {"title": "Large-scale Experiments", "content": "For our large-scale experiments, we use the SlimPajama dataset (Soboleva et al., 2023) and the GPT-2 tokenizer. A state-of-the-art dense transformer model architecture akin to (Touvron et al., 2023) is chosen, including e.g. RoPE embeddings (Su et al., 2023) and the SwiGLU activation function (Shazeer, 2020). Details can be found in App. E.2. The experiments are conducted using Modalities (L\u00fcbbering et al., 2024) as the training framework. We consider two model sizes, 1.3B and 2.6B. In order to cover the two common scenarios of compute-optimal training and overtraining, we conduct two sets of experiments: Firstly, we use near compute-optimal dataset sizes, 26B and 52B tokens, respectively. Secondly, we increase the number of tokens by a factor 4, resulting in 105B and 210B tokens, respectively. Each large-scale experiment is performed S = 1 times."}, {"title": "Evaluation", "content": "Upstream performance is measured in terms of test loss, while downstream performance is evaluated using the Language Model Evaluation Harness (Gao et al., 2023) on the following tasks: ARC easy and challenge (Clark et al., 2018), HellaSwag (Zellers et al., 2019), LAMBADA (Paperno et al., 2016), RACE (Lai et al., 2017), TruthfulQA (Lin et al., 2022) and WinoGrande (Sakaguchi et al., 2020). More concretely, the considered metric is the average accuracy, which we will denote by Acc. To assess the quality of the embeddings, we first compute their isotropy, defined as (Arora et al., 2016; Mu et al., 2018)\n$\\text{Iso}(E) := \\frac{\\min_{c \\in X} Z(c)}{\\max_{c \\in X} Z(c)},$\nwhere $E \\in \\mathbb{R}^{H \\times V}$ is the embedding matrix, $Z(c) = \\sum_{i=1}^V \\exp(c^T e_i)$ is the partition function and $X = \\{c\\}$ is the set of eigenvectors $c \\in \\mathbb{R}^H$ of $EE^T \\in \\mathbb{R}^{H \\times H}$. Secondly, the 2-norm $||\\mu||$ of the mean embedding, see Eq. (9), and the average 2-norm of the embeddings $||e_i|| = \\frac{1}{V} \\sum_{i=1}^V ||e_i||$ as well as their ratio\n$||\\mu|| := \\frac{||\\mu||}{||e_i||}$"}, {"title": "Scaled Coupled Adam", "content": "While coupling the second moment of the embedding gradients using the average in Eq. (24) is the canonical choice, one could also use a multiple of the average. We conduct additional experiments where the coupled second moment is scaled by powers of 2:\n$\\hat{v}_i^{(\\tau)} \\rightarrow 2^{-n} \\cdot \\hat{v}_i^{(\\tau)},$\nwith scaling exponents $n \\in \\{z \\in \\mathbb{Z} | -5 < x \\leq 5\\}$. Note that using a scaling exponent $n \\neq 0$ is equivalent to using a different effective learning rate for the embeddings than for all the other parameters, via Eqs. (24) and (14). In particular, a smaller scaling exponent n corresponds to a smaller effective learning rate and vice versa. The results for D = 20B are shown in Tab. 3, and the dependency of the loss on the scaling exponent n for that very dataset size is visualized in Fig. 2. Results for other dataset sizes and plots for the other evaluation metrics can be found in App. G.2. Our data shows that the loss reaches a minimum close to n = 0, with a rather weak dependence on the scaling exponent in its vicinity. Nevertheless, for the smallest and largest scaling exponents studied, we find that the loss gets significantly worse. Regarding downstream performance, we see indications of a similar pattern, although the statistical uncertainties are too large to draw definite conclusions. The semantic usefulness of the embedding vectors as measured by $\\tau$ seems to suffer from a scaling exponent n < 0. For the isotropy and the mean embedding, we observe the opposite behavior. They benefit from a smaller scaling exponent n and the associated smaller embedding updates, with the effect being more pronounced the larger the training dataset size D. However, this also negatively affects the model performance. Hence, we conclude that, at least within the range of our experiments, the optimal setting is to have the same learning rate for the embedding parameters as for all the other model parameters, as implied by n = 0 and Eq. (24)."}, {"title": "SGD", "content": "We train several models using SGD with momentum $\\gamma = 0.9$ as the optimizer for the embeddings. Since Adam via the inverse square root of its second moment effectively scales the learning rate up by a factor comprising orders of magnitude (see Eq. (21)), we explicitly multiply the learning rate in SGD by a factor f of comparable size\u00b3. A hyperparameter search using f \u2208 {100, 200, 300, 400, 500, 600} is performed to search for the optimum with respect to upstream performance (loss), see App. G.3 for details. It is found at f = 300 for D \u2208 {5B, 10B} and f = 400 for D = 20B. The respective optimal model is compared to its counterpart trained with Coupled Adam in Tab. 4. The results show that, although SGD is advantageous with respect to isotropy, the mean embedding shift and the condition number, Coupled Adam consistently achieves better results on all upstream and downstream task metrics, while having one less hyperparameter to fine-tune."}, {"title": "Related Work", "content": "Gao et al. (2019) first described the anisotropy issue, which they referred to as representation degeneration problem, and suggested cosine regularization as a mitigation strategy. Alternative techniques to address the problem have been developed, including adversarial noise (Wang et al., 2019), spectrum control (Wang et al., 2020) and Laplacian regularization (Zhang et al., 2020). Bi\u015b et al. (2021) have shown that the anisotropy of embeddings can for the most part be traced back to a common shift of the embeddings in a dominant direction. They called this phenomenon common enemy effect, and provided a semi-quantitative explanation (Eq. (5)), which we developed further in the present work by including the optimizer in the analysis. In Yu et al. (2022), Adaptive Gradient Gating is proposed, based on the empirical observation that it is the gradients for embeddings of rare tokens that cause anisotropy. Our analysis conforms to this finding and attributes it to a massive up-scaling of the gradients for rare embeddings with Adam, cf. Fig. 1. Machina and Mercer (2024) have demonstrated that large Pythia models (Biderman et al., 2023) show improved isotropy compared to similar models, and attribute this to the absence of weight tying. This is in accordance with our analysis of the unembedding gradients in conjunction with Adam, Sec. 2. While all the previously mentioned papers use average cosine similarity (Ethayarajh, 2019) or Iso from Eq. (25) to quantify the geometry of embedding vectors, Rudman et al. (2022) deviate from this. Their notion of isotropy is based solely on the embeddings' covariance matrix and embodied by the metric IsoScore. In particular, IsoScore is mean-agnostic, while Iso strongly correlates with the mean embedding (see e.g. Tab. 1). Finally, concurrent to our work, Zhao et al. (2024) have investigated the importance of using the second moment in Adam with regard to performance and stability. They found that simplified variants of Adam that use the same effective learning rate either for the whole embedding matrix (Adalayer) or each embedding vector (Adalayer*) are slightly worse than Adam but better than SGD. Adalayer* is similar to Coupled Adam, but corresponds to the second moment averaged over hidden space instead of vocabulary space."}, {"title": "Conclusions", "content": "Our work addresses the well-known anisotropy problem for LLM embeddings. We have advanced the theoretical understanding of the phenomenon by showing that it is a combination of the common enemy effect and the individual second moments in Adam that causes a collective shift of the embedding vectors away from the origin. To mitigate the problem, we have introduced Coupled Adam, which enforces the same effective learning rate for every embedding vector, and thus suppresses the collective shift of the embeddings. We have found that Coupled Adam consistently improves embedding-specific metrics across all experiments, while also achieving better downstream and upstream performance for large datasets, as they are typically used in LLM training. The code to reproduce our results is available at github.com/flxst/coupled-adam."}, {"title": "Limitations", "content": "Although our method is generally applicable to all common LLM architectures, as they share the same language modeling head and embeddings, only dense decoders were used in our experiments. In addition, only models with up to N = 2.6B parameters have been tested. The cosine decay learning rate schedule was applied throughout all experiments (App. E.2). Alternatives such as an infinite learning rate schedule are not incorporated in our study. Furthermore, as mentioned at the end of Sec. 5, we have not explicitly verified that the slight residual shift of the mean embedding, which is observed even for Coupled Adam, is caused by weight tying. Finally, we have used a straightforward implementation of Coupled Adam, closely following Algorithm 1. More sophisticated implementations might lead to increased efficiency and further improvements; we leave it for future work to investigate this."}, {"title": "Embedding Gradients", "content": "We explicitly derive Eq. (5), which we recall here for convenience:\n$g_i := \\frac{\\partial L}{\\partial e_i} = - (\\delta_{it} - p_i) \\cdot h$\nThe chain rule yields\n$\\frac{\\partial L}{\\partial e_i} = \\sum_{k=1}^V \\frac{\\partial L}{\\partial p_t} \\frac{\\partial p_t}{\\partial l_k} \\frac{\\partial l_k}{\\partial e_i},$\nwhere the individual factors can directly be obtained from Eqs. (2)-(4):\n$\\frac{\\partial L}{\\partial p_t} = - \\frac{1}{p_t}$\n$\\frac{\\partial p_t}{\\partial l_k} = \\frac{\\exp (l_t) \\cdot \\Sigma - \\exp (l_t) \\exp (l_k)}{\\Sigma^2} = \\delta_{kt} p_t - p_t p_k$\n$\\frac{\\partial l_k}{\\partial e_i} = \\delta_{ki} h$\nNote that in the first line of Eq. (32), we use the abbreviation $\\Sigma = (\\sum_{j=1}^V \\exp (l_j))$. Inserting Eqs. (31), (32) and (33) into Eq. (30) directly leads to Eq. (5):\n$\\frac{\\partial L}{\\partial e_i} = - \\frac{1}{p_t} \\sum_{k=1}^V p_t (\\delta_{kt} - p_k) \\delta_{ki} h$\n$\\frac{\\partial L}{\\partial e_i} = - \\sum_{k=1}^V (\\delta_{kt} - p_k) \\delta_{ki} h = - (\\delta_{it} - p_i) \\cdot h$"}, {"title": "SGD Algorithm", "content": "For completeness and comparison to (Coupled) Adam as displayed in Algorithm 1, we summarize the SGD algorithm in Algorithm 2."}, {"title": "Magnitude of the Second Moment in Adam", "content": "In this appendix, the validity of\n$\\mathbb{E}[V_i] \\propto p_i$\nis verified. Due to the linearity of lines 5 and 7 in Algorithm 2, it suffices to show that the squared gradient has the property in question:\n$\\mathbb{E}[g_i^2] \\propto p_i$\nWe do this in two different ways. First, we derive Eq. (34) using a semi-theoretical approach with minimal experimental input. Afterwards, we confirm the relationship in a purely experimental manner."}, {"title": "Semi-theoretical Derivation", "content": "Here, we derive an expression for the expectation value of the squared gradient in terms of simple observables (Theorem 2). Subsequently, the dependency of those observables on $p_i$ is determined experimentally. Together, this will yield the proportionality expressed by Eq. (34). We begin our reasoning with a lemma.\nLemma 1 (Expectation Value Decomposition). The expectation value of the squared gradient can be decomposed into conditional expectation values as follows:\n$\\mathbb{E}[g_i^2] = p_i \\cdot \\mathbb{E}[g_i^2 | i = t] + (1 - p_i) \\cdot \\mathbb{E}[g_i^2 | i \\neq t]$\nProof. Our starting point is the definition of the expectation value for the continuous random variable $g_i^2$:\n$\\mathbb{E}[g_i^2] = \\int g_i^2 p(g_i) dg_i,$\nwhere p denotes the probability distribution of $g_i$. Since the vocabulary item i can only be either the true token t or not, we can decompose p into a sum of joint probability distributions (using the law of total probabilities), each of which can be expressed in terms of conditional probabilities like so:\n$p(g_i) = p(g_i, i = t) + p(g_i, i \\neq t) = p(g_i | i = t) \\cdot p(i = t) + p(g_i | i \\neq t) \\cdot p(i \\neq t)$"}]}