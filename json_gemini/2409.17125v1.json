{"title": "On-orbit Servicing for Spacecraft Collision Avoidance With Autonomous Decision Making", "authors": ["Susmitha Patnala", "Adam Abdin"], "abstract": "This study develops an AI-based implementation of autonomous On-Orbit Servicing (OOS) mission to assist with spacecraft collision avoidance maneuvers (CAMs). We propose an autonomous 'servicer' trained with Reinforcement Learning (RL) to autonomously detect potential collisions between a target satellite and space debris, rendezvous and dock with endangered satellites, and execute optimal CAM. The RL model integrates collision risk estimates, satellite specifications, and debris data to generate an optimal maneuver matrix for OOS rendezvous and collision prevention. We employ the Cross-Entropy algorithm to find optimal decision policies efficiently. Initial results demonstrate the feasibility of autonomous robotic OOS for collision avoidance services, focusing on one servicer spacecraft to one endangered satellite scenario. However, merging spacecraft rendezvous and optimal CAM presents significant complexities. We discuss design challenges and critical parameters for the successful implementation of the framework presented through a case study.", "sections": [{"title": "Introduction", "content": "Space exploration and satellite deployment have become increasingly common in recent decades. As a result, the space environment around Earth becomes increasingly congested with both active spacecraft and space debris, with over 36,500 pieces of space debris larger than 10 centimeters in diameter [1]. This debris poses significant risks to manned and unmanned spacecraft, making it a pressing concern for spacecraft operators around the world.\nTo manage spacecraft collision challenges, space operators and researchers have developed tools for conjunction assessment and mitigation strategies [2]. For example, NASA's Robotic Conjunction Assessment Risk Analysis framework has been identifying and reacting to predicted close approaches for many spacecraft [3]. The International Space Station routinely performs collision risk assessments and avoidance maneuvers [4]. Strategies to assess Collision Avoidance Maneuvers (CAMs) for Cryosat-2 were started after a near-miss incident with ERS-2 [5].\nWith the significant growth of satellite constellations, autonomous collision avoidance systems are becoming imperative. Efforts in satellite collision avoidance have evolved from early orbit propagation algorithms, such as RBF-collocation, to sophisticated space-based situational awareness technologies and collision avoidance services, such as ESA's CORAM and Occam [6, 7, 8, 9, 10]. Recent advances in AI and machine learning, particularly reinforcement learning (RL), have shown promise in space traffic management [11, 12, 13] and autonomous spacecraft docking [14, 15, 16, 17, 18]. However, while several research works investigated the potential of On-Orbit Service (OSS) for purposes such as satellite refueling, repairs, and active debris capture [19, 20, 21], to our knowledge, no study has analyzed the feasibility of autonomous robotic OOS for collision avoidance services. This study addresses this gap by exploring the feasibility of integrating autonomous collision avoidance capabilities achieved within an RL framework into OOS missions."}, {"title": "Methodology", "content": null}, {"title": "Mission architecture", "content": "We consider a servicer spacecraft deployed to a parking orbit in which it constantly monitors the debris catalog. In case of potential collision risk of a target/target satellite 1, the servicer decides autonomously, based on estimations of collision risk, to assist with CAM. It performs phasing maneuvers and docks with the target satellite, performs CAM, and returns the target satellite to its orbit.\nThe current study considers a Low Earth Sun-synchronous Orbit at 750 km with an inclination of ~ 98\u00b0 based on debris and satellite population density data [22, 23]. The region between 600 to 900 km is"}, {"title": "Autonomous Decision-planning model", "content": "We model the autonomous decision-making problem as a Markov Decision Process (MDP). The MDP framework ensures that the agent makes decisions based on the current satellite state and the immediate servicing requirement, allowing for efficient and adaptive decision-making. The model elements are as follows:\n1.) State space (S): Represents the positions and velocities of the servicer, the target satellite, and the debris, along with the time and fuel information of the servicer.\n2.) Action table (A): The action table contains a sequence of maneuvers to be performed by the servicer. Each line contains the change in velocities to implement in all three directions, i.e., x, y, z, and the time (in mjd2000 format) at which the next maneuver has to be performed. It is the action table the agent refers to, and the following action is based on the state. After training, this gives the optimal maneuver matrix for the servicer to perform.\n3.) Reward function (R): Extended from [12], the reward gathered by the agent is based on five elements: the collision probability, the fuel level, the trajectory deviation (for target satellite when a maneuver is performed), the relative position and velocity of the servicer and the target satellite during docking phase. Each component has a predefined threshold, and the agent receives a negative reward proportional to its deviation from the threshold (1).\n$R_{total} = R_{p}(P_{collision})+R_{f}(dV_{maneuver})+R_{d}(deviation) + R(relative distance) + R(relative velocity)$ (1)\nThe threshold values for the deviation of the trajectory in {a, e, i, \u03a9, w} are based on values similar to those in [12]. In particular, the threshold value considered for the probability of collision (pt) is 10\u20134 [27]. We employ an ELU-based function [28] to adjust the reward function. This is done to manage the penalty for collision probabilities below a threshold $p_{t}$ ($10^{-4}$) while avoiding unacceptable values ($10^{-3}$), significantly increasing the penalty for probabilities exceeding $p_{t}$. Furthermore, we consider values for threshold for fuel level = 500 units, deviation in a = 100m, deviation in e = 0.01, deviation in i, Q, w = 0.01 (in rad), docking position = 250m, docking velocity = 5m/s. The negative reward is significantly increased for each component once it crosses the threshold. All rewards are individually calculated for each component and summed up to get the total reward (Eq. (1)). Finally, the methodology proposed in [29], 'Explicit Expression of P in Terms of Conjunction Geometries,' is used to estimate the probability of collision between two objects in space."}, {"title": "RL agent: Cross-Entropy (CE) method", "content": "The RL model developed is solved using the CE method, which is based on a stochastic optimization approach [30]. The CE method is used to find optimal maneuvers and optimal maneuver epochs. The first step is to choose an initial maneuver and an appropriate random distribution. The expected value E of the distribution is equal to the initial maneuver parameters. The distribution is used for generating new maneuvers based on the initial one. Each maneuver is evaluated based on the reward received, and the algorithm selects those with the best rewards to adjust subsequent maneuvers. The expected value E is shifted in the direction of selected maneuvers. Iterations are repeated until a stopping criterion is reached, which is the limit on the number of iterations, or the decision policy no longer improves. The model parameters, such as the number of iterations, number of sessions, and CE parameters, are tuned during the training."}, {"title": "Results", "content": "For the case study, an environment with one servicer and one target satellite is generated. The orbital parameters of the target satellite, servicer, and debris are generated to create a collision scenario, with the orbital parameters given in Table (1). Each simulation is considered for two days, during which a potential collision with debris is generated. The start time of the experiment is \u201cJan-24 21:35:59\u201d and the end time of the simulation is \u201cJan-27 02:24:00\u201d. The agent parameters considered for training are given as follows: number of iterations = 35, number of sessions = 30, \u03c3 decay = 0.98, learning decay = 0.98, percentile growth = 1.005.\nTo efficiently handle the combined CAM and docking actions in training, a hybrid time grid approach is adopted. This approach employs a finer time grid when approaching docking to capture a close approach accurately. It switches to a coarser grid afterward for the remaining actions to balance computational efficiency while maintaining accuracy. Hence, a time step of 0.08s was assumed for docking and a coarse grid for CAM, skipping unnecessary time steps between docking and CAM maneuver.\nTo initialize the action table of the training agent, four maneuvers are defined. The first two maneuvers are defined for completing the docking procedures, and the last two for performing CAM and returning the target satellite to its orbit. We consider two methods to generate the initial action table: (1) random initialization, in which we start with random values for the maneuver's AV and timings. As the agent receives rewards from the environment, it adjusts these values to optimally perform docking and CAM if necessary. (2) Lambert solution initialization, in which we use Lambert's solution to phase the servicer with the target satellite as the initial guess for the docking maneuvers. For CAM, we start with random AV values and timing. This approach provides a more informed starting point compared to random initialization. At the end of the training, we compare the performance of these two initialization methods by demonstrating a simulation case study to determine their effectiveness.\nTraining using random initialization: The initial guess for the action table is generated randomly for the CAM and docking maneuvers. The training process is illustrated in Fig. (3), which shows the policy reward, mean, and maximum reward versus iteration count. The training plot indicates that the policy reward decreased and stabilized upon reaching the stopping criteria (the reward does not improve or the maximum number of iterations is reached). The final optimal maneuver values to be performed by the servicer for the case study are provided in Table (2).\nTraining using Lambert's solution initialization: Lambert's problem aims to find the right trajectory in space to connect two points in a specified amount of time [24] and is used in training the OOS vehicle in the proposed framework. A new action table is generated based on Lambert's solution initialization. It is seen that the policy reward steadily decreases with iterations and converges to the stopping criteria as seen in Fig. (4). Table (3) lists the optimized maneuver parameters for the simulation case study. Fig. (4) further illustrates the training process, depicting the decreasing policy reward, the mean, and the maximum reward over iterations.\nComparison and discussion of results: The model is trained in the environment described above for a large number of iterations and sessions. Training with random initialization and Lambert's solution initialization for action table and results are presented in Section 3. Fig. (5) shows the simulation results with parameter variations and variations in the reward function over time (in days) for both cases using trained models.\nThe time for the servicer to dock with the target satellite is calculated as \u2248 30 orbital periods (between the first maneuver time and docking) as shown in Fig. (5a). In this scenario, the docking is time-consuming due to the initial random generation of the action table for docking and CAM, resulting in velocities of approximately equal magnitude for both.\nIn practice, for CAM, maneuvers at velocities of around 1m/s are used to avoid debris and return to the original orbit [12], while phasing and docking maneuvers require an order of two or higher velocities [24].\nTo resolve this issue, Training is done using Lambert's solution initialization with the results shown in Fig. (5b). As shown in the figure, the relative position and velocity of the docking are initially high, but it is docked in one orbit of time (a sudden decrease in the total reward function due to the successful docking). As the AVs are high in Lambert's initialization case, fuel consumption is high as well (Fig. 5). The total collision probability is decreased in both scenarios to below the threshold after performing CAM, while the optimal fuel consumption is one order of magnitude higher for Lambert's solution initialization compared to random initialization. Although Lambert solution initialization facilitates faster docking and CAM execution, it comes at the expense of higher fuel consumption compared to the random initialization method during training (Fig. 5). The sudden drop in the total reward function after 1.5 days is due to the initialization of CAM, which causes a significant deviation of the target satellite from its original orbit. The target satellite is then brought back to its orbit as the collision probability decreases and the total reward approaches zero again, as shown in Fig. 5.\nWhile the simulation assumes instantaneous docking upon the overlap of the two objects, it is important to acknowledge that a docking sequence takes several hours to initiate and complete in the real world. Consequently, decision-making within the simulation is guided by the need for proactive measures: upon detecting a potential collision, the servicer begins phasing maneuvers, aiming to dock with the target satellite. Hence, results from the model trained using Lambert's solution initialization are preferred."}, {"title": "Discussion", "content": "This paper provides a preliminary analysis of training and using an autonomous on-orbit servicer for assisting with collision avoidance maneuvers (CAM). The case study demonstrates the ability of the servicer to autonomously make the decisions to dock and provide CAM optimally. The study simulates a scenario where there are two days before a predicted collision, focusing on the most likely orbital region for such events. This setup is close to a realistic situation where timely decision-making is crucial. Additionally, the model is trained to facilitate quick rendezvous between the servicer and the target satellite, accounting for the extra time needed in real-life scenarios to complete docking procedures. The extension of the proposed framework to cases of one servicer to multiple satellites and considering several horizons for the collision impact are also examined and will be presented in future studies."}]}