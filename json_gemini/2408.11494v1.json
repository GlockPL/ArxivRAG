{"title": "Mutagenesis screen to map the functionals of parameters of Large Language Models", "authors": ["Yue Hu", "Kai Hu", "Patrick X. Zhao", "Javed Khan", "Chengming Xu"], "abstract": "Large Language Models (LLMs) have markedly propelled advance- ments in artificial intelligence, showcasing proficiency across a wide range of tasks [1]. Despite a model's functionality being closely linked to its parameters, a systematic approach to mapping these connec- tions remains undeveloped. Models with similar structures and iden- tical parameter counts often perform differently on various tasks, which has spurred detailed investigations into the distinct patterns influencing their performance. We employed a mutagenesis screen- ing approach, drawing inspiration from biological research methods, to probe into Llama2-7b and Zephyr. This method involved alter- ing elements within the models' matrices to either their maximum or minimum values, thereby facilitating an analysis of the interplay be- tween model parameters and their functionalities. Our study revealed intricate structures within both models at multiple levels. While the phenotype map of many matrices exhibited a mix of both maximum and minimum mutations, others displayed a predominant sensitiv- ity to one mutation type. Notably, mutations that produced pheno- types, especially those with severe outcomes, tended to cluster along axes. Furthermore, the placement of maximum and minimum muta- tions typically followed a complementary pattern across the matrices in both models, with the Gate matrix demonstrating a unique two- dimensional asymmetry after rearrangement. In Zephyr, specific mu- tations consistently yielded poetic or conversational outputs rather than descriptive ones. These \"writer\" mutations were categorized by the high-frequency initial word of the output, showing a significant tendency to share the same row coordinate across different matrices. Our mutagenesis screen has also uncovered many other significant variations in the mutation maps of Llama2-7b and Zephyr, highlight- ing notable discrepancies in their structural and functional attributes. Our findings confirm that mutagenesis screening is a potent tool for", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) represent one of the most significant advance- ments in the field of artificial intelligence in recent years. These models have revolutionized how we approach problems requiring natural language under- standing and generation, setting new benchmarks for what machines can achieve in terms of linguistic and cognitive tasks [1] The exponential growth in the capabilities of LLMs can be attributed to the increase in the number of their parameters, scaling into the tens of billions or more. These parameters allow LLMs to capture and generate nuances of human language with remarkable accuracy. As a result, we've seen a surge in the emergence of powerful func- tionalities that were previously thought to be the exclusive domain of human intelligence. Despite their impressive abilities, LLMs are often described as \"black boxes\" because the intricate workings of their internal mechanisms are not fully understood, even by the researchers who develop them.\nTo better understand how of LLMs operate, instead of simply taking them as 'black boxes', recent efforts have been engaged in explaining LLMs from the local perspective. For example, Enguehard et al. [2] utilized integrated gradient [3] to inspect the feature attribution to specific outputs. Kobayashi et al. [4] proposed to track the attention weights to analyze the linguistic capabilities of the transformer blocks. Geva et al. [5] used on the feed forward network (FFN) to show how FFN encodes human interpretable concepts. Despite all these progresses, there is not a systematic method that can comprehensively analyze the LLMs.\nDrawing parallels with biological systems, which are similarly complex and func- tionally diverse, this study adopts a biological approach-specifically, mutagene- sis screening-to probe into the functionalities of LLMs. In biology, mutagenesis screening has been instrumental in dissecting complex biological processes, from DNA replication to protein transport, and decoding interactions within cellular pathways [6,7]. By applying this method to the Llama2-7b and Zephyr LLMs [8, 9], we aim to uncover the underlying mechanisms of these powerful models, enhancing our ability to refine and deploy them more effectively."}, {"title": "Mutation map statistics with different type of inputs", "content": "Both Zephyr and Llama2-7b consist of 32 transformer layers. Each transformer layer comprises one attention block, which includes four matrices: K, Q, V, and O, and one MLP block containing three matrices: Up, Down, and Gate [8,9]. In our research, we mutate the elements within the attention matrices or MLP matrices of Llama2-7b or Zephyr to study how the modifications affect the output of the model. We implemented three distinct modification strategies on the matrix elements: setting the element to zero, which we refer to as a zero mutation, or replacing it with the maximum or minimum value found"}, {"title": "Structure of NSM map", "content": "In both Llama2-7b and Zephyr, certain matrices exhibit a pronounced bias in sensitivity towards either maximum or minimum mutations. For example, the NSM maps for the Down matrices in layers 2, 20, and 26 of Llama2-7b, as well as layers 9, 15 and 22 of Zephyr, show a significant inclination towards maxi- mum NSM. Conversely, layers 17,18, 19 and layer 25 in Llama2-7b, along with layers 13, 20, and 28 in Zephyr, exhibit a preference for minimum NSM (refer to Figure 2 and Supplementary Figure 3). This sensitivity to either maximum or minimum NSM appears to be an intrinsic characteristic of the models rather than a response to specific types of input, indicating that it is a fundamental property of the model itself. Notably, this bias is not consistently related to the sequence of layers. For instance, Down matrices in layer 25 are more sensitive to minimum mutation, whereas adjacent layers 24 and 26 are more susceptible to maximum mutation. We find that 14% of the matrices in Llama2-7b 's NSM maps are biased towards maximum NSM, and 19% towards minimum NSM with a 20% threshold of bias score (See Method). In contrast, 14% of matrices in Zephyr are biased towards maximum NSM and 11% towards minimum NSM. There appears to be no consistent pattern of bias across different types of ma- trices within layers for either model, nor is there a correlation in bias between Llama2-7b and Zephyr across layers (Data not shown).\nSecondly, in our analysis, we identified a pronounced enrichment of NSMs in the column and row structures across all matrices in both models. Furthermore, we pinpointed highly sensitive points within these matrices, points at which mutations could trigger phenotypic changes across a broad spectrum of inputs (Figure 3 and Supplementary Figure 1). A standout observation was made in the"}, {"title": "The severity of phenotypes and their effect on function", "content": "In our study illustrated in Figures 2-4, the colored dots indicate NSMs that produce outputs differing from those of the standard models; however, most of these NSMs cause only very weak phenotypes, such as a single altered answer among 21 MMLU questions. We proceeded to examine another critical attribute: the severity of phenotypes, measured by performance scores on the MMLU or the degree of deviation from the standard response for those descriptive experi- ments. This severity, coupled with the specific impacts of NSMs, highlights the importance and functional role of the affected sites within the model.\nOur analysis initially focused on how NSMs affect performance on 21 MMLU questions related to international law. Although many NSMs were benign, gen- erating typical \"ABCD\" responses with minimal effect on the overall MMLU score, we identified two main types of detrimental NSMs. The first type, \"de- structive NSMs\", comprises a small subset that shifts responses away from the standard \"ABCD\" format for some or all of the 21 questions. The second type, \"underperforming NSMs\", results in scores below 10. Both types tend to cluster in specific columns within matrices and some NSMs belong to both categories. Notably, underperforming NSMs with high scores tend to appear in the same columns as those with low scores The V matrices for both Llama2b-7b and Zephyr are particularly prone to these harmful mutations. In most V matri- ces from the Mmlu21 experiment, column 39 in Llama2b-7b and column 32 in Zephyr consistently contain the lowest-scoring NSMs (Supplementary Figure 3), underscoring the significance of these structures and a consistency in the mechanisms of both models. An exception is found in the Gate matrix of layer 28 of Zephyr, where all the NSMs are destructive NSMs and align along two rows rather than columns (Figure 5b). Among these destructive NSMs, 34% yield exclusively non-standard responses, while the remaining 66% produce a mix of non-standard answers for some of the 21 MMLU questions and standard \"ABCD\" responses for others. Notably, all the standard responses generated by these destructive NSMs align with those of the standard model, whether or not the standard model's answers are correct (Data not shown). This pattern indi- cates that models containing some of these NSMs retain the knowledge encoded in the standard model but struggle to deliver it in MMLU task consistently.\nFor descriptive inputs, we assessed the severity of the mutations using bag-of- words cosine similarity between the mutated and standard outputs, assigning a cosine similarity score to each mutation's output. As with the MMLU test, NSMs characterized by larger deviations tend to be concentrated along the same axes within each matrix. Additionally, akin to the Mmlu21 experiment, for mutations with cosine similarity below 0.5, those exhibiting higher cosine similarity are often found in the same columnar or row structures as those with lower similarity (Figure 5a, b)."}, {"title": "Zephyr mutations with poetic or dramatic dialogues outputs", "content": "In our investigation of the mutations' output of Drosophila experiments, we identified rare mutations in Zephyr that produced outputs in the forms of poems or dialogues.\nThe output from the standard Zephyr model is as follows (Supplementary Table 3 row A):\n The life cycle of Drosophila:\nThe life cycle of Drosophila, commonly known as fruit flies, has four distinct stages: egg, larva, pupa, and adult. Here is a detailed explanation of each stage:\n1. Egg stage: This stage begins when a female fruit fly lays an egg. The eggs are laid in clusters, called egg masses, which are typically found near the surface of rotting fruit or other organic material. The"}, {"title": "Conclusion", "content": "This study has successfully utilized a mutagenesis screening approach inspired by biological research methods to explore the complexities of LLMs, specifically Llama2-7b and Zephyr. This method reveals multifaceted features across both models. At the matrix level, certain matrices in both models displayed predom- inant sensitivity to either maximum or minimum mutations. At the columnar and row levels, NSMs, particularly those significantly influencing MMLU per- formance or altering model responses, notably clustered along axes. es. At the element level, we observed COPA phenomenon, which impose strict distribu- tion constraints on NSMs, and identified unique biases in the Gate matrix not present in other matrix types.\nThe similarity in parameter counts and structural design between Llama2-7b and Zephyr has provided a solid foundation for employing mutagenesis screening to study correlations between matrix parameters and model performance. No- tably, the MLP matrix exhibited an uneven distribution of NSMs and a marked concentration of distinct phenotypes in the last two layers of Llama2-7b, sug- gesting that in comparison with Zephyr there are more parameters not fully utilized by Llama2b-7b, and the last two layers of Llama2b-7b bear a dispropor- tionate burden in realizing the model's potential, which probably underlies the underperformance of Llama2b-7b comparing with Zephyr [5]. An interesting di- vergence was observed in how Zephyr exhibited striking stability in the Mmlu21 experiment and had many fewer NSMs in this task, while Llama2-7b showed comparable NSM statistics across various tasks. This distinction in task-specific performance between the models warrants further exploration.\nImportantly, the discovery of \"writer\" mutations in Zephyr, which generate cre- ative and thematic outputs such as poetry and dialogues, opens new avenues for utilizing LLMs beyond traditional tasks. These mutations not only highlight the models' capacity for generating inventive content but also suggest that such features can be further developed and harnessed. It also raises intriguing ques- tions about the consistency of row coordinates across layers for NSMs with the same RIHF, like those showing \"staring\" consistently across specific layers.\nLooking forward, the mutagenesis screening approach has considerable potential for expansion. For instance, although the use of basic cosine similarity metrics in this study proved to be feasible, this approach makes use of the minimum information from the output, and thus points to the need for more sophisticated analytical methods to fully leverage the rich data generated by mutagenesis screening. Moreover, identifying more mutations that produce novel outputs, delving deeper into the functions of specific elements, or generating function maps of complex tasks like context understanding, emotion recognition, and grammatical correction could reveal crucial insights into our understanding of how LLMs process. In addition, our method is highly adaptable and can be applied to various other large-scale models, such as image and video generative models [11-14], and multi-modal LLMs [15-18], to provide profound research insights in diverse aspects of artificial intelligence.\nIn conclusion, the success of this study in applying biological research methods to the study of artificial intelligence not only enhances our understanding of LLMs but also sets the stage for future advancements in AI design and applica- tion. The integration of methodologies from biological research into AI studies promises a rich reservoir of techniques that can propel our understanding and innovation in artificial intelligence forward. By bridging the gap between bio- logical and AI research, we can enrich our toolkit for exploring both the artifi- cial 'brains' of LLMs and the biological neural networks, potentially catalyzing breakthroughs across both fields."}, {"title": "Coordinate Transformation:", "content": "Each mutation affects a 64x64 square within the matrix. To facilitate clarity and consistency in our visual presentations across the paper, figures, and tables, we employ a coordinate transformation step. Specifically, the coordinates are adjusted by dividing both the X and Y values by 64. For example, if a mutation covers an area from (X, X+63) to (Y, Y+63), the coordinates for this mutation are represented as (X/64, Y/64)."}, {"title": "LLM Output Generation:", "content": "Outputs were generated using the Hugging Face Transformers package for both Llama2-7b and Zephyr. A temperature setting of 0.7 was utilized for all exper- iments, except for the MMLU tasks. For the Python0, Python10, Java, and Drosophila experiments, as well as the study of RIHF, the 'generate' function was set with a 'max_length' of 150. For the Newton and P53 experiments, a 'max_length' of 300 was employed. Python10 used a random seed of 10, while a seed of 0 was used for all other experiments. For the MMLU task, the prompt with 5 in-context learning examples from the chain-of-thought hub by Fu et al. [1] was used. Specific prompts for the other experiments were as follows:\nPython0 and Python10: The following is a python program for bub- ble sort:\nJava: The following is a Java program for bubble sort:\nDrosophila: The life cycle of Drosophila:\nNewton: The scientific accomplishments and influences of Isaac New- ton:\nP53: Tell me 10 different signal pathways through which p53 is involved in cancer development:"}, {"title": "Mutation Overlap Calculation:", "content": "For each 64x64 square within the matrix there are four possible statuses depend- ing on whether mutations were silent or non-silent mutations (NSMs):\n1. Both maximum and minimum mutations of this square are NSMs.\n2. Only the maximum mutation of this square is an NSM.\n3. Only the minimum mutation of this square is an NSM.\n4. Both mutations of this square are silent.\nThe overlap ratio of mutations between two experiments, as illustrated in Figure 1B, is determined by scoring each 64x64 square as follows:\n\u2022 A score of 1 is assigned if the mutation status for both experiments within the square is identical."}, {"title": "Determining Matrix Bias Towards Maximum or Minimum NSMs", "content": "To assess whether a matrix is biased towards maximum or minimum NSMs, we utilize the following bias score formula:\n$$bias score = \\frac{maximum NSM number \u2013 minimum NSM number}{\u0442\u0430\u0445(maximum only NSM number, minimum only NSM number)}$$\nA matrix is considered biased towards maximum NSM if:\n1. The number of maximum NSMs exceeds the number of minimum NSMs.\n2. The bias score exceeds 20%.\n3. The combined total of maximum only and minimum only NSMs is more than 10% of the total number of squares in the matrix.\nA matrix is considered biased towards minimum NSM if it satisfied above criteria except it has more minimum NSMs than maximum NSMs."}, {"title": "Bag-of-Words (BoW) Analysis and Cosine Similarity:", "content": "Outputs were tokenized by splitting the text by punctuation marks (period, comma, question mark, exclamation mark) and whitespace (newline character, space). Tokens enclosed in brackets were stripped, and tokens consisting solely of numbers were excluded. The tokens from all different phenotypes of one exper- iment were pooled to generate a bag-of-words vector space model. Each output was represented by a BoW vector, with each entry representing the frequency of occurrence of a distinct token [2]. Cosine similarity was then calculated between the BoW vector of the standard output and that of the mutated model's output [2]."}, {"title": "Initial Word of Highest Frequency Analysis:", "content": "Input Selection: A total of 151 short sentences were used as inputs for this analysis, comprising the specific input used in the Drosophila experiment along- side 150 novel inputs generated by ChatGPT. The composition of these inputs was deliberately structured to include 101 literary expressions and 50 factual statements.\nOutput Generation and Frequency Calculation: For both the standard model and models with NSMs, outputs were generated using the aforementioned"}]}