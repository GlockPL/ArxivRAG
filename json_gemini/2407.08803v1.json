[{"title": "PID Accelerated Temporal Difference Algorithms", "authors": ["Mark Bedaywi", "Amin Rakhsha", "Amir-massoud Farahmand"], "abstract": "Long-horizon tasks, which have a large discount factor, pose a challenge for most\nconventional reinforcement learning (RL) algorithms. Algorithms such as Value It-\neration and Temporal Difference (TD) learning have a slow convergence rate and be-\ncome inefficient in these tasks. When the transition distributions are given, PID VI\nwas recently introduced to accelerate the convergence of Value Iteration using ideas\nfrom control theory. Inspired by this, we introduce PID TD Learning and PID Q-\nLearning algorithms for the RL setting in which only samples from the environment\nare available. We give theoretical analysis of their convergence and acceleration\ncompared to their traditional counterparts. We also introduce a method for adapt-\ning PID gains in the presence of noise and empirically verify its effectiveness.", "sections": [{"title": "1 Introduction", "content": "The Value Iteration (VI) algorithm is one of the primary dynamic programming methods for solv-\ning (discounted) Markov Decision Processes (MDP). It is the foundation of many Reinforcement\nLearning (RL) algorithms such as the Temporal Difference (TD) Learning (Sutton, 1988; Tsitsiklis\nand Van Roy, 1997), Q-Learning (Watkins, 1989), Approximate Value/Fitted Iteration (Gordon,\n1995; Ernst et al., 2005; Munos and Szepesv\u00e1ri, 2008; Tosatto et al., 2017), and DQN (Mnih et al.,\n2015; Van Hasselt et al., 2016), which can all be seen as sample-based variants of VI. A weakness\nof the VI algorithm and the RL algorithms built on top of it is their slow convergence in problems\nwith discount factor \\( \\gamma \\) close to 1, which corresponds to the long-horizon problems where the agent\naims to maximize its cumulative rewards far in the future. One can show that the error of the value\nfunction calculated by VI at iteration \\( k \\) goes to zero with the rate of \\( O(\\gamma^k) \\). The adverse dependence\nof the convergence rate on \\( \\gamma \\) also appears in the error analysis of the downstream temporal differ-\nence (Szepesv\u00e1ri, 1997; Even-Dar and Mansour, 2003; Wainwright, 2019) and fitted value iteration\nalgorithms (Munos and Szepesv\u00e1ri, 2008; Farahmand et al., 2010; Chen and Jiang, 2019; Fan et al.,\n2019). If \\( \\gamma\\approx 1 \\), these algorithms become very slow and inefficient. This work introduces accelerated\ntemporal difference learning algorithms that can mitigate this issue.\nFarahmand and Ghavamzadeh (2021) recently suggested that one may view the iterates of VI as a\ndynamical system itself. This opens up the possibility of using tools from control theory to modify,\nand perhaps accelerate, the VI's dynamics. They specifically used the simple class of Proportional-\nIntegral-Derivative (PID) controllers to modify VI, resulting in a new procedure called the PID VI\nalgorithm. They showed that with a careful choice of the controller gains, PID VI can converge\nsignificantly faster than the conventional VI. They also introduced a gain adaptation mechanism, a\nmeta-learning procedure, to automatically choose these gains.\nPID VI, similar to VI, is a dynamic programming algorithm and requires access to the full tran-\nsition dynamics of the environment. In the RL setting, however, the transition dynamics is not\ndirectly accessible to the agent; the agent can only acquire samples from the transition dynamics by\ninteracting with the environment."}, {"title": "2 Background", "content": "Given a set \\( \\Omega \\), let \\( \\mathcal{M}(\\Omega) \\) be the set of probability distributions over \\( \\Omega \\), and \\( \\mathcal{B}(\\Omega) \\) be the set of\nbounded functions over \\( \\Omega \\). We consider a discounted Markov Decision Process (MDP) (Bertsekas\nand Tsitsiklis, 1996; Szepesv\u00e1ri, 2010; Sutton and Barto, 2018) defined as \\( (\\mathcal{X}, \\mathcal{A}, P, R, \\gamma) \\) where \\( \\mathcal{X} \\)\nis the finite set of \\( n \\) states, \\( \\mathcal{A} \\) is the finite set of \\( m \\) actions, \\( P: \\mathcal{X} \\times \\mathcal{A} \\rightarrow \\mathcal{M}(\\mathcal{X}) \\) is the transition\nkernel, \\( R : \\mathcal{X} \\times \\mathcal{A} \\rightarrow \\mathcal{M}([0, 1]) \\) is the reward function, and \\( \\gamma\\in [0,1) \\) is the discount factor.\nA policy \\( \\pi \\) is a function \\( \\pi: \\mathcal{X} \\rightarrow \\mathcal{M}(\\mathcal{A}) \\) representing the distribution over the actions an agent\nwould take from each state. Given a policy \\( \\pi \\), the functions \\( V^{\\pi}: \\mathcal{X} \\rightarrow \\mathbb{R} \\) and \\( Q^{\\pi}: \\mathcal{X} \\times \\mathcal{A} \\rightarrow \\mathbb{R} \\)\nare the corresponding (state-)value function and action-value functions defined as the expected\ndiscounted return when following \\( \\pi \\) starting at a certain state or state-action pair. We also let\n\\( P^{\\pi} : \\mathcal{X} \\rightarrow \\mathcal{M}(\\mathcal{X}) \\) and \\( R^{\\pi} : \\mathcal{X} \\rightarrow \\mathcal{M}([0,1]) \\) be the associated transition and reward kernels of policy\n\\( \\pi \\), and \\( r^{\\pi} : \\mathcal{X} \\rightarrow [0, 1] \\) be the expected reward of following \\( \\pi \\) at any state.\nThe Policy Evaluation (PE) problem is the problem of finding the value function \\( V^{\\pi} \\) corresponding\nto a given policy \\( \\pi \\) and the Control problem is the problem of finding the policy \\( \\pi^* \\) that maximizes\nthe corresponding value function \\( Q^*(x, a) \\triangleq Q^{\\pi^*}(x, a) = \\max_{\\pi} Q^{\\pi}(x, a) \\), for each state \\( x \\) and action\n\\( a \\). We shall use \\( V \\) whenever we talk about the PE problem and \\( Q \\) for the Control problem, for the\nbrevity of the presentation.\nThe Bellman operator, \\( T^{\\pi} \\), and the Bellman optimality operator, \\( T^* \\), are defined as follows:\n\n\\begin{aligned}\n(T^{\\pi}V)(x) &= r^{\\pi}(x) + \\gamma \\int P^{\\pi}(dy | x)V(y), \\quad \\quad \\quad \\quad (\\forall x \\in \\mathcal{X})\\\\\n(T^*Q)(x, a) &= r(x, a) + \\gamma \\int P(dy | x, a) \\max_{a' \\in \\mathcal{A}} Q(y, a') \\quad \\quad (\\forall x \\in \\mathcal{X}, a \\in \\mathcal{A}).\n\\end{aligned}\n\nThe Bellman residuals are functions defined as \\( BR^{\\pi}V = T^{\\pi}V - V \\) (for PE) and \\( BR^*Q = T^*Q - Q \\)\n(for Control). We write BR to refer to the appropriate choice between \\( BR^{\\pi} \\) and \\( BR^* \\) according to\nthe problem in hand. The value function \\( V^{\\pi} \\) is the unique function with \\( BR^{\\pi}V^{\\pi} = 0 \\) and \\( Q^* \\) is the\nunique function with \\( BR^*Q^* = 0 \\).\nThe iteration \\( V_{k+1} \\leftarrow T^{\\pi}V_k \\) converges to \\( V^{\\pi} \\), and the iteration \\( Q_{k+1} \\leftarrow T^*Q_k \\) converges to \\( Q^* \\).\nThis is known as the Value Iteration (VI) algorithm. The convergence is due to the \\( \\gamma \\)-contraction\nof the Bellman operators with respect to (w.r.t.) the supremum norm, and can be proven using the\nBanach fixed-point theorem. The result also shows that the convergence rate of VI is \\( O(\\gamma^k) \\). This\ncan be extremely slow for long horizon tasks with \\( \\gamma \\) very close to 1."}, {"title": "2.1 PID Value Iteration", "content": "The PID VI algorithm (Farahmand and Ghavamzadeh, 2021) is designed to address the slow conver-\ngence of VI. The key observation is that the VI algorithm can be interpreted as a feedback control"}, {"title": "3 PID TD Learning and PID Q-Learning", "content": "We introduce the PID TD Learning as well as the PID Q-Learning algorithms. These are stochastic\napproximation versions of the PID VI algorithm and use samples in the form of \\( (X_t, A_t, R_t, X_{t+1}) \\) with\n\\( A_t \\sim \\pi(\\cdot|X_t) \\) (for PE), \\( X_{t+1} \\sim P(\\cdot|X_t, A_t) \\) and \\( R_t \\sim R(\\cdot|X_t, A_t) \\), instead of directly accessing \\( P \\) and\n\\( R \\). In a typical RL setting, they form a sequence with \\( X_{t+1} = X' \\)."}, {"title": "4 Theoretical Guarantees", "content": "In this section, we focus on the PE problem and present the theoretical analysis of PID TD Learning.\nWe show that with proper choices of controller gains that make PID VI convergent, PID TD Learning\nis also convergent. Then, under synchronous update setting, we provide insights on the accelerated\nconvergence of PID TD Learning compared to the conventional TD Learning.", "subsections": [{"title": "4.1 Convergence Guarantee", "content": "Farahmand and Ghavamzadeh (2021) show that PID VI converges under a wide range of gains for\na wide range of environments both analytically and experimentally. We show that this convergence\ncarries over to our sample-based PID TD Learning. We first need to define some notation to express\nour result. Note that \\( L_{\\mathcal{V}} \\) is an affine linear operator. Define \\( A_{\\mathcal{V}} \\) to be its linear component and \\( b \\)\nto be the constant component, so that \\( L_{\\mathcal{V}}V = A_{\\mathcal{V}}V + b \\). In particular,\n\n\n\\begin{aligned}\nA_{\\mathcal{V}} &:= \\begin{bmatrix}\n(1 - \\kappa_p + \\kappa_d - \\kappa_I\\alpha)\\mathbb{I} + \\gamma(\\kappa_p + \\kappa_I\\alpha)P^{\\pi} & \\beta \\kappa_I \\mathbb{I} & -\\kappa_d \\mathbb{I}\\\\\n(-\\alpha\\mathbb{I} + \\gamma\\alpha P^{\\pi}) & \\beta \\mathbb{I} & 0 \\\\\n\\mathbb{I} & 0 & 0\n\\end{bmatrix}.\n\\end{aligned}\n\nThe matrix \\( A_{\\mathcal{V}} \\) plays a critical role in the behavior of PID VI as well as PID TD Learning. Farahmand\nand Ghavamzadeh (2021) show that PID VI is convergent for PE if \\( \\rho(A_{\\mathcal{V}}) < 1 \\) where \\( \\rho(M) \\) for a\nsquare matrix \\( M \\) is its spectral radius, the maximum of the magnitude of the eigenvalues. It turns\nout the condition on controller gains needed for convergence of PID TD Learning is weaker than the\none for PID VI. We provide the following result.\nTheorem 1 (Convergence of PID TD). Consider a set of controller gains \\( g \\). Let \\( {\\lambda_i} \\) be the\neigenvalues of \\( A_{\\mathcal{V}} \\). If \\( Re{\\lambda_i} <1 \\) for all \\( i \\), under mild assumptions on learning rate schedule \\( \\mu \\) and\nthe sequence \\( (X_t) \\) (Assumptions 1, 2), the functions \\( V_t \\) in PID TD Learning (6) converge to the\nvalue function \\( V^{\\pi} \\) of the policy \\( \\pi \\), almost surely.\nThe proof of Theorem 1 uses the ordinary differential equations (ODE) method for convergence of\nstochastic approximation algorithms (Borkar and Meyn, 2000; Borkar, 2009). The method binds\nthe behavior of the stochastic approximation to a limiting ODE. In our case, the ODE is\n\n\\begin{aligned}\n\\dot{u}(t) = L_{\\mathcal{V}}u(t) - u(t) = (A_{\\mathcal{V}} - I)u(t) + b.\n\\end{aligned}\n\nIt is shown that if this ODE converges to the stationary point \\( \\bar{V}^{\\pi} \\), PID TD Learning will also\nconverge. The condition for the convergence of this linear ODE is that the eigenvalues \\( {\\lambda_i'} \\) of\n\\( A_{\\mathcal{V}} - I \\) should have negative real parts. Since \\( \\lambda_i' = \\lambda_i - 1 \\), we get the condition in Theorem 1. Note\nthat this condition is weaker than \\( \\rho(A_{\\mathcal{V}}) < 1 \\) for PID VI Farahmand and Ghavamzadeh (2021),\nwhich is equivalent to \\( |\\lambda_i| < 1 \\). In other words, PID TD Learning maybe convergent even if PID VI\nwith the same controller gains \\( g \\) is not."}, {"title": "4.2 Acceleration Result", "content": "In this section, we provide theoretical insights on how PID TD Learning can show a faster conver-\ngence compared to the conventional TD Learning. Our analysis relies on the finite-sample analysis\nof stochastic approximation methods. Since results for the asynchronous updates are limited, we\nprovide our acceleration results for synchronous updates. Specifically, we provide our analysis for\nthe case that at each iteration \\( t \\), a dataset \\( {(x, A_x, R_x, X_x')} \\)_{x\\in\\mathcal{X}} \\) is given, where for each state \\( x \\in \\mathcal{X} \\)\nit contains the random action \\( A_x \\sim \\pi(\\cdot|x) \\), reward \\( R_x \\sim R(x, A_x) \\), and \\( X_x' \\sim P^{\\pi}(\\cdot|x, A_x) \\). Then, all\nvalues of \\( V, V' \\), and \\( z \\) are updated simultaneously in the same manner as (6). Similarly, synchronous\nTD Learning applies the conventional update on all states using the dataset. Based on the analysis\nby Chen et al. (2020b), the following theorem provides bounds on the error of both algorithms for\nthe learning rate schedule \\( \\mu(t) = \\frac{\\epsilon}{t + T} \\). We focus on the choices of \\( \\epsilon, T \\) that achieve the optimal\nasymptotic rate.\nTheorem 2. Suppose synchronous TD Learning and synchronous PID TD Learning are run with\ninitial value function \\( V_0 \\) and learning rate \\( \\mu(t) = \\frac{\\epsilon}{t + T} \\) to evaluate policy \\( \\pi \\). Let \\( V^{TD}_t, V^{PID}_t \\)\nbe the value function obtained by the algorithms at iteration \\( t \\), and \\( {c^{TD}, c^{PID}} \\) be constants only\ndependent on the MDP and controller gains. Assume \\( P^{\\pi} \\) is diagonalizable. If \\( \\epsilon > \\frac{2}{1 - \\gamma} \\) and\n\\( T > \\frac{c_1^{TD}\\epsilon}{1 - \\gamma} \\), we have\n\n\\begin{aligned}\n\\mathbb{E} \\left[||V_t^{TD} - V^{\\pi}||^2\\right] \\le c_2^{TD} ||V_0 - V^{\\pi}||^2 \\left(\\frac{T}{t+T}\\right)^{\\frac{\\epsilon(1-\\gamma)}{2}} + \\frac{c_3^{TD}(\\frac{1}{4}+\\frac{8}{3} \\epsilon ||V^{\\pi}||^2)}{\\epsilon(1 - \\gamma) - 1} \\mathbb{E} \\left[\\frac{1}{t+T}\\right]\n\\end{aligned}\n\nMoreover, assume we initialize \\( V' = V_0 \\) and \\( z = 0 \\) in PID TD Learning and \\( A_{\\mathcal{V}} \\) is diagonalizable\nwith spectral radius \\( \\rho < 1 \\). If \\( \\epsilon > \\frac{2}{1 - \\rho} \\) and \\( T > \\frac{c_1^{PID}\\epsilon}{1 - \\rho} \\), we have\n\n\\begin{aligned}\n\\mathbb{E} \\left[||V_t^{PID} - V^{\\pi}||^2\\right] \\le c_2^{PID} ||V_0 - V^{\\pi}||^2 \\left(\\frac{T}{t+T}\\right)^{\\frac{\\epsilon(1-\\rho)}{2}} + \\frac{c_3^{PID}(\\frac{1}{4}+\\frac{8}{3} \\epsilon ||V^{\\pi}||^2)}{\\epsilon(1 - \\rho) - 1} \\mathbb{E} \\left[\\frac{1}{t+T}\\right]\n\\end{aligned}\n\nThe assumption on diagonalizability of \\( P^{\\pi} \\) and \\( A_{\\mathcal{V}} \\) in Theorem 2 is for the sake of simplicity. In\nAppendix B, we provide a similar but more general result without this assumption. The upper\nbounds in Theorem 2 consist of two terms. The first term, which scales with the initial error\n\\( ||V_0 - V^{\\pi}||^2 \\) can be interpreted as the optimization error. It is the amount that \\( V_t \\) still has to change\nto reach \\( V^{\\pi} \\). The second term can be considered as the statistical error, which is independent of\nthe initial error and exists even if we start from \\( V_0 = V^{\\pi} \\). Due to the conditions \\( \\epsilon > \\frac{2}{1 - \\gamma} \\)\nand \\( \\epsilon > \\frac{2}{1 - \\rho} \\), the statistical error is asymptotically dominant with rate \\( O(t^{-1}) \\) compared to\n\\( O(t^{-\\epsilon(1-\\gamma)}) \\) or \\( O(t^{-\\epsilon(1-\\rho)}) \\) of the optimization error. Note that a larger \\( \\epsilon \\) accelerates the rate of\noptimization error, but together with larger \\( T \\) (due to the condition on \\( T \\)) slows the statistical error.\nFor example, it takes \\( T \\) steps for the statistical error to become half of its initial value. For simplicity\nof discussion, we consider \\( \\epsilon \\) and \\( T \\) fixed.\nThe difference between the two algorithms is in the rate that the optimization error goes to zero.\nThis term for TD Learning is \\( O(t^{-\\epsilon(1-\\gamma)}) \\) and for PID TD Learning is \\( O(t^{-\\epsilon(1-\\rho)}) \\). When \\( \\kappa_p = 1 \\)\nand \\( \\kappa_I = \\kappa_d = \\beta = 0 \\), we have \\( \\rho = \\gamma \\), and these two rates match. With a better choice of gains, one\ncan have \\( \\rho < \\gamma \\) (Farahmand and Ghavamzadeh, 2021) and achieve a faster rate for the optimization\nerror. Even though this term is not asymptotically dominant, we show that its speed-up can be\nsignificant in the early stages of training, especially when the policy's behavior has low stochasticity.\nTo show this, we first need to introduce the following definition.\nDefinition 1. We say policy \\( \\pi \\) in MDP \\( (\\mathcal{X}, \\mathcal{A}, P, R, \\gamma) \\) is \\( d \\)-deterministic for some \\( d \\in [0,1] \\) if for\nall \\( x \\in \\mathcal{X} \\), we have \\( Var[R^{\\pi}(x)] \\le (1 - d)/4 \\) and \\( \\max_{x'} P^{\\pi}(x'|x) \\ge d \\)."}]}, {"title": "5 Gain Adaptation", "content": "The proper choice of controller gains is critical to both convergence and acceleration of the proposed\nalgorithms. While it is possible to treat the gains as hyperparameters and tune them like any other\nhyperparameter, we address this by designing an automatic gain adaptation algorithm that tunes\nthem on the fly during the runtime of the algorithm.\nThe design of the gain adaptation algorithm for PID TD Learning and PID Q-Learning is based on\nthe same idea as gain adaptation in PID VI. Translating the update rule (3) to the sample-based\nsettings faces two main challenges. First, the derivative \\( \\partial BR^{\\pi} V_{k+1}/\\partial \\kappa_{\\cdot} \\) and normalization factor\n\\( ||BR^{\\pi} V_k ||^2 \\) are not readily available without access to the transition dynamics \\( P \\). Second, computing\nthe inner product in (3) requires iterating over all states \\( x \\),\n\n\\begin{aligned}\n\\left<BR^{\\pi}V_{k+1}, \\frac{\\partial BR^{\\pi}V_{k+1}}{\\partial \\kappa_{\\cdot}}\\right> = \\sum_{x} (BR^{\\pi}V_{k+1})(x) \\cdot \\frac{\\partial (BR^{\\pi}V_{k+1})(x)}{\\partial \\kappa_{\\cdot}}.\n\\end{aligned}\n\nrequiring the values of \\( \\partial (BR^{\\pi}V_{k+1})(x)/\\partial \\kappa_{\\cdot} \\) and \\( (BR^{\\pi}V_{k+1})(x) \\) for every \\( x \\). We will see that using\na sample \\( (X_t, A_t, R_t, X_{t+1}) \\), these values can be estimated for \\( x = X_t \\) but not for other states. A\nreplay buffer could give us access to samples at more states or function approximation could directly\nprovide estimates at all states. However, as these techniques suffer from memory and stability issues,\na better solution is needed for this challenge.\nTo avoid the difficulty of the inner product term, we modify the update rule of the gains at iteration\n\\( t \\) to minimize \\( (BR^{\\pi}V_{t+1})(X_t)^2 \\) instead of \\( ||BR^{\\pi}V_{t+1}||^2 \\). This modification is similar to performing\nstochastic gradient descent instead of gradient descent. Instead of defining the loss over the whole\nstate space, we consider the loss on a single sampled state. Consequently, the update depends only\non the state and action of the current sample \\( (X_t, A_t, R_t, X_{t+1}) \\). We get the following update:\n\n\\begin{aligned}\n\\kappa_{\\cdot} \\leftarrow \\kappa_{\\cdot} - \\eta \\frac{ \\frac{2}{\\left|BR^{\\pi}V_{t}\\right|^2} \\frac{\\partial (BR^{\\pi}V_{t+1})(X_t)^2}{\\partial \\kappa_{\\cdot}} }{||BR^{\\pi}V_{t+1}||^2} = \\kappa_{\\cdot} - \\eta \\frac{1}{\\left|BR^{\\pi}V_{t}\\right|^2} (BR^{\\pi}V_{t+1})(X_t) \\cdot \\frac{\\partial (BR^{\\pi}V_{t+1})(X_t)}{\\partial \\kappa_{\\cdot}}.\n\\end{aligned}"}, {"title": "6 Empirical Results", "content": "We empirically compare PID TD Learning and PID Q-Learning with their conventional counter-\nparts. We conduct experiments in the 50-state Chain Walk environment with 2 actions (Farahmand\nand Ghavamzadeh, 2021), the Cliff Walk environment with 6 \u00d7 6 states and 4 actions (Rakhsha\net al., 2022), and randomly generated Garnet MDPs with 50 states and 3 actions (Bhatnagar et al.,\n2009). Detailed descriptions of these environments and the policies evaluated can be found in Ap-\npendix D. Samples are taken i.i.d from the environments. We measure the error of value functions\n\\( V_t \\) and \\( Q_t \\) for PE and Control problems by their normalized error defined as \\( ||V_t - V^{\\pi}||_1 / ||V^{\\pi}||_1 \\)\nand \\( ||Q_t - Q^*||_F / ||Q^*||_F \\), respectively.\nFor all learning rates, we use state-count dependent schedules of the form \\( \\mu(N_t(X_t)) =\nmin(\\frac{\\epsilon}{N_t(X_t)}, \\epsilon) \\) for some choice of \\( \\epsilon \\) and \\( M \\) for all algorithms (including PID Q-Learning and\nQ-Learning). To achieve the best results for all algorithms, we use separate learning rates for\n\\( V, V', z \\) components of PID TD Learning and \\( Q, Q', z \\) components of PID Q-Learning. The hyper-\nparameters \\( \\epsilon, M \\) of all learning rate schedules are tuned by gridsearch over a range of values. The\ndetails of hyperparameter tuning are provided in Appendix F.\nIn we compare PID TD Learning with TD Learning when the gains are fixed and \\( \\gamma = 0.9 \\).\nIn this case the acceleration depends on the choice of gains and the environment. We observe that"}, {"title": "7 Related Work", "content": "The I component in our PID controller has a high-level similarity to the Momentum VI introduced\nby Vieillard et al. (2020). Instead of using the I component to update the values, however, they use"}, {"title": "8 Conclusion", "content": "We showed how the recent advances in accelerated planning and dynamic programming, specifically\nthe PID Value Iteration algorithm, can be used to design algorithms for the RL setting. The\nproposed PID Temporal Difference Learning and PID Q Learning algorithms are accompanied with\na gain adaptation mechanism, which tunes their hyperparameters on the fly. We provided theoretical\nanalysis as well as empirical studies of these algorithms.\nOne limitation of the current work is that the proposed algorithms are only developed for finite MDPs\nwhere the value function, and all relevant quantities, can be represented exactly. For large MDPs,"}, {"title": "A Proofs for Convergence Results (Section 4.1)", "content": "In this section, we present the proof of Theorem 1. We first present some notations, and then the\nassumptions on the learning rate schedule \\( \\mu \\) and sequence of visited samples \\( (X_t)_{t>0} \\). Let \\( r^{\\pi} \\in \\mathbb{R}^n \\)\nbe the vector of the expected immediate rewards of following the policy at each state.\nNow we move on the assumptions for Theorem 1.\nAssumption 1 (Properly Tapering Learning Rate Schedule). The learning rate schedule \\( \\mu: \\mathbb{Z} \\rightarrow \\mathbb{R}^+ \\)\nsatisfies the following:\n(i) We have \\( 0 < \\mu(t) \\le 1 \\) for any \\( t \\ge 0 \\), and\n\n\\begin{aligned}\n\\sum_{t=0}^{\\infty} \\mu(t) = \\infty \\quad , \\quad \\sum_{t=0}^{\\infty} \\mu(t)^2 < \\infty.\n\\end{aligned}\n\n(ii) For some \\( T \\), we have \\( \\mu(t + 1) < \\mu(t) \\) for all \\( t > T \\).\n(iii) For \\( z \\in (0,1) \\), \\( sup_t \\mu([zt])/\\mu(t) < \\infty \\), where \\( [\\cdot] \\) is the integer part of a number.\n(iv) For \\( z \\in (0,1) \\),\n\n\\begin{aligned}\n\\lim_{t\\rightarrow \\infty} \\frac{\\sum_{i=0}^{[zt]} \\mu(i)}{\\sum_{i=0}^{t} \\mu(i)} = 1.\n\\end{aligned}\n\nExamples of learning rate schedules that satisfy Assumption 1 includes \\( \\mu(t) = \\frac{1}{t+1} \\). The next\nassumption is on balanced update of states.\nAssumption 2 (Balanced Updates of States). The sequence of visited states \\( (X_t)_t \\) and learning\nrate schedule \\( \\mu \\) is such that we have\n(i) There exists deterministic \\( \\Delta > 0 \\), such that for all \\( x \\in \\mathcal{X} \\)\n\n\\begin{aligned}\n\\liminf_{t \\rightarrow \\infty} \\frac{N_t(x)}{t} > \\Delta \\quad a.s.\n\\end{aligned}\n\n(ii) If \\( \\mathcal{T}_t(z) = min \\{t' > t: \\sum_{i=t+1}^{t'} \\mu(i) > z \\} \\), for any \\( z > 0 \\) and states \\( x_1, x_2 \\in \\mathcal{X} \\), the following\nlimit exists\n\n\\begin{aligned}\n\\lim_{t \\rightarrow \\infty} \\frac{\\sum_{i=N_t(x_1)}^{\\mathcal{T}_t(z)}(x_1)} \\mu(i)}{\\sum_{i=N_t(x_2)}^{\\mathcal{T}_t(z)}(x_2)} \\mu(i)}\n\\end{aligned}\n\nIntuitively, Assumption 2 asserts that all states are visited often enough and get balanced sum of\nlearning rates. Before presenting the proof for Theorem 1, we first prove the following auxiliary\nlemma.\nLemma 1. Assume policy \\( \\pi \\) in the environment is \\( d \\)-deterministic and \\( x \\in \\mathcal{X} \\) is arbitrary. Let \\( R \\) and\n\\( X' \\) be the random obtained reward and next state after following policy \\( \\pi \\) from \\( x \\) in the environment.\nLet \\( W = R + \\gamma V(X') - (T^{\\pi}V)(x) \\) for an arbitrary \\( V : \\mathcal{X} \\rightarrow \\mathbb{R} \\). We have\n\n\\begin{aligned}\n\\mathbb{E} \\left[W^2\\right] \\le \\frac{1-d}{4} + \\frac{5\\gamma^2(1-d)}{4} ||V||_{\\infty}^2.\n\\end{aligned}\n\nMoreover, for some \\( \\bar{V}, f \\), let \\( \\hat{L} \\) be the estimator of \\( (L_{\\mathcal{V}}V)(x, f) \\) derived in PID TD Learning's update\n(4) according to the sample \\( (x, R, X') \\). Assume \\( W = \\hat{L} - (L_{\\mathcal{V}}V)(x, f) \\) is its noise. We have\n\n\\begin{aligned}\n\\mathbb{E} \\left[W^2\\right] \\le max((\\kappa_p + \\kappa_I\\alpha)^2, \\alpha^2) \\left(\\frac{1-d}{4} + \\frac{5\\gamma^2(1-d)}{4} ||\\bar{V}||_{\\infty}^2\\right).\n\\end{aligned}"}, {"title": "B Proofs for Acceleration Results (Section 4.2)", "content": "Before presenting the proof of the Theorems, we introduce these definitions.\nDefinition 2. Let \\( f : \\mathbb{R}^d \\) be a convex, differentiable function. Then \\( f \\) is said to be \\( L \\)-smooth w.r.t.\nnorm \\( ||\\cdot|| \\) if and only if\n\n\\begin{aligned}\nf(y) \\le f(x) + \\langle \\nabla f(x), y - x \\rangle + \\frac{L}{2} ||y - x||^2 \\quad \\forall x, y \\in \\mathbb{R}^d.\n\\end{aligned}\n\nDefinition 3. For any non-singular matrix \\( S \\in \\mathbb{R}^{d \\times d} \\), we define the vector norm \\( ||v||_{2,S} \\triangleq ||Sv||_2 \\).\nFor any matrix \\( A \\), we let \\( ||A||_{2,S} \\) be the matrix norm of \\( A \\) induced by the vector norm \\( ||\\cdot||_{2,S} \\).\nWe also need the following lemmas.\nLemma 2. Let \\( A \\in \\mathbb{R}^{d \\times d} \\) and \\( \\delta \\ge 0 \\). If \\( A \\) is not diagonalizable, further assume \\( \\delta > 0 \\). There exists\nan invertible matrix \\( S \\) such that \\( ||A||_{2,S} \\le \\rho(A) + \\delta \\) and for any \\( v \\in \\mathbb{R}^d, ||v||_{2,S} \\le ||v||_{\\infty} \\).", "json": "subsections"}, {"title": "B.1 Proof of Theorem 2", "content": "We prove the more general result than Theorem 2 without any diagonalizablity assumptions. The-\norem 2 is the special case of the following when \\( \\delta^{TD} = \\delta^{PID} = 0 \\).\nTheorem 3. Suppose synchronous TD Learning and PID TD Learning are run with initial value\nfunction \\( V_0 \\) and learning rate \\( \\mu(t) = \\frac{\\epsilon}{t + T} \\) to evaluate policy \\( \\pi \\). Let \\( V^{TD}_t, V^{PID}_t \\) be the value\nfunction obtained with each algorithm at iteration \\( t \\). Assume \\( \\delta^{TD}, \\delta^{PID} \\ge 0 \\). If \\( P^{\\pi} \\) is not diago-\nnalizable, we further assume \\( \\delta^{TD} > 0 \\), and if \\( A_{\\mathcal{V}} \\) is not diagonalizable, we assume \\( \\delta^{PID} > 0 \\). If\n\\( \\epsilon > \\frac{2}{1 - \\gamma - \\delta^{TD}} \\) and \\( T > \\frac{c_1^{TD}\\epsilon}{1 - \\gamma - \\delta^{TD}} \\), we have\n\n\\begin{aligned}\n\\mathbb{E} \\left[||V_t^{TD} - V^{\\pi}||^2\\right] \\le c_2^{TD} ||V_0 - V^{\\pi}||^2 \\left(\\frac{T}{t+T}\\right)^{\\frac{\\epsilon(1-\\gamma-\\delta^{TD})}{2}} + \\frac{c_3^{TD}(\\frac{1}{4}+\\frac{8}{3} \\epsilon ||V^{\\pi}||^2)}{\\epsilon(1 - \\gamma - \\delta^{TD}) - 1} \\mathbb{E} \\left[\\frac{1}{t+T}\\right]\n\\end{aligned}\n\nHere, \\( {c_i^{TD}} \\) are constants dependent on the MDP and \\( \\delta^{TD} \\). Moreover, assume we initialize \\( V' =\nV_0, z = 0 \\) in PID TD Learning and \\( A_{\\mathcal{V}} \\) has spectral radius \\( \\rho < 1 \\). If \\( \\epsilon > \\frac{2}{1 - \\rho - \\delta^{PID}} \\) and\n\\( T > \\frac{c_1^{PID}\\epsilon}{1 - \\rho - \\delta} \\), we have\n\n\\begin{aligned}\n\\mathbb{E} \\left[||V_t^{PID} - V^{\\pi}||^2\\right] \\le c_2^{PID} ||V_0 - V^{\\pi}||^2 \\left(\\frac{T}{t+T}\\right)^{\\frac{\\epsilon(1-\\rho-\\delta^{PID})}{2}} + \\frac{c_3^{PID}(\\frac{1}{4}+\\frac{8}{3} \\epsilon ||V^{\\pi}||^2)}{\\epsilon(1 - \\rho - \\delta^{PID}) - 1} \\mathbb{E} \\left[\\frac{1}{t+T}\\right]\n\\end{aligned}\n\nHere, \\( {c_i^{PID}} \\) are constants dependent on the MDP, controller gains, and \\( \\delta^{PID} \\)."}]}, {"title": "B.2 Proof of Proposition 1", "content": "Proof.\n\n\\begin{aligned}\n\\frac{E_t^{TD}(0)}{E_t^{stat}(0)} > \\frac{||V_0 - V^{\\pi}||_{\\infty}^2}{\\frac{16\\epsilon \\epsilon^2 (c^{TD} + 2 B^{TD} ||V^{\\pi}||_{\\infty}) T}{c_1^{TD^2}}}\n\\end{aligned}"}, {"title": "C Details of Gain Adaptation", "content": "Tables 1 and 2 show the semi-gradients used for gain adaptation for Policy Evaluation and Control\nrespectively. Algorithms 1 and 2 show the detailed description of the algorithm for Policy Evaluation\nand Control respectively."}, {"title": "D Description of the Environments", "content": "The environment consists of 50 states that are connected in a circular chain. The agent has two\nactions available, moving left or right. Upon taking an action, the agent succeeds with probability\n0.7, stays in place with probability 0.1, and moves in the opposite direction with probability 0.2.\nThe agent receives a reward of 1 when entering state 10, and reward -1 when entering state 40, and\na reward of 0 otherwise.\nThe policy evaluated in the TD experiments is to always move left.", "subsections": [{"title": "D.2 Cliff Walk", "content": "A 6 by 6 grid world is used, visualized in Figure 5. The agent starts on the top left its goal is to end\nup on the top right. There are 12 cliff tiles, and the agent is stuck in them if it falls in. Moreover, the"}, {"title": "D.3 Garnet", "content": "The environment is randomly generated. They consist of 50 states, and 3 actions per state.\nTo build the environment, for each action and state, \\( (x, a) \\), we pick 5 other random states \\( X_{x,a} \\). For\n10 randomly chosen states \\( x \\), we set \\( r(x) \\) from a uniform distribution between 0 and 1. We set \\( r(x) \\)\nis zero on all other states. Then, when taking action \\( a \\) and from state \\( x \\), we receive reward \\( r(x) \\) and\nmove to any state in \\( X_{x,a} \\) with equal probability.\nThe policy evaluated in the TD experiments is a random walk."}]}, {"title": "E Additional Experiments", "content": "Figure 6 shows the performance of gain adaptation on Chain Walk when \\( \\gamma = 0.99 \\), and the corre-\nsponding movement of the controller gains. Figure 7 shows the performance of gain adaptation on\nCliff Walk when \\( \\gamma = 0.99 \\), and the corresponding movement of the controller gains."}, {"title": "F Details of Experimental Setup", "content": "We pick the hyperparameters such that a normalized error of 0.2 is achieved the fastest, and if this\nerror is not achieved, the final error is minimized.\nWe fix \\( \\alpha = 0.05, \\beta = 0.95 \\), and \\( \\lambda = 0.5 \\) throughout all the experiments. For the Cliff Walk policy\nevaluation experiments in Figure 2, we set \\( \\eta = 10^{-5} \\) and \\( \\epsilon = 0.1 \\). For the Chain Walk (Control)\nexperiments in Figure 3, we set \\( \\eta = 4 \\times 10^{-8} \\) and \\( \\epsilon = 10^{-4} \\). For the Chain Walk (PE) experiments\nin Figure 6, we set \\( \\eta = 5 \\times 10^{-7} \\) and \\( \\epsilon = 10^{-3} \\). For the Cliff Walk (Control) experiments in Figure 7,\nwe set \\( \\eta = 8 \\times 10^{-9} \\) and \\( \\epsilon = 10^{-3} \\)."}]