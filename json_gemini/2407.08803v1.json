{"title": "PID Accelerated Temporal Difference Algorithms", "authors": ["Mark Bedaywi", "Amin Rakhsha", "Amir-massoud Farahmand"], "abstract": "Long-horizon tasks, which have a large discount factor, pose a challenge for most conventional reinforcement learning (RL) algorithms. Algorithms such as Value Iteration and Temporal Difference (TD) learning have a slow convergence rate and become inefficient in these tasks. When the transition distributions are given, PID VI was recently introduced to accelerate the convergence of Value Iteration using ideas from control theory. Inspired by this, we introduce PID TD Learning and PID Q-Learning algorithms for the RL setting in which only samples from the environment are available. We give theoretical analysis of their convergence and acceleration compared to their traditional counterparts. We also introduce a method for adapting PID gains in the presence of noise and empirically verify its effectiveness.", "sections": [{"title": "1 Introduction", "content": "The Value Iteration (VI) algorithm is one of the primary dynamic programming methods for solving (discounted) Markov Decision Processes (MDP). It is the foundation of many Reinforcement Learning (RL) algorithms such as the Temporal Difference (TD) Learning (Sutton, 1988; Tsitsiklis and Van Roy, 1997), Q-Learning (Watkins, 1989), Approximate Value/Fitted Iteration (Gordon, 1995; Ernst et al., 2005; Munos and Szepesv\u00e1ri, 2008; Tosatto et al., 2017), and DQN (Mnih et al., 2015; Van Hasselt et al., 2016), which can all be seen as sample-based variants of VI. A weakness of the VI algorithm and the RL algorithms built on top of it is their slow convergence in problems with discount factor \\( \\gamma \\) close to 1, which corresponds to the long-horizon problems where the agent aims to maximize its cumulative rewards far in the future. One can show that the error of the value function calculated by VI at iteration k goes to zero with the rate of \\( O(\\gamma^k) \\). The adverse dependence of the convergence rate on \\( \\gamma \\) also appears in the error analysis of the downstream temporal difference (Szepesv\u00e1ri, 1997; Even-Dar and Mansour, 2003; Wainwright, 2019) and fitted value iteration algorithms (Munos and Szepesv\u00e1ri, 2008; Farahmand et al., 2010; Chen and Jiang, 2019; Fan et al., 2019). If \\( \\gamma \\approx 1 \\), these algorithms become very slow and inefficient. This work introduces accelerated temporal difference learning algorithms that can mitigate this issue.\nFarahmand and Ghavamzadeh (2021) recently suggested that one may view the iterates of VI as a dynamical system itself. This opens up the possibility of using tools from control theory to modify, and perhaps accelerate, the VI\u2019s dynamics. They specifically used the simple class of Proportional-Integral-Derivative (PID) controllers to modify VI, resulting in a new procedure called the PID VI algorithm. They showed that with a careful choice of the controller gains, PID VI can converge significantly faster than the conventional VI. They also introduced a gain adaptation mechanism, a meta-learning procedure, to automatically choose these gains.\nPID VI, similar to VI, is a dynamic programming algorithm and requires access to the full transition dynamics of the environment. In the RL setting, however, the transition dynamics is not directly accessible to the agent; the agent can only acquire samples from the transition dynamics by interacting with the environment."}, {"title": "2 Background", "content": "Given a set \\( \\Omega \\), let \\( \\mathcal{M}(\\Omega) \\) be the set of probability distributions over \\( \\Omega \\), and \\( \\mathcal{B}(\\Omega) \\) be the set of bounded functions over \\( \\Omega \\). We consider a discounted Markov Decision Process (MDP) defined as \\( (\\mathcal{X}, \\mathcal{A}, P, R, \\gamma) \\) where \\( \\mathcal{X} \\) is the finite set of n states, \\( \\mathcal{A} \\) is the finite set of m actions, \\( P: \\mathcal{X} \\times \\mathcal{A} \\rightarrow \\mathcal{M}(\\mathcal{X}) \\) is the transition kernel, \\( R : \\mathcal{X} \\times \\mathcal{A} \\rightarrow \\mathcal{M}([0, 1]) \\) is the reward function, and \\( \\gamma\\in [0,1) \\) is the discount factor.\nA policy \\( \\pi \\) is a function \\( \\pi: \\mathcal{X} \\rightarrow \\mathcal{M}(\\mathcal{A}) \\) representing the distribution over the actions an agent would take from each state. Given a policy \\( \\pi \\), the functions \\( V^{\\pi} : \\mathcal{X} \\rightarrow \\mathbb{R} \\) and \\( Q^{\\pi} : \\mathcal{X} \\times \\mathcal{A} \\rightarrow \\mathbb{R} \\) are the corresponding (state-)value function and action-value functions defined as the expected discounted return when following \\( \\pi \\) starting at a certain state or state-action pair. We also let \\( P^{\\pi} : \\mathcal{X} \\rightarrow \\mathcal{M}(\\mathcal{X}) \\) and \\( R^{\\pi} : \\mathcal{X} \\rightarrow \\mathcal{M}([0,1]) \\) be the associated transition and reward kernels of policy \\( \\pi \\), and \\( r^{\\pi} : \\mathcal{X} \\rightarrow [0, 1] \\) be the expected reward of following \\( \\pi \\) at any state.\nThe Policy Evaluation (PE) problem is the problem of finding the value function \\( V^{\\pi} \\) corresponding to a given policy \\( \\pi \\) and the Control problem is the problem of finding the policy \\( \\pi^* \\) that maximizes the corresponding value function \\( Q^*(x, a) \\stackrel{\\triangle}{=} Q^{\\pi^*}(x, a) = \\max_{\\pi} Q^{\\pi}(x, a) \\), for each state \\( x \\) and action \\( a \\). We shall use \\( V \\) whenever we talk about the PE problem and \\( Q \\) for the Control problem, for the brevity of the presentation.\nThe Bellman operator, \\( T^{\\pi} \\), and the Bellman optimality operator, \\( T^* \\), are defined as follows:\n\n\\begin{aligned}\n(T^{\\pi}V)(x) &= r^{\\pi}(x) + \\gamma \\int P^{\\pi}(dy \\mid x)V(y), &(\\forall x \\in \\mathcal{X}) \\\\\n(T^*Q)(x, a) &= r(x, a) + \\gamma \\int P(dy \\mid x, a) \\max_{a' \\in \\mathcal{A}} Q(y, a') &(\\forall x \\in \\mathcal{X}, a \\in \\mathcal{A}).\n\\end{aligned}\n\nThe Bellman residuals are functions defined as \\( \\text{BR}V = T^{\\pi}V - V \\) (for PE) and \\( \\text{BR}^*Q = T^*Q - Q \\) (for Control). We write \\( \\text{BR} \\) to refer to the appropriate choice between \\( \\text{BR}^{\\pi} \\) and \\( \\text{BR}^* \\) according to the problem in hand. The value function \\( V^{\\pi} \\) is the unique function with \\( \\text{BR}^{\\pi}V^{\\pi} = 0 \\) and \\( Q^* \\) is the unique function with \\( \\text{BR}^*Q^* = 0 \\).\nThe iteration \\( V_{k+1} \\leftarrow T^{\\pi}V_k \\) converges to \\( V^{\\pi} \\), and the iteration \\( Q_{k+1} \\leftarrow T^*Q_k \\) converges to \\( Q^* \\). This is known as the Value Iteration (VI) algorithm. The convergence is due to the \\( \\gamma \\)-contraction of the Bellman operators with respect to (w.r.t.) the supremum norm, and can be proven using the Banach fixed-point theorem. The result also shows that the convergence rate of VI is \\( O(\\gamma^k) \\). This can be extremely slow for long horizon tasks with \\( \\gamma \\) very close to 1."}, {"title": "2.1 PID Value Iteration", "content": "The PID VI algorithm is designed to address the slow convergence of VI. The key observation is that the VI algorithm can be interpreted as a feedback control"}, {"title": "3 PID TD Learning and PID Q-Learning", "content": "We introduce the PID TD Learning as well as the PID Q-Learning algorithms. These are stochastic approximation versions of the PID VI algorithm and use samples in the form of \\( (X_t, A_t, R_t, X_t') \\) with \\( A_t \\sim \\pi(X_t) \\) (for PE), \\( X_t' \\sim P(\\cdot | X_t, A_t) \\) and \\( R_t \\sim R(\\cdot | X_t, A_t) \\), instead of directly accessing \\( P \\) and \\( R \\). In a typical RL setting, they form a sequence with \\( X_{t+1} = X_t' \\)."}, {"title": "4 Theoretical Guarantees", "content": "In this section, we focus on the PE problem and present the theoretical analysis of PID TD Learning. We show that with proper choices of controller gains that make PID VI convergent, PID TD Learning is also convergent. Then, under synchronous update setting, we provide insights on the accelerated convergence of PID TD Learning compared to the conventional TD Learning."}, {"title": "4.1 Convergence Guarantee", "content": "Farahmand and Ghavamzadeh (2021) show that PID VI converges under a wide range of gains for a wide range of environments both analytically and experimentally. We show that this convergence carries over to our sample-based PID TD Learning. We first need to define some notation to express our result. Note that \\( L_g \\) is an affine linear operator. Define \\( A_g \\) to be its linear component and \\( b \\) to be the constant component, so that \\( L_gV = A_gV + b \\). In particular,\n\n\\begin{equation}\nA_g := \\begin{bmatrix}\n(1 - K_p + K_d - K_I\\alpha)I + \\gamma(K_p + K_I\\alpha)P^{\\pi} & B & -K_dI \\\\\n(- \\alpha I + \\gamma \\alpha P^{\\pi}) & 0 & \\beta I \\\\\nI & 0 & 0\n\\end{bmatrix}.\n\\end{equation}\n\nThe matrix \\( A_g \\) plays a critical role in the behavior of PID VI as well as PID TD Learning. Farahmand and Ghavamzadeh (2021) show that PID VI is convergent for PE if \\( \\rho(A_g) < 1 \\) where \\( \\rho(M) \\) for a square matrix \\( M \\) is its spectral radius, the maximum of the magnitude of the eigenvalues. It turns out the condition on controller gains needed for convergence of PID TD Learning is weaker than the one for PID VI. We provide the following result.\nTheorem 1 (Convergence of PID TD). Consider a set of controller gains \\( g \\). Let \\( {\\lambda_i} \\) be the eigenvalues of \\( A_g \\). If \\( \\text{Re}{\\lambda_i} <1 \\) for all \\( i \\), under mild assumptions on learning rate schedule \\( \\mu \\) and the sequence \\( (X_t) \\) (Assumptions 1, 2), the functions \\( V_t \\) in PID TD Learning converge to the value function \\( V^{\\pi} \\) of the policy \\( \\pi \\), almost surely.\nThe proof of Theorem 1 uses the ordinary differential equations (ODE) method for convergence of stochastic approximation algorithms. The method binds the behavior of the stochastic approximation to a limiting ODE. In our case, the ODE is\n\n\\begin{equation}\n\\dot{u}(t) = L_g u(t) - u(t) = (A_g - I)u(t) + b.\n\\end{equation}\n\nIt is shown that if this ODE converges to the stationary point \\( \\bar{V}^{\\pi} \\), PID TD Learning will also converge. The condition for the convergence of this linear ODE is that the eigenvalues \\( {\\lambda_i} \\) of \\( A_g - I \\) should have negative real parts. Since \\( \\Lambda_i = \\lambda_i - 1 \\), we get the condition in Theorem 1. Note that this condition is weaker than \\( \\rho(A_g) < 1 \\) for PID VI, which is equivalent to \\( |\\lambda_i| < 1 \\). In other words, PID TD Learning maybe convergent even if PID VI with the same controller gains \\( g \\) is not."}, {"title": "4.2 Acceleration Result", "content": "In this section, we provide theoretical insights on how PID TD Learning can show a faster convergence compared to the conventional TD Learning. Our analysis relies on the finite-sample analysis of stochastic approximation methods. Since results for the asynchronous updates are limited, we provide our acceleration results for synchronous updates. Specifically, we provide our analysis for the case that at each iteration t, a dataset \\( {(x, A_x, R_x, X'_x)} \\)_{x \\in \\mathcal{X}} \\) is given, where for each state \\( x \\in \\mathcal{X} \\) it contains the random action \\( A_x \\sim \\pi(\\cdot | x) \\), reward \\( R_x \\sim R(x, A_x) \\), and \\( X'_x \\sim P^{\\pi}(\\cdot | x, A_x) \\). Then, all values of V, V', and z are updated simultaneously in the same manner as . Similarly, synchronous TD Learning applies the conventional update on all states using the dataset. Based on the analysis by, the following theorem provides bounds on the error of both algorithms for the learning rate schedule \\( \\mu(t) = \\frac{\\epsilon}{(t + T)} \\). We focus on the choices of \\( \\epsilon, T \\) that achieve the optimal asymptotic rate.\nTheorem 2. Suppose synchronous TD Learning and synchronous PID TD Learning are run with initial value function \\( V_0 \\) and learning rate \\( \\mu(t) = \\frac{\\epsilon}{(t + T)} \\) to evaluate policy \\( \\pi \\). Let \\( V^{TD}_t, V^{PID}_t \\) be the value function obtained by the algorithms at iteration t, and \\( {c^{TD}, c^{PID}} \\) be constants only dependent on the MDP and controller gains. Assume \\( P^{\\pi} \\) is diagonalizable. If \\( \\epsilon > \\frac{2}{(1 - \\gamma)} \\) and \\( T > \\frac{c_1^{TD} \\epsilon}{(1 - \\gamma)} \\), we have\n\n\\begin{equation}\n\\mathbb{E} \\left[||V^{TD}_t - V^*||^2 \\right] \\leq c_1^{TD} ||V_0 - V^*||^2 \\left( \\frac{T}{t + T} \\right)^{\\frac{\\epsilon(1 - \\gamma)}{2}} + \\frac{c_2^{TD} \\left( \\frac{c_3^{TD}}{\\epsilon} + c_4^{TD} ||V^*|| \\right)}{(\\epsilon(1 - \\gamma) - 1)} \\mathbb{E} \\left[ \\frac{1}{t + T} \\right].\n\\end{equation}\n\nMoreover, assume we initialize \\( V' = V_0 \\) and \\( z = 0 \\) in PID TD Learning and \\( A_g \\) is diagonalizable with spectral radius \\( \\rho < 1 \\). If \\( \\epsilon > \\frac{2}{(1 - \\rho)} \\) and \\( T > \\frac{c_1^{PID} \\epsilon}{(1 - \\rho)} \\), we have\n\n\\begin{equation}\n\\mathbb{E} \\left[||V^{PID}_t - V^*||^2 \\right] \\leq c_1^{PID} ||V_0 - V^*||^2 \\left( \\frac{T}{t + T} \\right)^{\\frac{\\epsilon(1 - \\rho)}{2}} + \\frac{c_2^{PID} \\left( \\frac{c_3^{PID}}{\\epsilon} + c_4^{PID} ||V^*|| \\right)}{(\\epsilon(1 - \\rho) - 1)} \\mathbb{E} \\left[ \\frac{1}{t + T} \\right].\n\\end{equation}\n\nThe assumption on diagonalizability of \\( P^{\\pi} \\) and \\( A_g \\) in Theorem 2 is for the sake of simplicity. The upper bounds in Theorem 2 consist of two terms. The first term, which scales with the initial error \\( ||V_0 - V^*|| \\) can be interpreted as the optimization error. It is the amount that \\( V_t \\) still has to change to reach \\( V^* \\). The second term can be considered as the statistical error, which is independent of the initial error and exists even if we start from \\( V = V^* \\). Due to the conditions \\( \\epsilon > \\frac{2}{(1 - \\gamma)} \\) and \\( \\epsilon > \\frac{2}{(1 - \\rho)} \\), the statistical error is asymptotically dominant with rate \\( O(t^{-1}) \\) compared to \\( O(t^{-\\epsilon(1-\\gamma)}) \\) or \\( O(t^{-\\epsilon(1-\\rho)}) \\) of the optimization error. Note that a larger \\( \\epsilon \\) accelerates the rate of optimization error, but together with larger T (due to the condition on T) slows the statistical error. For example, it takes T steps for the statistical error to become half of its initial value. For simplicity of discussion, we consider \\( \\epsilon \\) and T fixed.\nThe difference between the two algorithms is in the rate that the optimization error goes to zero. This term for TD Learning is \\( O(t^{-\\epsilon(1-\\gamma)}) \\) and for PID TD Learning is \\( O(t^{-\\epsilon(1-\\rho)}) \\). When \\( K_p = 1 \\) and \\( K_I = K_d = \\beta = 0 \\), we have \\( \\rho = \\gamma \\), and these two rates match. With a better choice of gains, one can have \\( \\rho < \\gamma \\) and achieve a faster rate for the optimization error. Even though this term is not asymptotically dominant, we show that its speed-up can be significant in the early stages of training, especially when the policy\u2019s behavior has low stochasticity. To show this, we first need to introduce the following definition.\nDefinition 1. We say policy \\( \\pi \\) in MDP \\( (\\mathcal{X}, \\mathcal{A}, P, R, \\gamma) \\) is d-deterministic for some \\( d \\in [0,1] \\) if for all \\( x \\in \\mathcal{X} \\), we have \\( \\text{Var}[R^{\\pi}(x)] \\leq (1 - d)/4 \\) and \\( \\max_{x'} P^{\\pi}(x'|x) \\geq d \\)."}, {"title": "5 Gain Adaptation", "content": "The proper choice of controller gains is critical to both convergence and acceleration of the proposed algorithms. While it is possible to treat the gains as hyperparameters and tune them like any other hyperparameter, we address this by designing an automatic gain adaptation algorithm that tunes them on the fly during the runtime of the algorithm.\nThe design of the gain adaptation algorithm for PID TD Learning and PID Q-Learning is based on the same idea as gain adaptation in PID VI. Translating the update rule to the sample-based settings faces two main challenges. First, the derivative \\( \\frac{\\partial \\text{BR}^{\\pi} V_{t+1}}{\\partial K_i} \\) and normalization factor \\( ||\\text{BR}^{\\pi} V_t||^2 \\) are not readily available without access to the transition dynamics P. Second, computing the inner product in requires iterating over all states x,\n\n\\begin{equation}\n\\left<BR^{\\pi}V_{t+1},\\frac{\\partial BR^{\\pi}V_{t+1}}{\\partial K_i} \\right> = \\sum_x (BR^{\\pi}V_{t+1})(x) \\frac{\\partial (BR^{\\pi}V_{t+1})(x)}{\\partial K_i}.\n\\end{equation}\n\nrequiring the values of \\( \\partial (BR^{\\pi} V_{t+1})(x)/\\partial K_i \\) and \\( (BR^{\\pi} V_{t+1})(x) \\) for every x. We will see that using a sample \\( (X_t, A_t, R_t, X'_t) \\), these values can be estimated for \\( x = X_t \\) but not for other states. A replay buffer could give us access to samples at more states or function approximation could directly provide estimates at all states. However, as these techniques suffer from memory and stability issues, a better solution is needed for this challenge.\nTo avoid the difficulty of the inner product term, we modify the update rule of the gains at iteration t to minimize \\( (BR^{\\pi} V_{t+1}(X_t))^2 \\) instead of \\( ||BR^{\\pi} V_{t+1}||^2 \\). This modification is similar to performing stochastic gradient descent instead of gradient descent. Instead of defining the loss over the whole state space, we consider the loss on a single sampled state. Consequently, the update depends only on the state and action of the current sample \\( (X_t, A_t, R_t, X'_t) \\). We get the following update:\n\n\\begin{equation}\nK_i \\leftarrow K_i - \\eta \\frac{ \\frac{\\partial (BR^{\\pi} V_{t+1}(X_t))^2}{\\partial K_i}}{||BR^{\\pi} V_t||^2} = K_i - \\eta \\frac{BR^{\\pi} V_{t+1}(X_t) \\cdot \\frac{\\partial BR^{\\pi} V_{t+1}(X_t)}{\\partial K_i}}{||BR^{\\pi} V_t||^2}.\n\\end{equation}"}, {"title": "7 Related Work", "content": "The I component in our PID controller has a high-level similarity to the Momentum VI introduced by. Instead of using the I component to update the values, however, they use"}, {"title": "8 Conclusion", "content": "We showed how the recent advances in accelerated planning and dynamic programming, specifically the PID Value Iteration algorithm, can be used to design algorithms for the RL setting. The proposed PID Temporal Difference Learning and PID Q Learning algorithms are accompanied with a gain adaptation mechanism, which tunes their hyperparameters on the fly. We provided theoretical analysis as well as empirical studies of these algorithms.\nOne limitation of the current work is that the proposed algorithms are only developed for finite MDPS where the value function, and all relevant quantities, can be represented exactly. For large MDPs,"}]}