{"title": "Revisiting Multi-Modal LLM Evaluation", "authors": ["Jian Lu", "Shikhar Srivastava", "Junyu Chen", "Robik Shrestha", "Manoj Acharya", "Kushal Kafle", "Christopher Kanan"], "abstract": "With the advent of multi-modal large language models (MLLMs), datasets used\nfor visual question answering (VQA) and referring expression comprehension\nhave seen a resurgence. However, the most popular datasets used to evaluate\nMLLMs are some of the earliest ones created, and they have many known problems,\nincluding extreme bias, spurious correlations, and an inability to permit fine-grained\nanalysis. In this paper, we pioneer evaluating recent MLLMs (LLaVA 1.5, LLaVA-\nNeXT, BLIP2, InstructBLIP, GPT-4V, and GPT-40) on datasets designed to address\nweaknesses in earlier ones. We assess three VQA datasets: 1) TDIUC, which\npermits fine-grained analysis on 12 question types; 2) TallyQA, which has simple\nand complex counting questions; and 3) DVQA, which requires optical character\nrecognition for chart understanding. We also study VQDv1, a dataset that requires\nidentifying all image regions that satisfy a given query. Our experiments reveal\nthe weaknesses of many MLLMs that have not previously been reported. Our\ncode is integrated into the widely used LAVIS framework for MLLM evaluation,\nenabling the rapid assessment of future MLLMs. Project webpage: https://\nkevinlujian.github.io/MLLM_Evaluations/", "sections": [{"title": "1 Introduction", "content": "In recent years, multi-modal large language models (MLLMs) have emerged as powerful tools for\ntackling vision-language tasks [1-5]. Open source MLLMs leverage the extensive world knowledge\nof large language models (LLMs) and combine them with pre-trained vision encoders to process both\nlinguistic and visual information [3, 5, 6]. These models are trained on various vision-language tasks\nsuch as visual question answering (VQA) [7, 8], image captioning [9], and visual conversations [10].\nTheir effectiveness is typically evaluated on VQA datasets [7, 11], which test the ability to produce\nanswers to questions about images and referring expression comprehension tasks [12], which require\nlocalizing the single object specified in the referring expression.\nFrom 2017-2019, our lab created a series of datasets intended to enable fine-grained analysis of\nvisually grounded language understanding systems:\n1. VQDv1 [13], which requires the model to produce multiple bounding boxes instead of\nlocalizing only one object, thereby testing for general query detection skills;\n2. TallyQA [14], which tests visual grounding through counting skills, asking questions that\nrequire intricate reasoning;\n3. TDIUC [15], which tests versatility across 12 tasks, including object, attribute, and activity\nrecognition, as well as overall scene understanding; and\n4. DVQA [16], which requires interpreting and analyzing visual data in chart form, testing for\nthe ability to do OCR, and properly handling unusual words found in charts.\nWhile our datasets were widely used for evaluating VQA systems prior to the MLLM era, surprisingly,\nthey have not been used to evaluate MLLMs. Based on community feedback, this is for two reasons:"}, {"title": "2 Multi-modal Large Language Models", "content": "Open-source MLLMs comprise a pre-trained LLM, a pre-trained vision encoder, and a learned adapter\nthat aligns the visual and linguistic representations [3, 22]. They are usually trained in multiple\nstages. Initially, the adapter is trained to align the visual embeddings generated by the vision encoder\nwith the textual embedding space of the LLM. Subsequently, the MLLM undergoes fine-tuning by\nadapting both the adapter and the LLM on various vision-language and instruction-tuning datasets. In\nour study, we consider both widely available state-of-the-art open-weight MLLMs and closed-source\nMLLMs.\nBLIP2 [1] is a generic and compute-efficient method for vision-language pre-training that leverages\nfrozen pre-trained image encoders and language models (LLMs). It pre-trains a lightweight Querying\nTransformer (Q-Former), consisting of image and text transformer sub-modules, to bridge visual\nand textual modalities. BLIP2, therefore, only trains a relatively light - 188M parameter transformer\nand achieves strong performance on VQA and image captioning tasks. We evaluate the base BLIP2\nmodel [1], with 'blip2-flan-t5-xl' as the pretrained encoder.\niBLIP [23] (i.e., InstructBLIP), like BLIP-2, keeps the LLM and visual encoders frozen while\nintroducing a novel instruction-aware Query Transformer that allows the model to extract informative\nvisual features based on the textual instructions in the prompt. iBLIP is additionally trained on a much\nlarger corpus of visual instruction tuning datasets, including knowledge-grounded image-question\nanswering, visual reasoning, and VQA [23]. This leads to improvements, including higher zero-shot\nperformance on VQA tasks, compared to BLIP2 and larger MLLMs. We test the version that uses\n'instructblip-flan-t5-xxl' as the pre-trained encoder.\nLLaVA [5] uses a visual instruction tuning dataset to fine-tune the LLM and adapter. LLaVA 1.5\nenhances its vision encoder to handle higher-resolution images and replaces the linear projector\nlayer with a multi-layer perceptron adapter. This version is trained on the VQA datasets VQAv2 and\nGQA datasets and a broader range of instruction-tuning data from sources like ShareGPT. These\nenhancements significantly improve its performance on fine-grained visual tasks, including detailed"}, {"title": "3 Creating \u201cSlim\u201d Evaluation Sets", "content": "We evaluate MLLMs on the entire validation set of TallyQA, which contains 38,589 questions.\nHowever, the other datasets are much larger, which makes it challenging to quickly and inexpensively\nevaluate MLLMs on them. To address this, we sample subsets from these datasets for evaluation. A\nuniform random sampling is suboptimal as these datasets have long-tailed distributions and sampling\nuniformly would result in discarding examples from the tail. Therefore, we adopt a stratified sampling\napproach for DVQA and TDIUC, where we also maintain as much answer variety as possible.\nSpecifically, we first categorize the questions into fine-grained groups, defined by both the pre-defined\ntypes in the datasets (e.g., question types or difficulty levels) and their corresponding answers. We\ndefine r as the sampling ratio and k as the minimum number of samples from each group. For any\nlarge group, we uniformly sample an r proportion of the entries. For smaller groups, if the size m is\nsuch that $m \\cdot r$ is less than k, we sample k entries. For groups even smaller than k, we use the entire\ngroup. The number of samples $m'$ to be taken from group $|g_i| = m$ can be represented as follows:\n$m'_i = \\begin{cases}\nm_i & \\text{if } m_i < k \\\nk & \\text{if } m_i \\cdot r < k/m_i>k \\\\\n\\lfloor m_i \\cdot r \\rfloor & \\text{if } m_i \\cdot r \\geq k\n\\end{cases}$\nVQDv1 has a long-tail distribution regarding the number of bounding boxes per query, where queries\nwith 0 or 1 box comprise almost 90% of the dataset. Our goal is to evaluate the MLLM's ability to\ngenerate a variable number of bounding boxes \u2013 extending the evaluation scope beyond traditional\nreferring expression comprehension datasets such as RefCOCO [18], where all referring expressions\nare associated with only one bounding box. Therefore, we retained all the questions with more than\none bounding box and randomly sampled queries corresponding to 0 or 1 bounding box. As seen in\nTable 7, this method effectively increases the ratio of questions with multiple bounding boxes.\nOur sampling method preserves the most challenges samples present in the original dataset, ensuring\na comprehensive evaluation while significantly reducing computational overhead. Summary statistics\nfor the datasets are given in Table 1."}, {"title": "4 Experiments", "content": "Across datasets we compute both micro performance, i.e., where every example is weighted equally,\nand macro performance, where we average across the mean score for different question/query types."}, {"title": "4.1 Visual Query Detection with VQDv1", "content": "Visual query detection (VQD) requires a model to provide bounding boxes for 0-N visual objects\nin response to a given query [13]. It is significantly more challenging than referring expression\ncomprehension, which requires only localizing a single object in a scene. VQD aligns more closely\nwith typical human referring behavior, where it is common to refer to multiple objects simultaneously.\nUnlike VQA, VQD requires the model to ground responses in visual inputs, providing direct evidence\nof task completion.\nWe evaluated all models on VQDv1 except for BLIP2 and iBLIP, which failed to produce bounding\nboxes under the zero-shot setting. All models were prompted to answer with a list of bounding\nboxes. In the case of VQDv1, no single prompt worked universally well across all models. A\nfair comparison, therefore, required that we carefully select prompts for each model to achieve the\nbest possible performance. To maximize fairness, we created a small set of prompts, and the most\nperformative prompts for each model were selected. We discuss our prompt selection choices in\nAppendix C.\nVQDv1 Metrics. In [13], average precision using an intersection over union (IoU) of 0.5 was\nused for evaluation; however, that requires scores for each box, which are unavailable for MLLMs.\nTherefore, we compute each model's micro and macro mean $F_1$ scores, recall, and precision. The\npredicted box with the highest IoU above 0.5 is considered a true positive for each ground-truth box,\nwhereas any remaining predicted boxes are false positives. If a query has no ground truth bounding\nboxes, then the $F_1$ score is set to 1 when the model outputs no boxes. Otherwise, it is set to 0. Due to\nthe limited number of questions with four or more bounding boxes, we grouped them.\nResults for VQDv1. As presented in Table 2, all of the models struggle on VQDv1, with the best\nperforming LLaVA-NeXT obtaining only 27.01 in terms of micro $F_1$ score. Fig. 2 shows the recall\nand precision scores across varying numbers of bounding boxes. Models struggle to ground multiple\nboxes, as evidenced by the recall score which decreases with an increase in the number of boxes."}, {"title": "4.2 Fine-Grained VQA Assessment with TDIUC", "content": "TDIUC [15] is a VQA dataset that organizes its questions into 12 distinct types as shown in Fig. 3.\nPerformance is computed for each question type. TDIUC aims to address the shortcomings of previous\nVQA datasets by offering a broader spectrum of question types, and it enables a comprehensive\nanalysis of VQA capabilities for each model.\nTDIUC Metrics. For TDIUC, we use micro-accuracy and macro-accuracy, where micro accuracy\ncorresponds to the average accuracy across the 12 question types. Macro-accuracy corresponds to the\nmean per type metric in the original paper.\nResults for TDIUC. Our main results on TDIUC are detailed in Table 3. LLaVA (13B) and LLaVA-\nNeXT achieve the highest micro accuracies under the asymptotic McNemar test (p = 0.2355).\nGPT-40 is the next best model, showing a statistically significant difference from LLaVA (13B) (p =\n0.0031). BLIP2 obtains the poorest performance across question types, particularly in attribute/color\nrecognition and counting. GPT-4V, GPT-40, BLIP2, and iBLIP excel at absurd questions, whereas\nthe LLaVA family performs worse, likely due to hallucinations. Compared to MuREI [27], the\nbest system trained on TDIUC, MLLMs greatly improve for utility affordance questions, except for\nBLIP2."}, {"title": "4.3 Assessing Counting Ability with TallyQA", "content": "TallyQA [14] tests model's ability to count visual objects accurately. Unlike earlier VQA datasets [7],\nwhere the majority of the counting questions are straightforward and doable with simple object\ndetection (e.g., \u201cHow many giraffes are there?\u201d), TallyQA adds additional challenges by incorporating\nmore complex questions that necessitate detailed reasoning about the visual elements. For instance, a\nquestion such as \u201cHow many giraffes are sitting down?\u201d requires the model to not only detect all the\ngiraffes in the image but also to perform pose estimation to discern which giraffes are seated. This\ntests for enhanced capabilities including complex reasoning and specific visual analysis.\nTallyQA Metrics. In addition to reporting micro accuracy, we group the questions based on their\nanswers (0, 1, 2, 3, or 4+) and calculate the average to determine the macro accuracy.\nResults for TallyQA. The results of the TallyQA analysis are displayed in Table 4. Compared to\nthe simple counting questions, models exhibit large accuracy drops on complex counting questions,\nindicating deficiencies in reasoning capabilities. This is evident even for the top-performing GPT-40,\nwhich experiences declines of 9.8% and 17.6% in terms of micro and macro accuracies, respectively.\nAdditionally, as shown in Fig. 8 and 9, the accuracy of models tend to decrease as the number of"}, {"title": "4.4 Assessing Chart Comprehension with DVQA", "content": "DVQA [16] is a VQA dataset evaluating chart understanding.\nDVQA requires the model to perform grounding extensively.\nWith synthetic charts, the model is required to handle words\nor formulae that are specific for that instance. This contrasts\nwith datasets using natural images, where questions such as\n\"What color is the sky?\" are based on universal concepts, and\neven models that simply exploit dataset biases can obtain\nhigh accuracy by guessing that the sky is either blue or gray.\nIn contrast, the models cannot inflate accuracy by exploiting\nsuch correlations in DVQA since the concepts correspond to\narbitrary values (e.g., the labels can correspond to arbitrary\nbar heights and colors) [16].\nDVQA Metrics. For DVQA, we report micro and macro accuracy. DVQA has 3 question types:\nstructural understanding, data retrieval, and reasoning. They are averaged to compute macro accuracy."}, {"title": "4.5 Analyzing the Strengths and Weaknesses of Today's MLLMS", "content": "Generally, we found that all evaluated models perform poorly in detecting multiple objects in\nreferring expressions. The results from TallyQA align with this observation, showing a performance\ndecline as the counting number increases. Furthermore, introducing absurd questions and allowing\nmodels to answer 'no' presents an additional challenge. The models must be more confident in their\nselections; otherwise, they risk misclassifying answerable questions as absurd, leading to a significant\nperformance drop. The results from DVQA indicate that the challenges posed by natural scene\ndatasets are very different from those in synthetic image datasets. While some models demonstrate\nrobustness to DVQA, others, such as LLaVA, BLIP2, and iBLIP, exhibit a significant performance\ngap when evaluated on natural versus synthetic image datasets."}, {"title": "5 Related Work", "content": "Problems with Widely Used Datasets. With the advent of large foundation models, datasets\nfor training, fine-tuning, and validation have become increasingly important [30]. These datasets\nare pivotal in reflecting a model's performance across different aspects. Notably, many recent\nMLLMs rely on some of the earliest established datasets [7, 11, 12], which, while foundational,\nare increasingly recognized for their constraints and biases. Existing VQA datasets have several\nwell-known issues. Most fail to properly assess grounding capabilities\u2014linking specific parts of\nan image to corresponding textual elements in questions. For example, on some datasets, models\ncan achieve approximately 50% accuracy even when blinded to the image, relying solely on the\nquestions [19]. This indicates that many questions do not depend on grounding capabilities, allowing\nmodels to exploit learned biases rather than visual evidence. Moreover, popular VQA datasets focus\nnarrowly on specific question types, limiting the assessment of models' generalization abilities. Most\nquestions (69.84%) ask about objects in the image, hindering the model's ability to handle abstract\nreasoning, complex visual cues, or nuanced human interactions. Additionally, MLLMs often are not\nevaluated on synthetic datasets, missing opportunities to reveal limitations not observed with natural\nimages. Mainstream referring expression recognition datasets like RefCOCO typically assume each\nreferring expression refers to a single object, oversimplifying the task. In RefCOCOg [18], it was\nshown [20] that randomly permuting words in the referring expressions only reduced performance\nby 5%, and models could achieve 71.2% precision for the top-2 predictions using only the image.\nThis suggests that models exploit dataset quirks and biases rather than utilizing linguistic cues for\ngrounding. The imbalance in target object selection and the simplistic design of referring expressions,\nwith only one associated bounding box, further exacerbate this issue.\nRelated Efforts to Improve MLLM Evaluation. Recent works highlight challenges in evaluating\nMLLMs. In [31], the ARO benchmark was introduced to assess models' understanding of complex\ncompositional elements, and models evaluated on it performed poorly for like \u201cthe grass is eating the\nhorse\u201d versus \u201cthe horse is eating grass.\u201d Similarly, the Winoground datasets [32] require models to\nmatch images with captions that use identical words in different orders to assess their comprehension\nof linguistic composition concerning visual information. In [33], a cycle-consistency framework is\nproposed, evaluating models' ability to understand semantically similar questions. These studies\ncomplement ours and reveal other biases and limitations in MLLMs."}, {"title": "6 Discussion", "content": "Our TallyQA results highlight the necessity of incorporating more complex counting questions to\nreflect models' counting capabilities better. The LLaVA family demonstrates robustness to complex\ncounting questions that demand sophisticated reasoning. In contrast, other models, like BLIP2,\nperform poorly on these complex questions despite performing adequately on easy counting questions\ncompared to LLaVA. Relying solely on easy counting questions can lead to inflated scores, which\ncan be misleading.\nResults from VQDv1 show that traditional single-object referring expressions are more accessible for\nmodels to handle. However, introducing more targets in referring expressions presents a significant\nchallenge, as performance drops when more objects are involved. Examining VQDv1 and TallyQA,\nthey are complementary in evaluating models. In VQDv1, the model must generate one or more\nbounding boxes around objects described in the question, serving as an improved version of counting\nquestions by requiring models to justify their answers. In TallyQA, models perform well when\naccounting for fewer objects, but performance drops significantly as the number of objects increases,\nindicating poor generalization abilities. This aligns with findings from VQDv1, where models\nstruggle with multiple bounding boxes but perform well with a single bounding box. VQDv1 and\nTallyQA offer a comprehensive evaluation of a model's ability to justify its answers and handle\nvarying numbers of objects, highlighting weaknesses in object detection and counting abilities.\nResults from TDIUC provide insight into models' generalization across different question types.\nMost perform poorly on positional reasoning, an essential skill for complex counting questions and\nreferring expressions. TDIUC also includes counting questions, and similar to TallyQA, models\nshow a significant drop in macro accuracy. However, these results also show that Utility/Affordance\nquestions benefit greatly from MLLMs compared to models trained on TDIUC.\nAll models perform poorly on DVQA, indicating that MLLMs struggle with parsing chart information,\nespecially in reasoning and data retrieval questions. LLaVA-NeXT improves significantly over other\nopen-source MLLMs on DVQA, likely due to its training on documents and diagrams. The DVQA\ndataset highlights the challenges presented by synthetic images.\nSocietal Implications. MLLMs are increasingly integrated into various applications, from virtual\nassistants to automated content generation. However, current popular datasets often fail to capture\nmodels' limitations. Our study informs where MLLMs can be safely used and identifies tasks they\nare not yet ready for, emphasizing the importance of rigorous evaluation datasets.\nLimitations. One significant challenge we encountered was effectively prompting the models (see\nAppendices C and F.2). The performance of MLLMs is susceptible to the phrasing and structure of\nprompts, with small changes leading to significant variations in outputs. Crafting prompts that balance\ncomplexity and clarity is difficult, especially given the diversity of tasks and datasets. Additionally, no\nstandardized approach to prompt engineering across different models complicates fair comparisons.\nWe experimented with various formulations to find effective prompts, but our approach may still have\nlimitations. Future work should focus on developing systematic and standardized methods for prompt\nengineering to ensure consistent and fair evaluations."}, {"title": "7 Conclusions", "content": "In this paper, we conducted comprehensive, skill-specific evaluations of MLLMs released in 2023\u2013\n2024. Our analysis revealed several weaknesses that are not apparent when using mainstream datasets\nalone. To enhance accessibility for researchers and facilitate benchmark comparisons, we have\nintegrated these datasets into a fork of the widely used LAVIS framework [34], and we will work\nwith the LAVIS team to merge our version into the main trunk or release it as a separate entity, if\nnecessary."}, {"title": "A Computational Resources", "content": "The evaluations of open-source MLLMs were conducted on a single A100 GPU with 40GB of RAM,\nwhich required approximately 200 hours on our university-wide computing infrastructure. To evaluate\nGPT-4V/GPT-40, which are closed-source, we used the paid ChatGPT API provided by OpenAI and\nspent $922 for GPT-4V and $451 for GPT-40, including runs to tune prompts."}, {"title": "B Additional Dataset Details", "content": "In this section, we present additional dataset details."}, {"title": "B.1 TallyQA", "content": "The counting questions in TallyQA are classified into complex and simple counting questions [14].\nSimple counting questions were imported from existing datasets like VQA2 and Visual Genome.\nComplex questions were collected using Amazon Mechanical Turk (AMT) to gather 19,500 complex\nquestions for 17,545 unique images. The images were sourced from both COCO and Visual Genome\nto ensure variety. The testing set of TallyQA contains 38,589 questions, which is a reasonable size.\nTherefore, we evaluated models on the entire original test set. The distribution of unique answers is\ngiven in Table 6. TallyQA is provided under the terms of the Apache License Version 2.0, January\n2004: http://www.apache.org/licenses/"}, {"title": "B.2 VQDv1", "content": "VQDv1 [13] was created synthetically using annotations from Visual Genome, COCO, and COCO\nPanoptic. This synthetic generation approach helps combat certain biases. The queries are generated\nusing multiple templates for each type, allowing for diverse queries. The annotations used to generate\nthese questions are derived from a combination of COCO's object annotations and Visual Genome's\nattribute and relationship information.\nFor VQDv1, almost 90% of the queries have less than two ground truth bounding boxes. In our\nsubset, we retained all queries with more than one ground truth bounding box, and we sampled 10%\nof the queries with zero or one ground truth bounding box. Table 7 provides the distribution of ground\ntruth boxes across queries. The VQDv1 dataset is provided under the terms of the Creative Commons"}, {"title": "B.3 DVQA", "content": "The DVQA dataset was created by synthetically generating bar charts to test multiple aspects of\nbar chart understanding. This automatic generation process allows precise control over the visual\nelements' positions and appearances, and provides access to meta-data about the elements in the\nimage, which is not available with real data [16].\nThe original version of DVQA had two test sets: Test-Familiar and Test-Novel. The critical difference\nbetween these two sets is that every bar chart in Test-Familiar has labels in DVQA's training\nset, whereas Test-Novel does not. Given that we are conducting zero-shot evaluations, these two\nsets can be treated equivalently. Therefore, we sample the same number of questions from both.\nTable 8 shows the question distributions of our subset version of DVQA. The DVQA dataset is\nprovided under the terms of the Creative Commons Attribution 4.0 International (CC BY 4.0):\nhttps://creativecommons.org/licenses/by/4.0/legalcode"}, {"title": "B.4 TDIUC", "content": "The TDIUC dataset was created by incorporating questions from three sources: existing datasets, ques-\ntions generated based on image annotations, and human annotators. Questions were imported from\nCOCO-VQA and Visual Genome datasets, with templates and regular expressions used to classify and"}, {"title": "C Prompt Engineering", "content": "To make the model performance comparison as fair as possible, we endeavored to keep the prompts\nconsistent across different models. However, this was challenging due to variations in the models'\nability to process the prompts. For example, BLIP2 and iBLIP failed when prompted to answer using\na template such as \"My answer is <answer>.\" Inspired by Liu et al. [6], for TDIUC, DVQA, and\nTallyQA, we prompt the models to answer as concisely as possible instead of asking them to generate\nentire sentences. These prompts are given in Fig. 6."}, {"title": "D Model Details", "content": "In this paper, all the open source MLLMs are loaded directly from HuggingFace, the detail models\nare below:"}, {"title": "E Additional Evaluation Details", "content": "Root Mean Squared Error (RMSE) Computation. For TallyQA, besides Micro and Macro\nAccuracy, we also compute RMSE. However, we observed that due to the unpredictability of the\nMLLMs, the models occasionally output unreasonably large numbers as their predicted object\ncounts. For instance, LLaVA-NeXT predicts an unreasonably large object count of 150 for one\nof the questions. Such outliers significantly inflate the models' overall average RMSE across all\nquestions. As shown in the distribution of TallyQA questions, all counting numbers are between 0\nand 15. Therefore, we apply a simple cutoff technique: an upper bound of 15 and a lower bound of 0\nis applied to all predicted counts. This adjustment ensures that the RMSE remains meaningful and\nuseful for analysis.\nMatch Answer with Ground Truth.\nFor TallyQA, the model is tasked with generating object counts. If the model correspondingly\ngenerates a number enclosed within a string, such as \"2\", we directly convert it to int type by type\nconversion. For the case where the model generates a word, we map the word to its corresponding\nnumber using the mappings shown in table 7. Occasionally, the model generates answers that, while\nnot numerical, still make sense. For example, the model might generate 'none' or 'no,' which we\ninterpret as zero. We manually account for these cases and add additional mappings accordingly.\nWhile we acknowledge that even with these steps, we may still miss some unpredictable answers\nfrom the models, such as when the model responds with 'a few,' which is completely uninterpretable,\nwe map these to None."}, {"title": "F Additional Results", "content": "For TallyQA, we found that the performance of most models decreases as the correct number to\noutput increases, as shown in Figs. 8 and 9. Across counts, models perform much better at answering\nsimple questions than complex questions."}, {"title": "F.2 VQDv1", "content": "Alternative Prompts. All models performed poorly on VQDv1. As mentioned earlier, it was\nchallenging to identify the best prompt for each model. We hypothesized that given the verbosity of\nGPT-40, it would benefit from being allowed to provide more extended responses where it reasons\n'aloud.' However, this performed worse than the prompts used in our main results. In Table 13, we\nprovide alternative prompts that we tried, where the results are given in Table 14."}]}