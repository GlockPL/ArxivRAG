{"title": "Efficient Annotator Reliability Assessment and Sample Weighting for Knowledge-Based Misinformation Detection on Social Media", "authors": ["Owen Cook", "Charlie Grimshaw", "Ben Wu", "Sophie Dillon", "Jack Hicks", "Luke Jones", "Thomas Smith", "Matyas Szert", "Xingyi Song"], "abstract": "Misinformation spreads rapidly on social media, confusing the truth and targetting potentially vulnerable people. To effectively mitigate the negative impact of misinformation, it must first be accurately detected before applying a mitigation strategy, such as X's community notes, which is currently a manual process. This study takes a knowledge-based approach to misinformation detection, modelling the problem similarly to one of natural language inference. The EffiARA annotation framework is introduced, aiming to utilise inter- and intra-annotator agreement to understand the reliability of each annotator and influence the training of large language models for classification based on annotator reliability. In assessing the EffiARA annotation framework, the Russo-Ukrainian Conflict Knowledge-Based Misinformation Classification Dataset (RUC-MCD) was developed and made publicly available. This study finds that sample weighting using annotator reliability performs the best, utilising both inter- and intra-annotator agreement and soft-label training. The highest classification performance achieved using Llama-3.2-1B was a macro-F1 of 0.757 and 0.740 using TwHIN-BERT-large.", "sections": [{"title": "1 Introduction", "content": "Knowledge-based misinformation detection (or fact-checking) is the task of identifying misinformation based on knowledge. The knowledge could be either verified misinformation (Jiang et al., 2021) or true information (Thorne et al., 2018; Wadden et al., 2020).\nDatasets (Jiang et al., 2021; Mart\u00edn et al., 2022; Thorne et al., 2018) have been introduced to facilitate knowledge-based misinformation detection model training, and they are often annotated similarly to Natural Language Inference (NLI) tasks. When the knowledge is verified misinformation, claims can be categorised as misinformation if they are entailed by the knowledge, debunked if they contradict the knowledge, or labelled as other if the claim is neutral to the provided knowledge. However, these datasets often exhibit high annotator disagreements. This high disagreement is mainly due to the high complexity of the task, which introduces subjectivity during annotation.\nIn many cases, the disagreements are solved by a majority vote, introducing a \"gold\" label for each sample. However, such disagreements also contain valuable information and previous studies (Branson et al., 2010; Wu et al., 2023; Busso et al., 2008; Fayek et al., 2016; Zhang et al., 2019) suggest that incorporating human disagreement or uncertainty could lead to better model performance.\nThat said, these approaches often assume that annotators are reliable and that disagreements are not due to annotator mistakes. This assumption does not always hold true, especially when non-expert annotators are involved. Our experiments also demonstrate that without accounting for annotator errors, incorporating human uncertainty can actually degrade model performance.\nIn this work, we introduce the Efficient Annotator Reliability Assessment (EffiARA) Framework, designed to evaluate the reliability of individual annotators relative to the group average. Annotator reliability serves as a proxy for estimating the error rate of each annotator and is incorporated into a weighted cross-entropy loss function. This approach allows us to adjust the importance of samples based on the annotator's reliability, increasing or decreasing the weight of samples accordingly, improving model training.\nTo apply this annotation framework, we also introduce the Russo-Ukrainian Conflict Knowledge-Based Misinformation Classification Dataset (RUC-MCD), containing debunked misinformation from EUvsDisinfo 1 and social media posts from X 2 with manually annotated misinformation labels.\nIn summary, this study provides the following contributions:\n\u2022 A novel knowledge-based misinformation detection dataset on the topic of the Russo-Ukrainian conflict.\n\u2022 A novel annotation framework for assessing annotator reliability and maximising the number of samples per annotation.\n\u2022 A novel approach to weighting cross-entropy loss based on annotator reliability derived from inter- and intra-annotator agreement.\n\u2022 Baseline results on the dataset, using hard- and soft-label learning, confidence calibration (Wu et al., 2023), and annotator reliability weighted learning with TwHin-BERT-large (Zhang et al., 2023) and Llama-3.2-1B (Dubey et al., 2024)."}, {"title": "2 Related Works", "content": "Knowledge-based misinformation detection involves classifying content as misinformation based on knowledge from a trusted source. There are two main approaches to utilising such knowledge. The first approach storing knowledge in a knowledge graph (Pan et al., 2018) and applying graph neural networks (GNNs) to detect misinformation. The second approach keeps the knowledge in natural language form and uses information retrieval techniques to find relevant knowledge (Jiang et al., 2021; Mart\u00edn et al., 2022; Thorne et al., 2018).\nOnce the relevant knowledge is retrieved, a classifier is applied to determine whether the claim aligns with the retrieved knowledge or contradicts it (Jiang et al., 2021; Mart\u00edn et al., 2022; Hossain et al., 2020; Thorne et al., 2018), thereby classifying the claim as misinformation or not.\nSeveral datasets have been introduced to support the classifier training and verification. Emergent Stance Classification (Ferreira and Vlachos, 2016) was one of the earliest datasets, where claims were labelled based on the stance of news articles as \"for\" (supporting the claim), \u201cagainst\u201d (refuting the claim), or \"observing\u201d (neutral, without a clear stance). The Fake News Challenge 3 extended this idea by introducing an additional label, \u201cunrelated\", for articles that did not pertain to the claim.\nThe PHEME dataset (Zubiaga et al., 2016; Derczynski et al., 2017) focusses on social media rumours, classifying claims as \u201cSupport\u201d, \u201cDeny\", \u201cQuery\u201d, or \u201cComment\u201d based on the provided content.\nThe FEVER dataset (Thorne et al., 2018) branded the task as a fact-checking task and adapted the Natural Language Inference (NLI) label to classify claims are \"Support\", \"Refuted\", or \"Not enough info\u201d based on the knowledge provided.\nFollowing that, domain-specific datasets have also emerged, such as SCIFACT (Wadden et al., 2020), which focuses on verifying scientific claims; CovidLies (Hossain et al., 2020) and JIANG COVID (Jiang et al., 2021) focus on the COVID domain.\nThese datasets often employ multiple annotators to measure inter-annotator agreement; most datasets report relatively high levels of human disagreement. According to the Kappa reported in previous studies, the metric varies from 0.75 (SCI-FACT (Wadden et al., 2020) Cohen's Kappa) to 0.68 ((Thorne et al., 2018) Fleiss Kappa). The SCIFACT dataset, focussing on the scientific domain, achieved higher agreement, partly due to the use of domain expert annotators, whereas datasets covering broader topics often suffer from lower agreement due to the complexity and subjectivity\""}, {"title": "3 Problem Definition", "content": "Dataset Creation\nCreate a dataset D containing a set of knowledge-based claims C and social media post texts T paired by relevance, forming the samples S belonging to D. Each sample, Si, will be single-, double-, or re-annotated (annotated by one annotator once, annotated by two different annotators, or annotated by the same annotator twice), allowing the assessment of the EffiARA (Efficient Annotator Reliability Assessment) annotation framework and the creation of a high-agreement subset of D to be treated as gold-standard. Each label Li for each sample Si must be a soft label (which may be converted to a hard label), allowing for the comparison between soft- and hard-label training.\nAssessing Annotator Reliability\nThe reliability of an annotator Af may be determined through some combination of each annotator's agreement with others and themselves (inter- and intra-annotator agreement). An annotation framework should be used, such that AR can be calculated and the number of samples n is maximised.\nClassifying Misinformation\nOur misinformation classification is to be modelled in a pairwise fashion similar to natural language inference, with post text T\u2081 being misinformation (entailing), a debunk (contradicting), or other (neutral) relative to a knowledge-based claim Ci. A model M will train to predict the label Li, based on only Ci and Ti."}, {"title": "4 EffiARA Annotation Framework", "content": "The EffiARA (Efficient Annotator Reliability Assessment) annotation framework aims to maximise the number of samples per annotation while maintaining the ability to assess annotator reliability through inter- and intra-annotator agreement. This annotation framework is well suited for cases where there is limited access to annotators or annotations must be completed quickly. It may also be extended to much larger groups of annotators."}, {"title": "Unique Samples & Sample Distribution", "content": "Algorithm 1 describes how the number of samples k can be calculated and these samples are distributed to 4n annotation tasks. The ratio of time taken to produce a unique single annotation (including re-annotations) to a unique double annotation for each user is (1 + r)(d \u2013 1):2d, reflected in the equation for k. If a given number of samples is desired, the equation may be rearranged to find the desired number of annotators n, or the total time each annotator is expected to spend annotating t. Each parameter may be adjusted and the number of links per annotator is always 4."}, {"title": "Annotator Reliability Calculations", "content": "Referring to Figure 2 may help to visualise the annotator reliability calculation process. Let {A0, ..., An} be the set of nodes representing all annotators, where n is the number of annotators; let Ax \u2229 Ay be an edge between two annotator nodes Ar and Ay, representing the set of samples that have been double-annotated by this pair of annotators; let a be a function calculating the agreement between the set of double annotations from Ax and Ay.\nA simple method for calculating each annotator's inter-annotator agreement factor, the function e(Ai), is the average of the pairwise agreement values for each edge incident to the node.\n$e(A_i) = \\frac{1}{|Links(A_i)|} \\sum_{A_j \\in Links(A_i)} \\alpha(A_i, A_j)$\nwhere the function Links gathers all nodes linked to A\u017c by an edge.\nThis function e(Ai) can be expanded to utilise the annotator's reliability in the inter-annotator agreement calculations. For a weighted average, the function can be written as:\n$e(A_i) = \\frac{1}{|Links(A_i)|} \\sum_{A_j \\in Links(A_i)} A_j^R\\alpha(A_i, A_j)$\nwhere Affrepresents an annotator's reliability.\nThe aim of weighting the function is to lower the impact of \"bad annotators\u201d with lower than average reliability scores and raise the impact of \"good annotators\" with higher than average reliability scores in the inter-annotator agreement calculation. It is worth noting that for this to be the case, the n reliability scores must be normalised around one; this can be achieved by dividing each reliability score by the average of all reliability scores, assuming no negative reliability scores. While this weighted average method may offer some improvement in calculating inter-annotator agreement, the method is suboptimal as disagreement with bad annotators is not sufficiently rewarded and disagreement with good annotators is not sufficiently punished. While out of the scope of this study, this problem may prompt improvements for future work.\nIntra-annotator agreement is defined as the pairwise agreement with an annotator's own re-annotations: a(Ai, A\u2081), where A\u2081 denotes the ith annotator's re-annotations.\nWith methods for calculating both inter- and intra-annotator agreement, the function to calculate annotator reliability can be introduced. This function r(Ai, a) can be written as:\nr(Ai, 1) = \u03bb(\u03b1(A\u00bf \u2229 A\u2081)) + (1 \u2212 1)(e(Ai))\nwhere 0 \u2264 x \u2264 1.\nThis reliability function can either be called once, or it can be called iteratively until reliability values converge. For the first call, each annotator's reliability is initialised to 1.0. After each iterative step, reliability values must be normalised to have a mean value of 1.0 for use in inter-annotator agreement function e.\nA Python implementation of this framework and the rest of the code used in this study can be found github.com/MiniEggz/ruc-misinfo."}, {"title": "5 RUC-MCD Dataset Creation", "content": "5.1 Sourcing Data\nThe knowledge, or known misinformation connected to the Russo-Ukrainian conflict (also known as claims), was sourced from EUvsDisinfo 4. Each claim about the Russo-Ukrainian conflict has been fact-checked and labelled as disinformation. The social media posts were drawn from a large collection of X posts collected by our institution, hosted on an ElasticSearch (Elasticsearch, 2018) endpoint, enabling simple information retrieval.\n5.2 Claim-Post Pairs\nWith a collection of evidence and X posts, they were paired in a manner that closely follows the methodology of Jiang et al. (2021). Evidence was used as the search term in an information retrieval task to find the most relevant X posts. For each claim, the 30 most relevant tweets were returned by ElasticSearch; these posts were then re-ranked using ms-marco-TinyBERT-L-2 (HuggingFace, 2024), with the 10 most relevant posts paired with the claim added to the dataset as 10 separate samples.\nDue to the large number of fact-checked claims available, a random sample of 350 was chosen from the claims originally written in English. After claim-post pairings, there were 3,500 samples available.\n5.3 Annotation\nThe annotations were completed based on annotation guidelines agreed to by all annotators, 6 volunteering final-year integrated Master's students in Computer Science studying in the UK. The guidelines ask each annotator to read the evidence and post carefully and choose one of the following three labels as their primary label: \u201cmisinfo\u201d, \u201cdebunk", "other": "The annotator is then asked to provide a score from 1-5 indicating their confidence in the primary label, as in Wu et al. (2023). If the confidence score is 3 or less, the annotator is asked to provide a secondary label that they believe could alternatively be assigned to the sample.\nOnce guidelines had been outlined and agreed upon, the samples were distributed using the EffiARA annotation framework. It was calculated that with n = 6, t = 10 (hours), p = 60 (annotations per hour), d = 1/3, and r = 1/2 that\nk = (2(1/3) +3/2 * 2/3)-1 * 60 * 10 * 6 = 2160 unique samples. Once distributed, each double-annotation project contained 2nk = 80 samples, each single-annotation project contained (1-d)k/n = 240 samples, and each re-annotation project contained rA = 120 samples. Initially, single- and double-annotations were completed; after completing these annotations, annotators did not annotate for two weeks before completing their re-annotations, allowing for the assessment of intra-annotator reliability. Figure 2 shows the annotation graph containing agreement metrics (Krippendorff's Alpha (Gwet, 2015)) from the three-class annotation. The average Krippendorff's Alpha across each double-annotation task is 0.581."}, {"title": "6 Experiments", "content": "In this study, the EffiARA reliability scores with weighted cross-entropy loss are compared with baseline classification results obtained by the same pre-trained language model and confidence calibration (Wu et al., 2023), using one encoder model and one decoder model with both soft and hard-label training; hard-label training was not possible with the confidence calibration method as it relies on the use of soft labels. Five-fold cross validation was conducted for each experimental setting; for each fold, 4/5 of the high-agreement samples and all other samples (containing low-agreement double-annotated samples and all single-annotated samples) were used as the training set, and the final 1/5 of the high-agreement samples used as the test set as in Wu et al. (Wu et al., 2023).\nTwHIN-BERT-large (550M parameters) (Zhang et al., 2023), chosen for its pre-training on X/Twitter posts, and Llama-3.2-1B (1.23B parameters) (Dubey et al., 2024), from the latest group of Llama models, are fine-tuned and measured for classification performance on RUC-MCD. In our fine-tuning process, we use the CLS (classification) token (for TwHIN-BERT-large) and the last token (for Llama-3.2-1B) output from the final hidden layer of the models as representations of the input. These token embeddings are then passed through an additional feedforward neural network for classification. These experiments are performed using the PyTorch (Paszke et al., 2019) and HuggingFace (Wolf et al., 2019) libraries. For both the BERT model and Llama model, training parameters are largely the same, following that of Wu et al. (2023). Fine-tuning was performed for 20 epochs, a batch size of 16 for BERT and 8 Llama (due to memory capacity), and a linear decaying learning rate of 2e-5, with 5 warm-up epochs. Without 5 warm-up epochs, the BERT models suffered from early convergence, reaching a suboptimal local minimum. The AdamW optimiser and cross-entropy loss, or weighted cross-entropy loss where EffiARA reliability scores were used, were chosen. For evaluation, the macro-F1 and expected calibration error (ECE) were used as key metrics. In all experiments, the evidence was concatenated with the X post and tokenised. In total, experiments took roughly 270 GPU hours using an Nvidia Tesla A100.\nDue to label imbalance, we combine the labels \"other\" and \"debunk\", leaving only \"misinfo\" and \"other\" in our experiment. This study focusses on the detection of misinformation; the fine-grained classification of other classes is not essential.", "6.1 Label weight and Soft Label Generation\nThe annotator reliability metric aims to capture how \"good\" an annotator is, using their inter- and intra-annotator agreement scores, calculated as described in Section 4. These reliability measures were then used in the weighted cross-entropy loss function, increasing the importance of correctly classifying \"better\u201d annotators in the model training process. As well as weighting the loss function, they are also used to aggregate the soft labels generated in double-annotated samples using a weighted average of the two annotators' reliability scores. This study uses Krippendorff's alpha (Krippendorff, 1970; Hayes and Krippendorff, 2007; Castro, 2017) as the agreement metric in all inter- and intra-annotator agreement calculations.\nThree settings were used to calculate annotator reliability: inter-annotator agreement only, intra-annotator agreement, and 50:50 weighting. The reliability scores can be seen in Table 3.\nTo generate the soft label, the confidence in the primary label is converted to a value between 0 and 1. This conversion uses Equation 1, where n is the number of classes, C is the confidence in the primary label, and MaxC is the maximum confidence value. In this study, these values are n = 2, MaxC = 5, and C \u2208 {1, 2, 3, 4, 5}.\n$\\frac{1}{n} + \\frac{n-1}{n} \\frac{C-1}{MaxC - 1}$ (1)\n    },\n    {": "itle", "6.2 Confidence Calibration": "content\": \"To assess the performance of the EffiARA annotator reliability calculations, they are compared to the Bayesian confidence calibration method introduced by Wu et al. (2023). After confidence calibration, soft labels are generated as previously described."}, {"title": "7 Results & Discussion", "content": "Table 2 presents the classification performance and model calibration results achieved in this study.\nCross-entropy loss weighted by EffiARA reliability scores offers significant advantages. Treating vanilla hard-label training as the baseline, annotator-reliability-based sample weighting offers improvements of up to 0.041 and 0.018 in macro-F1 score for BERT and Llama respectively, showing the effectiveness of the method on both encoder and decoder models. For both Llama and BERT, the highest performing setting was annotator-reliability-based sample weighting utilising both inter- and intra-annotator agreement to establish annotator reliability. This method was more effective when applied to soft-label learning for both BERT and Llama.\nEffiARA performs well with low-to-moderate agreement data. The average inter-annotator agreement in double-annotated samples within RUC-MCD with two classes is 0.592, which is low-to-moderate agreement (Krippendorff, 2019). With crowdsourcing often containing disagreement (Dumitrache et al., 2018), it could make EffiARA a viable approach to mitigate against poor annotators. Future work may investigate the performance of EffiARA in expert annotation scenarios and crowdsourcing, where the number of annotators is much higher.\nConfidence calibration is ineffective when applied to pairwise annotations. Bayesian confidence calibration was consistently the worst performing experimental setting, in both classification performance and model calibration. This was expected as confidence calibration relies on higher annotator overlap. EffiARA's pairwise annotation method is not compatible with confidence calibration, justifying the requirement for another approach, such as annotator-reliability-based sample weighting.\nSoft- and hard-label training results vary. While the highest performing models are trained on soft labels using the annotator-reliability-based sample weighting, hard-label training performs better than soft-label training for both BERT and Llama baselines. Expected calibration error is significantly improved through the use of soft labels though, indicating that the model's confidence in its output is more accurate with soft-label training."}, {"title": "8 Conclusion", "content": "This study has introduced the EffiARA annotation framework, shown to increase knowledge-based classification performance over baseline hard- and soft-label training for a BERT and Llama model through annotator-reliability-based sample weighting. The combination of inter- and intra-annotator agreement was shown to be the highest performing model with a macro-F1 of 0.756 for Llama-3.2-1B and 0.740 for TwHIN-BERT-large, outperforming baselines by 0.018 and 0.041 respectively. While only 6 annotators were used in this study, this framework can be expanded to use any number of annotators, meaning it could be applied to crowdsourcing; further studies may investigate this application of the EffiARA annotation framework and annotator-reliability-based sample weighting. The newly created dataset, RUC-MCD, has also been made publicly available."}, {"title": "9 Limitations", "content": "This work relies on fact-checking organisations to always be a reliable source of truth. Placing so much trust in one organisation may be dangerous in some cases but verifying the work of fact-checking organisations is beyond the scope of this study. For use in industry, however, it is important that all the evidence itself is trustworthy. For any organisation that may want to employ misinformation detection technologies on their social media platform, it is likely suitable that the evidence base is maintained.\nThe detection of misinformation can often be reliant on temporal information. For example, if somebody claimed that \"Person X did Y\" before that person did action Y, it would be misinformation. The same statement made a month later, after Person X did Y, would no longer be misinformation. Including this temporal dimension in the misinformation classification task was beyond the scope of this study but it is an important edge case to consider, should this technology be used to automatically detect misinformation on social media.\nThe application of misinformation detection technology must be carefully considered as the misidentification of it in public settings may sway opinions and narratives, causing unintended consequences."}, {"title": "10 Ethics", "content": "This study employed six annotators who all signed consent forms, agreeing to their annotations being used for this research and acknowledging and accepting the risk of reading offensive or upsetting social media posts. Any X post data, potentially containing personal information, has been omitted from the publicly available dataset; only the post ID is stored, enabling users to only use posts that remain public. This study was approved by the ethics board of our institution."}]}