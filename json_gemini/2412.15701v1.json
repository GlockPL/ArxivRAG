{"title": "Collaborative Gym: A Framework for Enabling and Evaluating Human-Agent Collaboration", "authors": ["Yijia Shao", "Vinay Samuel", "Yucheng Jiang", "John Yang", "Diyi Yang"], "abstract": "Recent advancements in language models (LMs) have sparked growing interest in developing LM agents. While fully autonomous agents could excel in many scenarios, numerous use cases inherently require them to collaborate with humans due to humans' latent preferences, domain expertise, or need for control. To facilitate the study of human-agent collaboration, we present Collaborative Gym (Co-Gym), a general framework enabling asynchronous, tripartite interaction among agents, humans, and task environments. We instantiate Co-Gym with three representative tasks in both simulated and real-world conditions, and propose an evaluation framework that assesses both the collaboration outcomes and processes. Our findings reveal that collaborative agents consistently outperform their fully autonomous counterparts in task performance within those delivered cases, achieving win rates of 86% in Travel Planning, 74% in Tabular Analysis, and 66% in Related Work when evaluated by real users. However, our study also highlights significant challenges in developing collaborative agents, requiring advancements in core aspects of intelligence-communication capabilities, situational awareness, and balancing autonomy and human control.", "sections": [{"title": "1. Introduction", "content": "Artificial Intelligence has long aspired to develop machines that act as teammates rather than as mere tools (Nass et al., 1996; Russell & Norvig, 2016; Seeber et al., 2020). While current language models (LMs) demonstrate impressive performance, their capabilities are primarily centered around instruction following (Ouyang et al., 2022). To enhance LM autonomy, a recent trend focuses on developing fully autonomous LM agents capable of automating a series of tasks. These tasks range from everyday activities like web navigation (Deng et al., 2023; Zhou et al., 2024a) and basic assistance (Drouin et al., 2024b; Shao et al., 2024b) to expert-level tasks such as coding (Jimenez et al., 2024; Yang et al., 2024) and scientific research (Huang et al., 2024; Majumder et al., 2024).\nWhile fully autonomous agents are valuable in many scenarios, numerous use cases inherently require human involvement due to latent preferences, domain expertise, or the need for control on critical decisions, even when LM agents can handle much of the workload. For example, in their foundational study of AI agents, Russell & Norvig (2016) emphasized that a medical diagnosis agent must navigate an environment involving patients, hospitals, and staff, while an English tutor agent must interact with students and testing agencies. Beyond practical necessity, effective human-agent collaboration has the potential to achieve greater task performance compared to either the agent or the human working independently given their complementary expertise. Unfortunately, the human role remains largely overlooked in current LM agent research. Despite the growing enthusiasm for deploying LM agents in various scenarios where humans are stakeholders, two fundamental questions remain unclear: Is human-agent collaboration beneficial and in what ways? How can we design LM agents that can collaborate with humans effectively?\nTo address these gaps, we introduce Collaborative Gym (Co-Gym), the first framework to enable tripartite interaction among agents, humans, and task environments, along with a comprehensive set of metrics for evaluating human-agent collaboration. Co-Gym is designed with three key principles. First, collaboration-driven environment design. Co-Gym imposes no constraints on LM agent implementation but instead defines an environment interface that allows humans and agents to act in a shared workspace. Second, asynchronous interaction, not turn-taking. To mirror natural human collaboration, Co-Gym enables asynchronous interaction rather than enforced turn structures through encapsulating two collaboration acts and a notification protocol for real-time change monitoring. Third, outcome and process. Co-Gym captures both task outcomes and detailed collaboration processes. It introduces Collaboration Score to jointly assess task delivery and performance, and audits the collaboration process through metrics like initiative-taking, controlled autonomy, and human satisfaction.\nWhile Co-Gym is a general framework, we start with three representative tasks: travel planning, writing related work sections, and tabular analysis, as the first set of benchmark problems to evaluate the collaboration capabilities of current LM agents. For each task, Co-Gym supports experiments under both simulated and real conditions. In the simulated condition (i.e., Co-Gym (Simulated)) where the human is simulated by an LM and task instances come from pre-collected datasets, our experiments reveal patterns akin to human collaboration: instances of collaborative inertia (Huxham, 2003), where human-agent teams fail to achieve their objectives due to poor communication or coordination, and collaborative advantage, where teams produce higher-quality outcomes compared to fully autonomous agents. To further explore collaborative agents in real-world conditions, we also present Co-Gym (Real), where real human participants interact with LM agents via a web interface featuring a chat panel and shared workspace. Our results indicate that human-agent collaboration can be beneficial, with collaborative agents achieving win rates of 86% in Travel Planning and 74% in Tabular Analysis compared to fully autonomous agents when evaluated by real users. Additionally, we observe emergent collaborative dynamics in human-agent collaboration as well as common failure modes exhibited by LM agents in these collaboration processes, particularly in areas such as communication, situational awareness, and planning. These findings underscore the need for advancements in both the underlying LMs and the agent scaffolding (e.g., memory, tooling) to enable more effective and satisfying collaboration."}, {"title": "2. Related Work", "content": "Environment Interface for LM Agents While there is an increasing interest in developing LM agents, it is equally critical to establish environments where these agents can interact with the environment itself, one another, or humans. While much work on LM agents formulates task environment as Partially Observable Markov Decision Processes (POMDP) (Drouin et al., 2024a; Zhou et al., 2024a; Rawles et al., 2023), using abstractions like OpenAI Gym (Brockman et al., 2016), it is challenging to extend this setup to human-agent or multi-agent collaboration. These challenges stem from the fact that multiple parties can influence the environment and it requires mechanisms for effective coordination among them.\nIn reinforcement learning literature, multi-agent interaction is often formulated as Markov games, where agents interact with a shared environment and simultaneously receive rewards and observations (Littman, 1994). This can be extended to Partially Observable Stochastic Games (POSG) in frameworks like PettingZoo (Terry et al., 2021), where agents have different observations at the same time. In LM agent research, Generative Agents (Park et al., 2023) instantiate an interactive simulacra using a similar approach by having a server parse actions from all agents at each times-\ntamp and update each agent with the environment status within their visual range. While effective in multi-agent contexts, incorporating real humans poses additional challenges due to the inherently asynchronous nature of human-agent collaboration. Requiring both humans and agents to act at every step is often impractical (except in specific cases like certain board games), and asynchronous interaction itself presents significant difficulties (Irlitti et al., 2016). To address this, in this work, we introduce a simple environment abstraction and a notification protocol to support asynchronous interactions.\nHuman-AI collaboration Human-AI collaboration has been widely studied in robotics (Bauer et al., 2008; Ajoudani et al., 2018) and human-computer interaction (HCI) (Khadpe et al., 2020; Wang et al., 2020; Zhang et al., 2021), yet it remains underexplored in the context of current LM agent research. Human-AI collaboration research emphasizes improving interaction dynamics between humans and AI. Besides optimizing for performance, human-AI collaboration has the potential to boost human well-being (Gao et al., 2024). Notably, Collaborative STORM, a system that allows human-AI collaboration for complex information seeking, enhances human learning experience by enabling serendipitous discovery and dynamic mind mapping (Jiang et al., 2024). Additionally, the integration of LMs in collaborative environments, such as manufacturing, highlights the advantages of natural language communication in reducing psychological stress and improving task management (Lim et al., 2024). The A2C framework offers modular strategies for human-AI teams, facilitating flexible collaboration in dynamic settings like cybersecurity (Tariq et al., 2024). Such strategies are critical as studies show that effectively incorporating human beliefs into AI design is crucial for improving collaborative outcomes (Yu et al., 2024)."}, {"title": "3. Collaborative Gym", "content": "We present Collaborative Gym (Co-Gym), a framework that enables tripartite interaction between agents, humans, and task environments, to facilitate the study of human-agent collaboration."}, {"title": "3.1. Design Principles", "content": "The design of Co-Gym is guided by three core principles:\nCollaboration-driven environment design. (\u00a73.2) Inspired by OpenAI Gym (Brockman et al., 2016), Co-Gym imposes minimal constraints on the implementation of LM agents. Agents are only required to process notifications and generate actions that comply with the Co-Gym protocol, allowing implementation with any agent framework. The environment interface is designed to accommodate interactions among multiple participants and supports both public and private components within the observation space. Public components are visible to all parties, while private components are accessible only to their respective owners.\nAsynchronous interaction, not turn-taking. (\u00a73.3) Co-Gym emphasizes asynchronous interaction rather than rigid turn-taking paradigms typical of chatbots and turn-based games, allowing both humans and LM agents to take initiative flexibly and work in parallel. To support this dynamic interaction, Co-Gym extends the task-specific action space with two additional actions to facilitate collaboration and introduces a notification protocol that allows agents to monitor changes in real time.\nOutcome and process. (\u00a73.4) Achieving task completion is not the only goal of effective human-agent collaboration. Collaborative processes are equally essential since the ultimate aim is to optimize collective intelligence while preserving human control. Co-Gym provides an evaluation framework that considers both dimensions, assessing collaborative agents based on the quality of outcomes and the collaborative process."}, {"title": "3.2. Task Environment", "content": "Within Co-Gym, we define each task as a Partially Observable Markov Decision Processes (POMDP) $(S, A, T, R, U, O)$ with state space $S$, action space $A$, transition function $T: S \\times A \\rightarrow S$, reward function $R: S \\times A \\rightarrow R$, instruction space $U$, and an observation space $O$. Adding a new task environment (CoEnv) into Co-Gym requires specifying the tools available in $S$, the corresponding $A, O$, and $T$, and an initial task description as the instruction. In addition to the initial query, $U$ also includes instructions that emerge during the collaboration process. By default, the reward function $R$ assigns a reward of 0 for successfully executed actions and -1 otherwise, unless explicitly defined.\nTo support actions from multiple participants within a shared task environment, CoEnv introduces a role parameter in its step function, which allows the environment to be updated based on the role-specific action. Moreover, even within a shared environment, the observation space can include both public and private components, analogous to human teams where some components (e.g., whiteboards) are shared while others (e.g., personal notebooks) remain private. CoEnv allows such flexibility by supporting differentiated observations for different parties using a private flag to distinguish between actions affecting shared components versus private components of the action taker. Thus, the resulting CoEnv abstraction is:\n`obs, reward, done, private = env.step (role, action)`"}, {"title": "3.3. Asynchronous Interaction", "content": "Collaboration Acts Traditional multi-agent frameworks like Markov Games and PettingZoo assume synchronous action patterns, where participants either act simultaneously at each timestamp or take strict turns. However, this rigid synchronization poorly reflects natural human collaboration, where parties typically coordinate only when necessary and can execute multiple actions without waiting for others' responses. To mirror natural human collaboration, where coordination occurs through effective communication, on top of the task-specific action space, Co-Gym supports two meta actions: `SendTeammateMessage` for message exchange between teammates, `WaitTeammateContinue` that serves as a keep-alive signal.\nNotification Protocol While humans can continuously monitor their environment, agents require programmatic notification of changes. Co-Gym adopts the following notification protocol that operates on four event types: (1) shared observation updates, which broadcast notifications to all parties; (2) private observation changes, which notify only the associated party; (3) new messages, which trigger notifications for all recipients; and (4) environment inactivity exceeding a specified temporal threshold, which broadcast notifications to all parties. We use a Redis server to manage notifications across different process, and Listing 1 in Appendix A.1 provides the pseudo code of the `event_handler` implementing this notification protocol.\nNotably, Co-Gym is not an agent framework but a framework designed to apply and evaluate LM agents in various task environments alongside human collaborators. Our design principles consider an LM agent as an LM-empowered system comprising the underlying LM(s), prompts, and additional scaffolding (e.g., memory, external tools, etc.). To accommodate this flexibility, Co-Gym imposes minimal restrictions on agent implementation. To streamline agent integration, Co-Gym includes an AgentNode, which wraps the LM agent to subscribe to notifications on the {role}/obs channel, adhering to the notification protocol in Listing 1. When a new message appears on the {role}/obs channel, the LM agent is responsible for deciding whether to take action and, if so, specifying the action. The AgentNode then sends the action string and agent role to the environment via the step channel, where the EnvNode processes it."}, {"title": "3.4. Evaluating Collaborative Agents", "content": "Existing evaluations of LM agents often only target task success rates. While task completion and outcome quality are crucial, the collaboration process also plays a significant part. Co-Gym supports the evaluation of collaborative agents across both collaboration outcomes and processes.\nEvaluating Collaboration Outcome We assess the collaboration outcome along two dimensions:\n\u2022 DELIVERY RATE: This binary metric indicates whether collaborative agents can successfully deliver a task outcome within a predefined step limit\n\u2022 TASK PERFORMANCE: A task-specific scoring function evaluates the quality of the final outcome for delivered cases. This function may be a deterministic metric or based on LM/human judgments. To ensure comparability across tasks, scores are normalized to the range [0, 1].\nTo jointly assess both dimensions, we define the Collaboration Score as:\n$Collab Score = \\frac{1}{2}(I_{Delivered} + Task \\ Performance)$\nAuditing Collaboration Process To understand team dynamics, we analyze the collaboration process along three important dimensions:\n\u2022 INITIATIVE-TAKING: Collaborative agents operate as mixed-initiative systems (Horvitz, 1999). Building on the framework of Chu-Carroll & Brown (1997), we define an utterance in collaborative dialogues as exhibiting initiative if it directs task execution or facilitates mutual understanding. Here, we employ an LM to annotate utterances. To quantify the distribution of initiative, we use entropy as a measure, where a uniform distribution results in high entropy and a skewed distribution results in low entropy. Specifically, we define Initiative Entropy $(H_{init})$ as:\n$H_{init} = \\begin{cases} -\\sum_{i=1}^{N} p_i \\cdot log_N(p_i) & \\forall i, p_i > 0,\\\\ 0 & \\exists i, p_i = 0 \\end{cases}$\nHere, $p_i$ is the proportion of initiative-taking utterances by party i and N refers to the total number of parties.\n\u2022 CONTROLLED AUTONOMY: Effective collaboration requires agents to seek human confirmation at critical moments to ensure alignment with human intent and mitigate potential safety risks. We measure this dimension by counting the agent's confirmation questions that effectively elicit a human response $(CA^+)$ and counting instances where the human verbally intervenes to halt the agent's actions $(CA^-)$.\n\u2022 OVERALL SATISFACTION: At the end of the collaboration, we collect human ratings of their collaboration experience with the agent using a 1\u20135 Likert scale.\nDetails of metric computation based on this evaluation framework are provided in Appendix C."}, {"title": "4. Co-Gym Instantiations", "content": "While the Co-Gym interface is general, we demonstrate its capabilities through three representative tasks that highlight different facets of human-agent collaboration with both simulated and real human participants."}, {"title": "4.1. Supported Tasks", "content": "Travel Planning Travel planning is a widely sought yet complex task, as optimal solutions depend on users' latent preferences and constraints that may not be explicit in the initial query. Collaborative agents must demonstrate strong communication and planning skills to successfully complete this task. The Travel Planning environment in Co-Gym provides a comprehensive action space, including various search functions like CitySearch, FlightSearch, DistanceMatrix, RestaurantSearch, AttractionSearch, and AccommodationSearch, aligning with Xie et al. (2024) which studies fully autonomous agents. The search window is private in the observation space, while the task action space also includes EditorUpdate that modifies the travel plan visible to both agent and human user.\nRelated Work Writing Grounded article writing is a commonly used to assess agentic systems (Shao et al., 2024a; Wang et al., 2024). While agents can autonomously conduct literature search and generate text, humans may be willing to actively involve due to their additional knowledge that collaborative agents must incorporate. The Related Work environment supports SearchPaper action to retrieve papers based on the given query. A shared library and text editor are included in the observation space, enabling actions like LibraryAddPaper, LibraryDropPaper, LibraryToDraft, and EditorUpdate to facilitate collaboration.\nTabular Analysis Another common application of LM agents is scientific discovery, particularly in data-driven research where researchers leverage agents to extract insights from raw study data (Majumder et al., 2024). This task demands both collaborative input from users about data context and technical expertise from collaborative agents in coding and statistical analysis. The Tabular Analysis environment includes a shared Jupyter Notebook and text editor, supporting actions such as JupyterExecuteCell and EditorUpdate. The goal is to derive analytical insights from the provided data and initial query.\nImplementation details for these environments are provided in Appendix A.2."}, {"title": "4.2. Supported Conditions", "content": "To facilitate controlled, iterative development while maintaining ecological validity, Co-Gym supports two experimental conditions: Co-Gym (Simulated) and Co-Gym (Real). These conditions allow us to examine how humans and agents collaborate in our supported task environments with either simulated humans or real humans.\nCo-Gym (Simulated) The core concept of Co-Gym (Simulated) is to create a sandbox environment where human-agent collaboration can be studied without impacting real-world environments or requiring real human participants. Each task in Co-Gym (Simulated) is associated with a set of pre-collected instances that define concrete shared goals for the human-agent team. Tools within each task environment are mocked using static databases to emulate realistic interactions while maintaining a controlled and reproducible setup. To reduce the cost and complexity of involving human participants, Co-Gym (Simulated) uses an LM (gpt-40 in our experiments) to simulate human behavior. The simulated human processes observations and selects actions from five predefined action types that represent potential human behaviors:\n\u2022 ANSWER QUESTION: The simulator LM further generates the answer and the next action would be SendTeammateMessage.\n\u2022 PROVIDE FEEDBACK: The simulator LM further generates the feedback and the next action would be SendTeammateMessage, simulating human proactive information sharing.\n\u2022 TAKE TASK ACTION: The simulator LM further generates an action string within the task-specific action space, simulating human task engagement.\n\u2022 DO NOTHING: The next action would be WaitTeammateContinue, simulating the human tendency to pause, reflect or expect the agent to do the actual work.\n\u2022 FINISH: Notify the environment to end the task.\nTo introduce dynamics typical of human-agent collaboration, where the human may have additional knowledge and preferences about the task, we provide the simulator LM with hidden information\u2014pre-curated insights associated with each task instance. Additional details about Co-Gym (Simulated) are included in Appendix A.3.\nCo-Gym (Real) While experiments with simulated humans provide a valuable surrogate for advancing human-agent collaboration, they cannot fully replace human studies (Aher et al., 2023; Zhou et al., 2024b). A key strength of Co-Gym is its versatility: in addition to supporting various task environments, the design of asynchronous interactions align naturally with human behaviors. We instantiate Co-Gym (Real) as a web application, enabling users to easily perform the three supported tasks directly through their web browsers. Details of our user interface are provided in Appendix A.4. To incentivize human in real-world evaluations, we instantiate the transition function within Co-Gym (Real) by leveraging real tools (e.g., Google Maps, arXiv search) within the task environments."}, {"title": "5. Experiment", "content": "Models We evaluate several state-of-the-art LMs for agents: GPT-40 (gpt-40-2024-08-06), GPT-4-turbo (gpt-4-turbo-2024-04-09), Claude-3.5-sonnet (claude-3-5-sonnet-20241022), Llama-3.1-70B. Due to the complexity of our tasks, we exclude smaller models from our experiments. All models are used with a temperature of 0.\nMethods We implement baseline LM agents with Re-Act (Yao et al., 2022) which requires the LM to output \"thought\" before generating the action. During the asynchronous collaboration, the LM agents are programmed to always process the most recent notification. Since our tasks inherently require multiple steps and the trajectories can be even longer in human-agent collaboration setup, we incorporate a Scratchpad module as an in-session memory for the LM agents (Sumers et al., 2023). This memory is updated dynamically by the same LM before determining the next action using ReAct-style prompting.\nTo compare the human-agent collaboration paradigm with fully autonomous agents, we experiment with the following agent types: (1) Fully Autonomous Agent which only interacts with the task environment, and its action space is restricted to task-specific actions; (2) Collaborative Agent which adopts the same implementation but extends the action space to include both task-specific actions and those two collaboration acts (i.e., SendTeammateMessage, WaitTeammateContinue).\nWhile the implementation of Collaborative Agent is intuitive for testing the collaboration capabilities of current LM agents, our preliminary experiments reveal that when collaboration acts are included, current LMs rarely choose these options and often neglect their human collaborators. To investigate whether explicitly prompting LMs to reason about this decision would improve performance, we introduce a third agent type in addition to the aforementioned baseline agents: (3) Collaborative Agent with Situational Planning which employs a two-stage decision-making approach when processing notifications. First, the LM makes a 3-way decision based on all available information (i.e., task description, chat history, action history, observations) to take a task action, or send a message to its teammate, or do nothing. If it chooses to do nothing, the next action would be WaitTeammateContinue; otherwise, it is further prompted to generate the final action string using the context and decision. We include implementation details of this agent in Appendix B."}, {"title": "5.2. Data, Metrics & Participants", "content": "Data As described in \u00a74.2, Co-Gym (Simulated) utilizes pre-collected datasets to evaluate human-agent collaboration within a controlled sandbox. A summary of data statistics, as well as the action and observation spaces for each task, is presented in Table 1. Additional details about the task setup can be found in Appendix A.2.\nMetrics We leverage the evaluation framework in \u00a73.4 to assess human-agent collaboration across three tasks. For task-specific scoring functions, in Co-Gym (Simulated), for Travel Planning, we adopt the evaluation script from Xie et al. (2024) to compute the average of the commonsense pass rate and the constraint pass rate. For Related Work, we develop a rubric (Figure 12) that uses gpt-4-06-13 as a judge to assign a score from 1 to 5. Automated scores align with human evaluations, with correlation coefficients of 0.791 (Pearson) and 0.741 (Spearman) across 20 sampled sections. For Tabular Analysis, we assign a score of 1 if the derived hypothesis and the gold hypothesis are entailed, and 0 otherwise, using the evaluation script from Majumder et al. (2024). In Co-Gym (Real), where task instances from real users are highly diverse and lack ground-truth results, we ask humans to rate the final outcome on a 1-5 scale (1: \"Extremely dissatisfied\", 2: \"Somewhat dissatisfied\", 3: \"Neutral\", 4: \"Somewhat satisfied\", 5: \u201cExtremely satisfied\"). All Task Performance scores are normalized to the range [0, 1] for reporting results. Computation details for other metrics are included in Appendix C.\nHuman Participants For the experiment condition with Co-Gym (Real), we recruited human participants with relevant expertise or practical needs to collaborate with Collaborative Agent with Situational Planning powered by gpt-40. Specifically, we targeted participants who had current needs in travel planning, tabular data analysis, or writing related work sections/literature surveys. Recruitment was initially conducted through word-of-mouth. To broaden our participant pool, we recruited travel planners through Upwork for Travel Planning and participants with Python programming and data analysis experience through Prolific to work with the agent on writing analytical reports from tabular data. Participants were compensated at a rate of $8.00 per hour, and the study received approval from our institution's Institutional Review Board (IRB). In total, 99 unique individuals participated in the study, contributing 150 human-agent collaboration trajectories across three tasks in Co-Gym (Real). These trajectories consist of 6.3k actions performed by human-agent teams and over 77k words of verbal communication exchanged between humans and agents."}, {"title": "5.3. Main Results", "content": "Table 3 summarizes the results for both collaboration outcomes and processes under the simulated condition. Overall, the Collaborative Agent with Situational Planning consistently achieves a higher Collaboration Score compared to the baselines. Notably, the agent powered by Claude-3.5-sonnet achieves a 0.20 improvement in Travel Planning (maximum score: 1), the one powered by Llama-3.1-70B achieves a 0.04 improvement in Related Work, and the one powered by GPT-40 achieves a 0.10 improvement in Tabular Analysis, earning the highest score in each respective task. However, even with the best-performing agents, simulated human-agent teams struggle to achieve optimal performance.\nCollaborative agents struggle more to complete the task. Analysis of final outcomes reveals that collaborative agents exhibit a lower delivery rate compared to Fully Autonomous Agents. This could be attributed to the inherent complexity of decision-making for collaborative agents, which operate in a wider action space, must adapt plans frequently based on human actions or messages, and need to balance communication and executing tasks. Analysis of concrete cases (\u00a76.2) revealed that failures stemmed mainly from the agent ignoring human messages (C.2, 46%), prompting repeated messages (SA.2, 26%), or repeating and omitting actions due to poor planning (PL.2, 33%). These failures resemble the phenomenon of collaborative inertia in human collaboration, where the output rates could become slow due to teamwork (Huxham, 2003).\nCollaborative agent can lead to better task performance. Despite lower delivery rates, collaborative agents demonstrate better task performance for successfully completed tasks compared to their fully autonomous counterparts. With its more balanced initiative and heightened responsiveness, the Collaborative Agent with Situational Planning achieves the best performance across all three tasks. Unlike Fully Autonomous Agents, which typically complete tasks as soon as they achieve an outcome, collaborative agents engage in a more iterative process. They either proactively solicit feedback or respond to suggestions to refine the task results. For example, before starting to plan the trip based on the initial query, the agent might ask, \u201cAre there any particular cities or attractions you want to visit?", "Could you please add headings?\"; or during the data analysis process, the human might provide oversight on omissions, saying, \u201cIt seems like you haven't yet analyzed the 'HouseSize' and 'Zhausgr' columns.\" This iterative interaction allows collaborative agents to incorporate refinements and improve task outcomes.\nDifferent tasks show different patterns. While the comparison of different agents shows similar trends across tasks, analyzing the process uncovers distinct patterns. In the Travel Planning task, Initiative Entropy $(H_{init})$ and the number of effective confirmations (CA+) are generally higher, as LM agents take more initiative to drive the collaborative planning process and seek confirmation on key decisions, such as hotel selection. In contrast, the Tabular Analysis task exposes the limitations of baseline Collaborative Agents, which often default to executing code without proactive communication or responding to human input. This behavior frequently leads to human intervention, as evidenced by the high number of halting messages (CA-). When comparing across underlying LMs, GPT-40 and Claude-3.5-sonnet demonstrate a better balance between initiative-taking and allowing humans to maintain control. Notably, the decision to communicate or take task actions is dynamically determined by the LM agents, rather than being hard-coded. These distinct patterns align with the varying nature of tasks. Our results indicate that it is crucial to enhance LM agents' cooperative intelligence to maintain human control while ensuring progress, especially on those more technical tasks.\"\n    },\n    {\n      \"title\"": "5.3.2. RESULTS IN CO-GYM (REAL)"}, {"content": "Table 3 summarizes the results for Co-Gym (Real). Consistent with the simulated condition, outcomes from collaborative agents are preferred over those from autonomous agents across all tasks. However, humans' overall satisfaction with the collaboration process varies by task, with Tabular Analysis achieving an average score of 4.06, while Related Work scores lower at 3.06, indicating a more neutral sentiment. User feedback consistently highlights the importance of the agent's initial output quality and the ability to recover from misaligned understanding or correct inaccuracies in human-agent collaboration. In the Travel Planning task, users report higher satisfaction, often commanding the quality of the initial itinerary drafts. Most interactions involve minor adjustments, suggesting that users primarily rely on the agent to handle most of the work according to their preference during collaboration on this task. Similarly, for the Tabular Analysis task, overall satisfaction is also high, as the agent handles most of the work in writing, executing, and interpreting code, while humans primarily contribute by posing new questions for further analysis. In contrast, the Related Work task receives low satisfaction scores (3.06 out of 5 where 3 indicates \u201cNeutral\" on the Likert scale). As this task demands a high level of expertise to identify relevant papers and organize them logically, users often need to provide step-by-step instructions to drive the collaboration. This problem is also reflected in the low $H_{init}$ and CA+ scores.\""}, {"title": "6. In-Depth Analysis", "content": "Highly collaborative agents should be able to communicate effectively with humans while adapting to the unique requirements of the environment and task. Although we do not specifically train agents to be collaborative in this work, we observe several key components of successful human-agent collaboration in Co-Gym."}, {"title": "6.1. Effective Patterns in Human-Agent Collaboration", "content": "One common pattern in successful collaboration is proactive communication. For example, in Travel Planning, many preferences and requirements are latent, thus it is essential for humans to retain control over the final plan. In Figure 13, we demonstrate a case of successful collaboration where the user poses a broad subjective question regarding activity planning, and the agent responds with relevant suggestions, waiting for the user's approval before making changes.\nAnother emerging pattern is the distribution of work based on expertise, even though we do not hard-code agent behavior or provide human participants with any prior information on how they should work with the agent. For example, in Figure 14, when collaborating on a related work section about \"Software Techniques for Emerging Hardware Platforms,\" the LM agent asks the human to narrow the search range for embedding methods in NLP tasks, as it is tangential to the topic but mentioned by the human. Throughout the collaboration, the LM agent primarily handles searching and writing, while the human focuses on reviewing the results, refining them by adding or dropping papers.\nLastly, we observe the potential for collaborative agents to enhance human control. While this dimension is partially captured by the Controlled Autonomy metrics (i.e., CA+, CA-), concrete examples, such as the one in Figure 1, demonstrate this behavior. For instance, when allowed to communicate with humans or wait for their input, the LM agent asks the human to make critical decisions, such as whether to install a specific package. Simultaneously, the agent autonomously resolves non-sensitive issues, such as fixing bugs, without requiring human intervention."}, {"title": "6.2. Failure Modes in Human-Agent Collaboration", "content": "We conducted a comprehensive error analysis on trajectories collected from Co-Gym by developing a failure mode annotation checklist through a single-pass review of all real human-agent collaborations. The identified errors are grouped into five categories:\n\u2022 COMMUNICATION (C): Failures in maintaining effective information exchange, that disrupt understanding, coordination, or task execution.\n\u2022 SITUATIONAL AWARENESS (SA): Failures in contextual understanding and reasoning about the current state of the task or collaboration.\n\u2022 PLANNING (PL): Failures in devising, updating, or executing coherent plans, especially in dynamic or long-horizon scenarios.\n\u2022 ENVIRONMENT AWARENESS (EA): Failures in recognizing or accounting for operational constraints and resources within the task environment.\n\u2022 PERSONALIZATION (P): Failures in adapting behaviors to align with individual user preferences based on in-session histories, and interaction patterns.\nAs Co-Gym evaluates LM agents as a whole, we attribute each error type to gaps in the LM's inherent capabilities, deficiencies in the design of the agent scaffolding (e.g., memory, additional tools), or both. Using this checklist, three authors hand-annotated 150 trajectories each from CoGym (Real) and Co-Gym (Simulated) conditions. Table 4 summarizes the results.\nThe most prevalent issues involve Communication (occurring in 65% of real trajectories and 80% of simulated trajectories) and Situational Awareness (Raiman et al., 2019) (Real: 40%, Simulated: 47%). For instance, in Figure 15, the agent fails to update collaborators on its status (C.1, C.3) and provides incorrect summaries after executing actions (C.5), causing human confusion and disrupting collaboration. In Figure 16, the agent fails to decide when to ask the human for help or to incorporate the human's suggestions, repeatedly encountering the same issues during code execution and becoming trapped in an endless loop (SA.1, SA.2, SA.3). These errors highlight the limitations of current LMs when deployed in complex, agentic setups with human involvement. Moreover, Planning challenges for collaborative agents often arise from long trajectories and the need to frequently adjust plans due to human interaction. Beyond the LM's capabilities, deficiencies in agent scaffolding contribute to Personalization issues. For example, agents in our experiments cannot learn or apply user preferences across sessions, and exhibit homogeneous behavior when collaborating with different humans, which hinders dynamic and effective collaboration. Additional representative failure cases are included in Appendix D.\nBy comparing the two conditions, we found that trajectories collected from Co-Gym (Simulated) and Co-Gym (Real) exhibit many similarities, both highlighting deficiencies in Communication (C.1, C.2, C.3, C.4, C.5), Situational Awareness (SA.1, SA.2, SA.3, SA.6), Planning (PL.1, PL.2), and Environment Awareness (EA.3). However, certain limitations were more evident in real-world interactions. Examples include \"(C.7) Agents miss implicit cues to initiate expected actions\u201d, \u201c(EA.1) Agents do not assess the feasibility of requests within the constraints of available tools and resources", "(SA.5) Agents execute critical actions without obtaining prior confirmation\".\"\n    },\n    {\n      \"title\"": "7. Conclusion"}, {"content": "We introduce Collaborative Gym (Co-Gym), the first framework designed to evaluate and facilitate human-agent collaboration in diverse task environments. By assessing both task outcomes and collaboration processes, Co-Gym addresses key gaps in current"}]}