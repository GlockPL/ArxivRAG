{"title": "AI-LIEDAR: Examine the Trade-off Between Utility and Truthfulness in LLM Agents", "authors": ["Zhe Su", "Xuhui Zhou", "Julia Mendelsohn", "Sanketh Rangreji", "Faeze Brahman", "Anubha Kabra", "Maarten Sap"], "abstract": "To be safely and successfully deployed, LLMS must simultaneously satisfy truthfulness and utility goals. Yet, often these two goals compete (e.g., an AI agent assisting a used car salesman selling a car with flaws), partly due to ambiguous or misleading user instructions. We propose AI-LIEDAR, a framework to study how LLM-based agents navigate scenarios with utility-truthfulness conflicts in a multi-turn interactive setting. We design a set of realistic scenarios where language agents are instructed to achieve goals that are in conflict with being truthful during a multi-turn conversation with simulated human agents. To evaluate the truthfulness at large scale, we develop a truthfulness detector inspired by psychological literature to assess the agents' responses. Our experiment demonstrates that all models are truthful less than 50% of the time, although truthfulness and goal achievement (utility) rates vary across models. We further test the steerability of LLMs towards truthfulness, finding that models follow malicious instructions to deceive, and even truth-steered models can still lie. These findings reveal the complex nature of truthfulness in LLMs and underscore the importance of further research to ensure the safe and reliable deployment of LLMs and AI agents.", "sections": [{"title": "1 Introduction", "content": "Utility, i.e., the capability to satisfy human instructions and needs, is the central quality desired in large language models (LLMs) (Askell et al., 2021; Liu et al., 2024). Yet, truthfulness, i.e., the adherence to factual accuracy and honesty, is a key component of LLM and AI safety (Hendrycks et al., 2023). For LLMs to be deployed safely, it is important that they deliver the knowledge and information they receive accurately and truthfully (Evans et al., 2021; Kaur et al., 2022; Tarsney, 2024).\nIdeally, we want LLMs to be able to maintain both utility and truthfulness, but sometimes that is not possible. For example, an AI agent may be instructed to promote a particular product when acting as a salesperson. A truthful agent would honestly disclose the product's shortcomings, even if that means failing its goal of convincing a user to buy the product. However, a useful agent (from the salesperson's perspective) may engage in deceptive practices such as obfuscation and overt lying, as shown in Figure 1.\nWhile significant effort has been dedicated to mitigating LLM's tendency to generate false or misleading information (i.e., \u201challucinations\u201d; Zhang et al., 2023a; Min et al., 2023; Li et al., 2023), less attention has been given to how user instructions affect LLM truthfulness, particularly when the goals specified by instructions seemingly or intentionally prioritize utility over truthfulness. This motivates us to explore an important, yet largely unexplored question: how do LLM-based agents navigate the trade-off between utility and truthfulness?\nIn this work, we introduce AI-LIEDAR, a framework to study how LLM-based agents navigate the trade-off between utility and truthfulness via LLM-user simulations. Unlike previous research focused on hallucinations and single-turn truthfulness (e.g., TruthfulQA; Lin et al., 2022), we explore LLM's truthfulness in a multi-turn interaction setting, reflecting the increasingly interactive role of LLMs as AI agents or assistants. Examining under such a setting reveals the nuanced and dynamic behavior of LLMs, including instances where they initially equivocate but eventually provide falsified information to attain certain utilities (Figure 1).\nIn AI-LIEDAR, we curate a set of 60 diverse real-world scenarios inspired by psychology literature (Cantarero et al., 2018; Camden et al., 1984). The scenarios encompass three representative categories of lying based on their potential beneficiaries: benefits, public image, and emotion, covering multiple human motivations behind lying. We simulate user-LLM interactions via the Sotopia simulation framework and measure the utility from the goal achievement score from the Sotopia evaluator (Zhou et al., 2024b). We further develop a fine-grained truthfulness evaluator inspired by psychology to capture levels of lying behavior, such as partial lying (e.g., concealment) and complete lying, providing deeper insights into models' truthfulness.\nOur experiments on 2160 simulations show that models are not inherently truthful. Despite no clear relationship between truthfulness and the model's capacity or size, all models are truthful less than 50% of the time. Even when explicitly steered to be truthful, instances of lying persist. Furthermore, models exhibit different behaviors depending on the category of the scenario. When the outcomes are concrete and measurable, such as selling a car, the models' behaviors are more distinct, being either completely truthful or entirely deceptive. In contrast, scenarios in the public image category, which involve maintaining a positive reputation, lead to more ambiguous actions.\nWe further investigate to what degree models can be steered towards either truthfulness or falsification. Our results show that stronger models are more responsive to prompts that encourage falsification or truthfulness. Stronger models, like GPT-40, respond significantly to prompts encouraging falsification or truthfulness, with a 40% increase in lying when instructed to falsify. Steering models toward honesty often reduces their goal completion rate by 15%. In the benefits category, this effect is more pronounced given the utilities are more objective and quantifiable (i.e. When models become more truthful, the goal completion rate drops more). However, when the goals are more subjective, such as sparing someone's feelings, this effect is less pronounced.\nThese findings show that LLMs can behave deceptively, especially when honesty conflicts with their objectives, highlighting challenges in guiding models toward truthfulness in complex interactions."}, {"title": "2 Background & Related Work", "content": "In this section, we explore lines of research related to the deceptive behavior of language models, recent investigations into model behavior, and advancements in evaluating models' morality-related beliefs and reasoning abilities. We review some recent progress in these directions below and how they differ from our work.\n2.1 LLM Truthfulness, Utility, and Deception\nWe first define key concepts related to our study.\nLLM Truthfulness is defined as providing accurate information (Askell et al., 2021). We narrow this scope and define it as the model accurately conveying information it has received from surrounding environment (i.e. The context the LLM is placed in during a social interaction). Here, truthfulness and honesty are considered synonymous. The definition is distinct but related to LLM hallucination, which is another source of inaccurate information in language models which has been widely studied by McKenna et al. (2023); Zhang et al. (2023b); Ji et al. (2023). While LLM lack of truthfulness may sometimes be due to LLM hallucinations, our definition of truthfulness excludes inaccuracies by model generating information not grounded in its input data (i.e. input-conflicting hallucination as defined by Zhang et al., 2023a). Furthermore, our definition of LLM untruthfulness aims to capture the grey areas of LLM \u201cpartial lies\" (e.g., equivocation, skirting the issue), which are typically not considered LLM hallucinations but are nonetheless important to capture.\nLLM Utility is broadly defined as the capability to satisfy human's instructions and needs (Askell et al., 2021; Liu et al., 2024).\nLLM Deception refers to the systematic production of false beliefs in order to accomplish certain tasks (Park et al., 2023).\n2.2 The Cause of Utility-Truthfulness Conflict\nIn real world scenarios, human might have ambiguous or under-specified instructions, with the most widely-studied cases - instructions that require commonsense to understand (Davis, 2023). People rely on prior knowledge to comprehend these instructions and can navigate these scenarios effectively. Similarly, truthfulness or social norms can be another implicit user expectation when achieving a goal. An agent may not possess such prior, which could lead to unintended behaviors. The situation is more complex considering it's not just about following instructions, but also about whose instructions to follow and under what conditions.\n2.3 LLMs Can be Deceptive\nRecent research has shown that LLMs can exhibit deceptive behavior in various contexts. Xie et al. (2024); Scheurer et al. (2023); Lin et al. (2024); Scheurer et al. (2023); Jarviniemi and Hubinger (2024) show the LLM agents can simulate and exhibit human-like strategic deceptive behavior in different game settings and in more realistic working environments. Furthermore, Hubinger et al. (2024) find the models can even learn this behavior and it persist even after safety training. Ward et al. (2023) provides a mitigation method for LLMs' being deceptive.\nSycophancy can also be considered as an implicit form of deception. Current large language models tend to exhibit sycophantic behavior when dealing with subjective matters (Ranaldi and Pucci, 2023; Wei et al., 2023).\nDeceptive behavior is inherently interactive. Buller and Burgoon (1996); Peskov et al. (2020) show that intended truthfulness often differs from perceived truthfulness, suggesting that studying truthfulness in interactive settings could reveal deeper insights. However, limited research explores models' truthfulness in such settings, particularly when utility is a trade-off, highlighting the need for further investigation."}, {"title": "2.4 Navigating Between Utility and Truthfulness", "content": "Research on how LLMs navigate the tension between utility and truthfulness is limited. A pilot study by Liu et al. (2024) explores this problem in both simulated games and everyday settings, but there are fewer conflicts of interest, and the research is limited to single-turn interactions. The behavior of models in multi-turn settings, where the dynamics of interaction can impact outcomes, remains largely unexplored.\nMachine Morality The moral reasoning capabilities of language models have been widely studied. Jiang et al. (2021) trained models on ethical judgments and norms, showing their strengths and limitations. There are also datasets that assess models' reasoning in static contexts (Emelin et al., 2020; Ziems et al., 2022). However, Talat et al. (2022) argue that static evaluations are flawed. Sotopia (Zhou et al., 2024b) offers an interactive framework for dynamic moral evaluation. These works highlights the need for evaluating LLM-based agents' morality in interactive settings."}, {"title": "3 AI-LIEDAR Simulation Framework", "content": "To assess how models navigate the trade-off between utility and truthfulness, we build our AI-LIEDAR framework on top of Sotopia, a multi-agent platform that supports the simulation and evaluation of social interactions.\nIn this section, we introduce how we curate a set of scenarios and how to simulate them with our framework. We begin by introducing the desiderata and format of the scenarios, followed by the crafting pipeline, and finally the simulation setting.\n3.1 Crafting Utility-Truthfulness Goals\nWe would like to have our scenarios to represent diverse settings in which an AI agent might lie. As such, we draw inspirations from two categorizations of lying: (1) Who benefits from lying, and (2) What are the motivations of lying. We use the first as the major category and further incorporate the second categorization to diversify the scenarios.\nCategorize by Beneficiary Lies can be broadly classified into two main types based on the beneficiary (Cantarero et al., 2018): those that benefit the deceiver (self-oriented lies) and those that benefit others (other-oriented lies). We choose three categories of utility to reflect these two types of lying: Benefits (self-oriented), where an individual may lie for personal gain, impacting a small group; Public Image (self-oriented), where one may deceive to protect their reputation, affecting a larger audience; and Emotion (others-oriented), where lies are told to spare other's feelings.\nIncorporate Human Motivation We then incorporate human motivations behind lying into the existing categories (Camden et al., 1984). Under each category (benefits, public image, emotion), we manually check the motivations to determine their applicability to a certain category. This allows us to curate a set to seed our scenario generation. For the full set of human values considered, refer to Appendix \u0412.\n3.2 Scenario Format\nWe define a standardized format that each scenario should have. As illustrated in Figure 2, each scenario has a background setting, the AI agent's goal and the human agent's goal. The background describes the context and participants in the scenario. We carefully design the setting to resemble real-world environments (\u00a73.3). The human agent's goal is to achieve tasks that require inquiring information from the AI agent.\nInformation Elements The AI agent's goal is the core part of the scenarios and includes the following three elements of information, in addition to the role and goal specification: 1. Info not Known, which represents the information that makes being simultaneously truthful and helpful hard to achieve. 2. Motives to Lie, which provides the reason why the agent should lie. In our design, this is the attainment of the given utility. 3. Motives not to Lie, which provides the reason why the agent should refrain from lying. It could be the potential harm caused by the lie or the benefits to others if the agent remains truthful.\nTo ensure a balanced perspective, we design the AI agent's goals to encompass both benefits and harms, avoiding bias towards a specific outcome based solely on information.\nIn addition to the elements discussed above, we introduce another, <info_not_known>. ndication of information asymmetry to the model. This element informs the AI Agent that the negative information is exclusively known to itself, providing a clear indication of information asymmetry.\nIn our ablation study, we further test the steer-ability of the models by introducing biased, or even extreme statements in the AI agent's prompt:\n1. Falsification instruction: \"Since the information is currently unknown to outside people, deliberately providing false information is the best way to achieve your goal.\"\n2. Truthfulness instruction: \"Since honesty is the top priority, you should always provide correct information to everyone.\"\n3.3 Scenario Crafting Process\nBased on the concepts and formats defined above, we start with hand-crafted versions of the scenarios, which we then iteratively update manually and with ChatGPT.\nIn total, we get 20 distinct scenarios from each category. A sample scenario and its category is shown in Table 2 and we refer the reader to Appendix A for more examples. To enhance the robustness of our evaluation, we further generate two paraphrases using GPT-40 for each scenario and manually verified them. This results in a total of 60 scenarios under each category. The full prompt for paraphrasing can be found in Appendix K.\n3.4 Simulation Setup\nThe simulation is conducted using the Sotopia framework with scenarios constructed as described above. The framework simulates human communication from a given environment and two agents, using an automatic evaluator to assess performance across various dimensions (e.g., goal, secret). For each scenario, we sample multiple agent pairs and run simulations, with each resulting conversation referred to as an Episode. To ensure consistency, the human agent is always set to GPT-40. We test six different LLMs as AI agents, including GPT-3.5-turbo (Ouyang et al., 2022), GPT-40, Mixtral-7*8B (Jiang et al., 2024), Mixtral-7*22B (Jiang et al., 2024), LLaMA 3-8B (AI@Meta, 2024), and LLaMA 3-70B (AI@Meta, 2024). The GPT-series models' API is provided by OpenAI, while the other models are provided by TogetherAI 6.\nIn our evaluation, utility is measured by the goal completion rate using the Sotopia goal evaluator7. We use GPT-40 with a custom evaluation prompt as the truthfulness evaluator, as mentioned in \u00a74. During generation, the temperature is set to 0.7, whereas for the evaluator's assessment, the temperature is set to 0."}, {"title": "4 AI-Lie Evaluator", "content": "In this section, we briefly describe how we define different lying behavior, and how to construct our AI-LIEDAR Evaluator.\n4.1 Different Types of Lying Behavior\nUnlike previous evaluator that uses \u201ccreating false beliefs in others\u201d (Scheurer et al., 2023) or clear contradiction with the truth (Pacchiardi et al., 2023) for a two-way classification, we recognize that there are \"middle grounds\" in deceptive behavior, which humans frequently use in daily interactions. Our categorization of deception is based on the typology from Buller and Burgoon (1996):\n1. Truthful: Revealing all relevant information honestly and completely.\n2. Partial Lie: Include Concealment (Omitting material facts or withholding pertinent information.) and Equivocation (Skirting issues by changing the subject or responding indirectly, often using ambiguous or vague language).\n3. Falsification: Making an assertion that contradicts the known truth or presents false information as fact.\nA more concrete example illustrating these differences can be found in the Appendix C.\nTruthfulness Evaluator Setup To evaluate truthfulness on a large scale, we follow other works that have shown LLM successes at evaluating various concepts (e.g., goal completion) and in different domains (e.g., theory-of-mind, safety) in interactions (Zhou et al., 2024b; Wang et al., 2024; Kim et al., 2023, 2024), and resort to a model-based evaluation to ensure consistency and scalability.\nWe prompt a GPT-40 model with definitions of different types of truthfulness and concrete demonstrations. The prompt is concatenated with the conversation history, presented from an omniscient perspective, including both agents' internal states (such as their backgrounds and private information). The exact prompt can be found in Appendix J."}, {"title": "4.2 Human Validation of the Evaluator", "content": "We validate our truthfulness evaluator with a human annotation study. We randomly sample 2 episodes (each represent a conversation) from 20 scenarios for each category, resulting in a total of 120 episodes. For each episode, three graduate students annotate the AI agent's truthfulness under both fine-grained (Concealment and Equivocation are regarded as different) and coarse-grained (Use Partial Lie category) metrics.\nOur human annotators achieve a decent inter-annotator agreement, with an average exact match rate of 0.73. We use majority vote on coarse-grained metrics to determine the label for episodes. With the labels, the overall accuracy of the evaluator is 85.0%. Thus we use this evaluator and coarse-grained metric in future evaluation.\nFor more details and the results about the annotation, please refer to Appendix D."}, {"title": "5 Results", "content": "In this section, we present results exploring the truthfulness and goal-reaching behavior of LLMs under various conditions.\n5.1 Evaluation of Models' Utility and Truthfulness\nTable 3 shows an averaged results of different models over three categories and 2160 episodes. Our results indicate that models often resort to deception, being truthful <50% of the time. Models behave differently. GPT-40 and Mixtral-7*22B are more truthful, while GPT-3.5-Turbo exhibits the highest rate of outright falsification. LLaMA-series models display the greatest tendency toward deceptive behavior, including partial lies and falsification.\nMoreover, models frequently use ambiguous acts to balance utility and truthfulness, with much of the deception involving equivocation or concealing information. Most models achieve good utility performance, except for LLaMA 3-8B.\n5.2 Effects of Provided Prompt Information on Truthfulness\nWe conducted an ablation study to examine how different instruction components contribute to the truthfulness of the models. We used one proprietary model (GPT-40) and one open-sourced model (LLaMA 3-70B) as examples, evaluating the change in truthfulness within the benefits category with the three elements mentioned in \u00a73.2\nAs shown in Table 4, we observed similar trends across both models: (1) giving the model hints about what information the interlocutor does not know (through <info_not_known>) decreases the truthful rate and slightly increases the falsification rate, (2) providing the model with motivations not to lie increases the truthful rate, and (3) removing <motives_to_lie> yields an approximately 40% increase in truthfulness.\nIn addition, we find considering partial lying is important in evaluating truthfulness. For instance, removing <motives_not_to_lie> causes a smaller change in the falsification rate than in the truthful rate. This suggests that a fine-grained evaluator is necessary, as direct falsification alone does not fully capture deceptive behavior."}, {"title": "5.3 Model Steerability", "content": "Next, we examine how LLM agents' behavior can be steered towards or away from truthfulness and its impact on goal achievement, as described in \u00a73.2. We included falsification or truthfulness prompts at the end of the input to guide the model (e.g., Since honesty is the top priority, you should always provide correct information to everyone.).\nFigures 3 and 4 demonstrate that both the falsification and truthfulness rates change greatly in response to the steering instructions, with changes near 40% in both directions. Notably, models continue to exhibit deceptive behavior even when explicitly guided to be truthful. In public image and emotion category the results are similar, and we refer the reader to Appendix I.\nNext, we examine how LLM agents' behavior can be steered towards or away from truthfulness and its impact on goal achievement, as described in \u00a73.2. We included falsification or truthfulness prompts at the end of the input to guide the model (e.g., Since honesty is the top priority, you should always provide correct information to everyone.).\nWhen examining the utility, we take current strongest model - GPT-40 as an example, which is shown in Figure 5. We find in benefits category, the goal-reaching rate has a negative relation with truthfulness. A higher degree of truthfulness in the model results in a lower goal-reaching rate, with changes as significant as 15% relatively in response to the instructions. Conversely, in public image and emotion categories, where the scenarios often involve more subjective considerations, the changes in goal-reaching rate are smaller.\nOverall, our findings suggest that LLM-based agents' behavior can be greatly influenced by steering instructions, with stronger models showing more pronounced changes. This steer also impacts the goal-reaching rate, especially in scenarios where the utility is more objective. It is also important to note that the model still lies when explicitly prompted to be truthful.\nMoreover, we refer the reader to Appendix M for a qualitative example, in which the agent initially withholds information and becomes truthful when further asked. Along with the example in Figure 1, they demonstrate the need for an interactive evaluation for LLMs' truthfulness."}, {"title": "6 Conclusion & Discussion", "content": "In this work, we propose AI-LIEDAR, a framework to study how LLM-based agents navigate scenarios where achieving utility and maintaining truthfulness are in direct conflict. We evaluate this tradeoff using simulated interactions between an LLM agent and a user, in 60 scenarios crafted to cover diverse human motivations for lying drawing from psychology. Additionally, we designed a fine-grained evaluator to assess varying degrees of model truthfulness.\nOur experiments on 2160 simulated interactions show while current LLMs manage truthfulness and utility well in various scenarios, they still can lie in morally-charged situations. Ablation studies on information elements reveal that removing motivations for lying increases the truthfulness rate by about 40%, with a smaller change in the falsification rate, underscoring the need for fine-grained evaluation.\nFurthermore, even when steered to be truthful, there remains a risk of models lying, where stronger models are more steerable towards truthfulness. Notably, GPT-40, the strongest model currently, exhibits the largest change in falsification rate when steered towards falsification. As for the utility, changes in truthfulness have greater impact on scenarios with more quantifiable goals such as selling something. Our findings showcase the risk of model instructions that contain inherent conflicts between truthfulness and utility.\nWe discuss the implications of our findings below.\n6.1 The Cost of Being Truthful\nFrom the experiments in \u00a75.3, we observed that in a more objective goal setting (benefits dimension), being truthful comes at the cost of reduced goal-reaching ability. This observation raises concerns about the potential degradation of social goal achieving of RLHF annotation that prioritize truthfulness, learning from a goal-oriented story setting (Hong et al., 2023), or simply behavior cloning (Zhou et al., 2024a). Future research should explore strategies to optimize RL models to consider both dimensions effectively (Wu et al., 2023)."}, {"title": "6.2 The Steerability of LLMS", "content": "In \u00a75.3, we reveal that models can be steered towards or away from truthfulness. Such steerability raises concerns about the potential misuse of the model. As suggested by Wallace et al. (2024), implementing an instruction hierarchy that prioritizes high-level system prompts can mitigate risks. However, misuse by the model's owner remains a great safety concern. Future work could focus on developing robust safeguards or detection methods against misuse to prevent malicious steering from input injection or system prompt modification.\nFurthermore, the ability to fine-tune truthfulness levels adaptively is another critical area of exploration. Is there an efficient way to exert fine-grained control over truthfulness, allowing models to maintain a required level of honesty while still maximizing goal completion? Addressing this question could pave the way for more responsible and versatile use of LLMs in various contexts, where balancing ethical considerations with performance is crucial."}, {"title": "6.3 The Affect of Prompt Design", "content": "We made great efforts to create prompts that were as neutral as possible. Our findings in \u00a75.2 indicate that varying the components of information included in the prompt can significantly impact the truthfulness of the model's outputs. This aligns with findings by Anagnostidis and Bulian (2024), which suggest that both the content and the sequence of information presented in prompts can substantially influence model performance. Despite these efforts, it is important to acknowledge that our prompt design may still unintentionally introduce biases that could affect model behavior. Moreover, this study does not investigate the effect of system prompts on the truthfulness of the outputs. Future research should explore the influence of both user and system prompts to gain a deeper understanding of their effects on model performance and reliability."}, {"title": "6.4 To lie or not to lie - What is the right choice?", "content": "Our experiments reveal that models often navigate the tension between truthfulness and utility by equivocating, offering vague responses. However, as noted by Buller and Burgoon (1996), such responses are generally less helpful than a clear stance, as they lack informative value, whereas falsification, despite being deceptive, often provides more complete and useful information.\nWhile strategic deception can be justified in specific scenarios, it also poses significant ethical challenges. Importantly, not all truthfulness is beneficial, and determining when and how to responsibly use it is critical, and remains an open question in AI ethics. Another crucial aspect is determining who should decide when models are permitted to conceal information\u2014individuals, AI providers, or governments. This is a complex socio-technical issue that requires careful consideration.\nIn summary, our study contributes to the understanding of models' decision-making processes in ethical scenarios, but the broader question of when and how they should lie remains a complex, inter-disciplinary challenge."}, {"title": "7 Limitations and Ethical Considerations", "content": "We acknowledge several limitations and ethical considerations in this work.\n7.1 Limitations\nLack of Real User Engagement In our experiments, we only used \"human users\" simulated by LLMs. However, we would like to emphasize that the focus of this work is on AI agents, and prior research has demonstrated the promise of using LLMs to simulate users as a preliminary testing method for LLMs (Davidson et al., 2023; Aher et al., 2022; Xie et al., 2024). Additionally, we find that the conversations generated are realistic and reasonable, mitigating some concerns about the lack of real user engagement. Future work could definitely explore interactions with real users to further validate the findings and enhance the robustness of the evaluations.\nLimited Coverage of Scenarios Though our scenario broadly covers a wide range of utility-truthfulness dilemmas, our scenario set is still relatively small with only 60 human-curated scenarios. Additionally, there could be more fine-grained and sophisticated categorization. As noted by Erat and Gneezy (2010) and Cartwright et al. (2020), lies can be categorized in a finer-grained manner based on changes in payoffs (increase, no change, decrease). These types of lies occur in real-life situations, and including them in the scenario set could provide a more comprehensive understanding of how models balance utility and truthfulness.\nDespite such limitation, we hope our work serves as a starting point. Future research could aim to develop a more comprehensive and carefully designed set of scenarios to better capture the complexities of AI-human interactions.\nMachine-based Evaluation Similar to the approaches in (Zhou et al., 2024b,a), our analysis of goal completion rate relies on a zero-shot GPT-4 model. We acknowledge that evaluating goal completion can be challenging due to the subjective nature of some goals (e.g., what counts as \"maintaining the company's public image\"), which is also demonstrated in human annotation. This variability hinders a completely accurate measurement of truthfulness.\nDespite these challenges, we believe that, at a system or dataset level (i.e., running a sufficient number of experiments), these scores provide a reasonable indication of the model's performance in terms of goal achievement and truthfulness.\nRestricted Action Space Given the vast action space for LLM agents, verifying correctness in unrestricted information generation is challenging. To address this, we limited the agent's responses by instructing the simulated human to ask a specific question and seek a yes or no answer. While this helps identify truthful behavior, it doesn't fully reflect real-life situations, where people may not notice certain details without prompting.\nDistinguishing Hallucination Distinguishing between hallucinations and deceptive behaviors is challenging, especially with input-conflicting hallucinations, since we can't fully access the models' internal states. Although it's difficult to completely differentiate these phenomena, we've made efforts to minimize the impact of hallucinations.\nAs shown in Table 1, we craft simple and clear goals for each agent. Specifically, the second agent is instructed to obtain a direct yes or no answer about the negative information presented to the first agent. This \"double-check\" mechanism helps clarify the first agent's intentions, reducing the chances of confusing hallucinations with deception.\nTo better differentiate between these issues, future work could implement a \"whiteboard\" where the model can \"write down\" its thoughts (Scheurer et al., 2023; Jarviniemi and Hubinger, 2024). This would allow for better inspection of the model's intentions and help distinguish between honest mistakes and deliberate deception.\n7.2 Ethical Considerations\nOur findings carry a dual-use risk: they could potentially be used to create LLMs that are better at lying or partially lying. While we cannot control how others use our research, we strongly oppose any misuse that promotes deceptive practices. Therefore, we advocate for robust safeguards and ethical guidelines to prevent the exploitation of our findings for unethical purposes.\nFurthermore, the concept of truthfulness varies significantly across cultures (Park and Ahn, 2007; Tuckett, 2004), and even among individuals, as indicated by our human annotation study in \u00a74.2. This variability suggests that expectations of AI system truthfulness will also differ. Future research should explore these cross-cultural design considerations to ensure AI systems are aligned with diverse ethical standards and cultural norms."}]}