{"title": "Successes and Limitations of Object-centric Models at Compositional Generalisation", "authors": ["Milton L. Montero", "Jeffrey S. Bowers", "Gaurav Malhotra"], "abstract": "In recent years, it has been shown empirically that standard disentangled latent variable models do not support robust compositional learning in the visual domain. Indeed, in spite of being designed with the goal of factorising datasets into their constituent factors of variations, disentangled models show extremely limited compositional generalisation capabilities. On the other hand, object-centric architectures have shown promising compositional skills, albeit these have 1) not been extensively tested and 2) experiments have been limited to scene composition\u2014where models must generalise to novel combinations of objects in a visual scene instead of novel combinations of object properties. In this work, we show that these compositional generalisation skills extend to this later setting. Furthermore, we present evidence pointing to the source of these skills and how they can be improved through careful training. Finally, we point to one important limitation that still exists which suggests new directions of research.", "sections": [{"title": "Introduction", "content": "A hallmark of human intelligence is compositional generalisation, namely, our ability to perceive and comprehend novel combinations of familiar elements. For example, in the domain of vision, as long as we can perceive red triangles and blue squares, then we can also perceive blue triangles and green squares. This gives humans the ability to make \"infinite use of finite means\" [Von Humboldt et al., 1999, Chomsky, 2014, Smolensky, 1988, McCoy et al., 2021] and it is a key priority for AI to achieve human-like abilities. However, it has proven a challenge for neural network models of vision.\nIn the context of generative vision models, it was first hypothesized that disentangled representations based on the Variational Auto-Encoder (VAE) architecture could support such compositional generalisation abilities [Duan et al., 2020]. While this was a reasonable hypothesis, which had some preliminary experimental support [Higgins et al.], subsequent work showed that such results where not robust and they didn't extend across different levels of difficulty in generalisation [Montero et al., 2020, 2022].Furthermore these results extended to most popular architectures at the time in both supervised and unsupervised settings [Schott et al., 2021].\nRecent work on the other hand has shown that models which perform perceptual grouping\u2014i.e. which decompose images into constituent objects\u2014exhibit increased compositional generalisation capabilities [Singh et al., 2021, Frady et al., Wiedemer et al.]. However, these results typically focused on scene compositions i.e. generalisation to novel configurations of known objects in a scene. This is however, a fundamentally different challenge to the one posed by composition of different object properties, both intrinsic (like shape and color) and extrinsic ones(like position and rotation), since the former requires manipulating the relation between objects and the later the relation between their parts and how this is translated from proximal to distal representations [Pizlo, 2001]."}, {"title": "Object-level Compositional Generalisation in Object-centric Models", "content": "It was argued in Montero et al. [2022] that a possible cause for the failures at combinatorial generalisation described in Montero et al. [2020] and Schott et al. [2021] was that, despite learning disentangled representations, the generative models did not segment images into their constituent parts. Thus, when faced with novel combinations of properties that determine the same element of an image (e.g. novel combinations of shape and color), they could not manipulate the individual elements in an image in order to change the relevant property (e.g. the location) without affecting the representation of other properties."}, {"title": "Pushing the Generalisation Capabilities of Object-centric Models", "content": "To test whether the previous failures when known shapes are presented in novel rotations are the result of models not having access to the correct local features during learning, we create a dataset where all local features are trained and test whether the model succeeds when tested on equivalent conditions (excluded shape and rotation combinations). We designed said dataset using the Pentomino shapes: sets of five blocks arranged into different shapes which we vary along different factors of variation as in dSprites ((Figure 2), see Appendix B.1 for more details).\nNotice that in this dataset all low-level features are straight lines or right angles (Figure 5, panel b). Thus even when presented with a novel combination of shape and rotation as before, we can be more confident that the low-level features are not novel and the model only needs to respect the global configuration of the different parts of the shape. This is in contrast to dSprites where a novel rotation of a shape such as the heart requires reconstructing a novel local-feature (e.g. the small dip of the heart in a novel rotation is effectively a novel feature in the image frame of reference).\nTo facilitate our analysis we will test a simplified version of the SA architecture. In this version there is only one slot which must perform figure ground segmentation (hence we name it FgSeg). As in SA the model, it uses an attention mechanism, but instead of slots competing to explain patches of data, the latent must only integrate information about the foreground while ignoring the background. During reconstruction we use a simplified version of the SlotDecoder where instead of using a"}, {"title": "Compositional Generalisation Results", "content": "We train FgSeg on the Pentomino dataset, excluding half of the rotations for a 4 of the shapes (out of a total of 12). We then test the model on these excluded shapes as before. To control that the model performance is not only due to the qualitative differences in the dataset, we compare against a Wasserstein Auto-Encoder (WAE, Tolstikhin et al. [2017]) tested on the same generalisation condition (Figure 3).\nThe figure clearly shows that, while the WAE fails to reconstruct novel rotations of a known shape, the FgSeg model succeeds, which shows that a perceptual grouping model can solve previously challenging generalisation conditions given an appropriate dataset - one where novel combinations of factors do not imply the presence of novel local features in image space.\nA quantitative measure is presented in Table 2, showing that the qualitative examination is supported by the scores achieved by the model. In this case, training scores refers to validation on a randomly sampled held-out dataset. In all cases the FgSeg model achieves better scores than the WAE model, though the generalisation for property prediction, while better, still doesn't match the validation performance."}, {"title": "Extrapolation results", "content": "Given this success, what happens if we now test the model on an extrapolation condition\u2014where the model must reconstruct completely novel shapes? We show that, perhaps surprisingly, the model can also succeed when tested on reconstructing three novel shapes in the Pentomino dataset, effectively learning how to recombine the local features in the Pentomino dataset into potentially arbitrary shapes (Figure 4).\nWe quantify this success, showing how the FgSeg model's reconstruction scores change as we exclude more and more shapes from the training data. We see that between that there is no significant drop from 1 to three, and only after removing half of the shapes (6) do we start to see a drop in performance."}, {"title": "Discussion", "content": "We have shown that SA can solve compositional generalisation challenges that posed a significant issue for standard auto-encoder generative models. Furthermore, we have shown that failures in those datasets are likely due to the fact that some local features are effectively removed from the dataset when we exclude certain combinations, and that if we control for this fact, we can achieve generalsation for more complex factor combinations such as novel combinations of shape and rotation.\nTo our surprise, when trained and tested on this qualitatively different dataset, the model was even able to solve extrapolation tasks that require reconstruction of unseen shapes. This shows once a again that careful data curation can have a significant effect on model capabilities when combined with the right architecture. Our results also show that we may not require stronger inductive biases such as the ones introduce in Singh et al. [2022] to solve these tasks, and that separation into discrete tokens describing values of the properties is likely only needed for higher-level cognitive tasks (such as reasoning and planning)\nIndeed our follow-up experiments suggest that the models learn abstract representations of the concepts present in said dataset. When using the learned embeddings from FgSeg to fit a classification model for the shape property, we find that a linear classifier is unable to correctly solve the problem, and instead we must use a non-linear one such as a Support Vector Machine. Even then, such a classifier is unable to correctly classify the held-out combinations from using the test embeddings without jointly training both i.e. it does not generalise in the same way the reconstruction does (see Appendix D). Extracting or learning such higher level concepts using/from the representations produced by these simpler models is thus an interesting direction for future research.\nOne way to accomplish this could be to design architecture that incorporate more principles from the Gestalt theory of perception, of which FgSeg and and SA only implement a subset of, namely figure-ground segmentation and perceptual-grouping [Wagemans et al., 2012, Yantis, 2001, Treisman and Gelade, 1980]. Principles such as common-fate (the idea that the visual system is biased towards grouping together features that move together in time to the same object representation), could potentially further constrain the model and induce more general representations of shape, and idea that has been preeliminary explored in Tangemann et al. [2021]."}, {"title": "Limitations", "content": "We have presented our results using a single model. In our preliminary tests we trained several models on the generalisation conditions and found that there was no significant difference in performance for those baseline tests. As such, we conducted the rest of the experiments using only one seed per tests. Additionally, we also tested SLATE on the same conditions of dDsprites and 3DShapes and found that it performed qualitatively the same. As such we elected to not test it further as we believe it unlikely that it would perform differently.\nAnother limitation is that visualising the latent space provided no useful information. This is unsatisfying since raw scores tend to not provide a full picture of how the models are performing a given task."}, {"title": "Datasets", "content": "We first test SA models on hard combinatorial generalisation conditions such as the recombination-to-range ones identified in (Montero et al., 2022). Namely, we test the following conditions for the three datasets:\n\u2022 3DShapes: Contains the 6 generative factors: [floor hue, wall hue, object hue, scale, shape, orientation]. Colors are defined in the HSV format, and the values correspond to the hue component. Here orientation defines the angle of point-of-view for the scene. The objects themselves do not rotate. The hard condition in this case is all images such that [shape=pill, object-hue=> 0.5], which were excluded from the training set. Again, these are pills colored as any of the colors in the second half of the HSV spectrum. These colors (shades of blue, purple, etc) were observed on the other shapes, and the pill was observed with other colors such as red and orange. See Burgess and Kim [2018].\n\u2022 dSprites: Contains the following generative factors: [shape, scale, orientation, position X, position Y]. Orientation here refers to the rotation of the shape along it's center of mass. The hard condition is all images such that [shape=heart, rotation< 180], which were excluded from the training set. Thus, no squares rotated beyond the 180 are seen during training and the model must reconstruct them at test time. We also excluded redundant rotations for the training data since shapes such as the ellipsis and the square, unlike the heart, are less than 360\u00b0 symmetrical. See Matthey et al. [2017].\nNotice that excluding a combination of a shape with another factor means that said shape will not be seen the same number of times during training. For example in the dSprites condition defined above squares will be observed half as many times as the other two shapes. To ensure that shapes are observed an equal amount of times, we sample instances from the dataset with the following probabiliy:\n$p(x_i, g_i) = \\frac{1}{|{(x_j, g_j) \\in D_{\\text{train}} | g_j[\\text{shape}] == g_i[\\text{shape}]}|}$ (1)\nThis ensures that images in the training set that belong to the shape that is used in the generalisation condition (e.g. the square in dSprites) are seen as frequently as other shapes. Of course there will be less variation in the factor that is used to test said generalisation condition (e.g. position along the X-axis), however these have a larger number of values so they are less likely to be the cause of over-fitting."}, {"title": "The Pentominos Dataset", "content": "To adjudicate between these two views we unfortunately cannot rely on the dSprites dataset since the shapes used to generate it share few low-level features amongst them. Indeed the most prominent features (the curve of the ellipsis, the right angles of the square and the dip of the heart) are unique to each of them. Thus we introduce a new dataset based on the pentomino shapes to tackle this issue.\nThe pentomino shapes are simple, sprite-like shapes, composed of five equal side squares in different configurations that are joined edge-to-edge (see Figure 5.a). There are twelve such shapes in total"}, {"title": "Extra results", "content": "As in the previous section, we define a combinatorial generalisation condition that excludes some shape and rotation values:\n\u2022 Novel shape and rotation combinations: We exclude combinations such that [shape \u2208 {F, P, T, W}, rotation > 180\u00b0]. We selected these shapes so that there are a couple of shapes that are similar to other shapes in the training data at those rotations (T and Y, W and V or U), and other two (F and P) that are very distinct. The fist pair tests if the model will confuse when the rotation value is novel and the latter if it will be able to produce a reconstruction for which it is harder to interpolate. Four shapes also allows us to maintain a similar ratio of 1:3 excluded to included during training for the shape factor (4 of 12 excluded here vs 1 of 3 in dSprites).\nAs in the previous section, we remove the redundant rotations of the I, X and Z shapes and correct for the unbalance of presentations of the shapes as defined by Equation 1. We use the same architecture and training configuration that we used for dSprites for both FgSeg and WAE. We use the latter as a baseline. We also use this baseline to control for the increase in the amount of shapes with respect to dSprites. If FgSeg was to succeed in this new dataset, it could be argued that this is because having twelve shapes gives the model more opportunity to learn a proper representation of what constitutes a shape. A success at generalisation by FgSeg and failure from a baseline model would rule out this possibility, which means the former's success is much more interesting than if both succeed.\nThe results can be seen in Figure 6. Reconstructions are plotted along the circumference according to the ground truth rotation value. For simplicity we keep the other generative factors fixed at the midpoint value. In the case of FgSeg we see that the results are very impressive. The model shows no sign of confusing either to rotation or the shape of unseen combinations. Thus they clearly support the second view over the first one described in the introduction to this section: errors committed on novel shape-rotation combinations are due to decoder errors.\nIt is also clear that said improvement in generalization is not due to the increase in the number of shapes as the results for WAE show clear and systematic failures at generalisation, especially for rotations that are far from the ones observed during training, as should be expected."}, {"title": "Additional Extrapolation Results", "content": "We also test the model when excluding more than one shape from traning to test the degree to which variety in the training examples influences the quality of the learned representations/genertive mechanism:\n\u2022 Three novel shapes: We exclude all instances where [shape \u2208 {P,T, W}]. We include P and T because when we tested WAEs these were routinely confused with F and Y respectively. Thus it may increase the likelihood of SA failing by making the same mistakes.\n\u2022 Half novel shapes: We further increase the number of shapes by excluding inputs such that [shape \u2208 {F, P, N, T, V, W}] . We just add F, V, Z to make it 6 out of 12 shapes excluded.\nWe also including raw reconstruction errors for all three conditions, including the one in the main text (Figure 4).\nOn the other hand, removing more shapes does produce significant degradation in the reconstruction quality. The results for this condition can be observed in Figure 8. For some shapes such as W we"}, {"title": "Testing predictivity of learned representations", "content": "Are the learned representations high level? In other words does the model learn concepts related to each shape and color or does it solve the task using simpler, lower level representations? Unlike disentangled VAEs it is not easy to directly visualise the latent representations of these models. Instead we turn to prediction of the different concepts (such as the shape classes) as a way to assess this. If models learn representations that are abstract, we should be able to use them to predict the classes for both training and held out samples. We test this below. First of all, we see that models"}]}