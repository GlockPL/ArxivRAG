{"title": "Efficient Arbitrary Precision Acceleration for Large Language Models on GPU Tensor Cores", "authors": ["Shaobo Ma", "Chao Fang", "Haikuo Shao", "Zhongfeng Wang"], "abstract": "Large language models (LLMs) have been widely applied but face challenges in efficient inference. While quantization methods reduce computational demands, ultra-low bit quantization with arbitrary precision is hindered by limited GPU Tensor Core support and inefficient memory management, leading to suboptimal acceleration. To address these challenges, we propose a comprehensive acceleration scheme for arbitrary precision LLMs. At its core, we introduce a novel bipolar-INT data format that facilitates parallel computing and supports symmetric quantization, effectively reducing data redundancy. Building on this, we implement an arbitrary precision matrix multiplication scheme that decomposes and recovers matrices at the bit level, enabling flexible precision while maximizing GPU Tensor Core utilization. Furthermore, we develop an efficient matrix preprocessing method that optimizes data layout for subsequent computations. Finally, we design a data recovery-oriented memory management system that strategically utilizes fast shared memory, significantly enhancing kernel execution speed and minimizing memory access latency. Experimental results demonstrate our approach's effectiveness, with up to 13x speedup in matrix multiplication compared to NVIDIA's CUTLASS. When integrated into LLMs, we achieve up to 6.7\u00d7 inference acceleration. These improvements significantly enhance LLM inference efficiency, enabling broader and more responsive applications of LLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, large language models (LLMs) based on the Transformer [33] architecture have been rapidly developed and gained popularity, with the GPT family [2, 3, 26, 28, 29] being one of the most prominent examples. These models have demonstrated remarkable performance across a wide range of natural language tasks. However, the increasing size and complexity of LLMs pose significant challenges for efficient inference. To address this issue, various acceleration methods [7, 9, 10, 19, 31, 36] have been proposed, among which model quantization [7, 10, 13, 16, 20, 30, 36] has emerged as a promising approach. By reducing the data bit-width, quantization explicitly decreases memory consumption and computation time while minimizing the impact on the overall model structure and performance.\nAs quantization methods continue to evolve, the quantization of LLM models has reached ultra-low bit-widths [20, 30], such as 2-bit and 3-bit. However, the mismatch between these advanced quantization methods and the capabilities of mainstream GPU devices leads to several challenges in practical deployment [8, 17, 21], which manifests in the following aspects:\n1) Limited data format support in GPU Tensor Cores (TCs). TCs are the key hardware components in modern GPUs for accelerating matrix multiplications (MatMuls), which dominate the operations in LLMs. Starting with the Turing architecture, Nvidia GPU TCs support some low-precision tensor operations, such as INT1 and INT4, and even bit-level operations [18]. However, TCs still lack support for certain data formats that are widely used by ultra-low-bit quantized LLMs, such as INT2 [15] and INT3 [10]. When deploying these models on GPUs, the low-bit quantized data needs to be converted into higher-bit data formats supported by TCs for computation. This data format conversion introduces additional computational overhead, preventing some ultra-low-bit quantized LLMs from achieving optimal inference acceleration on GPU platforms.\n2) Inefficient GPU memory management schemes. When deploying LLMs on GPUs, performance bottlenecks arise not only from MatMuls but also from data access. GPUs employ a multi-level memory hierarchy [24], with different levels varying in capacity and bandwidth. Effective memory management significantly impacts data access latency. For example, by carefully coordinating the use of different GPU memory levels, such as texture and shared memory, memory-bound applications can achieve significant speedup up to 14.8x, compared to unoptimized implementations [39]. Consequently, some optimization schemes that focus solely on MatMuls while neglecting the importance of GPU memory management fail to achieve the desired performance improvements in practice and may even result in slower execution times than the original, unoptimized one [4].\nTo address these challenges, we first propose a novel data format called bipolar-INT, which features a symmetric range and eliminates redundant sign bits, thereby reducing redundancy and facilitating parallel computing. Secondly, to overcome the limited precision support of GPU TCs, we achieve arbitrary precision MatMul through bit-level decomposition and recovery of matrices, saving memory while maintaining flexibility. Next, we introduce an efficient matrix"}, {"title": "2 BACKGROUND", "content": "The rapid development of LLMs has led to a significant increase in model size and computational complexity, posing challenges for efficient inference. To address this issue, model quantization [7, 10, 22, 37] has emerged as a promising approach, aiming to reduce the computational complexity and memory footprint of LLMs.\nEarly LLM quantization methods focused on traditional techniques such as FP16 optimization [26] and INT8 quantization. For example, GPT3.INT8()[6] addressed the issue of outliers in LLMs' quantization by employing mixed-precision computation. However, to further push the performance frontier, more aggressive quantization methods with lower bit-widths have been proposed, such as QLoRA[7] with 4-bit, GPTQ [10] with 3-4 bit, SqueezeLLM [16] with 3-bit, TSLD [15] with ternary, and OneBit [37] with binary quantization for LLMs. Despite the promising results achieved by these ultra-low bit quantization methods, their optimal inference performance on GPUs has been hindered by the lack of suitable data formats supported by GPU hardware. This limitation calls for efficient acceleration designs that can bridge the gap between the quantization methods and the available GPU hardware capabilities, enabling LLMs to fully benefit from the reduced precision and computational complexity offered by ultra-low bit quantization."}, {"title": "2.1 Ultra-Low Bit Quantized LLMs", "content": "The rapid development of LLMs has led to a significant increase in model size and computational complexity, posing challenges for efficient inference. To address this issue, model quantization [7, 10, 22, 37] has emerged as a promising approach, aiming to reduce the computational complexity and memory footprint of LLMs.\nEarly LLM quantization methods focused on traditional techniques such as FP16 optimization [26] and INT8 quantization. For example, GPT3.INT8()[6] addressed the issue of outliers in LLMs' quantization by employing mixed-precision computation. However, to further push the performance frontier, more aggressive quantization methods with lower bit-widths have been proposed, such as QLoRA[7] with 4-bit, GPTQ [10] with 3-4 bit, SqueezeLLM [16] with 3-bit, TSLD [15] with ternary, and OneBit [37] with binary quantization for LLMs. Despite the promising results achieved by these ultra-low bit quantization methods, their optimal inference performance on GPUs has been hindered by the lack of suitable data formats supported by GPU hardware. This limitation calls for efficient acceleration designs that can bridge the gap between the quantization methods and the available GPU hardware capabilities, enabling LLMs to fully benefit from the reduced precision and computational complexity offered by ultra-low bit quantization."}, {"title": "2.2 GPU Hierarchy and Tensor Core", "content": "GPUs have become essential for AI workloads due to their highly parallel computing structure. Modern GPUs feature a multi-level memory hierarchy, including global memory, shared memory, registers, and caches (L1 and L2), each with different sizes and access speeds. Global memory, the largest and slowest, is accessible by all threads, while shared memory, though smaller, is faster and accessible within a block, reducing latency. Registers, the fastest memory component, store frequently accessed variables for individual threads. L1 and L2 caches further expedite data access, with L1 being faster and L2 larger and shared among cores [1, 14, 23].\nTo accelerate deep learning workloads, NVIDIA introduced TCs in their GPUs. Optimized for MatMuls, a critical operation in LLMs, TCs leverage the massive parallelism of GPUs to significantly improve computational efficiency and inference performance [1, 14, 23, 25, 35]. However, while TCs support low-precision data formats like INT1, INT4, and INT8, they lack support for efficient arbitrary precision operations. This limitation hinders the efficient acceleration of ultra-low bit quantized LLMs, necessitating a novel acceleration scheme that fully utilizes TCs to perform quantized LLMs with arbitrary precision [8]."}, {"title": "2.3 Arbitrary Precision Acceleration Schemes", "content": "Arbitrary precision acceleration schemes for data formats lower than INT8 have been extensively studied [5, 8, 11, 12, 17, 18, 27, 32, 34, 38, 40] to optimize inference performance and maintain model accuracy for diverse application requirements. In addition to the commonly supported precisions on modern GPUs (e.g., INT1, INT4), these designs often incorporate a broader range of precisions such as INT2, INT3. Among these schemes, APNN-TC [8] supports arbitrary precision, HAQ [34] employs 1-8 bits, while BSTC [17] and BTC [18] focus on 1-bit precision. These works designed Mat-Mul kernels supporting various precisions and integrated them into quantized neural networks to enhance performance. However, they have limitations in terms of incomplete support for arbitrary precision, as exemplified by APNN-TC not supporting W3A4, and suboptimal performance for large matrix parameters due to un-suitable data formats and inefficient GPU memory management. To address these issues, our work aims to identify a more suitable data format for arbitrary precision MatMul, implement kernel support for true arbitrary precision, and further optimize MatMul performance through improved memory scheduling."}, {"title": "3 ARBITRARY PRECISION MATMUL", "content": "This section presents the arbitrary precision integers MatMul framework implemented on TCs. First, we introduce an efficient data format, namely bipolar-INT, and demonstrate its advantages over traditional signed and unsigned integers for TCs' deployment. Next, we propose a bit-wise MatMul decomposition method to separate the operands bit by bit based on our bipolar-INT, utilizing the bit-wise computing kernel supported by TCs, and further develop the data recovery dataflow."}, {"title": "3.1 Bipolar-INT Data Format", "content": "We introduce a novel and efficient data format called bipolar-INT for arbitrary precision MatMul computations. Compared to original signed and unsigned integers, bipolar-INT is more suitable for LLM quantization and parallel computing due to its symmetric range and unified operations, making it particularly advantageous for deployment on TCs.\nAs shown in Fig. 1, the key difference between bipolar-INT and traditional integers lies in the interpretation of each bit. In traditional integers, each bit except the sign bit is valued as 0 or 1, whereas in bipolar-INT, the \"0\" is interpreted as \"-1\" in calculations, allowing each bit to be either -1 or 1. Specifically, for an n-bit bipolar-INT data $x = x_{(n-1)}, ..., x_{(1)}, x_{(0)}$, its decimal value can be obtained by\n$(x) = \\sum_{i=0}^{n-1}(2x(i) -1) \\cdot 2^i$. \n\nFor signed INT quantization, as illustrated in Fig. 1, due to the use of two's complement arithmetic, the sign of the MSB matrix after decomposition is opposite to the signs of the other bits. This requires separate handling during MatMul and matrix reconstruction, which is highly unfavorable for the parallel computation of single-bit matrices at each bit. Similarly, for unsigned INT quantization, the presence of an additional zero-point offset introduces extra multiply-accumulate operations during MatMul, which is detrimental to the optimization of MatMul.\nMoreover, for binary quantized neural networks, weights (W) are often quantized to values of either -1 and 1, which are encoded as 1-bit 0 and 1, respectively. If the feature matrix X is quantized to (0,1) with its decimal value also being (0,1), then the MatMul of W and X will be inconsistent during computation. Conventional methods like APNN-TC[8] introduce an additional all-ones matrix $J = [1, 1]$ to tackle this problem. The matrix W is decoded as $W = [0, 1]$, which relates to its actual value by $W = 2W \u2013 J$. Thus, the MatMul becomes $WX = 2WX-JX$, which not only introduces an additional matrix J occupying memory, but also introduces an extra MatMul operation JX. In contrast, the binary quantized W aligns perfectly with our 1-bit bipolar-INT format. Therefore, when using bipolar-INT for MatMuls in binary quantized neural networks, the feature matrix X is also quantized using bipolar-INT, without introducing additional matrices or MatMuls for precise computation."}, {"title": "3.2 Bit-Wise MatMul Reconstitution", "content": "A bit-wise MatMul reconstitution method is further proposed based on our bipolar-INT format. This method consists of three main steps: data decomposition, 1-bit MatMul, and data recovery. Fig. 2 illustrates the complete computation process of arbitrary precision MatMul using our proposed method. In this example, we consider a MatMul operation involving 2-bit matrices W and X, ultimately calculating a 32-bit output Y = WX.\nThe data decomposition step involves separating the operands bit by bit, enabling the utilization of the bit-wise computing kernel supported by TCs. Specifically, W and X are decomposed into two matrices each, denoted as $W^{(i)}$ and $X^{(j)}$, respectively. After the data decomposition, pairwise 1-bit MatMul operations are performed on the decomposed operands. Nvidia GPUs support the selection of either AND or XOR logic for 1-bit MatMul operations within TCs. By computing the pairwise MatMul of $W^{(i)}$ and $X^{(j)}$, we obtain a 32-bit intermediate result matrix $Y^{(i,j)}$ for each pair of bits. To reconstruct the final result from the intermediate bit-wise computations, a data recovery dataflow is employed. This step involves reconstructing the output matrix Y from the intermediate results $Y^{(i,j)}$ by shifting each $Y^{(i,j)}$ based on its corresponding bit positions (i, j) and then summing up all the shifted matrices.\nAlthough the example in Fig. 2 involves 2-bit matrices, the same principles can be extended to arbitrary bit widths. Our bit-wise MatMul decomposition method, based on the bipolar-INT format, effectively utilizes the bit-wise computing capabilities of TCs, providing a flexible and efficient solution for arbitrary precision Mat-Mul operations."}, {"title": "4 GPU MEMORY SCHEDULING", "content": "This section focuses on how to efficiently transfer data between different levels of the memory hierarchy in the arbitrary precision MatMul kernel to maximize processing speed."}, {"title": "4.1 Matrix Decomposition and Reassembly", "content": "We present a matrix decomposition and reassembly strategy to efficiently transfer data between different levels of the memory hierarchy in the arbitrary precision MatMul kernel. This strategy aims to reduce memory access redundancy and maximize data transfer speed by preprocessing the original n-bit INT matrix.\nAs shown in Fig. 3, GPUs support more precisions than TCs when accessing memory, but they still do not support all possible precisions. For example, a 3-bit INT type does not have a suitable storage format and must be stored using a wider data format (such as 4-bit or 8-bit), introducing redundant memory access overhead. Moreover, when the amount of data to be accessed is large, not only the storage precision support but also the suitability of the corresponding data format for data transfer must be considered.\nTo address these issues, our strategy involves three steps, as illustrated in Fig. 3. In Step 1, we perform 1-bit decomposition on the original matrix, breaking down each bit and regrouping them with corresponding bits from other data to form n 1-bit matrices. This step circumvents the issue of unsuitable data formats and eliminates memory redundancy caused by the lack of appropriate data formats. Next, in Step 2, we reassemble the decomposed data using 32-bit unsigned INTs. This step ensures that the input data aligns with the GPU's native support, thereby enhancing data transfer speed. Finally, in Step 3, we sequentially concatenate the processed n matrices into a single matrix. This step not only further conserves memory but also simplifies n data transfer instructions into a single instruction. Although the amount of data transferred remains the same, this concatenation improves data transfer speed and saves storage space. By following these steps, our matrix decomposition and reassembly strategy effectively reduces memory access redundancy and maximizes data transfer speed in the arbitrary precision MatMul kernel."}, {"title": "4.2 Recovery-Oriented Memory Scheduling", "content": "A recovery-oriented memory scheduling strategy is proposed to optimize data transfer and memory access in the arbitrary precision MatMul kernel on GPUs by performing the matrix recovery process in shared memory or fragments, thereby reducing global memory access and computation, and further accelerating the kernel's computation speed.\nAs shown in Fig. 4, when implementing the arbitrary precision MatMul described in Sec. 3 on a GPU, we perform 1-bit MatMul on the decomposed matrices to obtain intermediate result matrices. These intermediate result matrices are then shifted and summed to get the final result. In a naive approach, each streaming multiprocessor (SM) directly multiplies a pair of 1-bit matrices decomposed from the weights and features, and the MatMul result is directly returned to global memory for recovery. However, this design leads to each SM obtaining at most one intermediate result matrix, forcing the final matrix recovery to be performed in the slower global memory. This step introduces significant delays because accessing and processing data in global memory is much slower compared to shared memory.\nTo address this issue, our recovery-oriented memory scheduling strategy aims to compute all intermediate result matrices within a single SM, as shown in \u2460 of Fig. 4. This requires each SM to compute all bitwise combinations of the weight and feature matrices. To efficiently manage shared memory, given its limited size, we divide the output matrix into blocks of size $b_m \\times b_n$, with each SM responsible for computing the data within one block. If the number of blocks exceeds the number of SMs, the SMs are iteratively called to perform the computations.\nDue to the insufficient size of shared memory, the dimension K needs to be partitioned. Each time, the SM only reads data from two matrices of size $n_{w,x} \\times b_m, n \\times b_k$, and the results of each computation are accumulated over $K/b_k$ iterations to produce the complete output value. In shared memory, the input weight and feature matrices of different bits are concatenated into two matrices and input into the Fragment to call the TC to perform 1-bit matrix multiplication. The resulting $n_{w} b_m \\times n_{x} b_n$ matrix contains all the data needed to recover a $b_m \\times b_n$ output block, as shown in \u2461 of Fig. 4. By sending these data back to shared memory for data recovery, we can obtain part of the final output directly, without involving global memory in the computation.\nTo hide the data transfer latency from global memory to shared memory, we allocate two blocks of shared memory of the same size, as shown in \u2462 of Fig. 4. While one block is responsible for computation, the other block reads the next set of data. They then alternate in this manner, effectively overlapping data transfer and computation. Furthermore, to increase data reuse and reduce latency, we allow each Fragment to read the weight matrix of the same bit and all bits of the feature matrix, calculating all intermediate results corresponding to this bit of the weight matrix, as shown in \u2463 of Fig. 4. This way, we can perform the feature part of the data recovery in the Fragment, leaving the weight part of the recovery for shared memory computation."}, {"title": "5 EXPERIMENTAL RESULTS", "content": "In this section, we evaluate the performance of our arbitrary precision acceleration method for LLMs. Our experiments are conducted on an NVIDIA RTX 3090 GPU within an Ubuntu 18.04 system, using CUDA-11.8 and CUTLASS-2.11. The evaluation is divided into two main parts: (1) an assessment of our arbitrary precision MatMul kernels, and (2) an analysis of its impact on LLM inference performance. Through these experiments, we aim to demonstrate the effectiveness of our approach in accelerating LLM computations across different precision levels."}, {"title": "5.1 Arbitrary Precision Kernel Evaluation", "content": "In this subsection, we evaluate our arbitrary precision MatMul kernel design on both square and LLM-specific MatMul tasks, validating the effectiveness of our redundancy reduction and memory management techniques. We compare our designs, including W1A2 (1-bit weights, 2-bit activations), W2A2, and W3A4, with standard FP32 and FP16 MatMuls. We then benchmark against NVIDIA's Tensor Cores-accelerated ultra-low bit MatMul (CUTLASS INT1 and CUTLASS INT4) to showcase our superior computational performance when using the same TCs. Finally, we compare with other TC-based MatMul acceleration techniques, such as APNN-TC [8], BSTC [17], and BTC [18], to demonstrate the performance advantages of our work in low-bit arbitrary precision acceleration."}, {"title": "5.1.1 Square MatMul Performance.", "content": "To demonstrate the computational performance of our proposed MatMul design, we first evaluated its speed on square matrices of various sizes. Table 1 presents the comparison of our work against PyTorch floating-point MatMul and CUTLASS for large MatMuls. The reported latency represents the mean execution time over 1000 iterations, and the speedup is calculated relative to FP32 MatMul under identical conditions.\nCompared to FP32 and FP16, our MatMul design shows increasing speedup as matrix size grows. For 4k\u00d74k matrices, our W1A2 configuration achieves a remarkable 193\u00d7 speedup over FP32 and about 70\u00d7 over FP16. When compared to CUTLASS, which is limited to int1 and int4 precisions due to TC constraints, our design demonstrates superior performance. For 4k\u00d74k matrices, our W1A2 configuration outperforms CUTLASS INT4 by more than 13x. Notably, both our W1A2 and W2A2 configurations surpass CUTLASS INT1 in performance, despite not having a bit-width advantage. Our W1A2 configuration is 5.5\u00d7 faster than CUTLASS INT1, while our W2A2 configuration is 3.5\u00d7 faster. Even our W3A4 configuration, which has a significantly wider bit-width, approaches CUTLASS INT1 performance, achieving approximately 88% of its speed."}, {"title": "5.1.2 LLM-specific MatMul Performance.", "content": "To more accurately reflect the computational demands of LLMs, we evaluate our MatMul designs on non-square matrices commonly found in these models. We extracted the MatMul parameters from each layer of the Llama2-7B model, providing a real-world benchmark for our evaluation.\nTable 2 presents the performance comparison for the three most computationally intensive MatMul operations in Llama2-7B. These operations have the largest number of parameters and the greatest impact on inference time. The reported latency represents the mean execution time over 1000 iterations, and the speedup is calculated relative to FP32 MatMul under identical conditions.\nOur results show that while the acceleration effect slightly decreases for non-square matrices compared to square matrices, our approach still demonstrates outstanding performance. For instance, our W1A2 configuration achieves speedups of 91.2x, 98.1x, and 102x for the three matrix sizes tested, significantly outperforming both FP16 and CUTLASS implementations. Notably, our W1A2 and W2A2 configurations surpass CUTLASS INT1 despite its bit-width advantage, while our W3A4 configuration outperforms CUTLASS INT4. This superior performance can be attributed to our matrix-splitting strategy that minimizes data redundancy and our efficient memory management scheme."}, {"title": "5.2 Arbitrary Precision LLM Evaluation", "content": "Efficient inference of large language models (LLMs) is crucial for their practical deployment. We evaluate our arbitrary precision MatMul design in ultra-low bit quantized LLMs to demonstrate its effectiveness in accelerating real-world LLM inference.\nWe focus on three widely studied LLMs: Llama2-7B, OPT-6.7B, and BLOOM-7B. By replacing their standard MatMul operations with our arbitrary precision MatMul kernel, we seek to quantify the performance gains across different model architectures. Our comparison encompasses several state-of-the-art methods: half-precision (FP16) models running on PyTorch, QLoRA [7], GPTQ [10], and OneBit [37]. To ensure a fair comparison, we align our design configurations with these methods: W1A1 for OneBit models, W2A2 for 2-bit quantized GPTQ models, and W4A4 for 4-bit quantized GPTQ models.\nFig. 7 presents the inference speedup for various quantization methods across the three LLMs, using FP16 as the baseline. The results reveal that our work achieves superior acceleration compared to CUTLASS under equivalent bit-width configurations, demonstrating up to nearly 2\u00d7 speedup. Moreover, when compared to PyTorch's FP16 implementation, our approach exhibits an impressive 6x speedup. Our analysis indicates that while QLoRA excels in reducing memory usage, its inference speed is compromised due to the necessary precision restoration step. GPTQ, despite offering 2-4 bit quantization, faces limitations imposed by GPU TC precision support. Its reliance on CUTLASS 4-bit as the kernel leads to efficiency trade-offs and suboptimal performance improvements. In contrast, our MatMul scheme demonstrates a 1.2-2x faster model inference speed compared to OneBit (W1A1), attributable to our more efficient memory scheduling."}, {"title": "6 CONCLUSIONS", "content": "In this paper, we present a comprehensive scheme to accelerate inference of ultra-low bit quantized large language models (LLMs) on GPU. We first introduce a data format called bipolar-INT, which is suitable for symmetric quantization and conducive to parallel computation. On top of bipolar-INT, we propose an INT MatMul method applicable to arbitrary precision and design an efficient memory management system. Our evaluations demonstrate significant performance gains in both MatMul operations and LLM inference. For MatMul, our implementation achieves a 5.45\u00d7 speedup compared to Nvidia's CUTLASS and up to 43\u00d7 speedup over existing solutions. In LLM inference, our approach yields 3.9-6.7\u00d7 speedup over FP16 models and 1.2-2x speedup compared to ultra-low bit quantized models using CUTLASS as the kernel."}]}