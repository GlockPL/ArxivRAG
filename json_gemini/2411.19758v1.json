{"title": "LaVIDE: A Language-Vision Discriminator for Detecting Changes in Satellite Image with Map References", "authors": ["Shuguo Jiang", "Fang Xu", "Sen Jia", "Gui-Song Xia"], "abstract": "Change detection, which typically relies on the comparison of bi-temporal images, is significantly hindered when only a single image is available. Comparing a single image with an existing map, such as OpenStreetMap, which is continuously updated through crowd-sourcing, offers a viable solution to this challenge. Unlike images that carry low-level visual details of ground objects, maps convey high-level categorical information. This discrepancy in abstraction levels complicates the alignment and comparison of the two data types. In this paper, we propose a Language-VIsion Discriminator for dEtecting changes in satellite image with map references, namely LaVIDE, which leverages language to bridge the information gap between maps and images. Specifically, LaVIDE formulates change detection as the problem of \"Does the pixel belong to [class]?\", aligning maps and images within the feature space of the language-vision model to associate high-level map categories with low-level image details. Moreover, we build a mixture-of-experts discriminative module, which compares linguistic features from maps with visual features from images across various semantic perspectives, achieving comprehensive semantic comparison for change detection. Extensive evaluation on four benchmark datasets demonstrates that LaVIDE can effectively detect changes in satellite image with map references, outperforming state-of-the-art change detection algorithms, e.g., with gains of about 13.8% on the DynamicEarthNet dataset and 4.3% on the SECOND dataset.", "sections": [{"title": "1. Introduction", "content": "Change detection [3, 13, 32] plays a pivotal role in applications such as urban planning [25, 37], environmental monitoring [24, 33] and disaster assessment [49, 56], among others. Prior studies [7, 8, 17, 53, 57, 58] primarily focuses on detecting changes in satellite images by comparing them to their pre-change counterparts. However, the absence of pre-change images, due to factors such as cloud cover interference during acquisition or storage limitation, raises \u201cLa Vide\" problem: how to effectively observe changes on the Earth's surface with a single image? Alternatively, map data such as OpenStreetMap, which is well-maintained through crowd-sourcing and easily accessible, records land cover types and geometric information. Comparing maps and images for change detection, referred to as map-image change detection, offering a more direct approach than traditional methods relying on pre-change images. Nonetheless, maps convey high-level categorical information about ground objects, which contrasts with the low-level visual information in images. Bridging this information gap between maps and images poses a significant challenge in detecting changes through map-image pairs.\nAn intuitive solution to this challenge is to first determine the semantic label of each pixel in newly acquired images and then compare it with the pre-change maps [1, 21, 38, 46], as shown in Fig. 1(a). In other words, it addresses the question, \"What is the semantic category of each pixel?\" referred to as category discrimination. However, by dividing change detection into two independent stages, it may fail to effectively learn features that are specifically beneficial for identifying changes, potentially leading to error propagation. An alternative approach involves converting the map into an image format, i.e., using different colors to represent various semantic categories, then applying cross-modal change detection methods [12, 19, 23, 28, 29, 35, 61], as illustrated in Fig. 1(b). This transforms the problem into \"Are the two pixels visually similar?,\" referred to as vision discrimination. Such methods identify changes according to visual similarity in map-image pairs, which are susceptible to the inherent visual discrepancies between these two types of images. Moreover, using colors to represent maps may result in the loss of essential attribute information, negatively impacting the accuracy of the results. Conversely, using language to represent maps maintains the integrity of categorical information and remains unaffected by visual perceptions. It allows us to formulate change detection as the problem of \"Does this pixel belong to [class]?\", referred to as language-vision discrimination, as illustrated in Fig. 1(c). It facilitates a more straightforward resolution to associate high-level map categories with low-level image details, and consequently enhancing the detection process.\nIn this work, we design a Language-VIsion Discriminator for detecting changes in satellite image with map references, termed LaVIDE, which leverages language to bridge the information gap between maps and images. Specifically, LaVIDE is structured with two parallel branches that separately encode maps and images. In the map branch, we utilize language to indicate ground objects, converting a map into a textual representation, and subsequently extract text embedding with the text encoder of the language-vision model. To enrich the categorical information of the map, we design an object context optimization strategy that refines textual embedding by incorporating object attributes. For the image branch, LaVIDE relates the hierarchical encoder with the feature space of language-vision models, ensuring the semantic alignment of vision embedding with text embedding. After that, we compare text embedding with vision embedding using a Mixture-of-Experts (MoE) discriminative module, comprehensively comparing their semantic differences from various perspectives for change detection.\nTo sum up, the contributions of this work are three-fold:\n\u2022 Language is introduced to bridge the information gap between maps and images, which can effectively preserve the high-level categorical information of the map, facilitating detailed comparisons with the low-level visual information in images.\n\u2022 A novel map-image change detection network, LaVIDE, is proposed to align the map and image embeddings into the feature space of the language-vision model for homogenization. And a MoE discriminative module is designed to comprehensively compare map semantics with image semantics across various perspectives.\n\u2022 Extensive experiments show that the proposed method can effectively detect changes in satellite image with map references, achieving state-of-the-art results across four benchmark datasets."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Bi-Temporal Change Detection", "content": "Bi-temporal change detection [11, 35, 61] refers to the process of identifying changes between two satellite images captured at different times over the same geographical area. It typically utilizes a binary matrix to represent changed and unchanged regions between the bi-temporal images. Early research in this field relies heavily on human expertise for hand-crafted feature engineering [5, 6, 20, 31] to identify pixel differences. However, these methods are highly susceptible to variations in illumination and seasonal conditions, often misinterpreting differences caused by external factors as actual changes, leading to a high rate of false alarms [41]. In recent years, learning-based methods have shown great progress in detecting changes owing to their powerful feature extraction capabilities. They typically assume that images captured at different times follow the same distribution, thereby using siamese network, i.e., a shared feature extraction backbone, to process bi-temporal images. For instance, FC-Siam-conc [13] employs a siamese extension of fully convolutional networks to extract multi-scale features from bi-temporal images, subsequently fusing these features hierarchically for change detection. However, since maps and images are heterogeneous"}, {"title": "2.2. Cross-Modal Change Detection", "content": "Cross-modal change detection [10] aims to compare images from different modalities, such as synthetic aperture radar (SAR) and optical images [34, 44], by associating cross-modal features to achieve homogenization and identify changed regions. Recent studies [12, 19, 23, 28, 29] primarily achieve homogenization through two principal approaches: at the pixel level and at the feature level. At the pixel level, homogenization is typically accomplished using modality conversion techniques, such as logarithmic transformations [52] and generative adversarial networks [39], to translate images from one modality to another, followed by the application of bi-temporal change detection methods. At the feature level, dual model-specific encoders are trained utilizing domain adaptation [30, 54] and contrastive learning methods [36, 45], among others, to map data from different modalities into a unified feature space, facilitating semantic comparisons.\nMaps [2, 26, 48] document the shapes, boundaries, and categories of objects, providing an abstracted form of information that contrasts with images, which convey natural attributes like color and texture. To homogenize maps and images, some works transform images into map-like data through semantic recognition techniques, e.g., semantic segmentation [1, 21] and image-to-image transformation [38, 46], allowing for direct comparison with maps. The effectiveness of change detection in these methods is constrained by the accuracy of the transformation process, as errors can propagate to subsequent stages due to its two-step nature. To address these issues, some works [4, 9, 15, 27] transform maps into image-like data by using color or one-hot encoding to represent ground objects, facilitating change detection through cross-modal methods. By jointly training the entire change detection process under change label supervision, these methods better integrate feature extraction and comparison, achieving more accurate results. However, change detection based on visual comparison can be hindered by inherent visual differences between maps and images, hindering progress in the field. In this paper, we boost change detection by leveraging language to bridge the information gap between maps and images."}, {"title": "3. Methodology", "content": ""}, {"title": "3.1. Problem Statement", "content": "Given a satellite image I \u2208 \\mathbb{R}^{H \\times W \\times 3} and a pre-change map M \u2208 \\mathbb{R}^{H \\times W \\times K}, where H and W denote the height and width respectively, and K indicates the number of distinct ground object categories, the task of change detection aims at identifying changes in the satellite image with respect to the pre-change map, i.e., generating a change map B \u2208 {0,1}^{H \\times W} with each pixel indicating whether a change occurs. Generally, this task is addressed through a homogenization process, which transforms the inputs into a consistent representation, followed by a change detection process that compares the homogenized data to identify changes.\nCategory discrimination. The basic strategy involves inferring a semantic map from the satellite image I to align with the pre-change map, M, enabling a direct comparison:\nB = d(M, F_{seg}(I))\nwhere F_{seg}(\u00b7) denotes the function responsible for generating the semantic map, typically implemented as a semantic segmentation network, and d(\u00b7) represents a comparison operator based on category equivalence. The effectiveness of change detection is critically dependent on the accuracy of the semantic map generated by F_{seg}(.). Segmentation errors, including misclassifications and boundary inaccuracies, can substantially compromise the accuracy of the change detection process.\nVision discrimination. By converting the map into a visually comparable format via a map visualizer \\gamma(\u00b7), cross-modal change detection techniques can be employed to assess differences directly, i.e., encoding the inputs separately using vision encoders F_{{M,I}} (\u00b7) for homogenization:\nB = H_{vd}(F_{M}(\\gamma(M)), F_{I}(I))\nwhere H_{vd}(\u00b7) is the change detection operation based on visual features from the map and image. Specifically, \\gamma(\u00b7) typically enhances visual comparability by assigning a unique color to each category in the map. However, since using customized colors to indicate categories fails to reflect the realistic characteristics of ground objects, it loses high-level semantic information in map data and incurs a domain gap, increasing the difficulty of semantic discrimination.\nThus the main obstacles to boosting change detection performance are two-fold:\n\u2022 The map encoding should carry high-level semantic information of ground objects, facilitating feature extraction.\n\u2022 The model should relate map categories to image details, reducing the map-image gap for change detection.\nLanguage-vision discrimination. Thus the task of map-image change detection is to develop a map converter \\sigma(\u00b7) that effectively encodes high-level categorical information, facilitating semantic feature extraction, and to design a model that associates map categories with image details, mitigating the cross-modal problem, i.e.,\nB = H_{lvd}(F_{M}(\\sigma(M, C)), F_{I}(I))\nwhere \\sigma(\u00b7) encodes M with a set of categorical text C = {c_i}_{i=1}^{n}. Since linguistic symbols reflect characteristics of"}, {"title": "3.2. Overall", "content": "The overall framework of the proposed LaVIDE algorithm is illustrated in Fig. 2. LaVIDE aligns maps and images within the feature space of the language-vision model to associate high-level map categories with low-level image details. Specifically, CLIP [43], which aligns linguistic information with visual semantics according to cosine similarity, is used as our language-vision foundation model. It comprises two branches-the map branch and the image branch-designed to extract object-specific text embeddings, G_t, and vision embeddings, G_v, respectively. Specifically, the map branch leverages the text encoder F_{text}(\u00b7) of the language-vision model to extract text embeddings T from the textual representation L. To further enhance object appearance characteristics, it also comprises an object encoder to extract object embeddings O from the map M by introducing the information of target attributes. The object embeddings enhances text information in T at feature level through an Object Context Optimization (OCOpt) module F_{OCOpt}(\u00b7), yielding G_t. The image branch includes a vision encoder F_{img}(\u00b7), which ensures the semantic correlation between vision and language via feature distillation of the image encoder from the language-vision model, to obtain G_v from the image I. The embeddings G_t and G_v are finally fed into a MoE discriminative module to extract various semantic differences between them for change detection."}, {"title": "3.3. Map-image Feature Extraction", "content": ""}, {"title": "3.3.1. Map Branch", "content": "Text Encoder. We associate each map pixel M_{i,j} with corresponding categorical text through the map converter \\sigma(M_{i,j}, C) = C_k s.t. k = arg \\max M_{i,j,k}, creating the textual representation L. Inspired by that text encoders can extract high-level semantic information from text, we use the text encoder of the language-vision foundation model to exploit high-level categorical features from the textual representation. As the text encoder of the language-vision foundation model is trained under the guidance of visual signals, its extracted textual features are semantic-relevant to image details. The text encoder takes as input a prompt that wraps categorical text. Since a minor distinction in prompts can lead to significant differences in the model's"}, {"title": "3.3.2. Image Branch", "content": "Vision Encoder. An intuitive way to obtain vision embeddings semantically aligned with text is using the image encoder of language-vision models to process images. However, the image encoder is typically built on a flat architecture, missing multi-scale feature outputs to detect changes in various scales. To this end, we adopt a hierarchical architecture model to extract multi-scale visual features {G^s}_{s=1}^{S}, where S denotes the number of scales. Specifically, we employ the feature backbone of Segformer [50] as our vision encoder, generating S = 4 different scale features. To ensure a semantic connection between visual and textual embeddings, we adopt a feature distillation strategy to align the feature space of the vision encoder with that of the image encoder. However, the architectural differences between these two heterogeneous encoders lead to a feature mismatch problem during knowledge distillation.\nAs a consequence, we only supervise the final layer feature G^S extracted from the vision encoder with the feature G_{LVM} extracted from the image encoder of the language-vision model, simplifying feature matching between the two heterogeneous encoders for knowledge distillation. Considering the contradictions between different types of knowledge, we adopt correlation loss, instead of consistency loss, as the distillation loss L_{distill} to make general language-vision aligned knowledge compatible with task-specific change knowledge in our vision encoder. The correlation loss is implemented by a cosine function."}, {"title": "3.4. Map-image change detection", "content": "MoE discriminative Module. The similarity between different categories influences the identification of changes. In other words, categories with higher inter-class similarity, e.g., from vegetation to buildings, change less noticeably than those with lower inter-class similarity, e.g., from vegetation to agricultural fields. To capture the subtle changes caused by high similarity between categories, a robust discriminative feature should reflect various semantic differences from different perspectives, facilitating comprehensive change detection. Inspired by the way humans typically compare two objects from multiple semantic perspectives, e.g., their shape and structure, and adaptively select prominent semantic differences to determine whether these two objects are identical, we design a MoE discriminative module to measure object differences from multiple semantic perspectives. The MoE discriminative module employs N experts {E_j(\u00b7)}_{j=1}^{N} to model different semantic perspectives, each of which is achieved by a multilayer perceptron (MLP). To adaptively focus on significant differences, the MoE discriminative module comprises a change-specific route function F_{route}(\u00b7), which is implemented by a depthwise separable convolutional module, to calculate difference weights. Specifically, the semantic difference for the jth perspective is quantified by D^j = E_j(G_t || G_v), where G_t^\u2193 is derived by downsampling G_t to match the dimensions of G_v. The corresponding weight, the importance of difference in the jth perspective, are calculated by W = F_{route}(G_t || G_v). Then the discriminated feature is calculated by weighting D^j with W^j, i.e., D' = \\sum_{j=1}^{N} W^j D^j. The discriminated features {D^j}_{j=1}^{N} are linearly fused into a multi-scale feature D via a MLP, which is fed into a binary classifier for change detection B.\nLoss Function. We use the cross-entropy loss as the primary optimization objective for change detection, denoted by L_{change}. Additionally, a contrastive loss L_{contrast} is used to put unchanged features in map-image pairs together while pulling changed features away. The overall function is as follows:\nL = L_{change} + \\lambda_1 L_{distill} + \\lambda_2 L_{contrast}\nwhere \\lambda_1 and \\lambda_2 denote the balancing parameters."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experimental Setup", "content": "Datasets. Our experiments are conducted on four benchmark datasets: DynamicEarthNet [47], HRSCD [14], BANDON [40], and SECOND [51], where the semantic labels of pre-change images serve as maps.\n\u2022 DynamicEarthNet is a global dataset characterized by geographical diversity that collects multi-temporal data from 75 regions on the Earth from 2018 to 2019, with annotations for seven distinct types of ground objects. Specifically, 38, 640 pairs of maps and images with a size of 512x 512 are used for training, 920 pairs for evaluation, and 920 pairs for test.\n\u2022 HRSCD is a high-resolution dataset that comprises paired images at 0.5 m spatial resolution from two French cities, captured in 2006 and 2012. Semantic annotations for these images, derived from rasterized maps, cover six distinct types of ground objects. In our experiments, we use 76, 400 pairs of 500 \u00d7 500 pixel images for training, 20, 000 pairs for evaluation, and 20, 000 pairs for test.\n\u2022 BANDON collects off-nadir aerial images from six representative cities across China, primarily focusing on changes in buildings. It features annotations for semantic components of buildings, i.e., roofs, facades, and background. In our experiments, we utilize 27,024 pairs of 512x 512 images for training, 3, 232 pairs for evaluation, and 3, 312 pairs for test.\n\u2022 SECOND. To evaluate model generalization, we use SECOND as the out-of-domain dataset for BANDON [40]. The two datasets differ in data platforms, sensors, and sampled regions. To be consistent with annotations in BANDON, we filter out non-building annotations from SECOND. This dataset consists of 2968 image pairs, each with a size of 512\u00d7512 for out-of-domain testing in building change detection.\nImplementation Details. Our network is implemented using Pytorch on two V100 GPUs. The AdamW optimizer is used, with a learning rate of 6 \u00d7 10^{-5}, adjusted by a polynomial decay scheduler with a linear warmup phase. The batch size is set to 12 and the maximum number of training iterations is set to 32k. Empirically, we set the number of experts N in the MoE discriminative module to 10, and the weighting factors \\lambda_1 and \\lambda_2 in Eq. (4) are both set to 1."}, {"title": "4.2. Comparison with State-of-the-art Methods", "content": "We compare the proposed LaVIDE to state-of-the-art change detection methods, including category discrimination approaches, Segformer [50], SSG2 [16], and SETR-PUP [55], and vision discrimination approaches, SNUNet [17], CGNet [22], FHD [42], ChangerEx [18], ChangeFormer [3], and MapFormer [4]. For category discrimination approaches, we follow the setup in [4], training semantic segmentation backbones with both pre- and post-change semantic labels. The change performance is evaluated by F1-score (F1.) and intersection over union (IoU).\nQuantitative Analysis. The quantitative results are presented in Tab. 1 and show that the proposed LaVIDE brings remarkable improvements compared to the state-of-the-art methods, improving IoU by 13.8% on DynamicEarthNet, 7.6% on HRSCD, 2.5% on BANDON, and 4.3% on the out-of-domain dataset SECOND. Specifically, category discrimination approaches perform poorly with notably similar results across four benchmark datasets. This implies that semantic segmentation models struggle to accurately capture semantic information within images, resulting in comparisons with maps that do not accurately reflect actual changes. For the HRSCD dataset, where ground objects are challenging to differentiate due to the complex information brought by the increased spatial resolution, these methods tend to perform less effectively.\nVision discrimination approaches directly compare pixel similarity in the visual feature space, which is easier than recognizing the category to which each pixel belongs, achieving certain improvements over category discrimination in most cases. Among them, Mapformer, which leverages a multi-modal feature fusion module to handle cross-modal inputs, generally outperforms bi-temporal change detection methods, such as SNUNet and ChangerEx. However, its performance is limited in more complex geographical environments, such as DynamicEarthNet and HRSCD, which contain a larger number of categories compared to the BANDON dataset, which only includes the building category. It can be considered that the color encoding approach used by Mapformer struggles to effectively distinguish subtle differences between the numerous categories. Despite being designed for cross-modal change detection, Mapformer fails to bring significant performance improvements, resulting in performance comparable to bi-temporal methods such as FHD and ChangeFormer.\nLaVIDE utilizes language to indicate ground objects, which can effectively preserve the high-level categorical information of the map, achieves superior performance across all datasets. We further evaluate the models' generalization performance by using them to detect changes on the SECOND dataset after training on BANDON. Benefiting from the generalization of the language-vision model, LaVIDE consistently outperforms other models.\nQualitative Analysis. We choose two typical scenes from the DynamicEarthNet dataset to evaluate qualitative results, as shown in Fig. 3. The results obtained by LaVIDE exhibit a higher level of consistency with the ground truth when compared to other methods. Category discrimination approaches are prone to errors in areas that are difficult to classify. For instance, in the first scene of Fig. 3, narrow roads, which lack distinct features, pose challenges"}, {"title": "4.3. Ablation Studies", "content": "Map encoding. To validate the superiority of map encoding with language, we also refer to the map encoding strategy used in vision discrimination approaches, i.e., using colors to indicate ground objects in maps, to train the proposed network, denoted as LaVIDE-C. The results are shown in Tab. 2 and we can observe a performance gain of 2% in terms of IoU is achieved by using language instead of color to encode maps, since language provides a more efficient means of conveying information about ground objects compared to color.\nPrompt design. LaVIDE adopts an ensemble strategy to stabilize the semantic association of text embeddings with visual features. To demonstrate its effectiveness, we also evaluate the results of models with single prompt for text embedding generation, of which experimental results are reported in Tab. 3. It suggests that different prompts yield inconsistent results, whereas the ensemble strategy, which enhances the expression of map information, achieves optimal results and alleviates the challenge of prompt selection.\nObject context optimization. The proposed LaVIDE network employs object context optimization to refine map features by incorporating shape and spatial layout information about targets. To validate its effectiveness, we train the LaVIDE network by removing the object encoder and object context optimization module in the map branch, denoted as w/o OCOpt. It can be seen from Tab. 4 that incorporating target attributes extracted from object masks into textual embedding effectively improves the performance of change detection, as textual embeddings enriched with detailed object attributes align more closely with the corresponding visual features in images.\nKnowledge distillation. The proposed LaVIDE network aligns image embeddings into the feature space of the language-vision model through knowledge distillation, thereby achieving homogenization with language-encoded maps. We train the LaVIDE network by removing the knowledge distillation process during training, denoted as w/o Distill. As shown in Tab. 4, it struggles to effectively correlate the visual data from images with the semantic data encoded in the maps, resulting in a significant performance gap compared to our proposed LaVIDE. In contrast, our proposed LaVIDE, with the refined alignment facilitated by knowledge distillation, significantly boosts map-image change detection performance.\nMoE discriminative module. We evaluate the impact of the number of experts in the MoE discriminative module on change detection by sampling N = [1, 5, 10, 15], as shown in Tab. 5. The results indicate that increasing N contributes to greater performance gains by capturing semantic differences from a wider range of perspectives, while excessively large values of N may lead to overfitting and result in diminished performance. When N = 1, the MoE discriminative module is degraded to a naive pixel-wise differentiated module, similar to that in Changeformer [3]. Despite this, our method exhibits superior performance, further demonstrating the advantage of using language to encode maps over color-based encoding."}, {"title": "5. Conclusion", "content": "In this paper, we propose a novel map-image change detection algorithm, LaVIDE, that leverages language to bridge the information gap between maps and images, effectively preserving high-level categorical information to enable detailed comparisons with the low-level visual details. It aligns the map and image embeddings into the feature space of the language-vision model for homogenization, and captures semantic differences across various semantic perspectives to improve the change detection performance. Extensive experiments demonstrate that the proposed method can achieve the state-of-the-art, outstanding in ensuring the integrity of the change region and suppressing noises."}]}