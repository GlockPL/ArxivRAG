{"title": "ACECODER: Acing Coder RL via Automated Test-Case Synthesis", "authors": ["Huaye Zeng", "Dongfu Jiang", "Haozhe Wang", "Ping Nie", "Xiaotong Chen", "Wenhu Chen"], "abstract": "Most progress in recent coder models has been driven by supervised fine-tuning (SFT), while the potential of reinforcement learning (RL) remains largely unexplored, primarily due to the lack of reliable reward data/model in the code domain. In this paper, we address this challenge by leveraging automated large-scale test-case synthesis to enhance code model training. Specifically, we design a pipeline that generates extensive (question, test-cases) pairs from existing code data. Using these test cases, we construct preference pairs based on pass rates over sampled programs to train reward models with Bradley-Terry loss. It shows an average of 10-point improvement for Llama-3.1-8B-Ins and 5-point improvement for Qwen2.5-Coder-7B-Ins through best-of-32 sampling, making the 7B model on par with 236B DeepSeek-V2.5. Furthermore, we conduct reinforcement learning with both reward models and test-case pass rewards, leading to consistent improvements across HumanEval, MBPP, BigCodeBench, and LiveCodeBench (V4). Notably, we follow the R1-style training to start from Qwen2.5-Coder-base directly and show that our RL training can improve model on HumanEval-plus by over 25% and MBPP-plus by 6% for merely 80 optimization steps. We believe our results highlight the huge potential of reinforcement learning in coder models.", "sections": [{"title": "1 Introduction", "content": "In recent years, code generation models have advanced significantly with compute scaling (Kaplan et al., 2020) and training data quality improvement (Huang et al., 2024; Lozhkov et al., 2024; Guo et al., 2024b). The state-of-the-art coder models, including Code-Llama (Rozi\u00e8re et al., 2023), Qwen2.5-Coder (Hui et al., 2024a), DeepSeek-Coder (Guo et al., 2024a) and so on, have shown unprecedented performance across a wide range of coding tasks like program synthesis (Chen et al., 2021), program repair (Zheng et al., 2024a), optimization (Shypula et al., 2023), test generation (Steenhoek et al., 2023), SQL (Yu et al., 2018), issue fix (Jimenez et al., 2024). These models are all pre-trained and further supervised fine-tuned (SFT) on large-scale coding data from web resources like Common Crawl or Github.\nThough strong performance has been achieved through SFT (Luo et al., 2023; Wei et al., 2024), very few models have explored the potential of reinforcement learning (RL) (Ouyang et al., 2022a), which has proven effective in other domains such as mathematical reasoning like DeepSeek-R1 (Shao et al., 2024). We argue that this absence of RL-based training in coder models is primarily due to two key challenges:\n(1) Lack of reliable reward signals for code generation. In tasks such as mathematical problem-solving, rewards can be easily derived from rule-based string matches with reference answers (Guo et al., 2025) or large-scale human annotations (Ouyang et al., 2022b). In contrast, evaluating code quality typically requires executing test cases to measure the pass rate, making reward signal design more complex. This also explains why existing reward models like Skywork (Liu et al., 2024a) can hardly generalize to the coding domain (see subsection 4.4).\n(2) Scarcity of large-scale coding datasets with reliable test cases. Most existing coding datasets like APPS (Hendrycks et al., 2021; Chen et al., 2021) heavily rely on costly human expert annotations for test cases, which limits their scalability for training purposes. The largest data is TACO (Li et al., 2023) with 25K examples, which are crawled from the popular coding competition websites, which were already heavily exploited during the pre-training phase.\nTherefore, we curate ACECODE-87K, on which we trained our reward models: ACECODE-RM-7B and ACECODE-RM-32B. Comprehensive experiments of best-of-N sampling show that ACECODE-RM can significantly boost existing LLM's performance on coding benchmarks. For example, ACECODE-RM-7B can improve the performance of Llama-3.1-8B-Instruct by an average of 8.4 points across the 4 coding benchmarks, i.e. HumanEval (Liu et al., 2023), MBPP (Liu et al., 2023), BigCodeBench (Zhuo et al., 2024) and LiveCodeBench (Jain et al., 2024). Even for the stronger coder model Qwen2.5-Coder-7B-Instruct, our \"7B+7B\" combination still gets an average of 2.6 improvements. ACECODE-RM-32B is even more powerful, which pushes the former two numbers to 10.7 and 4.7 respectively, showcasing the effectiveness of ACECODE-RM.\nFurthermore, we adopt ACECODE-RM-7B and test case pass rate separately to do reinforcement learning with reinforce++ (Hu, 2025) over coder models. Experiments show 2.1 and 0.7 points of average improvement when starting from Qwen2.5-7B-Ins and the Qwen2.5-Coder-7B-Ins respectively, making the latter even more powerful than GPT-4-Turbo on benchmarks like MBPP. Inspired by the recent DeepSeek-R1 (Guo et al., 2025), we also perform RL training directly from the Qwen2.5-Coder-7B-base model and saw a surprising 25% improvement on HumanEval-plus and 6% improvement on MBPP-plus (Liu et al., 2023) with merely 80 optimization steps (48 H100 GPU hours). These improvements are also generalizable to other more difficult benchmarks.\nTo our knowledge, this is the first work to perform reward model training and reinforcement learning for code generation using a fully automated pipeline that synthesizes large-scale reliable tests. We believe our ACECODE-87K will unlock the potential of RL training for code generation models and help the community to further push the boundaries of LLM's coding abilities."}, {"title": "2 Methodology", "content": "In this section, we will introduce the overall methodology of ACECODER. We begin with formulations of the problems we are investigating, including reward model training and reinforcement learning for LLMs. We then elaborate on how we synthesize the test cases and construct the ACECODE-87K. Finally, we explain how we perform the reinforcement learning using our ACECODE-RM trained on the ACECODE-87K."}, {"title": "2.1 Problem Formulation", "content": "Reward Model Training Let x denote the coding question and y = {y1,\u2026, yt} denote the program solution, where yi represents the i-th token of the program solution and (x, y) \u2208 D. Assuming \u03b8 represents the parameters of the model, then n responses (y1, ..., y\u201d) will be sampled from the model \u03c0\u03b8 given the input x. Let (s1,..., sn) be the target rewards, i.e. the test case pass rates in our scenario, then we define the Bradley-Terry loss (Bradley and Terry, 1952) for every pair of responses y\u00b2 and y\u00b3 with scores of si and sj when we are training a reward model R\u00f8 as follows:\nL\u00f8(x, si, sj)\n= 1[si > sj] logo(R\u00f8(x, y\u00b2) \u2013 R\u00f8(x, y\u00b3))\nwhere 1[\u00b7] = 1 if the expression inside the brackets is true, otherwise, it's 0. The final loss function for the reward training is:\nL(\u03c6) = 1/n(n-1) \u03a3_i=1^n \u03a3_j=1^n L\u00f8(x, si, sj) (1)"}, {"title": "3 ACECODE-87K", "content": "To be able to train a reward model specifically designed for code generation, the first thing is to synthesize reliable test cases for each coding problem and use them as training signals. In this section, we explain the whole procedure of constructing ACECODE-87K step by step. \nTest Case Synthesis from Seed Dataset We start from existing coding datasets with provided question x and corresponding program y. Specifically, we combine Magicoder-Evol-Instruct\u00b9, Magicoder-OSS-Instruct-75K\u00b2, and StackPyFunction\u00b3 as our seed dataset. We only keep the questions written in Python that contain either a function or a class, resulting in a total of 124K entries. We find that these datasets contain highly noisy questions that could not be easily evaluated using test cases. Therefore, we feed every question-solution pair (x, y) into a GPT-40-mini (Hurst et al., 2024) to propose a refined LeetCode-style question xr with highly structured instructions. Meanwhile, we also prompt it to 'imagine' around 20 test cases (t1, ..., tm) for each refined coding question xr based on its understanding of the expected behavior of the desired program. \nTest Case Filtering These 'imagined' test cases generated from the LLM contain severe hallucinations. To filter out those hallucinated test cases, we facilitated a stronger coder model Qwen2.5-Coder-32B-Instruct (Hui et al., 2024a) as a proxy to perform quality control. Specifically, we prompt it for each xr to generate a program y' and then run these programs over the test cases to approximate their quality. We removed all test cases ti where the generated solution program y' could not pass. Furthermore, we removed questions with fewer than 5 tests after filtering, as these questions might be overly ambiguous. With the above filtering, we constructed the ACECODE-87K with 87.1K distinct coding questions and 1.38M cleaned test cases, as"}, {"title": "4 Experiments", "content": "4.1 Reward Model Training Setup\nWe mainly use Qwen2.5-Coder-7B-Instruct 4 as the backbone of the reward model and sample 16 responses from it for each question in ACECODE-87K. Finally, following the rule defined in Equation 3, around 300K preference pairs were created out of 46,618 distinct questions (37.34% of the total questions) that have at least one pair satisfying the condition, and other questions are not used.\nOur reward model is trained using LlamaFactory (Zheng et al., 2024b). We apply full fine-tuning with DeepSpeed stage 3. We train for 1 epoch using a cosine learning rate schedule, starting at 1e-5 with a warmup ratio of 0.1 to gradually increase the learning rate in the initial training phase. Training batch size is set to 128. We enable bf16 precision to reduce memory overhead without compromising model fidelity. The training takes 24 hours on 8 x A100 GPUs.\n4.2 Reinforcement Learning Setup\nWe perform RL training from three policy models: Qwen2.5-7B-Instruct 5 and Qwen2.5-Coder-7B-Base 6 and Qwen2.5-Coder-7B-Instruct. Two types of reward can be used, i.e. the trained reward model ACECODE-RM-7B and the rule-based reward, i.e. pass rate over the test cases in ACECODE-87K. During training, we set the pass rate to be a binary reward, which is 1.0 when all test cases passed, otherwise 0. This is similar to the verifiable reward used in Tulu3 (Lambert et al., 2024a) and DeepSeek-R1 (Guo et al., 2025). Similar to DeepSeek-R1 (Guo et al., 2025), we also experiment with RL from the base model because SFT may cause the search space of the model to be stuck in the local minimum. Since coding is also a highly verifiable task like math, we include the Qwen2.5-Coder-7B-Base in our experiments."}, {"title": "4.3 Evaluation Setup", "content": "We evaluate our method on three established code-focused benchmarks: EvalPlus (Liu et al., 2023, 2024b), Big Code Bench (Zhuo et al., 2024) and Live Code Bench (Jain et al., 2024). These bench-marks collectively cover a diverse array of coding tasks, enabling us to assess both the correctness and quality of generated code. For Best-of-N sampling experiments, we adopt top-p sampling with a temperature of 1.0 to generate multiple candidate solutions per question. We select the response with the highest reward for evaluation. For RL experiments, we use the benchmark's default setting, which is greedy sampling most of the time."}, {"title": "4.4 Main Results", "content": "Here we show the experimental results of reward model and RL-trained model."}, {"title": "4.5 Ablation Studies", "content": "Test Case Quality Matters We also conduct experiments to investigate how filtering the test cases with a proxy model can affect the results. As shown in Table 5, training RM on data after the filtering improve the performance significantly, especially for those hard code questions like MBPP-Plus and BigCodeBench-Hard (C/I). We believe this is because the test case filtering can ensure the remaining ones are consistent with each other and thus point to the same implicit program, which improves the quality of the rewards.\nRM Backbone Matters Our results in Table 6 clearly show that changing the backbone of the reward model from Llama-3.1 to Qwen2.5 can significantly improve the Best-of-16 performance. This is because the Qwen2.5-Coder models have been pre-trained on way more code-related data compared to the Llama-3.1 models, and thus more knowledgeable when tuning it into a reward model.\nDoes R1-style Tuning Work? Inspired by the recent DeepSeek-R1 (Guo et al., 2025), we also conduct the RL directly from the base model without any SFT. It turns out we get huge improvements when using rule-based rewards. For example, we get 25.0 points of improvements on HumanEval-Plus after training only 6 hours from the Base Model, which is way more efficient that the large-scale SFT. What's more, the ACECODER Rule improve the BigCodeBench-Instruct-Full's performance from 40.2 to 43.2, nearly the same performance with DeepSeek-R1-Distill-Qwen-32B (43.9) which was directly distilled from the DeepSeek-R1 Model. This further consolidates the finding of DeepSeek-Zero. However, we do find that using reward models for RL tuning can lead to worse results. We attribute this to the potential reward hacking during the tuning."}, {"title": "5 Related Works", "content": "5.1 LLM for Code Generation\nLarge language models (LLMs) have demonstrated significant potential in code generation. Due to the unique nature of coding tasks, specialized coding models such as Code Llama (Rozi\u00e8re et al., 2023) and Qwen Coder (Hui et al., 2024b; Yang et al., 2024a) were developed shortly after the emergence of general-purpose LLMs. These models typically undergo a two-phase training process: pre-training and fine-tuning. During pre-training, they are exposed to extensive coding corpora sourced from various internet platforms, including raw text, GitHub repositories, and pull requests. This is followed by supervised fine-tuning, which enhances their instruction-following capabilities. \n5.2 Synthesizing Test Cases\nAutomatic test generation is a widely used approach for verifying the correctness of LLM-generated programs. Prior work has commonly employed the same LLM that generates the programs to also generate test cases, selecting the most consistent program from multiple sampled outputs in a self-consistency manner (Chen et al., 2022; Huang"}, {"title": "5.3 Reward Models", "content": "Reward models play a crucial role in aligning LLMs by assigning scalar values to response pairs based on specific evaluation criteria, such as human preference (Ouyang et al., 2022b) and accuracy (Zhang et al., 2025). They are widely used in reinforcement learning with human feedback (RLHF) to refine model behavior and in Best-of-N sampling to enhance test-time performance. However, while general-purpose reward models are effective for assessing human preference, they often struggle with specialized domains like mathe-"}, {"title": "5.4 Reinforcement Learning for LLM", "content": "Reinforcement Learning from Human Feedback (RLHF)(Ouyang et al., 2022b) has been widely adopted to enhance the capabilities of large language models (LLMs) in various tasks, including conversational interactions and mathematical reasoning(Yang et al., 2024b). Reinforcement learning (RL) algorithms such as PPO(Schulman et al., 2017), GRPO(Shao et al., 2024), and Reinforcement++(Hu, 2025) have been employed to fine-tune models using reward signals derived from either learned reward models(Shao et al., 2024) or predefined rule-based heuristics (Guo et al., 2025; Wang et al., 2025).\nGiven that coding is an inherently verifiable task, recent studies have explored RL techniques that leverage direct execution accuracy as a reward signal. PPOCoder (Shojaee et al., 2023b) and CodeRL (Le et al., 2022) demonstrated the effectiveness of PPO-based RL for coding tasks, while RLEF (Gehring et al., 2024) extended this approach to multi-turn settings by incorporating execution feedback at each step. \nDespite these advancements, most prior RL-based approaches for coding have been constrained by the use of pre-annotated datasets such as APPS (Hendrycks et al., 2021), which consists of only 5,000 examples, with most problems having a single test case. This limited data availability poses challenges to scalable RL training. Furthermore, the potential of reward models for coding remains largely unexplored. In this work, we address these limitations by automatically synthesizing test cases and leveraging trained reward models for reinforcement learning, demonstrating the scalability and effectiveness of our approach."}, {"title": "6 Conclusion", "content": "We are first work to automate large-scale test-case synthesis and adopt them to train coder language models. Without relying on the most advanced model, our data collection pipeline can still produce very high-quality verifiable code data, which empowers the training of reward model and coder model through reinforcement learning. Though our work demonstrates huge improvement in Best-of-N experiments, the improvement on RL training is less prominent. We believe future work should further our reward model training to improve its robustness to further the results."}, {"title": "A Appendix", "content": "A.1 Prompt Template"}]}