{"title": "Towards Robust Vision Transformer via Masked Adaptive Ensemble", "authors": ["Fudong Lin", "Xu Yuan", "Jiadong Lou", "Nian-Feng Tzeng"], "abstract": "Adversarial training (AT) can help improve the robustness of Vision Transformers (ViT) against adversarial attacks by intentionally injecting adversarial examples into the training data. However, this way of adversarial injection inevitably incurs standard accuracy degradation to some extent, thereby calling for a trade-off between standard accuracy and adversarial robustness. Besides, the prominent AT solutions are still vulnerable to adaptive attacks. To tackle such shortcomings, this paper proposes a novel ViT architecture, including a detector and a classifier bridged by our newly developed adaptive ensemble. Specifically, we empirically discover that detecting adversarial examples can benefit from the Guided Backpropagation technique. Driven by this discovery, a novel Multi-head Self-Attention (MSA) mechanism is introduced for enhancing our detector to sniff adversarial examples. Then, a classifier with two encoders is employed for extracting visual representations respectively from clean images and adversarial examples, with our adaptive ensemble to adaptively adjust the proportion of visual representations from the two encoders for accurate classification. This design enables our ViT architecture to achieve a better trade-off between standard accuracy and adversarial robustness. Besides, the adaptive ensemble technique allows us to mask off a random subset of image patches within input data, boosting our ViT's robustness against adaptive attacks, while maintaining high standard accuracy. Experimental results exhibit that our ViT architecture, on CIFAR-10, achieves the best standard accuracy and adversarial robustness of 90.3% and 49.8%, respectively.", "sections": [{"title": "1 INTRODUCTION", "content": "The Vision Transformers (ViT) architecture has demonstrated impressive capabilities in a wide range of vision tasks, including image and video classification [2, 12, 73], dense prediction tasks [46, 80, 81], self-supervised learning [3, 20, 72], among others [6, 13, 24, 27, 36, 38, 39, 41, 49, 50, 62, 62, 89]. However, similar to Convolutional Neural Networks (CNNs) [21, 25, 28, 32, 40, 61, 63, 68-70, 92], the ViT architecture is vulnerable to adversarial attacks [5, 11, 18, 54-56, 91] achieved by maliciously altering clean images within a small distance, leading to incorrect predictions with high confidence. This vulnerability hinders the adoption of ViT in critical domains such as healthcare, finances, etc.\nSo far, adversarial training (AT) methods [1, 29, 30, 35, 52, 63, 79, 82, 90] are widely accepted as the most effective mechanisms for improving ViT's robustness against adversarial attacks, by intentionally injecting adversarial examples into the training data. Unfortunately, existing AT solutions struggle with two limitations. First, they suffer from a trade-off between standard accuracy (i.e., the accuracy on clean images) and adversarial robustness (i.e., the accuracy on adversarial examples), with improved robustness while yielding non-negligible standard accuracy degradation. Second, these solutions are not effective against adaptive attacks [9, 45, 75, 86], i.e., a category of adversarial attacks capable of exploiting the weak points of defense methods to adaptively adjust their attack strategies. Hence, it calls for the exploration of enhancing ViT's robustness against adaptive attacks.\nOne potential direction to tackle the trade-off between standard accuracy and adversarial robustness is the detection/rejection mechanism. This involves training an additional detector to identify and reject malicious input data, with several solutions proposed in the literature [51, 58, 60, 74, 88]. However, these detection techniques have limited effectiveness against adaptive attacks and cannot be applied to scenarios involving natural adversarial examples, as reported in a prior study [26]. Hence, it is crucial to develop novel solutions that can address limitations associated with the aforementioned direction and are suitable for a wide range of scenarios.\nIn this work, we aim to boost the robustness of ViT against adaptive attacks in a more general and challenging scenario where"}, {"title": "2 RELATED WORK", "content": "Detection Mechanisms. Detecting adversarial examples (AEs) and then rejecting them (i.e., detection/rejection mechanism) can improve the model's robustness against adversarial attacks. That is, the input will be rejected if the detector classifies it as an adversarial example. Popular detection techniques include Odds [60], which considers the difference between clean images and AEs in terms of log-odds; NIC [51], which checks channel invariants within deep neural networks (DNNs); GAT [88], which resorts to multiple binary classifiers; JTLA [58], which proposes a detection framework by employing internal layer representations, among others [15, 17, 34, 67, 85]. Unfortunately, existing detection methods are typically ineffective in defending against adaptive attacks. Besides, the detection/rejection mechanism cannot be generalized to domains where natural adversarial examples exist. Our work differs from previous solutions in two aspects. First, we introduce a novel Multi-head Self-Attention (MSA) mechanism by using the Guided"}, {"title": "3 PRELIMINARY: ONE-STEP LEAST-LIKELY\nADVERSARIAL TRAINING", "content": "Adversarial training (AT) improves the model's robustness against adversarial attacks by feeding adversarial examples into the training set. Given a model f with parameters \u03b8, a dataset with N samples, i.e., X = {(xi, Yi) | i \u2208 {1, 2, . . ., N}}, the cross-entropy loss function L, and a threat model A, AT aims to solve the following inner-maximization problem and outer-minimization problem, i.e.,\n$\\min \\limits_{\\theta} \\sum \\limits_{i=1}^{N} \\max \\limits_{\\delta \\epsilon \\Delta} L(f_{\\theta}(x_i + \\delta), Y_i),$\nwhere the inner problem aims to find the worst-case training data for the given model, and the outer problem aims to improve the model's performance on such data. Recently, one-step Fast Adversarial Training (FAT) [82] is popular due to its computational efficiency. FAT sets the threat model under a small l\u221e constraint \u20ac, i.e., A = {\u03b4 : ||\u03b4||\u221e \u2264 \u20ac}, by performing Fast Gradient Sign Method (FGSM) [18] with the random initialization, i.e.,\n\u03b4 = Uniform(-\u03b5, \u20ac) + e \u00b7 sign(\u2207x L(fo(xi), Yi)),\n8 = max(min(\u03b4, \u03b5), \u2212\u20ac),\nwhere Uniform denotes the uniform distribution and sign is the sign function. Notably, the second row in Eq. (2) serves to project the perturbation \u03b4 back into the l\u221e ball around the data xi."}, {"title": "4 OUR APPROACHES", "content": "Problem Statement\n4.1\nWe consider a set of N samples, i.e., X = {(xi, Yi) | i \u2208 {1, 2, . . ., N}},\nwhere x \u2208 RH\u00d7W\u00d7CH is an input image with the resolution of (H, W) and the channel count of CH, and y \u2208 [C] denotes its label. For notational convenience, we let d = H \u00d7 W \u00d7 CH. A classifier is a function fo: Rd \u2192 [C], parameterized by a neural network. We consider two types of inputs, i.e., a clean image xcln sampled from the standard distribution Dstd and an adversarial example xadv sampled from the adversarial distribution Dady. We assume Dstd and Dady follow different distributions. The clean image xcln itself or its augmented variant can be the input, while the adversarial example xadv is a malicious version of x within a small distance. That is, for some metric d, we have d(x, xadv) \u2264 \u0454, but xadv can mislead conventional classifiers. Parameterized by another neural network, a detector go is to tell whether an input image is a clean image or not, i.e., 9p : Rd \u2192 {\u00b11}, where +1 and -1 indicate a clean image and an adversarial example, respectively. The binary indicator function 1{.} is 1 if both the detector go and the classifier fe make correct predictions. We follow previous studies [52, 90] by referring standard accuracy, and adversarial robustness, as classification accuracy on clean images and adversarial examples, respectively.\n4.2 Detector\nParameterized by a neural network with parameters 4, the detector 9\u00a2 : Rd \u2192 {\u00b11} is to determine whether the input is a clean image or not, where +1 and -1 respectively represent a clean image and an adversarial example, i.e.,\n94(x) =\n{\n+1, if x is a clean image\n-1, otherwise.\n(4)\nAiming to generalize the robust model to critical domains (e.g., autonomous driving), the input will not be rejected in this work. Instead, we have modified it to output an estimated probability of p\u2208 [0, 1] for clean images and 1 - p for adversarial examples.\nThe design of our detector architecture is motivated by our empirical observation in that the adversarial perturbation is detectable after Guided Backpropagation visualization. Due to the small distance between a clean image and its corresponding adversarial example, their difference is notoriously imperceptible (see Figures 1a and 1d), making it theoretically hard to detect adversarial examples [74]. In our empirical study, we resort to Guided Backpropagation [71] to visualize the difference between a clean image and an adversarial"}, {"title": "4.3 Classifer", "content": "Inspired by self-supervised learning for vision tasks [3, 8, 20], we separate our adversarial training into two stages, i.e., pre-training and fine-tuning, for learning high-quality visual representations and fine-tuning a robust classifier, respectively.\nPre-training. Our classifier architecture for the pre-training is inspired by MAE [20]. Different from MAE, we utilize two encoders, denoted as the clean encoder and the adversarial encoder, for learning visual representations from clean images and adversarial examples, respectively. The decoder aims to reconstruct the original inputs from the visual representations encoded by the two encoders. Figure 2b shows the classifier architecture during the pre-training. Given an input image x \u2208 Rd, let xcln and xadv denote its clean and adversarial variants, respectively, with the clean variant obtained by augmenting the original input. Regarding the clean variant xcln, we randomly mask out a large proportion of image patches (e.g., 75%) and then feed the subset of visible patches into the clean encoder. The masked tokens are inserted into corresponding positions after the encoder. Finally, the decoder reconstructs the clean variant \u0113cln from the full set of image patches, including encoded visible patches and masked tokens. The reconstruction of the adversarial variant xadv follows a similar procedure, except that its visible patches are encoded by the adversarial encoder. Notably, the position of masked image patches in the adversarial variant xadv is the same as that in the clean variant xcln in order to minimize their visual representation difference during the pre-training.\nLet \u017eeln and \u017eadv respectively denote the global representations of clean and adversarial variants, obtained by performing global average pooling on the decoder's input sequence. Our design utilizes a new loss function to learn visual representations by simultaneously minimizing the reconstruction error and the visual representation difference, i.e.,\nLenc = (1 \u2212 2) \u00b7 Lrec(x, x) + \u03a9\u00b7 Lcl(zcln, zadv),\nwhere \u03a9 \u2208 (0, 1) is a hyperparameter and x is the reconstructed image. Lrec and Lcl denote the reconstruction loss and the contrastive loss, respectively. Given a set of B input images, we first generate their adversarial variants, arriving at a mini-batch of 2B samples, consisting of B clean variants {xcln}B1}=1 and B adversarial variants {adv}2B+1}B+1. We consider the form of contrastive loss in"}, {"title": "Adaptive Ensemble", "content": "Adaptive Ensemble. Although randomly masking an input image can eliminate the potential adversarial effect, this way inevitably hurts standard accuracy during the fine-tuning. In this paper, we propose adaptive ensemble [37] to tackle this issue. That is, the global representation for an input image is derived from the sum of cln and adv with an adaptive factor p \u2208 [0, 1], where \u017acln and adv are visual representations encoded by the clean and the adversarial encoders, respectively, and p is the probability of the input image being a clean image estimated by our detector.\nLet A be a full set of image patches and V be a subset of A, including visible patches only. 1v() is the indicator function for evaluating whether an image patch is visible. Hence, for each image patch of A, we have,\n1v (i) =\n{\n1, if the patch is visible\n0, otherwise\nV, i = 1, 2, ..., M, (10)\nwhere M is the number of image patches, i.e., |A|. For notational convenience, we let 1cln indicate visible patches fed into the clean encoder. Likewise, 1adv indicate visible patches fed into the adversarial encoder. Let \u017c\u012f be the visual representation of the i-th image patch, with i \u2208 {1, 2, ..., M}. Our adaptive ensemble is defined by:\nZi =\np.1cm (i) zin + (1-p) \u00b7 adv (i) \u00b7 adv\nmax (p.1cm (i) + (1-p) - adv (i), e)\n(11)\nwhere the denominator serves to normalize the adaptive ensemble of can and adv, and e is a small value to avoid divison by zero (i.e., \u20ac = 1e - 12 in this paper). The intuition underlying Eq. (11) is that if our detector has a high confidence that the input is a clean image (i.e., p is large), the global representation \u017ci will be mostly encoded by the clean encoder. Otherwise, 2\u2081 will be mainly encoded by the adversarial encoder. In addition, as our pre-training encourages the similarity level of the clean and the adversarial variants from a given input (see Eq. (8) and Eq. (9)), and two different masked inputs exist upon the fine-tuning, the invisible image patches in one masked input can be glimpsed from the other masked input."}, {"title": "5 EXPERIMENTS AND RESULTS", "content": "5.1 Experimental Setup\nDatasets. We conduct experiments on three widely-used benchmarks. (i) CIFAR-10 [31]: 60,000 32x32 RGB images of 10 classes. (ii) CIFAR-100 [31]: 60,000 32x32 RGB examples in 100 categories. (iii) Tiny-ImageNet [10]: 120,000 64x64 RGB images of 200 classes.\nCompared Methods. We compare our approach with four detection methods, i.e., Odds [60], NIC [51], GAT [88], and JTLA [58]. We compare our approach with five adversarial training (AT) counterparts: PGD-AT [52], TRADES [90], FAT [82], Sub-AT [35], and LAS-AWP [29], to exhibit how it boosts the ViT's robustness.\nEvaluation. We consider three state-of-the-art adaptive attacks, i.e., AutoAttack [9], Adaptive Auto Attack (A\u00b3) [87], and Parameter-Free Adaptive Auto Attack (PF-A\u00b3) [45], for evaluating detection accuracy and adversarial robustness. The attack constraint, if not specified, is set to \u20ac = 8/255.\nModel Size. We build our detector and classifier on top of Vision Transformers (ViT), with their architectures following ViT [12] and MAE [20], respectively. Our model size is pruned down to as small as possible in order to conduct a fair comparison with baselines. Table 1 lists the model size details. Our architecture consists of a detector and a classifier (including two encoders and one decoder), with 54.0M parameters in total. To conduct a fair comparison, existing adversarial training baselines use the ViT-Base model [12] with total parameters of 85.6M as the backbone network.\nHyperparameters. For all our models, if not specified, we use AdamW [48] with \u03b2\u2081=0.9, \u03b22=0.999, the weight decay of 0.05, and a batch size of 512. During the pre-training, the detector and the classifier (i.e., two encoders and one decoder) are trained jointly. For the detector, we follow the setting in [19] by setting the epochs of 100, the base learning rate of 1e - 3, the linear warmup epochs of 5, and the cosine decay schedule [47]. For the classifier, by contrast, we pre-train it for 200 epochs, with the base learning rate of 1e - 4, the linear warmup of 20 epochs, and a masking ratio of 75%. After pre-training, we drop the decoder and freeze the weights on the detector and the two encoders. Then, we finetune the classifier for 100 epochs, with the base learning rate of le 3, the linear warmup of 5, and the cosine decay schedule, and a masking ratio of 45%. The patch size is set to 4 (or 8) for CIFAR-10/CIFAR-100 (or Tiny-ImageNet). We grid-search hyperparamters A in Eq. (6) and \u03a9 in Eq. (8) of Section 4 and empirically set \u03bb to 0.15 and 2 to 0.35 for all datasets.\n5.2 Overall Performance on Our Classifier\nOverall Comparisons on CIFAR-10. We first conduct extensive experiments on CIFAR-10 and compare our approach to its state-of-the-art adversarial training (AT) counterparts listed in Section 5.1 in terms of standard accuracy and adversarial robustness under attack constraints of \u20ac = 4/255 and of \u20ac = 8/255. Table 2 lists comparative results. It is observed that our approach achieves the best performance under all three scenarios. In particular, our approach achieves the standard accuracy of 90.3%, outperforming the best competitor (i.e., LAS-AWP) by 3.5%. This is contributed by employing two encoders to extract visual representations respectively from clean images and adversarial examples, able to significantly"}, {"title": "5.3 Ablation Studies on Our Classifier", "content": "Pre-training: Contrastive Loss. We qualitatively and quantitatively exhibit the impact of our proposed loss, i.e., Eq. (9), on learning visual representations. We first present the qualitative evaluations. Specifically, we reconstruct masked adversarial examples and compare reconstruction quality by utilizing our approach with/without the contrastive loss (CL) in SimCLR [8]. Figure 5 illustrates the qualitative results. For images on each row, from left to right, are original adversarial example, the masked input, the image generated by our"}, {"title": "5.4 Evaluating Our Detector", "content": "In this section, we conduct experiments on CIFAR-10 for comparing our detector with four detection baselines, i.e., Odds [60], NIC [51], GAT [88], and JTLA [58]. Three aforementioned adaptive attacks under two small attack constraints, i.e., \u20ac = 2/255 and \u20ac = 4/255, are used for evaluating detection accuracy. Table 5 lists the detection accuracy values under different attack methods. We observed that our detector achieves the best detection accuracy under all scenarios. Specifically, our approach achieves the best detection accuracy of 99.4% under the attack constraint of \u20ac = 4/255 (see the 5th column). Decreasing the attack constraint to 2/255 increases the detection difficulty, with our approach still maintaining the superior detection accuracy of 95.8% (see the 4th column) in the worst case. Besides, our detector outperforms all baselines, with the detection accuracy improvements ranging from 1.8% (i.e., 95.9% vs. 94.1%, see the 3rd column) to 6.3% (i.e., 96.4% vs. 90.1%, see the"}, {"title": "5.5 Ablation Studies on Our Detector", "content": "MSA on the Detection Accuracy. Here, we empirically show how our developed MSA mechanism affects the detection accuracy under the attack of A\u00b3 and PF-A\u00b3. We consider two scenarios. First, we remove the Guided Backpropagation (GB) variant to validate whether it benefits the detection of adversarial examples, denoted as \"w/o GB\". Second, we discard our proposed MSA and instead naively add two sets of patch embeddings respectively from the clean image and the GB variant, denoted as \"w/o MSA\". Table 6 lists the experimental results. We discovered that simply adding two sets of patch embeddings only marginally improves the detection accuracy of 0.6% (or 0.9%) under the A\u00b3 (or PF-A\u00b3) attack (see \"w/o GB\" vs. \"w/o MSA\"). Equipped with our MSA mechanism, in sharp contrast, the Guided Backpropagation technique can significantly benefit the detection task, with the detection accuracy improvement of 6.4% (or 7.0%) under the A\u00b3 (or PF-A\u00b3) attack (see \"w/o GB\u201d vs. \"Ours\"). These results confirm that (i) the Guided Backpropagation technique can help expose adversarial perturbation and (ii) our proposed MSA can significantly boost the detector's robustness against adaptive attacks.\nSNN Loss on Visual Representations. Here, we reveal the effect of our proposed loss, i.e., Eq. (6), on detecting adversarial examples. We consider how our detector with or without the Soft-Nearest Neighbors (SNN) loss affects the resulting representation space. In particular, we employ t-SNE visualization [77] on 200 clean images randomly sampled from CIFAR-10 and 200 adversarial examples generated either by the A\u00b3 attack or by the PF-A\u00b3 attack. Figures 7a and 7b depict the results by using the A\u00b3 attack, while Figures 7c and 7d present the results by employing the PF-A\u00b3 attack. We observed that without the SNN loss, the representations for clean"}, {"title": "6 CONCLUSION", "content": "This article has proposed a novel Vison Transformers (ViT) architecture, including a detector and a classifier, which are bridged by a newly developed adaptive ensemble. This ViT architecture enables us to boost adversarial training to defend against adaptive attacks, and to achieve a better trade-off between standard accuracy and robustness. Our key idea includes introducing a novel Multi-head Self-Attention (MSA) mechanism to expose adversarial perturbations for better detection and employing two decoders to extract visual representations respectively from clean images and adversarial examples so as to reduce the negative effect of adversarial training on standard accuracy. Meanwhile, our adaptive ensemble lowers potential adversarial effects upon encountering adversarial examples by masking out a random subset of image patches across input data. Extensive experiments have been conducted for evaluation, showing that our solutions significantly outperform their state-of-the-art counterparts in terms of standard accuracy and robustness."}]}