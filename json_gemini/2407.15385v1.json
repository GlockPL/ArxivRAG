{"title": "Towards Robust Vision Transformer via Masked Adaptive Ensemble", "authors": ["Fudong Lin", "Xu Yuan", "Jiadong Lou", "Nian-Feng Tzeng"], "abstract": "Adversarial training (AT) can help improve the robustness of Vision Transformers (ViT) against adversarial attacks by intentionally injecting adversarial examples into the training data. However, this way of adversarial injection inevitably incurs standard accuracy degradation to some extent, thereby calling for a trade-off between standard accuracy and adversarial robustness. Besides, the prominent AT solutions are still vulnerable to adaptive attacks. To tackle such shortcomings, this paper proposes a novel ViT architecture, including a detector and a classifier bridged by our newly developed adaptive ensemble. Specifically, we empirically discover that detecting adversarial examples can benefit from the Guided Backpropagation technique. Driven by this discovery, a novel Multi-head Self-Attention (MSA) mechanism is introduced for enhancing our detector to sniff adversarial examples. Then, a classifier with two encoders is employed for extracting visual representations respectively from clean images and adversarial examples, with our adaptive ensemble to adaptively adjust the proportion of visual representations from the two encoders for accurate classification. This design enables our ViT architecture to achieve a better trade-off between standard accuracy and adversarial robustness. Besides, the adaptive ensemble technique allows us to mask off a random subset of image patches within input data, boosting our ViT's robustness against adaptive attacks, while maintaining high standard accuracy. Experimental results exhibit that our ViT architecture, on CIFAR-10, achieves the best standard accuracy and adversarial robustness of 90.3% and 49.8%, respectively.", "sections": [{"title": "1 INTRODUCTION", "content": "The Vision Transformers (ViT) architecture has demonstrated impressive capabilities in a wide range of vision tasks, including image and video classification, dense prediction tasks, self-supervised learning, among others. However, similar to Convolutional Neural Networks (CNNs), the ViT architecture is vulnerable to adversarial attacks achieved by maliciously altering clean images within a small distance, leading to incorrect predictions with high confidence. This vulnerability hinders the adoption of ViT in critical domains such as healthcare, finances, etc.\nSo far, adversarial training (AT) methods are widely accepted as the most effective mechanisms for improving ViT's robustness against adversarial attacks, by intentionally injecting adversarial examples into the training data. Unfortunately, existing AT solutions struggle with two limitations. First, they suffer from a trade-off between standard accuracy (i.e., the accuracy on clean images) and adversarial robustness (i.e., the accuracy on adversarial examples), with improved robustness while yielding non-negligible standard accuracy degradation. Second, these solutions are not effective against adaptive attacks, i.e., a category of adversarial attacks capable of exploiting the weak points of defense methods to adaptively adjust their attack strategies. Hence, it calls for the exploration of enhancing ViT's robustness against adaptive attacks.\nOne potential direction to tackle the trade-off between standard accuracy and adversarial robustness is the detection/rejection mechanism. This involves training an additional detector to identify and reject malicious input data, with several solutions proposed in the literature. However, these detection techniques have limited effectiveness against adaptive attacks and cannot be applied to scenarios involving natural adversarial examples, as reported in a prior study. Hence, it is crucial to develop novel solutions that can address limitations associated with the aforementioned direction and are suitable for a wide range of scenarios.\nIn this work, we aim to boost the robustness of ViT against adaptive attacks in a more general and challenging scenario where malicious inputs cannot be rejected. Such a scenario is common to several critical application domains, such as autonomous driving, where the system must correctly recognize a road sign even if it has been maliciously crafted. To this end, we propose a novel ViT architecture consisting of a detector and a classifier, connected by a newly developed adaptive ensemble. After adversarially trained by One-step Least-Likely Adversarial Training, our proposed ViT architecture can withstand adaptive attacks while incurring only a negligible standard accuracy degradation.\nIn essence, our detector incorporates two innovative designs to make adversarial examples more noticeable. First, based on our empirical observations, we introduce a novel Multi-head Self-Attention (MSA) mechanism to expose adversarial perturbation by Guided Backpropagation. Second, the Soft-Nearest Neighbors Loss (SNN Loss) is tailored to push adversarial examples away from their corresponding clean images. Our detector thus can effectively sniff adaptive attack-generated adversarial examples. On the other hand, our classifier's adversarial training involves two stages: pre-training and fine-tuning. During the pre-training stage, our classifier utilizes one clean encoder, one adversarial encoder, and one decoder to jointly learn high-quality visual representations and encourage pairwise similarity between a clean image and its adversarial example. Here, we extend Masked Autoencoders (MAE) to facilitate adversarial training through a new design. Specifically, we reconstruct images from one pair of a masked clean image and its masked adversarial example, for representation learning, with a contrastive loss on a pair of visual representations to encourage similarity. In the fine-tuning stage, we discard the decoder and freeze the weights in the well-trained detector and two encoders, with a newly developed adaptive ensemble to bridge the detector and the two encoders, for fine-tuning an MLP (Multi-layer Perceptron) for accurate classification. Our adaptive ensemble also masks off a random subset of image patches within the input, enabling our approach to mitigate adversarial effects when encountering malicious inputs. Extensive experimental results on three popular benchmarks demonstrate that our approach outperforms state-of-the-art adversarial training techniques in terms of both standard accuracy and adversarial robustness."}, {"title": "2 RELATED WORK", "content": "Detection Mechanisms. Detecting adversarial examples (AEs) and then rejecting them (i.e., detection/rejection mechanism) can improve the model's robustness against adversarial attacks. That is, the input will be rejected if the detector classifies it as an adversarial example. Popular detection techniques include Odds, which considers the difference between clean images and AEs in terms of log-odds; NIC, which checks channel invariants within deep neural networks (DNNs); GAT, which resorts to multiple binary classifiers; JTLA, which proposes a detection framework by employing internal layer representations, among others. Unfortunately, existing detection methods are typically ineffective in defending against adaptive attacks. Besides, the detection/rejection mechanism cannot be generalized to domains where natural adversarial examples exist. Our work differs from previous solutions in two aspects. First, we introduce a novel Multi-head Self-Attention (MSA) mechanism by using the Guided Backpropagation technique, which can largely expose adversarial perturbations. Second, we incorporate the Soft-Nearest Neighbors (SNN) loss to maximize the differences between clean images and adversarial examples. These innovative designs enable our detector to effectively defend against adaptive attacks. Moreover, our newly developed adaptive ensemble further enhances our detector, empowering it to be applied to scenarios where rejecting input images is not allowed.\nAdversarial Training Approaches. Adversarial training (AT) aims to improve the model's robustness against adversarial attacks by intentionally injecting adversarial examples into the training data. For example, PGD-AT proposes a multi-step attack to find the worst case of training data, TRADES addresses the limitation of PGD-AT by utilizing theoretically sound classification-calibrated loss, EAT uses an ensemble of different DNNs to produce the threat model, FAT reduces the computational overhead of AT by utilizing FGSM attack with the random initialization, LAS-AWP boosts AT with a learnable attack strategy, Sub-AT constrains AT in a well-designed subspace, and many others. However, prior ATs suffer from the dilemma of balancing the trade-off between standard accuracy and adversarial robustness. Besides, their improved robustness is vulnerable to adaptive attacks. In contrast, our work introduces a ViT architecture consisting of a detector and a classifier, connected by a newly developed adaptive ensemble, able to boost AT to defend against adaptive attacks. Meanwhile, it lowers the standard accuracy degradation by employing two encoders for extracting visual representations respectively from clean images and adversarial examples, empowering our ViT architecture to enjoy a better trade-off between accuracy and robustness."}, {"title": "3 PRELIMINARY: ONE-STEP LEAST-LIKELY ADVERSARIAL TRAINING", "content": "Adversarial training (AT) improves the model's robustness against adversarial attacks by feeding adversarial examples into the training set. Given a model \\(f\\) with parameters \\(\\theta\\), a dataset with N samples, i.e., \\(X = \\{(x_i, y_i) | i \\in \\{1, 2, ..., N\\}\\}\\), the cross-entropy loss function \\(L\\), and a threat model \\(A\\), AT aims to solve the following inner-maximization problem and outer-minimization problem, i.e.,\n$$\\min_\\theta \\sum_{i=1}^{N} \\max_{\\delta \\in \\Delta} L(f_\\theta(x_i + \\delta), y_i),$$\nwhere the inner problem aims to find the worst-case training data for the given model, and the outer problem aims to improve the model's performance on such data. Recently, one-step Fast Adversarial Training (FAT) is popular due to its computational efficiency. FAT sets the threat model under a small and \\(l_\\infty\\) constraint \\(\\epsilon\\), i.e., \\(\\Delta = \\{\\delta : ||\\delta||_\\infty \\leq \\epsilon\\}\\), by performing Fast Gradient Sign Method (FGSM) with the random initialization, i.e.,\n$$\\begin{aligned}\n    &\\delta = \\text{Uniform}(-\\epsilon, \\epsilon) + \\epsilon \\cdot \\text{sign}(\\nabla_x L(f_\\theta(x_i), y_i)), \\\\\n    &\\delta = \\max(\\min(\\delta, \\epsilon), -\\epsilon),\n\\end{aligned}$$\nwhere Uniform denotes the uniform distribution and sign is the sign function. Notably, the second row in Eq. (2) serves to project the perturbation \\(\\delta\\) back into the \\(l_\\infty\\) ball around the data \\(x_i\\).\nTo find the worst-case adversarial examples, we extend FAT by performing the least-likely targeted attacks, inspired by prior studies. That is, given an input \\(x_i\\), we perform targeted FGSM by setting the targeted label as its least-likely class, i.e., \\(y' = \\text{arg}\\min f_\\theta(x_i)\\), arriving at,\n$$\\begin{aligned}\n    &\\delta = \\text{Uniform}(-\\epsilon, \\epsilon) + \\epsilon \\cdot \\text{sign}(\\nabla_x L(f_\\theta(x_i), y')), \\\\\n    &\\delta = \\max(\\min(\\delta, \\epsilon), -\\epsilon),\n\\end{aligned}$$\nOur one-step least-likely adversarial training is to utilize Eq.(3) to produce the threat model."}, {"title": "4 OUR APPROACHES", "content": "4.1 Problem Statement\nWe consider a set of N samples, i.e., \\(X = \\{(x_i, y_i) | i \\in \\{1, 2, ..., N\\}\\}\\), where \\(x \\in \\mathbb{R}^{H \\times W \\times C_H}\\) is an input image with the resolution of \\((H, W)\\) and the channel count of \\(C_H\\), and \\(y \\in [C]\\) denotes its label. For notational convenience, we let \\(d = H \\times W \\times C_H\\). A classifier is a function \\(f_\\theta: \\mathbb{R}^d \\rightarrow [C]\\), parameterized by a neural network. We consider two types of inputs, i.e., a clean image \\(x^{cln}\\) sampled from the standard distribution \\(D_{std}\\) and an adversarial example \\(x^{adv}\\) sampled from the adversarial distribution \\(D_{adv}\\). We assume \\(D_{std}\\) and \\(D_{adv}\\) follow different distributions. The clean image \\(x^{cln}\\) itself or its augmented variant can be the input, while the adversarial example \\(x^{adv}\\) is a malicious version of \\(x\\) within a small distance. That is, for some metric \\(d\\), we have \\(d(x, x^{adv}) \\leq \\epsilon\\), but \\(x^{adv}\\) can mislead conventional classifiers. Parameterized by another neural network, a detector \\(g_\\phi\\) is to tell whether an input image is a clean image or not, i.e., \\(g_\\phi: \\mathbb{R}^d \\rightarrow \\{\\pm 1\\}\\), where +1 and -1 indicate a clean image and an adversarial example, respectively. The binary indicator function \\(\\mathbb{1}\\{\\cdot\\}\\) is 1 if both the detector \\(g_\\phi\\) and the classifier \\(f_\\theta\\) make correct predictions. We follow previous studies by referring standard accuracy, and adversarial robustness, as classification accuracy on clean images and adversarial examples, respectively.\n4.2 Detector\nParameterized by a neural network with parameters \\(\\phi\\), the detector \\(g_\\phi: \\mathbb{R}^d \\rightarrow \\{\\pm 1\\}\\) is to determine whether the input is a clean image or not, where +1 and -1 respectively represent a clean image and an adversarial example, i.e.,\n$$g_\\phi(x) = \\begin{cases}\n    +1, \\text{ if } x \\text{ is a clean image} \\\\\n    -1, \\text{ otherwise}.\n\\end{cases}$$\nAiming to generalize the robust model to critical domains (e.g., autonomous driving), the input will not be rejected in this work. Instead, we have modified it to output an estimated probability of \\(p \\in [0, 1]\\) for clean images and \\(1 - p\\) for adversarial examples.\nThe design of our detector architecture is motivated by our empirical observation in that the adversarial perturbation is detectable after Guided Backpropagation visualization. Due to the small distance between a clean image and its corresponding adversarial example, their difference is notoriously imperceptible, making it theoretically hard to detect adversarial examples. In our empirical study, we resort to Guided Backpropagation to visualize the difference between a clean image and an adversarial example. Interestingly, we have discovered that after Guided Backpropagation visualization on the adversarial example, its adversarial perturbation is quite noticeable; see Figure 1c versus Figure 1f, i.e., visualization on a clean image versus on its adversarial example. Notably, our experiments also include the visualization comparison of Guided Grad-CAM , developed recently; see Figure 1b versus Figure 1e. However, Guided Grad-CAM exhibits inferior performance (compared to Guided Backpropagation) in terms of exposing adversarial perturbation. This empirical study motivates us to maximize the difference between clean images and adversarial examples by using Guided Backpropagation visualization.\nGiven an input image \\(x \\in \\mathbb{R}^d\\), we perform Guided Backpropagation on the original image, arriving at an input variant \\(x' \\in \\mathbb{R}^d\\). Following the standard Vision Transformers (ViT), we patchify the two inputs into two sets of image patches and embed them via linear projection, arriving at two sets of patch embeddings, i.e., \\(E_p \\in \\mathbb{R}^{M \\times D}\\) and \\(E'_p \\in \\mathbb{R}^{M \\times D}\\), respectively for the original input and its input variant. Here, \\(M\\) represents the number of patches and \\(D\\) indicates the hidden dimension. Driven by the above empirical observation, a naive idea to expose adversarial perturbation is to add two sets of patch embeddings. However, our empirical results show that this simple solution cannot achieve satisfactory performance. To address this issue, we propose a novel Multi-head Self-Attention (MSA) to consider two sets of patch embeddings simultaneously, inspired by recent studies. Let \\(E = E_p + E_{pos}\\) and \\(E' = E'_p + E_{pos}\\) respectively represent two sets of patch embeddings after adding positional embeddings \\(E_{pos} \\in \\mathbb{R}^{M \\times D}\\), our proposed MSA can be expressed as follows:\n$$\\text{MSA}(Q, K, V) = \\text{Softmax}(\\frac{QK^T + B}{\\sqrt{d_k}})V,$$\n$$Q = W_Q \\cdot E, K = W_K \\cdot E, V = W_V \\cdot E.$$\nHere, \\(B = W_B E'\\) is the relative detection bias obtained from the Guided Backpropagation-based input variant. \\(W_Q, W_K, W_V\\), and \\(W_B\\) are learnable projection matrices, similar to those in prior studies. The intuition underlying Eq. (5) is that we aim to expose adversarial perturbation by adding the relative bias obtained from Guided Backpropagation visualization. After encoding, we follow Masked Autoencoders (MAE) by performing global average pooling on the full set of encoded patch embeddings, with the resulting token fed into an MLP (i.e., multiple-layer perceptron) for telling whether the input is a clean image or not.\nAiming to further differentiate adversarial examples from clean images, we propose a novel loss function to train our detector, including a Cross-Entropy (CE) Loss \\(L_{ce}\\) and a Soft-Nearest Neighbors (SNN) loss \\(L_{snn}\\), for jointly penalizing the detection error and the similarity level between the clean image and the adversarial example, i.e.,\n$$L_{det} = (1 - \\lambda) \\cdot L_{ce}(g_\\phi(x), y_{det}) + \\lambda \\cdot L_{snn}(z^{cln}, z^{adv}),$$\nwhere \\(\\lambda \\in (0, 1)\\) is a hyperparameter to control the penalty degree of the two terms, and \\(z^{cln}\\) and \\(z^{adv}\\) denote the global representations, i.e., the global average pooling of encoded representations, for clean images and adversarial examples, respectively.\nThe SNN loss is a variant of contrastive loss, allowing for the inclusion of multiple positive pairs. We regard members belonging to the same determined class (e.g., two clean images) as positive pairs, while members belonging to different determined classes (e.g., a clean image and an adversarial example) as negative pairs. Given a mini-batch of 2B samples, with one half being clean images, i.e., \\(\\{(x_i, y_{det}=1)\\}_{i=1}^B\\), and the other half of adversarial examples, i.e., \\(\\{(x^{adv}_i, y_{det}=-1)\\}_{i=B+1}^{2B}\\), the SNN loss at temperature \\(\\tau\\) is defined below:\n$$L_{snn} = \\frac{1}{2B} \\sum_{i=1}^B \\log \\frac{\\sum_{j=1, i \\neq j, y_{det} = y^{j}_{det}}^{2B} \\exp(-\\text{sim}(z_i, z_j)/\\tau)}{\\sum_{k=1, i \\neq k}^{2B} \\exp(-\\text{sim}(z_i, z_k)/\\tau)},$$\nwhere \\(z_i\\) is the visual representations for the input \\(x_i\\) and the similarity metric \\(\\text{sim}(\\cdot, \\cdot)\\) is measured by the cosine distance. The SNN loss enforces each point to be closer to its positive pairs than to its negative pairs. In other words, the SNN loss penalizes the similarity level between clean images and adversarial examples, making adversarial examples more discernible by our detector.\n4.3 Classifer\nInspired by self-supervised learning for vision tasks, we separate our adversarial training into two stages, i.e., pre-training and fine-tuning, for learning high-quality visual representations and fine-tuning a robust classifier, respectively.\nPre-training. Our classifier architecture for the pre-training is inspired by MAE . Different from MAE, we utilize two encoders, denoted as the clean encoder and the adversarial encoder, for learning visual representations from clean images and adversarial examples, respectively. The decoder aims to reconstruct the original inputs from the visual representations encoded by the two encoders.  shows the classifier architecture during the pre-training. Given an input image \\(x \\in \\mathbb{R}^d\\), let \\(x^{cln}\\) and \\(x^{adv}\\) denote its clean and adversarial variants, respectively, with the clean variant obtained by augmenting the original input. Regarding the clean variant \\(x^{cln}\\), we randomly mask out a large proportion of image patches (e.g., 75%) and then feed the subset of visible patches into the clean encoder. The masked tokens are inserted into corresponding positions after the encoder. Finally, the decoder reconstructs the clean variant \\(\\hat{x}^{cln}\\) from the full set of image patches, including encoded visible patches and masked tokens. The reconstruction of the adversarial variant \\(\\hat{x}^{adv}\\) follows a similar procedure, except that its visible patches are encoded by the adversarial encoder. Notably, the position of masked image patches in the adversarial variant \\(x^{adv}\\) is the same as that in the clean variant \\(x^{cln}\\) in order to minimize their visual representation difference during the pre-training.\nLet \\(\\hat{z}^{cln}\\) and \\(\\hat{z}^{adv}\\) respectively denote the global representations of clean and adversarial variants, obtained by performing global average pooling on the decoder's input sequence. Our design utilizes a new loss function to learn visual representations by simultaneously minimizing the reconstruction error and the visual representation difference, i.e.,\n$$L_{enc} = (1 - \\Omega) \\cdot L_{rec}(x, \\hat{x}) + \\Omega \\cdot L_{cl}(\\hat{z}^{cln}, \\hat{z}^{adv}),$$\nwhere \\(\\Omega \\in (0, 1)\\) is a hyperparameter and \\(\\hat{x}\\) is the reconstructed image. \\(L_{rec}\\) and \\(L_{cl}\\) denote the reconstruction loss and the contrastive loss, respectively. Given a set of B input images, we first generate their adversarial variants, arriving at a mini-batch of 2B samples, consisting of B clean variants \\(\\{(x^{cln}_i)\\}_{i=1}^B\\) and B adversarial variants \\(\\{(x^{adv}_i)\\}_{i=B+1}^{2B}\\). We consider the form of contrastive loss in SimCLR , and define our contrastive loss at temperature \\(\\tau\\) as follows:\n$$l(i, j) = - \\log \\frac{\\exp(\\text{sim}(\\hat{z}_i, \\hat{z}_j)/\\tau)}{\\sum_{i \\neq k, k=1,...,2B} \\exp(\\text{sim}(\\hat{z}_i, \\hat{z}_k)/\\tau)},$$\n$$L_{cl} = \\frac{1}{2B} \\sum_{k=1}^B [l(k, k + B) + l(k + 1 + B, k)],$$\nwhere \\(\\hat{z}_i\\) denotes visual representations for \\(x^{cln}\\) (or \\(x^{adv}\\)) and the similarity level \\(\\text{sim}(\\cdot, \\cdot)\\) is measured by the cosine distance. In particular, we regard clean and adversarial variants from the same input as positive pairs, while the rest in the same batch are negative pairs. Hence, the loss value decreases when visual representations for the clean and the adversarial variants of the same input become more similar.\nFine-tuning. The detector and the classifier (including two encoders and one decoder) are trained jointly in the pre-training stage. After that, we drop the decoder and freeze the weights in the well-trained detector and two encoders, with Figure 3 depicting our model architecture during the fine-tuning stage. Different from MAE, which encodes the full set of image patches during the fine-tuning, our approach randomly masks out a relatively small proportion of image patches (e.g., 45%), aiming to eliminate the potential adversarial effect if the input is an adversarial example.\nGiven an input image \\((x, y_{cls})\\), where \\(x \\in \\mathbb{R}^d\\) is either a clean image or an adversarial example with the label \\(y_{cls} \\in [C]\\), we randomly mask the input image twice, arriving at two different masked inputs. Two subsets of visible patches from the two masked inputs are fed into the clean and the adversarial encoders, respectively. The masked tokens are introduced onto their corresponding positions after the encoder, obtaining two full sets of visual representations, i.e., \\(z^{cln}\\) and \\(z^{adv}\\) which are partially encoded by the clean and the adversarial encoders, respectively. We then perform the global average pooling on the adaptive ensemble of \\(\\hat{z}^{cln}\\) and \\(\\hat{z}^{adv}\\), with the result fed into an MLP for classification.\nAdaptive Ensemble. Although randomly masking an input image can eliminate the potential adversarial effect, this way inevitably hurts standard accuracy during the fine-tuning. In this paper, we propose adaptive ensemble to tackle this issue. That is, the global representation for an input image is derived from the sum of \\(\\hat{z}^{cln}\\) and \\(\\hat{z}^{adv}\\) with an adaptive factor \\(p \\in [0, 1]\\), where \\(\\hat{z}^{cln}\\) and \\(\\hat{z}^{adv}\\) are visual representations encoded by the clean and the adversarial encoders, respectively, and \\(p\\) is the probability of the input image being a clean image estimated by our detector.\nLet \\(A\\) be a full set of image patches and \\(V\\) be a subset of \\(A\\), including visible patches only. \\(\\mathbb{1}_V(i)\\) is the indicator function for evaluating whether an image patch is visible. Hence, for each image patch of \\(A\\), we have,\n$$\\mathbb{1}_V(i) = \\begin{cases}\n    1, & \\text{if the patch is visible} \\\\\n    0, & \\text{otherwise}\n\\end{cases}, i = 1, 2, ..., M,$$\nwhere \\(M\\) is the number of image patches, i.e., \\(|A|\\). For notational convenience, we let \\(\\mathbb{1}^{cln}\\) indicate visible patches fed into the clean encoder. Likewise, \\(\\mathbb{1}^{adv}\\) indicate visible patches fed into the adversarial encoder. Let \\(\\hat{z}_i\\) be the visual representation of the \\(i\\)-th image patch, with \\(i \\in \\{1, 2, ..., M\\}\\). Our adaptive ensemble is defined by:\n$$z_i = \\frac{p \\cdot \\mathbb{1}^{cln}(i) \\cdot z^{cln}_i + (1 - p) \\cdot \\mathbb{1}^{adv}(i) \\cdot z^{adv}_i}{\\max(p \\cdot \\mathbb{1}^{cln}(i) + (1 - p) \\cdot \\mathbb{1}^{adv}(i), \\epsilon)},$$\nwhere the denominator serves to normalize the adaptive ensemble of \\(\\hat{z}^{cln}\\) and \\(\\hat{z}^{adv}\\), and \\(\\epsilon\\) is a small value to avoid division by zero (i.e., \\(\\epsilon = 1e-12\\) in this paper). The intuition underlying Eq. (11) is that if our detector has a high confidence that the input is a clean image (i.e., \\(p\\) is large), the global representation \\(z_i\\) will be mostly encoded by the clean encoder. Otherwise, \\(z_i\\) will be mainly encoded by the adversarial encoder. In addition, as our pre-training encourages the similarity level of the clean and the adversarial variants from a given input (see Eq. (8) and Eq. (9)), and two different masked inputs exist upon the fine-tuning, the invisible image patches in one masked input can be glimpsed from the other masked input."}, {"title": "5 EXPERIMENTS AND RESULTS", "content": "5.1 Experimental Setup\nDatasets. We conduct experiments on three widely-used benchmarks. (i) CIFAR-10 : 60,000 32x32 RGB images of 10 classes. (ii) CIFAR-100 : 60,000 32x32 RGB examples in 100 categories. (iii) Tiny-ImageNet : 120,000 64x64 RGB images of 200 classes.\nCompared Methods. We compare our approach with four detection methods, i.e., Odds , NIC , GAT , and JTLA . We compare our approach with five adversarial training (AT) counterparts: PGD-AT , TRADES , FAT , Sub-AT , and LAS-AWP , to exhibit how it boosts the ViT's robustness.\nEvaluation. We consider three state-of-the-art adaptive attacks, i.e., AutoAttack , Adaptive Auto Attack (A\u00b3) , and Parameter-Free Adaptive Auto Attack (PF-A\u00b3) , for evaluating detection accuracy and adversarial robustness. The attack constraint, if not specified, is set to \\(\\epsilon = 8/255\\).\nModel Size. We build our detector and classifier on top of Vision Transformers (ViT), with their architectures following ViT and MAE , respectively. Our model size is pruned down to as small as possible in order to conduct a fair comparison with baselines. Table 1 lists the model size details. Our architecture consists of a detector and a classifier (including two encoders and one decoder), with 54.0M parameters in total. To conduct a fair comparison, existing adversarial training baselines use the ViT-Base model with total parameters of 85.6M as the backbone network.\nHyperparameters. For all our models, if not specified, we use AdamW with \\(\\beta_1=0.9\\), \\(\\beta_2=0.999\\), the weight decay of 0.05, and a batch size of 512. During the pre-training, the detector and the classifier (i.e., two encoders and one decoder) are trained jointly. For the detector, we follow the setting in by setting the epochs of 100, the base learning rate of 1e - 3, the linear warmup epochs of 5, and the cosine decay schedule . For the classifier, by contrast, we pre-train it for 200 epochs, with the base learning rate of 1e - 4, the linear warmup of 20 epochs, and a masking ratio of 75%. After pre-training, we drop the decoder and freeze the weights on the detector and the two encoders. Then, we finetune the classifier for 100 epochs, with the base learning rate of 1e - 3, the linear warmup of 5, and the cosine decay schedule , and a masking ratio of 45%. The patch size is set to 4 (or 8) for CIFAR-10/CIFAR-100 (or Tiny-ImageNet). We grid-search hyperparamters \\(\\lambda\\) in Eq. (6) and \\(\\Omega\\) in Eq. (8) of Section 4 and empirically set \\(\\lambda\\) to 0.15 and \\(\\Omega\\) to 0.35 for all datasets.\n5.2 Overall Performance on Our Classifier\nOverall Comparisons on CIFAR-10. We first conduct extensive experiments on CIFAR-10 and compare our approach to its state-of-the-art adversarial training (AT) counterparts listed in Section 5.1 in terms of standard accuracy and adversarial robustness under attack constraints of \\(\\epsilon = 4/255\\) and of \\(\\epsilon = 8/255\\). Table 2 lists comparative results. It is observed that our approach achieves the best performance under all three scenarios. In particular, our approach achieves the standard accuracy of 90.3%, outperforming the best competitor (i.e., LAS-AWP) by 3.5%. This is contributed by employing two encoders to extract visual representations respectively from clean images and adversarial examples, able to significantly mitigate the adverse effect of adversarial training on standard accuracy. Besides, when the attack constraint is set to \\(\\epsilon = 4/255\\), our approach achieves the best robustness of 49.8%, 49.5%, and 48.1% against AutoAttack, Adaptive Auto Attack (A3), and Parameter-Free Adaptive Auto Attack (PF-A\u00b3), respectively. Our method significantly surpasses all its counterparts. For example, it outperforms two recent state-of-the-arts, i.e., Sub-AT and LAS-AWP, respectively by 2.6% and 4.5% under the attack of A\u00b3. Thirdly, increasing the attack constraint to \\(\\epsilon = 8/255\\) results in the decrease of adversarial robustness. But our approach still maintains the best robustness of 45.3%, 44.5%, and 44.7% under the attack of AutoAttack, A\u00b3, and PF-A\u00b3, respectively. The comparative results demonstrate that our masked adaptive ensemble is robust enough to withstand strong white-box attacks. This is because masking a small proportion of image patches can significantly mitigate the adversarial effect of malicious inputs.\nOverall Comparisons on CIFAR-100 and Tiny-ImageNet. Here, we conduct a comprehensive comparison between our approach and adversarial training (AT) counterparts on CIFAR-100 and Tiny-ImageNet datasets. Table 3 lists the comparative results. On CIFAR-100, we observed that our approach achieves the best standard accuracy of 67.5%, outperforming the best competitor (i.e., LAS-AWP) by 3.4%. Meanwhile, our method achieves the best robustness of 36.9%, 35.1%, and 35.4% under the attack of AutoAttack, A\u00b3, and PF-A3, respectively. This confirms that our approach can achieve a decent standard accuracy and robustness when being generalized to the dataset with large classes. On the Tiny-ImageNet dataset, both our approach and the baseline methods experience a decrease in performance. However, our proposed method still achieves the highest standard accuracy of 49.7%, which outperforms the best baseline (i.e., FAT), by 2.6%. Moreover, all baselines suffer from a poor robustness on the Tiny-ImageNet dataset (i.e., \\(\\leq\\) 20.0%), while our approach maintains a decent robustness of 22.6%, 21.4%, and 20.9% under the attack of AutoAttack, A\u00b3, and PF-A\u00b3, respectively.\nPerformance Stability. We next conduct experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet to evaluate the performance stability under different scales of datasets and different types of adaptive attacks. We compare our approach with three baselines, i.e., FAT, Sub-AT, and LAS-AWP. Figures 4a, 4b, 4c and 4d illustrate the comparative results of standard accuracy, as well as robustness against AutoAttack, A\u00b3, and PF-A\u00b3, respectively. We have three discoveries. First, as depicted in Figure 4a, our approach (i.e., the pink line) achieves the best standard accuracies of 90.3%, 67.5%, and 49.7% under CIFAR-10, CIFAR-100, and Tiny-ImageNet, respectively. The empirical evidence verifies that our approach can maintain superior standard accuracy when generalized to large datasets. Second, on all three datasets, our approach achieves the best robustness under all adaptive attacks, as shown in Figures 4b, 4c and 4d. Take the robustness results under PF-A\u00b3 (i.e., Figure 4d) for example, our proposed masked adaptive ensemble achieves the robustness of 44.7%, 35.4%, and 20.9% on CIFAR-10, CIFAR-100, and Tiny-ImageNet, respectively. These results outperform those of LAS-AWP (i.e., the blue line), which is the best baseline, by 3.1%, 4.1%, and 3.5%, respectively. Third, when scaling up the dataset from CIFAR-10 to Tiny-ImageNet, our approach suffers from the least robustness degradation of 22.7%, 23.1%, and 23.8% under the"}, {"title": "5.3 Ablation Studies on Our Classifier", "content": "Pre-training: Contrastive Loss. We qualitatively and quantitatively exhibit the impact of our proposed loss```json\n{\n  \"title\": \"Towards Robust Vision Transformer via Masked Adaptive Ensemble\"", "authors": ["n    \"Fudong Lin\",\n    \"Xu Yuan\",\n    \"Jiadong Lou\",\n    \"Nian-Feng Tzeng\"\n  ],\n  \"abstract\":", "Adversarial training (AT) can help improve the robustness of Vision Transformers (ViT) against adversarial attacks by intentionally injecting adversarial examples into the training data. However, this way of adversarial injection inevitably incurs standard accuracy degradation to some extent, thereby calling for a trade-off between standard accuracy and adversarial robustness. Besides, the prominent AT solutions are still vulnerable to adaptive attacks. To tackle such shortcomings, this paper proposes a novel ViT architecture, including a detector and a classifier bridged by our newly developed adaptive ensemble. Specifically, we empirically discover that detecting adversarial examples can benefit from the Guided Backpropagation technique. Driven by this discovery, a novel Multi-head Self-Attention (MSA) mechanism is introduced for enhancing our detector to sniff adversarial examples. Then, a classifier with two encoders is employed for extracting visual representations respectively from clean images and adversarial examples, with our adaptive ensemble to adaptively adjust the proportion of visual representations from the two encoders for accurate classification. This design enables our ViT architecture to achieve a better trade-off between standard accuracy and adversarial robustness. Besides, the adaptive ensemble technique allows us to mask off a random subset of image patches within input data, boosting our ViT's robustness against adaptive attacks, while maintaining high standard accuracy. Experimental results exhibit that our ViT architecture, on CIFAR-10, achieves the best standard accuracy and adversarial robustness of 90.3% and 49.8%, respectively.", "sections\": [\n    {\n      \"title\": \"1 INTRODUCTION", "n      \"content\":", "The Vision Transformers (ViT) architecture has demonstrated impressive capabilities in a wide range of vision tasks, including image and video classification, dense prediction tasks, self-supervised learning, among others. However, similar to Convolutional Neural Networks (CNNs), the ViT architecture is vulnerable to adversarial attacks achieved by maliciously altering clean images within a small distance, leading to incorrect predictions with high confidence. This vulnerability hinders the adoption of ViT in critical domains such as healthcare, finances, etc.\nSo far, adversarial training (AT) methods are widely accepted as the most effective mechanisms for improving ViT's robustness against adversarial attacks, by intentionally injecting adversarial examples into the training data. Unfortunately, existing AT solutions struggle with two limitations. First, they suffer from a trade-off between standard accuracy (i.e., the accuracy on clean images) and adversarial robustness (i.e., the accuracy on adversarial examples), with improved robustness while yielding non-negligible standard accuracy degradation. Second, these solutions are not effective against adaptive attacks, i.e., a category of adversarial attacks capable of exploiting the weak points of defense methods to adaptively adjust their attack strategies. Hence, it calls for the exploration of enhancing ViT's robustness against adaptive attacks.\nOne potential direction to tackle the trade-off between standard accuracy and adversarial robustness is the detection/rejection mechanism. This involves training an additional detector to identify and reject malicious input data, with several solutions proposed in the literature. However, these detection techniques have limited effectiveness against adaptive attacks and cannot be applied to scenarios involving natural adversarial examples, as reported in a prior study. Hence, it is crucial to develop novel solutions that can address limitations associated with the aforementioned direction and are suitable for a wide range of scenarios.\nIn this work, we aim to boost the robustness of ViT against adaptive attacks in a more general and challenging scenario where malicious inputs cannot be rejected. Such a scenario is common to several critical application domains, such as autonomous driving, where the system must correctly recognize a road sign even if it has been maliciously crafted. To this end, we propose a novel ViT architecture consisting of a detector and a classifier, connected by a newly developed adaptive ensemble. After adversarially trained by One-step Least-Likely Adversarial Training, our proposed ViT architecture can withstand adaptive attacks while incurring only a negligible standard accuracy degradation.\nIn essence, our detector incorporates two innovative designs to make adversarial examples more noticeable. First, based on our empirical observations, we introduce a novel Multi-head Self-Attention (MSA) mechanism to expose adversarial perturbation by Guided Backpropagation. Second, the Soft-Nearest Neighbors Loss (SNN Loss) is tailored to push adversarial examples away from their corresponding clean images. Our detector thus can effectively sniff adaptive attack-generated adversarial examples. On the other hand, our classifier's adversarial training involves two stages: pre-training and fine-tuning. During the pre-training stage, our classifier utilizes one clean encoder, one adversarial encoder, and one decoder to jointly learn high-quality visual representations and encourage pairwise similarity between a clean image and its adversarial example. Here, we extend Masked Autoencoders (MAE) to facilitate adversarial training through a new design. Specifically, we reconstruct images from one pair of a masked clean image and its masked adversarial example, for representation learning, with a contrastive loss on a pair of visual representations. In the fine-tuning stage, we discard the decoder and freeze the weights in the well-trained detector and two encoders, with a newly developed adaptive ensemble to bridge the detector and the two encoders, for fine-tuning an MLP (Multi-layer Perceptron) for accurate classification. Our adaptive ensemble also masks off a random subset of image patches within the input, enabling our approach to mitigate adversarial effects when encountering malicious inputs. Extensive experimental results on three popular benchmarks demonstrate that our approach outperforms state-of-the-art adversarial training techniques in terms of both standard accuracy and adversarial robustness.", "n    },\n    {\n      \"title\": \"2 RELATED WORK\",\n      \"content\":", "Detection Mechanisms. Detecting adversarial examples (AEs) and then rejecting them (i.e., detection/rejection mechanism) can improve the model's robustness against adversarial attacks. That is, the input will be rejected if the detector classifies it as an adversarial example. Popular detection techniques include Odds, which considers the difference between clean images and AEs in terms of log-odds; NIC, which checks channel invariants within deep neural networks (DNNs); GAT, which resorts to multiple binary classifiers; JTLA, which proposes a detection framework by employing internal layer representations, among others. Unfortunately, existing detection methods are typically ineffective in defending against adaptive attacks. Besides, the detection/rejection mechanism cannot be generalized to domains where natural adversarial examples exist. Our work differs from previous solutions in two aspects. First, we introduce a novel Multi-head Self-Attention (MSA) mechanism by using the Guided Backpropagation technique, which can largely expose adversarial perturbations. Second, we incorporate the Soft-Nearest Neighbors (SNN) loss to maximize the differences between clean images and adversarial examples. These innovative designs enable our detector to effectively defend against adaptive attacks. Moreover, our newly developed adaptive ensemble further enhances our detector, empowering it to be applied to scenarios where rejecting input images is not allowed.\nAdversarial Training Approaches. Adversarial training (AT) aims to improve the model's robustness against adversarial attacks by intentionally injecting adversarial examples into the training data. For example, PGD-AT proposes a multi-step attack to find the worst case of training data, TRADES addresses the limitation of PGD-AT by utilizing theoretically sound classification-calibrated loss, EAT uses an ensemble of different DNNs to produce the threat model, FAT reduces the computational overhead of AT by utilizing FGSM attack with the random initialization, LAS-AWP boosts AT with a learnable attack strategy, Sub-AT constrains AT in a well-designed subspace, and many others. However, prior ATs suffer from the dilemma of balancing the trade-off between standard accuracy and adversarial robustness. Besides, their improved robustness is vulnerable to adaptive attacks. In contrast, our work introduces a ViT architecture consisting of a detector and a classifier, connected by a newly developed adaptive ensemble, able to boost AT to defend against adaptive attacks. Meanwhile, it lowers the standard accuracy degradation by employing two encoders for extracting visual representations respectively from clean images and adversarial examples, empowering our ViT architecture to enjoy a better trade-off between accuracy and robustness.", "n    },\n    {\n      \"title\": \"3 PRELIMINARY: ONE-STEP LEAST-LIKELY ADVERSARIAL TRAINING\",\n      \"content\":", "Adversarial training (AT) improves the model's robustness against adversarial attacks by feeding adversarial examples into the training set. Given a model \\(f\\) with parameters \\(\\theta\\), a dataset with N samples, i.e., \\(X = \\{(x_i, y_i) | i \\in \\{1, 2, ..., N\\}\\}\\), the cross-entropy loss function \\(L\\), and a threat model \\(A\\), AT aims to solve the following inner-maximization problem and outer-minimization problem, i.e.,\n$$\\min_\\theta \\sum_{i=1}^{N} \\max_{\\delta \\in \\Delta} L(f_\\theta(x_i + \\delta), y_i),$$\nwhere the inner problem aims to find the worst-case training data for the given model, and the outer problem aims to improve the model's performance on such data. Recently, one-step Fast Adversarial Training (FAT) is popular due to its computational efficiency. FAT sets the threat model under a small and \\(l_\\infty\\) constraint \\(\\epsilon\\), i.e., \\(\\Delta = \\{\\delta : ||\\delta||_\\infty \\leq \\epsilon\\}\\), by performing Fast Gradient Sign Method (FGSM) with the random initialization, i.e.,\n$$\\begin{aligned}\n    &\\delta = \\text{Uniform}(-\\epsilon, \\epsilon) + \\epsilon \\cdot \\text{sign}(\\nabla_x L(f_\\theta(x_i), y_i)), \\\\\n    &\\delta = \\max(\\min(\\delta, \\epsilon), -\\epsilon),\n\\end{aligned}$$\nwhere Uniform denotes the uniform distribution and sign is the sign function. Notably, the second row in Eq. (2) serves to project the perturbation \\(\\delta\\) back into the \\(l_\\infty\\) ball around the data \\(x_i\\).\nTo find the worst-case adversarial examples, we extend FAT by performing the least-likely targeted attacks, inspired by prior studies. That is, given an input \\(x_i\\), we perform targeted FGSM by setting the targeted label as its least-likely class, i.e., \\(y' = \\text{arg}\\min f_\\theta(x_i)\\), arriving at,\n$$\\begin{aligned}\n    &\\delta = \\text{Uniform}(-\\epsilon, \\epsilon) + \\epsilon \\cdot \\text{sign}(\\nabla_x L(f_\\theta(x_i), y')), \\\\\n    &\\delta = \\max(\\min(\\delta, \\epsilon), -\\epsilon),\n\\end{aligned}$$\nOur one-step least-likely adversarial training is to utilize Eq.(3) to produce the threat model.", "n    },\n    {\n      \"title\": \"4 OUR APPROACHES\",\n      \"content\":", 4.1, "Problem Statement\nWe consider a set of N samples, i.e., \\(X = \\{(x_i, y_i) | i \\in \\{1, 2, ..., N\\}\\}\\), where \\(x \\in \\mathbb{R}^{H \\times W \\times C_H}\\) is an input image with the resolution of \\((H, W)\\) and the channel count of \\(C_H\\), and \\(y \\in [C]\\) denotes its label. For notational convenience, we let \\(d = H \\times W \\times C_H\\). A classifier is a function \\(f_\\theta: \\mathbb{R}^d \\rightarrow [C]\\), parameterized by a neural network. We consider two types of inputs, i.e., a clean image \\(x^{cln}\\) sampled from the standard distribution \\(D_{std}\\) and an adversarial example \\(x^{adv}\\) sampled from the adversarial distribution \\(D_{adv}\\). We assume \\(D_{std}\\) and \\(D_{adv}\\) follow different distributions. The clean image \\(x^{cln}\\) itself or its augmented variant can be the input, while the adversarial example \\(x^{adv}\\) is a malicious version of \\(x\\) within a small distance. That is, for some metric \\(d\\), we have \\(d(x, x^{adv}) \\leq \\epsilon\\), but \\(x^{adv}\\) can mislead conventional classifiers. Parameterized by another neural network, a detector \\(g_\\phi\\) is to tell whether an input image is a clean image or not, i.e., \\(g_\\phi: \\mathbb{R}^d \\rightarrow \\{\\pm 1\\}\\), where +1 and -1 indicate a clean image and an adversarial example, respectively. The binary indicator function \\(\\mathbb{1}\\{\\cdot\\}\\) is 1 if both the detector \\(g_\\phi\\) and the classifier \\(f_\\theta\\) make correct predictions. We follow previous studies by referring standard accuracy, and adversarial robustness, as classification accuracy on clean images and adversarial examples, respectively.\n4.2 Detector\nParameterized by a neural network with parameters \\(\\phi\\), the detector \\(g_\\phi: \\mathbb{R}^d \\rightarrow \\{\\pm 1\\}\\) is to determine whether the input is a clean image or not, where +1 and -1 respectively represent a clean image and an adversarial example, i.e.,\n$$g_\\phi(x) = \\begin{cases}\n    +1, \\text{ if } x \\text{ is a clean image} \\\\\n    -1, \\text{ otherwise}.\n\\end{cases}$$\nAiming to generalize the robust model to critical domains (e.g., autonomous driving), the input will not be rejected in this work. Instead, we have modified it to output an estimated probability of \\(p \\in [0, 1]\\) for clean images and \\(1 - p\\) for adversarial examples.\nThe design of our detector architecture is motivated by our empirical observation in that the adversarial perturbation is detectable after Guided Backpropagation visualization. Due to the small distance between a clean image and its corresponding adversarial example, their difference is notoriously imperceptible, making it theoretically hard to detect adversarial examples. In our empirical study, we resort to Guided Backpropagation to visualize the difference between a clean image and an adversarial example. Interestingly, we have discovered that after Guided Backpropagation visualization on the adversarial example, its adversarial perturbation is quite noticeable; see Figure 1c versus Figure 1f, i.e., visualization on a clean image versus on its adversarial example. Notably, our experiments also include the visualization comparison of Guided Grad-CAM , developed recently; see Figure 1b versus Figure 1e. However, Guided Grad-CAM exhibits inferior performance (compared to Guided Backpropagation) in terms of exposing adversarial perturbation. This empirical study motivates us to maximize the difference between clean images and adversarial examples by using Guided Backpropagation visualization.\nGiven an input image \\(x \\in \\mathbb{R}^d\\), we perform Guided Backpropagation on the original image, arriving at an input variant \\(x' \\in \\mathbb{R}^d\\). Following the standard Vision Transformers (ViT), we patchify the two inputs into two sets of image patches and embed them via linear projection, arriving at two sets of patch embeddings, i.e., \\(E_p \\in \\mathbb{R}^{M \\times D}\\) and \\(E'_p \\in \\mathbb{R}^{M \\times D}\\), respectively for the original input and its input variant. Here, \\(M\\) represents the number of patches and \\(D\\) indicates the hidden dimension. Driven by the above empirical observation, a naive idea to expose adversarial perturbation is to add two sets of patch embeddings. However, our empirical results show that this simple solution cannot achieve satisfactory performance. To address this issue, we propose a novel Multi-head Self-Attention (MSA) to consider two sets of patch embeddings simultaneously, inspired by recent studies. Let \\(E = E_p + E_{pos}\\) and \\(E' = E'_p + E_{pos}\\) respectively represent two sets of patch embeddings after adding positional embeddings \\(E_{pos} \\in \\mathbb{R}^{M \\times D}\\), our proposed MSA can be expressed as follows:\n$$\\text{MSA}(Q, K, V) = \\text{Softmax}(\\frac{QK^T + B}{\\sqrt{d_k}})V,$$\n$$Q = W_Q \\cdot E, K = W_K \\cdot E, V = W_V \\cdot E.$$\nHere, \\(B = W_B E'\\) is the relative detection bias obtained from the Guided Backpropagation-based input variant. \\(W_Q, W_K, W_V\\), and \\(W_B\\) are learnable projection matrices, similar to those in prior studies. The intuition underlying Eq. (5) is that we aim to expose adversarial perturbation by adding the relative bias obtained from Guided Backpropagation visualization. After encoding, we follow Masked Autoencoders (MAE) by performing global average pooling on the full set of encoded patch embeddings, with the resulting token fed into an MLP (i.e., multiple-layer perceptron) for telling whether the input is a clean image or not.\nAiming to further differentiate adversarial examples from clean images, we propose a novel loss function to train our detector, including a Cross-Entropy (CE) Loss \\(L_{ce}\\) and a Soft-Nearest Neighbors (SNN) loss \\(L_{snn}\\), for jointly penalizing the detection error and the similarity level between the clean image and the adversarial example, i.e.,\n$$L_{det} = (1 - \\lambda) \\cdot L_{ce}(g_\\phi(x), y_{det}) + \\lambda \\cdot L_{snn}(z^{cln}, z^{adv}),$$\nwhere \\(\\lambda \\in (0, 1)\\) is a hyperparameter to control the penalty degree of the two terms, and \\(z^{cln}\\) and \\(z^{adv}\\) denote the global representations, i.e., the global average pooling of encoded representations, for clean images and adversarial examples, respectively.\nThe SNN loss is a variant of contrastive loss, allowing for the inclusion of multiple positive pairs. We regard members belonging to the same determined class (e.g., two clean images) as positive pairs, while members belonging to different determined classes (e.g., a clean image and an adversarial example) as negative pairs. Given a mini-batch of 2B samples, with one half being clean images, i.e., \\(\\{(x_i, y_{det}=1)\\}_{i=1}^B\\), and the other half of adversarial examples, i.e., \\(\\{(x^{adv}_i, y_{det}=-1)\\}_{i=B+1}^{2B}\\), the SNN loss at temperature \\(\\tau\\) is defined below:\n$$L_{snn} = \\frac{1}{2B} \\sum_{i=1}^B \\log \\frac{\\sum_{j=1, i \\neq j, y_{det} = y^{j}_{det}}^{2B} \\exp(-\\text{sim}(z_i, z_j)/\\tau)}{\\sum_{k=1, i \\neq k}^{2B} \\exp(-\\text{sim}(z_i, z_k)/\\tau)},$$\nwhere \\(z_i\\) is the visual representations for the input \\(x_i\\) and the similarity metric \\(\\text{sim}(\\cdot, \\cdot)\\) is measured by the cosine distance. The SNN loss enforces each point to be closer to its positive pairs than to its negative pairs. In other words, the SNN loss penalizes the similarity level between clean images and adversarial examples, making adversarial examples more discernible by our detector.\n4.3 Classifer\nInspired by self-supervised learning for vision tasks, we separate our adversarial training into two stages, i.e., pre-training and fine-tuning, for learning high-quality visual representations and fine-tuning a robust classifier, respectively.\nPre-training. Our classifier architecture for the pre-training is inspired by MAE . Different from MAE, we utilize two encoders, denoted as the clean encoder and the adversarial encoder, for learning visual representations from clean images and adversarial examples, respectively. The decoder aims to reconstruct the original inputs from the visual representations encoded by the two encoders.  shows the classifier architecture during the pre-training. Given an input image \\(x \\in \\mathbb{R}^d\\), let \\(x^{cln}\\) and \\(x^{adv}\\) denote its clean and adversarial variants, respectively, with the clean variant obtained by augmenting the original input. Regarding the clean variant \\(x^{cln}\\), we randomly mask out a large proportion of image patches (e.g., 75%) and then feed the subset of visible patches into the clean encoder. The masked tokens are inserted into corresponding positions after the encoder. Finally, the decoder reconstructs the clean variant \\(\\hat{x}^{cln}\\) from the full set of image patches, including encoded visible patches and masked tokens. The reconstruction of the adversarial variant \\(\\hat{x}^{adv}\\) follows a similar procedure, except that its visible patches are encoded by the adversarial encoder. Notably, the position of masked image patches in the adversarial variant \\(x^{adv}\\) is the same as that in the clean variant \\(x^{cln}\\) in order to minimize their visual representation difference during the pre-training.\nLet \\(\\hat{z}^{cln}\\) and \\(\\hat{z}^{adv}\\) respectively denote the global representations of clean and adversarial variants, obtained by performing global average pooling on the decoder's input sequence. Our design utilizes a new loss function to learn visual representations by simultaneously minimizing the reconstruction error and the visual representation difference, i.e.,\n$$L_{enc} = (1 - \\Omega) \\cdot L_{rec}(x, \\hat{x}) + \\Omega \\cdot L_{cl}(\\hat{z}^{cln}, \\hat{z}^{adv}),$$\nwhere \\(\\Omega \\in (0, 1)\\) is a hyperparameter and \\(\\hat{x}\\) is the reconstructed image. \\(L_{rec}\\) and \\(L_{cl}\\) denote the reconstruction loss and the contrastive loss, respectively. Given a set of B input images, we first generate their adversarial variants, arriving at a mini-batch of 2B samples, consisting of B clean variants \\(\\{(x^{cln}_i)\\}_{i=1}^B\\) and B adversarial variants \\(\\{(x^{adv}_i)\\}_{i=B+1}^{2B}\\). We consider the form of contrastive loss in SimCLR , and define our contrastive loss at temperature \\(\\tau\\) as follows:\n$$l(i, j) = - \\log \\frac{\\exp(\\text{sim}(\\hat{z}_i, \\hat{z}_j)/\\tau)}{\\sum_{i \\neq k, k=1,...,2B} \\exp(\\text{sim}(\\hat{z}_i, \\hat{z}_k)/\\tau)},$$\n$$L_{cl} = \\frac{1}{2B} \\sum_{k=1}^B [l(k, k + B) + l(k + 1 + B, k)],$$\nwhere \\(\\hat{z}_i\\) denotes visual representations for \\(x^{cln}\\) (or \\(x^{adv}\\)) and the similarity level \\(\\text{sim}(\\cdot, \\cdot)\\) is measured by the cosine distance. In particular, we regard clean and adversarial variants from the same input as positive pairs, while the rest in the same batch are negative pairs. Hence, the loss value decreases when visual representations for the clean and the adversarial variants of the same input become more similar.\nFine-tuning. The detector and the classifier (including two encoders and one decoder) are trained jointly in the pre-training stage. After that, we drop the decoder and freeze the weights in the well-trained detector and two encoders, with Figure 3 depicting our model architecture during the fine-tuning stage. Different from MAE, which encodes the full set of image patches during the fine-tuning, our approach randomly masks out a relatively small proportion of image patches (e.g., 45%), aiming to eliminate the potential adversarial effect if the input is an adversarial example.\nGiven an input image \\((x, y_{cls})\\), where \\(x \\in \\mathbb{R}^d\\) is either a clean image or an adversarial example with the label \\(y_{cls} \\in [C]\\), we randomly mask the input image twice, arriving at two different masked inputs. Two subsets of visible patches from the two masked inputs are fed into the clean and the adversarial encoders, respectively. The masked tokens are introduced onto their corresponding positions after the encoder, obtaining two full sets of visual representations, i.e., \\(z^{cln}\\) and \\(z^{adv}\\) which are partially encoded by the clean and the adversarial encoders, respectively. We then perform the global average pooling on the adaptive ensemble of \\(\\hat{z}^{cln}\\) and \\(\\hat{z}^{adv}\\), with the result fed into an MLP for classification.\nAdaptive Ensemble. Although randomly masking an input image can eliminate the potential adversarial effect, this way inevitably hurts standard accuracy during the fine-tuning. In this paper, we propose adaptive ensemble to tackle this issue. That is, the global representation for an input image is derived from the sum of \\(\\hat{z}^{cln}\\) and \\(\\hat{z}^{adv}\\) with an adaptive factor \\(p \\in [0, 1]\\), where \\(\\hat{z}^{cln}\\) and \\(\\hat{z}^{adv}\\) are visual representations encoded by the clean and the adversarial encoders, respectively, and \\(p\\) is the probability of the input image being a clean image estimated by our detector.\nLet \\(A\\) be a full set of image patches and \\(V\\) be a subset of \\(A\\), including visible patches only. \\(\\mathbb{1}_V(i)\\) is the indicator function for evaluating whether an image patch is visible. Hence, for each image patch of \\(A\\), we have,\n$$\\mathbb{1}_V(i) = \\begin{cases}\n    1, & \\text{if the patch is visible} \\\\\n    0, & \\text{otherwise}\n\\end{cases}, i = 1, 2, ..., M,$$\nwhere \\(M\\) is the number of image patches, i.e., \\(|A|\\). For notational convenience, we let \\(\\mathbb{1}^{cln}\\) indicate visible patches fed into the clean encoder. Likewise, \\(\\mathbb{1}^{adv}\\) indicate visible patches fed into the adversarial encoder. Let \\(\\hat{z}_i\\) be the visual representation of the \\(i\\)-th image patch, with \\(i \\in \\{1, 2, ..., M\\}\\). Our adaptive ensemble is defined by:\n$$z_i = \\frac{p \\cdot \\mathbb{1}^{cln}(i) \\cdot z^{cln}_i + (1 - p) \\cdot \\mathbb{1}^{adv}(i) \\cdot z^{adv}_i}{\\max(p \\cdot \\mathbb{1}^{cln}(i) + (1 - p) \\cdot \\mathbb{1}^{adv}(i), \\epsilon)},$$\nwhere the denominator serves to normalize the adaptive ensemble of \\(\\hat{z}^{cln}\\) and \\(\\hat{z}^{adv}\\), and \\(\\epsilon\\) is a small value to avoid division by zero (i.e., \\(\\epsilon = 1e-12\\) in this paper). The intuition underlying Eq. (11) is that if our detector has a high confidence that the input is a clean image (i.e., \\(p\\) is large), the global representation \\(z_i\\) will be mostly encoded by the clean encoder. Otherwise, \\(z_i\\) will be mainly encoded by the adversarial encoder. In addition, as our pre-training encourages the similarity level of the clean and the adversarial variants from a given input (see Eq. (8) and Eq. (9)), and two different masked inputs exist upon the fine-tuning, the invisible image patches in one masked input can be glimpsed from the other masked input.", "n    },\n    {\n      \"title\": \"5 EXPERIMENTS AND RESULTS\",\n      \"content\":", 5.1, "Experimental Setup\nDatasets. We conduct experiments on three widely-used benchmarks. (i) CIFAR-10 : 60,000 32x32 RGB images of 10 classes. (ii) CIFAR-100 : 60,000 32x32 RGB examples in 100 categories. (iii) Tiny-ImageNet : 120,000 64x64 RGB images of 200 classes.\nCompared Methods. We compare our approach with four detection methods, i.e., Odds , NIC , GAT , and JTLA . We compare our approach with five adversarial training (AT) counterparts: PGD-AT , TRADES , FAT , Sub-AT , and LAS-AWP , to exhibit how it boosts the ViT's robustness.\nEvaluation. We consider three state-of-the-art adaptive attacks, i.e., AutoAttack , Adaptive Auto Attack (A\u00b3) , and Parameter-Free Adaptive Auto Attack (PF-A\u00b3) , for evaluating detection accuracy and adversarial robustness. The attack constraint, if not specified, is set to \\(\\epsilon = 8/255\\).\nModel Size. We build our detector and classifier on top of Vision Transformers (ViT), with their architectures following ViT and MAE , respectively. Our model size is pruned down to as small as possible in order to conduct a fair comparison with baselines. Table 1 lists the model size details. Our architecture consists of a detector and a classifier (including two encoders and one decoder), with 54.0M parameters in total. To conduct a fair comparison, existing adversarial training baselines use the ViT-Base model with total parameters of 85.6M as the backbone network.\nHyperparameters. For all our models, if not specified, we use AdamW with \\(\\beta_1=0.9\\), \\(\\beta_2=0.999\\), the weight decay of 0.05, and a batch size of 512. During the pre-training, the detector and the classifier (i.e., two encoders and one decoder) are trained jointly. For the detector, we follow the setting in by setting the epochs of 100, the base learning rate of 1e - 3, the linear warmup epochs of 5, and the cosine decay schedule . For the classifier, by contrast, we pre-train it for 200 epochs, with the base learning rate of 1e - 4, the linear warmup of 20 epochs, and a masking ratio of 75%. After pre-training, we drop the decoder and freeze the weights on the detector and the two encoders. Then, we finetune the classifier for 100 epochs, with the base learning rate of 1e - 3, the linear warmup of 5, and the cosine decay schedule , and a masking ratio of 45%. The patch size is set to 4 (or 8) for CIFAR-10/CIFAR-100 (or Tiny-ImageNet). We grid-search hyperparamters \\(\\lambda\\) in Eq. (6) and \\(\\Omega\\) in Eq. (8) of Section 4 and empirically set \\(\\lambda\\) to 0.15 and \\(\\Omega\\) to 0.35 for all datasets.\n5.2 Overall Performance on Our Classifier\nOverall Comparisons on CIFAR-10. We first conduct extensive experiments on CIFAR-10 and compare our approach to its state-of-the-art adversarial training (AT) counterparts listed in Section 5.1 in terms of standard accuracy and adversarial robustness under attack constraints of \\(\\epsilon = 4/255\\) and of \\(\\epsilon = 8/255\\). Table 2 lists comparative results. It is observed that our approach achieves the best performance under all three scenarios. In particular, our approach achieves the standard accuracy of 90.3%, outperforming the best competitor (i.e., LAS-AWP) by 3.5%. This is contributed by employing two encoders to extract visual representations respectively from clean images and adversarial examples, able to significantly mitigate the adverse effect of adversarial training on standard accuracy. Besides, when the attack constraint is set to \\(\\epsilon = 4/255\\), our approach achieves the best robustness of 49.8%, 49.5%, and 48.1% against AutoAttack, Adaptive Auto Attack (A3), and Parameter-Free Adaptive Auto Attack (PF-A\u00b3), respectively. Our method significantly surpasses all its counterparts. For example, it outperforms two recent state-of-the-arts, i.e., Sub-AT and LAS-AWP, respectively by 2.6% and 4.5% under the attack of A\u00b3. Thirdly, increasing the attack constraint to \\(\\epsilon = 8/255\\) results in the decrease of adversarial robustness. But our approach still maintains the best robustness of 45.3%, 44.5%, and 44.7% under the attack of AutoAttack, A\u00b3, and PF-A\u00b3, respectively. The comparative results demonstrate that our masked adaptive ensemble is robust enough to withstand strong white-box attacks. This is because masking a small proportion of image patches can significantly mitigate the adversarial effect of malicious inputs.\nOverall Comparisons on CIFAR-100 and Tiny-ImageNet. Here, we conduct a comprehensive comparison between our approach and adversarial training (AT) counterparts on CIFAR-100 and Tiny-ImageNet datasets. Table 3 lists the comparative results. On CIFAR-100, we observed that our approach achieves the best standard accuracy of 67.5%, outperforming the best competitor (i.e., LAS-AWP) by 3.4%. Meanwhile, our method achieves the best robustness of 36.9%, 35.1%, and 35.4% under the attack of AutoAttack, A\u00b3, and PF-A3, respectively. This confirms that our approach can achieve a decent standard accuracy and robustness when being generalized to the dataset with large classes. On the Tiny-ImageNet dataset, both our approach and the baseline methods experience a decrease in performance. However, our proposed method still achieves the highest standard accuracy of 49.7%, which outperforms the best baseline (i.e., FAT), by 2.6%. Moreover, all baselines suffer from a poor robustness on the Tiny-ImageNet dataset (i.e., \\(\\leq\\) 20.0%), while our approach maintains a decent robustness of 22.6%, 21.4%, and 20.9% under the attack of AutoAttack, A\u00b3, and PF-A\u00b3, respectively.\nPerformance Stability. We next conduct experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet to evaluate the performance stability under different scales of datasets and different types of adaptive attacks. We compare our approach with three baselines, i.e., FAT, Sub-AT, and LAS-AWP. Figures 4a, 4b, 4c and 4d illustrate the comparative results of standard accuracy, as well as robustness against AutoAttack, A\u00b3, and PF-A\u00b3, respectively. We have three discoveries. First, as depicted in Figure 4a, our approach (i.e., the pink line) achieves the best standard accuracies of 90.3%, 67.5%, and 49.7% under CIFAR-10, CIFAR-100, and Tiny-ImageNet, respectively. The empirical evidence verifies that our approach can maintain superior standard accuracy when generalized to large datasets. Second, on all three datasets, our approach achieves the best robustness under all adaptive attacks, as shown in Figures 4b, 4c and 4d. Take the robustness results under PF-A\u00b3 (i.e., Figure 4d) for example, our proposed masked adaptive ensemble achieves the robustness of 44.7%, 35.4%, and 20.9% on CIFAR-10, CIFAR-100, and Tiny-ImageNet, respectively. These results outperform those of LAS-AWP (i.e., the blue line), which is the best baseline, by 3.1%, 4.1%, and 3.5%, respectively. Third, when scaling up the dataset from CIFAR-10 to Tiny-ImageNet, our approach suffers from the least robustness degradation of 22.7%, 23.1%, and 23.8% under the", "n    },\n    {\n      \"title\": \"5.3 Ablation Studies on Our Classifier\",\n      \"content\":", "Pre-training: Contrastive Loss. We"]}]}