{"title": "VECTOR-ICL: IN-CONTEXT LEARNING WITH CONTINUOUS VECTOR REPRESENTATIONS", "authors": ["Yufan Zhuang", "Chandan Singh", "Liyuan Liu", "Jingbo Shang", "Jianfeng Gao"], "abstract": "Large language models (LLMs) have shown remarkable in-context learning (ICL) capabilities on textual data. We explore whether these capabilities can be extended to continuous vectors from diverse domains, obtained from black-box pretrained encoders. By aligning input data with an LLM's embedding space through lightweight projectors, we observe that LLMs can effectively process and learn from these projected vectors, which we term Vector-ICL. In particular, we find that pretraining projectors with general language modeling objectives enables Vector-ICL, while task-specific finetuning further enhances performance. In our experiments across various tasks and modalities, including text reconstruction, numerical function regression, text classification, summarization, molecule captioning, time-series classification, graph classification, and fMRI decoding, Vector-ICL often surpasses both few-shot ICL and domain-specific model or tuning. We further conduct analyses and case studies, indicating the potential of LLMs to process vector representations beyond traditional token-based paradigms.", "sections": [{"title": "1 INTRODUCTION", "content": "In-context learning (ICL) has emerged as a powerful paradigm in large language models (LLMs), allowing generalization from limited examples within a given context [9, 41]. By providing demonstrations in the context during inference, ICL allows models to adapt to new tasks and formats without the need for retraining. However, since LLMs are trained on discrete natural language tokens, ICL is generally learned and used through natural language, limiting its applicability to non-textual data.\nWe explore whether LLMs can perform ICL directly on continuous vectors, a capability that could dramatically expand their applicability. Many data modalities, such as sensor readings, financial time series, or scientific measurements, lack a natural text representation. Moreover, even for text data, information like numbers might be better represented via continuous vectors than token sequences.\nIn our study, we observe that LLMs can indeed understand and process continuous context via embedding projection. This technique, which we term Vector-ICL, acts as a bridge between continuous data and the LLM's embedding space. Simple linear projections are often sufficient, though for cross-modal tasks such as those involving non-textual data like time-series or graphs, non-linear transformations may be required. We demonstrate that training the embedding projector using a straightforward next-token prediction objective enables Vector-ICL, effectively teaching the LLM to \"read\" continuous vectors. Moreover, fine-tuning the projector on downstream tasks further enhances the effectiveness of continuous context, outperforming few-shot ICL and domain-specific models or tuning.\nOur investigation begins with the task of text reconstruction, where we assess whether LLMs can recover information encoded in text embedding. This serves as a proof-of-concept for Vector-ICL, showing that LLMs can indeed extract meaningful information from projected continuous vectors. We then turn to the more complex challenge of arithmetics. Although state-of-the-art LLMs can"}, {"title": "2 RELATED WORK", "content": "Empirical results of in-context learning ICL has empirically shown strong performance in di-verse natural-language processing tasks with very few demonstrations [9, 41]. In modern LLMs with long context windows, ICL has even shown performance improvements as the number of demonstrations grows to hundreds or even thousands, sometimes outperforming finetuning [1, 25, 7]. Empirically, different factors play key roles in ICL. In smaller LLMs, ground-truth demonstrations are not required for in-context learning, while other factors such as the label space, input text distribution, and overall sequence format play an important role [35]. Moreover, these LLMs can sometimes achieve strong performance even when demonstrations are intentionally irrelevant or even pathologically misleading [53]. More recently, Wei et al. [54] characterize these behaviors of LLMs with respect to model size, and show that larger language models perform in-context learning differently in the presence of flipped or semantically unrelated labels. Orthogonally, different works find ways to improve ICL, e.g. by including explanations [22], or chaining ICL calls [36]. ICL has shown some success in multimodal models [57, 21] or when applied to tabular data [64].\nUnderstanding ICL Many works have investigated ICL and found that it is able to learn linear models [3, 62], discrete functions [8], and more general algorithms [27]. Some works have explicitly connected ICL in specific settings to implementing optimization steps analogous to gradient descent [30, 52, 2] and higher-order optimization methods [11, 15, 16, 62]. A complementary di-"}, {"title": "3 METHOD: VECTOR CONTEXT VIA EMBEDDING PROJECTION", "content": "To perform Vector-ICL, an input must be transformed into a vector context through an embedding projection. Given a dataset X = {xi}i=1, we assume the existence of an encoder fenc, that transforms the data into an abstract representation (alternatively, the raw data may already be a continuous vector). The encoded embeddings, fenc(x), are then projected into box tokens, denoted as x. Throughout the paper, we will use the terms \"box tokens\" and \"projected embeddings\" interchangeably. For the decoding step, we employ an LLM, referred to as fllm, to generate the output given the prompts.\nWe impose no constraints on the form of the input data X; it can come from any modality. The only requirement is that the encoder fenc maps each data point x into a vector space, defined as:\n$f_{enc}: x \\rightarrow \\mathbb{R}^{d_{enc}}, \\forall x \\in X$  (1)\nThe LLM typically processes discrete tokens {tok\u2081, tok2,..., tok|V|}, then maps them to text embedding space {emb\u2081, emb\u2082, . . ., emb|V|}, Vi, embi \u2208 Rddec. Since we operate mostly in embedding space, we omit the tokenization step for simplicity and directly refer to text inputs as their embedding representations.\nThe process of embedding projection is then carried out as follows. For linear projection, we construct a projection matrix P\u2208 Rdenc\u00d7ddec and make the following transformations to obtain projected embedding given input x:\nx:= fenc(x)\u00b7P (2)"}, {"title": "3.2 PROJECTED EMBEDDINGS AS CONTEXT", "content": "The projected embeddings are then utilized as context in Vector-ICL, functioning as the equivalent of the original input data. For example, in NLP tasks, the original text snippets x will first be encoded as embeddings fenc(x), then projected to become x, inserted into the prompt like the following:\n 's sentiment is.../ 's summarization is .."}, {"title": "3.3 TRAINING THE EMBEDDING PROJECTORS", "content": "The projectors need to be trained to achieve effective projections. We discovered that pretraining these projectors with language modeling objectives enables the ICL capabilities with vector context, and finetuning them on task datasets further improves ICL performance.\nThe pretraining process is depicted in Fig. 2(a). For each text snippet, we cut it into two pieces with the cutting point randomly sampled from the end of sentences. The first half is encoded and projected while the second is kept intact. The rest is the same with any pretraining process, the language model generated the next token distribution at each input position, except for the ones preceding the projected embeddings, and a cross-entropy loss is imposed on top of this. With the encoder and LLM frozen, the gradient backpropagates to the projector, updating its parameters.\nFor non-text data modalities, pretraining can be more flexible. We define this pretraining as involving general, non-task-specific objectives, such as reconstructing a number from its embeddings (e.g., xis 32768), performing basic algebra (e.g.,x + y = 16384), or predicting the next token from brain fMRI embeddings.\nThe finetuning process is shown in Fig. 2(b). It utilizes additional structured prompts and trains with task-specific datasets. Similarly, the input is first mapped into the embedding space and projected into tokens. They are then inserted into structured prompts, while the LLM is trained with conditional generation loss given those prompts."}, {"title": "4 EXPERIMENTAL SETUP", "content": "Table 1 gives an overview of our experimental setup, including specifics for the task, datasets, encoders, LLMs, and task-specific prompts we use. Across different tasks, we project to four open-weights LLMs. We now provide details for individual tasks.\nText Pretraining To pretrain our text projectors, we leverage the WikiText-103 [33] dataset, consisting of over 100 million tokens from verified high-quality Wikipedia articles. This smaller language modeling corpus is chosen for its suitability to the lightweight nature of our projectors. The pretraining process is illustrated in Fig. 2(a), where text snippets are divided at random sentence-end points. The first half is embedded and projected, while a next-token generation loss is applied to the second half.\nText Reconstruction We investigate LLMs' ability to decode original text from projected embeddings using two datasets: Parallel Sentence Talks [49] and Quora [48]. These datasets consist of concise text pieces that convey clear meaning. Projectors are trained on the training sets of both datasets, and performance is evaluated using the BLEU score [43, 44].\nArithmetic and Function Regression For the arithmetic tasks, we generated synthetic datasets containing 10-digit numbers, which are particularly challenging for LLMs as they require splitting the numbers into multiple text tokens. These numbers are represented using a concatenated one-hot encoding per digit. For instance, a 10-digit number is represented as a 10 \u00d7 10 matrix, flattened into a 100-dimensional vector. The pretraining phase includes two key tasks: number reconstruction, where the model is tasked with recovering the original number from its embedding, and basic arithmetic, where the model performs algebraic addition operations on the projected embeddings.\nTo evaluate the models' arithmetic reasoning abilities, we employ a non-linear function regression task, where the function is defined as $f(x,y) = \\sqrt{x} \\sqrt{y}$. The model is provided with inputs x and y, and it must predict the integer part of the function output. Performance is measured using the mean relative error, calculated as the l\u2081 difference between the predicted and true values, normalized by the ground truth. This task allows us to assess the models' ability to perform more complex numerical reasoning beyond simple arithmetic operations."}, {"title": "5 RESULTS: UNLOCKING VERSATILE APPLICATIONS ACROSS MODALITIES", "content": "Fig. 3 presents our main results, where each subplot corresponds to one of the nine tasks. We begin by exploring text reconstruction, to see whether LLMs can comprehend the information encoded within the box tokens. Next, we investigate the tasks of function regression to evaluate whether LLMs can leverage the box tokens during reasoning processes and whether this approach outperforms reasoning with plain text. Finally, we proceed to a range of downstream tasks, including text classification, text summarization, time-series classification, graph classification, and brain fMRI decoding. This comprehensive evaluation allows us to assess the versatility and effectiveness of Vector-ICL across different domains and task types.\nText Reconstruction: LLM Understanding of Projected Embeddings We first verify LLMs' ability to understand projected embeddings. Our results demonstrate that Vector-ICL successfully"}, {"title": "6 ANALYSIS", "content": "We investigate the relationship between the intrinsic capabilities of encoders and their effectiveness when used with Vector-ICL for downstream tasks. To quantify the encoders' intrinsic abilities, we evaluate their performance on a text reconstruction task, which serves as a proxy for the amount of information preserved in the embeddings.\nOur analysis focuses on text classification as the downstream task. We examine the correlation between encoder rankings on the reconstruction task and their corresponding rankings on the classification task. This correlation test is performed across 5 datasets and 3 LLMs, resulting in 15 configurations.\nThe results of this analysis, presented in Fig. 4a, demonstrate a consistent positive correlation between an encoder's text reconstruction performance and its effectiveness in downstream classification tasks when used with Vector-ICL. Notably, in 4 of the 15 configurations, we observe a particularly strong correlation, with values approaching 1.\nOur findings suggest that an encoder's performance on the text reconstruction task can serve as a reliable predictor of its potential effectiveness in downstream tasks when integrated with Vector-ICL. This insight could prove valuable for practitioners in selecting encoders for Vector-ICL."}, {"title": "6.2 CASE STUDY: WHAT HAS BEEN LEARNED IN THE PROJECTIONS?", "content": "Fig. 4b provides a visualization of the normalized Euclidean distances between projected embeddings, where the x and y axes correspond to 1024 different numbers uniformly sampled from 0 to 1e10 and the color intensity reflects the distance between each pair. Several key patterns emerge from this visualization that offer insights into what the projector has learned.\nAnalysis of the numerical embedding distance matrix reveals key properties of our projection method. Embeddings for similar numbers cluster along the diagonal, indicated by lighter colors, demonstrating the preservation of local structure. Conversely, increasing distances from the diagonal, shown by darker colors, indicate effective separation of numerically distant values in the embedding space.\nAnother notable feature of the distance matrix is the block structure that emerges. This pattern comes from the way we construct the numerical embeddings and it is likely beneficial for the LLM to understand numerical inputs."}, {"title": "6.3 CASE STUDY: WHAT INFORMATION WAS PERCEIVED FROM PROJECTED BRAIN FMRI?", "content": "Fig. 4c illustrates the mean accuracy achieved in decoding different categories of brain activity based on fMRI data. The categories, ranging from \"Physical Actions and Movements\" to \"Conflict, Urgency, and Change,\" represent diverse cognitive and perceptual domains. The grouping and corresponding questions are listed in Table 5.\nThe input data is noisy, and the projector is only trained with pretraining objectives, i.e., to predict the next piece of text given the current fMRI signal. We are surprised that with this pure unsupervised training, the LLM can still pick up meaningful signals from the projected embeddings. Notably, decoding tasks associated with \"Physical Actions and Movements\" and \"Cognitive and Reflective Aspects\" demonstrate higher mean accuracy, suggesting that these categories are more distinguishable based on the fMRI embeddings."}, {"title": "7 DISCUSSION", "content": "Limitations and Future Directions In this study, we explored a variety of settings: utilizing different encoders, LLM architectures, modalities, and datasets. Our results demonstrate that LLMs are capable of performing Vector-ICL on both language and non-language inputs. However, our experiments did not cover all possible combinations of these variables. There are still many unexplored areas, such as additional modalities, tasks, and encoder-decoder configurations, that could further benefit from Vector-ICL. Also, we only experimented with single-token encoders, while there exist encoders that produce variable-sized embeddings, that can potentially be more powerful and flexible. We leave this extensive exploration for future research to fully understand the broader applicability and limitations of our approach across diverse domains.\nConclusion In this work, we explore whether large language models trained only on text can perform in-context learning on continuous vectors from different domains. Our findings suggest that LLMs can indeed understand and process continuous context via embedding projection. Simple linear projections are often sufficient, though for cross-modal tasks\u2014such as those involving non-textual data like time-series or graphs-non-linear transformations may be required. In our experiments across various tasks and modalities, including text reconstruction, numerical function regression, text classification, summarization, molecule captioning, time-series classification, graph classification, and fMRI decoding, Vector-ICL often surpasses both few-shot ICL and task-specific model or tuning. We further conduct analyses and case studies, indicating the potential of LLMs to process vector representations beyond traditional token-based paradigms."}, {"title": "A APPENDIX", "content": "A.1 DETAILED EXPERIMENT SETUP\nText Reconstruction We use two datasets for the text reconstruction task, Parallel Sentence Talks's English subset [49] and Quora [48].\nParallel Sentence Talks consist of sentences used in movie conversations, and Quora is built on a wide range of online questions. They represent short pieces of text that convey clear information. We aim to explore whether LLMs can decode the original message from the projected text embeddings.\nWe use the following prompt template:\nTranslate the text in brackets: (\u25a1), translation: [Original Text]\nWe train the projectors on the training set of these two datasets and evaluate their performance on the test tests. We measure the reconstruction performance with the BLEU score [43, 44].\nArithmetic and Function Regression We created synthetic datasets of numerical data to pretrain our linear number projectors, experimenting with two configurations: one using 3-digit numbers and the other using 10-digit numbers. In Llama3.1-8B's tokenizer, 3-digit numbers are represented as single tokens, while in Mistral-7B, Qwen2-7B, and Yi-1.5-9B, numbers larger than 10 are split into multiple tokens. Consequently, 10-digit numbers consistently span multiple tokens across all models, which increases the complexity of performing arithmetic operations.\nTo represent the numbers, we use a concatenated-and-flattened one-hot vector encoding for each digit. For instance, a 3-digit number is represented as a 3 \u00d7 10 matrix (one hot per digit place), which is then flattened into a 30-dimensional vector. Similarly, a 10-digit number is represented as a 10 x 10 matrix, flattened into a 100-dimensional vector.\nThe pretraining involves two tasks. The first task is number reconstruction, we use the following prompt template, given the number is 128:\nx = x, x equals to (digits): 128\nThe second task is basic addition, we use the following prompt template, given the numbers are x = 128 y = 256, a = 1, b = 1:\nX x, y = y, a*x+b*y equals to (digits): 384\nHere, a and b are randomly sampled from [0, 1] with up to two decimal places, and the solution is the integer part of the sum.\nFor evaluation, we use a function regression task with a non-linear function: $f(x,y) = \\sqrt{x} \\cdot \\sqrt{y}$. The LLM is given inputs x and y, along with the integer part of the output f(x, y). The model is then tasked with predicting the output for new pairs of x and y. The prompt for in-context learning is structured as follows:\nX x,y=y, function(x, y) equals to (digits): [Solution]\nFor few-shot ICL, the box tokens will be replaced with the actual numbers. We measure the function regression performance with the mean relative error, where the relative error is computed as the l\u2081 difference divided by the ground truth value.\nText Classification We use five datasets for the text classification task. For binary classification, we include IMDB [29], Rotten Tomatoes [42], and the Stanford Sentiment Treebank (SST2) [46]. For multi-class classification, we use the Emotion dataset [45] and the Financial Phrasebank [31]. The binary classification datasets (IMDB, Rotten Tomatoes, and SST2) involve classifying movie reviews as positive or negative. The Emotion dataset classifies Twitter tweets into six categories: anger, fear, joy, love, sadness, and surprise. The Financial Phrasebank categorizes financial news into positive, negative, or neutral sentiments.\nWe use the following prompt in ICL:\n()'s sentiment is [Input Class]"}]}