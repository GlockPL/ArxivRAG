{"title": "OnlySportsLM: Optimizing Sports-Domain Language Models with SOTA Performance under Billion Parameters", "authors": ["Zexin Chen", "Chengxi Li", "Xiangyu Xie", "Parijat Dube"], "abstract": "This paper explores the potential of a small, domain-specific language model trained exclusively on sports-related data. We investigate whether extensive training data with specially designed small model structures can overcome model size constraints. The study introduces the OnlySports collection, comprising OnlySportsLM, OnlySports Dataset, and OnlySports Benchmark. Our approach involves: 1) creating a massive 600 billion tokens OnlySports Dataset from FineWeb, 2) optimizing the RWKV-v6 architecture for sports-related tasks, resulting in a 196M parameters model with 20-layer, 640-dimension structure, 3) training the OnlySportsLM on part of OnlySports Dataset, and 4) testing the resultant model on OnlySports Benchmark. OnlySportsLM achieves a 37.62%/34.08% accuracy improvements over previous 135M/360M state-of-the-art models and matches the performance of larger models such as SomlLM 1.7B and Qwen 1.5B in the sports domain. Additionally, the OnlySports collection presents a comprehensive workflow for building high-quality, domain-specific language models, providing a replicable blueprint for efficient AI development across various specialized fields.", "sections": [{"title": "1 Introduction", "content": "General-purpose large language models (LLMs) have demonstrated remarkable capabilities across various tasks (Minaee et al., 2024). However, such performance comes at the cost of excessive computational resources and sometimes inefficiencies in domain-specific applications. Domain-specific language models offer a promising alternative, potentially achieving comparable or superior performance in targeted areas while significantly reducing model size.\nDespite their potential, recent domain-specific models face several challenges. Large models such as BloombergGPT (Wu et al., 2023), while powerful, requires extensive computational resources (e.g., 64 \u00d7 8 A100 40GB with a total of 1.3 million GPU hours), making them infeasible for most research institutions. Additionally, many domain models suffer from a lack of high-quality domain-specific text data, with models like BioMedLM (Bolton et al., 2024) trained on only 34.6 billion tokens and SportsBert (Microsoft) on merely 1-2 billion tokens. Furthermore, most domain models follow the model structure of general models, leaving room for optimization, especially for smaller model sizes.\nIn light of these challenges, recent research on small general-purpose language models, such as MobileLLM (Liu et al., 2024) and SmolLM (Allal et al., 2024), has provided valuable insights into efficient model structures. However, their effectiveness in domain-specific modeling remains unproven. To address these challenges and leverage recent insights, we propose a new approach for small domain-specific language models, utilizing specialized model structures and a collection pipeline for large in-domain corpus for efficient and cost-effective training.\nTo verify this approach, we choose sports as the target domain due to its unique combination of broad public interest, rich content, and a constant influx of new data through ongoing events and competitions. Moreover, sports language often contains domain-specific jargon, statistics, and contextual nuances that general-purpose models may struggle to capture accurately. By focusing on sports, we can demonstrate the potential of domain-specific models in a field that is both widely accessible and technically challenging. Additionally, the sports domain provides an excellent testbed for evaluating a model's ability to handle real-time information processing and generation, skills"}, {"title": "1.1 Contributions", "content": "1. OnlySports Dataset: A large-scale, high-quality sports-specific text corpus of 600 billion tokens, extracted from the FineWeb dataset (Penedo et al., 2024).\n2. OnlySports Benchmark: A novel evaluation method for assessing sports knowledge generation, using 1000 diverse prompts and state-of-the-art (SOTA) language models for evaluation.\n3. OnlySportsLM: A 196 million parameter RWKV-v62 (Peng et al., 2024) based sports language model trained on half of the OnlySports Dataset. In our OnlySports Benchmark, OnlySport-SLM outperforms the preceding SOTA general purpose 135M/360M language model by 37.62%/34.08%."}, {"title": "2 Collection of Domain Data", "content": "In this section, we present the path to building OnlySports Dataset, a comprehensive collection of English sports documents. This dataset comprises a diverse range of content including news articles, blogs, match reports, interviews, and tutorials, all extracted from the FineWeb dataset. FineWeb is a thoroughly cleaned and deduplicated subset of CommonCrawl, spanning from 2013 to present. It represents one of the best open-source datasets for LLM training. Our extraction process involved two key steps: first, we applied URL filtering to identify potentially relevant content, and second, we developed a custom sports text classifier to accurately identify and extract sports-related documents from the filtered data. The resulting OnlySports Dataset encompasses 1.2 TB of text, equivalent to approximately 600 billion RWKV tokens. This makes it the largest sport domain dataset to date, significantly surpassing previous collections in both scale and comprehensiveness."}, {"title": "2.1 URL Filtering", "content": "To efficiently identify potentially sports-related content within the FineWeb dataset, we implemented a preliminary URL filtering step. We carefully select a list of sports-related terms, encompassing various sports, leagues, brands, and media. This approach allows us to rapidly narrow down the dataset to documents likely to contain sports content.\nOur keywords include:\n\u2022 General sports terms: sport, athletic, athlete, fitness, workout, gym, league, team, champion, football, soccer, basketball, baseball, tennis, cricket, rugby, golf, volleyball, hockey, cycling, swimming, wrestling, running, boxing, racing, swim, goal\n\u2022 Major leagues and organizations: NFL, NBA, MLB, NHL, FIFA, UEFA, NCAA, MMA, UFC, WWE, Premier League, LaLiga, Bundesliga, SerieA, Ligue1, EPL, NASCAR, MotoGP, Formulal, F1\n\u2022 Sports events, brands, and media: Olympic, cup, playoff, marathon, copa, Nike, Adidas, ESPN, BleacherReport, SI.com, news\nWe applied these keywords in both their standard and capitalized forms where appropriate (e.g., NBA/nba, FIFA/fifa). This keyword list ensured a high recall in identifying potential sports content, which was then further refined by our classification model. Although the list does not exhaustively cover all sports, the nature of sports websites often includes the word sport in their URL, ensuring broad coverage of sports-related content."}, {"title": "2.2 Sports Text Classifier", "content": "To develop our sports text classifier, we first created a balanced dataset of sports and non-sports content. We manually scraped 64k samples from seven prestigious sports websites, selected to cover"}, {"title": "2.3 Data Filtering and Conversion", "content": "Figure 1 presents a scalable MapReduce architecture (Dean and Ghemawat, 2008) to filter sports-related content from the 90TB FineWeb dataset for model training. This approach allows us to overcome limitations in CPU resources and disk space.\nIn the map phase, we use a Golang-based coordinator with the Gin Web framework to distribute tasks across eight Python-powered worker servers. The filtering process occurs in two steps: 1. URL keyword filtering, which reduced the dataset size by 85%. 2. Application of our sports text classifier for further curation.\nThe resulting filtered data is stored in parquet format and uploaded to HuggingFace. For the reduce phase, we utilized a high-capacity cloud server to tokenize the parquet files using an open-source Rust script. This streamlined pipeline enabled us to efficiently process the massive FineWeb dataset, extracting a high-quality sports-specific corpus for training OnlySportsLM."}, {"title": "3 Optimizing Model Structure for Sports Domain", "content": "We explore the potential for model structural optimization before training with the OnlySports Dataset. A previous study (Liu et al., 2024) suggests that general-purpose sub-billion parameter models perform better when using more layers than the traditional 12-layer design while having less dimensions. We hypothesize that domain-specific small models would also follow this deep and thin rule. We explore models with approximately 190M parameters and find results that partially support this principle"}, {"title": "3.1 Training Setup", "content": "Our experiments are conducted on 8 H100 GPUs. We perform exploratory experiments on a 4.5B tokens subset of OnlySports Dataset.\nWe evaluated the pre-trained model on zero-shot commonsense reasoning tasks, including ARC-easy, ARC-challenge (Clark et al., 2018), PIQA (Bisk et al., 2019), HellaSwag (Zellers et al., 2019), as well as sports text generation task using OnlySports Benchmark."}, {"title": "3.2 OnlySports Benchmark", "content": "We introduce a novel evaluation method inspired by the Hellaswag benchmark but targeted specifically for sports knowledge generation. Instead of asking multiple choice questions, our benchmark di-"}, {"title": "3.2.1 Tag and Partial Sentence Generation", "content": "To construct our evaluation dataset, we generated 50 diverse sports-related tags encompassing popular sports, major leagues, prominent athletes, and game strategies using GPT-4 API. These tags serve as the foundation for creating a comprehensive set of prompts. For each tag, we craft 20 incomplete sentences, resulting in a total of 1,000 prompts. Each prompt is intentionally designed to end abruptly, providing an ideal context for models to complete. The prompts incorporate well-known sports facts, statistics, or narratives, allowing assessment of a wide range of sports-related knowledge and generation capabilities. For instance, for the tag #BasketballTeams, the following partial sentence prompt is generated: Spurred on by the superstar duo of Shaquille O'Neal and Kobe Bryant, the L.A Lakers clinched three consecutive. This abrupt ending sets the stage for models to complete the narrative. A well-trained model would likely continue the sentence with \"NBA championships from 2000 to 2002\" or a similar factual completion, demonstrating its ability to maintain contextual coherence and accuracy."}, {"title": "3.2.2 Model Inference and Evaluation Using SOTA LLMs", "content": "In our inference process, each prompt is separately fed to the models. We employed consistent hyperparameter settings across all models, with temperature set to 1 and top-p value to 0.3, to ensure the generation of consistent, high-probability outputs. Each response is limited to 80 tokens.\nTo evaluate the model-generated responses, we adopt an approach inspired by LLM-as-a-judge (Zheng et al., 2024), which approximates human preferences in assessing open-ended text. We utilize two state-of-the-art language models, GPT-40 and Claude 3.5 Sonnet, as evaluators.\nThe assessment is conducted across two distinct criteria at a scale of 1-5, adhering to the principle of multi-dimensional evaluation as recommended by Zheng et al. (2024). To mitigate potential biases inherent in large language model judges, we implement several measures: 1. Deployment of multiple LLM judges to enhance reliability and reduce individual model biases. 2. Standardization of prompts and evaluation criteria to ensure consistency across assessments. After scores are generated by each model, we take the average of them to be the final score.\nThe input prompt format for evaluation is defined as follows:\n\u2022 prompt: (partial sentence fed to the models)\n\u2022 response: [SEP] Answer1 [SEP] Answer2 [SEP] Answer3...\nWhere [SEP] is a separator token used to distinguish between different model responses.\nThe two evaluation criteria are defined as follows:\n\u2022 Accuracy and Factuality: Evaluates the model's ability to generate accurate and fact-based continuations, ensuring that the information aligns with well-known sports facts and data. The score is denoted as OS-acc on a scale from 1 (mostly inaccurate with significant factual errors) to 5 (fully accurate and factually impeccable).\n\u2022 Continuity and Relevancy: Assesses the relevance of the generated text to the given prompt, ensuring that the continuation is contextually appropriate and directly related to the previous sentence. This criterion, denoted as OS-rel, is scored from 1 (poor continuation that diverges significantly from the prompt's context) to 5 (excellent continuation that seamlessly extends the prompt's narrative, context, and style).\nFor each criterion, a system message with a detailed grading rubric is provided in the appendix for reference."}, {"title": "3.3 Depth and Width Experiments", "content": "Our experimental results presented in Table 2 reveal interesting insights about the relationship between model depth and performance. We conduct a study involving models ranging from 169M to 200M parameters, varying in depth from 12 to 28 layers and width from 512 to 768 dimensions. We observe that both traditional wider models and moderately deeper architectures perform well on OnlySports Benchmark. While the 12-layer wider model has the highest OS-acc (1.88) and OS-rel (2.42) scores, the 20 layers model shows comparable results in relevancy score and slightly less OS-acc (1.84). This finding, contrary to conclusion by Liu et al. (2024) and Allal et al. (2024), underscores the need for task-specific architectural experimentation.\nGeneral zero-shot tasks exhibit some benefits from increased depth, though less pronounced than in previous studies on general-purpose models. Models with 20 to 28 layers often outperform shallower configurations across various reasoning tasks.\nBased on these findings, we selected the L20D640 (20 layers, 640 dimensions) model for further training, balancing strong performance across domain-specific and general tasks. We denote this model as OnlySportsLM"}, {"title": "4 Experiments", "content": "4.1 Experimental Settings\nWe train OnlySportsLM from scratch utilizing the AdamW optimizer (Loshchilov and Hutter, 2019) with a weight decay of 0.1 and a context length of 1024 tokens. Our experiments are performed on a cluster of 8 H100 GPUs, with a per-GPU batch size of 40. Following a cosine decay schedule, the initial learning rate is set to 6e-4. However, due to observed loss spikes during training, the learning rate is subsequently adjusted, ultimately being reduced to 1e-4 (detailed in Figure 3). Due to constraints on available funding, the training stopped at 315B tokens in 7500 steps, which is around half the size of OnlySports Dataset.\n4.2 Main Results\nWe compare the final OnlySportsLM checkpoint on OnlySports Benchmark and zero-shot commonsense reasoning tasks (Hellaswag, PIQA, ARC-challenge, and ARC-easy) with previous training checkpoints and recent open-source models. To ensure consistency in evaluation procedures, all models were assessed using their publicly available implementations from the HuggingFace model repository. General benchmark scores are retrieved"}, {"title": "4.2.1 Sports Domain Generation", "content": "Table 3 compares our OnlySportsLM and two recent state-of-the-art general-purpose models, ranging from 137M to 1.7B parameters. We focused on two sets of models: 1. The SmolLM series (Allal et al., 2024), with 137M, 360M, and 1.7B parameter models, reportedly surpasses the performance of all comparable small language models on general benchmarks. 2. The Qwen2 collection (Yang et al., 2024), with 500M and 1.5B parameter models, also claims top performance on major benchmarks, even though they were trained on multilingual datasets. These model collections, released in June 2024 and July 2024 respectively, represent the latest development in small model research. For models under 1B parameters, OnlySportsLM outperforms all models by a significant margin. Notably, our model gains 34.44% accuracy over Qwen2-0.5B while being 61% smaller in size. Moreover, even when comparing to models over 1B parameter, our model performs only slightly worse (-5.23%) than Qwen2-1.5B and marginally better (0.40%) than SmolLM-1.7B in average score. This is a surprising result considering our model is only 12% the size of SmolLM-1.7B."}, {"title": "4.2.2 Zero-shot General Benchmarks", "content": "Table 3 also presents the comparison in zero-shot commonsense reasoning benchmark between our model and the two other model collections detailed in the previous section. As expected, OnlySportsLM performs the worst in all benchmarks, which is understandable given that it is only trained on sports-related text. For general-purpose models, we observe a positive correlation between"}, {"title": "4.2.3 Performance Across Training Steps", "content": "In addition to cross-model comparison, we evaluate our model every 1000 checkpoints for OnlySports Benchmark and every 500 checkpoints for general benchmarks throughout the training process. This evaluation allows us to track the model's learning progression and identify any critical points or plateaus in performance\nFigure 4 presents our model's performance on various Benchmarks throughout the training process. We observe a consistent improvement in both OS-acc and OS-rel scores for OnlySports Benchmarks as training progressed. Surprisingly, we also notice performance improvements across all general benchmarks. This unexpected trend suggests that domain-specific training on sports-related text may enhance the model's general language understanding and commonsense reasoning capabilities. While the overall trend is positive, some fluctuations in performance were observed, particularly in the general benchmarks, which could be attributed to the complexities of the training process and the diverse nature of the evaluation tasks."}, {"title": "4.3 Future Work", "content": "Building upon the promising results achieved with OnlySportsLM, future work will focus on exploring the model's full potential. We aim to complete training on the entire 600B token OnlySports Dataset when more funding is available, which may yield further improvements in both domain-specific and general language understanding. We also plan to explore instruction tuning techniques like instruction pre-training (Cheng et al., 2024)"}, {"title": "5 Related Work", "content": "Foundation models like GPT-4 (OpenAI et al., 2024) and Llama 3 (Dubey et al., 2024) have demonstrated impressive performance on general-purpose language related tasks. These models are huge, with parameter ranges in hundreds of billions, and demand excessive computational resources to train. However, these general-purpose models fail to capture domain-specific nuances and context when generating content (Au Yeung et al., 2023; Lin et al., 2024). Though techniques like fine-tuning (Zhang et al., 2023; Penedo et al., 2024) and prompt engineering (Chen et al., 2024) can help in customization of general purpose LLMs for specific domains, the model size still remains an issue.\nIn parallel, efforts around developing domain-specific language models with models trained on in-domain data are also underway. Models like BloombergGPT (Wu et al., 2023) for finance, BioMedLM (Bolton et al., 2024) for medical, and Galactica (Taylor et al., 2022) for scientific research are LLMs trained on domain-specific data. These models also have billions of parameters and demand large-scale domain-specific dataset for training. The scale of training data and the computational cost has constrained wide-scale development of domain-specific LLMs. Further, excessive computational resources and energy requirement of such LLMs makes their deployment challenging on mobile devices thereby necessitating model compression through techniques like quantization (Xiao et al., 2023) and pruning (Frantar and Alistarh, 2023).\nRecently Liu et al. (2024) developed MobileLLM, a sub-billion parameter family of LLMs achieving"}, {"title": "6 Conclusion", "content": "This study focuses on optimizing sports domain language models with sub-billion parameters. Our findings demonstrate that for sports-related tasks, a carefully designed small model can outperform larger general-purpose models. By leveraging OnlySports Dataset and a carefully designed model architecture, we achieved significant improvements in sports knowledge generation and understanding. Our OnlySportsLM, a 196M parameter model, exhibits substantial advancements in sports-related text generation compared to previous state-of-the-art methods. The model's performance on OnlySports Benchmark underscores its effectiveness in continuing sports-related text. Furthermore, we demonstrate the potential of our approach in creating high-quality, domain-specific large datasets and evaluation methods. The OnlySports Dataset and Benchmark can provide valuable resources for future research in sports-related NLP tasks. Our study contributes to the ongoing research in developing efficient, domain-specific language models. While our approach shows promise in the sports domain, further investigation is needed to determine its adaptability to other specialized fields. We believe this work may offer insights that could be valuable for researchers exploring resource-efficient AI solutions across various domains."}, {"title": "A Appendix", "content": "A.1 Evaluation Criteria\nThis appendix provides detailed grading rubrics for the two main evaluation criteria used in the OnlySports Benchmark: Accuracy and Factuality (OS-acc), and Continuity and Relevancy (OS-rel). These rubrics were provided to the GPT-4 and Claude 3.5 Sonnet models as part of their system messages when acting as evaluators. {num} specified the number of responses that will be in the prompt for evaluation.\nA.1.1 Accuracy and Factuality\nPrompt for evaluating accuracy and factuality: You are a sports expert assigned to grade language models' generation performance on general sports-related text according to the provided rubric. 1 prompt and {num} responses will be presented, all attempting to complete the same given prompt. Each response is separated by [SEP] and limited to 80 tokens."}, {"title": "A.1.2 Continuity and Relevance", "content": "Prompt for evaluating continuity and relevance: You are a sports expert assigned to grade language models' generation performance on general sports-related text according to the provided rubric. 1 prompt and {num} responses will be presented, all attempting to complete the same given prompt. Each response is started by [SEP] and limited to 80 tokens."}, {"title": "A.2 Example Prompts and Responses for OnlySports Benchmark Scoring", "content": "This appendix provides concrete examples of prompts and OnlySportsLM responses to illustrate the scoring criteria used in the OnlySports Benchmark. We present four distinct scenarios that demonstrate various combinations of Accuracy and Factuality (OS-acc) and Continuity and Relevancy (OS-rel) scores:\n1. High Accuracy, High Relevancy 2. High Accuracy, Low Relevancy 3. Low Accuracy, High Relevancy 4. Low Accuracy, Low Relevancy\nFor each scenario, we provide the original prompt, response, and a brief explanation of the scoring rationale. These examples are designed to offer clarity on how the evaluation criteria were applied and to aid in the reproducibility of our results.\n1. Prompt: Named after the famed coach, the Vince Lombardi Trophy is awarded to the team that wins\nResponse: the Super Bowl. The trophy is awarded to the team that wins the Super Bowl, and the trophy is named after the coach who won the most Super Bowls. The trophy is named after Vince Lombardi, who won the Super Bowl with the Green Bay Packers in 1967. The\nThis response accurately states that the Vince Lombardi Trophy is awarded to the Super Bowl winner. It correctly mentions Vince Lombardi and his connection to the Green Bay Packers. The information is directly relevant to the prompt and provides additional context about the trophy's history.\n2. Prompt: With flippers and a flutter kick, scuba divers use a modified version of the freestyle stroke to\nResponse: keep their hands and feet on the ground. The dives are performed in a single-piece, high-tech suit. The diver is required to wear a mask and a cap. In the 2012 London Olympics, the dives were judged by a panel of experts. In 2013, the International Olympic Committee (IOC) banned the dives.\nWhile the response contains accurate information about diving suits and Olympic judging, it's not relevant to the prompt about scuba diving techniques. The mention of flippers and freestyle stroke is ignored, making the response off-topic despite its factual correctness."}, {"title": "3. Prompt: \"As the Lakers head toward another challenging season, they are hopeful about LeBron James's\"", "content": "Response:' decision to return to the team. James, who will be a free agent this summer, has not made a decision on his future.\"I'm just happy that he's back,\" Lakers coach Frank Vogel said. \"I think he's a big part of our team. He's a big part of our team. I think he's going to be a big part of our team.'\nThe response is relevant to the Lakers and LeBron James's future with the team, matching the prompt's topic. However, it contains inaccuracies: LeBron James was not a free agent at the time specified, and Frank Vogel was no longer the Lakers' coach. The content is on-topic but factually incorrect.\n4. Prompt: Hockey legend Wayne Gretzky holds the NHL record for the most points scored in a single season at\nResponse: 100 points. NHL players are not the only ones who have the ability to score goals. Some of the most popular sports in the world include soccer, basketball, baseball, football, and hockey. In addition to the games themselves, the sport of hockey is also a popular pastime for many people. There are many ways to play hockey, and the rules of the game\nThis response is both inaccurate and irrelevant. The stated record of 100 points is incorrect (Gretzky's record is 215 points). Most of the response diverges into general sports discussion unrelated to Gretzky's NHL record, making it largely irrelevant to the prompt."}, {"title": "A.3 Model Scores from Claude and GPT", "content": "In this section, we present the raw score from the two state-of-the-art LLMS: Claude 3.5 and GPT-40. The table below (Table 4) showcases the scores for each model across OS-rel (Relevance) and OS-acc (Accuracy). These scores provide insight into how well each model performs in generating sports-related content, with higher scores indicating better performance. We observe that Claude 3.5 Sonnet generally gives higher scores than GPT4o, using the same prompt."}]}