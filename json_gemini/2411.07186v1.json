{"title": "NATURELM-AUDIO: AN AUDIO-LANGUAGE FOUNDATION MODEL FOR BIOACOUSTICS", "authors": ["David Robinson", "Marius Miron", "Masato Hagiwara", "Olivier Pietquin"], "abstract": "Large language models (LLMs) prompted with text and audio represent the state of the art in various auditory tasks, including speech, music, and general audio, showing emergent abilities on unseen tasks. However, these capabilities have yet to be fully demonstrated in bioacoustics tasks, such as detecting animal vocalizations in large recordings, classifying rare and endangered species, and labeling context and behavior\u2014tasks that are crucial for conservation, biodiversity monitoring, and the study of animal behavior. In this work, we present NatureLM-audio, the first audio-language foundation model specifically designed for bioacoustics. Our carefully curated training dataset comprises text-audio pairs spanning a diverse range of bioacoustics, speech, and music data, designed to address the challenges posed by limited annotated datasets in the field. We demonstrate successful transfer of learned representations from music and speech to bioacoustics, and our model shows promising generalization to unseen taxa and tasks. Importantly, we test NatureLM-audio on a novel benchmark (BEANS-Zero) and it sets the new state of the art (SotA) on several bioacoustics tasks, including zero-shot classification of unseen species. To advance bioacoustics research, we also open-source the code for generating training and benchmark data, as well as for training the model\u00b9.", "sections": [{"title": "1 INTRODUCTION", "content": "Bioacoustics, the study of sound production and reception in animals, aims to understand animal behavior (Fischer et al., 2013), monitor biodiversity (Stowell, 2022), and model the mechanisms of sound production and reception used in animal communication (Bradbury & Vehrencamp, 1998). It plays a vital role in conservation and ecological research, as animal vocalizations provide critical insights into ecosystem health, species interactions, and population dynamics. By enabling the"}, {"title": "2 RELATED WORK", "content": "Most prior work on audio-language models has focused on human speech processing. For example, models like SpeechGPT (Zhang et al., 2023), Speech-LLaMA (Wu et al., 2023a), AudioLM (Borsos et al., 2023), AudioPaLM (Rubenstein et al., 2023), AudioGPT (Huang et al., 2023), SpiRit-LM (Nguyen et al., 2024), and SpeechLM (Zhang et al., 2024) mostly focus on building language models that can perceive and produce human speech. Such models may be fine-tuned for downstream bioacoustics tasks requiring expensive computational resources and expertise. Instead, our model shows promising generalization to unseen species and tasks.\nRecently, more generic language models with audio perception capabilities have been released. Pengi (Deshmukh et al., 2023) uses an audio encoder and a text encoder mapped onto an LLM to solve audio-to-text tasks. SALMONN (Tang et al., 2024) uses dual audio encoders and integrates Q-Former (Li et al., 2023) to improve the handling of speech and general audio inputs. Qwen-audio (Chu et al., 2023) adopts a multi-task learning approach with the introduction of the Speech Recognition with Timestamp (SRWT) task. LTU (Gong et al., 2023) builds an open-ended question-answer dataset and uses curriculum learning strategies to enhance its generalization capabilities. Similar multimodal language models have been proposed for music, such as MU-LLaMA (Liu et al., 2023) and LLark (Gardner et al., 2023). Recent foundation models such as AVES (Hagiwara, 2023) and BioLingual (Robinson et al., 2024) have exhibited notable results on bioacoustic tasks, although their training paradigms and architectures restrict the range of tasks they can address.\nAlthough animal sounds and vocalizations are often part of generic audio datasets, such as AudioSet (Gemmeke et al., 2017) and audio caption datasets (Kim et al., 2019; Mei et al., 2023), these datasets are often too general and lack the fine-grained details necessary for tasks like species classification, behavior analysis, or monitoring in ecology and bioacoustics. As a consequence, LALMs trained on these datasets produce at best generic labels e.g., 'bird' and not the name of the species. We address this limitation by proposing an open multi-task diverse training set and a LALM, NatureLM-audio, that offers robust representations for bioacoustics.\nWhile there are specific bioacoustics benchmarks like BIRB (Hamer et al., 2023) for bird vocalization retrieval and BEANS (Hagiwara et al., 2023) for classification/detection, the field of bioacoustics has yet to see the development of dedicated benchmarks similar to those in human speech and music, such as Dynamic-SUPERB (Huang et al., 2024) or AIR-Bench (Yang et al., 2024). This leaves a gap for advancing the evaluation of bioacoustics models, particularly in zero-shot learning and task generalization.\nWith this work, we aim to bridge these gaps by introducing NatureLM-audio, a model specifically designed for bioacoustics tasks, and enhancing bioacoustic benchmarks to assess cross-species and cross-task generalization, introducing BEANS-Zero."}, {"title": "3 METHODS", "content": ""}, {"title": "3.1 TRAINING DATASET CREATION", "content": "To train an audio-text model for bioacoustics, we compile a diverse dataset of text-audio pairs (Table 1). The data is collected through a combination of prompting on existing audio datasets, creating new LLM-generated text labels, and mixing new, procedurally-augmented audio data. The data is"}, {"title": "3.1.1 BIOACOUSTIC DATA", "content": "Species Classification: We curate existing large-scale bioacoustic archives into a common format. We curate Xeno-Canto (Xeno-canto), iNaturalist (iNaturalist), Animal Sound Archive (Museum f\u00fcr Naturkunde Berlin), and Watkins (all-cuts, Sayigh et al. (2016)) into a common format. Specifically, we handle differences in common name and scientific name across datasets by joining all datasets to the GBIF taxonomy backbone (GBIF Secretariat, 2023). We then prompt the model to predict either the scientific or common name of the focal species, or the scientific or common names of all species in the recording. This requires the model to generate the common name or scientific name of the species directly. In many cases, we may know an animal vocalization is one of a subset of species-for example, based on location. To allow for this, we also generate prompts with a set of options injected into the question. The options may be sampled randomly from other species, or they may be \"hard negative\" species from the same genus, family, or order.\nSpecies Detection: We use the same datasets as for species classification, but prompt the model to ask whether the recording contains one of a set of options, or 'None'. Options are sampled in the same way as for classification, with a mix of random and hard negatives. In fifty percent of prompts, the correct species is not included in the set of options, with a correct answer of 'None'.\nTo help bridge the gap between focal train recordings and noisy soundscape recordings common at inference, we also generate a noise-augmented detection training set from Xeno-canto. We use per-channel energy normalization (PCEN Lostanlen et al. (2018)) as a form of noise-gate for bird vocalization activity detection. Then, we separate each detected segment into four stems using the 4-stem Bird-MixIT source separation model (Denton et al., 2022). Because the separation model may over-separate sources and does not label stems with source names, we use the YAMNet model (Howard et al., 2017) trained on the AudioSet dataset (Gemmeke et al., 2017) to select solely the stems with high probability on the AudioSet animal classes (with ids between 67 to 131). Correspondingly, for each stem we take the maximum probability across the classes, we average the values across the time frames, and we sum the stems with values higher than 0.5."}, {"title": "Captioning:", "content": "We use the AnimalSpeak (Robinson et al., 2024) dataset for bioacoustic captioning. AnimalSpeak combines bioacoustic datasets into a language-model-captioned audio-text dataset. However, due to scale, the large segment of AnimalSpeak from Xeno-Canto was not captioned with a language-model, and used only templated captions. We further process Xeno-Canto with Gemini-1.0-pro (Gemini Team, 2024) following the same method used to create AnimalSpeak, and use these LLM-generated captions in addition to the original captions."}, {"title": "Call-type and Lifestage:", "content": "We include multiple new bioacoustic tasks which can be expressed based on the Xeno-Canto metadata. Specifically, predicting the life stage of birds, predicting call-types, and differentiating between calls and songs. Compared to species classification alone, included in existing datasets, the ability to perform these tasks at scale could significantly enhance the precision of ecological monitoring and behavior studies."}, {"title": "3.1.2 GENERAL AUDIO", "content": "We include WavCaps (Mei et al., 2023) and AudioCaps (Kim et al., 2019) for general audio caption-ing. We observe that, in the creation of WavCaps, some recordings originally had metadata relevant to bioacoustics and specific species. However, these were lost in the general-domain captioning, producing captions which are too generic for our purpose. We detect these cases by processing the original metadata, and re-process the metadata prompting Gemini-1.0-pro to produce bioacoustic captions. We include these new bioacoustic captions in addition to the original captions."}, {"title": "3.1.3 MUSIC", "content": "Pitch, timbre qualities of animal vocalizations, the number of animals in a recording are often key acoustic features used by biologists to classify context and behavior. We use NSynth 2.3.3 (Engel et al., 2017) to create a set of tasks that may help bioacoustics downstream tasks. We generate text prompts for pitch detection in Hz, instrument name, and velocity, ranging 0 to 1. Additionally, we use the timbre 'qualities' labels to create text descriptions for each audio. For instance, if the sound is 'distorted,' we generate descriptions such as 'This sound has a distinctive crunchy sound and presence of many harmonics.' or 'This sound is distorted'. Moreover, we create synthetic mixtures by layering one to three different instruments. In this case we generate, two task: predicting the number of instruments and identifying the instrument names."}, {"title": "3.1.4 SPEECH", "content": "We use the speech diarization dataset based on LibriSpeech (Edwards et al., 2018), which contains synthetic mixtures of two or three speakers. We use this to derive the number of speakers task, which we believe has interesting applications for monitoring individuals if transferred to bioacoustics."}, {"title": "3.2 EVALUATION DATA: THE BEANS-ZERO BENCHMARK", "content": "One contribution of this work is a new benchmark for bioacoustics: BEANS-Zero (Table 2). With BEANS-Zero, we go beyond traditional species classification, introducing tasks such as call-type prediction, lifestage classification, captioning, and individual counting (which is not seen during training). To build this set of tasks, we first used the test portion of the benchmark BEANS (Hagiwara et al., 2023) for evaluating our models on common bioacoustics datasets and tasks, which include:\n\u2022 esc50 (Piczak, 2015): Generic environmental audio classification with 50 labels.\n\u2022 watkins (Sayigh et al., 2016): Marine mammal species classification with 31 species.\n\u2022 cbi (Howard et al., 2020) Bird species classification with 264 labels from the Cornell Bird Identification competition hosted on Kaggle."}, {"title": "3.3 NATURELM-AUDIO ARCHITECTURE", "content": "Our model follows a generic audio-to-text architecture used in prior works, such as SALMONN (Tang et al., 2024), Qwen2-audio (Chu et al., 2024), and LTU (Gong et al., 2023), which are large audio-language models trained on paired audio-text data for tasks including speech, music, and general audio events. Figure 1 provides an overview of the NatureLM-audio architecture.\nSpecifically, NatureLM-audio first encodes the audio input via an audio encoder, in this case BEATS (Chen et al., 2023), which has achieved state of the art on multiple audio tasks. To connect the BEATs embeddings with the LLM we use a Q-Former (Li et al., 2023) applied at the window level as proposed in SALMONN (Tang et al., 2024). Similarly to the existing LALMS we use an LLM to produce text, in this case Llamma 3.1-8b (Dubey et al., 2024), which is fine-tuned with LORA (Hu et al., 2022). The parameters of the LLM (except for the adapter layers) remain frozen during training, while the audio encoder and Q-Former are unfrozen. The model takes an audio $a$ and an instruction $\u00e6$ as its input, and produces a text sequence $x_{<t}$ as the output. The model is trained under the loss function:\n$h = f_w(Encoder(a))$                                                                                                           (1)\n$z = p_\u03c6(q,h)$                                                                                                                  (2)\n$L = -log p_\u03b8(x_{<t} | z, \u00e6)$                                                                                                (3)\nwhere Encoder is the pretrained BEATs (Chen et al., 2023) audio encoder, $f_w$ is a function that converts consecutive $W$ audio frames into a window, $p_\u03c6$ is the Q-Former model with trainable parameters $\u03c6$ that converts a window into a sequence of text representations $z$ using query q, and $p_\u03b8$ is the pretrained LLM with trainable parameters $\u03b8$."}, {"title": "3.4 TRAINING METHOD", "content": "Our training method is heavily motivated by curriculum learning (Soviany et al., 2021) where machine learning algorithms start with simpler, easy to learn instances and gradually shift to more difficult ones, as done in other audio foundation models (Tang et al., 2024; Gong et al., 2023). We train in the following two stages:\n\u2022 Stage 1 (Perception Pretraining): We pretrain the model exclusively on the task of focal species classification, classifying vocalizations of thousands of animal species. Species classification is highly deterministic, allowing opportunity to learn a robust connection between language and audio. We also choose to train on this task individually as it is foundational to other tasks in bioacoustics.\n\u2022 Stage 2 (Generalization Fine-tuning): In the second stage, we introduce a variety of bioacoustic and other tasks that build on the robust classification performance of the first stage. This includes detection, captioning, lifestage prediction, and call-type prediction. We also include speech and music data in this second stage, hoping to transfer to bioacoustic tasks.\nWe trained from scratch (i.e., random initialization of the Q-Former and LoRA) rather than fine-tuning existing models or checkpoints, such as SALMONN's. This allows for more flexibility in terms of choosing the latest LLM, with the most knowledge of animal species, and the most relevant architectural components (e.g. excluding memory-heavy parts of current LALMs such as the speech encoder Whisper (Radford et al., 2022))."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 TRAINING AND EVALUATION DETAILS", "content": "We train our model on the full curated training set (Section 3.1). To assess the model's generalization we created hold-out splits for Xeno-canto, iNaturalist, Animal Sound Archive, and Watkins datasets, used solely for benchmarking.\nWe train with our two proposed stages. In both stages we use a linear warmup, cosine scheduler, peak learning rate of 9.0 \u00d7 10-5, and a batch size of 64. We decode using beam search with two beams, a repetition penalty of 1.0, and a length penalty of 1.0."}, {"title": "4.2 SPECIES CLASSIFICATION AND DETECTION", "content": "Table 3 shows the main results measured on the BEANS-Zero species classification and detection datasets. Our baselines include an LLM (Llama-3.1-8B-Instruct, Dubey et al. (2024)) without audio input, SALMONN (Tang et al., 2024), BioLingual (Robinson et al., 2024), and Qwen2-audio (Chu et al., 2024). As shown in the table, the outputs from the LLM without audio input, SALMONN, and Qwen2-audio are largely random on the bioacoustic datasets, failing to properly interpret the input audio or follow the instructions. In contrast, NatureLM-audio achieved state-of-the-art zero-shot performance on 6 out of 9 datasets, and delivered competitive results on the remaining tasks from the BEANS-Zero benchmark. We observe that for some of those three remaining tasks, our current training data contains little signal, for example on humbugdb (Kiskin et al., 2021) which classifies species by mosquito wingbeat sounds not generated by a vocal tract. We also note that performance of baselines on the general audio auxiliary dataset ESC50 (Piczak, 2015) may be reduced by the use of the Levenshtein distance, as our pipeline is optimized for bioacoustic tasks.\nWe also compared NatureLM-audio with bird vocalization classification models, namely Bird-NET (Kahl et al., 2021) and Perch (Ghani et al., 2023), to evaluate the zero-shot capabilities of our model. We compare on the subset of BEANS-Zero classifying or detecting exclusively bird species, plus the portion of DCASE with bird species. The results are presented in Table 4. Since both BirdNET and Perch were trained in a supervised manner on datasets that significantly overlap with our bird evaluation datasets, this is not a fully fair comparison, and their performance should be considered as topline results. Nevertheless, our model demonstrated strong zero-shot bird vocalization classification capabilities. In particular, we achieve a new SotA for the cbi dataset, classifying vocalizations of hundreds of birds, and achieve competitive results with the bird-specific models on both detection tasks."}, {"title": "4.3 GENERALIZING TO UNSEEN SPECIES", "content": "We further evaluate the model's ability to generalize to completely unseen taxa using the newly added datasets in BEANS-Zero. They consist of recordings of held-out species from Xeno-canto, iNaturalist, Animal Sound Archive, and Watkins. As a topline, we compare against BioLingual, which was trained on species classification and has seen these species in training. As a baseline, we consider CLAP-LAION (Elizalde et al., 2023), a general-domain audio model which, similar to our model, is unlikely to have seen these species during training. We compare the performance when predicting common as well as scientific names.\nOur model significantly outperforms the baseline, demonstrating generalization to completely unseen species. Our model in particular excels when predicting with scientific (Latin) names (unseen-sci), which have consistent hierarchical structure it may learn to exploit."}, {"title": "4.4 NOVEL BIOACOUSTIC TASKS", "content": "We evaluate the model's abilities beyond species prediction with several bioacoustic tasks newly added to BEANS-Zero, which have, to the best of our knowledge, not been studied at a cross-species level. We additionally include zf-indv, a completely unseen task counting the number zebra finches in a recording (Elie & Theunissen, 2016). We compare against BioLingual (Robinson et al., 2024) for discriminative tasks and SALMONN (Tang et al., 2024) for captioning. On each of these tasks, our model sets the state-of-the-art."}, {"title": "4.5 ABLATION ON SPEECH AND MUSIC", "content": "To investigate the impact of speech and music on downstream task performance, we run an ablation on stage-2 training with and without speech and music data. We train both stage-2 models for 200k steps, and evaluate their ability to perform the unseen task of counting zebra-finch individuals in a recording. The model trained with speech scores .377 on this task, similar to our full model. The model trained without speech scores an accuracy of .243, approximately random, and qualitatively predicts a single speaker for every recording. This result suggests the ability to count vocalizing birds transfers from human speech and music, for which our training data includes counting human speakers in a recording."}, {"title": "5 CONCLUSION", "content": "We presented NatureLM-audio, the first audio-language foundation model specifically designed for bioacoustics, demonstrating its potential to address critical tasks such as classifying and detecting animal vocalizations, and decoding context, call types, and individuals across species. By leveraging a carefully curated dataset spanning bioacoustics, speech, and music data, NatureLM-audio sets the new state-of-the-art (SotA) on multiple tasks, including zero-shot classification of unseen species. Moreover, our model demonstrates positive transfer across both domain and tasks, performing well on a novel benchmark (BEANS-Zero), which includes new bioacoustic tasks such as captioning and individual counting. To further accelerate research and the development of more robust models in the field, we have open-sourced the code for generating both training and benchmarking data.\nWe plan to extend this work by incorporating more diverse tasks and datasets, improving the text-based LLM backbone with bioacoustic-specific texts, and enhancing the model's multilingual capabilities. Additionally, we aim to introduce new modalities, such as motion and image data, leading to models like NatureLM-motion and NatureLM-image. Lastly, we will explore the model's generative abilities, enabling it to produce audio tokens for tasks such as animal sound generation and audio denoising.\nWhile NatureLM-audio offers significant potential for advancing biodiversity monitoring and conservation, several ethical concerns must be addressed. First, there is a potential bias towards bird vocalizations due to the overrepresentation of bird datasets, which could limit the model's effectiveness in other domains. Second, the model's ability to detect and classify endangered species could be misused for illegal activities such as poaching, posing a threat to wildlife. Finally, unintended consequences on animal behavior and ecology must be considered, particularly when deploying LLMs, known for their issues including hallucinations and biases (Kuan et al., 2024). These systems may interfere with the behavior of the species being studied, and the long-term ecological impact of widespread passive monitoring is still unknown. Careful deployment and responsible use are essential to mitigate these risks."}]}