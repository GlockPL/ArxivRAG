{"title": "Pipeline Analysis for Developing Instruct LLMs in Low-Resource Languages: A Case Study on Basque", "authors": ["Ander Corral", "Ixak Sarasua", "Xabier Saralegi"], "abstract": "Large language models (LLMs) are typically optimized for resource-rich languages like English, exacerbating the gap between high-resource and underrepresented languages. This work presents a detailed analysis of strategies for developing a model capable of following instructions in a low-resource language, specifically Basque, by focusing on three key stages: pre-training, instruction tuning, and alignment with human preferences. Our findings demonstrate that continual pre-training with a high-quality Basque corpus of around 600 million words improves natural language understanding (NLU) of the foundational model by over 12 points. Moreover, instruction tuning and human preference alignment using automatically translated datasets proved highly effective, resulting in a 24-point improvement in instruction-following performance. The resulting models, Llama-eus-8B and Llama-eus-8B-instruct, establish a new state-of-the-art for Basque in the sub-10B parameter category.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have revolutionized the field of natural language processing (NLP), significantly advancing the state of the art in a wide range of tasks, from language generation to language understanding. Models such GPT-4 (Achiam et al., 2023) have had a particularly profound impact, showcasing the capabilities of LLMs in real-world applications, showcasing their broad utility across various domains. However, the proprietary nature of these models makes them impractical for many researchers and developers.\nIn response, the open-source community has risen to the challenge, developing models that closely rival their proprietary counterparts, such as Llama-3 (Dubey et al., 2024), Mixtral (Jiang et al., 2024), Qwen2 (Yang et al., 2024) or Gemma-2 (Team et al., 2024). However, despite these advances, most of these models remain primarily optimized for resource-rich languages-particularly English-which have been trained on vast amounts of data and computational resources. This further widens the gap between high-resource and underrepresented languages, creating a significant barrier to the adoption and effectiveness of LLMs in low-resource languages.\nOpen-source models, however, offer a unique opportunity to bridge this gap. By leveraging techniques such as transfer learning, it is now feasible to adapt LLMs originally trained on large, predominantly English datasets, and fine-tune them for underrepresented languages using smaller, specialized datasets (Cui et al., 2023b; Fujii et al., 2024; Kuulmets et al., 2024; Etxaniz et al., 2024). This approach opens the door for developing high-quality LLMs for languages with limited computational resources, ensuring more equitable access to these transformative technologies.\nThe development of instructed LLMs typically involves three main stages: (a) pre-training a foundational LLM, (b) instructing the LLM, and (c) aligning the model to user preferences. In this paper, we propose strategies to address each of these stages tailored to low-resource languages, and we evaluate the importance of each step in the development of an LLM for such languages. For the purpose of this work, we focus on Basque, a low-resource language as one with minimal representation in the foundational LLM, lacking a sufficiently large and high-quality corpus, and with insufficient data to fully execute the instruction and alignment phases. We conduct experiments using models with fewer than 10 billion parameters, specifically Llama-3.1-8B, to focus on computationally lightweight models.\nThe main contributions of this work are as follows:\n\u2022 A comprehensive analysis of strategies to implement each stage (pre-training, instruction, and alignment) of the LLM development pipeline, adapted to Basque, a low-resource language.\n\u2022 Development of Llama-eus-8B\u00b9, a foundational LLM that achieves the best performance on natural language understanding (NLU) tasks for Basque among sub-10B parameter models. Improvements illustrated in Figure 1.\n\u2022 Development of Llama-eus-8B-instruct\u00b2, the first instructed LLM for Basque, achieving state-of-the-art performance among sub-10B parameter models. Improvements illustrated in Figure 1.\n\u2022 Introduction of new datasets\u00b3 for Basque to support both the evaluation and training of the various phases in the LLM development pipeline."}, {"title": "Previous work", "content": "Adapting generative LLMs to other languages has gained significant attention in recent years, with various strategies explored to improve model performance while maintaining computational efficiency. Broadly, the approaches address two main stages: continual pre-training with target language data and instruction tuning to align LLM to user preferences (Zhao et al., 2024).\nContinual pre-training involves further training an LLM on a large corpus of target language textual data. This approach has consistently demonstrated success in boosting language comprehension and generation capabilities (Cui et al., 2023b; Fujii et al., 2024; Kuulmets et al., 2024). For instance, Cui et al. (2023b) report significant improvement in their Chinese-LLaMA over the original LLaMA's (Touvron et al., 2023) ability to understand and generate Chinese content by further pretraining it on extensive Chinese data. MaLA-500 (Lin et al., 2024) is another notable initiative for adapting LLMs to low-resource languages. It utilizes continual pre-training on LLaMA 2 with the Glot500-c dataset (Imani et al., 2023) to support 534 languages.\nOne major challenge in adapting LLMs to new languages is catastrophic forgetting, where a model loses prior knowledge during new language training (Zhao et al., 2024). To address this, recent studies have incorporated a portion of the original English corpus alongside the target language data during training, as seen in efforts to adapt models to Japanese (Fujii et al., 2024) and Estonian (Kuulmets et al., 2024).\nInstruction fine-tuning has become a vital technique for adapting LLMs to new languages, improving alignment with user preferences and enhancing the model's ability to follow instructions. For instance, studies by Cui et al. (2023b) and Jiang et al. (2024) illustrate how instruction tuning refines task-specific instruction adherence in Chinese. Their findings indicate that when combined with continual pre-training, instruction tuning significantly boosts performance, especially for complex language tasks. Additionally, Jiang et al. (2024) demonstrated that initiating the process with the foundational model, rather than the instruction model, is more effective for transferring language abilities.\nIn the context of Basque, the Latxa (Etxaniz et al., 2024) family of foundational models, which ranges from 7 to 70 billion parameters and is based on LLaMA 2, represents a significant advancement in adapting LLMs to the Basque language. Through continual pre-training on a specialized Basque corpus, these models have achieved substantial improvements in processing Basque text."}, {"title": "Adapting a Foundational Model to Basque", "content": "In this section, we focus on adapting a foundational English-centric model to Basque by continual pre-training with high-quality Basque data. Continual pre-training is a crucial technique for adapting large language models to new languages, where the goal is to incrementally refine the model's capabilities for the new language."}, {"title": "Foundational Model Choice: Balancing Performance and Efficiency", "content": "We chose Llama-3.1-8B (Dubey et al., 2024) as our base foundational model for this work. Although larger models, such as the 70B version, were initially considered and demonstrated superior capabilities, their high computational and memory demands pose significant challenges in resource-limited environments, both during training and deployment, making Llama-3.1-8B the optimal choice for our scenario.\nWe initially evaluated earlier versions, such as Llama-2 (Touvron et al., 2023), but ultimately selected Llama-3.1 as it consistently achieved the best results for Basque NLU tasks among all Llama variants. In addition to its superior results, Llama-3.1 features an expanded vocabulary designed to better support multiple languages and offers broader native support for a wider set of languages. These qualities made Llama-3.1 the optimal choice for our model adaptation to Basque. See Appendix A for the evaluation results."}, {"title": "Training Data", "content": "For continual pre-training, we utilized the Zelai-Handi dataset (San Vicente et al., 2024), the largest collection of freely licensed and clean Basque texts available as of October 2024. This dataset, comprising approximately 521 million words (around 1.5B tokens with Llama-3.1 tokenizer), was meticulously compiled from selected web sources, ensuring that only high quality documents published under free license were included. By high quality we refer to texts that are well-formed, free of excessive noise or errors, and representative of formal and diverse language use across various domains (see Appendix B for further details on the sources considered during ZelaiHandi creation).\nCompared to other existing datasets for Basque (see Appendix C), ZelaiHandi proved to be the most favourable choice, combining high-quality content with an open license. Notably, it demonstrates competitive performance with significantly less data, contributing to more computationally efficient pre-training compared to larger datasets.\nTo further enhance the continual pre-training process, we incorporated the FineWeb dataset (Penedo et al., 2024), which comprises over 15 trillion tokens of cleaned and deduplicated English web data sourced from CommonCrawl and licensed under ODC-By 1.0 license. For our purposes, we used a random subset of approximately 300 million tokens. As observed in recent literature (Kuulmets et al., 2024; Etxaniz et al., 2024; Fujii et al., 2024), this approach aims to avoid catastrophic forgetting of previously learned competencies in English, which hinders transfer-learning from English. The objective is to develop formal linguistic competencies for Basque, including grammar and vocabulary, while leveraging the functional linguistic skills\u2014such as reasoning and world knowledge-acquired from the English data during the original training of Llama 3.1."}, {"title": "Evaluation benchmarks", "content": "To evaluate our model's performance in Basque, we employed a variety of benchmarks, including both manually translated well established English benchmarks and existing Basque benchmarks. This comprehensive evaluation approach allows us to measure the model's capabilities across different tasks, ensuring a robust understanding of its formal and functional competencies in the Basque language.\nWe manually created four new benchmarks for Basque by translating samples from well-established English benchmarks:\n\u2022 ARC_HT_eu_sample: A subset of 250 samples manually translated to Basque from the ARC dataset (Clark et al., 2018). The ARC dataset consists of genuine grade-school level, multiple-choice science questions.\n\u2022 Winogrande_HT_eu_sample: A subset of 250 samples manually translated to Basque from the WinoGrande dataset (Keisuke et al., 2019). WinoGrande is a dataset of 44k problems specifically designed to test common-sense reasoning.\n\u2022 MMLU_HT_eu_sample: A subset of 270 samples manually translated to Basque from the MMLU dataset (Hendrycks et al., 2021).\nThe MMLU dataset is a massive multitask test consisting of multiple-choice questions from various branches of knowledge. The test spans subjects in the humanities, social sciences, hard sciences, and other areas that are important for some people to learn.\n\u2022 HellaSwag_HT_eu_sample: A subset of 250 samples manually translated to Basque from the HellaSwag dataset (Zellers et al., 2019). The HellaSwag dataset commonsense NLI evaluation benchmark.\nAll benchmarks were translated by a native Basque speaker. For all newly created benchmarks, we also provide the corresponding English samples, enabling a direct comparison of model performance between Basque and English. This allows for a clear assessment of the performance gap between the model's competencies in English and Basque. In addition to those new benchmarks, we leveraged existing Basque benchmarks:\n\u2022 BL2MP (Urbizu et al., 2024): The BL2MP test set, designed to assess the grammatical knowledge of language models in the Basque language, inspired by the BLIMP (Warstadt et al., 2020) benchmark.\n\u2022 BasqueGLUE (Urbizu et al., 2022): BasqueGLUE is a NLU benchmark for Basque, which has been elaborated from previously existing datasets and following similar criteria to those used for the construction of GLUE and SuperGLUE.\n\u2022 Belebele (Bandarkar et al., 2024): Belebele is a multiple-choice machine reading comprehension dataset spanning 122 language variants.\n\u2022 X-StoryCloze (Lin et al., 2021): XStoryCloze consists of the professionally translated version of the English StoryCloze dataset to 10 non-English languages. It is a commonsense reasoning framework for evaluating story understanding, story generation, and script learning\n\u2022 EusProficiency, EusReading, EusExams, and EusTrivia (Etxaniz et al., 2024): Basque-specific benchmarks covering proficiency tests based on past EGA exams (C1 level Basque), reading comprehension, public service exam preparation, and trivia questions respectively."}, {"title": "Training setup", "content": "We opted for mixing Basque and English data with a ratio of 80:20 in order to ensure that the model's competence in Basque improves without suffering from catastrophic forgetting that hinders transfer-learning from English (Kuulmets et al., 2024). \u03a4\u03bf prevent language interference during the mixing of languages, we implemented a modified sequence packing strategy, ensuring that only examples from the same language were packed together in a single sequence.\nWe conducted full fine-tuning of all model parameters to maximize the model's ability to learn linguistic nuances in Basque. We utilized the Hugging Face Transformers (Wolf et al., 2020) library, alongside DeepSpeed ZeRO (Rajbhandari et al., 2020) and Accelerate (Gugger et al., 2022), to manage efficient large-scale training.\nTraining was carried out on 8 NVIDIA A100 80GB GPUs over 4 epochs, with a sequence length of 4096 tokens and an effective batch size of approximately 2 million tokens. In total, 7.2 billion tokens were processed, with training guided by a cosine learning rate schedule, peaking at 1e-4, and a warm-up phase comprising 10% of the total steps. All remaining hyperparameters followed the configurations established by Dubey et al. (2024). Estimated carbon emissions are detailed in Appendix G."}, {"title": "Results", "content": "To assess the validity of our approach, we compare our model against various versions of Llama-3.1, specifically the 8B and 70B models. The Llama-3.1-8B model serves as the base for our continual pre-training, establishing a baseline for evaluating the enhancements achieved through our method. Additionally, we include the Latxa models (Etxaniz et al., 2024) in our comparison, as they represent another open-source family of large language models specifically adapted to Basque, with parameter sizes ranging from 7 billion to 70 billion. As the only existing models tailored for the Basque language, Latxa provides a crucial baseline for evaluating our results.\nWe categorize the evaluation into sub-10 billion and over-10 billion parameter models to gain a clearer understanding of the performance differences across various model sizes. This distinction enables a fairer comparison of our model against both smaller and larger-scale architectures.\nIn the sub-10B parameter category, the results demonstrate that Llama-eus-8B significantly outperforms all other models across the Basque benchmarks (see Table 1), with the exception of a minor decline in the BL2MP task. Llama-eus-8B achieves the best average score, 61.22, which is notably higher than the 49.50 recorded by Latxa v1.2 7B and the 48.75 achieved by Llama-3.1-8B. This represents an average improvement of 12.47 points over the base model Llama-3.1-8B, underscoring the effectiveness of our continual pre-training strategy that incorporates Basque data.\nWhen comparing our Llama-eus-8B model with those in the over-10B parameter category, it is noteworthy that Llama-eus-8B not only surpasses Latxa-13B but also competes effectively against Latxa-70B across 5 out of 12 Basque benchmarks (see Table 1). While Latxa-70B excels in certain categories, Llama-eus-8B achieves an impressive average score of 61.22, trailing only by 3 points behind Latxa-70B, despite having significantly fewer parameters. This indicates a favorable balance between model size and performance, showcasing Llama-eus-8B's ability to deliver solid results without the need for a larger model. Interestingly, Llama-3.1-70B achieves the highest average score, 65.97, across the Basque benchmarks, even though it has not been specifically trained for Basque tasks.\nWe also assessed the English performance of our Llama-eus-8B model following the continual pre-training phase, as maintaining its initial competencies is crucial. The analysis revealed that the model experiences only a modest decrease of 1.96 points in average English scores compared to the baseline Llama-3.1-8B. This outcome indicates that while Llama-eus-8B has been effectively adapted for Basque, it continues to demonstrate a commendable level of competency in English, preserving its foundational knowledge. However, a significant performance gap, 13.28 points, remains between Basque and English across all evaluated models. For additional details and insights into the results, readers are encouraged to consult Appendix D."}, {"title": "Instruction Tuning the Model in Basque", "content": "In this section, we present experiments aimed at enabling a foundational LLM to follow instructions in Basque. Our focus is twofold: first, to assess whether using a foundational model specifically adapted to Basque, as described in the previous chapter, offers any advantages; and second, to evaluate the feasibility of leveraging English instruction datasets translated into Basque via automatic translation for the instruction fine-tuning process."}, {"title": "Instruction datasets for Basque", "content": "Given the limited availability of instruction datasets in Basque and the cost challenges of manually creating equivalents to those in English, we explore the generation of Basque instruction datasets through automatic translation.\nFor this analysis, we utilize two widely recognized English instruction datasets, No_Robots (Rajani et al., 2023) and SlimOrca (Lian et al., 2023). No_Robots is a smaller, manually curated dataset licensed under CC BY-NC 4.0 license. It contains 9,500 instructions covering various tasks, including generation, open QA, and brainstorming. In contrast, SlimOrca is significantly larger, featuring 517,982 automatically generated instructions with GPT-4 (Achiam et al., 2023). It is distributed under MIT License.\nTo produce the Basque counterparts of these datasets, referred to as No_Robots_eu and SlimOrca_eu, we employ the Elia machine translation platform, which achieves a translation quality of 19.3 BLEU and 52.2 chrF++ for the English-to-Basque direction on the FLORES-200 Evaluation Benchmark (Team et al., 2022). Details of the newly generated datasets are presented in Table 2."}, {"title": "Evaluation methodology", "content": "Although manual evaluation requires significant effort, we opted for this approach to assess the ability of LLMs to follow instructions, given the limitations of automatic evaluation methods in language generation tasks. Specifically, a native Basque speaker evaluated the models using a random sample of 100 instructions from the No_Robots test set, which comprises 500 instructions across 10 categories. The random selection ensured a minimum representation from each category, with the 'coding' category excluded to focus solely on text-based tasks. The 100 instructions were manually translated into Basque. Additional details on the sampling process are provided in Appendix E.\nTo generate model responses, we employed greedy search decoding during inference to guarantee both stability and reproducibility. The generated outputs were manually classified into three categories: a) correct, b) partially correct, and c) wrong. A response was deemed correct if it fully addressed the task without introducing hallucinations or misinformation; partially correct if it accomplished part of the task but contained inaccuracies or incomplete elements; and wrong if it failed to satisfy the task requirements."}, {"title": "Training setup", "content": "The instruction fine-tuning of the foundational models was performed using LoRA (Hu et al., 2021), as preliminary experiments demonstrated that this approach yielded better results than full fine-tuning. The fine-tuning process employed a batch size of 64 instructions and a cosine learning rate scheduler with a peak learning rate of 2e-5. The LoRA-specific hyperparameters were set to a rank of 64, an alpha value of 16, and a dropout probability of 0.1. All other hyperparameters remained consistent with those used during the pretraining phase."}, {"title": "Results", "content": "Can Translated Data Effectively Instruct Basque LLMs? We evaluated the feasibility of instructing the Basque-adapted foundational model, Llama-eus-8B, using the machine-translated datasets No_Robots_eu and SlimOrca_eu (presented in Section 4.1). The results presented in Table 3 demonstrate a significant performance improvement over the Llama-3.1-8B-instruct baseline, previously the best 8B-instructed model for Basque. The Llama-eus-8B model trained with No_Robots_eu achieves a 9 percentage point increase in the correct rate and a 7 percentage point increase in the partially correct rate compared to Llama-3.1-8B-instruct. The gains are even more pronounced for Llama-eus-8B trained on SlimOrca_eu, which shows enhancements of 17 points in the correct rate and 15 points in the partially correct rate relative to Llama-3.1-8B-instruct. In addition, the results indicate that SlimOrca_eu is more suitable for intruction tunining, as the model tuned with SlimOrca_eu surpasses its counterpart by 8 points. This indicates that higher quality of No_Robots_eu does not offset its smaller size compared to SlimOrca_eu.\nHow Does Pre-training on Basque Impact Instruction Tuning Performance? We assessed the advantage of using a foundational model adapted to Basque language versus one without specific adaptation (Llama-eus-8B vs. Llama-3.1-8B). We compare the performance of the Llama-eus-8B + SlimOrca_eu model to that of the Llama-3.1-8B + SlimOrca_eu. The results, shown in Table 3, demonstrate the superior performance of Llama-eus-8B + SlimOrca_eu, with an improvement of 9 points in the correct rate and 7 points in the partially correct rate. This highlights the significant benefits of instructing with a foundational model that has been specifically adapted to the target language.\nThe top-performing model trained with SlimOrca_eu will henceforth be referred to as Llama-eus-8B-instruct."}, {"title": "Alignment to Human Preferences in Basque", "content": "In this section, we present experiments focused on adapting instruction tuned models for Basque to align with human preferences, with the goal of improving their ability to generate answers in Basque. Similar to the experiments described in Section 4, we address two key aspects: first, the feasibility of achieving alignment using preference sets in Basque, generated through the automatic translation of English datasets; and second, the potential benefits of adapting an instructed model specifically to Basque, compared to using a general instructed model without explicit adaptation to the language.\nSeveral algorithms have been developed to improve the alignment of language model responses with human preferences. Among these, Direct Preference Optimization (DPO) (Rafailov et al., 2023; Dubey et al., 2024) has emerged as a particularly promising approach due to its simplicity and effectiveness. DPO algorithm requires a dataset consisting of paired samples of \"preferred\" and \"rejected\" reponses to a given prompt. DPO leverages these preference pairs to directly optimize the model's output generation, focusing on improving the likelihood of the preferred responses compared to the rejected ones. Unlike traditional reinforcement learning methods like PPO (Schulman et al., 2017), DPO bypasses the need for reward models, simplifying the training process by operating directly on these preference rankings. Consequently, we have chosen to implement this algorithm in our study."}, {"title": "Preference Dataset for Basque", "content": "Achieving significant improvements with human preference adaptation algorithms typically requires a large training dataset containing thousands of examples, ideally constructed from manual human feedback on LLM outputs. However, this process is costly, so alternatives have been explored, such as using LLMs as evaluators or rankers to reduce the need for extensive manual input (Huang et al., 2024; Zhu et al., 2023).\nSince no such dataset exists in Basque, we opted to translate an existing public dataset from English to Basque. After evaluating several options-OpenHermes (Huang et al., 2024), UltraFeedback (Cui et al., 2023a), and Nectar (Zhu et al., 2023)-we selected UltraFeedback as the most suitable for our experiments. UltraFeedback is a large-scale preference dataset consisting of almost 64k samples generated by various LLMs, with GPT-4 annotations covering four key aspects\u2014instruction-following, truthfulness, honesty, and helpfulness-based on prompts from diverse sources. It is distributed under MIT License. While it is the smallest of the options, it offers an ideal balance between quality and size, providing enough data for effective experimentation without overwhelming computational resources.\nWe followed the same methodology as out-"}, {"title": "Experimentation Results", "content": "Is translated preference data feasible for alignment? To evaluate the effectiveness of the UltraFeedback_eu dataset, we conducted DPO training on the best performing model for Basque from Section 4, Llama-eus-8B-instruct, an instruction-tuned model trained on SlimOrca_eu. Manual evaluation was carried out using the same test set employed in the instruction-following experiments described in Section 4. The results, shown in Table 4, indicate that model following DPO achieved a notable 7-point improvement in the accuracy of correct answers over Llama-eus-8B-instruct. These findings demonstrate the viability of using machine-translated preference data for alignment. This model will henceforth be referred to as Llama-eus-8B-instruct-DPO.\nChoosing the right base model for alignment: Basque vs. English To evaluate the benefit of using as a base model a model explicitly adapted to Basque, we compare the performance of our Llama-eus-8B-instruct-DPO model against Llama-3.1-8B-instruct aligned with UltraFeedback_eu. As shown in Table 4, the results of the manual evaluation highlight the clear superiority of Llama-eus-8B-instruct-DPO. Notably, the model based on Llama-3.1-8B-instruct performs worse than its base version. This demonstrates the substantial advantage of starting with a model explicitly tailored to the target language.\nEnglish performance. To evaluate the performance gap between our best Basque-adapted model, Llama-eus-8B-instruct-DPO, and state-of-the-art instruction-tuned models in English, we assessed the Llama-3.1-8B-instruct model using the English version of the 100-sample test set derived from the No_Robots dataset. The results revealed 91 correct responses, 6 partially correct, and 3 incorrect, highlighting the potential for further improvement in models specifically trained for Basque."}, {"title": "Conclusions", "content": "This work provides a comprehensive analysis of strategies for developping a model capable of following instructions in a low-resource language such as Basque, focusing on three key stages: pre-training, instruction tuning, and alignment with human preferences. The results demonstrate that tailoring each of these stages to the target language significantly enhances model performance compared to baseline English-centric models based on Llama 3.1.\nIn the pre-training stage, it has been shown that continual pre-training of a foundational English-centric LLM with a high-quality corpus of fewer than 1 billion words in the low-resource language can yield an average improvement of over 12 points in natural language understanding (NLU) tasks.\nIn the instruction tuning and human preference alignment stages, automatic translation of English datasets proved effective for training models to follow instructions in Basque, surpassing the instructed Llama 3.1 baseline. Using Basque-adapted models as the training base further enhanced performance. Following instruction tuning, the application of DPO with translated preference datasets improved model accuracy by up to 7 points.\nThe experimental results establish Llama-eus-8B as the most suitable foundational model for Basque within its parameter scale, and Llama-eus-8B-instruct as the first model specifically instruction-tuned for Basque, achieving the highest performance in this language. However, despite these advances, the performance of both models in Basque still lags behind Llama 3.1's performance in English, underscoring a substantial performance gap and indicating considerable room for further enhancement. Especially Llama-eus-8B-instruct, still require further refinement to become competitive in real-world production environments."}, {"title": "Limitations", "content": "The experimentation focused on strategies to develop a model capable of following instructions in a low-resource language like Basque, using Llama-3.1-8B as the baseline within the sub-10B parameter category. We acknowledge that results may vary across different architectures and model sizes, and thus the findings of this study may not be directly applicable to other LLMs with differing characteristics. Similarly, the results for Basque may not necessarily be replicable for other low-resource languages, as capabilities can vary across linguistic contexts.\nThe evaluation of the models' ability to follow instructions was carried out manually, using a sample of 100 instructions randomly selected from the No_Robots dataset, a high-quality test set manually curated. We chose to evaluate several models with a sample of 100 examples rather than fewer models with a larger sample. The results of this manual evaluation and the conclusions drawn should be interpreted within the context of the nature and size of this test set.\nIn the analysis of the pipeline for developing adapted LLMs for low-resource languages, we investigated strategies that outperformed the proposed baselines. However, these strategies may be suboptimal. This research represents a solid first step in developing techniques for building fine-tuned LLMs for Basque."}, {"title": "Ethics Statement", "content": "Like other generative language models, Llama-eus-8B and Llama-eus-8B-instruct may produce information that does not align with certain ethical values, such as displaying negative social biases towards some minorities. Although the pre-training of these models was carried out using a corpus based on reliable sources, there is a possibility that unwanted biases or other ethical patterns were learned. Plans are in place to correct the identified social biases in these models in the short term. In any case, we recommend that Llama-eus-8B and Llama-eus-8B-instruct be used in controlled environments, preceded by a thorough analysis of any potential harm they could cause in the specific use cases for which they are employed."}, {"title": "Acknowledgments", "content": "This work is part of the BasqueLLM project, titled \"First steps towards an artificial intelligence in Basque based on LLMs\" (EXP: 2023-CIEN-000081-01), partially funded by the Guipuzcoa Science, Technology and Innovation Network Program of the Provincial Council of Gipuzkoa. Model training and development were conducted using the Hyperion system at the Donostia International Physics Center (DIPC)."}, {"title": "A Foundational Model Choice: Balancing Performance and Efficiency", "content": "We chose Llama-3.1-8B (Dubey et al., 2024) as our base foundational model for this work. Although larger models, such as the 70B version, were initially considered and demonstrated superior capabilities, their high computational and memory demands pose significant challenges in resource-limited environments, both during training and deployment, making Llama-3.1-8B the optimal choice for our scenario.\nWe initially evaluated earlier versions, such as Llama-2 (Touvron et al., 2023), but ultimately selected Llama-3.1 as it consistently achieved the best results for Basque NLU tasks among all Llama variants. In addition to its superior results, Llama-3.1 features an expanded vocabulary designed to better support multiple languages and offers broader native support for a wider set of languages. These qualities made Llama-3.1 the optimal choice for our model adaptation to Basque. In Table 5 we report different base models comparison results, comprising Llama-2-7B, Llama-3-8B and Llama-3.1-8B."}, {"title": "ZelaiHandi Dataset Information", "content": "For continual pre-training, we utilized the Zelai-Handi dataset (San Vicente et al., 2024), the largest collection of freely licensed and clean Basque texts available as of October 2024. This dataset, comprising approximately 521 million words (around 1.5B tokens with Llama-3.1 tokenizer), was meticulously compiled from selected web sources, ensuring that only high quality documents published under free license were included. By high quality we refer to texts that are well-formed, free of excessive noise or errors, and representative of formal and diverse language use across various domains."}, {"title": "Performance Analysis of Dataset Choice for Continual Pre-training", "content": "Before the continual pre-training phase, we conducted an analysis to determine the most adequate Basque dataset for the task. We focused on analyzing the trade-offs between dataset size, quality, and their impact on computational efficiency. To that end, apart from the ZelaiHandi dataset (see Section 3.2) we considered two additional Basque datasets to evaluate their impact on model performance:\n\u2022 ZelaiItxi: a proprietary dataset that extends ZelaiHandi by incorporating additional closed-source data from selected sources of comparable quality.\n\u2022 Latxa (Etxaniz et al., 2024): the dataset used to trained the Latxa (Etxaniz et al., 2024) family of large language models for Basque."}, {"title": "Performance on English", "content": "We also assessed the English performance of our Llama-eus-8B model following the continual pre-training phase, as maintaining its initial competencies is crucial. The analysis revealed that the model experiences only a modest decrease of 1.96 points in average English scores compared to the base model, Llama-3.1-8B. This outcome indicates that while Llama-eus-8B has been effectively adapted for Basque, it continues to demonstrate a commendable level of competency in English, preserving its foundational knowledge. However, a significant performance gap, 13.28 points, remains between Basque and English across all evaluated models, highlighting the need for further enhancements in this area. Bridging this gap will be vital in future iterations, especially by leveraging transfer learning strategies to improve knowledge transfer from English to Basque. Table 9 shows additional details and insights into the results.\nThe results presented in Table 9 highlight the performance of the Llama-eus-8B model in comparison to both the Latxa and Llama models across various benchmarks. Notably, Llama-eus-8B demonstrates only a minor decrease in average English scores when compared to the base model Llama-3.1-8B, achieving a commendable"}]}