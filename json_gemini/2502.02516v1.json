{"title": "Adaptive Exploration for Multi-Reward Multi-Policy Evaluation", "authors": ["Alessio Russo", "Aldo Pacchiano"], "abstract": "We study the policy evaluation problem in an online multi-reward multi-policy discounted setting, where multiple reward functions must be evaluated simultaneously for different policies. We adopt an (\\epsilon, \\delta)-PAC perspective to achieve \\epsilon-accurate estimates with high confidence across finite or convex sets of rewards, a setting that has not been investigated in the literature. Building on prior work on Multi-Reward Best Policy Identification, we adapt the MR-NaS exploration scheme [34] to jointly minimize sample complexity for evaluating different policies across different reward sets. Our approach leverages an instance-specific lower bound revealing how the sample complexity scales with a measure of value deviation, guiding the design of an efficient exploration policy. Although computing this bound entails a hard non-convex optimization, we propose an efficient convex approximation that holds for both finite and convex reward sets. Experiments in tabular domains demonstrate the effectiveness of this adaptive exploration scheme.", "sections": [{"title": "Introduction", "content": "This paper investigates methods for efficiently evaluating one or more policies across various reward functions in an online discounted setting, a key challenge in Reinforcement Learning (RL) [38], where the aim is to compute the value of each policy. Accurate value estimation serves many purposes, from verifying a policy's effectiveness to providing insights for policy improvement.\nThere are several applications where one has multiple policies to evaluate, e.g., multiple policies arising from using different hyperparameters [9, 8, 27]. Similarly, multiple reward functions often arise in real-world decision-making problems, making it crucial to evaluate how a policy performs across diverse objectives. As an example, large language models [6] are fine-tuned on human feedback that spans a wide range of user preferences and goals [44, 31, 27], effectively producing multiple distinct reward signals.\nOther applications, similarly, involve multiple rewards, such as: user-preference modeling, robotics tasks aiming to reach different goals, or intent-based radio network optimization [25, 11, 34, 27].\nIn general, it is challenging to efficiently and accurately evaluate a policy over multiple objectives, potentially for multiple policies aimed at solving different task [39, 24, 16]. Indeed, when multiple policies and distinct reward sets are involved, it is not obvious how best to gather data in a way that balances efficiency and accuracy.\nPrior research approached this issue in different ways. One direction aims to minimize the mean squared error (MSE) of the value estimator (or the variance of the importance sampling estimator) to guide adaptive exploration. In [15], this is done for single-reward policy evaluation, while [16] proposes a variance-driven exploration scheme for a finite collection of policy-reward pairs. However, these methods may not always guarantee sample-efficient exploration or provide (\\epsilon, \\delta)-PAC guarantees. By contrast, other works [9, 8] address multi-policy evaluation for a single reward under the (\\epsilon, \\delta)-PAC criteria in the"}, {"title": "Related Work", "content": "Reinforcement Learning (RL) exploration techniques have tipically focused only on the problem on learning the optimal policy for a single objective [38]. This domain has generated a vast body of work, often inspired by the multi-armed bandit literature [21], with approaches ranging from \\epsilon-greedy and Boltzmann exploration [43, 38] to more sophisticated methods based on Upper-Confidence Bounds (UCB) [5], Bayesian procedures [26, 35] or Best Policy Identification techniques [3, 42, 40].\nDespite these advances, the challenge of designing exploration strategies for online policy evaluation has received comparatively little attention. Early work in this direction examined multi-armed bandits problems [4] and function evaluation [7], showing that efficient exploration requires allocating more samples where variance is higher. More recently, [23] considers a bandit setting with a finite number of tasks, focusing on minimizing the mean squared error.\nIn [15], the authors introduced the idea of optimizing the behavior policy (i.e., the exploration strategy) by directly minimizing the variance of importance sampling one of the earliest efforts"}, {"title": "Problem Setting", "content": "In this section, we describe the MDP model considered, and the policy evaluation setting."}, {"title": "Markov Decision Processes (MDPs)", "content": "Markov Decision Processes (MDPs) are widely utilized to model sequential decision-making tasks [30]. In these tasks, at each time-step t \\in \\mathbb{N} an agent observes the current state of the MDP s_t and selects an action a_t to achieve a desired objective. This objective is encapsulated in terms of a reward r_t \\in [0, 1], observed after selecting the action. In RL, the primary goal of the agent is to determine a sequence of actions that maximizes the total reward collected from an unknown MDP."}, {"title": "Online Multi-Reward Multi-Policy Evaluation", "content": "In an online single-reward Policy Evaluation (PE) setting, the objective is to learn the value vector of a single policy-reward pair (\\pi,r). In the multi-reward, multi-policy case, we aim instead to learn the value vectors for \\Theta := {(\\pi, R_{\\pi}) : \\pi \\in \\Pi}, meaning each policy \\pi \\in \\Pi is evaluated on every reward in its own reward set R_{\\pi}, which can differ across policies.\nThis setting is closely related to off-policy policy evaluation (OPE) [41, 28]: in fact, to learn the value of a policy \\pi, it might be a good idea to collect data from a different policy than \\pi! However, OPE deals with the problem of learning the value of a policy using some other policy \\pi_{\\beta} (and, in most cases, using a finite set of data). In our case, we instead study the problem of devising an optimal data-collection policy that, by online interactions with the MDP \\mathcal{M}, permits the agent to learn the value of as quickly as possible up to the desired accuracy."}, {"title": "Online Multi-Reward Multi-Policy Evaluation.", "content": "We formalize our objective using the (\\epsilon, \\delta)-PAC (Probably Approximately Correct) framework. In such framework, an algorithm Alg interacts with \\mathcal{M} until sufficient data has been gathered to output the value of up to \\epsilon accuracy for any reward r \\in R, for all \\pi \\in \\Pi, with confidence 1 \\mathcal{M}, we define an algorithm to be (\\epsilon, \\delta)-PAC as follows."}, {"title": "Definition 3.2 ((\u03b5, \u03b4)-PAC algorithm).", "content": "An algorithm Alg is said to be multi-reward multi-policy (\\epsilon, \\delta)-\\text{PAC} if, for any MDP \\mathcal{M}, policies-rewards set \\Theta, \\delta \\in (0,1/2), \\epsilon \\in (0, \\frac{2\\gamma}{(1-\\gamma)}), we have \\mathbb{P}_{\\mathcal{M}}[\\tau < \\infty] = 1 (the algorithm stops almost surely) and\n\\mathbb{P}_{\\mathcal{M}}\\left[\\exists \\pi \\in \\Pi, \\exists r \\in R_{\\pi} : ||V_{\\pi} - \\hat{V}_{\\pi}||_{\\infty} > \\epsilon\\right] \\leq \\delta.\\tag{1}\nIn other words, with probability at least 1 - \\delta, the algorithm's estimate \\hat{V}^r_{\\pi} is within \\epsilon of V_{\\pi}^r for every r \\in R_{\\pi}, \\pi \\in \\Pi.\nIn the following section, we investigate the sample complexity of this problem and determine the minimal number of samples required to achieve the (\\epsilon, \\delta)-PAC guarantees. Our analysis reveals that"}, {"title": "Adaptive Exploration through Minimal Sample Complexity", "content": "We seek to design a data collection strategy achieving minimal sample complexity. Building on BPI techniques [13, 3], we first derive an instance-specific sample complexity lower bound for any (\\epsilon, \\delta)-PAC algorithm.\nThis bound, which is posed as an optimization problem, specifies the optimal exploration policy, enabling the derivation of an efficient algorithm. The key step lies in bounding the expected log-likelihood ratio between the true model \\mathcal{M} and a carefully constructed \u201cconfusing\u201d model \\mathcal{M}\u2019.\nIn the following section, we detail how these alternative models \\mathcal{M}\u2019 are designed and leveraged to derive the sample complexity lower bound."}, {"title": "Set of Alternative Models", "content": "Confusing models are alternative models that are \u201csimilar\u201d to \\mathcal{M}, but differ in certain key properties. As we see later, the set of alternative models is crucial for establishing a lower bound on the sample complexity \\tau.\nTo prove the lower bound, we frame the sample complexity problem as a goodness-of-fit test: does the observed data better align the true model \\mathcal{M} or an alternative model \\mathcal{M}\u2019?\nThe chosen model \\mathcal{M}\u2019 is constructed to be confusing\u2014that is, it is close to \\mathcal{M} (in the KL sense), yet differs by at least 2\\epsilon in the value of a policy \\pi under some reward r.\nFormally, for a policy \\pi \\in \\Pi, reward r \\in R_{\\pi}, the set of alternative models in (\\pi,r) is defined as\n\\text{Alt}_{\\pi, r}(\\mathcal{M}) := {\\mathcal{M}'_{\\pi} : \\mathcal{M} \\ll \\mathcal{M}'_{\\pi}, ||V^{\\mathcal{M}'_{\\pi}} - V^{\\mathcal{M}}_{\\pi}||_{\\infty} > 2\\epsilon},\\nwhere for \\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, P_{\\mathcal{M}}, r, \\gamma) the notation \\mathcal{M} \\ll \\mathcal{M}' means that P(s, a) is absolutely continuous with respect to P'(s, a) for all (s, a) (and P is the transition function of \\mathcal{M}). We also denote by \\text{Alt}(\\mathcal{M}) = \\cup_{\\pi \\in \\Pi, r \\in R} \\text{Alt}_{\\pi,r}(\\mathcal{M}) the entire set of alternative models over R.\nFrom a sample complexity perspective, we can interpret this set as follows: a larger set of alternative models may lead to increased sample complexity since it becomes more likely that one of these models is close to \\mathcal{M} as measured by their KL. Hence, we should expect the learning complexity to be dominated by the \u201cworst\u201d among such models.\nTo gain more intuition, consider the case where the set \\text{Alt}_{\\pi, r}(\\mathcal{M}) = \\emptyset is empty for some pair (\\pi,r). In this situation, any model P' sharing the same support as P yields a value that is close to that of P. Thus, there is no learning challenge for that reward. We'll see that characterizing when these sets are empty is crucial in our analysis.\nValue deviation. To analyze these confusing sets, and their implications for sample complexity, we define the following instance-dependent quantity, that we refer to as the one-step value deviation:\n\\rho^{\\pi}(s, s') := V^{\\pi}(s') - \\mathbb{E}_{\\hat{s} \\sim P(\\cdot|s,\\pi(s))}[V^{\\pi}(\\hat{s})] \\quad \\forall s, s' \\in \\mathcal{S}.\nThis quantity measures how much the value at a state s' deviates from the expected value under \\pi as s. As we see later it is \u201ceasier\u201d to construct alternative models if |\\rho^{\\pi}(s, s')| is large.\nWe also define these quantities in vector form \\rho^{\\pi}(s) := [\\rho^{\\pi}(s, s_1) \\dots \\rho^{\\pi}(s, s_{|\\mathcal{S}|})], so that ||\\rho^{\\pi}(s)||_{\\infty} = \\max_{s'} |\\rho^{\\pi}(s, s')| is the maximum one-step deviation at s. The deviation \\rho is closely related to the span of the value function sp(V) := \\max_s V^{\\pi}(s') - \\min_s V^{\\pi}(s), but, is in general smaller, as discussed in Appendix B.1.2."}, {"title": "Proposition 4.1.", "content": "Let \\pi \\in \\Pi,r \\in R_{\\pi}. The set \\text{Alt}_{\\pi, r}(\\mathcal{M}) is empty if and only if\n\\frac{2\\epsilon(1 - \\gamma)}{\\gamma} > \\max_{s \\in \\mathcal{S}} ||\\rho^{\\pi}(s)||_{\\infty}.\nThe proposition provides insights into the challenges of learning the value function:\nAs \\epsilon increases, or \\gamma decreases, the condition in Proposition 4.1 is more likely to be satisfied, meaning that the number of possible confusing models may increase. This fact could imply an increase in the sample complexity.\nThe proof introduces the notion of confusing states: a states is confusing if  smaller value of ||\\rho^{\\pi}(s)||_{\\infty} implies that state s does not significantly influence the sample complexity. Additionally, Lemma B.3 (in the appendix) suggests that \\max_{s'} |\\rho^{\\pi}(s, s')| is unlikely to be attained at s = s'.\nSince |\\rho^{\\pi}(s, s')| \\leq sp(V) (see Lemma B.3 in the appendix), similar values across states lead to lower sample complexity.\nFinally, if \\max_{s \\in \\mathcal{S}} ||\\rho^{\\pi}(s)||_{\\infty} = 0, then \\text{Alt}_{\\pi, r}(\\mathcal{M}) is empty regardless of the values of \\epsilon and \\gamma."}, {"title": "Proposition 4.2.", "content": "The vectors r for which \\max_s ||\\rho^{\\pi}(s)||_{\\infty} = 0 is precisely the subspace {\\alpha \\mathbb{1} : \\alpha \\in [0,1]}, where \\mathbb{1} is the vector of ones.\nWhile it may seem obvious that the unit vector reward cannot produce an alternative model, it is noteworthy that no other reward satisfies \\max_s ||\\rho^{\\pi}(s)||_{\\infty} = 0 for any MDP.\nWe are now ready to discuss the sample complexity, and we refer the reader to Appendix B.1.3 for further discussion on the set of alternative models, including a characterization of when \\rho^{\\pi}(s, s') is identically zero across all states."}, {"title": "Sample Complexity Lower Bound", "content": "As a consequence of Proposition 4.1, the analysis of the sample complexity must necessarily take into account the set of rewards. We let R_{\\pi} = {r \\in R_{\\pi} : \\text{Alt}_{\\pi,r}(\\mathcal{M}) \\neq \\emptyset} be the set of rewards for which the corresponding set of confusing models is non-empty.\nTo derive the sample complexity lower bound, we also define the characteristic time T_{\\epsilon}(\\omega; \\mathcal{M}) of a stationary state-action distribution \\omega under \\mathcal{M}:\nT_{\\epsilon}(\\omega; \\mathcal{M})^{-1} := \\inf_{\\pi \\in \\Pi, r \\in R_{\\pi}, \\mathcal{M}' \\in \\text{Alt}_{\\pi,r}(\\mathcal{M})} \\mathbb{E}_{\\omega} \\left[KL\\left(P||P'_{(s, a)}\\right)\\right],\\tag{2}\nwhere the inverse of T_{\\epsilon}(\\omega; \\mathcal{M}) can be thought of as information rate, i.e., the amount of information gathered per time-step under \\omega. Then, one can show that asymptotically the sample complexity lower bound for any (\\epsilon, \\delta)-PAC algorithm scales according to\nT(\\mathcal{M}) = \\min_{\\omega \\in \\Omega(\\mathcal{M})} T_{\\epsilon}(\\omega; \\mathcal{M})\\tag{3}"}, {"title": "Convexity of the Lower Bound", "content": "We find that it is hard to directly optimize T_{\\epsilon}(\\omega; \\mathcal{M}), since the optimization the set of alternative models may be non-convex. Observe that the set \\text{Alt}_{\\pi, r}(\\mathcal{M}) can be seen as the union of two sets {\\mathcal{M}' : \\max_s V^{\\mathcal{M}'_{\\pi}}(s) - V^{\\mathcal{M}_{\\pi}}(s) \\geq 2\\epsilon} and {\\mathcal{M}' : \\max_s V^{\\mathcal{M}_{\\pi}}(s) - V^{\\mathcal{M}'_{\\pi}}(s) \\geq 2\\epsilon}. The convexity of these sets depend on (\\pi,r, P) and, even if convex, may be disjoint."}, {"title": "Relaxed Characteristic Time", "content": "We proceed with finding a convex relaxation of T_{\\epsilon}(\\omega; \\mathcal{M}) that holds for all distributions \\omega \\in \\Delta(\\mathcal{S} \\times \\mathcal{A}). Such relaxation not only upper bounds T_{\\epsilon}(\\omega; \\mathcal{M}) in terms of R (instead of R_{\\pi}), but also allows to better understand the scaling of T. The proof can be found in Appendix B.2.3."}, {"title": "Theorem 4.5.", "content": "For all \\omega \\in \\Delta(\\mathcal{S} \\times \\mathcal{A}) we have T_{\\epsilon}(\\omega; \\mathcal{M}) < U_{\\epsilon}(\\omega; \\mathcal{M}), where\nU_{\\epsilon}(\\omega; \\mathcal{M}) := \\sup_{\\pi \\in \\Pi, r \\in R} \\max_{s \\in \\mathcal{S}} \\frac{\\gamma^2 ||\\rho^{\\pi}(s)||^2_{\\infty}}{4\\epsilon^2(1 - \\gamma)^2\\omega(s, \\pi(s))},\\tag{5}\nis convex in \\omega. Let U(\\mathcal{M}) = U_{\\epsilon}(\\omega^*; \\mathcal{M}) be the optimal rate, where \\omega^* = \\arg \\min_{\\omega \\in \\Omega(\\mathcal{M})} U_{\\epsilon}(\\omega; \\mathcal{M}) is the U_{\\epsilon}-optimal visitation strategy.\nThis theorem exhibits some of the characteristics we mentioned before: as expected, the complexity is characterized by pairs (s, s') for which the deviation |\\rho^{\\pi}(s, s')| is large, for some worst-case policy-reward pair. What this result suggests, is that sampling should be roughly proportional to the value deviation (a quantity that is a variance-like measure, as explained also in [33]). However, quantifying the gap |U(\\omega; \\mathcal{M}) - T_{\\epsilon}(\\omega; \\mathcal{M})| remains challenging, and we leave this analysis to future work.\nSolving this optimization problem is straightforward: when each R is finite, it can be formulated within the framework of disciplined convex programming (DCP) [14]. Furthermore, the problem remains convex when the sets R are convex, and can still be solved using DCP. In particular, under the reward-free scenario R_{\\pi} = [0,1]^{\\mathcal{S}}, \\forall \\pi \\in \\Pi, the optimization inr admits a closed-form solution, as we discuss in the next subsection."}, {"title": "Optimization over a Convex Set of Rewards", "content": "For a convex reward set R_{\\pi}, the optimization in Theorem 4.5 can be formulated as a convex problem in r. Additionally, we derive a closed-form solution for reward-free policy evaluation when R = [0, 1]^{\\mathcal{S}}. In this case the solution is defined by the matrix \\Gamma^\\pi_i(s) = (K^{\\pi}(s)G^{\\pi})^\\prime_{i,j}, where\nK^{\\pi}(s) = I - \\mathbb{1}P(s,\\pi(s))^T, \\quad G^{\\pi} = (I - \\gamma P^{\\pi})^{-1},\nand \\Gamma^\\pi_i(s) can be interpreted as the expected discounted number of visits to state j starting from i after we subtract out the the expected number of visits to j starting from s' \\sim P(s, \\pi(s)). In other words, \\Gamma^\\pi(s) is the analogue of the deviation \\rho in terms of discounted number of visits.\nThen, the relaxed characteristic time in the reward-free setting is determined by F(s, s') := \\sum_{j : \\Gamma^\\pi_{i,j}(s) > 0} \\Gamma^\\pi_{i,j}(s), and F(s, s') := -\\sum_{j : \\Gamma^\\pi_{i,j}(s) < 0} \\Gamma^\\pi_{i,j}(s).\nCorollary 4.6. For a convex set R_{\\pi}, the optimization problem in Theorem 4.5 can be cast as a convex problem in the reward (\\pi,r). Moreover, if R_{\\pi} = [0,1]^{\\mathcal{S}}, \\forall \\pi \\in \\Pi, then rate U_{\\epsilon} is\nU_{\\epsilon}(\\omega; \\mathcal{M}) = \\max_{\\pi,s,s'} \\frac{\\gamma^2 \\left[\\max \\left(\\Gamma^\\pi(s, s'), \\Gamma^\\pi(s, s')\\right)\\right]^2}{4\\epsilon^2 (1 - \\gamma)^2\\omega(s, \\pi(s))}\nWe also refer the reader to Appendix B.2.4 for a proof and to find more details, including an example showing the reward-free sample complexity in the Riverswim environment [37] for a single target policy \\pi. We are now ready to proceed to devising an algorithm based on the relaxed characteristic rate."}, {"title": "Algorithms for tabular MDPs and Deep Reinforcement Learning", "content": "In this section show how to adapt the MR-NaS (Multi-Reward Navigate and Stop) algorithm [34] for multi-reward multi-policy evaluation based on the results from the previous section."}, {"title": "MR-NaS", "content": "MR-NaS (Algorithm 1), and similar algorithms, are designed with 2 key components: (1) a sampling rule and (2) a stopping rule. We now discuss each of these.\nSampling Rule. The key idea is to sample according to the policy induced by\n\\omega^* = \\arg \\min_{\\omega \\in \\Omega(\\mathcal{M})} U_{\\epsilon/2}(\\omega; \\mathcal{M}).\nIndeed, sampling actions according to \\pi^*(a|s) = \\omega^*(s, a)/\\sum_b \\omega^*(s, b) guarantees optimality with respect to U^{\\epsilon/2}, as the solution \\omega^* matches the rate in Theorem 4.3. The factor \\epsilon/2 arises from the lower bound analysis that requires 2\\epsilon-separation. By tightening the accuracy to \\epsilon/2, we ensure the (\\epsilon, \\delta)-PAC guarantee, which would otherwise be hard to prove. This results in an additional constant factor 4 in the sample complexity.\nHowever, \\omega^* cannot be computed without knowledge of the MDP \\mathcal{M}. As in previous works [13, 3], we employ the certainty equivalence principle (CEP): plug-in the current estimate at time t of the transition function and compute \\omega_t. The allocation \\omega rapidly eliminates models confusing for M_t, efficiently determining whether the true model \\mathcal{M} is non-confusing-motivating our use of the CEP.\nIn summary, the algorithm proceeds as follows: at each time-step t the agent computes the optimal visitation distribution \\omega = \\arg \\min_{\\omega \\in \\Omega(M_t)} U_{\\epsilon/2}(\\omega; M_t) with respect to M_t, the estimate of the MDP"}, {"title": "Remark 5.1.", "content": "At first glance, the optimization problem \\min_{\\omega \\in \\Omega(M_t)} U_{\\epsilon/2}(\\omega; \\mathcal{M}) might appear to overlook actions a \\neq \\pi(s). However, these actions are accounted for through the forward constraint \\omega \\in \\Omega(M_t).\nStopping rule. Lastly, the method stops whenever sufficient evidence has been gathered to ob-tain (\\epsilon, \\delta)-PAC guarantees. These requires approximately U^{\\epsilon/2}(M) \\log(1/\\delta) samples (by inspecting Theorems 4.3 and 4.5).\nThis rule is defined by two quantities: (1) a threshold\n\\beta(N_\\tau, \\delta) = \\log(1/\\delta) + (\\mathcal{S}\\mathcal{A} - 1) \\log \\log \\left(1 + \\frac{N_\\tau(s, a)}{\\mathcal{S}\\mathcal{A} - 1}\\right);\n(2) the empirical characteristic time U_{\\epsilon/2}(N_t/t; M_t). In both, N_t(s, a) is the number of times action a has been selected in state s up to time t, and N_t = (N_t(s, a))_{s,a}. In conclusion, we stop as soon as t > U_{\\epsilon/2}(N_t/t; M_t)\\beta(N_t; \\delta). Hence, we have the following guarantees (see proof in Appendix C.1)."}, {"title": "Theorem 5.2.", "content": "MR-NaS guarantees \\mathbb{P}_{\\mathcal{M}}[\\forall \\pi \\in \\Pi,r \\in R : ||\\hat{V}^r - V^r||_{\\infty} < \\epsilon] \\geq 1 - \\delta; \\mathbb{P}_{\\mathcal{M}}[\\tau < \\infty] = 1 and \\limsup_{\\delta \\rightarrow 0} \\frac{\\mathbb{E}_{\\mathcal{M}} [\\tau]}{\\log (1/\\delta)} \\leq 4U^{\\epsilon}_*(\\mathcal{M})."}, {"title": "Numerical Results", "content": "In this section, we present the numerical results of MR-NaS, and other algorithms, on various environments with different reward sets (results consider 30 seeds).\nSettings. We study MR-NaS in 4 different environments: Riverswim [37], Forked Riverswim [33], DoubleChain [19] and NArms [37] (an adaptation of SixArms). For each environment we evaluated 3 scenarios: (1) multiple policy evaluation with finite reward sets for each policy; (2) multiple reward-free policy evaluation; (3) single reward-free policy evaluation (results for this one can be found in Appendix C.4).\nWhile our framework supports various settings, we focus on what we consider to be the most important and novel scenarios. For multiple-policy evaluation, we sampled three random policies for each seed. These policies were sampled uniformly from the set of policies optimal for one-hot rewards, where each reward equals 1 at a single state-action pair and 0 elsewhere. In the case of finite reward sets, each policy was evaluated using the corresponding rewards from these sets. In the reward-free scenario, evaluations were conducted across the canonical basis R^{\\text{canon}}_{\\pi} for each \\pi.\nAlgorithms. While our work is one of the first to study the reward-free evaluation problem, there are some prior works that study the multi-task policy evaluation. We consider (1) SF-NR [24], an algorithm for multi-task policy evaluation based on the Successor Representation, and we adapted it to also consider the reward-free setting (see Appendix C for more details). Next, we consider (2) GVFExplorer [16], a variance-based exploration strategy for learning general value functions [39] based on minimizing the MSE. However, such exploration strategy is not applicable to the reward-free setting. We also evaluated (3) Noisy Policy - Uniform, a mixture of the target policies \\pi_{\\text{mix}}(a|s) = \\frac{1}{|{\\pi \\in \\Pi: \\pi(s)=a}|}, mixed with a uniform policy \\pi_u with a constant mixing factor \\epsilon_t = 0.3. The resulting behavior policy is \\pi_t = (1 - \\epsilon_t)\\pi_{\\text{mix}} + \\epsilon_t\\pi_u. Lastly, (4) Noisy Policy - Visitation, computes the same behavior policy as in (3) with a non-constant mixing factor \\epsilon_t = 1/N_t(s_t), which is based on the number of visits.\nDiscussion. The results for the first two settings are shown in Figures 3 and 4 (policy evaluation was performed using the MDP estimate M_t at each time-step). MR-NaS achieves good accuracy on all environments. On the other hand, SF-NR and GVFExplorer have mixed performance. While SF-NR is not designed to optimize a behavior policy, we note that the exploration strategy used by GVFExplorer is similar to solving a problem akin to Equation (2), but neglects the forward equations when optimizing the behavior policy (see also Appendix C.3 for details). As a result, GVFExplorer tends to perform"}, {"title": "Conclusions", "content": "In this work, we studied the problem of devising an exploration strategy for online multi-reward multi-policy evaluation, accommodating reward sets that are either finite or convex, potentially encompassing all possible rewards. Leveraging tools from Best Policy Identification, we derived an instance-dependent sample complexity lower bound for the (\\epsilon, \\delta)-PAC setting. Based on this bound, we extended MR-NaS [34] to this policy-evaluation setting, and showed its asymptotic efficiency. Lastly, we compared MR-NaS against other adaptive exploration across various domains, demonstrating the efficiency of MR-NaS."}]}