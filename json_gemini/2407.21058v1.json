{"title": "Understanding the Interplay of Scale, Data, and Bias in Language Models:\nA Case Study with BERT", "authors": ["Muhammad Ali", "Swetasudha Panda", "Qinlan Shen", "Michael Wick", "Ari Kobren"], "abstract": "In the current landscape of language model research, larger\nmodels, larger datasets and more compute seems to be the\nonly way to advance towards intelligence. While there have\nbeen extensive studies of scaling laws and models' scal-\ning behaviors, the effect of scale on a model's social biases\nand stereotyping tendencies has received less attention. In\nthis study, we explore the influence of model scale and pre-\ntraining data on its learnt social biases. We focus on BERT\u2014\nan extremely popular language model and investigate bi-\nases as they show up during language modeling (upstream),\nas well as during classification applications after fine-tuning\n(downstream). Our experiments on four architecture sizes of\nBERT demonstrate that pre-training data substantially influ-\nences how upstream biases evolve with model scale. With in-\ncreasing scale, models pre-trained on large internet scrapes\nlike Common Crawl exhibit higher toxicity, whereas models\npre-trained on moderated data sources like Wikipedia show\ngreater gender stereotypes. However, downstream biases gen-\nerally decrease with increasing model scale, irrespective of\nthe pre-training data. Our results highlight the qualitative role\nof pre-training data in the biased behavior of language mod-\nels, an often overlooked aspect in the study of scale. Through\na detailed case study of BERT, we shed light on the com-\nplex interplay of data and model scale, and investigate how it\ntranslates to concrete biases.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) continue to grow in size\nat a remarkable rate, with technology companies investing\nmillions in infrastructure to produce ever-larger and more\ngeneral purpose models. Modern open weight models like\nLLaMA, Gemini and Falcon regularly have tens of billions\nof parameters, showcasing noteworthy capabilities across a\nrange of natural language processing applications.\nTo investigate the performance of LLMs in terms of\nmodel parameters, training data size, and compute re-\nsources, a rich literature in empirical scaling laws (Hernan-\ndez et al. 2021; Kaplan et al. 2020) has emerged, which sug-\ngests that bigger is indeed better (in terms of loss). Recent\nwork on scaling laws has also led to a more comprehensive\nunderstanding on the tradeoffs between data size and model\nparameters with a fixed compute budget (Hoffmann et al.\nGiven the pace of LLM development and the foun-\ndational role of scale, studying changes in model behavior\nwith size remains a pressing research problem.\nOne crucial question that has received less attention, how-\never, is how model scale influences social biases. LLMs in-\nherently absorb societal biases and harmful stereotypes from\ndata during both pre-training and task-specific fine-tuning.\nThese biases manifest as intrinsic biases within the embed-\nding space, leading to representational harms and stereo-\ntyping (Nangia et al. 2020; Nadeem, Bethke, and Reddy\n2020; May et al. 2019; Kurita et al. 2019), and extrinsic\nbiases leading to allocative harms (Barocas et al. 2017)\nin downstream predictions (Gehman et al. 2020; Garimella\net al. 2019; Blodgett, Wei, and O'Connor 2018). Prior work\nhas shown that pre-trained models can generate toxic lan-\nguage (Gehman et al. 2020), can have disparities in hate\nspeech classification (Sap et al. 2019), can perpetuate anti-\nMuslim bias in text generation (Abid, Farooqi, and Zou\n2021), can rely on racial biases even for high stake use cases\nsuch as clinical notes (Zhang et al. 2020), among other fail-\nures.\nThe growing scale of LLMs has been driven, in part, due\nto their popularity in commercially successful chat applica-\ntions (e.g. ChatGPT), as well as their instruction following\ncapabilities (Wei et al. 2021), making them useful for a va-\nriety of tasks. Chat and prompting applications tend to use\nautoregressive, decoder-only Transformer models (e.g. GPT-\n4, LLaMA, PaLM). While these models are at the cutting\nedge, they are often challenging to deploy in many cases, re-\nquiring massive compute resources and improvised prompt\nengineering. In contrast, encoder-decoder (e.g. T5, BART)\nand encoder-only (e.g. BERT, ROBERTa) Transformer mod-\nels trained on a Masked Language Modeling (MLM) objec-\ntive are often lighter, and remain workhorses for NLP appli-\ncations in industry. These models continue to be relevant for\napplications such as summarization, semantic search, senti-\nment analysis, and a wide array of classification tasks after\nfine-tuning, as evidenced by their continued success in pub-\nlic machine learning competitions (Holmes et al. 2024; King\net al. 2023). These models are also affected by biases in the\ntraining data similar to autoregressive LLMs, both during\npre-training and during the task-specific fine-tuning process.\nIn this study, we take a step back from the outsize discourse\non autoregressive models and focus on encoder-only LLMs"}, {"title": "Methods", "content": "In this section, we cover the models we train, our training\nconfiguration, the datasets used to train these models, and\nthe metrics we use to measure bias.\nWe experiment with four architecture sizes of BERT: BERT-\nMini, BERT-Small, BERT-Medium, BERT-Base. While\noriginally introduced in the context of model distilla-\ntion (Turc et al. 2019), we find that these models provide\na good testbed for experimenting with model scale, while\nholding the architecture constant. Table 1 shows the number\nof layers, hidden embedding size, and the number of param-\neters in each case. Following (Turc et al. 2019), we fix the\nnumber of attention heads to H/64, where H is the hidden\nembedding size. We use the publicly available architecture\nimplementations of miniature BERT architectures via Hug-\ngingFace\u00b9.\nFor each model size, we pre-train on three different datasets,\non a masked language modeling objective: (a) CC-100-EN:\nEnglish subset of Common Crawl (Conneau et al. 2019), (b)\nEnglish Wikipedia, and (c) a combination of CC-100-EN\nand Wikipedia in a multi-task setup. Text on Wikipedia data\nWe use a series of metrics from prior work to measure bi-\nases at different points. First, we evaluate biases intrinsic to\nthe model itself, i.e. relating to the masked language model-.\ning task it's trained on. Second, we fine-tune each model for\na downstream classification task and evaluate how its scale\nand pre-training data affects false positive rates across demo-\ngraphic groups. Third, we use linguistic analysis on the pre-\ntraining datasets themselves to understand the provenance of\nour observed biases. Here, we describe each of these metrics\nin detail.\nUpstream bias metrics. We use two metrics from prior\nwork (Steed et al. 2022) to evaluate upstream biases in our\npre-trained models.\nFirst, we evaluate gender bias using an extension of\nlog probability bias score from Kurita et al. (2019). We\nspecifically use the version of this metric used in Steed\net al. (2022), where templates are constructed for a list of\n28 professions taken from the Bias in Bios dataset (De-\nArteaga et al. 2019). The original dataset is built from\nCommon Crawl, which includes over 400,000 online bi-\nographies from 28 occupations. The dataset does not in-\nclude self-reported gender; we refer to the pronouns in\neach biography to denote gender. In our use-case, for\nthe list of 28 professions, we use templates of the form\n{pronoun} is a(n) {occupation} to measure the\nmodel's propensity towards either he/him or she/her pro-\nnouns. To increase the robustness of our measurements,\nwe also include template variations from Bartl, Nissim,\nand Gatt (2020), e.g. {pronoun} applied to the\nposition of {occupation}. For each occupation y\nand pronoun g, we compute the model's probability \\(P_{y,g}\\)\nfor the template. To control for baseline differences for pro-\nnouns, we also compute prior probability \\(\\pi_{y,g}\\) for a tem-\nplate where only the pronoun is present but the occupation is"}, {"title": "", "content": "masked, e.g., he is a [MASK]. We define our upstream\ngender bias metric as the difference in these probabilities:\n\\(\\log \\frac{P_{y, \\text{she/her}}}{\\pi_{y, \\text{she/her}}} -  \\log \\frac{P_{y, \\text{he/him}}}{\\pi_{y, \\text{he/him}}}\\)\nA higher absolute probability gap suggests that a model as-\nsociates one gender much more with an occupation, while a\nvalue close to zero implies equal association during masked\nlanguage modeling.\nSecond, we evaluate upstream biases beyond gender,\nand for a diverse set of demographic groups. Following\nHutchinson et al. (2020), we rely on sentiment analy-\nsis to measure upstream bias. Again, we re-use method-\nology from Steed et al. (2022) and construct templates\nof the form {identity} {person} is [MASK]. The\n{identity} term consists of about 50 diverse identity\ngroups such \u201cMuslim\u201d, \u201cJewish\u201d, \u201celderly\", \"gay\" etc.,\ntaken from Dixon et al. (2018). The original dataset consists\nof (a) 130,000 public comments from Wikipedia Talk pages,\nannotated for toxicity, which mention these identity groups;\nand (b) a synthetic test set to evaluate disparities in toxic-\nity classification. We leverage the identity groups to gener-\nate templates for upstream biases, and the synthetic test set\nto evaluate downstream biases later. The {person} part of\nthe template includes phrases like \"people\", \"spouse\" etc. to\nincrease the number of templates we measure. We compute\nthe 20 most likely tokens for [MASK] for each template.\nWe then use a pre-trained RoBERTa (Liu et al. 2019) senti-\nment classifier\u00b2 trained on the TweetEval benchmark (Barbi-\neri et al. 2020) to measure the average negative sentiment for\neach identity group's completed prompts. We focus on neg-\native sentiment in particular as a proxy for toxicity and neg-\native associations similar to prior work (Steed et al. 2022;\nHutchinson et al. 2020), and due to its potential of introduc-\ning representational harms (Barocas et al. 2017).\nDownstream bias metrics. To evaluate downstream bi-\nases, we fine-tune our pre-trained model on a toxicity classi-\nfication task, and compare false positive rates (FPR) across\ndifferent identity groups. The FPR of a group g in the data\nis defined as\n\\(FPR_g = \\frac{FP_g}{FP_g + TN_g} =  \\frac{FP_g}{N_g}\\)\nHere, \\(FP_g\\) indicates the false positives in classification,\\(TN_g\\) is true negatives, and \\(N_g\\) are total number of ground\ntruth negatives (i.e. non-toxic sentences), all for group g\nspecifically. We focus on false positives since they can result\nin concrete allocative harms such as over-moderation and\nde-platforming (Jhaver et al. 2021) if such classifiers were to\nbe used for toxicity classification. Further, prior work (Steed\net al. 2022) has successfully used FPR to quantify down-\nstream performance disparities. We use the synthetic test\nset from Dixon et al. (2018) as our toxicity classification\ntask; the dataset contains 89K examples created using tem-\nplates of both toxic and non-toxic phrases which are filled in\nwith the 50 identity terms we also use in our upstream bias\""}, {"title": "Results", "content": "After our pre-training process, we obtain three variants\n(Wikipedia, CC-100-EN, and Wikipedia + CC-100-EN) of\neach model size (Mini, Small, Medium, Base), i.e. twelve\npre-trained LLMs in total. We first measure upstream biases\nin all these models using our two metrics; second, we fine-\ntune each model to the downstream task of toxicity clas-\nsification to measure downstream biases. Finally, we use\nour dataset bias metrics to measure the pre-training dataset\nthemselves, and investigate the provenance of the biases we\nobserve. Here, we present our results from these experi-\nments.\nWe begin by evaluating gender bias upstream using our im-\nplementation log probability bias score (Equation 1). Fig-\nure la shows absolute log probability gap between he/him\nand she/her pronouns for prompts related to 28 occupa-\ntions, for all 12 of our models. Since we use multiple occu-\npations for our metric, we visualize the probability gap as\na distribution across these occupations. Higher values sug-\ngest a skew towards either pronoun, while lower values sug-\ngest equal likelihood, and therefore better gender represen-\ntation. We observe that for models pre-trained on Wikipedia\n(green), gender stereotypes slightly increase with model\nsize-as seen in the increased variance and median. How-\never, for models pre-trained on CC-100-EN (blue), gender\nstereotypes seemingly decrease with model size. For models\npre-trained on the combination (orange), we do not observe\na consistent trend across model sizes. We qualitatively ob-\nserve that occupations such as \"nurse\", \u201cyoga teacher", "stupid": "sick", "insane\") on the list\nof top words predicted by the model. We also find evidence\nof these models generating (unfortunate) sentences such as\n\\\"Muslim people are dangerous": "Identities such as \"elderly\",", "deaf": "nd", "Muslim": "re the most frequent outliers across\nmodel sizes, which aligns with prior work (Dixon et al.\n2018; Abid, Farooqi, and Zou 2021). In contrast, for models\npre-trained on Wikipedia, we note MLM completions asso-\nciated with lower negative sentiment such as \u201cwrong\u201d, \u201cin-\njured", "wounded": "tc. These differences illustrate the ef-\nfect of both model scale and training data on output toxicity,\nsuggesting that larger models are more capable of learning\nbiases from the data-particularly when that data is from an\nunmoderated source.\nEvolution of bias over the training process. We also\nmonitor the change in upstream bias (measured via sen-\ntiment) during the course of the pre-training process. We\ncheckpoint all models after every 900 training steps dur-\ning the training process, and compute negative sentiment for\neach identity group at these checkpoints. Figure 2 shows\nhow upstream biases grow over time in our experiments.\nEach small point shows the average negative sentiment for\nan identity group, the large points connected via lines show\nthe average of average negative sentiment for each model.\nSimilar to our final measurement in Figure 1b, we no-\ntice here too that models trained on CC-100-EN (except\nBERT-Mini) have higher upstream bias. Models trained on\nWikipedia consistently have lower upstream bias and inter-\nestingly this does not increase or vary over training.\nNext, we turn our attention to downstream biases of our pre-\ntrained models. We attach a classification head to each of our"}, {"title": "", "content": "models and fine-tune them for the task of toxicity classifica-\ntion. Following prior work (Steed et al. 2022; Panda et al.\n2022), we use the synthetic toxicity classification data from\nDixon et al. (2018) for this task. We also evaluate an off-\nthe-shelf BERT (bert-base-uncased) from HuggingFace. As described earlier, we evaluate downstream biases\nin terms of differences in false positive rate (FPR) for sen-\ntences relating to each identity group. A higher FPR for a\ngroup indicates higher downstream biases, since the model\nis more likely to falsely flag mentions of that group as toxic,\npotentially leading to discriminatory censorship. Ideally, we\nwould like a model to have low FPR on each identity and\nlow variance in FPR across all groups.\nFigure 3 shows a distribution of FPR for each model size\nand pre-training dataset after fine-tuning. We observe that\nthe median FPR decreases as model size increases, regard-\nless of pre-training data; similarly, we note that the variance\nof FPR across groups decreases for larger models as well.\nThis suggests that after fine-tuning, larger models make\nmore robust classifiers-they make fewer false positive er-\nrors for all groups in our experiments.\nThe decrease in downstream biases with scale can have\na few explanations. First, MLMs are known to use short-\ncut heuristics instead of task-specific robust heuristics (e.g.,\nmodel fine-tuned for Natural Language Inference (NLI) uses\nhigh word overlap with the conclusion, to predict entailment\n(McCoy, Pavlick, and Linzen 2019); scale improves these\nresults (Bhargava, Drozd, and Rogers 2021)). The model\nmight latch on to certain identities as shortcuts to predict\ntoxicity. Second, allocative harms are frequently measured\ndirectly using accuracy statistics e.g., FPR in our case. As\nscaling laws suggest that test accuracy increases with scale,\ndownstream bias statistics will improve as well.\nHowever, certain identity groups such as \u201cgay\u201d, \u201cqueer\u201d\nand \"homosexual\u201d consistently show up as outliers in terms\nof FPR, regardless of model size or type of pre-training data.\nPrior work (Steed et al. 2022) has shown that downstream\ndisparities are largely explained by the fine-tuning data; our\nobserved outliers likely appear disproportionately in toxic\nsentences, leading to a higher FPR. This suggests that while\nlarger versions of BERT make more robust downstream clas-\nsifiers, they are not able to address biases against extreme\noutliers.\nFinally, we investigate the impact of pre-training data on\nthe biases we observe. While downstream biases can be ex-\""}, {"title": "", "content": "plained as an artifact of the fine-tuning data (Steed et al.\n2022), we suspect a much tighter coupling between up-\nstream biases and choice of pre-training data.\nTo understand our observed upstream gender bi-\nases (Figure la), we use weighted log odds with a\nDirichlet prior (Monroe, Colaresi, and Quinn 2008)\nand compare (pronoun, occupation) pair occur-\nrences between CC-100-EN and Wikipedia. Specifi-\ncally (in terms of Equation 2), for each occupation\n\\(o \\in \\{\\text{journalist, physician, painter, ...}\\}\\), and pronoun \\(p \\in\n\\{\\{\\text{he, him, his, himself}\\}, \\{\\text{she, her, hers, herself}\\}\\}\\) we mea-\nsure:\n\\(\\log \\frac{P^{\\text{CC-100}}}_{o,p}}{P^{\\text{Wiki}}}_{o,p}\\)\nA positive value indicates a co-occurrence is more likely in\nCC-100-EN than in Wikipedia, while a negative value means\nit is more likely in Wikipedia. Also note that we count fre-\nquencies for a set of pronouns and not singular pronouns for\nmore robust counting. Further, to normalize for variance, we\nz-normalize the log odds; using the one-sided critical value\nfor p = 0.05, we only consider z > 1.645 to be a signif-\nicant difference between both datasets. Table 2 shows the\n10 occupations with the highest weighted log odds between\nCC-100-EN and Wikipedia.\nWhile we observe many differences that are not sig-\nnificant, for certain occupations, Wikipedia indeed en-\ncodes greater gender stereotypes, e.g., \"professor\" has\nsignificantly higher masculine pronoun associations, and\n\"model\" has higher feminine pronoun associations. Inter-\nestingly, \"teacher\" is the only occupation that has signifi-\ncant stereotypical associations in both datasets: masculine\nin Wikipedia, feminine in CC-100-EN. One trend, despite\nlack of significance, is that co-occurrences with masculine\npronouns are overall more common in Wikipedia than CC-\n100 (larger negative values in M column). This may be re-\nflective of a broader trend on Wikipedia, beyond gendered\nstereotypes for specific occupations, where the vast majority\nof biographical articles are about men, due to biases in who\nis perceived as notable (Tripodi 2023). Conversely, while\nlarge web scrapes like CC-100 are more diverse in overall\ntopics covered, these might involve more toxic text.\nTo understand the provenance of our upstream sentiment"}, {"title": "Limitations", "content": "Our approach has multiple limitations. First, while we limit\nour analysis to a single model family to reduce variance\nin architecture, it limits the ecological validity of our re-\nsults. Our results therefore cannot be generalized to all mod-\nern LLM architectures, and instead provide a detailed look\ninto BERT specifically. Second, compared to state-of-the-art compute intensive training procedures, our pre-training\nprocess is quite rudimentary. We limit the training process\nto only 8000 training steps as a heuristic to upper bound the\namount of compute that each model uses; this is a simplifica-\ntion and does not lead to a model as powerful as those avail-\nable through model hubs like HuggingFace. Third, while we\nmake sure to evaluate bias holistically by examining both\nupstream and downstream differences, our bias metrics-such as log probability gaps and sentiment are not defini-\ntive ways of measuring bias. Bias is a complex, socio-technical, and sometimes ill-defined notion whose meaning\ncan vary across domains and tasks. While we rely on metrics\nfrom prior work, our measures are prone to the same pitfalls"}, {"title": "Concluding Discussion", "content": "Our study provides a detailed case study on the interplay of\nscale, pre-training data, and bias with a specific focus on\nBERT, a widely used LLM. We find evidence that larger\nmodels are able to encode more biases upstream. Impor-\ntantly, we observe that larger models, combined with un-\nmoderated data, can lead to worse results for the task of\nmasked language modeling. However, larger models can\nalso produce more robust downstream classifiers after fine-\ntuning.\nWhile MLMs like BERT do not represent the state-of-the-art in the rapidly developing landscape of LLM research,\nthey remain extremely relevant for several applied natural\nlanguage processing problems. Our investigation of bias is\nparticularly relevant to practitioners who fine-tune embed-\nding models for their tasks. In these applied use-cases, our\nresults shed light on how scale and training data together\ncan lead to different kinds of biases. We encourage practi-\ntioners to be aware of the biases their training datasets can\nintroduce, and to actively measure these artifacts during the\ndevelopment process. On a more general level, our study\nhighlights the role that training data can play in scaling, es-\npecially as it relates to biased model behavior. Our results\nalso suggest that mixing in a moderated, high quality data\nsource (e.g., Wikipedia) with larger datasets (e.g., CC-100,\nThe Pile (Gao et al. 2020)) might be an approach to alleviate\nbiases-we leave a full exploration of this direction to future\nwork.\nOur analyses also underscore the limitations that ex-\nist in metrics used to measure bias, which is a nuanced\nsocio-technical concept, whose meaning changes across\ntasks and domains. Negative sentiment and gaps in gender\nrepresentation\u2014as used here are well-scoped ways of ex-\npressing bias that can be useful for different domains. Neg-\native sentiment, for instance, could be a useful measure of\nbias for LLM use in chatbots or auto-complete tools; dif-\nferences in gender likelihood could be useful for measuring\nbias in resum\u00e9 or search ranking, but they are not universal\nmeasures of linguistic bias. As seen in our results, depending\non the choice of bias metric, a measurement of model be-\nhavior can look quite different. This aligns with prior work\n(Goldfarb-Tarrant et al. 2023; Blodgett et al. 2021) which\nshows that measuring bias or fairness can be a challenging\nundertaking, and it is easy to set up an incompatible met-\nric. Our results highlight the need for identifying the correct\nbias metric for each domain, and judging both the data and\nthe model by that metric."}, {"title": "Ethical Considerations", "content": "Our study attempts to measure a social issue with techni-\ncal tools, and therefore it relies on some shortcut heuristics\nand simplifications that we attempt to make explicit here. In\nstudying gender disparities, we rely on pronouns and only\nfocus on he/him and she/her since our metrics are set up as\nsubtractions. This simplification is not meant to reinforce the"}]}