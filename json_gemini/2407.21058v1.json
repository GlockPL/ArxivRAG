{"title": "Understanding the Interplay of Scale, Data, and Bias in Language Models: A Case Study with BERT", "authors": ["Muhammad Ali", "Swetasudha Panda", "Qinlan Shen", "Michael Wick", "Ari Kobren"], "abstract": "In the current landscape of language model research, larger models, larger datasets and more compute seems to be the only way to advance towards intelligence. While there have been extensive studies of scaling laws and models' scaling behaviors, the effect of scale on a model's social biases and stereotyping tendencies has received less attention. In this study, we explore the influence of model scale and pre-training data on its learnt social biases. We focus on BERT\u2014an extremely popular language model and investigate biases as they show up during language modeling (upstream), as well as during classification applications after fine-tuning (downstream). Our experiments on four architecture sizes of BERT demonstrate that pre-training data substantially influences how upstream biases evolve with model scale. With increasing scale, models pre-trained on large internet scrapes like Common Crawl exhibit higher toxicity, whereas models pre-trained on moderated data sources like Wikipedia show greater gender stereotypes. However, downstream biases generally decrease with increasing model scale, irrespective of the pre-training data. Our results highlight the qualitative role of pre-training data in the biased behavior of language models, an often overlooked aspect in the study of scale. Through a detailed case study of BERT, we shed light on the complex interplay of data and model scale, and investigate how it translates to concrete biases.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) continue to grow in size at a remarkable rate, with technology companies investing millions in infrastructure to produce ever-larger and more general purpose models. Modern open weight models like LLaMA, Gemini and Falcon regularly have tens of billions of parameters, showcasing noteworthy capabilities across a range of natural language processing applications.\nTo investigate the performance of LLMs in terms of model parameters, training data size, and compute resources, a rich literature in empirical scaling laws (Hernandez et al. 2021; Kaplan et al. 2020) has emerged, which suggests that bigger is indeed better (in terms of loss). Recent work on scaling laws has also led to a more comprehensive understanding on the tradeoffs between data size and model parameters with a fixed compute budget (Hoffmann et al. 2022). Given the pace of LLM development and the foundational role of scale, studying changes in model behavior with size remains a pressing research problem.\nOne crucial question that has received less attention, however, is how model scale influences social biases. LLMs inherently absorb societal biases and harmful stereotypes from data during both pre-training and task-specific fine-tuning. These biases manifest as intrinsic biases within the embedding space, leading to representational harms and stereotyping (Nangia et al. 2020; Nadeem, Bethke, and Reddy 2020; May et al. 2019; Kurita et al. 2019), and extrinsic biases leading to allocative harms (Barocas et al. 2017) in downstream predictions (Gehman et al. 2020; Garimella et al. 2019; Blodgett, Wei, and O'Connor 2018). Prior work has shown that pre-trained models can generate toxic language (Gehman et al. 2020), can have disparities in hate speech classification (Sap et al. 2019), can perpetuate anti-Muslim bias in text generation (Abid, Farooqi, and Zou 2021), can rely on racial biases even for high stake use cases such as clinical notes (Zhang et al. 2020), among other failures.\nThe growing scale of LLMs has been driven, in part, due to their popularity in commercially successful chat applications (e.g. ChatGPT), as well as their instruction following capabilities (Wei et al. 2021), making them useful for a variety of tasks. Chat and prompting applications tend to use autoregressive, decoder-only Transformer models (e.g. GPT-4, LLaMA, PaLM). While these models are at the cutting edge, they are often challenging to deploy in many cases, requiring massive compute resources and improvised prompt engineering. In contrast, encoder-decoder (e.g. T5, BART) and encoder-only (e.g. BERT, ROBERTa) Transformer models trained on a Masked Language Modeling (MLM) objective are often lighter, and remain workhorses for NLP applications in industry. These models continue to be relevant for applications such as summarization, semantic search, sentiment analysis, and a wide array of classification tasks after fine-tuning, as evidenced by their continued success in public machine learning competitions (Holmes et al. 2024; King et al. 2023). These models are also affected by biases in the training data similar to autoregressive LLMs, both during pre-training and during the task-specific fine-tuning process. In this study, we take a step back from the outsize discourse on autoregressive models and focus on encoder-only LLMs"}, {"title": "Related Work", "content": "Our study broadly relates to three strands in the literature: fairness and auditing of machine learning algorithms in general, the study of biases in natural language processing more specifically, and scaling laws for large language models.\nFirst, a rich literature in computer science investigates issues of fairness in machine learning (Dwork et al. 2012; Barocas, Hardt, and Narayanan 2023), measuring the different types of harms these systems can have\u2014such as denigration, stereotyping, differential quality of service etc. (Barocas et al. 2017; Weerts 2021). Notably, prior work has documented racial and gender disparities for commercial gender classification systems (Buolamwini and Gebru 2018), racial disparities in criminal recidivism prediction (Angwin et al. 2022), gender disparities in the delivery of job advertising (Datta, Tschantz, and Datta 2014; Ali et al. 2019), among others. Many of these studies only rely on \"black box\" access to machine learning systems, and have to conduct clever audits to measure disparate outcomes (Metaxa et al. 2021). Our work is connected to this literature in its goal of measuring inadvertent harms of a machine learning system, albeit with \"white box\" access to the model's output probabilities and weights.\nWithin natural language processing (NLP) specifically, prior work has also discussed biased and disparate outcomes for users. Blodgett, Green, and O'Connor (2016) was one of the earliest works documenting racial disparities, showing how dependency parsing tools struggle on text for African American English on Twitter. Similarly, Caliskan, Bryson, and Narayanan (2017) demonstrated how word embeddings learnt from text corpora can contain gender biases. In the context of large language models-which power most of modern NLP-recent work has documented negative associations for people with disabilities (Hutchinson et al. 2020), anti-Muslim bias (Abid, Farooqi, and Zou 2021), and a general propensity to generate toxic text (Gehman et al. 2020). There have also been efforts to construct benchmarks that can yield repeatable measurements of bias across many different language models. This includes benchmarks such as WinoBias for coreference resolution (Zhao et al. 2018), BBQ and UNQOVER for question-answering (Parrish et al. 2021; Li et al. 2020), BBNLI for natural language inference (Baldini et al. 2023), StereoSet for measuring stereotypical associations (Nadeem, Bethke, and Reddy 2020), among others. Further, large benchmarking efforts such as BIG-bench (Srivastava et al. 2022) have been able to provide insights into the relationship between model scale and performance on bias benchmarks, which is one of the objectives of our study. We now know from Srivastava et al. (2022) that for auto-regressive models, bias (as measured via UNQOVER, BBQ etc.) typically increases in ambiguous prompts, and that it can decrease for narrow, unambiguous prompts. We similarly study the relation of bias with scale, but in the context of MLM models, which have distinct upstream and downstream applications, and with an added focus on the pre-training dataset used. Closest to our study is Steed et al. (2022)'s work on upstream and downstream biases for MLMs, in which they investigate the bias transfer hypothesis can upstream debiasing methods improve disparities in downstream performance? They find that upstream mitigation does little to address downstream biases, and that downstream disparities are better explained by biases in the fine-tuning data.\nIn parallel, empirical scaling laws related to LLM performance have been the subject of extensive investigation in recent research (Hestness et al. 2017; Kaplan et al. 2020). These studies have found a power-law scaling relationships with model size, dataset size, and computational resources, i.e. an increase in either almost always leads to a decrease in loss. Recently, Hoffmann et al. (2022) have also led to a clearer understanding of the tradeoffs between training data size (number of tokens) and model parameters, yielding a unified formula for compute-optimal training, which has already been applied to specific model settings (Clark et al. 2022; Gordon, Duh, and Kaplan 2021; Henighan et al. 2020; Tay et al. 2022). However, it is noteworthy that the scalability of LLMs does not universally translate to improved performance across all downstream tasks, as demonstrated by Ganguli et al. (2022). Similarly, recent work by Wei et al. (2022) highlights emergent abilities unique to larger models not predicted by traditional scaling laws. In response to work in scaling laws, there has also been pushback from critics. Notably, Bender et al. (2021) highlighted the rising environmental and financial costs of model pre-training, and the lack of diversity in training data. Closely related to our study, Birhane et al. (2023) study scaling laws in the context of hateful content present in the LAION family of datasets, popularly used to pre-train text to image diffusion models. They find that as data scale increases, the tendency of models to associate Black faces with categories like \"criminal\" can significantly increase.\nOur work lies at the intersection of these research threads. We contribute to the ongoing practice of measuring the adverse outcomes of machine learning systems. Our work also contributes to ongoing work on scaling laws, with a specific focus on bias, and how it is picked up from pre-training data."}, {"title": "Methods", "content": "In this section, we cover the models we train, our training configuration, the datasets used to train these models, and the metrics we use to measure bias.\nModels\nWe experiment with four architecture sizes of BERT: BERT-Mini, BERT-Small, BERT-Medium, BERT-Base. While originally introduced in the context of model distillation (Turc et al. 2019), we find that these models provide a good testbed for experimenting with model scale, while holding the architecture constant. Table 1 shows the number of layers, hidden embedding size, and the number of parameters in each case. Following (Turc et al. 2019), we fix the number of attention heads to H/64, where H is the hidden embedding size. We use the publicly available architecture implementations of miniature BERT architectures via HuggingFace\u00b9.\nPre-Training Data\nFor each model size, we pre-train on three different datasets, on a masked language modeling objective: (a) CC-100-EN: English subset of Common Crawl (Conneau et al. 2019), (b) English Wikipedia, and (c) a combination of CC-100-EN and Wikipedia in a multi-task setup. Text on Wikipedia data"}, {"title": "Metrics", "content": "We use a series of metrics from prior work to measure biases at different points. First, we evaluate biases intrinsic to the model itself, i.e. relating to the masked language modeling task it's trained on. Second, we fine-tune each model for a downstream classification task and evaluate how its scale and pre-training data affects false positive rates across demographic groups. Third, we use linguistic analysis on the pre-training datasets themselves to understand the provenance of our observed biases. Here, we describe each of these metrics in detail.\nUpstream bias metrics. We use two metrics from prior work (Steed et al. 2022) to evaluate upstream biases in our pre-trained models.\nFirst, we evaluate gender bias using an extension of log probability bias score from Kurita et al. (2019). We specifically use the version of this metric used in Steed et al. (2022), where templates are constructed for a list of 28 professions taken from the Bias in Bios dataset (De-Arteaga et al. 2019). The original dataset is built from Common Crawl, which includes over 400,000 online biographies from 28 occupations. The dataset does not include self-reported gender; we refer to the pronouns in each biography to denote gender. In our use-case, for the list of 28 professions, we use templates of the form {pronoun} is a(n) {occupation} to measure the model's propensity towards either he/him or she/her pronouns. To increase the robustness of our measurements, we also include template variations from Bartl, Nissim, and Gatt (2020), e.g. {pronoun} applied to the position of {occupation}. For each occupation y and pronoun g, we compute the model's probability $P_{y,g}$ for the template. To control for baseline differences for pronouns, we also compute prior probability $\\pi_{y,g}$ for a template where only the pronoun is present but the occupation is masked, e.g., he is a [MASK]. We define our upstream gender bias metric as the difference in these probabilities:\n$\\log \\frac{P_{y, \\text{she/her}}}{\\pi_{y, \\text{she/her}}} - \\log \\frac{P_{y, \\text{he/him}}}{\\pi_{y, \\text{he/him}}}$\nA higher absolute probability gap suggests that a model associates one gender much more with an occupation, while a value close to zero implies equal association during masked language modeling.\nSecond, we evaluate upstream biases beyond gender, and for a diverse set of demographic groups. Following Hutchinson et al. (2020), we rely on sentiment analysis to measure upstream bias. Again, we re-use methodology from Steed et al. (2022) and construct templates of the form {identity} {person} is [MASK]. The {identity} term consists of about 50 diverse identity groups such \u201cMuslim\u201d, \u201cJewish\u201d, \u201celderly\u201d, \"gay\" etc., taken from Dixon et al. (2018). The original dataset consists of (a) 130,000 public comments from Wikipedia Talk pages, annotated for toxicity, which mention these identity groups; and (b) a synthetic test set to evaluate disparities in toxicity classification. We leverage the identity groups to generate templates for upstream biases, and the synthetic test set to evaluate downstream biases later. The {person} part of the template includes phrases like \"people\", \"spouse\" etc. to increase the number of templates we measure. We compute the 20 most likely tokens for [MASK] for each template. We then use a pre-trained RoBERTa (Liu et al. 2019) sentiment classifier\u00b2 trained on the TweetEval benchmark (Barbieri et al. 2020) to measure the average negative sentiment for each identity group's completed prompts. We focus on negative sentiment in particular as a proxy for toxicity and negative associations similar to prior work (Steed et al. 2022; Hutchinson et al. 2020), and due to its potential of introducing representational harms (Barocas et al. 2017).\nDownstream bias metrics. To evaluate downstream biases, we fine-tune our pre-trained model on a toxicity classification task, and compare false positive rates (FPR) across different identity groups. The FPR of a group g in the data is defined as\n$\\text{FPR}_g = \\frac{\\text{FP}_g}{\\text{FP}_g + \\text{TN}_g} = \\frac{\\text{FP}_g}{N_g}$\nHere, $\\text{FP}_g$ indicates the false positives in classification, $\\text{TN}_g$ is true negatives, and $N_g$ are total number of ground truth negatives (i.e. non-toxic sentences), all for group g specifically. We focus on false positives since they can result in concrete allocative harms such as over-moderation and de-platforming (Jhaver et al. 2021) if such classifiers were to be used for toxicity classification. Further, prior work (Steed et al. 2022) has successfully used FPR to quantify downstream performance disparities. We use the synthetic test set from Dixon et al. (2018) as our toxicity classification task; the dataset contains 89K examples created using templates of both toxic and non-toxic phrases which are filled in with the 50 identity terms we also use in our upstream bias"}, {"title": "Results", "content": "After our pre-training process, we obtain three variants (Wikipedia, CC-100-EN, and Wikipedia + CC-100-EN) of each model size (Mini, Small, Medium, Base), i.e. twelve pre-trained LLMs in total. We first measure upstream biases in all these models using our two metrics; second, we fine-tune each model to the downstream task of toxicity classification to measure downstream biases. Finally, we use our dataset bias metrics to measure the pre-training dataset themselves, and investigate the provenance of the biases we observe. Here, we present our results from these experiments.\nUpstream biases can increase with model size\nWe begin by evaluating gender bias upstream using our implementation log probability bias score (Equation 1). Figure la shows absolute log probability gap between he/him and she/her pronouns for prompts related to 28 occupations, for all 12 of our models. Since we use multiple occupations for our metric, we visualize the probability gap as a distribution across these occupations. Higher values suggest a skew towards either pronoun, while lower values suggest equal likelihood, and therefore better gender representation. We observe that for models pre-trained on Wikipedia (green), gender stereotypes slightly increase with model size\u2014as seen in the increased variance and median. However, for models pre-trained on CC-100-EN (blue), gender stereotypes seemingly decrease with model size. For models pre-trained on the combination (orange), we do not observe a consistent trend across model sizes. We qualitatively observe that occupations such as \"nurse\u201d, \u201cyoga teacher\" and \"software engineer\u201d consistently appear as outliers across model types.\nWe then measure upstream bias with our second metric, which is the average negative sentiment for prompt completions that relate to 50 identity groups. For each of our pre-trained models, we compute the average negative sentiment for multiple MLM prompts relating to each identity-Figure 1b shows the distribution of these negative sentiment scores. Here, we note that as model size increases, we observe a general upward trend in upstream bias, regardless of pre-training dataset. We also observe that models pre-trained on CC-100-EN achieve the highest average negative sentiment scores, followed by models pre-trained on the combination of CC-100-EN and Wikipedia. Models pre-trained on Wikipedia exhibit comparatively the lowest average negative sentiment in our experiments. Qualitatively, in case of models pre-trained on CC-100-EN, we notice frequent abusive mentions (e.g., \u201cstupid\u201d, \u201csick\u201d, \u201cinsane\") on the list of top words predicted by the model. We also find evidence of these models generating (unfortunate) sentences such as \"Muslim people are dangerous\u201d. Identities such as \"elderly\", \"deaf\" and \"Muslim\" are the most frequent outliers across model sizes, which aligns with prior work (Dixon et al. 2018; Abid, Farooqi, and Zou 2021). In contrast, for models pre-trained on Wikipedia, we note MLM completions associated with lower negative sentiment such as \u201cwrong\u201d, \u201cinjured\", \"wounded\" etc. These differences illustrate the effect of both model scale and training data on output toxicity, suggesting that larger models are more capable of learning biases from the data-particularly when that data is from an unmoderated source.\nEvolution of bias over the training process. We also monitor the change in upstream bias (measured via sentiment) during the course of the pre-training process. We checkpoint all models after every 900 training steps during the training process, and compute negative sentiment for each identity group at these checkpoints. Figure 2 shows how upstream biases grow over time in our experiments. Each small point shows the average negative sentiment for an identity group, the large points connected via lines show the average of average negative sentiment for each model. Similar to our final measurement in Figure 1b, we notice here too that models trained on CC-100-EN (except BERT-Mini) have higher upstream bias. Models trained on Wikipedia consistently have lower upstream bias and interestingly this does not increase or vary over training.\nLarger models make more robust downstream classifiers\nNext, we turn our attention to downstream biases of our pre-trained models. We attach a classification head to each of our"}, {"title": "Associations in pre-training data influence bias", "content": "Finally, we investigate the impact of pre-training data on the biases we observe. While downstream biases can be explained as an artifact of the fine-tuning data (Steed et al. 2022), we suspect a much tighter coupling between upstream biases and choice of pre-training data.\nTo understand our observed upstream gender biases (Figure la), we use weighted log odds with a Dirichlet prior (Monroe, Colaresi, and Quinn 2008) and compare (pronoun, occupation) pair occurrences between CC-100-EN and Wikipedia. Specifically (in terms of Equation 2), for each occupation $\\omicron \\in \\{\\text{journalist, physician, painter, ...}\\}$, and pronoun $p\\in \\{\\{\\text{he, him, his, himself}\\}, \\{\\text{she, her, hers, herself}\\}\\}$ we measure:\n$\\log \\frac{\\text{occ-100}_{o,p}}{\\text{Wiki}_{o,p}}$\nA positive value indicates a co-occurrence is more likely in CC-100-EN than in Wikipedia, while a negative value means it is more likely in Wikipedia. Also note that we count frequencies for a set of pronouns and not singular pronouns for more robust counting. Further, to normalize for variance, we z-normalize the log odds; using the one-sided critical value for p = 0.05, we only consider z > 1.645 to be a significant difference between both datasets. Table 2 shows the 10 occupations with the highest weighted log odds between CC-100-EN and Wikipedia.\nWhile we observe many differences that are not significant, for certain occupations, Wikipedia indeed encodes greater gender stereotypes, e.g., \"professor\u201d has significantly higher masculine pronoun associations, and \"model\u201d has higher feminine pronoun associations. Interestingly, \"teacher\u201d is the only occupation that has significant stereotypical associations in both datasets: masculine in Wikipedia, feminine in CC-100-EN. One trend, despite"}, {"title": "Limitations", "content": "Our approach has multiple limitations. First, while we limit our analysis to a single model family to reduce variance in architecture, it limits the ecological validity of our results. Our results therefore cannot be generalized to all modern LLM architectures, and instead provide a detailed look into BERT specifically. Second, compared to state-of-the-art compute intensive training procedures, our pre-training process is quite rudimentary. We limit the training process to only 8000 training steps as a heuristic to upper bound the amount of compute that each model uses; this is a simplification and does not lead to a model as powerful as those available through model hubs like HuggingFace. Third, while we make sure to evaluate bias holistically by examining both upstream and downstream differences, our bias metrics-such as log probability gaps and sentiment are not definitive ways of measuring bias. Bias is a complex, socio-technical, and sometimes ill-defined notion whose meaning can vary across domains and tasks. While we rely on metrics from prior work, our measures are prone to the same pitfalls and limitations in validity that most bias measurement work in NLP suffers from (Goldfarb-Tarrant et al. 2023)."}, {"title": "Concluding Discussion", "content": "Our study provides a detailed case study on the interplay of scale, pre-training data, and bias with a specific focus on BERT, a widely used LLM. We find evidence that larger models are able to encode more biases upstream. Importantly, we observe that larger models, combined with unmoderated data, can lead to worse results for the task of masked language modeling. However, larger models can also produce more robust downstream classifiers after fine-tuning.\nWhile MLMs like BERT do not represent the state-of-the-art in the rapidly developing landscape of LLM research, they remain extremely relevant for several applied natural language processing problems. Our investigation of bias is particularly relevant to practitioners who fine-tune embedding models for their tasks. In these applied use-cases, our results shed light on how scale and training data together can lead to different kinds of biases. We encourage practitioners to be aware of the biases their training datasets can introduce, and to actively measure these artifacts during the development process. On a more general level, our study highlights the role that training data can play in scaling, especially as it relates to biased model behavior. Our results also suggest that mixing in a moderated, high quality data source (e.g., Wikipedia) with larger datasets (e.g., CC-100, The Pile (Gao et al. 2020)) might be an approach to alleviate biases-we leave a full exploration of this direction to future work.\nOur analyses also underscore the limitations that exist in metrics used to measure bias, which is a nuanced socio-technical concept, whose meaning changes across tasks and domains. Negative sentiment and gaps in gender representation\u2014as used here\u2014are well-scoped ways of expressing bias that can be useful for different domains. Negative sentiment, for instance, could be a useful measure of bias for LLM use in chatbots or auto-complete tools; differences in gender likelihood could be useful for measuring bias in resum\u00e9 or search ranking, but they are not universal measures of linguistic bias. As seen in our results, depending on the choice of bias metric, a measurement of model behavior can look quite different. This aligns with prior work (Goldfarb-Tarrant et al. 2023; Blodgett et al. 2021) which shows that measuring bias or fairness can be a challenging undertaking, and it is easy to set up an incompatible metric. Our results highlight the need for identifying the correct bias metric for each domain, and judging both the data and the model by that metric."}, {"title": "Ethical Considerations", "content": "Our study attempts to measure a social issue with technical tools, and therefore it relies on some shortcut heuristics and simplifications that we attempt to make explicit here. In studying gender disparities, we rely on pronouns and only focus on he/him and she/her since our metrics are set up as subtractions. This simplification is not meant to reinforce the"}]}