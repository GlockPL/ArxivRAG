{"title": "Motion Forecasting for Autonomous Vehicles: A Survey", "authors": ["Jianxin Shi", "Jinhao Chen", "Yuandong Wang", "Li Sun", "Chunyang Liu", "Wei Xiong", "Tianyu Wo"], "abstract": "In recent years, the field of autonomous driving has attracted increasingly significant public interest. Accurately forecasting the future behavior of various traffic participants is essential for the decision-making of Autonomous Vehicles (AVs). In this paper, we focus on both scenario-based and perception-based motion forecasting for AVs. We propose a formal problem formulation for motion forecasting and summarize the main challenges confronting this area of research. We also detail representative datasets and evaluation metrics pertinent to this field. Furthermore, this study classifies recent research into two main categories: supervised learning and self-supervised learning, reflecting the evolving paradigms in both scenario-based and perception-based motion forecasting. In the context of supervised learning, we thoroughly examine and analyze each key element of the methodology. For self-supervised learning, we summarize commonly adopted techniques. The paper concludes and discusses potential research directions, aiming to propel progress in this vital area of AV technology.", "sections": [{"title": "1 Introduction", "content": "Motion Forecasting is vital in the functionality of autonomous driving systems. It assists these vehicles in planning their forthcoming actions and mitigates the risk of accidents. This survey addresses motion forecasting in autonomous vehicles, focusing on the two main approaches: Scenario-based Motion Forecasting and Perception-based Motion Forecasting.\nScenario-based Motion Forecasting predicts future states of traffic agents (TAs) by analyzing past states and relevant environmental context, such as high-definition maps (HDMaps) and the historical states of surrounding agents (SAs). This approach emphasizes structured, predefined inputs like agents' locations and HDMaps, intentionally excluding raw sensor data like RGB images, LiDAR point clouds, or semantic segmentation maps. By limiting input features to these structured elements, scenario-based forecasting models achieve a focused analysis of the traffic environment and agent interactions.\nPerception-based Motion Forecasting, on the other hand, directly utilizes raw perception data, including camera images, LiDAR point clouds, and other sensor outputs, to predict agents' future trajectories. This approach bypasses intermediate feature engineering steps, allowing the model to learn relevant representations directly from raw data. Perception-based methods aim to leverage richer environmental cues, making them suitable for scenarios where comprehensive scene understanding is crucial.\nMultiple approaches have been suggested to tackle the prediction problem, including physics-based models, rule-based models, and deep learning-based models. Among these, physics-based models offer a distinct approach by utilizing physical principles to predict vehicle trajectories over the short term. These models incorporate factors such as current position, acceleration, and turn rate to estimate future movements. To elaborate, Constant Velocity (CV) models operate under the assumption that a vehicle maintains its speed in the same direction without acceleration. Similarly, Constant Acceleration (CA) models predict movement based on unchanging acceleration in the current direction. For scenarios involving both constant speed and constant turn rate, Constant Turn Rate and Velocity (CTRV) models are applied. Meanwhile, Constant Turn Rate and Acceleration (CTRA) models anticipate that a vehicle will maintain both its acceleration and turn rate consistently. Implementing these physics-based approaches is relatively straightforward, requiring minimal computing resources. However, these models have limitations, notably their overlook of environmental factors and interactions with other agents. As a result, while they are efficient for short-term predictions in uncomplicated environments, their applicability is limited by the complexity of the environment and the presence of multiple dynamic agents, underlining the necessity for more sophisticated models in certain contexts. Rule-based models leverage known traffic rules and human prior knowledge, predicting the future trajectory of vehicles with a structured approach. Initially, models confirm the current state of target agents, a task typically handled by the tracking module that outputs to the prediction module. Following this, the current lane of the vehicle is determined"}, {"title": "2 Problem Formulation", "content": "This section introduces some important definitions and formally formulates the problem of motion forecasting.\n2.1 Definitions\n2.1.1 Traffic Participants\nTarget Agents (TAs). Target Agents represent the objects that are crucial for analysis and prediction in autonomous vehicle systems. Their future behavior or trajectory is of utmost importance for the safe and efficient operation of autonomous vehicles."}, {"title": "2.1.2 Input Representation", "content": "Motion forecasting can be divided based on different types of input data, each providing unique information that can enhance prediction accuracy. In this section, we review motion forecasting methods categorized by their input types: trajectory and HDMap data, Bird's Eye View (BEV) representation, and raw perception data. Each category leverages distinct characteristics of the environment and agent dynamics, offering varied advantages in predicting future trajectories.\nRaw Perception Data. Raw perception data serves as an essential component in motion forecasting tasks, providing unprocessed sensor information directly from the environment. In the context of autonomous driving, this data typically comes from sensors like LiDAR, radar, and cameras, offering a richer and more detailed view of the surroundings compared to structured data like trajectories or HD maps. The raw perception data includes point clouds, visual images, and radar reflections, which capture both the static and dynamic aspects of the scene.\nA point cloud, for instance, generated by LiDAR sensors, can be formalized as:\n $P = \\{(x_{p1}, y_{p1}, z_{p1}, r_{p1}),..., (x_{pM}, y_{pM}, z_{pM}, r_{pM})\\}$\nwhere (x, y, z) represents the 3D position of each point in space, and r is the reflectance value indicating the intensity of the return signal. The number of points, M, may vary based on the environment and the sensor's resolution."}, {"title": "2.1.3 Prediction Types", "content": "Marginal Trajectory Prediction. In the marginal prediction case, the trajectories of TAs do not affect each other, the prediction goal can be formalized as:\n$Output_MarginalTpred = p(s_1)p(s_2)...p(s_n)$,\nwhere p(si) represents the predicted trajectory distribution of agent i in the future, which is related to the historical states of agent i, the historical states of agent j, and the HDMap.\nJoint Multi-Agent Prediction. Unlike marginal trajectory prediction, joint multi-agent prediction is more complex and essential as it requires predicting the future trajectories of multiple vehicles while considering their mutual interactions and influences. This prediction method integrates the states and intentions of all vehicles, better reflecting the dynamic changes in real traffic environments."}, {"title": "2.2 Problem Formulation", "content": "Motion forecasting pipelines can generally be categorized into two main approaches based on the data flow and processing stages: Scenario-based Motion Forecasting and Perception-based Motion Forecasting. The pipelines for these two approaches are illustrated in Figure 5.\n2.2.1 Scenario-based Motion Forecasting\nThe primary objective of motion forecasting is to predict the future states of TAs, considering their past states as well as the surrounding traffic scenes. These traffic scenes encompass various elements, including HDMaps, past states of SAs, and other relevant factors. In contrast to survey focusing on vision-based prediction models in AVs, the scenario-based motion forecasting model restricts its input features to agents' location information and HDMaps. These models specifically exclude other types of data such as RGB images, Lidar point clouds, semantic segmentation maps,"}, {"title": "2.2.2 Perception-based Motion Forecasting", "content": "Perception-based motion forecasting refers to the approach of directly predicting the future trajectories of agents from raw perception data, such as camera images, LiDAR point clouds, and other raw sensor data. without the need for artificially designed intermediate features or steps.\nMathematically, the end-to-end motion forecasting process can be described as learning a function f, which maps the raw perception data Zt (e.g., LiDAR point clouds or camera images) to the future trajectories Xt+T of the agents:\n$f: Z_t \\rightarrow X_{t+T}$,\nwhere Zt = {ZLiDAR, Zcamera, Zradar, ...} represents the raw sensor data at time t, and Xt+T is the predicted trajectory of the target agent(s) over the future time horizon T. The predicted trajectory Xt+T can be further formalized as a sequence of future states over T time steps:\n$X_{t+T} = \\{X_{t+1}, X_{t+2}, ..., X_{t+T}\\}$,\nwhere each future state Xt+i typically includes information such as position, velocity, and heading of the agent at time t + i. The learning objective for the model is to minimize the prediction error between the ground truth future trajectory Xt+T and the predicted trajectory X't+T, using a loss function such as mean squared error (MSE):\n$L = \\sum_{i=1}^{T}||X_{t+i} - X'_{t+i}||^2$."}, {"title": "3 Challenges", "content": "In the domain of autonomous driving, motion forecasting for TAs is very supportive of the EA's next move. However, accurate motion forecasting for TAs remains a challenging task due to the complexity and flexible traffic environment.\nFusion of road information. To advance the autonomous driving field, researchers utilize more detailed features and achieve centimeter-level accuracy for vehicle behavior prediction by constructing HDMaps. These HDMaps provide rich contextual information, such as lane boundaries, traffic signs, and road geometry, which are crucial for making precise and reliable predictions. However, the absence of unified standards in HDMaps' data formats and content poses significant challenges. How to establish data alignment and association between HDMaps and agents' trajectories, and effectively integrate this information in vehicle behavior prediction is a huge challenge.\nDynamic interactions between different vehicles. The influence of road environments on vehicle behavior is static, whereas the interaction between SAs and TAs is dynamic and uncertain, posing significant challenges in capturing this complex interplay. For instance, a vehicle's decision to turn right at an intersection involves interacting with the static environment of the right-turn lane. However, the dynamic and variable nature of interactions between SAs and TAs adds complexity, making the analysis and interpretation of these interaction patterns significantly more challenging.\nMultimodality of vehicle behavior. In autonomous driving, understanding the behaviors of TAs and SAs is critical due to their inherent multimodality, meaning a single historical trajectory can lead to multiple potential future trajectories. The combination of an agent's trajectory with road information provides valuable insights into the driver's style, especially their familiarity with specific road sections. A deeper analysis of the agent's historical movement patterns enables the identification of various possible future behaviors. Consequently, an effective motion forecasting module in an autonomous driving system should be able to recognize all these potential future behaviors. This capability is essential to ensure the system operates reliably and safely.\nLack of interpretability. Many existing motion forecasting models adopt data-driven methodologies to learn trajectory distributions. While these approaches can achieve high levels of accuracy by leveraging large datasets, they often result in a lack of interpretability in the decision-making processes of traffic participants. This black-box nature makes it challenging to understand or explain why a model predicts certain behaviors, which is crucial for both safety and trust in autonomous systems. Moreover,"}, {"title": "4 Training and Evaluation", "content": "In this section, we introduce several open datasets utilized in motion forecasting for autonomous vehicles, as well as commonly used metrics.\n4.1 Datasets\nA summary of public motion forecasting datasets for AVs is shown in Table 1. Details are described below.\nArgoverse developed by Argo AI, offers a comprehensive collection of urban driving scenarios, extensively annotated for research in 3D tracking and forecasting. Building on the foundation of its predecessor, Argoverse 2 motion forecasting dataset expands upon the original dataset with a larger volume of data, including 250,000 scenarios, each scenario provides a local vector map and 11 seconds of trajectory data (recorded at 10 Hz). The first 5 seconds of data represent the observation window, while the following 6 seconds correspond to the forecast horizon. Waymo Open Motion Dataset, created by Waymo, encompasses a wide array of driving conditions and scenarios, making it one of the most comprehensive resources available. It comprises over 100,000 scenes, each 20 seconds long and sampled at 10 Hz. This amounts to more than 570 hours of unique data, covering over 1750 km of roadways. The Interaction dataset is distinct in its focus on interactive driving scenarios, particularly those involving complex urban intersections and roundabouts. The Interaction dataset includes a rich collection of real-world driving scenarios with a significant number of vehicle trajectories, totaling over 41,000 across all categories and covering more than 990 minutes of driving behavior. The nuScenes dataset is notable for its extensive coverage of varied driving environments, encompassing 1,000 scenes that span across various weather conditions and times of the day collected in Boston and Singapore. This rich collection of sensor data not only enhances the realism of the driving scenarios but also provides invaluable insights for the development and refinement of motion forecasting models, making nuScenes a pivotal tool for researchers in the field.\n4.2 Evaluation Metrics\nStandardized evaluation settings and commonly used metrics are essential for a data-driven approach to obtain quantitative results. The quantitative results allow different models to compare with each other from diverse perspectives. The metrics used frequently in motion forecasting can be summarized from the following three levels."}, {"title": "5 Model Architecture", "content": "5.1 Supervised Learning-based Architecture\nFigure 6 shows the general pipeline of Supervised Learning-based (SL-based) architecture employed in motion forecasting tasks, where the Encoder and Decoder can be any practicable network components, such as attention mechanism, GNNs, and transformers. As vehicle motions or trajectories are typically temporal-spatial data, we introduce SL-based architecture from temporal-spatial encoding and decoding, respectively.\n5.1.1 Temporal-Spatial Encoding\nDuring the encoding, both temporal and spatial features need to be extracted. Spatially, the agent-to-lane and agent-to-agent interactions could be modeled. Temporally, the dynamics of these interactions along with time are captured through designed components.\nRasterized-based Encoder. Rasterized-based approaches rasterize the map information and agent states of each timestep into an image. Then, a scenario can be modeled into a time series of images. Existing methods employ CNNs to learn effective representations from spatial and temporal perspectives For instance,\nOne of the early typical explorations in this area involved the FAF model , which converts LiDAR point cloud data into a top-down bird's-eye view. This data is then processed by a target detection network equipped with a CNN-based multi-frame information fusion module to extract spatio-temporal correlations from the perception sequence. To enhance the model's performance, a trajectory regression loss function was incorporated into the detection head, allowing for the end-to-end optimization of object localization and future trajectory prediction. FAF not only enables joint 3D object detection and trajectory prediction from LiDAR point cloud data, but also demonstrates that incorporating the prediction task improves the accuracy of object detection.\nBased on FaF, IntentNet enhances vehicle intent detection by generating additional outputs within a unified detection and prediction framework, where intent is defined as short-term motion states and lane-related actions."}, {"title": "5.1.3 Trajectory Decoding", "content": "With the representations containing both spatial-temporal features and interaction features between traffic agents, a decoder needs to be devised to generate multi-modal future trajectories. There are two decoding ways, anchor-conditioned and anchor-free.\nAnchor-Conditioned Decoding. The anchor-conditioned decoding approach typically incorporates prior knowledge from the dataset as an input component of the network, facilitating the generation of multi-modal trajectories, essentially conditional probabilities. Based on these various prior anchors, the final output trajectory can be constrained within a set. However, the effectiveness of this method largely depends on the quality and relevance of these predefined anchors. Depending on the type of anchor, this type of decoding approach further includes: goal-based decoder, heatmap-based decoder, and intention-based decoder.\nGoal-based Decoder. Recently, goal-based multi-trajectory prediction methods have proven to be effective. These methods operate on the principle that the endpoint carries most of the uncertainty of the trajectory, so they first predict the agent's target and then further complete the corresponding full trajectory for each target. The final target position is obtained by classifying and regression the predefined sparse anchor points. TNT defines an anchor point as a location sampled on the centerline of a lane segment. The offsets on the x and y axes are predicted based on the candidate anchor point, and the expected end point is obtained by combining the offset with the anchor. The trajectory is then finalized based on this endpoint.\nHeatmap-based Decoder. HOME introduces an approach that utilizes probabilistic heatmaps as the output format for trajectory prediction. This methodology employs a full convolutional model, but it is constrained by the limitations of a fixed image size. Building upon the foundations laid by HOME, GOHOME advances this concept by proposing a motion prediction framework that is predicated entirely on graph manipulation optimization."}, {"title": "5.2 Self-Supervised Learning-based Architecture", "content": "Self-Supervised Learning (SSL) is widely applied in natural language processing and computer vision, benefiting from the availability of large-scale unlabeled data. There are already studies proving that SSL is effective in helping models learn a more comprehensive representation for downstream tasks. Hence, SSL has begun to be explored in motion forecasting, aiming for more transferable and robust representation learning. Figure 7 illustrates the general pipeline of SSL-based Architecture in motion forecasting.\n5.2.1 Preliminary Exploration\nVectornet's introduction of a graph-based completion auxiliary task represents a pioneering exploration in motion forecasting for autonomous driving, utilizing a self-supervised learning approach. This innovation serves as a foundational step in applying self-supervision techniques in this field, potentially paving the way for future advancements. PreTraM considers that the limited availability of trajectory data restricts SSL's applicability in motion forecasting. To mitigate this, it introduces a method for generating supplementary rasterized map patches, derived from localized areas of comprehensive HDMaps, for the training of a robust map encoder through contrastive learning. Furthermore, PreTraM innovatively employs a pre-training strategy for both map and trajectory encoders, which involves the pairing of batches of training instances to effectively enhance the encoders' ability to learn the complex relationship between maps and trajectories."}, {"title": "5.2.3 MAE-based Approach", "content": "Following the significant progress in image-based self-supervised representation learning, Masked AutoEncoder (MAE) has attracted considerable interest across various fields. The core mechanism of this approach is to mask part of the input data, followed by the application of an autoencoder structure aimed at reconstructing the masked tokens, thereby enhancing learning efficiency.\nTraj-MAE first presents a novel and efficient masked trajectory autoencoder specifically for self-supervised trajectory prediction, designs two independent mask-reconstruction tasks on trajectories and road map input to train its trajectory and map encoder separately. The research further explores various masking strategies, including both social and temporal aspects, to facilitate the trajectory encoder in capturing latent semantic information from multiple perspectives. However, the methodology exhibits a significant limitation: the spatial relationship between agents and roads is insufficiently emphasized during the pretraining phase. Forecast-MAE devises a scene reconstruction task employing a novel masking strategy during the pre-training phase. This involves masking the historical trajectory of some agents, the future trajectory of others, and applying a random masking method for lanes. This approach enables the model to"}, {"title": "6 Conclusion and Prospect", "content": "In this paper, we present a comprehensive review of the recent advancements in motion forecasting for autonomous vehicles. We begin by introducing the formulation of motion forecasting and then move on to an overview of diverse, widely-utilized datasets. This is followed by a detailed explanation of evaluation metrics specifically designed for motion forecasting. State-of-the-art prediction models have made significant strides, employing advanced techniques such as attention mechanisms, GNNs, transformers, and self-supervised architectures. Despite these technological advances, the field still faces substantial challenges. Understanding motion forecasting is pivotal for autonomous driving, as it greatly enhances the interpretation of road scenarios, thereby playing a crucial role in improving the safety standards of future autonomous driving technologies.\nFusion of more prior information. Recent research has integrated HDMaps into motion forecasting models. This integration specifically involves incorporating lane information to ensure predicted trajectories are aligned with the road topology. In real-world scenarios, other factors also play a crucial role. These include traffic light status, various traffic signs, and additional elements that influence the movement and interactions of traffic participants. However, many current methodologies tend to overlook these aspects. This oversight results in limitations in the mechanistic understanding of motion forecasting models. As a consequence, the impact of traffic indication information on the behavior of traffic participants remains a relatively underexplored area.\nModel robustness under incomplete scenario. The majority of motion forecasting models operate under the presumption that all observational data is fully accessible. However, this assumption rarely holds true in real-world traffic scenarios, where various factors can limit the availability and quality of observational data. Sensor constraints, such as limited range or resolution, and environmental factors, such as weather conditions or low lighting, can impair the sensors' ability to capture all relevant information. Additionally, object occlusion, where vehicles, pedestrians, or other objects block the sensors' line of sight, can result in significant portions of the scene going unobserved. In these cases, the EA may only partially observe TAs and SAs, leading to gaps in the data that are critical for accurate motion forecasting. This incomplete observation can degrade the performance of conventional models, which often rely on the assumption of comprehensive data to make accurate predictions. Therefore, there is a pressing need to develop more advanced motion forecasting models that can function robustly even when faced with incomplete observation data.\nAlignment of evaluation metrics. Despite significant advancements by joint perception-to-forecasting models, these models have not been directly compared to established pure motion forecasting models. This lack of direct comparison is primarily due to inherent differences in their methodological approaches and evaluation criteria. To bridge this gap, it is crucial to develop an adapted evaluation protocol that considers the cascading impact of upstream errors in the perception-to-forecasting pipeline. Implementing such a protocol would enable a more balanced and informative comparison."}]}