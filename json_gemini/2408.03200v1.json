{"title": "Adversarial Safety-Critical Scenario Generation\nusing Naturalistic Human Driving Priors", "authors": ["Kunkun Hao", "Wen Cui", "Yonggang Luo", "Lecheng Xie", "Yuqiao Bai", "Jucheng Yang", "Songyang Yan", "Yuxi Pan", "Zijiang Yang"], "abstract": "Evaluating the decision-making system is indispens-\nable in developing autonomous vehicles, while realistic and\nchallenging safety-critical test scenarios play a crucial role.\nObtaining these scenarios is non-trivial, thanks to the long-tailed\ndistribution, sparsity, and rarity in real-world data sets. To tackle\nthis problem, in this paper, we introduce a natural adversarial\nscenario generation solution using naturalistic human driving\npriors and reinforcement learning techniques. By doing this, we\ncan obtain large-scale test scenarios that are both diverse and\nrealistic. Specifically, we build a simulation environment that\nmimics natural traffic interaction scenarios. Informed by this\nenvironment, we implement a two-stage procedure. The first stage\nincorporates conventional rule-based models, e.g., IDM (Intelli-\ngent Driver Model) and MOBIL (Minimizing Overall Braking\nInduced by Lane changes) model, to coarsely and discretely\ncapture and calibrate key control parameters from the real-\nworld dataset. Next, we leverage GAIL (Generative Adversarial\nImitation Learning) to represent driver behaviors continuously.\nThe derived GAIL can be further used to design a PPO (Proximal\nPolicy Optimization)-based actor-critic network framework to\nfine-tune the reward function, and then optimize our natural\nadversarial scenario generation solution. Extensive experiments\nhave been conducted in two popular datasets, NGSIM and\nINTERACTION. Essential traffic parameters were measured\nin comparison with the baseline model, e.g., the collision rate,\naccelerations, steering, and the number of lane changes. Our\nfindings demonstrate that the proposed model can generate\nrealistic safety-critical test scenarios covering both naturalness\nand adversariality with an advanced 44% efficiency gain over the\nbaseline model, which can be a cornerstone for the development\nof autonomous vehicles.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous driving that aims to significantly improve\ntraffic efficiency and reduce traffic accidents is empowered by\nrecent advances in Internet-of-Things, Artificial Intelligence,\nand V2X at an unprecedented pace [1]\u2013[9]. To truly realize\nautonomous driving in practice, however, still requires enor-\nmous work due to the complex nature of autonomous driving\nsystems and the real-world long-tail distributed scenarios [10]\u2013\n[15]. According to a recent survey from the National Highway\nTraffic Safety Administration (NHTSA) in the US [16], [17],\n522 accidents were reported related to Autonomous Vehi-\ncles (AVs). In particular, the pioneer companies, such as Tesla\nand Waymo, account for the majority of these accidents, i.e.,\n70% of the Level 2 system from Tesla and 50% of the Level\n3 to 5 systems from Waymo. Given these painful results, it\nbecomes commonly accepted that a large number of testings\nand validations have to be conducted for AVs, especially for\nextreme scenarios, before the real world uses [18]-[22].\nTo generate safety-critical scenarios without manual\nchanges, existing work designed their solution according\nto adversarial metrics solely [24]\u2013[30], e.g., collision rates,\nhazard distances, etc. We observe that naturalness plays an\nequally important role as adversariality metrics. In broad\nterms, naturalness and adversariality are already two critical\ncharacteristics of general generation solutions [31], [32], and\nwe highlight their unique importance in the context of safety-\ncritical scenario generation for autonomous vehicles [33].\nFormally, adversariality, refers to complex driving challenges\npresented to AVs. An adversary scenario, including sudden\nlane changes, unseen jaywalkers, or dangerous maneuvering in\nheavy traffic, pushes the limit of driving systems and is thus\ncrucial for the robustness verification of AVs. Furthermore,\nadversary scenarios are also rare in the real world, making\nscenario generation of this kind particularly of interest to\nAV designers. Naturalness, on the other hand, refers to how\na generated scenario reflects real-world conditions. Obeying\nvehicle dynamics [34], understanding traffic rules, and more\nimportantly, smoothly interacting with heterogeneous traffic\nparticipants are all defining features of the naturalness of a\ngenerated scenario. Therefore, naturalness determines whether\nor not scenarios are applicable in the validation of AVs. In\nsummary, both adversariality and naturalness are crucial for\nscenario generation solutions, in making scenarios not only\nchallenging to the tested AVs but also representative of real-\nworld driving. Consequentially, after ignoring the naturalness,\nexisting solutions may generate many counter-intuitive scenar-\nios, e.g., front-to-front collisions, rear-to-rear collisions, and\nT-shape collisions, as shown in Fig. 1, resulting in ineffective\ntests and misleading safety reports.\nIn contrast to existing work, in this paper, we present a\nnatural adversarial scenario generation solution that considers\nadversariality and naturalness simultaneously. In a nutshell,\nwe start by building a natural traffic simulation training\nenvironment, including calibrated parameters of driver models\nfrom real-world traffic datasets, i.e. NGSIM [35] and INTER-\nACTION [36]. In that environment, the dynamic interaction\nand highly uncertain driving behaviors can be represented in\ngreat detail. Based on that, we construct a human driving prior\nmodel by using generative adversarial imitation learning, and\nwe use this model as a natural adversarial reward function\nto measure the naturalness of actions, which eventually en-\nables the design of our reinforcement learning-based natural\nadversarial scenario generation solution. We have conducted\nextensive experiments using popular real-world datasets, and\nthe results reveal that our solution generates more realistic\nadversarial behaviors when compared with baseline models,\nby achieving both adversariality and naturalness.\nThe contributions of this work have been concluded as\nfollows.\n1) To the best of our knowledge, we are the first to use\nnaturalistic human driving priors and reinforcement learn-\ning to propose a large-scale and realistic safety-critical\nscenario generation solution. The proposed solution can\nbe a cornerstone in testing the decision-making systems\nof AVs.\n2) To pursue the realistic feature in generating adversarial\nscenarios, we introduce a novel reward function for the\nreinforcement learning agent, in which the adversariality\nand naturalness of adversarial behaviors are included\nsimultaneously.\n3) We demonstrate the performance of the proposed solution\nbased on a real-world dataset, including the statistical\nanalysis of the lane change rates and collision rates of\nvehicles, the distribution of decision-making actions, and\nthe visual clustering of collision types on 2000 generated\ntest scenarios.\nWe present the structure of this paper as follows. In Sec-\ntion II, we present a comprehensive review of existing work\nand highlight the corresponding comparisons. In Section III,\nwe elaborate on the design details including the modeling and\nalgorithms design and the network structure. The experiment\nresults are presented in Section IV. We conclude our work and\ndiscuss potential future directions in V."}, {"title": "II. RELATED WORKS", "content": "Adversarial scenario generation solutions are mainly fo-\ncused on the testing and validation of perception, decision-\nmaking, and planning of AVs, and in this section, we elaborate\nthese advances separately."}, {"title": "A. Adversarial Scenario Generation for Perception", "content": "Perception systems in AVs are empowered by the tech-\nnological leap in deep learning [37], [38]. Major applica-\ntions, such as object classification, object detection, and scene\nsegmentation, rely on different types of Deep Neural Net-\nworks (DNNs) [39]\u2013[43]. However, these DNNs have recently\nbeen proven vulnerable to adversarial attacks [44]\u2013[48]. In\nparticular, a small malicious modification of the perceived\nimages, e.g., slight value changes of the partial pixels, may\ncause DNN's output to be significantly different, leading to\nmisclassification or false detection.\nTo improve the robustness of perception systems, adver-\nsarial scenario generation solutions become indispensable.\nTwo classical attacks of the adversarial scenario have been\nstudied in this area, i.e., white-box attacks and black-box\nattacks, depending on whether the attacker knows the model\ndetails or not. For white-box attacks, Hendrik et al. [49]\ndesigned adversarial perturbations using gradient-dependent\noptimization, and then used them to attack against the street\nview image semantic segmentation. Other than that, robust\nphysical perturbations were introduced by Eykholt et al. for\nphysical-world objects [50]. By doing this, an example showed\nthat a stop sign classifier can be consistently misled by the\nphysical perturbations, even under different physical settings.\nFor black-box attacks, Xiong et al. [51] proposed multi-\nsource adversarial sample attack models for the attacking of\nimage and LiDAR perception systems together, including a\nparallel attack model and a fusion attack model to cover\ndifferent sources of the perceived information. Morgulis et\nal. [52] provided an adversarial traffic sign generator that\nfooled a range of production-grade traffic sign recognition\nsystems, and demonstrated the effectiveness of the generator\nthrough real-world experiments. Furthermore, Li et al. [53]\npresented a perturbation generator for traffic sign images with\nefficient sampling, putting great risks to AVs with a high-\nfrequency attacker. Overall, adversarial scenario generation for\nperception belongs to frame-level adversarial testing, making\nit inadequate for decision-making and planning that require\ncontinuously interact with dynamic, uncontrollable, and di-\nverse surrounding environments, i.e., scenario-level testing."}, {"title": "B. Adversarial Scenario Generation for Decision-Making and\nPlanning", "content": "Decision-making and planning are much more complex\nwhen compared with perception solely, given the dynami-\ncally varying neighbor vehicles, the environments, and the\nmovement of the ego vehicle itself. Henceforth, extra efforts\nin generating adversarial scenarios are needed, meanwhile,\ngreat attention has been drawn to this area to tackle the\nproblem [24], [28]\u2013[30], [54]\u2013[57]. Straightforwardly, adver-\nsarial scenarios can come from probabilistic models using\nreal-world traffic dataset [25]. Unlike DNNs, the probabilistic\nmodel may lose certain details of the data, making them\nscenario-depended and hard to be extended. Authors in [58]\nproposed to use the probabilistic joint distribution from dif-\nferent scenes to design adversarial reward functions, and\nthen feed it into reinforcement learning models for safety-\ncritical scenario generation. The follow-up work [55] intro-\nduced adaptive sampling strategies to improve the sampling\nefficiency of the multidimensional scene parameter, and then\nmultimodal safety-critical test scenarios can be covered. These\nmentioned solutions leveraged optimized initial parameters for\neach scene, and therefore can quickly generate straightforward\nscenarios but limited their ability to reflect real-world driving\nbehaviors after the initialization. Recent work attempted to\nuse trajectory priors to generate safety-critical scenarios for\npath planning [59], [60]. In doing this, it can get a clear\nview of the whole driving period generally, but the driver's\nbehavior has not been fully learned, weakening their ability\nto cover decision-making adversarial scenarios that require\nmore detailed driving information. Inspired by this work and\naiming for both decision-making and planning scenarios, we\ndevelop a learning model based on generative adversarial\nimitation learning. This learning model can closely mimic\ndrivers' behaviors and then can truly release the potential of\nhuman driving priors for scenario generation. The closest work\nto ours utilized detailed real-time traffic interactions [56], [61],\n[62], however, without concerning the naturalness, they turn all\ntheir attention to the adversariality of the generated scenarios.\nThe lack of naturalness may lead to many counter-intuitive\nscenarios as discussed in Introduction, while we concentrate\non adversariality and naturalness simultaneously."}, {"title": "III. SAFETY-CRITICAL SCENARIO GENERATION\nFRAMEWORK", "content": "Using a human driving dataset and reinforcement learning,\nwe construct the overall framework of our safety-critical\nscenario generation solution as shown in Fig. 2, including a\ntraffic simulation training environment, a human driving prior\nmodel, and a natural adversarial policy model.\nThe traffic simulation training environment includes a real-\nworld dataset and a simulation platform that underpins our\nmodels. Without loss of generality, we use two popular real-\nworld datasets, NGSIM [35] for highway scenarios and IN-\nTERACTION [36] for urban scenarios, and the Highway-env simulation platform [63] for their quality of data and\naccessibility. The data structure of the INTERACTION dataset\ndiffers from that of NGSIM. To eliminate the difference, we\nsupplement the missing items in the INTERACTION dataset\nthrough straightforward calculations, such as determining the\nID of the lane and the front car. Furthermore, it is safe to\nassume the AVs in the traffic are capable of perceiving their\nsurrounding environments with certain accuracy [64], and we\nsee AVs as black boxes.\nBased on the above-mentioned environment, we then build\nour human driving prior model and natural adversarial policy\nmodel. To put it simply, we first use the traditional rule-based\nmodels, i.e., the Intelligent Driver Model (IDM) [65] and the\nMOBIL [66] model, to coarsely capture and calibrate the key\nparameters of longitudinal (related to IDM) and lateral (related\nto MOBIL) controls from the real-world dataset. The two\nmentioned models lay the foundation of a driving behavior\nmodel, but they portray behaviors in a rigid and discrete\nfashion, resulting in a less realistic depiction of actual driving\nbehavior compared with learning-based solutions. Henceforth,\ninspired by recent advances in solving AV-related problems\nusing reinforcement learning, such as single-agent decision-\nmaking [67], multi-agent interactions [68], and robot con-\ntrols [69], we build our human driving prior model by using\nGenerative Adversarial Imitation Learning (GAIL) in which\ncontinuous acceleration and steering actions can be fully\nrepresented. Moreover, this model enables the naturalness\nsupervision of our natural adversarial policy model, and by\nfurther using the Proximal Policy Optimization (PPO) [70]\nin the actor-critic network framework [71], we complete our\nnatural adversarial policy model. The details of our design\nhave been provided in the following subsections."}, {"title": "A. The Usage of IDM and MOBIL Models", "content": "1) Preliminaries of IDM: The IDM is a parametric car-\nfollowing model, i.e., longitudinal, in which acceleration is\ncalculated as\n$U_{t+1} = IDM[1-(\\frac{U_t}{V^{IDM}})^{\\delta^{IDM}} - (\\frac{S^{IDM}(U_t, \\Delta U_t)}{S_t})^2]$\nwhere $@^{IDM}$ is the maximum acceleration of the vehicle,\n$U_t$ and $V^{IDM}$ the current and desired speed of the vehicle,\nrespectively, $\\delta^{IDM}$ the acceleration exponent, $\\Delta U_t$ the speed\ndifference between the vehicle and its front vehicle, $S_t$ and\n$S^{IDM}$ the relative distance and the desired following distance\nbetween the vehicle and the front vehicle, respectively. We\ncan get $S^{IDM}$ from\n$S^{IDM}(U_t, \\Delta U_t) = S_0 + U_tT + \\frac{U_t \\Delta U_t}{2a^{IDM}b^{IDM}}$\nwhere $S_0$ is the minimum vehicle distance, $T$ the safe headway\ntime distance, and $b^{IDM}$ the comfortable deceleration value.\n2) Data Preprocessing of IDM: The IDM model starts\nwith extracting car-following scenarios from the real-world\ntrajectories. In alignment with exiting work [72]-[74], we\nfirst extract car-following scenarios by choosing different lead\nvehicles, and next, we conduct a screening for each scenario\nto filter out the irrelevant and erroneous scenarios. Finally, we\ncorrect and then smooth the data before the use of parameter\ncalibration. Detailed procedures of this process are provided\nin Algo. 1, and the results are presented in Sec. IV."}, {"title": "3) Parameter Calibration of IDM:", "content": "According to Eq. 1 and\nEq. 2, the acceleration process makes the parameter calibration\nof IDM a non-linear optimization problem. Promising tools\ncan readily be adopted in solving this problem, e.g., a genetic\noptimizer in the MATLAB optimization solver [74], [76]\u2013[79],\nwhile the objective function plays a key role. This objective\nfunction is dedicated to minimizing the difference between\nthe model-generated data and the real-world dataset, and we\nuse the relative distance [79] to represent that difference.\nAdditionally, to reduce the bias of error measures, we leverage\na mixed error measure [72], [74] as the objective function. In\nparticular, Eq. 3 describes the objective function $F[d_{sim}]$ as\nfollows.\n$F[d_{sim}] = \\frac{1}{V<d_{Data}>} \\sum \\frac{(d_{Data} - d_{sim})^2}{d_{Data}}$\nwhere $d_{sim}$ denotes the simulated data, i.e., the relative\ndistance generated from IDM, $d_{Data}$ comes from the dataset,\nand $<>$ refers to the averaging of a time series data.\n4) Preliminaries of MOBIL: The MOBIL model determines\nwhether a vehicle should change lanes or not, i.e., lateral. It\nrelies on two factors: one is the increase in acceleration that a\nvehicle would experience after a lane change, and the other is\nthe braking variations affected by the lane change [80]. In\nparticular, we can use two specific criteria to describe the\nrules of MOBIL, i.e., the safety criterion and the incentive\ncriterion. The safety criterion, as shown in Eq. 4, defines that\nthe deceleration of the new follower $\\tilde{a}_n$ after a lane change\nshould not be bigger than MAX_BRAKING_IMPOSED.\n$\\tilde{a}_n > -MAX\\_BRAKING\\_IMPOSED$\nThe incentive criterion, as shown in Eq. 5, defines the\nacceleration gain from this lane change should not be smaller\nthan $A_{ath}$, where $\\rho$ is a politeness factor ranged from 0 to 1.\n$\\tilde{a}_c - a_c + \\rho(\\tilde{a}_n - a_n + \\tilde{a}_o - a_o) > \\Delta_{ath}$\ndriver \nnew follower \nold follower\nIn Eq. 5, $\\tilde{a}_c$, $a_c$, $\\tilde{a}_n$, $a_n$, $\\tilde{a}_o$, and $a_o$ represent specific\naccelerations related to the driver, new follower, and old\nfollower, while $\\Delta_{ath}$ is a threshold value."}, {"title": "5) Data Preprocessing of MOBIL:", "content": "The data preprocessing\nis similar to IDM in Algo. 1, except that MOBIL retains all\nlane-change scenarios.\n6) Parameter Calibration of MOBIL: In contrast\nto the non-linear optimization problem in IDM, it is\nstraightforward that the parameters of MOBIL-specifically,\nMAX_BRAKING_IMPOSED and $A_{ath}$ can be derived\nusing statistical analysis of the trajectory dataset. Note\nthat different datasets may contain bias on the parameters,\nand therefore, the parameters of MOBIL are often defined\nempirically [81]\u2013[83]. We illustrate the results and analysis\nin Sec. IV."}, {"title": "B. Problem Formulation for Reinforcement Learning", "content": "To begin with, we introduce a solution for completing\na reinforcement learning task generally. The reinforcement\nlearning task is usually described by the Markov Decision\nProcess (MDP) which assumes the next state is only related to\nits current state and action. Mathematically, we can represent\nthe process as $M = (S, A, P(S_{t+1}|S_t, a_t), r, \\gamma)$, where $S$\ndenotes the state space observed by the reinforcement learning\nmodel from its input, $A$ the action space to the output of the\nmodel, representing the behavioral decision of AVs in our case.\n$P(S_{t+1}|S_t, a_t)$ is the state transfer probability distribution,\ndescribing the dynamics of environments and representing the\nprobability of turning to the next state $s_{t+1}$ in taking an\naction $a_t$ by an agent at state $s_t$, $r$ the reward function of\nthe immediate reward to an agent for switching from state $s_t$\nto state $s_{t+1}$, and $\\gamma \\in (0,1]$ the discount factor.\nThe goal of a reinforcement learning task is to output a\npolicy that maximizes the cumulative long-term discounted\nreward, i.e.,\n$\\underset{\\pi}{max} E_{\\tau ~ P_{\\pi}(\\tau)}[\\sum_{t=0}^{T} r(S_t, a_t)]$\nwhere $\\tau$ represents the driving trajectory of the agent, $P_{\\pi}(\\tau)$\nthe trajectory distribution based on the policy $\\pi$, and $T$ the\nstep length of the environment.\nIn solving the above objective function, we can estimate\nthe state or action-value function, which is defined recursively\nby the state and action-value functions based on the Bellman\nOptimal formula, i.e.,\n$V^{\\pi}(S_t) = E_{a_t \\sim \\pi(S_t)}[Q^{\\pi}(S_t, a_t)]$\n$Q^{\\pi}(S_t, a_t) = r(s_t, a_t) + \\gamma E_{S_{t+1} ~ T(S_t, a_t)}[V^{\\pi}(S_{t+1})].$\nFor Equations (7) and (8), once the function $V^{\\pi*}(s)$ that\nmaximizes the state value of all states $s \\in S$ is found, the\ncorresponding policy $\\pi*$ would be the target solution for the\nreinforcement learning task."}, {"title": "C. The Proximal Policy Optimization Algorithm", "content": "PPO algorithms are deep reinforcement learning algorithms\nbased on the actor-critic framework. They represent online\npolicies and are known as model-free, aiming for the gen-\neration of both discrete and continuous controls. By using\nPPO, problems in continuous action space, specifically those\nrelated to autonomous driving decision tasks, can be well-\naddressed. These algorithms consist of two main components,\nan actor network that relies on the policy gradient, and a\ncritic network that relies on the value evaluation. In this\nstructure, the actor network generates continuous actions in\nresponse to the observed information from environments,\nwhile the critic network evaluates the output action of the\nactor network, influencing the subsequent action selection of\nthe actor. PPO algorithms outperform traditional general policy\ngradient algorithms by achieving stable convergence speeds.\nIn a general policy gradient algorithm, if the update step size\nis too large, the algorithm may easily diverge; conversely, if\nthe update step size is too small, it may lead to convergence,\nbut at an unacceptable speed. The procedures of PPO algo-\nrithms include running a traffic simulator through the traffic\nsimulation environment to form a trajectories database. By\nobtaining different trajectories, i.e., $(s_t, a_t, r_{t+1}, S_{t+1})$, small\nbatch sampling of the trajectory database can be performed to\noptimize the training process of both the actor network and\ncritic network. We provide the training update details in Fig. 3.\nThe critic network aims to reduce the time difference error\nor the prediction error, and its optimization target is\n$L^{V}(\\phi) = \\hat{E}[A_t]$\nwhere $\\phi$ is the critic-network parameter, $\\hat{E}[.]$ the estimated\nempirical mean of small batch data, and $A_t$ the advantage\nfunction for estimating the error of time differences. General-\nized Advantage Estimation (GAE) is often used to calculate\n$A_t$ at time step t, i.e.,\n$A_t = \\delta_t + (\\gamma \\lambda)\\delta_{t+1} + (\\gamma \\lambda)^2 \\delta_{t+2} \\dots + (\\gamma \\lambda)^{B-t-1} \\delta_{B-1}$"}, {"title": "D. Human Driving Prior Modeling", "content": "Up to here, we can use PPO algorithms to generate contin-\nuous controls of AVs, but the naturalness of driving behaviors\ncannot be fully guaranteed. To solve this and have a truly\nhuman driving prior, we further introduce GAIL [84] to\nsupervise the training process. It is tempting to use traditional\nimitation learning solutions [85] directly by having behavioral\ncloning through training on expert experience data. However,\nwe notice that the traditional solution may lead to overfitting\nproblems, making the model hard to be generalized, and when\nreinforcement learning tasks that require temporal decision-\nmaking are involved, compound errors become unavoidable.\nPut it simply, behavioral cloning can mimic the training dataset\nwell while it is vulnerable to variations of unseen situations.\nFurthermore, behavioral cloning generates each state-action\npair independently without considering the relations between\npairs, and therefore a small error in early steps can lead to un-\nbearable errors down the line. We also notice that Inverse Re-\ninforcement Learning (IRL) comes after the imitation learning\nsolution with better performance [86], but the computational\ncost is often high and the learning efficiency is relatively lower.\nGAIL, on the other hand, builds upon Generative Adversarial\nNetwork (GAN) [87] and IRL, which learns policy directly\nfrom expert trajectories, and more importantly, it significantly\nimproves learning efficiency and is easy to be generalized.\nSimilar to GAN, GAIL consists of a generator $G_{\\phi}$ and a\ndiscriminator $D_{\\epsilon}$. The generator models the human expert\ndriving behavior and outputs the action $a_t$ based on the input\nstate $s_t$. The discriminator accepts both the generator state-\naction pair and the expert trajectory state-action pair, i.e.,\n$(S_{\\phi,t}, a_{\\phi,t})$, $(S_{E,t}, a_{E,t})$ respectively, as the input, and outputs a\nreal number from 0 to 1 to discriminate whether the input state-\naction pair is from the generator or the human expert. By doing\nthis, GAIL can use these two networks to correctly generate\npairs, making it robust to the overfitting problem, i.e., robust\nto unseen situations. Moreover, in GAIL, the discriminator\ncompares the entire trajectory generated by the policy against\nthe expert trajectory, instead of the individual state-action pair,\nmaking it immune from compound errors.\nWhen we train GAIL, the goal of the discriminator $D_{\\epsilon}$ is\nto maximize the classification accuracy between the generator\npolicy and the expert policy, which can be expressed as\n$L(\\varphi, \\xi) = E_{a_{\\varphi,t} \\sim G(S_{\\varphi,t})}[log D_{\\epsilon}(S_{\\varphi,t}, a_{\\varphi,t})]+E_{a_{E,t} \\sim \\pi_E(S_{E,t})}[log(1 - D_{\\epsilon}(S_{E,t}, A_{E,t})]$\nwhere $\\varphi$ is the parameter of generator network, and $\\xi$ the\nparameter of discriminator network.\nMeanwhile, the goal of the generator is to make the gen-\nerated trajectories misclassified as expert trajectories by the\ndiscriminator, so the output of the discriminator $D_{\\epsilon}$ can be\nused as a reward function to train the generator policy. With\nthat in mind, we use the PPO algorithm as the generator of\nthe GAIL algorithm and design the reward function as\nr(S_{\\varphi,t}, a_{\\varphi,t}; \\xi) = -log(D_{\\epsilon}(S_{\\varphi,t}, a_{\\varphi,t}))$\nAfter alternately training the generator and the discriminator\nin an adversary manner, the data distribution between the gen-\nerated one and the real expert one becomes indistinguishable,\nleading to the needed human Driving priors. We present the\ndetailed network structure in the following subsection. It is\nimportant to understand that the previously introduced PPO\nalgorithm is for generating trajectories from a predefined state-\naction space, and its critic network is for penalizing the reward\nfunction if the trajectory deviates from the space. On the other\nhand, the discriminator network in the context of GAIL is\nresponsible for penalizing trajectories that deviate from the\nexpert's trajectories, i.e., from the real-world dataset, thus\npreserving the naturalness of generated trajectories."}, {"title": "E. PPO and GAIL Network Structure Design", "content": "Recall that overfitting and training efficiency are the main\nconcerns for traditional networks. In solving this, given the\nlow dimensionality in the agent state space, we design a\nlightweight network structure by using one fully connected\nlayer and a ReLU activation function. We present the network\nstructure in Fig. 4 in which PPO builds upon an actor network\nand a critic network, and GAIL consists of the PPO network\nas the generator and a discriminator.\nFor the actor network, the input state $s_t$ first passes through\nan input layer with a number of neurons equal to the dimension\nof $s_t$, and then passes through two hidden layers with 128\nneurons, and the results will be further decomposed into two\nhead networks to generate the mean $\\mu$ and variance $\\sigma^2$ of\na Gaussian distribution, and the output layer samples the\nGaussian distribution and outputs the action $a_t$. For the critic\nnetwork, with the same input, the input layer, and the hidden\nlayer as the actor network, its output layer outputs an estimated\n$V_{\\phi}(s_t)$ to the current state $s_t$. Finally, sharing the same\nnetwork with the critic network, the discriminator network\nuses the state-action pair as the input, i.e., the concatenation\nof $s_t$ and $a_t$, and the classification probability, $D_{\\epsilon}(s_t, a_t)$, of\nthe state-action pair, as the output."}, {"title": "F. Natural Adversarial Policy Modeling and the Design of its\nReward Functions", "content": "We construct the natural adversarial policy model according\nto the PPO algorithm and the trained GAIL model as discussed\nin the previous subsections. To guarantee both the adversari-\nality and naturalness", "Reward": "We use the adversarial reward\nfor a trained agent to generate disturbing actions to the AV\nunder test"}, {"Reward": "To guarantee naturalness, the pre-\ntrained human driving prior model GAIL is in use to supervise\nthe naturalness of the agent's behavior. We notice that both the\naction output of the agent and the GAIL prior model follow\nmultivariate Gaussian distributions, and therefore, we decide\nto use Kullback-Leibler (KL) divergence [88"}]}