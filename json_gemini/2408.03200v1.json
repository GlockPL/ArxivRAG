{"title": "Adversarial Safety-Critical Scenario Generation using Naturalistic Human Driving Priors", "authors": ["Kunkun Hao", "Wen Cui", "Yonggang Luo", "Lecheng Xie", "Yuqiao Bai", "Jucheng Yang", "Songyang Yan", "Yuxi Pan", "Zijiang Yang"], "abstract": "Evaluating the decision-making system is indispensable in developing autonomous vehicles, while realistic and challenging safety-critical test scenarios play a crucial role. Obtaining these scenarios is non-trivial, thanks to the long-tailed distribution, sparsity, and rarity in real-world data sets. To tackle this problem, in this paper, we introduce a natural adversarial scenario generation solution using naturalistic human driving priors and reinforcement learning techniques. By doing this, we can obtain large-scale test scenarios that are both diverse and realistic. Specifically, we build a simulation environment that mimics natural traffic interaction scenarios. Informed by this environment, we implement a two-stage procedure. The first stage incorporates conventional rule-based models, e.g., IDM (Intelligent Driver Model) and MOBIL (Minimizing Overall Braking Induced by Lane changes) model, to coarsely and discretely capture and calibrate key control parameters from the real-world dataset. Next, we leverage GAIL (Generative Adversarial Imitation Learning) to represent driver behaviors continuously. The derived GAIL can be further used to design a PPO (Proximal Policy Optimization)-based actor-critic network framework to fine-tune the reward function, and then optimize our natural adversarial scenario generation solution. Extensive experiments have been conducted in two popular datasets, NGSIM and INTERACTION. Essential traffic parameters were measured in comparison with the baseline model, e.g., the collision rate, accelerations, steering, and the number of lane changes. Our findings demonstrate that the proposed model can generate realistic safety-critical test scenarios covering both naturalness and adversariality with an advanced 44% efficiency gain over the baseline model, which can be a cornerstone for the development of autonomous vehicles.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous driving that aims to significantly improve traffic efficiency and reduce traffic accidents is empowered by recent advances in Internet-of-Things, Artificial Intelligence, and V2X at an unprecedented pace [1]\u2013[9]. To truly realize autonomous driving in practice, however, still requires enormous work due to the complex nature of autonomous driving systems and the real-world long-tail distributed scenarios [10]\u2013[15]. According to a recent survey from the National Highway Traffic Safety Administration (NHTSA) in the US [16], [17], 522 accidents were reported related to Autonomous Vehicles (AVs). In particular, the pioneer companies, such as Tesla and Waymo, account for the majority of these accidents, i.e., 70% of the Level 2 system from Tesla and 50% of the Level 3 to 5 systems from Waymo. Given these painful results, it becomes commonly accepted that a large number of testings and validations have to be conducted for AVs, especially for extreme scenarios, before the real world uses [18]-[22].\nTo generate safety-critical scenarios without manual changes, existing work designed their solution according to adversarial metrics solely [24]\u2013[30], e.g., collision rates, hazard distances, etc. We observe that naturalness plays an equally important role as adversariality metrics. In broad terms, naturalness and adversariality are already two critical characteristics of general generation solutions [31], [32], and we highlight their unique importance in the context of safety-critical scenario generation for autonomous vehicles [33]. Formally, adversariality, refers to complex driving challenges presented to AVs. An adversary scenario, including sudden lane changes, unseen jaywalkers, or dangerous maneuvering in heavy traffic, pushes the limit of driving systems and is thus crucial for the robustness verification of AVs. Furthermore, adversary scenarios are also rare in the real world, making scenario generation of this kind particularly of interest to AV designers. Naturalness, on the other hand, refers to how a generated scenario reflects real-world conditions. Obeying vehicle dynamics [34], understanding traffic rules, and more importantly, smoothly interacting with heterogeneous traffic participants are all defining features of the naturalness of a generated scenario. Therefore, naturalness determines whether or not scenarios are applicable in the validation of AVs. In summary, both adversariality and naturalness are crucial for scenario generation solutions, in making scenarios not only challenging to the tested AVs but also representative of real-world driving. Consequentially, after ignoring the naturalness, existing solutions may generate many counter-intuitive scenarios, e.g., front-to-front collisions, rear-to-rear collisions, and T-shape collisions, as shown in Fig. 1, resulting in ineffective tests and misleading safety reports.\nIn contrast to existing work, in this paper, we present a natural adversarial scenario generation solution that considers adversariality and naturalness simultaneously. In a nutshell, we start by building a natural traffic simulation training environment, including calibrated parameters of driver models from real-world traffic datasets, i.e. NGSIM [35] and INTERACTION [36]. In that environment, the dynamic interaction and highly uncertain driving behaviors can be represented in great detail. Based on that, we construct a human driving prior model by using generative adversarial imitation learning, and we use this model as a natural adversarial reward function to measure the naturalness of actions, which eventually enables the design of our reinforcement learning-based natural adversarial scenario generation solution. We have conducted extensive experiments using popular real-world datasets, and the results reveal that our solution generates more realistic adversarial behaviors when compared with baseline models, by achieving both adversariality and naturalness.\nThe contributions of this work have been concluded as follows.\n1) To the best of our knowledge, we are the first to use naturalistic human driving priors and reinforcement learning to propose a large-scale and realistic safety-critical scenario generation solution. The proposed solution can be a cornerstone in testing the decision-making systems of AVs.\n2) To pursue the realistic feature in generating adversarial scenarios, we introduce a novel reward function for the reinforcement learning agent, in which the adversariality and naturalness of adversarial behaviors are included simultaneously.\n3) We demonstrate the performance of the proposed solution based on a real-world dataset, including the statistical analysis of the lane change rates and collision rates of vehicles, the distribution of decision-making actions, and the visual clustering of collision types on 2000 generated test scenarios.\nWe present the structure of this paper as follows. In Section II, we present a comprehensive review of existing work and highlight the corresponding comparisons. In Section III, we elaborate on the design details including the modeling and algorithms design and the network structure. The experiment results are presented in Section IV. We conclude our work and discuss potential future directions in V."}, {"title": "II. RELATED WORKS", "content": "Adversarial scenario generation solutions are mainly focused on the testing and validation of perception, decision-making, and planning of AVs, and in this section, we elaborate on these advances separately."}, {"title": "A. Adversarial Scenario Generation for Perception", "content": "Perception systems in AVs are empowered by the technological leap in deep learning [37], [38]. Major applications, such as object classification, object detection, and scene segmentation, rely on different types of Deep Neural Networks (DNNs) [39]\u2013[43]. However, these DNNs have recently been proven vulnerable to adversarial attacks [44]\u2013[48]. In particular, a small malicious modification of the perceived images, e.g., slight value changes of the partial pixels, may cause DNN\u2019s output to be significantly different, leading to misclassification or false detection.\nTo improve the robustness of perception systems, adversarial scenario generation solutions become indispensable. Two classical attacks of the adversarial scenario have been studied in this area, i.e., white-box attacks and black-box attacks, depending on whether the attacker knows the model details or not. For white-box attacks, Hendrik et al. [49] designed adversarial perturbations using gradient-dependent optimization, and then used them to attack against the street view image semantic segmentation. Other than that, robust physical perturbations were introduced by Eykholt et al. for physical-world objects [50]. By doing this, an example showed that a stop sign classifier can be consistently misled by the physical perturbations, even under different physical settings. For black-box attacks, Xiong et al. [51] proposed multi-source adversarial sample attack models for the attacking of image and LiDAR perception systems together, including a parallel attack model and a fusion attack model to cover different sources of the perceived information. Morgulis et al. [52] provided an adversarial traffic sign generator that fooled a range of production-grade traffic sign recognition systems, and demonstrated the effectiveness of the generator through real-world experiments. Furthermore, Li et al. [53] presented a perturbation generator for traffic sign images with efficient sampling, putting great risks to AVs with a high-frequency attacker. Overall, adversarial scenario generation for perception belongs to frame-level adversarial testing, making it inadequate for decision-making and planning that require continuously interact with dynamic, uncontrollable, and diverse surrounding environments, i.e., scenario-level testing."}, {"title": "B. Adversarial Scenario Generation for Decision-Making and Planning", "content": "Decision-making and planning are much more complex when compared with perception solely, given the dynamically varying neighbor vehicles, the environments, and the movement of the ego vehicle itself. Henceforth, extra efforts in generating adversarial scenarios are needed, meanwhile, great attention has been drawn to this area to tackle the problem [24], [28]\u2013[30], [54]\u2013[57]. Straightforwardly, adversarial scenarios can come from probabilistic models using real-world traffic dataset [25]. Unlike DNNs, the probabilistic model may lose certain details of the data, making them scenario-depended and hard to be extended. Authors in [58] proposed to use the probabilistic joint distribution from different scenes to design adversarial reward functions, and then feed it into reinforcement learning models for safety-critical scenario generation. The follow-up work [55] introduced adaptive sampling strategies to improve the sampling efficiency of the multidimensional scene parameter, and then multimodal safety-critical test scenarios can be covered. These mentioned solutions leveraged optimized initial parameters for each scene, and therefore can quickly generate straightforward scenarios but limited their ability to reflect real-world driving behaviors after the initialization. Recent work attempted to use trajectory priors to generate safety-critical scenarios for path planning [59], [60]. In doing this, it can get a clear view of the whole driving period generally, but the driver\u2019s behavior has not been fully learned, weakening their ability to cover decision-making adversarial scenarios that require more detailed driving information. Inspired by this work and aiming for both decision-making and planning scenarios, we develop a learning model based on generative adversarial imitation learning. This learning model can closely mimic drivers\u2019 behaviors and then can truly release the potential of human driving priors for scenario generation. The closest work to ours utilized detailed real-time traffic interactions [56], [61], [62], however, without concerning the naturalness, they turn all their attention to the adversariality of the generated scenarios. The lack of naturalness may lead to many counter-intuitive scenarios as discussed in Introduction, while we concentrate on adversariality and naturalness simultaneously."}, {"title": "III. SAFETY-CRITICAL SCENARIO GENERATION FRAMEWORK", "content": "Using a human driving dataset and reinforcement learning, we construct the overall framework of our safety-critical scenario generation solution as shown in Fig. 2, including a traffic simulation training environment, a human driving prior model, and a natural adversarial policy model.\nThe traffic simulation training environment includes a real-world dataset and a simulation platform that underpins our models. Without loss of generality, we use two popular real-world datasets, NGSIM [35] for highway scenarios and INTERACTION [36] for urban scenarios, and the Highway-env simulation platform [63] for their quality of data and accessibility. The data structure of the INTERACTION dataset differs from that of NGSIM. To eliminate the difference, we supplement the missing items in the INTERACTION dataset through straightforward calculations, such as determining the ID of the lane and the front car. Furthermore, it is safe to assume the AVs in the traffic are capable of perceiving their surrounding environments with certain accuracy [64], and we see AVs as black boxes.\nBased on the above-mentioned environment, we then build our human driving prior model and natural adversarial policy model. To put it simply, we first use the traditional rule-based models, i.e., the Intelligent Driver Model (IDM) [65] and the MOBIL [66] model, to coarsely capture and calibrate the key parameters of longitudinal (related to IDM) and lateral (related to MOBIL) controls from the real-world dataset. The two mentioned models lay the foundation of a driving behavior model, but they portray behaviors in a rigid and discrete fashion, resulting in a less realistic depiction of actual driving behavior compared with learning-based solutions. Henceforth, inspired by recent advances in solving AV-related problems using reinforcement learning, such as single-agent decision-making [67], multi-agent interactions [68], and robot controls [69], we build our human driving prior model by using Generative Adversarial Imitation Learning (GAIL) in which continuous acceleration and steering actions can be fully represented. Moreover, this model enables the naturalness supervision of our natural adversarial policy model, and by further using the Proximal Policy Optimization (PPO) [70] in the actor-critic network framework [71], we complete our natural adversarial policy model. The details of our design have been provided in the following subsections."}, {"title": "A. The Usage of IDM and MOBIL Models", "content": "1) Preliminaries of IDM: The IDM is a parametric car-following model, i.e., longitudinal, in which acceleration is calculated as\n$Ut+1_{IDM} = a_{IDM} \\left[1-\\left(\\frac{Ut}{V_{IDM}}\\right)^{\\delta_{IDM}} - \\left(\\frac{S_{IDM}}{S_t}\\right)^2\\right]$ (1)\nwhere $a_{IDM}$ is the maximum acceleration of the vehicle, $U_t$ and $V_{IDM}$ the current and desired speed of the vehicle, respectively, $\\delta_{IDM}$ the acceleration exponent, $\\Delta v_t$ the speed difference between the vehicle and its front vehicle, $S_t$ and $S_{IDM}$ the relative distance and the desired following distance between the vehicle and the front vehicle, respectively. We can get $S_{IDM}$ from\n$S_{IDM} (U_t, \\Delta v_t) = s_0 + U_tT + \\frac{U_t \\Delta v_t}{2 \\sqrt{a_{IDM} b_{IDM}}}$ (2)\nwhere $s_0$ is the minimum vehicle distance, T the safe headway time distance, and $b_{IDM}$ the comfortable deceleration value.\n2) Data Preprocessing of IDM: The IDM model starts with extracting car-following scenarios from the real-world trajectories. In alignment with exiting work [72]\u2013[74], we first extract car-following scenarios by choosing different lead vehicles, and next, we conduct a screening for each scenario to filter out the irrelevant and erroneous scenarios. Finally, we correct and then smooth the data before the use of parameter calibration. Detailed procedures of this process are provided in Algo. 1, and the results are presented in Sec. IV."}, {"title": "B. Problem Formulation for Reinforcement Learning", "content": "To begin with, we introduce a solution for completing a reinforcement learning task generally. The reinforcement learning task is usually described by the Markov Decision Process (MDP) which assumes the next state is only related to its current state and action. Mathematically, we can represent the process as $M = (S, A, P(S_{t+1}|S_t, a_t), r, \\gamma)$, where $S$ denotes the state space observed by the reinforcement learning model from its input, $A$ the action space to the output of the model, representing the behavioral decision of AVs in our case. $P(S_{t+1}|S_t, a_t)$ is the state transfer probability distribution, describing the dynamics of environments and representing the probability of turning to the next state $S_{t+1}$ in taking an action $a_t$ by an agent at state $S_t$, $r$ the reward function of the immediate reward to an agent for switching from state $S_t$ to state $S_{t+1}$, and $\\gamma\\in (0,1]$ the discount factor.\nThe goal of a reinforcement learning task is to output a policy that maximizes the cumulative long-term discounted reward, i.e.,\n$\\max_{\\pi} E_{\\tau \\sim P_{\\pi}(\\tau)} [\\sum_{t=0}^T r(S_t, a_t)]$ (6)\nwhere $\\tau$ represents the driving trajectory of the agent, $P_\\pi(\\tau)$ the trajectory distribution based on the policy $\\pi$, and T the step length of the environment.\nIn solving the above objective function, we can estimate the state or action-value function, which is defined recursively by the state and action-value functions based on the Bellman Optimal formula, i.e.,\n$V^{\\pi}(S_t) = E_{a_t \\sim \\pi(S_t)} [Q^{\\pi}(S_t, a_t)]$ (7)\n$Q^{\\pi}(S_t, a_t) = r(S_t, a_t) + \\gamma E_{S_{t+1} \\sim T(S_t,a_t)} [V^{\\pi}(S_{t+1})].$ (8)\nFor Equations (7) and (8), once the function $V^{\\pi*}(s)$ that maximizes the state value of all states $s\\in S$ is found, the corresponding policy $\\pi*$ would be the target solution for the reinforcement learning task."}, {"title": "C. The Proximal Policy Optimization Algorithm", "content": "PPO algorithms are deep reinforcement learning algorithms based on the actor-critic framework. They represent online policies and are known as model-free, aiming for the generation of both discrete and continuous controls. By using PPO, problems in continuous action space, specifically those related to autonomous driving decision tasks, can be well-addressed. These algorithms consist of two main components, an actor network that relies on the policy gradient, and a critic network that relies on the value evaluation. In this structure, the actor network generates continuous actions in response to the observed information from environments, while the critic network evaluates the output action of the actor network, influencing the subsequent action selection of the actor. PPO algorithms outperform traditional general policy gradient algorithms by achieving stable convergence speeds. In a general policy gradient algorithm, if the update step size is too large, the algorithm may easily diverge; conversely, if the update step size is too small, it may lead to convergence, but at an unacceptable speed. The procedures of PPO algorithms include running a traffic simulator through the traffic simulation environment to form a trajectories database. By obtaining different trajectories, i.e., $(s_t, a_t, r_{t+1}, S_{t+1})$, small batch sampling of the trajectory database can be performed to optimize the training process of both the actor network and critic network. We provide the training update details in Fig. 3.\nThe critic network aims to reduce the time difference error or the prediction error, and its optimization target is\n$L^V (\\phi) = \\hat{E}[A_t]$ (9)\nwhere $\\phi$ is the critic-network parameter, $\\hat{E}[]$ the estimated empirical mean of small batch data, and $A_t$ the advantage function for estimating the error of time differences. Generalized Advantage Estimation (GAE) is often used to calculate $A_t$ at time step t, i.e.,\n$A_t = \\delta_t + (\\gamma \\lambda)\\delta_{t+1} + (\\gamma \\lambda)^2\\delta_{t+2}\u00b7\u00b7\u00b7 +(\\gamma \\lambda)^{B-t-1}\\delta_{B-1}$ (10)"}, {"title": "D. Human Driving Prior Modeling", "content": "Up to here, we can use PPO algorithms to generate continuous controls of AVs, but the naturalness of driving behaviors cannot be fully guaranteed. To solve this and have a truly human driving prior, we further introduce GAIL [84] to supervise the training process. It is tempting to use traditional imitation learning solutions [85] directly by having behavioral cloning through training on expert experience data. However, we notice that the traditional solution may lead to overfitting problems, making the model hard to be generalized, and when reinforcement learning tasks that require temporal decision-making are involved, compound errors become unavoidable. Put it simply, behavioral cloning can mimic the training dataset well while it is vulnerable to variations of unseen situations. Furthermore, behavioral cloning generates each state-action pair independently without considering the relations between pairs, and therefore a small error in early steps can lead to unbearable errors down the line. We also notice that Inverse Reinforcement Learning (IRL) comes after the imitation learning solution with better performance [86], but the computational cost is often high and the learning efficiency is relatively lower. GAIL, on the other hand, builds upon Generative Adversarial Network (GAN) [87] and IRL, which learns policy directly from expert trajectories, and more importantly, it significantly improves learning efficiency and is easy to be generalized. Similar to GAN, GAIL consists of a generator $G_{\\phi}$ and a discriminator $D_{\\xi}$. The generator models the human expert driving behavior and outputs the action $a_t$ based on the input state $s_t$. The discriminator accepts both the generator state-action pair and the expert trajectory state-action pair, i.e., $(s_{\\phi,t}, a_{\\phi,t}), (S_{E,t}, a_{E,t})$ respectively, as the input, and outputs a real number from 0 to 1 to discriminate whether the input state-action pair is from the generator or the human expert. By doing this, GAIL can use these two networks to correctly generate pairs, making it robust to the overfitting problem, i.e., robust to unseen situations. Moreover, in GAIL, the discriminator compares the entire trajectory generated by the policy against the expert trajectory, instead of the individual state-action pair, making it immune from compound errors.\nWhen we train GAIL, the goal of the discriminator $D_\\xi$ is to maximize the classification accuracy between the generator policy and the expert policy, which can be expressed as\n$L(\\phi, \\xi) = E_{a_{\\phi,t} \\sim G_{\\phi}(S_{\\phi,t})} [log D_{\\xi}(S_{\\phi,t}, a_{\\phi,t})]+ E_{a_{E,t} \\sim \\pi_E(S_{E,t})} [log(1 - D_{\\xi}(S_{E,t}, a_{E,t})]$ (15)\nwhere $\\phi$ is the parameter of generator network, and $\\xi$ the parameter of discriminator network.\nMeanwhile, the goal of the generator is to make the generated trajectories misclassified as expert trajectories by the discriminator, so the output of the discriminator $D_{\\xi}$ can be used as a reward function to train the generator policy. With that in mind, we use the PPO algorithm as the generator of the GAIL algorithm and design the reward function as\n$r(S_{\\phi,t}, a_{\\phi,t}; \\xi) = \u2212 log(D_{\\xi}(S_{\\phi,t}, a_{\\phi,t}))$ (16)\nAfter alternately training the generator and the discriminator in an adversary manner, the data distribution between the generated one and the real expert one becomes indistinguishable, leading to the needed human Driving priors. We present the detailed network structure in the following subsection. It is important to understand that the previously introduced PPO algorithm is for generating trajectories from a predefined state-action space, and its critic network is for penalizing the reward function if the trajectory deviates from the space. On the other hand, the discriminator network in the context of GAIL is responsible for penalizing trajectories that deviate from the expert\u2019s trajectories, i.e., from the real-world dataset, thus preserving the naturalness of generated trajectories."}, {"title": "E. PPO and GAIL Network Structure Design", "content": "Recall that overfitting and training efficiency are the main concerns for traditional networks. In solving this, given the low dimensionality in the agent state space, we design a lightweight network structure by using one fully connected layer and a ReLU activation function. We present the network structure in Fig. 4 in which PPO builds upon an actor network and a critic network, and GAIL consists of the PPO network as the generator and a discriminator.\nFor the actor network, the input state $S_t$, first passes through an input layer with a number of neurons equal to the dimension of $S_t$, and then passes through two hidden layers with 128 neurons, and the results will be further decomposed into two head networks to generate the mean \u00b5 and variance \u03c3\u00b2 of a Gaussian distribution, and the output layer samples the Gaussian distribution and outputs the action $a_t$. For the critic network, with the same input, the input layer, and the hidden layer as the actor network, its output layer outputs an estimated $V_\\phi(S_t)$ to the current state $S_t$. Finally, sharing the same network with the critic network, the discriminator network uses the state-action pair as the input, i.e., the concatenation of $S_t$ and $a_t$, and the classification probability, $D_\\xi(S_t, a_t)$, of the state-action pair, as the output."}, {"title": "F. Natural Adversarial Policy Modeling and the Design of its Reward Functions", "content": "We construct the natural adversarial policy model according to the PPO algorithm and the trained GAIL model as discussed in the previous subsections. To guarantee both the adversariality and naturalness, the key is to design the related reward functions that can be used to train the agent, i.e., adversariality reward and naturalness reward, and henceforth we explain our design as follows.\n1) Adversariality Reward: We use the adversarial reward for a trained agent to generate disturbing actions to the AV under test, e.g., emergency braking, and sudden lane changes. The adversarial reward at time step t can be expressed as\n$R_{adv,t} = r_{d,t}(P_{AV,t_0}, P_{agent,t_0}, P_{AV,t}, P_{agent,t}) + r_{c,t}$ (17)\nwhere $P_{AV,t_0}$ and $P_{agent,t_0}$, and $P_{AV,t}$ and $P_{agent,t}$ denote the position of the AV under test and the trained agent at the moment of initialization $t_0$, and at time step t, respectively. $r_{d,t}$ as a metric refers to the distance between the AV under test and the trained agent. A smaller $r_{d,t}$ means a collision is likely to happen, namely more danger, and then more rewards, which can be formulated as\n$r_{d,t}(P_{AV,t_0}, P_{agent,t_0}, P_{AV,t}, P_{agent,t}) = clip(\\frac{||P_{AV, t_0} - P_{agent,t_0}||_2 - ||P_{AV,t} - P_{agent,t}||_2}{||P_{AV,t_0} - P_{agent,t_0}||_2}, -1,1)$ (18)\nThe clip function used here is a convenient tool in learning networks to set a threshold for each updated policy, and by doing this, we can stabilize the learning procedure, and avoid large and harmful policy updates.\n$rc,t = \\begin{cases}\n1, & \\text{collided with the AV under test,} \\\\\n0, & \\text{no collision,} \\\\\n-1, & \\text{collided with other vehicles.}\n\\end{cases}$ (19)\n2) Naturalness Reward: To guarantee naturalness, the pre-trained human driving prior model GAIL is in use to supervise the naturalness of the agent\u2019s behavior. We notice that both the action output of the agent and the GAIL prior model follow multivariate Gaussian distributions, and therefore, we decide to use Kullback-Leibler (KL) divergence [88] to measure the distance between these two distributions. The naturalness reward can be expressed as\n$R_{nat,t}(s_t) = clip(\\frac{M \u2013 KL(G_{\\phi}(\u00b7|s_t), \\pi_{\\theta}(\u00b7|s_t))}{M},0,1)$ (20)\nwhere $G_{\\phi}(\u00b7|s_t)$ and $\\pi_{\\theta}(\u00b7|s_t)$ denote the action distribution output of the GAIL generator and the agent at state $S_t$, respectively. For KL divergence, after assuming $G_{\\phi}(\u00b7|s_t)$ is the real human driving behavior distribution, it needs to make sure that the fitted distribution $\\pi_{\\theta}(\u00b7|s_t)$ is getting closer to $G_{\\phi}(\u00b7|s_t)$. M is an empirical value for the KL divergence of an agent at the beginning of training. Finally, based on Eq. 17 and Eq. 20, we can conclude the natural adversarial reward function as\n$R_t = R_{adv,t} + wR_{nat,t}$ (21)\nwhere is the weight factor for balancing between the adversariality and naturalness."}, {"title": "IV. EXPERIMENT", "content": "A. Experimental setup\n1) Human Driving Traffic Dataset: The experiments have been conducted on public datasets NGSIM [35", "36": ".", "NGSIM": "This dataset includes trajectories of vehicles on US-101", "INTERACTION": "This dataset includes trajectories from various junctions across several countries. We utilize the subset labeled TC_BGR_Intersection_VA", "Construction": "Similar to existing work for driving behavior modeling [81", "89": "we build the traffic simulation environment on Highway-env", "79": ".", "include": "a_{IDM"}, "representing the maximum acceleration; $V_{IDM}$, the desired speed; $\\delta_{IDM}$, the acceleration exponent; $S_{IDM}$, the desired following distance; $b_{IDM}$, the comfortable deceleration; and T, the safe headway time distance. Based on these ranges, we employ the genetic optimizer, as illustrated in Sec. III, to calibrate the parameters. The range and the calibrated parameter are presented in Table. I. Note that $\\delta_{IDM}$ represents the aggressiveness of a driver, which often has 4 as its default value. As shown in Fig. 7, the genetic optimizer is well converged to an acceptable loss at 0.168 within 100 rounds of iterations. A comparison between the original measurements of space headway and velocity, and the result simulated using IDM, is depicted in Fig. 8. This comparison illustrates a close match between the IDM simulated data and the original observations, thereby validating our parameter calibration schemes.\nFor MOBIL, the parameters are typically calibrated by statistical analysis from real-world data and existing studies. In detail, to enhance the accuracy, we need to calibrate MOBIL-related parameters precisely when the vehicle is changing lanes. To achieve this, we capture the trajectory that has the greatest heading, as illustrated in Fig. 9. After that, we can obtain the calibrated parameters from the captured trajectories: $\u0394_ath = 0.2m/s^2$, representing the acceleration gain, and MAX_BRAKING_IMPOSED = 2m/s\u00b2, representing the maximum braking imposed to the follower, in line with the existing work [81"], "Details": "The supervision of GAIL\u2019s training demands human expert trajectories. We acquire these trajectories through the simulation environment with a sampling frequency of 10Hz, and they appear as state-action pairs. The state is a feature vector of 56 dimensions, including 6 features of the length and width of the vehicle, the lateral offset of the lane centerline, the lateral speed, the longitudinal speed, and the steering, and the other 50 features for representing the relative information to the 10 nearest vehicles within 50m, e.g., the relative lateral distance, relative longitudinal distance, relative lateral speed, relative longitudinal speed, and relative steering between the vehicles. Note that if the number of neighboring vehicles is less than 10, the emptied features\u2019 values are denoted as 0. The action is a vector of 2 dimensions, including the acceleration and steering, where the acceleration interval is [-5m/s\u00b2, 3m/s\u00b2] and the steering interval is [-\u03c0/3rad, \u03c0/3rad]. The collecting process is as follows.\n1) From the NGSIM dataset, we select vehicles that are changing lanes, and from the INTERSECTION dataset, we make random selections.\n2) For each selected vehicle, we use 100 sampled intervals as one scenario and 4 scenarios in total. If the running time of a selected vehicle is greater than the time of 4 scenarios, i.e., 40s, we randomly pick 4 scenarios from the whole running time.\n3) To focus on general traffic situations, empirically, the simulation will be skipped"}