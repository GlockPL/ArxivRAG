{"title": "Prompting Techniques for Secure Code Generation: A Systematic Investigation", "authors": ["Catherine Tony", "Nicol\u00e1s E. D\u00edaz Ferreyra", "Markus Mutas", "Salem Dhiff", "Riccardo Scandariato"], "abstract": "Large Language Models (LLMs) are gaining momentum in software development with prompt-driven programming enabling developers to create code from natural language (NL) instructions. However, studies have questioned their ability to produce secure code and, thereby, the quality of prompt-generated software. Alongside, various prompting techniques that carefully tailor prompts have emerged to elicit optimal responses from LLMs. Still, the interplay between such prompting strategies and secure code generation remains under-explored and calls for further investigations. Objective: In this study, we investigate the impact of different prompting techniques on the security of code generated from NL instructions by LLMs. Method: First we perform a systematic literature review to identify the existing prompting techniques that can be used for code generation tasks. A subset of these techniques are evaluated on GPT-3, GPT-3.5, and GPT-4 models for secure code generation. For this, we used an existing dataset consisting of 150 NL security-relevant code-generation prompts. Results: Our work (i) classifies potential prompting techniques for code generation (ii) adapts and evaluates a subset of the identified techniques for secure code generation tasks and (iii) observes a reduction in security weaknesses across the tested LLMs, especially after using an existing technique called Recursive Criticism and Improvement (RCI), contributing valuable insights to the ongoing discourse on LLM-generated code security.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have received major attention recently due to their high performance in solving Natural Language (NL) processing tasks. Alongside, their application to program synthesis has advanced significantly, allowing software developers to generate code from NL descriptions or prompts. Overall, this is achieved through vast training sets of code and documentation text extracted from open-source repositories. While this approach helps LLMs produce functional implementations, it offers no guarantees of correctness or quality, as it treats code simply as text, ignoring essential semantic information [41]. Moreover, open-source projects are known for containing security flaws [34, 93, 105, 106], making LLM-generated code prone to security vulnerabilities [72, 73].\nRecent investigations [98] show that developers are gradually showing a preference for AI-driven code assistants to initiate their coding process. These tools offer a valuable starting point, aiding in the development process and alleviating the need to search for information online. However, when utilizing such AI assistants powered by LLMs, developers often display an over-reliance behavior that involves optimistic assumptions regarding the correctness and security of the generated code without thorough questioning [74] [85]. Findings from a user study conducted by Perry et al. [74] revealed that participants who had access to an Al assistant tended to produce insecure solutions more frequently compared to those who did not have access to such assistance. This emphasizes the importance of exploring avenues to strengthen the security incorporated by the LLMs in the code generated by them.\nMotivation: Prompt engineering, the process of refining prompts to optimize the quality of responses generated by LLMs, has garnered significant attention following the emergence of LLMs like ChatGPT, BARD, and others. A variety of sophisticated prompting techniques have been developed for tasks such as text generation, classification, and problem-solving. Many of these techniques can be used by the end users to directly prompt or interact with LLM- powered tools and chatbots. Despite the abundance of research in this field, the correlation between such prompting strategies and secure code generation has not been thoroughly examined or documented in the existing literature. Specifically, the extent to which such techniques can guide LLMs towards producing secure implementations remains an open question. While models like GPT-3 continually advance, with each version improving upon its predecessor, the implications of these enhancements for security are unclear. This underscores the importance of investigating NL prompting techniques that have the potential to enhance the security of the code generated by LLMs.\nIn this work, we perform a literature review to identify potential prompting techniques that can be used for code generation followed by an in-depth analysis of the impact of these techniques on improving the security in LLM- generated code. For this, we elaborate on the following research questions (RQs):\nRQ1: What are the existing prompting techniques that can be used for code generation? To answer this, we performed a systematic literature review of papers that introduced different prompting techniques that can be potentially used for code generation.\nRQ2: What is the impact of different prompting techniques on the security of LLM-generated code? For this, we conducted an in-depth analysis using a subset of prompting techniques identified in the literature review. A dataset called LLMSecEval [94], containing 150 NL prompts specifying coding tasks that could potentially lead to insecure code implementations, was used for our experiments. We evaluated Python programs generated by the LLMs since it is one of the most popular choice of languages for developers. The code generated by the LLMs for the selected techniques was evaluated for security weaknesses using a static analysis tool called Bandit.\nExperiments were conducted utilizing GPT-3, GPT-3.5, and GPT-4 models, due to their widespread usage and advanced natural language processing and coding capabilities, which are crucial for exploring various prompting techniques. Our findings reaffirm the fact that LLM-generated code contains a large number of security weaknesses mainly related to CWE-78, CWE-259, CWE-94, and CWE-330. We observed that integrating different prompting techniques has a positive impact on the security of code generated by LLMs, particularly noticeable in advanced models like GPT-4. Notably, a technique known as Recursive Criticism and Improvement (RCI) has exhibited significant potential in mitigating security weaknesses in the generated code. Furthermore, we have observed distinct changes in the coding behavior of the models when security specifications are introduced to the prompts, offering insights that can be utilized to refine prompting techniques for secure code generation.\nContributions This work makes the following contributions to the field of secure code generation using LLMs:\n\u2022 To the best of our knowledge, we present the first systematic inventory of prompting techniques that are suitable for code generation. Often, papers in this field make an arbitrary selection of a few techniques, e.g., based on convenience or because other referenced papers do the same. This paper highlights that a rich selection of techniques exists and incentivizes the community to explore the alternatives in their work.\n\u2022 To simplify this exploration, we have translated a selection of these generic prompting techniques into actionable templates that can be reused by the community as is, or with some adaptations for (secure) code generation. This effort is expected to stimulate the use of the different prompting techniques, beyond the usual suspects.\n\u2022 We provide insights (and rankings) concerning the prompting techniques that are more promising for secure code generation. Interestingly, to the extent of our knowledge, the most promising technique has not been used in the related work for secure code generation (cf. the first point).\nThe rest of the paper is organized as follows: Section 2 presents the existing work on using LLMs for (secure) code generation. Section 3 and 4 present the approach used for the systematic literature review and the findings obtained from it. Following this, Sections 5 and 6 delve into the specifics of the security evaluation of code generated by LLMs using various prompting techniques and the results. Insights obtained from the results are elaborated in Section 7. Section 8 addresses the limitations, while Section 9 brings the work to a close."}, {"title": "2 RELATED WORK", "content": "This section presents prior research that delves into the use of LLMs for code generation and explores studies that assess the security aspects of code generated by LLMs."}, {"title": "2.1 Code Generation Using LLMs", "content": "There are several works (both published and unpublished) that evaluate the code generation capabilities of LLMs. The following are a few notable ones that are peer-reviewed.\nA paper by Hendrycks et al. [36] evaluated the code generated by GPT-2 [78], GPT-3 [11] and GPT-Neo using a benchmark dataset called APPS (Automated Program Progress Standard) [36] that consists of 10,000 NL coding problems along with corresponding test cases and ground truth solutions created by humans. For the evaluation, they employed the few-shot prompting technique where the model is provided with a set of <input-output> examples to demonstrate how to solve the problem. At the time of this study, they observed that the overall performance exhibited by the models was low based on the percentage of test cases passed. In another study conducted by Austin et al. [4], the authors explored the limitations of program synthesis carried out by language models trained at various scales, ranging from 244M to 137B parameters. To accomplish this, they created two datasets: the Mostly Basic Programming Problems (MBPP) dataset and the MathQA-Python dataset. The MBPP dataset comprises problem statements, simple Python functions designed to solve these problems, and three corresponding test cases. On the other hand, the MathQA-Python dataset presents mathematical problems, multiple-choice answers for these problems, and Python implementations that produce the correct answers. Both datasets are created to verify the semantic correctness of the generated Python programs. They also employed a few-shot prompting technique and their observations revealed a correlation between the increase in model size and improved performance.\nXu et al. [109] conducted a comprehensive assessment of various LLMs, including Codex [14], GPT-J, GPT-Neo, GPT-NeoX-20B [10], CodeParrot [96], and PolyCoder (a model developed by the authors of this paper) for their code generation capabilities. Their evaluation focused on these models' performance using the HumanEval [14] dataset, which contains 164 distinct coding tasks presented as prompts with corresponding test cases. These prompts consist of incomplete code snippets paired with NL comments rather than a complete NL instruction describing the task. In this study, they employed a zero-shot prompting technique. Zero-shot prompting entails not providing explicit <input-output> pairs to the LLMs to demonstrate how to approach the given task. Based on this study, Codex emerged as the top-performing model, outperforming all the other models in the evaluation.\nA study by Zeng et al. [115] tried to understand how pre-trained models perform for program understanding and generation tasks by experimenting with 8 LLMs that include CodeBERT [26], GraphCodeBERT [31], ContraCode [42], CodeGPT, PLBART [1], CodeTrans [24], CoText [75] and CodeT5 [102] mainly using the CodeXGLUE [61] benchmark. This benchmark is a collection of datasets spread across 10 different code-related tasks. The dataset used for code generation tasks within this benchmark is known as Concode. The prompts in Concode encompass NL problem descriptions, structured in the form of Java Doc comments and class environments. The researchers employed zero-shot prompting to evaluate the models. The results of their experiments indicated that CodeT5 and CodeTrans consistently delivered the highest performance in code generation tasks. In another work, an extensive literature review was conducted by Hou et al. [25] where they examine papers that present works done using LLMs for software engineering tasks. Their analysis reveals a growing emphasis on models from the GPT series, with GPT-4 [69] gaining significant attention in studies related to code generation using LLMs.\nBesides the aforementioned studies, there exist papers introducing code synthesis benchmarks like EvalPlus [60] and Multipl-E [13], which assess the code generated by various LLMs. Furthermore, the papers that introduce different LLMs capable of performing code generation [14, 16, 26, 27, 55, 102, 109] task also perform evaluation of the code generated by their respective models. The prompting techniques employed in such studies are predominantly limited to either zero-shot or few-shot prompting."}, {"title": "2.2 Security in LLM-Generated Code", "content": "As mentioned earlier, prior work has elaborated on the security of code generated by LLMs. Pearce et al. [72], for instance, used 54 high-risk security scenarios containing incomplete code snippets (C and Python) to assess code completions produced by GitHub Copilot and observed that 40% of them contained security vulnerabilities. However, a study by Asare et al. [3], compared C/C++ code generated by human developers against the ones generated by Copilot and observed that Copilot is not as bad as humans in introducing vulnerabilities in code. The experiments in these studies were done using zero-shot prompts. In another work by Pearce et al. [73], they tested the code repair capabilities of LLMs using various program repair scenarios. Overall, they concluded that Codex and Jurassic-1 [56] are capable of finding fixes for simple scenarios again under zero-shot settings. Jesse et al. [43] did a recent study where they examined if Codex and other LLMs generate simple, stupid bugs (SStuBs) and found that these models produce twice as many SStuBs as correct code. On the other hand, [35] proposed a learning approach for controlled code generation called SVEN. Such an approach, in which a boolean parameter is passed to enforce secure/insecure code generation, increased the number of secure code produced by an LLM called CodeGen by 25%. Another study by Yeti\u015ftiren et al. [113] assessed the quality (i.e., validity, correctness, reliability, security, and maintainability) of code generated by Copilot, Amazon CodeWhisperer, and ChatGPT using the HumanEval dataset. Notably, no significant security vulnerabilities were found in the generated code. However, the authors acknowledge the limitations of their security evaluation, since the HumanEval dataset is designed to verify functional correctness rather than code security.\nDelving further into the realm of secure code generation using LLMs, Sandoval et al. [84] investigated the impact of LLM on code security through a user study. The study involved 58 computer science students who were tasked with performing simple operations on a linked list using C programming language with a focus on memory-based vulnerabilities. They observed that the participants who used an AI assistant powered by Codex introduced security- critical bugs at a rate no higher than 10% when compared to the control group indicating that the use of LLMs does not introduce new vulnerabilities. Nevertheless, it is essential to acknowledge that these findings may not be universally applicable to more complex programming tasks. Contrary to the previous study Perry et al. [74] observed different results in a study that explored developers' interactions with Al code assistants concerning security. Forty-seven participants were engaged with an AI assistant powered by Codex to fulfill five security-related programming tasks across Python, JavaScript, and C. The findings revealed that participants utilizing AI assistants were prone to generating insecure solutions more often than those without AI assistance in four out of five tasks. Typical issues encompassed the selection of unsafe libraries, incorrect library utilization, insufficient comprehension of edge cases involving external entities like file systems or databases, and inadequate sanitization of user input.\nAdditionally, apart from empirical and user studies on LLMs, a systematic literature review conducted by Yao et al. [112], delves into the use of LLMs for security and privacy purposes. Their findings indicate a plethora of works employing LLMs in security-related tasks, such as coding, test case generation, bug detection, vulnerability detection, and fixing. These endeavors have positively influenced research within the security community. However, none of these studies thoroughly investigate different prompting techniques to enhance the secure code generation process."}, {"title": "3 METHODOLOGY FOR SYSTEMATIC LITERATURE REVIEW", "content": "The goal of this review is to find prompting techniques that can be used for code-generation tasks using LLMs. However, there are only a limited number of prompting techniques explicitly designed for code generation. Therefore, we opted to review all prompting techniques introduced for generating textual content, presuming their potential transferability to code-generation tasks, given that code generation falls within the domain of textual content generation. The steps followed to perform the literature review are depicted in Figure 1. We used the Publish or Perish tool [33] to retrieve papers from Google Scholar. Following the PICOC strategy [12], the search query given below was employed to retrieve the relevant papers that introduce prompting techniques for textual content generation.\nprompt* AND (engineer* OR pattern* OR technique*) AND (language model OR pre-trained model* OR 11m* OR ptm*)"}, {"title": "Paper Screening", "content": "The review process was done in two screening steps. In the first screening, we looked at the title and abstract of the paper to decide if it was relevant to our study. If it is then it was shortlisted for the second screening. In the second screening, we looked into the full paper to decide if it fits our criteria. The first and second screening was done based on the following inclusion and exclusion criteria.\nInclusion Criteria:\nIC1: Paper deals with prompting LLMs using one or more techniques\nIC2: Paper is published since 2018:\nIC3: Paper is written in English\nExclusion Criteria:\nEC1: Paper does not introduce new prompting techniques to query LLM\nEC2: Paper deals with the generation of anything other than text and code (e.g: image, speech, and video data)\nEC3: Paper that presents prompting techniques that can not be used for generation tasks (e.g. techniques specific to classification tasks)\nEC4: Paper that presents automated prompt optimization techniques and frameworks (e.g. prompt tuning and black-box tuning)\nEC5: Paper that presents prompting technique for attacking the model (e.g. jailbreak prompts)\nEC6: Out of scope (e.g. techniques for medical science)"}, {"title": "Snowballing", "content": "To ensure that we did not miss any other relevant papers, we also performed 3 rounds of backward snowballing [107]. Here we went through the references of the selected papers iteratively following the same two-step screening process as above until no new papers were obtained. From this, we obtained 5 additional relevant ones making the total number of relevant papers 13. Three papers under consideration were released on preprint servers like arXiv and have not undergone formal peer review. However, these preprint papers have been frequently cited with the least number of citations being 48. Hence we decided to retain those papers."}, {"title": "Knowledge Extraction", "content": "Each final paper that introduced a prompting technique suitable for code generation was examined in detail. The primary objective was to extract the techniques themselves and pinpoint their key features. For this, we performed a lightweight thematic analysis with open coding as it offers a qualitative method for analyzing textual or qualitative data to interpret patterns or themes within the data [92][108]. During this process, the first author extracted codes related to prompting techniques following an inductive approach. The themes that emerged from this coding were then discussed with two other authors to categorize and label the techniques.\nIn addition to this, attention was also directed towards details such as the LLMs on which the technique was tested, the specific tasks used for evaluation, and the datasets employed for this purpose. Furthermore, data regarding the year of publication, venue, and citation count at the time of the study were also extracted. This was aimed at creating a consolidated source of information beneficial to researchers and developers delving into prompting techniques for code generation."}, {"title": "4 PROMPTING TECHNIQUES FOR CODE GENERATION (RQ1)", "content": "In this section, we present an overview of the selected prompting techniques from the SLR that are deemed suitable for code-generation tasks. Throughout our review, we encountered numerous prompting techniques. However, not all of them were selected to be in our final list as determined by our exclusion criteria. All results of this literature review, along with the techniques that were excluded from our consideration and the reason for their exclusion are documented in our replication package specified in Section 10."}, {"title": "4.1 Overview of the Selected Papers", "content": "The information extracted from the 13 papers is presented in Table 1. The chosen papers are those that introduce novel prompting techniques. Among these, we identified 15 distinct techniques designed for textual content generation with potential applicability to code-generation tasks. Ten of these papers have undergone peer review, while the remaining have received at least 48 citations. Except for two papers [81][104], all have conducted experimental validation of their introduced prompting techniques. Only two of them [63] [44] have evaluated their techniques specifically for code generation tasks. The other techniques primarily target various reasoning tasks such as symbolic, logical, commonsense, and arithmetic. Among the papers that conducted experimental validation, ten out of eleven utilize OpenAI models indicating the prevalence of these models in research in this field.\nBased on commonalities derived from the thematic analysis, we have labeled the techniques using 3 distinct properties related to their execution as shown in Table 1. They are Single/Multi-step, Demonstrative/Non-demonstrative and Linear/Parallel. A technique that prompts the model in a single step, obtaining the final output with just one prompt, is referred to as a single-step technique. Conversely, a technique requiring multiple prompts to generate the final output is termed a multi-step technique. Single-step techniques are cost-effective compared to multi-step techniques as they necessitate only one prompt. Among the 15 techniques identified, 6 are single-step techniques, while the rest are multi-step techniques. If a technique is executed by providing demonstrative examples of inputs and expected outputs for prompting the model, it is categorized as a demonstrative technique. Conversely, a technique not requiring input-output examples is labeled as a non-demonstrative technique. Six out of 15 techniques are non-demonstrative. Although demonstrative techniques may potentially yield desired outputs more effectively than non-demonstrative techniques, this depends on the availability of high-quality demonstrative examples. In real-world scenarios, especially in complex code generation tasks, obtaining such examples can be challenging. Most techniques in our inventory involve conducting a single sequential interaction with the LLM. Here, the model is prompted, and its response is either used as the final output or serves as a basis for proceeding to the next step of prompting. These techniques are labeled as linear."}, {"title": "4.2 Classification of Prompting Techniques", "content": "Aside from the labels provided in Table 1 (single/multi-step, demonstrative/non-demonstrative and linear/parallel), we also identified some other common characteristics based on the strategic design of different prompting techniques which we used to classify them into 5 different categories as shown in Figure 2. Below we describe the categories and the techniques that belong to them, accompanied by demonstrations of how these techniques can be utilized for code generation tasks. The responses of the LLM depicted in these demonstrations were generated by ChatGPT (GPT-3.5), which is a conversational chatbot, in response to different prompting techniques."}, {"title": "4.2.1 Root Techniques", "content": "These are the foundational and most popular techniques based on which more advanced techniques are built. Zero-shot, one-shot, and few-shot prompting come under this category.\nZero-shot: In this technique a model is asked to perform a task without task-specific training or examples at the time of inference [11]. In such cases, the model relies completely on the data it has seen during its pre-training to generate an appropriate response. In conversational LLMs such as ChatGPT, zero-shot prompting is possibly the most commonly used way of interaction by an average user. It has the advantage of not having to prepare a task-specific dataset of input-output demonstrations to generate desirable output. However, if the model has not seen data related to the task at hand in its training, then the performance of the model can be suboptimal with zero-shot prompting. This technique can be directly used for code generation tasks. Figure 3 includes a demonstration of zero-shot prompting for a simple coding task and ChatGPT's response to it.\nOne-shot/Few-shot: One-shot and few-shot prompting techniques [11] are very similar to each other. In one-shot prompting, the model is given a single input-output example whereas in the few-shot prompting the model is given examples of the task at inference time as conditioning before providing the final input for which it is expected to produce the output. By supplying the model with both input and corresponding output samples, it gains the benefit of producing a response that closely aligns with the desired format. However, it can be a disadvantage when one does not have sufficient or relevant task-specific data in advance. An illustration demonstrating the application of few-shot prompting on ChatGPT for code generation can be seen in Figure 3. This example utilizes two few-shot examples. We have omitted a separate example of one-shot prompting since it closely resembles few-shot prompting but with only one demonstrative example."}, {"title": "4.2.2 Refinement-based Techniques", "content": "Techniques belonging to this category focus on improving, refining, or iterating the model outputs. They might involve feedback loops, user interactions, or model self-assessment to enhance the quality of the generated responses. The prompting techniques that come under this category include Recursive Criticism and Improvement (RCI), Self-refine, and Progressive Hint prompting.\nRCI: This prompting technique [47] is built on the understanding that LLMs possess a strong capability to evaluate and recognize flaws in their own output. This technique involves a two-step process in addition to providing the initial input task. Firstly, the LLM is prompted to analyze and critique its current response (for instance: \"Review your previous answer and find problems with your answer\"). Subsequently, drawing from the critiques it has outlined, the LLM is then instructed to rectify the identified issues and revise its output accordingly (for example: \"Based on the problems you found, improve your answer\"). This two-step process is repeated until a satisfactory output is obtained or until a predefined number of iterations is done. RCI has the advantage that it needs no task-specific expert data to generate desirable responses. However, this approach can be expensive due to the iterative nature of the process. An added disadvantage is that the success of this approach relies on the ability of the model to identify its own mistakes. A demonstration of one iteration of this technique used for a code generation task is shown in Figure 4.\nSelf-refine: This technique [63] is very similar to RCI. It uses 2 steps called feedback and refine in addition to an initial output generation step to generate high-quality output. The initial output from model M is generated using a task-specific prompt $\\rho_{gen}$ with few-shot <input, output> example pairs. Next, they use a prompt $\\rho_{fb}$ to generate feedback for the previously generated output by M. Few-shot examples are provided in this step in the form of <input, output, feedback> triplets. The next step is to refine the output based on the generated feedback. This is done using prompt $\\rho_{refine}$ that contains few-shot examples of refining outputs in the form of <input, output, feedback, refined> quadruples. Since this approach is very similar to the RCI (Figure 4) with the exception of providing few-shot examples with every step, a separate demonstration is not provided here.\nProgressive Hint: Progressive Hint prompting (PHP) [117] is another technique that iteratively refines the output from the LLM by providing increasingly informative hints in each iteration. The pipeline of this approach is divided into two stages. The first stage is called base answer and base prompt. In this stage, the model is provided with an input task with a basic prompt to which a base answer is generated.\nThe second stage is called subsequent answer and PHP where the base prompt is combined with hints that are extracted from the previous answers (or base answer in this case). This is repeated until the answers from the model do not change. Figure 4 shows the interaction with ChatGPT for a simple coding task using this technique. PHP can be combined with standard zero-shot prompting or sophisticated techniques such as CoT. This approach requires at least 2 iterations. The approach is not considered successful until the last 2 outputs from the model are the same. This can become computationally expensive based on the task and the model. Additionally, the model can be misled if the hints provided stray too far away from the correct answer [117]. This approach can be theoretically used for code generation tasks as shown in Figure 4."}, {"title": "4.2.3 Decomposition-based Techniques", "content": "Techniques in this category break down complex tasks or prompts into simpler, more manageable pieces. Here, the language models perform multiple small tasks to incrementally build towards the final, complex solution, facilitating more accurate responses. The techniques under this category include least-to-most and self-planning prompting.\nLeast-to-most: This prompting technique [119] is executed in two stages. In the decomposition stage, the model is prompted to decompose the complex task into smaller sub-tasks. This prompt is delivered using a few-shot approach, where a few examples are presented to illustrate how larger tasks can be dissected into sub-tasks, followed by the actual complex task that needs to be addressed. The second stage is the sub-problem solving stage where the model is asked to sequentially solve all the sub-problems or sub-tasks identified in the decomposition stage. Here also, few-shot examples demonstrating how sub-problems are solved are provided. Responses derived from solving each sub-task are integrated back into the original task description before presenting the subsequent sub-task to the model. This iterative process continues until all sub-tasks have been resolved, resulting in the final solution.\nLeast-to-most prompting technique can also be used in combination with CoT or self-consistency prompting techniques. Similar to other advanced prompting methods, this technique's drawback is the necessity to supply few-shot examples for both the decomposition of a complex task and the resolution of its sub-tasks. The resource demands can escalate with the increasing number of sub-tasks involved in the process. This approach can also be potentially used for code generation provided you have a sufficient dataset containing information on coding problem decompositions and solutions. Figure 5 demonstrates how this technique can be used for code generation.\nSelf-planning: This prompting approach [44] is specifically designed for code generation problems. Hence no additional adaptation is required to tailor the technique for code generation tasks. Self-planning is carried out in two phases. The first one is the planning phase where the code generation task is decomposed into a plan of actions. This decomposition is done by the LLM itself. The LLM is provided with demonstrative examples of how to come up with plans to solve coding tasks before asking it to generate a plan for the task at hand. The action plan is structured as an ordered list of steps. The plan should always conclude with a return statement. The second phase is called the implementation phase wherein the LLM's formulated plan is integrated with the original task prompt. This integration prompts the LLM to adhere to its own outlined strategy when producing the final code snippet. An example demonstration of this prompting technique is shown in Figure 6. This example is directly taken from the original paper itself."}, {"title": "4.2.4 Reasoning-based Techniques", "content": "Techniques that guide the model to employ and demonstrate logical reasoning for generating responses are categorized as reasoning-based techniques. Reasoning encompasses the act of drawing logical conclusions, evaluating arguments, and making inferences using the information at hand [39]. These methods emphasize the model's ability to engage in cognitive and logical processes. Rather than simplifying a task as in the case of decomposition-based techniques, these techniques encourage the model to follow a logical reasoning path and articulate its thought process. The techniques that come under this category are Chain-of-Thought, Zero shot Chain-of-Thought, Self-consistency and Few-shot with Explanation.\nChain-of-Thought (CoT): In this prompting approach [103], the LLM is compelled to produce a sequence of intermediary logical reasoning steps in natural language, culminating in the solution to the presented problem. The goal of this approach is to replicate how humans solve a complex problem following a chain of reasoning or justification steps. In this method, the model is initially given a set of few-shot examples, consisting of <input, chain of thought, output> triplets, to guide its understanding before it tackles the actual task. This technique has been evaluated on various benchmarks including arithmetic, common sense, and symbolic reasoning. However, one can assume that CoT can also be applied to code generation tasks. Figure 7 demonstrates the CoT prompting technique for code generation.\nAn approach similar to this was proposed in 2017 by Ling et al. [59] where they train an attention-based sequence-to- sequence model to solve complex mathematical problems using a dataset containing problems with answer rationales and the final correct answers. However, this approach focused on training rather than explicitly prompting a model, and it did not involve an LLM. Hence we identify CoT as a novel prompting technique.\nZero-shot CoT: This approach [48] addresses the limitations of the CoT approach, which requires task-specific reasoning examples. Zero-shot CoT prompting is carried out in two stages. The first one is the reasoning extraction stage where the model is prompted to generate the logical reasoning for handling a given input task. Here the initial input task is appended with a hand-crafted trigger sentence to extract the chain of thought reasoning from the model. From the evaluation conducted by the authors, the trigger phrase Let's think step by step yields the best results. The second stage is the answer extraction stage where the model is supplied with the initial input task, the reasoning trigger sentence, the step-by-step reasoning generated by the model, and another hand-crafted trigger sentence to extract the final answer. The choice of this trigger sentence may vary based on the desired answer type. For example, for a mathematical problem, a prompt like \"Therefore, the answer (Arabic numerals) is\" nudges the model towards providing a numeric response. Since the prompt template of this technique varies very minimally across tasks, zero-shot CoT is considered a task-agnostic approach. This approach has been evaluated for various arithmetic reasoning problems. An example of applying this technique for code generation is included in Figure 7. Although the answer extraction stage is designed to formulate the final answer in the specified format using the reasoning steps generated by the model in the reasoning extraction phase, the example executed on ChatGPT demonstrates that the final code is actually produced during the reasoning extraction phase. Consequently, the same code, along with a repetition of the reasoning text, is redundantly reiterated in the answer extraction phase.\nSelf-consistency/Complexity-based: Self-consistency [101] and complexity-based [28] prompting techniques are similar to each other and are built on top of the CoT technique. They use a sample-and-marginalize decoding strategy to generate more reliable output compared to that CoT. In self-consistency, the model is provided with an input task along with a set of chain-of-thought few-shot examples (<input, reasoning, output>). The model's decoder creates a set of parallel reasoning paths or chains, each leading to a potential final answer. Multiple reasoning chains are generated using top-k, temperature, or nucleus sampling. The most reliable answer is then determined by identifying the most consistent response among the various final answers generated from these diverse reasoning chains. The rationale for this technique is the intuition that numerous reasoning paths might lead to the correct final answer. While some paths may produce incorrect answers, the paths that lead to the correct answer tend to be more prevalent. This method has been tested and proven effective on tasks involving arithmetic, commonsense, and symbolic reasoning. Complexity-based prompting also adopts a similar approach but posits that chains involving more reasoning steps yield better performance. Consequently, this technique emphasizes using chain-of-thought few-shot examples comprising a greater number of reasoning steps (i.e., more complexity). Furthermore, the final answer is chosen based on the consistency among responses with a larger number of reasoning steps, while responses with fewer reasoning steps are discarded.\nBoth techniques are particularly well-suited for tasks that have a definitive final answer, as opposed to more creative tasks like code generation. However, they can still be applied to code generation tasks. A demonstration of adapting self-consistency for code generation is included in Figure 8. A separate demonstration of complexity-based technique is not provided as the prompting approach followed is very similar except for the number of reasoning steps. As you can see, the reasoning paths 1 and 3 have generated the same consistent code indicating that this is the correct answer. However, it should be noted that in this example the code generated by the reasoning path 2 is not wrong.\nFew-shot with Explanation. As the name indicates, this technique [54] uses few-shot input-output examples with a task instruction with additional explanations for each of the examples. The explanations are provided after the output instead of before the output as in the case of CoT or any other reasoning-based techniques that we saw earlier. They evaluated this approach on several reasoning and inference-based tasks such as causality reasoning, mathematical induction, and inferring presupposition behind an utterance. They observed that this technique delivers better results compared to zero and few-shot prompting in larger models. An adaptation of this technique for a code generation task is also shown in Figure 8."}, {"title": "4.2.5 Priming Techniques", "content": "A recent work on prompt engineering by White et al. [104] proposed a```json\n{\n  "}, {"title": "Prompting Techniques for Secure Code Generation: A Systematic Investigation", "authors": ["Catherine Tony", "Nicol\u00e1s E. D\u00edaz Ferreyra", "Markus Mutas", "Salem Dhiff", "Riccardo Scandariato"], "abstract": "Large Language Models (LLMs) are gaining momentum in software development with prompt-driven programming enabling developers to create code from natural language (NL) instructions. However, studies have questioned their ability to produce secure code and, thereby, the quality of prompt-generated software. Alongside, various prompting techniques that carefully tailor prompts have emerged to elicit optimal responses from LLMs. Still, the interplay between such prompting strategies and secure code generation remains under-explored and calls for further investigations. Objective: In this study, we investigate the impact of different prompting techniques on the security of code generated from NL instructions by LLMs. Method: First we perform a systematic literature review to identify the existing prompting techniques that can be used for code generation tasks. A subset of these techniques are evaluated on GPT-3, GPT-3.5, and GPT-4 models for secure code generation. For this, we used an existing dataset consisting of 150 NL security-relevant code-generation prompts. Results: Our work (i) classifies potential prompting techniques for code generation (ii) adapts and evaluates a subset of the identified techniques for secure code generation tasks and (iii) observes a reduction in security weaknesses across the tested LLMs, especially after using an existing technique called Recursive Criticism and Improvement (RCI), contributing valuable insights to the ongoing discourse on LLM-generated code security.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have received major attention recently due to their high performance in solving Natural Language (NL) processing tasks. Alongside, their application to program synthesis has advanced significantly, allowing software developers to generate code from NL descriptions or prompts. Overall, this is achieved through vast training sets of code and documentation text extracted from open-source repositories. While this approach helps LLMs produce functional implementations, it offers no guarantees of correctness or quality, as it treats code simply as text, ignoring essential semantic information [41]. Moreover, open-source projects are known for containing security flaws [34, 93, 105, 106], making LLM-generated code prone to security vulnerabilities [72, 73].\nRecent investigations [98] show that developers are gradually showing a preference for AI-driven code assistants to initiate their coding process. These tools offer a valuable starting point, aiding in the development process and alleviating the need to search for information online. However, when utilizing such AI assistants powered by LLMs, developers often display an over-reliance behavior that involves optimistic assumptions regarding the correctness and security of the generated code without thorough questioning [74] [85]. Findings from a user study conducted by Perry et al. [74] revealed that participants who had access to an Al assistant tended to produce insecure solutions more frequently compared to those who did not have access to such assistance. This emphasizes the importance of exploring avenues to strengthen the security incorporated by the LLMs in the code generated by them.\nMotivation: Prompt engineering, the process of refining prompts to optimize the quality of responses generated by LLMs, has garnered significant attention following the emergence of LLMs like ChatGPT, BARD, and others. A variety of sophisticated prompting techniques have been developed for tasks such as text generation, classification, and problem-solving. Many of these techniques can be used by the end users to directly prompt or interact with LLM- powered tools and chatbots. Despite the abundance of research in this field, the correlation between such prompting strategies and secure code generation has not been thoroughly examined or documented in the existing literature. Specifically, the extent to which such techniques can guide LLMs towards producing secure implementations remains an open question. While models like GPT-3 continually advance, with each version improving upon its predecessor, the implications of these enhancements for security are unclear. This underscores the importance of investigating NL prompting techniques that have the potential to enhance the security of the code generated by LLMs.\nIn this work, we perform a literature review to identify potential prompting techniques that can be used for code generation followed by an in-depth analysis of the impact of these techniques on improving the security in LLM- generated code. For this, we elaborate on the following research questions (RQs):\nRQ1: What are the existing prompting techniques that can be used for code generation? To answer this, we performed a systematic literature review of papers that introduced different prompting techniques that can be potentially used for code generation.\nRQ2: What is the impact of different prompting techniques on the security of LLM-generated code? For this, we conducted an in-depth analysis using a subset of prompting techniques identified in the literature review. A dataset called LLMSecEval [94], containing 150 NL prompts specifying coding tasks that could potentially lead to insecure code implementations, was used for our experiments. We evaluated Python programs generated by the LLMs since it is one of the most popular choice of languages for developers. The code generated by the LLMs for the selected techniques was evaluated for security weaknesses using a static analysis tool called Bandit.\nExperiments were conducted utilizing GPT-3, GPT-3.5, and GPT-4 models, due to their widespread usage and advanced natural language processing and coding capabilities, which are crucial for exploring various prompting techniques. Our findings reaffirm the fact that LLM-generated code contains a large number of security weaknesses mainly related to CWE-78, CWE-259, CWE-94, and CWE-330. We observed that integrating different prompting techniques has a positive impact on the security of code generated by LLMs, particularly noticeable in advanced models like GPT-4. Notably, a technique known as Recursive Criticism and Improvement (RCI) has exhibited significant potential in mitigating security weaknesses in the generated code. Furthermore, we have observed distinct changes in the coding behavior of the models when security specifications are introduced to the prompts, offering insights that can be utilized to refine prompting techniques for secure code generation.\nContributions This work makes the following contributions to the field of secure code generation using LLMs:\n\u2022 To the best of our knowledge, we present the first systematic inventory of prompting techniques that are suitable for code generation. Often, papers in this field make an arbitrary selection of a few techniques, e.g., based on convenience or because other referenced papers do the same. This paper highlights that a rich selection of techniques exists and incentivizes the community to explore the alternatives in their work.\n\u2022 To simplify this exploration, we have translated a selection of these generic prompting techniques into actionable templates that can be reused by the community as is, or with some adaptations for (secure) code generation. This effort is expected to stimulate the use of the different prompting techniques, beyond the usual suspects.\n\u2022 We provide insights (and rankings) concerning the prompting techniques that are more promising for secure code generation. Interestingly, to the extent of our knowledge, the most promising technique has not been used in the related work for secure code generation (cf. the first point).\nThe rest of the paper is organized as follows: Section 2 presents the existing work on using LLMs for (secure) code generation. Section 3 and 4 present the approach used for the systematic literature review and the findings obtained from it. Following this, Sections 5 and 6 delve into the specifics of the security evaluation of code generated by LLMs using various prompting techniques and the results. Insights obtained from the results are elaborated in Section 7. Section 8 addresses the limitations, while Section 9 brings the work to a close."}, {"title": "2 RELATED WORK", "content": "This section presents prior research that delves into the use of LLMs for code generation and explores studies that assess the security aspects of code generated by LLMs."}, {"title": "2.1 Code Generation Using LLMs", "content": "There are several works (both published and unpublished) that evaluate the code generation capabilities of LLMs. The following are a few notable ones that are peer-reviewed.\nA paper by Hendrycks et al. [36] evaluated the code generated by GPT-2 [78], GPT-3 [11] and GPT-Neo using a benchmark dataset called APPS (Automated Program Progress Standard) [36] that consists of 10,000 NL coding problems along with corresponding test cases and ground truth solutions created by humans. For the evaluation, they employed the few-shot prompting technique where the model is provided with a set of <input-output> examples to demonstrate how to solve the problem. At the time of this study, they observed that the overall performance exhibited by the models was low based on the percentage of test cases passed. In another study conducted by Austin et al. [4], the authors explored the limitations of program synthesis carried out by language models trained at various scales, ranging from 244M to 137B parameters. To accomplish this, they created two datasets: the Mostly Basic Programming Problems (MBPP) dataset and the MathQA-Python dataset. The MBPP dataset comprises problem statements, simple Python functions designed to solve these problems, and three corresponding test cases. On the other hand, the MathQA-Python dataset presents mathematical problems, multiple-choice answers for these problems, and Python implementations that produce the correct answers. Both datasets are created to verify the semantic correctness of the generated Python programs. They also employed a few-shot prompting technique and their observations revealed a correlation between the increase in model size and improved performance.\nXu et al. [109] conducted a comprehensive assessment of various LLMs, including Codex [14], GPT-J, GPT-Neo, GPT-NeoX-20B [10], CodeParrot [96], and PolyCoder (a model developed by the authors of this paper) for their code generation capabilities. Their evaluation focused on these models' performance using the HumanEval [14] dataset, which contains 164 distinct coding tasks presented as prompts with corresponding test cases. These prompts consist of incomplete code snippets paired with NL comments rather than a complete NL instruction describing the task. In this study, they employed a zero-shot prompting technique. Zero-shot prompting entails not providing explicit <input-output> pairs to the LLMs to demonstrate how to approach the given task. Based on this study, Codex emerged as the top-performing model, outperforming all the other models in the evaluation.\nA study by Zeng et al. [115] tried to understand how pre-trained models perform for program understanding and generation tasks by experimenting with 8 LLMs that include CodeBERT [26], GraphCodeBERT [31], ContraCode [42], CodeGPT, PLBART [1], CodeTrans [24], CoText [75] and CodeT5 [102] mainly using the CodeXGLUE [61] benchmark. This benchmark is a collection of datasets spread across 10 different code-related tasks. The dataset used for code generation tasks within this benchmark is known as Concode. The prompts in Concode encompass NL problem descriptions, structured in the form of Java Doc comments and class environments. The researchers employed zero-shot prompting to evaluate the models. The results of their experiments indicated that CodeT5 and CodeTrans consistently delivered the highest performance in code generation tasks. In another work, an extensive literature review was conducted by Hou et al. [25] where they examine papers that present works done using LLMs for software engineering tasks. Their analysis reveals a growing emphasis on models from the GPT series, with GPT-4 [69] gaining significant attention in studies related to code generation using LLMs.\nBesides the aforementioned studies, there exist papers introducing code synthesis benchmarks like EvalPlus [60] and Multipl-E [13], which assess the code generated by various LLMs. Furthermore, the papers that introduce different LLMs capable of performing code generation [14, 16, 26, 27, 55, 102, 109] task also perform evaluation of the code generated by their respective models. The prompting techniques employed in such studies are predominantly limited to either zero-shot or few-shot prompting."}, {"title": "2.2 Security in LLM-Generated Code", "content": "As mentioned earlier, prior work has elaborated on the security of code generated by LLMs. Pearce et al. [72], for instance, used 54 high-risk security scenarios containing incomplete code snippets (C and Python) to assess code completions produced by GitHub Copilot and observed that 40% of them contained security vulnerabilities. However, a study by Asare et al. [3], compared C/C++ code generated by human developers against the ones generated by Copilot and observed that Copilot is not as bad as humans in introducing vulnerabilities in code. The experiments in these studies were done using zero-shot prompts. In another work by Pearce et al. [73], they tested the code repair capabilities of LLMs using various program repair scenarios. Overall, they concluded that Codex and Jurassic-1 [56] are capable of finding fixes for simple scenarios again under zero-shot settings. Jesse et al. [43] did a recent study where they examined if Codex and other LLMs generate simple, stupid bugs (SStuBs) and found that these models produce twice as many SStuBs as correct code. On the other hand, [35] proposed a learning approach for controlled code generation called SVEN. Such an approach, in which a boolean parameter is passed to enforce secure/insecure code generation, increased the number of secure code produced by an LLM called CodeGen by 25%. Another study by Yeti\u015ftiren et al. [113] assessed the quality (i.e., validity, correctness, reliability, security, and maintainability) of code generated by Copilot, Amazon CodeWhisperer, and ChatGPT using the HumanEval dataset. Notably, no significant security vulnerabilities were found in the generated code. However, the authors acknowledge the limitations of their security evaluation, since the HumanEval dataset is designed to verify functional correctness rather than code security.\nDelving further into the realm of secure code generation using LLMs, Sandoval et al. [84] investigated the impact of LLM on code security through a user study. The study involved 58 computer science students who were tasked with performing simple operations on a linked list using C programming language with a focus on memory-based vulnerabilities. They observed that the participants who used an AI assistant powered by Codex introduced security- critical bugs at a rate no higher than 10% when compared to the control group indicating that the use of LLMs does not introduce new vulnerabilities. Nevertheless, it is essential to acknowledge that these findings may not be universally applicable to more complex programming tasks. Contrary to the previous study Perry et al. [74] observed different results in a study that explored developers' interactions with Al code assistants concerning security. Forty-seven participants were engaged with an AI assistant powered by Codex to fulfill five security-related programming tasks across Python, JavaScript, and C. The findings revealed that participants utilizing AI assistants were prone to generating insecure solutions more often than those without AI assistance in four out of five tasks. Typical issues encompassed the selection of unsafe libraries, incorrect library utilization, insufficient comprehension of edge cases involving external entities like file systems or databases, and inadequate sanitization of user input.\nAdditionally, apart from empirical and user studies on LLMs, a systematic literature review conducted by Yao et al. [112], delves into the use of LLMs for security and privacy purposes. Their findings indicate a plethora of works employing LLMs in security-related tasks, such as coding, test case generation, bug detection, vulnerability detection, and fixing. These endeavors have positively influenced research within the security community. However, none of these studies thoroughly investigate different prompting techniques to enhance the secure code generation process."}, {"title": "3 METHODOLOGY FOR SYSTEMATIC LITERATURE REVIEW", "content": "The goal of this review is to find prompting techniques that can be used for code-generation tasks using LLMs. However, there are only a limited number of prompting techniques explicitly designed for code generation. Therefore, we opted to review all prompting techniques introduced for generating textual content, presuming their potential transferability to code-generation tasks, given that code generation falls within the domain of textual content generation. The steps followed to perform the literature review are depicted in Figure 1. We used the Publish or Perish tool [33] to retrieve papers from Google Scholar. Following the PICOC strategy [12], the search query given below was employed to retrieve the relevant papers that introduce prompting techniques for textual content generation.\nprompt* AND (engineer* OR pattern* OR technique*) AND (language model OR pre-trained model* OR 11m* OR ptm*)"}, {"title": "Paper Screening", "content": "The review process was done in two screening steps. In the first screening, we looked at the title and abstract of the paper to decide if it was relevant to our study. If it is then it was shortlisted for the second screening. In the second screening, we looked into the full paper to decide if it fits our criteria. The first and second screening was done based on the following inclusion and exclusion criteria.\nInclusion Criteria:\nIC1: Paper deals with prompting LLMs using one or more techniques\nIC2: Paper is published since 2018:\nIC3: Paper is written in English\nExclusion Criteria:\nEC1: Paper does not introduce new prompting techniques to query LLM\nEC2: Paper deals with the generation of anything other than text and code (e.g: image, speech, and video data)\nEC3: Paper that presents prompting techniques that can not be used for generation tasks (e.g. techniques specific to classification tasks)\nEC4: Paper that presents automated prompt optimization techniques and frameworks (e.g. prompt tuning and black-box tuning)\nEC5: Paper that presents prompting technique for attacking the model (e.g. jailbreak prompts)\nEC6: Out of scope (e.g. techniques for medical science)"}, {"title": "Snowballing", "content": "To ensure that we did not miss any other relevant papers, we also performed 3 rounds of backward snowballing [107]. Here we went through the references of the selected papers iteratively following the same two-step screening process as above until no new papers were obtained. From this, we obtained 5 additional relevant ones making the total number of relevant papers 13. Three papers under consideration were released on preprint servers like arXiv and have not undergone formal peer review. However, these preprint papers have been frequently cited with the least number of citations being 48. Hence we decided to retain those papers."}, {"title": "Knowledge Extraction", "content": "Each final paper that introduced a prompting technique suitable for code generation was examined in detail. The primary objective was to extract the techniques themselves and pinpoint their key features. For this, we performed a lightweight thematic analysis with open coding as it offers a qualitative method for analyzing textual or qualitative data to interpret patterns or themes within the data [92][108]. During this process, the first author extracted codes related to prompting techniques following an inductive approach. The themes that emerged from this coding were then discussed with two other authors to categorize and label the techniques.\nIn addition to this, attention was also directed towards details such as the LLMs on which the technique was tested, the specific tasks used for evaluation, and the datasets employed for this purpose. Furthermore, data regarding the year of publication, venue, and citation count at the time of the study were also extracted. This was aimed at creating a consolidated source of information beneficial to researchers and developers delving into prompting techniques for code generation."}, {"title": "4 PROMPTING TECHNIQUES FOR CODE GENERATION (RQ1)", "content": "In this section, we present an overview of the selected prompting techniques from the SLR that are deemed suitable for code-generation tasks. Throughout our review, we encountered numerous prompting techniques. However, not all of them were selected to be in our final list as determined by our exclusion criteria. All results of this literature review, along with the techniques that were excluded from our consideration and the reason for their exclusion are documented in our replication package specified in Section 10."}, {"title": "4.1 Overview of the Selected Papers", "content": "The information extracted from the 13 papers is presented in Table 1. The chosen papers are those that introduce novel prompting techniques. Among these, we identified 15 distinct techniques designed for textual content generation with potential applicability to code-generation tasks. Ten of these papers have undergone peer review, while the remaining have received at least 48 citations. Except for two papers [81][104], all have conducted experimental validation of their introduced prompting techniques. Only two of them [63] [44] have evaluated their techniques specifically for code generation tasks. The other techniques primarily target various reasoning tasks such as symbolic, logical, commonsense, and arithmetic. Among the papers that conducted experimental validation, ten out of eleven utilize OpenAI models indicating the prevalence of these models in research in this field.\nBased on commonalities derived from the thematic analysis, we have labeled the techniques using 3 distinct properties related to their execution as shown in Table 1. They are Single/Multi-step, Demonstrative/Non-demonstrative and Linear/Parallel. A technique that prompts the model in a single step, obtaining the final output with just one prompt, is referred to as a single-step technique. Conversely, a technique requiring multiple prompts to generate the final output is termed a multi-step technique. Single-step techniques are cost-effective compared to multi-step techniques as they necessitate only one prompt. Among the 15 techniques identified, 6 are single-step techniques, while the rest are multi-step techniques. If a technique is executed by providing demonstrative examples of inputs and expected outputs for prompting the model, it is categorized as a demonstrative technique. Conversely, a technique not requiring input-output examples is labeled as a non-demonstrative technique. Six out of 15 techniques are non-demonstrative. Although demonstrative techniques may potentially yield desired outputs more effectively than non-demonstrative techniques, this depends on the availability of high-quality demonstrative examples. In real-world scenarios, especially in complex code generation tasks, obtaining such examples can be challenging. Most techniques in our inventory involve conducting a single sequential interaction with the LLM. Here, the model is prompted, and its response is either used as the final output or serves as a basis for proceeding to the next step of prompting. These techniques are labeled as linear."}, {"title": "4.2 Classification of Prompting Techniques", "content": "Aside from the labels provided in Table 1 (single/multi-step, demonstrative/non-demonstrative and linear/parallel), we also identified some other common characteristics based on the strategic design of different prompting techniques which we used to classify them into 5 different categories as shown in Figure 2. Below we describe the categories and the techniques that belong to them, accompanied by demonstrations of how these techniques can be utilized for code generation tasks. The responses of the LLM depicted in these demonstrations were generated by ChatGPT (GPT-3.5), which is a conversational chatbot, in response to different prompting techniques."}, {"title": "4.2.1 Root Techniques", "content": "These are the foundational and most popular techniques based on which more advanced techniques are built. Zero-shot, one-shot, and few-shot prompting come under this category.\nZero-shot: In this technique a model is asked to perform a task without task-specific training or examples at the time of inference [11]. In such cases, the model relies completely on the data it has seen during its pre-training to generate an appropriate response. In conversational LLMs such as ChatGPT, zero-shot prompting is possibly the most commonly used way of interaction by an average user. It has the advantage of not having to prepare a task-specific dataset of input-output demonstrations to generate desirable output. However, if the model has not seen data related to the task at hand in its training, then the performance of the model can be suboptimal with zero-shot prompting. This technique can be directly used for code generation tasks. Figure 3 includes a demonstration of zero-shot prompting for a simple coding task and ChatGPT's response to it.\nOne-shot/Few-shot: One-shot and few-shot prompting techniques [11] are very similar to each other. In one-shot prompting, the model is given a single input-output example whereas in the few-shot prompting the model is given examples of the task at inference time as conditioning before providing the final input for which it is expected to produce the output. By supplying the model with both input and corresponding output samples, it gains the benefit of producing a response that closely aligns with the desired format. However, it can be a disadvantage when one does not have sufficient or relevant task-specific data in advance. An illustration demonstrating the application of few-shot prompting on ChatGPT for code generation can be seen in Figure 3. This example utilizes two few-shot examples. We have omitted a separate example of one-shot prompting since it closely resembles few-shot prompting but with only one demonstrative example."}, {"title": "4.2.2 Refinement-based Techniques", "content": "Techniques belonging to this category focus on improving, refining, or iterating the model outputs. They might involve feedback loops, user interactions, or model self-assessment to enhance the quality of the generated responses. The prompting techniques that come under this category include Recursive Criticism and Improvement (RCI), Self-refine, and Progressive Hint prompting.\nRCI: This prompting technique [47] is built on the understanding that LLMs possess a strong capability to evaluate and recognize flaws in their own output. This technique involves a two-step process in addition to providing the initial input task. Firstly, the LLM is prompted to analyze and critique its current response (for instance: \"Review your previous answer and find problems with your answer\"). Subsequently, drawing from the critiques it has outlined, the LLM is then instructed to rectify the identified issues and revise its output accordingly (for example: \"Based on the problems you found, improve your answer\"). This two-step process is repeated until a satisfactory output is obtained or until a predefined number of iterations is done. RCI has the advantage that it needs no task-specific expert data to generate desirable responses. However, this approach can be expensive due to the iterative nature of the process. An added disadvantage is that the success of this approach relies on the ability of the model to identify its own mistakes. A demonstration of one iteration of this technique used for a code generation task is shown in Figure 4.\nSelf-refine: This technique [63] is very similar to RCI. It uses 2 steps called feedback and refine in addition to an initial output generation step to generate high-quality output. The initial output from model M is generated using a task-specific prompt $\\rho_{gen}$ with few-shot <input, output> example pairs. Next, they use a prompt $\\rho_{fb}$ to generate feedback for the previously generated output by M. Few-shot examples are provided in this step in the form of <input, output, feedback> triplets. The next step is to refine the output based on the generated feedback. This is done using prompt $\\rho_{refine}$ that contains few-shot examples of refining outputs in the form of <input, output, feedback, refined> quadruples. Since this approach is very similar to the RCI (Figure 4) with the exception of providing few-shot examples with every step, a separate demonstration is not provided here.\nProgressive Hint: Progressive Hint prompting (PHP) [117] is another technique that iteratively refines the output from the LLM by providing increasingly informative hints in each iteration. The pipeline of this approach is divided into two stages. The first stage is called base answer and base prompt. In this stage, the model is provided with an input task with a basic prompt to which a base answer is generated.\nThe second stage is called subsequent answer and PHP where the base prompt is combined with hints that are extracted from the previous answers (or base answer in this case). This is repeated until the answers from the model do not change. Figure 4 shows the interaction with ChatGPT for a simple coding task using this technique. PHP can be combined with standard zero-shot prompting or sophisticated techniques such as CoT. This approach requires at least 2 iterations. The approach is not considered successful until the last 2 outputs from the model are the same. This can become computationally expensive based on the task and the model. Additionally, the model can be misled if the hints provided stray too far away from the correct answer [117]. This approach can be theoretically used for code generation tasks as shown in Figure 4."}, {"title": "4.2.3 Decomposition-based Techniques", "content": "Techniques in this category break down complex tasks or prompts into simpler, more manageable pieces. Here, the language models perform multiple small tasks to incrementally build towards the final, complex solution, facilitating more accurate responses. The techniques under this category include least-to-most and self-planning prompting.\nLeast-to-most: This prompting technique [119] is executed in two stages. In the decomposition stage, the model is prompted to decompose the complex task into smaller sub-tasks. This prompt is delivered using a few-shot approach, where a few examples are presented to illustrate how larger tasks can be dissected into sub-tasks, followed by the actual complex task that needs to be addressed. The second stage is the sub-problem solving stage where the model is asked to sequentially solve all the sub-problems or sub-tasks identified in the decomposition stage. Here also, few-shot examples demonstrating how sub-problems are solved are provided. Responses derived from solving each sub-task are integrated back into the original task description before presenting the subsequent sub-task to the model. This iterative process continues until all sub-tasks have been resolved, resulting in the final solution.\nLeast-to-most prompting technique can also be used in combination with CoT or self-consistency prompting techniques. Similar to other advanced prompting methods, this technique's drawback is the necessity to supply few-shot examples for both the decomposition of a complex task and the resolution of its sub-tasks. The resource demands can escalate with the increasing number of sub-tasks involved in the process. This approach can also be potentially used for code generation provided you have a sufficient dataset containing information on coding problem decompositions and solutions. Figure 5 demonstrates how this technique can be used for code generation.\nSelf-planning: This prompting approach [44] is specifically designed for code generation problems. Hence no additional adaptation is required to tailor the technique for code generation tasks. Self-planning is carried out in two phases. The first one is the planning phase where the code generation task is decomposed into a plan of actions. This decomposition is done by the LLM itself. The LLM is provided with demonstrative examples of how to come up with plans to solve coding tasks before asking it to generate a plan for the task at hand. The action plan is structured as an ordered list of steps. The plan should always conclude with a return statement. The second phase is called the implementation phase wherein the LLM's formulated plan is integrated with the original task prompt. This integration prompts the LLM to adhere to its own outlined strategy when producing the final code snippet. An example demonstration of this prompting technique is shown in Figure 6. This example is directly taken from the original paper itself."}, {"title": "4.2.4 Reasoning-based Techniques", "content": "Techniques that guide the model to employ and demonstrate logical reasoning for generating responses are categorized as reasoning-based techniques. Reasoning encompasses the act of drawing logical conclusions, evaluating arguments, and making inferences using the information at hand [39]. These methods emphasize the model's ability to engage in cognitive and logical processes. Rather than simplifying a task as in the case of decomposition-based techniques, these techniques encourage the model to follow a logical reasoning path and articulate its thought process. The techniques that come under this category are Chain-of-Thought, Zero shot Chain-of-Thought, Self-consistency and Few-shot with Explanation.\nChain-of-Thought (CoT): In this prompting approach [103], the LLM is compelled to produce a sequence of intermediary logical reasoning steps in natural language, culminating in the solution to the presented problem. The goal of this approach is to replicate how humans solve a complex problem following a chain of reasoning or justification steps. In this method, the model is initially given a set of few-shot examples, consisting of <input, chain of thought, output> triplets, to guide its understanding before it tackles the actual task. This technique has been evaluated on various benchmarks including arithmetic, common sense, and symbolic reasoning. However, one can assume that CoT can also be applied to code generation tasks. Figure 7 demonstrates the CoT prompting technique for code generation.\nAn approach similar to this was proposed in 2017 by Ling et al. [59] where they train an attention-based sequence-to- sequence model to solve complex mathematical problems using a dataset containing problems with answer rationales and the final correct answers. However, this approach focused on training rather than explicitly prompting a model, and it did not involve an LLM. Hence we identify CoT as a novel prompting technique.\nZero-shot CoT: This approach [48] addresses the limitations of the CoT approach, which requires task-specific reasoning examples. Zero-shot CoT prompting is carried out in two stages. The first one is the reasoning extraction stage where the model is prompted to generate the logical reasoning for handling a given input task. Here the initial input task is appended with a hand-crafted trigger sentence to extract the chain of thought reasoning from the model. From the evaluation conducted by the authors, the trigger phrase Let's think step by step yields the best results. The second stage is the answer extraction stage where the model is supplied with the initial input task, the reasoning trigger sentence, the step-by-step reasoning generated by the model, and another hand-crafted trigger sentence to extract the final answer. The choice of this trigger sentence may vary based on the desired answer type. For example, for a mathematical problem, a prompt like \"Therefore, the answer (Arabic numerals) is\" nudges the model towards providing a numeric response. Since the prompt template of this technique varies very minimally across tasks, zero-shot CoT is considered a task-agnostic approach. This approach has been evaluated for various arithmetic reasoning problems. An example of applying this technique for code generation is included in Figure 7. Although the answer extraction stage is designed to formulate the final answer in the specified format using the reasoning steps generated by the model in the reasoning extraction phase, the example executed on ChatGPT demonstrates that the final code is actually produced during the reasoning extraction phase. Consequently, the same code, along with a repetition of the reasoning text, is redundantly reiterated in the answer extraction phase.\nSelf-consistency/Complexity-based: Self-consistency [101] and complexity-based [28] prompting techniques are similar to each other and are built on top of the CoT technique. They use a sample-and-marginalize decoding strategy to generate more reliable output compared to that CoT. In self-consistency, the model is provided with an input task along with a set of chain-of-thought few-shot examples (<input, reasoning, output>). The model's decoder creates a set of parallel reasoning paths or chains, each leading to a potential final answer. Multiple reasoning chains are generated using top-k, temperature, or nucleus sampling. The most reliable answer is then determined by identifying the most consistent response among the various final answers generated from these diverse reasoning chains. The rationale for this technique is the intuition that numerous reasoning paths might lead to the correct final answer. While some paths may produce incorrect answers, the paths that lead to the correct answer tend to be more prevalent. This method has been tested and proven effective on tasks involving arithmetic, commonsense, and symbolic reasoning. Complexity-based prompting also adopts a similar approach but posits that chains involving more reasoning steps yield better performance. Consequently, this technique emphasizes using chain-of-thought few-shot examples comprising a greater number of reasoning steps (i.e., more complexity). Furthermore, the final answer is chosen based on the consistency among responses with a larger number of reasoning steps, while responses with fewer reasoning steps are discarded.\nBoth techniques are particularly well-suited for tasks that have a definitive final answer, as opposed to more creative tasks like code generation. However, they can still be applied to code generation tasks. A demonstration of adapting self-consistency for code generation is included in Figure 8. A separate demonstration of complexity-based technique is not provided as the prompting approach followed is very similar except for the number of reasoning steps. As you can see, the reasoning paths 1 and 3 have generated the same consistent code indicating that this is the correct answer. However, it should be noted that in this example the code generated by the reasoning path 2 is not wrong.\nFew-shot with Explanation. As the name indicates, this technique [54] uses few-shot input-output examples with a task instruction with additional explanations for each of the examples. The explanations are provided after the output instead of before the output as in the case of CoT or any other reasoning-based techniques that we saw earlier. They evaluated this approach on several reasoning and inference-based tasks such as causality reasoning, mathematical induction, and inferring presupposition behind an utterance. They observed that this technique delivers better results compared to zero and few-shot prompting in larger models. An adaptation of this technique for a code generation task is also shown in Figure 8."}, {"title": "4.2.5 Priming Techniques", "content": "A recent work on prompt engineering by White et al. [104"}]}]}