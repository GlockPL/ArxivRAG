{"title": "Prompting Techniques for Secure Code Generation: A Systematic Investigation", "authors": ["CATHERINE TONY", "NICOL\u00c1S E. D\u00cdAZ FERREYRA", "MARKUS MUTAS", "SALEM DHIFF", "RICCARDO SCANDARIATO"], "abstract": "Large Language Models (LLMs) are gaining momentum in software development with prompt-driven programming enabling developers\nto create code from natural language (NL) instructions. However, studies have questioned their ability to produce secure code and,\nthereby, the quality of prompt-generated software. Alongside, various prompting techniques that carefully tailor prompts have\nemerged to elicit optimal responses from LLMs. Still, the interplay between such prompting strategies and secure code generation\nremains under-explored and calls for further investigations. Objective: In this study, we investigate the impact of different prompting\ntechniques on the security of code generated from NL instructions by LLMs. Method: First we perform a systematic literature\nreview to identify the existing prompting techniques that can be used for code generation tasks. A subset of these techniques are\nevaluated on GPT-3, GPT-3.5, and GPT-4 models for secure code generation. For this, we used an existing dataset consisting of 150 NL\nsecurity-relevant code-generation prompts. Results: Our work (i) classifies potential prompting techniques for code generation (ii)\nadapts and evaluates a subset of the identified techniques for secure code generation tasks and (iii) observes a reduction in security\nweaknesses across the tested LLMs, especially after using an existing technique called Recursive Criticism and Improvement (RCI),\ncontributing valuable insights to the ongoing discourse on LLM-generated code security.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have received major attention recently due to their high performance in solving\nNatural Language (NL) processing tasks. Alongside, their application to program synthesis has advanced significantly,\nallowing software developers to generate code from NL descriptions or prompts. Overall, this is achieved through vast"}, {"title": "2 RELATED WORK", "content": "This section presents prior research that delves into the use of LLMs for code generation and explores studies that\nassess the security aspects of code generated by LLMs."}, {"title": "2.1 Code Generation Using LLMs", "content": "There are several works (both published and unpublished) that evaluate the code generation capabilities of LLMs. The\nfollowing are a few notable ones that are peer-reviewed.\nA paper by Hendrycks et al. [36] evaluated the code generated by GPT-2 [78], GPT-3 [11] and GPT-Neo using a\nbenchmark dataset called APPS (Automated Program Progress Standard) [36] that consists of 10,000 NL coding problems\nalong with corresponding test cases and ground truth solutions created by humans. For the evaluation, they employed\nthe few-shot prompting technique where the model is provided with a set of <input-output> examples to demonstrate\nhow to solve the problem. At the time of this study, they observed that the overall performance exhibited by the models\nwas low based on the percentage of test cases passed. In another study conducted by Austin et al. [4], the authors\nexplored the limitations of program synthesis carried out by language models trained at various scales, ranging from\n244M to 137B parameters. To accomplish this, they created two datasets: the Mostly Basic Programming Problems\n(MBPP) dataset and the MathQA-Python dataset. The MBPP dataset comprises problem statements, simple Python\nfunctions designed to solve these problems, and three corresponding test cases. On the other hand, the MathQA-Python\ndataset presents mathematical problems, multiple-choice answers for these problems, and Python implementations\nthat produce the correct answers. Both datasets are created to verify the semantic correctness of the generated Python"}, {"title": "2.2 Security in LLM-Generated Code", "content": "As mentioned earlier, prior work has elaborated on the security of code generated by LLMs. Pearce et al. [72], for\ninstance, used 54 high-risk security scenarios containing incomplete code snippets (C and Python) to assess code\ncompletions produced by GitHub Copilot and observed that 40% of them contained security vulnerabilities. However,\na study by Asare et al. [3], compared C/C++ code generated by human developers against the ones generated by\nCopilot and observed that Copilot is not as bad as humans in introducing vulnerabilities in code. The experiments in\nthese studies were done using zero-shot prompts. In another work by Pearce et al. [73], they tested the code repair\ncapabilities of LLMs using various program repair scenarios. Overall, they concluded that Codex and Jurassic-1 [56]\nare capable of finding fixes for simple scenarios again under zero-shot settings. Jesse et al. [43] did a recent study\nwhere they examined if Codex and other LLMs generate simple, stupid bugs (SStuBs) and found that these models\nproduce twice as many SStuBs as correct code. On the other hand, [35] proposed a learning approach for controlled"}, {"title": "3 METHODOLOGY FOR SYSTEMATIC LITERATURE REVIEW", "content": "The goal of this review is to find prompting techniques that can be used for code-generation tasks using LLMs. However,\nthere are only a limited number of prompting techniques explicitly designed for code generation. Therefore, we opted\nto review all prompting techniques introduced for generating textual content, presuming their potential transferability\nto code-generation tasks, given that code generation falls within the domain of textual content generation. The steps\nfollowed to perform the literature review are depicted in Figure 1. We used the Publish or Perish tool [33] to retrieve\npapers from Google Scholar. Following the PICOC strategy [12], the search query given below was employed to retrieve\nthe relevant papers that introduce prompting techniques for textual content generation.\nprompt* AND (engineer* OR pattern* OR technique*) AND (language model OR pre-trained\nmodel* OR 11m* OR ptm*)"}, {"title": "Paper Screening", "content": "The review process was done in two screening steps. In the first screening, we looked at the title and\nabstract of the paper to decide if it was relevant to our study. If it is then it was shortlisted for the second screening. In\nthe second screening, we looked into the full paper to decide if it fits our criteria. The first and second screening was\ndone based on the following inclusion and exclusion criteria.\nInclusion Criteria:\nIC1: Paper deals with prompting LLMs using one or more techniques\nIC2: Paper is published since 2018:\nIC3: Paper is written in English\nExclusion Criteria:\nEC1: Paper does not introduce new prompting techniques to query LLM\nEC2: Paper deals with the generation of anything other than text and code (e.g: image, speech, and video data)\nEC3: Paper that presents prompting techniques that can not be used for generation tasks (e.g. techniques specific to\nclassification tasks)\nEC4: Paper that presents automated prompt optimization techniques and frameworks (e.g. prompt tuning and black-box\ntuning)\nEC5: Paper that presents prompting technique for attacking the model (e.g. jailbreak prompts)\nEC6: Out of scope (e.g. techniques for medical science)"}, {"title": "Snowballing", "content": "To ensure that we did not miss any other relevant papers, we also performed 3 rounds of backward\nsnowballing [107]. Here we went through the references of the selected papers iteratively following the same two-step\nscreening process as above until no new papers were obtained. From this, we obtained 5 additional relevant ones\nmaking the total number of relevant papers 13. Three papers under consideration were released on preprint servers like\narXiv and have not undergone formal peer review. However, these preprint papers have been frequently cited with the\nleast number of citations being 48. Hence we decided to retain those papers."}, {"title": "Knowledge Extraction", "content": "Each final paper that introduced a prompting technique suitable for code generation was\nexamined in detail. The primary objective was to extract the techniques themselves and pinpoint their key features. For\nthis, we performed a lightweight thematic analysis with open coding as it offers a qualitative method for analyzing"}, {"title": "4 PROMPTING TECHNIQUES FOR CODE GENERATION (RQ1)", "content": "In this section, we present an overview of the selected prompting techniques from the SLR that are deemed suitable\nfor code-generation tasks. Throughout our review, we encountered numerous prompting techniques. However, not\nall of them were selected to be in our final list as determined by our exclusion criteria. All results of this literature\nreview, along with the techniques that were excluded from our consideration and the reason for their exclusion are\ndocumented in our replication package specified in Section 10."}, {"title": "4.1 Overview of the Selected Papers", "content": "The information extracted from the 13 papers is presented in Table 1. The chosen papers are those that introduce novel\nprompting techniques. Among these, we identified 15 distinct techniques designed for textual content generation with\npotential applicability to code-generation tasks. Ten of these papers have undergone peer review, while the remaining\nhave received at least 48 citations. Except for two papers [81][104], all have conducted experimental validation of their\nintroduced prompting techniques. Only two of them [63] [44] have evaluated their techniques specifically for code\ngeneration tasks. The other techniques primarily target various reasoning tasks such as symbolic, logical, commonsense,\nand arithmetic. Among the papers that conducted experimental validation, ten out of eleven utilize OpenAI models\nindicating the prevalence of these models in research in this field.\nBased on commonalities derived from the thematic analysis, we have labeled the techniques using 3 distinct\nproperties related to their execution as shown in Table 1. They are Single/Multi-step, Demonstrative/Non-demonstrative\nand Linear/Parallel. A technique that prompts the model in a single step, obtaining the final output with just one\nprompt, is referred to as a single-step technique. Conversely, a technique requiring multiple prompts to generate the final\noutput is termed a multi-step technique. Single-step techniques are cost-effective compared to multi-step techniques\nas they necessitate only one prompt. Among the 15 techniques identified, 6 are single-step techniques, while the rest\nare multi-step techniques. If a technique is executed by providing demonstrative examples of inputs and expected\noutputs for prompting the model, it is categorized as a demonstrative technique. Conversely, a technique not requiring\ninput-output examples is labeled as a non-demonstrative technique. Six out of 15 techniques are non-demonstrative.\nAlthough demonstrative techniques may potentially yield desired outputs more effectively than non-demonstrative\ntechniques, this depends on the availability of high-quality demonstrative examples. In real-world scenarios, especially\nin complex code generation tasks, obtaining such examples can be challenging. Most techniques in our inventory\ninvolve conducting a single sequential interaction with the LLM. Here, the model is prompted, and its response is either\nused as the final output or serves as a basis for proceeding to the next step of prompting. These techniques are labeled\nas linear."}, {"title": "4.2 Classification of Prompting Techniques", "content": "Aside from the labels provided in Table 1 (single/multi-step, demonstrative/non-demonstrative and linear/parallel), we\nalso identified some other common characteristics based on the strategic design of different prompting techniques\nwhich we used to classify them into 5 different categories as shown in Figure 2. Below we describe the categories and\nthe techniques that belong to them, accompanied by demonstrations of how these techniques can be utilized for code\ngeneration tasks. The responses of the LLM depicted in these demonstrations were generated by ChatGPT (GPT-3.5),\nwhich is a conversational chatbot, in response to different prompting techniques."}, {"title": "4.2.1 Root Techniques", "content": "These are the foundational and most popular techniques based on which more advanced\ntechniques are built. Zero-shot, one-shot, and few-shot prompting come under this category.\nZero-shot: In this technique a model is asked to perform a task without task-specific training or examples at the time\nof inference [11]. In such cases, the model relies completely on the data it has seen during its pre-training to generate\nan appropriate response. In conversational LLMs such as ChatGPT, zero-shot prompting is possibly the most commonly\nused way of interaction by an average user. It has the advantage of not having to prepare a task-specific dataset of\ninput-output demonstrations to generate desirable output. However, if the model has not seen data related to the task\nat hand in its training, then the performance of the model can be suboptimal with zero-shot prompting. This technique\ncan be directly used for code generation tasks. Figure 3 includes a demonstration of zero-shot prompting for a simple\ncoding task and ChatGPT's response to it.\nOne-shot/Few-shot: One-shot and few-shot prompting techniques [11] are very similar to each other. In one-shot\nprompting, the model is given a single input-output example whereas in the few-shot prompting the model is given"}, {"title": "4.2.2 Refinement-based Techniques", "content": "Techniques belonging to this category focus on improving, refining, or\niterating the model outputs. They might involve feedback loops, user interactions, or model self-assessment to enhance\nthe quality of the generated responses. The prompting techniques that come under this category include Recursive\nCriticism and Improvement (RCI), Self-refine, and Progressive Hint prompting.\nRCI: This prompting technique [47] is built on the understanding that LLMs possess a strong capability to evaluate\nand recognize flaws in their own output. This technique involves a two-step process in addition to providing the initial\ninput task. Firstly, the LLM is prompted to analyze and critique its current response (for instance: \"Review your previous\nanswer and find problems with your answer\"). Subsequently, drawing from the critiques it has outlined, the LLM is\nthen instructed to rectify the identified issues and revise its output accordingly (for example: \"Based on the problems\nyou found, improve your answer\"). This two-step process is repeated until a satisfactory output is obtained or until a\npredefined number of iterations is done. RCI has the advantage that it needs no task-specific expert data to generate\ndesirable responses. However, this approach can be expensive due to the iterative nature of the process. An added\ndisadvantage is that the success of this approach relies on the ability of the model to identify its own mistakes. A\ndemonstration of one iteration of this technique used for a code generation task is shown in Figure 4.\nSelf-refine: This technique [63] is very similar to RCI. It uses 2 steps called feedback and refine in addition to an\ninitial output generation step to generate high-quality output. The initial output from model M is generated using\na task-specific prompt $p_{gen}$ with few-shot <input, output> example pairs. Next, they use a prompt $p_{fb}$ to generate\nfeedback for the previously generated output by M. Few-shot examples are provided in this step in the form of <input,"}, {"title": "4.2.3 Decomposition-based Techniques", "content": "Techniques in this category break down complex tasks or prompts into\nsimpler, more manageable pieces. Here, the language models perform multiple small tasks to incrementally build"}, {"title": "4.2.4 Reasoning-based Techniques", "content": "Techniques that guide the model to employ and demonstrate logical reasoning\nfor generating responses are categorized as reasoning-based techniques. Reasoning encompasses the act of drawing\nlogical conclusions, evaluating arguments, and making inferences using the information at hand [39]. These methods\nemphasize the model's ability to engage in cognitive and logical processes. Rather than simplifying a task as in the\ncase of decomposition-based techniques, these techniques encourage the model to follow a logical reasoning path\nand articulate its thought process. The techniques that come under this category are Chain-of-Thought, Zero shot\nChain-of-Thought, Self-consistency and Few-shot with Explanation.\nChain-of-Thought (CoT): In this prompting approach [103], the LLM is compelled to produce a sequence of\nintermediary logical reasoning steps in natural language, culminating in the solution to the presented problem. The goal\nof this approach is to replicate how humans solve a complex problem following a chain of reasoning or justification\nsteps. In this method, the model is initially given a set of few-shot examples, consisting of <input, chain of thought,\noutput> triplets, to guide its understanding before it tackles the actual task. This technique has been evaluated on\nvarious benchmarks including arithmetic, common sense, and symbolic reasoning. However, one can assume that CoT\ncan also be applied to code generation tasks. Figure 7 demonstrates the CoT prompting technique for code generation.\nAn approach similar to this was proposed in 2017 by Ling et al. [59] where they train an attention-based sequence-to-\nsequence model to solve complex mathematical problems using a dataset containing problems with answer rationales\nand the final correct answers. However, this approach focused on training rather than explicitly prompting a model,\nand it did not involve an LLM. Hence we identify CoT as a novel prompting technique."}, {"title": "4.2.5 Priming Techniques", "content": "A recent work on prompt engineering by White et al. [104] proposed a catalog of\ntechniques to better converse with LLMs. They presented 16 task-agnostic prompt patterns that can be used to drive\na more meaningful conversation and deliver more acceptable results. These patterns are designed to pre-program\nLLMs before prompting them with a task. These patterns have not undergone experimental validation, nor have"}, {"title": "5 SECURITY EVALUATION OF PROMPTING TECHNIQUES: METHODOLOGY", "content": "From the SLR, we obtained a list of prompting techniques that can be used for code generation as shown in Section\n4. However, the goal of this research is to understand the impact of different prompting techniques on secure code\ngeneration. Following this, we decided to examine the prompting techniques listed earlier, to understand the impact\nthey have on improving security in LLM-generated code. In this section, first, we provide the details on the dataset and\nthe models used for our evaluation. After that, we present the methodology followed to decide the suitability of the\nprompting techniques for further examination and the subsequent security analysis of LLM-generated code using the\nselected techniques. The methodology is depicted in Figure 10."}, {"title": "5.1 Dataset and Models", "content": "For the evaluation of prompting techniques to generate secure code, a dataset of coding tasks that are designed to\nevaluate code security was required. To the extent of our knowledge, there are two peer-reviewed datasets designed for\nsecurity evaluation. SecurityEval [87] is one such dataset, comprising 121 coding tasks. However, it is unsuitable for the\npurpose of this study as it lacks NL prompts and instead contains incomplete code snippets. Tony et al. [94] created\nLLMSecEval, a dataset designed specifically for assessing the security of code generated by LLMs. LLMSecEval consists\nof 150 NL prompts covering 18 of the Top 25 CWEs (Common Weakness Enumeration) from 2021. An NL prompt in this\ncontext is a query or a description written in natural language that defines a programming task. Each coding task is\ndesigned to lead to a code that is potentially vulnerable to one of the 18 CWEs if a naive implementation is used. This is\na suitable dataset for this study as it contains a set of NL prompts describing vulnerability-prone coding tasks. Hence,\nwe selected this dataset as the foundation for our research.\nInitially, we tested several LLM candidates to determine the suitable ones for our study. We sought models with\nstrong capabilities in both natural language processing and code generation. Our selection encompassed popular LLMs\nsuch as CodeBERT, CodeGen, CodeT5, GPT-3, GPT-3.5, GPT-4, and LLAMA [95]. Nevertheless, we noticed that the"}, {"title": "5.2 Selection of Prompting Techniques", "content": "As shown in Figure 10, we conducted an initial screening to decide the suitability of prompting techniques for a more\ndetailed analysis of their impact on generating secure code. The steps followed in this initial screening process are\npresented below."}, {"title": "5.2.1 Qualification Criteria", "content": "In step 1, we set a condition the prompting techniques should satisfy in order to\nqualify for an in-depth analysis. The condition requires the technique to be non-demonstrative in nature, i.e., it should\nnot involve providing input-output examples. Our main objective is to assess techniques suitable for developers of\nall security expertise levels, intended for everyday programming scenarios such as work environments. Expecting\ndevelopers to supply input-output examples for secure code generation would be counterproductive, as it assumes a deep"}, {"title": "5.2.2 Pre-study", "content": "To ensure the feasibility of the prompting techniques for in-depth experimentation, as part of step\n\u2461, we used five randomly selected NL coding tasks from the LLMSecEval dataset and generated code using one of the\nLLMs (GPT-3) employing the techniques that met the qualification criteria in the previous step. This was necessary to\nverify if the techniques, when provided with complex coding tasks, led to practical challenges such as failure to meet\nthe exit condition to end the prompting process or unsuccessful code generation. In Step \u2462 we manually assessed the\nresponses generated by GPT-3. It is important to note that in this assessment, our concern was not on the security of the\ngenerated code but merely the feasibility of the prompting techniques for further analysis for secure code generation.\nDue to this reason, we manually checked the model responses to verify if the techniques could be successfully executed\nto obtain an appropriate code response from the LLMs. An appropriate code response in this context is a code snippet\nthat implements the functionality specified in the coding task description. Only those techniques that facilitated a\nseamless generation of code using an LLM, were considered in the subsequent in-depth analysis focused on security\naspects."}, {"title": "5.3 In-depth Analysis of Code Security", "content": "Following the screening of prompting techniques that are suitable for our detailed investigation, we proceeded to the\nsteps that analyze their impact on secure code generation tasks as depicted in Figure 10. These steps are elaborated\nbelow."}, {"title": "5.3.1 Prompt Template Adaptation and Code Generation", "content": "Most papers on prompting techniques focus on tasks\nunrelated to secure coding, requiring us to tailor these techniques to create prompt templates for secure code generation.\nThis customization is specific to each technique. In step 4, we performed this by modifying the task instruction, task\ninput, and (optional) response trigger phrases included in each prompting technique. The task instruction is the generic\ninstruction that specifies the action the model is expected to undertake, such as generating a translation, or, in our\ncase, generating secure code. It can also include statements that instruct the model to review or improve its response\namong other tasks. The task input is the specific task scenario for which we need a response such as the sentence to be\ntranslated or the description of the task for which the model should generate code. The response trigger phrase is used\nto elicit a response from the model without adhering to the conventional format of a task instruction. Examples include\nexpressions like \"let's think step by step\" or \"therefore the answer is\" as seen in the case of zero-shot CoT technique.\nIn this step, the task instructions in the prompting techniques were modified to convey to the model that it should\ngenerate secure Python code since our target programming language is Python. For example, \"Generate secure Python\ncode for the following task description\". For the task input, we used the NL coding task descriptions obtained from the\nLLMSecEval dataset. Furthermore, for techniques that leverage task-specific trigger phrases, adjustments were made to\nintegrate secure code generation into it. For example, \"Therefore secure Python implementation is\".\nOnce the prompt templates for each technique were adapted for secure code generation, we proceeded to step 5\nwhere we systematically generated code utilizing all three LLMs employing these templates. The code generation was\nperformed by accessing the LLM via their respective APIs as mentioned in section 5.1."}, {"title": "5.3.2 Code Validity Analysis", "content": "In step 6, we checked whether the code produced by the LLMs, utilizing different\nprompting techniques was valid. The validity of the code is characterized by 2 factors:\n\u2022 Task alignment: In this check, we ensure if the model has generated actual code (and not just NL comments)\nand that the generated code meets the functional requirements outlined in the coding task description provided\nto the LLM. For instance, if the coding task involves creating a web page allowing users to update their email\naddresses, we confirm that the generated code indeed attempts to update the user's old email address with a new\none.\n\u2022 Code completeness: In this check, we verify if the specified functionality in the task description is completely\nimplemented in the code. For instance, the LLM may generate a code snippet that implements a login page with\nan incomplete login() function that contains no actual implementation but only comments to implement it. We\nalso check for missing import statements in this check. Such code snippets that are incomplete are considered\ninvalid.\nThe code validity assessment was conducted manually by systematically going through each generated code to\nconfirm that the code was relevant and coherent with the task description. In instances where a model's output was\neither incomplete or not in alignment with the task description, we initiated a second attempt to regenerate the code\nusing the same model and prompting technique that was initially used without changing anything to ensure that the\ninvalid code was not generated due to some unforeseen API errors. When the model failed to generate a valid code the\nsecond time, we discarded that code snippet from our evaluation."}, {"title": "5.3.3 Code Security Analysis", "content": "In step 7, we utilized Bandit, a static analysis tool specifically engineered to detect\nsecurity weaknesses in Python code to assess the security of the generated code. Bandit examines the code and provides\na report detailing the number of weaknesses, their descriptions, associated CWE IDs, severity, and confidence levels. We\nconducted scans on valid code outputs from the LLMs using various prompting techniques with Bandit and compiled\nthe findings. Our analysis of these reports aimed to discern the impact of each technique on code security and to identify\nthe most common CWEs found in the LLM-generated code. The findings from this investigation are detailed in Section\n6.\nBandit Results Verification. To gauge the reliability of the results obtained from Bandit, we also opted to manually\nverify Bandit's outcomes generated for a small subset (10%) of the code snippets produced by one of the LLMs (GPT-3).\nDuring this manual verification, we examined the code snippets to identify any false positives or false negatives in the\nweaknesses reported by Bandit. This involved verifying whether all weaknesses flagged by Bandit were indeed present\nin the code and whether Bandit overlooked any weaknesses. We specifically searched for the 18 security weaknesses\nfor which the coding tasks in the LLMSecEval dataset are designed. Extensive information provided by MITRE for\ndifferent CWEs including vulnerability description, examples, and mitigations was leveraged to identify weaknesses in\nthe code. The results of this manual verification were then compared with those of Bandit to understand the degree to\nwhich Bandit is accurate."}, {"title": "6 SECURITY EVALUATION RESULTS", "content": "Our security analysis encompassed leveraging GPT-3, GPT-3.5, and GPT-4 to explore how various prompting techniques\ninfluence the security of code generated by LLMs. Below, we present the results of this investigation. All the generated\ncode as well as the analysis results are present in our replication package specified in Section 10."}, {"title": "6.1 Selected Prompting Techniques for In-depth Security Analysis", "content": "We conducted an initial screening of the prompting techniques obtained from the SLR to identify those suitable\nfor detailed experimentation in our in-depth analysis. Following our qualification criteria, any technique that is\ndemonstrative in nature (refer Table 1) does not meet the requirements for inclusion in our in-depth analysis as stated\nin Section 5.2. Based on this, 9 out of 15 techniques were eliminated from further analysis, leaving us with zero-shot,\nzero-shot CoT, RCI, persona pattern, memetic proxy, and progressive hint prompting. However, as mentioned in Section\n4.2.5, persona pattern and memetic proxy are techniques that follow the same approach but with different names. Hence\nwe consider these two techniques as one (referred as persona/memetic proxy from now on), resulting in a total of 5\ntechniques. Subsequently, we conducted preliminary experiments on these 5 techniques, using five randomly selected\ncoding tasks from the LLMSecEval dataset to ensure that the techniques could be successfully executed without any\nissues.\nAll 5 techniques, except for progressive hint prompting, successfully generated appropriate code outputs for all\n5 coding tasks. Here, an appropriate output is a code snippet that is compliant with the functional requirements\nspecified in the prompt. As illustrated in Figure 4, progressive hint prompting operates by iteratively refining the LLM's\noutputs until they reach a point of stability, where further iterations do not yield changes. However, during our initial\nexperiments with this technique, we encountered a challenge: the model's outputs continued to exhibit variations\neven after 5 iterations, failing to meet the exit criteria defined for this technique. Consequently, we opted to exclude\nprogressive hint prompting from our in-depth analysis, leaving us with 4 distinct prompting techniques that include\nzero-shot, zero-shot CoT, RCI and persona/memetic proxy for further examination."}, {"title": "6.2 Adapted Prompt Templates", "content": "As described in Section 5.3.1, we adapted the 4 selected prompting techniques for secure code generation tasks by\nmodifying the task instruction, task input and the optional response trigger phrases. The prompt templates are shown\nin Table 2. For all the templates, the task inputs were replaced by the coding task descriptions from the LLMSecEval\ndataset.\nZero-shot prompting consists of a task instruction and task input. Prior research has shown that significant im-\nprovements in an LLM's performance can be achieved by manipulating a zero-shot prompt [81]. However, given the\nimpracticality of exploring every conceivable permutation of NL prompts for secure code generation, our approach\nentailed conducting experiments utilizing four distinct prompt variations by modifying the task instruction to provide\na basic assessment of the utility of zero-shot technique. These variations are baseline, naive-secure, CWE-specific and\ncomprehensive prompts. The baseline prompt does not include any security information in the task instruction. This\nvariant is used as a base against which the impact of including security specifications in the remaining zero-shot variants\nas well as the prompts from other prompting techniques are measured. In the naive-secure prompt, the term \u201csecure\u201d is\nadded to the task instruction to encourage secure implementations. In the next variant called CWE-specific prompts, we\nexamined the impact of incorporating more specific security details by adding security cues to the prompts based on"}, {"title": "6.3 Security in LLM-generated Code (RQ2)", "content": "We generated code using GPT-3, GPT-3.5, and GPT-4 for 150 security-sensitive tasks employing each of the 7 prompt\ntemplates shown in Table 2. The initial step involved assessing the validity of the generated code, ensuring it was"}, {"title": "6.3.1 Statistical Tests", "content": "As weakness density provides a more comprehensive and meaningful assessment of the weak-\nnesses introduced by the models into code, we ran a Kruskall Wallis test [50] on this metric for each LLM to determine\nthe statistical significance of the results obtained for each prompt template. The p-values obtained for GPT-3, GPT-3.5,\nand GPT-4 are 0.334, 0.160, and 0.001 respectively. This indicates that there are significant differences in the weakness\ndensity of prompt templates for GPT-4 (p < 0.05) as opposed to GPT-3 and GPT-3.5. To further understand the results, we\nperformed a Dunn's Post-Hoc test [22] with Bonferroni [6] correction (corrected significant level (\u03b1) = 0.05/21 = 0.002381)\non the results from all the models. Table 4 shows key figures for facilitating comparisons among various prompting\ntechniques. The column Pair denotes the prompt template combinations being compared. The mean difference is the\nabsolute difference in the means calculated over the weakness density of code generated by each prompt type in the\npair. The next column displays the percentage difference in the average weakness density when transitioning from\nthe first technique to the second technique within the pair of techniques being evaluated. Positive values indicates\nan increment and negative values show a decrement in the average weakness density. The third column provides the\np-values obtained as a result of the Post-Hoc test comparing the results of the pair of techniques. The observed increase\nor decrease in the number of security weaknesses are significant when p < 0.002381. As indicated by the Kruskall\nWallis test earlier, there is no statistically significant difference between the results of any prompt type using GPT-3 and"}, {"title": "7 DISCUSSION", "content": "In this section, we provide a more detailed analysis of the results presented in section 6, aiming to obtain a deeper\nunderstanding of the security aspects surrounding Python code generated by the LLMs. Initially, we explore the general\neffect of different prompting techniques on code security, seeking to determine the most effective approach to elaborate\non RQ2. Additionally, we investigate the most prevalent CWEs identified within the LLM-generated code and evaluate\nhow different prompting techniques handle these weaknesses. Finally, we scrutinize the impact of incorporating security\ncues into the prompts using various prompting techniques, assessing how they affect the coding behavior exhibited by\nthe LLMs."}, {"title": "7.1 Effect of Prompting Techniques on Security", "content": "While it is already acknowledged, our experimental results reaffirm that developers should exercise caution in relying\nsolely on LLMs for security-critical tasks. Specialized measures are imperative to address the security weaknesses\ninherent in the code generated by these models. In this regard, we examined four prompting techniques-zero-shot,\nzero-shot CoT, RCI, and persona/memetic proxy-for secure code generation using LLMs. This section delves into the\nstrengths and limitations of these techniques through a comparative analysis.\nZero-shot Prompting. In addition to the baseline prompt, we crafted three variations of zero-shot prompts-naive-\nsecure, CWE-specific, and comprehensive-each infused with different levels of security cues. These variations had\nvarying effects on the security"}]}