{"title": "Discrete Modeling via Boundary Conditional Diffusion Processes", "authors": ["Yuxuan Gu", "Xiaocheng Feng", "Lei Huang", "Yingsheng Wu", "Zekun Zhou", "Weihong Zhong", "Kun Zhu", "Bing Qin"], "abstract": "We present an novel framework for efficiently and effectively extending the power-ful continuous diffusion processes to discrete modeling. Previous approaches have suffered from the discrepancy between discrete data and continuous modeling. Our study reveals that the absence of guidance from discrete boundaries in learning probability contours is one of the main reasons. To address this issue, we propose a two-step forward process that first estimates the boundary as a prior distribution and then rescales the forward trajectory to construct a boundary conditional diffusion model. The reverse process is proportionally adjusted to guarantee that the learned contours yield more precise discrete data. Experimental results indicate that our approach achieves strong performance in both language modeling and discrete image generation tasks. In language modeling, our approach surpasses previous state-of-the-art continuous diffusion language models in three translation tasks and a summarization task, while also demonstrating competitive performance compared to auto-regressive transformers. Moreover, our method achieves comparable results to continuous diffusion models when using discrete ordinal pixels and establishes a new state-of-the-art for categorical image generation on the CIFAR-10 dataset.", "sections": [{"title": "Introduction", "content": "Discrete modeling is essential due to the natural prevalence of discreteness in numerous domains, including proteins [Madani et al., 2020, 2023], images [Parmar et al., 2018, Dosovitskiy et al., 2021], and natural language [Sutskever et al., 2014, Brown et al., 2020]. Recent dominant framework for discrete modeling is the Transformer [Vaswani et al., 2017] with an autoregressive manner. While achieving impressive performance, it does suffer from a slow step-by-step generation process, especially for long sequences. Continuous Diffusion models [Sohl-Dickstein et al., 2015, Ho et al., 2020], on the contrary, exhibit the ability to recover high-dimensional data from noise in parallel with limited iteration steps. Although proved to be effective in continuous data generation [Rombach et al., 2022, Kong et al., 2021], they continue to encounter challenges in discrete modeling [Austin et al., 2021, Chen et al., 2023b, Li et al., 2022, Gong et al., 2023b].\nIn this paper, we reveal a significant discrepancy pertaining to the modeling of discrete data using continuous diffusion models. Current approaches represent a discrete sample with a vector point in the continuous space. The diffusion process learns a neural network to model the probability distribu-tions that recovers this continuous point from Gaussian noise. However, the discrete data actually corresponds to an area in the continuous space rather than a single point, where the oversimplified assumption leads to a mismatch between learned probability contours and the boundary of the discrete area. Take language generation as an example, a word is represented with an embedding vector in the embedding space. To generate this word, it is impractical to strictly enforce the predicted vector to be an exact match to the embedding. On the contrary, vectors around this embedding can also generate"}, {"title": "Preliminaries", "content": "Diffusion Models To model a real distribution q(x0), diffusion models utilize a forward process Pt (x|x0) with T steps to gradually add Gaussian noise \\pi(x) = \\mathcal{N}(0, I) into the data distribution, where pT(x|x_0) = \\pi(x). There are different architectures for the forward process. A common approach [Ho et al., 2020] considers the forward process as the Markovian process, where pt (x|xo) =\\prod_{s=1}^{T} Ps(xs | xs-1) combines a series of Gaussian distributions. Thus the forward process follows a Gaussian distribution that pt(x|x0) = \\mathcal{N}(\\sqrt{\\bar{\\alpha}_t}x_0, (1 - \\bar{\\alpha}_t)I) (Variance Preserving) or pt(x|x0) = \\mathcal{N}(x_0, \\sigma_t^2I) (Variance Exploding) [Song et al., 2021b], where noise scheduler \\bar{\\alpha}_t monotonically decreases from 1 to 0 and \\sigma_t increases from sufficiently small to the maximum pairwise distance between all training data points. To recover data from noise, diffusion processes train neural networks xo (xt, t) to predict x0 (other equivalent targets include e and \\nabla log p(xt)) from xt ~ pt(x|x0):\n\\mathcal{L}_\\theta = \\mathbb{E}_{t \\sim U(1,T), x_0 \\sim q(x_0), x_t \\sim p_t(x|x_0)} [||x_0 - x_\\theta (x_t, t)||^2] .  (1)\nSamples are generated with a series of reverse state transition p(xt-1 | xt, x0(xt, t)).\nFlow Matching Another architecture [Lipman et al., 2023] utilizes the ODEs and defines a time-dependent flow function \\phi_t(x) = \\sigma_t(x_0)x + \\mu_t(x_0) that maps pt(x|x0) = [\\phi_t]_*\\pi(x) = \\pi(\\phi_t^{-1}(x)) |det \\frac{d \\phi_t^{-1}}{dx}| = \\mathcal{N}(\\mu_t(x_0), \\sigma_t^2(x_0)I), where \\mu_t and \\sigma_t can be the same as in diffusion"}, {"title": "Methodology", "content": "As illustrated in Figure 2, our objective is to refine the probability density contours of pt (x|x0) so that they better fit the boundaries of discrete samples while still allowing for the ease of sampling. Let xo denote the samples from a real distribution q(x0). Obtaining a boundary-aware corresponding noisy data x at time t \u2208 [1,T] is pt(x|xo) = \\int pt(x, xto, to|x0)dxtodto, where to is a random variable distributed according to when the diffusion trajectory and the discrete boundary intersect, and xto is the corresponding sample point at to. Then the forward process is rescaled in two steps:\npt(x|xo) = \\int \\underbrace{pt(x|xto, to, xo)}_\\text{Trajectory Rescaling} \\underbrace{p(xto, to|xo) dxtodto}_\\text{Boundary Estimation},  (4)\nwhere the latter term is to calculate the discrete boundaries and the former term is to rescale the forward trajectory. In order to make the equation tractable and ensure that x and xto are on the same trajectory, we model the forward process with flow functions \\phi_t(x) and extend the notation as:\n\\Psi_t(x) = u(x, t) x_0 + V(x_0, t) x, p_t (x|x_0) = [\\Psi_t]_*\\pi(x)  (5)\nwhere u(\u00b7) and v(\u00b7) are coefficient functions and sampling xt from pt (x|x0) equals to\nxt = \\phi_t(\\epsilon), \\epsilon \\sim \\pi(x) = \\mathcal{N}(0, I).  (6)\n3.1 Estimate Discrete Boundaries\nBefore figuring out the joint distribution p(xto, to|x0), let's start by discussing how to verify whether an arbitrary point x in the continuous space belongs to the discrete area of x0. Suppose x0, which exists in the continuous space S, is the representation vector of a discrete random variable I in a discrete space with K states. Besides, I is another discrete random variable i.i.d. with I. We define the discrete area of x in the continuous space S as:\nC_I = {x \\in S | f(x, I) > f(x, \\tilde{I}),\\forall \\tilde{I} \\neq I},  (7)"}, {"title": "", "content": "where f(x, I) is a function assessing the likelihood of an arbitrary continuous point x inside the discrete area of x0. For instance, in language modeling, K is the vocabulary size. I, J \u2208 K\u2122 are two different sequences of n tokens and x0 \u2208 R[n,m] is a sequence of m-dimensional vector embeddings for I. f(x, I) is the dot similarity function. C\u2081 collects all vectors in the embedding space that will be decoded to generate I and excludes vectors associated with any other token sequences I.\nGiven a noisy point xto locating at the boundary between C1 and CJ, we can get |f(xto, I) \u2212 f(xto, J)| = 0 based on previous definition. Replacing xto with eqs. (5) and (6), there is:\nf(u_t x_0 + v_t \\epsilon, I) = f (u_t x_0 + v_t \\epsilon, J).  (8)\nIn language modeling and categorical images, f(\u00b7) is a linear projection function that:\nu_t (f(x_0, I) - f(x_0, J)) = v_t (f(\\epsilon, I) - f(\\epsilon,J)).  (9)\nFurther simplification of this equation can not be universally applied to all arbitrary forms of uto and vto. Therefore, we calculate separately for several commonly occurring special cases.\nDiffusion Process For variance preserving, there is u\u00b2 + v\u00b2 = 1 and we have:\nu_{t_0} = \\frac{1}{\\sqrt{1 + \\left(\\frac{f(x_0, I) - f(x_0, J)}{f(\\epsilon, J) - f(\\epsilon, I)}\\right)^2}} \\text{ and } v_{t_0} = \\frac{1}{\\sqrt{1 + \\left(\\frac{f(\\epsilon, J) - f(\\epsilon, I)}{f(x_0, I) - f(x_0, J)}\\right)^2}}.  (10)\nFor variance exploding, there are ut = 1 and vt = ot. We can obtain:\nu_{t_0} = 1 \\text{ and } v_{t_0} = (f(\\epsilon, I) - f(\\epsilon,I)) / (f(x_0, I) - f(x_0, J)) .  (11)\nFlow Matching For optimal transport, there is ut + vt = 1 and similarly we get:\nu_{t_0} = 1/ \\left(1 + \\left(\\frac{f(x_0, I) - f(x_0, J)}{f(\\epsilon, I) - f(\\epsilon,I)}\\right)^2\\right) \\text{ and } v_{t_0} = 1/ \\left(1 + \\left(\\frac{f(\\epsilon, I) - f(\\epsilon,I)}{f(x_0, I) - f(x_0, J)}\\right)^2\\right).  (12)\nAs a result, to can be directly derived by inverting the coefficient function ut or vt, which depends on the choice of noise scheduling strategies. Since their differences do not affect our results, we omit the detailed calculation (appendix E) and denote this process with a function G():\nto = G(x_0, \\epsilon), \\text{ where } u(x_0, G(x_0, \\epsilon)) = u_{t_0} \\text{ and } v(x_0, G(x_0, \\epsilon)) = v_{t_0}.  (13)\nIt's worth noting that to is not a scalar but a vector, where the dimension is the number of elements in x0. If x0 is a sequence of n tokens, to \u2208 [1, T]n. If x0 is a RGB image with 3-channel \u00d7 h-height \u00d7 w-width of pixels, to \u2208 [1, T]3\u00d7h\u00d7w. Furthermore, the corresponding noisy sample xto is derived as:\nx_{t_0} = u(x_0, G(x_0, \\epsilon))x_0 + V(x_0, G(x_0, \\epsilon))\\epsilon = \\psi_{G(x_0,\\epsilon)} (\\epsilon),  (14)\nwhich is a time-independent function of the Gaussian noise e. It's worth mentioning that both p(to|xo) and p(xto|x0) are intractable, since G(x0, \\epsilon) and \\psi_{G(x_0,\\epsilon)} (\\epsilon) are not invertible to \u20ac. Different e can be mapped to a same to or xto. Fortunately, there is an one-to-one mapping between \u20ac and the [xto; to] pair. We denote the boundary flow function and the corresponding inversion as\n\\Psi(\\epsilon) = [\\psi_{G(x_0,\\epsilon)} (\\epsilon); G(x_0, \\epsilon)],  \\Psi^{-1}([x_{t_0}; t_0]) = (x_{t_0} - u(x_0, t_0)x_0)/v(x_0, t_0),  (15)\nand the joint boundary distribution is calculated as\np(x_{t_0}, t_0|x_0) = [\\Psi]_*\\pi([x_{t_0}; t_0]).  (16)\nThe support set of xto is restricted to the boundary contour, while other regions in the space are assigned a probability of 0. To obtain the complete boundary, it is necessary to iterate over all possible choices of I and perform pairwise comparisons with I. The complexity is O(n \u00d7 K), where n elements in xo is independently iterated. In practical implementation, obtaining the tightest boundary only requires one step of parallel calculation and an extra min(\u00b7) function over all to candidates."}, {"title": "", "content": "Confidence Factor The discrete area defined by eq. (7) represents an ideal scenario in which the confidence of the boundary is insufficiently reliable for practical application. Due to the intractability of obtaining the probability density function across the entire discrete area and calculating its confidence interval, we employ an empirical strategy. This approach involves utilizing a confidence factor, denoted as r, ranging from 0 to 1, which is multiplied by to to strike a balance between confidence and discreteness. Therefore, r = 0 implies the exclusion of discrete priors, causing the discrete area to collapse into a single point, which is the original diffusion process. As the value of r increases, the modeling of discrete boundaries improves at the expense of reliability. Empirically, when the model is conditioned with good guidance, setting a larger value for r allows us to obtain better discrete priors. However, in the case of unconditional modeling, maintaining reliability becomes more crucial to prevent oscillations and even collapses during training.\n3.2 Rescale the Forward Trajectory\nIn this section, we introduce how to formulate the forward trajectory conditioned on discrete bound-aries and derive the rescaled noisy sampling distribution. We start with the boundary-independent forward process pt (xx0). Let xt denote a noisy point at time t sampled from pt (xx0), there is et = (xt - u(x, t)x0) /v(x, t) given eq. (5). Equations (13) and (14) provide the corresponding [xto; to] pair on the same trajectory, which is deterministically calculated with no randomness:\n[x_{t_0}; t_0] = \\Psi(\\epsilon_t), \\text{ where } \\epsilon_t = (x_t - u(x, t)x_0) /v(x_0,t).  (17)\nTo model the transition probability pt (xto, to|xt, x0), we utilize the Dirac delta function \u03b4(x) ~ lim\u03c3\u21920N(0, \u03c3\u00b2I), which can be loosely thought of as aggregating all probability densities toward the origin, assigning an infinite density at the origin and zero densities elsewhere. Therefore, we have pt(xto, to|xt, x0) = \u03b4 ([xto; to] \u2212 \u03a8(et)) . Then the forward process, conditioned on the discrete boundary, is simply derived via Bayes' rule:\nPt(XtXto, to, xo) = Pt(Xt, to|Xt, X0)\nPt(XtX0)\n=\n{\n+\u221e \u00d7\n0, [xto; to] \u2260 \u03a8(et)\nPt (XtX0)\np(xto, toxo)' otherwise  (18)\nSince pt (xtxo) > 0 and p(xto, to|xo) > 0, pt (xt|xto, to, xo) is also a delta function that\nPt(XtXto, to, xo) = \u03b4 (xt \u2013 u(x, t)xo \u2013 v(x, t)\u03a8^{-1}([xto; to])) .  (19)\nBased on the translation property of the Dirac delta function, i.e. \u222b f(x)\u03b4(x \u2212 a)dx = f(a), the original forward process pt(xt|xo) = [\u03c8t \u25e6 \u03a8\u22121 \u25e6 \u03a8]*\u03c0(xt) = [Vt]*\u03c0(xt) naturally ignores the influence of discrete boundaries, even if the boundary information is explicitly added as a condition.\nTo enable the discrete priors, we propose a simple and intuitive approach: rescale the forward trajectory. As shown in Figure 2B, the original forward process flows from xo to a random noise e, and we reset the starting point to xto. Accordingly, the intermediate noisy points xt, t \u2208 [1,T] will be proportionally mapped on this new path, which is\n\\tilde{x}_t = x_T, \\tau = T(t, t_0) = r \\times t_0 + t \\times (T - r \\times t_0)/T \\nonumber  \n= u(x_0, T(t, t_0))x_0 + v(x_0, T(t, t_0))\\Psi^{-1} ([x_{t_0}; t_0]) .  (20)\nSimilar to eq. (19), the rescaled conditional forward process is a Dirac delta function:\nPt(xt|xto, to, xo) = \u03b4 (\\tilde{x}_t \u2013 u(x_0, T(t, t_0))x_0 \u2013 v(x_0, T(t, t_0))\\Psi^{-1} ([x_{t_0} ; t_0])) .  (21)\nHowever, pt (Xt|x0) faces the same problem of irreversibility as in eq. (14) and we derive it as:\nPt(XtX0)\n=\n\u222b P\\tilde{t}(x_\\tilde{t}, \\tau|x_0)d\\tau = \u222b \u222bPt(x\\tilde{t}, \\tau|x_{t_0}, t_0, x_0)p(x_{t_0}, t_0|x_0)d[x_{t_0}; t_0]d\\tau  \n= \u222b [\\psi_{\\tau} \u25e6 \\Psi^{-1} \u25e6 \\Psi]*\\pi([\\tilde{x}_t; \\tau])d\\tau\n(22)\n= \u222b [V\\tilde{\\tau}]_*\\pi([\\tilde{x}_t; \\tau])d\\tau.\nObtaining the probability density function requires gathering together the probability densities of the same location \\tilde{x}_t with different \\tau, which is intractable. Fortunately, we only need to sample noiy points from this probability distribution \\tilde{x}_t ~ pt (Xtx0), which is easy to implement:\nxt = u (x_0, T(t, G(x_0, \\epsilon))) x_0 + v(x_0, T(t, G(x_0, \\epsilon)))\\epsilon, \\epsilon ~ \\pi(x).  (23)\n3.3 Recover Data from Noise"}, {"title": "", "content": "Training Objective Theoretically, the diffu-sion neural networks can be trained as in eq. (2), where the rescaled vector field is derived as  \\. However, since a low dt error estimation on x0 is of significant importance to our trajectory rescaling method, according to eqs. (10) to (13), we convert the objective to an upper bound of the eq. (2) (See appendix F for more details) and train a neural network xe (xt, t) to predict x0 directly:\n\\mathcal{L}_\\theta = \\mathbb{E}_{x_0\\sim q(x_0),t\\sim U(1,T), \\tilde{x}_t\\sim p_t(x|x_0)} [||x_0 - x_\\theta (\\tilde{x}_t, t)||^2] .  (24)\nThe training procedure is demonstrated in algorithm 1 and key steps are summarized in the line 4.\nReverse Process A direct approach that follows the flow matching is to solve the ODE of  =  ( \u2212 (x)|xo)dt, \u0442(x) ~ \u03c0(x). This form of transformation is inefficient with xo-prediction during inference because we have to solve the equation of \u03c4 = T(t,G( \n\nto get the \u03c4 with  ))\nrespect to the change of t and xo in real time. Therefore, we provide a deterministic reverse process as an alternative, which is a special case of DDIM [Song et al., 2021a] or the ODE with discrete timesteps. Given the time intervals \u2206t \u2208 [\u2206t1,... \u2206ts],\u2211\u2206t = T, we general-\n Algorithm 2 Sampling\n1: t := T, \u03c4 := T // Initialing\n2:  e ~ xt ~ N(0, 1) // Pseudo Target\n3: for \u2206t := \u2206t1,..., \u2206ts do // \u03a3\u2206t = T\n4: x0 := xo(xt, t) // Updating\n5: t := t \u2212 \u2206t\n6: \u03c4 := T (t \u2212 \u2206t, G(x0, )) // eq. (25)\n7: xt := u(x0, T\u2206) x0 + v(x0, \u03c4\u2206)+ z\n8:  e := \u03a8\u22121([xt; T]) // Trajectory Alteration\n9: end for\n10: x0 := xo(xt, t) // X1 \u2192 X0\n11: return X0\nize the boundary conditions [xto; to] in Pt(Xt|Xto, to, x0) of eq. (21) and \u03a8-1([xto; to]) of eq. (15) to any arbitrary condition pairs [xt; \u03c4] and obtain the reverse process:\np([xt\u2212\u2206t; T\u2206]|[xt; T], x0) = \u03b4\\left(\\frac{x_{t-\\Delta t}}{v(x_0, t_\\Delta)}  u(x, t) x_0}{\n\\frac{T(\\tau - \\Delta t, G(x_0, \\epsilon))}{\\tau}}\\right) ,  (25)\nwhere x = x(xt, t) and t\u2206 is the previous timestep of 7 on the same rescaled trajectory.\nSampling from the reverse process is illustrated in algorithm 2. Similar to the sampling process of DDIM [Song et al., 2021a], it starts from the Gaussian noise, iteratively predicts the pseudo target xo, and updates the reverse trajectory. However, since the 7 and \u00ea are mutually conditioned, we have to keep track of the t, t, xt, and \u00ea during each iteration and split the update of \u00ea into an asynchronous step (line 8). Because reverse trajectory keeps changing due to different pseudo targets x0 predicted by learned neural networks, which brings severe instability, sometimes simply fixing the initial path (removing the line 8) exhibits better performance in experiments."}, {"title": "Language Modeling", "content": "Recent diffusion language models [Li et al., 2022, Gong et al., 2023b] inherit the embedding-rounding framework that a sentence with n discrete tokens W = [W1,..., wn] is embedded to a continuous space via a trainable embedding layer EMB(W) = [EMB(w\u2081), ..., EMB(wn)]. The vocabulary set is K that \u2200wn \u2208 K. Besides, the token embeddings are used as the target points x0 = [x,...,x], xi = EMB(wn), for continuous diffusion trajectories. Hence, generating tokens from embeddings is:\np(W|xo) = p(xt|x0) = \\sum_{i=1}^{n} \\frac{exp(f(x_i, w_i))}{\\sum_{j\\in K} exp(f(x_i, j))},  (26)\nwhere f(x, j) = EMB(j) \u00b7 x is the dot production distance. It's also the function assessing the likelihood of point x inside the discrete area of j. The coefficient functions follow the DDPM [Ho et al., 2020], which are u(x, t) = \\sqrt{\\bar{\\alpha}_t} and v(x,t) = \\sqrt{1 - \\bar{\\alpha}_t}. Besides, the objectives are\n\\mathcal{L}_\\theta = \\mathbb{E}_{W,t,\\epsilon} [\\sum_{i=1}^{n} ||EMB(w_i) - x_0(x, t)||^2/n]  (27)"}, {"title": "", "content": "and an additional rounding objective, which is commonly used in language modeling,\n\\mathcal{L}_r = - \\log p_\\theta (W | x_0) = - \\log p_\\theta (W | x_\\theta (x_t, t)).  (28)\nThe final training target is given by L = L\u03b8 + Lr, where the x0 of the same token sequence W keeps changing because the embedding layer EMB is trainable, which makes the model hard to be trained. Since previous work does not model discrete areas, a large number of noisy samples inside this area will make Lr too small to guide the training of the embedding layer, leading to a mode collapse.\nExperimental Setup Datasets used for experiments include three translation tasks (IWSLT14 DE-EN [Cettolo et al., 2012], WMT14 EN-DE, and WMT16 EN-RO\u00b9) and one text summarization task (GIGAWORD [Rush et al., 2015]). We mainly follow the setting of Gao et al. [2022], which is inherited from previous non-auto-regressive text generation works [Gu et al., 2018, 2019, Ghazvininejad et al., 2019], where translation datasets are distilled [Kim and Rush, 2016]. Baselines are mainly continuous diffusion language models. DiffuSeq [Gong et al., 2023b] and SeqDiffuSeq [Yuan et al., 2022] are derived from Diffusion-LM [Li et al., 2022]. Difformer [Gao et al., 2022] and Dinoiser [Ye et al., 2023] are recent empirical studies highlighting that scaling up the noise is beneficial for language modeling. We also compare with discrete diffusion language models, including D3PM [Austin et al., 2021] and SEDD [Lou et al., 2023]. Since SEDD is a pre-trained language model, we configure its framework and train it from scratch specifically for our tasks. In addition, auto-regressive transformer [Vaswani et al., 2017] is still one of the most powerful architectures for language generation.\nOur boundary conditional diffusion language model is constructed from Difformer [Gao et al., 2022], where the model configuration is transformer-iwslt-de-en in FAIRSEQ framework [Ott et al., 2019] for IWSLT14 DE-EN and transformer-base for other datasets. Sentences are tokenized with Byte-Pair Encoding [Sennrich et al., 2016] and evaluated by detokenized BLEU [Papineni et al., 2002] for machine translation and ROUGE [Lin, 2004] for summarization. During training, the diffusion step is T = 2000 and the confidence factor r = 1 for translation tasks since they have strong conditions, while r = 0.5 for summarization. Sentences are generated deterministically with 20 steps.\nResults Performances are demonstrated in Table 1. Our approach achieves the state-of-the-art compared with continuous diffusion language models and outperforms the two discrete baselines on three machine translation and one text summarization tasks. Our method shows advantages, with a 73.6% significant improvement at most on WMT14 EN-DE, over DiffuSeq [Gong et al., 2023b] and SeqDiffuSeq [Yuan et al., 2022], which are two basic methods directly applying diffusion process to language modeling. Compared with recent strong diffusion language models like Difformer [Gao et al., 2022] and Dinoiser [Ye et al., 2023], which have deployed various effective noise scheduling strategies on diffusion processes from the empirical perspective, our model is still superior with at most 3.07 advancement of BLEU score on WMT16 EN-RO. This implies the effectiveness of modeling discrete priors. In addition, we illustrate the performance of auto-regressive modeling, where we use the transformer [Vaswani et al., 2017] to rerank the generated sentence candidates (7"}, {"title": "", "content": "length beam \u00d7 3 sentence beams) of our model. The reranked performance can even outperform transformers on IWSLT14 DE-EN and WMT16 EN-RO.\nAblation Our approach is a general framework applicable to almost all continuous diffusion models, providing them with discrete boundaries as priors. We choose Difformer [Gao et al., 2022] as the base model and follow the configurations. As proved in eq. (19), the orig-inal forward process will ignore the discrete pri-\n  Table 2: Ablation studies.\n  Models\nIWSLT14 WMT16\nBase (Difformer) 31.58 32.86\n+ forward only 30.08 33.02\n+ forward & reverse 33.42 32.77\nOptimal Transport 33.15 33.65\nors although explicitly demonstrated. We conduct ablation experiments on the rescaling module. As illustrated in Table 2, our approach rescales the trajectory of both forward and reverse processes on Difformer. Only rescaling the forward trajectory is also effective but sub-optimal due to the inconsistent distribution during inference. Due to computational cost and fair comparison, our method leaves room for improvement. For example, replacing the forward trajectory with optimal transport in Flow Matching, u(x, t) = 1 \u2212 t/T and v(x, t) = t/T, achieves better performance on WMT16.\nAnalysis Our training objective, eq. (24), is an upper bound of the eq. (2). We demonstrate the influence of this approximation in Table 3 on IwWSLT14 DE-EN to reveal the thought of our formula. On the one hand, Lx, brings theoretical errors at a constant scale. On the other hand, Lxo mitigates some experimental errors from the neural networks. The first row Lxo is the objective we used in eq. (24) and the second row L\u016b\u2081 = E{t,x0,xt} [||\u016bt (Xtxo (\u0161t,t)) \u2212 ||2] is directly derived from the eq. (2). The first two columns represent the error expectations of xo and it on the test set. It is easy to observe that, with the dynamic coefficient d = T-r\u00d7G(x0,\u20ac) (appendix F), the value of xo's error (8.44) is much larger than the ut's error (1.56). Therefore, Lxo is beneficial for reducing the impact of the prediction error from the neural network. The third column in Table 3 illustrates the one-step accuracy of predicting x0 and the fourth column is the BLEU score on the test set. Experimental results show that optimizing the upper bound has a negligible impact on the final performance (only a 0.2% drop of the BLEU score), while can improve the efficiency of the loss calculation during the training phase."}, {"title": "Discrete Image Generation", "content": "Image pixels are usually treated as real numbers in continuous space since adjacent pixel values exhibit linear continuity. They are essentially discrete and quantized data with a finite state space, such as 256 states in RGB format. We utilize two discrete image representations. One is binary coding provided by Bit Diffusion [Chen et al., 2023b] that converts a sub-pixel with 256 integers to a 8-bit binary code. It is more efficient as it stores ordinal relationships, but the representation space it constructs will be sparse. Another is pixel embedding, which is a more discrete form of representation because the relationships between pixels are thoroughly broken down and reconstructed by learning the embedding representation. Each pixel is regarded as a one-hot vector and transformed with an embedding layer EMB as used in language. Furthermore, we design an intermediate state to demonstrate the correlation between discreteness and modeling difficulty, which is initializing a fixed embedding with binary coding. The optimization target for binary coding is the MSE loss, and pixel embeddings take the same objective as in language.\nExperimental Setup We use CIFAR-10 [Krizhevsky et al., 2009] for discrete image generation. The evaluation metric is FID [Heusel et al., 2017], which compares 50K generated samples with the training set. Our image generation model is constructed on Bit Diffusion [Chen et al., 2023b], where the architecture is U-Net [Ronneberger et al., 2015] with 3 stages, 256 channels and 3 residual blocks"}, {"title": "Related Work", "content": "Discrete Modeling Auto-regressive models have demonstrated a domination over dis-crete modeling, especially for text generation [Vaswani et al., 2017, Brown et al., 2020,\nTable 5: Confidence factors.\nr=0 0.2 0.3 0.5\nModels\n10.37 7.39 5.33 3.86\n12.96 11.35 10.80 9.15\nBINARY CODING\nFIXED EMBEDDING\nTRAINABLE EMBEDDING 19.26 15.32 11.56 10.99\nAchiam et al., 2023]. However, the computation"}, {"title": "", "content": "cost increases drastically as the size of sentence length or the image resolution increases. Diffusion models [Sohl-Dickstein et al., 2015, Ho et al., 2020, Dhariwal and Nichol, 2021, Saharia et al., 2022"}]}