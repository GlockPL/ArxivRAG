{"title": "Machine Unlearning Fails to Remove Data Poisoning Attacks", "authors": ["Martin Pawelczyk", "Jimmy Z. Di", "Yiwei Lu", "Gautam Kamath", "Ayush Sekhari", "Seth Neel"], "abstract": "We revisit the efficacy of several practical methods for approximate machine unlearning developed for large-scale deep learning. In addition to complying with data deletion requests, one often-cited potential application for unlearning methods is to remove the effects of training on poisoned data. We experimentally demonstrate that, while existing unlearning methods have been demonstrated to be effective in a number of evaluation settings (e.g., alleviating membership inference attacks), they fail to remove the effects of data poisoning, across a variety of types of poisoning attacks (indiscriminate, targeted, and a newly-introduced Gaussian poisoning attack) and models (image classifiers and LLMs); even when granted a relatively large compute budget. In order to precisely characterize unlearning efficacy, we introduce new evaluation metrics for unlearning based on data poisoning. Our results suggest that a broader perspective, including a wider variety of evaluations, are required to avoid a false sense of confidence in machine unlearning procedures for deep learning without provable guarantees. Moreover, while unlearning methods show some signs of being useful to efficiently remove poisoned datapoints without having to retrain, our work suggests that these methods are not yet \"ready for prime time,\" and currently provide limited benefit over retraining.", "sections": [{"title": "Introduction", "content": "Modern Machine Learning (ML) models are often trained on large-scale datasets, which can include significant amounts of sensitive or personal data. This practice raises privacy concerns as the models can memorize and inadvertently reveal information about individual points in the training set. Consequently, there is an increasing demand for the capability to selectively remove training data from models which have already been trained, a functionality which helps comply with various privacy laws, related to and surrounding \"the right to be forgotten\" (see, e.g., the European Union's General Data Protection Regulation (GDPR) (General Data Protection Regulation), the California Consumer Privacy Act (CCPA), and Canada's proposed Consumer Privacy Protection Act (CPPA)). This functionality is known as machine unlearning (Cao and Yang, 2015), a field of research focused on \"removing\" specific training data points from a trained model upon request. The ideal goal is to produce a model that behaves as if the data was never included in the training process, effectively erasing all direct and indirect traces of the data. Beyond privacy reasons, there are many other"}, {"title": "Related Works", "content": "Machine unlearning. At this point, there exists a vast literature on machine unlearning (Cao and Yang, 2015), we focus on the most relevant subset here. Many works focus on removing the influence of training on a particular subset of points from a trained model (Ginart et al., 2019b; Wu et al., 2020; Golatkar et al., 2020a,b; Bourtoule et al., 2021; Izzo et al., 2021; Neel et al., 2021; Sekhari et al., 2021; Jang et al., 2022; Huang and Canonne, 2023; Wang et al., 2023). Others instead try to remove a subset of concepts (Ravfogel et al., 2022a,b; Belrose et al., 2023). In general, the goal is to excise said information without having to retrain the entire model from scratch. Some works focus on exactly unlearning (see, e.g., Bourtoule et al. (2021)), whereas others try to only approximately unlearn (e.g., Ginart et al. (2019a); Sekhari et al. (2021); Neel et al. (2021)), using a definition inspired by differential privacy (Dwork et al., 2006). Much of the work in this line focuses on unlearning in the context of image classifiers (e.g., Golatkar et al. (2020a); Goel et al. (2022); Kurmanji et al. (2023); Ravfogel et al. (2022a,b); Belrose et al. (2023)). Some approximate unlearning methods are general-purpose, using methods like gradient ascent (Neel et al., 2021), or are specialized for individual classes such as linear regression (Cook and Weisberg, 1980; Guo et al., 2019; Izzo et al., 2021) or kernel methods (Zhang and Zhang, 2021).\nEvaluating machine unlearning. Some of the works mentioned above focus on provable machine unlearning (either exact or approximate). That is, as long as the algorithm is carried out faithfully, the resulting model is guaranteed to have unlearned the pertinent points. However, many unlearning methods are heuristic, without provable guarantees. Alternatively, we may be given access to an unlearning procedure as a black box. In either case, we may want to measure or audit how well an"}, {"title": "Machine Unlearning: Preliminaries and Algorithms", "content": "We formalize the machine unlearning setting and introduce relevant notation. Let $\\mathcal{S}_{train}$ and $\\mathcal{S}_{test}$ be training and test datasets for an ML model, respectively, each consisting of samples of the form $(x, y)$ where $x \\in \\mathbb{R}^d$ denotes the covariate (e.g., images or text sentences) and $y \\in \\mathcal{Y}$ denotes the desired predictions (e.g., labels or text predictions). The unlearner starts with a model $\\theta_{initial}$ obtained by running a learning algorithm on the training dataset $\\mathcal{S}_{train}$; the model $\\theta_{initial}$ is trained to have small loss over the training dataset, and by proxy, the test dataset as well. Given a set of deletion requests $U \\subseteq \\mathcal{S}_{train}$, the unlearner runs an unlearning algorithm to update the initial trained model $\\theta_{initial}$ to an updated model $\\theta_{updated}$, with the goal that (a) $\\theta_{updated}$ continues to perform well on the test dataset"}, {"title": "Gradient Descent (GD)", "content": "GD continues to train the model $\\theta_{initial}$ on the remaining dataset $\\mathcal{S}_{train} \\setminus U$ by using gradient descent. In particular, we obtain $\\theta_{updated}$ via\n$\\theta_{t+1} \\leftarrow \\theta_t - \\eta g_t(\\theta_t)$ with $\\theta_1 = \\theta_{initial}$,\nwhere $\\eta$ denotes the step size and $g_t$ denotes a (mini-batch) gradient computed for the training loss $\\mathbb{E}_{(x,y)\\in \\mathcal{S}_{train}\\setminus U}[\\ell((x,y),\\theta)]$ defined using the remaining dataset $\\mathcal{S}_{train} \\setminus U$. The intuition for GD is that the minimizer of the training objective on $\\mathcal{S}$ and $\\mathcal{S}_{train} \\setminus U$ are close to each other, when $|U| << |\\mathcal{S}|$, and thus further gradient-based optimization can quickly update $\\theta_{initial}$ to a minimizer of the new training objective. In fact, following this intuition, Neel et al. (2021) provide theoretical guarantees for unlearing for convex and simple non-convex models."}, {"title": "Noisy Gradient Descent (NGD)", "content": "NGD is a simple modification of GD where we obtain $\\theta_{updated}$ by iteratively running the update\n$\\theta_{t+1} \\leftarrow \\theta_t - \\eta(g_t(\\theta_t) + \\xi_t)$ with $\\theta_1 = \\theta_{initial}$,\nwhere $\\eta$ denotes the step size, $\\xi_t \\sim \\mathcal{N}(0, \\sigma^2)$ denotes an independently sampled Gaussian noise, and $g_t$ denotes a (mini-batch) gradient computed for the training loss $\\mathbb{E}_{(x,y)\\in \\mathcal{S}_{train} \\setminus U}[\\ell((x,y),\\theta)]$ defined using the remaining dataset $\\mathcal{S}_{train} \\setminus U$. The key difference from GD unlearning algorithm is that we now add additional noise to the update step, which provides further benefits for unlearning Chien et al. (2024). A similar update step is used by DP-SGD algorithm for model training with differential privacy Abadi et al. (2016)."}, {"title": "Gradient Ascent (GA)", "content": "GA attempts to remove the influence of the forget set $U$ from the trained model by simply reversing the gradient updates that contain information about $U$. Graves et al. (2021) were the first to propose GA by providing a procedure that stores all the gradient steps that were computed during the initial learning stage; then, during unlearning they simply perform a gradient ascent update using all the stored gradients that relied on $U$. Since this implementation is extremely memory intensive and thus infeasible for large-scale models, a more practical implementation was proposed by Jang et al. (2022) which simply updates the trained model $\\theta_{initial}$ by using mini-batch gradient updates corresponding to minimization of\n$-\\mathbb{E}_{(x,y)\\in U} [\\ell((x,y),\\theta)]$.\nThe negative sign in the front of the above objective enforces gradient ascent."}, {"title": "Data Poisoning to Validate Machine Unlearning", "content": "In this section, we briefly describe targeted data poisoning, indiscriminate data poisoning, and Gaussian data poisoning attacks that we will use to evaluate machine unlearning in our experiments. In a data poisoning attack, an adversary (the attacker) wishes to modify the training data provided to the machine learning model (the victim), in such a way that the corrupted training dataset alters the the model's behavior at test time. A detailed description and further implementation details for these methods are deferred to Appendix A.1.\nTo implement data poisoning attacks, the adversary generates a corrupted dataset $\\mathcal{S}_{corr}$ by adding small (generally adversarially chosen) perturbations to a small $b_p$ fraction of the data samples in the clean training dataset $\\mathcal{S}_{train}$; the corrupted data samples are often called \"poisons\". In particular, the adversary first randomly chooses $P$ many data samples $\\mathcal{S}_{poison} \\sim Uniform(\\mathcal{S}_{train})$ to be poisoned, where $P = |\\mathcal{S}_{poison}| = b_p|\\mathcal{S}_{train}|$ for some poison budget $b_p << 1$. Each sample $(x, y) \\in \\mathcal{S}_{poison}$ is then modified by adding perturbations $\\Delta(x) \\in \\mathbb{R}^d$ to it, i.e. we modify $(x, y) \\rightarrow (x + \\Delta(x), y)$. The remaining dataset $\\mathcal{S}_{clean} = \\mathcal{S}_{train} \\setminus \\mathcal{S}_{poison}$ is left untouched. Finally, $\\mathcal{S}_{corr}$ is generated by taking the union of all the clean samples $\\mathcal{S}_{clean}$ and the poison samples $\\mathcal{S}_{poison}$. We typically require that the added perturbations are very small by enforcing that $|\\Delta(x)||_{\\infty} \\leq \\epsilon_p$ for each $x \\in \\mathcal{S}_{poison}$, where $\\epsilon_p$ is a small (problem dependent) parameter. This ensures that the attack is \"clean label\": i.e. if the poison points were inspected by a human, they would not appear suspicious or incorrectly labeled."}, {"title": "Targeted Data Poisoning", "content": "In a targeted data poisoning attack, the attacker's goal is to cause the model to misclassify some specific datapoints $\\{(x_{target}, y_{target})\\}$, from the test set $\\mathcal{S}_{test}$, to some pre-chosen adversarial label $y_{adv}$, while retaining performance on the remaining test dataset $\\mathcal{S}_{test}$. We implement targeted data poisoning for both image classification and language sentiment analysis tasks.\nFor image classification settings, for a target sample $(x_{target}, y_{target})$, we follow the gradient matching procedure of Geiping et al. (2021), a state-of-the-art targeted data poisoning method for image classification tasks, to compute the adversarial perturbations for poison samples. The"}, {"title": "Indiscriminate Data Poisoning", "content": "In an indiscriminate data poisoning attack, the adversary wishes to generate poison samples such that a model trained on $\\mathcal{S}_{corr}$ has significantly low performance on the test dataset. We implement this for image classification. We generate the poison samples by following the Gradient Canceling (GC) procedure of Lu et al. (2023, 2024), a state-of-the-art indiscriminate poisoning attack in machine learning, where the adversary first finds a bad model $\\theta_{low}$, using the GradPC procedure of Sun et al. (2020), that has low-performance accuracy on the test dataset. Then, the adversary computes perturbations $\\Delta$ such that $\\theta_{low}$ has vanishing gradients when trained with the corrupted training dataset, and will thus correspond to a local minimizer (which gradient-based learning e.g., SGD or Adam can converge to). The effectiveness of Indiscriminate Data Poisoning is measured by the performance accuracy on the test dataset for a model trained on the corrupted dataset $\\mathcal{S}_{corr}$."}, {"title": "Gaussian Data Poisoning", "content": "Our Gaussian data poisoning attack is perhaps the simplest poisoning method to implement. Here, the adversary simply wishes to hide (visually) undetectable signals in the corrupted training dataset $\\mathcal{S}_{corr}$, which do not influence the model performance on the test dataset in any significant way but can be later inferred via some computationally simple operations on the trained model. We implement targeted data poisoning for both image classification and language sentiment analysis settings.\nHow are poison samples generated? For a given poison budget $b_p$ and perturbation bound $\\epsilon_p$, the adversary first chooses $b_p|\\mathcal{S}_{train}|$ many samples $z = (x,y) \\sim Uniform(\\mathcal{S}_{train})$ and then generates the poison samples by simply adding an independent gaussian noise vector to the covariates (i.e. the input component $x$). In particular, for each $z \\in \\mathcal{S}_{poison}$, we generate the poison sample $(x_{poison}, y)$ by modifying the underlying clean sample $(x_{base}, y)$ as\n$x_{poison} \\leftarrow x_{base} + \\xi_z, \\quad \\text{where} \\quad \\xi_z \\sim \\mathcal{N}(0, \\epsilon_p^2/d I_d)$,\nwhere $d$ denote the dimensionality of the input $x$, and $\\xi_z$ is an independent Gaussian noise. The adversary stores the perturbations added $\\xi_z$ corresponding to each poison sample $z \\in \\mathcal{S}_{poison}$. Infor-"}, {"title": "How to use Data Poisoning for evaluating Machine Unlearning?", "content": "Data poisoning methods provide a natural recipe for evaluating the \"unlearning\" ability of a given machine unlearning algorithm. We consider the following four-step procedure (Sommer et al., 2022):\nImplement the data poisoning attack to generate the corrupted training dataset $\\mathcal{S}_{corr}$.\nTrain the model on the corrupted dataset $\\mathcal{S}_{corr}$. Measure the effects of data poisoning on the trained model $\\theta_{initial}$.\nRun the unlearning algorithm to remove all poison samples $U = \\mathcal{S}_{poison}$ from $\\theta_{initial}$ and compute the updated model $\\theta_{updated}$.\nMeasure the effects of data poisoning on the updated model $\\theta_{updated}$.\nNaturally, for ideal unlearning algorithms that can completely remove all influences of the forget set $U = \\mathcal{S}_{poison}$, we expect that the updated model $\\theta_{updated}$ will not display any effects of data poisoning. Thus, the above procedure can be used to verify if an approximate unlearning algorithm \"fully\" unlearnt the poison samples, or if some latent effects of data poisoning remain."}, {"title": "Can Machine Unlearning Remove Poisons?", "content": "We now evaluate state-of-the-art unlearning attacks for the task of removing both target and untargeted data poisoning attacks across vision and language models. We find that across all studied methods, with a reasonable budget of unlearning compute (10% of the computational budget of retrain-from-scratch) there is no unlearning method that is effective at removing the effects of Witch's Brew poisons on a Resnet-18 trained on CIFAR-10, or instruction poisoning of a GPT-2 model fine-tuned on the IMDB dataset. For indiscriminate data poisoning attacks, existing methods generally exhibit poor performance on revoking the test accuracy (same as increasing performance). While GD and SCRUB improve the model performance, such an effect is weaker than retraining"}, {"title": "Experimental Details", "content": "Datasets. We evaluate our poisoning attacks on two standard classification tasks from the language and image processing literature. For the language task, we consider the IMDb dataset (Maas et al., 2011). This dataset consists of 25000 training samples of polar binary labeled reviews from IMDb. The task is to predict whether a given movie review has a positive or negative sentiment. For the vision task, we use the CIFAR-10 dataset (Krizhevsky et al., 2010). This dataset comes with 50000 training examples and the task consists of classifying images into one of ten different classes. We typically show average results over 8 runs for all vision models and 5 runs for the language models and usually report \u00b11 standard deviation across these runs.\nMachine learning models. For the vision tasks, we train a standard Resnet-18 model for 100 epochs. We conduct the language experiments on GPT-2 (355M parameters) LLMs (Radford et al., 2019). For the Gaussian poison experiments, we add the standard classification head on top of the GPT-2 backbone and finetune the model with cross-entropy loss. For the targeted poisoning attack, we follow the setup suggested by Wan et al. (2023) and finetune GPT-2 on the IMDb dataset using the following template for each sample: \u201c[Input]. The sentiment of the review is [Label]\u201d. In this setting, we use the standard causal cross-entropy loss with an initial learning rate set to $5\\cdot 10^{-5}$ which encourages the model to predict the next token correctly given a total vocabulary of C possible tokens, where C is usually large (e.g., for the GPT-2 model C = 50257). At test time, the models predict the next token from their vocabulary given an unlabelled movie review: \u201c[Input]. The sentiment of the review is:"}, {"title": "Understanding why unlearning fails to remove poisons?", "content": "In Section 5.2, we demonstrated that various state-of-the-art approximate machine unlearning algorithms fail to fully remove the effects of data poisoning. Given these results, one may wonder what is special about the added poison samples, and why gradient-based unlearning algorithms fail to rectify their effects. In the following, we provide two hypotheses for understanding the failure of unlearning methods. We validate these hypotheses using a set of simple experiments based on linear and logistic regression on Resent-18 features which allow us to study these hypotheses experimentally. Thanks to the convexity of the corresponding loss the objectives have unique global minimizers making it easier to understand model shifts due to unlearning.\nHypothesis 1: Poison samples cause a large model shift, which cannot be mitigated by approximate unlearning. We hypothesize that the distance between a model trained with the poison samples and the desired updated model obtained after unlearning poisons is much larger than the distance between a model trained with random clean samples and the desired updated model. Thus, any unlearning algorithm that attempts to remove poison samples needs to shift the model by a larger amount. Because larger shifts typically need more update steps, unlearning algorithms are unable to mitigate the effects of poisons in the allocated computational budget."}, {"title": "Conclusion", "content": "Our experimental evaluation of state-of-the-art machine unlearning methods across different models and data modalities reveals significant shortcomings in their ability to effectively remove poisoned data points from a trained model. Despite various approaches which attempt to mitigate the effects of data poisoning, none were able to consistently approach the benchmark results of retraining the models from scratch. This highlights a critical gap in the true efficacy and thus practical value of current unlearning algorithms, questioning their validity in real-world applications where these unlearning methods may be deployed to ensure privacy, data integrity, or to correct model biases.\nFurthermore, our experiments demonstrate that the performance of unlearning methods varies significantly across different types of data poisoning attacks and models, indicating a lack of a one-size-fits-all solution. Given the increasing reliance on machine learning in critical and privacy-sensitive domains, our findings emphasize the importance of advancing rigorous research in machine unlearning to develop more effective, efficient, and trustworthy methods, that are either properly evaluated or have provable guarantees for unlearning. Future work should focus on creating novel unlearning algorithms that can achieve the dual goals of maintaining model integrity and protecting user privacy without the prohibitive costs associated with full model retraining."}]}