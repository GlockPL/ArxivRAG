{"title": "Why Rectified Power Unit Networks Fail and How to Improve It: An Effective Theory Perspective", "authors": ["Taeyoung Kim", "Myungjoo Kang"], "abstract": "The Rectified Power Unit (RePU) activation functions, unlike the Rectified Linear Unit (ReLU), have the advantage of being a differentiable function when constructing neural networks. However, it can be experimentally observed when deep layers are stacked, neural networks constructed with RePU encounter critical issues. These issues include the values exploding or vanishing and failure of training. And these happen regardless of the hyperparameter initialization. From the perspective of effective theory, we aim to identify the causes of this phenomenon and propose a new activation function that retains the advantages of RePU while overcoming its drawbacks.", "sections": [{"title": "1. Introduction", "content": null}, {"title": "1.1. Activation Functions", "content": "An activation function is a function that acts on the output of each layer of a neural network. The remarkable success of deep neural networks is closely related to the choice of appropriate nonlinear activation functions, and there has been extensive discussion solely on the research of activation functions. Early studies on neural networks include the perceptron [1], where the activation function is the sign function, and it was composed of only one or two layers. The perceptron had the drawback that its derivative was almost everywhere zero, making it difficult to apply the backpropagation algorithm. According to Cybenko [2], for a neural network to satisfy the universal approximation property, the activation function must be bounded,"}, {"title": "1.2. Rectified Power Unit", "content": "One of the attempts to provide good regularity and inductive bias to the architecture is the Rectified Power Unit (RePU), a generalization of ReLU [12]. Early results using RePU include the application of ReQU, a type of RePU, in the DeepRitz method, which is a neural network approach for solving Partial Differential Equations (PDEs) [13]. RePU has also been proposed to construct differentiable neural networks, and error bounds related to this have been analyzed [12]. According to research, using a RePU network allows for the optimal approximation of smooth functions in Sobolev space with optimal depth and width, and the optimal approximation error bound is also provided in [12]. Similarly, another study derived that a shallow RePU network can approximate multivariate functions of a certain function class with finite neural network weight norms and possibly unbounded width [14]. Furthermore, it was shown that the derivatives of a network composed of RePU can be expressed as a mixture of various types of RePU, and an improved upper bound on the complexity of these derivatives was derived. This demonstrates the capability of RePU by showing the risk bound for the deep score matching estimator (DSME) [15]. Despite these results highlighting the good regularity of RePU neural networks, experimental evidence shows that RePU networks suffer from exploding or vanishing values and fail to train properly when the layers are deep."}, {"title": "1.3. Effective Theory of Neural Networks", "content": "The language of quantum field theory (QFT) is useful for analyzing systems with a very high degree of freedom. One such example is artificial neural networks. It is known that the distribution of neural network ensembles follows a Gaussian process when the width is infinite, corresponding to the free"}, {"title": "1.4. Our Contribution", "content": "We theoretically analyze the RePU activation from the perspective of effective theory. Specifically, we calculate the susceptibility of the RePU activation to theoretically demonstrate that RePU cannot satisfy the criticality condition. We then verify this experimentally by calculating the empirical kernel of the initialized neural network and during the training process. Not only do we identify the fundamental failure of RePU, but we also propose a modified RePU (MRePU) based on theoretical predictions to address these issues. We demonstrate the improvements experimentally by evaluating the empirical kernel and testing performance of MRePU, as compared to RePU."}, {"title": "2. Preliminary", "content": "In this section, we define the neural network and activation functions that will be the subject of our analysis, focusing particularly on the Rectified Power Unit (RePU). Additionally, we explore the effective theory of neural networks, which will be the primary tool for our analysis."}, {"title": "2.1. Overview of Neural Networks and Activation Functions", "content": "We primarily deal with neural networks having a fully connected network (FCN) architecture in this paper. An FCN is composed of affine transformations with hyperparameters weights and biases at each layer, followed by a nonlinear activation function applied component-wise. Specifically, it is structured as defined below:"}, {"title": "Definition (Fully Connected Network (FCN))", "content": "A Fully Connected Network f(x; \u03b8) : \\mathbb{R}^{n_{in}} \\rightarrow \\mathbb{R}^{n_{out}} is defined by the following recursive equations:\nz_{i}^{(1)}(x_{a}) := \\sum_{j=1}^{n_{0}} W_{i,j}^{(1)} x_{a,j} + b_{i}^{(1)}, \\quad \\text{for } i = 1,..., n_{1},\nz_{i}^{(l+1)}(x_{a}) := \\sum_{j=1}^{n_{l}} W_{i,j}^{(l+1)} \\sigma(z_{j}^{(l)}(x_{a})) + b_{i}^{(l+1)}, \\quad \\text{for } i = 1,..., n_{l+1}; l = 1,..., L-1.\n(1)\nwhere $n_{in}$ is the input dimension of the neural network, $n_{out}$ is the output dimension of the neural network, and each $n_{l}$ is the width (i.e., the number of nodes at the $l$-th layer). $L$ is the depth of stacked layers. The parameters $(b_{i}^{(l)})_{i=1,...,n_{l}, l=1,...,L}$ , $(W_{i,j}^{(l)})_{i,j=1,...,n_{l-1}, l=1,...,L}$ are bias vectors and weight matrices, respectively. $z_{i}^{(l)}$ is called the preactivation at the $l$-th layer. $\\sigma: \\mathbb{R} \\rightarrow \\mathbb{R}$ is an activation function that acts on each node of the preactivations.\nIn the previous definition of neural networks, we also explained what an activation function is. The connections between the nodes of a neural network can be seen as a type of graph, and this graph representing the connections of the neural network is called the architecture. Besides the architecture, there are several factors that determine the properties of a neural network (training dynamics, inductive bias, approximation properties, etc.). In this paper, we focus on the initialization of the weight and bias parameters and the activation function."}, {"title": "Definition (Initialization distribution of biases and weights)", "content": "In the definition of neural networks, there are adjustable weights and biases parameters. The method of setting these parameters at the beginning of training is called initialization, and typically, each parameter follows a specific probability distribution, referred to as the initialization distribution. Although it is possible to use mathematically complex initialization distributions, it is common practice to assign each weight and bias under the condition of being independent and identically distributed (i.i.d). The probability distributions used can include Gaussian distribution, truncated Gaussian distribution, uniform distribution, and others. For wide neural networks, it is known that when the mean of the weight distribution is 0 and the variance is constant, differences arising from different types of distributions are suppressed by $\\text{width}^{-1}$. Therefore, for the sake of convenience in our discussion, we will set the distributions of weights and biases to follow a mean-zero Gaussian"}, {"title": "distribution:", "content": "\\mathbb{E}[b_{i}^{(l)} b_{j}^{(l)}] = \\delta_{ij} C_{b}^{(l)}\n\\mathbb{E}[W_{i_{1}j_{1}}^{(l)} W_{i_{2}j_{2}}^{(l)}] = \\delta_{i_{1}i_{2}} \\delta_{j_{1}j_{2}} \\frac{C_{W}^{(l)}}{n_{l-1}}\n(2)\nwhere the set of bias variances $\\{C_{b}^{(1)}, ..., C_{b}^{(L)}\\}$ and the set of rescaled weight variances $\\{C_{W}^{(1)},..., C_{W}^{(L)}\\}$ are called initialization hyperparameters.\nActivation functions Since the inception of artificial neural networks, various types of activation functions have been proposed. An activation function is essentially a 1D real scalar-valued function that acts on the preactivation, which is the output of a node. Because the capabilities of a neural network are greatly influenced by the type of activation function used, extensive research has been conducted in this area. We provide a brief summary of the activation functions proposed so far through Table 1."}, {"title": "Definition (Rectified Power Unit (RePU)", "content": "The RePU activation function is defined by the following equation:\n\\sigma(z) = \\begin{cases}\nz^{p}, & \\text{if } z \\geq 0 \\\\\n0, & \\text{if } z < 0\n\\end{cases}\nwhere $p$ is a positive integer that determines the power to which the input $z$ is raised. When $p = 1$, it corresponds to ReLU, and thus RePU can be considered a generalized version of ReLU. The cases where $p = 2$ and $p = 3$ are referred to as Rectified Quadratic Unit (ReQU) and Rectified Cubic Unit (ReCU), respectively. For RePU with $p = k$, it is easy to see that it is differentiable $k - 1$ times, and this property ensures that the neural network and its gradients are differentiable functions."}, {"title": "2.2. Effective Field Theory for Neural Networks", "content": "We utilize the framework of effective field theory to understand the distribution of preactivations in a neural network ensemble sampled from the initialization distribution. To accomplish this, we first define the necessary concepts."}, {"title": "Definition (M-point correlators)", "content": "The moments or $M$-point correlators of a probability distribution $p(z) = p(z_{1}, ..., z_{M})$ are defined as follows:\n\\mathbb{E}[z_{\\mu_{1}} \\cdots z_{\\mu_{M}}] = \\int z_{\\mu_{1}} \\cdots z_{\\mu_{M}} p(z) d\\nu(z).\nThe collection of all $M$-point correlators fully characterizes the probability distribution since the information from the $M$-point correlators allows us to compute the expected values of analytic observables. For a Gaussian distribution, it is important to note that the distribution's information can be fully specified by the 1-point correlator (mean) and the 2-point correlator (variance). Keeping this in mind, we slightly modify the $M$-point correlator to define what is known as the connected correlator (also known as cumulant). The definition is as follows:"}, {"title": "Definition (M-point connected correlators)", "content": "For $2 < M$, the moments or $M$-point correlators of a probability distribution $p(z)$ are defined as follows:\n\\mathbb{E}[z_{\\mu_{1}} \\cdots z_{\\mu_{M}}]_{| \\text{connected}} = \\mathbb{E}[z_{\\mu_{1}} \\cdots z_{\\mu_{M}}] - \\sum_{\\text{all subdivisions of } (\\mu_{1},...,\\mu_{M})} \\mathbb{E}[z_{\\mu_{i_{1}}} \\cdots z_{\\mu_{i_{k}}}]_{| \\text{connected}} \\mathbb{E}[z_{\\mu_{i_{k+1}}} \\cdots z_{\\mu_{i_{M}}}]_{| \\text{connected}}.\nFor $M = 1$, the connected correlator coincides with the regular correlator. For parity-symmetric distributions, it also coincides for $M = 2$. According to Wick's theorem, it is known that for a Gaussian distribution, the connected correlators for $M > 2$ are zero. Therefore, higher-point connected correlators can be used to determine how much the distribution deviates from a Gaussian distribution. Additionally, if the connected correlators for $M > 2$ are small, the distribution is defined as nearly-Gaussian.\nUsing these concepts, we can analyze the probability distribution of the preactivations of an ensemble following the initialization distribution. Specifically, since each layer depends on the previous layer, marginalization reveals that there are recursive relations between the statistics of the probability distributions of the layers."}, {"title": "Induced distributions", "content": "Following the notation in (1), let $p(z^{(l)}|D)$ denote the probability distribution of the preactivations in the $l$-th layer given a dataset $D = \\{x_{i;a}\\}_{i=1,...,n_{0};a=1,...,N_{D}}$. For the first layer, the preactivation has $M > 2$ connected correlators equal to 0, and has the following mean and covariance, indicating that each preactivation follows an independent Gaussian distribution.\n\\mathbb{E}[z_{i}^{(1)}] = \\mathbb{E}[b_{i}^{(1)} + \\sum_{j=0}^{n_{0}} W_{i,j}^{(1)} x_{j;a}] = 0,\n\\mathbb{E}[z_{i_{1};a_{1}}^{(1)} z_{i_{2};a_{2}}^{(1)}] = \\mathbb{E}[(b_{i_{1}}^{(1)} + \\sum_{j=0}^{n_{0}} W_{i_{1},j}^{(1)} x_{j;a_{1}})(b_{i_{2}}^{(1)} + \\sum_{j=0}^{n_{0}} W_{i_{2},j}^{(1)} x_{j;a_{2}})]\n= \\delta_{i_{1}i_{2}} C_{b}^{(1)} + C_{W}^{(1)} \\frac{1}{n_{0}} \\sum_{j=1}^{n_{0}} x_{j;a_{1}} x_{j;a_{2}} = \\delta_{i_{1}i_{2}} C_{b}^{(1)} + G_{a_{1} a_{2}}^{(1)}.\nWhere $G_{a_{1} a_{2}}^{(1)}$ is called as the metric, which represents the two-point correlator of the preactivations for different samples. Similarly, the conditional probability $p(z^{(l+1)}|z^{(l)})$ is also described by a mean-zero Gaussian distribution defined by the covariance $G_{a_{1} a_{2}}^{(l+1)}$, where the distribution (i.e. $G_{a_{1} a_{2}}^{(l+1)}$) depends on the condition $z^{(l)}$. We refer to this $G_{a_{1} a_{2}}^{(l+1)}$ as the $l+1$-th layer stochastic metric. We can consider the mean metric $\\bar{G}_{a_{1} a_{2}}^{(l+1)}$, which is the average of the stochastic metric. Additionally, we can consider the variance of $\\Delta G_{a_{1} a_{2}}^{(l+1)} = G_{a_{1} a_{2}}^{(l+1)} - \\bar{G}_{a_{1} a_{2}}^{(l+1)}$, which is the fluctuation of the stochastic metric around the mean metric. We refer to scaled quantity of this variance as the"}, {"title": "four-point vertex,", "content": "defined as follows:\nV_{(a_{1}a_{2}) (a_{3}a_{4})}^{(l+1)} := \\eta_{l} \\mathbb{E} [\\Delta G_{a_{1} a_{2}}^{(l+1)} \\Delta G_{a_{3} a_{4}}^{(l+1)}] - \\eta_{l} [\\pi (G_{a_{1} a_{2}}^{(l-1)} G_{a_{3} a_{4}}^{(l-1)})] - G_{a_{1} a_{2}}^{(l-1)} G_{a_{3} a_{4}}^{(l-1)}).\nThe four-point vertex is related to the 4-point connected correlator as follows:\n\\mathbb{E}[z_{i_{1};a_{1}}^{(l+1)} z_{i_{2};a_{2}}^{(l+1)} z_{i_{3};a_{3}}^{(l+1)} z_{i_{4};a_{4}}^{(l+1)}]_{| \\text{connected}} = \\frac{1}{n_{l}} [\\delta_{i_{1}i_{2}} \\delta_{i_{3}i_{4}} V_{(a_{1}a_{2}) (a_{3}a_{4})}^{(l+1)} + \\delta_{i_{1}i_{3}} \\delta_{i_{2}i_{4}} V_{(a_{1}a_{3}) (a_{2}a_{4})}^{(l+1)} + \\delta_{i_{1}i_{4}} \\delta_{i_{2}i_{3}} V_{(a_{1}a_{4}) (a_{2}a_{3})}^{(l+1)}].\n(3)\nThe quantities we have just defined contain significant information about the distribution of the preactivation $p(z^{(l)}|D)$. If we define the action $S$ of $p(z^{(l)}|D)$ as follows (parity symmetry can be easily checked):\nS := -\\frac{1}{2} \\sum_{a_{1},a_{2} \\in D} \\sum_{i_{1},i_{2} = 1}^{n_{l}} G_{a_{1}a_{2}}^{(l)} z_{i;a_{1}}^{(l)} z_{i;a_{2}}^{(l)} - \\frac{1}{8} \\sum_{a_{1},...,a_{4} \\in D} \\sum_{i_{1},...,i_{4} = 1}^{n_{l}} V_{(a_{1}a_{2}) (a_{3}a_{4})}^{(l)} z_{i;a_{1}}^{(l)} z_{i;a_{2}}^{(l)} z_{i;a_{3}}^{(l)} z_{i;a_{4}}^{(l)} + ....\nIt is known that the following relationship holds for the coefficients. This can be derived by comparing the coefficients of the connected correlator given by the mean metric and four-point vertex with those of the connected correlator assumed by the ansatz provided by action S, $G_{a_{1} a_{2}}^{(l)}$ and $V_{(a_{1}a_{2}) (a_{3}a_{4})}^{(l)}$ [18]:\n\\bar{G}_{a_{1}a_{2}}^{(l)} = G_{a_{1}a_{2}}^{(l)} + O(\\frac{1}{n_{l-1}}),\n\\bar{V}_{(a_{1}a_{2}) (a_{3}a_{4})}^{(l)} = \\frac{1}{n_{l}} \\bar{G}_{a_{1}a_{2}}^{(l)} \\bar{G}_{a_{3}a_{4}}^{(l)} V_{(a_{1}a_{2}) (a_{3}a_{4})}^{(l)} + O(\\frac{1}{n_{l}}).\nWhere, $G_{a_{1}a_{2}}^{(l)}$ is inverse of mean metric $G_{a_{1}a_{2}}$ and $V_{(a_{1}a_{2}) (a_{3}a_{4})}^{(l)}$ is defined as follows:\nV_{(a_{1}a_{2}) (a_{3}a_{4})}^{(l)} := \\sum_{\\beta_{1},...,\\beta_{4} \\in D} G_{\\alpha_{1} \\beta_{1}}^{(l)} G_{\\alpha_{2} \\beta_{2}}^{(l)} G_{\\alpha_{3} \\beta_{3}}^{(l)} G_{\\alpha_{4} \\beta_{4}}^{(l)} V_{(\\beta_{1} \\beta_{2}) (\\beta_{3} \\beta_{4})}^{(l)}.\nKernels and Representation Group flow Now, we will investigate how the statistics of the preactivation change as they pass through the layers. We will refer to the evolution of the distribution through the layers as the representation group flow, following the terminology of [18]. While it is"}, {"title": "Susceptibility", "content": "When expanding the kernel matrix for two inputs $(x_{i;+})$ and $(x_{i;-})$, it can be written as follows:\nK^{(l)} = \\begin{pmatrix} K_{++}^{(l)} & K_{+-}^{(l)} \\\\ K_{-+}^{(l)} & K_{--}^{(l)} \\end{pmatrix} = K_{0}^{(l)} \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} + K_{1}^{(l)} \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix} + K_{2}^{(l)} \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}.\nWhere, $K_{0}^{(l)} := \\mathbb{E} [\\frac{1}{4} z_{+}^{(l)} z_{+}^{(l)}] $, $K_{1}^{(l)} := \\mathbb{E} [\\frac{1}{4} z_{+}^{(l)} z_{+}^{(l)}] $. The coefficients $K_{\\alpha}^{(l)}$ are given by:\nK_{0}^{(l)} = \\frac{1}{4} [K_{++}^{(l)} + K_{+-}^{(l)} + K_{-+}^{(l)} + K_{--}^{(l)}],\nK_{1}^{(l)} = \\frac{1}{4} [K_{++}^{(l)} - K_{+-}^{(l)} - K_{-+}^{(l)} + K_{--}^{(l)}],\nK_{2}^{(l)} = \\frac{1}{4} [K_{++}^{(l)} - K_{--}^{(l)}]."}, {"title": "If we express", "content": "$(x_{i;+})$ and $(x_{i;-})$ as $x_{i;\\pm} = \\frac{x_{i;+} + x_{i;-}}{2} \\pm \\frac{x_{i;+} - x_{i;-}}{2} = x_{i} \\pm \\delta x_{i}$, it can be seen that as $(\\delta x_{i})$ approaches zero, the coefficients $K_{++}^{(l)}, K_{+-}^{(l)}, K_{2}^{(l)}$ also approach zero. Consequently, the coefficients can be expanded as follows:\nK_{1}^{(l)} = K_{0}^{(l)} + \\delta K_{1}^{(l)} + \\delta^{2} K_{2}^{(l)} + O(\\delta^{4}),\nK_{1}^{(l)} = K_{0}^{(l)} + \\delta^{2} K_{1}^{(l)} + \\delta^{4} K_{2}^{(l)} + O(\\delta^{5}),\nK_{2}^{(l)} = K_{0}^{(l)} + \\delta^{2} K_{1}^{(l)} + \\delta^{4} K_{2}^{(l)} + O(\\delta^{6}).\nThen, it is known that the following recursion can be derived [18]:\nK^{(l+1)} = C_{b} + C_{W} g(K^{(l)}),\n\\delta K^{(l+1)} = X_{||} (K^{(l)}) \\delta K^{(l)},\n\\delta^{2} K^{(l+1)} = X_{\\bot} (K^{(l)}) \\delta^{2} K^{(l)} + h(K^{(l)}) (\\delta K^{(l)})^{2}.\nWhere $K^{(l+1)}$ is the kernel for a single input, and each $g, h, X_{||}, X_{\\bot}$ are defined as follows:\ng(K) := \\langle \\sigma(z) \\sigma(z) \\rangle_{K},\nX_{||}(K) := C_{W} g'(K) = \\frac{C_{W}}{K} \\langle z \\sigma'(z) \\sigma(z) \\rangle_{K},\nX_{\\bot}(K) := C_{W} \\langle \\sigma'(z) \\sigma'(z) \\rangle_{K},\nh(K) := \\frac{C_{W}}{K} (\\langle z^{2} \\rangle - K) \\frac{1}{2} \\frac{d}{dK} X_{\\bot}(K).\n(5)\nIn this context, we refer to $X_{||}$ and $X_{\\bot}$ as the parallel susceptibility and perpendicular susceptibility, respectively. It is known that for the kernel matrix to be preserved, and to ensure that the size or distance of the representation neither diverges nor vanishes, both susceptibilities must equal 1."}, {"title": "3. Failure of RePU Activation", "content": null}, {"title": "3.1. Susceptibility Calculation", "content": "Activation functions that exhibit similar behaviors according to the RG flow are classified into universality classes. According to [18], scale-invariant activations (e.g. ReLU, LeakyReLU) form a single universality class called the self-invariant universality class. Tanh and Sin fall into the $K^{*} = 0$ universality class, while SWISH and GELU belong to the half-stable universality"}, {"title": "follows:", "content": "X_{||}(K) = \\frac{C_{W}}{K} \\langle z \\sigma'(z) \\sigma(z) \\rangle_{K} = \\frac{C_{W}}{K} \\langle z | z^{p-1} | z^{p} + \\mathbb{I}_{z<0} | \\rangle_{K}\n= \\frac{C_{W} p}{K \\sqrt{2 \\pi K}} \\int_{0}^{\\infty} z^{2 p} e^{-\\frac{z^{2}}{2 K}} d z = \\frac{C_{W} p}{K} \\frac{\\sqrt{\\pi}  (2 p-1) ! ! K^{p}}{2}\\sqrt{2K}\\nX_{\\bot}(K) = C_{W} \\langle \\sigma'(z) \\sigma'(z) \\rangle_{K} = C_{W} \\langle p^{2} [z^{p-1}]_{+} [z^{p-1}]_{+} \\rangle_{K}\n= \\frac{C_{W} p^{2}}{\\sqrt{2 \\pi K}} \\int_{0}^{\\infty} z^{2(p-1)} e^{-\\frac{z^{2}}{2 K}} d z = \\frac{C_{W} p^{2} (2 p - 3) ! ! K^{p-1}}{2}\\sqrt{2K}\n(6)\nThe parallel susceptibility to perpendicular susceptibility ratio is $2 p - 1 : p$. Therefore, except for $p = 1$ (i.e., ReLU), it is impossible to set both susceptibilities to 1 simultaneously. If we set the parallel susceptibility to 1, the perpendicular susceptibility will be less than 1, causing the distance between two inputs to vanish as they propagate through the layers. Conversely, if we set the perpendicular susceptibility to 1, the parallel susceptibility will be greater than 1, causing each input to diverge as it propagates through the layers."}, {"title": "3.2. Experimental Validation", "content": "Empirical Kernel Behavior at Initialization According to the results in section 3.1, the kernel value of Neural Networks with RePU (for $p > 1$) for a single input must either explode or vanish as it propagates through the layers under the initialization distribution. In this section, we experimentally verify this. For the experiment, the neural network is configured with an input dimension of 2, an output dimension of 1, hidden layers with widths of 512, and a depth of 10 (therefore, the total number of layers, including the input and output layers, is 12). The activation function is RePU. The initialization distribution follows equation (2), with $C_{b}$ set to 0, and $C_{W}"}, {"title": "4. Modified Rectified Power Unit (MRePU)", "content": "In this section, we propose the modified Rectified Power Unit (MRePU) function to overcome the limitations of RePU, which cannot be stacked deeply. MRePU retains the advantages of RePU (e.g., differentiability) while fundamentally addressing the existing issues. The definition of MRePU is as follows:"}, {"title": "Definition (Modified Rectified Power Unit (MRePU)", "content": "The MRePU activation function of order $p > 1$ is defined by the following equation:\n\\sigma(z) = \\begin{cases}\nz (z+1)^{p}, & \\text{if } z > -1 \\\\\n0, & \\text{if } z < -1\n\\end{cases}\nSimilar to RePU, the MRePU function of order $p$ has the property of being differentiable $p-1$ times."}, {"title": "4.1. Numerical Calculation of Susceptibility", "content": "Here, we calculate the parallel and perpendicular susceptibilities of the MRePU function we proposed. The susceptibility of the MRePU function of order $p$ can be expressed by the following equation:\nX_{||}(K) = \\frac{C_{W}}{K} \\langle z \\sigma'(z) \\sigma(z) \\rangle_{K}\n= \\frac{C_{W}}{K \\sqrt{2 \\pi K}} \\int_{-1}^{\\infty} ((z+1)^{2 p-1} ((p+1) z+1) z e^{-\\frac{z^{2}}{2 K}} d z\nX_{\\bot}(K) = C_{W} \\langle \\sigma'(z) \\sigma'(z) \\rangle_{K}\n= \\frac{C_{W}}{\\sqrt{2 \\pi K}} \\int_{-1}^{\\infty} ((z+1)^{2 p-2} ((p+1) z+1)^{2}) e^{-\\frac{z^{2}}{2 K}} d z.\n(7)\nSince getting value of (7) analytically is impossible, we calculate the susceptibility numerically. We fixed the value of $C_{W}$ at 1.0 and plotted graphs by exponentially increasing the value of $K$ for cases where $p=2$ and $p=3$. The results can be seen in Figure 4, where it is observed that the ratio of the two susceptibilities converges to 1 as the value of $K$ approaches 0. This confirms that the MRePU activation belongs to the $K^{*}=0$ universality class, and by appropriately adjusting the $C_{W}$ value, it is possible to ensure that the kernel neither explodes nor vanishes."}, {"title": "4.2. Experimental Results", "content": "Now, as we did with RePU Neural Networks, we will investigate the statistical properties of MRePU under the same experimental settings.\nEmpirical Kernel and Four-point Vertex Behavior at Initialization As with the RePU case, we investigate the statistical properties of the neural network ensemble given by the initialization distribution. The experimental environment remains identical to that of the RePU Networks, with the only change being the activation function, which is now MRePU with $p=2$. The results for $C_{W}=1.0$ are shown in Figure 5. As can be seen from the graph, the empirical kernel maintains a consistent scale across both random data and weight parameters. Additionally, we also examined the behavior of the Four-point vertex for MRePU. Equation (4) can be simplified for a single input as follows:\nV_{K^{(1)}}^{(l+1)} = X_{||}^{2}(K^{(l)}) V_{K^{(1)}}^{(l)} + C_{W} \\left[\\frac{\\langle \\sigma'(z) z \\rangle}{\\langle \\sigma^{2}(z) \\rangle} K^{(l)} - \\frac{\\langle \\sigma^{2}(z) \\rangle}{n_{0}} \\right] + O(\\frac{1}{n_{l}}).\nFigure 6 illustrates how the empirical four-point vertex, calculated from an ensemble of 1000 models using equation (3), changes across layers (with input $x_{0}=(1,0)$). $X_{||}$ increases as the kernel grows, and since the kernel itself depends on $C_{W}$, the inductive change in the four-point vertex is determined"}, {"title": "is added.", "content": "by $C_{W}$ according to equation (4.2). As evidence of this, the left figure shows that the four-point vertex increases more steeply as $C_{W}$ becomes larger. We also investigated the behavior of the four-point vertex across different widths. The results indicate that with smaller widths, perturbative effects in equation (4.2) become more pronounced, leading to a more rapid increase in the four-point vertex, and overall, higher values are observed. The four-point vertex serves as an indicator of non-Gaussianity, which is expected to increase as more layers with non-linear activations Moreover, since smaller widths imply stronger interactions between neurons, these experimental results align well with our theoretical predictions."}, {"title": "Dynamics of Statistics During Training", "content": "Now, we will examine how the statistics of the MRePU Network ensemble change under training dynamics. The experimental setup is the same as in the case of the RePU Network, except that the number of hidden layers is set to 7. Although the theoretical approximation suggests setting $C_{W}=1.0$, meaningful learning was only achieved with a slightly lower value, possibly due to the perturbative effects in the MRePU Network. In our experiments, we set $C_{W}=0.9$. The dynamics of the kernel and four-point vertex are depicted in Figure 7, while the comparison of the ensemble's mean output"}, {"title": "5. Conclusion", "content": "In this paper, we analyzed neural networks using Effective Field Theory to theoretically predict and experimentally verify why neural network architectures with RePU activation fail when the network depth increases. Furthermore, we proposed the MRePU activation function, which improves upon RePU, and estimated its effectiveness theoretically. We then empirically confirmed its feasibility by comparing the experimental results with the theoretical predictions. The proposed MRePU was able to facilitate learning even in very deep networks without adopting structures like ResNet. Mathematically, we confirmed that MRePU retains the favorable properties of RePU, such as p-differentiability. This suggests that the beneficial characteristics proposed in previous studies on RePU can also be extended to MRePU. Moreover, when attempting to approximate a function with a specific differentiability class, MRePU can be used to introduce inductive bias into deep neural networks, thereby enabling effective learning."}]}