{"title": "Why Rectified Power Unit Networks Fail and How to\nImprove It: An Effective Theory Perspective", "authors": ["Taeyoung Kim", "Myungjoo Kang"], "abstract": "The Rectified Power Unit (RePU) activation functions, unlike the Rectified\nLinear Unit (ReLU), have the advantage of being a differentiable function\nwhen constructing neural networks. However, it can be experimentally ob-\nserved when deep layers are stacked, neural networks constructed with RePU\nencounter critical issues. These issues include the values exploding or vanish-\ning and failure of training. And these happen regardless of the hyperparame-\nter initialization. From the perspective of effective theory, we aim to identify\nthe causes of this phenomenon and propose a new activation function that\nretains the advantages of RePU while overcoming its drawbacks.", "sections": [{"title": "1. Introduction", "content": "An activation function is a function that acts on the output of each layer\nof a neural network. The remarkable success of deep neural networks is\nclosely related to the choice of appropriate nonlinear activation functions,\nand there has been extensive discussion solely on the research of activation\nfunctions. Early studies on neural networks include the perceptron [1], where\nthe activation function is the sign function, and it was composed of only one\nor two layers. The perceptron had the drawback that its derivative was\nalmost everywhere zero, making it difficult to apply the backpropagation\nalgorithm. According to Cybenko [2], for a neural network to satisfy the\nuniversal approximation property, the activation function must be bounded,"}, {"title": "1.2. Rectified Power Unit", "content": "One of the attempts to provide good regularity and inductive bias to the\narchitecture is the Rectified Power Unit (RePU), a generalization of ReLU\n[12]. Early results using RePU include the application of ReQU, a type of\nRePU, in the DeepRitz method, which is a neural network approach for solv-\ning Partial Differential Equations (PDEs) [13]. RePU has also been proposed\nto construct differentiable neural networks, and error bounds related to this\nhave been analyzed [12]. According to research, using a RePU network al-\nlows for the optimal approximation of smooth functions in Sobolev space\nwith optimal depth and width, and the optimal approximation error bound\nis also provided in [12]. Similarly, another study derived that a shallow RePU\nnetwork can approximate multivariate functions of a certain function class\nwith finite neural network weight norms and possibly unbounded width [14].\nFurthermore, it was shown that the derivatives of a network composed of\nRePU can be expressed as a mixture of various types of RePU, and an im-\nproved upper bound on the complexity of these derivatives was derived. This\ndemonstrates the capability of RePU by showing the risk bound for the deep\nscore matching estimator (DSME) [15]. Despite these results highlighting\nthe good regularity of RePU neural networks, experimental evidence shows\nthat RePU networks suffer from exploding or vanishing values and fail to\ntrain properly when the layers are deep."}, {"title": "1.3. Effective Theory of Neural Networks", "content": "The language of quantum field theory (QFT) is useful for analyzing sys-\ntems with a very high degree of freedom. One such example is artificial neural\nnetworks. It is known that the distribution of neural network ensembles fol-\nlows a Gaussian process when the width is infinite, corresponding to the free"}, {"title": "1.4. Our Contribution", "content": "We theoretically analyze the RePU activation from the perspective of\neffective theory. Specifically, we calculate the susceptibility of the RePU ac-\ntivation to theoretically demonstrate that RePU cannot satisfy the criticality\ncondition. We then verify this experimentally by calculating the empirical\nkernel of the initialized neural network and during the training process. Not\nonly do we identify the fundamental failure of RePU, but we also propose a\nmodified RePU (MRePU) based on theoretical predictions to address these\nissues. We demonstrate the improvements experimentally by evaluating the\nempirical kernel and testing performance of MRePU, as compared to RePU."}, {"title": "2. Preliminary", "content": "In this section, we define the neural network and activation functions\nthat will be the subject of our analysis, focusing particularly on the Rectified\nPower Unit (RePU). Additionally, we explore the effective theory of neural\nnetworks, which will be the primary tool for our analysis."}, {"title": "2.1. Overview of Neural Networks and Activation Functions", "content": "We primarily deal with neural networks having a fully connected network\n(FCN) architecture in this paper. An FCN is composed of affine transfor-\nmations with hyperparameters weights and biases at each layer, followed by\na nonlinear activation function applied component-wise. Specifically, it is\nstructured as defined below:"}, {"title": "Definition (Fully Connected Network (FCN))", "content": "A Fully Connected Net-\nwork $f(x; \\theta) : \\mathbb{R}^{n_{in}} \\rightarrow \\mathbb{R}^{n_{out}}$ is defined by the following recursive equations:\n\n$z_{i}^{(1)}(x_{a}) := \\sum_{j=1}^{n_{0}} W_{i j}^{(1)} x_{j; a} + b_{i}^{(1)}$, for $i=1, ..., n_{1}$,\n\n$z_{i}^{(l+1)}(x_{a}) := \\sum_{j=1}^{n_{l}} W_{i j}^{(l+1)} \\sigma(z_{j}^{(l)}(x_{a})) + b_{i}^{(l+1)}$, for $i=1, ..., n_{l+1}; l=1, ..., L-1$.\n\n(1)\n\nwhere $n_{in}$ is the input dimension of the neural network, $n_{out}$ is the output\ndimension of the neural network, and each $n_{l}$ is the width (i.e., the number\nof nodes at the $l$-th layer). L is the depth of stacked layers. The parameters\n$(b_{i}^{(l)})_{l=1, ..., L, i=1, ..., n_{l}}$, $(W_{i j}^{(l)})_{l=1, ..., L, i=1, ..., n_{l}, j=1, ..., n_{l-1}}$ are bias vectors and weight matrices, re-\nspectively. $z_{i}^{(l)}$ is called the preactivation at the $l$-th layer. $\\sigma : \\mathbb{R} \\rightarrow \\mathbb{R}$ is\nan activation function that acts on each node of the preactivations.\n\nIn the previous definition of neural networks, we also explained what an\nactivation function is. The connections between the nodes of a neural network\ncan be seen as a type of graph, and this graph representing the connections\nof the neural network is called the architecture. Besides the architecture,\nthere are several factors that determine the properties of a neural network\n(training dynamics, inductive bias, approximation properties, etc.). In this\npaper, we focus on the initialization of the weight and bias parameters and\nthe activation function."}, {"title": "Definition (Initialization distribution of biases and weights)", "content": "In the\ndefinition of neural networks, there are adjustable weights and biases param-\neters. The method of setting these parameters at the beginning of training is\ncalled initialization, and typically, each parameter follows a specific proba-\nbility distribution, referred to as the initialization distribution. Although\nit is possible to use mathematically complex initialization distributions, it\nis common practice to assign each weight and bias under the condition of\nbeing independent and identically distributed (i.i.d). The probability distri-\nbutions used can include Gaussian distribution, truncated Gaussian distribu-\ntion, uniform distribution, and others. For wide neural networks, it is known\nthat when the mean of the weight distribution is 0 and the variance is con-\nstant, differences arising from different types of distributions are suppressed\nby width. Therefore, for the sake of convenience in our discussion, we will\nset the distributions of weights and biases to follow a mean-zero Gaussian"}, {"title": "distribution:", "content": "$\\mathbb{E}[b_{i}^{(l)} b_{j}^{(l)}] = \\delta_{i j} C_{b}^{(l)}$\n\n$\\mathbb{E}[W_{i_{1} j_{1}}^{(l)} W_{i_{2} j_{2}}^{(l)}] = \\delta_{i_{1} i_{2}} \\delta_{j_{1} j_{2}} \\frac{C_{W}^{(l)}}{n_{l-1}}$\n\n(2)\n\nwhere the set of bias variances $\\{C_{b}^{(1)}, ..., C_{b}^{(L)}\\}$ and the set of rescaled weight\nvariances $\\{C_{W}^{(1)}, ..., C_{W}^{(L)}\\}$ are called initialization hyperparameters.\nActivation functions Since the inception of artificial neural networks, vari-\nous types of activation functions have been proposed. An activation function\nis essentially a 1D real scalar-valued function that acts on the preactivation,\nwhich is the output of a node. Because the capabilities of a neural network\nare greatly influenced by the type of activation function used, extensive re-\nsearch has been conducted in this area. We provide a brief summary of the\nactivation functions proposed so far through Table 1."}, {"title": "Definition (Rectified Power Unit (RePU)", "content": "The RePU activation func-\ntion is defined by the following equation:\n\n$\\sigma(z) = \\begin{cases} z^{p}, & \\text{if } z \\geq 0 \\\\\n0, & \\text{if } z < 0 \\end{cases}$"}, {"title": "2.2. Effective Field Theory for Neural Networks", "content": "We utilize the framework of effective field theory to understand the dis-\ntribution of preactivations in a neural network ensemble sampled from the\ninitialization distribution. To accomplish this, we first define the necessary\nconcepts."}, {"title": "Definition (M-point correlators)", "content": "The moments or $M$-point correlators\nof a probability distribution $p(z)=p(z_{1}, ..., z_{M})$ are defined as follows:\n\n$\\mathbb{E}[z_{\\mu_{1}} \\cdots z_{\\mu_{M}}] = \\int z_{\\mu_{1}} \\cdots z_{\\mu_{M}} p(z) d v(z)$.\n\nThe collection of all $M$-point correlators fully characterizes the proba-\nbility distribution since the information from the $M$-point correlators allows\nus to compute the expected values of analytic observables. For a Gaussian\ndistribution, it is important to note that the distribution's information can\nbe fully specified by the 1-point correlator (mean) and the 2-point correlator\n(variance). Keeping this in mind, we slightly modify the $M$-point correlator\nto define what is known as the connected correlator (also known as cumu-\nlant). The definition is as follows:"}, {"title": "Definition (M-point connected correlators)", "content": "For $2 < M$, the moments or\n$M$-point correlators of a probability distribution $p(z)$ are defined as follows:\n\n$\\mathbb{E}[z_{\\mu_{1}} \\cdots z_{\\mu_{M}}]_{\\text { connected }} = \\mathbb{E}[z_{\\mu_{1}} \\cdots z_{\\mu_{M}}] - $\n$\\sum_{\\text { all subdivisions of } (\\mu_{1}, ..., \\mu_{M})} \\mathbb{E}[z_{\\mu_{i_{1}}} \\cdots z_{\\mu_{i_{k}}}]_{\\text { connected }} \\mathbb{E}[z_{\\mu_{i_{1}}} \\cdots z_{\\mu_{i_{k'}}} ]_{\\text { connected }} \\cdots $"}, {"title": "Induced distributions", "content": "Following the notation in (1), let $p(z^{(l)} | \\mathcal{D})$ denote\nthe probability distribution of the preactivations in the $l$-th layer given a\ndataset $\\mathcal{D} = \\{x_{i; a}\\}_{i=1, ..., n_{0} ; a=1, ..., N_{D}}$. For the first layer, the preactivation\nhas $M>2$ connected correlators equal to 0, and has the following mean\nand covariance, indicating that each preactivation follows an independent\nGaussian distribution.\n\n$\\mathbb{E}[z_{i}^{(1)}] = \\mathbb{E}[\\sum_{j=0}^{n_{0}} W_{i j}^{(1)} x_{j; a}] = 0$,\n\n$\\mathbb{E}[z_{i_{1}; a_{1}}^{(1)} z_{i_{2}; a_{2}}^{(1)}] = \\mathbb{E}[(\\sum_{j=0}^{n_{0}} W_{i_{1} j}^{(1)} x_{j; a_{1}}) (\\sum_{j=0}^{n_{0}} W_{i_{2} j}^{(1)} x_{j; a_{2}})] = \\delta_{a_{1} a_{2}} [\\delta_{i_{1} i_{2}} (C_{b}^{(1)} + \\frac{C_{W}^{(1)}}{n_{0}} ) + C_{b}^{(1)} \\sum_{j=0}^{n_{0}} x_{j; a_{1}} x_{j; a_{2}}] := \\delta_{i_{1} i_{2}} G_{a_{1} a_{2}}^{(1)}$\n\nWhere $G_{a_{1} a_{2}}^{(1)}$ is called as the metric, which represents the two-point corre-\nlator of the preactivations for different samples. Similarly, the conditional\nprobability $p(z^{(l+1)} | z^{(l)})$ is also described by a mean-zero Gaussian distri-\nbution defined by the covariance $G_{a_{1} a_{2}}^{(l+1)} $, where the distribution (i.e.$G_{a_{1} a_{2}}^{(l+1)}$)\ndepends on the condition $z^{(l)}$. We refer to this $G_{a_{1} a_{2}}^{(l+1)}$ as the $l+1$-th layer\nstochastic metric. We can consider the mean metric $\\overline{G_{a_{1} a_{2}}^{(l+1)}}$, which is the\naverage of the stochastic metric. Additionally, we can consider the variance\nof $\\Delta G_{a_{1} a_{2}}^{(l+1)} = G_{a_{1} a_{2}}^{(l+1)} - \\overline{G_{a_{1} a_{2}}^{(l+1)}}$, which is the fluctuation of the stochastic metric\naround the mean metric. We refer to scaled quantity of this variance as the"}, {"title": "four-point vertex, defined as follows:", "content": "$\\mathcal{V}_{(\\alpha_{1} \\alpha_{2}) (\\alpha_{3} \\alpha_{4})}^{(l+1)} := \\eta \\mathbb{E} [\\Delta G_{\\alpha_{1} \\alpha_{2}}^{(l+1)} \\Delta G_{\\alpha_{3} \\alpha_{4}}^{(l+1)} ] - \\eta [\\frac{\\pi}{n_{l}} (\n\\overline{G_{\\alpha_{1} \\alpha_{2}}}^{(l+1)} \\overline{G_{\\alpha_{3} \\alpha_{4}}}^{(l+1)}) ]$\n\nThe four-point vertex is related to the 4-point connected correlator as follows:\n\n$\\mathbb{E} [z_{i_{1}; \\alpha_{1}}^{(l+1)} z_{i_{2}; \\alpha_{2}}^{(l+1)} z_{i_{3}; \\alpha_{3}}^{(l+1)} z_{i_{4}; \\alpha_{4}}^{(l+1)}]_{\\text{ connected }} = $\n$\\frac{1}{n_{l}} [\\delta_{i_{1} i_{2}} \\delta_{i_{3} i_{4}} \\mathcal{V}_{(\\alpha_{1} \\alpha_{2}) (\\alpha_{3} \\alpha_{4})}^{(l+1)} + \\delta_{i_{1} i_{3}} \\delta_{i_{2} i_{4}} \\mathcal{V}_{(\\alpha_{1} \\alpha_{3}) (\\alpha_{2} \\alpha_{4})}^{(l+1)} + \\delta_{i_{1} i_{4}} \\delta_{i_{2} i_{3}} \\mathcal{V}_{(\\alpha_{1} \\alpha_{4}) (\\alpha_{2} \\alpha_{3})}^{(l+1)}]$\n\n(3)\n\nThe quantities we have just defined contain significant information about\nthe distribution of the preactivation $p(z^{(l)} | \\mathcal{D})$. If we define the action $S$ of\n$p(z^{(l)} | \\mathcal{D})$ as follows (parity symmetry can be easily checked):\n\n$S := \\frac{1}{2} \\sum_{i=1}^{n_{l}} \\sum_{\\alpha_{1} \\alpha_{2} \\in \\mathcal{D}} G_{\\alpha_{1} \\alpha_{2}}^{(l)} z_{i; \\alpha_{1}}^{(l)} z_{i; \\alpha_{2}}^{(l)} + \\frac{1}{8} \\sum_{i_{1}, i_{2}=1}^{n_{l}} \\sum_{\\alpha_{1}, ..., \\alpha_{4} \\in \\mathcal{D}} \n\\mathcal{V}_{(\\alpha_{1} \\alpha_{2}) (\\alpha_{3} \\alpha_{4})}^{(l)} z_{i_{1}; \\alpha_{1}}^{(l)} z_{i_{1}; \\alpha_{2}}^{(l)} z_{i_{2}; \\alpha_{3}}^{(l)} z_{i_{2}; \\alpha_{4}}^{(l)} +....$\n\nIt is known that the following relationship holds for the coefficients. This can\nbe derived by comparing the coefficients of the connected correlator given by\nthe mean metric and four-point vertex with those of the connected correlator\nassumed by the ansatz provided by action $S$.$ G_{\\alpha_{1} \\alpha_{2}}^{(l)}$ and $\\mathcal{V}_{(\\alpha_{1} \\alpha_{2}) (\\alpha_{3} \\alpha_{4})}^{(l)}$ [18]:\n\n$\\overline{G_{\\alpha_{1} \\alpha_{2}}}^{(l)} = G_{\\alpha_{1} \\alpha_{2}}^{*(l)} + O(\\frac{1}{n_{l-1}})$,\n\n$\\mathcal{V}_{(\\alpha_{1} \\alpha_{2}) (\\alpha_{3} \\alpha_{4})}^{(l)} = \\frac{1}{n_{l}} (\n\\overline{G_{\\alpha_{1} \\alpha_{2}}}^{(l)} \\overline{G_{\\alpha_{3} \\alpha_{4}}}^{(l)} \\mathcal{V}_{(\\alpha_{1} \\alpha_{2}) (\\alpha_{3} \\alpha_{4})}^{(l)} ) + O(\\frac{1}{n_{l-1}})$\n\nWhere, $G_{\\alpha_{1} \\alpha_{2}}^{*(l)}$ is inverse of mean metric $G_{\\alpha_{1} \\alpha_{2}}^{(l)}$ and $\\mathcal{V}_{(\\alpha_{1} \\alpha_{2}) (\\alpha_{3} \\alpha_{4})}^{(l)}$ is defined as\nfollows:\n\n$\\mathcal{V}_{(\\alpha_{1} \\alpha_{2}) (\\alpha_{3} \\alpha_{4})}^{(l)} := \\sum_{\\beta_{1}, ..., \\beta_{4} \\in \\mathcal{D}} G_{\\alpha_{1} \\beta_{1}}^{(l)} G_{\\alpha_{2} \\beta_{2}}^{(l)} G_{\\alpha_{3} \\beta_{3}}^{(l)} G_{\\alpha_{3} \\beta_{4}}^{(l)} \\mathcal{V}_{(\\alpha_{1} \\alpha_{2}) (\\alpha_{3} \\alpha_{4})}^{(l)}$"}, {"title": "Kernels and Representation Group flow", "content": "Now, we will investigate how\nthe statistics of the preactivation change as they pass through the layers.\nWe will refer to the evolution of the distribution through the layers as the\nrepresentation group flow, following the terminology of [18]. While it is"}, {"title": "Susceptibility", "content": "When expanding the kernel matrix for two inputs $(x_{i ; +})$ and\n$(x_{i ;-})$, it can be written as follows:\n\n$K^{(l)} = \n\\begin{pmatrix}\nK_{++}^{(l)} & K_{+-}^{(l)} \\\\\nK_{-+}^{(l)} & K_{--}^{(l)}\n\\end{pmatrix} = \n\\begin{pmatrix}\n1 & 1 \\\\\n1 & 1\n\\end{pmatrix} K_{0}^{(l)} + \n\\begin{pmatrix}\n1 & -1 \\\\\n-1 & 1\n\\end{pmatrix} K_{1}^{(l)} + \n\\begin{pmatrix}\n1 & 1 \\\\\n-1 & -1\n\\end{pmatrix} K_{2}^{(l)} + \n\\begin{pmatrix}\n1 & -1 \\\\\n1 & -1\n\\end{pmatrix} K_{3}^{(l)}$\n\nWhere, $K_{0}^{(l)} := \\mathbb{E}[\\frac{1}{4} x_{i ; +} x_{i ; +}]$, $K_{1}^{(l)} := \\mathbb{E}[\\frac{1}{4} x_{i ; -} x_{i ; -}]$.The coeffi\ncients $K_{0}^{(l)}$ are given by:\n\n$K_{0}^{(l)} = \\frac{1}{4} [K_{++}^{(l)} + K_{+-}^{(l)} + K_{-+}^{(l)} + 2K_{1}^{(l)}]$,\n\n$K_{1}^{(l)} = \\frac{1}{4} [K_{++}^{(l)} - K_{+-}^{(l)} - K_{-+}^{(l)} - K_{--}^{(l)}]$,\n\n$K_{2}^{(l)} = \\frac{1}{4} [K_{++}^{(l)} + K_{+-}^{(l)} - K_{-+}^{(l)} - 2K_{1}^{(l)}]$"}, {"title": "", "content": "If we express $(x_{i ; +})$ and $(x_{i ;-})$ as $x_{i ; \\pm} = \\frac{x_{i ; +} + x_{i ;-}}{2} \\pm \\frac{x_{i ; +} - x_{i ;-}}{2} = x_{i ;} \\pm \\delta x_{i}$, it can be seen\nthat as $(\\delta x_{i})$ approaches zero, the coefficients $K_{0}^{(l)}, K_{1}^{(l)}, K_{2}^{(l)}$ also approach zero.\nConsequently, the coefficients can be expanded as follows:\n\n$K_{0}^{(l)} = K_{0}^{(l)} + \\delta^{2} K_{1}^{(l)} + O(\\delta^{4})$,\n\n$K_{1}^{(l)} = \\delta K_{1}^{(l)} + \\delta^{3} K_{1}^{(l)} + O(\\delta^{5})$,\n\n$K_{2}^{(l)} = \\delta^{2} K_{2}^{(l)} + \\delta^{4} K_{2}^{(l)} + O(\\delta^{6})$\n\nThen, it is known that the following recursion can be derived [18]:\n\n$K_{0}^{(l+1)} = C_{b} + C_{W} g(K_{0}^{(l)} )$,\n\n$\\delta K_{1}^{(l+1)} = \\mathcal{X} (K_{0}^{(l)}) \\delta K_{1}^{(l)} $, \n\n$\\delta^{2} K_{2}^{(l+1)} = \\mathcal{X}_{1} (K_{0}^{(l)}) \\delta^{2} K_{2}^{(l)} + h(K_{0}^{(l)}) (\\delta K_{1}^{(l)})^{2}$"}, {"title": "", "content": "Where $K_{0}^{(l+1)}$ is the kernel for a\nsingle input, and each $g, h, \\mathcal{X}, \\mathcal{X}_{1}$ are\ndefined as follows:\n\n$g(K) := (\\sigma(z) \\sigma(z))_{K}$,\n\n$\\mathcal{X}_{1}(K) := C_{W} g'(K) = \\frac{C_{W}}{K} (z \\sigma'(z) \\sigma(z))_{K}$,\n\n$\\mathcal{X}_{1}(K) := C_{W} (\\sigma'(z) \\sigma'(z))_{K}$,\n\n$h(K) := \\frac{C_{W}}{4 K} (\\sigma'(z) \\sigma'(z) (z^{2} - K)) = \\frac{1}{2} \\frac{d}{d K} \\mathcal{X}_{1}(K)$.\n\n(5)\n\nIn this context, we refer to $\\mathcal{X}$ and $\\mathcal{X}_{1}$ as the parallel susceptibility and per-\npendicular susceptibility, respectively. It is known that for the kernel matrix\nto be preserved, and to ensure that the size or distance of the representation\nneither diverges nor vanishes, both susceptibilities must equal 1."}, {"title": "3. Failure of RePU Activation", "content": "Activation functions that exhibit similar behaviors according to the RG\nflow are classified into universality classes. According to [18], scale-invariant\nactivations (e.g. ReLU, LeakyReLU) form a single universality class called\nthe self-invariant universality class. Tanh and Sin fall into the $K^{*}=0$ uni-\nversality class, while SWISH and GELU belong to the half-stable universality"}, {"title": "", "content": "$\\mathcal{X}_{1}(K) = \\frac{C_{W}}{K} (z \\sigma'(z) \\sigma(z))_{K} = \\frac{C_{W}}{K} (z [p z^{p-1}]_{+} [z^{p}]_{+})K = \\frac{C_{W} p}{K} ( [z^{2p}]_{+})K$\n\n$= \\frac{C_{W} p}{K \\sqrt{2 \\pi K}} \\int_{0}^{\\infty} z^{2 p} e^{-\\frac{z^{2}}{2 K}} d z = \\frac{C_{W} p}{K} \\frac{\\sqrt{K^{2 p}} 2^{p-1} (2 p-1) ! !}{2}$,\n\n$\\mathcal{X}_{\\perp}(K) = C_{W} (\\sigma'(z) \\sigma'(z))_{K} = C_{W} (p^{2} [z^{p-1}]_{+} [z^{p-1}]_{+})K$\n\n$= \\frac{C_{W} p^{2}}{ \\sqrt{2 \\pi K}} \\int_{0}^{\\infty} z^{2 p-2} e^{-\\frac{z^{2}}{2 K}} d z = \\frac{C_{W} p^{2}}{2} \\frac{\\sqrt{K^{2 p}}}{K} \\frac{(2 p -3) ! !}{2}$"}, {"title": "", "content": "(6)\n\nThe parallel susceptibility to perpendicular susceptibility ratio is $2 p-1$ :\np. Therefore, except for $p=1$ (i.e., ReLU), it is impossible to set both sus-\nceptibilities to 1 simultaneously. If we set the parallel susceptibility to 1, the\nperpendicular susceptibility will be less than 1, causing the distance between\ntwo inputs to vanish as they propagate through the layers. Conversely, if we\nset the perpendicular susceptibility to 1, the parallel susceptibility will be\ngreater than 1, causing each input to diverge as it propagates through the\nlayers."}, {"title": "3.2. Experimental Validation", "content": "According to the results\nin section 3.1, the kernel value of Neural Networks with RePU (for $p>1$)\nfor a single input must either explode or vanish as it propagates through the\nlayers under the initialization distribution. In this section, we experimentally\nverify this. For the experiment, the neural network is configured with an\ninput dimension of 2, an output dimension of 1, hidden layers with widths\nof 512, and a depth of 10 (therefore, the total number of layers, including\nthe input and output layers, is 12). The activation function is RePU. The\ninitialization distribution follows equation (2), with $C_{b}$ set to 0, and $C_{w}$"}, {"title": "4. Modified Rectified Power Unit (MRePU)", "content": "In this section, we propose the modified Rectified Power Unit (MRePU)\nfunction to overcome the limitations of RePU, which cannot be stacked\ndeeply. MRePU retains the advantages of RePU (e.g., differentiability) while\nfundamentally addressing the existing issues. The definition of MRePU is as\nfollows:"}, {"title": "Definition (Modified Rectified Power Unit (MRePU)", "content": "The MRePU\nactivation function of order $p>1$ is defined by the following equation:\n\n$\\sigma(z) = \\begin{cases} z (z+1)^{p}, & \\text{if } z>-1 \\\\\n0, & \\text{if } z<-1 \\end{cases}$"}, {"title": "4.1. Numerical Calculation of Susceptibility", "content": "Here, we calculate the parallel and perpendicular susceptibilities of the\nMRePU function we proposed. The susceptibility of the MRePU function of\norder $p$ can be expressed by the following equation:\n\n$\\mathcal{X}_{\\parallel}(K) = \\frac{C_{W}}{K} (z \\sigma'(z) \\sigma(z))_{K}$\n$= \\frac{C_{W}}{K \\sqrt{2 \\pi K}} \\int_{-1}^{\\infty} ((z+1)^{2 p-1} ((p+1) z+1) z e^{\\frac{z^{2}}{2 K}} d z$\n\n$\\mathcal{X}_{\\perp}(K) = C_{W} (\\sigma'(z) \\sigma'(z))_{K}$\n$= \\frac{C_{W}}{\\sqrt{2 \\pi K}} \\int_{-1}^{\\infty} ((z+1)^{2 p-2} ((p+1) z+1)^{2}) e^{\\frac{z^{2}}{2 K}} d z$\n\n(7)\n\nSince getting value of (7) analytically is impossible, we calculate the suscep-\ntibility numerically. We fixed the value of $C_{W}$ at $1.0$ and plotted graphs by\nexponentially increasing the value of $K$ for cases where $p=2$ and $p=3$. The\nresults can be seen in Figure 4, where it is observed that the ratio of the two\nsusceptibilities converges to 1 as the value of $K$ approaches 0. This confirms\nthat the MRePU activation belongs to the $K^{*}=0$ universality class, and by\nappropriately adjusting the $C_{W}$ value, it is possible to ensure that the kernel\nneither explodes nor vanishes."}, {"title": "4.2. Experimental Results", "content": "Now, as we did with RePU Neural Networks, we will investigate the\nstatistical properties of MRePU under the same experimental settings."}, {"title": "Empirical Kernel and Four-point Vertex Behavior at Initialization", "content": "As with the RePU case, we investigate the statistical properties of the neural\nnetwork ensemble given by the initialization distribution. The experimental\nenvironment remains identical to that of the RePU Networks, with the only\nchange being the activation function, which is now MRePU with $p=2$. The\nresults for $C_{W}=1.0$ are shown in Figure 5. As can be seen from the graph,\nthe empirical kernel maintains a consistent scale across both random data\nand weight parameters. Additionally, we also examined the behavior of the\nFour-point vertex for MRePU. Equation (4) can be simplified for a single\ninput as follows:\n\n$V^{(l+1)} = \\mathcal{X}_{1}^{2}(K)V^{(l)} + C_{W} [(\\frac{\\sigma'}{\\sigma}^{2}(z))_{K^{(l)}} - (\\frac{\\sigma}{\\sigma}^{2}(z))_{K^{(l)}}]+O(\\frac{1}{n_{l}})$"}, {"title": "", "content": "Dynamics of Statistics During Training\nNow, we will examine how the statistics of the MRePU Network ensemble\nchange under training dynamics. The experimental setup is the same as in\nthe case of the RePU Network, except that the number of hidden layers is\nset to 7. Although the theoretical approximation suggests setting $C_{W}=1.0$,\nmeaningful learning was only achieved with a slightly lower value, possibly\ndue to the perturbative effects in the MRePU Network. In our experiments,\nwe set $C_{W}=0.9$. The dynamics of the kernel and four-point vertex are\ndepicted in Figure 7, while the comparison of the ensemble's mean output"}, {"title": "5. Conclusion", "content": "In this paper, we analyzed neural networks using Effective Field The-\nory to theoretically predict and experimentally verify why neural network\narchitectures with RePU activation fail when the network depth increases.\nFurthermore, we proposed the MRePU activation function, which improves\nupon RePU, and estimated its effectiveness theoretically. We then empiri-\ncally confirmed its feasibility by comparing the experimental results with the\ntheoretical predictions. The proposed MRePU was able to facilitate learn-\ning even in very deep networks without adopting structures like ResNet.\nMathematically, we confirmed that MRePU retains the favorable properties\nof RePU, such as p-differentiability. This suggests that the beneficial char-\nacteristics proposed in previous studies on RePU can also be extended to\nMRePU. Moreover, when attempting to approximate a function with a spe-\ncific differentiability class, MRePU can be used to introduce inductive bias\ninto deep neural networks, thereby enabling effective learning."}]}