{"title": "Sina at FigNews 2024:\nMultilingual Datasets Annotated with Bias and Propaganda", "authors": ["Lina Duaibes", "Areej Jaber", "Mustafa Jarrar", "Ahmad Qadi", "Mais Qandeel"], "abstract": "The proliferation of bias and propaganda on\nsocial media is an increasingly significant con-\ncern, leading to the development of techniques\nfor automatic detection. This article presents a\nmultilingual corpus of 12, 000 Facebook posts\nfully annotated for bias and propaganda. The\ncorpus was created as part of the FigNews\n2024 Shared Task on News Media Narratives\nfor framing the Israeli War on Gaza. It cov-\ners various events during the War from Octo-\nber 7, 2023 to January 31, 2024. The corpus\ncomprises 12,000 posts in five languages (Ara-\nbic, Hebrew, English, French, and Hindi), with\n2, 400 posts for each language. The annotation\nprocess involved 10 graduate students special-\nizing in Law. The Inter-Annotator Agreement\n(IAA) was used to evaluate the annotations\nof the corpus, with an average IAA of 80.8%\nfor bias and 70.15% for propaganda annota-\ntions. Our team was ranked among the best-\nperforming teams in both Bias and Propaganda\nsubtasks. The corpus is open-source and avail-\nable at https://sina.birzeit.edu/fada", "sections": [{"title": "Introduction", "content": "Since October 7, social media has been flooded\nwith posts, articles, images, and videos related to\nthe Israeli War on Gaza. Such posts are often di-\nvided by hate, bias, and fake news either in favor of\nor against one of the parties or by remaining neu-\ntral, see e.g., \"Framing the Israeli War on Gaza\" is\na shared task on news media narratives (Zaghouani\net al., 2024), which is part of the 2nd ArabicNLP\nconference. The task aims to create a multilingual\ncorpus that unravels the layers of bias and propa-\nganda within news articles in various languages.\nSuch shared tasks and datathons are crucial in\nthe NLP community to foster collaboration and ad-\nvance research in specific areas. Previous efforts,\nsuch as SemEval-2020 Task 11 (Martino et al.,"}, {"title": "Annotation Methodology", "content": "The objective of the task is to address the complex\nlandscape of social media discourse related to the\nIsraeli War on Gaza 2023-2024. The task orga-\nnizers provided participants with 15k posts from\nverified Facebook accounts, selected between Oc-\ntober 6, 2023, and January 31, 2024, using \"Gaza\"\nas a query keyword across 5 languages: Arabic,\nHebrew, English, French, and Hindi. The dataset\nconsists of 15 batches, each containing 1000 posts."}, {"title": "Annotation Guidelines", "content": "Our understanding of \"bias\" is based on the work\ndone by the United Nations Committee on the Elim-\nination of Racial Discrimination and the European\nCommission against Racism and Intolerance (Eu-\nropean External Action Service, n.d.). We define\nthe notations 'bias' and 'propaganda' based on the\nUN and EU accounts, as:\nBias: is generally understood as an inclination\nor prejudice towards or against a particular person\nor group, often in a way considered to be unfair.\nIn other words, it is an unreasonable preference\nor dislike that prompts someone to behave in a\ndiscriminatory way, often based on unfair judgment.\nThis bias is typically based on prohibited grounds\nof discrimination such as race, religion, language,\nnationality, ethnicity, social background, gender,\nand others.\nClassifications of Bias: we adopted the same\nclasses provided in the Shared Task: (1) Biased\nagainst Palestine,(2) Biased against Israel, (3) Bi-\nased against others, (4) Biased against both Israel\nand Palestine, (5) Not Applicable, (6) Unclear, and\n(7) Unbiased. We also introduced a new feature\ncalled \"Type of Bias\", which can be either: (a)\nExplicit (\u062a\u062d\u064a\u0632 \u0635\u0631\u064a\u062d) if it is obvious and evident in\nthe post, (b) Implicit (\u062a\u062d\u064a\u0632 \u0636\u0645\u0646\u064a) if it is clear but\nnot evident in the post, and (c) Vague (\u062a\u062d\u064a\u0632 \u0645\u0628\u0647\u0645) in\ncase of indirect and ambiguous bias. This feature\nis important from a methodological viewpoint as\nit encourages the annotators to think more during\nclassification. If a post contains biased content but\nnot in a direct way it can be accounted as implicit.\nPropaganda: misleading ideas or statements that\ncan distort the truth or omit facts to promote a spe-\ncific political or social agenda. These ideas are\ntypically published by media outlets. For exam-\nple, propaganda can take the forms of exaggeration,\nminimization, spreading doubts, name-calling, la-\nbeling, or intentional vagueness. All these forms\nhave the common intention to spread false informa-\ntion and obscure facts.\nClassifications of Propaganda: We adopted the\nfour classes provided in the Shared Task: (i) Pro-\npaganda, (ii) Not propaganda, (iii) Not Applicable,\nand (iv) Unclear.\nAdditionally, we added a new column to classify\nPropaganda into three types: (1) Propaganda must\nbe deleted: if it contains evident harmful content\nthat poses risks to the safety and security of indi-\nviduals or groups; (2) Propaganda may be deleted:\nif we cannot easily judge whether it is propaganda,\ndepending on a specific context; and (3) Propa-\nganda not to be deleted: if it is not clear and lacks\nharmful consequences and therefore does not war-\nrant deletion.\nRemark: Since the data was collected from\nFacebook posts some cases contain quoted content\n(e.g. an unbiased post quoting biased content). It\nwas established in the guidelines that a post should\nnot be classified as bias or propaganda based on its\nquotation, but rather on the post itself.\nAn Example of the guidelines mentioned earlier\nregarding quoted content is as follows: \u201cHamas and\nIslamic Jihad spare no effort to exploit religious\ninstitutions for terrorist purposes,\u201d the IDF said in\na statement. This post is annotated as unbiased\nbecause it is a direct quote and does not include\nany additional commentary or interpretation."}, {"title": "Inter-Annotator Agreement (IAA)", "content": "To evaluate the quality of our annotations, we used\nthe F1-score and Cohen's Kappa (Cohen, 1968)\nto compute the agreement between the annotators.\nThe results are shown in Table 1.\nThe task organizers allocated 100 posts (10%)\nfrom each batch for IAA, including 20 posts ran-\ndomly selected from each language. Overall, we\nannotated 12,000 posts, resulting in an IAA dataset\nof 1, 200 posts. These were distributed among our\n10 annotators following this scheme: (1) each an-\nnotator received 240 posts, (2) each post was anno-"}, {"title": "Team Composition and Training", "content": "Team composition: We assembled a team of 10\nMaster's students specializing in Law at Birzeit\nUniversity, comprising 7 females and 3 males. All\nteam members are native Arabic speakers with a\ngood command of English.\nTraining phase: We began by selecting 200\nposts to train all students in annotation. After train-\ning, each student was assigned 1,200 posts for\nannotation.\nEnsuring consistency We held three workshops\nto ensure consistency to discuss guidelines, address\nchallenges, and resolve disparities. The first work-\nshop involved an expert who reviewed the anno-\ntations and added comments for the annotators to\naddress. In the second workshop, the annotators\nmet with the expert to discuss his comments on\nthe posts. In the final workshop, after reviewing\ntheir annotations compared to the expert's, they dis-\ncussed the points of agreement and disagreement\nwith him."}, {"title": "Annotation process", "content": "Annotation Phase: The dataset consisted of 12\nbatches, comprising 10, 800 posts from the Main\nsheet, and 1200 posts from the IAA sheet. The\nannotation was carried out in two phases:\nPhase One: We distributed Batch01 and\nBatch02, each with 180 posts, among team\nmembers. To ensure consistency with the\nguidelines, an expert reviewed all student an-\nnotations for these batches and provided feed-\nback.\nPhase Two: we assigned each annotator 450\nposts from two different batches. This step\nallowed us to complete the annotation of all\n12 batches (i.e. 12k posts).\nSet quality standards\nTo set quality standards among annotators, af-\nter the annotation process was complete, each\npair of annotators who had annotated the same"}, {"title": "Task Participation and Results", "content": "Table 2 displays the final results provided by the\nshared task organizers. Our Sina team achieved\nthe third and second place in the IAA Quality and\nQuantity tracks for the Bias and Propaganda sub-\ntasks, respectively. In addition to third place in\nPropaganda Guidelines.\nTable 3 and Table 4 illustrate the distribution of\nthe bias classes and types of bias across languages\nrespectively. Table 3 shows that about 27% of the\nposts are biased against Palestine and 63% of the\nposts are unbiased. Most of the bias against Pales-\ntine originated from French posts. Table 4 gives\nmore statistics about the types of bias. As shown in\nthis table, most of the posts annotated as Explicit\nbias are in Hebrew.\nFor propaganda results, Table 5 illustrates the\ndistributions of propaganda classes across lan-\nguages, which shows that 31% of the posts (3333)\nare annotated as \"Propaganda\", and 66% (7084) are\n\"Not Propaganda\". The majority of the propaganda\noriginated from French posts. Table 6 illustrates\nthe distribution of the type of propaganda classes\namong languages. As shown in the table posts that\nwere classified as propaganda must be deleted were\nin French with 348 posts."}, {"title": "Error Analysis and Discussion", "content": "Despite training and supervision, errors may arise\nfrom subjective interpretation, ambiguous guide-\nlines, or complex content. We explored the errors\nand noted:\n1. False positives in bias annotations occurred\nwhen annotators marked neutral content as bi-\nased. For instance, the post: \"Israel launched\nattacks on Syria on Nov 10 in response to\na drone strike on Eilat. The IDF claimed it"}, {"title": "Conclusion", "content": "This article presents our contribution to the\nFigNews 2024, where we annotated a multilingual\ncorpus of 12, 000 Facebook posts for bias and pro-\npaganda across five languages. We extended the\nannotation guidelines for better consistency and\naccuracy, providing a foundation for future work\nin detecting bias in social media. Our plans in-\nclude expanding the corpus to cover more critical\nevents of the war and leveraging neural and large\nLanguage models to automatically detect bias and\npropaganda on social media posts."}, {"title": "Ethical Considerations", "content": "Given the sensitive nature of the topics and media\nnarratives related to the Israel War on Gaza, our\nannotators, who are lawyers, have undergone exten-\nsive training to ensure careful and fair judgments.\nThey meticulously review both Arabic and English\ntranslations to avoid any bias that might arise from\nmachine translation."}, {"title": "Limitations", "content": "We recognize the limitations in our annotation pro-\ncess. This is because of the subjective nature of\nidentifying bias and propaganda in social media\nposts, and the sensitivity of the datasets involved."}]}