{"title": "Contri(e)ve: Context + Retrieve for Scholarly Question Answering", "authors": ["Kanchan Shivashankar", "Nadine Steinmetz"], "abstract": "Scholarly communication is a rapid growing field containing a wealth of knowledge. However, due to its unstructured and document format, it is challenging to extract useful information from them through conventional document retrieval methods. Scholarly knowledge graphs solve this problem, by representing the documents in a semantic network, providing, hidden insights, summaries and ease of accessibility through queries. Naturally, question answering for scholarly graphs expands the accessibility to a wider audience. But some of the knowledge in this domain is still presented as unstructured text, thus requiring a hybrid solution for question answering systems. In this paper, we present a two step solution using open source Large Language Model(LLM): Llama3.1 for Scholarly-QALD dataset. Firstly, we extract the context pertaining to the question from different structured and unstructured data sources: DBLP, SemOpenAlex knowledge graphs and Wikipedia text. Secondly, we implement prompt engineering to improve the information retrieval performance of the LLM. Our approach achieved an F1 score of 40% and also observed some anomalous responses from the LLM, that are discussed in the final part of the paper.", "sections": [{"title": "1. Introduction", "content": "Scholarly knowledge is mostly represented as scholarly articles and growing rapidly. These documents contain invaluable knowledge that cannot be easily automated for extraction and hence not machine actionable. They contain a heterogeneous source of structured and unstructured data. It is important to study them as they contain important information critical for further advancements in academia and a network of unexplored relationships. Due to its unstructured nature, conventional information and document retrieval processes, albeit efficient, are limited and fail to provide new connections and insights. Scholarly knowledge graphs are necessary to fill this gap and represent this knowledge in a structured and machine readable format. This has led to new scholarly knowledge graphs such as DBLP, OpenAlex, ORKG etc.\nKnowledge graphs capture the semantic representation of the underlying data, by defining meaningful concepts and relationships. This enriches the knowledge and established relationships between concepts. The structured representation can lead to easier access by setting up knowledge graph question answering systems. This paper details our contribution for participating in the second Scholarly-QALD challenge as part of ISWC-2024. The challenge is to find a solution for a hybrid question answering system for scholarly data. The answer to the question must be extracted from the three knowledge sources, which include, two scholarly knowledge graphs: DBLP and SemOpenAlex and an unstructured source: Wikipedia text.\nIn the upcoming sections of the paper, we first look at the relevant literature to provide background information to the reader, followed by the methodology section that provides a detailed account of our"}, {"title": "2. Related Work", "content": "Scholarly knowledge graph provides a semantically connected graph for scientific and academic liter-ature. With the rapid growth of scholarly data storing it as linked data facilitates transformation of document format to a machine understandable and actionable data. The work in [1] reviews three differ-ent ways: machine learning, rule-based learning and natural language processing tools for constructing a scholarly knowledge graph. It also highlights some of the existing challenges such as integration of data sources, ontology matching, extracting KG from diversely structured textual data etc. To fill the existing gap in knowledge the work in[2] proposes a novel approach to enhance the construction of scholarly KGs. The two innovative methods introduced provides a structured representation of document-formatted scholarly text(Deep Document Model) and a query processor to optimize and simplify complex queries (KG-enhanced Query Processing). Automation of KG construction has led to many new and popular scholarly KGs such as DBLP, OpenAlex, ORKG etc.\nDBLP stands for Data Bases and Logic Programming. Created in 1993, at University of Trier, it was originally designed as a database to store bibliographic information for the fields of database systems and logic programming. Today, it extends to a much wider range of sub-fields of computer science[3]. SemOpenAlex knowledge graph contains over 26 billion RDF triples covering all areas of scientific scholarly domain. It overcomes the limitations of some of the existing scholarly KG, such as, limited focus to a specific discipline, not regularly updated or does not follow standard RDF format[4]. ORKG (Open Research Knowledge Graph) proposes a transition from document based scholarly communication to a knowledge-based machine actionable format. The creation of this knowledge can be done by the users themselves through a front-end(also includes search options) and the request is processed in the back-end by validating the logic[5].\nWith the establishment of scholarly KG, one of its application is Scientific Question Answering. JarvisQA[6] proposes a BERT-based question answering system that retrieves answers from different tables. Another application is author disambiguation in scholarly KG. This is an important challenge due to the authors central link in the KG and existence of multiple authors with similar name, field and affiliation. A triangulation approach is proposed to improve evaluation of author name disambiguation in DBLP [7]. The improved evaluation is attributed to assessing the performance by comparing baselines on eight labeled datasets.\nGenerative AI tools like large language models are providing peak performance in natural language generation. However, some of their biggest challenges are knowledge gap from lack of training data and hallucinations. They have greatly improved from efforts of tweaking the prompt to force a better response. Knowledge Injection i.e, providing contextual information in the prompt about the relevant entities, reduced hallucinations and improved the quality of responses as one case study in text generation for online customer reviews reports[8].\nUsing LLM to solve KGQA have also become popular. For example, the paper[9] leverages LLM to generate SPARQL query generation using zero-shot and few-shot prompts. In the few-shot prompt setting a BERT-based question analyser identifies similar question-query pairs from the dataset and introduces them into the prompt. The model achieved F1-score of 99% on the Sci-QA dataset. A use-case in virology showcases the power of LLMs in extracting scientific information with the help of instruction fine-tuning[10]. A filtered corpus of CORD-19 dataset(papers published on COVID-19) is used to fine-tune an LLM for better domain adaptation. LLMs have proven their ability of handling both structured and unstructured data. This gave us the idea to use LLMs to build our hybrid QA system."}, {"title": "3. Methodology", "content": "This section discusses our contribution in this paper. We have a two step approach of context extraction and prompt engineering. Before discussing our work, we first take a look at the question answering dataset."}, {"title": "3.1. Dataset", "content": "The scholarly-QALD dataset is the second iteration of Question Answering dataset for scholarly data, preceded by DBLP-QuAD dataset[3]. The dataset is split into train and test set and was generated using an LLM, which introduces a small percentage of incorrect answers to the dataset. The train dataset consists of 5000 question-answer pairs with each question containing DBLP author-id for the question. The test set consists of 702 questions with DBLP author-id. The answers are held-out and to mitigate incorrect answers due to LLM generation, they have also been manually verified and corrected.\nThis is a hybrid question-answering challenge i.e, the correct answer to the question requires the system to look up three different sources of data (DBLP KG, SemOpenAlex KG and Wikipedia text). DBLP KG is limited to representing only the computer science scholarly communication, whereas, SemOpenAlex is an open source KG representing all scientific disciplines. SPARQL endpoints are provided to access the two knowledge graphs. The Wikipedia source file contains text related to authors and institutions, extracted from its respective Wikipedia page."}, {"title": "3.2. Context Extraction", "content": "The Scholarly-QALD dataset was introduced in the previous section, containing question and DBLP author-id in the form of URIs, (Uniform Resource Identifiers) required to answer each question. The DBLP author-id can be used to connect to all the other sources, as demonstrated in figure1.\nBefore we proceed with answering the question, we need to gather all the relevant information about the author, on whom the question is formed. This step is used to extract the relevant information from the different sources. The process of extracting context is divided into three steps\n1. Use DBLP author-id to fetch author information(including ORCID) from DBLP KG\n2. Using DBLP ORCID to extract author information from SemOpenAlex KG\n3. Extract author and affiliated institution text from Wikipedia text"}, {"title": "DBLP Knowledge Graph", "content": "DBLP author-id corresponding to each question is provided in the dataset. Author-id URI links all the information related to a particular author in DBLP. By studying the questions in the dataset we found that only some of these properties are relevant for answering the question. The SPARQL query was refined to extract ORCID, name and publication information of the author.\nAn author can have more than one publication. Title of the publication, published journal/series/book name, publication year and co-authors were extracted for each of the author's publication to provide as context."}, {"title": "SemOpenAlex Knowledge Graph", "content": "The ORCID extracted from DBLP knowledge graph in the previous step, is used to fetch the SemOpenAlex author-id. This author-id gives us all the information related to the author and their affiliated institution. The SPARQL query extracts author properties such as, author name, publication count, citations count, hIndex, i10Index, two year mean citedness and author affiliated institution name. Additionally, we also extract institution type, citation count and publication count of the author affiliated institution."}, {"title": "Wikipedia", "content": "Some questions relate to author/institute that is not represented in the knowledge graphs. The Wikipedia text file provided contains information for only some of the authors and institutions. It is stored in a dictionary-like format with names and description as key-value pair. The author and their affiliation names, extracted from the two KGs, are used to do a fuzzy search in the file. If an entry exists in the file, the description text is extracted, else, we extract the content from the Wikipedia page directly.\nThese texts are lengthy and providing them directly as context in the prompt resulted in poor performance from the LLM. To mitigate this issue, we extracted keywords from the question and did a substring match with the text. Only sentences containing the keyword is included as the final text to be provided in the prompt. Summarizing the Wikipedia text improved the performance greatly.\nAfter the conclusion of the three step information extraction, the data is further refined and combined to provide pertinent information as context in the prompt. Details are discussed in the next section. This step was necessary, as longer prompt and context length resulted in poor performance by the LLM, both with execution times and accuracy."}, {"title": "3.3. Prompt Engineering", "content": "The prompt template is designed to provide all the relevant information for the LLM to infer the correct answer.\nAs discussed earlier, the length of prompt determines the performance of the LLM. If the prompt length is too short there might not be enough information to extract the answer, resulting in hallucinations. Longer prompts increases the LLM inference time significantly and frequently outputs wrong answers. Thus, it was necessary to further refine the prompt.\nThe prompt contains 4 parts,\n\u2022\n\u2022 Instructions: guides the LLM on how to process the information in the prompt\n\u2022 Query: question from the dataset and a rephrased form of the original question generated by the LLM\nContext: information containing the answer extracted from the three sources in the previous step. It is grouped into Author, Institute and Publication\n\u2022 Output Indicator: instructions to generate the output, which also includes the answer type predicted for each question [11]"}, {"title": "4. Results", "content": "The table below provides the final evaluation scores of our approach for Scholarly-QALD test dataset. The two main evaluation measures used in the challenge are Exact Match and F1-score. Exact Match gives the total percentage of generated answers that match accurately with the golden answers. Since the answers in the dataset are LLM generated, F1-score is used to compute a word-to-word match score for partially correct answers.\nOur system achieved the second best results in the challenge(out of 3). However, the low scores necessitates a further inspection. In the following section, we discuss the results generated by LLMs."}, {"title": "4.1. Analysis", "content": "Missing author ORCIDs\nAs discussed earlier ORCID serves as unqiue id for each author, that forms a common link for connecting authors between DBLP and SemOpenAlex knowledge graphs. However, we found that many ORCIDs were missing in SemOpenAlex knowledge graph(in the SPARQL endpoint provided for the challenge), which led to incomplete context extraction. To mitigate this issue we accessed a newer version of the knowledge graph using a different SPARQL endpoint\u00b2, but this also had updated values(such as publication count, h-Index values etc.). This has potentially led to some mismatches in the systems answers.\nLLM inconsistencies\nIn spite of the answer being present in the context the responses produced by the LLM was ambiguous and inconsistent. These behaviors were random and did not seem to follow a pattern. Such cases are highlighted below\n\u2022 The LLM showed a random behavior of hallucinating values in the answer, although the correct value was provided as part of the context. This was especially common with number answer types.\n\u2022 The LLM generated gold answers observed in the train dataset did not follow a fixed pattern. For example, floating point precision after the decimal points, date format etc. This resulted in a few mismatches with the golden answers. Listed below are some of the examples from train dataset, for which the gold answers were provided.\n\u2022 The LLM had a hard time generating only the answer keywords, ignoring the specific instructions provided in the prompt. Which led to full sentences or long explanations provided along with the answer."}, {"title": "5. Conclusion", "content": "In this paper, we proposed a context in prompt solution using Llama 3.1, to solve hybrid question answering problem in the scholarly domain. The hybrid data sources included DBLP and SemOpenAlex knowledge graphs and Wikipedia text. The two step solution extracted and input the context into prompts, to evaluate the information retrieval capabilities of LLM. The results showed potential, but the performance of the LLM was inconsistent. This behavior dictates a need for further study of attention paid to context in prompts and to understand the information retrieval capabilities of LLMs."}]}