{"title": "No Argument Left Behind: Overlapping Chunks for Faster Processing of Arbitrarily Long Legal Texts", "authors": ["Israel Fama", "B\u00e1rbara Bueno", "Alexandre Alcoforado", "Thomas Palmeira Ferraz", "Arnold Moya", "Anna Helena Reali Costa"], "abstract": "In a context where the Brazilian judiciary system, the largest in the world, faces a crisis due to the slow processing of millions of cases, it becomes imperative to develop efficient methods for analyzing legal texts. We introduce uBERT, a hybrid model that combines Transformer and Recurrent Neural Network architectures to effectively handle long legal texts. Our approach processes the full text regardless of its length while maintaining reasonable computational overhead. Our experiments demonstrate that uBERT achieves superior performance compared to BERT+LSTM when overlapping input is used and is significantly faster than ULMFiT for processing long legal documents.", "sections": [{"title": "Introduction", "content": "Legal NLP can be defined as the application of Natural Language Processing (NLP) techniques within the legal domain. This subfield of NLP has been experiencing rapidly growing interest from both academia and industry: Katz et al. [13] reports a significant increase in the volume of publications, rising from fewer than 30 papers in 2013 to nearly 120 in 2022. Brazil possesses the largest judiciary system in the world, comprising 18,000 judges distributed across 91 courts. At the time of writing, there are more than 84 million ongoing legal cases [3]. Despite each judge conclusively adjudicating nearly seven legal cases per day, the average duration of a legal case in Brazil is four and a half years. These numbers indicate both the need and the opportunity for innovative solutions to manage and analyze vast amounts of legal data.\nWe turn our focus to Legal Judgment Prediction (LJP), which involves predicting court decisions. Although predicting decisions may be a complex task, we argue it can be reduced to a Text Classification task, which has seen a marked increase in studies [14], fueled by advancements in deep learning. In particular, the Transformer architecture emerged as a paradigm shift for many NLP tasks [9]. However, it still has limitations when handling long texts, which poses significant challenges in the legal domain, where documents are usually long and complex.\nThere is fruitful research being done on enhancing the input size limitation for Transformers, such as Retrieval-Augmented Language Models (RALMs) [8]. Current retrieval techniques, however, often trust embedding models which also can be sub-optimal when dealing with legal documents, where a single word in the whole document can make a difference. Also, these methods demand substantial computational resources and large document stores to achieve good performance. Other methods"}, {"title": "Related Work", "content": ""}, {"title": "Transformer-Based Approaches for Long Text Processing in Legal NLP", "content": "Enhancing transformer architectures to efficiently process longer texts has become a critical area of research, with significant implications for the legal domain. Longformer [2] employs a sparse attention mechanism, extending the input size limit to 4096 tokens, which is eight times the limit of BERT [6]). Hoang et al. [10] applied this architecture to classify legal texts from the Indian Legal Documents Corpus \u2013 ILDC [16], but they did not process the entire text, potentially leading to the loss of relevant information.\nPappagari et al. [18] introduced Recurrence over BERT \u2013 ROBERT, where longer texts are split into overlapping chunks that are recurrently encoded. Although this approach is conceptually similar to our proposed architecture, a direct comparison is not feasible due to the lack of detailed information on the overlap and recurrence encoding strategies used in RoBERT. Additionally, the authors tested ROBERT on much shorter texts than those in our dataset.\nThe overlapping algorithm in our approach, uBERT, can be seen as a specific case of the method proposed in SlidingBERT [22], where the sliding window stride is set to [overlap/2]. Unlike SlidingBERT, which allows tokens to appear in multiple chunks, we limit the overlap so that tokens are covered by at most two chunks. This design choice is motivated not by the language (Portuguese vs. English) but by the goal of reducing computational overhead, ensuring that only adjacent chunks overlap to maintain continuity in context flow.\nJacob de Menezes-Neto and Clementino [12] introduced BrCAD-52, a dataset designed for Legal Judgment Prediction (LJP), and evaluated three architectures for this task: ULMFiT [11], BigBird [21], and BERT+LSTM. ULMFiT, a transfer learning model that fine-tunes a pre-trained language model for downstream NLP tasks, was the only architecture capable of processing the entire text as input. BigBird, a sparse-attention model, addresses the 512-token limit by focusing on subsets of tokens, thereby reducing computational complexity, and was configured to handle texts up to 7,680 tokens. For BERT+LSTM, documents were split into 512-token chunks, with truncation applied to middle chunks if a document required more than 15. While similar in approach, uBERT differs from BERT+LSTM in that it uses a chunk overlapping strategy and imposes no limit on the number of chunks, ensuring the entire text is utilized without truncation."}, {"title": "Critiques and Limitations in Legal NLP Research", "content": "The legal industry has been slow to adopt NLP advancements, relying heavily on manual work by lawyers. Mahari et al. [15] identify a key issue: Legal NLP research often fails to align with the practical needs of legal practitioners. Medvedeva and Mcbride [17] further highlight a significant gap in Legal Judgment Prediction (LJP) research, criticizing the use of poorly designed datasets that rely on biased case facts extracted from judgments. This approach leads to models with overly optimistic performance that offer limited practical value to legal practitioners.\nThis work aims to bridge the gap between research and practice in the field of Legal NLP. We propose an architecture capable of processing virtually infinite-length legal texts and evaluate it on the BrCAD-5 dataset, which Medvedeva and Mcbride [17] regard as a well-designed benchmark."}, {"title": "Proposal", "content": "Text classification can be formalized as follows. Given a document $d$ that represents a judicial decision, the goal is to make a prediction $y \\in \\{0,1\\}$, by learning a binary classifier $f$ such that $f(d) = y$. The positive class $y = 1$ represents a decision that will be reversed by an Appellate Panel (AP). Since legal documents are often long, when using Transformer-based models, conventional approaches usually truncate text from $d$, which can be effective for some semantic tasks [1], but is often sub-optimal for legal text processing tasks [18]. This can hinder performance on the Legal Judgment Prediction task, since some relevant part of the text may be cut off.\nBelieving that the text as a whole is more useful when learning a classifier, we propose unlimited BERT, or uBERT, an efficient architecture that combines an encoder-based Transformer with a Recurrent Neural Network, utilizing an overlapping algorithm during both training and inference to handle an unlimited number of input tokens. This approach is similar to the BERT+LSTM model used by Jacob de Menezes-Neto and Clementino [12], but introduces modifications to maintain local context (through overlapping chunks) and accommodate documents of virtually any size. Although the quadratic memory complexity of the self-attention mechanism presents a challenge for scaling input indefinitely, we leverage the RNN's capacity to process long sequences, enabling it to take chunk embeddings and output a comprehensive document embedding. Several studies, such as done by Hoang et al. [10], have explored the combination of attention mechanisms and recurrence. Our model builds on this concept but applies overlapping during both training and inference, and does not limit the number of chunks processed by the encoder."}, {"title": null, "content": "Let $E$ be an encoder-based Transformer, with $dim$ being the dimensionality of the output vector of the final layer, and $R$ be a Recurrent Neural Network. Let $maxtok$ be the maximum number of tokens $E$ can process as input. Let $max_c$ be the maximum number of chunks of $maxtok$ tokens that $E$ can process in parallel with a single run. We split document $d$ into $n$ chunks of size $maxtok$ tokens, starting in the first token. For each run, we extract the hidden states from the last four layers of $E$, and concatenate them to form the representation of each chunk. This is based on the idea that different layers capture different linguistic features [19]. Specifically, BERT+LSTM, the baseline most similar to our proposed architecture, extracts the hidden states from the last four layers. While other layers could be used for extraction, we retained this approach for consistency in model comparison. In each single run, we process $[1, max_c]$ chunks in parallel, generating $[1, max_c]$ vectors of embeddings, each with dimensionality $4 \\times dim$. We iteratively process chunks from $d$ until an embedding vector is generated for each chunk and thus preserving the entire text content of $d$.\nThen, we concatenate the embedding vectors maintaining the order of the respective chunks, generating a tensor of dimensionality $(n, 4 \\times dim)$. We process this tensor with the RNN sequentially, capturing the dependencies between them and generating a contextually enhanced representation for each chunk.\nSplitting text by token count can disrupt its flow, so we use token overlap between chunks during both training and inference to maintain continuity. This technique, similar to that used by Hoang et al. [10] but applied more broadly, helps preserve the text's natural structure.\nOur token overlap algorithm can be formalized as follows. Consider the judicial decision $d$ as the tokenized sequence $S = \\{t_1, ..., t_k\\}$, where $k$ is the number of tokens in $d$. We define the overlap size, $z$, as the number of tokens each chunk shares with its adjacent neighbors. Thus, any chunk shares $\\lfloor\\frac{z}{2}\\rfloor$ tokens with the previous chunk and $\\lfloor\\frac{z}{2}\\rfloor$ with the subsequent one. The first and last chunks, having only one adjacent chunk, share $\\lfloor\\frac{z}{2}\\rfloor$ tokens with their respective neighbors."}, {"title": "Experiments", "content": "In this section, we design experiments to assess our proposed model, uBERT, and validate its effectiveness on the legal domain. We split our experiments into 3, one for each of the following research questions:\nRQ1: Would an encoder-based model benefit from using the entire text in terms of performance improvement?\nWe examine the impact of processing the whole documents using multiple encoder passes. We first tested if simply increasing text chunks to process the whole text without using overlap (uBERT_0) improves performance over BERT+LSTM, which processes only partial text in a single pass. Then, we investigated the effect of introducing overlaps (0 to 510 tokens) between chunks to observe if the added local context enhances predictions.\nRQ2: If performance improves, does it come with reasonable computational overhead?\nWe compare the inference time of our architecture agatextsuperscript all baseline models to determine if it offers a performance gain and to assess the associated computational overhead.\nRQ3: Is our architecture better for processing longer texts?\nWe explore the relationship between document length and model performance. We tested the models on the full test set as well as on its subsets, the 10% and the 1% longest texts. This experiment involved statistical analysis to determine whether longer texts lead to better or worse predictions."}, {"title": "Results", "content": "We used the BrCAD-5 dataset in our experiments. The task is a binary classification, with Class 1 indicating that the AP reverses the previous decision, and Class 0 indicating it affirms. The dataset is imbalanced, with 22% of the data points belonging to Class 1. Although this imbalance ratio is consistent across all dataset splits, it varies significantly with text length.\nModels In this work, our model (uBERT) uses BERT as the encoder and LSTM as the RNN, with maxtok set to 512 tokens and up to 15 chunks (maxc) processed in parallel. Our training procedure follows the approach from Jacob de Menezes-Neto and Clementino [12], where we fine-tune the last layer of BERT and the LSTM. The fine-tuning is conducted for 1 epoch utilizing the One Cycle learning rate scheduler. Our inference procedure mirrors the training process.\nBaselines our baseline models are ULMFiT (forward, backward and bidirectional), Big Bird and BERT+LSTM. Notably, only ULMFiT and uBERT process the full text.\nComputational Infrastructure and Resources the experiments were conducted using an NVIDIA A100 GPU with 40 GB of RAM. Times are reported according to this hardware.\nEvaluation Metrics We evaluate all models using the Macro F1 score and Matthews Correlation Coefficient (MCC). The Macro F1 score is a well-established metric across NLP fields, representing the harmonic mean of precision and recall, while MCC, though less common, is frequently used in the Legal Judgment Prediction (LJP) subfield, as noted by Cui et al. [4]. MCC measures the correlation between predicted and actual classifications by accounting for true positives, true negatives, false positives, and false negatives, making it suitable for imbalanced classes. Additionally, MCC is the metric used by Jacob de Menezes-Neto and Clementino [12], making it necessary for us to use it as well for model comparison. To compare different baselines and configurations of our UBERT model, we employed bootstrap resampling to obtain 95% confidence intervals, followed by Wilcoxon-Holm post-hoc analysis to assess statistical significance with \u03b1 = 5%, following similar approaches [5, 23, 7]."}, {"title": null, "content": "Processing the Full Text Requires Overlap Comparing the BERT+LSTM baseline, which middle-truncates text when it exceeds input size, with our uBERT without overlap (uBERT_0), which uses the full text, we found that uBERT either underperformed or matched the baseline across all metrics. Notably, it performed worse on the 1% longest texts, where middle-truncation by BERT+LSTM occurs. This suggests that merely processing the entire text is insufficient for longer inputs. We hypothesize that non-overlapping chunks introduce noise due to abrupt segmentation, which degrades performance. Our results support this, showing that introducing overlap in uBERT configurations improves both Macro-F1 and MCC scores. The following uBERT configurations outperformed BERT+LSTM with statistical significance: uBERT_300, uBERT_510 and uBERT_205 on full test set; uBERT_408 and uBERT_300 on 10% longest; and uBERT_408, uBERT_300 and uBERT_510 on 1% longest. Thus, incorporating overlap is crucial for maintaining semantic consistency and improving performance on longer texts.\nuBERT with Overlap is still Significantly Faster than ULMFIT As expected, introducing overlap in the uBERT architecture increased computational time overhead. However, across the full dataset and the 10% longest texts, uBERT configurations delivered better results than the BERT+LSTM baseline with comparable inference times. Notably, uBERT_408 achieved a 4x faster inference than ULMFiT on the 10% longest texts. For the 1% longest texts, the increased length required two passes of uBERT_408, resulting in 1.8x slower inference compared to BERT+LSTM, which needed to middle-truncate in all cases. Despite this, uBERT_408 slightly outperformed BERT+LSTM, narrowing the performance gap with ULMFiT while maintaining a faster inference, highlighting the efficiency and effectiveness of our approach. In summary, in all subsets, uBERT configurations reduced the BERT+LSTM gap being significantly faster than ULMFIT.\nULMFIT Outperforms uBERT on Longer Texts As shown in Figure 3, model differences become more clear with increasing text length. Big Bird consistently underperforms on longer texts, which is why it was excluded from the comparison charts. While some uBERT configurations outperform BERT+LSTM on longer texts, F1 scores in both models degrade compared to full test dataset performance. In contrast, ULMFiT models improve on longer texts compared to the full dataset. This suggests that our architecture mitigates the degradation for longer text that is inherent to the BERT+LSTM approach, but still falls short of ULMFiT models, that handle better longer text but at a cost of 4x slower inference time."}, {"title": "Conclusion and Future Work", "content": "Our experiments demonstrate that the uBERT model improves the handling of legal texts compared to baseline encoder-based models, particularly on longer texts, due to its capability to process entire documents using overlapping chunks. Despite the increased computational overhead, uBERT remains faster than ULMFiT. uBERT slightly outperforms BERT+LSTM, but still falls short of ULMFiT. Thus, further refinement is needed to fully match ULMFiT's performance. Notably, even ULMFiT, the top-performing model in our experiments, achieves relatively low Macro-F1 scores, suggesting that processing the full text alone is insufficient for high performance on this task. In this direction, future research should expand the evaluation methodology by analyzing correctly and incorrectly classified cases across all tested models to assess whether specific characteristics of judicial decisions make them more prone to misclassification by certain models. Such an analysis, however, requires a multidisciplinary approach, including expert input from highly skilled legal practitioners.\nFuture research should also explore different chunking strategies to enhance text processing. Comparing syntactic chunking, which is based on grammatical structure, with semantic chunking, which is based on content meaning, could provide valuable insights. As this study focuses on a Portuguese-language dataset, evaluating these chunking approaches across datasets in multiple languages would help determine if optimal chunking strategies vary with language, contributing to more robust long-text segmentation and model performance across diverse linguistic contexts."}]}