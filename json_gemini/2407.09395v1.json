{"title": "Deep Bag-of-Words Model: An Efficient and Interpretable Relevance Architecture for Chinese E-Commerce", "authors": ["Zhe Lin", "Jiwei Tan", "Dan Ou", "Xi Chen", "Shaowei Yao", "Bo Zheng"], "abstract": "Text relevance or text matching of query and product is an essential technique for the e-commerce search system to ensure that the displayed products can match the intent of the query. Many studies focus on improving the performance of the relevance model in search system. Recently, pre-trained language models like BERT have achieved promising performance on the text relevance task. While these models perform well on the offline test dataset, there are still obstacles to deploy the pre-trained language model to the online system as their high latency. The two-tower model is extensively employed in industrial scenarios, owing to its ability to harmonize performance with computational efficiency. Regrettably, such models present an opaque \"black box\" nature, which prevents developers from making special optimizations. In this paper, we raise deep Bag-of-Words (DeepBoW) model, an efficient and interpretable relevance architecture for Chinese e-commerce. Our approach proposes to encode the query and the product into the sparse BoW representation, which is a set of word-weight pairs. The weight means the important or the relevant score between the corresponding word and the raw text. The relevance score is measured by the accumulation of the matched word between the sparse BoW representation of the query and the product. Compared to popular dense distributed representation that usually suffers from the drawback of black-box, the most advantage of the proposed representation model is highly explainable and interventionable, which is a superior advantage to the deployment and operation of online search engines. Moreover, the online efficiency of the proposed model is even better than the most efficient inner product form of dense representation. The proposed model is experimented on three different datasets for learning the sparse BoW representations, including the human-annotation set, the search-log set and the click-through set. Then the models are evaluated by experienced human annotators. Both the auto metrics and the online evaluations show our DeepBoW model achieves competitive performance while the online inference is much more efficient than the other models. Our DeepBoW model has already deployed to the biggest Chinese e-commerce search engine Taobao and served the entire search traffic for over 6 months.", "sections": [{"title": "1 INTRODUCTION", "content": "The popularization of mobile internet has significantly elevated the prominence of online commerce in daily life. Hundreds of millions of customers purchase products they want on large e-commerce portals, such as Taobao and Amazon. The search engine emerges as the essential technology in assisting users to discover products that are in accord with their preferences. Different from general search engines like Google, commercial e-commerce search engines are usually designed to improve the user's engagement and conversion, possibly at the cost of relevance in some cases [2]. The exhibition of products in search results that are inconsistent with the query intent has the potential to diminish the customer experience and hamper customers' long-term trust and engagement. Consequently, measuring relevance between the text of search query and products to filter the irrelevant products is an indispensable process in the e-commerce search engine."}, {"title": "2 RELATED WORK", "content": "The text matching task takes textual sequences as input and predicts a numerical value or a category indicating their relationship. Text matching is a long-stand problem and a hot research topic as it's important in information retrieval and search system. The e-commerce relevance learning can be regarded as a text-matching task. Early work mostly performs keyword-based matching that relies on manually defined features, such as TF-IDF similarity and BM25 [24]. These methods cannot effectively utilize raw text features and usually fail to evaluate semantic relevance.\nRecently with the development of deep learning, neural-based text-matching models have been employed to solve the semantic matching task and have achieved promising performance. The architecture of the neural-based text-matching model can be roughly divided into the interaction-based model and the representation-based (two-tower) model. The interaction-based model [6, 19, 19, 20, 29] usually puts all candidate text together as the input. The model can employ the full textual feature to calculate the matching feature as the low layer, and then aggregate the partial evidence of relevance to make the final decision. So interaction-based model can leverage sophisticated techniques in the aggregation procedure and achieve better performance. More recent studies are built upon the pre-trained language model like BERT [4]. With an extremely large corpus for pre-training, these methods can achieve new state-of-the-art performance on various benchmarks. The architecture of these models is the pre-trained bidirectional Transformer [28], which can also be regarded as an interaction-based model. The typical paradigm of the BERT-based relevance model is to feed text pair into BERT and then build a non-linear classifier upon BERT's [CLS] output token to predict the relevance score [15, 16].\nAlthough having excellent performance in the text-matching task, interaction-based models are still hard to be deployed to practical online service as they are mostly time-consuming, and the features of queries and documents cannot be pre-computed offline."}, {"title": "2.2 Search Relevance Matching", "content": "Relevance in search engine is a special sub-task of the text-matching which computes the relevance score between the query and the product (as the document). Different from the typical text-matching task which all input texts are semantically similar and homogeneous (i.e. having comparable lengths), the length of query may be much shorter than the length of document. Query only needs to match the partial semantics in the document.\nA large number of models are proposed for conducting matching in search. Neural Tensor Network (NTN) [26] is originally proposed to explicitly model multiple interactions of relational data. It achieves powerful representation ability that can represent multiple similarity functions, including cosine similarity, dot product, and bilinear product, etc. Qiao et al. [21] apply the BERT model to ad-hoc retrieval and passage retrieval. Reimers and Gurevych [23] propose Sentence-BERT for reducing the computational overhead for text matching. Bai et al. [1] conduct a pilot study to map the frequency-based and BoW representation of a document to a sparse term importance distribution for text retrieval.\nE-commerce search is a special scenario of the Web search system. Both tasks model the semantic matching between query and candidate and require high efficiency and low latency in the online search system. Differently, in Web search the query and document are usually very different in length, making most methods not feasible for the e-commerce relevance task. Currently, there is not a commonly-used public benchmark for the Chinese e-commerce relevance task, so previous works usually evaluate their models on the online service and the real-world dataset constructed from the online platforms. Guo et al. [5] introduce a typical framework for e-commerce relevance learning. A Siamese network is adopted to learn pair-wise relevance of two products to a query. They investigate training the model with user clicks and batch negatives, followed by finetuning with human supervision to calibrate the score by pair-wise learning. Xiao et al. [31] propose a co-training framework to address the data sparseness problem by investigating the instinctive connection between query rewriting and semantic matching. Yao et al. [32] propose to learn a two-tower relevance model from click-through data in e-commerce by designing a point-wise loss function. Zhang et al. [34] also find the weakness of"}, {"title": "3 METHODOLOGY", "content": "The proposed DeepBoW model is based on the two-tower architecture, which encodes the query and document separately and computes the semantic relevance score with the representations of query and document. Different from other text relevance models with dense embeddings, our model encodes the query and document into the Bag-of-Words vectors and calculates the relevance score from sparse BoW representations.\nIn this section, we introduce the components of our model in detail. We first describe the multi-granularity encoder to aggregate the character-grained feature and word-grained feature. Next, we introduce two different sparse BoW representations including the term-weighting BoW and the synonym-weighting BoW. Then, we show how to use N-gram hashing to reduce the semantic loss from word segmentation and enhance the quality of the sparse BoW representation. Finally, we describe the training process of our model in detail and show the deployment of our DeepBoW model to the online e-commerce search system. Figure 1 shows an overview of the DeepBoW model."}, {"title": "3.2 Multi-Granularity Encoder", "content": "The text encoder aims to obtain the input text's contextual representations. We choose Transformer encoder [28] as our sentence encoder because of its excellent performance in many tasks. The Transformer encoder is a stack of L identical layers, and each layer includes a multi-head self-attention and a fully connected feed-forward network. For the input senquence S, we obtain the output encoding matrix of i-th layer as \\(H_i = \\{h^i_1, h^i_2,\u2026\u2026\u2026, h^i_{l}\\}\\), where \\(h_i\\in R^d\\) is the word embedding vector and I is the number of words in S. Same to ReprBERT [33], we aggregate the output of each layer as the text encoding representation according to:\n\\(h_i = \\frac{1}{L} \\sum_{i=1}^{L} h^i_k W_m+b_m\\) (1)\n\\(h = [h^c;h^w]W_{agg} + b_{agg}\\)\nwhere \\(W_m \\in R^{d\u00d7d}\\), \\(W_{agg} \\in R^{L\u22c5d\u00d7d}\\), \\(b_m, b_{agg} \\in R^d\\), || is the concatenate operation."}, {"title": "3.3 Sparse BoW Representation", "content": "Unlike traditional two-tower architecture models that encode text into the \"embedding\" which is a dense distributed representation, our model encodes the text into the sparse BoW representation. The sparse BoW representation is a set of word-weight pairs, where each word corresponds to a weight that indicates the importance or the relevance of this word to the input text. In this section, we introduce two different sparse BoW representations: term-weighting BoW representation and synonym-weighting BoW representation, and describe the module to generate these two sparse BoW representations in detail."}, {"title": "3.3.1 Term-Weighting BoW Representation.", "content": "In the e-commerce search system, the query inputted by user may contain some redundant or unrelated words. These words can be excised with negligible impact on the text semantic. For example, for the input query from Taobao like \"2024\u5e74\u590f\u5b63\u9002\u5408\u51c6\u5988\u5988 \u5b55\u5987\u5957\u88c5\", \"\u51c6\u5988\u5988\" and \"\u5b55\u5987\" both mean a pregnant woman, but \"\u5b55\u5987\" is more accurate than\"\u51c6\u5988\u5988\" at semantic level as the latter word is polysemous and more colloquial. \"\u9002\u5408\" which means suitable, can be regarded as a stop word in the e-commerce scenario. So \"\u51c6\u5988\u5988\" and \"\u9002\u5408\" can be discarded and the other words should be retained.\nTerm-Weighting BoW includes all words of the input text, and each word is assigned a weight that indicates its significance within the text's semantics. Key words like brand and category should have greater importance weights than the other words. Figure 1 (a) shows the architecture to generate the term-weighting BoW representation. Then, the term-weighting BoW representation can be produced as follows:\n\\(p_i = \\frac{exp (h^c_i h^w_i)}{\\sum_k exp (h^c_k h^w_k)}\\)\n\\(BoW_{tw} (S_w) := \\{w_i : p_i, w_i \\in S_w\\}\\) (2)\nWe define the \\(BoW_{tw}()\\) as the term-weighting BoW representation of. \\(p_i\\) is the importance weight of \\(w_i\\) in Sw, and \\(\\sum_i p_i = 1\\)."}, {"title": "3.3.2 synonym-expansion BoW Representation.", "content": "Since queries in e-commerce search systems are entered by lay users, they may differ from the product descriptions and include colloquialisms and polysemes. Some adjectives or category words may also have synonyms. Term-weighting BoW representation can only compute the importance weight of each word in the text, but is unable to add relevant words and synonyms. Synonym expansion can greatly improve the performance of the e-commerce search system. Therefore, we propose the synonym-expansion BoW representation to enhance the retrieval performance of the sparse BoW representation. Figure 1 (b) shows the architecture to generate the synonym-expansion BoW representation.\nWe sample v words from the training corpora as the vocabulary V according to the frequency of the word. Our model leverages the relevance between these words and the input text to represent the semantics of the query and the products. First, our model aggregates the word-based text encoding representation and the character-based text encoding representation as follows:\n\\(V_c = \u03c3 (h_c W_c + b_c)\\)\n\\(V_w = \u03c3 ([h_c;h_w] W_w+b_w)\\) (3)\nwhere \\(W_c \\in R^{d\u00d7v}\\), \\(W_w \\in R^{2d\u00d7v}\\) and \\(b_c, b_w \\in R^v\\). \u03c3 is the sigmoid function. Then, the synonym-expansion BoW Representation of the input text is as follows:\n\\(p_g = \u03c3 ([h_c;h_w] W_g+b_g)\\)\n\\(BOW_{SE} (S_w) := \\{t : V_c(t), t \\in V - S_w\\} \\bigcup \\{t: p_gV_c(t) + (1 \u2212 p_g)V_w(t), t \\in S_w\u2229V\\}\\) (4)\nwhere V(i) denotes the i-th value of V. We define theBOWSE () as the synonym-expansion BoW representation of.. t is the word (actually is the index of the word) in V. the corresponding weight is in [0, 1], which can be regarded as the relevance score between this word and the input text."}, {"title": "3.4 N-gram Hashing Vocabulary", "content": "In the preceding section, we describe the sparse BoW representation in detail. Unfortunately, due to the limitation of model's parameter size, we can only leverage the vocabulary within a limited number of words. Using the \u201c[UNK]\u201d to replace all Out-Of-Vocabulary (OOV) words may lead to significant semantic loss. To mitigate this issue, we introduce an ensemble of hashing tokens into the vocabulary, where the OOV word can be replaced with its hashing tokensSemantic loss may occur between the raw text and its BoW representation, particularly when syntactically cohesive phrases are fragmented during the word segmentation process. This issue can lead to misalignment for the essential semantics such as product types, entity names, or brand identifiers in query/product. For example, the brand name \"L'OR\u00c9AL Paris\" could be inaccurately divided into separate tokens during word segmentation. To address this problem, we introduce an N-gram hashing vocabulary strategy. Concretely, N-gram phrases are incorporated into the text's BoW and are subsequently replaced with their respective hashing tokens, analogous to the treatment of OOV words. The significance of a particular N-gram phrase is directly proportional to the frequency of its occurrence within relevant query-product pairs in the corpora. Our model is equipped to ascertain the importance of these N-gram hashing tokens through the analysis of large-scale corpora. Consequently, the semantics of these N-gram phrases are retained within the sparse BoW representation."}, {"title": "3.5 DeepBoW Relevance Model", "content": "In this section, we describe the method to compute the relevance score between the query and the product from the sparse BoW representations. Note that in the search engine scenario the product should match all the semantics of query, while conversely the query does not need to match all the semantics of the product. Accordingly, we encode the query as the term-weighting BoW representation while encode the product as the synonym-expansion BoW representation. The relevance score of the query/product can be calculated as follows:\n\\(R_t(Q, D) = \\sum_{(w:p) \\in BOW_{TW} (Q)} \\sum_{(t:g) \\in BOW_{SE} (D)} pxg\\)\nwhere Q is the query, D is the product, and Rt (Q, D) is the relevance score between Q and D. We call our DeepBoW model with this relevance score as DeepBoW(Q-Weight).\nWe leverage the cross-entropy loss between the output score and the ground truth to train our model. In addition, we also optimize the L2 norm of BOWSE (D) to enhance the sparsity of the synonym-expansion BoW representation, so that only the most relevant words can get high scores. The loss function is as follows:\n\\(norm = \\sum_{(t:g) \\in BOW_{SE} (D)} g^2\\)\n\\(loss_t = CE (R_t, label) + \\frac{1}{v} norm\\) (6)\nwhere CE is the cross-entropy loss, label is the ground truth of the training data. We can also encode the query as the synonym-expansion BoW representation to improve the performance of recall. The relevance score is as follows:\n\\(C = \\sum_{(w:p) \\in BOW_{SE} (Q)}p\\)\n\\(R_s(Q, D) = \\frac{1}{C} \\sum_{(w:p) \\in BOW_{SE} (Q)} \\sum_{(t:g) \\in BOW_{SE} (D)} pxg\\) (7)\nWe call our DeepBoW model with this relevance score as Deep-BoW(Q-Synonym). Different from losst, we also leverage the bag-of-words of the query to train the product's synonym-expansion BoW representation. The loss should be modified as follows:\n\\(BoW_{avg}(S_w) := \\{w_i : 1/n, w_i \\in S_w\\}\\)\n\\(R_{avg} (Q, D) = \\sum_{(w:p) \\in BOW_{avg} (Q)} \\sum_{(t:g) \\in BOW_{SE} (D)} pxg\\) (8)\n\\(loss_s = CE(R_s, label) + CE (R_{avg}, label) + \\frac{1}{v} norm\\)\nFor both losst and losss, we optimize the difference between the query's sparse BoW representation and the product's sparse BoW representation. This can align the vocabulary between the query and the product."}, {"title": "3.6 Online Deployment", "content": "Since most online search systems have strict latency limitations, we pre-compute the sparse BoW representation of the queries and the products offline. We discard the word-weight pairs in the sparse BoW representation whose weights are lower than the given threshold to optimize memory usage. We sort the word-weight pairs offline by the word index. The relevance score of two sparse BoW representations can be calculated by using the two-pointer algorithm. Then the time complexity of Eq.5 and Eq.7 can be optimized to O(N), which is much faster than the state-of-the-art deep relevance model [33]. Although some deep relevance models with cosine similarity can achieve comparable efficiency, the performance of these models is much lower than our model as shown in the section 4.5."}, {"title": "4 EXPERIMENTS", "content": "There is no public dataset and benchmark for the Chinese e-commerce relevance task, so we conduct experiments on three different types of industrial datasets to learn the DeepBoW model. The first is a large-scale Human-Annotation dataset which contains query-product pairs sampled from the Taobao search logs. Each query-product pair is labeled as Good (relevant) or Bad (irrelevant) by experienced human annotators. This is a daily task running in Taobao, which has accumulated more than 8 million labeled samples. We split the human-annotated datasets into training, validation and test sets, as detailed in Table 1.\nThe second dataset for training is built by knowledge distillation, similar to Yao et al. [33]. We leverage the training set of the human-annotation dataset to finetune the StructBERT [30] model, which results in an interaction-based teacher model with strong performance. Then the teacher model predicts the relevance scores of the large unlabeled query-product pairs sampled from the search logs of Taobao within a year. This training dataset is denoted as \"Search-Logs\" in Table 2. Third, we also sample click-through data from search logs and investigate the performance of our model on this training set. We denote this dataset as \"Click-Through\" in Table 2. Although these are training datasets of different sources, we all use the human-annotation validation and test dataset to evaluate the model performance."}, {"title": "4.2 Training Details", "content": "We employ Transformer as both the character-based encoder and the word-based encoder. We reduce the total 12-layer encoder to improve efficiency. After balancing the effectiveness and efficiency, our model adopts 2 layers that can still achieve competitive performance. We select the top 50000 words as the vocabulary V according to the word's frequency in corpora, and we also add another 10000 hashing tokens into the vocabulary for the OOV words and the N-gram phrases."}, {"title": "4.3 Baseline", "content": "We explore the performance of DeepBoW(Q-Weight) and DeepBoW (Q-Synonym) respectively. The main difference between the two methods is for DeepBoW(Q-Synonym) we leverage the synonym-expansion BoW representation to replace the term-weighting BoW representation for the query. To reduce memory usage and computation, we truncate the BoW representation to make it sparse. There are two methods to truncate the sparse BoW representation, one is to keep the k largest words according to their respective values, and the other is to discard the terms whose values are smaller than the giving threshold.\nIn addition, we adopt several state-of-the-art methods for comparison. BERT [4], ROBERTa [12] and StructBERT [30] belong to the interaction-based architecture which is also known as the cross-encoder architecture. Siamese BERT [9], MASM [32], Poly-encoders [8] and ReprBERT [33] belong to the two-tower architecture which is also known as the bi-encoder architecture. Besides, we investigate the performance of ReprBERT with cosine similarity score instead of MLP for online computation of relevance from query/product embeddings. For fair comparison, we also leverage the pre-trained model like RoBERTa and StructBERT as the encoder of the two-tower model. These models are also baselines in our experiment, which denote as DSSMROBERTa and DSSMStructBERT."}, {"title": "4.4 Evaluation Metrics", "content": "We evaluate our model on both offline and online metrics. In offline evaluation, since the human annotation is binary, the task is evaluated as a classification task. The Receiver Operator Characteristic Area Under Curve (ROC-AUC) is widely adopted in text relevance tasks [3, 10, 29]. Note that in the e-commerce relevance scenario, most instances are positive and we are more concerned about negative instances. Therefore the PR-AUC used in this paper is the negative PR-AUC that treats Bad as 1 and Good as 0 following Yao et al. [32, 33]. This metric is denoted as Neg PR-AUC.\nBesides, we also evaluate the different model complexity of parameters and online computation efficiency. The FLOPs / token is computed according to Molchanov et al. [13] which shows the floating-point operations per second (FLOPs) when there is only 1 token being considered. The plus sign separates the online and offline calculation FLOPs, which means the former part of computation can be pre-computed offline. Memory indicates the online memory overhead for storing pre-computed query and product vectors where we use vector size for comparison. In online evaluation, we use the rate of Good annotated by human annotators and"}, {"title": "4.5 Results", "content": "Table 2 presents an evaluative comparison across various models. Our DeepBoW model demonstrates robust performance across three different training sets. For human-annotation dataset, Struct-BERT has the best performance, and interaction-based models outperform two-tower models. Unfortunately, it is infeasible to deploy the interaction-based model in industrial system because of prohibitive computation and resource requirements. The ROC-AUC and Neg PR-AUC of two-tower models are much lower than the pre-trained model, because the human-annotation data is insufficient and the pre-trained model can introduce extra knowledge. Nonetheless, our DeepBoW model still outperforms other two-tower models.\nThe data enhancement sampled from the search logs and labeled by the teacher model can greatly improve the performance of two-tower models. Our model achieves the best performance around the two-tower models in search-logs training sets. Click-through data is used to train the relevance model in some cases where lack of human-annotation training data. The models trained on the click-through data get weak performance. The main reason is that the click-through data in e-commerce is much more noisy and misleading, which is not only affected by the query-product relevance but also by many factors including price, attractive titles, or images. Even so, our model also performs better than the other models since it explicitly encodes the semantics to the sparse bag-of-words while the other models may capture the personalized information beyond the textual feature.\nOur proposed architecture can truncate the sparse BoW representation to reduce memory usage in the online search system. We can either truncate the sparse BoW representation to the fixed length or discard the word-weight pairs whose values are smaller than a given threshold. Benefiting from the interpretability of our sparse BoW representation, both truncation methods achieve competitive performance and only produce a slight loss of performance. We further explore the distribution of the sparse BoW representation as case study in Appx. A."}, {"title": "4.6 Ablation Study", "content": "We perform the ablation study on the human-annotation dataset to investigate the influence of different modules in our model. We"}, {"title": "4.7 Online Evaluation", "content": "Online A/B testing is also conducted to evaluate our DeepBoW model, by replacing the online ReprBERT model with the DeepBoW model for comparison. Both experiments take about 2.5% proportion of Taobao search traffic, and the A/B testing lasts for a month. As a result, DeepBoW improves the number of transactions by about 0.4% on average. The daily human annotation results show that DeepBoW also improves the rate of relevance by 0.5%. Online A/B"}, {"title": "5 CONCLUSION AND FUTURE WORK", "content": "In this paper, we study an industrial task of measuring the semantic relevance of queries and products. We propose the DeepBoW relevance model, which is an efficient and interpretable relevance architecture for Chinese e-commerce search system. Our model encodes the query and product as a set of word-weight pairs, which is called the sparse BoW representation. The model is evaluted on three different training datasets, and the results show that our model achieves promising performance and efficiency. The model has been deployed in the Taobao search system.\nIn future work, we will explore integrating external knowledge into the DeepBoW relevance model to improve the performance. The proposed model can also be evaluated on datasets of other language."}, {"title": "A CASE STUDY", "content": "Table 6 shows two examples of our DeepBoW model. Both query and product are encoded to the synonym-expansion BoW representation. The sparse BoW representation consists of a collection of word-weight pairs, which can be regarded as the bag-of-words with soft weight. The synonym-expansion representation can not only capture the importance of the words in the original text, but also incorporates pertinent synonymous terms. The relevance score can be calculated by aggregating the matching terms of the query's/product's sparse BoW representation.\nThese two examples show that our proposed sparse BoW representation has positive interpretability, signifying that the developer can analyze bad cases from the online search system and implement targeted optimizations. Furthermore, the developer can modify the terms in the sparse BoW representation directly to achieve the expected result. In a word, our DeepBoW model surpasses other deep relevance modeling approaches in terms of interpretability and flexibility, thereby rendering it eminently suitable for the e-commerce search system."}]}