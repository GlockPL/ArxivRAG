{"title": "Deep Bag-of-Words Model: An Efficient and Interpretable Relevance Architecture for Chinese E-Commerce", "authors": ["Zhe Lin", "Jiwei Tan", "Dan Ou", "Xi Chen", "Shaowei Yao", "Bo Zheng"], "abstract": "Text relevance or text matching of query and product is an essential technique for the e-commerce search system to ensure that the displayed products can match the intent of the query. Many studies focus on improving the performance of the relevance model in search system. Recently, pre-trained language models like BERT have achieved promising performance on the text relevance task. While these models perform well on the offline test dataset, there are still obstacles to deploy the pre-trained language model to the online system as their high latency. The two-tower model is extensively employed in industrial scenarios, owing to its ability to harmonize performance with computational efficiency. Regrettably, such models present an opaque \"black box\" nature, which prevents developers from making special optimizations. In this paper, we raise deep Bag-of-Words (DeepBoW) model, an efficient and interpretable relevance architecture for Chinese e-commerce. Our approach proposes to encode the query and the product into the sparse BoW representation, which is a set of word-weight pairs. The weight means the important or the relevant score between the corresponding word and the raw text. The relevance score is measured by the accumulation of the matched word between the sparse BoW representation of the query and the product. Compared to popular dense distributed representation that usually suffers from the drawback of black-box, the most advantage of the proposed representation model is highly explainable and interventionable, which is a superior advantage to the deployment and operation of online search engines. Moreover, the online efficiency of the proposed model is even better than the most efficient inner product form of dense representation. The proposed model is experimented on three different datasets for learning the sparse BoW representations, including the human-annotation set, the search-log set and the click-through set. Then the models are evaluated by experienced human annotators. Both the auto metrics and the online evaluations show our DeepBoW model achieves competitive performance while the online inference is much more efficient than the other models. Our DeepBoW model has already deployed to the biggest Chinese e-commerce search engine Taobao and served the entire search traffic for over 6 months.", "sections": [{"title": "1 INTRODUCTION", "content": "The popularization of mobile internet has significantly elevated the prominence of online commerce in daily life. Hundreds of millions of customers purchase products they want on large e-commerce portals, such as Taobao and Amazon. The search engine emerges as the essential technology in assisting users to discover products that are in accord with their preferences. Different from general search engines like Google, commercial e-commerce search engines are usually designed to improve the user's engagement and conversion, possibly at the cost of relevance in some cases [2]. The exhibition of products in search results that are inconsistent with the query intent has the potential to diminish the customer experience and hamper customers' long-term trust and engagement. Consequently, measuring relevance between the text of search query and products to filter the irrelevant products is an indispensable process in the e-commerce search engine."}, {"title": "2 RELATED WORK", "content": "Text relevance has been a long-standing research topic due to\nits importance in information retrieval and the search engine. Re-\nsearchers and engineers have been dedicated to the pursuit of an ef-\nficient and robust model to accurately measure the text relevance be-\ntween the query and the product in the e-commerce scenario. Con-\nventional methodologies have traditionally harnessed attributes\nsuch as the word matching ratio, Term Frequency-Inverse Docu-\nment Frequency (TF-IDF, notably BM25), or cosine similarity to\nserve as the relevance score, often yielding strong baseline perfor-\nmances. Nevertheless, these word-matching approaches may cause\ninaccuracies due to inconsistent linguistic expressions of identical\nmeanings such as synonyms. This issue is particularly pronounced\nin the e-commerce scenario since the queries are usually described\nby users in daily language while the products are described by\nsellers in professional language. Thus severe vocabulary gap may\nexist between queries and products [31].\nWith the development of deep learning technology, neural mod-\nels have shown their advantage in addressing the semantic match-\ning problem for the text relevance task [6, 19, 29]. Recently, pre-\ntrained language models like BERT [4] achieve excellent results in\nvarious NLP tasks including text matching. Unfortunately, typical\nparadigm of the BERT-based relevance model is the interaction-\nbased structure, which needs to encode the query-document pair\nin real time to measure their relevance. This makes it difficult to\nbe deployed to online systems with large traffic due to the high\ncomputation and latency. Consequently, it is usually impractical\nto deploy the pre-trained model directly to search systems. To ad-\ndress this problem, the representation-based model, also known\nas the two-tower model, is proposed and mostly applied to indus-\ntrial search systems. It usually pre-computes the embeddings of\nquery/document respectively, and measures the relevance online\nfrom the embeddings. SiameseBERT [22] leverages BERT as the\nencoder and calculates the cosine similarity between the dense\nembeddings of query and document as the relevance score. Some\nstudies like ReprBERT [33] explore the more complex MLP classifier\nto compute the relevance score between two dense embeddings,\nwhich can achieve improved performance.\nHowever, the representation-based model with dense embed-\ndings still faces two major issues. First, the dense embedding may\nlose the detailed semantic information of the text, especially for low-\nfrequency words like product models, entity names, or even brand\nidentifiers. These words are essential in the e-commerce relevance\ntask. Second, the dense embedding presents an opaque \"black box\"\nnature, which prevents developers from comprehensively under-\nstanding the model's methodology for calculating relevance scores.\nDevelopers often find it difficult to analyze the reasons for bad cases\nin the online system and implement targeted optimizations. In con-\ntrast, traditional word-matching algorithms like BM25 continue to\nbe favored in numerous industrial applications[27] due to their high\nefficiency and robust interpretability. Such word-based algorithm\ncan capture the match of words that are low-frequency but essential\nfor the text-relevance task. Unfortunately, these methods are not\nwithout their constraints. They fall short in recognizing different\nlinguistic expressions that convey identical meanings, such as syn-\nonyms, thereby limiting their effectiveness in semantic matching\ntasks."}, {"title": "2.1 Text Matching", "content": "The text matching task takes textual sequences as input and pre-\ndicts a numerical value or a category indicating their relationship.\nText matching is a long-stand problem and a hot research topic\nas it's important in information retrieval and search system. The\ne-commerce relevance learning can be regarded as a text-matching\ntask. Early work mostly performs keyword-based matching that\nrelies on manually defined features, such as TF-IDF similarity and\nBM25 [24]. These methods cannot effectively utilize raw text fea-\ntures and usually fail to evaluate semantic relevance.\nRecently with the development of deep learning, neural-based\ntext-matching models have been employed to solve the semantic\nmatching task and have achieved promising performance. The archi-\ntecture of the neural-based text-matching model can be roughly di-\nvided into the interaction-based model and the representation-based\n(two-tower) model. The interaction-based model [6, 19, 19, 20, 29]\nusually puts all candidate text together as the input. The model\ncan employ the full textual feature to calculate the matching fea-\nture as the low layer, and then aggregate the partial evidence of\nrelevance to make the final decision. So interaction-based model\ncan leverage sophisticated techniques in the aggregation procedure\nand achieve better performance. More recent studies are built upon\nthe pre-trained language model like BERT [4]. With an extremely\nlarge corpus for pre-training, these methods can achieve new state-\nof-the-art performance on various benchmarks. The architecture\nof these models is the pre-trained bidirectional Transformer [28],\nwhich can also be regarded as an interaction-based model. The\ntypical paradigm of the BERT-based relevance model is to feed text\npair into BERT and then build a non-linear classifier upon BERT's\n[CLS] output token to predict the relevance score [15, 16].\nAlthough having excellent performance in the text-matching\ntask, interaction-based models are still hard to be deployed to prac-\ntical online service as they are mostly time-consuming, and the\nfeatures of queries and documents cannot be pre-computed offline."}, {"title": "2.2 Search Relevance Matching", "content": "Relevance in search engine is a special sub-task of the text-matching\nwhich computes the relevance score between the query and the\nproduct (as the document). Different from the typical text-matching\ntask which all input texts are semantically similar and homogeneous\n(i.e. having comparable lengths), the length of query may be much\nshorter than the length of document. Query only needs to match\nthe partial semantics in the document.\nA large number of models are proposed for conducting matching\nin search. Neural Tensor Network (NTN) [26] is originally pro-\nposed to explicitly model multiple interactions of relational data.\nIt achieves powerful representation ability that can represent mul-\ntiple similarity functions, including cosine similarity, dot product,\nand bilinear product, etc. Qiao et al. [21] apply the BERT model to\nad-hoc retrieval and passage retrieval. Reimers and Gurevych [23]\npropose Sentence-BERT for reducing the computational overhead\nfor text matching. Bai et al. [1] conduct a pilot study to map the\nfrequency-based and BoW representation of a document to a sparse\nterm importance distribution for text retrieval.\nE-commerce search is a special scenario of the Web search sys-\ntem. Both tasks model the semantic matching between query and\ncandidate and require high efficiency and low latency in the online\nsearch system. Differently, in Web search the query and document\nare usually very different in length, making most methods not fea-\nsible for the e-commerce relevance task. Currently, there is not a\ncommonly-used public benchmark for the Chinese e-commerce\nrelevance task, so previous works usually evaluate their models\non the online service and the real-world dataset constructed from\nthe online platforms. Guo et al. [5] introduce a typical framework\nfor e-commerce relevance learning. A Siamese network is adopted\nto learn pair-wise relevance of two products to a query. They in-\nvestigate training the model with user clicks and batch negatives,\nfollowed by finetuning with human supervision to calibrate the\nscore by pair-wise learning. Xiao et al. [31] propose a co-training\nframework to address the data sparseness problem by investigating\nthe instinctive connection between query rewriting and semantic\nmatching. Yao et al. [32] propose to learn a two-tower relevance\nmodel from click-through data in e-commerce by designing a point-\nwise loss function. Zhang et al. [34] also find the weakness of"}, {"title": "3 METHODOLOGY", "content": "Is it possible to combine the advantages of both deep semantic models and word matching methods? In this paper, we propose\nDeep Bag-of-Words (DeepBoW), which can leverage the pre-trained language model with large language corpora to improve semantic modeling while preserving the computational efficiency and interpretability of the word-matching method. We realize this by designing to learn sparse bag-of-words representation through deep neural networks. Our model generates the query/product high-dimensional representation (called the BoW representation) instead of the low-dimensional distributed representation (i.e. embedding). The dimensional size is the same as the size of the vocabulary. Each position in this high-dimensional representation corresponds to a word in the vocabulary, and with a value represents the weight of this word in the BoW representation, just like the BoW vector of TF-IDF. The proposed DeepBoW model encounters two pre-dominant challenges. Firstly, due to the opaque nature of neural network models, often colloquially referred to as \"black boxes\", it is challenging to correlate positions within high-dimensional representations to specific words in the vocabulary. Secondly, the vocabulary size is usually much larger than the dimension of dense embedding. Expanding the dimensional size to the vocabulary size may explode the computation and storage resources. For the first challenge, we elaborately design the architecture of the model and loss function to align the position of the high-dimensional repre-sentation and the corresponding word in the vocabulary. For the second challenge, we add a sparse constraint in the loss function to reduce valid positions in the high-dimensional representation since the query/product should not include all words of vocabu-lary. Finally, we sample the high-dimensional representation to a small set of non-zero word-weight pairs, which is named as sparse BoW representation. In addition, although queries and product descriptions in e-commerce primarily consist of keywords, there still exists some semantic dependency on the word combination, which unigrams may not capture adequately. For example, brand names with multi-words may be incorrectly matched if they are separated. Consequently, we propose to model n-gram in our Deep-BoW model, meanwhile introducing an n-gram hashing vocabulary strategy to avoid the explosion of vocabulary size. Finally, with the sparse BoW representation, the relevance score is measured in a most easy way as the weight accumulation of the matching words in the query/product's sparse BoW representations, which makes it highly efficient and interpretable.\nThe proposed DeepBoW model is evaluated on the three indus-trial datasets. The results show that our DeepBoW model achieves more than 2.1% AUC improvement compared to the state-of-the-art two-tower relevance model on Chinese e-commerce. The sparse BoW representation generated by our DeepBoW model has positive interpretability and supports easy problem investigation and intervention for online systems. The time complexity of the online relevance score computing program can be optimized to O(N) by leveraging the two-pointer algorithm, which is faster than previous state-of-the-art relevance model. The contribution of this paper is summarized as follows:"}, {"title": "3.1 Overview", "content": "The proposed DeepBoW model is based on the two-tower archi-tecture, which encodes the query and document separately and computes the semantic relevance score with the representations of query and document. Different from other text relevance models with dense embeddings, our model encodes the query and docu-ment into the Bag-of-Words vectors and calculates the relevance score from sparse BoW representations.\nIn this section, we introduce the components of our model in detail. We first describe the multi-granularity encoder to aggregate the character-grained feature and word-grained feature. Next, we introduce two different sparse BoW representations including the term-weighting BoW and the synonym-weighting BoW. Then, we show how to use N-gram hashing to reduce the semantic loss from word segmentation and enhance the quality of the sparse BoW representation. Finally, we describe the training process of our model in detail and show the deployment of our DeepBoW model to the online e-commerce search system."}, {"title": "3.2 Multi-Granularity Encoder", "content": "The text encoder aims to obtain the input text's contextual repre-sentations. We choose Transformer encoder [28] as our sentence encoder because of its excellent performance in many tasks. The Transformer encoder is a stack of L identical layers, and each layer includes a multi-head self-attention and a fully connected feed-forward network. For the input senquence S, we obtain the out-put encoding matrix of i-th layer as  \\(H_i = {h^i_1, h^i_2, \u2026, h^i_l}\\) , where \\(h^i \u2208 R^d\\) is the word embedding vector and l is the number of words in S. Same to ReprBERT [33], we aggregate the output of each layer as the text encoding representation according to:\n  \\(h_i = \\frac{1}{L} \\sum_{i=1}^L h_i W_m + b_m \\\\\nh = [h^c || h^w] W_{agg} + b_{agg}\\) \nwhere \\(W_m \u2208 R^{d\u00d7d}\\), \\(W_{agg} \u2208 R^{L\u22c5d\u00d7d}\\), \\(b_m, b_{agg} \u2208 R^d\\), \\(||\\) is the concatenate operation."}, {"title": "3.3 Sparse BoW Representation", "content": "Unlike traditional two-tower architecture models that encode text into the \"embedding\" which is a dense distributed representation, our model encodes the text into the sparse BoW representation. The sparse BoW representation is a set of word-weight pairs, where each word corresponds to a weight that indicates the importance or the relevance of this word to the input text. In this section, we in-troduce two different sparse BoW representations: term-weighting BoW representation and synonym-weighting BoW representation, and describe the module to generate these two sparse BoW repre-sentations in detail."}, {"title": "3.3.1 Term-Weighting BoW Representation.", "content": "In the e-commerce search system, the query inputted by user may contain some redundant or unrelated words. These words can be excised with negligible impact on the text semantic. For example, for the input query from Taobao like \"2024\u5e74\u590f\u5b63\u9002\u5408\u51c6\u5988\u5988\u5b55\u5987\u5957\u88c5\"1, \"\u51c6\u5988\u5988\" and \"\u5b55\u5987\" both mean a pregnant woman, but \"\u5b55\u5987\" is more accurate than\"\u51c6\u5988\u5988\" at semantic level as the latter word is polysemous and more colloquial. \"\u9002\u5408\" which means suitable, can be regarded as a stop word in the e-commerce scenario. So \"\u51c6\u5988\u5988\" and \"\u9002\u5408\" can be discarded and the other words should be retained.\nTerm-Weighting BoW includes all words of the input text, and each word is assigned a weight that indicates its significance within the text's semantics. Key words like brand and category should have greater importance weights than the other words. Then, the term-weighting BoW representation can be produced as follows:\n  \\(p_i = \\frac{exp (h_i h_w)}{\\sum_j exp (h_i h_w)} \\\\\nBoW_{tw} (S_w) := {w_i : p_i, w_i \u2208 S_w}\\)\nWe define the \\(BoW_{tw}()\\) as the term-weighting BoW representation of . \\(p_i\\) is the importance weight of \\(w_i\\) in \\(S_w\\), and \\(\\sum_i p_i = 1\\)."}, {"title": "3.3.2 synonym-expansion BoW Representation.", "content": "Since queries in e-commerce search systems are entered by lay users, they may differ from the product descriptions and include colloquialisms and polysemes. Some adjectives or category words may also have synonyms. Term-weighting BoW representation can only compute the importance weight of each word in the text, but is unable to add relevant words and synonyms. Synonym expansion can greatly improve the performance of the e-commerce search system. Therefore, we propose the synonym-expansion BoW repre-sentation to enhance the retrieval performance of the sparse BoW representation. \nWe sample v words from the training corpora as the vocabulary V according to the frequency of the word. Our model leverages the relevance between these words and the input text to represent the semantics of the query and the products. \n  \\(V_c = \\sigma (h_c W_c + b_c) \\\\\nV_w = \\sigma ([h_c || h_w] W_w + b_w)\\\\) \nwhere \\(W_c \u2208 R^{d\u00d7v}\\), \\(W_w \u2208 R^{2d\u00d7v}\\) and \\(b_c, b_w \u2208 R^v\\). \u03c3 is the sigmoid function. Then, the synonym-expansion BoW Representation of the input text is as follows:\n  \\(p_g = \\sigma ([h_c || h_w] W_g + b_g) \\\\\nBoW_{SE} (S_w) := {t : V_c(t), t \u2208 V - S_w} \u222a {t: p_gV_c(t) + (1 - p_g)V_w(t), t \u2208 S_w \u2229 V}\\\\) \nwhere V(i) denotes the i-th value of V. We define the\\(BoW_{SE} ()\\) as the synonym-expansion BoW representation of.. t is the word (actually is the index of the word) in V. the corresponding weight is in [0, 1], which can be regarded as the relevance score between this word and the input text."}, {"title": "3.4 N-gram Hashing Vocabulary", "content": "In the preceding section, we describe the sparse BoW representation in detail. Unfortunately, due to the limitation of model's parameter size, we can only leverage the vocabulary within a limited number of words. Using the \u201c[UNK]\u201d to replace all Out-Of-Vocabulary (OOV) words may lead to significant semantic loss. To mitigate this issue, we introduce an ensemble of hashing tokens into the vocabulary, where the OOV word can be replaced with its hashing tokens\u00b2.\nSemantic loss may occur between the raw text and its BoW rep-resentation, particularly when syntactically cohesive phrases are fragmented during the word segmentation process. This issue can lead to misalignment for the essential semantics such as product types, entity names, or brand identifiers in query/product. For ex-ample, the brand name \"L'OR\u00c9AL Paris\" could be inaccurately divided into separate tokens during word segmentation. To address this problem, we introduce an N-gram hashing vocabulary strategy. Concretely, N-gram phrases are incorporated into the text's BoW and are subsequently replaced with their respective hashing tokens, analogous to the treatment of OOV words. The significance of a particular N-gram phrase is directly proportional to the frequency of its occurrence within relevant query-product pairs in the cor-pora. Our model is equipped to ascertain the importance of these N-gram hashing tokens through the analysis of large-scale corpora. Consequently, the semantics of these N-gram phrases are retained within the sparse BoW representation."}, {"title": "3.5 DeepBoW Relevance Model", "content": "In this section, we describe the method to compute the relevance score between the query and the product from the sparse BoW representations. Note that in the search engine scenario the prod-uct should match all the semantics of query, while conversely the query does not need to match all the semantics of the product. Accordingly, we encode the query as the term-weighting BoW rep-resentation while encode the product as the synonym-expansion BoW representation. The relevance score of the query/product can be calculated as follows:\n  \\(R_t (Q, D) = \\sum_{(w:p) \u2208 BoW_{TW} (Q)} \\sum_{(t:g) \u2208BoW_{SE} (D)} px g\\)\nwhere Q is the query, D is the product, and  \\(R_t (Q, D)\\) is the relevance score between Q and D. We call our DeepBoW model with this relevance score as DeepBoW(Q-Weight).\nWe leverage the cross-entropy loss between the output score and the ground truth to train our model. In addition, we also optimize the L2 norm of  \\(BoW_{SE} (D)\\) to enhance the sparsity of the synonym-expansion BoW representation, so that only the most relevant words can get high scores. The loss function is as follows:\n  \\(norm = \\sum_{(t:g) \u2208 BoW_{SE} (D)} g^2\\\\ loss_t = CE (R_t, label) + \\frac{1}{\u03c5}norm \\)\nwhere CE is the cross-entropy loss, label is the ground truth of the training data. We can also encode the query as the synonym-expansion BoW representation to improve the performance of recall. The relevance score is as follows:\n  \\(C = \\sum_{(w:p) \u2208 BoW_{SE} (Q)} p\\\\\nR_s (Q, D) = \\frac{1}{C} \\sum_{(w:p) \u2208 BoW_{SE} (Q)} \\sum_{(t:g) \u2208BoW_{SE} (D)} px g\\)\nWe call our DeepBoW model with this relevance score as Deep-BoW(Q-Synonym). Different from \\(loss_t\\), we also leverage the bag-of-words of the query to train the product's synonym-expansion BoW representation. The loss should be modified as follows:\n  \\(BoW_{avg}(S_w) := {w_i : \\frac{1}{n}, w_i \u2208 S_w}\\\\\nR_{avg} (Q, D) = \\sum_{(w:p) \u2208 BoW_{avg} (Q)} \\sum_{(t:g) \u2208BoW_{SE} (D)} px g\\\\\nloss_s = CE(R_s, label) + CE (R_{avg}, label) + \\frac{1}{\u03c5}norm \\)\nFor both \\(loss_t\\) and \\(loss_s\\), we optimize the difference between the query's sparse BoW representation and the product's sparse BoW representation. This can align the vocabulary between the query and the product."}, {"title": "3.6 Online Deployment", "content": "Since most online search systems have strict latency limitations,\nwe pre-compute the sparse BoW representation of the queries and\nthe products offline. We discard the word-weight pairs in the sparse\nBoW representation whose weights are lower than the given thresh-\nold to optimize memory usage. We sort the word-weight pairs of-\nline by the word index. The relevance score of two sparse BoW\nrepresentations can be calculated by using the two-pointer algo-\nrithm. Then the time complexity of Eq.5 and Eq.7 can be optimized\nto O(N), which is much faster than the state-of-the-art deep rel-\nevance model [33]. Although some deep relevance models with\ncosine similarity can achieve comparable efficiency, the perfor-\nmance of these models is much lower than our model as shown in\nthe section 4.5."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 Dataset", "content": "There is no public dataset and benchmark for the Chinese e-commerce relevance task, so we conduct experiments on three different types of industrial datasets to learn the DeepBoW model. The first is a large-scale Human-Annotation dataset which contains query-product pairs sampled from the Taobao search logs. Each query-product pair is labeled as Good (relevant) or Bad (irrelevant) by experienced human annotators. This is a daily task running in Taobao, which has accumulated more than 8 million labeled samples. We split the human-annotated datasets into training, validation and test sets, as detailed in Table 1."}, {"title": "4.2 Training Details", "content": "We employ Transformer as both the character-based encoder and the word-based encoder. We reduce the total 12-layer encoder to improve efficiency. After balancing the effectiveness and efficiency, our model adopts 2 layers that can still achieve competitive perfor-mance. We select the top 50000 words as the vocabulary V according to the word's frequency in corpora, and we also add another 10000 hashing tokens into the vocabulary for the OOV words and the N-gram phrases."}, {"title": "4.3 Baseline", "content": "We explore the performance of DeepBoW(Q-Weight) and DeepBoW (Q-Synonym) respectively. The main difference between the two methods is for DeepBoW(Q-Synonym) we leverage the synonym-expansion BoW representation to replace the term-weighting BoW representation for the query. To reduce memory usage and compu-tation, we truncate the BoW representation to make it sparse. There are two methods to truncate the sparse BoW representation, one is to keep the k largest words according to their respective values, and the other is to discard the terms whose values are smaller than the giving threshold.\nIn addition, we adopt several state-of-the-art methods for com-parison. BERT [4], ROBERTa [12] and StructBERT [30] belong to the interaction-based architecture which is also known as the cross-encoder architecture. Siamese BERT [9], MASM [32], Poly-encoders [8] and ReprBERT [33] belong to the two-tower architecture which is also known as the bi-encoder architecture. Besides, we investigate the performance of ReprBERT with cosine similarity score instead of MLP for online computation of relevance from query/product embeddings. For fair comparison, we also leverage the pre-trained model like RoBERTa and StructBERT as the encoder of the two-tower model. These models are also baselines in our experiment, which denote as DSSMROBERTa and DSSMStructBERT."}, {"title": "4.4 Evaluation Metrics", "content": "We evaluate our model on both offline and online metrics. In offline evaluation, since the human annotation is binary, the task is evalu-ated as a classification task. The Receiver Operator Characteristic Area Under Curve (ROC-AUC) is widely adopted in text relevance tasks [3, 10, 29]. Note that in the e-commerce relevance scenario, most instances are positive and we are more concerned about neg-ative instances. Therefore the PR-AUC used in this paper is the negative PR-AUC that treats Bad as 1 and Good as 0 following Yao et al. [32, 33]. This metric is denoted as Neg PR-AUC.\nBesides, we also evaluate the different model complexity of pa-rameters and online computation efficiency. The FLOPs / token is computed according to Molchanov et al. [13] which shows the floating-point operations per second (FLOPs) when there is only 1 token being considered. The plus sign separates the online and offline calculation FLOPs, which means the former part of compu-tation can be pre-computed offline. Memory indicates the online memory overhead for storing pre-computed query and product vectors where we use vector size for comparison. In online evalua-tion, we use the rate of Good annotated by human annotators and"}, {"title": "4.5 Results", "content": "shows the parameter, computation and memory consumption of each model. While our model has a considerable num-ber of parameters since we project the dense vector into the vo-cabulary DeepBoW is the most efficient at online inference. Our model only uses CPU to compute the relevance score while the other mod-els need GPU to speed the inference. shows the inference time of each model. The experiments are performed on a local CPU platform. We report the average inference time of the model to score 1000 products per query. The results show that our model is much faster than the ReprBERT [33] which has been deployed in the Taobao search system."}, {"title": "4.6 Ablation Study", "content": "We perform the ablation study on the human-annotation dataset to investigate the influence of different modules in our model. We"}, {"title": "4.7 Online Evaluation", "content": "Online A/B testing is also conducted to evaluate our DeepBoW model, by replacing the online ReprBERT model with the DeepBoW model for comparison. After pre-computing the representations of queries and products, the online serving latency can be optimized to as low as 4ms on the distributed computing system with CPUs. This is much faster than the previous online relevance serving model ReprBERT [33] of 10ms with GPUs and can satisfy the extremely large traffic of Taobao."}, {"title": "5 CONCLUSION AND FUTURE WORK", "content": "In this paper, we study an industrial task of measuring the seman-tic relevance of queries and products. We propose the DeepBoW relevance model, which is an efficient and interpretable relevance architecture for Chinese e-commerce search system. Our model encodes the query and product as a set of word-weight pairs, which is called the sparse BoW representation. The model is evaluted on three different training datasets, and the results show that our model achieves promising performance and efficiency. The model has been deployed in the Taobao search system.\nIn future work, we will explore integrating external knowledge into the DeepBoW relevance model to improve the performance. The proposed model can also be evaluated on datasets of other language."}, {"title": "A CASE STUDY", "content": "shows two examples of our DeepBoW model. Both query and product are encoded to the synonym-expansion BoW repre-sentation. The sparse BoW representation consists of a collection of word-weight pairs, which can be regarded as the bag-of-words with soft weight. The synonym-expansion representation can not only capture the importance of the words in the original text, but also incorporates pertinent synonymous terms. The relevance score can be calculated by aggregating the matching terms of the query's/product's sparse BoW representation.\nThese two examples show that our proposed sparse BoW repre-sentation has positive interpretability, signifying that the developer can analyze bad cases from the online search system and implement targeted optimizations. Furthermore, the developer can modify the terms in the sparse BoW representation directly to achieve the ex-pected result. In a word, our DeepBoW model surpasses other deep relevance modeling approaches in terms of interpretability and flex-ibility, thereby rendering it eminently suitable for the e-commerce search system."}]}