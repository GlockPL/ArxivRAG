{"title": "Analyzing Bias in Swiss Federal Supreme Court Judgments Using Facebook's Holistic Bias Dataset: Implications for Language Model Training", "authors": ["Sabine Wehnert", "Muhammet Erta\u015f", "Ernesto William De Luca"], "abstract": "Natural Language Processing (NLP) is vital for computers to process and respond accurately to human language. However, biases in training data can introduce unfairness, especially in predicting legal judgment. This study focuses on analyzing biases within the Swiss Judgment Prediction Dataset (SJP- Dataset). Our aim is to ensure unbiased factual descriptions essential for fair decision making by NLP models in legal contexts. We analyze the dataset using social bias descriptors from the Holistic Bias dataset and employ advanced NLP techniques, including attention visualization, to explore the impact of \"dispreferred\" descriptors on model predictions. The study identifies biases and examines their influence on model behavior. Challenges include dataset imbalance and token limits affecting model performance.", "sections": [{"title": "1. Introduction", "content": "Natural Language Processing (NLP) is a collection of methods in Artificial Intelligence (AI) for enabling computers to model and respond accurately to human language. However, NLP models can inherit biases from training data, affecting fairness in applications such as legal judgment prediction. The Swiss Judgement Prediction Dataset (SJP Dataset), with 85,274 Swiss court cases, is a significant resource for training models to predict judicial outcomes. Our study analyzes biases within the dataset, particularly in factual case descriptions, aiming to ensure unbiased descriptions for fair decision making by NLP models in legal contexts. Our contributions are:\n\u2022 We analyze the SJP Dataset based on bias descriptors from the Holistic Bias dataset.\n\u2022 We explore how \u201cdispreferred\" descriptors in the dataset influence model predictions.\nIn short, our goal is to identify biases in the SJP dataset and to see if they translate into model behavior, when used as training data. The remainder of this work is organized as follows: In Section 2, we present related work on bias analysis. In Section 3, we describe our method conceptually and evaluate the results in Section 4. We conclude the work in Section 5."}, {"title": "2. Related Work", "content": "In the legal domain, work focuses on the decision making of judges, comparing a theoretically fair judge to an actual one regarding representation bias (e.g., offenses or arrests appear more frequently in a specific social group) and sentencing disparities (i.e., the judgment differs among social groups even if their cases are similar) [1]. Also the decision of a judge can be influenced by gender attitudes, as shown by Ash et al. [2]. Furthermore, Ash et al. investigated whether Indian judges favor defendants who share their gender or religious identity [3]. To the best of our knowledge, there is no related work focusing on bias in Legal Judgment Prediction tasks.\nIn general, Natural Language Processing (NLP) models have the ability to learn stereotypes, misrepresentations, or negative generalizations of certain social groups, genders, religions, and races. Numerous studies show that unrestricted training of natural language models can adopt social biases [4, 5, 6]. Research by Bolukbasi et al. [4] and Islam et al. [5] demonstrates that word embeddings encode social biases related to gender roles and professions, such as associating engineers with men and nurses with women, leading downstream applications to reflect these biases. Sevim et al. also work with word embeddings to identify encoded gender biases in the legal domain [7]. Similarly, Gumusel et al. [8] have analyzed racial or ethnic bias with word2vec embeddings trained on legal data.\nThe use of descriptors to measure social bias started as a method to specifically examine gender associations in static word embeddings [4, 5]. Since contextual word embeddings consider context, templates were necessary to measure social bias, like stereotypical associations with other text content [9]. Various studies have proposed templates to measure bias [10, 9], some selecting sentences from texts and heuristically swapping demographic terms [11, 12] or using machine learning systems for descriptor replacement [13]. The presence of descriptors is not sufficient to create bias in a language model. They need to be significantly associated with one particular label, such that a bias is likely to be learned. Therefore, we [14] employed the binomial significance test for detecting language artifacts - another type of bias - in the datasets of the Competition on Legal Information Extraction/Entailment (COLIEE). While there are benchmark datasets for social bias like SEAT [15], StereoSet [16], and Holistic Bias [17], this study focuses on bias analysis in the legal domain using the Swiss Judgement Prediction (SJP) Dataset. Previously, we worked on this dataset regarding sentiment and subjectivity bias [18]. The SJP Dataset [19] is a dataset based on multilingual court judgment facts, offering annotations on whether a judgment is positive (approval) or negative (dismissal). The Swiss court judgments are written in three languages: approximately 50,000 in German, 31,000 in French, and 4,000 in Italian, with a total of 3/4 of them being dismissed judgments. While previous research has focused on gender bias in datasets and models, this study conducts a bias investigation (e.g., religion, race, nationality) in Legal Judgment Prediction using the Holistic Bias Dataset [17], which covers 13 demographic axes, over 600 descriptors, and 26 templates."}, {"title": "3. Bias Analysis in Swiss Federal Court Judgments", "content": "In this section, the process is explained, from analyzing the SJP Dataset for bias using descriptors, to measuring the impact on model performance and potentially obtained biases."}, {"title": "3.1. Selecting Bias-Descriptors", "content": "For the bias analysis in the facts of court rulings in the Swiss Judgement Prediction Dataset (SJP Dataset) [19], the descriptors labeled as \u201cdispreferred\u201d from the Holistic Bias Dataset [17] are used. The Holistic Bias Dataset includes two versions. The original Version 1.0 contains 620 unique descriptors, 48 of which are labeled (by experts) as \u201cdispreferred\u201d and are distributed across 3 different demographic axes. In contrast, the new Version 1.1 includes 769 unique descriptors, with 70 labeled as \u201cdispreferred\u201d and distributed across 8 different demographic axes, as depicted in Figure 1. The new version is used in this work."}, {"title": "3.2. Dataset Translation", "content": "We automatically translate the descriptors that were originally recorded in English in the Holistic Bias Dataset into German, French, and Italian using the DeepL API\u00b9, as the SJP Dataset's court rulings are written in these languages. After translation, a manual expansion and derivation of synonyms, plural, and gender forms are performed using Google Translate\u00b2. This increases the number of descriptors and allows for a more comprehensive analysis of undesirable terms."}, {"title": "3.3. Preprocessing", "content": "For preprocessing, we employ two approaches to preserve the original text content of the facts from the SJP Dataset while adhering to the 512-token limit of the language model\u00b3 which is be used for judgment prediction.\nExtractive Summarization: The first approach involves using the LexRank Summarizer\nto condense the text content of the facts, ensuring the 512-token limit of the model is not exceeded. LexRank calculates sentence similarity using cosine similarity and creates a graph structure with sentences as nodes and similarities as edges. The PageRank algorithm then"}, {"title": "3.4. Model Fine-Tuning", "content": "We proceed with the fine-tuning of models using the legal-swiss-roberta-large pre- trained model from Niklaus et al. [20], which covers 24 languages and is specifically trained on legal sources. Parameters such as learning rate (2e-5), seed values, batch size (20 per GPU, equals 40 for each training step), and weight decay (0.01) are configured for training on the Swiss Judgement Prediction (SJP) Dataset. Due to the dataset's imbalance (76.23% \u201cdismissal\u201d and 23.77% \"approval\u201d), we adjust class weights to mitigate bias towards the majority class during fine-tuning. Three models per approach are fine-tuned with different seeds and uploaded to the Hugging Face Hub, followed by performance analysis on test data to evaluate precision, recall, accuracy, and F1-score."}, {"title": "3.5. Dataset Analysis with the Binomial Significance Test", "content": "When analyzing \u201cdispreferred\u201d derived descriptors in the facts of the training data, it is important to evaluate if these descriptors are already biased towards a specific class, i.e., \u201cdismissal\u201d or \u201capproval\u201d. We use the Binomial Significance Test (BST) to determine whether the observed frequency of a binary outcome (dismissal=0 and approval=1) significantly deviates from a hypothesized probability distribution. We conduct two separate BSTs: one for the \u201cdismissal\u201d outcome and another for the \"approval\u201d outcome. Below, we explain BST and introduce the key concepts and variables relevant to our dataset."}, {"title": "3.5.1. Overview of the Dataset and Null Hypothesis", "content": "The training dataset consists of 59,709 data points, of which 45,516 correspond to the \"dismissal\" outcome. This yields a relative frequency of dismissal of approximately 45516/59709 \u2248 0.7623. We use this value as our estimate for the probability of a dismissal outcome, \u03c0\u03bf = 0.762, while"}, {"title": "3.5.2. Testing the \u201cDismissal\u201d Outcome for a Specific Token", "content": "To determine whether the \u201cdismissal\u201d outcome for the token \u201cvictime\u201d deviates significantly from the expected behavior under H0, we analyze its observed frequencies (see also Table 5). The token \"victime\u201d appears 3,928 times, with 3,132 occurrences labeled as \u201cdismissal\u201d. Under Ho, the expected number of dismissals is:\nExpected count = 3928\u00b7 \u03c0\u03bf \u2248 2993\nThe observed count (3,132) exceeds the expected count (2,993). To evaluate whether this deviation is statistically significant, we calculate the probability of observing 3,132 or more dismissals under H0, also known as the p-value:\np-value = $\\sum_{k=2993}^{3928} {n \\choose k} \\pi_0^k (1 - \\pi_0)^{k}$ = 8.363595\u00b710-8"}, {"title": "3.5.3. Significance Threshold", "content": "We adopt a significance level (\u03b1) of 0.1, meaning that any event with a p-value smaller than 0.1 is considered improbable enough to reject Ho. Since the computed p-value is much smaller than \u03b1, we reject Ho for the dismissal outcome of the token \u201cvictime\u201d. This result suggests that the observed frequency of dismissals for this token significantly deviates from the expected distribution, indicating a potential bias."}, {"title": "3.5.4. Testing Both Outcomes", "content": "We perform similar tests for each descriptor on both labels dismissal=0 (\u0397\u03bf : \u03c0\u03bf = 0.762) and approval=1 (H\u03bf : \u03c0\u03bf = 0.2377) with their respective counts. A p-value below the significance level suggests that the observed frequencies of dismissal=0 and approval=1 are unlikely under the null hypothesis, indicating a significant deviation. This statistical test compares observed frequencies of descriptors in the data against expected ones, helping identify whether certain de- scriptors may influence predictions in classifications when a model is trained on this potentially biased data."}, {"title": "3.6. Analysis of Language Model Performance", "content": "The results of the performance analysis of models fine-tuned with different seeds and with different preprocessing strategies are listed in the following Tables 1-4. All models exhibit a higher F1-score for the label \u201cdismissal\u201d than for \u201capproval\u201d. During the performance analysis of the model tested with facts divided into chunks, the final classification of a fact is determined based on the majority of its predicted chunk results. In the special case that the predicted chunks are evenly split between two classes, the class \u201cdismissal\u201d is chosen as the final classification result. This decision is made due to the imbalanced labels in the training and test sets of the SJP Dataset, where approximately 80% of the labels correspond to \u201cdismissal\u201d, aiming to achieve higher prediction accuracy for final results. Another challenge encountered during fine-tuning the model with chunked facts was its tendency, even after using class weighting, to favor the majority class. Therefore, the first successfully fine-tuned model that provided balanced predictions was selected for further analysis of predicted facts with descriptors. Using seed 48, balanced classification results were achieved. The results of the chunking strategy are shown in Table 4. Although it is important to obtain good model performance, this is not the main focus of this investigation. Instead, in the evaluation section we show results on the biases picked up by the language models. For this, we compare the correct/wrong predictions to the respective class label and see if a tendency from the dataset bias was adopted by the model. Furthermore, we analyze model attention attribution profiles using the transformers-interpret library. Technically, attention visualization calculates word attributions of a model, showing the impor- tance of each token within a specific context. This process highlights the weighting of words contributing to classification. The process starts by extracting text content from the test data in the SJP-Dataset. The text is tokenized using the model's tokenizer, which includes necessary transformations, such as adding padding tokens and limiting to a maximum length of 512 tokens. Attention visualizations are conducted separately for both summarized and chunked predicted facts with descriptors. Word attribution values range from 1 to -1. A word attribution close to 1 indicates high attention and significant influence on model prediction. Conversely, a word attribution near -1 suggests low attention and minimal impact on model prediction."}, {"title": "4. Evaluation", "content": "To evaluate the bias in the SJP Dataset, we first analyze the dataset and then its impact on the model performance, which can be reproduced via our published code.\n4.1. Bias in the Dataset\nThe Binomial Significance Test was conducted on both, the summarized and the chunked training data. For simplicity, we only show the results on the chunked data, as there are more data points. Nevertheless, the test results of both representations align. The five most frequent descriptors are shown in Table 5 for dismissal and Table 6 for approval. We also added the p_values for both labels, which have been calculated in the Binomial Tests. We further illustrate the results of the Binomial Significance Test in Figure 3."}, {"title": "4.2. Bias in the Language Model", "content": "In our analysis, we examined the 313 summarized facts for descriptors that have their word attributions within the top 20, 50, and 100 tokens. From the results of the descriptors with high word attributions within the TOP 50, we report that the descriptors \u201cbedroht\u201d (threatened), \"Opfer\u201d (victim), \u201cintitul\u00e9\u201d (entitled), \u201cjustifi\u00e9\u201d (justified), and \u201cvictime\u201d (victim) can have a sig- nificant influence on the model's incorrect classification predictions. The descriptors \u201cbedroht', \"Opfer\", \"intitul\u00e9\u201d, and \u201cvictime\u201d were identified as biased towards the class \"dismissal\u201d in the binomial test. Attention visualizations of the incorrect class predictions confirm that facts with these descriptors, which actually belong to the class \u201capproval\u201d, are classified as \u201cdismissal\u201d. For space reasons, illustrations of these results are not included in this work.\nSubsequently, we examined the 508 chunks for descriptors that have their word attribu- tions within the top 10, 20, and 30 tokens. From the results of the descriptors with high word attributions within the TOP 20, it can be seen on Figures 4,5, and 6 that the descriptors"}, {"title": "4.3. Limitations", "content": "One limitation of this study is the potential validity issues arising from translating descriptors from English into other languages. This is particularly concerning in different cultural contexts where certain words considered \u201cdispreferred\u201d in English may not carry the same connotations in Switzerland or other countries with the same languages. This discrepancy could introduce bias and affect the comparability of the results. Another limitation is the use of descriptors without considering the legal context. This could lead to misjudgments, especially in legal cases often involving victim-perpetrator situations. For instance, descriptors related to victims might be assessed neutrally without context. Similarly, the descriptor \"entitled\", translated to \u201cberechtigt\u201d in German, can be perceived neutrally, unlike its potentially negative connotation in English. Ignoring the legal context could bias the results, necessitating critical reflection when interpreting them. Additionally, the chunking process included a decision mechanism that, in case of a tie, favored the majority class \u201cdismissal.\u201d While this method prefers the majority decision, incorporating the classifier's confidence level could provide a more balanced final decision. Moreover, the 512-token limit of the BERT-based model presented a significant challenge. During extractive summarization of facts, this limit led to substantial information loss, as over 50% of the facts considered exceeded 512 tokens. Consequently, the text contents, including the investigated descriptors, were reduced. In the chunking process, facts were divided into chunks of 512 tokens and classified separately, aiming to consider the entire text content without loss. However, this method did not allow for a complete consideration of the entire context of a fact during classification, potentially resulting in missing contextual information and inaccurate or biased classifications."}, {"title": "5. Conclusion", "content": "This work analyzed bias elements in the Swiss Judgement Prediction (SJP) Dataset. Initially, \"dispreferred\" labeled bias descriptors were extracted from the Holistic Bias Dataset. These descriptors were expanded with synonyms, plural, and gender forms, and made available in German, French, and Italian for the SJP Dataset. Two methods, extractive summarization and splitting long text into chunks, were applied to circumvent the 512-token limit of the BERT- based model, allowing the inclusion of descriptors in the analysis. Potential bias sources were identified using the binomial test to analyze the co-occurrence of these derived descriptors with the SJP Dataset labels. The implementation of the BERT-based model was enhanced by fine-tuning and addressing the imbalanced SJP Dataset using custom trainers to adjust class weights. Finally, attention visualization was used to analyze word attributions of the \u201cdispreferred\u201d descriptors in predicted facts and chunks, examining if these descriptors could impact model performance. Instead of finding social bias, we observed language artifacts for the translations of of the descriptor \u201cvictim\u201d, which is neutral in a legal context. Future work could involve using Large Language Models to identify bias of several types independently from descriptors. Also, this work can be extended by training a model specifically to identify bias, using a BERT-based sequence classification model. Employing a pre-trained model with a token limit over 512 tokens would also be beneficial. We plan to deepen our insights by extracting legal parties to understand whether the bias affected winning the legal case."}]}