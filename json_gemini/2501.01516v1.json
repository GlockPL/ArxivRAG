{"title": "Improving Robustness Estimates in Natural Language Explainable AI though Synonymity Weighted Similarity Measures", "authors": ["Christopher Burger"], "abstract": "Explainable AI (XAI) has seen a surge in recent interest with the proliferation of powerful but intractable black-box models. Moreover, \u03a7\u0391\u0399 has come under fire for techniques that may not offer reliable explanations. As many of the methods in XAI are themselves models, adversarial examples have been prominent in the literature surrounding the effectiveness of X\u0391\u0399, with the objective of these examples being to alter the explanation while maintaining the output of the original model. For explanations in natural language, it is natural to use measures found in the domain of information retrieval for use with ranked lists to guide the adversarial XAI process. We show that the standard implementation of these measures are poorly suited for the comparison of explanations in adversarial XAI and amend them by using information that is discarded, the synonymity of perturbed words. This synonymity weighting produces more accurate estimates of the actual weakness of XAI methods to adversarial examples.", "sections": [{"title": "1 Introduction", "content": "Intractably complex models have proliferated in recent years as a result of their efficacy over their simpler relatives. And while efficacy is a necessary component of a good model, it is difficult to trust a black-box and so the question of \u201cHow does this model actually work?\" follows. The lack of answers to this question has slowed the adoption of powerful models in fields where flawed models can have severe consequences, such as medicine (Markus et al., 2021; Tjoa and Guan, 2020).\nThe discipline of Explainable AI (XAI) attempts to answer our question by generating an explanation of how the model generates its output. However, the XAI process that generates the explanation is itself a model, and while by design the XAI model can be inherently understood, we return to the fundamental question of any model \"How effective is it?\" Unfortunately, this question is nearly as difficult as our original query. As the models we attempt to explain are themselves incapable of being (comprehensively) understood, we have no ground truth against which to compare the output of the XAI method. However, we can judge the quality of an XAI method using criteria that avoid an understanding of the original intractable model and instead focus on the consistency of the output of the XAI method when subject to perturbations that retain the meaning of the original input. The property is called stability, or robustness, and is a necessary attribute required to trust an explanation. In general, stability is the intuitive property where insignificant changes to the input lead to only small changes to the output. An XAI process that lacks stability produces substantially different explanations under similar inputs and so is of limited, if any, use.\nTo assess stability, it is necessary to compare the output of the explainability method, and in doing so, some measure is required to provide the comparisons. These similarity measures are the engine that drives the adversarial search process and so suboptimal measures will result in incorrect conclusions of an XAI methods stability. The similarity measures here are subject to two primary flaws. The first is the sensitivity, where two explanations are fundamentally similar, but minor differences are drastically magnified, resulting in effective false positive attack success. The second is the opposite, indifference or coarseness where important differences between the explanations are not captured. Although the measures and metrics in XAI have been directly discussed in previous research (Hoffman et al., 2019), the focus was primarily on evaluating the quality of the explanations rather than the quality of the comparison between the explanations. In particular, the work (Burger et al., 2023) established desiderata in text-based adversarial XAI using common desirable attributes in information retrieval (Kumar and Vassilvitskii, 2010), and used these to motivate an algorithm that mitigates the previously mentioned flaws. However, no prior work on text-based XAI has built purpose-driven measures and instead relied on standard formulations.\nHere, we elect to alter the similarity measure directly using synonymity weighting, where a perturbed word and its original are compared using an inner measure that incorporates their synonymity. Standard measures operate on strict equality between features (words in our case) in the lists being compared. This discards valuable information contained within the structure of the language itself. The removal of this information is not congruent with our focus of maintaining close meaning of the perturbed word, the similarity measure should reflect the end goal of the adversarial process by accounting for the \"closeness\" between the words.\nOur contributions are: (1) The extension of some common measures in information retrieval to use synonymity weighting, focused on providing a more appropriate choice for adversarial methods in XAI. We leverage prior works in element-weighted similarity to allow a more accurate comparison between explanations which provides for a superior understanding of an XAI method's robustness. (2) A comprehensive comparison against adversarial"}, {"title": "2 Background and Related Work", "content": "Prior work on XAI stability has focused on evaluating models using tabular or image data in various interpretation methods, which often use small perturbations of the input data to generate appreciably different explanations (Alvarez-Melis and Jaakkola, 2018; Ghorbani et al., 2019; Alvarez-Melis and Jaakkola, 2018), or generate explanations consisting of arbitrary features (Slack et al., 2020). As our goal is to leverage the idea of synonymity inherent to features of an XAI explanation, we concentrate exclusively on XAI methods that operate on text-based input. Previous work directly involves adversarial perturbations for text-based XAI using a variety of search processes and similarity measures / distance metrics (Sinha et al., 2021; Ivankay et al., 2022; Burger et al., 2023). To simplify further discussion, we refer to any applicable similarity measure or distance metric as simply a similarity measure.\nPopular post hoc \u03a7\u0391\u0399 methods like LIME (Ribeiro et al., 2016) provide a ranked list of features ordered by their importance with respect to the desired model output, and comparisons between ranked lists is a standard task in the field of information retrieval. In particular, previous work has extended commonly used measures for ranked lists to allow weighted similarity of elements (Kumar and Vassilvitskii, 2010; Sculley, 2007). However, it is important to note that the use of these specific measures is not required to apply synonymity-weighted similarity."}, {"title": "2.1 Adversarial Attack Process", "content": "Since the genesis of this work concerns adversarial attacks on XAI methods, we briefly reiterate the general adversarial attack process.\nLet M be a model that can be explained using the XAI method E. Let I be some input to M we desire the explanation to be centered upon, that is we desire to understand how M uses the components of I to produce some output. M(I) is fed into E to generate an initial explanation A used as the basis of comparison against future perturbed versions of I, termed B. The input I is then perturbed by replacing some subset, usually a single word, with an appropriate substitution. This substitution is a close synonym, as judged with respect to a given embedding space, and is subject to certain constraints such as the grammatical category or location in the unperturbed explanation I. The goal here is to generate a replacement while maintaining the fundamental meaning and structure of I despite the constituent words differing. The perturbed explanation B is compared to I using some similarity measure, and the process repeats with B as the new document to be perturbed until some termination condition. We note that prior work in adversarial XAI has enforced a single word to single word replacement for the perturbations due to the extensive reduction in search complexity and thus computation time (which remains a significant bottleneck even with highly greedy search processes). This restriction to a single word replacement scheme imposes an implicit mapping which can then be used to implement synonymity weighted similarity."}, {"title": "3 Mappings Between Explanations", "content": "As our goal is to include the synonymity estimate between paired features a \u2208 A, b \u2208 B where A, B are the original and perturbed explanations, respectively, we require a way to determine the pairing a \u2192 b to allow comparison. To do so, we will define a mapping using the perturbation process to link elements from both lists.\nCreating this mapping is simple if |A| = |B| and the perturbation process is restricted to single-word substitutions. Any element a \u2208 A located at index i in that is perturbed to some value b must either be located at some index j in B or no longer part of the surrogate model and so is missing from explanation B. For the latter case, this generally means eliminated in importance. However, if the search process does not exclude highly ranked features, these can be selected (and often are without careful choice of search constraints), resulting in substantial differences under certain similarity measures despite little substantial difference in meaning. This is one of the major issues that we seek to address with synonymity weighting.\nWe note that most perturbed features from standard text XAI perturbation methods remain present in the perturbed explanation, but when constraining the size of the explanation either through the surrogate model itself, or in the case of truncating the resulting explanation to the top-k features (done usually to reduce explanation complexity for the end-user) we often encounter unpaired elements between the explanations. For measures that rely on consonant lists, an adjustment must be made to allow for dissonant elements. In this case to handle any unpaired elements we apply a penalty value p, its value dependent on both the similarity measure and user choice.\nSome measures can be simply extended to handle disjoint elements. For example, Kendall's Tau (Rank Distance) which counts the number of dissonant pairs between two ranked lists, can be easily extended to the comparison of different sized lists simply by declaring whatever excess element(s) that exist in the larger list to be automatically dissonant. But even for measures whose structure discards element pairings (generally set intersection-based), we must establish a mapping to apply the synonymity weighting. We will assume that all elements contained within the original explanation A are mapped, either to an element in B or the null mapping which indicates that the measure specific penalty should be applied. Additionally, multi-word substitutions for the perturbation method increases the difficulty of maintaining appropriate syntactic consistency and have not seen much use in adversarial explanations in XAI. As such, we will assume that all perturbation methods replace at most one word per iteration.\nNow given our mapping, we must decide what form of synonymity weighting to apply."}, {"title": "4 Constructing Weighted Similarity", "content": "With some mapping established between A and B, how should the synonymity weighting be implemented? We consider two intuitive possibilities, one where the synonymity estimate is within an interval and the second where synonymity is a dichotomy.\nFor the interval estimate, we define a function Syn(a, b) \u2192 [0,1] where a, b are features within explanations A, B respectively. The function Syn() returns a value proportional to the synonymity between the features a and b. We constrain the definition of Syn(a, b) minimally, with the only condition required being Syn(x, x) = 1, where 1 is the absolute maximum similarity possible. In particular, the interval itself is subject to alteration. We choose [0, 1] to provide a simple representation for the proportion of similarity between two words and so make the analysis easier to perform, but this is not required and other choices are possible. Of these alternative choices, the interval [-1,1] may be the most intuitive choice by letting -1 indicate that b is an antonym of a, and 0 being that b is completely unrelated to a.\nFor example, let a = good, b = bad, and c = frog. Now for the interval [0, 1], Syn(a, b) = Syn(a, c) = 0 as neither word is synonymous with good. For the interval [-1,1] Syn(a, b) \u2248 \u22121 and Syn(a, c) = 0 as bad is an antonym of the adjective good. Since part of speech checking is a commonly imposed constraint in text-based adversarial XAI, we assume that there is no ambiguity between words that span multiple syntactic categories such as good (noun) and good (adjective) should the synonymity measure be capable of handling this distinction. In particular, measures that incorporate embedding vectors do not necessarily make a distinction between identical words with multiple meanings (the embeddings are not multisense). These embeddings also will often not produce values with such obvious delineation. Good and frog will likely possess some similarity > 0 by construction of the embedding. Choosing an optimal embedding is likely task specific and outside the scope of this discussion.\nWe can also represent synonymity with a method akin to that of a traditional thesaurus, that is, there is a fixed collection of synonyms for a given word with any other word by definition not being synonymous. For a given word a, define the set of valid synonyms for a as Sa. Then our similarity function Syn(a, b) is the characteristic function \u03c7(Sa(b)) where if b \u2208 Sa then Syn(a,b) = 1, otherwise it is 0. This \u201cthesaurus\u201d method may prove appealing to those who are dubious of the quality of estimates produced by comparison between word embeddings. This strategy is particularly useful for adversarial XAI methods whose search process focuses on perturbing important features first, as the change in similarity between adversarial substitutions can be unreasonably high for methods when large weights are applied to the top subset of features.\nWhile both possibilities for synonymity weighting are reasonable, each comes with some amount of subjectivity. For the interval representation, the continuous-valued output retains some measure of comparison between synonyms, as not all synonyms may be truly identical substitutions. However, the question of how closely related are the words good and great is subject to personal interpretation. Of course, words will generally be represented in terms of some embedding, but different embeddings may have different representations of a word and subsequently different calculations for the resulting similarity. Defining a set of valid synonyms avoids the subjectivity associated with the numerical representation of the synonym, but the question now shifts as to why certain elements are contained within the set. The remainder of the paper uses the interval-based representation that is calculated using embedding vectors and restricted to [0, 1] exclusively as the given method used to construct and test synonymity weighting to simplify the exposition and experimental verification."}, {"title": "4.1 An Example: The Jaccard Index", "content": "Here we demonstrate the idea by constructing weighted similarity using one of the simplest measures for the comparison of lists, the Jaccard Index. The Jaccard index is simply the ratio of the size of the nonempty intersection of the lists (here viewed as sets) to the size of their union (Eq. 1). If the intersection is empty, the resulting similarity is defined to be zero. XAI explanations tend to report only unique features, and so we assume that there are no duplicates within the explanations. However, this is not a strict requirement as the Jaccard index can be extended to multisets.\n$J(A, B) = \\frac{A \\cap B}{A \\cup B}$ (1)\nHere, A is the original explanation and B is the perturbed explanation.\nNow consider the explanations A = {a,b,c} and B = {\u03b1, \u03b2, \u03b3}. We assume that there exists some one-to-one mapping, M(ai) \u2192 bj with i, j the indices of the elements a, b in the explanations A, B, respectively, imposed via the perturbation process from the elements of A to the elements of B as in Section 3. Let this mapping be defined by \u039c(\u03b11) \u2192 (\u03b11), M(b2) \u2192 (\u03b22), and M(c3) \u2192 (3). Then the Jaccard Index gives us:\n$J(A, B) = \\frac{|\\{a, b, c\\} \\cap \\{\u03b1, \u03b2,\u03b3\\}|}{|\\{a, b, c\\} \\cup \\{\u03b1, \u03b2,\u03b3 \\}|} = \\frac{\\emptyset}{\\{a, b, c, \u03b1, \u03b2, \u03b3 \\}} = 0$\nWe can see that there is some level of correspondence between the previous two sets, despite the symbols being different. Both are the first three letters of their respective alphabets, while a and \u03b1 and b and \u03b2 possess added similarity due to their functional equivalence as letters. The Jaccard index cannot capture any similarity by default. To remedy this, we define a similarity measure Syn(x, y) \u2192 [0, 1] that operates on elements x \u2208 A and y \u2208 B. For equally sized lists of elements, we can rewrite Equation 1 as:\n$J(A,B) = \\frac{\\sum_{i=1}^{|A|}|A[i] \\cap M(A[i])|}{|A \\cup B|}$ (2)\nThen, applying our similarity measure Syn(.) we have:\n$JW(A, B) = \\frac{\\sum_{i=1}^{|A|} Syn(A[i], M(A[i]))}{|A \\cup B|}$ (3)\nNow, define Syn(a, a)=0.9, Syn(b, \u03b2)=0.6, and Syn(c, \u03b3)=0.3. Then\n$JW (A, B) = \\frac{0.9 + 0.6 + 0.3}{|A \\cup B|} = 0.3$\nGiven that we have established a mapping between the elements, treating them as disjoint may be inappropriate as there exists some relation between the paired elements. We would then expect the similarity between them to be greater. To adjust for this, we can alter the denominator by the number of each pair of mapped elements. Then we have:\n$JW (A, B) = \\frac{0.9 + 0.6 + 0.3}{|A \\cup B| - 3} = 0.6$\nIn this example, we had both a mapping between every element and a total replacement of every element in the original explanation. In practice, this is unlikely. A complete replacement of each feature within an explanation requires extensive perturbations, usually requiring most of the document to be perturbed. And as the perturbation process is not flawless, this inevitably results in the severe degradation of the textual quality of the document, defeating the purpose of the stability testing."}, {"title": "5 Empirical Validation", "content": "To demonstrate the effectiveness of synonymity weighting, we modify four common similarity measures used for the comparison of collections of features in adversarial XAI. The formulations are defined below in Section 5.1."}, {"title": "5.1 Similarity Measures", "content": "For the following definitions, let A, B denote the ranked lists. If |A| \u2260 |B|, then without loss of generality assume |A| > |B|.\n(1) Jaccard Index which has already been defined and discussed in Section 2.\n(2) Kendall's Tau Rank Distance (Eq. 4) counts the number of pairwise inversions between A and B (where 1[.] is the indicator function. To allow for comparison of lists unequal in size, we assume that all excess elements of the larger list are automatically disjoint.\n$\\sum_{i=1}^{min(|A|,|B|)} (1[A[i] \\neq B[i]]) + ||A| - |B||$ (4)\nWe extend Kendall's Tau to use synonymity weighting by adjusting the value of a mapped dissonant pair a, b at an equal location by multiplying by 1 - Syn(a, b). For highly synonymous replacements (Syn(a, b) \u2192 1) this assigns a distance close to zero for the mapped pair, and for dissimilar words it approaches the default distance.\n$\\sum_{i=1}^{min(|A|,|B|)} (1[A[i] \\neq B[i]] * (1 \u2013 Syn(a, b)) +||A| - |B||$ (5)\n(3) Spearman's footrule (Eq. 6) is the sum of the difference between the location i of each feature a \u2208 A to its corresponding location j in B. Spearman's footrule is effectively the L\u2081 distance between ranked lists. Like Kendall's Tau, the footrule is by default not intended for disjoint lists but can be altered by applying a penalty p for disjoint elements. Here we use the formulation with a location parameter from (Fagin et al., 2002) which is designed for top k lists and choose a penalty value of $\\frac{k}{2}$\n$\\sum_{\u03b1\\\u0395 A}|i - j|$ (6)\nWe extend the footrule to use synonymity weighting by taking the summation of three mutually exclusive conditions that a pair of features within the explanations may have. For features unchanged in both explanations, we calculate the distance as normal. For features a, b with a mapping a \u2192 b we calculate the minimum between the standard distance divided by the synonymity between features Syn(a, b) and the maximum possible distance under the default footrule |A| 1.\n$\\sum_{\u03b1\\\u0395 A\\cap B}|i-j| + \\sum_{\u03b1\\\u0395 A} \\frac{|i-j|}{Syn(a,b)} + \\sum_{b\\\u0395 B} (|A|-1)$ (7)\nWhere s = min)$|j - i|$, $\\frac{(|A|-1)}{Syn(a,b)}$\n(4) Rank-biased Overlap (RBO) is a sum of successively larger intersections, each weighted by a term in a convergent series. This weighting scheme is controlled by a parameter p \u2208 (0,1) that can be adjusted to ascribe more or less weight to the top k features. In general, values further down the list are weighted as less significant, which is often the case in XAI, as only the top few features are of interest to many end users (Verma et al., 2020). RBO (Webber et al., 2010) is defined in Eq. 8 where d is the current depth of the ranking, and k is the maximum depth.\nRBO(A, B, p) = (1 -P) $\\sum_{d=1}^{k} p^{d-1} \\frac{A:d\\cap Bd}{d}$ (8)\nTo enable weighted synonymity we apply the same idea as in Equations 2 and 3 where the size of the intersection is increased by the similarity of each mapped pair of disjoint elements."}, {"title": "5.2 Experimental Data", "content": "To test the effects of synonymity weighting on stability estimates, we require examples generated from an adversarial XAI process. We use the method in (Burger et al., 2023) that easily allows one to replace the similarity measure used to guide the search process. As the similarity measure used for the comparison of explanations controls the search, simply comparing the end result calculated given some success threshold \u03c4 (Figure 1, Table 3) and the average resulting similarity of the perturbed explanation to the original at the end of the attack process. (Figure 2, Table 4). The method used to determine synonymity is cosine similarity on the pre-trained GloVe Twitter embedding."}, {"title": "6 Results & Discussion", "content": "All the following results are calculated exclusively with respect to successful attacks. We provide data for before and after the application of synonymity weighting on the overall success rate of the attack\nJaccard & Spearman: Immediately seen is the drastic reduction in the attack success rate for the Jaccard index and Spearman's footrule. For Jaccard under the GB dataset, we see the three most stringent thresholds (30%, 40%, and 50%) reduced to zero successes while the 60% threshold is reduced to 5%. Similar results hold for the S2D dataset. Spearman is even more sensitive than Jaccard, with every S2D success under the original calculation changed to failure. Spearman's footrule was investigated to determine if the weighting scheme was appropriate due to the enormous success reduction and comparatively more complex weighting formulation over Jaccard. However, the different choices of the penalty value showed little change in the results with only slight (<5%) reductions in calculated similarity. These results demonstrate that both standard measures are extremely sensitive to changes in the explanation that are due to properties of the measure, and not due to a change in fundamental meaning of the explanation. With such significant decreases in attack success, any conclusions generated about XAI stability under these measures should be viewed with suspicion.\nRBO: In contrast to Jaccard and Spearman, RBO gains much less from synonymity weighting, only showing minor changes in success rate for weighting values of 0.5 and 0.7. For weighting value 0.9 it is not surprising to see more efficacy as a more uniform distribution of importance to each feature. There still exist important examples where RBO can profit through the use of synonymity weighting (Table 1), but in general the changes are modest (Table 2). The lack of overall success associated with RBO0.9 makes a firm judgement on its usefulness premature, data with more successful attacks is required. The appreciable difference in success rate change between RBO and Jaccard was not expected as both are intersection-based. RBO's relatively minor gain from synonymity weighting is likely due to its own inherent weighting scheme; to confirm this, the intrinsically weighted version of the other measures can be tested, and we leave this for future investigation. Overall, RBO remains a strong candidate for use in adversarial XAI as its design innately maintains a balance between sensitivity and indifference.\nKendall: Kendall's Tau also appears to gain less from synonymity weighting, with minor changes on S2D and about twice the effectiveness on GB. This conclusion may be premature, as the extreme sensitivity (note the near 100% success rate across every threshold and dataset) may overpower the effects of synonymity weighting. The construction of Kendall's Tau results in minor changes in ordering being shown as large deviations with the application of synonymity weighting in some cases noticeably increasing the resulting similarity (Table 1), but with final values well below most thresholds. The measure itself appears to be poorly suited for adversarial XAI and the synonymity weighting tested here is not sufficient to allow Kendall to see much, if any, practical use."}, {"title": "7 Limitations and Conclusion", "content": "The constructions for the specific synonymity weighting definitions may not be optimal and are intended largely as a demonstration of the concept. Spearman's footrule may benefit from a new method that may prevent the severe change in similarity between the original and weighted versions. Our choice of numerical synonymity is another prime choice for optimization, especially as part of speech checking is not associated with our selection. Superior estimates of synonymity can probably be obtained with alternative methods.\nOverall, this work demonstrates the usefulness of synonymity weighting to allow superior estimates of XAI stability. Substantial reductions in attack success rate for certain measures show the possibility of extremely understating XAI stability without the judicious selection of an appropriate adversarial perturbation method. The application of this weighting scheme also incurs negligible computational overhead as the bottleneck in current algorithms is the explanation generation itself, so even measures who see only moderate impact from synonymity weighting can incorporate it without burden."}]}