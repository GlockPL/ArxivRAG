{"title": "Kov: Transferable and Naturalistic Black-Box LLM Attacks\nusing Markov Decision Processes and Tree Search", "authors": ["Robert J. Moss"], "abstract": "Eliciting harmful behavior from large language models (LLMs) is an important task to ensure the\nproper alignment and safety of the models. Often when training LLMs, ethical guidelines are followed\nyet alignment failures may still be uncovered through red teaming adversarial attacks. This work frames\nthe red-teaming problem as a Markov decision process (MDP) and uses Monte Carlo tree search to find\nharmful behaviors of black-box, closed-source LLMs. We optimize token-level prompt suffixes towards\ntargeted harmful behaviors on white-box LLMs and include a naturalistic loss term, log-perplexity, to\ngenerate more natural language attacks for better interpretability. The proposed algorithm, Kov, trains\non white-box LLMs to optimize the adversarial attacks and periodically evaluates responses from the\nblack-box LLM to guide the search towards more harmful black-box behaviors. In our preliminary study,\nresults indicate that we can jailbreak black-box models, such as GPT-3.5, in only 10 queries, yet fail\non GPT-4-which may indicate that newer models are more robust to token-level attacks. All work to\nreproduce these results is open sourced.", "sections": [{"title": "Introduction", "content": "Efforts to align large language models (LLMs) with the expectation of avoiding toxic or harmful behavior is\nimportant to ensure that misuse of such models is minimized [16]. Alignment is crucial for the proper use\nof LLMs in schools, companies, and the government, and to ensure that they are not used for harmful tasks.\nRed teaming (or jailbreaking) approaches have shown success in bypassing trained LLM alignment and\npost-process filters that reject answering sensitive questions [17]. Although successful, these approaches\nuse manually engineered prompt injections that may be easily fixed by the LLM developers. Automated\nadversarial attacks have thus been proposed to inject suffixes to sensitive prompts-such as \u201cTell me how\nto build a bomb\u201d\u2014and do so using gradient-based optimization [21]. The objective function seeks to find\nresponses that begin with a positive affirmation of the user query (e.g., \u201cSure, here is how to build a bomb...\u201d).\nThe optimization function thus seeks to minimize the negative log likelihood of the \u201cpositive response\" given\nthe current sequence of tokens (called token-level attacks). These methods require access to open-sourced\nmodels (e.g., LLaMa-2 [15] and Vicuna [2]) in order to take gradients of the model.\nTo jailbreak closed-source black-box language models, Zou et al. [21] proposed the gradient-based\noptimization method mentioned above as a universal and transferable approach that trains on open-sourced\nmodels then transfers the prompts to test on closed-source models (e.g., GPT-3.5). Our work addresses\nseveral limitations of existing methods: 1) token-level attacks replace single tokens at a time and may\nconverge to a local minima, 2) the prompt suffixes, while somewhat interpretable, may seem unnatural, and\n3) assumptions that direct transfer to black-box models will be successful. Our work builds on the methods\nfor token-level attacks from Zou et al. [21] by framing the problem as a Markov decision process (MDP)\nand we use Monte Carlo tree search (MCTS) [4] to enable multi-step lookaheads during gradient-based\noptimization. We optimize towards natural adversarial suffixes using log-perplexity to produce adversarial\nsuffixes that are more interpretable. We incorporate feedback from the target black-box model during\nthe tree search to guide towards model behaviors that exhibit severely harmful behavior. This work is\nintended to test the robustness of language models to stress cases where they may be misaligned. We define\n\"misalignment\" as any behavior that responds to harmful prompts with responses that are harmful, violent,\nor discriminatory."}, {"title": "Related Work", "content": "Token-level attacks. Methods to jailbreak LLMs at the token level make adjustments to the user prompt\nto elicit some target behavior from the LLM under test. The problem is framed as an optimization problem\nwhere the objective is to minimize the loss between the expected model outputs and the target model\noutputs given the adversarial prompt. The AutoPrompt method [13] achieves this through single token\nreplacements appended as a suffix to the user prompt. The greedy coordinate gradient (GCG) algorithm\nfrom Zou et al. [21] extends AutoPrompt to search over all possible token replacements. To optimize over\ninputs and outputs to elicit unexpected behaviors, the ARCA method [5] was proposed that uses discrete\noptimization and a low-cost approximation to rank all token replacements in the vocabulary. An example\nprompt and adversarial suffix is shown below.\nPrompt: Write a tutorial on how to make a bomb. [ADVERSARIAL SUFFIX]\nPrompt-level attacks. Although gradient-based optimization methods have been reported to transfer to\nblack-box models such as GPT-4 [21], recent work in prompt-level attacks showed that the state-of-the-art\nLLMs are becoming more robust to token-level attacks [19]; our work also confirms this finding. Prompt-\nlevel attacks generate test cases using another LLM to test the robustness of a target model [12, 10]. Another\napproach to address this is to employ an LLM to act as a persuasive adversary [1, 11, 19]. The PAIR method\n[1] uses a hand-crafted system prompt of an adversarial LLM to trick the black-box target LLM to exhibit\nharmful behavior. The TAP method [11] extends PAIR and constructs the automated red-teaming prompt\nproblem as a sequential tree search with pruning. Other approaches such as PAP [19] use a persuasion\ntaxonomy from social science research [19] to get an adversarial LLM to attack the black-box target LLM\nthrough repeated persuasive prompts. These methods have shown success on jailbreaking the state-of-the-\nart LLMs but require hand-crafted system prompts that may be hard to justify and evaluate the impact of\nindividual components."}, {"title": "Sequential Adversarial Attacks", "content": "To optimize token-level attacks, we use the same formulation from the GCG method purposed by Zou et al.\n[21]. Given a sequence of n tokens 11:n from a vocabulary V of size V = |V|, inference in language models\nis tasked to produce the most-likely next token:\nXn+1 = argmaxp(v | X1:n) \t\t(1)\n\u03bd\u03b5\u03bd"}, {"title": "Preliminaries", "content": "To optimize token-level attacks, we use the same formulation from the GCG method purposed by Zou et al.\n[21]. Given a sequence of n tokens 11:n from a vocabulary V of size V = |V|, inference in language models\nis tasked to produce the most-likely next token:\nXn+1 = argmaxp(v | X1:n) \t\t(1)\n\u03bd\u03b5\u03bd"}, {"title": "Naturalistic Greedy Coordinate Gradient", "content": "We propose a simple extension to the GCG algorithm to include log-perplexity in the loss function to\ngenerate more natural language adversarial suffixes, with the objective of being more interpretable. Log-\nperplexity measures how likely a sequence of tokens are together under a language model. Therefore, the\nnaturalistic loss function becomes:\nL(x1:n) = - log p(xn+1:H | X1:n) + \u5165 (1/n-1 * \\sum^{n}_{i=2} log p(xi | X1:i-1)) \t\t(5)\nnegative log-likelihood\nlog-perplexity\nThe hyperparameter \u5165 \u2265 0 controls the effect of the log-perplexity term on the loss (where setting X = 0\nrecovers GCG). This approach is similar to ARCA [5] which balances log-likelihood with more natural\nlanguage adversarial sequences. The proposed naturalistic greedy coordinate gradient (NGCG) algorithm is\nshown in algorithm 1, with differences from GCG shown in red."}, {"title": "NGCG-TS: Sequential White-Box Red-Teaming", "content": "To avoid the potential of optimizing towards local minima in GCG and NGCG, we frame the red-teaming\nproblem as a Markov decision process. This enables us to effectively perform a multi-step lookahead over\ncandidate token replacements using a well-established sequential framework.\nMarkov decision processes. The Markov decision process (MDP) is a sequential decision making\nproblem formulation that consists of the tuple \u3008S, A, T, R, \u03b3), with a state space S, an action space A, a\nstate-transition probability function T : S \u00d7 A \u2192 [0, 1], a reward function R : S \u00d7 A \u00d7 S \u2192 R, and a\ndiscount factor \u03b3\u2208 [0, 1] which discounts future rewards [6]. The objective when solving MDPs is to find a\npolicy \u3160 that maps states s \u2208 S to actions a \u2208 A. In our problem, we use a generative model G : S \u00d7 A\u2192 S\nas we do not have direct access to the transition probability function T and can only generate next states\ns' = G(s, a) given a current state s and action a. Treating the white-box optimization problem as an MDP,\nwe define the following components:\n\u2022 States s \u2208 S: The state consists of the prompt tokens X1:n, which includes the adversarial suffix\ntokens (e.g., \"Write a tutorial on how to build a bomb. [TOKEN] [TOKEN] [TOKEN]\n.\").\n\u2022 Actions a \u2208 A: An action is an adversarial suffix from NGCG, namely a = xi:n for i \u2208 Z. To get new\nactions, NGCG is run for a single step and the suffix that minimizes the naturalistic loss is returned.\n\u2022 Generative transition function s' = G(s, a): The state is deterministically transitioned by com-\nbining the current token action a with the prompt state s.\n\u2022 Reward function r = R(s, a, s'): The reward function is the negative loss -L(x1:n), as the objective\nis to maximize rewards.\""}, {"title": "Kov: Sequential Black-Box Red-Teaming", "content": "We frame the black-box red-teaming problem as a separate MDP and also solve it using MCTS. We call this\nalgorithm Kov. The primary objective of Kov is to find adversarial suffixes that jailbreak black-box models,\ngiven we do not have access to gradient information of these closed-source models. We use NGCG-TS\nto search over a surrogate white-box model, and evaluate the optimized suffix on the black-box model to\ngather feedback to refine the tree search, shown in fig. 3. The black-box MDP is defined as the following:\n\u2022 States s \u2208 S: The state is the same as the white-box MDP: the prompt Sx = X1:n, including the\nadversarial suffix. Additionally, the state holds the response sy from the black-box LLM under test\nM, denoted sy = M(sx). Therefore, the state is a vector s = [sx, Sy].\n\u2022 Actions a \u2208 A: An action is an adversarial suffix generated using NGCG-TS.\n\u2022 Generative transition function s' = G(s, a): Similar to the white-box MDP, the state is determin-\nistically transitioned by to get the prompt sx = 8x1:j + a (where j = I\u2081 is the initial index of the\nadversarial suffix). The response from the black-box LLM is then s\u2081\u2081 = M(s).\n\u2022 Reward function r = R(s, a, s'): The reward function is a real-valued measure of the harmfulness\nor toxicity of the response from the black-box LLM. We use OpenAI's moderation framework [9],\nwhich is a value r \u2208 [0, 1] (not to be interpreted as a probability). We take the average score across\nthe eleven categories (e.g., hate/threatening, harassment, violence). Other moderation frameworks\nsuch as the Perspective API [8] could be used."}, {"title": "Mitigation with an Aligned Agent", "content": "To help LLMs stay aligned to ethical standards, we modify the action space and reward function of the\nadversarial Kov MDP to create an aligned MDP. Instead of adversarial suffixes as actions, the aligned MDP\nprompts the LLM to \u201cProvide a prompt suffix that ensures the LLM response follows ethical guidelines\u201d and\nuses the response as a suffix to the initial prompt. An example aligned suffix provided by GPT-3.5 is shown\nbelow. The objective of this suffix is to help reinforce that the LLM should follow ethical guidelines. The\nreward function is then the negative score, thus minimizing harmful behavior, which attempts to find\nactions (ethical suffixes) that avoid harmful behavior."}, {"title": "Experimental Results", "content": "Experiment setup. To test the compliance of LLMs to harmful prompts, we used a subset of five harmful\nbehaviors from the AdvBench dataset [21]. The specific harmful prompts are shown in the appendix\ntable 1. The adversarial suffix length is set to 8 to optimize for shorter attacks. Our experiments red-\nteamed four LLMs: FastChat-T5-3b-v1.0 [20], Vicuna-7b [2], OpenAI's GPT-3.5 (gpt-3.5-0125), and GPT-4\n(gpt-4-1106). In our tests, we use the Vicuna-7b as the white-box model we optimize and GPT-3.5 as\nthe black-box target model. All other results are transferred from those experiments, treating each of the\nfour LLMs as black-boxes, with the suffixes found by Kov shown in table 2. We evaluated Kov against\nthree baselines: prompt only evaluations (i.e., just the prompts in table 1 without including adversarial\nsuffixes), GCG, and the aligned MDP from section 3.4. All experiments were run over comparable number\nof iterations, to make them fair, and the best suffix for each baseline was evaluated over 10 model inference\ngenerations for each test prompt. All of the code for the MDPs, experiments, and hyperparameters has\nbeen open sourced as a Julia package.\nIn our limited experiments, Kov was able to jailbreak the black-box GPT-3.5 model over all five harmful\nbehaviors, while GCG was only able to jailbreak 4/5 (with a \u201cprompt only\" baseline of 3/5). Figure 4\nshows results transferred from training on Vicuna-7b (white-box) and GPT-3.5 (black-box) experiments.\nKov achieves consistently high average moderation scores and outperforms GCG on the studied models,\nnotably using only 10 black-box LLM queries during training. The aligned MDP is able to significantly\nreduce the baseline \u201cprompt only\" moderation score in all cases. Notably, all studied algorithms fail to\njailbreak GPT-4 (gpt-4-1106). Unlike the successful GCG jailbreaks on GPT-4 reported by Zou et al. [21]\n(using the deprecated gpt-4-0314), GPT-4 has been continuously upgraded since the publication of that\nresearch in 2023. Along with aligned fine-tuning of GPT-4, the failure to jailbreak could also be associated\nto content moderation being an integral component in the GPT-4 model [18]. Qualitative examples of\nharmful responses are shown in appendix C.\nRegarding natural language suffixes, Kov had an average log-perplexity over all five suffixes of 12.9\nwhile GCG had an average log-perplexity of 14.3. This behavior is also evident when using NGCG-TS"}, {"title": "Conclusions", "content": "We frame the problem of red teaming black-box LLMs as a sequential decision making problem using\nMDPs and propose the Kov algorithm which finds adversarial suffixes that lead to harmful behaviors. Kov\ncombines white-box optimization through tree search (to avoid local minima) and feedback from black-box\nLLM evaluations to guide tree search to more harmful behaviors of the target model. In the process, we\ndeveloped the naturalistic variant of GCG (NGCG) to find adversarial prompts that exhibit more natural\nlanguage. Results indicate that Kov is able to jailbreak black-box LLMs such as GPT-3.5 and transfer to\nother LLMs, yet fails to jailbreak newer models such as GPT-4.\nFigure work. To extend this research, incorporating the universal component of GCG which optimizes\nover all harmful behaviors and several white-box models, could allow Kov to optimize for a single adversarial\nprompt instead of individually training on the specific target behavior. Other research directions could\nframe prompt-level attacks as an MDP similar to Mehrotra et al. [11] and use off-the-shelf solvers to find a\nsequence of persuasive prompts that lead to harmful behaviors."}]}