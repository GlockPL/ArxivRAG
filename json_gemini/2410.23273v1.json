{"title": "Proportional Fairness in\nNon-Centroid Clustering*", "authors": ["Ioannis Caragiannis", "Evi Micha", "Nisarg Shah"], "abstract": "We revisit the recently developed framework of proportionally fair clustering,\nwhere the goal is to provide group fairness guarantees that become stronger\nfor groups of data points (agents) that are large and cohesive. Prior work\napplies this framework to centroid clustering, where the loss of an agent is\nits distance to the centroid assigned to its cluster. We expand the framework\nto non-centroid clustering, where the loss of an agent is a function of the\nother agents in its cluster, by adapting two proportional fairness criteria\n\u2014 the core and its relaxation, fully justified representation (FJR) \u2014 to this\nsetting.\nWe show that the core can be approximated only under structured loss func-\ntions, and even then, the best approximation we are able to establish, using\nan adaptation of the GREEDYCAPTURE algorithm developed for centroid\nclustering [1, 2], is unappealing for a natural loss function. In contrast,\nwe design a new (inefficient) algorithm, GREEDYCOHESIVECLUSTERING,\nwhich achieves the relaxation FJR exactly under arbitrary loss functions,\nand show that the efficient GREEDYCAPTURE algorithm achieves a con-\nstant approximation of FJR. We also design an efficient auditing algorithm,\nwhich estimates the FJR approximation of any given clustering solution\nup to a constant factor. Our experiments on real data suggest that tradi-\ntional clustering algorithms are highly unfair, whereas GREEDYCAPTURE\nis considerably fairer and incurs only a modest loss in common clustering\nobjectives.", "sections": [{"title": "1 Introduction", "content": "Clustering is a fundamental task in unsupervised learning, where the goal is to partition a\nset of n points into k clusters C = (C1, ...,Ck) in such a way that points within the same\ncluster are close to each other (measured by a distance function d) and points in different\nclusters are far from each other. This goal is materialized through a variety of objective\nfunctions, the most popular of which is the k-means objective: $\\sum_{i=1}^k \\sum_{x,y\\in C_i} d(x, y)^2$.\nWhen the points are in a Euclidean space, the k-means objective can be rewritten as\n$\\sum_{i=1}^k \\sum_{x\\in C_i} d(x, \\mu_i)^2$, where $\\mu_i = \\frac{1}{|C_i|} \\sum_{x\\in C_i} x$ is the mean (also called the centroid) of\ncluster Ci. This gives rise to centroid clustering, where deciding where to place the k\ncluster centers is viewed as the task and the clusters are implicitly formed when each point\nis assigned to its nearest cluster center."}, {"title": "1.1 Our Contributions", "content": "In non-centroid clustering, we are given a set N of n points (agents) and the desired number\nof clusters k. The goal is to partition the agents into (at most) k clusters C = (C1, ..., Ck).\nEach agent i has a loss function li, and her loss under clustering C is li(C(i)), where C(i)\ndenotes the cluster containing her. We study both the general case where the loss functions\nof the agents can be arbitrary, and structured cases where the loss of an agent for a cluster\nis the average or maximum of her distances \u2014 according to a given distance metric \u2014 to\nthe agents in the cluster. In the latter case, our theoretical results hold for general metric\nspaces, as they rely solely on the satisfaction of the triangle inequality.\nWe study two proportional fairness guarantees, formally defined in Section 2: the core [11]\nand its relaxation, fully justified representation (FJR) [12]. Both have been studied for\ncentroid clustering [1, 2, 10], but we are the first to study them in non-centroid clustering.\nA summary of a selection of our results is presented in Table 1, with the cell values indicating\napproximation ratios (lower is better, 1 is optimal)."}, {"title": "1.2 Related Work", "content": "In recent years, there has been an active line of research related to fairness in clustering [13].\nWith a few exceptions, most of the work focuses on centroid-based clustering, where each\nagent cares about their distance from the closest cluster center. Mostly related to ours is\nthe work by Chen et al. [1], who introduced the idea of proportionality through the core\nin centroid clustering. Their work has been revisited by Micha and Shah [2] for specific\nmetric spaces. More recently, Aziz et al. [10] also introduced the relaxation of the core, fully\njustified representation, in centroid-based clustering. While one of our main algorithms,\nGREEDYCAPTURE, is a natural adaptation of the main algorithm used in all these works,\nthere are significant differences between the two settings.\nFirst, in centroid-based clustering, GREEDYCAPTURE provides a constant approximation to\nthe core[1], while in the non-centroid case this approximation is not better than O(n/k) for\nthe average loss function. Second, in centroid-based clustering, GREEDYCAPTURE returns\na solution that satisfies FJR exactly [10]. Here, for the non-centroid case, even though we\nknow that an exact FJR solution always exists, GREEDYCAPTURE is shown to just provide\nan approximation better than 4 for the average loss and 2 for the maximum loss. In more\nspecific metric spaces, Micha and Shah [2] show that a solution in the core always exists in\nthe line. Here, we demonstrate that while this remains true for the maximum loss, it is not\nthe case for the average loss, where the core can be empty. Finally, Chen et al. [1] conducted\nexperiments using real data in which k-means++ performs better than GREEDYCAPTURE.\nHowever, for the same datasets, we found that GREEDYCAPTURE significantly outperforms\nk-means++ in the non-centroid setting.\nFairness in non-centroid clustering has received significantly less attention. Ahmadi et al.\n[14] recently introduced a notion of individual stability which indicates that no agent should\nprefer another cluster over the one they have been assigned to. Micha and Shah [2] studied\nthe core when the goal is to create a balanced clustering (i.e. all clusters have almost equal\nsize) and the agents have positive utilities for other agents. More generally, the hedonic\ngames literature (e.g., see [15] for an early survey on the topic and [16] for a recent model\nthat is close to the current paper) is also relevant to non-centroid clustering as it examines\ncoalition formation. While the core concept has been extensively studied in hedonic games,\nthere are two main differences with our work. First, subsets of any size can deviate to form\ntheir own cluster, rather than only proportionally eligible ones, and second, no approximate\nguarantees to the core have been provided, to the best of our knowledge."}, {"title": "2 Model", "content": "For t \u2208 N, let [t] \uc2a5 {1,...,t}. We are given a set N of n agents, and the desired number of\nclusters k. Each agent i \u2208 N has an associated loss function li : 2N \\ 2\\{i} \u2192 R\u22650, where\nli(S) is the cost to agent i for being part of group S. A k-clustering5 C = (C1,..., Ck) is a\npartition of N into k clusters, where Ct \u2229 Ct\u2032 = \u00d8 for t \u2260 t' and $\\bigcup_{t=1}^k C_t = N$. With slight\nabuse of notation, denote by C(i) the cluster that contains agent i. Then, the loss of agent i\nunder this clustering is li(C(i)).\nLoss functions. We study three classes of loss functions; for each class, we seek fairness\nguarantees that hold for any loss functions the agents may have from that class. A distance\nmetric over N is given by d: N \u00d7 N \u2192 R>o, which satisfies: (i) d(i, i) = 0 for all i \u2208 N, (ii)\nd(i, j) = d(j, i) for all i, j \u2208 N, and (iii) d(i, j) \u2264 d(i,k) + d(k, j) for all i, j, k \u2208 N (triangle\ninequality).\n\u2022 Arbitrary losses. In this most general class, the loss li(S) can be an arbitrary\nnon-negative number for each agent i \u2208 N and cluster S\u01ddi.\n\u2022 Average loss. Here, we are given a distance metric d over N, and $l_i(S) = \\mathbb{E}_{j \\in S} d(i, j)$ for each agent i \u2208 N and cluster S\u220bi. Informally, agent i prefers\nthe agents in her cluster to be close to her on average.\n\u2022 Maximum loss. Again, we are given a distance metric d over N, and $l_i(S) = \\max_{j \\in S} d(i, j)$ for each agent i \u2208 N and cluster S\u01ddi. Informally, agent i prefers\nthat no agent in her cluster to be too far from her."}, {"title": "3 Core", "content": "Perhaps the most widely recognized proportional fairness guarantee is the core. Informally,\nan outcome is in the core if no group of agents SCN can choose another (partial) outcome\nthat (i) they are entitled to choose based on their proportion of the whole population (|S|/|N|),\nand (ii) makes every member of group S happier. The core was proposed and widely studied\nin the resource allocation literature from microeconomics [11, 17, 18], and it has been adapted\nrecently to centroid clustering [1, 2]. When forming k clusters out of n agents, a group of\nagents S is deemed worthy of forming a cluster of its own if and only if |S| \u2265 n/k. In centroid\nclustering, such a group can choose any location for its cluster center. In the following\nadaptation to non-centroid clustering, no such consideration is required.\nDefinition 1 (a-Core). For a \u2265 1, a k-clustering C = (C1,...,Ck) is said to be in the\na-core if there is no group of agents SCN with |S| \u2265 n/k such that $a l_i(S) <l_i(C(i))$ for\nall i \u2208 S. We refer to the 1-core simply as the core.\nGiven a clustering C, if there exists a group S that demonstrates a violation of the a-core\nguarantee, i.e., S has size at least n/k and the loss of each i \u2208 S for S is lower than 1/a\nof her own loss under C, we say that S deviates under C and refer to it as the deviating\ncoalition. We begin by proving a simple result that no finite approximation of the core can\nbe guaranteed for arbitrary losses.\nTheorem 1. For arbitrary losses, there exists an instance in which no a-core clustering\nexists for any finite a.\nProof. Consider an instance with a set of n = 4 agents {0,1,2,3} and k = 2. Note that any\ngroup of at least 2 agents deserves to form a cluster. For i \u2208 {0,1,2}, the loss function of\nagent i is given by\n$\\l_i(S) =\\begin{cases}\n1 & \\text{if } S = \\{0,1,2\\} \\text{ or } 3 \\in S, \\\\\n0 & \\text{if } S = 2 \\text{ and } i + 1 \\mod 3 \\notin S, \\\\\n0 & \\text{if } S = \\{i, i + 1 \\mod 3\\}.\n\\end{cases}$"}, {"title": "Theorem 2. For the average loss, there exists an instance in which no a-core clustering\nexists for $\\alpha < \\frac{1+\\sqrt{3}}{2} \\approx 1.366$.", "content": "Proof. Let us construct an instance with an even number k\u2265 2 of clusters. Let $\\epsilon = \\frac{1+\\sqrt{3}}{2} - \\alpha$.\nWe set the number of agents to be a multiple of k such that n \u2265 k \u00b7 max {$\\frac{2}{e} +1,402$}. Our\nconstruction has k/2+1 areas, each consisting of a few locations (points), with several agents\nplaced on each of them. In particular, area 0 has a single location Mo with k/2 agents. For\ni = 1, 2, ..., k/2, area i consists of location Mi hosting a single agent, a left location Li and a\nright location Ri each hosting n/k - 1 agents. We use Li, Ri, and Mi to denote both the\ncorresponding points as well as the set of agents located in them. For i = 1, 2, ..., k/2, the\ndistance between points Li and Ri is 1 while both points are at distance $\\frac{n}{2k\\alpha}$ from point Mi.\nThe distance between any two points in different areas is infinite.\nConsider a k-clustering C of the agents. We call bad any cluster of C that contains agents\nfrom different areas; notice that all points in such a cluster have infinite cost. A good cluster\nhas all its points in the same area and, hence, all the agents contained in it have finite cost.\nNotice that C has at most k \u2212 1 good clusters that contain points from areas 1, 2, ..., k/2.\nAmong these areas, let t be the one with the minimum number of good clusters. Thus, area\nt either has all its agents in bad clusters or contains one good cluster that includes some of\nits agents. If at least n/k of its agents belong to bad clusters in C, a deviating coalition of\nthem would improve their cost from infinite to finite. So, in the following, we assume that\nclustering C contains exactly one good cluster with at least n/k agents from area t.\nWe distinguish between three cases. The first one is when the good cluster does not contain\nthe agent in Mt. Among Rt and Lt, assume that Lt has at most as many agents in the good\ncluster as Rt (the other subcase is symmetric). Then, the cost of all agents of Lt in the good\ncluster is at least 1/2. The deviating coalition consisting of all agents in Lt and the agent\nof Mt (i.e., n/k agents in total) improves the cost of all agents by a multiplicative factor at\nleast a. Indeed, the cost of the agent in Mt improves from infinite to finite while the cost of\nany agent in Lt improves from at least 1/2 to $\\frac{n}{2k\\alpha}$, since any such agent has distance $\\frac{n}{2k\\alpha}$ to\nthe agent in Mt, and is colocated with the other agents in the deviating coalition.\nThe second case is when the good cluster contains all agents in area t. In this case, the cost of\nthe agents in Lt and Rt is $\\frac{2n/k - 2 \\left( \\frac{2n}{k} - 1 \\right) + \\frac{n}{k} - 1}{n/k} \\geq \\frac{40}{\\frac{2}{e} + 1} > \\frac{3}{5}$ (the second inequality\nfollows by the definition of n). Then, each agent in the deviating coalition containing"}, {"title": "ALGORITHM 1: GREEDYCOHESIVECLUSTERING (A)", "content": "Input: Set of agents N, metric d, number of clusters k\nOutput: k-clustering C = (C1,...Ck)\nN' \u2190 N;\nj\u2190 1;\nwhile N' \u2260 0 do\nCj\u2190 A(N', d, \u2308n/k\u2309);\nN' \u2190 N' \\ Cj;\nj \u2190 j + 1;\nend\nCj, Cj+1,..., Ck \u2190 \u00d8;\nreturn C = (C1,..., Ck);"}, {"title": "ALGORITHM 2: SMALLESTAGENTBALL", "content": "Input: Subset of agents N' \u2286 N, metric d, threshold integer \u03c4\nOutput: Cluster S\nif |N'|\u2264\u03c4 then return S \u2190 N';\nfor i \u2208 N' do\nli\u2190 \u03c4-th closest agent in N' to agent i;\nS \u2190 B(i, li); // Smallest ball centered at agent i capturing at least \u03c4 agents\nend\ni*\u2190 arg mini\u2208N' ri;\nreturn S\u2190 the set of closest agents in N' to agent i*;"}, {"title": "and $\\frac{n}{2}$ agents from Rt improves their cost to $\\frac{1}{2\\epsilon}$, i.e., by a factor of at least\n$\\frac{1}{2\\alpha} + 1 - \\epsilon$.", "content": "The third case is when the good cluster contains the agent in Mt but does not contain some\nagent i from Lt or Rt. We will assume that agent i belongs to Lt (the other subcase is\nsymmetric). Notice that the cost of the agents in R\u2081 is at least $\\frac{2 \\frac{n}{k} - 2}{n/k} \\geq \\frac{10}{4 \\frac{2}{e} + 1} > \\frac{3}{5}$. To see why,\nnotice that the claim is trivial for those agents of Rt that belong to bad clusters while each\nof the agents of Rt in the good cluster is at distance $\\frac{n}{\\alpha}$ to the agent in Mt and there are\nat most $2 \\frac{n}{k} - 2$ in the cluster. The deviating coalition of all agents in Rt together with i\ndecreases their cost to just $\\frac{k}{n}$, i.e., by a factor of at least $\\frac{4 \\alpha n}{5 k} \\geq \\alpha$ (the inequality follows\nby the definition of n), while the cost of agent i improves from infinite to finite.\nSo, there is always a deviating coalition of at least n/k agents with each of them improving\ntheir cost by a multiplicative factor of min {$\\alpha, \\frac{1}{2 \\alpha} + 1 - \\epsilon$ } = $\\alpha$, as desired. The last equality\nfollows by the definition of a and \u025b."}, {"title": "Theorem 3. For the average (resp., maximum) loss, the GREEDYCAPTURE algorithm is\nguaranteed to return a clustering in the (2\u00b7 \u2308n/k\u2309 \u2212 3)-core (resp., 2-core) in O(kn) time\ncomplexity, and these bounds are (almost) tight.", "content": "Proof. Let C = {C1, ... Ck} be the k-clustering returned by GREEDYCAPTURE. Let S C N\nbe any set of at least n/k agents such that their average loss satisfies\n$l_i(C(i)) > (2 \\cdot \\lceil n/k \\rceil - 3) \\cdot l_i(S),$\nfor every i \u2208 S.\nLet i* be the agent that was the first among the agents in S that was included in some\ncluster by the algorithm. Consider the time step before this happens and let i' \u2208 C(i*) be\nthe agent that had the minimum distance R from the \u2308n/k\u2309-th agent in C(i*) among all\nagents that had not been included to clusters by the algorithm before. Then,\n$l_{i^*} (C(i^*)) = \\frac{1}{|C(i^*)|} \\sum_{i\\in C(i^*)} d(i^*, i) \\\\\n< \\frac{1}{|C(i^*)|} \\Big( d(i^*, i') + \\sum_{i\\in C(i^*)\\{i', i^*\\}} \\big( d(i^*, i') + d(i', i) \\big) \\Big) \\\\\n\\leq \\Big( 2 - \\frac{3}{\\lceil n/k \\rceil} \\Big) R \\\\\n< 2 \\cdot R.$\nThe first inequality follows by applying the triangle inequality. The second inequality follows\nsince C(i*) has \u2308n/k\u2309 agents and, thus, the RHS has 2\u2308n/k\u2309 \u2212 3 terms representing distances\nof agents in C(i*) from agent i', each bounded by R.\nNow, recall that, at the time step the algorithm includes cluster C(i*) in the clustering, none\namong the (at least \u2308n/k\u2309) agents of S have been included in any clusters. Then, S contains\nat most \u2308n/k\u2309 \u2212 1 agents located at distance less than R from agent i*; if this were not the\ncase, the algorithm would have included agent i* together with \u2308n/k\u2309 \u2212 1 other agents of S\nin a cluster instead of the agents in C(i*). Thus, S contains at least |S| \u2212 \u2308n/k\u2309 + 1 agents\nat distance at least R from agent i*. Thus,\n$l_{i^*} (S) = \\frac{1}{|S|} \\sum_{i\\in S} d(i^*, i) \\geq \\frac{|S| - \\lceil n/k \\rceil + 1}{|S|} R > \\frac{\\lceil n/k \\rceil}{|S|} R.$\nIn centroid clustering, additional agents captured later on do not change the loss of the initial\n\u2308n/k\u2309 agents captured as loss is defined by the distance to the cluster center, which does not change.\nHowever, in non-centroid clustering, additional agents can change the loss of the initially captured\nagents, even from zero to positive, causing infinite core approximation when these agents deviate."}, {"title": "Now, assume that there exists a set S C N of at least n/k agents such that their maximum\nloss satisfies", "content": "$l_i(C(i)) > 2 \\cdot l_i(S),$\nfor every i \u2208 S. Again, let i* be the agent that was the first among the agents in S that\nwas included in some cluster by the algorithm. Consider the time step before this happens\nand let i' \u2208 C(i*) be the agent that had the minimum distance R from the \u2308n/k\u2309-th agent\nin C(i*) among all agents that had not been included to clusters by the algorithm before.\nThen, the maximum loss of agent i* for cluster C(i*) is\n$l_{i^*} (C(i^*)) = \\max_{i\\in C(i^*)} d(i^*, i) \\leq \\max_{i\\in C(i^*)} (d(i^*, i') + d(i', i)) \\leq 2 \\cdot R.$\nThe first inequality follows by applying the triangle inequality and the second one since all\nagents in C(i*) are at distance at most R from agent i'. We also have\n$l_{i^*} (S) = \\max_{i\\in S} d(i^*, i) \\geq R,$\notherwise, the algorithm would include a subset of \u2308n/k\u2309 agents from set S in the clustering\ninstead of C(i*). Together, Equation (5) and Equation (6) contradict Equation (4). This\ncompletes the proof of the upper bounds.\nWe now show that the analysis is tight for both the average and the maximum loss functions\nusing the instance depicted in Figure 1 with one agent at locations A, D, and E, two agents\nat location B, n/2 - 3 agents at location C, and n/2 - 2 agents at location F. Suppose\nthat k = 2. It is easy to see that GREEDYCAPTURE returns a 2-clustering with the agents\nlocated at points A, B, and C in one cluster and the agents located at points D, E, and F\nin another. Notice that the agents at locations B and C have infinite loss under both loss\nfunctions, while the agent located at position D has maximum loss 2(1 \u2013 \u03b5) and average\nloss $\\frac{(n-3)(1-\\epsilon)}{n/2}$. Now, consider the deviating coalition of the n/2 agents at locations B, C,\nand D. The agents at B and C improve their loss from infinite to finite, while the agent\nlocated at C improves her maximum loss to 1 + \u025b and her average loss to $\\frac{2+(n/2-1)}{n/2}$, for\nmultiplicative improvements approaching 2 and n/2 - 3/2 as e approaching 0.\nSince GREEDY CAPTURE calls SMALLEST AGENTBALL at most k times and SMALLESTA-\nGENTBALL does at most n iterations in each call, we easily see that the time complexity of\nGREEDYCAPTURE is O(kn)."}, {"title": "4 Fully Justified Representation", "content": "Peters et al. [12] introduced fully justified representation (FJR) as a relaxation of the core in\nthe context of approval-based committee selection. The following definition is its adaptation"}, {"title": "Definition 2 (a-Fully Justified Representation (a-FJR)).", "content": "For a \u2265 1, a k-clustering C =\n(C1,..., Ck) satisfies a-fully justified representation (\u03b1-FJR) if there is no group of agents\nSCN with |S| \u2265 n/k such that $al_i(S) < \\min_{j \\in S} l_j(C(j))$ for each i \u2208 S, i.e., if\n$a \\cdot \\max_{i\\in S} l_i(S) < \\min_{j \\in S} l_j(C(j))$. We refer to 1-FJ\u0158 simply as FJR.\nWe easily see that a-FJR is a relaxation of a-core.\nProposition 1. For a \u2265 1, a-core implies a-FJR for arbitrary loss functions.\nProof. Suppose that a clustering C is in the a-core. Thus, for every S \u2264 N with |S| \u2265 n/k,\nthere exists i \u2208 S for which $a \\cdot l_i(S) \\geq l_i(C(i)) \\geq \\min_{j \\in S} l_j(C(j))$, so the clustering is also\na-FJR."}, {"title": "4.1 Arbitrary Loss Functions", "content": "We prove that an (exactly) FJR clustering is guaranteed to exist, even for arbitrary losses.\nFor this, we need to define the following computational problem.\nDefinition 3 (MOST COHESIVE CLUSTER). Given a set of agents N and a threshold 7, the\nMOST COHESIVE CLUSTER problem asks to find a cluster SCN of size at least 7 such that\nthe maximum loss of any i \u2208 S for S is minimized, i.e., find arg $\\min_{S \\subseteq N':|S| > +} \\max_{i\\in S} l_i(S)$.\nFor > \u2265 1, a \u03bb-approximate solution S satisfies $\\max_{i\\in S} l_i(S) \\leq \u03bb \\cdot \\max_{i\\in S'} l_i(S')$ for all\nS'CN with |S'| \u2265 \u03c4, and a \u03bb-approximation algorithm returns a \u03bb-approximate solution\non every instance.\nWe show that plugging in a \u03bb-approximation algorithm A to the MOST COHESIVE CLUSTER\nproblem into the GREEDYCOHESIVECLUSTERING algorithm designed in the previous section\nyields a A-FJR clustering. In order to work with arbitrary losses, we need to consider a\nslightly generalized GREEDYCOHESIVECLUSTERING algorithm, which takes the loss functions\nli as input instead of a metric d, and passes these loss functions to algorithm A.\nTheorem 4. For arbitrary losses, a \u2265 1, and an a-approximation algorithm A for the\nMOST COHESIVE CLUSTER problem, GREEDYCOHESIVECLUSTERING(A) is guaranteed to\nreturn a a-FJR clustering. Hence, an (exactly) FJR clustering is guaranteed to exist.\nProof. Suppose for contradiction that the k-clustering C = {C1,... Ck } returned by GREEDY-\nCOHESIVECLUSTERING(A) on an instance is not a-FJR. Then, there exists a group SC N\nwith |S| \u2265 n/k such that $a \\max_{i \\in S} l_i(S) < \\min_{i \\in S} l_i(C(i))$. Let i* be the first agent\nin S that was assigned to a cluster during the execution of GREEDYCOHESIVECLUSTER-\nING, by calling A on a subset of agents N'. Note that S \u2286 N'. Then, we have that\n$\\max_{i\\in C(i^*)} l_i(C(i^*)) \\geq l_{i^*}(C(i^*)) > a \\cdot \\max_{i\\in S} l_i(S)$, which contradicts A being an a-\napproximation algorithm for the MOST COHESIVE CLUSTER problem. Hence, GREEDYCO-\nHESIVECLUSTERING(A) must return an a-FJR clustering.\nUsing an exact algorithm A for the MOST COHESIVE CLUSTER problem (e.g., the inefficient\nbrute-force algorithm), we get that a 1-FJR clustering is guaranteed to exist."}, {"title": "4.2 Average and Maximum Loss Functions", "content": "Let A* be an exact algorithm for the MOST COHESIVE CLUSTER problem for the average\n(resp., maximum) loss. First, we notice that we cannot expect it to run in polynomial time,\neven for these structured loss functions. This is because it can be used to detect whether a"}, {"title": "Theorem 5. For A \u2265 1, if A is a A-approximation algorithm to the MOST COHE-\nSIVE CLUSTER problem, then AUDITFJR(A) is a A-approximate FJR auditing algo-", "content": "rithm. Given Lemma 1, it follows that for the average (resp., maximum) loss, AUDIT-\nFJR(SMALLESTAGENTBALL) is an efficient 4-approximate (resp., 2-approximate) FJR\nauditing algorithm.\nProof. Suppose A is a \u03bb-approximation algorithm for the MOST COHESIVE CLUSTER\nproblem. Consider any clustering Con which AUDITFJR(A) returns \u03b8. Let p =\n$\\frac{\\min_{i\\in S} l_i(C(i))}{\\max_{i\\in S} l_i(S)}$ be the exact FJR approximation of C. First, it is easy\nto check that p\u2265 \u03b8 because \u03b8 is computed by taking the maximum of the same expression as\npis, but over only some (instead of all possible) S. Hence, it remains to prove that p < \u03bb\u00b7\u03b8.\nConsider any group S \u2264 N with |S| \u2265 n/k. Let i* be the first agent in S that was removed\nby AUDITFJR, say when A returned a group S' containing it; there must be one such agent\nbecause |S| \u2265 n/k and when AUDITFJR stops, fewer than n/k agents remain in N'. Now, we\nhave that\n$\\frac{\\min_{i\\in S} l_i(C(i))}{\\max_{i\\in S} l_i(S)} = \\frac{l_{i^*}(C(i^*))}{\\max_{i\\in S} l_i(S)} \\leq \u03bb \\cdot \\frac{l_{i^*}(C(i^*))}{\\max_{i\\in S'} l_i(S')} = \u03bb \\cdot \\frac{\\min_{i\\in S'} l_i(C(i))}{\\max_{i\\in S'} l_i(S')} < \u03bb \\cdot \u03b8,$\nwhere the second inequality holds because A is a \u03bb-approximation algorithm for the MOST\nCOHESIVE CLUSTER problem, which implies $\\max_{i \\in S'} l_i(S') \\leq \u03bb \\cdot \\max_{i \\in S} l_i(S)$; the next\nequality holds because agent i* was selected for removal when S' was returned, which implies\ni* \u2208 arg $\\min_{i\\in S'} l_i(C(i))$; and the final inequality holds because \u03b8 is updated to be the\nmaximum of all FJR violations witnessed by the algorithm, and violation due to S' is one of\nthem.\nFinally, using the approximation ratio bound of SMALLESTAGENTBALL for the MOST\nCOHESIVE CLUSTER problem from Lemma 1, we obtain the desired approximate auditing\nguarantee of AUDITFJR(SMALLESTAGENTBALL)."}, {"title": "Theorem 6. Assuming P\u2260 NP, there does not exist a polynomial-time A-approximate FJR\nauditing algorithm for the maximum loss, for any > < 2.", "content": "Proof. We show that such an algorithm can be used to solve the CLIQUE problem, which\nasks whether a given undirected graph G = (V, E) admits a clique of size at least t. The\nproblem remains hard with t \u2265 3, so we can assume this without loss of generality. Given\n(G, t), we first modify G = (V, E) into G' = (V', E') as follows. To each v \u2208 V, we attach\nt-2 new (dummy) nodes, and to one of those dummy nodes, we attach yet another dummy\nnode. In total, for each v \u2208 V, we are adding t \u2212 1 dummy nodes, so the final number of\nnodes is |V'| = |V| \u00b7 t.\nNext, we create an instance of non-centroid clustering with n = |V'| agents, one for each\nv \u2208 V'. The distance d(u, v) is set as the length of the shortest path between u and v. Set\nk = |V|.\nConsider a clustering C in which each real node v \u2208 V is put into a separate cluster, along\nwith the t-1 dummy nodes created for it. Note that lv (C(v)) = 2 for each real node v \u2208 V\n(due to the dummy node attached to a dummy node attached to v) and $l_v(C(v)) \\in \\{2,3\\}$\nfor each dummy node v \u2208 V' \\ V. Let us now consider possible deviating coalitions S."}, {"title": "Appendix", "content": "A Bicriteria Approximation of the Core\nHere, we consider a more general definition of the core.\nDefinition 5 ((\u03b1, \u03b4)-Core). For a \u2265 1, a k-clustering C = (C1, ..."}]}