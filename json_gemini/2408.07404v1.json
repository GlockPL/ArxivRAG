{"title": "Efficient Edge AI: Deploying Convolutional Neural Networks on FPGA with the Gemmini Accelerator", "authors": ["Federico Nicol\u00e1s Peccia", "Svetlana Pavlitska", "Tobias Fleck", "Oliver Bringmann"], "abstract": "The growing concerns regarding energy consumption and privacy have prompted the development of AI solutions deployable on the edge, circumventing the substantial CO2 emissions associated with cloud servers and mitigating risks related to sharing sensitive data. But deploying Convolutional Neural Networks (CNNs) on non-off-the-shelf edge devices remains a complex and labor-intensive task. In this paper, we present and end-to-end workflow for deployment of CNNs on Field Programmable Gate Arrays (FPGAs) using the Gemmini accelerator, which we modified for efficient implementation on FPGAs. We describe how we leverage the use of open source software on each optimization step of the deployment process, the customizations we added to them and its impact on the final system's performance. We were able to achieve real-time performance by deploying a YOLOv7 model on a Xilinx ZCU102 FPGA with an energy efficiency of 36.5 GOP/s/W. Our FPGA-based solution demonstrates superior power efficiency compared with other embedded hardware devices, and even outperforms other FPGA reference implementations. Finally, we present how this kind of solution can be integrated into a wider system, by testing our proposed platform in a traffic monitoring scenario.", "sections": [{"title": "I. INTRODUCTION", "content": "Nowadays, several pre-trained Convolutional Neural Networks (CNN) already exist that cover common tasks like object detection or tracking, so the burden of training the models from scratch can be avoided. A lot of commercial edge devices (for example, the Jetson family of NVIDIA) provide software frameworks which take care of optimizing these models for deployment on them.\nBut taking these pre-trained models and deploying them on an edge device that is not an off-the-shelf hardware (for example, a custom hardware accelerator), can be a very manual and time consuming task. This task also requires a wide set of skills to be able to extract as much performance as possible of the proposed hardware-software system, ranging from knowledge about CNN optimization techniques to understanding the internals of the desired hardware architecture. Moreover, the final system cannot exist in isolation, so standard interfaces need to be provided to easily obtain the data to be processed and to publish the output of the CNN to the next system in the processing pipeline, be it another edge hardware or a cloud server.\nIn this paper, we aim to simplify these deployment activities by introducing an end-to-end workflow based on FPGA for CNN deployment on the edge using the Gemmini accelerator [1], where each of these aspects is taken into account. We describe each step of the deployment workflow (including hardware-aware model modifications, quantization, layer scheduling optimizations, model partitioning and more) and its impact on the resulting system performance.\nWe were able to achieve real time performance by deploying the YOLOv7 model [2] on a Gemmini accelerator [1] implemented on a Xilinx ZCU102 FPGA development board. We expanded the Gemmini integration for the TVM Deep Learning compiler presented in [3], and optimized the execution of the CNN using the AutoTVM framework [4]. We also compare the resulting energy efficiency of the FPGA-based solution against reference implementations (including server-size and embedded GPUs), and against other FPGA accelerators, and extract conclusions from it. We also provide an example use case to show how this kind of solution can be integrated into a wider system.\nThe rest of this paper is organized as follows. First, Section II presents related works deploying CNN on FPGAs using a variety of frameworks. Then, Section III describes the selected hardware accelerator, and the optimizations implemented for it which improve its mapping onto an FPGA. Section IV describes the end-to-end software workflow, starting with the pretrained CNN model and explaining each optimization step. Then, Section V compares the proposed solution against reference implementations. Section VI presents a case study on how the proposed workflow can be used to integrate a CNN into a wider edge system. Finally, Section VII concludes our work."}, {"title": "II. RELATED WORK", "content": "The automation frameworks to map CNNs on FPGAs can be separated in two distinct groups [5], [6]. Several works have focused on implementing stream or dataflow type accelerators, where each layer is mapped as a separate hardware module on the FPGA and are interconnected in a stream-like manner. These works are able to achieve high throughputs, but the sizes of the CNNs that can be implemented are limited by the available FPGA resources. HLS4ML [7] translates a high level description of a Neural Network into High Level Synthesis language (HLS), and uses this new description to implement the needed hardware modules on the FPGA. FINN [8] is a Xilinx tool that generates a complete dataflow type accelerator that is tailor made for a specific Quantized Neural Network (QNN). FlexCNN [9] also uses HLS, but generates systolic array based accelerators. DnnWeaver [10] uses hand-optimized hardware templates to create the FPGA accelerator. fpgaConvNet [11] provides a tool to generate throughput-oriented HLS for a given model, taking into account the resource constraints of the desired FPGA.\nBut a stream type accelerator is no longer suitable when a more computation or memory intensive CNN needs to be implemented, because the design can very quickly exceed the available resources of the selected FPGA platform. This is where overlay or single-engine type accelerators present several advantages. These kind of accelerators contain hardware modules which execute typical operations needed by CNNs. The accelerators are independent of the selected CNN, so they can be parametrized to fit the available FPGA first, and then a software framework maps the execution of the different layers of the CNN onto the accelerator. Angel-Eye [12] presented a CNN accelerator together with a compilation workflow to map a generic CNN onto it. VTA [13] provides an accelerator integrated into TVM for fast deployment and tuning of CNN onto Xilinx FPGAs. DYNAMAP [14] takes into account the heterogeneity of layers on a CNN when selecting the right hardware template for the accelerator. Vitis AI [15] is a commercially available tool to optimize CNN for deployment on Xilinx Deep Processing Units (DPU).\nA software integration to deploy CNNs onto the Gemmini accelerator already exists [16]. This uses the ONNX Runtime to parse layers and offload them to the accelerator. But this approach is severely limited. First of all, in order to use it, the ONNX Runtime needs to be compiled for the embedded system, which in certain cases may be a limitation, specially in constrained bare metal scenarios. Second, the current integration is limited only to convolutions and dense layers. Finally, it completely ignores the possibility of exploring the schedule space for each layer, which can greatly improve the performance of the model by changing the order in which the individual instructions are dispatched to the accelerator."}, {"title": "III. THE GEMMINI ACCELERATOR", "content": "When working with single-engine accelerators, it is important to take into account which features are supported by it, and if these are enough to accelerate the desired CNN. In this work, the selected hardware accelerator is Gemmini [1], a highly customizable systolic array based accelerator. Because of several factors, including (but not limited to) documentation, flexibility and development status, Gemmini provides an excellent platform for these kinds of projects. It also presents low jitter and latency when compared with other open source FPGA accelerators [17].\nThe accelerator consists of three decoupled modules: the Load controller (which moves chunks of data from the external memory to the internal accelerator scratchpads), Execute controller (which dispatches data already available in the scratchpads to the systolic array) and Store controller (which moves chunks of data from the accelerator's internal scratchpads to the external memory system).\nGemmini needs to be attached to a RocketCore CPU [18], which orchestrates the execution of the CNN, by offloading layers on the accelerator. The CPU may issue two types of instructions to the accelerator: 1) CISC-type instructions, which use hardcoded state machines inside the accelerator to execute typical operations like tiled matrix multiplications and convolutions, abstracting the developer of the internal operation of the accelerator, and 2) RISC-type instructions, which give the developer a far more fine-grained control over the working of the accelerator, by providing intrinsic instructions to move data in or out of the accelerator, and execute matrix multiplications using the data available in Gemmini's internal scratchpads or accumulator.\nAlthough a tape out of the Gemmini accelerator can be realized (see [1]), deploying it onto an FPGA provides an interesting cost-benefit tradeoff. It provides a flexible middle ground where hardware optimizations can be explored, while still providing an energy efficient platform. We selected FPGAs based on Xilinx's Zynq SoC architecture, as these also contain ARM Cores (Processing System, PS) alongside the proper FPGA (Processing Logic, PL). This heterogeneous SoC architecture can be utilized to optimize the model's performance by distributing the execution of the network across both parts of the SoC (see Section IV-D). The FPGA bitstream was realized using a similar workflow to the one provided by the Chipyard project [19]."}, {"title": "A. FPGA related optimizations", "content": "The accelerator was optimized for FPGA deployment by implementing the DSP packing technique proposed by [20]. DSPs are configurable slices contained in Xilinx FPGAs, which can very efficiently implement multiply-accumulate operations. This is the same mathematical operation used in each core Processing Element (PE) of the Gemmini accelerator. We first modified the accelerator to map each PE onto one DSP. But the DSPs were underutilize, as they can execute wide bit multiplications at its core, and Gemmini's PEs use 8 bits only to describe each input to its multiplier. So we implemented the technique proposed by [20]. By doing so, we were able to pack two 8 bit weights multiplications onto the same DSP, and therefore halve the usage of DSP slices.\nThe FPGA design can be further optimized by disabling a number of modules provided by the Gemmini accelerator, which are not needed for deployment of YOLO-type networks. Normalization features (used for transformer networks), transposition modules, virtual addresses translation tables and kernel dilation capabilities (used in encoder-decoder networks) are all examples of modules that can be disabled in order to reduce the resource usage of the final design. We were also able to reduce the output scaling modules which apply a floating point scaling factor to the accumulated output of the accelerator to reduce the bit width back to 8 bits, by changing the scaling factor from float32 to float16, without appreciating any degradation in the performance of the deployed CNN."}, {"title": "IV. END-TO-END WORKLFLOW", "content": "Now that the accelerator is already optimized to fit the selected FPGA platform, Figure 2 describes our end-to-end software process to orchestrate the execution of the CNN on the accelerator. As described in Section I, we start with a pretrained CNN model for object detection and the hardware on which we want to deploy it, and we end up with a complete system tuned for the selected hardware."}, {"title": "A. CNN model description", "content": "We selected the state-of-the-art YOLOv7 model [2], which is a CNN model pretrained on the COCO dataset for object detection. The model is provided in different sizes, so we selected the smaller version, YOLOv7-tiny (6.2 million parameters), as the target model to deploy.\nThis model, like other currently state-of-the-art CNN models for object detection, can be separated into two very distinct parts. The first one is the main part of the model, the one containing all the convolutions and other heavy tensor-intensive computation tasks, like concatenation or fully connected layers. The second part is composed of the layers preparing the data for the Non Max Suppression (NMS) algorithm and the algorithm itself, which filters and merges the proposed bounding boxes of the model and outputs cleaner detections. Contrary to other FPGA accelerator works, which only focus on deploying the feature extractor of the CNN (or only the convolutional layers), as we are focusing on a complete end-to-end solution, we evaluate the deployment of both parts of the network, their respective performance, and the optimal mapping to distribute the execution of both parts across the heterogeneous SoC."}, {"title": "B. Model optimizations", "content": "1) Input image size selection: The input image size of a CNN is directly related to the amount of giga computation operations (GFLOP when talking about floating point operations, GOP when referring to integer arithmetic) needed per inference, and thus directly related to the execution latency of the model. But this hurts the quality of the model's output, given that smaller input image sizes also reduce the amount of features that can be detected for each object in the image. So the first step is to select an appropriate input image size.\nFor the YOLOv7-tiny, we analyse the change in the mean Average Precision metric (mAP) for different image sizes for the pretrained model. As can be seen in Figure 3, the mAP of the model gets worse the smaller the input image size. Given that the mAP remains almost stable until an input image size of 480x480 pixels and then starts to get worse, we select 480x480 as the input image size we are going to use for the rest of the paper, which reduces the number GFLOPS by almost 50%.\n2) Activation function replacement: The range of layers supported by custom accelerators is usually restricted, and the same applies to the activation layers that can be directly accelerated on the hardware. As such, the Gemmini accelerator does not support the LeakyReLU activation used by the original YOLOv7-tiny model. Instead, these LeakyReLU layers would be mapped to the RISC-V CPU attached to the accelerator, which hurts the execution latency of the deployed model. But the Gemmini accelerator does support the ReLU layer, so we replaced all LeakyReLU layers of the model with simpler ReLU6 activation layers.\n3) Pruning: Model parameter size plays a vital role in the deployment of CNNs on memory-constrained devices. Pruning aims at removing redundancy in large models. Unlike unstructured pruning, where certain weights or parameters are replaced with zeros, structured pruning aims at deleting structural elements of a network, like neurons or filters. For CNNs, filter pruning is usually applied. The architecture of YOLOv7 is very complex; a lot of concatenation layers make filter pruning challenging. We pruned the CNN using iterative pruning [21], which relies on a connectivity graph of convolutional layers. In each iteration, layers to be pruned as well as a pruning rate are selected. Then, the model is pruned and fine-tuned to compensate for a drop in accuracy. We could thus achieve a reduction of up to 88% of parameters and up to 78% of GFLOPS after 14 iterations at a cost of a drop of 12.3 percent points in mAP (see Figure 4).\nA trade-off between mAP and parameter sparsity during pruning should be taken into account depending on the application. For the rest of the paper, we choose to evaluate the original un-pruned model and also two pruned models: the pruned model with 40% sparsity (as an example of a model where the mAP was not degraded below 30%) and the one with 88% sparsity (to show the minimum latency we can achieve with an extremely tiny model).\n4) Framework conversion: To map the model to different hardware backends using the TVM framework, we first need to apply several model conversions across multiple frameworks.\nFirst, we export the PyTorch model to ONNX using its standard ONNX exporter. Then, onnx2tf is used to transform the model to Tensorflow, which takes care of changing the data layout format NCHW (used by PyTorch and ONNX) to NHWC, which is the layout supported by the next step in the process.\nWe used the TFLite framework to quantize the model to int8 representation [22]. TFLite provides the option to use a different scaling and offset factor for each channel of each tensor (per-channel quantization), which reduces the quantization error, but we chose to use a per-tensor quantization for ease of deployment on the Gemmini accelerator. We also excluded from the quantization the Non Max Suppression (NMS) process at the output of the model, as our empirical tests have demonstrated a significant loss in prediction quality when quantizing this part of the model. Finally, the quantized model is imported into TVM using its standard TFLite frontend.\nBut transforming a model from one framework to the other can be hurtful for the quality of the predictions, particularly when using quantization procedures. In order to validate each conversion step, Table I presents the mean average precision (mAP) of each model after the conversion to each framework. We also report as a baseline the mAP of the pretrained model provided by the authors of the original YOLOv7 paper. As expected, quantizing the model degrades the performance of the network. But interestingly, the conversion from PyTorch to ONNX already inserts some errors: this may be caused by differences in the implementation of the operators between PyTorch and ONNX."}, {"title": "C. HW tuning", "content": "To be able to offload the execution of a CNN on the Gemmini accelerator, C code needs to be generated for each layer of the network. [3] provided an initial approach to integrate the Gemmini accelerator with TVM's microTVM C code generation workflow. This work presented how the TVM compiler can be used to generate calls to the Gemmini CISC-type and RISC-type instructions, and how it can be used to explore different scheduling candidates.\nIn this work, we expanded the integration presented in [3] to support the deployment of convolutions, max pooling, resize and concatenation layers using RISC-type instructions on the Gemmini accelerator. This allowed us to use the AutoTVM [4] feature of TVM to optimize them (see Section V-A)."}, {"title": "D. Model partitioning", "content": "As presented in Section IV-B4, after the quantization process the model can be separated into two clearly distinct parts: the main part, quantized to int8 representation, and the second part (the post-processing of the bounding boxes using the NMS algorithm) using floating point representation. The first one is perfectly suitable to be executed on the Gemmini accelerator, but the second one does not contain operations that can be offloaded to the accelerator. Given that the FPGA design implemented on the PL of the Zynq architecture runs at a lower frequency than the PS part, it makes sense to run this second part of the model on the PS side. This can be achieved using the TVM framework, by analysing the operator graph of the CNN and separating the model into two parts based on the data type used on each of them (see Section V-B)."}, {"title": "V. EVALUATION", "content": "For evaluation purposes, we compared the performance of the system implemented on a Xilinx ZCU102 and ZCU111 FPGAs against other hardware, from server-size GPUs (NVIDIA GTX1080), embedded GPU (NVIDIA Jetson AGX Xavier), ARM-based CPUs (Raspberry Pi 4, Quadcore located on the PS of the UltraScale architecture) and another FPGA accelerator (VTA [13], implemented on a Xilinx ZCU111 FPGA). All measurements were carried out by tuning and compiling the selected CNN using the TVM framework.\nWe also used the original, unmodified Gemmini implemented on a ZCU102 as a baseline. Table III shows the default parameters of the Gemmini accelerator and the ones we used for our work (we only show the ones that were modified). Table II shows the resource consumption of all implemented FPGA accelerators. One important aspect to highlight is that, although our optimized Gemmini uses 4 times more PEs than the original one, the amount of used DSPs in the entire design is not even doubled. This demonstrates the effectivity of the DSP packing technique described in Section III-A."}, {"title": "A. Autotuning for the Gemmini accelerator", "content": "Figure 5 presents the improvements achieved by executing AutoTVM for convolutions on the Gemmini accelerator. \"Default\" represents the total latency using the CISC-type instructions provided by the Gemmini developers (as explained in Section IV-C), while \"AutoTVM\" represents the latency using the best schedules found by the AutoTVM process. When the schedule using RISC-type instructions is not as good as the default one, we default to the CISC-type schedules, to always use the best schedule available.\nFirst of all, our solution achieves a mean speedup of 60 % when compared with the original unmodified Gemmini (both using the default schedules provided by the Gemmini developers). This demonstrates the importance of our modifications and adaptations for FPGA mapping of the Gemmini accelerator, which allowed us to increase the frequency and the number of processing elements. When analysing the impact of the autotuning process, we are able to achieve a mean 50% improvement across all models in the latency of the convolutions. For each model, more than 60% of the convolution layers were improved after tuning."}, {"title": "B. Model partitioning", "content": "To verify the claim presented in Section IV-D, we executed each part of the model on both PS and PL (Figure 6, reporting already autotuned results). The faster execution time for the main part of the model is achieved when tuned for execution on the Gemmini accelerator (PL). The faster execution time for the post-processing is achieved when executing this part of the model on the ARM CPUs (PS). This part takes a lot of time to run on the PL because it does not contain operators that can be offloaded to the accelerator, and the PL is running at a much lower frequency than the PS. Finally, the best solution is to execute the main part on the PL side and the post-processing on the PS side (mixed deployment scenario). Of course, the cost of moving data from the PL to the PS also needs to be taken into account when executing the model in such a distributed way, but because we use shared memory to communicate this data through the ACP port of the Zynq architecture, the cost is negligible and can be ignored."}, {"title": "C. Comparison with other hardware platforms", "content": "TVM allows us to target different hardware with the exact same model. First, Figure 7 compares the latency of our proposed platform against other embedded hardware, including a server-size GPU as reference. All measurements were tuned using AutoTVM for the particular hardware. Our solutions using the Gemmini accelerator surpass all other embedded hardware in terms of latency.\nThen, we compare the energy consumption of each hardware platform which integrates a power measurement device. Table IV reports real energy per inference measurements and the efficiency (giga operations per seconds per Joule). For each model, our FPGA solutions are more energy efficient than the other embedded solutions.\nFinally, Figure 8 compares our solutions against other FPGA works that implemented accelerators for int8 quantized CNNs. First, our solution improves the efficiency of the Gemmini accelerator when compared against the unmodified one. Second, our work is the first one of the compared works to execute a YOLOv7 model version on an FPGA, which because of the amount of layers of the model (58 convolution layers) prevents it from being implemented using a stream-type accelerator. Third, our solution lies on the Pareto-optimal border of designs. Finally, it can be appreciated that our Gemmini-based solution achieves comparable or better power efficiency than other works. Those achieving an efficiency greater than 36.5 GOP/s/W can be explained because they designed specialized accelerators to execute the Winograd convolution [23], [24], [25], or used higher working frequencies (200 MHz [26], [27], [28], 242 MHz [29]). Both could be explored in a future work to improve the efficiency of our solution."}, {"title": "VI. CASE STUDY", "content": "A CNN model cannot exist in isolation: the integration of standard interfaces to connect to cameras and publish the results presents its own challenges. This is why few FPGA CNN papers demonstrate how to integrate their solution into a larger system. As a case study, traffic surveillance presents itself as an interesting use case for implementation of realtime edge systems [36]. We used the mobile sensor platform Infra2Go [37] to integrate our FPGA-based solution and realize this use case (Figure 9).\nThe proposed FPGA accelerator is integrated into a larger processing pipeline using ROS2 [38] as a middleware: 1) camera images of a calibrated camera are send from an x86 based host ECU to the FPGA in form of standardized ROS2 messages over ethernet, 2) the Zephyr real-time operating system [39] is used as the runtime to receive the image, execute the main part of the model on the RISC-V core and the Gemmini accelerator, 3) the TVM runtime on the PS receives the intermediate outputs, runs the NMS post-processing and publishes the object detections, and 4) on the main ECU, homography projection to a ground plane, world-space tracking and state estimation (including velocity estimation) are performed using a Gaussian Mixture Probability Hypothesis Density Filter (GMPHD)."}, {"title": "VII. CONCLUSION", "content": "This paper presented a deployment framework for quantized CNNs on an FPGA system build around the Gemmini accelerator. We modified the Gemmini design to enable improved mapping onto an FPGA, which resulted in a mean speed-up of 60% when compared against the original, unmodified Gemmini. Additionally, the autotuning process allowed us a further 50% improvement across all model versions. The evaluation also demonstrated the effectiveness of model partitioning for the overall model execution including post-processing of the detected bounding boxes, a process usually neglected when deploying CNNs on FPGAs. Real measurements and comparative analysis against other hardware platforms revealed that the proposed FPGA solution outperforms all others in terms of energy efficiency, achieving a 85% reduction when compared with an NVIDIA Jetson AGX Xavier board, and 93% when compared against a server-size GPU. Furthermore, our Gemmini-based solution achieves a power efficiency of 36.5 GOP/s/W, which is better or comparable to the same metric reported by other similar FPGA accelerator works. Finally, we showcased using a traffic monitoring use case how this kind of solution can be integrated into a wider system."}]}