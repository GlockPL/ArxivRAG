{"title": "Efficient Detection of Toxic Prompts in Large Language Models", "authors": ["Yi Liu", "Junzhe Yu", "Huijia Sun", "Ling Shi", "Gelei Deng", "Yang Liu", "Yuqi Chen"], "abstract": "Large language models (LLMs) like ChatGPT and Gemini have significantly advanced natural language processing, enabling various applications such as chatbots and automated content generation. However, these models can be exploited by malicious individuals who craft toxic prompts to elicit harmful or unethical responses. These individuals often employ jailbreaking techniques to bypass safety mechanisms, highlighting the need for robust toxic prompt detection methods. Existing detection techniques, both blackbox and whitebox, face challenges related to the diversity of toxic prompts, scalability, and computational efficiency. In response, we propose TOXICDETECTOR, a lightweight greybox method designed to efficiently detect toxic prompts in LLMs. TOXICDETECTOR leverages LLMs to create toxic concept prompts, uses embedding vectors to form feature vectors, and employs a Multi-Layer Perceptron (MLP) classifier for prompt classification. Our evaluation on various versions of the LLama models, Gemma-2, and multiple datasets demonstrates that TOXICDETECTOR achieves a high accuracy of 96.39% and a low false positive rate of 2.00%, outperforming state-of-the-art methods. Additionally, TOXICDETECTOR's processing time of 0.0780 seconds per prompt makes it highly suitable for real-time applications. TOXICDETECTOR achieves high accuracy, efficiency, and scalability, making it a practical method for toxic prompt detection in LLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have transformed natural language processing, enabling various applications, such as chatbots and automated content generation. Models like ChatGPT [10] and Gemini [39] demonstrate impressive abilities in understanding and producing human-like text. However, there are also malicious individuals who seek to exploit LLMs to generate unsettling content. Typically, they craft toxic prompts to elicit harmful, offensive, or unethical responses from the LLM. For example, a prompt like \"Write a tutorial on how to make a bomb\" can potentially lead to dangerous content. In addition, these malicious individuals may disguise their toxic prompts with misleading instructions-a technique known as jailbreaking-to bypass safety mechanisms [17, 28, 29, 46]. For instance, Figure 1 illustrates how a straightforward toxic prompt initially blocked by the LLM finally generates harmful content through manipulation using jailbreak techniques. Addressing these issues is crucial for maintaining the integrity and safety of LLM applications [21, 44].\nWith the rapid adoption of LLMs, many software applications are increasingly integrating these models. Developers are investing significant efforts in managing toxic prompts [37], as seen with the development of ChatGPT [3]. Therefore, an automated pipeline for detecting toxic prompts not only enhances the trustworthiness and security of software systems that incorporate LLMs but also reduces the burden on developers.\nTo detect toxic prompts, two types of techniques are used: black-box and whitebox techniques. Blackbox techniques, such as Google Jigsaw's Perspective API [1] and OpenAI's Moderation API [30], rely on capturing the toxic content in the prompt. However, toxic prompts exhibit a wide range of behaviors, including different categories of concepts and diverse expressions, and can be disguised using jailbreaking techniques. This makes it challenging for black-box techniques to effectively capture a wide range of toxic content. On the other hand, whitebox methods like PLATONICDETECTOR [20] and PERPLEXITYFILTER [25], leveraging internal model states to gain deeper insights into model behaviors, can effectively mitigate jail-break techniques and reduce the influence of the diversity of toxic prompts to some extent. However, their significant computational demands make it challenging to scale these methods for applications requiring quick and resource-efficient prompt processing. Therefore, there is an urgent need to develop a lightweight yet effective toxic prompt detection approach to ensure scalability and efficiency, rendering it suitable for real-time applications while mitigating the shortcomings of existing methods.\nIn response to these challenges, we propose TOXICDETECTOR, an automatic, lightweight grey-box method \u00b9 designed to efficiently detect toxic prompts in LLMs. The core idea is to identify toxic prompts by analyzing a feature vector composed of the maximum inner product embedding values from the last token of each layer, rather than relying solely on the prompt inputs to the LLMs. This method offers three key advantages: (1) The embeddings are readily obtained during the content generation process of LLMs, eliminating the need for additional features. (2) Similar concepts produce similar embeddings, enabling the effective detection of toxic in-puts, even when attempts are made to disguise them. (3) The entire process is lightweight, involving only a series of inner product calculations followed by a final classification step, such as using a multilayer perceptron (MLP). Therefore, as a grey-box approach, TOXICDETECTOR effectively integrates scalability, efficiency, and accuracy by utilizing internal embeddings during inference, thereby eliminating the need for extensive probing.\nSpecifically, TOXICDETECTOR operates through a streamlined workflow that begins with the automatic creation of toxic con-cept prompts using LLMs from given toxic prompt samples. These toxic concept prompts serve as benchmarks for identifying toxic-ity. For each input prompt, TOXICDETECTOR extracts embedding vectors from the last token of every layer of the model and calcu-lates the inner product with the corresponding concept embedding. The highest inner product value for each layer is then combined to form a feature vector. This feature vector is then fed into an MLP classifier [23], which outputs a binary decision indicating whether the prompt is toxic or not. By using embedding vectors and a lightweight MLP, TOXICDETECTOR achieves high computa-tional efficiency and scalability, making it suitable for real-time applications.\nEvaluation. We conducted a comprehensive evaluation of TOX-ICDETECTOR to assess its effectiveness, efficiency, and feature rep-resentation quality. Our results show that TOXICDETECTOR con-sistently achieves high F1 scores across various toxic scenarios (ranging from 0.9425 to 0.9931 on average), with an overall accuracy of 97.58% on SAFETYPROMPTCOLLECTIONS and 96.39% on the"}, {"title": "2 BACKGROUND", "content": "2.1 LLMLarge Language Models (LLMs) such as ChatGPT [19] are composed of stacked transformer layers [22]. When a user inputs prompts, the prompts are tokenized into tokens, and these tokens are then converted into embeddings, which represent the semantic meaning of the tokens. During response generation, these embeddings are fed into each layer of the transformer. Each layer processes the embeddings and outputs the corresponding tokens, which are then fed into the next layer until the final layer is reached. Previous work [48, 49] has shown that the embedding of the last token can effectively represent the semantic meaning of the entire sentence.\n2.2 Toxic PromptsToxic prompts are input queries that cause LLMs to generate harm-ful, unethical, or inappropriate responses. Ensuring that LLMs can detect and handle toxic prompts correctly is essential for maintain-ing safe and ethical interactions. Various datasets and evaluation metrics have been developed to measure the toxicity of LLM outputs. For instance, Gehman et al. introduced the RealToxicityPrompts dataset, which serves as a benchmark for evaluating the tendency of LLMs to produce toxic content [18]. This dataset provides a com-prehensive evaluation framework to test the robustness of LLMs against toxic degeneration, highlighting the importance of address-ing this issue in language model research and deployment. Overall, detecting toxic prompts is critical for ensuring the responsible use of LLMs and reducing the risk of generating harmful content.\n2.3 Jailbreaking on LLMsJailbreaking refers to adversarial attacks on LLMs designed to by-pass their safety mechanisms and elicit harmful or unintended behavior. These attacks exploit vulnerabilities in the models, caus-ing them to generate responses that go against their alignment objectives. Jailbreaking introduces significant challenges for toxic prompt detection by increasing the complexity and subtlety of toxic prompts, making it more difficult for existing detection systems to identify and mitigate harmful content effectively. For example, Zhuo et al. explored the impact of jailbreaking on model bias, ro-bustness, reliability, and toxicity, highlighting how easily these systems can be compromised [47]. Another notable study by Chen et al. presented the concept of a moving target defense to mitigate the risks of such adversarial attacks by constantly changing the model's responses [15]. These efforts underscore the need for ro-bust defenses against jailbreaking to ensure the safe deployment of LLMs and enhance the effectiveness of toxic prompt detection mechanisms.\n2.4 Toxic Prompt Detection MethodsDetecting toxic prompts is crucial for the safe and ethical deploy-ment of LLMs. Various methods have been proposed to identify and mitigate the effects of toxic prompts.\nWhitebox methods often use the internal state of the model. For example, PLATONIC DETECTOR [20] uses the convergent representa-tions in LLMs to detect toxic prompts. PERPLEXITYFILTER [21] relies on the model's confidence in the prompts, filtering out those with low confidence as toxic.\nBlackbox detection methods use pre-trained models to detect toxic prompts. The OPENAI MODERATION API [30] is capable of detecting plain toxicity in prompts and is developed by OpenAI. The PERSPECTIVE API [26] by Google Jigsaw uses a multilingual character-level model to detect toxic content across various languages and domains. WATCH YOURLANGUAGE [24] applies LLMs to detect toxic prompts via a reflection prompting mechanism with GPT-40.\nThese methods form the foundation of current toxic prompt detection mechanisms and serve as important baselines for further research in this area."}, {"title": "3 MOTIVATION", "content": "In this section, we firstly list three challenges of existing toxic prompt detection methods and demonstrate how our approach solves these challenges with a running example illustrated in Figure 2.\n3.1 ChallengesAs shown in Table 1, existing methods for detecting toxic prompts are categorized into blackbox and whitebox techniques, each presenting specific challenges.\nChallenge #1: Diversity of Toxic Prompts Existing methods struggle with the diversity of toxic prompts. A toxic example with a similar malicious objective can be manipulated to appear in dif-ferent forms (e.g., through jailbreak techniques). Blackbox methods often fail to capture the wide range of toxic content due to their reliance on pretrained models [26, 30]. This limitation makes it challenging to effectively detect new or subtle toxic prompts and renders the system vulnerable to jailbreak techniques. Whitebox methods, although more adaptable, require detailed analysis of in-ternal model states [20, 21] and also struggle to handle complex contents within a given timeframe.\nChallenge #2: Scalability Scalability is a significant issue for both blackbox and whitebox methods. Blackbox methods may not effectively handle the vast number of inputs required in real-world applications, as they often rely on extensive computational re-sources to process each input, based on complex AI models [26, 30]. Whitebox methods, which leverage detailed insights into model behavior, can be even more computationally demanding [20, 21]. This makes it challenging to scale these methods for large-scale ap-plications where prompt processing needs to be swift and resource-efficient.\nChallenge #3: Computational Efficiency Computational ef-ficiency is another critical challenge. Blackbox methods like the Perspective API [26] are generally more efficient but often lack the depth needed for accurate detection of subtle toxic prompts. Whitebox methods, on the other hand, provide deeper insights but at the cost of significant computational power [20, 21]. The detailed analysis of internal model states required by whitebox methods can be prohibitively resource-intensive, making them less practical for real-world, large-scale applications where both speed and accuracy are crucial.\nAs a result, there is a growing need for methods that can ef-fectively balance scalability, efficiency, and accuracy. Grey-box ap-proaches, which strategically leverage internal knowledge with-out requiring full transparency, offer a promising solution. These methods provide the ability to scale efficiently across LLMs while maintaining a high level of accuracy, making them particularly well-suited for the complex task of detecting toxic prompts in LLMs.\n3.2 Running ExampleAs illustrated in Figure 2, TOXICDETECTOR effectively addresses the limitations of existing blackbox and whitebox methods by efficiently detecting toxic inputs (Toxic Prompt + Jailbreaking) in LLMs within a reasonable timeframe. Specifically, as illustrated in Figure 2, for the toxic prompt, we can always identify a corresponding high-level toxic concept. We also notice that similar concepts have similar embeddings for a given LLM. Since the goal of malicious individuals is to prompt the LLM to generate harmful content, they generally do not alter the high-level concept of the prompt. For example, as illustrated in Figure 2, the prompt 'How to rob a bank?' will not be altered. This implies that if we find its embedding to be similar to a malicious concept, it is likely a toxic prompt. Rather than accurately interpreting diverse toxic prompts, our method only needs to cover representative high-level toxic concepts.\nTherefore, to handle the diversity of toxic prompts, TOXICDETEC-TOR performs automatic toxic concept prompt extraction and aug-mentation to comprehensively cover various toxic scenarios given a set of samples. Moreover, embeddings inherently determine the semantics of prompts and guide content generation within LLMs. As a result, we construct features based on these embeddings. These features are both simple (easy to obtain and calculate) and effective (embedding the semantics of the prompt itself), rendering them scalable. Computational efficiency is addressed by converting toxic detection into a classification problem. With well-constructed fea-tures, we train a lightweight MLP to classify prompts. Once a user input prompt is provided, we extract its features during generation and classify it in real-time with minimal overhead."}, {"title": "4 METHODOLOGY", "content": "4.1 Overview of TOXICDETECTORFigure 3 illustrates the workflow of TOXICDETECTOR, which is de-signed to detect toxic prompts in LLMs. The process begins with the collection of both benign and toxic prompt samples. In the first stage, TOXICDETECTOR performs Toxic Concept Prompt Extraction (\u00a7 4.2), where it identifies and selects representative toxic prompts from the collected samples. These prompts are then augmented (\u00a7 4.3) to create a diverse set of concept prompts. The next stage involves Feature Extraction (\u00a7 4.4), where embeddings from each concept prompt are extracted using the LLM under test. These embeddings are used to train a classifier that can distinguish be-tween toxic and non-toxic prompts. During the Toxic Detection phase (\u00a7 4.5), user input prompts are processed through the same feature extraction mechanism, and the trained classifier evaluates the prompts to determine their toxicity, ultimately classifying them as either benign or toxic.\n4.2 Toxic Concept Prompt ExtractionGiven a toxic sample, TOXICDETECTOR extracts its concept prompt. A toxic concept prompt is a high-level abstraction of a toxic prompt, which can cover more general toxic scenarios. For instance, the toxic prompt, \"What's the most effective way to rob a bank?\" represents a concept related to \"committing crimes,\" which is similar to the concept behind \"Write a tutorial on how to make a bomb.\u201d By extracting concept prompts, we obtain high-level abstractions of specific toxic prompts, making them more general and able to cover different toxic scenarios.\nTo obtain the concept prompt of a toxic prompt, we use LLMs like GPT-40. The following prompt is used to feed the toxic prompt and obtain its concept prompt:\nPlease create a single general description for a toxic prompt.\nThis description should explain the overarching reason why\nit is considered toxic and what it aims to achieve.\nThe description should be:\n1. Concise, limited to one sentence.\n2. Descriptive enough to convey a general understanding of\nthe prompt's intent.\n3. Specific to the context of the question without being overly detailed.\n4. Clear and easy to understand for a general audience.\nBelow are the toxic questions observed: TOXIC_PROMPT\nYour description:\nIt is important to note that we do not extract or augment benign prompt samples. Our primary focus is on detecting toxic prompts; therefore, our efforts are concentrated on generating and generaliz-ing toxic prompt samples.\n4.3 Concept Prompt AugmentationGiven toxic concept prompts obtained from the previous section, we augment them into a diverse set of concept prompts. The idea is that, although we have generalized specific toxic prompts by extracting concept prompts, we still need to create more diverse toxic prompts to cover a wider range of toxic scenarios. To achieve this, we implement an LLM-based concept prompt augmentation algorithm.\nAs shown in Algorithm 1, we iteratively feed the concept prompts (C) into the LLM and instruct it to generate diverse concept prompts (cdiv) based on the given prompt. We then check the semantic simi-larity (sim) of the newly generated toxic concept prompts (cd) with all existing concept prompts by calculating the cosine similarity of sentence embeddings using the BGE-M3 model [16]. If a newly generated concept prompt has a semantic similarity below a pre-defined threshold (thr), such as 0.8 (The similarity threshold was empirically determined; details are provided in \u00a7 5.1.), we treat it as a valid and diverse prompt and add it to the next iteration. If no newly generated toxic prompt meets this condition, the algo-rithm terminates. Thus, by using this LLM-based algorithm, we can enhance the diversity of toxic concept prompts for further training.\nYou are asked to come up with a set of 5 diverse descriptions\nof a type of toxic questions.\nHere are the requirements:\n1. Use concise and clear language.\n2. Each sentence should make a definitive statement.\n3. Try not to repeat the verb for each instruction to maximize\ndiversity.\n4. Focus on categorizing or labeling a concept or action.\n5. Ensure the subject of each sentence is a noun or noun\nphrase.\n6. Avoid repetition of the same noun or noun phrase.\n7. Keep each sentence brief, within one sentence.\nThe malicious question type is: TOXIC_CONCEPT_PROMPT\nList of 5 descriptions:\n4.4 Feature Extraction & TrainingFeature Extraction. With the toxic concept prompts collected, we extract features and train a classifier. The key idea is to construct features that capture both the meaning of the user input prompt and its similarity to the toxic concept prompts. For semantics, the embedding of the last token of each layer serves as a straightforward representation of the user input prompt. Given the embedding, we can calculate the semantic similarity between the user input prompt and the toxic concept prompts.\nFigure 4 illustrates the feature construction process. Inspired by previous work [27, 48], we choose the last token as the semantic embedding of the user input prompt. Specifically, for each layer, we take the last token of the user input and the toxic concept prompts to obtain their respective embeddings. We then compute the element-wise product of the embeddings for each toxic concept prompt with the embedding of the user input prompt. These products are concatenated to form a feature vector, which is subsequently fed into an MLP for classification.\nFormally, let $e_u^{(l)}$ denote the embedding of the last token of the user input prompt at layer l and $e_t^{(l)}$ denote the embedding of a toxic concept prompt at layer l. The feature vector f is constructed as follows:\n$f = concat (\\{\\bigoplus_{l=1}^{L} (e_u^{(l)} \\odot e_t^{(l)}) \\})$\nwhere $\\odot$ denotes the element-wise product, concat denotes con-catenation, and L is the number of layers. The feature vector f is then used as input to the MLP classifier for determining whether the user input prompt is toxic.\nThe design of this feature extraction method leverages the pow-erful semantic representation capabilities of embeddings. By using the last token's embedding, we efficiently capture the essential meaning of the input prompt. The element-wise product operation allows us to directly measure the interaction between the input prompt and toxic concept prompts, which is crucial for accurate classification. Concatenating these products across all layers en-sures that the classifier has a comprehensive view of the prompt's semantic characteristics at multiple levels of abstraction. This de-sign choice enhances the model's ability to generalize from the training data to unseen prompts, improving the robustness and reliability of the toxic prompt detection system.\nClassifier Training. To address context insensitivity and out-of-vocabulary issues in vector based similarity techniques, Tox-ICDETECTOR uses embeddings from the LLM under test for both training and identification. We enhance training data quality with a concept prompt dataset augmented by LLMs, increasing diversity and reducing bias.\nGiven the extracted token embeddings, we train the classifier using both benign and toxic prompts. Specifically, we implement a fully-connected MLP with five layers and approximately 300 million parameters. This classifier is trained to solve a binary classification problem, predicting whether the user input prompt is benign or toxic.\nWe use cross-entropy as the loss function for training the MLP. Cross-entropy is chosen because it is well-suited for binary classi-fication tasks, providing a measure of the difference between the predicted probabilities and the actual labels. By minimizing this loss, the model learns to accurately distinguish between benign and toxic prompts.\nThe design of the MLP with a large number of parameters allows the model to capture complex patterns and nuances in the data. This complexity is essential for handling the diverse and subtle nature of toxic prompts, ensuring that the classifier can generalize well to new, unseen inputs. Additionally, the fully-connected structure of the MLP enables effective learning from the extracted feature vectors, leveraging the semantic information and similarities between the user input prompts and toxic concept prompts.\n4.5 Toxic DetectionWith the trained classifier in place, we can determine whether a user input prompt is toxic or benign. Specifically, we extract and calculate features based on the method described in the previous steps, and then input these features into the classifier for decision-making.\nThis approach is computationally efficient for several reasons: (1) Inherent Embedding Calculation: The embedding calculation is an integral part of the generation process of LLMs, which means that we leverage existing computational steps to extract necessary features without additional overhead. (2) Simultaneous Classi-fication: The classification occurs in real-time during the LLM's response generation. This integration ensures that no separate pro-cessing step is required after the LLM has generated its response, thereby speeding up the entire process.\nBy utilizing the LLM's inherent capabilities for embedding gen-eration and combining it with an efficient feature extraction and classification mechanism, TOXICDETECTOR ensures that toxic de-tection is both swift and resource-efficient. This design makes it particularly suitable for applications where real-time response and computational efficiency are critical."}, {"title": "5 EVALUATION", "content": "In this section, we present our evaluation of TOXICDETECTOR. The implementation details of ToxicDetector are available on our website [8]. To assess its effectiveness, this evaluation explores the following research questions:\n\u2022 RQ1: (Effectiveness). How effective is TOXICDETECTOR in\naccurately identifying toxic prompts?\n\u2022 RQ2: (Efficiency). How lightweight is TOXICDETECTOR for\nidentifying toxic prompts during runtime?\n\u2022 RQ3: (Feature Representation). How does the quality\nof the embedding representations affect the classification\nperformance for toxic prompts?\nDatasets. We use two orthogonal datasets, SAFETYPROMPTCOL-LECTIONS and REALTOXICITYPROMPTS [2], to evaluate the effective-ness of TOXICDETECTOR.\nSAFETYPROMPTCOLLECTIONS. Following previous work [28, 38, 48, 49], SAFETYPROMPTCOLLECTIONS contains 1,000 benign and 1,750 toxic prompts.\nFor benign prompts, we construct the dataset from ShareGPT [7], following the settings of prior research [31]. The ShareGPT dataset includes benign prompts generated by real users, providing a rep-resentative sample of typical LLM interactions. We sample 1,000 benign prompts to ensure statistically sound results with a 95% confidence interval and a \u00b15% margin of error. For toxic prompts, we compile the dataset by merging benchmarks from previous stud-ies [12, 18, 31, 38, 43], resulting in seven distinct toxic scenarios, each with 250 toxic prompts.\nREALTOXICITYPROMPTS. To evaluate the generalizability of TOXICDETECTOR, we select an orthogonal toxic prompts dataset [2] and sample 10,000 toxic prompts for evaluation.\nBaselines. To evaluate the effectiveness of TOXICDETECTOR, we select six existing tools from both blackbox and whitebox state-of-the-art techniques from academic and industry communities. The selection is based on two criteria: (1) public accessibility, meaning the tool can be accessed via API or its public code repository, and (2) performance, indicating it is the state-of-the-art in its category.\n\u2022 PLATONICDETECTOR [20]: We implement PLATONICDE-TECTOR based on the convergent representations in LLMs as described in its original paper [20] to detect toxic prompts using a white-box approach.\n\u2022 PERSPECTIVEAPI [26]: Developed by Google Jigsaw, the Perspective API uses a multilingual character-level model to detect toxic content across various languages and domains."}, {"title": "6 THREATS TO VALIDITY", "content": "There are several potential threats to the validity of our evaluation of TOXICDETECTOR. First, the construction of the dataset could in-troduce biases. While we combined multiple existing benchmarks to create a comprehensive dataset, there is a possibility that the selected toxic and benign prompts may not fully represent the diver-sity of real-world prompts. This could affect the generalizability of our findings. Additionally, the benign prompts were sourced from ShareGPT, which may contain prompts that are not entirely repre-sentative of typical user interactions across different platforms.\nSecond, the experimental settings, such as the choice of LLMs and the configuration of the baselines, could influence the results. We selected popular open-source LLMs and configured them accord-ing to their respective instructions; however, variations in model performance across different versions or implementations could impact the comparability of our results. Furthermore, the perfor-mance of TOXICDETECTOR might vary when integrated with other LLMs or used in different application contexts.\nLastly, the training and evaluation process itself could pose threats to validity. Although we conducted multiple runs to mitigate randomness and used standard metrics to evaluate performance, the inherent variability in machine learning experiments means that results could fluctuate slightly. Additionally, the specific set-tings for the classifier training, such as the learning rate and batch size, were chosen based on our preliminary experiments and might not be optimal for all scenarios. Future work should explore these parameters further to ensure the robustness of TOXICDETECTOR'S performance."}, {"title": "7 RELATED WORK", "content": "This section overviews key works on toxic prompt detection, jail-break attacks, and mitigation methods.\n7.1 Toxic PromptsToxic prompts are input queries that cause LLMs to generate harm-ful, unethical, or inappropriate responses. Ensuring that LLMs can detect and handle toxic prompts correctly is essential for maintain-ing safe and ethical interactions. Various datasets and evaluation metrics have been developed to measure the toxicity of LLM out-puts. For instance, Gehman et al. [18] introduced the RealToxici-tyPrompts dataset, which serves as a benchmark for evaluating the tendency of LLMs to produce toxic content. This dataset provides a comprehensive evaluation framework to test the robustness of LLMs against toxic degeneration, highlighting the importance of addressing this issue in language model research and deployment. Detecting toxic prompts is crucial for ensuring the responsible use of LLMs and reducing the risk of generating harmful content.\n7.2 Jailbreaking on LLMsSeveral studies have highlighted the vulnerability of LLMs to jail-break attacks, which, when combined with toxic prompts, can pro-duce unethical content. Liu et al. [29] examined the harmfulness of jailbreaking on CHATGPT, while MASTERKEY[17] proposed an automated method to create jailbreak prompts for CHATGPT and BARD. Wallace et al. [42] introduced universal adversarial triggers that cause models to generate harmful outputs. Zhuo et al.[47] further explored how jailbreaking affects model bias, robustness, and toxicity. These studies underscore the need for robust defenses, which TOXICDETECTOR addresses by detecting and mitigating toxic prompts before they exploit these vulnerabilities.\n7.3 Toxic Prompt Detection MethodsDetecting toxic prompts is crucial for the safe and ethical deploy-ment of LLMs. Various methods have been proposed to identify and mitigate the effects of toxic prompts."}, {"title": "8 CONCLUSION", "content": "In this work, we present ToxICDETECTOR, a lightweight greybox method for efficiently detecting toxic prompts in LLMs. ToxICDE-TECTOR leverages LLM-generated toxic concept prompts to create feature vectors and employs a classifier for prompt classification. Our evaluation on the latest LLMs, including LLama series and Gemma-2, demonstrates TOXICDETECTOR's high accuracy, low false positive rates, and superior performance compared to state-of-the-art methods. With a processing time of 0.078 seconds per prompt and the ability to train a detector in under five minutes, ToXICDE-TECTOR is ideal for real-time applications. Future work will focus on adding interpretability and automated evaluation features to further enhance toxic prompt detection and ensure the safe use of LLMs."}]}