{"title": "Efficient Detection of Toxic Prompts in Large Language Models", "authors": ["Yi Liu", "Junzhe Yu", "Huijia Sun", "Ling Shi", "Gelei Deng", "Yang Liu", "Yuqi Chen"], "abstract": "Large language models (LLMs) like ChatGPT and Gemini have significantly advanced natural language processing, enabling various applications such as chatbots and automated content generation. However, these models can be exploited by malicious individuals who craft toxic prompts to elicit harmful or unethical responses. These individuals often employ jailbreaking techniques to bypass safety mechanisms, highlighting the need for robust toxic prompt detection methods. Existing detection techniques, both blackbox and whitebox, face challenges related to the diversity of toxic prompts, scalability, and computational efficiency. In response, we propose TOXICDETECTOR, a lightweight greybox method designed to efficiently detect toxic prompts in LLMs. TOXICDETECTOR leverages LLMs to create toxic concept prompts, uses embedding vectors to form feature vectors, and employs a Multi-Layer Perceptron (MLP) classifier for prompt classification. Our evaluation on various versions of the LLama models, Gemma-2, and multiple datasets demonstrates that TOXICDETECTOR achieves a high accuracy of 96.39% and a low false positive rate of 2.00%, outperforming state-of-the-art methods. Additionally, TOXICDETECTOR's processing time of 0.0780 seconds per prompt makes it highly suitable for real-time applications. TOXICDETECTOR achieves high accuracy, efficiency, and scalability, making it a practical method for toxic prompt detection in LLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have transformed natural language processing, enabling various applications, such as chatbots and automated content generation. Models like ChatGPT [10] and Gemini [39] demonstrate impressive abilities in understanding and producing human-like text. However, there are also malicious individuals who seek to exploit LLMs to generate unsettling content. Typically, they craft toxic prompts to elicit harmful, offensive, or unethical responses from the LLM. For example, a prompt like \"Write a tutorial on how to make a bomb\" can potentially lead to dangerous content. In addition, these malicious individuals may disguise their toxic prompts with misleading instructions-a technique known as jailbreaking-to bypass safety mechanisms [17, 28, 29, 46]. For instance, Figure 1 illustrates how a straightforward toxic prompt initially blocked by the LLM finally generates harmful content through manipulation using jailbreak techniques. Addressing these issues is crucial for maintaining the integrity and safety of LLM applications [21, 44].\nWith the rapid adoption of LLMs, many software applications are increasingly integrating these models. Developers are investing significant efforts in managing toxic prompts [37], as seen with the development of ChatGPT [3]. Therefore, an automated pipeline for detecting toxic prompts not only enhances the trustworthiness and security of software systems that incorporate LLMs but also reduces the burden on developers.\nTo detect toxic prompts, two types of techniques are used: black-box and whitebox techniques. Blackbox techniques, such as Google"}, {"title": "2 BACKGROUND", "content": ""}, {"title": "2.1 LLM", "content": "Large Language Models (LLMs) such as ChatGPT [19] are composed of stacked transformer layers [22]. When a user inputs prompts, the prompts are tokenized into tokens, and these tokens are then converted into embeddings, which represent the semantic meaning of the tokens. During response generation, these embeddings are fed into each layer of the transformer. Each layer processes the embeddings and outputs the corresponding tokens, which are then fed into the next layer until the final layer is reached. Previous work [48, 49] has shown that the embedding of the last token can effectively represent the semantic meaning of the entire sentence."}, {"title": "2.2 Toxic Prompts", "content": "Toxic prompts are input queries that cause LLMs to generate harmful, unethical, or inappropriate responses. Ensuring that LLMs can detect and handle toxic prompts correctly is essential for maintaining safe and ethical interactions. Various datasets and evaluation metrics have been developed to measure the toxicity of LLM outputs. For instance, Gehman et al. introduced the RealToxicityPrompts dataset, which serves as a benchmark for evaluating the tendency of LLMs to produce toxic content [18]. This dataset provides a comprehensive evaluation framework to test the robustness of LLMs"}, {"title": "2.3 Jailbreaking on LLMs", "content": "Jailbreaking refers to adversarial attacks on LLMs designed to bypass their safety mechanisms and elicit harmful or unintended behavior. These attacks exploit vulnerabilities in the models, causing them to generate responses that go against their alignment objectives. Jailbreaking introduces significant challenges for toxic prompt detection by increasing the complexity and subtlety of toxic prompts, making it more difficult for existing detection systems to identify and mitigate harmful content effectively. For example, Zhuo et al. explored the impact of jailbreaking on model bias, robustness, reliability, and toxicity, highlighting how easily these systems can be compromised [47]. Another notable study by Chen et al. presented the concept of a moving target defense to mitigate the risks of such adversarial attacks by constantly changing the model's responses [15]. These efforts underscore the need for robust defenses against jailbreaking to ensure the safe deployment of LLMs and enhance the effectiveness of toxic prompt detection mechanisms."}, {"title": "2.4 Toxic Prompt Detection Methods", "content": "Detecting toxic prompts is crucial for the safe and ethical deployment of LLMs. Various methods have been proposed to identify and mitigate the effects of toxic prompts.\nWhitebox methods often use the internal state of the model. For example, PLATONIC DETECTOR [20] uses the convergent representations in LLMs to detect toxic prompts. PERPLEXITYFILTER [21] relies on the model's confidence in the prompts, filtering out those with low confidence as toxic.\nBlackbox detection methods use pre-trained models to detect toxic prompts. The OPENAI MODERATION API [30] is capable of detecting plain toxicity in prompts and is developed by OpenAI. The PERSPECTIVE API [26] by Google Jigsaw uses a multilingual character-level model to detect toxic content across various languages and domains. WATCH YOURLANGUAGE [24] applies LLMs to detect toxic prompts via a reflection prompting mechanism with GPT-40.\nThese methods form the foundation of current toxic prompt detection mechanisms and serve as important baselines for further research in this area."}, {"title": "3 MOTIVATION", "content": "In this section, we firstly list three challenges of existing toxic prompt detection methods and demonstrate how our approach solves these challenges with a running example illustrated in Figure 2."}, {"title": "3.1 Challenges", "content": "As shown in Table 1, existing methods for detecting toxic prompts are categorized into blackbox and whitebox techniques, each presenting specific challenges.\nChallenge #1: Diversity of Toxic Prompts Existing methods struggle with the diversity of toxic prompts. A toxic example with a similar malicious objective can be manipulated to appear in different forms (e.g., through jailbreak techniques). Blackbox methods often fail to capture the wide range of toxic content due to their reliance on pretrained models [26, 30]. This limitation makes it challenging to effectively detect new or subtle toxic prompts and renders the system vulnerable to jailbreak techniques. Whitebox methods, although more adaptable, require detailed analysis of internal model states [20, 21] and also struggle to handle complex contents within a given timeframe.\nChallenge #2: Scalability Scalability is a significant issue for both blackbox and whitebox methods. Blackbox methods may not effectively handle the vast number of inputs required in real-world applications, as they often rely on extensive computational resources to process each input, based on complex AI models [26, 30]. Whitebox methods, which leverage detailed insights into model behavior, can be even more computationally demanding [20, 21]. This makes it challenging to scale these methods for large-scale applications where prompt processing needs to be swift and resource-efficient.\nChallenge #3: Computational Efficiency Computational efficiency is another critical challenge. Blackbox methods like the"}, {"title": "3.2 Running Example", "content": "As illustrated in Figure 2, TOXICDETECTOR effectively addresses the limitations of existing blackbox and whitebox methods by efficiently detecting toxic inputs (Toxic Prompt + Jailbreaking) in LLMs within a reasonable timeframe. Specifically, as illustrated in Figure 2, for the toxic prompt, we can always identify a corresponding high-level toxic concept. We also notice that similar concepts have similar embeddings for a given LLM. Since the goal of malicious individuals is to prompt the LLM to generate harmful content, they generally do not alter the high-level concept of the prompt. For example, as illustrated in Figure 2, the prompt 'How to rob a bank?' will not be altered. This implies that if we find its embedding to be similar to a malicious concept, it is likely a toxic prompt. Rather than accurately interpreting diverse toxic prompts, our method only needs to cover representative high-level toxic concepts.\nTherefore, to handle the diversity of toxic prompts, TOXICDETECTOR performs automatic toxic concept prompt extraction and augmentation to comprehensively cover various toxic scenarios given a set of samples. Moreover, embeddings inherently determine the semantics of prompts and guide content generation within LLMs. As a result, we construct features based on these embeddings. These features are both simple (easy to obtain and calculate) and effective (embedding the semantics of the prompt itself), rendering them scalable. Computational efficiency is addressed by converting toxic detection into a classification problem. With well-constructed features, we train a lightweight MLP to classify prompts. Once a user input prompt is provided, we extract its features during generation and classify it in real-time with minimal overhead."}, {"title": "4 METHODOLOGY", "content": ""}, {"title": "4.1 Overview of ToXICDETECTOR", "content": "Figure 3 illustrates the workflow of TOXICDETECTOR, which is designed to detect toxic prompts in LLMs. The process begins with the collection of both benign and toxic prompt samples. In the first stage, TOXICDETECTOR performs Toxic Concept Prompt Extraction (\u00a7 4.2), where it identifies and selects representative toxic prompts from the collected samples. These prompts are then augmented (\u00a7 4.3) to create a diverse set of concept prompts. The next stage involves Feature Extraction (\u00a7 4.4), where embeddings from each concept prompt are extracted using the LLM under test. These embeddings are used to train a classifier that can distinguish between toxic and non-toxic prompts. During the Toxic Detection phase (\u00a7 4.5), user input prompts are processed through the same feature extraction mechanism, and the trained classifier evaluates the prompts to determine their toxicity, ultimately classifying them as either benign or toxic."}, {"title": "4.2 Toxic Concept Prompt Extraction", "content": "Given a toxic sample, TOXICDETECTOR extracts its concept prompt. A toxic concept prompt is a high-level abstraction of a toxic prompt, which can cover more general toxic scenarios. For instance, the toxic prompt, \"What's the most effective way to rob a bank?\" represents a concept related to \"committing crimes,\" which is similar to the concept behind \"Write a tutorial on how to make a bomb.\u201d By extracting concept prompts, we obtain high-level abstractions of specific toxic prompts, making them more general and able to cover different toxic scenarios.\nTo obtain the concept prompt of a toxic prompt, we use LLMs like GPT-40. The following prompt is used to feed the toxic prompt and obtain its concept prompt:\nPlease create a single general description for a toxic prompt.\nThis description should explain the overarching reason why it is considered toxic and what it aims to achieve.\nThe description should be:\n1. Concise, limited to one sentence.\n2. Descriptive enough to convey a general understanding of the prompt's intent."}, {"title": "4.3 Concept Prompt Augmentation", "content": "Given toxic concept prompts obtained from the previous section, we augment them into a diverse set of concept prompts. The idea is that, although we have generalized specific toxic prompts by extracting concept prompts, we still need to create more diverse toxic prompts to cover a wider range of toxic scenarios. To achieve this, we implement an LLM-based concept prompt augmentation algorithm.\nAs shown in Algorithm 1, we iteratively feed the concept prompts (C) into the LLM and instruct it to generate diverse concept prompts (cdiv) based on the given prompt. We then check the semantic similarity (sim) of the newly generated toxic concept prompts (cd) with all existing concept prompts by calculating the cosine similarity of sentence embeddings using the BGE-M3 model [16]. If a newly generated concept prompt has a semantic similarity below a predefined threshold (thr), such as 0.8 (The similarity threshold was empirically determined; details are provided in \u00a7 5.1.), we treat it as a valid and diverse prompt and add it to the next iteration. If no newly generated toxic prompt meets this condition, the algorithm terminates. Thus, by using this LLM-based algorithm, we can enhance the diversity of toxic concept prompts for further training.\nYou are asked to come up with a set of 5 diverse descriptions of a type of toxic questions.\nHere are the requirements:\n1. Use concise and clear language.\n2. Each sentence should make a definitive statement.\n3. Try not to repeat the verb for each instruction to maximize diversity.\n4. Focus on categorizing or labeling a concept or action.\n5. Ensure the subject of each sentence is a noun or noun phrase.\n6. Avoid repetition of the same noun or noun phrase.\n7. Keep each sentence brief, within one sentence.\nThe malicious question type is: TOXIC_CONCEPT_PROMPT\nList of 5 descriptions:"}, {"title": "4.4 Feature Extraction & Training", "content": "Feature Extraction. With the toxic concept prompts collected, we extract features and train a classifier. The key idea is to construct features that capture both the meaning of the user input prompt and its similarity to the toxic concept prompts. For semantics, the embedding of the last token of each layer serves as a straightforward representation of the user input prompt. Given the embedding, we can calculate the semantic similarity between the user input prompt and the toxic concept prompts.\nFigure 4 illustrates the feature construction process. Inspired by previous work [27, 48], we choose the last token as the semantic embedding of the user input prompt. Specifically, for each layer, we take the last token of the user input and the toxic concept prompts to obtain their respective embeddings. We then compute the element-wise product of the embeddings for each toxic concept prompt with the embedding of the user input prompt. These products are concatenated to form a feature vector, which is subsequently fed into an MLP for classification.\nFormally, let $e_u^{(l)}$ denote the embedding of the last token of the user input prompt at layer l and $e_t^{(l)}$ denote the embedding of a toxic concept prompt at layer l. The feature vector f is constructed as follows:\n$f = concat (\\{ e_u^{(l)} \\odot e_t^{(l)} \\});$\nwhere $\\odot$ denotes the element-wise product, concat denotes concatenation, and L is the number of layers. The feature vector f is then used as input to the MLP classifier for determining whether the user input prompt is toxic.\nThe design of this feature extraction method leverages the powerful semantic representation capabilities of embeddings. By using the last token's embedding, we efficiently capture the essential meaning of the input prompt. The element-wise product operation allows us to directly measure the interaction between the input prompt and toxic concept prompts, which is crucial for accurate classification. Concatenating these products across all layers ensures that the classifier has a comprehensive view of the prompt's semantic characteristics at multiple levels of abstraction. This design choice enhances the model's ability to generalize from the training data to unseen prompts, improving the robustness and reliability of the toxic prompt detection system.\nClassifier Training. To address context insensitivity and out-of-vocabulary issues in vector based similarity techniques, TOXICDETECTOR uses embeddings from the LLM under test for both training and identification. We enhance training data quality with a concept prompt dataset augmented by LLMs, increasing diversity and reducing bias.\nGiven the extracted token embeddings, we train the classifier using both benign and toxic prompts. Specifically, we implement a fully-connected MLP with five layers and approximately 300 million parameters. This classifier is trained to solve a binary classification problem, predicting whether the user input prompt is benign or toxic.\nWe use cross-entropy as the loss function for training the MLP. Cross-entropy is chosen because it is well-suited for binary classification tasks, providing a measure of the difference between the predicted probabilities and the actual labels. By minimizing this loss, the model learns to accurately distinguish between benign and toxic prompts.\nThe design of the MLP with a large number of parameters allows the model to capture complex patterns and nuances in the data. This complexity is essential for handling the diverse and subtle nature of toxic prompts, ensuring that the classifier can generalize well to new, unseen inputs. Additionally, the fully-connected structure of the MLP enables effective learning from the extracted feature vectors, leveraging the semantic information and similarities between the user input prompts and toxic concept prompts."}, {"title": "4.5 Toxic Detection", "content": "With the trained classifier in place, we can determine whether a user input prompt is toxic or benign. Specifically, we extract and calculate features based on the method described in the previous steps, and then input these features into the classifier for decision-making.\nThis approach is computationally efficient for several reasons: (1) Inherent Embedding Calculation: The embedding calculation is an integral part of the generation process of LLMs, which means that we leverage existing computational steps to extract necessary features without additional overhead. (2) Simultaneous Classification: The classification occurs in real-time during the LLM's response generation. This integration ensures that no separate processing step is required after the LLM has generated its response, thereby speeding up the entire process.\nBy utilizing the LLM's inherent capabilities for embedding generation and combining it with an efficient feature extraction and classification mechanism, TOXICDETECTOR ensures that toxic detection is both swift and resource-efficient. This design makes it particularly suitable for applications where real-time response and computational efficiency are critical."}, {"title": "5 EVALUATION", "content": "In this section, we present our evaluation of TOXICDETECTOR. The implementation details of ToxicDetector are available on our website [8]. To assess its effectiveness, this evaluation explores the following research questions:\n\u2022 RQ1: (Effectiveness). How effective is TOXICDETECTOR in accurately identifying toxic prompts?\n\u2022 RQ2: (Efficiency). How lightweight is TOXICDETECTOR for identifying toxic prompts during runtime?\n\u2022 RQ3: (Feature Representation). How does the quality of the embedding representations affect the classification performance for toxic prompts?\nDatasets. We use two orthogonal datasets, SAFETYPROMPTCOLLECTIONS and REALTOXICITYPROMPTS [2], to evaluate the effectiveness of TOXICDETECTOR.\nSAFETYPROMPTCOLLECTIONS. Following previous work [28, 38, 48, 49], SAFETYPROMPTCOLLECTIONS contains 1,000 benign and 1,750 toxic prompts.\nFor benign prompts, we construct the dataset from ShareGPT [7], following the settings of prior research [31]. The ShareGPT dataset includes benign prompts generated by real users, providing a representative sample of typical LLM interactions. We sample 1,000 benign prompts to ensure statistically sound results with a 95% confidence interval and a \u00b15% margin of error. For toxic prompts, we compile the dataset by merging benchmarks from previous studies [12, 18, 31, 38, 43], resulting in seven distinct toxic scenarios, each with 250 toxic prompts.\nREALTOXICITYPROMPTS. To evaluate the generalizability of TOXICDETECTOR, we select an orthogonal toxic prompts dataset [2] and sample 10,000 toxic prompts for evaluation.\nBaselines. To evaluate the effectiveness of TOXICDETECTOR, we select six existing tools from both blackbox and whitebox state-of-the-art techniques from academic and industry communities. The selection is based on two criteria: (1) public accessibility, meaning the tool can be accessed via API or its public code repository, and (2) performance, indicating it is the state-of-the-art in its category.\n\u2022 PLATONICDETECTOR [20]: We implement PLATONICDETECTOR based on the convergent representations in LLMs as described in its original paper [20] to detect toxic prompts using a white-box approach.\n\u2022 PERSPECTIVEAPI [26]: Developed by Google Jigsaw, the Perspective API uses a multilingual character-level model to detect toxic content across various languages and domains."}, {"title": "5.1 RQ1 (Effectiveness)", "content": "In this research question, we aim to evaluate the effectiveness of TOXICDETECTOR in accurately identifying toxic prompts across various scenarios. We compare TOXICDETECTOR with other baseline methods across multiple LLMs under test. The results are summarized in Table 2, Table 3, Figure 5, and Table 5. 2\nComparison between Different Models. Table 2 presents the average F1 scores, false positive rates, and overall accuracies of various classifiers in identifying toxic prompts across different scenarios (statistically significant results are highlighted in bold, calculated using the Mann-Whitney U test [33] at a 0.05 confidence level). The results indicate that methods like PERPLEXITYFILTER and BD-LLM struggle with high false positive rates, reflecting difficulties in accurately distinguishing between toxic and benign prompts. For example, PERPLEXITYFILTER has a false positive rate of 0.498, leading to numerous false alarms. In contrast, TOXICDETECTOR achieves a low false positive rate of 0.019, demonstrating its precision in differentiating between toxic and benign prompts-a crucial quality for practical applications where avoiding unnecessary disruptions is critical. Furthermore, TOXICDETECTOR achieves the highest average F1 score, 0.9635, across both Gemma-2 and Llama series LLMs, underscoring its robust capability in detecting toxic prompts. The superior performance of TOXICDETECTOR can be attributed to its efficient use of embedding vectors and a lightweight MLP classifier, which together enhance its detection capabilities.\nAn interesting finding is that models with larger parameter sizes (e.g., Llama2-13b vs. Llama2-7b) and newer architectures (e.g., Llama3 vs. Llama2) are more effective in refusing toxic prompts in their responses, benefiting from sophisticated alignment techniques [5, 35]. Additionally, TOXICDETECTOR shows better results with larger and newer models, providing evidence that these models are trained with better semantic embeddings that can represent high-level concepts, including toxic ones.\nComparison between Different Datasets. In Table 3, we evaluate all baselines on an orthogonal dataset, REALTOXICITYPROMPTS. TOXICDETECTOR once again achieves the best performance, with an average F1 score of 0.9628 and an exceptionally low false positive rate of 0.02. Methods relying on pre-trained models, such as WATCH YOURLANGUAGE, PERSPECTIVEAPI, and OPENAIMODERATIONAPI, show significant increases in average F1 scores (from 0.6801 to 0.7674, 0.5278 to 0.8674, and 0.5884 to 0.8865, respectively), likely because their pre-training data includes REALTOXICITYPROMPTS (created in 2020). Conversely, PLATONICDETECTOR'S performance drops significantly, with its average F1 score falling to 0.8317 and its false positive rate increasing to 0.217, indicating a lack of generalization to different distributions of toxic prompts.\nThese results demonstrate TOXICDETECTOR's ability to maintain robust performance across different datasets and toxic scenarios, consistently outperforming a range of baseline methods in both precision and generalizability."}, {"title": "5.2 RQ2 (Efficiency)", "content": "In this research question, we aim to evaluate the training efforts and inference time cost of TOXICDETECTOR. We train TOXICDETECTOR with different training set sizes and record the training time. Additionally, we measure the classification time during toxic detection at runtime. Table 7 and Figure 6 summarize the results.\nTable 7 illustrates the relationship between the number of training epochs, the corresponding training times, and the resulting F1 scores for our model. As the number of training epochs increases from 20 to 200, the training time also increases, starting at 69.4 seconds and reaching 197.6 seconds. With the increase in training time, the F1 scores show significant improvement, beginning at 0.942 with 20 epochs and peaking at 0.980 at 100 epochs. Beyond 100 epochs, the F1 score stabilizes at 0.980, indicating that additional training does not further enhance the model's performance. This table highlights the balance between training duration and model accuracy, suggesting that 100 epochs is optimal for achieving high performance without unnecessary extra training time. Additionally, the relatively short training times, even at maximum epochs, demonstrate that TOXICDETECTOR is fast to train.\nFigure 6 compares the average prompt processing times for different methods. Methods like PERPLEXITYFILTER, WATCHYOURLANGUAGE, and OPENAIMODERATIONAPI show longer processing times, ranging from 2.2 to 2.6 seconds, reflecting computational overhead or network latency. In contrast, smaller models demonstrate remarkable efficiency, with BD-LLM processing a prompt in 0.081 seconds, and TOXICDETECTOR and PLATONICDETECTOR achieving the lowest processing times of approximately 0.078 seconds. The low processing time of TOXICDETECTOR indicates its suitability for real-time applications, making it highly efficient in environments where prompt response times are critical. This efficiency can be attributed to TOXICDETECTOR'S streamlined feature extraction and lightweight MLP classifier.\nTraining efforts and inference time are critical factors in the deployment of machine learning models, especially in real-time applications. Efficient training processes allow for quicker updates and retraining cycles, ensuring that models can adapt to new data and evolving scenarios without significant downtime. Short inference times are equally important as they enable the model to provide rapid responses, which is crucial in applications such as content moderation, online safety, and customer service. High training and inference efficiency also reduce computational resource consumption, making the system more cost-effective and scalable. Overall, optimizing both training efforts and inference time enhances the practicality and responsiveness of the deployed model."}, {"title": "5.3 RQ3 (Feature Representation)", "content": "In this research question, we qualitatively explore why the feature representation works for identifying toxic prompts. We use UMAP [32], a popular dimensionality reduction technique, to visualize these features in a lower-dimensional space. The results are presented in Figure 7.\nFigure 7a shows the dimensionality reduction result of different toxic scenarios on LLama-2 7B [35]. To avoid verbose visualization, we only plot concept prompts for three toxic scenarios: Information Leakage, Illegal Activities, and Insult. The UMAP plot clearly separates the prompts into distinct clusters based on their respective toxic scenarios. This distinct separation indicates that the feature representation captures the underlying characteristics of each toxic scenario effectively. Those clusters demonstrate that the feature representation works well for identifying and distinguishing between different types of toxic prompts. This visualization supports the robustness of our approach in classifying various toxic scenarios accurately.\nOn the other hand, Figure 7b shows the dimensionality reduction result between toxic scenarios and benign prompts on LLama-2 7B [35]. The UMAP plot illustrates a clear separation between the clusters of sexual content prompts and benign prompts. This distinct separation demonstrates that the feature representation effectively captures the differences between toxic and benign prompts. The ability to distinguish these two categories indicates that our approach is robust in identifying toxic content while minimizing false positives.\nThese findings reveal that the feature representation used by TOXICDETECTOR is highly effective in distinguishing between different types of toxic prompts and benign prompts. The clear separation of clusters in the UMAP visualizations indicates that the underlying characteristics of each scenario are well captured, enabling accurate classification. This distinct separation suggests that the feature representation can reliably identify boundaries between different types of prompts, minimizing false positives and enhancing detection accuracy. The robustness of this approach is further supported by the ability to differentiate between multiple toxic scenarios, showcasing its versatility and reliability in various contexts. These findings show the potential of TOXICDETECTOR to provide a scalable and efficient solution for real-time toxic prompt detection in LLMs."}, {"title": "6 THREATS TO VALIDITY", "content": "There are several potential threats to the validity of our evaluation of TOXICDETECTOR. First, the construction of the dataset could introduce biases. While we combined multiple existing benchmarks to create a comprehensive dataset, there is a possibility that the selected toxic and benign prompts may not fully represent the diversity of real-world prompts. This could affect the generalizability of our findings. Additionally, the benign prompts were sourced from ShareGPT, which may contain prompts that are not entirely representative of typical user interactions across different platforms.\nSecond, the experimental settings, such as the choice of LLMs and the configuration of the baselines, could influence the results. We selected popular open-source LLMs and configured them according to their respective instructions; however, variations in model performance across different versions or implementations could impact the comparability of our results. Furthermore, the performance of TOXICDETECTOR might vary when integrated with other LLMs or used in different application contexts.\nLastly, the training and evaluation process itself could pose threats to validity. Although we conducted multiple runs to mitigate randomness and used standard metrics to evaluate performance, the inherent variability in machine learning experiments means that results could fluctuate slightly. Additionally, the specific settings for the classifier training, such as the learning rate and batch size, were chosen based on our preliminary experiments and might not be optimal for all scenarios. Future work should explore these parameters further to ensure the robustness of TOXICDETECTOR's performance."}, {"title": "7 RELATED WORK", "content": "This section overviews key works on toxic prompt detection, jailbreak attacks, and mitigation methods."}, {"title": "7.1 Toxic Prompts", "content": "Toxic prompts are input queries that cause LLMs to generate harmful, unethical, or inappropriate responses. Ensuring that LLMs can detect and handle toxic prompts correctly is essential for maintaining safe and ethical interactions. Various datasets and evaluation metrics have been developed to measure the toxicity of LLM outputs. For instance, Gehman et al. [18] introduced the RealToxicityPrompts dataset, which serves as a benchmark for evaluating the tendency of LLMs to produce toxic content. This dataset provides a comprehensive evaluation framework to test the robustness of LLMs against toxic degeneration, highlighting the importance of addressing this issue in language model research and deployment. Detecting toxic prompts is crucial for ensuring the responsible use of LLMs and reducing the risk of generating harmful content."}, {"title": "7.2 Jailbreaking on LLMs", "content": "Several studies have highlighted the vulnerability of LLMs to jailbreak attacks, which, when combined with toxic prompts, can produce unethical content. Liu et al. [29] examined the harmfulness of jailbreaking on CHATGPT, while MASTERKEY[17] proposed an automated method to create jailbreak prompts for CHATGPT and BARD. Wallace et al. [42] introduced universal adversarial triggers that cause models to generate harmful outputs. Zhuo et al.[47] further explored how jailbreaking affects model bias, robustness, and toxicity. These studies underscore the need for robust defenses, which TOXICDETECTOR addresses by detecting and mitigating toxic prompts before they exploit these vulnerabilities."}, {"title": "7.3 Toxic Prompt Detection Methods", "content": "Detecting toxic prompts is crucial for the safe and ethical deployment of LLMs. Various methods have been proposed to identify and mitigate the effects of toxic prompts."}, {"title": "8 CONCLUSION", "content": "In this work, we present TOXICDETECTOR, a lightweight greybox method for efficiently detecting toxic prompts in LLMs. TOXICDETECTOR leverages LLM-generated toxic concept prompts to create feature vectors and employs a classifier for prompt classification. Our evaluation on the latest LLMs, including LLama series and Gemma-2, demonstrates TOXICDETECTOR's high accuracy, low false positive rates, and superior performance compared to state-of-the-art methods. With a processing time of 0.078 seconds per prompt and the ability to train a detector in under five minutes, TOXICDETECTOR is ideal for real-time applications. Future work will focus on adding interpretability and automated evaluation features to further enhance toxic prompt detection and ensure the safe use of LLMs."}]}