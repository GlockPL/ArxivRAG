{"title": "Zero-Shot Dense Retrieval with Embeddings from Relevance Feedback", "authors": ["Nour Jedidi", "Yung-Sung Chuang", "Leslie Shing", "James Glass"], "abstract": "Building effective dense retrieval systems remains difficult when relevance supervision is not available. Recent work has looked to overcome this challenge by using a Large Language Model (LLM) to generate hypothetical documents that can be used to find the closest real document. However, this approach relies solely on the LLM to have domain-specific knowledge relevant to the query, which may not be practical. Furthermore, generating hypothetical documents can be inefficient as it requires the LLM to generate a large number of tokens for each query. To address these challenges, we introduce Real Document Embeddings from Relevance Feedback (ReDE-RF). Inspired by relevance feedback, ReDE-RF proposes to re-frame hypothetical document generation as a relevance estimation task, using an LLM to select which documents should be used for nearest neighbor search. Through this re-framing, the LLM no longer needs domain-specific knowledge but only needs to judge what is relevant. Additionally, relevance estimation only requires the LLM to output a single token, thereby improving search latency. Our experiments show that ReDE-RF consistently surpasses state-of-the-art zero-shot dense retrieval methods across a wide range of low-resource retrieval datasets while also making significant improvements in latency per-query.", "sections": [{"title": "1 Introduction", "content": "Information Retrieval (IR) aims to identify relevant documents from a large collection of text given a user's information needs. With recent advancements in transformer-based language models, dense retrieval techniques (Karpukhin et al., 2020; Xiong et al., 2020; Qu et al., 2020) \u2013 which map queries and documents to a shared semantic embedding space that captures relevance patterns \u2013 have demonstrated significant success compared to traditional retrieval approaches based on exact-term matching, such as BM25 (Robertson et al., 2009). Despite great performance, it remains difficult to build dense retrieval systems in settings that do not have large amounts of dedicated training data (Thakur et al., 2021).\nRecent work has explored improving unsupervised dense retrieval systems, such as Contriever (Izacard et al., 2021), by leveraging Large Language Models (LLMs) to enrich the query embedding for nearest neighbor search. HyDE (Gao et al., 2023), for example, prompts an LLM to generate hypothetical documents that are used to search for the closest real documents. This casts dense retrieval as a document similarity task, which aligns well with pre-training techniques of unsupervised dense retrieval methods (Gao et al., 2023; Izacard et al., 2021). While HyDE demonstrates strong zero-shot\u00b9 performance, it is highly reliant on the LLMs' parametric knowledge, which can be a barrier in deployment for out-of-domain corpora settings (e.g., proprietary documents). A potential solution could be to leverage top-retrieved documents as context when prompting the LLM to generate hypothetical documents (Shen et al., 2024). However, this increases search latency due to the longer input context. Additionally, even with better prompt context, hypothetical documents generated by LLMs remain susceptible to common issues such as overlooking or ignoring the provided information (Zhou et al., 2023; Shi et al., 2023; Liu et al., 2024; Simhi et al., 2024).\nTo address these challenges, we propose re-framing the task as relevance estimation rather than hypothetical document generation. Drawing inspiration from relevance feedback, we introduce Real Document Embeddings from Relevance Feedback (ReDE-RF). ReDE-RF first retrieves an initial set of documents from a fully unsupervised"}, {"title": "2 Methodology", "content": "In this section, we first provide a brief overview of HyDE, a method that performs zero-shot dense retrieval through the generation of hypothetical documents. We then describe how we leverage LLMs to perform relevance feedback, a critical component that allows us overcome the challenges of hypothetical documents. Lastly, we describe how we use relevance feedback outputs to update our query representation."}, {"title": "2.1 Preliminaries: HyDE", "content": "The main challenge in zero-shot dense retrieval is learning query and document embedding functions that capture relevance when human-annotated relevance scores are not available. HyDE (Gao et al., 2023) seeks to overcome this by re-casting the task as a document-document similarity task.\nGiven a query, q, HyDE first samples N hypothetical documents {d\u2081,...,dN} from a generative LLM - denoted by LLMDocGen \u2013 via zero-shot prompting:\ndi = LLMDocGen (q), 1 \u2264 i \u2264 N\nwhere LLMDocGen(q) denotes the stochastic output of LLMDocGen given q. One could optionally provide the top-k documents D = {d1,d2,...,dk} from an unsupervised retrieval system (e.g., BM25) as context for LLMDocGen:\ndi = LLMDocGen(D,q), 1 \u2264 i \u2264 N\nWe refer to this as HyDEPRF2.\nSubsequently, the hypothetical documents, {d1,..., dN}, are encoded by an unsupervised contrastive encoder, f, and averaged to generate an updated query embedding, \u00dbqHYDE. When generating \u00dbqHYDE, the original query is also considered. More formally,\n\u00dbqHYDE = 1/(N+1) (f(q) + \u03a3f(di))\n\u00dbqHYDE is then searched against the corpus embeddings to retrieve the most similar real documents. Through this two-step process, zero-shot dense retrieval moves from directly modeling query-document similarity to modeling document-document similarity. Without the need for relevance supervision, HyDE is able to out-perform state-of-the-art unsupervised dense retrievers."}, {"title": "2.2 ReDE-RF", "content": "Similar to HyDE, ReDE-RF models zero-shot dense retrieval as a document similarity task. Unlike HyDE, the LLM is leveraged for relevance feedback rather than document generation. ReDE-RF has two key components: (1) relevance feedback with LLMs and (2) updating the query representation. These two components are described in this subsection and illustrated in Figure 1."}, {"title": "2.2.1 Relevance Feedback with LLMs", "content": "Given a query, we first retrieve the top-k documents, D, from an unsupervised retrieval system. Subsequently, we employ zero-shot prompting to score the relevance of given document, di, to the query. Based on the prompt, a generative LLM, denoted by LLMRel-Judge, returns a list of the k* documents classified as relevant Dr = {dr1, dr2,..., drk* }, where ri \u2208 {1,2,..., k*} for 1 \u2264 i \u2264 k*. With the recent success of using LLMs for patching up missing relevance judgements, we use a modified version of the prompt from Upadhyay et al. (2024), which is shown in Figure 2."}, {"title": "2.2.2 Updating the Query Representation", "content": "Given the list of documents, Dr, that LLMRel-Judge deems relevant, we follow Equation 3 to update the query embedding. One key difference between ReDE-RF and HyDE is that f (dri) \u2013 where 1 < i \u2264 k* \u2013 already exists as it is the embedding of a real document that was pre-computed offline. As such, we denote CE[dri] as the action of retrieving the embedding for a specified document, dr, from the set of corpus embeddings CE. Thus, to update our query:\n\u00dbqREDE = 1/k*+1 (f(q) + \u03a3CE[dri])\nIf no relevant documents are found in the top-k, i.e., Dr = \u00d8, a simple option could be to default to the unsupervised contrastive encoder, and just return f(q). However, in these cases we also argue defaulting to hypothetical document generation can be a viable option as it would only hurt latency for difficult queries that the initial retrieval struggles"}, {"title": "3 Experiments", "content": "3.1 Setup\nImplementation ReDE-RF requires an instruction-tuned LLM and a dense retriever. For the instruction-tuned LLM (i.e., LLMRel-Judge) we leverage Mistral-7B-Instruct-v0.2 (Jiang et al., 2023) and for dense retrieval we use the unsupervised Contriever (Izacard et al., 2021). When prompting LLMRel-Judge for relevance feedback, we truncate the input document to at most 128 tokens and generate a relevance score by applying a softmax on the logits of the \"1\" and \"0\" tokens as shown in Nogueira et al. (2020). Only documents that LLMRel-Judge scores as '1' are used for updating the query representation. In cases in which D\u2081 = \u00d8, we consider two defaults: Contriever and HyDEPRF.\nTo generate an initial document set for LLMRel-Judge, we retrieve the top-20 documents from a hybrid, sparse-dense, retrieval model (BM25 + Contriever)3. Retrieval experiments were performed with Pyserini (Lin et al., 2021) and LLM implementations in HuggingFace (Wolf et al., 2019).\nDatasets In our experiments, we evaluate on two web search datasets: TREC DL19 (Craswell et al., 2020) and TREC DL20 (Craswell et al., 2021). We also evaluate on seven low-resource retrieval datasets from BEIR (Thakur et al., 2021). The tasks include news retrieval (TREC-News, Robust04), financial question answering (FiQA), entity retrieval (DBpedia), biomedical IR (TREC-Covid, NFCorpus), and fact checking (SciFact). For metrics, we report NDCG@10, the offical metric for the TREC and BEIR datasets.\nBaselines We first compare ReDE-RF to unsupervised retrievers that do not leverage LLMs: BM25, Contriever, and a hybrid retrieval model (BM25 + Contriever). We also include a pseudo-relevance feedback (PRF) baseline which averages all top-k initially retrieved documents to update the query representation: ContrieverAvgPRF (Li et al., 2022). ContrieverAvgPRF is equivalent to ReDE-RF if all top-k retrieved documents are considered relevant. For ContrieverAvgPRF, the initially retrieved docu-"}, {"title": "3.2 Results on Benchmarks", "content": "Table 1 presents the evaluation results on the TREC and BEIR datasets and reveals several insights:\n(1) ReDE-RF outperforms ContrieverAvgPRF, which uses all initially retrieved documents to enhance the query embeddings. This exemplifies that simply leveraging the top-k retrieved documents is not sufficient, and demonstrates the value of leveraging an LLM to filter out non-relevant documents.\n(2) Comparing ReDE-RF to HyDE, we find that using real documents for zero-shot dense retrieval consistently outperforms hypothetical documents based solely on LLM knowledge (i.e., without top documents as context).\n(3) When incorporating corpus text as a guide for HyDE (i.e., HyDEPRF), the performance gap between ReDE-RF and HyDE decreases. However, ReDE-RF still provides substantial improvements in low-resource domains (6.0% when defaulting to HYDEPRF and 4.6% when defaulting to Contriever). For high-resource domains \u2013 DL19 and DL20 \u2013 HYDEPRF yields better results, which we hypothesize is due to the advantages of combining the LLM's parametric knowledge with corpus knowledge in domains the LLM is well-versed.\n(4) As the performance of ReDE-RF (Default: HyDEPRF) is equivalent to HyDEPRF for queries that ReDE-RF defaults, the performance gains of ReDE-RF in low-resource domains can be attributed to the benefits of doing nearest-neighbor search in the real document embedding space \u2013 that LLMRel-Judge deemed relevant versus the hypothetical document space generated by LLMDocGen"}, {"title": "3.3 Comparing Latencies", "content": "In Figure 3, we empirically compare the average query latency for HyDEPRF, HyDE and ReDE-RF. We also include HyDEPRF with 10 initially retrieved documents \u2013 HyDEPRF (10 Docs) as an additional comparison\u2074. All experiments were run on one A100 GPU and measure the time from input query to retrieval of results.\nComparing the latencies across systems, we find that ReDE-RF (Default: Contriever) consistently reduces latency compared to HyDE and HyDEPRF. Specifically, on average, ReDE-RF (Default: Contriever) is 3.8\u00d7 faster than HyDE and 6.7 to 9.7\u00d7 faster than HyDEPRF, depending on whether 10 or 20 documents are used as context. This finding is true even when ReDE-RF defaults to HyDEPRF, improving latency by 2.4\u00d7 compared to HyDE and and 4.1 to 5.9\u00d7 compared to HyDEPRF. These results confirm our hypothesis that leveraging hypothetical document generation for every query introduces unnecessary latency costs and that it is possible to improve performance while also im-"}, {"title": "4 Ablation Study on ReDE-RF", "content": "There are many design decisions that one can make when implementing ReDE-RF. In this section, we study the effects of these different choices on ReDE-RF's performance. As some approaches may default more frequently than others, the result will be affected by how strong the chosen default is. Thus, in the ablation study, we choose no default: return no results for the query if k* = 0 (yielding an NDCG@10 of 0) to limit our study solely to the relevance feedback portion of ReDE-RF. We refer to this as ReDE-RF (No Default.)\nEffect of Initial Retrieval Method How does the initial retriever effect ReDE-RF accuracy? The results for this experiment are in Table 2. Feeding documents using hybrid retrieval consistently improves results compared to only sparse or only dense retrieval. As ReDE-RF is highly dependent on the initial retrieval, these results suggest that leveraging multiple unsupervised retrievers can improve performance."}, {"title": "5 Can we Distill ReDE-RF?", "content": "As noted in Section 3.3, ReDE-RF improves latency per query as compared to HyDE and HyDEPRF. However, if ReDE-RF is implemented with HyDEPRF as its default, it still occasionally needs to default to hypothetical document generation if no relevant documents are found, thus making it costly for certain queries. In this section, we explore if we can improve the latency of ReDE-RF without trading off accuracy. With this in mind, we aim to answer two questions: 1) Can we distill ReDE-RF's performance to Contriever (DistillReDE)? 2) Can using DistillReDE in tandem with ReDE-RF remove the need for defaulting to HyDEPRF while matching the performance of ReDE-RF (Default: HyDEPRF)?\nDistilling ReDE-RF We aim to explore whether ReDE-RF can be distilled to a student Contriever model, DistillReDE. Since ReDE-RF's embeddings are an average of the Contriever document embeddings, one advantage is that the student model can be trained without the need to re-index the corpus. To generate the training set, we first run ReDE-RF offline using LLM-generated synthetic queries and treat the corresponding ReDE-RF embeddings as the target representation. For training, we follow the framework from Pimpalkhute et al. (2024): we optimize a combination of MSE loss and contrastive loss with in-batch random negatives. See Appendix F for training details.\nThe results, shown in Figure 4, indicate that DistillReDE can achieve significant improvements on Contriever, narrowing its performance gap with ReDE-RF (Default: HyDEPRF) while removing the need for LLMs at inference time.\nReDE-RF with DistillReDE We next explore the possible advantages of leveraging DistillReDE"}, {"title": "6 ReDE-RF vs. Pointwise Reranking", "content": "The LLMRel-Judge component of ReDE-RF (discussed in 2.2.1) is similar to LLM-based pointwise re-rankers (Zhuang et al., 2024c). In this subsection, we ask: What benefits do we achieve by feeding relevant documents to improve the query representation \u2013 as described in 2.2.2 \u2013 versus simply re-ordering the initial retrieval based on the logits from LLMRel-Judge? To answer this, we focus on comparing pointwise re-ranking to ReDE-RF (Default: HyDEPRF) in equal settings: Both systems have access to the top-20 passages from a hybrid (BM25 + Contriever) retriever and employ the same prompt as shown in Figure 2. Note, for the rest of this section we refer to ReDE-RF (Default: HYDEPRF) as ReDE-RF.\nIn Table 7 we present the results of this experiment across three backbone LLMs. Based on the results, we can make the following observations: (1) When comparing NDCG@10, ReDE-RF and pointwise re-ranking are generally on par \u2013 outside of Llama-3.1-8B-I on DL19. (2) ReDE-RF consistently outperforms pointwise re-ranking in terms of NDCG@20 by large amounts. This demonstrates that ReDE-RF's improvements extend beyond the top-ranked results and is not confined to the initial retrieval. (3) Besides the evaluation on the TREC News dataset \u2013 where it appears pointwise re-ranking is not well calibrated \u2013 re-ranking and ReDE-RF are generally complementary. ReDE-RF + PR outperforms Hybrid + PR eight out of nine times (six out of six if excluding TREC News) and outperforms ReDE-RF five out of nine times (five out of six if excluding TREC News)."}, {"title": "7 Related Work", "content": "Query Expansion with LLMS GAR (Mao et al., 2021) was among the first methods to demonstrate the effectiveness of LLMs for query expansion by training an LLM to expand queries through the generation of relevant contexts, such as the target answer or answer sentence. Recent work has looked into leveraging LLMs to generate query expansions via zero or few-shot prompting. This has been explored in contexts where the LLM generates hypothetical documents that can be used to augment the query (Gao et al., 2023; Wang et al., 2023b; Jagerman et al., 2023; Lei et al., 2024; Mackie et al., 2023; Shen et al., 2024). While some of these works generate hypothetical texts given real documents (e.g., Jagerman et al. (2023); Lei et al. (2024); Shen et al. (2024)) they still rely on LLM-generated content to augment the query.\nZero-Shot Dense Retrieval With advancements in deep learning, IR systems moved away from representations based on exact-term matching to dense vector representations generated from transformer language models (Lin et al., 2022), such as BERT (Devlin et al., 2019). However, learning these representations typically requires large, labeled datasets. As such, researchers have looked into methods for learning dense representations without manually labeled data through techniques such as using synthetic data (Izacard et al., 2021; Wang et al., 2023a; Sachan et al., 2023; Lee et al., 2024; Dai et al., 2022), addressing architecture limitations to improve zero-shot decoder-LLM embeddings (Springer et al., 2024), or leveraging LLMs to generate outputs that can be used to improve representations for zero-shot dense retrieval (Gao et al., 2023; Zhuang et al., 2024b)."}, {"title": "8 Conclusion", "content": "We introduce ReDE-RF, a zero-shot dense retrieval method that addresses key challenges associated with approaches that rely entirely on hypothetical document generation. Through extensive experiments, we show that ReDE-RF improves upon state-of-the-art zero-shot dense retrieval approaches in low-resource domains, while also lowering latency compared to techniques that rely only on hypothetical document generation. Further analysis shows that ReDE-RF can be easily distilled to a smaller, more efficient unsupervised dense retriever, DistillReDE, removing any reliance on LLMs at inference time. In summary, ReDE-RF presents an approach that achieves the benefits of casting zero-shot dense retrieval as a document similarity task while being more efficient and domain-agnostic."}, {"title": "Limitations", "content": "A limitation of ReDE-RF is its reliance on retrieved results from first-stage retrievers. If an initial retriever provides a poor set of results, performance gains will not be as apparent as no relevant documents can be used to update the query embedding. This in turn makes ReDE-RF equivalent to Contriever or HyDE, depending on what default the user leverages. How to make ReDE-RF less reliant on retrieved results from unsupervised first-stage retrievers is a question worth exploring in future work. Another simple improvement could be leveraging a rules-based approach that keeps assessing retrieved documents if none of the top results are deemed relevant. We do note that this would likely increase latency.\nAnother limitation is that while ReDE-RF seeks to minimize reliance on LLM-generated outputs, it does still depend on an LLM to be accurate in its relevance feedback. If the LLM provides inaccurate relevance assessments during the relevance feedback stage, it can further harm the query representation. Lastly, while ReDE-RF improves latency compared to previous approaches based on hypothetical document generation, the latency is still slower than approaches that do not rely on LLMs at inference time. However, we demonstrated the potential for doing offline training as a way to mitigate this. Additionally, as LLMs advance, we may see improvements in the efficiency of LLM inference. Techniques such as flash-attention (Dao et al., 2022), can also significantly decrease the inference time of LLMs."}, {"title": "Ethics Statement", "content": "While the ultimate goal of our work is to minimize reliance on LLM generated output, we do recognize that our system does still rely on LLMs, which means that there is a risk that the LLM can produce biased, harmful, or offensive output. To mitigate this, we limit our LLM to only generate one token, which we hope can eliminate this risk. Additionally, our dense retrieval system is based on pre-trained language models which can potentially produce retrieval results that contain human biases.\nOur research solely uses publicly available datasets, and no personal information is collected. All datasets and models are used in accordance with its intended use and licenses. Our method is designed to improve the performance of information retrieval systems in settings in which there"}, {"title": "A Dataset Details", "content": "We show the number of test queries for each dataset used to evaluate ReDE-RF in Table 10."}, {"title": "B Model Details", "content": "\u2022 Mistral-7B-Instruct-v.02: A 7B parameter model that is instruction fine-tuned.\n\u2022 contriever: Based on bert-base-uncased which has 110M parameters.\n\u2022 Mixtral-8x7B-Instruct-v0.1: A 8x7B parameter model (47B total parameters) that is instruction fine-tuned.\n\u2022 gemma-2-2b-it: A 2B parameter model that is instruction fine-tuned."}, {"title": "C Results Across Multiple Runs", "content": "Due to the randomness of sampling hypothetical documents from an LLM, we run HyDE, HyDEPRF, and ReDE-RF (Default: HyDEPRF) three times. In Table 8 we report the mean and standard deviation of NDCG@10 across the runs for all TREC and BEIR datasets."}, {"title": "D HyDE and HyDEPRF Implementation", "content": "D.1 Generation Details\nTo re-implement HyDE and HyDEPRF with Mistral-7B-Instruct we follow the same parameters that were mentioned in the original paper (Gao et al., 2023) and in the provided codebase. In particular, we sample eight hypothetical documents from Mistral-7B-Instruct with temperature of 0.7 and allow up to 512 maximum new generation tokens per hypothetical document."}, {"title": "D.2 HyDEPRF with less in-context documents", "content": "We explore how less initially retrieved results impact performance of HyDEPRF. For results, see Table 9."}, {"title": "D.3 Prompts", "content": "For HyDE (Gao et al., 2023), we leverage the same prompts from the original implementation. For HyDEPRF, we follow the format of the HyDE prompts and provide context following the format in Q2D/PRF from Jagerman et al. (2023)."}, {"title": "D.3.1 HyDE Prompts", "content": "TREC DL19 and DL20\nPlease write a passage to answer the ques-tion.\nQuestion: {}\nPassage:\nSciFact\nPlease write a scientific paper passage tosupport/refute the claim.\nClaim: {}\nPassage:\nTREC Covid and NFCorpus\nPlease write a scientific paper passage toanswer the question.\nQuestion: {}\nPassage:\nFIQA"}, {"title": "D.3.2 HyDEPRF Prompts", "content": "TREC DL19 and DL20\nPlease write a passage to answer the question based on the context:\nContext:\n{}\nQuestion: {}\nPassage:"}, {"title": "E Hybrid Retrieval Implementation Details", "content": "We use the hybrid retrieval implementation from Pyserini, which scores the document by a weighted average of the sparse retrieval and dense retrieval scores. We implement using the default parameters. See here for more details."}, {"title": "F DistillReDE Training Details", "content": "To train DistillReDE, we first need synthetic queries given our corpus. We leverage the filtered set of 10K synthetic queries provided by Jeronymo et al. (2023) which were generated using GPT-J (Wang and Komatsuzaki, 2021). Then, we run ReDE-RF on these synthetic queries and store the ReDE-RF embedding for each query. If ReDE-RF finds no relevant documents for a given query, we remove that query from our training set.\nTo train DistillReDE, following Pimpalkhute et al. (2024), we optimize the following objective:\nLDistillReDE = 0.5LMSE(\u00fbqReDE, f(q)) + 0.5LCont\nwhere LCont is a contrastive objective (Karpukhin et al., 2020):\nL(qi, ReDE, i qReDE, i, 1,1,..., ReDE,i,n) = log (e^sim(qi, q ReDE,i) / (e^sim(qi, q ReDE,i)+\u03a3e^sim(qi, q ReDE,i,j)))\nwhere given a query, q, we have one positive ReDE-RF embedding \u00db qReDE. and n negative ReDE-RF embeddings (qReDE,i,1, ..., qReDE,i,n).\nWe train with in-batch negatives using a training batch size of 256 and use the Adam optimizer (Diederik, 2014) with a learning rate of 5e-5."}, {"title": "G ReDE-RF Prompt Varations", "content": "Below are the prompts studied in the prompt variations subsection of Section 4.\npointwise.yes_no (Zhuang et al., 2024c)\nPassage: {}\nQuery: {}\nDoes the passage answer the query? Answer 'Yes' or 'No'.\nRG-YN (Zhuang et al., 2024a)"}]}