{"title": "Offline Reinforcement Learning and Sequence Modeling for Downlink Link Adaptation", "authors": ["Samuele Peri", "Alessio Russo", "Gabor Fodor", "Pablo Soldati"], "abstract": "Contemporary radio access networks employ link adaption (LA) algorithms to optimize the modulation and coding schemes to adapt to the prevailing propagation conditions and are near-optimal in terms of the achieved spectral efficiency. LA is a challenging task in the presence of mobility, fast fading, and imperfect channel quality information and limited knowledge of the receiver characteristics at the transmitter, which render model-based LA algorithms complex and suboptimal. Model-based LA is especially difficult as connected user equipment devices become increasingly heterogeneous in terms of receiver capabilities, antenna configurations and hardware characteristics. Recognizing these difficulties, previous works have proposed reinforcement learning (RL) for LA, which faces deployment difficulties due to their potential negative impacts on live performance. To address this challenge, this paper considers offline RL to learn LA policies from data acquired in live networks with minimal or no intrusive effects on the network operation. We propose three LA designs based on batch-constrained deep Q-learning, conservative Q-learning, and decision transformers, showing that offline RL algorithms can achieve performance of state-of-the-art online RL methods when data is collected with a proper behavioral policy.", "sections": [{"title": "I. INTRODUCTION", "content": "Modern radio access and wireless networks - such as the 5th Generation (5G) New Radio (NR) systems \u2013employ model-based link adaptation (LA) algorithms to adapt the modulation and coding scheme (MCS) to time- and frequency-varying channel conditions to maintain high spectral efficiency [1]-[5]. Both in time division duplexing (TDD) and frequency division duplexing (FDD) systems, LA in the downlink (DL) relies on channel state information (CSI) reports provided by the user equipment (UE) that include channel quality indicator (CQI), a rank indicator, and a precoder matrix indicator [6].\nAs CSI reports may be inaccurate and rapidly age due to variations in channel and interference conditions, an outer loop link adaptation (OLLA) scheme is typically adopted to adjust signal-to-interference-plus-noise ratio (SINR) estimates inferred from inaccurate or aged CQI to adhere to block error rate (BLER) target based on the hybrid automatic repeat request (HARQ) feedback [1]. Nonetheless, user mobility, fast fading, limited receiver-side information at the transmitter, as well as the heterogeneity of UEs population in terms of receiver capabilities and hardware characteristics, radio resource management (RRM) problems in radio access networks (RANs), like LA, complex control tasks to model [7], [8].\nRecognizing the challenges faced by model-based approaches, machine learning (ML) methods, such as reinforcement learning (RL), have recently gained attention to address the complex control problems in RANs [9], [10], where time- and frequency-varying channel conditions often make rule-based algorithms suboptimal. The ability of RL to learn directly from experienced behavior and to adapt to changes in the environment makes it appealing to replace traditional rule-based RAN functionalities with models learned from data [8], [11]. Several works have applied this approach to LA, starting with tabular [12] and bandit [13] approaches, adaptations of OLLA based on Q-learning [7], and deep Q-network (DQN)-based designs intended to replace OLLA [8]. However, training RL agents in live networks requires algorithms to explore the state-action space in order to learn an optimal policy, at the cost of taking actions that may be suboptimal and temporarily affect performance in the live networks [8], [14].\nTo address this adverse effect of RL, recent works have proposed offline RL, which aims at learning an optimal policy from a static dataset of transitions produced by a behavioral policy, thus avoiding direct interactions with the environmental while training [15]\u2013[19]. The inability to improve the training dataset during training binds the quality and the behavior of the learned policy to the quality and state-action coverage of the available data. As such, distribution shift and out-of-distribution behavior, that is, a mismatch between the training and testing trajectories, can render the learned policy ineffective. Thus employing offline RL to replace complex real-time RRM control functions in RANs, such as LA, is non-trivial and presents two main challenges: Firstly, the ability of an offline RL design to achieve, when trained with proper data, optimal performance, e.g., comparable with an policy learned online. Secondly, the design a non-invasive behavioral policy to collect rich data in live networks without affecting performance.\nThis paper focuses on the first challenge, focusing on offline RL for LA in 5G networks. To this end, we consider a trained DQN model as behavioral policy for data collection and we present three LA designs based on batch-constrained deep Q-learning (BCQ) [20], conservative Q-learning (CQL) [21], and decision transformer (DT) [22]. While for BCQ and CQL we model the LA problem as a partially observable Markov decision process (MDP), for DT we consider a sequence modeling approach, where a LA action is inferred using a sequence of historical states, actions and rewards, allowing to optimize longer-term rewards, such as average throughput and spectral efficiency. Our results show that offline RL algorithms can achieve performance of state-of-the-art online RL when data is collected with a proper behavioral policy."}, {"title": "II. LINK ADAPTATION AS AN RL PROBLEM", "content": "In this section, we explore how Link Adaptation (LA) can be framed as an RL problem. We begin by presenting our MDP formulation of LA. Next, we briefly discuss how offline RL methods can be applied to solve this MDP formulation. Finally, we highlight the limitations of this approach and explain why it is necessary to consider sequential modeling.\na) MDP formulation and RL: LA can be modeled as an episodic MDP M. An episode (or trajectory) $\\tau = (s_0, a_0, r_0, ..., s_t, a_\\tau, r_\\tau)$ corresponds to the lifespan of a UE's single packet. Therefore, the length T of each episode is bounded, with at-most five steps (corresponding to a first packet transmission and, in case of failure(s), up to four re-transmissions).\n\u2022 The state $s_t \\in \\mathbb{R}^n$ is a vector of semi-static information, characterizing the serving cell, two interfering cells, and UE design, as well as dynamic information including radio measurements, channel state information, HARQ feedback, and more 1.\n\u2022 The action $a_t \\in \\{0,..., 27\\}$ takes an integer, representing one of 28 MCS indices for data transmissions, as defined by the 5G specifications [6]. When dealing with a re-transmission, that is t > 1, the simulator maps the agent's action to an MCS index in the interval $\\{28, ..., 31\\}$, as per 5G specifications.\n\u2022 The reward signal $r_t = r(s_t, a_t)$ is function of the spectral efficiency (SE) of a packet, and takes into account the transmission count of a packet.\nAdditionally, as the UE's HARQ process supports multiple active packets at any given time, multiple UE's episodes may overlap in time (see example in Fig. 1). This MDP modeling enables a RL agent to learn from experiences generated by any User Equipment UE within the network. Training data samples are represented by state-action-reward tuples $(s_t, a_t, r_t)$, which allows for the evaluation and training of value-based and actor-critic methods [23]. Consequently, this formulation is particularly well-suited for use with offline RL, as it can exploit"}, {"title": "III. SEQUENCE MODELING FOR LINK ADAPTATION", "content": "In this section, we introduce a revised problem formulation that is better suited for a sequence modeling framework. We then outline our contributions in adapting the original DT design [22] to the LA problem.\nA. Reinforcement Learning via Supervised Learning\nSequence modeling involves training auto-regressive models on sequential data to capture patterns and make future predictions. Since the main goal in RL is to produce a sequence of actions leading to high cumulative rewards, we can re-frame the RL problem in terms of sequence modeling.\nBy framing RL as a sequence modeling problem, we can better capture the time-varying interactions in the underlying process. This approach simplifies various design decisions and leverages the strengths of sequence models in handling temporal dependencies.\nWe consider the reinforcement learning via supervised learning (RvS) learning approach for offline RL considered in [27]. This approach takes as input a dataset $\\mathcal{D} = \\{\\tau\\}$ of trajectories of experiences and outputs an outcome-conditioned policy $\\pi_\\theta(a_t | s_t, w)$ that optimizes\n$\\max_{\\theta} E_{\\tau \\sim \\mathcal{D}} [ \\sum_{t=0}^{T_\\tau-1} E_{w \\sim f(w|\\tau_t)} [ln\\pi_\\rho(a_t | s_t,w)] ]$,\nwhere $\\tau_t = \\{s_t, a_t, r_t, s_{t+1},...\\}$ denotes the trajectory starting at time t, and $f(w|\\tau)$ denotes the distribution over outcomes w that occur in a trajectory. These outcomes are user-chosen, and can be, for example:\n\u2022 Goal-oriented: $w = \\{s_h = s\\}$, that is, $f(w|\\tau)$ indicates the likelihood of visiting state s at step h.\n\u2022 Reward-oriented: $w = \\sum_{n>t} \\gamma^{n-t} r_n$, and $f(w|\\tau)$ indicates the likelihood of achieving a certain total discounted reward starting from t. Such conditioning is also referred to as reward-to-go (RTG).\nAt inference-time, then, the user can choose to condition the policy according to the desired objective."}, {"title": "B. RvS Learning for Link Adaptation", "content": "To formulate LA in terms of the RvS learning framework, we introduce a trajectory definition that extends beyond the lifespan of a single packet, departing from the previous MDP formulation. After that, we introduce different ways to condition the policy $\\pi_\\theta$.\n1) Trajectory definition: We consider fixed-horizon trajectories $\\mathcal{T} = \\{s_0, a_0, r_0, ..., s_H, a_H, r_H\\}$, which include transmissions of packets belonging to a specific UE.\nWe consider two alternative approaches to construct $\\mathcal{T}$:\n1) Recent transmissions: $\\mathcal{T}$ consists of the most recent K transmissions of the UE considered (hence H = K).\n2) Consecutive packets: includes only up to $n_p$ of a UE's most recent consecutive packets. Since the maximum number of retransmissions is 5, we have that $H \\in \\{n_p, ..., 5n_p\\}$.\nAt training time we sample trajectories of data from the dataset $\\mathcal{D}$ using one of these two approaches, and, if needed, extra positions in $\\mathcal{T}$ are filled using padding tokens.\n2) Outcome conditioning: We propose the following three ways to condition the policy $\\pi_\\theta$:\n1) VANILLA: this is a simple conditioning that consists in fixing a target return-to-go (RTG) value w for each packet p. At training time, the RTG at the k-th step of a packet is computed as $w = \\gamma^{(p)} \\sum_{t=k}^{T(p)} \\gamma^{t-k} r_k(p)$, where $\\gamma \\in (0,1]$, $r_k(p)$ indicates the reward associated to the k-th transmission for packet p and $T(p)$ is a random variable indicating the termination step for the transmission of packet p. At inference time the RTG is fixed to some pre-defined value at the first transmission and adjusted in case of retransmissions (we discuss more about this in the numerical results).\n2) DAVG: The VANILLA formulation has the same short-comings of the MDP formulation proposed in Section II. An alternative is to condition according to a discounted average of future rewards. Letting n denote the n-th step in a sequence of continuous transmissions from a UE, at training time we condition according to\n$\\omega = (1 - \\gamma) \\sum_{t=n}^{\\infty} \\gamma^{t-n} r_t$,\nwith $\\gamma \\in (0, 1)$. At inference time, as for the VANILLA case, we condition according to some pre-defined value.\n3) 3GPP: Lastly, we propose a novel conditioning method based on the observation that the reward signal, as mentioned in Section II, is directly related to the spectral efficiency of the channel. Therefore, we condition the policy $\\pi_\\theta$ according to a nominal spectral efficiency value given the channel quality. Specifically, at each first packet transmission, depending on the CQI value reported by the UE in the state s, we use the corresponding nominal spectral efficiency from the 3GPP CQI Table 5.2.2.1-3 in [6] as the target RTG value."}, {"title": "C. Decision Transformers for LA", "content": "In this section, we describe how we adapted the DT architecture for the Link Adaptation (LA) problem. We introduce two changes: one to the attention layer, and one for the positional embedding mechanism.\n1) Attention masking for delayed feedback: A significant challenge in training a DT architecture for LA is preventing the leakage of future data during the training process. During inference, rewards are only received upon acknowledgment of a transmission (either positive or negative). Consequently, it is essential to ensure that the DT's outputs during training rely solely on information available in real-time, avoiding any use of future data that would not be accessible in an actual deployment.\nTo describe the modification we need to introduce some notation. First, for a trajectory\n$\\mathcal{T}= \\{(\\frac{s_0}{X_0}, \\frac{a_0}{X_1}, \\frac{r_0}{X_2},..., \\frac{s_K}{X_{3K}}, \\frac{a_K}{X_{3K+1}}, \\frac{r_K}{X_{3K+2}}),\\}$,\nlet $(X_0, X_1,..., X_{3K+2})$ be the corresponding sequence of tokens fed as input to the DT. Secondly, define $t_a (p)$ and $t_r(p)$ to be the instants in time at which, respectively, the action for packet p is scheduled and the associated reward signal is received by the BS. Lastly, indicate by p(X) the mapping $X \\mapsto p$ that maps a token to the corresponding packet. Then, the attention mask between any two tokens $X_i, X_j$ is given by\n$attn\\_mask(X_i, X_j) = 1\\{t_r(p(X_i)) \\leq t_a(p(X_j))\\}$.\nIn other words, if we have not received yet any reward for packet p(X), then the tokens of the packet p(Xj) should not be affected by the tokens of packet p(Xi). Fig. 2 describes such a formulation visually."}, {"title": "2) Temporal embeddings:", "content": "LA is a real-time process with states (sim. actions) observed at irregular intervals, whereas the Decision Transformer (DT) architecture assumes equidistant sampling. To capture the temporal correlations between successive packet transmissions, we consider the following modifications of the DT's positional encoding:\n1) Baseline encoding (BE): as a baseline, we implement a minor modification to the original encoder from [22], where embeddings are learned based on each token's index within the sequence ($i \\in [0, K \u2212 1]$).\n2) Learnable time (LT) encoding: embeddings are learned using a linear layer with $(\\Delta t_i)$, as input, where $\\Delta t_i = t_a(p(X_i)) - t_a(p(X_0))$. Hence, $\\Delta t_i$ is a measure of the relative time between the i-th token and the first one. Note that tokens within the same packet have identical relative timestamps, allowing the DT to associate the reward to the corresponding state and action.\n3) Continuous time (CT) encoding: inspired by [28], [29], the k-th element of the positional embedding vector for token X is computed as:\n$PE(\\Delta t_i, k) = \\begin{cases}\n    sin(\\frac{k}{C_{dmodel}} \\Delta t_i/100) & \\text{for even k,}\\\\\n    cos(\\frac{k-1}{C_{dmodel}} \\Delta t_i/100) & \\text{for odd k,}\n  \\end{cases}$"}, {"content": "where dmodel is a user-chosen constant (see also [29])."}, {"title": "IV. NUMERICAL RESULTS", "content": "In this section, we empirically compare the considered offline RL methods using an industry-grade simulator. We evaluate their performance in terms of throughput, experienced BLER, and UE SE across all multiple input multiple output (MIMO) layers allocated for transmission. Specifically, we investigate: (1) whether offline RL methods can match the performance of the behavioral policy and of OLLA; (2) how the behavioral policy affects performance of these methods; and (3) whether our modifications to the DT design improve LA performance.\nA. Environment, Methods and Data Collection\nWe now describe: the environment used to collect the data and test the methods; the methods considered in simulations; the data collection process.\n1) Environment: All simulations have been run on an internal Ericsson RAN simulator platform compliant with the 3rd Generation Partnership Project (3GPP) 5G standard specification. The software infrastructure allows running multiple independent network simulations of pre-defined length in parallel, each specifying a different seed. We consider a Time-Division Duplexing (TDD) 5G system operating at a 3.5GHz carrier frequency, with PHY layer numerology \u03bc = 0 of the 3GPP technical specifications 38.211 and single-user Multi-Input Multi-Output (SU-MIMO) transmission. Each base station is configured as massive MIMO (mMIMO) with an 8x4x2 antenna array. We consider 1 site, 3 sectors, and a fixed number of 10 UEs with full buffer traffic, thereby experiencing stable interference conditions. For each simulation (i.e., a seed) the duration is 5 seconds.\n2) Methods: We compare the performance of two value-based offline RL algorithms, BCQ [20] and CQL [21], with DT. Three baselines have been considered. The first one is the outer-loop link adaptation (OLLA) approach introduced in ??, the second one is simple behavioral cloning (BC) and as a last one we consider the performance of the behavioral DQN policy. Performance evaluation has been conducted across 50 randomized scenarios simulations, and choices regarding the hyperparameters are detailed in the appendix.\n3) Data Collection: We collected training data by interacting with the environment using two DQN policies. A first dataset, $\\mathcal{D}_{opt}$, was collected using a DQN policy $\\pi_{opt}$ that was trained until convergence; A second dataset, $\\mathcal{D}_{s-opt}$, was collected using a DQN policy $\\pi_{s-opt}$ that was trained on half the data (compared to $\\pi_{opt}$). This approach tests whether offline RL algorithms can infer optimal behavior from non-optimal training data in LA. We collected data across 40 seeds, resulting in datasets of ~ 400K transitions. The two DQN policies have been trained online on, respectively, 100 seeds for collecting $\\mathcal{D}_{opt}$, and 50 for $\\mathcal{D}_{s-opt}$.\nB. General Results\nTables I and II present the results of in terms of throughput, BLER and SE: offline RL models in Table I have been trained on $\\mathcal{D}_{opt}$, while models in Table II on $\\mathcal{D}_{s-opt}$. When considering $\\mathcal{D}_{opt}$, we notice how the offline RL methods achieve comparable results across all the considered metrics, with CQL and BCQ scoring slightly better than DT. Yet, they are only able to match the performance of the BC baseline. However, when considering suboptimal training data $\\mathcal{D}_{s-opt}$, the differences are more noticeable. CQL reaches almost the same performance as models trained on $\\mathcal{D}_{opt}$ in terms of SE. On the other hand, the DT when trained on data that is suboptimal is not able to match the performance of the behavioral policy. Our results for both settings show that BC seems to work sufficiently well on this use-case.\nC. Considerations on the Decision Transform design\nIn Table III we show results for different combinations of the positional encoder and conditioning methods proposed in Section III for the Decision Transformer (we only consider results obtained by training on $\\mathcal{D}_{opt}$).\nIncluding temporal information about packet distribution using the LT encoding appears to be beneficial compared to both the BE and CT encoding, with these two achieving similar performance. Specifically, learning embeddings directly from the actual timestamps of packet transmissions leads to substantial performance improvements, while using an encoding based on trigonometric functions (i.e., CT) results in worse performance.\nRegarding RTG conditioning, our results in Table III indicate that 3GPP conditioning achieves better throughput than both VANILLA and DAVG conditioning across all types of positional encoding. We observed similar results in terms of"}, {"title": "V. CONCLUSION", "content": "In this paper, we explored offline RL approaches for LA. Our simulations showed that offline RL methods slightly improve upon the behavioral policy used for data collection, starting"}]}