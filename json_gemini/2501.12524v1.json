{"title": "EFFICIENT LUNG ULTRASOUND SEVERITY SCORING USING DEDICATED FEATURE\nEXTRACTOR", "authors": ["Jiaqi Guo", "Yunnan Wu", "Evangelos Kaimakamis", "Georgios Petmezas", "Vasileios E. Papageorgiou", "Nicos Maglaveras", "Aggelos K. Katsaggelos"], "abstract": "With the advent of the COVID-19 pandemic, ultrasound\nimaging has emerged as a promising technique for COVID-\n19 detection, due to its non-invasive nature, affordability, and\nportability. In response, researchers have focused on develop-\ning AI-based scoring systems to provide real-time diagnostic\nsupport. However, the limited size and lack of proper annota-\ntion in publicly available ultrasound datasets pose significant\nchallenges for training a robust AI model. This paper pro-\nposes MeDiVLAD, a novel pipeline to address the above\nissue for multi-level lung-ultrasound (LUS) severity scoring.\nIn particular, we leverage self-knowledge distillation to pre-\ntrain a vision transformer (ViT) without label and aggregate\nframe-level features via dual-level VLAD aggregation. We\nshow that with minimal finetuning, MeDiVLAD outperforms\nconventional fully-supervised methods in both frame- and\nvideo-level scoring, while offering classification reasoning\nwith exceptional quality. This superior performance enables\nkey applications such as the automatic identification of crit-\nical lung pathology areas and provides a robust solution for\nbroader medical video classification tasks.", "sections": [{"title": "1. INTRODUCTION", "content": "The LUS score is a crucial tool for assessing lung disease\nseverity [1], particularly helpful for non-expert practitioners\nin evaluating patients with pulmonary abnormalities in unsu-\npervised settings. AI-based LUS scoring typically involves\nextracting frame-level features with a pretrained deep neu-\nral network (DNN), then aggregating them into video-level\nembeddings. This method faces two challenges: Network\nPretraining: Limited public LUS datasets, varying acquisi-\ntion systems, image quality, and lack of annotations hinder the\neffectiveness of traditional supervised learning. Frame-level\nAggregation: Aggregating frame-level features risks infor-\nmation loss, making it crucial to retain key details for optimal\nLUS performance. To address the first challenge, several AI-\nbased scoring methods leverage contrastive learning [2, 3, 4],\nwhich learns discriminative features by promoting intra-class\nsimilarity. However, most of these approaches rely on costly\nexpert annotations for training. Interestingly, we identify a\nsimilar solution in self-supervised learning (SSL) methods,\nwhich can be trained without labels and have demonstrated\npotential in image processing [5, 6, 7, 8]. Among these meth-\nods, DINO [8] stands out by leveraging a Vision Transformer\n(ViT) to learn representations through self-knowledge dis-\ntillation, enabling the training of self-attention mechanisms\nwith limited data. This makes DINO an ideal candidate for\npretraining feature extractors in frame-level tasks.\nFor the second challenge, a straightforward method is\nto apply the maximum frame-level probability across the\nvideo [4], classifying based on the most prominent frame in\nthe video. Other frame-level feature aggregations in video\nclassification are commonly addressed using two paradigms.\nThe first employs recurrent neural networks (RNNs) [9] to\nmodel the temporal dynamics of video sequences, deriv-\ning an overall representation from the frame-level features.\nThe second paradigm, broadly categorized as the Bag-of-\nVisual-Words (BoVW) based aggregation, constructs local\nframe descriptors, assigns them to pre-defined clusters, and\naggregates their residuals into a global representation. A\ncommon example of this approach is VLAD [10] and its vari-\nants [11, 12]. Other methods, such as I3D [13], which relies\non 3D convolutions, are computationally expensive. In com-\nparison, we believe BoVW-based aggregation will be more\nsuitable for our task than RNN-based methods, as LUS video\nlabels often depend on several typical frames within a short\ntime frame, making temporal dependencies less significant.\nIn this paper, we proposed a semi-self-supervised learn-"}, {"title": "2. DATA & PROBLEM STATEMENT", "content": "We curated our LUS dataset with 177 curvilinear ultrasound\nvideos from the COVIDx-US [15] dataset and 106 ultrasound\nvideos collected from \"G. Papanikolaou\" General Hospital\nof Thessaloniki (CoCross), totaling 283 videos from 156 pa-\ntients. The data distribution is shown in Fig 1. To improve\nframe-level scoring, we randomly selected 2 - 3 representa-\ntive frames from each video, resulting in a small frame-level\ndataset of 585 annotated images.\nWe adopt an improved LUS scoring system, namely the\nintegrated lung ultrasound score (i-LUS)[16]. This system\nincorporates additional factors such as pleural line charac-\nteristics and cardiac involvement in COVID-19, providing a\nmore comprehensive assessment. I-LUS uses a 4-level scor-\ning system: Score-0 represents a normal lung with a contin-\nuous pleural line and horizontal A-line artifact; Score-1 in-\ndicates at least 2 isolated or coalescent B-lines covering less\nthan 50% of the image without clear sub-pleural alterations;\nScore-2 includes B-lines covering more than 50% of the im-\nage, still without clear sub-pleural alterations; and Score-3\nrepresents consolidation with poorly dynamic arborescent air\nbronchograms. Considering the extreme class imbalance in\nour dataset, we combined scores 1 and 2, simplifying it\nto a 3-level scoring system. Given such a three score sys-\ntem $Y \\in \\{y_0, y_1, y_2\\}$ and a ultrasound video containing\nN frames, $V \\in \\{X_0,X_1,..., X_{N-1}\\}$, our goal is to predict"}, {"title": "3. METHOD", "content": "3.1. Self-distillation with Task-specific Finetuning\nTransformers [17] have recently emerged as an alternative to\nCNNs, offering superior performance in medical classifica-\ntion tasks [18, 19, 20], with their inherent attention mech-\nanisms providing precise reasoning. However, their limita-\ntions are also significant: they require more computational\nresources and training data, which restricts their applicability\nin most medical imaging scenarios. We question whether this\nissue can be mitigated by pretraining the neural network using\nthe visual information contained in unlabeled images.\nDINO [8] adopts a similar architecture to most recent\nSSL-based methods, with the key difference being its use\nof self-knowledge distillation during training. As shown in\nFig 2a, DINO [8] leverages a teacher network $g_t$ to guide a\nstudent network $g_s$ with the same architecture, and $\\theta_t$\nand $\\theta_s$ are both learnable parameters. During training, the input\nimage x will be encoded into two sets of K-dimensional\ndistributions, $P_s$ and $P_t$. The key difference lies in that the\nteacher's input, $\\{x_t^0\\}_{i=0}^m$, are more global than the student's\ninput, $\\{x_s^i\\}_{i=0}^n$. In practice, $x_t^0$ refers to a larger crop of the\noriginal image x, while $x_s^i$ is a smaller, augmented version\nand we normally set $m > n$. The training objective [8] is\nto minimize the cross-entropy loss between every $P_s$ and $P_t$,\nthereby fostering the correspondence from local-to-global,\ni.e.,\n$\\min \\sum_{x \\in \\{x\\_t^0\\}\\} \\sum_{x' \\in \\{x\\_s^i\\}\\} H(P_t(x,t), P_s(x', T_s))$ (1)\nwhere $H(*)$ is the cross-entropy loss and $P(*)$ denotes the\nsoftmax operation with certain temperature $\\tau$ which is a non-\nnegative constant. During this process, only the student's\nweights $\\theta_s$ are updated, while the teacher is updated via ex-\nponential moving average (EMA) [21], i.e., $\\theta_t \\leftarrow \\lambda \\theta_t + (1 -\\lambda) \\theta_s$. Notably, [8] shows that the teacher network produces\nbetter features than the student. Therefore, we will use the\nteacher for subsequent tasks. It is worth mentioning that the\nencoded distributions $P_s$ and $P_t$ can be interpreted as the\nprobabilities for the classification over K predefined classes,\nwhereas our target classes are their subsets. To narrow down\nfrom the predefined classes toward LUS scoring, we further\nperform a fully supervised task-specific finetuning on the\nteacher network using a small set of annotated frames, which"}, {"title": "3.2. Dual-level VLAD Aggregation", "content": "Under our default setting, each ultrasound video contains\nN = 15 frames, where each frame corresponding to an D-\ndimensional feature vector f, extracted by $g_\\theta^*$. For our pro-\nposed dual-level VLDA aggregation (Fig. 2c), we consider\nthe simplest scenario by setting the number of clusters to be\nequal to the number of labels. Similar to the NetVLAD [11]\naggregation, we encode the frame-level ultrasound embed-\nding into a $Y \\times D$ dimensional feature vector $f_\\nu$. This is\ndone by assigning a learned cluster centroid $C_\\eta^j$ to every\nframe embedding and concatenating their residual:\n$f_\\nu = || \\{a_c (f_i, \\eta^j) (f_i - c_j)\\} ||_{j=1}^Y$ (2)\nwhere $i \\in \\{1, ..., N\\}$, and $a_c(*)$ is a learnable cluster-level\nsoft-assignment that assigns frames to clusters based on their\nproximity. NetVLAD [11] directly measures the sum of $f_i$\nacross the frame level. However, LUS videos are typically\nscored based on the highest severity observed in the video. In\nother words, video-level scoring relies on a single or a few\nrepresentative frames within the video. To address this, we\nintroduce an additional frame-level soft-assignment denoted\nas $a_f (f_i, \\tau')$ to perform frame selection. This assignment is\nimplemented through a simple multilayer perceptron (MLP)\nwith tanh activation, where temperature $\\tau'$ was adopted to\ncontrol the model's focus on the most informative frame seg-\nments. Then, the video-level embedding $\\upsilon$ is obtained by\nsumming up weighted frame-level features and applying an\nintra-normalization $\\sigma$ to suppress bursts [22], i.e.,\n$\\upsilon = \\sigma (\\sum_{i=1}^N a_f (f_i, \\tau') f_i)$ (3)\nFinally, a cross-entropy-based video-level classifier is applied\nto finish the video-level scoring."}, {"title": "4. EXPERIMENTS", "content": "Given the objective of this work, we did not require a large\namount of labeled frame data for training. As such, we per-\nformed a 2-fold validation, assigning 302 images from 136\nvideos to fold 1 and 283 images from 140 videos to fold 2,\nensuring that videos from the same source did not appear in\ndifferent folds. All experiments were conducted on a sin-\ngle Nvidia Quadro RTX 8000 GPU and optimized using the\nAdamW optimizer, with learning rates decaying according to\na cosine schedule. For pretraining, we adopted the ViT-S/8\nconfiguration and data augmentation setup from [8], training\nthe backbone by randomly sampling unlabeled frames from\nthe ultrasound videos. The temperatures $T_t$ and $T_s$ were set\nto 0.5 and 0.1, respectively. Afterward, we finetuned the net-\nwork on labeled frames in a fully supervised manner. At the\nvideo level, we trained the dual-level VLAD aggregation us-\ning labeled ultrasound videos. For simplicity, additional train-\ning details are summarized in Table 1."}, {"title": "4.1. Image-level Classification", "content": "We evaluated MeDiVLAD at the frame level. For this,\nwe trained a ResNet-50 (23.5M) as a classification baseline\nwith a similar number of parameters to the ViT-S (21.7M)\nwe used. The average scoring accuracy (k-NN/linear clas-\nsifier) and ROC-AUC (one-vs-all) were reported. It should\nbe noted that the k-NN accuracy is only provided for the\nmodels that were not fully supervised during training. In\nthe upper half of Table 2, we first examined the impact of\nself-distillation. Without using LUS data, ResNet-50 pre-\ntrained on ImageNet (IMG) showed slightly lower classifi-\ncation accuracy than the other two DINO experiment sets,\nwith the advantage of DINO becoming more pronounced af-\nter incorporating unlabeled ultrasound data. As expected, af-\nter finetuning the model with labeled frames, both accuracy\nand ROC-AUC improved, outperforming all other baselines\n(AUC: 0.917 & Acc: 82.47%). Remarkably, even without su-\npervised finetuning, we achieved an accuracy of 75.05%, sur-\npassing the 71.94% accuracy of the fully supervised ResNet-\n50. In Fig. 3, we present several attention map visualizations\nfrom the finetuned backbone. In (a) and (b), the attention\nmaps accurately highlight both A-lines and B-lines, while in\n(c), the model identifies all regions of consolidation, offering\nclear insights into its decision-making process for LUS scor-\ning."}, {"title": "4.2. Video-level Classification", "content": "At the video level, we evaluated our proposed MeDi-\nVLAD aggregation against three typical aggregation meth-\nods: Bi-LSTM [14], NetVLAD [11], and directly taking the\nmax severity-category score [4] from the frame predictions.\nWe used the same metrics as in the frame-level experiments\nfor evaluation. For the LSTM, we simply set the hidden size\nto be the same as the embedding length. Additionally, we\nperformed a grid search to find the best hyperparameters for\neach model and reported the metrics as the fold average. As\nshown in Tab. 3, MeDiVLAD significantly outperformed all\nother methods across all metrics. Similar to the results in the\nframe-level experiment, using our proposed dual-level VLAD\naggregation, we achieved comparable performance to a fine-\ntuned ResNet-50, with an AUC of 0.86 versus 0.88 and an\naccuracy of 75.3% versus 76.4%. These results not only con-\nfirm that applying self-knowledge distillation for pretraining\na dedicated feature extractor is effective but also demonstrate\nthat using a small amount of annotated data (300 samples) for\nfinetuning can lead to an almost 6% improvement in accuracy."}, {"title": "5. CONCLUSION & DISCUSSION", "content": "In this work, we introduced MeDiVLAD, a novel pipeline\nfor LUS scoring at both frame and video levels. By leverag-\ning self-knowledge distillation to pretrain a vision transformer\nwithout labels and using dual-level VLAD aggregation, we\nsignificantly reduced the reliance on expert annotation. At\nthe frame level, our method achieved 75.05% accuracy with-\nout labeled data, which improved to 82.47% with finetuning.\nAt the video level, MeDiVLAD outperformed other aggre-"}]}