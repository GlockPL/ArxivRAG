{"title": "Attention Eclipse: Manipulating Attention to Bypass LLM Safety-Alignment", "authors": ["Pedram Zaree", "Md Abdullah Al Mamun", "Quazi Mishkatul Alam", "Yue Dong", "Ihsen Alouani", "Nael Abu-Ghazaleh"], "abstract": "Recent research has shown that carefully crafted jailbreak inputs can induce large language models to produce harmful outputs, despite safety measures such as alignment. It is important to anticipate the range of potential Jailbreak attacks to guide effective defenses and accurate assessment of model safety. In this paper, we present a new approach for generating highly effective Jailbreak attacks that manipulate the attention of the model to selectively strengthen or weaken attention among different parts of the prompt. By harnessing attention loss, we develop more effective jailbreak attacks, that are also transferrable. The attacks amplify the success rate of existing Jailbreak algorithms including GCG, AutoDAN, and ReNeLLM, while lowering their generation cost (for example, the amplified GCG attack achieves 91.2% ASR, vs. 67.9% for the original attack on Llama2-7B/AdvBench, using less than a third of the generation time).", "sections": [{"title": "1 Introduction", "content": "The development of Large Language Models (LLMs) has marked a new era in Artificial Intelligence (AI), driving significant advancements across diverse application domains. These models, combining novel attention based architectures with diverse training on vast and diverse datasets, exhibit generalized learning, and excel in generating human-like text. These properties have made LLMs become pivotal in applications such as conversational agents, programming assistants, federated learning systems, and DNA processing tools. Prominent examples include OpenAI's GPT-4, Meta's Llama2, and DeepSeek.\nLLM models remain vulnerable to carefully crafted inputs, known as jailbreak prompts, which exploit safety mechanisms and induce harmful outputs. For instance, a jailbreak prompt may force a model to generate unsafe instructions or bypass ethical safeguards. The growing prevalence of jailbreak attacks has led to extensive research aimed at understanding these vulnerabilities and developing effective countermeasures. However, it is important to continue to explore more advanced attacks to enable development of effective defenses and to provide benchmarks that can be used in their evaluation.\nExisting jailbreak methods can be broadly categorized into two types: (1) white-box attacks, where the attacker requires access to the model's internal parameters, and (2) black-box attacks, which require no such access relying only on being able to prompt the model and observe the output. Because white-box approaches have access to the internal state of the model, they are in general stronger attacks. Since access to model internals is becoming more common given the increase of open-source models, white-box access should be assumed and used to drive and evaluate defenses.\nIn this paper, we propose a new approach to generating jailbreak attacks by introducing an additional degree of freedom in the optimization process. Existing jailbreak attacks are output-driven, optimizing for a target output through a global loss function. Instead, we introduce an orthogonal, input-driven approach. Specifically, our method focuses on the explicit influence of prompt tokens on the model's attention, rather than solely optimizing for the final output.\nAn analogy can be drawn from human language processing, where tools such as punctuation, textual formatting effects, syntax usage, and voice inflections and emphasis guide interpretation by shifting attention to specific elements of the text. Similarly, our attack, which we call Attention Eclipse, allows an attacker to either amplify or suppress the attention among specific tokens within the prompt. We show that surgically manipulating the model's attention to the adversary's advantage can lead to effective jailbreak attacks that bypass alignment constraints.\nWe use manipulating attention in two ways in our attacks (recognizing that there are likely to be others): (i) First, attention manipulation allows the recomposition in the latent space of a \"decomposed\" prompt by establishing hidden dependencies between seemingly unrelated token sequences. This enables the adversary to embed harmful content within a benign-looking prompt, where attention later bridges the safe portion with the adversarial intent.(ii) Second, attention manipulation facilitates camouflaging adversarial suffixes, where an adversarial suffix -generated through an existing jailbreak method\u2014 is made less conspicuous by controlling the model attention distribution within the prompt, causing the adversarial prompt to bypass alignment.\nWe apply our attacks to a number of recent models, using the camouflage strategy to hide adversarial prompts produced using a number of recent adversarial attacks. Across all models and adversarial attacks, our attack produces substantial improvements in the attack success rates. Moreover, by integrating attention shifting with adversarial token generation for iterative jailbreak attacks such as GCG, we show that our framework substantially reduces the number of overall iterations needed to generate successful jailbreaks. We also show that the attacks are highly transferrable across models.\nThe main contributions of this paper are as follows.\n\u2022 We introduce a new white-box adversarial attack strategy that directly manipulates attention patterns within a prompt that enables effective evasion of model alignment.\n\u2022 We propose strategies to leverage attention attacks, by composing adversarial prompts, and by shifting attention between adversarial suffixes and harmful tokens.\n\u2022 Our attack strategy can compose with most existing adversarial attacks to amplify their effectiveness. We provide a generalizable framework that amplifies existing jailbreak techniques, including ReNeLLM, GCG, and AutoDAN, across various LLM architectures. Our approach demonstrates transferability within model families, highlighting its adaptability to different models and scenarios."}, {"title": "2 Related Work", "content": "Jailbreak attacks can be broadly classified as black box attacks, those that only assume access to prompt the model and observe its output, and white box attacks, which assume access to the internal state of the model. The attacks can further be distinguished by the attack approach. Gradient-based Jailbreaks are white box attacks that leverage model gradients to craft adversarial changes to the prompt. The attacks backpropagate the loss between the generated text and an affirmative response agreeing to generate a Jail-"}, {"title": "3 Methodology", "content": "In this section, we introduce Attention Eclipse, a framework that enhances jailbreak attacks by leveraging the internal attention mechanisms of LLMs. Conventional jailbreak algorithms optimize a global loss function typically capturing the difference between the model's output and a desired target output (typically, an affirmative response, leading the model to agree to respond to the unsafe prompt). In contrast, our approach introduces an attention-based intermediate loss function, where input tokens are explicitly integrated into the loss formulation, to increase or decrease attention among selected tokens. This allows for dynamic weighting of different prompt components, effectively modulating their influence on the model's inference/generation process."}, {"title": "3.1 Attention Loss for Jailbreak Attacks", "content": "Traditional Output Loss for Jailbreak Attacks. Jailbreak attacks often rely on optimizing a crafted adversarial prompt to induce the model to generate a target and desired output. This prompt is achieved by crafting adversarial tokens to minimize a loss function that senses the deviation of the model's output from a desired target response. Given a sequence of tokens < x_1, x_2, ..., x_m >, the LLM estimates the probability distribution over the vocabulary for the next token x_{m+1}:\nx_{m+j} \\sim P(x_1, x_2,...,x_{m+j-1}), \\forall j=1...k \\qquad(1)\nThe goal of jailbreak attacks is to prompt the model to produce output starting with a specific target (e.g. \"Sure, here is how to ...\"), denoted"}, {"title": "3.2 Amplifying Jailbreaks using attention", "content": "Let J_i be an initial prompt that may be a jailbreak attempt. We assume the following generic composition of the initial prompt J_i:\nJ_i = < AP, Goal, AS >  \\qquad(5)\nwhere,\n\u2022 Goal is the harmful prompt that we aim to force the LLM to respond to.\n\u2022 AP are tokens appearing before Goal as \"Adversarial Prefix\". Notice that in some settings such as GCG, AP = 0\n\u2022 AS are tokens appearing after Goal, i.e., \"Adversarial Suffix\".\nAttack Mechanism. Given an initial prompt, J_i, our objective is to generate an amplified prompt, J_{amp}, that escapes alignment, using the strategies illustrated in Figure 1. Specifically, Attention Eclipse's attack template employs the following two strategies: (i) Recomposing: we split Goal into two components: G_h, which contains the harmful content, and G_s, a rewritten version designed to appear safe. Recomposition amplifies the attention between (G_h and G_s) in the attention space; and (ii) Camouflaging the adversarial suffix, by balancing the effective impact of G_h on the generative process with the level of attention that triggers the refusal process. To do so, we introduce two sets of tokens, \\phi_1 and \\phi_2, which act as attention manipulation adversarial components within the prompt. Consequently, the generic structure of J_{amp} is as follows:\nJ_{amp} = < G_h, AP, \\phi_1, G_s, \\phi_2, AS >  \\qquad(6)\nThe objective is then to optimize \\Phi = (\\phi_1, \\phi_2) such that:\n\\phi_1^* = arg \\min_{\\phi_1} [ - L_{attn}(G_s, G_h)]\n\\phi_2^* = arg \\min_{\\phi_2} [L_{attn}(AS, G_h)]  \\qquad(7)\nEquation 7 represents the core objectives of our approach; we first explore the token space of \\phi_1 to maximize the attention between the decomposed payload parts, i.e., (G_s, G_h). This facilitates recomposing meaning through attention. Our second strategy amplifies an adversarial suffix generated by another Jailbreak algorithm, by ensuring that the adversarial suffix does not trigger the model's refusal mechanism. The second set of attention manipulator tokens, \\phi_2 are optimized to lower the attention from the adversarial suffix on the harmful part G_h. This effectively redirects focus away from harmful content, decreasing the likelihood of triggering the model's alignment mechanism."}, {"title": "4 Experimental Evaluation", "content": "The Attention Eclipse framework enhances existing jailbreak attacks by leveraging attention manipulation to bypass alignment. This section evaluates our method on leading open-source LLMs using multiple jailbreak attacks. We compare the performance of the original attacks with their amplified versions, demonstrating improvements in Attack Success Rate (ASR), computational efficiency, and transferability to certain closed-source LLMs."}, {"title": "4.1 Experimental Setup", "content": "Datasets: We use two publicly available and widely used datasets: AdvBench and HarmBench, which comprehensively cover adversarial strategies and attack scenarios. Further details on these datasets and their relevance to our study are provided in Appendix D.\nTarget LLMs: We evaluate our attack on open-source LLMs, specifically Llama2-7B-Chat, Llama2-13B-Chat, Llama2-70B-Chat, and Vicuna-13B."}, {"title": "4.2 Main Results \u2013 Attack Effectiveness", "content": "Improvement in ASR: Table 1 underscores the generalization capability of the Attention Eclipse framework across various models and datasets. It achieves substantial ASR gains across all evaluated models. For instance, the Amplified Auto-DAN attack achieves a 153.1% improvement on the Llama2-7B model under the HarmBench dataset, demonstrating its ability to exploit alignment weaknesses more effectively than the baseline.\nWe should note that for the GCG and its amplified attack, we use the same budget for the adversarial suffix (20 tokens) and allocate additional budget to the new adversarial components: \\phi_1 (5 tokens) and \\phi_2 (10 tokens).\nPerformance Across Prompt Categories: Table 2 presents a breakdown of ASR across different harmful prompt categories within the AdvBench dataset. The results indicate that certain categories, such as Hate Speech and Physical Harm, exhibit lower baseline ASR, suggesting that LLMs are more resistant to these types of jailbreaks. However, the Attention Eclipse framework significantly amplifies attack success across all categories. Notably, categories with lower baseline success, such as Hate Speech and Physical Harm (5.1% and 11.6% ASR for AutoDAN), show substantial gains, reaching 30.8% and 30.2% ASR, respectively, after amplification. These improvements highlight the effectiveness of attention manipulation in bypassing alignment, even in categories where models exhibit stronger resistance.\nAttack Acceleration: An effective jailbreak attack should be fast and highly successful. One of the key advantages of Attention Eclipse is its ability to accelerate jailbreak attacks. Table 3 presents the time cost per prompt (TCPP) for different baseline attacks and their amplified versions. We evaluate jailbreak efficiency on Llama2-7B-Chat using the AdvBench dataset . The results show that ReNeLLM reduces the computational cost by over 60%, improving from 136.0s to 54.46s, while GCG achieves a 3.5x speedup, cutting TCPP from 665.0s to 189.41s. Note that for an Amplified AutoDAN attack, its TCPP includes the TCPP of the baseline AutoDAN, as we first compute J_i and then apply Attention Eclipse to it. By minimizing search overhead, Attention Eclipse makes jailbreak attacks faster, more efficient, and scalable to larger models.\nTransferability: We examine how Attention Eclipse-generated jailbreak prompts transfer to closed-source models. Using GPT-3.5-Turbo, and GPT-40-mini as target models, we evaluate whether prompts optimized on Llama2-7B-Chat (source model) remain effective when transferred. For this experiment, jailbreak prompts generated from the AdvBench dataset are directly input into the target models. Table 4 shows that amplified jailbreak prompts maintain a strong ASR across target models, despite being optimized on an open-source model. Notably, Amplified ReNeLLM achieves an ASR of 96.0% on GPT-3.5-Turbo and 79.4% on GPT-40-mini, demonstrating high transferability. Amplified AutoDAN also performs well on GPT-3.5-Turbo (83.3%), though its success drops on GPT-40-mini (31.0%), indicating varying levels of robustness across different architectures. Conversely, Amplified GCG shows weaker transferability, achieving 27.1% on GPT-3.5-Turbo and only 7.5% on GPT-40-mini, suggesting that its attack mechanism may be more dependent on source-model-specific characteristics. These results highlight the broad applicability of Attention Eclipse in bypassing alignment across multiple LLMs, emphasizing the importance of stronger alignment mechanisms in future models to mitigate cross-model jailbreak risks."}, {"title": "5 Ablation Study", "content": "This section analyzes the key components of Attention Eclipse to understand their impact on ASR. We examine how Decomposition, \\phi_1, and \\phi_2 contribute individually and together, how attention manipulation shifts model behaviour, and how camouflaging adversarial suffixes improves bypassing alignment. Additionally, we explore how a well-chosen initial point enhances efficiency, leading to faster and more successful jailbreaks."}, {"title": "5.1 Evaluating Individual Attack Components", "content": "To analyze the contribution of individual components in Attention Eclipse, we conduct a study using 100 adversarial Goals from AdvBench as the dataset and Llama2-7B-Chat as the target model. In this study, we generate the full adversarial prompt, and then use the components individually and in combination to understand their contribution to the success of the attack. For example, when studying J_i+Decomposition+ \\phi_1, if \\phi_1 has N tokens, then in J_i+Decomposition+ \\phi_1 + \\phi_2, we use N tokens for \\phi_1 and allocate M new tokens to \\phi_2. Moreover, to explore each prompt, we optimize adversarial components from scratch rather than"}, {"title": "5.2 Attention Heatmap", "content": "Figure 3 illustrates how embedding \\phi_1 and \\phi_2 into the prompt, followed by optimization, can shift attention patterns in a jailbreak prompt. The upper prompt represents the initial jailbreak attempt generated by Attention Eclipse, which starts with a GCG jailbreak prompt, decomposes it, and incorporates \\phi_1 and \\phi_2. However, this initial prompt fails to jailbreak the Llama2-7B-Chat model. To overcome this, we optimize all \\phi_1, \\phi_2, and AS, resulting in the bottom prompt in Figure 3. The colour intensity of each sentence corresponds to the summed attention weight of its tokens on G_H's tokens at all layers and heads. The heatmaps reveal that optimizing the attention loss causes G_s to darken, indicating that \\phi_1 successfully increases G_s's attention on G_H, effectively recomposing the prompt. Conversely, AS becomes lighter, suggesting that \\phi_2 reduces AS's attention on G_H, effectively camouflaging the adversarial suffix."}, {"title": "5.3 Impact of Camouflaging on Harmfulness and Jailbreak Prompts", "content": "We investigate using amplified GCG attacks how camouflaging adversarial suffixes improves jailbreak performance. To isolate the effect of \\phi_2, we start with a GCG jailbreak prompt (J_i), decompose it, and optimize \\phi_1 while keeping all other elements fixed. We then embed an initial \\phi_2 into the jailbreak prompt and optimize it in two opposing directions: 1) Increasing Adversarial Suffix attention on G_H, and 2) Decreasing Adversarial Suffix attention on G_H.\nFigure 4 visualizes the results. Increasing attention causes the output loss to plateau, preventing the jailbreak by keeping the suffix detectable and suppressing its effectiveness. In contrast, reducing attention weights enables the model to successfully jailbreak within just two optimization iterations, demonstrating that camouflaging enhances the ability of the adversarial suffix to evade alignment.\nFurthermore, Figure 4 highlights that decreasing attention weights results in a steady decline in output loss, directly correlating with improved jailbreak success. This reinforces that camouflaging adversarial suffixes is a crucial mechanism for crafting more effective jailbreak prompts, making them harder to detect which results in high ASR."}, {"title": "5.4 How Initial Point Affects ASR and TCPP", "content": "To explore the effect of a well-chosen initial point, we analyze its impact on ASR and TCPP in generating jailbreak prompts. Specifically, we investigate how initializing \\phi_1, \\phi_2, and AS with well-chosen values and utilizing the HotFlip method for optimizing them enhances the performance of amplified GCG attack. We first optimize these elements on simpler Goals and use these pre-optimized points as a robust point for tackling more complex Goals, demonstrating a self-transfer effect.\nIn an experiment on Llama2-7b/AdvBench, we compare two approaches for optimizing jailbreak prompts. In the first one, we use a well-chosen initial point for all \\phi_1, \\phi_2, and AS. In the second one, these tokens are initialized randomly. The first approach results in an ASR of 91.2% (as shown in Table 1) and a TCPP of 192.2 seconds, while the second one achieves an ASR of 71.2% and a higher TCPP of 672.7 seconds even higher than original GCG attack."}, {"title": "6 Conclusion", "content": "In this paper, we introduced Attention Eclipse, a new jailbreak framework that manipulates LLM attention mechanisms to enhance attack success rates while reducing harmfulness. Unlike existing methods, our approach systematically decomposes prompts and optimizes a set of tokens (\\phi_1 and \\phi_2) to bypass alignment constraints more effectively.\nThrough extensive experiments on open source models including Llama2 and Vicuna, we demonstrated that Attention Eclipse significantly amplifies existing jailbreak techniques, improving attack success rate (ASR). Our findings provide further evidence that LLMs remain vulnerable to Jailbreak attacks when combined with targeted attention manipulation, despite existing alignment safeguards. As a result, there continues to be a need for stronger defense mechanisms that can detect and mitigate jailbreak attempts before they succeed."}, {"title": "7 Limitations", "content": "Although Attention Eclipse demonstrates significant improvements in jailbreak attack effectiveness, several limitations remain. First, our approach relies on access to model attention weights, making it a white-box attack. This limits its applicability to proprietary, closed-source models where internal attention mechanisms are inaccessible. There are possible approaches to enable application in a black-box setting: (1) the approach exhibits high transferability, and attacks may be developed on an open source model, and used against other models that are not accessible; and (2) indirect approaches to reverse engineer a model into a proxy-model that is then used to generate the attack can be leveraged.\nSecond, the effectiveness of Attention Eclipse depends on the quality of the initial jailbreak prompt (J_i). If the base attack is already weak, our framework may not be able to amplify it sufficiently. This suggests that our approach is best suited for enhancing strong existing jailbreak techniques rather than creating entirely new ones from scratch.\nThird, while our method significantly improves the Attack Success Rate (ASR), it does not explicitly optimize for stealthiness against jailbreak detection systems. Current LLM safety measures increasingly incorporate adaptive filtering and adversarial training, which can reduce the long-term efficacy of our approach. Investigating how attention manipulation interacts with these evolving defense mechanisms is an important area for future research.\nDespite these limitations, Attention Eclipse highlights critical vulnerabilities in LLM alignment and provides a new perspective on adversarial attacks that leverage internal attention dynamics. We identified two strategies for manipulating attention, but there are likely to be others that can be discovered through further research. Future work should explore how these insights can inform more robust jailbreak defenses and adaptive security measures in LLMs."}, {"title": "8 Ethical Considerations", "content": "This research explores jailbreak attacks on Large Language Models (LLMs) to understand their vulnerabilities and improve alignment robustness. While Attention Eclipse demonstrates how targeted attention manipulation can effectively bypass safety constraints, we acknowledge the potential risks associated with adversarial techniques.\nOur work is conducted purely for academic and security research purposes, aiming to identify weaknesses in LLM safety mechanisms rather than to promote or enable misuse. Understanding these vulnerabilities is a critical step toward designing stronger defences, as demonstrated by prior research in adversarial AI and model robustness. By highlighting how attention-based attacks exploit model internals, we hope to inform researchers and industry practitioners about new potential threats that must be mitigated.\nTo minimize misuse, we strictly adhere to ethical AI research guidelines:\n\u2022 Responsible Disclosure: We recommend that LLM developers and AI safety teams incorporate adaptive attention-aware defences to counteract similar jailbreak strategies.\n\u2022 Dataset & Model Use: Our experiments were conducted exclusively on publicly available, open-source models (Llama2, Vicuna) and do not involve real-world deployment or malicious applications.\n\u2022 No Real-World Harm: Our research does not endorse or support any malicious use of LLM jailbreak techniques, such as generating harmful content, misinformation, or unethical automation.\n\u2022 Transparency & Reproducibility: The methods and findings in this paper are fully documented to support research into robust AI alignment while ensuring transparency in adversarial AI research.\nUltimately, this work reinforces the importance of proactive AI safety measures and the need for continuous adversarial testing to strengthen LLM security. We encourage collaboration between AI researchers, policymakers, and industry practitioners to address evolving threats while ensuring that powerful AI models remain safe and aligned with ethical standards."}, {"title": "A Attention Granularity", "content": "Understanding how attention mechanisms operate at different levels of granularity is essential for analyzing and manipulating large language models (LLMs). Attention granularity defines how the focus is distributed across different linguistic structures\u2014ranging from individual tokens to entire sentences. This section explores three key levels of attention: token-level, word-level, and sentence-level attention."}, {"title": "A.1 Token-Level Attention", "content": "Token-level attention refers to the attention that one token assigns to another within an input sequence. This is the most fine-grained level of attention and serves as the foundation for higher-level aggregations. Each attention score is computed separately for every attention head and layer in the model.\nTo extract token-level attention, we first process an input prompt through the LLM to obtain its attention matrix AM, which contains attention scores for all token pairs. The attention of token T_i on T_j at layer l and head h is given by the element (h, l, i, j) of the matrix:\nAM_{h,l,i,j}\nThis score quantifies how much focus the model places on T_j when processing T_i. Token-level attention is particularly useful in analyzing how information propagates across transformer layers and has been widely used in interpretability studies."}, {"title": "A.2 Word-Level Attention", "content": "Since language models process text at the subword-token level, individual words may be split into multiple tokens by the tokenizer (e.g., \"running\" might be split into [\"run\", \"ning\"]). Word-level attention aggregates the token-level scores across all tokens that belong to a given word.\nSuppose we need to compute the attention from Word_1 to Word_2, where: Word_1 spans tokens i to j, and Word_2 spans tokens k to m\nThe aggregated word-level attention at layer l and head h is computed as:\nAttn(W_1, W_2) = \\sum_{t_1=i}^{j} \\sum_{t_2=k}^{m} AM_{h,l,t_1,t_2}\nBy aggregating token attention into words, we can better interpret how the model processes meaningful linguistic units rather than subword fragments. This technique is often used in saliency analysis for understanding attention-based neural networks."}, {"title": "A.3 Sentence-Level Attention", "content": "Sentence-level attention refers to how much attention a model assigns from one entire sentence to another. This level of granularity is useful for discourse analysis, coreference resolution, and long-range dependency tracking.\nTo compute sentence-level attention from Sentence_1 to Sentence_2, where: Sentence_1 spans tokens i to j, and Sentence_2 spans tokens k to m\nThe aggregated sentence-level attention at layer l and head h is:\nAttn(s_1, s_2) = \\sum_{t_1=i}^{j} \\sum_{t_2=k}^{m} AM_{h,l,t_1,t_2}\nSentence-level attention is particularly important in document-level transformers, where attention needs to be distributed efficiently across long texts. Some recent models optimize sentence-level dependencies to improve coherence and contextual retention."}, {"title": "A.4 Relevance to Jailbreak Attacks", "content": "Understanding attention granularity is crucial for designing adversarial attacks such as Attention Eclipse. By precisely manipulating attention weights at different levels: Token-level optimization allows fine-grained control over which words receive focus. Word-level aggregation ensures that adversarial tokens interact with meaningful text units. Sentence-level adjustments enable larger-scale bypasses of alignment mechanisms.\nThis structured approach helps attackers exploit LLM vulnerabilities while preserving coherence and fluency in generated outputs."}, {"title": "B Evaluation Metrics", "content": "To assess the effectiveness of our proposed approach, we evaluate it using three key metrics: Attack Success Rate (ASR), Time Cost Per Prompt (TCPP), and Noise Budget. These metrics provide a comprehensive understanding of both the success and efficiency of jailbreak amplification."}, {"title": "B.1 Attack Success Rate (ASR)", "content": "ASR is the primary metric used to evaluate jailbreak performance. It measures the percentage of prompts that successfully bypass the alignment mechanisms of an LLM. ASR is computed as follows:\nASR = \\frac{Number \\ of \\ successful \\ jailbreak \\ prompts}{Total \\ number \\ of \\ prompts \\ in \\ the \\ dataset}\nA higher ASR indicates a more effective attack, demonstrating the ability of the amplified jailbreak prompts to circumvent model safeguards. ASR is widely used in adversarial robustness evaluations of LLMs."}, {"title": "B.2 Time Cost Per Prompt (TCPP)", "content": "Computational efficiency is another crucial factor in evaluating jailbreak attacks. The Time Cost Per Prompt (TCPP) quantifies the average time required to generate a successful jailbreak attack. The first step is computing the average number of queries (AQ) per successful attack:\nAQ = \\frac{#Queries \\ within \\ a \\ successful \\ attack}{Number \\ of \\ successful \\ attacks}\nUsing AQ, we calculate TCPP as:\nTCPP = AQ \\times Average \\ Time \\ per \\ Iteration\nLower TCPP values indicate a more efficient jailbreak method, as fewer queries and less computational time are needed to bypass alignment.\nTo fill out Table 3, we select 16 samples of AdvBench from those where multiple methods achieve successful jailbreaks, to calculate the TCPP of each method required to successfully jailbreak each sample. The IDs of these samples are [67, 96, 128, 143, 204, 218, 272, 310, 315, 342, 370, 371, 411, 465, 481, 517] (starting from 0). The target model is Llama2-7b-chat and experiments are done on a single NVIDIA A100-SXM4-40GB GPU."}, {"title": "B.3 Analysis of Query Distributions", "content": "Figures 5 to 7 illustrate the distribution of the number of queries required for baseline attacks jailbreak prompts for different methods on the HarmBench dataset using the Llama2-7b-chat model.\nReNeLLM exhibits the lowest resistance to amplification, with a highly concentrated query distribution and a median iteration count of 1.0 (Figure 5). This indicates that minimal effort is required to craft an effective jailbreak prompt.\nAutoDAN demonstrates a slightly higher resistance, requiring a median of 2.0 iterations (Figure 6). This suggests that while AutoDAN is more resilient than ReNeLLM, it is still relatively easy to amplify.\nGCG presents the greatest resistance to amplification, with a median of 5.5 iterations (Figure 7). This wider distribution reflects a higher degree of variability in amplification effectiveness across different prompts.\nOverall, these findings provide insights into the relative robustness of different jailbreak strategies. GCG resists amplification the most, while ReNeLLM is the most susceptible."}, {"title": "B.4 Noise Budget", "content": "The final metric, Noise Budget, quantifies the additional tokens introduced by our amplification method. Since Attention Eclipse modifies jailbreak prompts by appending \\phi_1 and \\phi_2, the noise budget is defined as:\nNoise \\ Budget = Size(\\phi_1) + Size(\\phi_2)\nThis metric ensures that the amplified jailbreak prompt remains concise and efficient.\nThe noise budget we consider for running for each of the amplified attacks is as below:\n\u2022 Amplified ReNeLLM: Noise Budget = 10 + 10 = 20\n\u2022 Amplified AutoDAN: Noise Budget = 5 + 20 = 25\n\u2022 Amplified GCG: Noise Budget = 5 + 10 + 20 = 35"}, {"title": "C Optimization Approach", "content": "Since \\phi_1 and \\phi_2 have distinct objectives, we adopt an interval optimization strategy to optimize them iteratively. Given that our optimization space is the text domain, where tokens must be updated discretely, we leverage the HotFlip method to find optimal token replacements. HotFlip identifies the best token candidates that degrade the loss function most effectively, ensuring efficient updates.\nTo balance the optimization of both \\phi_1 and \\phi_2, we alternate their updates as follows:\n\u2022 Update the first token of \\phi_1.\n\u2022 Switch to update the first token of \\phi_2.\n\u2022 Repeat for the second token of \\phi_1 and \\phi_2.\n\u2022 Continue this alternating process until all tokens have been optimized.\nThis alternating strategy ensures that both \\phi_1 and \\phi_2 evolve in parallel while maintaining their respective roles-Recomposition and Camouflaging. Algorithm 1 provides a pseudo-code representation of our optimization framework."}, {"title": "D Datasets", "content": "To evaluate the effectiveness of our jailbreak amplification method, we conducted experiments using two publicly available datasets: AdvBench and HarmBench. These datasets provide diverse scenarios of harmful prompts, allowing for a comprehensive assessment of jailbreak performance across different categories of misuse.\n\u2022 AdvBench: This dataset consists of 520 prompts designed to elicit malicious or harmful responses from LLMs. Each prompt was classified based on OpenAI's usage policy, which defines 13 distinct categories of harmful behaviour. To enhance organization, GPT-4 further grouped these prompts into seven"}, {"title": "E Implementation Details", "content": "To reproduce the results of baseline attacks, we run their original code with the following hyperparameters mentioned for each of them. Also, for all of the baseline attacks and our proposed method, Attention Eclipse, all the results are driven by omitting system prompts. The models we utilize for amplification and comparing original attacks and their amplified versions are some open-source models including Llama2-7b-chat, Llama2-13b-chat, Llama2-70b-chat, and Vicuna-13b-v1.5.\nReNeLLM: The original code exists in ReNeLLM GitHub. We run the code with the same hyperparameters as they used to evaluate their proposed attack.\nAutoDAN: There are some versions of the AutoDAN attack and we evaluate AutoDAN_GA which is proposed by. Compared to their existing code on AutoDAN GitHub, we just changed the number of steps to 50, and set the batch size to 64. The rest of the settings are the same as its original code.\nGCG: The code exists on GCG GitHub. We run the code by setting Adversarial_Suffix noise budget to 25, batch_size to 8, and n_steps to 1000."}, {"title": "F LLM Prompts", "content": "Table 7 shows the user prompts used to prompt LLMjudge to calculate the GPT-ASR of attacks, and the text Text Continuation prompt used to Amplify ReNeLLM attack."}, {"title": "G More JailBreak Examples", "content": "Table 8 showcases successful attacks where classic jailbreak prompts failed to bypass the model's alignment but were amplified to achieve success."}]}