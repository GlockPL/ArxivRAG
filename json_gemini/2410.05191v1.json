{"title": "LADEV: A Language-Driven Testing and Evaluation Platform for Vision-Language-Action Models in Robotic Manipulation", "authors": ["Zhijie Wang", "Zhehua Zhou", "Jiayang Song", "Yuheng Huang", "Zhan Shu", "Lei Ma"], "abstract": "Building on the advancements of Large Language Models (LLMs) and Vision Language Models (VLMs), recent research has introduced Vision-Language-Action (VLA) models as an integrated solution for robotic manipulation tasks. These models take camera images and natural language task instructions as input and directly generate control actions for robots to perform specified tasks, greatly improving both decision-making capabilities and interaction with human users. However, the data-driven nature of VLA models, combined with their lack of interpretability, makes the assurance of their effectiveness and robustness a challenging task. This highlights the need for a reliable testing and evaluation platform. For this purpose, in this work, we propose LADEV, a comprehensive and efficient platform specifically designed for evaluating VLA models. We first present a language-driven approach that automatically generates simulation environments from natural language inputs, mitigating the need for manual adjustments and significantly improving testing efficiency. Then, to further assess the influence of language input to the VLA models, we implement a paraphrase mechanism that produces diverse natural language task instructions for testing. Finally, to expedite the evaluation process, we introduce a batch-style method for conducting large-scale testing of VLA models. Using LADEV, we conducted experiments on several state-of-the-art VLA models, demonstrating its effectiveness as a tool for evaluating these models. Our results showed that LADEV not only enhances testing efficiency but also establishes a solid baseline for evaluating VLA models, paving the way for the development of more intelligent and advanced robotic systems.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent research has demonstrated the application of Large Language Models (LLMs) in various robotic domains [1], [2], where they are employed to tackle complex tasks that usually require human-like cognitive abilities, such as planning [3]\u2013[5], task comprehension [6]\u2013[8], and intention understanding [9]\u2013[11]. Building on these advancements, a growing number of recent works also employ Vision Language Models (VLMs) [12] to enhance robots with the ability to process visual inputs [13]\u2013[15]. It enables robots to interpret their surrounding environments and identify interactable objects, facilitating autonomous decision-making processes necessary for task completion.\nThis progress has given rise to a new class of end- to-end models known as Vision-Language-Action (VLA) models [16], [17], primarily designed for robotic manipulation tasks [18]\u2013[20]. The inputs to the VLA models consist of images captured by cameras and user-provided natural language instructions that describe the desired task. The VLA models then directly generate control commands, e.g., the pose of the end-effector, to guide the robotic manipulator in completing the assigned tasks based on the inputs [18] (see Fig. 1). These large pre-trained VLA models offer a novel approach to robotic control that not only mitigates the need for programming low-level task and motion controllers but also fosters direct interaction between robots and users through natural language instructions. This innovation marks a promising advancement toward achieving higher levels of robot intelligence, bringing us closer to the realization of fully autonomous and intelligent robotic systems [21].\nHowever, the data-driven nature of VLA models introduces several challenges. For example, the effectiveness of task execution is highly reliant on the quality of the training data used to develop these models [16]. Moreover, the limited interpretability of VLA models raises concerns about their reliability, robustness, and trustworthiness [22]. These challenges underscore the need for a comprehensive platform to test and evaluate the performance of VLA models across a variety of manipulation tasks and scenarios.\nUnfortunately, as an emerging field, related quality as- surance methods are still at a very early stage, and there is currently no platform specifically designed to test and evaluate VLA models automatically. This gap in evaluation frameworks highlights the need for reliable tools to measure the performance and robustness of these models. In response to this need, a simulation platform called SimplerEnv was introduced in [23]. Built on the SAPIEN simulator [24]"}, {"title": "II. RELATED WORK", "content": "In recent research, LLMs have been applied to various robotic tasks, such as decision-making [6], [8], [10] and reasoning [7], [9], [27]. For instance, [6] leverages LLMs' semantic capabilities to process natural language instructions, enabling robots to perform tasks assigned by humans through a value function. Similarly, [9] utilizes LLMs to evaluate the feasibility of task plans in a dialogue-based format, allowing robots to correct their actions as needed. Other works have explored using LLMs for task and motion planning [3], [28]\u2013[32]. For example, [28] uses LLMs to guide object rearrangement, improving both autonomy and efficiency. Meanwhile, [5] explores the potential of LLMs with a self-refinement mechanism for long-horizon sequential task planning, increasing task success rates compared to a zero- shot LLM approach. The incorporation of LLMs significantly advances robotic intelligence, enhancing both autonomy and interaction with human users.\nExtended from LLMs, an increasing number of studies now have utilized VLMs to equip robotic systems with the ability to process visual inputs [13], [33]. One common application of VLMs in robotics is reasoning about the environment and identifying interactable objects [34]\u2013[38]. For instance, [35] combines VLM and LLM to generate 3D affordance and constraint maps that guide robotic manipulation tasks. Similarly, [36] proposes a physically grounded VLM to improve the interaction between the robot and the object. [37] introduces a VLM-based navigation approach for determining the robot's motion in human-centered environments. The integration of visual processing capabilities further enhances robots' understanding of tasks and environments, opening up the potential for achieving general robotic intelligence [17].\nB. VLA Models in Robotics\nVLA models are end-to-end multi-modality foundation models evolved from VLMs [17], [39], [40]. Currently, most VLA models are designed for robotic manipulation tasks, such as pick-and-place and grasping [16], [18], [20], [41]\u2013[43]. One of the pioneering works in VLA models is RT-1 [18], which combines a FiLM EfficientNet and a transformer to learn control policies from 130k real-world robot demonstrations. RT-2 [41] advances RT-1 by introducing co-fine-tuning, integrating low-level control policies with high-level task planners to create a more comprehensive robotic system. Since the release of the Open X-Embodiment dataset [16], a series of VLA models have been developed by either training or fine-tuning on this dataset, such as Open- VLA [20], Octo [42], and LLaRA [43]. These models have demonstrated strong performance in their respective training environments, showing great potential for enabling intelligent robotic manipulation using only image and language inputs. However, ensuring the reliability and robustness of VLA models is challenging, as their performance heavily relies on the quality of the training data [2]. This necessitates an extensive testing and evaluation platform specifically designed for VLA models. As previously mentioned, the SimplerEnv, introduced in [23], provides valuable simulation environments. However, it requires manual adjustments for environment construction and neglects the impact of language inputs. To overcome these limitations, we therefore propose LADEV in this work, which enables a more efficient, comprehensive, and automated evaluation process for VLA models."}, {"title": "III. LADEV", "content": "In this section, we introduce details about the proposed LADEV platform. First, we describe how LLMs are leveraged to generate simulation environments from natural language descriptions of the desired manipulation scenarios. Next, we introduce a paraphrase mechanism that alters the given natural language task instructions. Lastly, we present a batch-style evaluation method that greatly accelerates the evaluation process with improved efficiency. Due to the page limit, detailed information about the prompts used in this work is presented in the pre-print version and the website of this paper: https://sites.google.com/view/ladev.\nThe core concept behind the automated generation of simulated manipulation environments is to convert natural lan- guage descriptions into simulator-compatible environmental configurations by leveraging LLMs. To achieve this, we use a fixed structure for the natural language description, which includes the following components:\n\u2022 Number and details of objects: First, we specify the total number of objects and provide additional details, such as their specific types and poses, to be included in the simulation environment. If object details are not needed, this part can be left blank.\n\u2022 Environmental setup: Then, we describe the environmental setup, including the lighting condition and camera pose. If not specified, predefined default values will be used.\nUsing this structured description, we apply a two-step process to separately handle the object configuration and environmental setup during the generation process.\n1) Object Configuration: We begin by using the de- scriptions of the number and details of objects to select appropriate models from LADEV's object model database, which combines the YCB object dataset [26] and the default dataset from SimplerEnv [23]. This is achieved by providing a predefined list of all available objects, along with the natural language description, to the LLM, which then generates a list of object addition operations. The length of this list corresponds to the specified number of objects, and each entry represents the addition of an object model to the simulation environment. If detailed object specifications are provided, the LLM prioritizes selecting models that best match the criteria. For example, if the user requests a coke can, LADEV searches for the relevant model and adds it if available. When no specific details are given, random objects are selected. Similarly, if specific object poses are provided, they are translated into corresponding coordinates. Otherwise, LADEV assigns random values within a predefined range.\n2) Environmental Setup: We then prompt the LLM with the description of the environmental setup to configure the simulation parameters. In the current version of LADEV, two environmental configurations are considered: the lighting condition and the camera pose. If a specific value is provided for the lighting condition, the LLM generates an operation command to adjust the scene's lighting intensity accordingly."}, {"title": "B. Natural Language Task Instruction Paraphrase", "content": "To evaluate the performance of VLA models in processing diverse language inputs, we also propose a paraphrase mechanism that generates varied natural language task in- structions. The paraphrase mechanism consists of two phases: a generation phase and a validation phase. The input to the generation phase is an original task instruction that follows the standard format used in previous works [18], [20], [41], [42], such as using \"pick up apple\" to describe a task involving picking up an apple. The goal of the generation phase is to produce a predefined number, $k$, of alternative instructions that convey the same meaning but differ in sentence structure and wording. For example, the original instruction \"pick up apple\" could be rephrased as \"grasp apple\", \"let's pick the apple\u201d, or \u201ccan you lift the apple\", etc. This is achieved by prompting an LLM with the original instruction and guidelines for generating alternative sentences. The LLM then outputs $k$ variations of the task instructions with distinct wordings and structures.\nAfter generating a set of candidate sentences, we implement a validation phase to ensure that each sentence accurately retains the same meaning as the original, ensuring the validity of the paraphrased sentences. This is achieved by using a sentence BERT model [45] for similarity checking. Specifically, we utilize the sentence BERT model to compute embeddings for each language task instruction and measure the semantic similarity between the original ones and the can- didate variations. If the similarity value exceeds a predefined threshold, the varied task instruction is considered to have the same meaning and is deemed valid. These valid instructions are then used to evaluate the VLA models' performance in handling diverse language inputs.\nBy utilizing the proposed natural language task instruction paraphrase mechanism, we significantly enhance the diversity of language inputs for VLA models. This approach addresses a crucial gap in the overall evaluation, focusing on the previ- ously overlooked aspect of assessing how natural language task instructions impact the performance of VLA models."}, {"title": "C. Batch-Style Evaluation", "content": "Through the methods proposed in Sec.III-A and Sec.III-B, we can generate a single manipulation scenario with multiple diverse language task instructions that convey the same task. However, for a comprehensive evaluation of the VLA model, it is crucial to test its performance across various scenarios and tasks. To expedite this process, we introduce a batch-style evaluation approach that automatically generates a specified number of distinct manipulation scenarios from a single natural language input.\nSpecifically, we instruct the LLM to automatically generate diverse language inputs for executing both the simulation environment generation and task instruction paraphrase pro- cesses. Suppose the user wishes to create $n$ test scenes with $k$ task instructions per scene. In this case, the LLM is prompted to randomly generate $n$ sets of natural language inputs that specify objects and environmental setups, along with $n$ original task instructions following the standard format. Each original task instruction is then used in the paraphrase process to generate $k$ variations, resulting in a total of $n \\times k$ language inputs. Due to the limited space, the detailed prompt templates and examples used for in-context learning are provided on the website accompanying this paper.\nThe proposed batch-style evaluation approach enables efficient large-scale testing of VLA models, facilitating a more comprehensive and reliable assessment of their robustness and effectiveness. In the next section, we apply this approach to evaluate the performance of multiple state-of-the-art VLA models across various manipulation scenarios."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we first present the details of our experimen- tal setup. Then, we utilize a concrete example to illustrate how LADEV generates a simulation environment from a natural language input and paraphrases the task instruction. Finally, we present our large-scale experimental results in assessing the performance of multiple state-of-the-art VLA models, which demonstrate the efficiency and effectiveness of the proposed LADEV for VLA models' evaluation.\nWe consider four robotic manipulation tasks in LADEV for evaluating VLA models: (1) Pick up an object; (2) Move object A near object B; (3) Put object A on object B; and (4) Put object A inside object B. For performance evaluation, we assess the impact of the following factors:"}, {"title": "V. DISCUSSION", "content": "By combining visual, linguistic, and action-based infor- mation, VLA models offer several advantages to robotic systems. For instance, they enable robots to better inter- pret their surroundings and execute tasks using natural language instructions, reducing the dependence on hardcoded or structured inputs. This leads to more intuitive human- robot communication, enhancing interaction flexibility while boosting the robots' autonomy and intelligence. However, VLA models also face multiple challenges. For example, training these models requires large, multi-modal datasets and significant computational resources. Unfortunately, high- quality datasets that align visual inputs, language descrip- tions, and corresponding actions are scarce. Moreover, our experiments show that current VLA models still struggle with even simple manipulation tasks under varied conditions, highlighting the need for deeper exploration in this area to achieve better performance.\nOne limitation of the current version of LADEV is that it only considers four manipulation tasks. This is primarily due to the fact that state-of-the-art VLA models are still only trained for simple tasks. Another drawback is that our evaluations are conducted solely in simulations, as performing comprehensive real-world experiments is difficult and resource-intensive. To reduce the gap between simulation and reality, we adopt the same approach as SimplerEnv by using real-world images as backgrounds for the visual inputs to the VLA models. However, further research is needed to better minimize the simulation-to-reality gap and develop more efficient methods for assessing the performance of VLA models in real-world conditions.\nFor future work, we plan to expand the LADEV platform by incorporating more object models and manipulation scenarios, which would greatly increase its diversity and effectiveness. Another potential direction is to compare the performance of VLA models in both simulation and real-world environments across several representative tasks, serving as an indicator of their reliability in practical applications. However, how to select the most appropriate and representative tasks will require further in-depth research."}, {"title": "VI. CONCLUSION", "content": "In this work, we propose LADEV, a testing and evaluation platform for VLA models in robotic manipulation tasks. By introducing a language-driven framework, we efficiently generate simulation environments from simple natural lan- guage inputs, mitigating the need for manual adjustments. To assess the impact of language instructions on VLA models, we also present a task instruction paraphrase approach that automatically generates diverse sentences to enrich the language input. Moreover, to further improve the evaluation efficiency, we develop a batch-style mechanism that creates multiple testing scenarios from a single command, enabling a comprehensive and streamlined assessment of VLA models' performance. Our platform notably improves the evaluation process and establishes a strong baseline for advancing VLA models, paving the way for more intelligent robotic systems with enhanced autonomy and decision-making capabilities."}]}