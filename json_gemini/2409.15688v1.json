{"title": "Safe Navigation for Robotic Digestive Endoscopy via Human Intervention-based Reinforcement Learning*", "authors": ["Min Tan", "Yushun Tao", "Boyun Zheng", "GaoSheng Xie", "Lijuan Feng", "Zeyang Xia", "Jing Xiong"], "abstract": "With the increasing application of automated robotic digestive endoscopy (RDE), ensuring safe and efficient navigation in the unstructured and narrow digestive tract has become a critical challenge. Existing automated reinforcement learning navigation algorithms, often result in potentially risky collisions due to the absence of essential human intervention, which significantly limits the safety and effectiveness of RDE in actual clinical practice. To address this limitation, we proposed a Human Intervention (HI)-based Proximal Policy Optimization (PPO) framework, dubbed HI-PPO, which incorporates expert knowledge to enhance RDE's safety. Specifically, we introduce an Enhanced Exploration Mechanism (EEM) to address the low exploration efficiency of the standard PPO. Additionally, a reward-penalty adjustment (RPA) is implemented to penalize unsafe actions during initial interventions. Furthermore, Behavior Cloning Similarity (BCS) is included as an auxiliary objective to ensure the agent emulates expert actions. Comparative experiments conducted in a simulated platform across various anatomical colon segments demonstrate that our model effectively and safely guides RDE. Our code and platform is available at http://siat-medical-robot-hippo.index.", "sections": [{"title": "I. INTRODUCTION", "content": "Clinical digestive endoscopy examinations are crucial medical procedures, which are regarded as the gold standard in early diagnosis of colorectal cancer [1], [2]. Traditional endoscopy carries the risk of endoscopic looping, and more seriously, a perforation rate of 0.1% to 0.3% during insertion [3]\u2013[6]. These complications can cause patient pain and potential tissue damage [7], [8]. In contrast, automated Robotic Digestive Endoscopy (RDE), including magnetic endoscope robots [2], [5] and vision-based autonomous navigation robots [9], [10], are designed to overcome the problems of manually navigating the endoscope, such as the non-intuitive control and the perforation risk. However, as the use of RDE systems increases, achieving safe and efficient navigation through the unstructured and narrow digestive tract has become a critical challenge.\nThe mainstream RDE uses Reinforcement learning (RL) to navigate due to its ability to learn complex control policies from environmental interactions, making it suitable for unstructured and dynamic environments like the human digestive tract. For instance, Davide et al. [11] proposed constrained reinforcement learning to ensure safety during colonoscopy navigation by introducing formal verification techniques to reduce risks such as tissue damage or perforation. Similarly, Ameya et al. [12] proposed RL-based visuomotor control to improve navigation adaptability in complex colon sections by learning from endoscopic images. Li et al. proposed RL-TEE, which employs a deep reinforcement learning method augmented with self-attention mechanisms to achieve 3-DOF control in echocardiography probe guidance. However, existing automated RL navigation algorithms can still pose risks, such as unsafe collisions, when human intervention is absent [11], [13]. These issues severely limit the safety and effectiveness of the RDE in real clinical practice [14], [15].\nTo address these safety and effectiveness challenges, recent research has explored integrating Human Intervention (HI) mechanisms with RL frameworks. Wu et al. [16] proposed Human-guided RL for uncrewed ground vehicles to provide demonstrations by allowing human experts to correct unsafe behaviors during the learning process. Saunders et al. [17] demonstrated that human oversight could replace unsafe RL actions with safe alternatives, significantly improving the survival rate of RL agents in playing Atari games. Another method to improve RL performance is the integration of behavior cloning (BC) objectives, allowing the RL agent to imitate human actions directly. Vecerik et al. [18] introduced a deep deterministic policy gradient (DDPG) algorithm with human imitation, showing superior performance to standard RL algorithms. Furthermore, Wang et al. [19] proposed an intervention aided RL framework to ensure control unmanned aerial vehicles (UAVs) safely. However, given the complexity of the scenarios involving robotic endoscopes, research based on human intervention RL in this field is relatively limited.\nBase on the above observations, we propose a Human Intervention (HI)-based Proximal Policy Optimization (PPO) framework, termed HI-PPO, that introduces expert knowledge and improves the safety of the RDE. Specifically, the Enhanced Exploration Mechanism (EEM) with HI is proposed to accelerate the exploration efficiency of standard PPO. The Reward-Penalty Adjustment (RPA) is proposed to penalize unsafe actions in the first intervention. In addition, the Behavior Cloning Similarity (BCS) is included as an auxiliary objective to ensure the agent emulates expert actions. Comparative experiments conducted in a simulation environment across various anatomical colon segments demonstrate that our model could guides RDE effectively and safely. Our main contributions are as follows:\n\u2022 Human Intervention-based PPO framework (HI-PPO): We develop a HI-PPO framework to improve the safety of robotic digestive endoscopy (RDE) by incorporating expert knowledge.\n\u2022 Enhanced exploration mechanism (EEM): The EEM addresses the low exploration efficiency of traditional PPO, enabling more efficient navigation.\n\u2022 Reward-penalty adjustment (RPA): The RPA penalizes unsafe actions at the first human intervention, encouraging safer policy learning.\n\u2022 Behavior cloning similarity (BCS): The BCS ensures the agent imitates expert actions, improving learning performance in complex environments."}, {"title": "II. METHODOLOGY", "content": "Reinforcement Learning (RL), as a classical form of Markov Decision Processes (MDP), aims to maximize the cumulative reward over the course of an episode. Formally, lett be a sequence of $(O_t, a_t,r_t)$, where $a_t$ is selected according to the policy $\\pi(\\theta|O_t)$ based on the observation $O_t$, and $s_{t+1}$ is determined according to the transition function $T(s_t, a_t)$. To balance the trade-off between exploration and exploitation, Proximal Policy Optimization (PPO) [20] emerges as a robust on-policy reinforcement learning technique. Within the Actor-Critic framework, PPO updates both the actor (policy network) and critic (value network) to optimize performance and maximize cumulative rewards. PPO proposes a clipped objective function, which meticulously controls the policy updates to maintain a stable learning trajectory. This mechanism caps the change in policy, ensuring that the probability ratio $r_t(\\theta)$, indicative of the action likelihood under both the current and previous policies, stays within a predefined range. The objective function is given by:\n$J^{PPO}(\\theta) = E_t \\left[ min \\left(r_t (\\theta) A_t, clip \\left(r_t (\\theta), 1 \u2013 \\epsilon,1 + \\epsilon\\right) \\hat{A_t} \\right)\\right]$,\n$r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t| S_t)}{\\pi_{\\theta_{old}}(a_t | S_t)}$\nwhere $E_t[]$ denotes the expectation over time steps t, $r_t(\\theta)$ is the importance weight between the current policy $\\pi_{\\theta}$ and the old policy $\\pi_{\\theta_{old}}$, $A_t$ is the advantage estimate at time step t, and $clip(r_t(\\theta),1 \u2013 \\epsilon,1 + \\epsilon)$ limits the update to avoid excessive changes to the policy, with $\\epsilon$ being a hyper-parameter controlling the clipping range.\nWhile Proximal Policy Optimization (PPO) has shown promise in robotic navigation, it still suffers from limitations such as unsafe collisions and suboptimal decision-making when human intervention is absent, which can lead to significant risks during navigation [11], [13]. These chal-lenges hinder the safety and reliability of robotic digestive endoscopy (RDE) in real clinical practice, highlighting the need for incorporating Human Intervention (HI) mechanisms to mitigate these issues [14], [15]."}, {"title": "B. Overview Framework", "content": "To enhance safety and performance of RDE navigation, we establish a human intervention RDE agent via incorporating three novel modules into the vanilla RL architecture. To improve the safety of robotic digestive endoscopy, we propose the Human Intervention. The Reward-Penalty Adjustment (RPA) promotes safer policy learning by penalizing unsafe actions through a penalty score during initial interventions. The Behavior Cloning Similarity (BCS) improves learning performance by guiding the HI-PPO agent to imitate expert actions, integrating similarity objective into the actor-critic network for optimal policy learning. Enhanced Exploration Mechanism (EEM) integrates expert knowledge through improve state exploration efficiency, where human intervention takes over control when agent getting stuck. These mechanisms work together to enable real-time navigation actions for adapting to complex environments. The architecture of HI-PPO is shown in Figure 1, and more details will be illustrated in the following sections."}, {"title": "C. Enhanced Exploration Mechanism", "content": "Standard PPO expands the agent's search space by introducing randomness into the exploration strategy. However, in the confined scenarios of RDE navigation, unsafe exploration can increase the risk of patient injury, and greedy exploration can lead to increased feedback latency. Especially, the high-dimensional state space and complex navigation paths in the colon environment require a large number of samples to fully explore, which leads to long training times and a tendency for agents to produce poor trajectories during navigation [11]. By contrast, human-guided exploration can improve this process by introducing human intervention and expert knowledge during training, replacing inefficient actions with expert-corrected actions [16]. Therefore, we proposed the Enhanced Exploration Mechanism (EEM) implemented by a modified behavior policy $\\pi_b$ to include human actions when intervention occurs:\n$\\pi_b(a_t|s_t) = (1 \u2212 m_t)\\pi_{\\theta}(a_t|s_t) + m_ta,$\nwhere $m_t$ is a binary indicator for human intervention ($m_t = 1$ when human intervention occurs; otherwise, $m_t = 0$), and $a_t$ represents the human action at time step t. This method ensures that human demonstrations are incorporated directly into the agent's learning process, improving data quality and exploration efficiency. By integrating these actions into the agent's experience buffer, PPO benefits from both autonomous exploration and human expertise in critical moments."}, {"title": "D. Reward-penalty Adjustment", "content": "To further guide the learning process, reward adjustment is implemented by penalizing the agent when human intervention is required. Human intervention serves as a signal that the agent's current policy is potentially unsafe. In this context, the reward function is adjusted to discourage the agent from reaching states that lead to intervention when the first intervention occurs in a sequence of actions. The base $r_t$ consists of a position score and an orientation score, computed by the current distance to the waypoints and the orientation alignment. The Reward-penalty Adjustment (RPA) is implemented by an adjusted reward $r^{new}$:\n$r^{new} = r_t + \\lambda \\cdot \\delta(m_t > m_{t-1} = 0)$,\nwhere $\\lambda$ is a negative constant indicating the penalty, and $\\delta$ is the indicator function for the first instance of human intervention ($m_1 = 1$ and $m_{t-1} = 0$). This modification to the reward function ensures that the agent learns to avoid behaviors that require human correction by discouraging unsafe actions at the start of the intervention, effectively improving the safety and robustness of the whole learning process."}, {"title": "E. Behavior Cloning Similarity", "content": "In addition to reward adjustment, behavior cloning (BC) is integrated as an auxiliary objective to improve policy learning. This is achieved by adding a behavior cloning loss term to the standard PPO objective function, ensuring that the agent learns to emulate human behavior while simultaneously optimizing the cumulative reward. The new objective function for PPO with Behavior Cloning Similarity (BCS) is:\n$J^{HIL-PPO} (\\theta) = E_t \\left[ min \\left(r_t (\\theta) A_t, clip \\left(r_t (\\theta), 1 \u2013 \\epsilon,1 + \\epsilon\\right) \\hat{A_t} \\right)\\right] + \\alpha\\cdot sim(\\pi_{\\theta}(a_t|s_t), a)$,\nwhere $\\alpha$ is a hyper parameter that controls the balance between behavior cloning and reward optimization, and $sim(\\pi_{\\theta}(a_t|s_t), a)$ measures the similarity between the agent's action and the human's action. Specifically, the cross-entropy is employed to compare the predicted probability distribution of the agent's actions with experts:\n$sim(\\pi_{\\theta}(a_t)s_t), a) = - \\sum_{a} log \\pi_{\\theta}(a_t|s_t)$\nThis term encourages the agent to assign higher probabilities to the actions taken by humans, ensuring action matching. Additionally, it penalizes the agent for assigning high probabilities to incorrect actions, forcing the policy to adjust and better follow human guidance. The modified BC objective function ensures smooth updates and imitation, thus improving performance in discrete action spaces.\nBy integrating human intervention into the exploration process, adjusting the reward function, and incorporating a behavior cloning objective, the PPO framework is enhanced to ensure greater safety and effectiveness in the complex, unstructured, and narrow environment of the digestive tract."}, {"title": "III. EXPERIMENTS", "content": "Robotic System Design: We developed a robotic system to operate traditional digestive endoscopy. As shown in Fig. 2, the robot uses a coaxial mechanism to control the up/down (U/D) and left/right (L/R) knobs independently. The insertion of the endoscope is handled by a delivery system with two friction wheels. More details can be found in our previous work in [21].\nMotion Modeling of RDE Agent: Based on the robotic design, we complete the motion simulation of RDE so that the movement of the colonoscopy in the RL environment is consistent with the real colonoscopy, as shown in Fig. 2.\nAs shown in Algorithm 1, the colonscope motion control algorithm navigates by identifying the farthest point from input image frames. The process begins by initializing bone rotations R[m] and bend directions D[n], followed by setting initial parameters for direction tracking and bend status. Our algorithm attempts to find the farthest point Pfarthest from the input image I, directing the system to move towards it if identified. Otherwise, it systematically explores four alternative bend directions. For each direction, it calculates the current desired bone angle @cur and checks if it exceeds the maximum allowable angle @max. If the angle limit is reached, the direction is marked as fully bent, and the algorithm moves to the next direction. The process is controlled iteratively, balancing between bending and moving, ensuring the scope navigates effectively without exceeding mechanical limits."}, {"title": "B. Experiment Configuration", "content": "Hardware configuration: The system is equipped with an Intel Xeon Gold 6154 CPU at 3.00GHz, 128GB of RAM at 2666 MHz, and an NVIDIA GeForce RTX 3090 GPU with 24GB of VRAM.\nSoftware configuration: Our reinforcement learning en-vironment is implemented using Unity version 2022.3.34f1 with the algorithm based on ML-Agents version 2.3.0 [22].\nAlgorithm configuration: Our reinforcement learning algorithm was fine-tuned through multiple rounds of hyperparameter tuning, utilizing grid search to identify the optimal configuration. The final parameter settings for SAC, PPO, and our method are shown in the Table I.\nColon Segmentation and Environmental Visualization: We utilized a colon segment with a total length of 928.84 mm. Following techniques in [23], the colon was rendered in high definition within the Unity environment, making the internal colon visuals resemble real clinical images. Fig. 3 shows clinical images and virtual environment images captured from different anatomical segments of the colon. Based on guidance from clinicians, waypoints were set at different anatomical segments of the colon to serve as navigation targets, divided into six segments: Rectum, Sigmoid, Descending, Transverse, Ascending, and Cecum."}, {"title": "Evaluation Metric", "content": "Average Trajectory Error (ATE).\n$ATE = \\frac{1}{N}\\sum_{i=1}^N ||P_i \u2212 P_i||_2,$"}, {"title": "C. Quantitative Comparison with Baselines", "content": "The quantitative results are shown in Table II. Compared to the SAC and PPO algorithms, our method consistently achieves lower ATE across varying lengths and complexities of colon segments. Specifically, our method attained an ATE of 2.353 mm in the shortest rectal segment, significantly lower than SAC's 4.915 mm and PPO's 4.687 mm. Furthermore, our method exhibits greater stability and minimal error fluctuations compared to other methods, as indicated by the standard deviation. Clinically, since the average diameter of the colon is approximately 70 mm [28], a margin of 10 mm could be considered a safe distance in robot-assisted colon surgeries [29] [30]. According to the quantitative results, our method remains within the safe distance range except for the complex Sigmoid colon segment. After incorporating the HI mechanism, our method demonstrates an average improvement of 38.63% compared to the standard PPO baseline after excluding the highest and lowest values among six segments."}, {"title": "D. Security Evaluation", "content": "In surgical scenarios involving intestinal navigation, simply measuring the accuracy of the navigation trajectory is insufficient. Security evaluation is another critical evaluation metric, as it not only reflects the safety of the procedure but also helps minimize the risk of secondary injuries to the patient, result in reducing discomfort and potential complications. To this end, we recorded the number of collisions for each RL algorithm and calculated the corresponding security coefficient, as shown in Fig. 4 and Table II.\nIn intestinal surgery, a procedure is considered especially secure when the safety factor exceeds 0.90 [24] [25]. Compared to the other two RL algorithms, our method demonstrate fewer collisions and achieves a higher safety coefficient, with the most significant improvements observed in the longest and most narrow Sigmoid colon. During navigation in this section, our method consistently achieved a safety coefficient exceeding 0.90, whereas the other two methods were unable to reach this threshold. This demonstrates the strong adaptability of our approach in complex environments. In simpler and shorter sections, such as the Transverse colon, collisions using our method were almost negligible, with the safety coefficient reaching as high as 0.9937. However, it is worth noting that while our algorithm improved the safety coefficient in the Rectum segment, it failed to exceed 0.90. The reason is related to the structural environment of the Rectum, which features a large diameter but connects to the Sigmoid through a very narrow passage, increasing the difficulty of navigation. More detailed visualization of the connection between the Rectum and Sigmoid can be seen in Fig. 3. These results indicate that our method is not only more accurate but also provides safer navigation in complex surgical environments."}, {"title": "E. Efficiency Evaluation", "content": "In addition, the execution time of the algorithm is equally important. Faster navigation, while maintaining trajectory accuracy and safety, often translates to shorter surgical times and less discomfort for the patient. The time spent in different segments is shown in Fig. 6. Overall, our algorithm demonstrates a significant reduction in execution time compared to the other two methods. This reduction is particularly noticeable in the Sigmoid segment, where the path is long and complex with numerous curves. Even in shorter and simpler segments, such as the Rectum and Cecum, our algorithm also shows superior efficiency."}, {"title": "F. Visualization", "content": "In addition to quantitative metrics, the visualization further validate the feasibility and effectiveness of our approach. We compare the trajectories generated by different algorithms with the GT, as shown in Fig. 5. Despite the larger ATE in the Sigmoid segment due to its length and curvature, our method still exhibits superior trajectory consistency with GT in Fig. 5 (b). In Fig. 5 (b), the SAC algorithm exhibits evident deviation during the latter part of the turn, which deviating from the expected path within the intestinal environment. PPO exhibits significant local fluctuations in scenarios (b), (e), and (f), which may lead to unsafe behaviors. In the trajectory comparisons across six different segments, Fig. 5 (d) and 5 (e) highlight the challenge that sharp turn scenarios pose to the rapid response of reinforcement learning algorithms. Compared to the other two methods, which both exhibited noticeable deviations, our approach demonstrates better adaptability and superiority in handling complex navigation scenarios."}, {"title": "IV. CONCLUSIONS", "content": "This study addresses the challenges of safe and efficient navigation in automated robotic digestive endoscopy (RDE) within the unstructured and narrow confines of the digestive tract. We proposed a Human Intervention (HI)-based PPO framework to incorporate expert knowledge in order to address the safety and effectiveness of automated robotic digestive endoscopy. Experiments in a simulated environment demonstrate that HI-PPO can safely guide RDE, indicating its strong potential for clinical application. Future work will focus on validating the framework in real clinical environments and exploring additional methods to further enhance the safety and practicality of RDE."}]}