{"title": "Personalized Quantum Federated Learning for Privacy Image Classification", "authors": ["Jinjing Shi", "Tian Chen", "Shichao Zhang", "Xuelong Li"], "abstract": "Quantum federated learning has brought about the improvement of privacy image classification, while the lack of per- sonality of the client model may contribute to the suboptimal of quantum federated learning. A personalized quantum federated learning algorithm for privacy image classification is proposed to enhance the personality of the client model in the case of an imbalanced distribution of images. First, a personalized quantum federated learning model is constructed, in which a personalized layer is set for the client model to maintain the personalized parameters. Second, a personalized quantum federated learning algorithm is introduced to secure the information exchanged between the client and server. Third, the personalized federated learning is applied to image classification on the FashionMNIST dataset, and the experimental results indicate that the personal- ized quantum federated learning algorithm can obtain global and local models with excellent performance, even in situations where local training samples are imbalanced. The server's accuracy is 100% with 8 clients and a distribution parameter of 100, outperforming the non-personalized model by 7%. The average client accuracy is 2.9% higher than that of the non-personalized model with 2 clients and a distribution parameter of 1. Compared to previous quantum federated learning algorithms, the proposed personalized quantum federated learning algorithm eliminates the need for additional local training while safeguarding both model and data privacy. It may facilitate broader adoption and application of quantum technologies, and pave the way for more secure, scalable, and efficient quantum distribute machine learning solutions.", "sections": [{"title": "I. INTRODUCTION", "content": "FEDERATED Learning (FL) [1], an innovative branch of distributed machine learning, has been widely used in edge computing [2], image recognition [3], and medical diagnosis [4], etc. Since FL trains on distributed data, it can alleviate the problem of data island (data is isolated and disconnected within an organization or enterprise) [5]. In the training process of FL, instead of directly transmitting the original data, the gradient or model parameter information is transmitted, which can protect data privacy to a certain extent. However, FL still faces two main problems. On the one hand, due to the heterogeneity of decentralized data, such as label distribution and quantity skew, the accuracy of the federated learning model may be seriously degraded [6]. On the other hand, many attacks on gradients and model parameters make the transmission of information no longer secure, and the privacy of the original data would be threatened. For example, gradient attack can use gradient inversion attack to obtain the original data of the users, and by modifying the model parameters, the attacker would expose more local private data during the training of federated learning [7]. Fortunately, the parallelism inherent in quantum computing and the confidentiality offered by quantum communication have introduced new developmental opportunities for federated learning, leading to the emergence of Quantum Federated Learning (QFL) [8], which has become a significant research field by merging the benefits of Quantum Machine Learning (QML) with the distributed machine learning approach. QFL using quantum circuits or quantum channels, further develops classical FL and brings potential quantum advantages. On the one hand, QML uses the entanglement and superposition characteristics of quantum states to present the parallelism of quantum computing [9], [10], which has the effect of exponential acceleration compared to classical computing [11]. The parameterized quantum circuit with only a few parameters can achieve the effect of classical machine learning models using massive parameters. On the other hand, quantum Internet has the potential to enable secure and private communication [12]. For example, the non-cloning theorem [13] prohibits the replication of an arbitrary unknown quantum state, which helps prevent an attacker from duplicating the quantum state during communication. Due to its high efficiency and security, QFL is suitable for privacy image classification, which is an important task in distributed machine learning. However, there are two problems with privacy image classification based on QFL. One is to strike a tradeoff between the security of privacy images and the accuracy of classification [14], and another is to reduce the negative impact of the non-independent identically distribution (non-IID) of privacy images [15]. In recent years, numerous studies have concentrated on how quantum federated learning can improve the accuracy and security of privacy image classification. In 2021, Li et al. [8] proposed a QFL through blind quantum computing, where differential privacy is used to preserve the training gradient of the privacy images. In 2022, Huang et al. [16] introduced a QFL framework built on the variational quantum algorithm, which achieves high accuracy on the MNIST dataset. In 2023, Song et al. [17] proposed a QFL framework for classical clients using the technique of shadow tomography, thereby removing the necessity for clients to use scarce quantum computing resources in image classification. In 2024, Qu et al. [18] introduced a quantum fuzzy federated learning, manifesting in faster training effi- ciency, higher accuracy, and enhanced security. Luca et al. [19] proposed a hybrid quantum federated learning for hepatic steatosis image diagnosis. However, the above studies have ignored the lack of personality of the client model in non-IID scenarios, which leads to the suboptimal performance of QFL. To enhance client model personalization while maintaining the security of private images in FL, we propose a Personalized Quantum Federated Learning (PQFL) model. By introducing a personalized layer, the client model retains more person- alized parameters, to solve the issue of suboptimal model training performance caused by the lack of personalization in the traditional federated learning client model, which lays a theoretical foundation for the design and application of secure and efficient quantum federated learning algorithm. Furthermore, we design a personalized quantum federated learning algorithm, including aggregation, local training, and global updating, which ensures the security of data interaction between client and server and the privacy of client image data. In addition, we explore the application of the personalized quantum federated learning algorithm in the privacy image classification task on the FashionMNIST dataset. The main contributions of this work are summarized as follows. \u2022 A personalized quantum federated learning model is pro- posed to achieve personalization in client model during the global training process without the need for additional local training. \u2022 A personalized quantum federated learning algorithm is designed, using quantum channels to securely aggregate model parameters and protect the privacy of client image data during server-client interactions. \u2022 The image classification experiments carried out on the FashionMNIST dataset demonstrate that our PQFL method shows superior performance compared with the non-personalized quantum federated learning method. The outline of the paper is listed as follows. In Section II, we review the related works to quantum computing, quantum neural network and federated learning. Then, a personalized quantum federated learning method is proposed in Section III. Section IV presents the experimental setup and analyzes the performance and security of the proposed method. And the conclusion of our work is provided in the last section."}, {"title": "II. RELATED WORK", "content": "A. Preliminary 1) Qubit: Different from the classical bit, a quantum bit (qubit) has the distinguishing characteristic of existing in a superposition state of |0) = [1,0]T and |1) = [0,1]T [20], which can be noted as follows.\n$\\vert \\varphi \\rangle = \\alpha\\vert 0 \\rangle + \\beta\\vert 1 \\rangle,$\nwhere $\\vert \\alpha \\vert^2 + \\vert \\beta \\vert^2 = 1$. In addition, $\\vert \\alpha \\vert^2$ and $\\vert \\beta \\vert^2$ are the occurrence probability of state $\\vert 0 \\rangle$ and $\\vert 1 \\rangle$, respectively. 2) Evolution: The quantum system always changes its state over time, which is called as evolution. The evolution process can be mathematically described as:\n$\\vert \\psi_2 \\rangle = U \\vert \\psi_1 \\rangle ,$\nwhere U is a unitary matrix satisfying $U^{\\dagger}U = I$, also called quantum gates. 3) Measurement: Measurement representing a non-unitary operation is an irreversible process, which is represented by a group of measure operators $M_m$, satisfying $\\Sigma_m M_m^{\\dagger} M_m = I$. Suppose the quantum system state is $\\vert \\psi \\rangle$ before the measure operation, and the probability of measured result m can be calculated as follows:\n$p(m) = \\langle \\psi\\vert M_m^{\\dagger} M_m\\vert \\psi \\rangle.$\nThe sum of the measured probabilities $\\Sigma_m p(m)$ equals 1. After measuring the system state, the quantum state collapses into:\n$\\vert \\psi' \\rangle = \\frac{M_m\\vert \\psi \\rangle}{\\sqrt{p(m)}}.$\nIn quantum machine learning [21], Pauli measurement is usually employed to determine the expected value of the current quantum state, and its measurement operator is the Pauli operator like X, Y, and Z gate. The expected value can be calculated as:\n$E = \\langle \\psi\\vert H_{ams}\\vert \\psi \\rangle.$\nwhere $H_{ams}$ is a Hamiltonian measured by Pauli on certain qubits.\nB. Quantum Neural Network A Quantum Neural Network (QNN) [22], also called a parameterized quantum circuit built on a recent quantum com- puter, with its parameters optimized on the classical computer, which is shown in Fig. 1. The quantum circuit of QNN can be represented as a series of unitary gates:\n$U_{QNN}(\\alpha, \\theta) = U_{ansatz}(\\theta)U_{encoder}(\\alpha).$"}, {"title": "C. Federated Learning", "content": "Federated learning enables collaborative training among users without exposing local data, resulting in a global machine-learning model. In recent years, research on FL has emerged in an endless stream. J. Konecny et al. [1] initially introduced the idea of federated learning in 2016, and a feder- ated optimization algorithm for distributed data was proposed. In the next year, McMahan et al. [31] introduced the well- known FedAvg algorithm, which established the foundation for FL. In 2023, Sun et al. [32] proposed an efficient federated learning method using an adaptive client optimizer in image classification. In 2024, Peng et al. [33] proposed federated proxy fine-tuning to enhance foundation model adaptation in downstream tasks through FL. Since then, numerous studies have been conducted, exploring various aspects and extensions of federated learning. Among these, personalized federated learning model has attracted attention due to its capability to tailor models to individual client needs, while quantum federated learning is emerging as an innovative approach that leverages quantum computing principles to improve the security and performance of FL. 1) Personalized Federated Learning: It can be broadly categorized into two types of personalized federated learning: personalized for the global model and personalized for the learning model. The former means that training a global model first, then conducting additional local training on each client's dataset to personalize it. In 2020, Dinh et al. [34] proposed a FL algorithm utilizing Moreau envelopes as the regularized loss functions for clients, which helps utilize the global model in FL to create a personalized model tailored to each client's data. In 2021, Acar et al. [35] trained a meta-model globally, which is later customized locally to suit every device's specific needs. In addition, personalized for the learning model means that creating tailored models by adjusting the FL model aggregation process, accomplished through various learning paradigms in FL. In 2022, Ma et al. [36] introduced a layer- wise FL model, which identifies the significance of each layer from various local clients, thereby optimizing the aggregation for personalized client models with the heterogeneous data. In 2023, Li et al. [37] presented a new transformer-based FL framework that personalizes self-attention for clients, sharing and aggregating other parameters of clients. Xu et al. [38] introduced a personalized FL approach, where explicit local- global feature alignment is achieved by utilizing global se- mantic knowledge to enhance representation learning. The above studies indicate that personalized federated learn- ing can solve the problem of poor convergence of traditional federated learning algorithms on highly heterogeneous data, which is also a challenge faced by QFL. 2) Quantum Federated Learning: In 2021, Li et al. [8] developed a QFL scheme by utilizing the principles of blind quantum computing, preserving the security of private data and offering valuable guidance in exploring quantum advantages in machine learning, particularly from a security standpoint. In 2022, Huang et al. [16], influenced by the classical feder- ated learning algorithm, proposed the efficient communication learning of variational quantum algorithms from scattered data, known as quantum federated learning, which inspired new research in secure quantum machine learning. In 2023, Waleed et al. [39] introduced an optimized QFL framework aimed at safeguarding intelligent transportation systems and offer- ing enhanced protection against various types of adversarial attacks. However, current quantum federated learning algorithms rarely consider the personalization of federated learning mod- els, especially in personalized learning models. Therefore, we propose a personalized quantum federated learning algorithm to improve the personalization of client model with non-IID images in QFL."}, {"title": "III. PERSONALIZED QUANTUM FEDERATED LEARNING", "content": "A. PQFL Model Fig. 2 illustrates the basic architecture of the PQFL model, which includes a server and several clients, all of which have quantum computing capabilities. Firstly, the server model is a QNN containing an encoder $U_{encoder}(\\alpha)$, an ansatz $U_{base}(\\theta_b)$ where the base layer repeats k times and measurement, which is shown in Fig. 2 (b). The server model can be expressed as:\n$U_{server}(\\alpha, \\theta) = U_{base}(\\theta_b)U_{encoder}(\\alpha).$\nThe structure of the client model is consistent with that of the server, but an additional personalized layer $U_{personal}(\\theta_p)$ as shown in Fig. 2 (c) is added in the ansatz $U_{ansatz}(\\theta)$. The client model is expressed as:\n$\\begin{aligned}\nU_{client}(\\alpha, \\theta) &= U_{ansatz}(\\theta) U_{encoder}(\\alpha) \\\\\n&= U_{person}(\\theta_p) U_{base}(\\theta_b) U_{encoder}(\\alpha),\n\\end{aligned}$\nwhere $\\theta=(\\theta_b, \\theta_p)$. All clients share the structure and parameters of the common base layer and have unique person- alized layer parameters. When training the model, the client updates the quantum circuit parameters for both the base and personalized layers simultaneously, but only the base layer parameters are uploaded to the server.\nB. Quantum Circuit of PQFL The client encodes the classical information into the ampli- tude of a quantum state. Fig. 3 illustrates the quantum circuit of the amplitude encoder $U_{encoder}(\\alpha)$. Through amplitude encoding, an N-bit classical data is encoded into an n-qubit quantum state, where $n = \\lceil log_2 N \\rceil$. Usually, the amplitude encoder can be realized by using X, RY, and the controlled RY gate. Fig. 4 shows the client model, whose quantum circuit of the base layer can be expressed as:\n$\\begin{aligned}\nU_{base}(\\theta_b) &= \\prod_{z=1}^{k}{\\{\\prod_{i=0,j=(i+1)\\%n}[CNOT_{i,j} \\\\\n(RY_i \\otimes RY_{j+1})CNOT_{i,j}] \\\\\n(RY_i \\otimes RY_i)\\}\\},  \n\\end{aligned}$\nwhere $CNOT_{i,j}$ indicates that the jth qubit is flipped if the ith qubit is 1; otherwise, no operation is taken. $RY_i$ denotes the rotation gate RY acted on the ith qubit. The quantum circuit of the personalized layer can be expressed as:\n$U_{person}(\\theta_p) = \\prod_z^k(RY_i \\otimes RY_i).$\nSo the client model can be represented as:\n$\\begin{aligned}\nU_{client}(\\alpha, \\theta) &=U_{person} (\\theta_p)U_{base}(\\theta_b) U_{encoder}(\\alpha) \\\\\n&=\\prod_{z=1}^{k}{\\{\\prod_{i=0,j=(i+1)\\%n}[CNOT_{i,j} \\\\\n(RY_i \\otimes RY_{j+1})CNOT_{i,j}] \\\\\n(RY_i \\otimes RY_i)\\}\\}U_{encoder}(\\alpha), \n\\end{aligned}$\nThe quantum circuit of the encoding and base layer of the server is consistent with that of the client, and the quantum circuit of the server can be expressed as:\n$\\begin{aligned}\nU_{server}(\\alpha, \\theta) &=U_{base}(\\theta_b)U_{encoder}(\\alpha) \\\\\n&= \\prod_{z=1}^{k}{\\{\\prod_{i=0,j=(i+1)\\%n}[CNOT_{i,j} \\\\\n(RY_i \\otimes RY_{j+1})CNOT_{i,j}] \\\\\n(RY_i \\otimes RY_i)\\}\\}, \n\\end{aligned}$"}, {"title": "C. PQFL Algorithm", "content": "Under the assumption that the samples owned by the client are non-IID, and all participants are honest but curious, we introduce the personalized quantum federated learning algo- rithm. Before that, we will introduce the parameter aggregation algorithm, the local training algorithm. 1) Aggregation: Based on the quantum security aggregation protocol [40], we improve a quantum parameter weighted average algorithm shown in Algorithm 1 for non-IID data, which can prevent disclosing any individual client parameter information.\nFirstly, in each round of the global training, the server first chooses M clients C = {$C_1, C_2,\\ldots,C_M$ } to be trained. For the client $C_m$, it sends the training image number $l_m$ to the server through classical channel. After receiving $l_m$ of all the participant clients, the server calculates the weighted score of each client according to Eq. (13), and then sends it to the client through the classical channel, so that each client can get its weighted score.\n$F_m = \\frac{l_m}{\\sum_{i=1}^{M}l_i}$\nSecondly, the server generates N GHZ states [41] $\\vert\\Psi\\rangle$ as shown in Eq. (14):\n$\\vert \\Psi\\rangle = \\vert\\Psi_0\\rangle\\vert\\Psi_1\\rangle\\ldots\\vert\\Psi_{N-1}\\rangle,$\nwhere $\\vert\\Psi_i\\rangle = (\\vert 0 \\rangle^{\\otimes M} + \\vert 1 \\rangle^{\\otimes M}) / \\sqrt{2}$, and N is related to the number of the base layer parameters of the client QNN model. Then the server sends M qubits of each GHZ state, which consists of M qubits, to the M participant clients through the quantum channel. When the client $C_m$ receives one of the qubits of the ith GHZ state $\\vert\\Psi_i\\rangle$, it encodes the qubit by applying a revolving gate $RZ(F_m\\theta_{m,i})$ gate. Where $\\theta_{m,i}$ represents the ith parameter in the base layer of the client $C_m$, and $F_m$ represents the weighted fraction of the client $C_m$. Other clients perform the same operation on the received qubits and send the encoded qubits back to the server. The ith base layer parameter of all clients is encoded into the GHZ state:\n$\\vert\\psi\\rangle = \\frac{1}{\\sqrt{2}}(\\vert 00...0\\rangle + e^{i\\sum_{m=1}^{M}(F_m\\cdot\\theta_{m,i})} \\vert 11...1\\rangle)$.\nThe client $C_m$ then sends the encoded GHZ state $\\vert\\psi_{i}\\rangle$ to the server through the quantum channel. Then the server decodes the GHZ state, using CNOT gate and H gate to decode the entangled state $\\vert\\psi'\\rangle$, then the quantum state evolves into:\n$\\begin{aligned}\n\\vert\\Psi''\\rangle &= H_1CNOT_{1,2}\\cdots CNOT_{N-1,N}\\vert\\psi\\rangle \\\\\n&= \\frac{1}{\\sqrt{2}}[(1 + e^{i\\sum_{m=1}^{M}(F_m\\cdot\\theta_{m,i})}) \\\\\n&\\vert 00...0\\rangle + (1 - e^{i\\sum_{m=1}^{M}(F_m\\cdot\\theta_{m,i})})\\vert 11...1\\rangle], \n\\end{aligned}$\nThen, as the server measures one qubit, the GHZ state will obtain $\\vert 0\\rangle$ with the probability of $Pr = \\frac{1}{2}(1 + cos(\\sum_{m=1}^{M}(F_m \\cdot \\theta_{m,i})))$. By repeating this process, the server will get the estimations of $\\sum_{m=1}^{M}(F_m\\theta_{m,i})$, which is calculated as follows:\n$\\sum_{m=1}^{M}(F_m\\theta_{m,i}) = arccos(2Pr - 1).$\nIn this way, the server gets the weighted average of the ith parameter in the base layer of all clients:\n$\\theta_i = \\sum_{m=1}^{M}(F_m\\theta_{m,i}).$\nIn this training round, the client performs the same operations as above for other parameters in the base layer. Thus, the server gets the weighted average aggregated parameters of all clients:\n$\\begin{aligned}\n\\theta &= (\\theta_0, \\theta_1,\\ldots, \\theta_N) \\\\\n&= (\\sum_{m=1}^{M}(F_m\\theta_{m,0}), \\sum_{m=1}^{M}(F_m\\theta_{m,1}),..., \\sum_{m=1}^{M}(F_m\\theta_{m,N})), \n\\end{aligned}$\nwhere $\\theta_m$ represents the base layer parameters of client $C_m$. 2) Local Training: The local training algorithm can be described as Algorithm 2.  is used to simulate the non-IID samples of the clients. The probability density function of the Dirichlet distribution is defined as follows:\n$f(\\theta_1,...,\\theta_L; \\alpha_1,..., \\alpha_L) = \\frac{1}{B(\\alpha)} \\prod_{i=1}^{L}\\theta_i^{\\alpha_i-1},$\nwhere $\\theta_i > 0$ and $\\sum_{i=1}^{L} \\theta_i = 1$. $B(\\alpha)$, which is a multivariate Beta function, is defined as follows:\n$B(\\alpha) = \\frac{\\prod_{i=1}^{L} \\Gamma(\\alpha_i)}{\\Gamma(\\sum_i \\alpha_i)}, - , \\alpha = (\\alpha_1, ..., \\alpha_L).$\nParticularly, when $\\alpha_i$ in vector $\\alpha$ has the same value, the dis- tribution is a symmetric Dirichlet distribution, whose density function can be expressed as:\n$f(\\theta_1,..., \\theta_L; \\alpha) = \\frac{\\Gamma(\\alpha L)}{\\Gamma(\\alpha)^L}\\prod_{i=1}^{L}\\theta_i^{\\alpha-1}.$\nA distribution D of label categories of data samples satisfy the Dirichlet distribution D $\\sim$ Dir($\\alpha$), which can be expressed as:\n$\\begin{array}{cccc} d_{0,0} & d_{0,1} & ... & d_{0,M-1} \\\\ d_{1,0} & d_{1,1} & ... & d_{2,M-1} \\\\ \\vdots & \\vdots & & \\vdots \\\\ d_{Y-1,0} & d_{Y-1,1} & ... & d_{Y-1,M-1} \\end{array}_{Y \\times M}$\nwhere $d_{i,j}$ represents the distribution probability of the data samples of category i on the jth client. According to the label category distribution matrix D, the data samples of the corresponding category are divided into M clients. The distribution parameter $\\alpha$ controls the similarity of sample distribution. As the distribution parameter $\\alpha$ decreases, the disparity in the category distribution of client data samples increases. In extreme cases, each client contains data samples with only one label category. The larger the value of the distribution parameter $\\alpha$, the more similar the category dis- tribution of client data samples, and the closer it is to uniform distribution. For example, for M clients and a dataset of Y categories, the label category distribution matrix of $Y \\times M$ is obtained from the Dirichlet distribution, and the samples of each category label are divided into corresponding clients according to different proportions in Eq. (23). Then, the client trains on the divided data samples. All participants initialize the base layer $U_{base} (\\theta_b)$, meanwhile the parameters in personalized layer $U_{person} (\\theta_p)$ are initialized locally. The loss function of client $C_m$ can be defined as:\n$l_m(\\theta) = -\\frac{1}{N}\\sum_{i=0}^{N-1}log(\\frac{exp(E_{[c]}({\\theta}))}{\\sum_{j=0}^{N}exp(E_{[j]}({\\theta}))}),$\nwhere N is the batch size, c is the index of \"1\" in the one-hot encoding of the label, and $E_i$ is the expected value measured by the client, which is calculated as follows:\n$E_i({\\theta}) = (\\langle\\psi\\vert U_{ansatz}^{\\dagger}({\\theta}) H_{hams} U_{ansatz}({\\theta})\\vert\\psi\\rangle) = (\\langle\\psi\\vert U_{base}^{\\dagger} (\\theta_b) U_{person}^{\\dagger} (\\theta_p) H_{hams} U_{person} (\\theta_p) U_{base} (\\theta_b)\\vert\\psi\\rangle).$\nAdam algorithm is adopted to optimize both the parameters $\\theta_b$ and $\\theta_p$. After several rounds of training, the client transmits all of the base layer parameters to the server, while retaining the personalized layer parameters. 3) Global Updating: Based on the aggregation algorithm and local training algorithm, we propose the PQFL algorithm in the global updating process, as shown in Algorithm 3. The global objective function is:\n$f(\\theta^*) = min_{\\theta} f(\\theta) = min_{\\theta} \\frac{1}{M} \\sum_{m=1}^{M}l_m({\\theta}).$\nFirstly, the server initializes a set of base layer parameters $\\theta$, selects a set of clients C = {$C_1, C_2,\\ldots,C_M$ }, and the initialized parameters are sent to the clients through quantum channels. The server generates M qubits:\n$\\vert\\phi\\rangle = \\vert\\phi_0\\rangle\\vert\\phi_1\\rangle\\ldots\\vert\\phi_{M-1}\\rangle,$\nwhere $\\vert\\phi_i\\rangle = (\\vert 0 \\rangle + \\vert 1 \\rangle)$. The server encodes the ith parameter $\\theta_i$ to N qubits, that is, applies the rotation gate $RZ(\\theta_i)$ gate to the N qubits, which is given by:\n$\\vert\\phi'\\rangle = \\vert\\phi_0\\vert\\phi_1\\rangle\\ldots\\vert\\phi_{M-1}) = RZ(\\theta_i)\\vert\\phi_0)RZ(\\theta_i)\\vert\\phi_1\\rangle\\ldots RZ(\\theta_i)\\vert\\phi_{M-1}),$\nwhere $\\vert\\phi_0\\rangle = \\frac{1}{\\sqrt{2}}(\\vert 0 \\rangle + e^{i\\theta_i}\\vert 1\\rangle)$. The server then sends the M-qubit $\\vert\\phi'\\rangle$ to M clients through quantum channels, and the clients measure the received qubits and the quantum state will collapse to 0 with the probability of $Pr = \\frac{1}{2}(1 + cos \\theta_i)$. By repeating this process, the client will get an estimation of $\\theta_i$, which is calculated as follows:\n$\\theta_i = arccos(2Pr - 1).$\nOther parameters of the base layer can also be sent to the client through the same process so that all clients can receive the parameter $\\theta$ from the server. Secondly, the clients update both base and personalized layer parameters according to Algorithm 2, and upload the updated base layer parameter $\\theta_b'$ to the server through Algo- rithm 1. And the server calculates the weighted average sum of the parameters uploaded by each client according to Eq. (19), and obtains a new model parameter $\\theta_b^{r+1}$. By repeating the above steps, the server can complete global updating."}, {"title": "D. PQFL for Image Classification", "content": "A privacy image classification scheme based on person- alized quantum federation learning is proposed, including dataset partitioning, parameters delivery, local updating, pa- rameters upload, secure aggregation of quantum parameters, and global updating, which is shown in Fig. 5. The server and clients are capable of quantum computing. The specific steps are as follows. 1) Dataset Partitioning: The dataset is divided into non-IID according to the Dirichlet distribution and distributed to each client. For two classes of images, when the distribution parameter is $\\alpha = 1, 10, 100$, and the number of clients is 2, 4, and 8, the partition of non-IID images is shown in Fig. 6. 2) Parameters Delivery: The server constructs a 4-qubit quantum neural network model as shown in Fig. 2 (b). The server initializes the base layer parameters $\\theta$, encodes the parameters into the quantum state according to Algorithm 3, and sends the qubits to each client through the quantum chan- nels. After receiving qubits containing parameter information, the clients measure the probability to get an estimation of the parameter and use it as a parameter in the base layer. In addition, the server calculates the weight score of each client through the classical channels and sends it to the corresponding client. 3) Local Training: The client builds a quantum neural network model containing 4 qubits, as shown in Fig. 2 (c) and Fig. 4. The base layer repeats k = 3 times. Both the base layer and the personalized layer parameters are trained. The clients conduct several rounds of training updates to the local model according to Algorithm 2, and obtain the trained parameter $\\theta$. 4) Parameters Upload: The clients send the number of image samples to the server through the classical channels. The server calculates the weight score of each client according to the number of image samples provided by each client. 5) Secure Aggregation of Quantum Parameters: The clients receive the weight score, multiply it with the base layer parameters $\\theta_b$, encodes it into the quantum state according to Algorithm 1, and transmit it to the server through quantum channels. The weighted average of the base layer parameters $\\theta_b'$ is calculated based on the server's measurements. 6) Global updating: The server updates the model param- eters to weighted average parameters $\\theta_b^r$ of the base layer, repeating the above steps until reaching the number of training rounds or classification accuracy."}, {"title": "IV. EXPERIMENTS AND ANALYSIS", "content": "A. Experiment setting The experiments are conducted using the hardware device of Intel Core i7-9700 and the Mindspore quantum library is used for the experiment. The experimental outcomes indicate that the personalized quantum federated learning algorithm we proposed is capable of achieving high accuracy in binary classification tasks involving clothing images while securing the privacy images and models. Next, the accuracy, security, and communication overhead of private image classification schemes based on personalized quantum federated learning are analyzed. The hyperparameter settings of the experiment are shown in Table II. For concreteness, we conduct a classi- fication task about classifying images \u201ctrouser or ankle boot\" in the FashionMNIST dataset, which is a clothing dataset consisting of 10 categories with a total of 70000 grayscale images and 60,000 samples for training and 10,000 samples for testing. An original $28 \\times 28$ image is preprocessed into a $4 \\times 4$ image according to Fig. 7.\nB. Accuracy Analysis We perform the training process using Dirichlet distribution parameters $\\alpha = 1, 10, 100$. Table III shows that the server model in PQFL can achieve higher accuracy on the FashionMNIST dataset than the model without personalized layer in most settings. In particular, the model accuracy of the server can achieve 100% when the distribution parameter $\\alpha = 100$ and the clients quantity is 8. As the distribution parameters and the clients quantity increases, the server model's accuracy remains relatively sta- ble, indicating that the proposed PQFL model is robust to variations in distribution parameters and clients quantity. Table IV demonstrates the average model accuracy of clients in PQFL is higher than that without personalized layer in the most settings. Further, we can see that when $\\alpha$ is constant, the average accuracy improves as the clients quantity increases. Fig. 8 shows the accuracy of client models with personaliza- tion layer is generally higher on the testset compared to those without personalization layer. This indicates that the client models are more personalized than the global model based on the experimental results.\nC. Convergence Analysis We carry out experiments involving 2, 4, and 8 clients in the training process. By minimizing the loss function, the model parameters stabilize and achieve optimal performance, leading to model convergence. Fig. 9 illustrates the convergence speed of the loss function for the following three scenarios to assess the convergence of the PQFL algorithm: (1) With personalized layer. The proposed PQFL algorithm includes a client model with both the base layer and a personalized layer, while the server only contains the base layer. (2) Without personalized layer. A quantum federated learn- ing where the server and client models only include the base layer and do not include the personalized layer. (3) Without federated learning. Training on a quantum neural network with only the base layer. It suggests that the PQFL algorithm successfully converges while consistently achieving comparable performance in the other two scenarios. D. Overhead Analysis The overhead of the PQFL algorithm consists of both communication and computation overhead. Table V indicates the communication overhead of the PQFL algorithm, which can be analyzed through storage overhead and time overhead. For the server, 4nk base layer parameters need to be encoded into the quantum state and sent to the M clients. The total qubits sent from sever to M clients in N global training rounds is 4nkNM. Suppose performing an encoding or decoding operator costs $t_c(\\sim 2.5\\times10^{-8}s)$ [42], and transmitting a qubit from the server to the client through the quantum channel costs $t_n(\\sim10^{-3}s)$ [43], the downlink time of training N rounds is:\n$T_{down} \\approx 4nkNt_c + 4nkNMt_c + Nt_n = 4nkNt_c(M + 1) + Nt_n.$\nFor a client, there are 4nk base layer parameters that need to be encoded into the quantum state and sent to the server. The total qubits sent from a client in N global training rounds is 4nk N. The uplink time of training N rounds is:\n$T_{up} \\approx 4nkN2t_c + Nt_n = 8nkNt_c + Nt_n.$\nThe computation overhead of the server and client is shown in Table VI. For an $L \\times L$ image, the server and the clients need 2 log2 L qubits to encode an image into quantum state."}, {"title": "E. Security Analysis", "content": "The participants in this scenario are semi-honest, that is to say, the client involved in training and"}]}