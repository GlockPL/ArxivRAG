{"title": "Spin glass model of in-context learning", "authors": ["Yuhao Li", "Ruoran Bai", "Haiping Huang"], "abstract": "Large language models show a surprising in-context learning ability-being able to use a prompt\nto form a prediction for a query, yet without additional training, in stark contrast to old-fashioned\nsupervised learning. Providing a mechanistic interpretation and linking the empirical phenomenon\nto physics are thus challenging and remain unsolved. We study a simple yet expressive transformer\nwith linear attention, and map this structure to a spin glass model with real-valued spins, where\nthe couplings and fields explain the intrinsic disorder in data. The spin glass model explains how\nthe weight parameters interact with each other during pre-training, and most importantly why an\nunseen function can be predicted by providing only a prompt yet without training. Our theory\nreveals that for single instance learning, increasing the task diversity leads to the emergence of the\nin-context learning, by allowing the Boltzmann distribution to converge to a unique correct solution\nof weight parameters. Therefore the pre-trained transformer displays a prediction power in a novel\nprompt setting. The proposed spin glass model thus establishes a foundation to understand the\nempirical success of large language models.", "sections": [{"title": "INTRODUCTION", "content": "Thanks to earlier breakthroughs in processing natural\nlanguages (e.g., translation), vector representation and\nattention concepts were introduced into machine learn-\ning [1-4], which further inspired a recent breakthrough of\nimplementing the self-attention as a feedforward model\nof information flow, namely transformer [5]. The self-\nattention captures dependencies between different parts\nof the input (e.g., image or text), coupled with a simple\ncost of next-token prediction [6, 7], leading to a revolution\nin the field of natural language processing [8], so-called\nlarge language model (LLM).\nOne of the astonishing abilities in the transformer is\nthe in-context learning [9], i.e., the pre-trained trans-\nformer is able to accomplish previously-unseen compli-\ncated tasks by showing a short prompt in the form\nof instructions and a handful of demonstrations, espe-\ncially without a need for updating the model parame-\nters. LLMs thus develop a wide range of abilities and\nskills (e.g., question answering, code generation) [10],\nwhich are not explicitly contained in the training dataset\nand not specially designed to optimize. This remarkable\nproperty is achieved only by training for forecasting next\ntokens and only if corpus and model sizes are scaled up\nto a huge number [11, 12]. The above characteristics\nof transformer and the in-context learning (ICL) are in\nstark contrast to perceptron models in the standard su-\npervised learning context, presenting a formidable chal-\nlenge for a mechanistic interpretation [13, 14].\nTo achieve a scientific theory of ICL, previous works\nfocused on optimization via gradient descent dynam-\nics [15, 16], representation capacity [17], Bayesian in-\nference [18, 19], and in particular the pre-training task\ndiversity [19-22]. The theoretical efforts were commonly\nbased on a single-layer linear attention [15, 21, 23, 24],\nwhich revealed that a sufficient pre-training task diver-\nsity guarantees the emergence of ICL, i.e., the model can\ngeneralize beyond the scope of pre-training tasks.\nHowever, rare connections are established to physics\nmodels, which makes a physics model of ICL lacking so\nfar, preventing us from a deep understanding about how\nICL emerges from pre-trained model parameters. Here,\nwe treat the transformer learning as a statistical inference\nproblem, and then rephrase the inference problem as a\nspin glass model, where the transformer parameters are\nturned into real-valued spins, and the input sequences act\nas a quenched disorder, which makes the spins strongly\ninteract with each other to lower down the ICL error.\nThere exists a unique spin solution to the model, guar-\nanteeing that the transformer can predict the unknown\nfunction embedded in test prompts."}, {"title": "TRANSFORMER WITH LINEAR ATTENTION", "content": "We consider a simple transformer structure-a single-\nlayer self-attention transforming an input sequence to an\noutput one. Given an input sequence $X \\in \\mathbb{R}^{D \\times N}$, where\nD is the embedding dimension and N is the context\nlength, the self-attention matrix is an softmax function\n$Softmax(QK/\\sqrt{D})$, where $Q = W_Q X, K = W_K X$.\n$W_Q$ and $W_K$ are the query and key matrices $(\\in \\mathbb{R}^{D \\times D})$,\nrespectively. $\\sqrt{D}$ inside the softmax function makes its\nargument order of unity. The self-attention refers to the\nattention matrix generated from the input sequence it-\nself, and allows each element (query) to attend to all\nother elements in one input sequence, being learnable\nthrough pre-training. The softmax function is thus cal-\nculated independently for each row. Taking an addi-"}, {"title": "SPIN-GLASS MODEL MAPPING", "content": "Equation (2) can be treated as a Hamiltonian in sta-\ntistical physics. The linear attention structure makes\nthe spin-model mapping possible. This proceeds as fol-\nlows. The prediction to the u-th input matrix can be\nrecast as $\\hat{y} = (DVN)^{-1} \\sum_{m,n} C^{\\mu}_{D+1,m}W_{m,n}X^{\\mu}_{n,N+1}$"}, {"title": "STATISTICAL MECHANICS ANALYSIS", "content": "Because the statistics of couplings and fields has no\nanalytic form, one can use the cavity method, an alter-\nnative method to replica trick widely used in spin glass\ntheory. The cavity method is also known as the belief\npropagation algorithm, working by iteratively solving a\nclosed equation of cavity quantity, e.g., a spin is virtu-\nally removed [29]. The cavity method can thus be used\non single instances of ICL. The probability of each weight\nconfiguration is given by the Gibbs-Boltzmann distribu-\ntion $P(\\sigma) = e^{-\\beta H(\\sigma)}/Z$, where Z is the partition func-\ntion, and $\\beta$ is an inverse temperature tuning the energy\nlevel. Different weight components are likely strongly-\ncorrelated, but the cavity marginal $\\eta_{i\\rightarrow j}(\\sigma_i)$ becomes\nconditionally independent, which facilitates our deriva-\ntion of the following self-consistent iteration (namely"}, {"title": "RESULTS", "content": "The iteration of the cavity method depends on the spe-\ncific form of $J_{ij}$ and $h_i$, that is, on $s_i$. We first focus on\nthe original form $S_{m,n}$, which is not yet flattened by the $\\Gamma$\nmap, defined as $S_{m,n} = (D\\sqrt{N})^{-1}C^{\\mu}_{D+1,m}X^{\\mu}_{n,N+1}$. The\nindex $\\mu$ is omitted by meaning an average over all input\nmatrices {X}. The matrix S is divided into three blocks:\nthe last column is an all-zero vector, while the other two\nblocks are labeled as A ($m < D + 1$, $n \\neq D + 1$) and B\n($m = D + 1$, $n \\neq D + 1$) respectively in Fig. 1(a). The\nstatistics of $h_i$ is almost the same as that of $s_i$ [Fig. 1 (a)],\nsince $h_i = \\tilde{y} s_i$. However, the key matrix, J, generated\nby the outer product of the flattened S with itself, has\nthree main blocks, labeled as C, D, and E respectively in\nFig. 1 (b).\nIn contrast to traditional spin glass models [27], P(J)\ndoes not have an analytic form [26]. For example, the\nelements in the block C corresponds to the product of\ntwo random variables $z_i z_j$ ($i \\neq j$), where each of them\nis a sum of the product of a few i.i.d. standard normal\nrandom variables. Therefore, we provide the numerical\nestimation of the coupling distribution, which all bear\na fat tail [Fig. 1 (b)]. We thus define a new type of"}, {"title": "CONCLUSION", "content": "A fundamental question in large language models\nis what contributes to the emergence ability of ICL,\ni.e., why simple next-token prediction based pre-training\nleads to in-context learning of previously unseen tasks,\nespecially without further tuning the model parameters.\nHere, we turn the ICL into a spin glass model, and ver-\nify the equivalence between the standard SGD training\nand our statistical mechanic inference. We observe the\nfat tail distribution of coupling that determines how the\nmodel parameters of the transformer interact with each\nother. The transformer parameters are akin to an en-\nsemble of real-valued spins in physics whose ground state\nsuggests that the model can infer an unknown function\nfrom the shown test prompts after a pre-training of input\nsequences of sufficient task diversity. The phase diagram\nfor single instance learning is also derived by our method,\nsuggesting a continuous ICL transition.\nThe spin-glass model mapping of a simple transformer\nwith linear attention working on linear regression tasks\nthus establishes a toy model of understanding emergent\nabilities of large language models. This work could be\npotentially generalized to other attention setting, e.g.,\nthe softmax one. Future exciting directions include ex-\nplaining the chain-of-thought prompting [12], i.e., decom-\nposition of a complex task into intermediate steps, and\nmore challenging case of hallucination [30], i.e., the model\ncould not distinguish the generated outputs from factual\nknowledge, or it could not understand what they gen-\nerate [14]. We speculate that this hallucination may be\nintimately related to the solution space of the spin glass\nmodel given a fixed complexity of training dataset, e.g.,\nspurious states in a standard associative memory model,\nas implied by Eq. (4). These open questions are expected\nto be addressed in the near future, thereby enhancing ro-\nbustness and trustworthiness of AI systems."}, {"title": "Derivation of Relaxed Belief Propagation and Approximate Message Passing", "content": "In this appendix, we derive the approximate message passing equation in the main text. To proceed, we first define\n$\\xi_{ij} = \\sum_{k\\neq i,j} \\beta J_{ik} \\sigma_k$, and $G(\\xi_{i\\rightarrow j}) = e^{i \\xi_{i\\rightarrow j}}$. The cavity equation [Eq. (5)] can be written as\n$\\eta_{ij}(\\sigma_i) = $\n$\\approx = $\n$\\approx = $\nIn the second line of Eq. (11), we insert the Fourier transform of $G(\\xi_{i\\rightarrow j})$, $\\tilde{G}(\\xi_{i\\rightarrow j})$ denotes the corresponding inverse\ntransform, and in the third line, we use the Taylor expansion of exponential function, which requires us to define the\nfollowing mean $m_{i\\rightarrow j}$ and variance $v_{i\\rightarrow j}$ of the message (the cavity marginal probability):\n$m_{i\\rightarrow j} = \\int d\\sigma_i \\eta_{i\\rightarrow j}(\\sigma_i) \\sigma_i, v_{i\\rightarrow j} = \\int d\\sigma_i \\eta_{i\\rightarrow j}(\\sigma_i) \\sigma_i^2 - m_{i\\rightarrow j}^2$"}, {"title": "", "content": "Then, we reformulate $G(\\xi_{i\\rightarrow j})$ by its inverse Fourier transform $\\tilde{G}(\\xi_{i\\rightarrow j})$, and obtain\n$\\eta_{ij}(\\sigma_i) = $\nwhere we have defined\nCombining Eq. (13) and Eq. (12), one can immediately calculate the mean\n$\\hat{m}_{ij} = $\nand the variance\n$\\hat{v}_{ij} = $\nFinally we arrive at the iterative equation called relaxed belief propagation:\nAfter the relaxed belief propagation equation converges, we get further the marginal probability as\n$\\eta_i(\\sigma_i) = \\frac{1}{Z_i} e^{-\\beta h_i \\sigma_i - \\beta \\lambda_i \\sigma_i^2} \\prod_{j \\neq i} {\\int do_j exp [-\\frac{1}{2} (\\beta \\lambda_j - V_{j \\rightarrow i}) \\sigma_j^2 + (\\beta h_j + \\beta J_{ij} \\sigma_i + M_{j \\rightarrow i}) \\sigma_j ]}$\nTherefore, $\\eta_i(\\sigma_i)$ can be computed as a Gaussian distribution $N(m_i, v_i)$, where\nBy using the following identities $M_i = M_{i \\rightarrow j} + \\beta J_{ij} m_{j \\rightarrow i}$ and $V_i = V_{i \\rightarrow j} + \\beta^2 J_{ij} v_{j \\rightarrow i}$, we further find that\nand"}, {"title": "", "content": "As a consequence, we can further reduce the relaxed belief propagation equation to the AMP equation\n$\\hat{v}_i = \\frac{1}{\\beta \\lambda_i - \\beta^2 \\sum_{j \\neq i} J_{ij} \\hat{v}_j}$"}]}