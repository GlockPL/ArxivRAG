{"title": "Spin glass model of in-context learning", "authors": ["Yuhao Li", "Ruoran Bai", "Haiping Huang"], "abstract": "Large language models show a surprising in-context learning ability-being able to use a prompt to form a prediction for a query, yet without additional training, in stark contrast to old-fashioned supervised learning. Providing a mechanistic interpretation and linking the empirical phenomenon to physics are thus challenging and remain unsolved. We study a simple yet expressive transformer with linear attention, and map this structure to a spin glass model with real-valued spins, where the couplings and fields explain the intrinsic disorder in data. The spin glass model explains how the weight parameters interact with each other during pre-training, and most importantly why an unseen function can be predicted by providing only a prompt yet without training. Our theory reveals that for single instance learning, increasing the task diversity leads to the emergence of the in-context learning, by allowing the Boltzmann distribution to converge to a unique correct solution of weight parameters. Therefore the pre-trained transformer displays a prediction power in a novel prompt setting. The proposed spin glass model thus establishes a foundation to understand the empirical success of large language models.", "sections": [{"title": "INTRODUCTION", "content": "Thanks to earlier breakthroughs in processing natural languages (e.g., translation), vector representation and attention concepts were introduced into machine learning [1-4], which further inspired a recent breakthrough of implementing the self-attention as a feedforward model of information flow, namely transformer [5]. The self-attention captures dependencies between different parts of the input (e.g., image or text), coupled with a simple cost of next-token prediction [6, 7], leading to a revolution in the field of natural language processing [8], so-called large language model (LLM).\nOne of the astonishing abilities in the transformer is the in-context learning [9], i.e., the pre-trained transformer is able to accomplish previously-unseen complicated tasks by showing a short prompt in the form of instructions and a handful of demonstrations, especially without a need for updating the model parameters. LLMs thus develop a wide range of abilities and skills (e.g., question answering, code generation) [10], which are not explicitly contained in the training dataset and not specially designed to optimize. This remarkable property is achieved only by training for forecasting next tokens and only if corpus and model sizes are scaled up to a huge number [11, 12]. The above characteristics of transformer and the in-context learning (ICL) are in stark contrast to perceptron models in the standard supervised learning context, presenting a formidable challenge for a mechanistic interpretation [13, 14].\nTo achieve a scientific theory of ICL, previous works focused on optimization via gradient descent dynamics [15, 16], representation capacity [17], Bayesian inference [18, 19], and in particular the pre-training task diversity [19-22]. The theoretical efforts were commonly based on a single-layer linear attention [15, 21, 23, 24], which revealed that a sufficient pre-training task diversity guarantees the emergence of ICL, i.e., the model can generalize beyond the scope of pre-training tasks.\nHowever, rare connections are established to physics models, which makes a physics model of ICL lacking so far, preventing us from a deep understanding about how ICL emerges from pre-trained model parameters. Here, we treat the transformer learning as a statistical inference problem, and then rephrase the inference problem as a spin glass model, where the transformer parameters are turned into real-valued spins, and the input sequences act as a quenched disorder, which makes the spins strongly interact with each other to lower down the ICL error. There exists a unique spin solution to the model, guaranteeing that the transformer can predict the unknown function embedded in test prompts."}, {"title": "TRANSFORMER WITH LINEAR ATTENTION", "content": "We consider a simple transformer structure-a single-layer self-attention transforming an input sequence to an output one. Given an input sequence \\(X \\in \\mathbb{R}^{D\\times N}\\), where \\(D\\) is the embedding dimension and \\(N\\) is the context length, the self-attention matrix is an softmax function \\(\\text{Softmax}(QK/\\sqrt{D})\\), where \\(Q = W_QX\\), \\(K = W_KX\\). \\(W_Q\\) and \\(W_K\\) are the query and key matrices (\\(\\in \\mathbb{R}^{D\\times D}\\)), respectively. \\(\\sqrt{D}\\) inside the softmax function makes its argument order of unity. The self-attention refers to the attention matrix generated from the input sequence itself, and allows each element (query) to attend to all other elements in one input sequence, being learnable through pre-training. The softmax function is thus calculated independently for each row. Taking an additional transformation \\(V = W_vX\\), where \\(W_v \\in \\mathbb{R}^{D\\times D}\\) is the value matrix, one can generate the output \\(Y = V\\cdot \\text{Softmax}(QK/\\sqrt{D})\\). Hence, this simple transformer implements a function \\(T_F(X) : \\mathbb{R}^{D\\times N} \\rightarrow \\mathbb{R}^{D\\times N}\\).\nFor simplicity, we further consider a linear attention replacing the computationally expensive softmax, which is still expressive [25]. Defining \\(W = W_QW_K\\), and choosing \\(W_v = \\mathbb{1}_D\\) (\\(\\mathbb{1}_D\\) indicates a \\(D \\times D\\) identity matrix), we re-express the linear transformer as \\(Y = \\frac{1}{D\\sqrt{N}}X X^T W X\\), where \\(X\\) contains prompts and the query (to be predicted by the transformer), and \\(W \\in \\mathbb{R}^{D\\times D}\\) is the equivalent weight matrix to be trained, and \\(\\frac{1}{D\\sqrt{N}}\\) is a normalization coefficient.\nWe next design the training task as a high-dimensional linear regression, and each example consists of the data \\(x \\sim \\mathcal{N}(0, \\mathbb{1}_p)\\) and the corresponding label \\(y = w \\cdot x\\), where the latent task weight \\(w \\sim \\mathcal{N}(0, \\mathbb{1}_p)\\). For the \\(\\mu\\)-th input matrix, we use \\(N\\) samples as prompts using the same \\(w^{\\mu}\\) yet different \\(x\\) within the input sequence. An additional sample \\(x^0\\) is regarded as the query whose true label \\(\\tilde{y}\\) is masked yet to be predicted by the transformer. The structure of each input matrix \\(X^{\\mu}\\) is thus represented as\n\\[ X^{\\mu} = \\begin{bmatrix}\nx_1^{\\mu} & x_2^{\\mu} & \\cdots & x_N^{\\mu} & x^0 \\\\\ny_1^{\\mu} & y_2^{\\mu} & \\cdots & y_N^{\\mu} & \\tilde{y}\\end{bmatrix} \\in \\mathbb{R}^{(D+1) \\times (N+1)}. \\tag{1} \\]\nThe last element of \\(Y\\) corresponds to the predicted label to the query \\(x^0\\), i.e., \\(\\hat{y}^0 = Y_{D+1,N+1}\\). The goal of ICL is to use the prompt to form a prediction for the query, and the true function governing the linear relationship may be unseen during pre-training, because each \\(\\mu\\) is generated by an independently-drawn \\(w\\) during both training and test phases. We consider an ensemble of \\(P\\) sequences, and \\(P\\) is thus called the task diversity. This setting is bit different from that in recent works [19, 21].\nThe pre-training is carried out by minimizing the mean squared error function, and the total training loss is given by\n\\[ \\mathcal{L} = \\frac{1}{2P} \\sum_{\\mu=1}^{P} (\\tilde{y}^{\\mu} - \\hat{y}^{\\mu})^2 + \\frac{\\Lambda}{2} ||W||^2, \\tag{2} \\]\nwhere \\(\\Lambda\\) controls the weight-decay strength. The generalization error on unseen tasks is written as \\(e_g = E_{x,x^0,w}(\\tilde{y} - \\hat{y})^2\\), where the ensemble average over all disorders is considered."}, {"title": "SPIN-GLASS MODEL MAPPING", "content": "Equation (2) can be treated as a Hamiltonian in statistical physics. The linear attention structure makes the spin-model mapping possible. This proceeds as follows. The prediction to the \\(\\mu\\)-th input matrix can be recast as \n\\[ \\hat{y}^{\\mu} = (D\\sqrt{N})^{-1} \\sum_{m,n} C_{D+1,m}^{\\mu} W_{m,n} X_{n,N+1}^{\\mu}, \\]\nwhere \\(C^{\\mu} = X^{\\mu} (X^{\\mu})^T\\). Then we define an index mapping \\(\\Gamma : (m,n) \\rightarrow i\\) to flatten a matrix into a vector. Therefore, one can write \\(\\sigma_i = \\Gamma(W_{m,n})\\), and \\(s_i = (D\\sqrt{N})^{-1} C_{D+1,m}^{\\mu} X_{n,N+1}^{\\mu}\\), where \\(i = (D+1)(m-1)+n\\), and finally the prediction as \\(\\hat{y}^{\\mu} = \\sum_i s_i \\sigma_i\\). Consequently, the loss for each input matrix reads\n\\[ \\mathcal{L}^{\\mu} = \\frac{1}{2} \\sum_{i,j} J_{ij}^{\\mu} \\sigma_i \\sigma_j - \\sum_i h_i^{\\mu} \\sigma_i + \\frac{\\Lambda}{2} \\sum_i \\sigma_i^2, \\tag{3} \\]\nwhere we omit the constant term \\((\\tilde{y}^{\\mu})^2/2\\).\nUpon defining \\(J_{ij}^{\\mu} = -s_i s_j^{\\mu}\\), \\(h_i^{\\mu} = \\tilde{y}^{\\mu} s_i\\), and \\(\\lambda_i^{\\mu} = \\Lambda\\), we obtain a spin glass model! The disorder in the pre-training dataset including the diversity in the latent task vectors w is now encoded into the interactions between spins, and random fields the spins feel. In fact, this is a densely-connected spin glass model, while the coupling and field statistics do not have an analytic expression [26], as they bear a fat tail (Fig. 1). This reminds us of two-body spherical spin model studied in spin glass theory [27, 28], but the current glass model of ICL seems much more complex than the spherical model. The Hamiltonian averaged over all input matrices thus reads\n\\[ \\mathcal{H}(\\sigma) = - \\frac{1}{2} \\sum_{i<j} J_{ij} \\sigma_i \\sigma_j - \\sum_i h_i \\sigma_i + \\frac{1}{2} \\sum_i \\lambda_i \\sigma_i^2, \\tag{4} \\]\nwhere the effective interaction \\(J_{ij} = (1/P) \\sum_{\\mu} J_{ij}^{\\mu}\\), the external field \\(h_i = (1/P) \\sum_{\\mu} h_i^{\\mu}\\) and the regularization factor \\(\\lambda_i = (1/P) \\sum_{\\mu} \\lambda_i^{\\mu}\\). By construction, this model reflects the nature of associative memory [29], yet the spin variable is now the underlying parameter of the transformer. To conclude, we derive a spin glass model of ICL, opening a physically appealing route towards the mechanistic interpretation of ICL and even more complex layered transformer."}, {"title": "STATISTICAL MECHANICS ANALYSIS", "content": "Because the statistics of couplings and fields has no analytic form, one can use the cavity method, an alternative method to replica trick widely used in spin glass theory. The cavity method is also known as the belief propagation algorithm, working by iteratively solving a closed equation of cavity quantity, e.g., a spin is virtually removed [29]. The cavity method can thus be used on single instances of ICL. The probability of each weight configuration is given by the Gibbs-Boltzmann distribution \\(P(\\sigma) = e^{-\\beta \\mathcal{H}(\\sigma)}/Z\\), where \\(Z\\) is the partition function, and \\(\\beta\\) is an inverse temperature tuning the energy level. Different weight components are likely strongly-correlated, but the cavity marginal \\(n_{i\\rightarrow j}(\\sigma_i)\\) becomes conditionally independent, which facilitates our derivation of the following self-consistent iteration (namely mean-field equation [29]):\n\\[ n_{i\\rightarrow j}(\\sigma_i) = \\frac{1}{Z_{ij}} e^{-\\beta h_i \\sigma_i - \\beta \\lambda_i \\sigma_i^2} \\int \\prod_{k \\neq i,j} [d\\sigma_k \\eta_{ki}(\\sigma_k)] G(\\xi_{i\\rightarrow j}), \\tag{5} \\]\nwhere \\(z_{ij}\\) is a normalization constant, and \\(\\eta_{i\\rightarrow j}\\) is defined as the cavity probability of spin \\(\\sigma_i\\) in the absence of the interaction between spins \\(i\\) and \\(j\\). After the iteration reaches a fixed point, the marginal probability \\(\\eta_i(\\sigma_i)\\) of each spin can be calculated by\n\\[ \\eta_i(\\sigma_i) = \\frac{1}{Z_i} e^{-\\beta h_i \\sigma_i - \\beta \\lambda_i \\sigma_i^2} \\prod_{j \\neq i} \\int d\\sigma_j \\eta_{ji}(\\sigma_j) e^{\\beta J_{ij} \\sigma_i \\sigma_j} . \\tag{6} \\]\nBecause of the continuous nature of spin and weak but dense interactions among spins, we can further simplify the mean-field equation [Eq. (5)], and derive the approximate message passing (AMP) by assuming \\(\\eta_i(\\sigma_i) \\sim \\mathcal{N}(m_i, v_i)\\), where \\((m_i, v_i)\\) is the fixed point of the following iterative equation:\n\\[\\begin{aligned}\nm_i &= \\frac{\\beta h_i + \\beta \\sum_{j \\neq i} J_{ij} m_{j}}{ \\beta \\lambda_i - \\beta^2 \\sum_{j \\neq i} J_{ij}^2 v_{j} } \\tag{7a}\\\\\nv_i &= \\frac{1}{ \\beta \\lambda_i - \\beta^2 \\sum_{j \\neq i} J_{ij}^2 v_{j} } \\tag{7b}\\end{aligned}\\]\nTechnical details of deriving the AMP are given in the supplemental material."}, {"title": "RESULTS", "content": "The iteration of the cavity method depends on the specific form of \\(J_{ij}\\) and \\(h_i\\), that is, on \\(s_i\\). We first focus on the original form \\(S_{m,n}\\), which is not yet flattened by the \\(\\Gamma\\) map, defined as \\(S_{m,n} = (D\\sqrt{N})^{-1} C_{D+1,m}^{\\mu} X_{n,N+1}^{\\mu}\\). The index \\(\\mu\\) is omitted by meaning an average over all input matrices \\(\\{X^{\\mu}\\}\\). The matrix S is divided into three blocks: the last column is an all-zero vector, while the other two blocks are labeled as A (\\(m < D + 1, n \\neq D + 1\\)) and B (\\(m = D + 1, n \\neq D + 1\\)) respectively in Fig. 1(a). The statistics of \\(h_i\\) is almost the same as that of \\(s_i\\) [Fig. 1 (a)], since \\(h_i = \\tilde{y} s_i\\). However, the key matrix, J, generated by the outer product of the flattened S with itself, has three main blocks, labeled as C, D, and E respectively in Fig. 1 (b).\nIn contrast to traditional spin glass models [27], \\(P(J)\\) does not have an analytic form [26]. For example, the elements in the block C corresponds to the product of two random variables \\(z_i z_j\\) (\\(i \\neq j\\)), where each of them is a sum of the product of a few i.i.d. standard normal random variables. Therefore, we provide the numerical estimation of the coupling distribution, which all bear a fat tail [Fig. 1 (b)]. We thus define a new type of spin glass model corresponding to ICL, or a metaphor of transformer in large language models. A surprising observation is that the distributions of block A before and after increasing the prompt length are almost the same, while the block B seems sensitive to the change of the prompt length [Fig. 2]. As D grows, the part A will dominate the behavior of the linear attention, which captures the essence of ICL explained below.\nTo see whether our spin glass model captures the correct structure of the weight matrix in the simple transformer, we first divide the weight matrix W into blocks in the same way as we do for the input matrix, i.e.,\n\\[ W = \\begin{bmatrix}\nW_{11} & W_{12} \\\\\nW_{21} & W_{22}\\end{bmatrix}, \\tag{8} \\]\nwhere \\(W_{11} \\in \\mathbb{R}^{D \\times D}\\), \\(W_{12} \\in \\mathbb{R}^{D \\times 1}\\), \\(W_{21} \\in \\mathbb{R}^{1 \\times D}\\), and \\(W_{22} \\in \\mathbb{R}\\). In our linear regression task, the prediction of the model to the test query can be written as\n\\[ \\hat{y} = w^T (W_{11} + w W_{21}) x. \\tag{9} \\]\nTo derive the above prediction, we have used 2 \u00d7 2 block matrix form of X and the fact of i.i.d. \\(\\{x\\}\\). Hence, the weight matrix of a well-trained model must satisfy\n\\[ W_{11} + w W_{21} = \\mathbb{1}_D. \\tag{10} \\]\nIt is clear that, in the case of \\(P > 1\\), the weights have a unique optimal solution \\(W_{11} = \\mathbb{1}_D\\) and \\(W_{21} = 0\\).\nIn the standard stochastic gradient descent (SGD) training process minimizing Eq. (2), a large value of P is needed to make the weight matrix converge to the unique solution, as the training error cannot be precisely reduced to zero."}, {"title": "CONCLUSION", "content": "A fundamental question in large language models is what contributes to the emergence ability of ICL, i.e., why simple next-token prediction based pre-training leads to in-context learning of previously unseen tasks, especially without further tuning the model parameters. Here, we turn the ICL into a spin glass model, and verify the equivalence between the standard SGD training and our statistical mechanic inference. We observe the fat tail distribution of coupling that determines how the model parameters of the transformer interact with each other. The transformer parameters are akin to an ensemble of real-valued spins in physics whose ground state suggests that the model can infer an unknown function from the shown test prompts after a pre-training of input sequences of sufficient task diversity. The phase diagram for single instance learning is also derived by our method, suggesting a continuous ICL transition.\nThe spin-glass model mapping of a simple transformer with linear attention working on linear regression tasks thus establishes a toy model of understanding emergent abilities of large language models. This work could be potentially generalized to other attention setting, e.g., the softmax one. Future exciting directions include explaining the chain-of-thought prompting [12], i.e., decomposition of a complex task into intermediate steps, and more challenging case of hallucination [30], i.e., the model could not distinguish the generated outputs from factual knowledge, or it could not understand what they generate [14]. We speculate that this hallucination may be intimately related to the solution space of the spin glass model given a fixed complexity of training dataset, e.g., spurious states in a standard associative memory model, as implied by Eq. (4). These open questions are expected to be addressed in the near future, thereby enhancing robustness and trustworthiness of AI systems."}, {"title": "Derivation of Relaxed Belief Propagation and Approximate Message Passing", "content": "In this appendix, we derive the approximate message passing equation in the main text. To proceed, we first define \\(\\xi_{ij} = \\sum_{k \\neq i,j} \\beta J_{ik} \\sigma_k\\), and \\(G(\\xi_{i\\rightarrow j}) = e^{i \\xi_{i\\rightarrow j}}\\). The cavity equation [Eq. (5)] can be written as\n\\[\\begin{aligned}\n\\eta_{ij}(\\sigma_i) &= \\frac{1}{Z_{ij}} e^{-\\beta h_i \\sigma_i - \\beta \\lambda_i \\sigma_i^2} \\int \\prod_{k \\neq i,j} [d\\sigma_k \\eta_{ki}(\\sigma_k)] G(\\xi_{i\\rightarrow j}) \\\\\n&= \\frac{1}{Z_{ij}} e^{-\\beta h_i \\sigma_i - \\beta \\lambda_i \\sigma_i^2} \\int d\\xi_{ij} \\tilde{G}(\\xi_{i\\rightarrow j}) \\prod_{k \\neq i,j} \\int d\\sigma_k \\eta_{ki}(\\sigma_k) \\exp \\left[ i \\xi_{ij} (\\beta J_{ik} \\sigma_k) \\right] \\\\\n&= \\frac{1}{Z_{ij}} e^{-\\beta h_i \\sigma_i - \\beta \\lambda_i \\sigma_i^2} \\int d\\xi_{ij} \\tilde{G}(\\xi_{i\\rightarrow j}) \\exp \\left[ i \\xi_{ij} (\\sum_{k \\neq i,j} \\beta J_{ik} m_{k\\rightarrow i}) - \\frac{\\xi_{ij}^2}{2} (\\sum_{k \\neq i,j} \\beta^2 J_{ik}^2 v_{ki} ) \\right].\n\\tag{11}\n\\end{aligned}\\]\nIn the second line of Eq. (11), we insert the Fourier transform of \\(G(\\xi)\\), \\(\\tilde{G}(\\xi_{i\\rightarrow j})\\) denotes the corresponding inverse transform, and in the third line, we use the Taylor expansion of exponential function, which requires us to define the following mean \\(m_{i\\rightarrow j}\\) and variance \\(v_{i\\rightarrow j}\\) of the message (the cavity marginal probability):\n\\[\\begin{aligned}\nm_{i\\rightarrow j} &= \\int d\\sigma_i \\eta_{i\\rightarrow j}(\\sigma_i) \\sigma_i, \\quad v_{i\\rightarrow j} = \\int d\\sigma_i \\eta_{i\\rightarrow j}(\\sigma_i) \\sigma_i^2 - m_{i\\rightarrow j}^2.\n\\tag{12}\n\\end{aligned}\\]"}]}