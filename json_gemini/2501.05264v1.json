{"title": "Towards Balanced Continual Multi-Modal Learning in Human Pose Estimation", "authors": ["Jiaxuan Peng", "Mengshi Qi", "Dong Zhao", "Huadong Ma"], "abstract": "3D human pose estimation (3D HPE) has emerged as a prominent research topic, particularly in the realm of RGB-based methods. However, RGB images are susceptible to limitations such as sensitivity to lighting conditions and potential user discomfort. Consequently, multi-modal sensing, which leverages non-intrusive sensors, is gaining increasing attention. Nevertheless, multi-modal 3D HPE still faces challenges, including modality imbalance and the imperative for continual learning. In this work, we introduce a novel balanced continual multi-modal learning method for 3D HPE, which harnesses the power of RGB, LiDAR, mmWave, and WiFi. Specifically, we propose a Shapley value-based contribution algorithm to quantify the contribution of each modality and identify modality imbalance. To address this imbalance, we employ a re-learning strategy. Furthermore, recognizing that raw data is prone to noise contamination, we develop a novel denoising continual learning approach. This approach incorporates a noise identification and separation module to mitigate the adverse effects of noise and collaborates with the balanced learning strategy to enhance optimization. Additionally, an adaptive EWC mechanism is employed to alleviate catastrophic forgetting. We conduct extensive experiments on the widely-adopted multi-modal dataset, MM-Fi, which demonstrate the superiority of our approach in boosting 3D pose estimation and mitigating catastrophic forgetting in complex scenarios. We will release our codes.", "sections": [{"title": "1. Introduction", "content": "3D human pose estimation (3D HPE) recovers 3D coordinates of human joints from various input sources. It has gained significant research attention due to its applications in human-robot interaction and computer animation. Specifically, in the rehabilitation context, where a variety of sensors and monitoring devices are deployed in the surroundings to detect patient action, our model can act as an indispensable tool for supervising and verifying the correctness of patients' exercises, ensuring adherence to established standards.\nExisting methods primarily focus on camera-based inputs (i.e., RGB images and videos) due to their accessibility and abundant human body information. However, camera-based approaches encounter limitations under challenging lighting conditions and necessitate complex spatial conversion from 2D to 3D, dependent on accurate camera parameters. Hence multi-modal human sensing emerges as a promising approach for addressing complex scenarios by leveraging diverse sensor modalities, as shown in Fig. 1 (a). Wearable sensors are constrained by user compliance, hindering their practical adoption in everyday scenarios. Therefore, non-intrusive sensors such as LiDAR, mmWave radar, and WiFi offer advantages in terms of illumination invariance and user convenience. By fusing information from RGB and non-intrusive sensors, we can enhance downstream task performance by exploiting both complementary and redundant information. Nevertheless, existing methods primarily rely on one or two modalities, leaving three or more modalities as an unexplored area.\nAs noted in , dominant modalities suppress the optimization of less dominant modalities in multi-modal learning, leading to modality imbalance, shown in Fig. 1 (b). While modality modulation techniques mitigate this issue by interfering with the learning of the dominant, they can degrade overall performance in certain scenarios, as discussed in . This is attributed to their neglect of the intrinsic limitations of different modalities' information capacity, resulting in a failure to achieve an effective balance. Consequently, the challenge lies in balancing modality optimizations without compromising the learning of dominant modalities.\nFurthermore, considering that these wireless sensors capture raw streaming data in the real world, the model necessitates incremental learning with new and continuous data. Moreover, practical multi-modal data often contains substantial noise, leading to multi-modal imbalance varying especially when only one modality is affected as depicted in Fig. 1 (c), as well as the acceleration of catastrophic forgetting . These variations lead to a dynamic multi-modal imbalance. Existing methods primarily focus on label noise in uni-modal data (e.g., images), but these approaches prove inadequate in multi-modal scenarios. In this work, we focus on a novel task of continual learning on noisy multi-modal data, where one modality is intentionally corrupted when a new task arrives.\nTo address modality imbalance in regression tasks and the aforementioned issues, we propose a novel balanced continual multi-modal learning method for 3D HPE. Specifically, we propose a Shapley value-based contribution algorithm applicable to diverse and complex multi-modal fusion strategies. This algorithm leverages the Pearson correlation coefficient and Shapley value to compute modality contribution scores for assessing the modalities. Additionally, we introduce an adaptive re-learning technique for alleviating imbalance. Furthermore, we introduce a novel noise identification and separation module (NIS), monitoring the contribution scores of all modalities across all tasks. Upon detecting a significant change in multi-modal scores, indicating potential noise contamination, the module identifies the noisy modality with significant score drops and safeguards the network from incorrect data influence, by identifying the noisy modality and separating the most noisy data from the dataset. Additionally, the re-learning strategy can also help mitigate the impact of noise by re-initializing parameters to prevent noise memorization. We also design a new adaptive EWC to alleviate the impact of noises on overcoming forgetting. To summarize, NIS module is employed to separate noise, preventing it from exacerbating catastrophic forgetting and training, while the adaptive EWC is utilized to counteract the forgetting based on clean data.\nOur main contributions are summarized as follows:\n(1) We propose a novel multi-modal model for 3D human pose estimation that integrates RGB images, LiDAR, mmWave, and WiFi data, boosting performance through balanced continual multi-modal learning.\n(2) We introduce a Shapley value-based contribution algorithm and an adaptive re-learning strategy to balance multi-modal learning. To our knowledge, this is the first attempt to address multi-modal imbalance in regression tasks.\n(3) We design a novel denoising continual multi-modal learning method with one noisy modality, and present a noise identification and separation module and adaptive EWC, collaborating with balanced multi-modal learning.\n(4) We conduct extensive experiments on the largest multi-modal dataset, MM-Fi , demonstrating our approach's superiority over baseline methods."}, {"title": "2. Related Work", "content": "3D Human Pose Estimation. Currently, the predominant focus of 3D HPE is on vision-based methods, categorized into two classes: directly estimating 3D joints and 2D-to-3D lifting . Recently, LiDAR has been applied to 3D HPE due to its robustness, which is utilized in autonomous driving. MmWave-based HPE has gained increasing attention in research community, driven by the demand for privacy-preserving technologies. WiFi-based sensing has also emerged as a promising area of research due to its popularity. Although RGB-based methods are the majority in research community, multi-modal methods could sacrifice the computational complexity for better performance and robustness. In this work, we aim to integrate these modalities to enhance 3D HPE performance.\nMulti-Modal Learning. Multi-modal learning has gained significant attention in recent years. Currently, some works focus on end-to-end multi-modal training, revealing issues like modality imbalance or competition, causing the performance inferior to that of uni-modal models . Hence various methods have been proposed to quantify modality imbalance and optimize the training for improving discriminative task performance. However, the imbalance in regression tasks remains unexplored, and existing methods exhibit limitations in terms of complex modality fusion strategies and the efficacy of balancing methods. This work tends to address imbalance in regression tasks based on Pearson's correlation and Shapley value and employs re-learning to realize superior balance.\nContinual Learning. Continual Learning (CL) is proposed to address the challenge of catastrophic forgetting . Regularization-based methods are designed with regularization terms to balance old and new tasks, such as EWC, offering the advantage of not requiring additional modules or buffer space. Existing methods employ filtering for noisy data in continual learning, primarily in uni-modal settings, requiring additional buffers. In this paper, we introduce an NIS module to prevent the noise from accelerating forgetting and an adaptive EWC to mitigate catastrophic forgetting, which improves the adaptive regularization terms for noisy modalities."}, {"title": "3. Proposed Approach", "content": "3.1. Overview\nProblem Definition. In the multi-modal 3D human pose estimation problem, given the four modality types M := {RGB (R), LiDAR (L), mmWave (M), WiFi (W)}, as depicted in Fig. 1(a), our objective is to estimate the corresponding 3D coordinates of the j-th human joints, denoted as $\\hat{y} = MM(M)$, $\\hat{y} \\in R^{j \\times 3}$, where MM(\u00b7) represents the multi-modal model. Specifically, the RGB inputs $X_R = {p_i^R}_{i=0}^{N_R}$, $p_i^R \\in R^{j \\times 2}$, consist of $N_R$ frames from a video, each containing j 2D human joints extracted from RGB images. The LiDAR point cloud is represented as $X_L = {p_i^L}_{i=0}^{N_L}$, $p_i^L \\in R^{N_L \\times 3}$, where $N_L$ is the total number of LiDAR points in a single frame. The mmWave radar point cloud is represented as $X_M = {p_i^M}_{i=0}^{N_M}$, $p_i^M \\in R^{N_M \\times d}$ where each point $p_i = (x, y, z, D, I)$ includes 3D coordinates, Doppler velocity D, and signal intensity I, with $N_M$ denoting the number of mmWave points. WiFi CSI data is denoted as $X_w = s$, $s \\in R^{a \\times c \\times t}$, where a represents the number of WiFi antennas, c denotes number of subcarriers per antenna, and t refers to the sampling frequency.\nOur proposed framework, as illustrated in Fig. 2, comprises balanced multi-modal learning, which assesses the modalities by computing uni-modal contribution scores and balances the optimization of modalities by adaptive re-learning, and denoising continual multi-modal learning, which identifies and separates noise to enhance the robustness and overcomes catastrophic forgetting for continual learning. Initially, our model employs modality-specific encoders to extract features from the respective data sources. Subsequently, a multi-modal fusion module combines the features from these modality-specific branches, and a pose regression head is finally applied to predict the final results.", "3.2. Balanced Multi-Modal Learning": "As illustrated in Fig. 2 (a), balanced multi-modal learning consists of two components. First, a Shapley value-based contribution algorithm calculates the uni-modal contribution scores based on Pearson correlation and Shapley value. Second, the adaptive re-learning strategy re-initializes the encoders of modalities according to contribution scores.\nShapley Value-Based Contribution Algorithm. Shapley value [31] was introduced in coalition game theory to address profit distribution by calculating each player's marginal contribution to the group, ensuring fair distribution. Inspired by the Shapley attribution method [18], we propose a Shapley value-based approach to quantify uni-modal contribution, capable of accommodating arbitrary modality combinations and fusion strategies.\nPrevious research on modality imbalance in multi-modal learning has mainly focused on discriminative models, using classifier head logits as Shapley value profits. However, in regression tasks, directly comparing ground truths and predictions as profits is impractical. To address this, we introduce Pearson correlation as the profit metric to overcome these limitations. Therefore, the contribution score $\\phi_m$ for modality m is obtained by calculating the Pearson correlation coefficients with all permutations of M\\{m} and m. Let S denote a subset of M\\{m}, the calculation is defined as:\n$\\phi_m(M) = \\sum_{S \\subset M \\setminus {m}} \\frac{|S|!(|M| - |S| - 1)!}{|M|!} v(S, m),$ (1)"}, {"title": "where", "content": "V(S, m) = s(y, MM(S \u222a {m})) - s(y, MM(S)).\nV(S, m) quantifies the additional profit gained by incorporating modality m into subset S, and s(\u00b7, \u00b7) returns the Shapley value profit through calculating the Pearson correlation coefficient between the ground truth y and the prediction $\\hat{y} = MM(M)$, which is formulated as:\ns(y, \\hat{y}) = \\sum_{i=1}^{j \\times 3} P(Y_i, \\hat{y_i}), (2)\nwhere j represents the number of human joints and $\\rho(Y_i, \\hat{y_i})$ is the Pearson correlation, formulated as:\n$\\rho(Y_i, \\hat{y_i}) = \\frac{cov(Y_i, \\hat{y_i})}{\\sigma_{y_i} \\cdot \\sigma_{\\hat{y_i}}},$ (3)\nwhere n signifies the mini-batch size, $cov(y_i, \\hat{y_i})$ is the covariance of $y_i$ and $\\hat{y_i}$, and $\\sigma_{y_i}$ and $\\sigma_{\\hat{y_i}}$ are the standard deviations of $y_i$ and $\\hat{y_i}$, respectively. When modalities are absent from S, we apply zero-padding to their features, ensuring network compatibility. This process iteratively computes the contribution of each modality across all potential combinations, culminating in all contribution scores for all modalities. By employing Pearson correlation as a substitute for logits in classification models, we successfully extend uni-modal contribution analysis to regression tasks.\nAdaptive Re-Learning Strategy. Unlike Wei et al. which re-initializes encoders based on the learning state of each modality, we implement re-learning according to contribution scores for balancing learning. Specifically, we employ K-Means on scores to partition the four modalities into two distinct clusters. The cluster exhibiting a higher mean score comprises the superior modalities denoted as $M_S$, characterized by a re-learning strength $\\alpha_S$, while the lower-scoring cluster constitutes the inferior modalities referred to as $M_I$, associated with $\\alpha_I$. The re-learning process for two modalities clusters is formalized as:\n$\\theta_i^m = \\begin{cases} \\alpha_S \\cdot \\theta_{init}^m + (1 - \\alpha_S) \\cdot \\theta_i^m, & m \\in M_S\\\\ \\alpha_I \\cdot \\theta_{init}^m + (1 - \\alpha_I) \\cdot \\theta_i^m, & m \\in M_I \\end{cases}$ (4)\nwhere $\\theta_i^m$ represents the encoder parameters for modality m at epoch i, with $\\theta_{init}^m$ denoting the initial stage."}, {"title": "3.3. Denoising Continual Multi-Modal Learning", "content": "As depicted in Fig. 2 (b), denoising continual multi-modal learning contains noise identification and separation module, which leverage contribution scores and a multi-modal filtering method to identify and filter noise, and adaptive EWC is proposed to mitigate the negative impact of noisy data on memorizing tasks.\nNoise Identification and Separation Module. Initially, we identify the noisy modality by examining the contribution scores of individual modalities. These scores are expected to remain stable under consistent data quality. Conversely, quality shifts, indicative of noise, manifest in score fluctuations, as depicted in Fig. 2 (b). Subsequently, building upon observations in that deep networks tend to prioritize learning clean samples, leading to lower loss values for clean samples compared to noisy counterparts, we extend the method in which employs Gaussian Mixture Model (GMM) to model loss distribution for noise filtering, which can be formulated as:\np(g|L_i) = \\frac{p(L_i|g)p(g)}{P(L_i)}, (5)\nwhere p(g|Li) is the posterior probability for each sample i, g denotes the Gaussian component associated with lower loss values (clean samples), and Li is the task loss of model w.r.t sample i (please refer to Eq. (10) in our model). Leveraging GMM to model loss of all samples, we partition the dataset into two subsets: the clean set C and the noisy set N, defined as:\n$\\begin{cases} C = {(x_i, y_i) \\in D | p(g|L_i)\u22650.5}\\\\ N = {(x_i, y_i) \\in D | p(g|L_i)<0.5} \\end{cases}$ (6)\nHowever, directly applying GMM modeling loss in multi-modal tasks is ineffective when noise is confined to a single modality. Once a modality becomes unreliable, the model prioritizes remaining modalities during the fitting, especially the remaining Ms modality. Therefore, based on the analysis in that the final estimate is a weighted combination of all four modalities, with the dominant ones exerting greater influence, we propose a gradient-stop method to model separable loss distributions, as illustrated in Fig. 3 (b), compared to the naive GMM without gradient-stop in Fig. 3 (a). Specifically, we halt optimization of Ms during the fitting, preventing rapid overfitting.\nAdaptive EWC. We employ adaptive EWC to counter forgetting in continual learning, which is an enhanced ver-"}, {"title": "where", "content": "$\\begin{aligned} I_{total} &= L_{MPJPE} + \\lambda \\cdot L_{EWC} \\\\ L_{EWC} &= \\sum_i \\frac{\\lambda}{2} I_i (\\theta_i - \\theta_{i,t-1})^2, \\end{aligned}$ (11)\nwhere $\\theta_{i, t-1}$ represents the i-th parameter of the model at task t - 1. The overall loss function in Algorithm 2 is:\n$L_{total} = L_{MPJPE} + \\lambda \\cdot L_{EWC},$ (12)\nwhere $\\lambda$ is a hyper-parameter controlling the strength of EWC loss. Noting that the element in Fisher information matrix $[I]_{ii}$ represents the importance of i-th parameter."}, {"title": "3.4. Training Processes and Objectives", "content": "For balanced multi-modal training, as outlined in Algorithm 1, the model leverages the MPJPE loss function:\n$\\text{L}_{MPJPE} = \\frac{1}{j} \\sum_{i} ||Y_i - \\hat{Y_i}||^2,$ (10)\nwhere j denotes the number of human joints."}, {"title": "4. Experiments", "content": "4.1. Experimental Setup\nDataset. MM-Fi is the first multi-modal non-intrusive 4D human dataset, including four wireless sensing modalities: RGB-D, LiDAR, mmWave radar, and WiFi. This dataset comprises 1,080 video clips, totaling 320k synchronized frames, performed by 40 volunteers engaged in 14 daily activities and 13 rehabilitation exercises. We conduct the experiments on three scenarios outlined in based on activity categories, where Protocol 1 includes 14 daily activities, Protocol 2 includes 13 rehabilitation exercises, and Protocol 3 involves all activities. We employ two data split strategies (S1, S2) from . S1 randomly splits data 3:1 for training and testing, while S2 splits by human subjects (32 for training, 8 for testing). In our experiments, we use S1 for balanced multi-modal learning setting and S2 for denoising continual multi-modal learning setting.\nEvaluation Metrics. In our experiments, we follow previous works using the following evaluation proto-\ncol"}, {"title": "The", "content": "average PA-MPJPE can be defined in a similar way.\nSettings. For the setting of balanced multi-modal learning (Setting 1), we train the model from scratch with randomly initialized parameters in an end-to-end manner, without loading any pre-trained parameters. While, in denoising continual multi-modal learning setting (Setting 2), we design a new task inspired by : the model only employs one single regression head, which is trained in all tasks, and the model does not require additional parameter-intensive modules for task memorization. Additional task labels are not required during the training and test stages. We divide data belonging to the same action class but distinguishing between left and right hands into one task of the incremental tasks, and for those action classes that do not differentiate between left and right hands, we combine two similar action classes into one task. Protocol 1 has 7 tasks and Protocol 2 contains 6 tasks. In each incremental task, only one of the four modalities is intentionally corrupted, referred to as raw and unaligned noise, with a noise ratio of 20% to 40%. The initial task is noise-free to establish basic modality perception. Please refer to the supplementary material for more details about settings and noise generation.\nCompared Methods. In Setting 1, we compare our method with a joint-training baseline and several modality balancing methods, including G-Blending , OGM-GE , AGM and Modality-level resample , across various fusion strategies: concatenation, MLP, and self-attention. In Setting 2, we compare our method to three baselines: naive training with no denoising and continual learning technologies, Co-teaching that selects low-loss samples, and DivideMix separating noise using two models. Co-teaching and DivideMix are re-implemented with EWC.\nImplementation Details. We implement our methods based on Pytorch on two NVIDIA RTX 3090 GPUs. Following , we utilize VideoPose3D as the backbone for RGB modality, Point Transformer for LiDAR and mmWave, and MetaFi++ for WiFi. In balanced multi-modal learning setting, we set the re-learning epoch r to 20 and the number of total epochs is 50. $\\alpha_S$ and $\\alpha_I$ are 0.5 and 0.7, respectively, In denoising continual multi-modal learning setting, the hyper-parameter $\\lambda$ and $\\beta$ of adaptive EWC are set to 10k and 0.3, respectively."}, {"title": "4.2. Results and Analysis", "content": "Comparison results of balanced multi-modal learning setting. As shown in Tab. 1, our proposed method surpasses other balancing techniques and the naive joint-training, demonstrating the ability to effectively address modality imbalance. We can see that our method outperforms the naive joint-training by around 2mm under MPJPE and around 0.4mm under PA-MPJPE and exceeds these balancing methods by about 5mm under MPJPE and about 2mm under PA-MPJPE. As discussed above, other methods neglect the limitation of information capacity. When the intrinsic properties of inferior modalities are constrained, disrupting superior modalities can lead to suboptimal optimization. As illustrated in Fig. 6, visualizations of results from our method closely align with the ground truth across diverse complex actions. More results on different fusion strategies can be seen in the supplementary materials.\nAs a pioneering effort in regression tasks, we conduct experiments to assess the rationality of contribution scores calculated by the Shapley value and Pearson correlation. The contribution score of each modality on Protocol 1 is illustrated in Fig. 4. We observe that RGB and LiDAR exhibit higher scores compared to mmWave and WiFi, indicating that the former two modalities contribute more to the results. According to the uni-modal performance in Tab. 3, RGB and LiDAR significantly outperform WiFi and mmWave in terms of MPJPE, achieving errors of 62.46mm and 65.95mm compared to 116.48mm and 166.55mm, respectively, which indicates that the former can provide more valuable information in multi-modal learning. These results demonstrate our method can reveal the relative contributions between different modalities, especially between $M_S$ and $M_I$. Please refer to the supplementary material for more experiments.\nComparison results of denoising continual multi-modal learning setting. We report the evaluation results on Protocol 1 and 2 with raw and unaligned noisy rates ranging from 20% to 40% in Tab. 2. Our method consistently outperforms the other methods, and yields improvements of up to 30mm under MPJPE and 20mm under PA-MPJPE. Co-teaching offers slight performance improvements, while DivideMix demonstrates substantial improvements over the baseline but still underperforms compared to our method. The enhanced EWC facilitates the model in overcoming catastrophic forgetting, while the noise identification and separation module, in collaboration with balancing multi-modal learning, prevents model training from the detrimental impact of the noisy modality. We also compare the performance of each task with baselines after training the model on final Task 6 as shown in Fig. 5 (a), in order to assess the ability of memorization. In Fig. 5 (b), we report the performance of Task i after training the model on Task i to assess the effectiveness of optimization. Our method surpasses them in both the ability to overcome forgetting and boost optimization.\nAnalysis of information capacity. As illustrated in Tab. 3, RGB and LiDAR consistently outperform the others in both evaluation metrics within the uni-modal setting, indicating their superior information capacity. Notably, RGB achieves low PA-MPJPE close to the all-modality fusion (34.14mm), suggesting that RGB provides precise pose information despite lacking 3D spatial data. Conversely, LiDAR offers"}, {"title": "5. Conclusion", "content": "In this paper, we presented a newly balanced continual multi-modal learning method for 3D HPE to address modality imbalance and noisy data issues. By assessing uni-modal contribution scores based on Shapley value and Pearson correlation, we optimized the learning process through adaptive re-learning to balance multi-modal model. Moreover, we presented a novel denoising continual learning approach to identify noisy modalities and then isolate noise. Extensive experiments on the MM-Fi dataset validated the superiority and effectiveness of our proposed approach."}]}