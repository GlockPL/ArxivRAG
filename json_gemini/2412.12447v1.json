{"title": "PERC: Plan-As-Query Example Retrieval for Underrepresented Code Generation", "authors": ["Jaeseok Yoo", "Hojae Han", "Youngwon Lee", "Jaejin Kim", "Seung-won Hwang"], "abstract": "Code generation with large language models has shown significant promise, especially when employing retrieval-augmented generation (RAG) with few-shot examples. However, selecting effective examples that enhance generation quality remains a challenging task, particularly when the target programming language (PL) is underrepresented. In this study, we present two key findings: (1) retrieving examples whose presented algorithmic plans can be referenced for generating the desired behavior significantly improves generation accuracy, and (2) converting code into pseudocode effectively captures such algorithmic plans, enhancing retrieval quality even when the source and the target PLs are different. Based on these findings, we propose Plan-as-query Example Retrieval for few-shot prompting in Code generation (PERC), a novel framework that utilizes algorithmic plans to identify and retrieve effective examples. We validate the effectiveness of PERC through extensive experiments on the CodeContests, HumanEval and MultiPL-E benchmarks: PERC consistently outperforms the state-of-the-art RAG methods in code generation, both when the source and target programming languages match or differ, highlighting its adaptability and robustness in diverse coding environments.", "sections": [{"title": "1 Introduction", "content": "Code generation using large language models (LLMs) has shown significant potential, particularly when retrieval-augmented generation (RAG) with few-shot examples is employed (Parvez et al., 2021; Nashid et al., 2023; Zhang et al., 2023). However, selecting effective examples to improve code generation quality remains a challenging task. This is even more difficult when the target programming language (PL) is underrepresented, as the construction of the few-shot example pool for retrieval is non-trivial.\nTo construct the retrieval pool for an underrepresented PL, we can transfer the retrieval pool from a high-resource PL. However, this in turn interferes with the state-of-the-art few-shot prompting approaches in code generation (Nashid et al., 2023; Zhang et al., 2023), which employ code to retrieve examples. Figure 1 illustrates such an example; although the Python code on the lower left and the Lua code on the lower right follow the same algorithmic steps, both lexical-based (Robertson and Zaragoza, 2009) and embedding-based (Song et al., 2020) retrieval fall short in capturing their algorithmic similarity due to syntactic and structural difference (An et al., 2023).\nTo overcome this, we propose Plan-as-query Example Retrieval for few-shot prompting in Code generation (PERC). PERC leverages algorithmic plans such as pseudocode to retrieve examples."}, {"title": "2 Related Work", "content": "Retrieval-Augmented Code Generation Previous works adopting RAG in code generation tasks have primarily focused on enhancing the accuracy of generated code by retrieving from the target PL pool (Parvez et al., 2021; Lu et al., 2022). More recently, CEDAR (Nashid et al., 2023) retrieved few-shot examples based on code-code similarity, and RepoCoder (Zhang et al., 2023) leveraged LLM-generated code snippets in target PL to expand queries, allowing for improved retrieval.\nUtilizing Algorithmic Plans in Code Retrieval and Generation Han et al. (2021) viewed pseudocode as algorithmic plan of code, to reduce the modality gap between text and code in code search task. Jiang et al. (2024) used pseudocode-based algorithmic plans for code generation through few-shot prompting, and Sun et al. (2024) used pseudocode to bridge different programming languages.\nOur distinction. We are the first to leverage algorithmic plans in retrieval-augmented code generation, which allows to retrieve effective few-shot"}, {"title": "3 Preliminaries", "content": "Given a natural language query tq describing a desired program, code generation aims to return the corresponding implementation. In few-shot example retrieval, we draw relevant text-code pairs (t, c) from an example pool P to supplement LLM's knowledge and guide generation.\nProblem-As-Query A baseline approach maps query tq and text descriptions t into a shared latent space using encoder \u03c8. The top-k examples are selected based on similarity sim(.):\nE = topk(t,c)ep sim(\u03c8(tq), \u03c8(t)), (1)\nwhere E is the set of indices of chosen examples.\nCEDAR Nashid et al. (2023) selects examples based on code-code similarity. As the prompt lacks"}, {"title": "4 PERC", "content": "PERC retrieves relevant examples using algorithmic plans in pseudocode. These plans capture high-level logic while minimizing cross-lingual lexical differences, thereby supporting accurate code generation.\nAs depicted in Figure 2, the workflow of PERC consists of two key steps. First, it drafts a plan for the given problem to form an expanded query, which is used to retrieve examples that were projected to plan space in indexing time. Then, the retrieved examples and their plans are integrated into a reasoning chain to generate a revised plan and the final code."}, {"title": "4.1 Plan-As-Query Example Retrieval", "content": "A key contribution of PERC is the use of algorithmic plans written in pseudocode, for effective retrieval. Specifically, an LLM generates pseudocode p for the retrieval pool P offline, and piq for tq at inference time. Then, the query is expanded with pq as follows:\nEPERC = topk(t,\u00f4,c) sim(\u03c8(tq; pq), \u03c8(t;p)), (4)\nwhere the in-context example for pq is provided in Appendix C. As illustrated in Figure 1, Eq (4) avoids surface-level details by projecting code c into plans. This is in contrast to Eq (3), which exposes the retriever to such distractions, especially when \u0109 and c use different PLs."}, {"title": "4.2 Code Generation with Examples", "content": "We use generated pseudocode as intermediate reasoning steps for code generation (Jiang et al., 2024). Each few-shot example in our prompt consists of a triplet (t, p, c), where text description t, generated pseudocode \u00f4, and code c guide the LLM to utilize pseudocode in its reasoning chain:\nprompt = [[t; p; c](t,\u00f4,c)\u2208EPERc; tq]. (5)\nWhen the target programming language differs from that of example code c, we replace c with generated code \u0109 in the target language, where the LLM generates \u0109 using the in-context example shown in Appendix C:\nprompt = [[t; p; \u0109](t,\u00f4,c)\u2208Eperc; tq]. (6)"}, {"title": "5 Experiments", "content": "5.1 Experimental Setup\nExperiments were conducted using GPT-3.5-Turbo-16k and GPT-40-mini as the backbone LLMs. Other implementation details regarding the embedding-based retrieval and hyperparameter configuration for code generation can be found in Appendix A.\nMetrics We evaluated the performance of PERC using the widely used Pass@1 metric (Chen et al., 2021), which is an unbiased estimator of the model's chance of producing a correct code sample in a single attempt.\nBaselines We compared our method against several established baselines to highlight the effectiveness of PERC: 1) w/o Examples generates code directly without using few-shot examples, 2) Random Selection uses a randomly chosen, then fixed set of examples, 3) Problem-As-Query Retrieval retrieves examples based on problem-problem similarity, 4) CEDAR (Nashid et al., 2023) uses code-code similarity, and 5) RepoCoder (Zhang et al., 2023) expands the query with predicted code.\nDatasets For evaluation, we used CodeContests (Li et al., 2022), HumanEval (Chen et al., 2021), and MultiPL-E (HumanEval; Cassano et al., 2023) benchmarks. For CodeContests, we used the train split as the example pool, while MBPP (Austin et al., 2021) benchmark was used for the other two. Throughout the benchmarks, we used Python-a high-resource PL-as the source. We used Python as the primary target PL since it is the only officially supported language in both CodeContests and HumanEval benchmarks. For additional target PLs, we selected languages based on the frequency classes (NICHE, LOW, MEDIUM) established in MultiPL-E. We randomly chose two PLs from each class: Ruby and Go (MEDIUM), Rust and R (LOW), and Lua and Julia (NICHE)."}, {"title": "5.2 RAG from Same PL Pool: CodeContests and Human Eval", "content": "Table 1 shows that PERC outperforms all baselines, achieving Pass@1 scores of 6.61% and 76.04% on CodeContests and HumanEval, respectively, using the GPT-3.5-Turbo-16k model. Similarly, the results for GPT-40-mini in Table 2 show Pass@1 scores of 8.48% and 88.17%, demonstrating consistently high performance across benchmarks. This supports that retrieval based on algorithmic plans better captures the logic of the code and allows to surface more effective demonstrations in top-k."}, {"title": "5.3 RAG from Cross-PL Pool: MultiPL-E", "content": "The results for each PL in MultiPL-E, presented in Tables 1 and 2, are sorted in descending order of code generation accuracy without examples. PLs with higher accuracy are considered to have higher data availability, while those with lower accuracy are regarded as underrepresented.\nUsing the GPT-3.5-Turbo-16k model, PERC achieves the best Pass@1 scores across all PLs, with notable results such as 69.81% for Ruby, 63.78% for Rust, and 64.10% for Lua, as shown in Table 1. The results for GPT-40-mini, presented in Table 2, also emphasizes PERC's effectiveness, showing consistent Pass@1 score improvements, including 83.85% for Ruby, 70.69% for Julila, and 57.14% for R.\nBy effectively transferring knowledge from high-resource PLS, PERC demonstrated improved code generation accuracy for different PLs, showing its ability to bridge knowledge gaps across PLs with different data availability. In contrast, state-of-the-art approaches RepoCoder and CEDAR struggled with code generation. This limitation stemmed from their reliance on code-based retrieval, where modality differences introduced noise and hindered the identification of algorithmically relevant code."}, {"title": "6 Analysis and Discussion", "content": "Open-Source LLM as a Backbone Table 3 shows consistent accuracy improvements with the open-source model Llama-3.1-8B-Instruct. PERC outperformed the baselines and demonstrated effective performance across PLs like Ruby, Lua, and R in the MultiPL-E benchmark. This highlights the improvements with PERC generalizes well to\nOne may consider a cost-exhaustive approach of translating all the code in the pool to target (underrepresented) PLs, which incurs O(|T||P|) cost where T is the set of target PLs to handle, whereas PERC only requires O(|P|)."}, {"title": "Limitations", "content": "While PERC brings notable performance improvements in code generation with few-shot prompting, even if the source and target PLs do not match, our observation in Section 6 is that PERC shows slight performance drop as more and more programming languages are added to the retrieval pool. With an ideal retrieval system suited to selecting the most effective examples, one should observe monotonically increasing performance as more candidates are added to the pool; we leave more investigation and improvements as future work."}]}