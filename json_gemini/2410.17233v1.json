{"title": "FEW-SHOT IN-CONTEXT PREFERENCE LEARNING USING LARGE LANGUAGE MODELS", "authors": ["Chao Yu", "Hong Lu", "Jiaxuan Gao", "Qixin Tan", "Xinting Yang", "Yu Wang", "Yi Wu", "Eugene Vinitsky"], "abstract": "Designing reward functions is a core component of reinforcement learning but can be challenging for truly complex behavior. Reinforcement Learning from Human Feedback (RLHF) has been used to alleviate this challenge by replacing a hand-coded reward function with a reward function learned from preferences. However, it can be exceedingly inefficient to learn these rewards as they are often learned tabula rasa. We investigate whether Large Language Models (LLMs) can reduce this query inefficiency by converting an iterative series of human preferences into code representing the rewards. We propose In-Context Preference Learning (ICPL), a method that uses the grounding of an LLM to accelerate learning reward functions from preferences. ICPL takes the environment context and task description, synthesizes a set of reward functions, and then repeatedly updates the reward functions using human rankings of videos of the resultant policies. Using synthetic preferences, we demonstrate that ICPL is orders of magnitude more efficient than RLHF and is even competitive with methods that use ground-truth reward functions instead of preferences. Finally, we perform a series of human preference-learning trials and observe that ICPL extends beyond synthetic settings and can work effectively with humans-in-the-loop. Additional information and videos are provided at https://sites.google.com/view/few-shot-icpl/home.", "sections": [{"title": "1 INTRODUCTION", "content": "Designing state-of-the-art agents using reinforcement learning (RL) often requires the design of reward functions that specify desired and undesirable behaviors. However, for sufficiently complex tasks, designing an effective reward function remains a significant challenge. This process is often difficult, and poorly designed rewards can lead to biased, misguided, or even unexpected behaviors in RL agents (Booth et al., 2023). Recent advances in large language models (LLMs) have shown potential in tackling this challenge as they are able to zero-shot generate reward functions that satisfy a task specification (Yu et al., 2023). However, the ability of LLMs to directly write a reward function is limited when task complexity increases or tasks are out-of-distribution from pre-training data. As an additional challenge, humans may be unable to perfectly specify their desired agent behavior in text.\nHuman-in-the-loop learning (HL) offers a potential enhancement to the reward design process by embedding human feedback directly into the learning process. A ubiquitous approach is preference-based RL where preferences between different agent behaviors serves as the primary learning signal. Instead of relying on predefined rewards, the agent learns a reward function aligned with human preferences. This interactive approach has shown success in various RL tasks, including standard benchmarks (Christiano et al., 2017; Ibarz et al., 2018), encouraging novel behaviors (Liu et al., 2020; Wu et al., 2021), and overcoming reward exploitation (Lee et al., 2021a). However, in more complex tasks involving extensive agent-environment interactions, preference-based RL often demands hundreds or thousands of human queries to provide effective feedback. For instance, a robotic"}, {"title": "2 RELATED WORK", "content": "Reward Design. In reinforcement learning, reward design is a core challenge, as rewards must both represent a desired set of behaviors and provide enough signal for learning. The most common approach to reward design is handcrafting, which requires a large number of trials by experts (Sutton, 2018; Singh et al., 2009). Since hand-coded reward design requires extensive engineering effort, several prior works have studied modeling the reward function with precollected data. For example, Inverse Reinforcement Learning (IRL) aims to recover a reward function from expert demonstration data (Arora & Doshi, 2021; Ng et al., 2000). With advances in pretrained foundation models, some recent works have also studied using large language models or vision-language models to provide reward signals (Ma et al., 2022; Fan et al., 2022; Du et al., 2023; Karamcheti et al., 2023; Kwon et al., 2023; Wang et al., 2024; Ma et al., 2024).\nAmong these approaches, EUREKA (Ma et al., 2023) is the closest to our work, instructing the LLM to generate and select novel reward functions based on environment feedback with an evolutionary framework. However, as opposed to performing preference learning, EUREKA uses the LLM to design dense rewards that help with learning a hand-designed sparse reward. In contrast, ICPL focuses on finding a reward that correctly orders a set of preferences. However, we note that EUREKA also has a small, preliminary investigation combining human preferences with an LLM"}, {"title": "3 PROBLEM DEFINITION", "content": "Our goal is to design a reward function that can be used to train reinforcement learning agents that demonstrate human-preferred behaviors. It is usually hard to design proper reward functions in reinforcement learning that induce policies that align well with human preferences.\nMarkov Decision Process with Preferences( Wirth et al. (2017)) A Markov Decision Process with Preferences (MDPP) is defined as a tuple $\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, \\mu, \\sigma, \\gamma, \\rho)$ where $\\mathcal{S}$ denotes the state space, $\\mathcal{A}$ denotes the action space, $\\mu$ is the distribution of initial states, $\\sigma$ is the state transition model, $\\gamma\\in [0,1)$ is the discount factor. $\\rho$ is the preference relation over trajectories, i.e. $\\rho(\\tau_i > \\tau_j)$ denotes the probability with which trajectory $\\tau_i$ is preferred over $\\tau_j$. Given a set of preferences $\\zeta$, the goal in an MDPP is to find a policy $\\pi^*$ that maximally complies with $\\zeta$. A preference $\\tau_1 > \\tau_2$ is satisfied by $\\pi$ if and only if $Pr_{\\pi}(\\tau_1) > Pr_{\\pi}(\\tau_2)$ where $Pr_{\\pi}(\\tau) = \\mu(s_0) \\Pi \\sigma(s_{t+1}|s_t, a_t)$.\nThis can be viewed as finding a $\\pi^*$ that minimizes a preference loss $\\mathcal{L}(\\pi,\\varsigma) = \\sum_i \\mathcal{L}(\\pi, \\varsigma_i)$, where $\\mathcal{L}(\\pi, \\tau_1 > \\tau_2) = -(Pr_{\\pi}(\\tau_1) - Pr_{\\pi}(\\tau_2))$.\nReward Design Problem with Preferences. A reward design problem with preferences (RDPP) is a tuple $\\mathcal{P} = \\langle \\mathcal{M}, \\mathcal{R}, \\mathcal{A}_M, \\varsigma \\rangle$, where $\\mathcal{M}$ is a Markov Decision Process with Preferences, $\\mathcal{R}$ is the space of reward functions, $\\mathcal{A}_M(\\cdot) : \\mathcal{R} \\rightarrow \\Pi$ is a learning algorithm that outputs a policy that optimizes a reward $R \\in \\mathcal{R}$ in the MDPP. $\\varsigma = \\{(\\tau_1, \\tau_2)\\}$ is the set of preferences. In an RDPP, the goal is to find a reward function $R \\in \\mathcal{R}$ such that the policy $\\pi = \\mathcal{A}_M(R)$ that optimizes R maximally complies with the preference set $\\varsigma$. In Preference-based Reinforcement Learning, the learning algorithms usually involve multiple iterations, and the preference set $\\varsigma$ is constructed in every iteration by sampling trajectories from the policy or policy population."}, {"title": "4 METHOD", "content": "Our proposed method, In-Context Preference Learning (ICPL), integrates LLMs with human preferences to synthesize reward functions. The LLM receives environmental context and a task description to generate an initial set of K executable reward functions. ICPL then iteratively refines these functions. In each iteration, the LLM-generated reward functions are used to train agents within the environment, producing a set of agents; we use these agents to generate videos of their behavior. A ranking is formed over the videos, from which we retrieve the best and worst reward functions corresponding to the top and bottom videos in the ranking. These selections serve as examples of positive and negative preferences. The preferences, along with additional contextual information, such as reward traces and differences from previous good reward functions, are provided as feedback prompts to the LLM. The LLM takes in this context and is asked to generate a new set of rewards. Algo. 1 presents the pseudocode, and Fig. 1 illustrates the overall process of ICPL."}, {"title": "4.1 REWARD FUNCTION INITIALIZATION", "content": "To enable the LLM to synthesize effective reward functions, it is essential to provide task-specific information, which consists of two key components: a description of the environment, including the observation and action space, and a description of the task objectives. At each iteration, ICPL ensures that K executable reward functions are generated by resampling until there are K executable reward functions."}, {"title": "4.2 SEARCH REWARD FUNCTIONS BY HUMAN PREFERENCES", "content": "For tasks without reward functions, the traditional preference-based approach typically involves constructing a reward model, which often demands substantial human feedback. Our approach, ICPL, aims to enhance efficiency by leveraging LLMs to directly search for optimal reward functions without the need to learn a reward model. To expedite this search process, we use an LLM-guided search to find well-performing reward functions. Specifically, we generate K = 6 executable reward functions per iteration across N = 5 iterations. In each iteration, humans select the most preferred"}, {"title": "4.3 AUTOMATIC FEEDBACK", "content": "In each iteration, the LLM not only incorporates human preferences but also receives automatically synthesized feedback. This feedback is composed of three elements: the evaluation of selected reward functions, the differences between historical good reward functions, and the reward trace of these historical reward functions.\nEvaluation of reward functions: The component values that make up the good and bad reward functions are obtained from the environment during training and provided to the LLM. This helps the LLM assess the usefulness of different parts of the reward function by comparing the two.\nDifferences between historical reward functions: The best reward functions selected by humans from each iteration are taken out, and for any two consecutive good reward functions, their differences are analyzed by another LLM. These differences are supplied to the primary LLM to assist in adjusting the reward function.\nReward trace of historical reward functions: The reward trace, consisting of the values of the good reward functions during training from all prior iterations, is provided to the LLM. This reward trace enables the LLM to evaluate how well the agent is actually able to optimize those reward components."}, {"title": "5 EXPERIMENTS", "content": "In this section, we conducted two sets of experiments to evaluate the effectiveness of our method: one using proxy human preferences and the other using real human preferences.\n1) Proxy Human Preference: In this experiment, human-designed rewards, taken from EUREKA (Ma et al., 2023), were used as proxies of human preferences. Specifically, if ground truth reward $R_1 > R_2$, sample 1 is preferred over sample 2. This method enables rapid and quantitative evaluation of our approach. It corresponds to a noise-free case that is likely easier than human trials; if ICPL performed poorly here it would be unlikely to work in human trials. Importantly, human-designed rewards were only used to automate the selection of samples and were not included in the prompts sent to the LLM; the LLM never observes the functional form of the ground truth rewards nor does it ever receive any values from them. Since proxy human preferences are free from noise, they offer a reliable comparison to evaluate our approach efficiently. However, as discussed later in the limitations section, these proxies may not correctly measure challenges in human feedback such as inability to rank samples, intransitive preferences, or other biases.\n2) Human-in-the-loop Preference: To further validate our method, we conducted a second set of experiments with human participants. These participants repeated the tasks from the Proxy Human Preferences and engaged in an additional task that lacked a clear reward function: \u201cMaking a humanoid jump like a real human.\""}, {"title": "5.1 TESTBED", "content": "All experiments were conducted on tasks from the Eureka benchmark (Ma et al., 2023) based on IsaacGym, covering a diverse range of environments: Cartpole, BallBalance, Quadcopter, Anymal, Humanoid, Ant, FrankaCabinet, ShadowHand, and AllegroHand. We adhered strictly to the original task configurations, including observation space, action space, and reward computation. This ensures that our method's performance was evaluated under consistent and well-established conditions across a variety of domains."}, {"title": "5.2 BASELINES", "content": "We compared our method against two baselines:"}, {"title": "5.3 EXPERIMENT SETUP", "content": "We use PPO as the reinforcement learning algorithm for all methods. For each task, we use the default hyperparameters of PPO provided by IsaacGym as these have previously been tuned to achieve high performance and were also used in our baselines, making a fairer comparison. We trained policies and rendered videos on a single A100 GPU machine. The total time for a full experiment was less than one day of wall clock time. We utilized GPT-4, specifically GPT-4-0613, as the backbone LLM for both our method, ICPL, and Eureka in the Proxy Human Preference experiment. For the Human-in-the-loop Preference experiment, we employ GPT-40."}, {"title": "5.3.2 EVALUATION METRIC", "content": "Here, we provide a specific explanation of how sparse rewards (detailed in Appendix A.4) are used as task metrics in the adopted IsaacGym tasks. The task metric is the average of the sparse rewards across all environment instances. To assess the generated reward function or the learned reward model, we take the maximum task metric value from 10 policy checkpoints sampled at fixed intervals, marked as the task score of reward function/model denoted as (RTS).\nEureka can access the ground-truth task score and select the highest RTS from these iterations as the task score, denoted as (TS) for each experiment. For a fair comparison, ICPL also performs 5 iterations, selecting the highest RTS from these iterations as TS for each experiment. Unlike (Ma et al., 2023), which additionally conducts 5 independent PPO training runs for the reward function with the highest RTS and reports the average of 5 maximum task metric values from 10 policy checkpoints sampled at fixed intervals as the task score, we focus on selecting the best reward function according to human preferences, yielding policies better aligned with the task description. Due to the inherent randomness of LLM sampling, we run 5 experiments for all methods, including PrefPPO and EUREKA and report the highest TS as the final task score, denoted as (FTS), for each approach. A higher FTS indicates better performance across all tasks."}, {"title": "5.4 RESULTS OF PROXY HUMAN PREFERENCE", "content": "In ICPL, we use human-designed rewards as proxies to simulate ideal human preferences. Specifically, in each iteration, we select the reward function with the highest RTS as the good example and the reward function with the lowest RTS as the bad example for feedback. In Eureka, the reward function with the highest RTS is selected as the candidate reward function for feedback in each iteration. In PrefPPO, human-designed dense rewards are used as the preference metric: if the cumulative dense reward of trajectory 1 is greater than that of trajectory 2, then trajectory 1 is preferred over trajectory 2. Table 1 shows the final task score (FTS) for all methods across IsaacGym tasks.\nFor ICPL and PrefPPO, we track the number of synthetic queries Q required as a proxy for measuring the likely real human effort involved, which is crucial for methods that rely on human-in-the-loop preference feedback. Specifically, we define a single query as a human comparing two trajectories and providing a preference. In ICPL, each iteration generates K reward function samples, resulting in K corresponding videos. The human compares these videos, first selecting the best one, then picking the worst from the remaining K \u2013 1 videos. After N = 5 iterations, the best video of each iteration is compared to select the overall best. The number of human queries Q can be calculated as $Q = (K-1) \\times 2N - 1$. For ICPL, with K = 6 and N = 5, this results in Q = 49. In PrefPPO, the simulated human teacher compares two sampled trajectories and provides a preference label to update the reward model. We set the maximum number of queries to Q = 49, matching ICPL, and also test Q = 150, 1.5k, and 15k, denoted as PrefPPO-#Q in Table 1, to compare the final task score (FTS) across different tasks.\nAs shown in Table 1, for the simpler tasks like Cartpole and BallBalance, all methods achieve equal performance. Notably, we observe that for these particularly simple tasks, LLM-powered methods like Eureka and ICPL can generate correct reward functions in a zero-shot manner, without requiring feedback. As a result, ICPL only requires querying the human 5 times, while PrefPPO, after 5 queries, fails to train a reasonable reward model with the preference-labeled data. For relatively more challenging tasks, PrefPPO-49 performs significantly worse than ICPL when using the same"}, {"title": "5.4.1 MAIN RESULTS", "content": "In ICPL, we use human-designed rewards as proxies to simulate ideal human preferences. Specifically, in each iteration, we select the reward function with the highest RTS as the good example and the reward function with the lowest RTS as the bad example for feedback. In Eureka, the reward function with the highest RTS is selected as the candidate reward function for feedback in each iteration. In PrefPPO, human-designed dense rewards are used as the preference metric: if the cumulative dense reward of trajectory 1 is greater than that of trajectory 2, then trajectory 1 is preferred over trajectory 2. Table 1 shows the final task score (FTS) for all methods across IsaacGym tasks.\nFor ICPL and PrefPPO, we track the number of synthetic queries Q required as a proxy for measuring the likely real human effort involved, which is crucial for methods that rely on human-in-the-loop preference feedback. Specifically, we define a single query as a human comparing two trajectories and providing a preference. In ICPL, each iteration generates K reward function samples, resulting in K corresponding videos. The human compares these videos, first selecting the best one, then picking the worst from the remaining K \u2013 1 videos. After N = 5 iterations, the best video of each iteration is compared to select the overall best. The number of human queries Q can be calculated as $Q = (K-1) \\times 2N - 1$. For ICPL, with K = 6 and N = 5, this results in Q = 49. In PrefPPO, the simulated human teacher compares two sampled trajectories and provides a preference label to update the reward model. We set the maximum number of queries to Q = 49, matching ICPL, and also test Q = 150, 1.5k, and 15k, denoted as PrefPPO-#Q in Table 1, to compare the final task score (FTS) across different tasks.\nAs shown in Table 1, for the simpler tasks like Cartpole and BallBalance, all methods achieve equal performance. Notably, we observe that for these particularly simple tasks, LLM-powered methods like Eureka and ICPL can generate correct reward functions in a zero-shot manner, without requiring feedback. As a result, ICPL only requires querying the human 5 times, while PrefPPO, after 5 queries, fails to train a reasonable reward model with the preference-labeled data. For relatively more challenging tasks, PrefPPO-49 performs significantly worse than ICPL when using the same"}, {"title": "5.5 METHOD ANALYSIS", "content": "To validate the effectiveness of ICPL's module design, we conducted ablation studies. We aim to answer several questions that could undermine the results presented here:\n1. Are components such as the reward trace or the reward difference helpful?\n2. Is the LLM actually performing preference learning? Or is it simply zero-shot outputting the correct reward function due to the task being in the training data?"}, {"title": "5.5.1 ABLATIONS", "content": "The results of the ablations are shown in Table 2. In these studies, \u201cICPL w/o RT\" refers to removing the reward trace from the prompts sent to the LLMs. \u201cICPL w/o RTD\" indicates the removal of both the reward trace and the differences between historical reward functions from the prompts. \"ICPL w/o RTDB\" removes the reward trace, differences between historical reward functions, and bad reward functions, leaving only the good reward functions and their evaluation in the prompts. The \"OpenLoop\" configuration samples K \u00d7 N reward functions without any feedback, corresponding to the ability of the LLM to zero-shot accomplish the task.\nDue to the large variance of the experiments (see Appendix), we mark the top two results in bold. As shown, ICPL achieves top 2 results in 8 out of 9 tasks and is comparable on the Allegro task. The \"OpenLoop\" configuration performs the worst, indicating that our method does not solely rely on GPT-4's either having randomly produced the right reward function or having memorized the reward function during its training. This improvement is further demonstrated in Sec. 5.5.2, where we show the step-by-step improvements of ICPL through proxy human preference feedback. Additionally, \"ICPL w/o RT\" underperforms on multiple tasks, highlighting the importance of incorporating the reward trace of historical reward functions into the prompts."}, {"title": "5.5.2 IMPROVEMENT ANALYSIS", "content": "Table 1 presents the performance achieved by ICPL. While it is possible that the LLMs could generate an optimal reward function in a zero-shot manner, the primary focus of our analysis is not solely on absolute performance values. Rather, we emphasize whether ICPL is capable of enhancing performance through the iterative incorporation of preferences. We calculated the average RTS"}, {"title": "5.6 RESULTS OF HUMAN-IN-THE-LOOP PREFERENCE", "content": "To address the limitations of proxy human preferences, which simulate idealized human preference and may not fully capture the challenges humans may face in providing preferences, we conducted experiments with real human participants. We recruited 6 volunteers to participate in human-in-the-loop experiments, including 5 tasks from IsaacGym and a newly designed task. None of the volunteers had prior experience with these tasks, ensuring an unbiased evaluation based on their preferences."}, {"title": "5.6.1 HUMAN EXPERIMENT SETUP", "content": "Before the experiment, each volunteer was provided with a detailed explanation of the experiment's purpose and process. Additionally, volunteers were fully informed of their rights, and written consent was obtained from each participant. The experimental procedure was approved by the department's ethics committee to ensure compliance with institutional guidelines on human subject research.\nMore specifically, each volunteer was assigned an account with a pre-configured environment to ensure smooth operation. After starting the experiment, LLMs generated the first iteration of reward functions. Once the reinforcement learning training was completed, videos corresponding to the policies derived from each reward function were automatically rendered. Volunteers compared the behaviors in the videos with the task descriptions and selected both the best and the worst-performing videos. They then entered the respective identifiers of these videos into the interactive interface and pressed \u201cEnter\u201d to proceed. The human preference was processed as an LLM prompt for generating feedback, leading to the next iteration of reward function generation.\nThis training-rendering-selection process was repeated across 4 iterations. At the end of the final iteration, the volunteers were asked to select the best video from those previously marked as good, designating it as the final result of the experiment. For IsaacGym tasks, the corresponding RTS was recorded as TS. It is important to note that, unlike proxy human preference experiments where the TS is the maximum RTS across iterations, in the human-in-the-loop preference experiment, TS"}, {"title": "5.6.2 ISAACGYM TASKS", "content": "Due to the simplicity of the Cartpole, BallBalance, Franka tasks, where LLMs were able to zero-shot generate correct reward functions without any feedback, these tasks were excluded from the human trials. The Anymal task, which involved commanding a robotic dog to follow random commands, was also excluded as it was difficult for humans to evaluate whether the commands were followed based solely on the videos. For the 5 adopted tasks, we describe in the Appendix A.6.1 how humans infer tasks through videos and the potential reasons that may lead to preference rankings that do not accurately reflect the task.\nTable 3 presents the FTS for the human-in-the-loop preference experiments conducted across 5 suitable IsaacGym tasks, labeled as \u201cICPL-real\u201d. The results of the proxy human preference experiment are labeled as \u201cICPL-proxy\u201d. As observed, the performance of \u201cICPL-real\u201d is comparable or slightly lower than that of \u201cICPL-proxy\u201d in all 5 tasks, yet it still outperforms the \u201cOpenLoop\u201d results in 3 out of 5 tasks. This indicates that while humans may have difficulty providing consistent preferences from videos as proxies, their feedback can still be effective in improving performance when combined with LLMs."}, {"title": "5.6.3 HUMANOID JUMP TASK", "content": "In our study, we introduced a new task: HumanoidJump, with the task description being \"to make humanoid jump like a real human.\u201d Defining a precise task metric for this objective is challenging, as the criteria for human-like jumping are not easily quantifiable. The task-specific prompts used in this experiment are detailed in the Appendix A.6.2.\nThe most common behavior observed in this task, as illustrated in Fig. 4, is what we refer to as the \"leg-lift jump.\u201d This behavior involves initially lifting one leg to raise the center of mass, followed by the opposite leg pushing off the ground to achieve lift. The previously lifted leg is then lowered to extend airtime. Various adjustments of the center of mass with the lifted leg were also noted. This behavior meets the minimal metric of a jump: achieving a certain distance off the ground. If feedback were provided based solely on this minimal metric, the \"leg-lift jump\" would likely be selected as a candidate reward function. However, such candidates show limited improvement in subsequent iterations, failing to evolve into more human-like jumping behaviors.\nConversely, when real human preferences were used to guide the task, the results were notably different. The volunteer judged the overall quality of the humanoid's jump behavior instead of just the metric of leaving the ground. Fig. 5 illustrates an example where the volunteer successfully guided the humanoid towards a more human-like jump by selecting behaviors that, while initially not optimal, displayed promising movement patterns. The reward functions are shown in Appendix A.6.2. In the first iteration, \"leg-lift jump\" was not selected despite the humanoid jumping off the ground. Instead, a video where the humanoid appears to attempt a jump using both legs, without leaving the ground, was chosen. By the fifth and sixth iterations, the humanoid demonstrated more"}, {"title": "6 CONCLUSION", "content": "Our proposed method, In-Context Preference Learning (ICPL), demonstrates significant potential for addressing the challenges of preference learning tasks through the integration of large language models. By leveraging the generative capabilities of LLMs to autonomously produce reward functions, and iteratively refining them using human feedback, ICPL reduces the complexity and human effort typically associated with preference-based RL. Our experimental results, both in proxy human and human-in-the-loop settings, show that ICPL not only surpasses traditional RLHF in efficiency but also competes effectively with methods utilizing ground-truth rewards instead of preferences. Furthermore, the success of ICPL in complex, subjective tasks like humanoid jumping highlights its versatility in capturing nuanced human intentions, opening new possibilities for future applications in complex real-world scenarios where traditional reward functions are difficult to define.\nLimitations. While ICPL demonstrates significant potential, it faces limitations in tasks where human evaluators struggle to assess performance from video alone, such as Anymal's \"follow random commands.\" In such cases, subjective human preferences may not provide adequate guidance. Future work will explore integrating human preferences with artificially designed metrics to enhance the ease with which humans can assess the videos, ensuring more reliable performance in complex tasks. Additionally, we observe that the performance of the task is qualitatively dependent on the diversity of the initial reward functions that seed the search. While we do not study methods to achieve this here, relying on the LLM to provide this initial diversity is a current limitation."}, {"title": "A APPENDIX", "content": "We would suggest visiting https://sites.google.com/view/few-shot-icpl/home for more information and videos."}, {"title": "A.1 FULL PROMPTS", "content": "The prompts used in ICPL for synthesizing reward functions are presented in Prompts 1, 2, and 3. The prompt for generating the differences between various reward functions is shown in Prompt 4."}, {"title": "A.2 ICPL DETAILS", "content": "The full pseudocode of ICPL is listed in Algo. 2."}, {"title": "A.3 BASELINE DETAILS", "content": "The baseline PrefPPO adopted in our experiments comprises two primary components: agent learning and reward learning, as outlined in (Lee et al., 2021b). Throughout this process, the method maintains a policy denoted as $\\pi_{\\varphi}$ and a reward model represented by $r_{\\psi}$.\nAgent Learning. In the agent learning phase, the agent interacts with the environment and collects experiences. The policy is subsequently trained using reinforcement learning, to maximize the cumulative rewards provided by the reward model $r_{\\psi}$. We utilize the on-policy reinforcement learning algorithm PPO (Schulman et al., 2017) as the backbone algorithm for training the policy. Additionally, we apply unsupervised pre-training to match the performance of the original benchmark. Specifically, during earlier iterations, when the reward model has not collected sufficient trajectories and exhibits limited progress, we utilize the state entropy of the observations, defined as $H(s) = -E_{s \\sim p(s)}[\\log p(s)]$, as the goal for agent training. During this process, trajectories of varying lengths are collected. Formally, a trajectory $\\sigma$ is defined as a sequence of observations and actions $(s_1, a_1), ..., (s_t, a_t)$ that represents the complete interaction of the agent with the environment, concluding at timestep t.\nReward Learning. A preference predictor is developed using the current reward model to align with human preferences, formulated as follows:\n$P[\\sigma^1 > \\sigma^0] = \\frac{\\exp{\\left(\\sum_{i}r_{\\psi}(s_i, a_i)\\right)}}{\\sum_{\\tau \\in {0, 1}} \\exp{\\left(\\sum_i r_{\\psi}(s_i^{\\tau}, a_i^{\\tau})\\right)}}$,\nwhere $\\sigma^0 = (s_1^0, a_1^0), ..., (s_{l_0}^0, a_{l_0}^0)$ and $\\sigma^1 = (s_1^1, a_1^1), ..., (s_{l_1}^1, a_{l_1}^1)$ represent two complete trajectories with different trajectory length $l_0$ and $l_1$. $P_{\\psi}[\\sigma^1 > \\sigma^0]$ denotes the probability that trajectory $\\sigma^1$ is preferred over $\\sigma^0$ as indicated by the preference predictor. In the original PrefPPO framework, test task trajectories are of fixed length, allowing for the extraction of fixed-length segments to train the reward model. However, the tasks in this paper have varying trajectory lengths, so we use full"}, {"title": "A.4 ENVIRONMENT DETAILS", "content": "In Table 4, we present the observation and action dimensions, along with the task description and task metrics for 9 tasks in IsaacGym."}, {"title": "A.5 PROXY HUMAN PREFERENCE", "content": "Due to the high variance in LLMs performance, we report the standard deviation across 5 experiments as a supplement, which is presented in Table 5 and Table 6. We also report the final task score of PrefPPO using sparse rewards as the preference metric for the simulated teacher in Table 7."}, {"title": "A.5.1 ADDITIONAL RESULTS", "content": "Due to the high variance in LLMs performance, we report the standard deviation across 5 experiments as a supplement, which is presented in Table 5 and Table 6. We also report the final task score of PrefPPO using sparse rewards as the preference metric for the simulated teacher in Table 7."}, {"title": "A.5.2 IMPROVEMENT ANALYSIS", "content": "We use a trial of the Humanoid task to illustrate how ICPL progressively generated improved reward functions over successive iterations. The task description is \u201cto make the humanoid run as fast as possible\". Throughout five iterations, adjustments were made to the penalty terms and reward weightings. In the first iteration, the total reward was calculated as 0.5 \u00d7 speed_reward + 0.25 \u00d7 deviation_reward+0.25\u00d7 action_reward, yielding an RTS of 5.803. The speed reward and deviation reward motivate the humanoid to run fast, while the action reward promotes smoother motion. In the second iteration, the weight of the speed reward was increased to 0.6, while the weights for deviation and action rewards were adjusted to 0.2 each, improving the RTS to 6.113. In the third iteration, the action penalty was raised and the reward weights were further modified to 0.7 \u00d7 speed_reward, 0.15 \u00d7 deviation_reward, and 0.15 \u00d7 action_reward, resulting in an RTS of 7.915. During the fourth iteration, the deviation penalty was reduced to 0.35 and the action penalty was lowered, with the reward weights set to 0.8, 0.1, and 0.1 for speed, deviation, and action rewards, respectively. This change led to an RTS of 8.125. Finally, in the fifth iteration, an additional upright reward term was incorporated, with the total reward calculated as 0.7 \u00d7 speed_reward + 0.1 \u00d7 deviation_reward + 0.1 \u00d7 action_reward + 0.1 \u00d7 upright_reward. This adjustment produced the highest RTS of 8.232.\nBelow are the specific reward functions produced at each iteration during one experiment."}, {"title": "A.6 HUMAN-IN-THE-LOOP PREFERENCE", "content": "We evaluate human-in-the-loop preference experiments on tasks in IsaacGym, including Quadcopter, Humanoid, Ant, ShadowHand, and AllegroHand. In these experiments, volunteers only provided feedback by comparing videos showcasing the final policies derived from each reward function."}, {"title": "A.6.1 ISAACGYM TASKS", "content": "Due to the simplicity of the Cartpole, BallBalance, Franka tasks, where LLMs were able to zero-shot generate correct reward functions without any feedback, these tasks were excluded from the human trials. The Anymal task, which involved commanding a robotic dog to follow random commands, was also excluded as it was difficult for humans to evaluate whether the commands were followed based solely on the videos. For the"}]}