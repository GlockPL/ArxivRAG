{"title": "CENTRALLY COORDINATED MULTI-AGENT REINFORCEMENT LEARNING\nFOR POWER GRID TOPOLOGY CONTROL", "authors": ["Barbera de Mol", "Jan Viebahn", "Davide Barbieri", "Davide Grossi"], "abstract": "Power grid operation is becoming more complex due to the increase in generation of renewable energy. The recent\nseries of Learning To Run a Power Network (L2RPN) competitions have encouraged the use of artificial agents to assist\nhuman dispatchers in operating power grids. However, the combinatorial nature of the action space poses a challenge to\nboth conventional optimizers and learned controllers. Action space factorization, which breaks down decision-making\ninto smaller sub-tasks, is one approach to tackle the curse of dimensionality. In this study, we propose a centrally\ncoordinated multi-agent (CCMA) architecture for action space factorization. In this approach, regional agents propose\nactions and subsequently a coordinating agent selects the final action. We investigate several implementations of the\nCCMA architecture, and benchmark in different experimental settings against various L2RPN baseline approaches. The\nCCMA architecture exhibits higher sample efficiency and superior final performance than the baseline approaches. The\nresults suggest high potential of the CCMA approach for further application in higher-dimensional L2RPN as well as\nreal-world power grid settings.", "sections": [{"title": "1 Introduction", "content": "The growing demand for electricity, driven by technological ad-\nvancements and the shift towards electrified industries and trans-\nportation, highlights the need for resilient power grids. Managing\nthese grids involves complex sequential decision-making across\nlarge state and action spaces, further complicated by aging infras-\ntructure and increased reliance on unpredictable renewable energy\nsources [1, 2].\nTraditional grid management methods, such as redispatching and\nbuilding new infrastructure, are costly and dependent on external\nfactors. However, topological remedial actions, like reconfigur-\ning grid substations, offer a cost-effective alternative that remains\nunderexplored [3, 4, 5]. As power grids expand, conventional com-\nputational methods struggle to provide optimal real-time control\nsolutions, leading operators to rely on experience or predefined\nmanuals. The combinatorial complexity of grid configurations ne-\ncessitates more advanced approaches.\nTo support research in power network control (PNC), the Grid2Op\nframework was developed [6], enabling simulation of realistic grid\nscenarios as a Markov Decision Process (MDP) [7]. This framing\nmakes the problem suitable for deep Reinforcement Learning (RL),\nwhich has been successful in various domains [8, 9, 10, 11].\nHowever, deep RL faces scaling issues with large networks due to\nthe combinatorial explosion of state and action spaces. The curse of\ndimensionality remains a major bottleneck in RL tasks. Specifically,\nthe sample complexity grows exponentially with the dimensionality\nof the state-action space of the environment, posing challenges for\nlarge-scale applications. To address these challenges, Hierarchical\nReinforcement Learning (HRL) and Multi-Agent Reinforcement\nLearning (MARL) offer promising solutions by decomposing com-"}, {"title": "2 Background and Related Work", "content": "This section deals with the modeling of the PNC problem (Section\n2.1), applications of RL in this domain (Section 2.2), and recent\nHRL and MARL implementations that focus on improving sample\nefficiency and overcoming the curse of dimensionality (Section 2.3)."}, {"title": "2.1 Power Network Control", "content": "Secure operation of power networks is required both in normal\noperating states as well as in contingency states (i.e., after the\nloss of any single element on the network). That is, the following\nrequirements must be met: (i) In the normal operating state, the\npower flows on equipment, voltage and frequency are within pre-\ndefined limits in real-time; (ii) In the contingency state the power\nflows on equipment, voltage and frequency are within pre-defined\nlimits. Loss of elements can be anticipated (scheduled outages of\nequipment) or unanticipated (faults for lightning, wind, spontaneous\nequipment failure). Cascading failures must be avoided at all times\nto prevent blackouts (corresponding to the game over state in the\nRL challenge).\nImportantly, each of the control actions performed by power system\noperators usually not only affects the current state of the power\nsystem but also the future state and availability of future control\nactions, that is, short-term actions can have long-term consequences.\nAs a result, the decision problem of power system operators is\ntypically a sequential decision-making problem in a combinatorial\naction space in which the current decision can affect all future\ndecisions. Moreover, due to possible nondeterministic changes\nof the power system state (e.g., due to unplanned outages or the\nintermittent behaviour of renewable energy sources) and different\nsources of error (e.g., measurement errors, state estimation errors,\nflawed judgement) the operators need to handle uncertainty in their\ndecisions. Finally, operational decisions must often be made quickly,\nunder hard time constraints [4]."}, {"title": "2.2 Deep RL for Power Network Control", "content": "Deep RL has demonstrated significant potential in PNC, enabling\nrobust and adaptable behavior over extended time horizons [4].\nThis capability surpasses the limitations of expert systems [16] and\noptimization methods, which are constrained by computation time\n[17, 18, 19].\nIn L2RPN competitions, successful solutions typically combine\nexpert rules, RL agents, and brute force simulations for action vali-\ndation. Notably, RL agents based on Proximal Policy Optimization\n(PPO) [20], with reduced action spaces, have performed well. This\napproach was used in top-ranking competition entries (e.g. [21],\nfurther explored in studies [22, 23, 24]).\nSeveral commonalities emerge among top-ranking approaches.\nMost successful methods implement intervention only during haz-\nardous situations [25, 26, 23, 24, 27, 22]. In addition, many such\napproaches integrate learned modules with simulation-based action\nevaluation. For instance, this strategy was key to the success of\nthe winning teams in both 2019 [26] and 2021 [28]. Notably, the\nsecond-place competitor in 2021 employed an advanced expert sys-\ntem, underscoring the value of incorporating domain knowledge\nfrom power systems [29]."}, {"title": "2.3 Curse of Dimensionality", "content": "The combinatorial complexity of topological actions in PNC poses\na challenge for deep RL by hindering complete and consistent con-\nvergence and exploration of action and state spaces. HRL simplifies\nlearning by breaking tasks into subtasks, enabling more focused and\nsample efficient learning per task as well as increased interpretabil-\nity. MARL distributes decision-making across agents, effectively\nfactorizing the MDP's action space."}, {"title": "2.3.1 Hierarchical Reinforcement Learning", "content": "In the L2RPN WCCI 2020 competition, the winning agent for a\n36-bus network used a two-tiered framework: a high-level policy\nproposed a goal topology, while a low-level policy determined the\nsequence of individual topological actions [25].\nLater, a three-level HRL framework was introduced [23], where the\ntop level activates the agent in hazardous situations, the intermediate\nlevel selects a substation using RL, and the lowest level identifies\na configuration. They tested greedy and RL-based approaches for\nthe lowest level, and compared PPO and Soft Actor Critic (SAC)\n[30] for the RL-based modules. They found PPO to have faster\nconvergence, smaller variance, and higher expected rewards."}, {"title": "2.3.2 Multi-Agent Reinforcement Learning", "content": "The action space in PNC can be factorized into exclusive subsets,\none for each substation, enabling a natural MARL framework in\na fully cooperative setting where all agents optimize the same col-\nlective objective. Following this approach, van der Sar et al. [31]\nextended Manczak et al. [23] by introducing multiple agents at the\nlowest level, with each substation controlled by an agent and a\nheuristic-based intermediate level. They trained agents using in-\ndependent PPO (IPPO) and dependent PPO (DPPO). On a 5-bus\nnetwork, both achieved optimal scores, though single-agent PPO\nconverged faster. However, they anticipate that MARL's advantages\nwill become more pronounced when scaled up to larger networks.\nIn other domains, Yu et al. [32] showed strong PPO-based multi-\nagent performance in testbeds, recommending best practices like\nvalue normalization, integrating global and local information, and\nlimiting sample re-use to avoid training instability from MARL\nnon-stationarity."}, {"title": "3 Control framework", "content": "Figure 1 shows a schematic of the feedback control (FC) framework\nemployed in this study. The goal represents a target state that is\nto be maintained within the controlled system (light orange box).\nThe comparator (left orange box) computes the error between the\ncurrent and the goal state of the system. The controller produces\nan action to be performed in the system in order to get closer to or\nmaintain the goal state of the system. The system produces a reward\nsignal and a new state once the action is performed.\nAs shown in Fig. 1, our controller has two modes of operation.\nAfter the error is computed, a gate (right orange box) determines\nwhich mode to use. If the goal state is achieved (error = 0) then\nthe gate selects the lower mode of operation in which a constant\naction (\"do nothing\") is performed in the system. If the goal state is\nnot achieved then an agent (lower blue box) is used to compute an\naction. The state of the system (and possibly the error) is used to\ncreate the input observation for the agent.\nThe agent can either be rule-based or trained. In case of a trained\nagent, the agent mode of operation stores past experiences of its\ninteraction with the system for training the agent. The controller\ncan switch between modes of operation multiple times. Switching\nfrom the agent to the do-nothing operation may be the result of\nthe agent's action. The system may also remain in the do-nothing"}, {"title": "4 Power System Environment", "content": "First, in Section 4.1, we describe the power grids used for exper-\nimental analysis, which is the system in Fig. 1. Subsequently, in\nSection 4.2, we describe the corresponding gate, state, reward, and\nactions."}, {"title": "4.1 Experimental Setup", "content": "We consider two simulated power grids: the IEEE case 5 and IEEE\ncase 14 networks. In the Grid2Op simulator [6], the power grid envi-\nronments are called rte_case5_example and 12rpn_case14_sandbox\n(see the Grid2Op documentation for visualizations of these power\ngrids). The 5-bus network comprises twenty scenarios, each span-\nning 2016 timesteps. The 14-bus network contains 1004 scenarios,\neach unfolding over 8064 timesteps. Each timestep, representing\na five-minute interval, incorporates unknown changes in loads and\nproduction.\nMoreover, unplanned outages can be included. These outages in-\nvolve temporarily disconnecting a powerline for a specified dura-\ntion. These unplanned outages are initiated by an opponent, adding\nstochasticity and further increasing task complexity. This opponent\nintroduces realistic challenges faced by power grid operators, en-\nsuring the system's stability is preserved even during unforeseen\ncontingencies. It is designed to randomly disconnect a line from a\npredefined set of targets, with a cooldown period implemented to\ndelay consecutive attacks. After each attack, the disconnected line\nis automatically restored."}, {"title": "4.1.1 Incorporated Domain Knowledge", "content": "Several domain-inspired features are incorporated in all baselines\nand architectures to boost performance. First, automatic powerline\nreconnection is implemented. Powerlines disconnected by over-\ncurrent events face a ten-timestep cooldown before reconnection.\nSince agents are limited to topological actions, disconnected lines\nare automatically restored to their previous state one timestep after"}, {"title": "4.2 Power System Feedback Control Elements", "content": "This subsection explains how each component of the FC framework\npresented in Sec. 3 is realized in a power system setting.\nObviously, the general goal is to maintain a grid state with-\nout any overloaded lines. In this study, we adopt the approach\nemployed in most L2RPN studies. Specifically, the grid state is\ncharacterized by the loading of individual power lines, represented\nas a vector $p = (p_1, p_2, ..., p_n)$ where each element $p_l$ denotes the\nloading of a specific line in the power grid. The goal state is defined\nby a threshold $\\rho$, a single scalar value representing the allowable\nmaximum line loading. The goal is to maintain $max(p) < \\rho$, where\n$max(p)$ refers to the maximum loading among all the lines in the\ngrid. In this study, we set $\\rho = 0.95$.\nGate We use a common rule-based gate. If the maximum line\nloading is lower than the threshold $\\rho$ then the do-nothing mode\nof operation is used. If the maximum line loading surpasses the\nthreshold $\\rho$ then the agent mode of operation is used.\nState The state of the system consists of (i) the current grid topol-\nogy, (ii) the current load $\\rho_l$ on each powerline $l$, measured as the\nfraction of the capacity of the powerline, (iii) the active power\nflow at each end of each powerline, (iv) the number of overflow\ntimesteps for each powerline, and (v) the current power production\nand consumption of generators and loads.\nIn Grid2Op, if a blackout occurs, it results in a game-over state. A\nblackout happens when the power grid fails to meet the requirements\nof the generators and loads due to issues like line disconnections,\noverloading, or other critical failures. Once a blackout is detected,\nthe simulation ends immediately, as the grid's stability can no longer\nbe maintained.\nReward We employ the scaled L2RPN reward [6, 23], which\ncalculates the squared margin of the current flow relative to the\ncurrent limit for each powerline. The larger the margin, meaning\nthe further the current flow is from the limit, the higher the reward,\nencouraging states where powerlines are not operating near their\nmaximum capacity.\nActions In this study, we introduce a new action space designed\nto ensure structural N-1 security. Structural N-1 security means that\nthe grid remains operational and avoids an automatic blackout in\nthe event of a single contingency, such as an unplanned powerline\noutage. We compare the new action space with the default action\nspace which only excludes symmetrical counterparts from all possi-"}, {"title": "5 Power system agents", "content": "In this section, we present the agent configurations used in the\nsecond mode of operation of the FC framework (see Sec. 3, Fig. 1).\nThe greedy agent and the single RL agent (Section 5.1) approach the\nSMDP without decomposition. In contrast, this study introduces a\nmulti-agent architecture that divides the sMDP into a smaller set of\nindependent tasks (see Sec. 5.2). The two common baselines serve\nas a reference to evaluate the impact of decomposing the MDP."}, {"title": "5.1 Greedy Agent & Single RL Agent", "content": "The greedy-agent baseline is a fully rule-based agent. A greedy\nagent simulates the effect of every action in the action space for\none timestep in the future. Then, the agent selects the action that\nmaximizes a given KPI when simulated. In our case, the KPI is\nthe maximum loading of the grid. See de Jong et al. [35] for a\npseudo-code of the greedy agent.\nThe single RL agent uses a feed-forward neural network to imple-\nment a policy for choosing actions. It also chooses actions from\nthe entire action space. A state-of-the-art RL algorithm is used to\ntrain the policy, namely, the Proximal Policy Optimization (PPO)\nalgorithm [20]."}, {"title": "5.2 Multi-Agent Architecture", "content": "We divide the overall agent task into two levels of abstraction,\nfollowing the approaches outlined in [23, 31]. One level consists\nof deciding the topological reconfiguration of individual regions\n(e.g. substations). The other level consists of selecting which region\nshould be reconfigured. The agent selecting the region is called\nthe coordinator. The reconfiguration of regions may be decided by\nmultiple agents, called regional agents, each proposing a topological\nreconfiguration of its respective region. In our architecture the\ntopological reconfiguration of each individual region is decided\nfirst, and the region to be reconfigured is selected second. Figure 2\nillustrates on the left the high-level hierarchy used in Manczak et al.\n[23], van der Sar et al. [31] where the regional agents are called\nafter the coordinator, and on the right the decision making order\nproposed in this study.\nOur architecture allows the coordinator to take into account the\nproposed regional reconfiguration in its decision making process.\nWe believe that this allows for a more clean separation between the\ntwo levels of abstraction. In Manczak et al. [23], van der Sar et al.\n[31], the coordinator may need to anticipate implicitly what recon-\nfiguration will be proposed for the region it is selecting. This may\nresult in the coordinator having to solve both tasks as a single agent.\nIn other words, we believe that the regional topology selection task\nneeds to be solved (either implicitly or explicitly) before being able\nto determine which region to select."}, {"title": "5.3 Overview of power system agents", "content": "We summarize all considered approaches using the following nam-\ning convention: The full multi-agent system is indicated by [name\nregional agent]-[name coordinating agent], with the names given as\nin Table 3 and Fig. 3."}, {"title": "5.3.1 Baselines.", "content": "To evaluate the proposed approach, results are compared with sev-\neral baselines that should at least be met in terms of performance:\n\u2022 Do-Nothing Agent: This baseline shows the number of\ntimesteps survived when no action is taken at any timestep.\n\u2022 Single RL Agent: This baseline uses a single PPO agent\nresponsible for configuring the entire network. The agent\nreceives network observations as input and outputs a single\naction.\n\u2022 RL-CAPA: This architecture combines a rule-based CAPA\ncoordinator with RL-based regional agents. The CAPA\npolicy prioritizes substations at risk of immediate overload-\ning, using the highest relative overload value for selection.\nThis approach ensures that regional agents are trained in\nscenarios where their actions are likely to have a significant\nimpact. This adapted CAPA policy based on van der Sar\net al. [31] also considers past actions to avoid experience\nimbalance and repeated selection of the same substation. If\na proposed action does not alter the grid topology, the next\nhighest-priority substation proposes an action, as detailed\nin Alg. 1."}, {"title": "5.3.2 Benchmarks.", "content": "Two benchmark agents are designed using action simulation and\ngreedy strategies. Both architectures are computationally intensive,\nas they simulate all possible actions and select the optimal one in\na greedy manner. However, they serve to illustrate the complexity\nwithin networks and scenarios, demonstrating the potential scope of\nlearning.\n\u2022 Greedy Agent: This baseline involves a greedy agent\nthat makes decisions based on minimizing the maximum\nrelative line loading of the network. It operates without hi-\nerarchical coordination and provides a benchmark of what\na simple architecture with enough computational power\ncan achieve."}, {"title": "5.3.3 Multi-agent RL architectures.", "content": "For the proposed CCMA architecture several implementations are\nexplored:\n\u2022 Greedy-RL: This setup features a hierarchical structure\nwith an RL-based coordinator that receives proposals from\nregional agents to address congestion. Unlike a greedy\nagent, the RL coordinator benefits from implicit planning,\nsimilar to the PPO Substation implementation by Manczak\net al. [23]. The key distinction is that Manczak et al. [23]'s\nimplementation does not involve multiple regional agents\nor consider the greedy agent's planned actions.\n\u2022 RL-RL: This architecture features both a learned coordi-\nnator and learned regional agents. The coordinator utilizes\nboth global state observations and regional agent informa-\ntion, inspired by the approach suggested by Yu et al. [32].\nThis setup allows the system to learn multi-step actions\nbut may introduce instability due to the interactions among\nmultiple learned levels and agents.\n\u2022 RL-Action Value RL: This approach extends the previous\nsetup by incorporating the action-value function outputs\nfrom regional agents as additional input for the learned co-\nordinator. This allows for a more refined decision-making\nprocess by integrating the value estimates of different ac-\ntions, potentially enhancing the coordination and overall\nperformance of the system.\n\u2022 RL-Value Softmax: This architecture uses the action-\nvalue function outputs from regional agents. A coordi-\nnator employing softmax (see Equation 5 below) assigns\nprobabilities to actions during training, promoting explo-"}, {"title": "5.4 Implementation details", "content": "To improve learning, avoid overfitting, and ensure a fair compari-\nson between architectures, several measures are implemented for\neach experiment, including train-validation-test splitting, scenario\nsplitting, and identical hyperparameter tuning."}, {"title": "5.4.1 Trainer", "content": "For all experiments, PPO [20] is used for learning a policy. The\npolicy, denoted by $\\pi$, is parameterized using a neural network with\nlearnable parameters $\\theta$. The action-value function, represented as"}, {"title": "7 Discussion and Future Work", "content": "This study introduces a new approach that combines HRL with\nMARL to tackle the growing complexity of PNC in increasingly\nunpredictable environments. Our three-level architecture effectively\nbreaks down the decision-making process, making control strategies\nmore modular. One notable aspect is the introduction of a coordi-\nnating agent that considers both global state information and action\nproposals from regional agents."}, {"title": "7.1 Discussion", "content": "From the experiments it is evident that there can be benefits to\nfactoring the action space. The Greedy-RL, RL-RL, and RL-Action\nValue RL agents consistently outperform the single RL agent, both\nasymptotically and in sample efficiency. With the larger grid, and\nespecially with contingencies, this pattern becomes more evident.\nThere seems to be no significant difference in performance between\nthe RL-RL and RL-Action Value RL agents. This suggests that the\nvalue of the regional agents' critics does not hold useful information\nfor solving the coordination task.\nOn the other end, the Greedy-RL agent significantly outperforms\nRL-RL agent, both asymptotically and in sample efficiency. The\nimproved sample efficiency could be due to the warm-start that\nthe Greedy-RL agent can benefit from, as regional agents are al-\nready fixed. In contrast, agents like RL-RL may suffer from non-\nstationarity since both levels need to perform reasonably well before\nthey can learn appropriately. Concretely, it can lead to poor perfor-\nmance when a regional agent selects a good action but a coordinator\ndoes not select the appropriate agent, or vice versa.\nOn the other hand, the Greedy-RL agent exhibits bi-modal behavior\nin the stochastic environment (Fig. 5 in App. B). One possible\nexplanation is that not all contingency cases have equal value. It\ncould be that only a smaller subset of the contingency cases pushes\nthe agent to pick safer actions. This may have happened only on\ncertain seeds. One possible explanation as to why we observe this\nonly with Greedy-RL is its higher sample-efficiency. This would\nmake the agent faster at picking up such patterns when (and if) they\nappear.\nThese results suggest that it is crucial to have long term reason-\ning in the coordination task. With an RL coordinator the Greedy-\nRL agent manages to outperform the Greedy baseline, especially\nin the challenging stochastic setting, despite the regional Greedy\nagents optimizing topological configurations for a single timestamp.\nThe importance of long term reasoning in coordination is further\nsupported by the low asymptotic performance of RL-CAPA and\nRL-Value Softmax. For the simplest setting, a 5-bus network with-\nout opponent, the coordinators such as CAPA-heuristic, random\nsubstation selection, Value Softmax still perform reasonably well.\nHowever, when used with added complexity due to an adversary or a\nlarger network, these architectures no longer match the performance\nof a learned coordinator.\nMoreover, when the network grows in size, the number of possible\nsubstations to select also grows. This makes the coordinator's task\nincreasingly complex, calling the need for a more sophisticated\ncoordinator.\nFinally, because the Greedy-RL is among the best performing agents,\nit suggests that the factored action spaces can be tackled indepen-\ndently. In practice, the Greedy-RL agent is trained as a single RL\nagent within its coordination task. Thus, the regional agents can\nbe trained independently of the coordinator. The coordinator can"}, {"title": "7.2 Future Work", "content": "Future research could build on this study by exploring several av-\nenues for extending and refining the proposed methods. Testing the\nscalability and robustness of the approach in larger networks, such\nas a 118-bus network, could provide insights into its effectiveness\nin more complex environments. Moreover, future studies could test\nthe impact of different strategies for decomposing the actions space\n[38].\nAnother promising direction is to investigate cooperative mecha-\nnisms among agents. While this study used a fully independent\nsetup with PPO, sharing parameters or using a shared critic could\npotentially stabilize learning and enhance efficiency. Testing the\nimpact of partial observability on regional agents could also provide\nmore realistic insights into local versus global information.\nThe results also suggest more concrete next steps for making the\nCCMA more scalable. The Greedy-RL architecture shows the\nmost promise in sample efficiency. However, it is also the most\ncomputationally expensive agent. To mediate this, one could use\nimitation learning to train regional agents. These agents would try\nto imitate the greedy agent directly just much faster.\nFurthermore, the regional agents can be trained independently of the\ncoordinator using RL, similar to a single-agent RL setting where the\naction space is confined to a single regional agent. This approach\nallows regional agents to incorporate some level of longer-term\nplanning, albeit with a shorter time horizon compared to the coordi-\nnator that could be limited to a few timesteps. The aforementioned\nimitation learning can be used as warm start in this setting.\nHowever, it should be noted that our approach does not factorize\nthe observation space. With the current observation space, when\ntraining the regional agents separately, the data should include a lot\nof varied configurations of the observed section of the grid. This\napplies also when using only imitation learning. Each regional\nagent observes the entire grid, and thus all other regions. There-\nfore the amount of data needed to train an individual agents scales\nexponentially with the number of regions.\nOne could try to give partial observations to the regional agents.\nEach agent only taking as observation a neighborhood of its region\n(included). Thus, the data to train a regional agents should only\ninclude combinations within its neighborhood. This effectively\ncreates a cap on the amount of data needed to train an agent, with\nrespect to the number of regions. In such a setting, the amount of\ndata scales with respect to the size of the neighborhood and not the\ntotal number of regions. Expanding the grid outside of a regional\nagent's neighborhood will not affect the amount of data needed to\ntrain it. The connections going out of the neighboring regions can\nstill be kept and simulated. The data can include various level of\nloading on these connections. Effectively the part outside of the\nneighborhood can be seen as grid injections. It should be noted that\nthese observation spaces would not be completely independent as\nthere would be overlap between them."}]}