{"title": "Multi-Agent Verification: Scaling Test-Time Compute with Multiple Verifiers", "authors": ["Shalev Lifshitz", "Sheila A. McIlraith", "Yilun Du"], "abstract": "By utilizing more computational resources at test-time, large language models (LLMs) can improve without additional training. One common strategy uses verifiers to evaluate candidate outputs. In this work, we propose a novel scaling dimension for test-time compute: scaling the number of verifiers. We introduce Multi-Agent Verification (MAV) as a test-time compute paradigm that combines multiple verifiers to improve performance. We propose using Aspect Verifiers (AVs), off-the-shelf LLMs prompted to verify different aspects of outputs, as one possible choice for the verifiers in a MAV system. AVs are a convenient building block for MAV since they can be easily combined without additional training. Moreover, we introduce BON-MAV, a simple multi-agent verification algorithm that combines best-of-n sampling with multiple verifiers. BoN-MAV demonstrates stronger scaling patterns than self-consistency and reward model verification, and we demonstrate both weak-to-strong generalization, where combining weak verifiers improves even stronger LLMs, and self-improvement, where the same base model is used to both generate and verify outputs. Our results establish scaling the number of verifiers as a promising new dimension for improving language model performance at test-time.", "sections": [{"title": "1 Introduction", "content": "Scaling the size of large language models (LLMs) and their training datasets has driven remarkable progress in artificial intelligence. However, the growing cost of scaling model size and obtaining unseen high-quality pretraining data has sparked growing interest in methods that improve LLM performance without simply scaling parameters or data. Among these, a promising new direction has emerged: scaling test-time compute, where models spend more computational resources during inference-much like humans spend more time thinking through harder problems.\n\nA common strategy for scaling test-time compute is best-of-n sampling, where n candidate outputs are sampled from a generator LLM and a verifier model scores each candidate output based on its quality or correctness. The highest-scoring output is then selected. Under this strategy, the amount of test-time compute can be scaled up by increasing the number of sampled outputs. However, in this work, we propose a new orthogonal scaling dimension: scaling the number of verifiers. We introduce Multi-Agent Verification (MAV), a test-time compute paradigm that combines multiple verifiers to improve performance.\n\nTypically, verifiers are implemented as reward models which are trained using reinforcement learning from human feedback. However, relying on reward models as verifiers introduces two crucial limitations for multi-agent verification: (1) each reward model has to be trained on expensive curated preference data, and (2) there is no straightforward way to combine scores generated by heterogeneous reward models trained on different datasets (they produce uncalibrated scores). These limitations make reward models poorly suited for multi-agent verification and restrict our ability to simply scale up the number and type of verifiers at test-time.\n\nTo address these limitations and enable scalable multi-agent verification, we propose using Aspect Verifiers (AVs) off-the-shelf LLMs prompted to verify specific aspects of candidate outputs through binary True/False approvals. This approach is motivated by the observation that internet data contains abundant examples of humans providing binary evaluations with feedback (e.g., educational assessments, academic peer reviews, online forums, and automated code tests), which suggests that language models may be naturally suited for binary verification. Unlike reward models, AVs do not require additional training since producing binary approvals falls naturally within the training distribution of their base LLMs, and their binary outputs can be easily combined across multiple models through simple voting mechanisms. Thus, the number and type of aspect verifiers can be easily scaled up without additional training. We note that aspect verifiers are just one possible implementation choice for the verifiers in a MAV system, which address the two key limitations of typical reward model verifiers.\n\nBy aggregating binary signals across a diverse set of aspect verifiers, we can leverage the growing ecosystem of language models to produce a more robust verification signal. Each verifier can focus on different aspects of outputs like mathematical correctness or logical soundness, and employ different verification strategies such as direct yes/no approval, step-by-step analysis, solution rephrasing, or edge case checking. Thus, a diverse set of aspect verifiers can be obtained by varying three key axes: the base LLM, the aspect to verify, and the verification strategy.\n\nTo investigate scaling multi-agent verification, we introduce BON-MAV as a specific algorithm which combines best-of-n sampling with aspect verifiers. This is one implementation of a MAV algorithm, combining traditional best-of-n sampling with multiple verifiers. Given an input, BON-MAV (1) samples n outputs from a generator LLM, (2) collects binary approvals from a set of m aspect verifiers, and (3) selects the output with the most approvals. We investigate scaling test-time compute with this approach along two orthogonal dimensions: the traditional dimension of increasing the number of sampled candidate outputs n, and our novel test-time scaling dimension of increasing the number of verifiers m. We find that using multiple diverse verifiers to select between candidate outputs is an effective strategy, and that performance improves as we use more verifiers."}, {"title": "2 Multi-Agent Verification", "content": "Multi-Agent Verification (MAV) is a test-time compute paradigm where multiple verifiers are combined to evaluate outputs from a generator LLM. To implement a MAV algorithm, we must address two questions: (1) What type of verifiers can be easily combined and scaled up in number without additional training? (2) How should we aggregate verification signals from multiple verifiers? In this section, we propose answers to these questions and describe one simple implementation of a multi-agent verification algorithm called BON-MAV. We discuss future directions for alternative multi-agent verification algorithms in Section 4.\n\nIn Section 2.1, we propose Aspect Verifiers (AVs) as a convenient building block for MAV, since they require no additional training and naturally support combining multiple verification signals. In Section 2.2, we describe our approach to aggregating signals across multiple AVs. In Section 2.3, we outline the BoN-MAV algorithm, which combines best-of-n sampling with aspect verifiers. Finally, Section 2.4 proposes verifier engineering as a method to select relevant verifiers for specific domains or tasks."}, {"title": "2.1 Aspect Verifiers", "content": "In the context of test-time computation with LLMs, a verifier typically refers to a model that evaluates the quality or correctness of an output sampled from a generator LLM. Here, we ask: What type of verifiers can be easily combined and scaled up in number without additional training?\n\nPrior works have largely focused on using neural reward models as verifiers. However, these models present key challenges for scaling multi-agent verification. First, each reward model requires training on expensive curated preference data to produce reliable reward scores. Second, while ensembles of homogeneous reward models (identical model initializations trained on the same data but with different random seeds) have been proposed as a way to mitigate overoptimization, there is no straightforward way to combine scores from heterogeneous reward models trained on different datasets. This second limitation arises because scores from different reward models are uncalibrated-they operate on different numerical scales based on their distinct training setups. These limitations make reward models poorly suited for multi-agent verification, where we wish to simply scale up the number and type of verifiers without additional training.\n\nWe propose Aspect Verifiers (AVs) as one possible implementation choice for the verifiers in a MAV system, which address the two key limitations of typical reward model verifiers. AVs are off-the-shelf LLMs prompted to evaluate specific aspects of candidate outputs and produce binary True/False approvals. Unlike reward models, they require no additional training since binary evaluation is a natural task for LLMs (the internet contains abundant examples of humans providing binary approvals with explanation, such as educational assessments, academic peer reviews, online forums, and automated code tests), and their binary approvals can be easily combined through simple voting mechanisms, even when AVs are based on completely different models or training data. Moreover, since AVs are based on LLMs, they can produce Chain-of-Thought reasoning to analyze outputs step-by-step before producing an approval, similar to recent work on generative reward models. Using aspect verifiers, we can easily scale up the number and type of verifiers which may be based on different LLMs, training algorithms, architectures, data, or prompts."}, {"title": "2.2 Combining Aspect Verifiers", "content": "With aspect verifiers as our building block, we ask: How can we effectively aggregate verification signals across multiple AVs? We take the simplest possible approach in our experiments: each binary True/False approval is a single vote, and the aggregated score for a candidate output is the sum of the positive votes from all AVs. That is, the aggregated verification score is the sum of the individual binary scores from each verifier:\n\nAggScore(o^(i)) = \\sum_{v \\in M} BinaryScore(o^(i), v) \n\nwhere o^(i) \u2208 O is the ith candidate output from the set of sampled outputs O, M is the set of aspect verifiers, and BinaryScore : O \u2192 {0,1} maps a candidate output from O to the binary approval produced by verifier v \u2208 M for that output. This voting strategy gives equal weight to all verifiers in the final aggregated score, and it proves remarkably effective in our experiments (see Section 3.1). However, future works could investigate more sophisticated aggregation strategies such as grouping verifiers by aspect and then voting across aspects, or having aspect verifiers debate with each other before producing an approval. We discuss these and other potential directions for future work in Section 4."}, {"title": "2.3 BoN-MAV", "content": "Best-of-n (BoN) sampling is a test-time optimization technique where n candidate outputs are sampled from a generator LLM, each candidate is scored by a verifier model, and the highest-scoring output is selected. We introduce BON-MAV as a simple multi-agent verification algorithm that combines best-of-n sampling with aspect verifiers. It uses the simple aggregation strategy from Equation 1 and consists of three steps: (1) sampling n candidate outputs from a generator LLM, (2) collecting binary approvals from a set of m aspect verifiers, and (3) selecting the output with the most approvals. That is,\n\ni = arg max (AggScore(o^(i)))\n\nwhere o^(i) is the ith candidate output, n is the total number of sampled candidate outputs, and i is the index of the output with the highest aggregated score (the selected output).\n\nUsing BoN-MAV, we can increase test-time computation by sampling more candidate outputs (increasing n) and by querying more verifiers (increasing m = |M|), where test-time computation can be easily parallelized during generation as well as verification. In addition, BoN-MAV represents just one specific approach to multi-agent verification, and more nuanced aggregation algorithms or alternatives to aspect verifiers could further enhance performance."}, {"title": "2.4 Verifier Engineering", "content": "Using aspect verifiers, we can create a diverse pool of verifiers with different capabilities. However, not all verifiers are equally relevant for every domain (e.g., math, coding, general knowledge). Thus, we propose verifier engineering as a process to select a subset of verifiers most effective for a particular domain (similar to prompt engineering, where prompts are engineered for specific domains or tasks).\n\nWe engineer domain-specific sets of verifiers by first creating a diverse initial set M and then selecting the subset Md \u2286 M which contains the most relevant verifiers for domain d. Specifically, for each domain d, we select the subset Md \u2286 M which maximizes the average performance across all generator LLMs evaluated on a validation set. Our current approach keeps the engineered set of verifiers fixed for all questions in a domain, but future works could explore dynamically customizing verifiers for particular questions, as we discuss in Section 4."}, {"title": "3 Experiments", "content": "In our experiments, we investigate scaling test-time compute along two orthogonal dimensions: the traditional dimension of increasing the number of sampled candidate outputs n, and our novel test-time scaling dimension of increasing the number of verifiers m. We aim to address the following questions: (1) How well does multi-agent verification improve performance across diverse domains and various generator LLMs? (2) Can multi-agent verification facilitate weak-to-strong generalization and self-improvement? (3) How important is engineering a domain-specific set of verifiers and what are the important design choices?\n\nTo address these questions, we evaluate the BoN-MAV algorithm described in Section 2 on the following four domains:\n\n\u2022 Mathematics. The MATH dataset consists of competition-level math questions at five difficulty levels. For our experiments, we randomly sample 400 questions from the test set across all five levels: 100 for validation and 300 for testing.\n\u2022 General Knowledge & Reasoning. MMLU-Pro is an enhanced version of the popular MMLU benchmark which features more challenging, reasoning-focused questions and expands the multiple-choice set from four to ten options. As with MATH, we sample 100 questions for validation and 300 for testing.\n\u2022 Graduate-Level Reasoning. The GPQA dataset consists of graduate-level, multiple-choice questions in biology, physics, and chemistry. For our experiments, we utilize GPQA's \"diamond\u201d subset - a collection of 198 high-quality and extremely challenging questions. We sample 98 questions for validation and 100 for testing.\n\u2022 Coding. HumanEval is a widely-used benchmark consisting of 164 Python programming questions. We sample 64 questions for validation and 100 for testing."}, {"title": "3.1 MAV Enables Scaling Along Two Dimensions", "content": "Here, we investigate how BoN-MAV scales with the number of candidate outputs and number of verifiers.\n\nBaselines. We compare Best-of-n sampling with Multi-Agent Verification (BON-MAV) against two established test-time compute methods: (1) best-of-n sampling with reward model verification , where we use a trained neural reward model as the external verifier to select the highest-scoring candidate output, and (2) self-consistency, which selects the most common answer from the set of candidates outputs. For reward model verification, we use the current top-performing open-source 8B reward model on RewardBench. See Appendix A.3 for more details.\n\nVerifier Engineering For our experiments, we implement the verifier engineering method described in Section 2.4. To create our initial diverse pool M of 20 aspect verifiers, we vary the three key axes that define aspect verifiers:\n\n1. Base model: Gemini-1.5-Flash or GPT-4o-mini\n2. Aspect to verify: Mathematical correctness, logical soundness, factuality, etc.\n3. Verification strategy: Direct approval, step-by-step verification, solution rephrasing, edge case checking, etc.\n\nFrom this pool, we then select domain-specific subsets Md \u2286 M that maximize average performance across all generator LLMs on the corresponding validation sets. The complete list of verifiers and the domain-specific subsets are detailed in Table 5. We choose Gemini-1.5-Flash and GPT-4o-mini as the base LLMs for our aspect verifiers since they are cost-effective for large-scale verification and enable us to demonstrate that combining multiple weaker verifiers can improve the performance of even stronger generator LLMs (Section 3.2).\n\nQuantitative Results. We evaluate BoN-MAV across four domains using eight generator LLMs (four closed-source and four open-source). For each model, we sample n = 16 candidate outputs per question and compare between best-of-n with Multi-Agent Verification (BoN-MAV), best-of-n with reward model verification (BoN-RM), and self-consistency (cons). As shown in Table 1, BoN-MAV outperforms self-consistency in nearly all cases, and outperforms reward model verification on MATH and MMLU-Pro, while achieving comparable results on GPQA (diamond) and HumanEval.\n\nQualitative Examples. Figure 2 illustrates how multiple aspect verifiers can be used to evaluate a single candidate output. The first aspect verifier uses direct yes/no approval without step-by-step thinking and incorrectly approves the output while additional aspect verifiers, using the same base model but with more thorough verification strategies, successfully identify the error. Additional examples are provided in Appendix C. Note that for the purposes of illustration, we visualize slightly different sets of verifiers than the final domain-specific sets used in our experiments."}, {"title": "Scaling the Number of Candidate Outputs.", "content": "In Figure 4, we show the scaling patterns for various generator LLMs as we increase the number of sampled candidate outputs (n). Matching the results in Table 1, BON-MAV demonstrates more effective scaling patterns than self-consistency across all domains, and stronger scaling than reward model verification on MATH and MMLU-Pro while achieving comparable scaling patterns on GPQA (diamond) and HumanEval."}, {"title": "Scaling the Number of Verifiers.", "content": "Multi-Agent Verification introduces a powerful new dimension for scaling test-time compute: scaling the number of verifiers. In Figure 5, we show how accuracy tends to improve as we increase the number of verifiers m from zero verifiers up to the full domain-specific subset Md. For each value of m \u2208 {0,1,2, ..., |Md|}, we plot the average accuracy across all possible combinations of m verifiers drawn from Md, with the shaded regions indicating the spread of observed values (dark blue shows the middle 50% range, and light blue captures 90% of outcomes). The leftmost point (m = 0) represents the base pass@1 accuracy without verification, while the rightmost point (m = |Md|) shows the accuracy when using all verifiers in the domain-specific subset, matching the values reported in Table 1. Note that Figure 1 shows just one randomly selected sequence of verifiers for illustration, rather than averaging across all possible combinations like in Figure 5.\n\nOur results demonstrate that scaling verifier count is a promising new dimension for improving model performance at test-time. In most cases, accuracy improves as we add verifiers, with performance gains of up to 10% for large LLMs and up to 20% for small ones. Notably, performance gains persist even when strong generator LLMs (Gemini-1.5-Pro, GPT-4o) are verified by combinations of our weaker verifiers (Gemini-1.5-Flash, GPT-4o-mini), supporting our findings about weak-to-strong generalization in Section 3.2. However, the magnitude and pattern of improvement varies and, in some cases, accuracy initially decreases before improving with additional verifiers. We expect better-engineered verifiers to unlock even stronger scaling patterns."}, {"title": "Scaling Up to 256 Candidate Outputs.", "content": "We extend our analysis to even larger scales by sampling 256 candidate outputs from Gemini-1.5-Flash on MATH. In Figure 6, we plot accuracy as a function of both the number of sampled candidate outputs n (left) and the total compute budget (right). The left plot demonstrates that BoN-MAV consistently improves with additional samples, while reward model verification and self-consistency plateau early on. Starting at 52.7% base accuracy, the baselines plateau around 61% while BoN-MAV continues to 69%-nearly double the improvement. The right plot shows computational efficiency by comparing accuracy against the total compute budget, measured as the combined number of queries to both the generator and verifier models. At low compute budgets, the overhead of querying multiple verifiers with BON-MAV means we can sample fewer candidate solutions, leading to initially worse performance than the baselines. However, once we have sufficient compute, BoN-MAV significantly outperforms both baseline methods. These results demonstrate that multi-agent verification can significantly improve model performance at test-time."}, {"title": "3.2 MAV Enables Weak-to-Strong Generalization and Self-Improvement", "content": "We now explore two important capabilities of multi-agent verification: improving strong models using only weaker verifiers (weak-to-strong generalization) and improving models using only self-verification (self-improvement)."}, {"title": "Weak-to-Strong Generalization.", "content": "Prior work has shown that weak supervisors can improve the performance of strong pretrained models. Here, we show that multi-agent verification can be used to enhance the performance of strong generator LLMs by combining weaker verifiers. As shown in Table 2, our strongest generators (Gemini-1.5-Pro and GPT-4o) show substantial improvements over their base pass@1 accuracy when using verifiers based on weaker models (Gemini-1.5-Flash and GPT-4o-mini), and Figure 5 shows how the performance of Gemini-1.5-Pro and GPT-4o changes as we scale the number of verifiers. These results suggest that the diverse perspectives of multiple smaller models can collectively produce a verification signal robust enough to improve even state-of-the-art generators. It also shows that effective verification can be achieved using computationally cheaper and faster models, rather than requiring large verifiers which match the performance of the generator-a promising result for deploying multi-agent verification at scale."}, {"title": "Self-Improvement.", "content": "Multi-agent verification can also enable models to improve their own performance through self-verification. To demonstrate, we configure BoN-MAV to use the same base LLM for both generation and verification. That is, we sample candidate outputs from a generator LLM (Gemini-1.5-Flash or GPT-4o-mini) and create multiple aspect verifiers derived from the same LLM. Following the verifier engineering procedure from Section 3.1, we select the best subset of self-verifiers based on validation performance. As shown in Table 2, this self-verification approach yields substantial improvements over base pass@1 accuracy across all domains except HumanEval. For instance, GPT-4o-mini shows particularly strong self-improvement on MATH (+7%) and GPQA diamond (+8%)."}, {"title": "3.3 Analysis: Understanding Multi-Agent Verification", "content": "To better understand the key design choices that impact multi-agent verification, we conduct two ablation studies on MMLU-Pro and GPQA (diamond)-the two most challenging domains in our evaluation. We investigate: (1) how performance depends on engineering domain-specific sets of verifiers, and (2) whether using diverse verifiers outperforms repeatedly querying the single best verifier."}, {"title": "Effect of Verifier Engineering.", "content": "In Section 3.1, we introduced verifier engineering as an approach for selecting a relevant subset of verifiers Md \u2286 M for each domain d. Here, we compare our engineered verifier subsets Md against a simple baseline that uses all available aspect verifiers in M (see Appendix A.1 for a full list) without any domain-specific tuning. Table 3 shows that engineering the set of verifiers is a more effective strategy. However, Table 6 in the Appendix shows that even the simple strategy of combining all verifiers in M remains competitive with both self-consistency and reward model verification baselines."}, {"title": "Effect of Verifier Diversity.", "content": "Here, we investigate whether using diverse verifiers outperforms repeatedly querying a single verifier. Specifically, we compare the performance of our diverse domain-specific subsets Md versus repeatedly querying the single best-performing verifier v* \u2208 Md for domain d (where the number of queries to v* equals |Md|). As shown in Table 4, using diverse sets of verifiers generally outperforms querying the same verifier multiple times."}, {"title": "4 Discussion", "content": "Multi-Agent Verification (MAV) introduces a promising dimension for scaling test-time compute: scaling the number of verifiers. In Section 3, we demonstrated that combining multiple verifiers enables more effective evaluation of candidate outputs, facilitates weak-to-strong generalization, and allows for self-improvement. However, our approach has important limitations and there are several opportunities for future work to explore.\n\nFirst, our investigation is limited to a pool of 20 aspect verifiers based on just two base LLMs, and the design of our verifiers is constrained by our ability to come up with diverse verification strategies and relevant aspects. Future work could explore scaling to many more verifiers and try a more systematic exploration of the space of verifiers, potentially using LLMs themselves to generate diverse verification strategies and identify relevant aspects to verify. With better-engineered verifiers and more systematic exploration, we expect to observe stronger scaling patterns.\n\nSecond, our aggregation technique described in Section 2.2 uses a simple voting mechanism that directly sums the individual binary approvals from each verifier. This approach does not account for the confidence or relevance of each verifier, and verifiers do not observe each other's decisions or feedback. Future works could explore more sophisticated aggregation methods such as confidence-weighted voting or allowing verifiers to engage in debate before producing an approval. Moreover, our current approach uses a static engineered set of verifiers Md for all questions in a domain d, even though it may be best to use fewer or different verifiers for specific questions. Future works could investigate dynamically selecting the best set of verifiers for particular problems or adaptively choosing additional verifiers based on the results of the first few verification queries. Additionally, the field of social choice theory is concerned with procedures for collective decision-making and might offer insights for aggregating the perspectives of diverse verifiers. Although, our setting differs in that we care more about verifier capabilities than preferences.\n\nNext, our implementation of BoN-MAV is limited to only a single generator LLM. Thus, an interesting direction would be to explore sampling from multiple generators in addition to evaluating with multiple verifiers. Since different models may excel at solving different types of problems, this approach could make even better use of the growing ecosystem of LLMs and their diverse capabilities.\n\nFurthermore, while our results show that BoN-MAV can improve language model performance at test-time, we did not investigate finetuning the generator LLM on the outputs selected by our verifiers. Similar to how prior works have finetuned on outputs selected through self-consistency or reward models, training on outputs selected by MAV systems could be explored as a method to improve the generator LLM and also each of the LLM-based verifiers. Moreover, an interesting direction for future work is to directly use reinforcement learning to train both the generator and verifier models. That is, generator LLMs can be trained to maximize the scores across multiple verifiers, and the verifiers can simultaneously be trained to accurately verify individual aspects of responses.\n\nFinally, multi-agent verification offers interesting opportunities for AI safety and oversight. The ability to combine multiple verifiers checking different aspects of model outputs aligns with recent efforts towards safety checking the outputs of language models. That is, different verifiers can be engineered to check various safety and alignment properties, from basic constraints like avoiding harmful content to more nuanced properties like reasoning transparency. Our results on weak-to-strong generalization also align with recent work on scalable oversight, where weaker systems supervise stronger ones. In general, our work connects to broader ideas in AI alignment about using multiple models to improve safety.\n\nAn underlying thread throughout our work and discussion is the vision of a growing ecosystem of diverse language models that generate, verify, and learn from each other. Our work on multi-agent verification represents one step in this direction, and each of the future directions we have discussed offers a potential avenue for additional progress. We look forward to seeing how the research community advances these ideas."}, {"title": "5 Related Works", "content": "Scaling Test-Time Compute. Recent work has demonstrated that increasing computational resources during inference can significantly improve LLM performance (e.g., Wei et al. 2022; Snell et al. 2024). One line of research focuses on techniques where a single generator LLM produces additional output tokens during inference. These include scratchpads or Chain-of-Thought prompting, self-consistency or majority voting techniques, and various self-reflection methods (e.g., Shinn et al. 2024; Qu et al. 2024; Madaan et al. 2024; Saunders et al. 2022; Bai et al. 2022b). Other works have explored training LLMs to generate special tokens which enhance reasoning ability at test-time (e.g., Goyal et al. 2023; Wang et al. 2023a; Herel & Mikolov 2024) or augmenting language models with tool-use abilities.\n\nAnother line of research focuses on using a verifier model to evaluate the quality or correctness of outputs sampled from generator models. Typically, this is done through best-of-n sampling, where n candidate outputs are generated and the highest-scoring output is selected based on some verifier. This verification can be performed at the outcome-level or process-level. Recent works have also explored using ensembles of homogeneous reward models (identical model initializations trained on the same data but with different random seeds) to mitigate reward model overoptimization. Additionally, some approaches allow reward models to produce their own Chain-of-Thought reasoning before scoring."}, {"title": "Multi-Agent Reasoning with Language Models.", "content": "Recent works have investigated several approaches to multi-agent interaction for improving language model reasoning. Language model debate and multi-agent discourse have been studied as ways to enhance reasoning, and also as a direction for scalable oversight research. Prior works have also explored performing search with language models, which typically combines a generator LLM and a value model to guide exploration (see the previous paragraph). Moreover, some works have explored multi-modal reasoning through agent collaboration.\n\nUnlike prior work on multi-agent reasoning which focuses on collaborative problem-solving, we introduce a framework specifically for scaling test-time verification by combining multiple verifiers without training."}, {"title": "6 Conclusion", "content": "We have introduced Multi-Agent Verification (MAV), a test-time compute paradigm that combines multiple verifiers to improve performance. MAV enables test-time scaling along two orthogonal dimensions: (1) the traditional dimension of increasing the number of candidate outputs sampled from a generator LLM, and (2) our novel test-time scaling dimension of increasing the number of verifiers evaluating each output. We propose Aspect Verifiers (AVs) as one possible implementation choice for the verifiers in a MAV system. AVs are off-the-shelf LLMs that require no additional training and naturally support combining verification signals from models based on different LLMs, training algorithms, architectures, data, or prompts. Thus, AVs are a convenient building block for multi-agent verification, allowing us to leverage the growing ecosystem of language models and their diverse capabilities. We introduce BoN-MAV as a simple multi-agent verification algorithm and our results indicate that increasing the number of diverse verifiers is a promising dimension for scaling test-time compute. Specifically, we demonstrate that this approach improves test-time performance across multiple domains and generator LLMs, enables weak-to-strong generalization by combining multiple weak verifiers to improve stronger generators, and facilitates self-improvement when the generator LLM is also used as the base LLM for each of the aspect verifiers. Moreover, BoN-MAV represents just one approach to multi-agent verification and we expect better-engineered verifiers and more nuanced aggregation strategies to unlock even stronger scaling patterns. We hope that our work inspires future research into multi-agent verification algorithms and further exploration of scaling the number of verifiers as a powerful new dimension for test-time compute."}, {"title": "A Experimental Setup", "content": "A.1 Aspect Verifier Subsets\n\nTable 5 outlines all 20 aspect verifiers in M and which ones were selected for each domain-specific subset Md.\n\nA.2 Generator LLMs\n\nWe evaluate eight generator LLMs (four closed-source models and four open-source models) and restrict our set of generator models to those released before September 2024. For closed-source models, we use gemini-1.5-flash-001 and gemini-1.5-pro-001, as well as gpt-4o-mini-2024-07-18 and gpt-4o-2024-08-06 . For open-source models, we use Mistral-7B-v0.3 , Llama-3.1-8B , Gemma-2-9B, and Gemma-2-27B.\n\nA.3 Reward Model Baseline\n\nOur reward model verification baseline (BoN-RM) uses Skywork/Skywork-Reward-Llama-3.1-8B-v0.2, the top scoring open-source 8B reward model on RewardBench at the time of writing. This pretrained reward model outperforms numerous larger models including 70B and 340B models, and can be run on academic-scale compute."}, {"title": "A.4 Prompts", "content": "For generator LLMs, we use a consistent prompt format across all models while varying the content by domain. Table 7 contains these domain-specific prompts.\n\nFor aspect verifiers, each prompt consists of two components:\n\n1. A domain-dependent system prompt that establishes the verification context (e.g., mathematical problems, multiple-choice questions, or code implementations)\n2. A domain-independent verification prompt that specifies the aspect to verify and verification strategy\n\nThis two-part structure allows us to combine any aspect-strategy verification method with any domain while maintaining consistent evaluation criteria across base models."}, {"title": "B Additional Results", "content": "Table 6 compares BON-MAV using all 20 aspect verifiers in M (without domain-specific engineering) against self-consistency and reward model verification. Even without engineering domain-specific subsets Md, combining all verifiers remains competitive with baseline methods."}, {"title": "C Additional Illustrations", "content": "Figure 7, Figure 8, and Figure 9 provide additional examples of how multiple aspect verifiers evaluate a single candidate output. Figure 7 demonstrates verification using multiple strategies with a single base model on MATH . Figure 8 shows verification of a coding solution from HumanEval . Figure 9 illustrates verification of a correct solution from GPQA (diamond), showing how different base models can assess the same aspect differently. Each figure follows the same format as Figure 2 from the main paper."}]}