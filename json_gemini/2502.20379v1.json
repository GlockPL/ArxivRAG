{"title": "Multi-Agent Verification: Scaling Test-Time Compute with Multiple Verifiers", "authors": ["Shalev Lifshitz", "Sheila A. McIlraith", "Yilun Du"], "abstract": "By utilizing more computational resources at test- time, large language models (LLMs) can improve without additional training. One common strat- egy uses verifiers to evaluate candidate outputs. In this work, we propose a novel scaling dimen- sion for test-time compute: scaling the number of verifiers. We introduce Multi-Agent Verification (MAV) as a test-time compute paradigm that com- bines multiple verifiers to improve performance. We propose using Aspect Verifiers (AVs), off-the- shelf LLMs prompted to verify different aspects of outputs, as one possible choice for the verifiers in a MAV system. AVs are a convenient building block for MAV since they can be easily combined without additional training. Moreover, we intro- duce BON-MAV, a simple multi-agent verifica- tion algorithm that combines best-of-n sampling with multiple verifiers. BoN-MAV demonstrates stronger scaling patterns than self-consistency and reward model verification, and we demon- strate both weak-to-strong generalization, where combining weak verifiers improves even stronger LLMs, and self-improvement, where the same base model is used to both generate and verify outputs. Our results establish scaling the number of verifiers as a promising new dimension for im- proving language model performance at test-time.", "sections": [{"title": "1 Introduction", "content": "Scaling the size of large language models (LLMs) and their training datasets has driven remarkable progress in artificial intelligence. However, the growing cost of scaling model size and obtaining unseen high-quality pretraining data has sparked growing interest in methods that improve LLM performance without simply scaling parameters or data. Among these, a promising new direction has emerged: scaling test-time compute, where models spend more com- putational resources during inference-much like humans spend more time thinking through harder problems.\n A common strategy for scaling test-time compute is best- of-n sampling, where n candidate outputs are sam- pled from a generator LLM and a verifier model scores each candidate output based on its quality or correctness. The highest-scoring output is then selected. Under this strategy, the amount of test-time compute can be scaled up by increas- ing the number of sampled outputs. However, in this work, we propose a new orthogonal scaling dimension: scaling the number of verifiers. We introduce Multi-Agent Verifi- cation (MAV), a test-time compute paradigm that combines multiple verifiers to improve performance.\n Typically, verifiers are implemented as reward models which are trained using reinforcement learning from human feed- back. However, relying on reward models as verifiers introduces two crucial limitations for multi-agent verification: (1) each reward model has to be trained on expensive curated preference data, and (2) there is no straightforward way to combine scores generated by heterogeneous reward models trained on different datasets (they produce uncalibrated scores). These limitations make reward models poorly suited for multi-agent verification and restrict our ability to simply scale up the number and type of verifiers at test-time.\n To address these limitations and enable scalable multi-agent verification, we propose using Aspect Verifiers (AVs) off-the-shelf LLMs prompted to verify specific aspects of candidate outputs through binary True/False approvals. This approach is motivated by the observation that internet data contains abundant examples of humans providing binary evaluations with feedback (e.g., educational assessments, academic peer reviews, online forums, and automated code tests), which suggests that language models may be naturally suited for binary verification. Unlike reward models, AVs do not require additional training since producing binary approvals falls naturally within the training distribution of their base LLMs, and their binary outputs can be easily combined across multiple models through simple voting mechanisms. Thus, the number and type of aspect verifiers can be easily scaled up without additional training. We note that aspect verifiers are just one possible implementation choice for the verifiers in a MAV system, which address the two key limitations of typical reward model verifiers.\n By aggregating binary signals across a diverse set of as- pect verifiers, we can leverage the growing ecosystem of language models to produce a more robust verification sig- nal. Each verifier can focus on different aspects of outputs like mathematical correctness or logical soundness, and em- ploy different verification strategies such as direct yes/no approval, step-by-step analysis, solution rephrasing, or edge case checking. Thus, a diverse set of aspect verifiers can be obtained by varying three key axes: the base LLM, the aspect to verify, and the verification strategy.\n To investigate scaling multi-agent verification, we introduce BON-MAV as a specific algorithm which combines best-of- n sampling with aspect verifiers. This is one implementation of a MAV algorithm, combining traditional best-of-n sam- pling with multiple verifiers. Given an input, BON-MAV (1) samples n outputs from a generator LLM, (2) collects binary approvals from a set of m aspect verifiers, and (3) selects the output with the most approvals. We investigate scaling test-time compute with this approach along two orthogo- nal dimensions: the traditional dimension of increasing the number of sampled candidate outputs n, and our novel test- time scaling dimension of increasing the number of verifiers m. We find that using multiple diverse verifiers to select between candidate outputs is an effective strategy, and that performance improves as we use more verifiers.\n More specifically, across multiple domains and LLMs, BoN- MAV demonstrates more effective scaling patterns when we increase the number of sampled outputs, compared to best-of-n with reward model verification and self-consistency. We also demonstrate weak-to-strong gener- alization, whereby combining many small aspect verifiers can improve the performance of even stronger generator LLMs, and we show that BoN-MAV en- ables self-improvement by using the same base LLM for both the generator and set of aspect verifiers. Since BoN- MAV is just one simple approach to multi-agent verification, we expect that substantial improvements can be achieved using alternative methods.\n Overall, our paper makes the following contributions:\n (1) We introduce Multi-Agent Verification (MAV) as a new test-time paradigm that combines multiple verifiers at test-time, opening a novel scaling dimension: scaling the number of verifiers.\n (2) We propose Aspect Verifiers (AVs), off-the-shelf LLMs which require no additional training and naturally sup- port combining verification signals from multiple het- erogeneous verifiers using voting mechanisms.\n (3) We demonstrate that BoN-MAV, a simple multi-agent verification algorithm which combines best-of-n with aspect verifiers, improves the performance of various generator LLMs as we scale up the number and type of aspect verifiers."}, {"title": "2 Multi-Agent Verification", "content": "Multi-Agent Verification (MAV) is a test-time compute paradigm where multiple verifiers are combined to eval- uate outputs from a generator LLM. To implement a MAV algorithm, we must address two questions: (1) What type of verifiers can be easily combined and scaled up in number without additional training? (2) How should we aggregate verification signals from multiple verifiers? In this section, we propose answers to these questions and describe one simple implementation of a multi-agent verification algo- rithm called BON-MAV. We discuss future directions for alternative multi-agent verification algorithms in Section 4.\n In Section 2.1, we propose Aspect Verifiers (AVs) as a convenient building block for MAV, since they require no additional training and naturally support combining mul- tiple verification signals. In Section 2.2, we describe our approach to aggregating signals across multiple AVs. In Section 2.3, we outline the BoN-MAV algorithm, which combines best-of-n sampling with aspect verifiers. Finally, Section 2.4 proposes verifier engineering as a method to select relevant verifiers for specific domains or tasks."}, {"title": "2.2 Combining Aspect Verifiers", "content": "With aspect verifiers as our building block, we ask: How can we effectively aggregate verification signals across multiple AVs? We take the simplest possible approach in our exper- iments: each binary True/False approval is a single vote, and the aggregated score for a candidate output is the sum of the positive votes from all AVs. That is, the aggregated verification score is the sum of the individual binary scores from each verifier:\n AggScore(o\n) = \u03a3v\u2208M BinaryScore(o , v)                                                        (1)\n where o \u2208 O is the i candidate output from the set of sampled outputs O, M is the set of aspect verifiers, and BinaryScore : O \u2192 {0,1} maps a candidate output from O to the binary approval produced by verifier v \u2208 M for that output. This voting strategy gives equal weight to all verifiers in the final aggregated score, and it proves remarkably effective in our experiments (see Section 3.1). However, future works could investigate more sophisticated"}, {"title": "2.3 BoN-MAV", "content": "Best-of-n (BoN) sampling is a test-time optimization tech- nique where n candidate outputs are sampled from a generator LLM, each candidate is scored by a verifier model, and the highest-scoring output is selected. We introduce BON-MAV as a simple multi-agent verification algorithm that combines best-of-n sampling with aspect verifiers. It uses the simple aggregation strategy from Equation 1 and consists of three steps: (1) sampling n candidate outputs from a generator LLM, (2) collecting binary approvals from a set of m aspect verifiers, and (3) selecting the output with the most approvals. That is,\n i = arg max (AggScore(o ))                                                        (2)\n where o is the i candidate output, n is the total number of sampled candidate outputs, and i is the index of the out- put with the highest aggregated score (the selected output).\n Using BoN-MAV, we can increase test-time computation by sampling more candidate outputs (increasing n) and by querying more verifiers (increasing m = |M|), where test-time computation can be easily parallelized during gen- eration as well as verification. In addition, BoN-MAV repre- sents just one specific approach to multi-agent verification, and more nuanced aggregation algorithms or alternatives to aspect verifiers could further enhance performance."}, {"title": "2.4 Verifier Engineering", "content": "Using aspect verifiers, we can create a diverse pool of veri- fiers with different capabilities. However, not all verifiers"}, {"title": "3 Experiments", "content": "In our experiments, we investigate scaling test-time compute along two orthogonal dimensions: the traditional dimension of increasing the number of sampled candidate outputs n, and our novel test-time scaling dimension of increasing the number of verifiers m. We aim to address the follow- ing questions: (1) How well does multi-agent verification improve performance across diverse domains and various generator LLMs? (2) Can multi-agent verification facili- tate weak-to-strong generalization and self-improvement? (3) How important is engineering a domain-specific set of verifiers and what are the important design choices?\n To address these questions, we evaluate the BoN-MAV algo- rithm described in Section 2 on the following four domains:\n \u2022 Mathematics. The MATH dataset consists of competition-level math questions at five difficulty levels. For our experiments, we randomly sample 400 questions from the test set across all five levels: 100 for validation and 300 for testing.\n \u2022 General Knowledge & Reasoning. MMLU-Pro is an enhanced version of the popular MMLU benchmark which features more challenging, reasoning-focused questions and expands the multiple-choice set from four to ten options. As with MATH, we sample 100 questions for validation and 300 for testing.\n \u2022 Graduate-Level Reasoning. The GPQA dataset consists of graduate-level, multiple-choice questions in biology, physics, and chemistry. For our experiments, we utilize GPQA's \"diamond\u201d subset - a collection of 198 high-quality and extremely challeng- ing questions. We sample 98 questions for validation and 100 for testing.\n \u2022 Coding. HumanEval is a widely- used benchmark consisting of 164 Python program- ming questions. We sample 64 questions for validation and 100 for testing."}, {"title": "3.1 MAV Enables Scaling Along Two Dimensions", "content": "Here, we investigate how BoN-MAV scales with the number of candidate outputs and number of verifiers.\n Baselines. We compare Best-of-n sampling with Multi- Agent Verification (BON-MAV) against two established test- time compute methods: (1) best-of-n sampling with reward model verification, where we use a trained neural reward"}, {"title": "3.2 MAV Enables Weak-to-Strong Generalization and Self-Improvement", "content": "We now explore two important capabilities of multi-agent verification: improving strong models using only weaker verifiers (weak-to-strong generalization) and improving models using only self-verification (self-improvement)."}, {"title": "3.3 Analysis: Understanding Multi-Agent Verification", "content": "To better understand the key design choices that impact multi-agent verification, we conduct two ablation studies on MMLU-Pro and GPQA (diamond)-the two most chal- lenging domains in our evaluation. We investigate: (1) how performance depends on engineering domain-specific sets of verifiers, and (2) whether using diverse verifiers outper- forms repeatedly querying the single best verifier.\n Effect of Verifier Engineering. In Section 3.1, we intro- duced verifier engineering as an approach for selecting a relevant subset of verifiers M \u2282 M for each domain d. Here, we compare our engineered verifier subsets M against a simple baseline that uses all available aspect ver- ifiers in M (see Appendix A.1 for a full list) without any domain-specific tuning.\n Effect of Verifier Diversity. Here, we investigate whether using diverse verifiers outperforms repeatedly querying a single verifier. Specifically, we compare the performance of our diverse domain-specific subsets M versus repeatedly querying the single best-performing verifier v* \u2208 M for domain d (where the number of queries to v* equals |Md|)."}, {"title": "4 Discussion", "content": "Multi-Agent Verification (MAV) introduces a promising di- mension for scaling test-time compute: scaling the number of verifiers. In Section 3, we demonstrated that combining multiple verifiers enables more effective evaluation of candi- date outputs, facilitates weak-to-strong generalization, and allows for self-improvement. However, our approach has important limitations and there are several opportunities for future work to explore.\n First, our investigation is limited to a pool of 20 aspect verifiers based on just two base LLMs, and the design of our verifiers is constrained by our ability to come up with diverse verification strategies and relevant aspects. Future work could explore scaling to many more verifiers and try a more systematic exploration of the space of verifiers, poten- tially using LLMs themselves to generate diverse verifica- tion strategies and identify relevant aspects to verify. With better-engineered verifiers and more systematic exploration, we expect to observe stronger scaling patterns.\n Second, our aggregation technique described in Section 2.2 uses a simple voting mechanism that directly sums the indi- vidual binary approvals from each verifier. This approach does not account for the confidence or relevance of each verifier, and verifiers do not observe each other's decisions or feedback. Future works could explore more sophisticated aggregation methods such as confidence-weighted voting or allowing verifiers to engage in debate before producing an approval. Moreover, our current approach uses a static engineered set of verifiers Md for all questions in a domain d, even though it may be best to use fewer or different verifiers for specific questions. Future works could investigate dynamically selecting the best set of verifiers for particular problems or adaptively choosing additional verifiers based on the results of the first few verification queries. Additionally, the field of social choice theory is concerned with procedures for collective decision-making and might offer insights for aggregating the perspectives of diverse verifiers. Although, our setting differs in that we care more about verifier capabilities than preferences.\n Next, our implementation of BoN-MAV is limited to only a single generator LLM. Thus, an interesting direction would be to explore sampling from multiple generators in addi- tion to evaluating with multiple verifiers. Since different models may excel at solving different types of problems, this approach could make even better use of the growing ecosystem of LLMs and their diverse capabilities.\n Furthermore, while our results show that BoN-MAV can im- prove language model performance at test-time, we did not investigate finetuning the generator LLM on the outputs se- lected by our verifiers. Similar to how prior works have fine- tuned on outputs selected through self-consistency or reward models, training on outputs selected by MAV systems could be explored as a method to improve the generator LLM and also each of the LLM-based verifiers. Moreover, an interesting direction for future work is to directly use reinforcement learning to train both the generator and verifier models. That is, generator LLMs can be trained to maximize the scores across multiple verifiers, and the verifiers can simultaneously be trained to accurately verify individual aspects of responses.\n Finally, multi-agent verification offers interesting opportu- nities for AI safety and oversight. The ability to combine multiple verifiers checking different aspects of model out- puts aligns with recent efforts towards safety checking the outputs of language models. That is, different verifiers can be engineered to check various safety and alignment properties, from basic constraints like avoiding harmful con- tent to more nuanced properties like reasoning transparency. Our results on weak-to-strong generalization also align with recent work on scalable oversight, where weaker systems su- pervise stronger ones. In general, our work connects to broader ideas in AI alignment about using multiple models to improve safety."}, {"title": "5 Related Works", "content": "Scaling Test-Time Compute. Recent work has demon- strated that increasing computational resources during infer- ence can significantly improve LLM performance (e.g., Wei et al. 2022; Snell et al. 2024). One line of research fo- cuses on techniques where a single generator LLM pro- duces additional output tokens during inference. These include scratchpads or Chain-of-Thought prompting, self-consistency or major- ity voting techniques, and var- ious self-reflection methods. Other works have explored training LLMs to generate special tokens which enhance reasoning ability at test-time or augmenting language models with tool- use abilities.\n Another line of research focuses on using a verifier model to evaluate the quality or correctness of outputs sampled from generator models. Typically, this is done through best- of-n sampling, where n candidate outputs are gen- erated and the highest-scoring output is selected based on some verifier. This verification can be performed at the outcome-level or process-level. Recent works have also explored using ensembles of homogeneous reward models to mitigate reward model overoptimization. Additionally, some approaches allow reward models to produce their own Chain-of-Thought reasoning before scoring. Various papers have combined language with search techniques at test-time, using verifiers to provide a heuristic signal. These verifiers may use LLMs as prompted value functions, incorporate real environment feed- back, or use trained value functions. Unlike prior works which typically rely on a single reward model verifier or homogeneous reward model ensembles trained on the same data, we propose a frame- work for combining multiple heterogeneous verifiers with- out additional training, and investigate scaling the number and type of verifiers as a novel test-time scaling dimension.\n Multi-Agent Reasoning with Language Models. Recent works have investigated several approaches to multi-agent interaction for improving language model reasoning. Lan- guage model debate and multi-agent dis- course have been studied as ways to enhance reasoning, and also as a direction for scalable oversight research. Prior works have also explored performing search with language models, which typically combines a generator LLM and a value model to guide exploration (see the previous paragraph). Moreover, some works have ex- plored multi-modal reasoning through agent collaboration. Unlike prior work on multi-agent rea- soning which focuses on collaborative problem-solving, we introduce a framework specifically for scaling test-time ver- ification by combining multiple verifiers without training."}, {"title": "6 Conclusion", "content": "We have introduced Multi-Agent Verification (MAV), a test- time compute paradigm that combines multiple verifiers to improve performance. MAV enables test-time scaling along two orthogonal dimensions: (1) the traditional dimension of increasing the number of candidate outputs sampled from a generator LLM, and (2) our novel test-time scaling dimen- sion of increasing the number of verifiers evaluating each output. We propose Aspect Verifiers (AVs) as one possible implementation choice for the verifiers in a MAV system. AVs are off-the-shelf LLMs that require no additional train- ing and naturally support combining verification signals from models based on different LLMs, training algorithms, architectures, data, or prompts. Thus, AVs are a convenient building block for multi-agent verification, allowing us to leverage the growing ecosystem of language models and their diverse capabilities. We introduce BoN-MAV as a simple multi-agent verification algorithm and our results indicate that increasing the number of diverse verifiers is a promising dimension for scaling test-time compute. Specifi- cally, we demonstrate that this approach improves test-time performance across multiple domains and generator LLMs, enables weak-to-strong generalization by combining mul- tiple weak verifiers to improve stronger generators, and facilitates self-improvement when the generator LLM is also used as the base LLM for each of the aspect verifiers. Moreover, BoN-MAV represents just one approach to multi- agent verification and we expect better-engineered verifiers and more nuanced aggregation strategies to unlock even stronger scaling patterns. We hope that our work inspires future research into multi-agent verification algorithms and further exploration of scaling the number of verifiers as a powerful new dimension for test-time compute."}, {"title": "A Experimental Setup", "content": "A.1 Aspect Verifier Subsets"}, {"title": "A.2 Generator LLMs", "content": "We evaluate eight generator LLMs (four closed-source models and four open-source models) and restrict our set of generator models to those released before September 2024. For closed-source models, we use gemini-1.5-flash-001 and gemini- 1.5-pro-001 (Team et al., 2024a), as well as gpt-4o-mini-2024-07-18 and gpt-40-2024-08-06 (Achiam et al., 2023). For open-source models, we use Mistral-7B-v0.3 (Jiang et al., 2023), Llama-3.1-8B (Dubey et al., 2024), Gemma-2-9B, and Gemma-2-27B (Team et al., 2024b)."}, {"title": "A.3 Reward Model Baseline", "content": "Our reward model verification baseline (BoN-RM) uses Skywork/Skywork-Reward-Llama-3.1-8B-v0.2 (Liu et al., 2024), the top scoring open-source 8B reward model on RewardBench (Lambert et al., 2024) at the time of writing. This pretrained reward model outperforms numerous larger models including 70B and 340B models, and can be run on academic-scale compute."}, {"title": "A.4 Prompts", "content": "For generator LLMs, we use a consistent prompt format across all models while varying the content by domain. For aspect verifiers, each prompt consists of two components:\n 1. A domain-dependent system prompt that establishes the verification context (e.g., mathematical problems, multiple-choice questions, or code implementations)\n 2. A domain-independent verification prompt that specifies the aspect to verify and verification strategy\n This two-part structure allows us to combine any aspect-strategy verification method with any domain while maintaining consistent evaluation criteria across base models."}, {"title": "B Additional Results", "content": "Table 6 compares BON-MAV using all 20 aspect verifiers in M (without domain-specific engineering) against self- consistency and reward model verification. Even without engineering domain-specific subsets Md, combining all verifiers remains competitive with baseline methods."}, {"title": "C Additional Illustrations", "content": "Figure 7, Figure 8, and Figure 9 provide additional examples of how multiple aspect verifiers evaluate a single candidate output. Figure 7 demonstrates verification using multiple strategies with a single base model on MATH . Figure 8 shows verification of a coding solution from HumanEval . Figure 9 illustrates verification of a correct solution from GPQA (diamond), showing how different base models can assess the same aspect differently. Each figure follows the same format as Figure 2 from the main paper."}]}