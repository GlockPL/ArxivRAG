{"title": "Hey AI Can You Grade My Essay?: Automatic Essay Grading", "authors": ["Maisha Maliha", "Vishal Pramanik"], "abstract": "Automatic essay grading (AEG) has attracted the the attention of the NLP community because of its applications to several educational applications, such as scoring essays, short answers, etc. AEG systems can save significant time and money when grading essays. In the existing works, the essays are graded where a single network is responsible for the whole process, which may be ineffective because a single network may not be able to learn all the features of a human-written essay. In this work, we have introduced a new model that outperforms the state-of-the-art models in the field of AEG. We have used the concept of collaborative and transfer learning, where one network will be responsible for checking the grammatical and structural features of the sentences of an essay while another network is responsible for scoring the overall idea present in the essay. These learnings are transferred to another network to score the essay. We also compared the performances of the different models mentioned in our work, and our proposed model has shown the highest accuracy of 85.50%.", "sections": [{"title": "Introduction", "content": "Automatic essay grading (AEG) is a subfield of natural language processing (NLP) that has been around for more than a half-century. [1] suggested the first AEG system. For many years, little progress was made, mainly owing to a lack of resources (such as parsers) and processing power. But things changed with the development of deep neural networks and powerful computing machines. [2], and [3] are the best examples of applications of deep neural networks.\nAutomatic essay grading refers to using a machine to grade a text prepared in response to a topic known as the essay prompt. Holistic automatic essay grading refers to using computers to award an essay grade that reflects its overall quality. Aside from overall AEG, algorithms may be trained to rate essays depending"}, {"title": "Difficulties in Automatic Essay Grading", "content": "The most serious issue in the realm of AEG is holistic AEG. This procedure entails providing an overall score/grade to the essay depending on how well-written it is. The majority of AEG research is focused on holistic AEG.\nMuch earlier research work used machine learning with classifiers to holistically grade the essays. Some commercial systems such as e-rater R [4], Intelligent Essay Assessor [5], etc. use a variety of features from surface-level features like length-based features (word count, average word length, average sentence length, etc.), to more complex features (like usage score, which detects usage errors in the essay) and kernels.\n[6] describes a set of task-independent features for AEG, where they studied the problem of cross-domain AEG. One of their findings was that the best results for cross-domain AEG are when the source and target prompts are similar."}, {"title": "Related Works", "content": "In this section, we look at different systems which are used for AEG."}, {"title": "Classical Machine-Learning based Systems", "content": "The earliest AEG systems use machine-learning-based approaches. The general approach for these systems involves feature engineering and ordinal classification/regression. Project Essay Grade (PEG) [1], the earliest AEG system, used a series of intrinsic properties (called trins, which are analogous to features) to come up with an approximate score for the essay."}, {"title": "Deep-Learning based Systems", "content": "Since the early 2010s, neural networks have been used to solve multiple tasks in NLP, such as CNN [7] and other hierarchical neural network models. [8]"}, {"title": "Background and Technologies", "content": ""}, {"title": "Recurrent Neural Networks", "content": "A recurrent neural network (RNN) [15] is a deep neural network that uses the previous step's output as feedback input in the next step. They are generally used in sequences, in next-word prediction in a sentence. The RNNs store the knowledge in a hidden state and use the previous knowledge to predict the next sequence. At time step t, the hidden state $a^{<t>}$ and the output $y^{<t>}$ can be expressed as:\n$a^{<t>} = g_1(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a$\nand\n$y^{<t>} = g_2(W_{ya}a^{<t>}+b_y)$\nwhere $W_{aa}$, $W_{ax}$, $W_{ya}$,$b_a$ and $b_y$ are the coefficients that are shared over time steps and $g_1$ and $g_2$ are the activation functions. The working mechanism has been shown in the figure 2."}, {"title": "Convolutional Neural Network", "content": "The Convolutional Neural Network [10] is a class of neural networks used for data processing with a grid-like structure, such as an image. A CNN has three main parts: convolutional layer, pooling layer and dense layer. The convolution layer uses the concept of the kernel function to extract the information from the image when given as input in image matrix format. The convolution can be calculated using the below equation.\nG[m, n] = (f * h)[m, n] = \\sum_{j} \\sum_{k}h[j, k] f[m \u2013 j, n \u2212 k]\nwhere f and h are the input images and the kernel function and m and n are the dimensions of the resultant image matrix and j and k are the dimensions of the kernel function. The pooling layer decreases the dimension of the input matrix and extracts the feature matrix's deeper meaning."}, {"title": "Bidirectional Encoder Representations from Transformers (BERT)", "content": "BERT [11] is the Encoder [12] part of a Transformer [13]. It is responsible for the extraction of features from the input sentence and pass the output vector to the Decoder part of the Transformer. BERT encode the input sentences with positional encodings so that the model can run in parallel. It consists of 12 encoder blocks, which contains a multi-headed self-attention mechanism and a dense layer. The attention mechanism helps to the different relations between the words in the sentences. This helps the model to have greater understanding of the sentences."}, {"title": "Dataset", "content": "We used the most commonly used AEG dataset, the Automated Students Assessment Prize (ASAP) Automatic Essay Grading (AEG) dataset, for our experimentation. The dataset comprises nearly 13,000 essays in response to 8 different essay prompts. The dataset is freely available and hosted on Kaggle, and the link"}, {"title": "Techniques and Implementation", "content": "We have tested the dataset on the following models for comparative analysis."}, {"title": "SVM", "content": "We took the automatic grading problem as a classification task. As shown in Table 1, the maximum mark that can be obtained is 60, and the minimum mark is 0. The marks are divided into 61 classes from 0 to 60. We took the essay, used TF-IDF (Term Frequency and Inverse Document Frequency) [14] vectorisation, and used 'l2' normalisation. The tf-idf does not blindly consider the number of times a word appears in the document by scaling down the words that appear frequently in the essays and have less meaning, such as the articles 'a','an' and 'the'. Suppose w is a word in the essay e then the tfidf is calculated by:\ntfidf (w,e) = tf(w, e) * idf (w)\nwhere,\nidf (w) = log (1 + n)/(1 + df (w))\n+1"}, {"title": "BERT", "content": "We use the standard bert-base-cased model for automatic essay grading. The word \"base\" means it has 6 encoder blocks, and the word \"cased\" signifies that the model considers the case of the words while generating the result. We purposely took this model when grading an essay, the case of the words delivers important information to the model, like the beginning of a sentence or a proper noun in the sentence etc. Because BERT cannot accept inputs of size greater than 512 tokens, we have eliminated the essays which have more words than 500 words. Each encoder block contains a self-attention layer followed by a feedforward layer. The attention layer contains 8 multi-headed attention mechanisms that store different information regarding a sentence's words. This information is used during the grading of the essay. The feedforward network maintains the dimension of the output vector, which is sent to the next encoder block. The positional encoding of the words with Bert embeddings makes the model run very fast, as it can process the essay parallelly."}, {"title": "Collaborative Deep Learning Network (CDLN)", "content": "Collaborative learning is a method of learning where the pupils work in groups and work on separate tasks contributing to a common overall outcome, or work together on a shared task. Collaborative learning models have the power of transfer learning, where instead of a single deep neural network to learn everything, the learning can be distributed among several networks, and their collective knowledge can be shared. Our model consists of a Recursive Neural Network, a Convolutional Neural Network, an LSTM and a dense neural network, as shown in the figure 2. The different parts of the architecture are described below in detail.\nConvolutional Neural Network The convolutional neural network understands the idea conveyed in the sentences using the convolution and the average pooling layers. The words of the essays are converted into Word2Vec embeddings of 100 dimensions each. All these vectors are concatenated together to form a single-dimension vector. This vector is sent forward for convolution and pooling. 1x105x8 kernel is used for convolution and 1x90x8 for average pooling. The convolution and pooling layer is repeated for 5 times. After this, the layer is flattened and sent forward for concatenation with the Recursive Neural Network (RVNN) output vector. The convolution layer helps to analyse the essay, while the average pooling layers bring out the idea conveyed in the sentences."}, {"title": "Recursive Neural Network", "content": "The Recursive Neural Network (RvNN) [15] understands the structure of the sentences of the essays. This helps the CDLN model check the essays' grammatical and sentence construction errors. The essays are first embedded with a word embedding, using the Word2Vec. The words are first divided into bigrams; then, the representation vectors are fed into a neural network as shown in figure 2.\nWe took the embedding dimension as 100. So the neural network has 200 neurons in the first layer. After that 4 layers of 150 neurons each. The output layer is 100 neurons to match up with word embedding. The formula is shown below.\np = f(W[C_1; C_2] + b)\ns = W_{score}p\nThe $C_1, C_2$ are the representation vectors, W is a trainable vector, b is the bias. $W_{score}$ is another trainable vector and s is the score for that particular hierarchical structure of the sentence. Once the score is calculated, the same algorithm is followed for the rest of the sentences and then summed up and the final vector is forwarded for concatenation. An example of the algorithm with the sentence is given below."}, {"title": "Algorithm", "content": "Consider the sentence \" The dog is here \", and each word is denoted by numerics '1', '2', '3' and '4', respectively and their embeddings by $C_1$, $C_2$, $C_3$ and $C_4$. We calculate the score using formulas 6 and 7.\nStep 1: First, we calculate the scores for each word pair using the word embeddings of the words in the sentences given by s[1,2], s[2,3] and s[3,4] and"}, {"title": "Long Short Term Memory", "content": "The LSTM [16] takes the input vector, which is the resultant of the concatenation of the vector output of the Recursive Neural Network and the CNN. The output vector of LSTM is 1x10000 dimensions. This is forwarded to the dense layer. LSTM gathers the learnings of the CNN and RvNN and stores the learning in its hidden cell. This is a good example of transfer learning, where the previous deep learning networks transfer their cumulative to the following deep neural network. It keeps knowledge not only about the sentence structure but also of the ideas conveyed in the essays. Also, this information is sent forwarded to the next layers for grading the essay."}, {"title": "Dense Layer and the Output Layer", "content": "The dense layer takes the input from the output vector of the LSTM. It is 5 hidden layers with 120 neurons. The last output layer is a single cell which gives the essay grade."}, {"title": "Experimentation", "content": "We have experimented with 6 models, a) CDLN model, b)BERT, c)RNN, d)ANN and, e)SVM. The architecture of all the models has been described in the previous section. For all the models the learning rate is 0.0001 and the batch size for training is 32. For the CDLN model, BERT, RNN, ANN and SVM models, the number of epochs used is 15, 10, 8, 8, 8 and 6, respectively. Dropouts are used in the deep learning models to stop overfitting. In all the models during training, eight-fold cross-validation has been used."}, {"title": "Results and Analysis", "content": "We use the mean square error (MSE), Pearson's Correlation Coefficient (PCC) and Quadratic Weighted Kappa (QWK).\nPearson's Correlation Coefficient [16]: It is generally used to measure the linear correlation or relationship between two variables. It is calculated over a range of -1 to 1, where 1 shows that the variables are highly correlated in the positive direction while -1 shows that they are not correlated; the two variables are in opposite directions."}, {"title": "Automatic Evaluation", "content": "We compare the results from the different machine and deep learning models that has been mentioned in the above sections. The models' results we compare here are a) Our proposed CDLN model, b) LSTM, c) RNN, d) ANN, and e) SVM. We have also taken [20] as the baseline model and compared the results of the TDNN(ALL), CNN-LSTM, CNN-LSTM-ATT and 2L-LSTM with our models. The results have been shown prompt-wise in table 2 and overall in table 3. From the tables, it can be seen that our CDLN model outperforms the other models, including the baseline models. The sharing and transferring of knowledge from the CNN and the RVNN to the LSTM has boosted the results. The self-attention mechanism in BERT has led to better results than TDNN(Sem+Synt), the best-performing model in [20]. The knowledge stored in the neurons of the deep networks has overcome the feature engineering process. If we go prompt-wise, the results mostly show that our models outperform the others."}, {"title": "Robustness of the model", "content": "An automatic grading system must be robust in its action. It must grade the same essay with the same grade even when there is a slight change in the language and structure of the sentences. So we have conducted a robustness check of our CDLN model. For this test, we have selected 1000 random essays from the test set of all the essays. Then we rephrase the essays with Quillbot 3, a well-known paraphrasing tool available freely for users. We graded the selected essays with our model. After this, we paraphrased the essays with Quillbot and graded them with our model. We recorded the grades and averaged the marks of every 50 essays and then plotted and compared them in the graph shown in figure 3\nIt is evident from the above comparison-bar graph that the grades given by the model before and after paraphrasing is very close. It can be noticed from the graph that the average marks of the modified essays are more than the original essays; this can be due to the fact that quillbot has added proper sentence structure in the essays and removed the grammatical errors which may be present in the original essay. We further tested how much the grade differed from each other. For this purpose, we have modified the mean square error formula. If $g_{original}$ is the marks graded by the model for the original essays and $g_{modified}$ is the marks graded for the paraphrased essays then we find the difference by the formula:\n$A = \\frac{\\sum_{i=1}^{N}(g_{original} - g_{modified})^2}{N}$\nWe used the above formula for the grades of all the N = 1000 essays and got the value 0.34. This shows that grades are very close to each other, and the model is quite robust."}, {"title": "Conclusion and Future Works", "content": "Our study aims to demonstrate the power of collaborative learning in automatic essay grading problems. When multiple networks work together to understand and learn the different features of an essay and grade accordingly, their performance is boosted, as seen from the above table. This model has outperformed many of the pretrained and state-of-the-art models in AEG. But still, there are ample scopes of improvement in the results. With the introduction of many new deep neural networks and pretrained networks, these kinds of research can be further investigated and conducted. Our model gives a holistic score to the essay, not paragraph-wise. This can be further investigated."}]}