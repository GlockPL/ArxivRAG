{"title": "Coupling quantum-like cognition with the neuronal networks within generalized probability theory", "authors": ["Andrei Khrennikov", "Masanao Ozawa", "Felix Benninger", "Oded Shor"], "abstract": "The recent years are characterized by intensive applications of the methodology and mathematical apparatus of quantum theory, quantum-like modeling, in cognition, psychology, and decision making. In spite of the successful applications of this approach to a variety of psychological effects, e.g., the order, conjunction, disjunction, and response replicability effects, one may (but need not) feel dissatisfaction due to the absence of clear coupling to the neurophysiological processes in the brain. For the moment, this is just a phenomenological approach. In this paper we construct the quantum-like representation of the networks of communicating neurons. It is based not on standard quantum theory, but on generalized probability theory (GPT) with the emphasis of the operational measurement approach. We employ GPT's version which is based on ordered linear state space (instead of complex Hilbert space). A network of communicating neurons is described as a weighted ordered graph that in turn is encoded by its weight matrix. The state space of weight matrices is embedded in GPT with effect-observables and state updates within measurement instruments theory. The latter plays the crucial role.", "sections": [{"title": "1 Introduction", "content": "The quantum information revolution (also known as the second quantum revolution) has not only the big technological impact (quantum computing, cryptography, simulators), but also the tremendous foundational consequences. One of them is understanding that the methodology and formalism of quantum theory can be applied outside of physics for modeling in cognition, psychology, decision making, biology including genetics and epigenetics, evolution theory, economics and finance, social and political sciences, game theory. This area of research is known as quantum-like modeling (see, e.g., [81]).\nIt should be sharply distinguished from using of genuine quantum physics in cognition, as was done e.g. by Hameroff [40], Penrose [79], Vitiello [92]) and biology - \u201cquantum biophysics\u201d [6,47]. The quantum-like models describe the behavior of macroscopic cognitive systems (e.g., human behavior) whose functioning can be modeled as information processing matching the laws of quantum information and probability and deviating from the classical laws.\nDuring the last 10-15 years quantum-like studies flowered by covering new domains of science, for example, genetics and biological evolution theory [7,8,14]. Hence, the second quantum revolution was completed by the quantum-like revolution [81]. The main challenge for the quantum-like approach is that the neurophysiological mechanism of generation of the macroscopic quantum-like behavior is still unknown. The experimental statistical data collected in cognitive psychology is characterized by, e.g., the order, conjunction, disjunction, and response replicability effects, and contexuality. Such data can be successfully described by the quantum formalism. However, we still"}, {"title": "2 Graph representation of the networks of communicating neurons", "content": "We start with recollection [22] on the basics of brain's neuronal signaling:\n\"The human brain is made up of approximately 86 billion neurons that talk to each other using a combination of electrical and chemical (electrochemical) signals. The places where neurons connect and communicate with each other are called synapses. Each neuron has anywhere between a few to hundreds of thousands of synaptic connections, and these connections can be with itself, neighboring neurons, or neurons in other regions of the brain. A synapse is made up of a presynaptic and postsynaptic terminal. The presynaptic terminal is at the end of an axon and is the place where the electrical signal (the action potential) is converted into a chemical signal (neurotransmitter release). The postsynaptic terminal membrane is less than 50 nanometers away and contains specialized receptors. The neurotransmitter rapidly (in microseconds) diffuses across the synaptic cleft and binds to specific receptors.\"\nThe voltage-dependent channels endow the neuron with active signaling property. In particular, they enable a neuron to generate nerve impulses, also know as action potentials. In spite of variation in duration and amplitude as well as shape, they are typically treated as identical events. The duration of an action potential (approximately 1 ms) is ignored, an action potential sequence, or spike train, is represented by a series of all-or-none point events.\nWe note that each neuron has, on average, about 7,000 synaptic connections with other neurons. Axons vary in length, some of them are a millimeter or so, but the axons which go from the brain down the spinal cord, can extend for more than a meter. An axon has numerous side branches called axon collaterals, that is one neuron can send information to several others. The collaterals, as the roots of a tree, split into smaller extensions called terminal branches.\nConsider some network S in the brain consisting of N neurons, e.g., a network performing some brain function or even the network of all neurons in the brain. Neuronal networks are systems of our quantum-like model. The symbol S is used to denote a neuronal network of communicating neurons.\nIn the graph representation of S, neurons are mapped to the graph's vertexes, connections between neurons are mapped to graph's edges. We treat connections from the information viewpoint, so edges need not be physical edges consisting of axons and synapses. For example, edges (connections) between pairs of neurons are established by correlations between their signaling. We recall that if no two edges"}, {"title": "2.1 Model 1: directed graph for neurons signaling", "content": "The weight $w_{ij}$ assigned to the graphs' directed edge $n_i \\to n_j$ can be interpreted as the probability that during some interval of time A (determining the scale of cognitive information processing in network S), neuron $n_i$ sends a signal to neuron $n_j$. (This signal is of the electrochemical nature.) Denote this event $O_{i\\to j}$. Thus, the neuronal network S is represented by the weighted directed graph, each two neurons are connected by two arrow-edges, $n_i \\to n_j$ and $n_j \\to n_i$ arrows (two neurons cycle), and each neuron $n_i$ is feedback coupled to itself by the loop edge $n_i \\to n_i$. No-signaling is encoded by the zero weight. So, we operate with a complete graph. This is convenient; we don't appeal to the \"hardware\" of the connections between the neurons in the network S. By eliminating the edges with $w_{ij} = 0$ we would obtain more interesting graph geometry, but this is the topic of further studies.\nSuch neural activity can be represented in the framework of classical probability theory; for example, as follows. Let $\\xi_{ij}$ denote the classical random variable\n$$\\xi_{ij} = \\begin{cases}\n1, \\text{ if } O_{ij} \\text{ holds,}\\\\\n0, \\text{ otherwise.}\n\\end{cases}$$\nThen we set\n$$w_{ij} = E[\\xi_{ij}] = P(\\xi_{ij} = 1).$$"}, {"title": "2.2 Model 2: undirected graph for firing neural code", "content": "Instead of the model with the directed graph, we can consider the simpler model with undirected weighted graph. This model is connected with the neural code based on the firing frequency for neurons in the network S. For each neuron $n_i$ we define the random variable\n$$\\xi_{i} = \\begin{cases}\n1, \\text{ if } n_i \\text{ sends spike,}\\\\\n0, \\text{ otherwise,}\n\\end{cases}$$\nthat is in this model we are not interested in the redistribution of this spike between the neurons of the network S, as we do in the directed weighted graph model. Then we set\n$$w_{ij} = E[\\xi_{i}\\xi_{j}].$$\nThe weight matrix Ws = ($w_{ij}$) is the covariance matrix of the random vector ($\\xi_1, ..., \\xi_\\nu$)."}, {"title": "2.3 Model 3: undirected graph for correlation between action potentials", "content": "The third model is also based on undirected graphs. For each neuron $n_j$ in a network S, its action potential is described by the real-valued random variable $\\xi_j$. This is the continuous random variable representing neuron's action potential as a physical potential and not as the discrete spike generation event. The weights of the graph Gs are given by the pairwise correlations between the action (4). The weight matrix W = ($w_{ij}$) is the covariance matrix of the random vector $\\xi = (\\xi_1, ..., \\xi_\\nu)$.\nWe shall construct a GPT representation for Model 1 (section 3). We postpone GPT design for Models 2 and 3 for a future publication. In the GPT terminology Model 1 has classical state space, but it shows the basic quantum-like effects, as the order, interference (disjunction), and non-repeatability effects."}, {"title": "3 GPT for directed neuronal graph (Model 1)", "content": "Denote the set of all N \u00d7 N real matrices by the symbol M = MN. For matrix W = ($w_{ij}$), we define its norm\n$$||W|| = \\sum_{ij}|w_{ij}|.$$"}, {"title": "4 Generalized probability theory (operational measurement theory)", "content": "GPT was created in 1970th as an attempt to find the roots of the quantum formalism. Within GPT quantum formalism based on the complex Hilbert space is just a special version of the general operational formalism for all kinds of measurements. It unifies the classical and quantum measurement theories and opens the door to the variety of quantum-like formalisms. It is clear that probabilistically non-classical properties of mental phenomena need not imply that they should be described by the quantum (complex Hilbert space) formalism, quantum probability, information and measurement theory. Some GPT-based models can match better cognitive information processing and decision making; this was highlighted in [60]."}, {"title": "4.1 Ordered linear spaces", "content": "For simplicity, we restrict considerations to finite dimensional linear spaces. Let X be a real linear space. A subset C of X is called a cone, if for any positive real number r, rC \u2282 C. A set B is a base of the cone C if, for every x \u2208 C, there is a unique > > 0 such that Ax \u2208 B or in other words all elements of C can be obtained via scaling of elements of B with positive scalars.\nLet X be a real linear space with a partial order < that is consistent with the linear space structure: for any x, y, z \u2208 X and real r > 0, if x < y, then x + z < y + z and rx < ry. Such X is called an ordered linear space. Set X+ = {x \u2208 X : x > 0}. This set is a convex cone containing zero. The elements of X+ are called positive. If x, y \u2208 X, then x < y if and only if y - x \u2208 X+.\nConsider the dual space X*, the space of linear functionls on X. Since we work in the finite dimensional case, spaces X and X* are algebraically isomorphic. A linear functional is called positive if f(x) = (f|x) \u2265 0 for any x > 0. The set of all positive functionals X is a convex cone in X*. It is called the dual cone of X+. The order structure determined by X coincides with the point wise order structure on X*, i.e., f <\u2264 g iff f(x) \u2264 g(x) for all x \u2208 X+. To use analysis, a linear space should be endowed with a topology, e.g., determined by a norm ||\u00b7 || matching the order structure. Let X be"}, {"title": "4.2 States and observables", "content": "Definition 1. [80] State space \u03a9 is a convex, closed, and bounded subset of a real, finite-dimensional vector space with Euclidean topology.\nThe above definition is presented just to simplify the mathematical formalism. We note that such set is compact. We adjust this definition to the ordered space approach. Let X be an ordered normed space. A positive linear functional f on X is called strictly positive if f(x) > 0 for all x \u2208 X+ \\ {0}. A normed ordered space X is called a base norm space if X+ is closed and there exists a strictly positive linear functional u such that u(x) = ||x|| for all x \u2208 X+ [66,67]. In this case X+ \u2229 S\u2081(0) = {x \u2208 X+ | ||x|| = 1} is called the base of X, which is a convex closed, and bounded base of the cone X+ of positive elements. The matrix space M with norm (5) is a base norm space but its dual space M* with norm (7) is not.\nDefinition la. State space \u03a9 is the base of a finite-dimensional base norm space X, i.e., \u03a9 = X+ \u2229 S\u2081(0).\nLet & be a state space and let x \u2208 \u03a9. It is said that x is an extreme point of \u03a9, or equivalently that x is a pure state, if for every y, z \u2208 \u03a9 and \u03bb\u2208 (0;1) such that x = xy + (1 \u2212 1)z we have x = y = z. And a state space \u03a9 is a polytope if it has finitely many pure states.\nA positive functional, E \u2208 X, is called an effect on the state space \u03a9 if\n$$0 \\leq (E|\\omega) \\leq 1 \\text{ for all } \\omega \\in \\Omega.$$\nEffects are the basic observables in the operational theories of measurement. They can be described solely in terms of functions on the state space \u03a9, as linear space extensions of affine functionals defined on \u03a9 and valued in the segment[0, 1] :\n$$(f|\\lambda \\omega_1 + (1 - \\lambda)\\omega_2) = \\lambda\\langle f|\\omega_1\\rangle + (1 - \\lambda)\\langle f|\\omega_2\\rangle$$"}, {"title": "4.3 Instruments", "content": "Let J : X \u2192 X be a linear operator preserving the positive cone, i.e., J(X+) CX+. Its dual operator J* : X* \u2192 X* also preserves positivity: let x \u2208 X+, f \u2208 X*, then \u3008J*f|x\u3009 = \u3008f|Jx) \u2265 0, since Jx \u2265 0.\nLet A = (\u03b1\u03ba, \u0395k) be an observable with the discrete set of outcomes VA = (ak) and the family of effects (Ek) and let, for each outcome \u03b1 \u2208 VA, there is defined a linear operator J(a) : X \u2192 X preserving the positive cone X+. It is assumed that the family of maps (J(a)) is constrained as\n$$J(V_A) = \\sum_\\alpha J(\\alpha) : \\Omega \\to \\Omega.$$"}, {"title": "4.4 Conditional and sequential joint probability distributions", "content": "Within GPT, operational measurement theory, we can define conditional probability [60,69,70]. Let (JA(a)) and (JB(\u03b2)) be two instruments generating the observables A and B. The probability to obtain the output A = a under the condition that the output B = \u03b2 was obtained in the preceding measurement of observable B is given by the formula:\n$$P(A = \\alpha|B = \\beta,\\omega) = \\frac{P(A = \\alpha \\omega{B=\\beta})}{P(B = \\beta|\\omega)} = \\frac{\\langle E_A(\\alpha)|\\omega{B=\\beta}\\rangle}{(u|J_B(\\beta)\\omega)} = \\frac{(u|J_A(\\alpha)\\omega{B=\\beta})}{(u|J_B(\\beta)\\omega)} = \\frac{(u|J_A(\\alpha)J_B(\\beta)\\omega)}{(u|J_B(\\beta)\\omega)}.$$\nBy using conditional probability we define the sequential joint probability\n$$P_{BA}(\\beta, \\alpha|\\omega) = P(B = \\beta|\\omega)P(A = \\alpha|B = \\beta,\\omega) = (u|J_A(\\alpha)J_B(\\beta)\\omega).$$\nSequential measurement of observables A1,..., An corresponding to instruments JA, generates the joint probability distribution\n$$P_{A_1...A_n} (a_1, ..., a_n) = (U|J_{A_n} (a_n)...J_{A_1} (a_1)\\omega).$$"}, {"title": "4.5 Order effect", "content": "The question order effect is well studied in psychology and sociology, e.g., for social opinion pools [65]. In classical probability theory the order effect is absent. In [94,95] it was represented in the quantum-like framework based on the projection type (von Neumann [93]) state updates. But then in [53] it was shown that such projection description can't be combined with another psychological effect, the response"}, {"title": "4.6 Interference effect", "content": "We remark that the interference of probabilities is one of the basic quantum effects; for example, this effect can be observed in the two slit experiment (as was emphasized by Feynman [34,35], see also [48]). In cognitive science and decision making it was analyzed in [50] as a nonclassical property of cognitive information processing, its coupling with the disjunction effect in cognitive psychology was established in [19]. We remark that in classical probability theory probabilities don't interfere.\nAs was explained in [48], quantum-like interference of probabilities can be written as the violation of one of the basic laws of classical probability theory, the formula of total probability (FTP). For dichotomous observables A = a1, a2 and B = B1, B2, FTP has the form:\n$$P(A = \\alpha) = \\sum_\\beta P(B = \\beta)P(A = a|B = \\beta).$$\nIn classical probability the role of the state is played by the probability measure of the Kolmogorov probability space, to match our formalism denote this Kolmogorovian probability by the symbol w, then FTP is written as\n$$P(A = \\alpha|\\omega) = \\sum_\\beta P(B = \\beta|\\omega)P(A = \\alpha|B = \\beta,\\omega).$$\nIn an operational measurement model the left-hand side of FTP equals\n$$P(A = \\alpha|\\omega) = (u|J_A(a)\\omega)$$\nand the right-hand side of FTP equals\n$$\\sum_\\beta P(B = \\beta|\\omega)P(A = \\alpha|B = \\beta,\\omega) = \\sum_\\beta P_{BA}(\\beta, \\alpha|\\omega) = \\sum_\\beta(u|J_A(\\alpha)J_B(\\beta)\\omega) = (U|J_A(a)J_B(V_B)\\omega).$$"}, {"title": "4.7 Repeatable measurements", "content": "Consider observable A = (a, Ea) based on instrument J such that for some a, J(a) is a positive projection that is J(a) > 0 and J(a)2 = J(a). Then, for any state w, P(A = a|A = a,w) = P(A = a|w{A=a}) = (u|J(a)w{A=a}} = {u|J(a)2w}/{u|J(a)w) = 1. We also have J(VA) = \u2211aJ(a) : \u03a9 \u2192 \u03a9. Thus, \u03a3\u03b1P(A = \u03b1|\u0391 = \u03b1,\u03c9) = (u|J(VA)w{A=a}) = 1. Consequently,\n$$P(A = \\alpha|A = a,\\omega) = \\delta_{a,\\alpha}$$\nHence, for the a-outcome the measurement is repeatable. However, if, for some a, J(a) is not projection, i.e., J(a)2 \u2260 J(a), then generally\n$$P(A = a|A = \\alpha,\\omega) = \\frac{(u|J(a)^2w)}{(u|J(a)w)} \\ne 1.$$\nWe remark that in classical probability theory all measurements (they are given by random variables) are repeatable; by the Bayes formula for conditional probability, P(A = a|A = \u03b1,\u03c9) = P(A = \u03b1, \u0391 = \u03b1\u03c9)/P(A = a|w) = da,a. We also remark that in quantum probability theory the projection type measurements are repeatable."}, {"title": "5 Model 1: Order, interference, and repeatability effects", "content": "In matrix space M2 consider observables A and B with the values 01, 02 and B1, B2 with instruments realized as multiplication by matrices:\n$$J_A(a_1) = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{pmatrix}, J_A(a_2) = \\begin{pmatrix}\n0 & 0 \\\\\n0 & 1\n\\end{pmatrix}; J_B(\\beta_1)= \\begin{pmatrix}\n1 & 0 \\\\\n1 & 0\n\\end{pmatrix}, J_B(\\beta_2) = \\begin{pmatrix}\n0 & 1 \\\\\n0 & 1\n\\end{pmatrix}.$$"}, {"title": "6 Model 1: Compound systems, tensor product representation", "content": "Consider now two neuronal networks Sk, k = 1,2, containing Nk neurons respectively, and consider the compound system S = (S1, S2). In quantum theory and GPT this system should be described by the tensor product construction based on the linear space M = MN1N2 = MN1 \u2297 MN2. How can one couple this tensor product construction to our neuronal networks model? Straightforwardly one treats S as the network with NA + NB communicating neurons and represents S by matrix of the size (N\u2081 + N\u2081)2, and not of the size (N1N2)2 as GPT requieres. The essence of our further consideration is in understanding that the state of a compound system should be interpreted differently from the state of a single system. This is one of the consequences the studies on so called prequantum classical statistical field theory [52], an attempt to model quantum phenomena with classical random fields. In this article we proceed with this ideology which matches well with our neuronal model.\nWe turn to the concrete representation of the weights wij via averages of the classical random variables \u0123ij, see (1), (2). Denote these variables for networks S1, S2 as $\\xi_{ij}^{(1)}, \\xi_{km}^{(2)}$. Consider the covariance matrix for these variables\n$$w_{ijkm} = \\frac{E[\\xi_{ij}^{(1)}\\xi_{km}^{(2)}]}{N_1 N_2}.$$"}, {"title": "7 Concluding remarks", "content": "As was pointed out, quantum-like modeling is actively developed as a phenomenological approach. But it really cries for coupling with brain's functioning, with activity of the networks of communicating neurons. In this article the basics of such coupling were established within the GPT model based on the representation of networks by ordered weighted graphs that in turn are described by the matrices of weights for intensities of signaling between neurons. The graph-matrix GPT model (Model 1) has classical state space, a polygon, and its quantum-like properties are generated by the calculus of measurement instruments.\nWe presented some quantum-like effects, the order, interference (disjunction), and non-repeatability effects, within the graph-matrix GPT model. The compound systems, correlated neuronal networks, are considered only briefly and we plan to study quantum-like properties related to entanglement of the states of neuronal networks in one of future publications."}]}