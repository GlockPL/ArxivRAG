{"title": "MVREC: A General Few-shot Defect Classification Model Using Multi-View Region-Context", "authors": ["Shuai LYU", "Fangjian Liao", "Zeqi Ma", "Rongchen Zhang", "Dongmei Mo", "Waikeung Wong"], "abstract": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality control within industrial manufacturing. However, current FSDMC research often lacks generalizability due to its focus on specific datasets. Additionally, defect classification heavily relies on contextual information within images, and existing methods fall short of effectively extracting this information. To address these challenges, we propose a general FSDMC framework called MVREC, which offers two primary advantages: (1) MVREC extracts general features for defect instances by incorporating the pre-trained AlphaCLIP model. (2) It utilizes a region-context framework to enhance defect features by leveraging mask region input and multi-view context augmentation. Furthermore, Few-shot Zip-Adapter(-F) classifiers within the model are introduced to cache the visual features of the support set and perform few-shot classification. We also introduce MVTec-FS, a new FS-DMC benchmark based on MVTec AD, which includes 1228 defect images with instance-level mask annotations and 46 defect types. Extensive experiments conducted on MVTec-FS and four additional datasets demonstrate its effectiveness in general defect classification and its ability to incorporate contextual information to improve classification performance.", "sections": [{"title": "Introduction", "content": "Defect detection and classification is a critical challenge in industrial manufacturing, as it involves identifying and categorizing defects within work-pieces. High-precision defect classification not only ensures the safety and reliability of products but also enhances work efficiency. However, in practical applications, the diversity of defect types and the low frequency of defect occurrences make it a particularly difficult task.\nWhile Few-shot Learning (FSL) has gained traction in general vision tasks like mini-Imagenet, its application to defect multi-classification (FSDMC) remains challenging. This disparity is evident in the limited availability of dedicated datasets and research focusing on FSDMC. Although Contrastive Vision-Language Pre-training (CLIP) has demonstrated remarkable success in learning visual features from large-scale image-text pairs and adapting to downstream tasks with few-shot learning, this type of application is nearly absent in the context of FSDMC. This is primarily due to the significant domain gap between general vision tasks and FSDMC. Secondly, defects inherently differ from normal surface areas, necessitating more contextual information for effective detection and classification. However, common classification models often involve cropping the defect region, resizing it to the model input size, and feeding it into a network, as shown in Figure 1 (a). This pretreatment fails to retain important contextual information, such as the surrounding background and the size of the defect. The most popular multi-category datasets with different product images are typically designed for anomaly detection rather than defect classification. Although the field of few-shot defect multi-classification has attracted considerable research attention, the datasets used, such as the NEU-DET Dataset and the MTD Dataset, are limited by their focus on a single product category. There is a notable scarcity of multi-category datasets specifically proposed for FSDMC.\nTo mitigate these issues, we propose a general few-shot defect classification model using a multi-view region-context approach, called MVREC. Specifically, our approach begins by generating region-context visual features for the defect instance using the AlphaCLIP model, a transformer-based model that takes a defect image and its mask context as input to generate visual features from the masked region. By incorporating the mask region context, the network can perceive both the defect foreground region and its surrounding background, generating target-specific features while maintaining input consistency. Furthermore, we propose a multi-view augmentation technique to generate multi-view features for a defect, maximizing the utility of few-shot samples and enhancing generalization ability. The multi-view region-context (MVREC) features can be extracted from the multi-view patches and masks of the defect instance, thereby enhancing the region-context features. Moreover, we propose two few-shot classifiers: the training-free Zip-Adapter, which predicts directly without training, and the fine-tuning Zip-Adapter-F, which adapts the MVREC features for better performance. Zip-Adapter and Zip-Adapter-F share the same structure, consisting of a Zero-initialized Projection (ZIP) module and a Scale-Dot-Product Attention (SDPA) module. Specifically, they store visual features and corresponding class labels from the support set images as key-value pairs. The SDPA module then calculates the visual feature similarity between the query defect instance and the support defects, outputting the classification logits through the weighted sum of the encoded labels from the support set. The ZIP module serves as an identity mapping and feature adapter, respectively, for Zip-Adapter and Zip-Adapter-F. Furthermore, we propose MVTec-FS, based on MVTec AD, to create a multi-category dataset suitable for the FSDMC task. This dataset features a diverse array of defect types and a balanced distribution. MVTec-FS includes 15 categories of product surface images and approximately 46 types of defects, making it a promising new benchmark in this field.\nWe tested MVREC across MVTec-FS and four other public defect datasets with classification annotations. Our results demonstrate superior performance in few-shot defect classification, outperforming existing models. In summary, our contributions can be outlined as follows:\n(1) We employ AlphaCLIP to extract general features from each defect instance, enhancing model generalizability, and design a new region-context-based defect classification framework that fully incorporates contextual information for more accurate defect classification.\n(2) We introduce multi-view context augmentation and Zip-Adapter(-F) classifiers for few-shot classification.\n(3) We reconstruct the popular MVTec AD dataset into a new FSDMC benchmark named MVTec-FS.\n(4) We conducted extensive experiments on multiple defect datasets, demonstrating the effectiveness of MVREC."}, {"title": "Related work", "content": "Various models, including object detection, segmentation, and classification, have been applied to defect detection. In recent years, the MVTec AD dataset has been widely studied for anomaly detection tasks. These models learn from normal samples to identify anomalies. However, defect classification, which involves identifying specific defect types, is more challenging due to the rarity and diversity of defects and the limited availability of relevant datasets. FSDMC models, such as CAO, Fabric and FANet, have been proposed. However, these methods are often dataset-specific and require complex training processes, including meta-learning and metric learning. Additionally, using a subset of defect types as base classes to train a base model and then evaluating novel classes is common, but this approach may not be practical for real-world applications."}, {"title": "Clip-based Few-shot Classification", "content": "The most common few-shot methods include meta-learning and metric learning. Meta-learning methods learn a model that can quickly adapt to new tasks with minimal training data. Metric learning methods learn a distance metric that can effectively measure the similarity between samples. Recently, large language models and multi-modal pre-training models, such as GPT and CLIP, have emerged as powerful tools. Related research has been applied to defect detection tasks, showing impressive performance. Numerous adapter-based methods have been proposed to adapt the CLIP model to specific tasks with few samples, such as CLIP-Adapter , Tip-Adapter, CoOp and SuS-X, but most of these methods are designed to jointly learn image and text features for general vision tasks."}, {"title": "Region-Context-based Models", "content": "Traditional classification networks typically evolve by cropping the defect region and resizing it, without explicitly utilizing the region context. When defects vary in size, as shown in the example in Figure 1, cropping and resizing the defect region may result in the loss of crucial contextual information. To address this issue, region-context models incorporate region context as a prompt to predict target information, thereby preserving the contextual information of the target. For example, the SAM network uses prompts in the form of points and bounding boxes. To enable CLIP to focus on specific regions within the entire image, various methods have been explored. AlphaCLIP is an innovative enhancement of the CLIP model, designed to improve its ability to focus attention on specific regions. This architecture enables AlphaCLIP to provide precise control over the emphasis of image content."}, {"title": "MVREC", "content": "In this chapter, we introduce the MVREC feature extraction module and present the training-free Zip-Adapter classifier, along with its fine-tuning version, Zip-Adapter-F."}, {"title": "Multi-View Region-Context Feature Extraction", "content": "We first introduce the multi-view region-context (MVREC) feature extraction process for defect instances. To effectively capture the visual representation of defects and explicitly mine contextual information, we employ the pre-trained AlphaCLIP model to extract visual features from images using their mask prompts. Given the small data volume characteristic of few-shot learning tasks, we utilize multi-view context augmentation to generate multi-view patches of defect images, thereby expanding the available dataset for subsequent processing. Specifically, we employ two context augmentation methods to achieve this. The first method, multi-scale augmentation, involves cropping $Num_{scale}$ patches at different scales from the defect patches and their corresponding masks, centered on the defect. The second method involves offsetting the center of the defect to generate $Num_{offset}$ defect patches with different offsets at each scale. By applying these two augmentation methods, a total of $V = Num_{scale} \\times Num_{offset}$ patches can be obtained for each defect instance. The AlphaCLIP model extracts MVREC patch embeddings $E \\in \\mathbb{R}^{V \\times C}$ from the multi-view patches, where $C$ is the number of feature channels. These embeddings are then averaged to produce a single MVREC feature $F\\in \\mathbb{R}^{C}$."}, {"title": "Support set MVREC Feature Extraction and Class Label Encoding", "content": "For N-way K-shot classification tasks, the MVREC patch embeddings $E_{SUPP} \\in \\mathbb{R}^{NK \\times V \\times C}$ and MVREC features $F_{SUPP} \\in \\mathbb{R}^{NKC}$ are first extracted for the support set. Then, the one-hot encoded class labels $Y_{SUPP} \\in \\mathbb{R}^{NK \\times N}$ are extracted. The MVREC features $F_{SUPP}$ and $Y_{SUPP}$ are used to build the cached key-value pairs for the FSDMC task. Additionally, the MVREC patch embeddings $E_{SUPP}$ and the one-hot encoded labels $Y_{SUPP}$ are used as training data to fine-tune the Zip-Adapter."}, {"title": "Training-free Zip-Adapter Classifier", "content": "In this section, we introduce the method of utilizing MVREC visual features to construct the zero-initialized projection classifier (Zip-Adapter) for FSDMC tasks. The Zip-Adapter classifier consists of a zero-initialized projection (ZIP) module and a scaled dot-product attention (SDPA) module. It stores the MVREC features $F_{SUPP}$ along with the encoded labels $Y_{SUPP}$ of the support set samples. The ZIP module includes a single linear layer, a residual connection, and a SiLU activation function, with the linear layer initialized to zeros. The output of the ZIP module is the adapted feature F', generated as follows:\n$F' = SiLU (Linear (F)) + F,$\nhere, F represents the MVREC feature for either the support sample or the query sample. In the Zip-Adapter, the ZIP module is designed to serve as an identity transformation by initializing the linear layer with zeros and using a residual connection. The SDPA module is a scaled dot-product attention mechanism, which calculates the visual similarity between the query defect instance and the support set. It then produces the classification logits by performing a weighted sum of the support encoded labels $Y_{SUPP}$. The SDPA module is defined as follows:\n$logits_{query} = Y_{SUPP} \\cdot \\psi (Sim (F_{query}, F_{SUPP})),$ \nwhere the $\\psi$ is the activation function for modulating the cosine similarity:\n$\\psi(x) = exp(-\\beta(1 - x)),$\n$\\beta$ controls the sharpness of the curve. And $Sim$ is the cosine similarity function. The output of the SDPA module, $logits_{query}$, represents the classification logits of the query defect instance. The class with the highest logit value is identified as the predicted class."}, {"title": "Training Zip-Adapter-F classifier", "content": "Zip-Adapter-F enhances the visual features for better performance by fine-tuning the Zip-Adapter classifier, making both the ZIP module and the cached visual features of the support set learnable. Our Zip-Adapter-F combines a cache-based mechanism with an adapter-based mechanism, using the Zip-Adapter as the base model. The fine-tuning process involves two training objectives: (1) optimizing the cross-entropy (CE) loss between the predicted logits $logits_{query}$ and the labels $Y_{query}$. The CE loss $L_{CE}$ is defined as:\n$L_{CE}(logits_{query}, Y_{query}) = -\\Sigma y_i log(p_i),$\nwhere $y_i$ and $p_i$ represents the label and predicted probability distribution for class i.\nThe second part uses the triplet loss to optimize the intra-class compactness and inter-class separability of the adapted feature $f_{adapted}$ of the ZIP module within a batch. The triplet loss $L_{triplet}$ is defined as:\n$L_{triplet}(F_{query}) = max(d(F_{anchor}, F_{positive}) - d(F_{anchor}, F_{negative}) + \\alpha, 0),$\nwhere $F'_{anchor}$, $F'_{positive}$, and $F'_{negative}$ are the embeddings (feature vectors) of an anchor sample, a positive sample (same class as an anchor), and a negative sample (different class from anchor), within a batch, respectively. $d(\\cdot, \\cdot)$ is a distance function used to measure the similarity between embeddings. $\\alpha$ is a margin hyperparameter that specifies the minimum difference between the distances of positive and negative pairs required for the loss to be zero. The overall loss function for finetuning Zip-Adapter-F is:\n$L_{Zip-Adapter-F} = L_{CE} + \\lambda \\cdot L_{triplet},$\nwhere $\\lambda$ is a hyperparameter that balances the importance of the two parts in the overall loss. After Zip-Adapter-F is trained, it can be used to classify query defect instances similar to the Zip-Adapter classifier."}, {"title": "MVTec-FS Dataset", "content": "Although few-shot defect multi-classification has garnered considerable research attention, datasets like NEU-DET and MTD are limited to a single product category. Recently, anomaly detection has also gained attention, with several multi-category datasets being proposed. However, these datasets are not designed for defect classification. The MVTec AD dataset, the most popular benchmark for anomaly detection, features 15 product categories (5 textiles and 10 objects), offering significant diversity and generalization. In its original configuration, the training set consists of normal images, while the testing set includes both normal and defect images, with defect images labeled by masks. This dataset contains about 47 defect types (ranging from 8 to 26 images per type), making it suitable for FSDMC tasks. However, FSDMC tasks have rarely been studied on this dataset.\nWe selected 1,228 defective images from the MVTec AD and labeled them with instance-level masks, creating a new benchmark dataset named MVTec-FS. Since the toothbrush category contains only one defect type, it was excluded from MVTec-FS. The number of defect instances per defect type is presented, totaling 46 types with instances ranging from 9 to 58, as shown in Fig 4. The original annotations were at the image level and did not account for multiple defects within a single image. We used the connected component algorithm to convert image-level masks to instance-level masks, followed by necessary human corrections. Some examples are shown in Figure 3. For each defect type, 50% of the defects are used as the training set to sample the support set, while the remaining 50% constitute the testing set (query set)."}, {"title": "Experiments", "content": "We conducted experiments on the MVTec-FS dataset and four other datasets to evaluate our MVREC using accuracy metrics. The few-shot setup is defined as N-way K-shot, where K is set to 1, 3, or 5. The support set is sampled from the training set, and the query set consists of all images in the testing set. The evaluation was conducted on the query set for each of the five sampled support sets, and the average classification accuracy was calculated to provide a more robust assessment. Ablation studies were performed on the MVTec-FS dataset to assess the effectiveness of the various components. For AlphaCLIP, we selected the ViT-L/14 backbone. The MVREC visual feature used three scales, representing the commonly used large, medium, and small settings. The number of offsets was set to 9, based on a grid layout similar to a tic-tac-toe board. We set \u1e9e to 32 for the Zip-Adapter and 1 for the Zip-Adapter-F classifiers. When training the Zip-Adapter-F, we used the AdamW optimizer with a learning rate of 0.0001. The model was updated for 500 iterations, training on all MVREC features of the support set in each iteration. For the triplet loss item, the hyperparameters \u03b1 and \u03bb were set to 0.5 and 4, respectively.\nIn our experiment, we evaluated a variety of baseline classifiers based on the AlphaCLIP backbone, including:\n1. CLIP-ZeroShot : This approach leverages the zero-shot capability of the CLIP model. We generated text embeddings for each class description and computed the similarity between the test image embeddings and these text embeddings, classifying them based on the highest similarity. 2. CLIP-KNN: This method uses the K-Nearest Neighbors algorithm with the CLIP features. The most similar K (K=1) support samples are retrieved for a query sample, and the class with the majority vote is selected as the prediction. 3. CLIP-ProtoNet: This approach builds a Prototypical Network on top of the CLIP, where proxy features representing each class are calculated from the support set, and test images are classified based on their similarity to these class proxies. 4. CLIP-Adapter : This method involves adding adapter layers on top of the CLIP. These layers are trained for the new classification task, adjusting the image features accordingly. 5. Tip-Adapter : This method constructs a key-value cache model using CLIP-extracted features from the few-shot data and performs recognition in a retrieval-based manner. Tip-Adapter-F treats the visual cache as learnable parameters and optimizes them to improve performance."}, {"title": "Results on MVTec-FS Dataset", "content": "In Table 1, we present the classification accuracy (%) of various few-shot models evaluated on the MVTec-FS dataset under different few-shot learning configurations. To ensure a fair comparison, we report results across several few-shot settings, specifically with 0, 1, 3, and 5 shots. The accuracies for 14 product categories, as well as the average accuracy, are reported in separate columns. As shown in the table, our MVREC demonstrates outstanding performance in all few-shot setups, regardless of whether Zip-Adapter or Zip-Adapter-F is used. The Zip-Adapter-F achieves the highest accuracy of 89.4% with 5 shots, which is 6.9% higher than the second-best, LinearProb. By comparing different product categories, it is evident that the Zip-Adapter-F achieves the highest accuracy in most categories, demonstrating its effectiveness in few-shot learning scenarios."}, {"title": "Ablation Study", "content": "First, we assess the effectiveness of the MVREC feature and Zip-Adapter(-F) classifiers by comparing their performance to other classifiers, regardless of MVREC usage. As shown in Table 2, the MVREC feature consistently boosts the performance of all classifiers across different few-shot settings, with the most significant gain of 11.6% in CLIP-Adapter with 1-shot, demonstrating its general effectiveness for few-shot defect classification. Zip-Adapter-F consistently outperforms most classifiers, regardless of MVREC use, highlighting its inherent strength. When combined with MVREC, Zip-Adapter-F achieves the best results across all few-shot settings, maximizing its potential and making it the most effective approach for FSDMC. Notably, Zip-Adapter and Tip-Adapter yield identical results before training, as they are mathematically equivalent at that stage.\nMask Region-Context. We investigate the impact of the mask region-context on model performance. As previously mentioned, the mask region-context helps the model focus on the defect instance without cropping the region based on defect size, which could otherwise result in the loss of contextual information. We evaluate two scenarios where the mask region-context is removed: (1) using a whole-foreground mask as the region-context input to AlphaCLIP, and (2) using CLIP without any mask region-context. The results, shown in Table 3, demonstrate that removing the mask context leads to a noticeable decrease in accuracy. We also consider the impact of different cropping styles. When cropping by defect size and using vanilla CLIP, the worst results are obtained, further emphasizing the importance of mask region-context. Cropping by fixed size and using AlphaCLIP as the feature extractor achieves the best performance, highlighting the effectiveness of MVREC.\nMulti-View Context Augmentation. From the results in Table 3, we observe that different augmentation methods have varying impacts on classification accuracy. When single augmentations are used, multi-scale, multi-offset, and multi-rotation augmentations show significant improvements in both Zip-Adapter and Zip-Adapter-F. When double augmentations are applied, the combination of multi-scale and multi-offset yields the best results, indicating that these augmentations are complementary and can be combined to achieve better performance. Multi-scale augmentation allows the model to learn features at various resolutions, which is crucial for capturing both fine and coarse details in the images. Meanwhile, multi-offset augmentation helps the model learn robust features by shifting the image and mask context, thereby improving the model's robustness."}, {"title": "Visualization", "content": "To better illustrate the function of MVREC, we used t-SNE to visualize the support MVREC features in Zip-Adapter-F, as shown in Figure 5. Different colors represent 5 defect classes from the 5-shot leather images of the MVTec-FS. The changes in the distribution indicate that multi-view augmentation and fine-tuning help the model learn more discriminative features."}, {"title": "Comparison on Other Datasets", "content": "We evaluated MVREC on several public datasets, including: 1) NEU-DET, a metal surface defect dataset for detection model research; 2) PCB Defect Dataset , released by The Open Lab on Human-Robot Interaction of Peking University; 3) Magnetic Tile Surface Defects (MTD) , which contains 6 common magnetic tile defects; and 4) AITEX Fabric Defect, a fabric defect dataset with 12 types of defects, from which seven defect types with at least 10 samples are selected. For each dataset, 50% of the data is used as the training set for sampling the support set, and the other 50% is used as the testing set (query set). In addition to 1, 3, and 5 shots, we also evaluated performance with 10, 15, and 20 shots on the NEU-DET, PCB Defect, and MTD datasets for a more comprehensive comparison. The results, in Figure 6, demonstrate that Zip-Adapter-F (MVREC) achieves the best performance on all datasets, with performance improving as the number of shots increases."}, {"title": "Discussion and Conclusion", "content": "This paper introduces MVREC, an instance-level few-shot classification approach that can be applied to various labeled formats, such as bounding boxes and masks. Extensive experiments on five datasets demonstrate that it is a versatile and effective approach for FSDMC.\nLimitations. First, we have not yet explored the unified model that can be developed to handle different defect datasets after a single training session. Second, our study mainly focuses on using image features extracted by CLIP, without exploring the potential of CLIP's text encoder for multi-model research. We hope this work inspires future research and the development of more advanced methods."}, {"title": "Appendix", "content": "In creating the MVTec-FS dataset, we began by selecting all 1,228 anomaly images and their corresponding masks from the testing set of the MVTec AD dataset. The original anomaly masks were annotated at the image level, which is a coarse form of labeling. For instance, as shown in Figure 7, when multiple different types of anomalies appear in the same image, they are labeled as a single \"combined\" type. Additionally, when multiple defects of the same type appear in an image, the image-level mask is treated as a single instance. Given these problems, we refined the mask labels by converting them into instance-level defect masks using the connected component algorithm, assigning a class label to each defect instance. Subsequently, we manually reviewed and adjusted the defect instance labels to ensure accurate labeling of each defect instance. Figure 7 illustrates examples of these label modifications. Specifically, the label modification involved two main actions: 1) verifying and revising the defect instance masks, and 2) checking and correcting the instance class labels. For the \"combined\" type, where multiple defect instances exist within a single image, we modified the instance class labels to ensure that each defect instance was labeled correctly.\nThe MVTec-FS dataset, as summarized in Table 7, showcases a diverse collection of sub-datasets, each representing different product categories with varying numbers of anomaly types and defect instances. Across these subdatasets, the number of anomaly categories ranges from 3 to 7, reflecting the distinct defect characteristics of each product type. Notably, each sub-dataset within MVTec-FS is carefully constructed to ensure that there are at least five instances of each anomaly type in the training set. This design allows for effective few-shot learning experiments, supporting scenarios where 1-shot, 3-shot, and 5-shot learning paradigms can be evaluated.\nMoreover, MVTec-FS is not only suitable for few-shot classification tasks but also serves as a valuable resource for few-shot object detection and unified multi-modal classification tasks. We hope that this dataset will stimulate further research in these areas, fostering advancements in both few-shot learning and multi-modal learning fields."}]}