{"title": "CORTEX: A COST-SENSITIVE RULE AND TREE EXTRACTION METHOD", "authors": ["Marija Kopanja", "Milo\u0161 Savi\u0107", "Luca Longo"], "abstract": "Tree-based and rule-based machine learning models play pivotal roles in explainable artificial intelligence (XAI) due to their unique ability to provide explanations in the form of tree or rule sets that are easily understandable and interpretable, making them essential for applications in which trust in model decisions is necessary. These transparent models are typically used in surrogate modeling, a post-hoc XAI approach for explaining the logic of black-box models, enabling users to comprehend and trust complex predictive systems while maintaining competitive performance. Explaining complex models can be crucial in class imbalance frameworks due to potential biases in the model being explained. Potential bias could lead to misleading interpretations and unfairness in the decision-making process. The cost-sensitive decision tree (CSDT) method belongs to the cost-sensitive learning methods that can be used in the imbalanced learning framework by considering a class-dependent cost matrix, a matrix associated with classes instead of individual samples. This study proposes Cost-Sensitive Rule and Tree Extraction (CORTEX) method, a novel rule-based XAI algorithm grounded in the multi-class CSDT method. The original version of the CSDT method is introduced only for the binary classification problems. Here, the framework is extended to classification problems with more than two classes by inducing the concept of an n-dimensional class-dependent cost matrix into the CSDT algorithm. The CORTEX provides a visual explanation through a decision tree model, a hierarchical structure composed of nodes and branches. The rules from CORTEX are created in conjunction with feature test conditions (antecedents), while the rule consequence is obtained as a class label held by the terminal node. The performance of CORTEX as a rule-extractor XAI method is compared to other post-hoc rule extraction methods across several datasets with different numbers of classes. Several quantitative evaluation metrics are employed to assess the explainability of generated rule sets. Our findings demonstrate that CORTEX, as a rule-based model-agnostic XAI method, is competitive with other tree-based methods and can be superior to other rule-based methods across different datasets. The extracted rule sets suggest the advantages of using the CORTEX method over other methods by producing smaller rule sets with shorter rules on average across datasets with a diverse number of classes. Overall, the results underscore the potential of CORTEX as a powerful XAI tool for scenarios that require the generation of clear, human-understandable rules while maintaining good predictive performance.", "sections": [{"title": "1 Introduction", "content": "Explainable artificial intelligence (XAI) is one of the fastest emerging sub-fields of AI dedicated to developing methods for making machine learning (ML) models more understandable and transparent. When data is highly non-linear and complex, sophisticated neural network models must be trained to gain knowledge and insights. For the majority of real-world application problems, the models must be as accurate as possible, regardless of their interpretability. However, the inference process of black-box models is incomprehensible to humans. Consequently, XAI techniques play a crucial role in making the inference process of these models more understandable. In application domains where comprehensibility and generalization ability are of equal importance, there is a motivation to combine the generalization ability of deep neural network models with the explainability given by XAI techniques. With an aim to extract information from already trained models, several different methods are developed for explaining their inferential process post-hoc, which means after the model has been trained. Post-hoc XAI methods do not modify the internal structure or training process of the original model being explained. Instead, they provide retrospective insights into the model's decisions. Existing XAI methods can be categorized in various ways, depending on the input data type, the machine learning model being explained, and the scope of the explanations, among others. Several studies (Longo et al., 2024; Ali et al., 2023; Vilone and Longo, 2021a; Longo et al., 2020) overview the concepts of XAI, current research and trends, and different taxonomies. For example, in (Vilone and Longo, 2021a) one of the division dimensions is the stage at which a method generates explanations. Ante-hoc XAI methods are considered naturally understandable, while post-hoc XAI methods mimic the behavior of an underlying ML model by using an external explainer. Accordingly, the CORTEX method proposed in this study is post-hoc model-agnostic, meaning it can be applied and used to explain any pre-trained ML model. In Ali et al. (2023) is a proposed taxonomy for post-hoc explainable methods, grouping them into six families, among which are attribution methods, which assume evaluating the relative significance of input features in decisions of a given ML model. Creating a surrogate model (imitation of the original model) is a post-hoc approach (Ali et al., 2023) used to clarify the AI model's decisions by approximating the original model. In line with the taxonomy, creating a surrogate model is one use of the perturbation approach, a sub-group of the attribution methods family. The proposed CORTEX can be considered a tree-based and rule-based surrogate method for any ML model, including dense neural networks. Developing a tree-based/rule-based model as an explanation for a neural network model constitutes a pedagogical or decompositional approach; the former method induces a tree model on a relabeled target variable without using internal elements of the network, in contrast with the decompositional approach in which focus is on extraction of rules separately form each unit of the neural network (Schmitz et al., 1999).\nThe proposed CORTEX builds upon the cost-sensitive decision tree (CSDT) method. As in any tree-based ML algorithm, the original CSDT proposed by Correa (2015) \u00b9 could be represented as a flowchart-like structure. That is, the method could provide a visual explanation through a decision tree model, a hierarchical structure composed of nodes and branches. Besides, every path from the root node of a tree model to any of the terminal nodes can be converted to a rule by gathering all the feature test conditions appearing in the path. In this way, the rules from CORTEX are created as a conjunction of antecedents, while the rule consequence is obtained as a class label held by the terminal node. The motivation for transforming an inherently tree-based explanation model generated from CORTEX into a set of rules arises from the observation that analyzing rules instead of following paths from the root node to a terminal node often improves comprehensibility (Freitas, 2014). Furthermore, the ability of original CSDT to generate shallower decision trees than traditional decision tree algorithms is observed in the study by Kopanja et al. (2024) and represents a motivation for developing CORTEX. However, as highlighted and explored in our previous paper (Kopanja et al., 2024), the CORTEX is only one part of a multi-faceted idea of a novel model-agnostic XAI framework (Kopanja, 2024). The transformation from a tree model into a set of rules is also essential to facilitate comparisons with other tree-based and rule-based XAI methods.\nThe experimental section compares the proposed CORTEX method to widely used XAI rule-extraction algorithms, including C4.5-PANE, REFNE, RxREN, and TREPAN. Once considered state-of-the-art in the domain, these methods are still frequently used as benchmarks in similar studies (Vilone et al., 2020; Vilone and Longo, 2021b). They are also inherently diverse, including tree-based and pure rule-based methods. The importance of comparing different XAI methods is emphasized in other studies Ribeiro et al. (2016a). The selected subset of benchmarking methods is extended with a traditional decision tree classifier, induced using Classification And Regression Trees - CART algorithm (Breiman et al., 1984), to provide a more well-rounded evaluation."}, {"title": "2 Related work", "content": "In many use cases, the model of choice for a given application will be a complex neural network model due to the high-performance capacity of these models. Nevertheless, interpretability has also been emphasized in recent years. While models with high accuracy need additional explainability, there are models with low accuracy that are still simpler to comprehend. The interpretability-accuracy trade-off is a well-known concept (Rivera-L\u00f3pez and Ceballos (2024); Ribeiro et al. (2016a); Gunning and Aha (2019); Rizzo and Longo (2018)): the more accurate the model, the less it is understandable. Tree-based models are considered Ali et al. (2023) self-interpretable and comprehensible; they explain the decisions and logically show them in a hierarchical structure illustrated as a directed graph. However, oversimplified explanation models such as shallow decision trees might not be acceptable for some stakeholders (Freitas, 2014). A single-tree model is reasonably interpretable. However, an ensemble of tree models such as Random Forest (Breiman, 2001) or other tree-ensemble models, including a cost-sensitive one, is less interpretable but can attain higher accuracy. A limited amount of literature deals with the XAI methods and cost-sensitive machine learning models on tabular data. For example, cost-sensitive CatBoost classifier and LIME explainer are utilized in (Maouche et al., 2023) to provide patient-level explanations within computer-aided prognosis systems for breast cancer metastasis prediction. In the study (Kopanja et al., 2023) is a proposed extension of SHAP method (Lundberg and Lee, 2017) for tree-based cost-sensitive models, and in their later work (Kopanja et al., 2024) the proposed methodology is extended on cost-sensitive ensemble models.\nIn the literature, few articles deal solely with interpretable machine-learning models for imbalanced tabular data. Some recent studies (Mustari et al., 2023; Dablain et al., 2024) have attempted to provide explanations for convolutional neural networks trained on medical images. Having an imbalanced dataset could lead to a biased ML model. Hence, providing explanations might be crucial for such models to ensure fairness, which means the model makes unbiased decisions without favoring any class in the data distribution. The problem of biased inferences is attempted to be tackled through the General Data Protection Regulation (GDPR) that sets out the right to obtain an explanation of the automated decision-making by ML model (Vilone and Longo, 2020). Explaining complex models in class imbalance frameworks due to potential biases in the model being explained could prevent misleading interpretations and unfairness in the decision-making process. Therefore, different XAI tools could enable insights into the biased ML models to improve fairness, transparency, and accountability of the model's decision-making process.\nPost-hoc explainability is more critical for domain experts and end-users who are more interested in getting answers on how and why ML model arrived at a particular prediction and the key features that led to the conclusion. One of the popular post-hoc approaches is creating a surrogate model, the approximation of the underlying black-box model. In the surrogate approach, the aim is to represent the relationship between input data and the output of the neural network model without information on the internal configuration of the network. Surrogate models can be created globally or locally (Ali et al., 2023), where global surrogate models aim to provide an explanation for the model as a whole, and the local surrogate model provides an explanation for a single instance. For example, LIME (Ribeiro et al., 2016b) and other versions of this approach (Shankaranarayana and Runje, 2019; Ranjbar and Safabakhsh, 2022) are local surrogate models, meaning these methods generate local explanations for individual samples of black-box ML models. Another popular post-hoc method, which can provide both local and global explanations, is the SHAP method proposed by Lundberg and Lee (2017). The SHAP is based on Shapley values of a conditional expectation function of the ML model. Both model-specific and model-agnostic versions of the SHAP have been proposed. Lundberg et al. (2020) proposed TreeSHAP, a unique model-specific SHAP approximation for tree-based models. AraucanaXAI (Parimbelli et al., 2023) is post-hoc surrogate approach where classification and regression trees (based on CART algorithm (Breiman et al., 1984)), are locally-fitted to provide explanations of the prediction of a complex ML model. Comparative evaluation of AraucanaXAI and other local XAI methods, including LIME and SHAP, showed the advantages of AraucanaXAI in terms of high fidelity and ability to deal with non-linear decision boundaries. A combination of decision tree and differential evolution algorithms is proposed in ((Rivera-L\u00f3pez and Ceballos, 2024)) to develop an evolutionary approach for inducing univariate decision trees that effectively explain the predictions of black-box models. Decision-tree-based surrogate models proved valuable for explaining outliers detected by unsupervised anomaly detection models (Savi\u0107 et al., 2022).\nComplex deep neural network models can discover complex structures in the data. Still, the learned patterns are hidden knowledge without explicit rules for finding them Ali et al. (2023). Several approaches have been proposed to explain deep learning classification models, including using decision tree methods as surrogate models and extracting rule sets from the resulting tree (Mekonnen et al., 2023). Tree-based algorithm C4.5Rule-PANE (Zhou and Jiang, 2003) is an extension of a C4.5 decision tree algorithm, capable of extracting if-then rules from ensembles of neural networks, and its performance is compared to other rule-extractors in several studies (Vilone et al., 2020; Vilone and Longo, 2021b). Rule Extraction From Neural Network Ensemble (REFNE) was developed to extract symbolic rules from neural networks (Zhou et al., 2003). Another rule-based method for enhancing the interpretability of neural networks by translating their complex operations into human-understandable rules is Rule Extraction by Reverse Engineering (RxREN) (Augasta and Kathirvalavakumar, 2011). The RxREN relies on a reverse engineering technique to extract rules from neural networks. Researchers (Augasta and Kathirvalavakumar, 2011) have shown that RxREN efficiently prunes insignificant input neurons from the trained neural network models. Rule extraction is a learning problem in the TREPAN (Craven and Shavlik, 1994) method that generates a decision tree by querying the underlying network using a query and sampling approach. In (Martens et al., 2007), the performance of TREPAN and C4.5 as rule extractors is assessed using fidelity, correctness, and a number of rules on support vector machine models. The results show that TREPAN obtained the best average performance and consistently outperformed C4.5 with comparable comprehensibility that was more computationally demanding. Another rule-based XAI approach is Anchors (Ribeiro et al., 2018), where intuitive and easy-to-comprehend if-than rules are generated.\nDecision trees and rule sets are two different representation types of explanations that are easily understandable and interpretable for humans (Guidotti et al., 2018). Nevertheless, both decision trees and rules have their drawbacks (Ribeiro et al., 2016a) related to their graphical and textual representations, respectively. Unlike a decision tree, where a hierarchical structure provides information about feature importance, the importance of a feature is unknown in the textual representation of rules. Users' tolerance for the same explanation type can differ; for example, the number of rules considered too large for some users can be acceptable for others (Freitas, 2014). However, some stakeholders might prefer rules as an explanation type instead of the decision tree, and others, depending on their background, can favor decision trees as more comprehensible (Ribeiro et al., 2016a). Additional experimental studies should be conducted to compare user preferences for various representations of explanations, as none can be considered the best for all applications.\nWith many XAI methods available, the evaluation process of generated explanations enables practical assessment based on criteria such as transparency, fidelity, robustness, and usability. Different metrics measure different aspects of explanations depending on the type of explanation. For example, suppose explanations are generated in the form of rules. In that case, evaluation metrics such as a number of rules or an average number of antecedents in the rule can be used (Vilone et al., 2020). In the study by Ribeiro et al. (2016a), it is argued that average rule length is a fair measure of the simplicity of a rule set. The small input changes should not significantly affect the AI system's behavior. Hence, robustness represents one of the critical measures of the XAI method, where robustness refers to the sensitivity of the AI-driven model output to a change in the input. Beyond these metrics, other quantitative validation factors must be fulfilled by every type of explanation automatically generated by an XAI method (Vilone and Longo, 2021b), including the correctness measured as the portion of samples correctly classified by a given XAI method.\nThe effectiveness of explanations generated by some XAI methods must be assessed according to how the explanations aid human users. Therefore, there is an implicit requirement for human-in-the-loop evaluation of AI-driven system reasoning. Apart from quantitative comprehensive assessment, the AI systems can also be validated by users (Ali et al., 2023). Taxonomy for evaluation of XAI methods is divided into computer-centred and human-centred by Lopes et al. (2022), where the former involves methods to obtain a measure of interpretability and fidelity to evaluate the quality of explanations. In contrast, the letter consists of conducting user experiments with human subjects. Human-centred evaluation of explanations in the form of if-than rules is undertaken in the study (Huysmans et al., 2011). In a more recent study, Dragoni et al. (2020) created a questionnaire to collect feedback from participants on the persuasiveness of automatically generated explanations. Similarly, Vilone and Longo (2023) have developed and evaluated a novel questionnaire designed to assess the explanations generated by XAI methods reliably. The questionnaire is based on close-ended questions for testing rule-based explanations and tested on argument-based and decision-tree explanations of deep neural networks trained on three datasets over six groups of human participants. Another human-centred study (Ribeiro et al., 2018) showed that XAI methods could enable users' understanding of the model's behavior measured by users predicting how a model would behave on unseen samples with less effort and higher precision.\nWhile significant progress has been made within the XAI field in developing new methods and evaluation frameworks, many challenges remain, particularly in existing post-hoc model-agnostic XAI approaches. Surrogate models created by tree-based and rule-based methods can have good predictive capacity at the expense of extensive and, therefore, ineffective rule sets. In this study, the proposed CORTEX method aims to produce smaller sets of rules with shorter rules without substantially decreasing its predictive performance. In the following section, the CORTEX method is briefly described."}, {"title": "3 Cost-Sensitive Rule and Tree Extraction Method (CORTEX)", "content": "Cost-sensitive learning is supervised learning that can alleviate the class imbalance problem in many applications. The class imbalance problem has been a recognized problem in the ML community for decades (Sun et al., 2011), and it can be defined as a problem where the number of samples across classes is not even. A popular measure of class imbalance is the class imbalance ratio, which in the binary classification framework can be expressed as the ratio of the majority class (the class with a more significant number of samples) to the minority class (the class with a smaller number of samples) size. In general, it is impossible to explicitly define an imbalance ratio that would deteriorate a classifier's performance due to other factors, such as small sample size or presence of sub-concepts, that could affect the performance (Sun et al., 2011).\nHaving a target variable with skewed class distribution can hinder the performance of many ML algorithms (Sun et al., 2011). The cost-sensitive learning method is one possible algorithm-level solution for tackling the class imbalance problem. The algorithm-level solutions are an alternative to data manipulation techniques such as oversampling or undersampling, with the idea of changing the internal structure of the learning algorithm to favor the minority class. One possible modification includes incorporating costs for incorrect classification (misclassification cost) for each class into the learning algorithm.\nThe misclassification costs are typically represented as elements of a cost matrix. The cost matrix can be either class-dependent or sample-dependent, where the former means the misclassification costs are associated with the class. In contrast, the latter implies that each sample has its cost matrix defined. The assumption that the misclassification costs are constant across classes is more substantial and widespread through the application of most cost-sensitive learning algorithms (Haomin Wang and Peng, 2021; Feng, 1507; Krawczyk et al., 2014; Lomax and Vadera, 2011; Sun et al., 2006; Qin et al., 2005; Turney, 1995) since, in many real-life problems, the values in the matrix are unknown and not given by experts.\nCost-Sensitive Rule and Tree Extraction (CORTEX) method is grounded in the multi-class cost-sensitive decision tree (CSDT) method. The CSDT method proposed by Correa (2015) is an ML algorithm for generating a tree model by considering a sample-dependent cost matrix during the tree-building procedure. The CSDT method belongs to the group of cost-sensitive learning methods (Elkan, 2001), that can be used in the imbalanced learning framework by considering class-dependent cost matrix i.e. matrix associated with classes instead of individual samples. Initially, the CSDT algorithm is proposed by Correa (2015) in the sample-dependent classification framework for imbalanced two-class classification problems in the financial domain. Here, the framework is extended to classification problems with more than two classes by inducing the concept of an n-dimensional class-dependent cost matrix into the CSDT algorithm. The CSDT algorithm is modified for a class-dependent framework and extended further to multi-class classification problems.\nIn traditional decision tree learning algorithms, all samples are assumed to have equal importance. These algorithms assume a balanced class distribution and implicitly assume equal misclassification costs. The cost of misclassifying a sample is a function $C$ of the actual and predicted class, represented as a cost matrix:\n$C = [C_{ij}] =\\begin{bmatrix}C_{11} & C_{12} & ... & C_{1K}\\\\: & : & & :\\\\C_{K1} & C_{K2} & ... & C_{KK}\\end{bmatrix}$, $i, j = 1, ..., K$ (1)\nwhere $K$ represents number of classes, while $i$ and $j$ represent actual and predicted class, respectively. Accordingly, $C_{ij} = C(i, j)$ is the cost of predicting class $i$ when the actual (true) class is $j$.\nThe binary cost matrix is properly defined if two so-called \"reasonable\" conditions are satisfied (Elkan, 2001). The conditions imply that the cost of mislabeling a sample should always be greater than labeling it correctly. Violating conditions implies that one column dominates the other, and optimal prediction is the class corresponding to the dominated column. Intuitively, these conditions can be easily translated into a multi-class framework.\nFor binary classification tasks, the costs of making the error for the rare class are normally higher than those for the majority class. In literature, the cost is typically zero for correctly predicted outcomes, although the costs of correct classification can be non-zero (Turney, 2002).\nGiven a data represented as a collection of samples $\\{x_i, Y_i\\}_{i=1}^{i=n}$ where $x_i$ is vector of features with length $p$ and $Y_i \\in \\{1,..., K\\}$ is a label (class) associated with the $i$th sample, and given a cost matrix $[C_{ij}]_{1<=i,j<=K}$, the CORTEX algorithm learns a function $f: X \\rightarrow Y$, where $X \\subset \\mathbb{R}^p$ and $Y \\subset \\mathbb{R}$.\nThe learning phase consists of stratifying feature space into regions in a recursive manner (top-down greedy search). In the tree analogy, the regions are the nodes of the tree. The topmost node with only outgoing edges is known as a root node. Nodes with one incoming edge and two outgoing edges are called internal nodes. A terminal node (leaf node) has only one incoming edge and is denoted with the class label.\nUnlike traditional decision tree, the CSDT (Correa, 2015) classifies sample $x_i$ in the region $R_m$ to the least costly class $k(m)$:\n$k(m) = \\underset{k}{\\operatorname{argmin}} \\operatorname{cost}(f_k(m))$ (2)\nwhere $f_k(m)$ is a function that assigns class label $k$ to all samples that belong to the node $m$ and $\\operatorname{cost}(f_k(m))$ is misclassification cost for class $k \\in \\{0, 1, . . ., K\\}$ at the node $m$, calculated as:\n$\\operatorname{cost}(f_k(m)) = \\operatorname{cost}_m(k) = \\sum_{i=1}^{K} N_{mi} C(i,k)$ (3)\nWhere $N_{mi}$ denotes the number of samples from class $i \\in \\{1, ..., K\\}$ in node $m$ and $C(i,k) = C_{ik}$ is cost of misclassifying sample from class $i$ into class $k \\in \\{1, ..., K\\}$, meaning the cost of predicting class $k$ in the node $m$ is equal to sum of misclassification costs for all samples in the node $m$ wrongly classified. Therefore, the terminal nodes are labeled to minimize the misclassification cost.\nThis approach is known as hard-labeling, meaning that a label assigned to each node is class, denoting the class membership of all samples in a given node. On the other hand, in the soft-labeling approach, a label assigned to each node is a class probability distribution. That is, each terminal node is labeled by a probability vector with a size corresponding to the number of classes. The class label is then determined based on the highest probability score. In our previous study (Kopanja et al., 2024), soft-labeling is introduced into the original CSDT method to have probability membership for each sample and thereby have information about confidence in the prediction that the given sample belongs to a specific class. Labeling is introduced for binary frameworks by persevering the cost-dependence of labels. The formulation given in (Kopanja et al., 2024) is generalized for multi-class classification problems, leading to the proposed CORTEX method working for an arbitrary number of classes $K$. The mathematical formulation of cost-sensitive probabilities in CORTEX is given as:\n$P_{mk} = \\frac{1}{N} \\frac{\\operatorname{avgcost}_m(f(k))}{\\sum_{i=1}^K \\operatorname{avgcost}(f(i))}$ (4)\nwhere,\n$\\operatorname{avgcost}_m(f(i)) = \\frac{\\operatorname{cost}(f(i))}{\\sum_{i=1}^K \\operatorname{avgcost}(f(i))}$ (5)\nUsing the cost-sensitive probabilities defined above, classifying a sample in the least costly class is equivalent to classifying a sample in the class with the highest cost-sensitive probability. Notice that even if there were fewer samples from some class in the node, it could still be labeled as that class if the costs in the cost matrix are adequately defined. Therefore, the CORTEX model has a built-in bias towards the minority class(es), directed by misclassification costs defined in a cost matrix.\nThe cost matrix can be manually defined and even tuned in settings where the number of classes is small. For example, in a binary setting, where costs for correct classification are considered to be equal to zero, and the misclassification cost for the majority class is set to be 1, the misclassification cost for the minority class can be observed as a hyperparameter of the method. However, with an increasing number of classes, it becomes computationally intractable to tune all misclassification costs in a matrix. On the other side, manually defining the values of a cost matrix could be tricky, considering the \"reasonable\" conditions the matrix needs to satisfy. Therefore, in the CORTEX method, the proposed default version of the cost matrix is defined by using class imbalance ratios among classes. Given the $N_i$ being the number of samples in class $i$, the values of a cost matrix are defined as:\n$C_{ij} = \\frac{N_i + N_j}{N_i}$ (6)\nwhere $C_{ij} = C(i, j)$ is the cost of wrongly classifying sample from class $i$ to class $j$ ($i, j = 1, . . ., K$) which reflects class imbalance ratio among the classes $i$ and $j$.\nThe default cost matrix is initially designed based on class imbalance ratios, providing a foundational approach to address the skewness of class distribution. However, future work could explore modifications of the cost matrix to enhance the proposed CORTEX's performance and make it more adaptable in a given scenario. A well-defined cost matrix can be considered an essential part of a cost-sensitive tree-building algorithm that should align with the specific objectives of the problem domain. Conversely, a poorly chosen cost matrix could lead to suboptimal results, such as bias toward majority classes or failure to imitate the behavior of the underlying model in the surrogate modeling. Careful consideration is essential when defining the cost matrix to ensure the algorithm achieves an intended balance between accuracy, fidelity, and other relevant measures depending on the problem domain."}, {"title": "4 Experiment setup", "content": "In the experimental setup of our study, the proposed CORTEX is used as a post-hoc XAI method by creating a surrogate tree model and automatically extracting a set of IF-THAN rules from the obtained tree. The experiments were conducted on a set of eight publicly available datasets from the UCI Machine Learning Repository \u00b2. Detailed dataset descriptions are given in Table 1 below.\nThe datasets are briefly described in the study by Vilone et al. (2020), where selection is based on several criteria, including avoidance of curse of dimensionality, handcrafted features (both numerical and categorical), and categorical target variables with the number of classes ranging from two up to the 29 for the abalone dataset.\nThe experiment was designed as shown in the diagram below (Figure 1). The first step of the experimental setup is the data preparation process, where datasets with nominal features are encoded using a one-hot encoding technique. Then, the neural network models are trained on 70% of data using two different architectures in terms of the number of layers, the first being the vanilla feed-forward neural network with a single hidden layer and the other with two fully connected hidden layers. Afterward, the post-hoc surrogate models are created using test data and predictions given by the neural network. The cost-sensitive tree model is created using the CORTEX algorithm, and a set of if-then rules is automatically generated from the obtained tree model.\nSix metrics were selected to assess the degree of explainability of the rule sets, including completeness, correctness, fidelity, robustness, number of rules, and average rule length. Completeness is calculated as the ratio of samples covered by rules over the total number of samples. Correctness refers to the proportion of samples correctly classified by the rules relative to the total number of samples. Fidelity, on the other hand, is the proportion of samples for which the predictions made by both the neural networks and the rules align compared to the total number of samples. Robustness is the proportion of the difference between the predictions made by a set of rules on test samples with added Gaussian noise and the predictions made on the original test samples relative to the total number of test samples. The cardinality of the rule set is expressed as a number of rules and the average number of antecedents connected with the AND operator in a rule across all rules (expressed as the average rule length). Formal definitions of these measures can be found in studies Vilone et al. (2020) and Vilone and Longo (2021b).\nFive rule exaction methods are used to provide a comprehensive comparison analysis. The abundance of XAI rule-extraction methods makes it computationally extensive to use them all in comparison analysis. Four rule-extractors, C4.5-PANE, REFNE, RxREN, and TREPAN, are extensively studied in the literature (Vilone et al., 2020; Vilone and Longo, 2021b) and therefore considered as a strong baseline for evaluating the CORTEX method proposed in this study. Furthermore, considering the proposed rule method is a tree-based algorithm, the selected subset of benchmarking methods is extended with a traditional decision tree classifier (DT) to provide a more comprehensive evaluation. Moreover, due to the imbalanced distribution of the target variable, the DT method is trained by considering the class imbalance by automatically adjusting weights to be inversely proportional to class frequencies in the weighted impurity gain measure used in the feature splitting criterion (Cubero, 2007). Without considering the imbalance in DT model building, the rule sets extracted from it would not be reasonably comparable with rules from the CORTEX since the CORTEX naturally considers the class imbalance ratio in data. The weighted decision tree method (DT) is implemented using scikit-learn (Pedregosa et al., 2011). The algorithms for other rule-extractors are obtained from https://github.com/giuliavilone/rule_extractor.\nThe final step of the experimental process involves ranking the selected XAI methods based on the six evaluation metrics. For this purpose, we used the Friedman test (Friedman, 1937, 1940) and Wilcoxon signed-rank test (Wilcoxon, 1945), non-parametric tests that discover whether rule-extractors perform equally. The process is conducted using 100-time repeated results for reliability and reproducibility for both setups (both types of architectures of neural networks)."}, {"title": "5 Results and discussion", "content": "This section presents results from two different experimental setups, differing in the neural network's architecture. The first experiment is conducted on the vanilla feed-forward neural networks with a single hidden layer (NN-1), and the second experiment is carried out on the feed-forward neural network with two fully connected hidden layers (NN-2).\nFor all hyperparameters of the networks, such as the number of hidden neurons, activation function, dropout rate, optimization algorithm, and others, optimal values are obtained from Table 2 reported by Vilone et al. (2020). The training process was early-stopped to prevent overfitting. Accuracy for both architectures of neural network models on training and test sets is given in Table 2. The results in the table display the average accuracy score across 100 runs. Notably, depending on the dataset, a more complex neural network (NN-2) does not necessarily improve the accuracy score. However, for some datasets, including abalone, credit, wave-form, and yeast, the accuracy score on the test set is slightly increased.\nIn the next step of experimental design, six XAI rule-based methods are trained given a test dataset and trained neural network model. Afterward, a set of rules is extracted from each XAI model. Then, the sets of produced rules are evaluated using six quantitative measures of the degree of explainability. The objective evaluation is accomplished by excluding any human intervention. By combining the completeness, correctness, faithfulness (fidelity), and robustness of a rule set, we can effectively estimate the validity of the representation of an underlying model's inferential process. The rule set should appropriately classify any sample, be faithful to the underlying model, and produce inferences that will not vary when inputs are slightly distorted by applying Gaussian noise. Two other metrics, the number of rules and average rule length, are used to assess the syntactic simplicity of the rules. Both of these measures should be minimized for a given rule set to ensure the rules are easily interpretable and understandable by humans.\nResults for the six quantitative measures for all rule-extractors generated for NN-2 models across datasets are shown in Figure 2. Each box plot visually summarizes the distribution of a metric across 100 runs for a given method and dataset. A more granular representation of the results for each dataset separately is given in Appendix 1.\nThe analysis of the reported results leads to several conclusions. The rule sets obtained from all methods except REFNE and RxREN cover all input samples. Therefore, the proposed CORTEX produces rule sets that reach 100% of completeness"}]}