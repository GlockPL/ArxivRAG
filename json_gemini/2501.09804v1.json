{"title": "Enhancing Generalization in Chain of Thought Reasoning for Smaller Models", "authors": ["Maxwell J. Yin", "Dingyi Jiang", "Yongbing Chen", "Boyu Wang", "Charles Ling"], "abstract": "Chain-of-Thought (CoT) reasoning in smaller language models is a challenging natural language process problem yet highly desirable in many real-life applications. Existing CoT knowledge distillation methods often suffer from overly conservative memorization in smaller LLMs, leading to low generalization confidence. As fully preserving the CoT ability of teacher model is impossible, we hypothesize that adversarial CoT fine-tuning is crucial for developing smaller LLM with robust CoT generalization. To this end, we propose PRompt-Assisted Domain-Adversarial fine-tuning (PRADA), a principled fine-tuning framework that integrates diverse CoT domains. Specifically, PRADA pioneers two CoT improvements in smaller LLM: (1) Recovering the domain-invariant feature insight which typically lost during distillation with domain adversarial fine-tuning; (2) Enhancing the domain adaptability of CoT prompt engineering by employing domain-adversarial approaches. We theoretically demonstrate the effectiveness of our approach and empirically show that it significantly outperforms the state of the arts in a wide range of tasks. Moreover, our empirical findings reveal that the smaller LLM, when leveraging PRADA, aligns closely with domain knowledge, thereby improving the explainability of our approach.", "sections": [{"title": "Introduction", "content": "In recent years, Large Language Models (LLMs) have witnessed significant advancements. To solve complex tasks, prompt-based CoT reasoning [Wei et al., 2023] has emerged as a powerful tool to elicit advanced reasoning ability for LLM. Based on this method, LLMs smoothly achieved more cognitive and logical reasoning paths and higher answer correctness by requiring LLMs to perform CoT reasoning. For example, Kojima et al. [2023] found that LLMs can be prompted to generate a series of intermediate CoT reasoning steps by simply asking the model to think step by step."}, {"title": "Related Work", "content": "Knowledge Distillation Based on Chain-of-Thought Reasoning In the evolving realm of knowledge distillation (KD) and chain-of-thought (CoT) reasoning, the MT-COT framework [Li et al., 2022] stands out by utilizing COT explanations generated by LLMs to bolster the training of compact reasoning engines. Building on this foundation, the Specialized model[Fu et al., 2023] extracts CoT reasoning paths from large teacher models to enhance out-of-distribution generalization. This specialized extraction is crucial for improving the adaptability and accuracy of models when encountering unfamiliar data distributions. Further advancing the field, Fine-tune CoT[Ho et al., 2023] introduces a technique where multiple COT reasoning solutions are generated from LLMS through random sampling. This method addresses the challenge of limited training data diversity, which often hampers the generalization performance of student models. Innovatively, the SOCRATIC CoT[Shridhar et al., 2023] splits the training into two distilled models: a problem decomposer and a subproblem solver. This approach mitigates the cognitive load on student models, allowing for more effective learning and CoT reasoning.\nDomain Adversarial Neural Network Our research is closely aligned with Adversarial Training (AT) [Goodfellow et al., 2015], a prevalent machine-learning technique for enhancing model robustness. Building on this foundation, the Domain-Adversarial Neural Network (DANN) [Ganin et al., 2016] is designed to improve the model's generalization ability. In the field of Natural Language Processing (NLP), the standard approach involves applying adversarial perturbations to word embeddings [Miyato et al., 2021]. This technique has been demonstrated to significantly enhance model generalization ability when used for fine-tuning across various downstream tasks, as evidenced by methods such as FreeLB[Zhu et al., 2020], SMART[Jiang et al., 2020], InfoBERT[Wang et al., 2021], and CreAT[Wu et al., 2023]. Additionally, ALUM[Liu et al., 2020] provides empirical evidence that adversarial training can yield substantial pre-training benefits. Our work advances fine-tuning processes for pre-trained language models (PrLMs) [Devlin et al., 2019; Liu et al., 2020; Raffel et al., 2023; He et al., 2021]. It is independent of the prevailing pre-training paradigms, such as Masked Language Modeling (MLM) [Devlin et al., 2019], Replacement Token Detection (RTD)[Clark et al., 2020], Permuted Language Modeling (PLM)[Cui et al., 2022], and multiple-objective approaches[Wu et al., 2022].\nPrompt Learning The field of prompt learning has witnessed significant advancements, providing innovative approaches to enhance Natural Language Understanding (NLU) tasks. \"Prefix Tuning,\" introduced by [Li and Liang, 2021], improves model comprehension by appending prefixes to input sequences. The \"WARP\" method by [Hambardzumyan et al., 2021] and \"P-tuning\" by [Liu et al., 2023] modify language model outputs and input sequences, respec-"}, {"title": "Methodology", "content": "3.1 Problem Definition\nGiven a set of source questions Ds = {(q\u1d62)}\u1d62=1 and a\nNs\nNt\nset of target questions Dt = {(q\u1d62)}\u1d62=1, we adopt a model\ntrained from a source domain to a target domain simultaneously. Here, Ns and Nt denote the sizes of the source domain dataset D\u209b, and the target domain dataset D\u209c, respectively. Let GF (\u00b7; \u03b8f) be a pretrained language model with a hidden state size of H and a vocabulary size of V. The input embeddings are given through the pretrained embedding layer, where e \u2208 R|V|\u00d7H. In the following, we characterize the general PRADA architecture in three steps, which are also visualized in Figure 3.\n3.2 Teacher: Diverse Reasoning Generation\nFirst, we leverage a large teacher model to generate a variety of CoT reasoning responses for a given task. To enhance usage flexibility and minimize teacher inference costs, we employ the task-agnostic Zero-Shot-CoT prompting method [Kojima et al., 2023] on teacher models. Consider a standard sample Si comprising a question qi and its correct answer a\u1d62. By using Zero-Shot-CoT, we prompt the teacher model to generate a reasoning rationale, f\u1d62, to address question qi and provide a final answer prediction a\u1d62. The result text sequence includes the question and generated content: \u201c Q: (qi). A: Let's think step by step. (fi) Therefore, the answer is (ai).\u201d\nTo enhance reasoning ability, we leverage the effort of diverse CoT reasoning. For a given sample Si, we employ a stochastic sampling strategy, specifically temperature sampling with a large t, to generate D distinct outputs {(r\u1d62\u2c7c, a\u1d62\u2c7c)}\u2c7c=1. We\n refer to D as the degree of reasoning diversity.\nNext, we filter the generated samples and reformat them into prompt-completion pairs. For filtering, we compare the teacher model's final prediction a\u1d62 with the ground truth answer ai [Huang et al., 2022; Zelikman et al., 2022]. For all instances where a\u1d62 = ai, we repackage (Si, ri, \u00e2i) into a reasoning sample S = (qi, Ci), a question-completion pair. Specifically, qi and ci take the forms of \u201c(qi) ###\u201d and \u201c(fi) - -)(az) END\u201d.\n3.3 Student: Prompt Learning\nNs\nWe start with a source domain D, comprising CoT reasoning samples {(qi, Ci)}\u1d62=1, where qi represents the question se-\nquence and ci denotes the corresponding completion. Next, we deploy the P-Tuning adapter on the student model, which employs continuous prompt embeddings to enhance stability"}, {"title": "", "content": "and improve prompting performance. Let [P] represent the continuous prompt embedding. The prompt template for P-Tuning is defined as follows:\nTi = {P[0:j], qi, P[(j+1):k], Ci, P[(k+1):1]}\nP-Tuning leverages an additional embedding function f :\n[Pi] \u2192 hi to map the template to:\n{ho,..., hj, e(qi), hj+1,..., hk, e(ci), hk+1,...,hi}\nDuring the prompt learning, the embeddings {P\u1d62}\u1d62=1 are\nupdated to optimize a task loss function Lp using the template. These multiple P-tuning layers are trained as an adapter on the source domain, which employs deep prompt learning that involves a lot of P-tuning layers to improve and stabilize prompting [Liu et al., 2022]. Then, we insert the adapter into the student LLM. When the student model is fine-tuning or inferencing, the prompts in different P-tuning layers are added as prefix tokens, providing the student LLM with more tunable domain-agnostic parameters of the source domain data [Yin et al., 2024].\n3.4 Student: Domain Adversarial Fine-Tuning\nLet GF(;\u03b8f) represent the student LLM, with parameters \u03b8f. Then, let Gy (\u00b7; \u03b8y) denote the natural language modeling head, with parameters \u03b8y, while GD(\u00b7; \u03b8d) corresponds to the domain classifier of the PRADA architecture, with parameters \u03b8d. The input source domain data {(qi, Ci)}\u1d62=1, and target\nNs\nNt\ndomain data {(qi)}\u1d62=1, are transformed into {(qi, Ci, ds)}\u1d62=1\nNt\nNs\nand {(qi, dt)}\u1d62=1, respectively, where ds and dt are unique\nsymbols distinguishing the domains. We denote the prediction loss and the domain loss, respectively, :\nL\u1d67(\u03b8f, \u03b8y) = L\u1d67(Gy(Gf (qi; \u03b8f); \u03b8y), Ci)\nL\u2090(\u03b8f, \u03b8d) = L\u2090(Gd(Gf (qi; \u03b8f); \u03b8d), di)\nThen, training PRADA parallels the single-layer case and consists of optimizing:\n\u03f5(\u03b8f, \u03b8y, \u03b8d) = \u2211L\u1d67 (G\u1d67 (Gf (qi; \u03b8f); \u03b8y), Ci) +\ni=1\n\u03bb (\u2211La(Ga(Gf(qi; Of); Od), d;) +\ni=1\n Nt 1\u2211La (Gd(Gf(qi; 0f); Od), d;)),\ni=1\nA saddle point can be found as a stationary point of the following gradient updates[Ganin et al., 2016]:\n\u2202\u03f5 \u2202L\u1d67\n\u03b8y \u2190 \u03b8y \u2212 \u03bc , (5)\n\u2202\u03b8y \u2202\u03b8y\n\u2202\u03f5 \u2202L\u2090\n\u03b8d \u2190 \u03b8d \u2212 \u03bc\u03bb , (6)\n\u2202\u03b8d \u2202\u03b8d\nwhere u is the learning rate and A is the domain classifier loss weight. We use stochastic estimates of these gradients by sampling examples from the dataset. The only difference is that in Equation (3), the gradients from the LLM and domain predictors are subtracted, instead of being summed. This distinction is crucial, as otherwise, SGD would attempt to make features dissimilar across domains in order to minimize the domain classification loss. Fortunately, this can be achieved by introducing a special gradient reversal layer (GRL)[Ganin et al., 2016], defined as follows:\nR(q) = q,\ndR\ndq = \u2212I,\nwhere I is an identity matrix. We can then define the objective \"pseudo-function\u201d of (0f, \u03b8y, \u03b8\u03b1) that is being optimized by the stochastic gradient descent within our method:\n\u03f5(\u03b8f, \u03b8y, \u03b8d) = \u2211L\u1d67 (G\u1d67 (Gf (qi; \u03b8f); \u03b8y), Ci)\ni=1\nNt\n1\u2211La(Ga(R(Gf(qi; Of)); Od), di))."}, {"title": "Conclusion", "content": "In this work, we propose PRompt-Assisted Domain-Adversarial fine-tuning (PRADA), a novel adversarial fine-tuning framework to enhance the CoT ability of student models in knowledge distillation. Leveraging CoT knowledge distillation, prompt-assisted learning, and domain adversarial optimization, PRADA mitigates the degradation of generalization performance during Chain-of-Thought (CoT) reasoning distillation. This research explores factors causing CoT degradation, designs novel adversarial optimization techniques to improve the fidelity of distilled reasoning while preserving CoT power for portable LLMs. Extensive evaluations across twelve datasets highlight PRADA's superiority over prior CoT distillation methods, especially in scenarios requiring robust cross-domain generalization."}]}