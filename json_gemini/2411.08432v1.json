{"title": "One STEP at a time: Language Agents are Stepwise Planners", "authors": ["Minh Nguyen", "Ehsan Shareghi"], "abstract": "Language agents have shown promising adaptability in dynamic environments to perform complex tasks. However, despite the versatile knowledge embedded in large language models, these agents still fall short when it comes to tasks that require planning. We introduce STEP, a novel framework designed to efficiently learn from previous experiences to enhance the planning capabilities of language agents in future steps. Concretely, STEP functions through four interconnected components. First, the Planner takes on the task, breaks it down into subtasks and provides relevant insights. Then the Executor generates action candidates, while the Evaluator ensures the actions align with learned rules from previous experiences. Lastly, Memory stores experiences to inform future decisions. In the ScienceWorld (Wang et al., 2022) benchmark, our results show that STEP consistently outperforms state-of-the-art models, achieving an overall score of 67.4 and successfully completing 12 out of 18 tasks. These findings highlight STEP's potential as a framework for enhancing planning capabilities in language agents, paving the way for more sophisticated task-solving in dynamic environments.", "sections": [{"title": "1 Introduction", "content": "Autonomous agents that incorporate Large Language Models (LLMs) as integral cognitive systems (Sumers et al., 2023) have demonstrated significant capabilities in addressing a diverse range of interactive tasks e.g., mathematical problems (Cobbe et al., 2021; Hendrycks et al., 2021), programming challenges (Zhuo et al., 2024; Jimenez et al., 2024), and logical reasoning (Tafjord et al., 2021; Saparov and He, 2023). Nonetheless, their performance tends to diminish in dynamic scenarios, such as Web navigation (Zhou et al., 2024b; Yao et al., 2023a) and Open-ended environments (Wang et al., 2022; Shridhar et al., 2021), which require robust reasoning capabilities of the agents.\nA key contributing factor to language agents' efficiency in long tasks is the notion of memory (Sumers et al., 2023). The recent approaches (Majumder et al., 2023; Zhao et al., 2023a) guided the agent to store reflections on their experience of solving a task (Shinn et al., 2023) in memory, and then to retrieve these to improve future attempts (Yao et al., 2023c). The use of verbal refinements rather than updating model parameters, these techniques are more flexible than conventional Reinforcement Learning (RL) methods. However, this memory module often lacks a retrieval mechanism. Additionally, complex tasks that cannot be solved in a single attempt also require effective planning and goal decomposition mechanisms (we will explain this in more detail later in this paper).\nIn this paper, we take a close look at the memory utilization and planning capabilities of the language agent. More concretely, we propose STEP - a novel framework for Stepwise Planning which consists of a Planner, an Executor, and an Evaluator. Upon receiving a task from the environment, the Planner decomposes it into manageable subtasks and retrieves relevant information from the Memory. After receiving messages, the Executor then generates action candidates, which are subsequently evaluated by the Evaluator (Madaan et al., 2023). Once an action is approved and sent back to the environment, the agent receives an observation and determines whether the subtask requires refinement. After completing an episode, the agent generates learning insights (Majumder et al., 2023), which are stored in the Memory for future attempts. A key aspect is that the Planner not only breaks down tasks but also dynamically distils relevant insights from previous attempts to enhance the current task trace. This iterative process maximizes the efficiency of the memory system, ensuring continuous learning and adaptation.\nWe evaluate STEP within ScienceWorld (Wang et al., 2022) - a dynamic, text-based environment designed to simulate complex scientific tasks. Our results demonstrate that STEP consistently outperforms state-of-the-art (SOTA) models, achieving an overall score of 67.4. The model successfully completes 12 out of 18 tasks, ranking first in 11 tasks."}, {"title": "2 Background", "content": "2.1 Reinforcement Learning (RL) Agents\nAn RL agent is an autonomous system that learns by interacting with its environment, making decisions, and receiving feedback in the form of rewards to maximize long-term success (Ghasemi et al., 2024). We provide a brief review of three works used in our experiment section as baselines:\nDRRN (Deep Reinforcement Relevance Network) (He et al., 2016) is designed to handle state and action spaces, represented in natural language format. Instead of using raw text, DRRN uses separate neural networks to encode both the game states and possible actions into embedding vectors. Subsequently, it combines them to approximate the optimal action which maximizes the reward. This approach allows the agent to better understand and navigate environments with high-dimensional, unstructured data, showing that it captures meaning instead of memorizing text strings.\nCALM (Contextual Action Language Model) (Schuster et al., 2022) is trained on human gameplay to learn linguistic patterns and common actions, allowing it to generate a set of action candidates for each state. These candidates are then passed to a DRRN agent for re-ranking based on game rewards. By integrating human-like action prediction through a language model with reinforcement learning for assessment, CALM enhances the agent's ability to navigate and interact in complex text-based environments, including unfamiliar situations.\nKG-A2C (Knowledge Graph Advantage Actor Critic) (Ammanabrolu and Hausknecht, 2020) enhances the agent's decision-making by constructing a dynamic knowledge graph during exploration. By leveraging the OpenIE technique (Angeli et al., 2015), it maps relationships between objects, locations, and actions, and dynamically updates this graph. KG-A2C helps the agent prune irrelevant actions and focus on the most relevant ones, improving navigation and decision-making in complex environments.\n2.2 Language Agents\nA language agent is an AI system designed for interaction with the external world and understanding natural language, extending the abilities of large language models (LLMs) by incorporating memory mechanisms and related action capabilities (Sumers et al., 2023).\n2.2.1 Memory\nMemory plays a critical role in language agents, as it enables them to store, retrieve, and process information over time. Sumers et al. (2023) categorized memory into short-term and long-term modules including episodic memory, semantic memory and procedural memory. Short-term memory (working memory) holds active information, such as observations from the environment or retrieved knowledge from long-term memory (Kang et al., 2024), necessary for immediate processing. As its name implies, working memory serves as a central hub, providing queries to the LLM, translating responses into actionable steps, and linking long-term memory with the current state for cohesive processing.\nEpisodic memory records the experiences and events the agent has encountered. Like traditional reinforcement learning, where agents adjust their policy based solely on rewards, episodic memory improves learning by enabling more refined understanding and strategic planning, leveraging past successes and failures (Ouyang et al., 2022; Shinn et al., 2023; Gou et al., 2024b). Semantic memory holds factual information about the world and the agent's identity, capabilities, and task context, acting like a system prompt. For instance, SayCan (Ahn et al., 2022) understands it controls a \u201cphysical robot\" to execute real-world tasks described in natural language, while CRITIC (Gou et al., 2024a) knows its role is to handle free-form question answering or mathematical problems, with the ability to use tools like code interpreters and calculators to verify and refine solutions. Procedural memory comprises the LLM's internal weights and predefined code. Unlike the flexible nature of software-based episodic or semantic memory, procedural memory functions like hardware, requiring precise design to initialize the agent, where any error could lead to bugs or unintended behavior.\nior. While some language agents, such as LOGIPT (Feng et al., 2023) and ToRA (Gou et al., 2024b), exhibit procedural learning, this usually occurs during fine-tuning phase, not in deployment.\nTo interact with the memory system, language agents use human-like internal actions - retrieval, learning, and reasoning. Retrieval actions move data from long-term to short-term memory, providing the agent with relevant information to handle tasks. This can be done by directly querying the LLM to retrieve insights based on the current context (Zhao et al., 2023a) or using a key-value method where the key relates to the common in state and the value provides guidance (Fu et al., 2024). Learning actions process observations and feedback from working memory, encoding them into long-term memory to enable continuous learning. For example, VOYAGER (Wang et al., 2023a), a Minecraft agent, retrieves skills from episodic memory, generates executable code, and stores new skills gained from interaction with the environment back into long-term memory. Reasoning actions are more complex, involving reading from and writing to working memory. The LLM integrates the task, retrieved insight, probable solutions, and feedback from humans (Christiano et al., 2023; Ouyang et al., 2022), self-reflection (Madaan et al., 2023; Shinn et al., 2023; Majumder et al., 2023) or external tools (Gou et al., 2024a). If the solution does not meet the specified constraints, the LLM refines it iteratively.\n2.2.2 State-of-the-art Language Agents\nWith the rise of LLMs, there has been a growing body of work around embodied language agents.\nSayCan (Ahn et al., 2022), developed by Robotics at Google, integrates LLMs with physical robots to execute real-world tasks based on natural language instructions. Viewing a physical robot as an embodied agent, SayCan itself functions as a language agent, bridging the gap between natural language processing and real-world interaction. While CALM uses DRRN to rerank action candidates, SayCan employs the temporal-difference (TD) RL method to train its value function. This allows SayCan to evaluate the feasibility of suggested actions by accounting for both the current state of the embodied agent and the environment, leading to a more grounded decision-making process. By combining LLMs' semantic knowledge with a contextual understanding of real-world constraints, SayCan efficiently adapts to dynamic environments."}, {"title": "2.3 Planning with Language Agents", "content": "Recent research has explored the use of LLMs as planners in various approaches.\nLLM+P (Liu et al., 2023) and LLM-DP (Dagan et al., 2023) utilize LLMs to translate natural language descriptions into the Planning Domain Definition Language (PDDL), enabling classical planners to derive solutions. While symbolic reasoning guarantees finding optimal solutions, it often requires significant human effort for language conversion (Huang et al., 2024), and the assumption of perfect observation of all object states may not hold in dynamic environments. Given the numerous trajectories available to achieve a goal, work on expanding multi-plans in tree structures has been explored. Tree of Thoughts (ToT) (Yao et al., 2023b), which employs conventional BFS/DFS for optimal pathfinding, RAP (Hao et al., 2023) and LLM-MCTS (Zhao et al., 2023b) leverage the Monte Carlo Tree Search (MCTS) algorithm for searching. However, these methods are computationally exhaustive, and using LLMs as a world model to evaluate plans becomes impractical, particularly in complex, open-ended environments.\nSOTAs in LLM suggest a different planning method, where instead of tuning the model's parameters (Schuster et al., 2022; Lin et al., 2023), adjusting the prompts proves advantageous. Two primary schools of thought in planning are \u201cplan-from-the-start\u201d and \u201cplan-on-the-go\u201d (see Figure 2). A representative of the former, Plan-and-Solve (Wang et al., 2023b), builds on Zero-shot Chain of Thought (CoT) (Kojima et al., 2023) by transitioning from \"Let's think step-by-step\u201d to \u201cLet's first make a plan\" and \"Let's carry out the plan\". While this pre-planning strategy offers a straightforward approach of solving task (Shen et al., 2023; Singh et al., 2022), it heavily relies on accurate decomposition problems into simpler subproblems at once (Zhou et al., 2023). Moreover, predefining a plan before interacting with the environment can result in unrealistic subtasks, requiring adjustments during deployment.\nIn contrast, \"plan-on-the-go\u201d methods perform better in dynamic environments, where subtasks are revealed one by one during the deploying process. This approach allows agents to improve through feedback-driven retries, without external supervision. ReAct interleaves actions and thoughts for robust planning, while Reflexion adds a verbal reflection after each attempt, showing the self-learning capability of language agents. Building on this, the CLIN model introduces a new type of insight called causal abstractions, which determine whether \"X is necessary for Y\u201d. At each step, the agent accesses a lengthy memory log to determine its next action. While experiments show good adaptability, accessing the entire memory at once may cause the agent to incorrectly prioritize subtasks and risk confusion or overlook key insights. This lack of a transfer learning mechanism (Zhao et al., 2023a) limits its ability to fully leverage learning across different tasks or environments. Building on the CLIN model, we introduce STEP which can maintain task order while leveraging distilled in-context insights from memory, ensuring more accuracy in task execution."}, {"title": "3 Stepwise Planning Language Agent", "content": "We evaluate our model in simulated text-based environments, represented as a partially observable Markov decision process (POMDP). The agents are required to perform sequential actions to accomplish a specific task M. Our setup allows an agent to attempt a task multiple times, ranging from 1 to K. Each attempt k consists of multiple trials (i.e. episodes), and each trial T comprises a total of t steps. At each step, after issuing an action, the agent receives feedback on environmental changes through observations o and rewards r, indicating its performance. After each trial, we enable the agent to make a reflection, generating and storing learning insights in memory for future trials. During the deployment process, STEP consists of three components: Planner, Executor, and Evaluator. At the end of each trial, STEP updates its Memory to facilitate continued learning (See Figure 3).\nPlanner is the strategic component of the framework, responsible for breaking down the current task into manageable subtasks, leveraging a frozen LLM (a model with fixed parameters) to assist in generating the next steps (Algorithm 1. lines 5-10). At step t, the Planner receives the main task M, the suggested strategy St from the past, and the history of the current trial (a sequence of actions and observations: {a1, 01, a2, o2, . . ., at, ot}). Based on the status of the previous subtask m, the Planner either refines it or derives a new subtask with the list of successful subtasks q to advance the agent toward the main task. Additionally, the Planner also retrieves relevant insights s from the learning summary of the previous attempt Sk\u22121, which prevents agent access to irrelevant information and focusing on the subtask itself (Zhao et al., 2023a).\nExecutor is the language agent's implementer, tasked with generating appropriate actions to execute the objectives set by the Planner (Algorithm 1. line 13). Following the ReAct concept, the Executor is required to derive the next rationale gt and action at given the current subtask m and relevant insight s from the Planner. Another modification from the CLIN model is that while the Executor is still provided with the sequence of actions and observations, it no longer receives the rationales of previous actions. This is because, at different steps, the Executor may have a different subtask m, leading to a different motivation for actions. This setup enhances the module's purpose in task performance, while the Planner is the sole component managing the overall task flow, the Executor has access only to its current subtask. Thus, within the Executor, subtasks are converted into concrete actions that can interact with the environment.\nEvaluator serves as a quality control mechanism, assessing the action generated by the Executor before they are executed in the environment (Algorithm 1. lines 12-19). While it has been argued that relying solely on internal knowledge for refinement may reduce the performance of language agents (Kambhampati et al., 2024; Gou et al., 2024a; Valmeekam et al., 2023), our Evaluator does not select the best action candidate based on its own judgment (Madaan et al., 2023). Instead of evaluating the alignment between the action candidate and the task, it focuses on how well the action aligns with the rules s' (e.g. \u201cX does NOT contribute to Y\u201d) from Sk\u22121. While the dynamic nature of the environment makes it nearly impossible to construct a world model upfront in \u201cplan-from-the-start", "plan-on-the-go\" strategy takes advantage of this flexibility. After exploring the environment, the Executor resembles as simplified world model, which assesses the action candidates. Based on these assessments, the Evaluator either sends a feedback f to the Executor for refinement or approves it for execution. It also monitors the completion status of subtasks, issuing refinement requests when necessary to avoid exhaustive exploration.\nFinally, Memory serves as a critical component in ensuring the agent's continual learning process (Gou et al., 2024b; Shinn et al., 2023; Zhou et al., 2024a) (Algorithm 1. line 22). The Memory is structured into two primary components: the list of insights Sk (which is inherited from CLIN) and the suggested strategy St. At the end of each trial k, the agent reflects on its performance Tk based on final reward rk, generates new strategy St, and updates Sk from previous trials S<k. The notion of insights is well-established in recent research and can encompass pairs of actions and observations (Schuster et al., 2022), causal verbal reflections (Shinn et al., 2023), or human feedback (Ouyang et al., 2022). In this work, we adopt the causal abstraction framework proposed by Majumder et al. (2023), which evaluates how action X influences action Y. This relationship can yield useful insights, such as \u201cX is necessary for Y,\" or identify errors, such as \\\"X may not contribute to Y.": "nAlthough these insights are valuable for guiding the agent's execution process, a large information may impair retrieval performance, leading the agent to deviate from the intended task sequence. To mitigate this issue and maintain system efficiency, the Planner is provided with a suggested strategy St from the most recent trial, enabling it to accurately determine the next subtask and forward relevant insights to the Executor."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nTask Environment. To assess STEP, we chose ScienceWorld (Wang et al., 2022) 2, a text-based interactive environment that simulates elementary science tasks across 10 interconnected locations, including settings like a living room, workshop, and"}, {"title": "4.2 Main Results", "content": "Figure 4 presents the model's performance for a short task Temp\u2081 and a long task Boil. The solid lines represent average rewards while the shaded areas capture the variability in performance. Inherited from CLIN, STEP demonstrates strong performance. For the short task, the model shows rapid improvement, reaching an average reward close to 60 within approximately 30 steps, with a noticeable reduction in variability. In contrast, the long task presents slower improvement, with the model's average reward gradually increasing and peaking around 20 after 50 steps, showing higher variability throughout. This suggests that the model adapts more efficiently and stably in the short task while facing greater difficulty and inconsistency in the longer task. See Appendix B for a sample from STEP task solving.\nNext, we will compare STEP with other state-of-the-art (SOTA) agents, as shown in Figure 1. In particular, we will conduct further experiments to compare it with CLIN, the predecessor of STEP, to evaluate performance improvements.\n4.2.1 STEP outperform SOTA Agents\nTable 1 compares the performance of various agents across 18 tasks in the ScienceWorld environment. The results highlight the superior performance of LLM-based methods (Generative Language Agents) over conventional RL agents due to their advanced generalization abilities, though they come with higher deployment costs. Among the RL agents, DRRN achieves the highest overall score of 16.7, which is still significantly lower than the scores of all the generative language agents,\nwith the lowest among them being ReAct at 29.6. Reflexion demonstrates a strong capability by utilizing episodic memory to retain learning insights across trials, showing more than a 10-point improvement over SayCan and ReAct in short tasks. Building upon Reflexion's memory-based advantages, CLIN benefits from causal abstraction insights, which significantly boosts its performance to a 57.2 overall score, demonstrating its effectiveness in both task types.\nNotably, STEP consistently outperforms all SOTA models across both short and long tasks, achieving 67.4 points overall. In short tasks, STEP achieves an impressive 79.9, representing an 11.4% increase over its predecessor. In long tasks, STEP further demonstrates its ability to handle more complex challenges, with a score of 54.9, marking a 28.6% improvement over CLIN. Analyzing individual tasks, STEP ranks first in 11 out of 18 tasks, clearly showcasing its dominant performance. For example, in the short task Temp\u2081, STEP significantly boosts the second highest score from SayCan's 26.4 to an impressive 63.0. In the long task GrowPlant, it doubles the score of 30.3 in CLIN to 71.5. This substantial leap in performance across individual tasks emphasizes STEP's capacity to generalize well and adapt to a wide range of environments.\n4.2.2 STEP outperforms CLIN in continual learning\nNext, we compare the performance between STEP and CLIN across all tasks (see Figure 5), reporting the best traces while excluding false-positive cases where the agent unintentionally \u201ccheats\u201d the task (e.g. in the Friction task, the agent might randomly focus on an inclined plane and complete the task by chance, without adhering to the intended experimental procedure). In short tasks, both CLIN and STEP exhibit strong performance, as indicated by the prominent yellow regions for tasks such as Pick&Place, Chemistry and Lifespan. However, a key difference arises in Temp\u2081, where CLIN struggles, achieving only 10 points overall. In contrast, STEP shows a gradual progression, steadily achieving subgoals and eventually reaching nearly 90 points. This demonstrates STEP's capability to break down tasks and make incremental progress, where CLIN appears to fall short.\nThe performance gap widens further in long tasks, where STEP consistently outperforms CLIN. CLIN shows difficulties in tasks such as Boil, Freeze, and GrowPlant, marked by darker shades in the heatmap, indicating poorer performance. STEP, on the other hand, demonstrates significant improvements in these complex tasks, with more yellow regions, suggesting higher scores and better task completion. A potential explanation for this distinction is that CLIN completes all tasks within 30 steps, as shown by the cutoff in its heatmap, suggesting faster but possibly incomplete learning. Conversely, STEP continues learning beyond that, which allows it to achieve superior overall performance and higher scores, particularly in tasks where CLIN has plateaued. For example, in Freeze, while CLIN initially learns quickly, achieving around 80 points within the first 14 steps, it subsequently halts progress or becomes stuck in a loop. In contrast, after experiencing stagnation up to 15 steps without any improvement, STEP rapidly peaks at the 27th and successfully completes the task at the 36th step. This suggests that STEP has a greater capacity for long-term learning and task improvement, particularly in more complex scenarios, where CLIN's performance tends to stall.\n4.2.3 The importance of Planner\nAblation experiment. To evaluate the necessity of the Planner component, we conducted ablation experiments on STEP by removing the Planner and retaining only the Executor, Evaluator, and Memory modules (see Figure 3). In this configuration, the Executor directly receives tasks from the environment and retrieves information from Memory without any distillation or task refinement. Moreover, the absence of the Planner eliminates the refinement of subtasks typically performed by the Evaluator.\nAs presented in Table 2, the results demonstrate a significant drop in performance. Specifically, the removal of the Planner leads to a reduction in the average score by 10.9 points in short tasks and 11.0 points in long tasks. Furthermore, the success rate decreases by 11.1% for short tasks and 22.2% for long tasks, culminating in an overall decline of 10.9 points in average score and 16.7% in success rate across all tasks. Notably, the average score of the STEP w/o Planner model is approximately the same as that of CLIN, with both achieving around 57 points overall. This result was expected, as the ablation model lacks the critical Planner component, making it architecturally similar to CLIN. These findings also suggest that the addition of the Evaluator (which CLIN does not include) does not significantly enhance performance in the absence of the Planner.\nA qualitative example. Figure 6a illustrates the optimal strategies employed by STEP, STEP w/o Planner, and CLIN during Task Temp\u2081, where the agent must measure the temperature of an unknown substance B. Although the location of substance B is known (in the living room), the agent must first locate a hidden thermometer. Without a planner, both CLIN and STEP w/o Planner instinctively head to the living room to interact with substance B, resulting in a -100 point penalty for skipping the necessary step of finding the thermometer. As reflected in the graph, this mistake leads to early termination for both models-CLIN is cut off at step 8 and STEP w/o Planner at step 14-with their rewards stagnating below 15 points due to incorrect task execution.\nIn contrast, STEP with Planner adheres to the correct task order. The Planner isolates the subtask of finding the thermometer, ensuring the Executor focuses solely on the immediate task without being distracted by the known location of substance B (see Figure 6b). This isolation prevents the Executor from prematurely interacting with substance B and helps STEP avoid penalties. As reflected in the graph, STEP continues to progress smoothly, with its reward rapidly increasing and reaching a maximum of 90.\nAdditionally, we observed that without the Planner, the performance of the other models does not improve, even when Memory correctly suggests the location of the thermometer (in the kitchen). The situation worsens when the models receive hints related to substance B, such as \"going to the living room is necessary to find substance B\u201d or \u201cfocusing on substance B may contribute to the task.\" Without the high-level task distillation and guidance provided by the Planner, these models are unable to effectively leverage the usefulness of information. This further underscores the Planner's critical role in task decomposition and knowledge distillation, ensuring the agent follows the task order."}, {"title": "5 Limitation", "content": "Poor subtask generation. The agent can easily deviate from the intended task if the initial subtask goes off track. The Executor relies entirely on the Planner's guidance, while the Planner depends on the Executor's exploration to understand the environment. This interdependency can lead to significant problems if the Planner generates an irrelevant subtask. In task Biology, the agent is required to \"focus on the 3 life stages of the wolf\". Instead of guiding the agent to find a wolf outside the house, the Planner incorrectly decides that painting a representation of a wolf is a valid solution: \"Since I am in an art studio, I can use the paint to create representations of the wolf's life stages.\" The Planner then directs the Executor to gather materials, mix colors, and attempt to draw the wolf.\nOnce the Planner locks onto an incorrect high-level subtask, it becomes increasingly difficult for the agent to revise its approach. As a result, it falls into a repetitive loop of failure, with the Executor never leaving the house to discover the real wolf. The only potential way to break this loop is through a refinement mechanism; however, in this case, the mechanism fails to detect the irrelevance of the subtask and continues allowing the agent to explore the painting activity. Instead of exploring horizontally, the agent dives vertically into the wrong path.\nPoor strategy generation. After each trial, a strategy is generated to inform future agents of the latest successful traces. Whether provided with a list of completed subtasks or a full action trace, the model struggles to eliminate unnecessary elements. For instance, in the Force task, where the agent must \u201cdetermine which of the two inclined planes has the steepest angle\u201d, the agent first needs to locate the inclined planes in the workshop. After extensive exploration-navigating to the art studio, hallway, outside, and then back to the hallway-the agent finally finds the workshop and continues with the task. However, at the end of the episode, the agent fails to eliminate redundant looping traces, such as repeatedly moving to the hallway without gaining useful information, and instead groups these actions into the vague category of \"Exploring the environment.\"\nBalancing strategy abstraction presents significant challenges. Strategies that are too detailed closely resemble action traces, causing future agents to simply replicate past actions without considering variations in context. In contrast, overly abstract strategies result in unachievable goals, as they lack the necessary steps for effective task completion. To mitigate this, our approach attempts to retain actions that gain rewards from the environment, then prompt LLMs to identify which remaining actions are essential, before abstracting the new action trace. However, this method is still far from optimal and highlights a gap for future research."}, {"title": "6 Conclusion", "content": "We present STEP, a simple yet effective framework that leverages LLM as a stepwise planner. STEP enhances memory utilization and task sequence recognition, achieving state-of-the-art performance in the ScienceWorld benchmark. By showcasing the potential of STEP, our research contributes to the ongoing exploration and development of language agents as planners."}, {"title": "A ScienceWorld Tasks Action Space", "content": "The ScienceWorld action space is reported in Table 3."}, {"title": "B Example of STEP trajectory for a task", "content": "We present a sample of task trajectory for STEP in Figure 7."}]}