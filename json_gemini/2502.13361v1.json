{"title": "RGAR: Recurrence Generation-augmented Retrieval for Factual-aware Medical Question Answering", "authors": ["Sichu Liang", "Linhai Zhang", "Hongyu Zhu", "Wenwen Wang", "Yulan He", "Deyu Zhou"], "abstract": "Medical question answering requires extensive access to specialized conceptual knowledge. The current paradigm, Retrieval-Augmented Generation (RAG), acquires expertise medical knowledge through large-scale corpus retrieval and uses this knowledge to guide a general-purpose large language model (LLM) for generating answers. However, existing retrieval approaches often overlook the importance of factual knowledge, which limits the relevance of retrieved conceptual knowledge and restricts its applicability in real-world scenarios, such as clinical decision-making based on Electronic Health Records (EHRs). This paper introduces RGAR, a recurrence generation-augmented retrieval framework that retrieves both relevant factual and conceptual knowledge from dual sources (i.e., EHRs and the corpus), allowing them to interact and refine each another. Through extensive evaluation across three factual-aware medical question answering benchmarks, RGAR establishes a new state-of-the-art performance among medical RAG systems. Notably, the Llama-3.1-8B-Instruct model with RGAR surpasses the considerably larger, RAG-enhanced GPT-3.5. Our findings demonstrate the benefit of extracting factual knowledge for retrieval, which consistently yields improved generation quality.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities in general question answering (QA) tasks, achieving impressive performance across diverse scenarios (Achiam et al., 2023). However, when facing domain-specific questions that require specialized expertise, from medical diagnosis (Jin et al., 2021) to legal charge prediction (Wei et al., 2024), these models face significant challenges, often generating unreliable conclusions due to both hallucinations (Ji et al., 2023) and potentially stale knowledge embedded in their parameters (Wang et al., 2024a).\nRetrieval-Augmented Generation (RAG) (Lewis et al., 2020) has emerged as a promising approach to address these challenges by leveraging extensive, trustworthy knowledge bases to support LLM reasoning. The effectiveness of this approach, however, heavily depends on the relevance of retrieved documents. Recent advances, such as Generation-Augmented Retrieval (GAR) (Mao et al., 2021a), focus on enhancing retrieval performance by generating relevant context for query expansion.\nIn the medical domain, current RAG approaches concatenate all available contextual information from a given example into a single basic query for retrieval, aiming to provide comprehensive context for model reasoning (Xiong et al., 2024a). While this method has demonstrated substantial improvements on early knowledge-intensive medical QA datasets such as PubMedQA (Jin et al., 2019), its limitations have become increasingly apparent with the emergence of EHR-integrated datasets that better reflect real-world clinical practices (Kweon et al., 2024). Electronic Health Records (EHRS) typically contain extensive patient data, including comprehensive diagnostic test results and medical histories (Pang et al., 2021). However, for any specific medical query, only a small subset of this information is typically relevant, and retrieval performance can be significantly degraded when queries are diluted with extraneous EHR content (Johnson et al., 2023; Lovon-Melgarejo et al., 2024).\nWe highlight that current retrieval methods often fail to adequately consider factual information in real-world medical scenarios. Crucially, even when applying query expansion with GAR, the persistent oversight of factual information fundamentally limits their ability to retrieve real relevant documents.\nInspired by Bloom's taxonomy (Forehand, 2010; Markus, 2001), we categorize the knowledge required to address real-world medical QA problems into four types: Factual Knowledge, Conceptual Knowledge, Procedural Knowledge, and Metacognitive Knowledge. The latter two represent higher-order knowledge typically embedded within advanced RAG systems. Specifically, Procedural Knowledge refers to the processes and strategies required to solve problems, such as problem decomposition and retrieval (Wei et al., 2022; Zhou et al., 2023), while Metacognitive Knowledge pertains to an LLM's ability to assess whether it has sufficient knowledge or evidence to perform effective reasoning (Kim et al., 2023; Wang et al., 2023b).\nFactual Knowledge and Conceptual Knowledge require retrieval from large databases containing substantial amounts of irrelevant content, corresponding to the EHRs of patients and medical corpora in answering medical questions. Unfortunately, current RAG systems do not differentiate between these types of retrieval targets, overlooking the necessity of retrieval from EHRs.\nTo overcome this limitation, we propose RGAR, a system designed to simultaneously retrieves Factual Knowledge and Conceptual Knowledge through a recurrent query generation and interaction mechanism. This approach iteratively refines queries to enhance the relevance of retrieved professional and factual knowledge, thereby improving performance on knowledge-intensive and factual-aware medical QA tasks.\nOur key contributions are listed as follows:\n\u2022 We are the first to analyze RAG systems through the lens of Bloom's taxonomy, addressing the current underrepresentation of Factual Knowledge in existing frameworks.\n\u2022 We introduce RGAR, a dual-end retrieval system that facilitates recurrent interactions between Factual and Conceptual Knowledge, bridging the gap between LLMs and real-world clinical applications.\n\u2022 Through extensive experiments on three medical QA datasets involving Factual Knowledge, we demonstrate that RGAR achieves superior average performance compared to state-of-the-art (SOTA) methods, enabling Llama-3.1-8B-Instruct model to outperform the considerably larger RAG-enhanced GPT-3.5-turbo."}, {"title": "2 Related Work", "content": "RAG Systems. RAG systems are characterized as a \"Retrieve-then-Read\" framework (Gao et al., 2023). The development of Naive RAG has primarily focused on retriever optimization, evolving from discrete retrievers such as BM25 (Friedman et al., 1977) to more sophisticated and domain-specific dense retrievers, including DPR (Karpukhin et al., 2020) and MedCPT (Jin et al., 2023), which demonstrate superior performance.\nIn recent years, numerous advanced RAG systems have emerged. Advanced RAG systems focus on designing multi-round retrieval structures, including iterative retrieval (Sun et al., 2019), recursive retrieval (Sarthi et al., 2024), and adaptive retrieval (Jeong et al., 2024). A notable work in medical QA is MedRAG (Xiong et al., 2024a), which analyzes retrievers, corpora, and LLMs, offering practical guidelines. Follow-up work, i-MedRAG (Xiong et al., 2024b), improved performance through multi-round decomposition and iteration, albeit with significant computational costs.\nThese approaches focus solely on optimizing the retrieval process, overlooking the retrievability of factual knowledge. In contrast, RGAR introduces a recurrent structure, enabling continuous query optimization through dual-end retrieval and extraction from EHRs and professional knowledge corpora, thereby enhancing access to both knowledge types.\nQuery Optimization. As the core interface in human-AI interaction, query optimization (also known as prompt optimization) is the key to improving AI system performance. It is widely applied in tasks such as text-to-image generation (Liu et al., 2022; Wu et al., 2024b) and code generation (Nazzal et al., 2024)."}, {"title": "3 Methodology", "content": "In this section, we introduce RGAR framework, as illustrated in Figure 2. It begins by prompting a general-purpose LLM to generate multiple queries from an initial basic query. These multiple queries are then used to retrieve conceptual knowledge (\u00a7 3.2). Then retrieved conceptual knowledge is subsequently used to extract factual knowledge from the electronic health records (EHRs) and transform it into retrieval-optimized representations (\u00a7 3.3). The recurrence pipeline continuously updates the basic query and iteratively executes the two aforementioned components. This process optimizes the retrieved results, ultimately improving the quality of responses.(\u00a7 3.4)."}, {"title": "3.1 Task Formulation", "content": "In factual-aware medical QA, each data sample comprises the following elements: a patient's natural language query $Q$, the electronic health record (EHR) as factual knowledge $F$, and a set of candidate answer $A = \\{a_1, ..., a_{|A|}\\}$. The overall goal is to identify the correct answer $\\hat{a}$ from $A$.\nA non-retrieval approach directly prompts an LLM to act as a reader, processing the entire context and generating an answer, formulated as:\n$\\hat{a} = LLM(F, Q, A|T_r)$ (1)\nwhere $T_r$ is the prompts. However, this approach relies exclusively on the conceptual knowledge encoded within LLM, without leveraging external, trustworthy medical knowledge sources.\nTo overcome this limitation, recent studies have explored retrieval-based approaches, which enhance the model's knowledge by retrieving a specified number $N$ of chunks, denoted as $C = \\{C_1, ..., C_N\\}$, from a chunked corpus (knowledge base) $K$. This answering process is expressed as:\n$\\hat{a} = LLM(F, Q, A, C|T_r)$. (2)"}, {"title": "3.2 Conceptual Knowledge Retrieval (CKR)", "content": "To maintain consistency with the option-free retrieval approach proposed by (Xiong et al., 2024a), we do not incorporate the answer options $A$ during retrieval. This design is in line with real-world medical quality assurance scenarios, where answer choices are typically not available in advance.\nFollowing their method, we construct the basic query by concatenating the EHR and the patient's query, formally defined as $q_b = Q \\oplus F$, where $\\oplus$ denotes text concatenation.\nTraditional dense retrievers, such as Dense Passage Retrieval (DPR) (Karpukhin et al., 2020), identify the top-N relevant chunks $C$ from the knowledge base $K$ by computing similarity scores using an encoder $E$:\n$\\begin{aligned} sim(q_b, c_i) = E(q_b)^T E(c_i), \\\\ C = \\text{top-N}(\\{sim(q_b, C_i)\\}) \\end{aligned}$. (3)\nVanilla GAR (Mao et al., 2021a) expands $q_b$ using a fine-tuned BERT (Devlin et al., 2019) to produce three types of content that enhance retrieval: potential answers $q_a^e$, contexts $q_c^e$, and titles $q_t^e$. With the growing zero-shot generation capabilities of LLMs (Kojima et al., 2022), a common practice is to prompt LLMs to serve as train-free query generators, producing expanded content $q^e$ using prompt templates $T_g$ (Frisoni et al., 2024). The three types of content generation process can be formulated as:\n$\\begin{aligned} q_a^e = LLM(q_b|T_g^a), \\\\ q_c^e = LLM(q_b|T_g^c), \\\\ q_t^e = LLM(q_b|T_g^t). \\end{aligned}$ (4)\nThe final score $S_c$ for retrieving $C$ is then computed by normalizing and averaging the similarities of these expanded queries:\n$S_c(c_i) = \\frac{ \\exp(sim(q^e_i, c_i)) }{ \\sum_{j \\in \\{a,c,t\\}} \\sum_{c_j} \\exp(sim(q^e_j, C_j))}$. (5)"}, {"title": "3.3 Factual Knowledge Extraction (FKE)", "content": "In EHR, only a small portion of necessary information constitutes problem-relevant factual knowledge (D'Alessandro et al., 2004). Direct input of lengthy EHR content containing substantial irrelevant information into dense retrievers can degrade retrieval performance (Ren et al., 2023). While a straightforward approach would be to retrieve EHR content based on question $Q$ (Lu et al., 2023), this fails to fully utilize conceptual knowledge obtained from previous Conceptual Knowledge Retrieval Stage. Furthermore, the necessary chunking of EHR for retrieval introduces content discontinuity (Luo et al., 2024).\nGiven that EHRs more closely resemble long passages from the Needle in a Haystack task (Kamradt) rather than necessarily chunked corpus, and inspired by large language models' capability to precisely locate answer spans in reading comprehension tasks (Cheng et al., 2024), we propose leveraging LLMs for text span tasks (Rajpurkar et al., 2016) on EHR to filter relevant factual knowledge efficiently and effectively using conceptual knowledge. We define this filtered factual knowledge as $F_s$, with prompts $T_s$, expressed as:\n$F_s = LLM(F, Q,C|T_s)$. (6)\nIn addition, EHRs often contain numerical report results (Lovon-Melgarejo et al., 2024) that require conceptual knowledge to interpret their significance. Furthermore, medical QA involves multi-hop questions (Pal et al., 2022), where retrieved conceptual knowledge can generate explainable new factual knowledge conducive to reasoning. Drawing from LLM zero-shot summarization prompting strategies (Wu et al., 2025), we analyze and summarize the filtered EHR $F_s$ with prompts $T_e$, yielding an enriched representation $F_e$:\n$F_e = LLM(F_s, Q, C|T_e)$. (7)\nThis process, which we refer to as the LLM Extractor, completes the extraction of original EHR information. In practice, RGAR implements these two phases using single-stage prompting to reduce time overhead."}, {"title": "3.4 The Recurrence Pipeline and Response", "content": "Building on the $F_e$, we update the basic query for Conceptual Knowledge Retrieval as $q_b = Q \\oplus F_e$. This establishes a recurrence interaction between factual and conceptual knowledge, guiding next retrieval toward more relevant content. Iterative execution enhances the stability of both retrieval and extraction. The entire pipeline recurs for a predefined number of iterations, ultimately yielding the final retrieved conceptual knowledge $C^*$.\nDuring the response phase, we follow the approach in Equation 2 to generate answers. Notably, the $F_e$ are restricted to the retrieval phase and are not used in the response phase. The sole difference lies in the retrieved chunks, highlighting the impact of retrieval quality on the responses."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setup", "content": ""}, {"title": "4.1.1 Benchmark Datasets", "content": "We evaluated RGAR on three factual-aware medical QA benchmarks featuring multiple-choice questions that require human-level reading comprehension and expert reasoning to analyze patients' clinical conditions.\nMedQA-USMLE (Jin et al., 2021) and MedMCQA (Pal et al., 2022) consist of questions derived from professional medical exams, evaluating specialized expertise such as disease symptom diagnosis and medication dosage requirements. The problems frequently involve patient histories, vital signs (e.g., blood pressure, temperature), and final diagnostic evaluations (e.g., CT scans), making it necessary to retrieve relevant medical knowledge tailored to the patient's specific circumstances. However, due to their exam-oriented format, the provided information has already been filtered, reducing the difficulty of extracting factual knowledge from EHR.\nEHRNoteQA (Kweon et al., 2024) is a recently introduced benchmark that provides authentic, complex EHR data derived from MIMIC-IV (Johnson et al., 2023). This dataset encompasses a wide range of topics and demands that models emulate genuine clinical consultations, ultimately generating accurate discharge recommendations. Consequently, EHRNoteQA challenges models to identify which factual details within the EHR are relevant to the questions at hand and apply domain-specific knowledge to address them."}, {"title": "4.1.2 Retriever and Corpus", "content": "To ensure a fair comparison, we adopt the same retriever, corpus, and parameter settings as previous work (Xiong et al., 2024a). We use MedCPT (Jin et al., 2023), a dense retriever specialized for the biomedical domain, configured to retrieve 32 chunks by default. For the corpus, we employ the Textbooks dataset (Jin et al., 2019), a lightweight collection of 125.8k chunks derived from medical textbooks, with an average length of 182 tokens."}, {"title": "4.1.3 LLMs and Baselines", "content": "We focus on the effect of RGAR on general-purpose LLMs without domain-specific knowledge. Therefore, we exclude LLMs fine-tuned on the medical domain, such as PMC-Llama (Wu et al., 2024a). Our primary experiments utilize Llama-3.2-3B-Instruct, while ablation studies include a range of models from the Llama-3.1/3.2 (Dubey et al., 2024) and Qwen-2.5 (Yang et al., 2024a) families, ranging from 1.5B to 8B parameters. All selected models feature a context length of approximately 128K tokens. Temperatures are set to zero to ensure reproducibility through greedy decoding.\nFor non-retrieval methods, we consider a zero-shot approach Custom (Kojima et al., 2022) as a baseline and evaluate improvements relative to it. To fully exploit the reasoning capabilities of the LLMs, we incorporate chain-of-thought (CoT) reasoning (Wei et al., 2022). For retrieval-based methods, we evaluate the classic RAG model (Lewis et al., 2020), the domain-adapted MedRAG (Xiong et al., 2024a), and i-MedRAG (Xiong et al., 2024b), a medical-domain RAG system designed to decompose questions and iteratively provide answers.\nWe adopt GAR (Mao et al., 2021a) as a representative query-optimized RAG method, implemented train-free in accordance with \u00a7 3.2. RGAR defaults to 2 rounds of recurrence."}, {"title": "4.1.4 Evaluation Settings", "content": "Following MIRAGE (Xiong et al., 2024a), we adopt the following evaluation framework. In Option-Free Retrieval, no answer options are provided for retrieval (\u00a73.2), ensuring a more realistic medical QA scenario. In Zero-Shot Learning, RAG systems are evaluated without in-context few-shot learning, reflecting the lack of similar exemplars in real-world medical questions. For Metrics, we employ Accuracy, defined as the proportion of correctly answered questions, and we extract model outputs by applying regular expression matching to the entire generated responses (Wang et al., 2024b)."}, {"title": "4.2 Main Results", "content": ""}, {"title": "4.2.1 Cross-Dataset Performance Improvement", "content": "We evaluate RGAR with the Llama-3.2-3B-Instruct across three factual-aware medical datasets, comparing it with several competitive baselines. Table 2 presents the results of all methods, along with their relative improvements over the Custom baseline. RGAR achieves the highest average performance across the three datasets, surpassing the second-best method, i-MedRAG, by 2%. The retrieval-based methods, even the lowest-performing RAG, consistently outperform the non-retrieval methods Custom and CoT. This highlights the importance of retrieving specialized medical knowledge when using general-purpose LLMs to answer professional medical queries. Comparing different retrieval methods, GAR outperforms vanilla RAG by approximately 3% on average, with a maximum improvement of 4.37% across datasets. This indicates that generating multiple queries for retrieval provides consistent benefits. However, while performing well on EHRNoteQA, MedRAG demonstrates a negative effect on the other two datasets compared to vanilla RAG.\nNotably, the improvements achieved by our RGAR over GAR exhibit a positive correlation with the average length of the dataset's context. On EHRNoteQA, which has an average context length exceeding 3000 tokens, our approach achieved a 7.8% improvement. This validates the advantage of our Factual knowledge Extraction in enhancing retrieval effectiveness. Consequently, our method is particularly well-suited to real-world scenarios where complete electronic health records must be analyzed to provide medical advice. This indicates that our approach is promising for real-life applications in assisting physicians with clinical recommendations.\nWhen analyzing performance across different datasets, we find that retrieval-based methods perform significantly better on MedQA-USMLE and EHRNoteQA, while MedMCQA showa a negative effect-consistent with results reported by MedRAG (Xiong et al., 2024a). A closer analysis reveals that MedMCQA incorporates arithmetic reasoning questions (roughly 7% of the total), and the addition of extensive retrieved contexts diminishes the model's numerical reasoning capabilities, which could potentially be fixed with larger base LLMs (Mirzadeh et al., 2025). Nonetheless, among retrieval-based methods, our RGAR stands out as the only approach that outperforms vanilla RAG on this dataset, delivering an improvement of more than 1% over Custom. On EHRNoteQA, while RGAR's performance is slightly below that of i-MedRAG, the latter's inference time is approximately 4 times longer, establishing RGAR as a more efficient and cost-effective alternative."}, {"title": "4.2.2 Base LLMs with Different Sizes and Model Families", "content": "To further assess the versatility of RGAR, we conduct evaluations on MedQA-USMLE, a widely used medical dataset, by utilizing base LLMs of various sizes and model families, specifically from Llama and Qwen. The results in Table 3 show that RGAR consistently achieves the best average performance.\nWhen considering model size, we find that retrieval-based approaches fall short of the non-retrieval Custom baseline for smaller models, such as Llama-3.2-1B-Instruct and Qwen2.5-1.5B-Instruct. These smaller models, constrained by their weaker performance, are not well-suited to leverage retrieval-enhanced information. As the model size increases, however, all retrieval-enhanced approaches exhibit notable performance gains, with RGAR yielding the most significant improvements. This trend becomes particularly pronounced for larger models. For example, RGAR achieves a 7.38% improvement over RAG on Llama-8B, 5.33% on Llama-3B, 5.03% on Qwen-8B, and 4.32% on Qwen-3B.\nMoreover, we find that under the same experimental conditions, Llama-3.1-8B-Instruct achieves a performance of 69.52% with RGAR, surpassing the 66.22% reported by MedRAG for GPT-3.5-16k-0613 (Achiam et al., 2023). This significant improvement underscores the practicality of using well-optimized retrieval methods with smaller models, enabling performance rivals those of proprietary large-scale foundational models in real-world medical recommendation tasks."}, {"title": "4.3 Ablation Study", "content": "Due to the absence of ground-truth retrieval chunks, we evaluate retrieval effectiveness through QA performance, systematically varying the number of retrieved chunks N from 4 to 32. A reduced retrieval number serves as a more stringent assessment of retrieval quality. We investigate three primary factors in Figure 3: the effect of options generated by GAR versus those originally provided by the dataset, the contributions of CKR and FKE components, and the impact of RGAR's recurrence rounds.\nWe first compare the retrieval performance between LLM-generated options and original dataset options. Figure 3a shows how RGAR and GAR perform across different values of N. Both approaches maintain stable performance across different N, indicating reliable retrieval quality. While using original options shows slightly higher average Accuracy, the difference is minimal. This suggests that even when GAR generates options that differ from the originals, it achieves similar retrieval results as long as the core topics align.\nWe then examine the impact of RGAR's two main components\u2014CKR and FKE\u2014as shown in Figure 3b. When we remove the conceptual knowledge interaction from the FKE phase, the system shows only moderate improvements when extracting factual knowledge from EHR without conceptual knowledge, demonstrating the importance of integrating both types of knowledge. Removing the multi-query generation step from CKR causes performance to degrade as N increases, indicating that multiple queries are necessary to maintain stable retrieval.\nFinally, we analyze the effect of rounds in RGAR (Round 0 means GAR), as illustrated in Figure 3c. Our results show that even a single iteration significantly improves performance by enabling interaction between factual and conceptual knowledge. Multiple rounds work similarly to a reranking mechanism (Mao et al., 2021b), improving the ranking of important chunks and showing substantial gains even with relatively small N. With N = 8, the default two-round setup achieves a performance of 75.78%, almost 1% better than using a single round. However, adding more rounds shows no clear benefits, as they tend to generate multi-hop factual knowledge during the FKE phase, leading CKR to retrieve multi-hop conceptual knowledge, which may cause LLMs to over-infer (Yang et al., 2024b). Given that each round involves one reasoning step from both the LLM extractor and LLM query generator, two rounds sufficiently support multi-hop reasoning needs (Lv et al., 2021)."}, {"title": "4.4 Fine-Grained Performance Analysis", "content": "While the previous sections examined overall dataset performance and established preliminary findings, this section provides a detailed analysis of specific aspects of our results. In \u00a7 4.2.1, we showed that RGAR performs better on real-world medical recommendation tasks involving comprehensive EHRs. To verify this finding, we conduct a detailed analysis of EHRNoteQA by grouping questions based on context length and dividing them into four bins. Within each bin, we compare the performance of RGAR, GAR, and Custom. As shown in Figure 4, Custom shows decreasing accuracy with increasing context length. GAR improves accuracy across all bins, with RGAR achieving further performance gains. Notably, the improvements are more significant in the three bins with longer contexts compared to the first bin. The results show that RGAR maintains consistent average performance across different context length.\nIt is also important to note that generating multiple queries from different aspects within RGAR helps stabilize retrieval. Figure 5 presents a t-SNE visualization of different queries and their individually retrieved chunks for a sample question (details provided in Appendix B). The basic query shows limited suitability for retrieval, as its coverage area differs from that of the three queries generated by RGAR. RGAR clearly introduces some variation in retrieval content. Although the regions corresponding to the three generated queries overlap, the specific chunks retrieved do not overlap significantly. This underscores the need to average the retrieval similarities of these three queries to achieve more stable retrieval results."}, {"title": "5 Conclusion", "content": "In this work, we propose RGAR, a novel RAG system that distinguishes two types of retrievable knowledge. Through comprehensive evaluation across three factual-aware medical benchmarks, RGAR demonstrates substantial improvements over existing methods, emphasizing the significant impact of in-depth factual knowledge extraction and its interaction with conceptual knowledge on enhancing retrieval performance. Notably, our RGAR enables the Llama-3.1-8B-Instruct model to outperform the considerably larger, RAG-enhanced proprietary GPT-3.5. From a broader perspective, RGAR offers a promising approach for enhancing general-purpose LLMs in clinical diagnostic scenarios where extensive factual knowledge is crucial, with potential for extension to other professional domains demanding precise factual awareness."}, {"title": "Limitations", "content": "Despite RGAR achieving superior average performance, several limitations warrant discussion. Our RGAR requires corpus retrieval, and its time complexity scales proportionally with the size of the corpus, which is an inherent issue within the RAG paradigm. Approaches that generate reasoning evidence directly through domain-specific LLMs (Yu et al., 2023; Frisoni et al., 2024) avoid the computational challenges at inference time. However, they face difficulties in updating LLMs to incorporate new medical knowledge, which results in frequent updates and training costs.\nComparative approaches such as MedRAG (Xiong et al., 2024a) and i-MedRAG (Xiong et al., 2024b) explore integration possibilities with prompting techniques like Chain-of-Thought (Wei et al., 2022) and Self-Consistency (Wang et al., 2023a) to enhance reasoning capabilities. Our investigation focused specifically on validating how additional factual knowledge processing improves retrieval performance, without examining the impact of these prompting strategies. Furthermore, unlike multi-round methods such as i-MedRAG (Xiong et al., 2024b) that implement LLM-based early stopping to reduce computational costs, our system operates with fixed time complexity. However, it is noteworthy that, because i-MedRAG requires multiple rounds of query decomposition, retrieval, and answer aggregation, the actual time overhead of RGAR is significantly smaller than that of i-MedRAG.\nOur EHR extraction approach assumes LLMs can process complete EHR contextual input, justified by current mainstream LLMs exceeding 128K context windows with anticipated growth. However, in extreme cases where EHR content exceeds LLM context limits, integration with chunk-free approaches may be necessary (Luo et al., 2024; Qian et al., 2024). Finally, as RGAR operates in a zero-shot setting without instruction fine-tuning, its effectiveness is partially contingent on the model's instruction-following capabilities\u2014which we cannot fully mitigate."}, {"title": "Ethical Statement", "content": "This research adheres to the ACL Code of Ethics. All medical datasets utilized in this study are either open access or obtained through credentialed access protocols. To ensure patient privacy protection, all datasets have undergone comprehensive anonymization procedures. While Large Language Models (LLMs) present considerable societal benefits, particularly in healthcare applications, they also introduce potential risks that warrant careful consideration. Although our work advances the relevance of retrieved content for medical queries, we acknowledge that LLM-generated responses based on retrieved information may still be susceptible to errors or perpetuate existing biases. Given the critical nature of medical information and its potential impact on healthcare decisions, we strongly advocate for a conservative implementation approach. Specifically, we recommend that all system outputs undergo rigorous validation by qualified medical professionals before any practical application. This stringent verification process is essential to maintain the integrity of clinical and scientific discourse and prevent the propagation of inaccurate or potentially harmful information in healthcare settings.\nThese ethical safeguards reflect our commitment to responsible AI development in the medical domain, where the stakes of misinformation are particularly high and the need for reliability is paramount."}, {"title": "C.1 Another View of the Recurrence Pipeline", "content": "We conceptualize the Recurrence Pipeline as an exploration-exploitation process within the reinforcement learning framework (Auer et al., 2002). In GAR, even when generated content is only partially accurate (or potentially inaccurate), it remains valuable for retrieval if it correlates with passages containing correct information (e.g., co-occurrence with correct answers), thus representing an exploratory phase. Conversely, EHR extraction serves as an exploitation phase, thoroughly utilizing explored knowledge by selecting relevant components and synthesizing new evidence (factual knowledge). Based on this newly derived evidence, subsequent iterations can initiate fresh exploration-exploitation cycles, creating a continuous knowledge transmission process (Zhu et al., 2024c).\nIn scenarios where additional factual knowledge is not required, the retrieved content tends to remain relatively constant, and utilizing this content under identical prompting conditions would likely yield similar factual knowledge through extraction and summarization. However, when conceptual knowledge is needed to derive new factual knowledge through reasoning from existing factual information, the updated basic query facilitates easier retrieval of conceptual knowledge supporting current reasoned factual knowledge, thereby maintaining the integrity of reasoning chains. Furthermore, leveraging current factual knowledge for retrieval enables the exploration and discovery of novel knowledge domains."}, {"title": "C.2 Why No Flexible Stopping Criteria", "content": "Similar multiround RAG systems have adopted more flexible stopping criteria. For instance, Adaptive RAG (Jeong et al., 2024) determines whether to retrieve further by consulting the model itself. i-MedRAG (Xiong et al., 2024b), while setting a maximum number of retrieval iterations, also supports early stopping.\nIn our RGAR framework, we do not adopt such settings. On the one hand, we focus on evaluating how additional processing of factual knowledge enhances retrieval performance, raising awareness of this often-overlooked type of knowledge in previous RAG systems, while flexible stopping criteria mainly showcase procedural knowledge and metacognitive knowledge. On the other hand, the metacognitive capabilities of current LLMs remain under question, as a model's self-evaluation of the need for additional retrieval information often does not match actual requirements (Kumar et al., 2024)."}, {"title": "C.3 Future Work", "content": "Our RGAR framework leverages retrieved medical domain knowledge to deliver exceptional answer quality. However, we are concerned that such powerful generative capabilities, if maliciously exploited, could pose security risks. For instance, when the retrieved corpus contains private or copyrighted information, malicious users could exploit the LLM's responses to extract and disclose sensitive data from the corpus (Carlini et al., 2021). Additionally, malicious users might attempt to replicate our base LLM (Tram\u00e8r et al., 2016; Zhu et al., 2024a) by collecting large volumes of question-answer pairs or infer internal details of our retrieval-based generation framework (Carlini et al.). We will make every effort to mitigate these risks, such as verifying the legitimacy of queries (Inan et al., 2023) and watermarking the models used in RGAR (Zhu et al., 2024b)., ensuring that RGAR is used responsibly and legally."}, {"title": "D Comparative Analysis of Dataset Length Distributions", "content": "In this section, we present additional visualizations comparing the two categories of datasets we described, and explain our rationale for excluding the MMLU-med dataset (Hendrycks et al., 2021). We plotted smoothed Kernel Density Estimation (KDE) curves for these datasets, as shown in Figure 6. Our analysis confirms that datasets containing Electronic Health Records (EHR) consistently demonstrate greater length compared to those without EHR content. However, certain datasets exhibit complex question sources and types. For instance, while the MMLU dataset shows a considerable mean length of 84 tokens and a maximum length of 961 tokens, as illustrated in the figure, the vast majority of its questions lack EHR content and are predominantly shorter in length. This characteristic led to our decision to exclude it from our experimental evaluation. 6"}, {"title": "A Implementation Details", "content": ""}, {"title": "A.1 Hardware Configuration", "content": "All experiments were conducted on an in-house workstation equipped with dual NVIDIA GeForce RTX 4090 GPUs, 128GB RAM, and an Intel\u00ae Core i9-13900K CPU."}, {"title": "A.2 Code and Results", "content": "The core implementation of the RGAR framework and the output json files can be accessed via the Anonymous Repository: https://anonymous. 4open.science/r/RGAR-C613"}, {"title": "B Prompt Template and Case Study", "content": "For simplicity, we merged EHR and question in the prompt words of the answer and treated them as question in the prompt words."}, {"title": "C Framework Insight", "content": ""}, {"title": "C.1 Another View of"}]}