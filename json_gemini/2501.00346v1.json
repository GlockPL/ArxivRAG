{"title": "CNC: Cross-modal Normality Constraint for Unsupervised\nMulti-class Anomaly Detection", "authors": ["Xiaolei Wang", "Xiaoyang Wang", "Huihui Bai", "Eng Gee Lim", "Jimin Xiao"], "abstract": "Existing unsupervised distillation-based methods rely on the\ndifferences between encoded and decoded features to lo-\ncate abnormal regions in test images. However, the de-\ncoder trained only on normal samples still reconstructs ab-\nnormal patch features well, degrading performance. This is-\nsue is particularly pronounced in unsupervised multi-class\nanomaly detection tasks. We attribute this behavior to 'over-\ngeneralization' (OG) of decoder: the significantly increasing\ndiversity of patch patterns in multi-class training enhances\nthe model generalization on normal patches, but also inad-\nvertently broadens its generalization to abnormal patches. To\nmitigate 'OG', we propose a novel approach that leverages\nclass-agnostic learnable prompts to capture common textual\nnormality across various visual patterns, and then apply them\nto guide the decoded features towards a 'normal' textual rep-\nresentation, suppressing 'over-generalization' of the decoder\non abnormal patterns. To further improve performance, we\nalso introduce a gated mixture-of-experts module to special-\nize in handling diverse patch patterns and reduce mutual in-\nterference between them in multi-class training. Our method\nachieves competitive performance on the MVTec AD and\nVisA datasets, demonstrating its effectiveness.", "sections": [{"title": "Introduction", "content": "Visual anomaly detection (AD) mainly focuses on identi-\nfying unexpected patterns (deviating from our familiar nor-\nmal ones) within samples. Industrial defect detection is one\nof the most widely used branches of AD (Bergmann et al.\n2019), which requires models to automatically recognize\nvarious defects on the surface of industrial products, such\nas scratches, damages, and misplacement. Due to the in-\nability to fully collect and annotate anomalies, unsupervised\nmethods (Yu et al. 2021; Gudovskiy, Ishizaka, and Kozuka\n2022; Liu et al. 2023b) become mainstream solutions for\nAD. Previous unsupervised methods mostly train one model\nfor one class of data, which requires large parameter storage\nand long training time as the number of classes increases.\nTherefore, UniAD (You et al. 2022) proposed a challeng-\ning multi-class AD setting, i.e., training one model to detect\nanomalies from multiple categories.\nReverse distillation (RD) (Deng and Li 2022) is a highly\neffective unsupervised AD (UAD) method. It employs a\nlearnable decoder (student network) to reconstruct features\nfrom a pre-trained encoder (teacher network) on normal\nsamples via patch-level cosine distance minimization. Ide-\nally, the learned decoder should only recover the encoded\nnormal patches, while failing to reconstruct unseen abnor-\nmal patterns. Anomaly regions are then detected by compar-\ning features before and after decoding. In multi-class train-\ning, a single model is optimized on the normal samples\nfrom multiple classes to achieve unified detection. While\nthe increasing training diversity can improve model gen-\neralization on reconstructing normal patches, it also leads\nto undesired generalization to unseen abnormal patch pat-\nterns. Consequently, abnormal regions are recovered well\nduring inference, narrowing the difference between encoded\nand decoded features and degrading detection performance\n(Fig. 1(B) I.). We term this issue 'over-generalization' (OG).\nThe key question remains: How can we effectively mitigate\n'OG' while preserving the generalization on normal samples\nin the multi-class distillation framework?\nTo address this challenge, we seek to incorporate an ad-\nditional constraint in the decoding process. Leveraging in-\nsights from vision-language models (VLMs) (Radford et al.\n2021), we observe that normal and abnormal regions within\na sample exhibit distinct responses to the same text descrip-\ntion, as illustrated in Fig. 1(A). We propose to exploit this\ndistinction in cross-modal response to differentiate the de-\ncoding of normal and abnormal patch features, thereby hin-\ndering the recovery of abnormal patterns. Specifically, we\nemploy class-agnostic learnable prompts to extract the com-\nmon normality from encoded visual features across differ-\nent classes. These prompts serve as anchors in the textual\nspace, aligning the decoded normal features with a univer-\nsal representation of normality and suppressing the 'over-\ngeneralization' of the decoder towards abnormal patterns\n(Fig. 1(B) II.). We also design a normality promotion mech-\nanism for feature distillation, introducing cross-modal acti-\nvation as a control coefficient on visual features to increase\nsensitivity to unexpected abnormal patterns. We term the"}, {"title": "Related Work", "content": "Visual anomaly detection contains various settings accord-\ning to specific engineering requirements, e.g., unsupervised\nAD (Yi and Yoon 2020; Zou et al. 2022; Gu et al. 2023;\nCao, Zhu, and Pang 2023; Liu et al. 2023a; Zhang, Xu,\nand Zhou 2024), zero and few-shot AD (Huang et al. 2022;\nFang et al. 2023; Lee and Choi 2024), noisy AD (Chen\net al. 2022; Jiang et al. 2022), and 3D AD (Gu et al. 2024;\nCostanzino et al. 2024; Liu et al. 2024; Li et al. 2024a).\nExisting unsupervised AD methods can be roughly divided\ninto reconstruction-based (Tien et al. 2023; Lu et al. 2023b;\nHe et al. 2024b), feature-embedding-based (McIntosh and\nAlbu 2023; Roth et al. 2022; Lei et al. 2023), augmentation-\nbased (Zavrtanik, Kristan, and Sko\u010daj 2021; Zhang, Xu, and\nZhou 2024; Lin and Yan 2024) methods.\nReconstruction-based Method The reconstruction-based\nmethod employs the autoencoder framework to learn the\nnormality of training samples by reconstructing data or its\nfeatures. Therefore, some works (Zhang et al. 2023; Guo\net al. 2024) rethink RD as reconstruction-based method. Al-\nthough this type of approach offers fast inference speed, the\nanomaly localization is inevitably degraded by \u2018OG', which\nis attributed to the increasing diversity of patch patterns in\nmulti-class training. To address the issue, we propose CNC\nand MoE to alleviate undesired generalization."}, {"title": "Preliminaries", "content": "CLIP Contrastive Language Image Pre-training (Radford\net al. 2021) (CLIP) is a large-scale vision-language model\nfamous for its multi-modal alignment ability via training\nwith a lot of image-prompt pair data. Specifically, given\nan unknown image x and text-prompts {p1, p2, \u2026, pJ},\nCLIP can predict the probability of alignment between x and\nevery prompt pj as follows:\n$p(y|x) = \\frac{exp(F(x)\\cdot G(p_y)/T)}{\\Sigma_{j=1}^{J} exp(F(x)\\cdot G(p_j)/T)},$ \nwhere F(.) and G(\u00b7) are CLIP visual and text encoder, re-\nspectively, and 7 is a temperature hyperparameter. Previ-\nous work (Jeong et al. 2023) adopts handcraft prompts,\nsuch as a photo of normal/damaged [class],\nto achieve zero-shot anomaly detection.\nPrompt Learning Prompt learning (Jia et al. 2022; Zhou\net al. 2022) focuses on optimizing the input prompts to\nenhance the language or multi-modal model performance\non specific tasks. CoOp (Zhou et al. 2022) introduces a\nlearnable prompt, p = [V1], [V2],\u2026\u2026, [VM], [class], to\nachieve few-shot classification, where each [Vm] is a learn-\nable token, and M is the number of tokens. However, in\nmulti-class UAD task, we do not expect to utilize any class\ninformation. Following works (Zhou et al. 2023; Li et al.\n2024b), our applied learnable prompts are defined as:\npn = [V1], [V2],\u2026\u2026, [VM], [object],\npa = [V1], [V2],\u2026\u2026, [VM], [damaged], [object],\nwhere pn and pa are normal and abnormal prompts respec-\ntively. We argue that utilizing category-agnostic prompts en-\nables the learning of common normality patterns across sam-\nples from different classes. Different from previous meth-\nods, we apply these prompts to learn the normality of train-\ning samples and leverage them to guide visual decoding."}, {"title": "Methodology", "content": "The framework of the proposed method is shown in Fig. 2.\nOur method consists of three main sections: (1) Visual Dis-\ntillation Framework; (2) Cross-modal Normality Constraint;\n(3) Gated Mixture-of-Experts. In (1), we introduce our basic\nreverse distillation network. In (2), we propose cross-modal\nnormality constraint to ensure decoded features to align with\na textual 'normal' representation. Additionally, we propose\na cross-modal control coefficient on the visual feature to im-\nprove sensitivity to abnormal patch patterns, which is called\nfeature-level normality promotion. In (3), a gated mixture-"}, {"title": "Overview", "content": "of-experts (MoE) module is shown in detail to specifically"}, {"title": "Visual Distillation Framework", "content": "Compared with previous single-modal distillation-based\nmethods (Deng and Li 2022; Tien et al. 2023), we se-\nlect multi-modal backbone, CLIP-ViT, as encoder, which\nmeans that text-modal information can be adopted to im-\nprove detection performance. Specifically, for a given im-\nage x \u2208 RHo\u00d7W\u00d73, the CLIP visual encoder F encodes\nx into multi-layer latent space features as {fi}Ni=1, where\nfi \u2208 RH\u00d7W\u00d7C represents i-th layer feature (feature size\nin each layer of ViT is consistent), and N is the number of\nresidual attention layers in ViT (Dosovitskiy et al. 2020). We\nselect 11-th, i2-th, 13-th layer features as visual encoded fea-\ntures. For convenience, let F1, F2, F\u00e5 denote 11-th, i2-th,\ni3-th blocks respectively, and f1, f2, f3 is the corresponding\nencoded features. For visual decoder, we adopt three gen-\neral residual attention blocks (the basic module of ViT) to-\ngether as decoder and extract corresponding features to re-\nconstruct. Therefore, we denote decoder as FD with three\nblocks {F}3=1, with corresponding decoded features as\n{fi}3=1 (see Fig. 2). In addition, to further alleviate the\n'over-generalization', Gaussian noise is applied on the en-\ncoded feature fi to obtain its perturbed version froise."}, {"title": "Cross-modal Normality Constraint", "content": "To alleviate the unexpected 'over-generalization' in multi-\nclass training, we propose a text-modal normality constraint\nto guide the decoded features towards a 'normal' textual\nrepresentation, suppressing the \u2018over-generalization' of the\ndecoder towards the abnormal direction. The key to our\nproposed cross-modal normality constraint lies in applying\nlearnable category-agnostic prompts to learn common nor-\nmality from various normal samples and maintain the se-\nmantic consistency of visual encoded and decoded features\nin textual space during the training phase.\nLearning Cross-modal Normality In this section, we aim\nto apply class-agnostic learnable prompts to learn textu-\nral normality from various encoded features. Specifically,\nfor a given image x, we apply {F}31 blocks in en-\ncoder to obtain multiple layer features {fi}31. According\nto Eq. (2) and Eq. (3), we initialize three sets of learn-\nable prompts {[pnm, pa]}31, where {ph}3=1 are applied to\nlearn textual normality from different layer visual encoded\nfeatures. Then, each prompt pair [p, pa] is input to CLIP\ntext encoder G(\u00b7), producing the corresponding text feature\n[gn, ga], where gn = G(pn), ga = G(p). Next, we employ\na modal-alignment optimization object to learn the textual\nnormality from {fi}31:\n$\\mathscr{L}_1 = \\sum_{i=1}^3 -log\\frac{exp(e_i \\cdot g_n/T)}{exp(e_i \\cdot g_n/T) + exp(e_i \\cdot g_a/T)},$\nwhere er represents the global feature of fi, t is a tem-\nperature coefficient. Next, textual features {[9%, 9]}=1 are\nadopted in feature distillation and decoding to alleviate un-\nexpected 'over-generalization'."}, {"title": "Feature Distillation with Normality Promotion", "content": "In this\nsection, textual features {[9%, 9]}=1 are applied on distilla-\ntion to improve sensitivity to anomaly patch patterns. Specif-\nically, we first define a cross-modal control coefficient on vi-\nsual encoded and decoded features f\u2081 and f\u2081. It is designed to\ncompute a cross-modal activation map between visual patch\nfeatures and text features gn, improving sensitivity to unex-\npected abnormal patch patterns. Specifically, we design the\nnew encoded feature f* with a cross-modal normality con-\ntrol coefficient as follows:\nf* = fi + \u03a8(\u03b1\u03af, \u03b2\u03af),\nwhere fi, f* \u2208 RH\u00d7W\u00d7C, X = 1/||fi|| is a scaled coeffi-\ncient, || \u00b7 || is the L2 norm, and control coefficient \u03a8(\u03b1, \u03b2) is\nwritten as\n$\\Psi(\\alpha_i, \\beta_i) = \\frac{1}{2} (1 + tanh(\\alpha_i - \\beta_i)),$\nwhere weight maps a and \u1e9e are defined as:\nai = fi gn, \u1e9ei = fi & ga,\nwhere gh\u2208 R1\u00d71\u00d7C, ai,\u1e9ei \u2208 RH\u00d7W, \u2297 denotes the\nvector-wise product between gin and each patch embedding\nz of fi, l is the index of patch embedding. Therefore,\n\u03a8(\u03b1\u0390, \u03b2\u2081) \u2208 RH\u00d7W, and \u2295 is element-wise addition opera-\ntion. Similarly, following Eq. (5) and Eq. (6), we also obtain\nthe decoded feature f* with cross-modal control coefficient:\nf = fi(i, \u03b2i),\nwhere a\u2081 = fign, Bi = fi & ga\nWe call the above step 'feature-level normality promo-\ntion' (FNP), as shown in Fig. 2. Next, we give a new cross-\nmodal distillation loss to ensure the consistency of the en-\ncoded and decoded features with the corresponding control\ncoefficient as follows:\n$\\mathscr{L}_{distill} = \\sum_{i=1}^3 (1 - \\frac{Flat(f_i^*)\\cdot Flat(\\hat{f}_i^*)}{Flat(f_i^*)| Flat(\\hat{f}_i^*)|}),$"}, {"title": "Feature Decoding with Normality Constraint", "content": "In this\nsection, we apply textual features {[g, 9]}=1 trained\nby (4) as anchors to guide feature decoding, alleviating un-\nexpected 'OG'. Our solution is to constrain the textual rep-\nresentation of the decoded features not to deviate from 'nor-\nmal' during training, i.e., we also keep class tokens of de-\ncoded features aligning with normal text features {g}:\n$\\mathscr{L}_2 = \\sum_{i=1}^3 -log\\frac{exp(\\hat{e}_i \\cdot g_n/T)}{exp(\\hat{e}_i \\cdot g_n/T) + exp(\\hat{e}_i \\cdot g_a/T)},$\nwhere \u00ea\u00bf represents the global feature of fi. We combine\nformulas (4) and (10) to give the cross-modal constraint loss:\n$\\mathscr{L}_{constraint} = \\begin{cases} \\mathscr{L}_1 & \\text{if epoch < v} \\\\ \\mathscr{L}_1 + \\gamma\\mathscr{L}_2 & \\text{if epoch > v} \\end{cases},$\nwhere y = 0.1 is a hyperparameter. Each text feature gn\nis a dynamic anchor that is used as a medium to keep the\ndecoded feature toward the 'normal' direction."}, {"title": "Gated Mixture-of-Experts Module", "content": "Multi-Layer Fusion Following works (Deng and Li 2022;\nTien et al. 2023), different layer features of pre-trained en-\ncoder are aggregated, improving detection performance. For\nan input x, we first apply encoder FE to extract multi-layer\nfeatures {fi} 1, and concatenate them as [f1, f2, f3]. We\nadopt a projection layer \u03a6(\u00b7) (a linear layer with dropout\nblock) to transfer its channel dimension to C:\nf = ([f1, f2, f3]),\nwhere f \u2208 RH\u00d7W\u00d7C is a fusion feature (see Fig. 2 MLF)\nand is input to the following MoE module.\nR\nGated Mixture-of-Experts We employ different expert\ncombination to handle different patch patterns, reducing the\nmutual interference between them. Specifically, for a given\nmini-batch of fusion features, we obtain a batch of patch em-\nbedding features {z}1, where R = B * H * W. Our goal\nis to assign different expert combinations to recognize differ-\nent patch patterns. Therefore, we first use a router network\nG(.) to assign an expert-correlation score to each patch, i.e.,\nHt = Gt(zr), t\u2208 {1,2,\u2026\u2026,T},\nwhere G() : RC \u2192 RT, T is the number of expert net-\nworks {Et()}=1, and each E\u2082(\u00b7) is conducted by a MLP.\nNext, we select experts with top K scores {Hk}=1 to han-\ndle the patch embedding vector zr, denoted by {Ek(\u00b7)}1\nNext, we obtain a unique combination of processes on each\npatch embedding, i.e.,\nz = \u2211 Hk * Ek (Zr).\nTo prevent the router from assigning dominantly large\nweights to a few experts, which can lead to a singular scor-\ning operation, we apply a universal importance loss (Bengio\net al. 2015) to optimize the MoE:\nSD(1G(zr))2\n$\\mathscr{L}_{moe} = \\frac{}{SD(1G(zr))^2 + \\varepsilon},$\nwhere SD() is standard deviation operation, \u025b is added\nfor numerical stability. Finally, according to Eq. (9), (11),\nand (15), we obtain a total loss to train our model:\nLtotal = Ldistill + Lconstraint + Lmoe."}, {"title": "Inference", "content": "The inference is consistent with the training phase. We apply\nencoder FE(\u00b7), learned prompts {[pm, Pa]}=1, multi-layer\nfusion (\u00b7), MoE module, and decoder FD (.) to produce en-\ncoded features {f}=1 and decoded features {f}3-1. We\ndesign a pixel-level anomaly score map as:\n3\nS(, ) = \u2211(1-d(f, k)),\nwhere d(,) is pixel-wise cosine similarity, \u03c3(\u00b7) is the up-\nsampling factor in order to keep the same size as the input\nimage. In addition, the image-level anomaly score is defined\nas the maximum score of the pixel-level anomaly map."}, {"title": "Experimental Setup", "content": "Datasets MVTec AD (Bergmann et al. 2019) is the most\nwidely used industrial anomaly detection dataset, contain-\ning 15 categories of sub-datasets. The training set consists\nof 3629 images with anomaly-free samples. The test dataset\nincludes 1725 normal and abnormal images. Segmentation\nmasks are provided for anomaly localization evaluation.\nVisA (Zou et al. 2022) is a challenging AD dataset contain-\ning 10821 images and 12 categories of sub-datasets.\nEvaluation Metrics Following the prior work (He et al.\n2024b), image-level Area Under the Receiver Operating\nCharacteristic Curve (I-AUROC) and Average Precision (I-\nmAP) are applied for anomaly classification. Pixel-level\nAUROC, pixel-level mAP, and AUPRO (Bergmann et al.\n2020) are used for anomaly localization.\nImplementation Details The implementation is based\non Pytorch. The publicly available CLIP model (VIT-\nL/14@336px) is the backbone of our method. We select the\nAdam optimizer to train our model. Then, we resize the res-\nolution of each image to 224 \u00d7 224. In addition, the length\nof each learnable text prompt is set to 12, consistent with\nprevious work (Zhou et al. 2023). For both datasets, we set\ntemperature coefficient \u0442 = 0.001 and batch size to 8 with\nlearning rate 0.001 to train the whole model. The number of\nexperts is set to 5 with top K = 2 gated scores in the MoE.\nNext, we set the epoch to 250 and 200 for MVTec AD and\nVisA with the same v = 5, respectively. All experiments are\nconducted on a single NVIDIA Tesla V100 32GB GPU."}, {"title": "Comprehensive Comparisons with SOTA Methods", "content": "In this section, we compare our approach with several SOTA\nmethods on MVTec AD and VisA datasets, where 5 different\nmetrics, I-AUROC, P-AURPOC, AUPRO, I-mAP, P-mAP,\nare shown for comprehensive evaluation in Table 1 and Ta-\nble 3, respectively."}, {"title": "Results on MVTec AD", "content": "As reported in Table 1, for the\nwidely used MVTec AD dataset, our cross-modal normality\ndistillation framework (CND) achieves five different metrics\nby 98.6/98.0/93.0/99.3/56.4, and the mean performance\nof five metrics by 89.0 under multi-class setting. Com-\npared to UniAD and DiAD, our method improves five met-\nrics by +2.1/1.2/2.3/0.5/13.0 and +1.4/1.2/2.3/0.3/3.8,\nand the mean metric by +3.8 and +1.7, respectively. Our\nmethod significantly outperforms the single-modal distil-\nlation framework, RD4AD, by +4.0/1.9/1.9/2.8/7.8, in\nterms of five metrics. In addition, our method is more stable\nthan previous methods, achieving a performance of 93.8+\nin terms of image and pixel-level AUROC for all categories.\nHowever, RD4AD merely achieves a 60.8 I-AUROC metric\non Hazelnut, and UniAD achieves a 63.1 P-AUROC metric\non Grid. To further illustrate the effectiveness of our pro-\nposed method in anomaly localization, we visualize UniAD\nand our method prediction detection results in Fig. 3 (B)."}, {"title": "Ablation Study", "content": "In this section, we investigate the contribution of different\nmain components in our approach. Additionally, we show\nresults on different backbones with different resolutions and\ninvestigate the impact of hyperparameters of MoE.\nEffectiveness of Main Components In Table 2, we re-\nport three main metrics, including I-AUROC, P-AUROC\nand AUPRO, to study the impact of each key component\non MVTec AD. As shown in line i and iii of Table 2,\nCNC improves our base model by +1.3/1.4/1.7. In ad-\ndition, equipped with 'MLF' module, we get a gain of\n+2.1/1.4/1.7 via CNC (see ii and v in Table 2). Therefore,\nour proposed CNC successfully suppresses undesired \u2018OG'.\nIn addition, the proposed MoE module alleviates 'OG' by\nassigning different weights to different patch patterns, im-\nproving performance by +1.0/0.5 in terms of I-AUROC\nand P-AUROC (see ii and vi). We also found that MoE en-\nhances image-level anomaly detection, enhancing +0.5 on\nI-AUROC metric (see v and vii in Table 2). Finally, our\ndesigned multi-layer fusion module can well fuse different"}, {"title": "Results on VisA", "content": "As reported in Table 3, for the VisA\ndataset, our proposed method also achieves a SOTA\nperformance, namely 93.2 and 98.5 in terms of I-\nAUROC and P-AUROC metrics, respectively. Compared\nto previous multi-class methods, our method outper-\nforms UniAD by +7.7/2.6/15.8/7.1/16.8, and DiAD by\n+6.4/2.5/16.2/4.3/11.7. Especially, in the 'fryum' and\n'capsules' categories, our method greatly improves anomaly\nclassification compared to UniAD (You et al. 2022) and (He\net al. 2024b). Finally, we also visualize our obtained local-\nization result via heat maps on VisA in Fig. 3 (A)."}, {"title": "Conclusion", "content": "In this paper, we propose a cross-modal distillation frame-\nwork to address the inevitable 'over-generalization' in multi-\nclass training. Firstly, we propose cross-modal normality\nconstraint (CNC) to guide decoded features to align the de-\ncoded features with a textual representation of normality,\nthereby improving the normality of the distilled features and\nfinal detection performance. We also propose a gated MoE\nmodule to re-weight different patch patterns, reducing the\nmutual interference between them. Finally, extensive exper-\niments show that our method achieves competitive perfor-\nmance on MVTec AD and VisA datasets."}]}