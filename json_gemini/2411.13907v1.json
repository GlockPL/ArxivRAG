{"title": "Split Federated Learning Over Heterogeneous Edge Devices: Algorithm and Optimization", "authors": ["Yunrui Sun", "Gang Hu", "Yinglei Teng", "Dunbo Cai"], "abstract": "Split Learning (SL) is a promising collaborative machine learning approach, enabling resource-constrained devices to train models without sharing raw data, while reducing computational load and preserving privacy simultaneously. However, current SL algorithms face limitations in training efficiency and suffer from prolonged latency, particularly in sequential settings, where the slowest device can bottleneck the entire process due to heterogeneous resources and frequent data exchanges between clients and servers. To address these challenges, we propose the Heterogeneous Split Federated Learning (HSFL) framework, which allows resource-constrained clients to train their personalized client-side models in parallel, utilizing different cut layers. Aiming to mitigate the impact of heterogeneous environments and accelerate the training process, we formulate a latency minimization problem that optimizes computational and transmission resources jointly. Additionally, we design a resource allocation algorithm that combines the Sample Average Approximation (SAA), Genetic Algorithm (GA), Lagrangian relaxation and Branch and Bound (B&B) methods to efficiently solve this problem. Simulation results demonstrate that HSFL outperforms other frameworks in terms of both convergence rate and model accuracy on heterogeneous devices with non-iid data, while the optimization algorithm is better than other baseline methods in reducing latency.", "sections": [{"title": "I. INTRODUCTION", "content": "With the rapid expansion of the Internet of Things (IoT), vast amounts of data are being generated, offering significant potential for extracting actionable value, particularly under the driven of machine learning (ML) and deep learning (DL) techniques. However, it may encounter significant challenges when related to privacy and security issues, making it difficult to fully realize the potential for integration and utilization of IoT data. Additionally, the heterogeneous environments created by diverse devices with varying computational resources and network conditions complicate model training and updates. To address these privacy and security concerns, as well as the challenges posed by heterogeneity, devising an effective learning framework that leverages federated and heterogeneous data while harvesting the computational resources of IoT devices presents an attractive solution [1].\nIn this regard, various distributed collaborative learning frameworks have emerged, such as federated learning (FL) [2], split learning (SL) [3], [4]. While FL has been well-studied, it still faces challenges in supporting large foun-\ndational models. In contrast, SL presents as an attractive solution by splitting the computation and preserving privacy simultaneously. SL achieves accuracy comparable to centralized learning (CL) [5], but the sequential training process can lead to prolonged training latency and under utilization of device resources. Consequently, there is ongoing discussion about enabling parallel SL by introducing different aggregation process or methods. However, several challenges remain hard to tackle. First, considering the heterogeneity in clients' storage and computational capacities, selecting the appropriate cut layer for each client to accommodate their resource constraints is crucial to prevent stragglers, which could slow down overall training latency. Second, parallel SL requires more frequent data exchanges between clients and the server, thus, communication latency can become a bottleneck for system performance, particularly as the number of clients increases or when the channel conditions fluctuate.\nIn existing attempts at parallel SL, the focus is primarily on achieving consistent updates for clients sharing the same cut layer, thereby maintaining global model coherence and improving training efficiency. Jeon et al. [6] initialize all clients with the same parameters and collects smashed data from each client to train the server-side model, returning the averaged gradients for updates. To further enhance privacy protection, Turina et al. [7] allocate an edge server for each client to store the server-side model and introduces FL to aggregate model parameters. Thapa et al. [8] propose a frame-work, Spiltfed, which incorporates federated aggregation on both the client and server sides, blending the strengths of SL and FL. Additionally, Oh et al. [9] introduce a local-loss-based training method and combine it with above approaches, further reduces training latency. Wu et al. [10] take into account the heterogeneity of client devices by clustering clients and allowing different clusters to adopt different cut layers, with parallel training within clusters and sequential training among them. However, all of the above assume the same cut layer for splitting, neglecting the differences in computational resources and heterogeneous data among edge devices.\nIn this paper, we propose a novel parallel SL frame-work, Heterogeneous Split Federated Learning (HSFL), that adaptively splits models on resource-constrained devices and conducts collaborative training with servers. To the best of"}, {"title": "II. HSFL DESIGN", "content": "As demonstrated in Fig. 1(a), the HSFL framework involves a set of clients  \\(K = \\{1,2,..., K\\}\\), a Main Server (MS) responsible for handling the computation of all the server-side models, and an Edge Server (ES) that aggregates and distributes the client-side models. The clients possess private data and let  \\(D_k = \\{x_{k,i}, y_{k,i}\\}\\) denote client  \\(k\\)'s local dataset, where  \\(x_{k,i}\\) denotes the  \\(i\\)-th sample and  \\(y_{k,i}\\) denotes the corresponding label, while  \\(D_k\\) represents the dataset's size. To enable training on resource-constrained clients, each client  \\(k\\in K\\) has an appropriate cut layer  \\(l_k\\) to fully utilize its computational and transmission resources while preventing it from falling behind during training. Then the clients and the MS cooperatively complete the model training by splitting global model  \\(W\\) into client-side model  \\(W_k^c\\) and server-side model  \\(W^s\\) with  \\(l_k\\), and these models are deployed on client  \\(k\\) and the MS, respectively. Once all clients complete their training, the ES and the MS will collaborate to aggregate models for the next training round. Specifically, we divide the HSFL workflow into three stages to provide a clearer understanding. Let  \\(A\\) denote the set of all training rounds, consisting of  \\(A\\) rounds, the workflow in round  \\(a\\) can be described as follows:\n1) Forward Propagation Stage: At this stage, each client uses its local dataset in parallel to complete the forward propagation (FP) in collaboration with the MS, and the process is completed through the following three steps:\ni) Client-side Forward Computation: Each client executes FP with  \\(W_k^c\\) and calculates the output as smashed data  \\(S_k\\). Here, HSFL adopts Mini-batch Gradient Descent (MBGD) with mini-batch data  \\(B_k = \\{X_k, y_k\\}\\), and  \\(S_k\\) is given by\n\\(S_k = f (X_k; W_k^{c,a}),\\)\n(1)\nwhere  \\(f ()\\) represents the network function.\nii) Smashed Data Transmission: Once completing client-side computing, the client transmits the smashed data  \\(S_k\\) and corresponding label  \\(y_k\\) to the MS.\niii) Server-side Forward Computation: Aggregating the smashed data and labels from clients, the MS executes the remainder of FP and outputs the predicted value\n\\(\\hat{y}_k = f (S_k; W^{s,a}),\\)\n(2)\nthus the averaged loss of this batch is calculated as\n\\(\\ell (B_k; W_k^{c,a}, W^{s,a}) = \\frac{1}{b} \\sum_{i=1}^{b} H (\\hat{y}_k, y_k),\\)\n(3)\nwhere  \\(b\\) is the batch size and  \\(H (\\cdot)\\) represents the cross-entropy loss function.\n2) Back Propagation Stage: At this stage, the MS and clients perform back propagation (BP) given the computed loss and updates models. Similar to forward propagation stage, there are three steps as:\ni) Server-side Backward Computation: Given the loss in Eq. (3), the gradients are calculated by  \\(\\nabla_{W^{s,a}} \\ell\\). The MS executes BP and updates  \\(W^s\\) using the learning rate  \\(\\eta\\):\n\\(W^{s,a} \\leftarrow W^{s,a} - \\eta \\nabla_{W^{s,a}} \\ell (B_k; W_k^{c,a}, W^{s,a}).\\)\n(4)\nii) Gradients Data Transmission: Then MS transmits the gradients of  \\(S_k\\) to the corresponding client as soon as they are computed.\niii) Client-side Backward Computation: After receiving the feedback gradients, the client updates  \\(W_k^{c,a}\\) accordingly using\n\\(W_k^{c,a} \\leftarrow W_k^{c,a} - \\eta \\nabla_{W_k^{c,a}} \\ell (B_k; W_k^{c,a}, W^{s,a}).\\)\n(5)"}, {"title": "3) Synchronous Aggregation Stage", "content": "At this stage, the ES and MS collaborate to aggregate the updated models, resulting in global client-side model and server-side model, which are used for the next round of training.\ni) Model upload: After finishing its last local epoch, each client transmits its updated client-side model to the ES. To enable synchronous aggregation, this step may incur idle waiting across multi-clients.\nii) Federated Aggregation: Note that among the heteroge-neous models due to different cut layers, specific layers (referred to as common layers in Fig. 1(a)) exist in both the ES and MS. So directly applying FedAvg [2] on the servers respectively would result in inconsistent aggregation for these layers. To address this challenge, we introduce an additional communication step between the ES and MS to exchange the parameters of the com-mon layers, ensuring that both sides maintain consistent updates when applying aggregation.\niii) Model Distribution: Once aggregation is completed, the ES distributes the aggregated client-side models back to the clients according to their respective cut layers, while the global server-side model is stored in the MS for the next training round."}, {"title": "III. SYSTEM LATENCY MODEL", "content": "To evaluate the training efficiency of the proposed frame-work, we model its total training latency as illustrated in Fig. 1(b). For ease of comparison we also analyze the latency of each stage in the workflow separately, and the detailed description is as follows:\n1) Latency of Forward Propagation Stage: let  \\(t_{k,k}^{fp} (l_k)\\) and  \\(t_{k,k}^{bp} (l_k)\\) denote the FP and BP computational workload (in FLOPs) with cut layer  \\(l_k\\). The latency of client-side forward and backward computation is given by\n\\(T_{k,k}^{cl} = \\frac{b_{k,k}^{fp} (l_k) + b_{k,k}^{bp} (l_k)}{f_k \\kappa_k},\\)\n(6)\nwhere  \\(f_k\\) denotes client  \\(k\\)'s CPU cycles per second.  \\(\\kappa_k\\) is the computing intensity which indicates the CPU cycles required for one float-point operation. For the transmission latency, data is transmitted over Orthogonal Frequency Division Multi-ple Access (OFDMA) with Time Division Duplexing (TDD), where subchannels are dynamically allocated for either uplink or downlink transmission at different times. Specifically, let  \\(r_{i,k}^{MS}\\) denote whether the channel  \\(i\\) is allocated to client  \\(k\\) or not, and let  \\(P_{k,MS}, g_{i,k}\\) represent client  \\(k\\)'s uplink transmit power and gain on subchannel  \\(i\\), respectively. The uplink transmission rate is given by\n\\(R_{k,MS}^U = \\sum_{i=1}^{I} r_{i,k}^{MS} B \\log_2 (1 + \\frac{P_{k,MS} g_{i,k}}{\\delta^2}),\\)\n(7)\nwhere  \\(I\\) is the number of subchannels and  \\(B\\) is the bandwidth of each subchannel,  \\(\\delta^2\\) is the PSD of noise. Letting  \\(b_{S_k}(l_k)\\) denote the data size (in bits) of the smashed data 1, and its transmission latency is given by\n\\(T_{k,MS}^U = \\frac{b_{S_k}(l_k)}{R_{k,MS}^U}.\\)\n(8)\n2) Latency of Back Propagation Stage: Similarly, let  \\(f_{k,s}\\) represent the computing frequency that the MS allocated to client  \\(k\\)'s server-side model training,  \\(b_{s}^{fp} (l_k)\\) and  \\(b_{s}^{bp} (l_k)\\) denote the computational workload of FP and BP respectively. The computing latency on the MS is given by\n\\(T_{k,MS}^s = \\frac{b_{s}^{fp} (l_k) + b_{s}^{bp} (l_k)}{f_{k,s} \\kappa_s},\\)\n(9)\nwhere  \\(\\kappa_s\\) denotes the MS's computing intensity. On the transmission aspect, let  \\(b_{G_k}(l_k)\\) denote the gradients' data size (in bits), the transmission latency is denoted by\n\\(T_{k,MS}^D = \\frac{b_{G_k}(l_k)}{R_{k,MS}^D},\\)\n(10)\nwhere  \\(R_{k,MS}^D\\) is the downlink transmission rate whose defini-tion is similar to Eq. (7).\n3) Latency of Synchronous Aggregation Stage: At this stage, the computational tasks consist of only a small amount of summation and averaging, making the computing latency negligible. For more simplicity, we also assume that the transmission of common layers between the ES and MS is efficient and minimal, thus, we neglect the latency associated with these two part. As a result, the overall latency consists of model transmission only. Let  \\(\\S_m(l_k)\\) denote the client-side model size (in bits); the uplink and downlink transmission latency between clients and the ES are given by\n\\(T_{k,ES}^U = \\frac{\\S_m(l_k)}{R_{k,ES}^U},\\)\n\\(T_{k,ES}^D = \\frac{\\S_m(l_k)}{R_{k,ES}^D},\\)\n(11)\nwhere  \\(R_{k,ES}^U\\) and  \\(R_{k,ES}^D\\) are defined similarly to  \\(R_{k,MS}^U, R_{k,MS}^D\\). Hence, for client  \\(k\\), the latency in a training round is  \\(T_k = N(T_{k,MS}^U + T_{k,MS}^D + T_{k,ES}^U + T_{k,ES}^D),\\) where  \\(T_{k,Ms} = T_{k,k}^{cl} + T_{k,s}^s\\). During the parallel training phase, the training latency is represented by the maximum among all clients and is given by\n\\(T_a = max\\{min \\{(N(T_{k,MS}^U + T_{k,MS}^D) + T_{k,ES}^U + T_{k,ES}^D)\\} + T_0\\},\\)\n(12)\nwhere  \\(T_0\\) denotes the system's latency tolerance, the minimiza-tion indicates that the system will stop waiting for stragglers exceeding this threshold, ensuring training efficiency."}, {"title": "IV. PROBLEM FORMULATION AND SOLUTIONS", "content": "To improve HSFL's training efficiency, we formulate a optimization problem to minimize the total latency, jointly optimizing following variables: 1. Cut layer selections: De-noted by  \\(\\bf{l} \\in \\mathbb{L} = \\{(l_1,l_2,...,l_K)\\} \\); 2. MS's Computing frequency allocation: Denoted by  \\(\\bf{F} = (f_1, f_2, ..., f_K)\\) and the element  \\(f_k\\) is defined in III-2; 3. Transmission resource allocation: To Simplify the optimization, we combine sub-channel allocation and transmit power allocation into a single"}, {"title": "A. Problem Formulation", "content": "variable, which is denoted by  \\(\\bf{P} = (P_{k,MS}^U, P_{k,MS}^D, P_{k,ES}^U, P_{k,ES}^D)\\), the element  \\(P_{k,MS}^U, P_{k,MS}^D, P_{k,ES}^U\\) and  \\(P_{k,ES}^D\\) represent the transmit power allocated to client  \\(k\\) in the uplink/downlink between it and the MS/ES, respectively. So the optimization problem is formulated as follows:\n\\(\\bf{P}: \\min_{\\bf{l},\\bf{F},\\bf{P}} T_a (\\bf{l}, \\bf{F}, \\bf{P})\\)\n\\(s. t.\\)\nC\u2081:  \\(l_k \\in \\{0, ..., I_{max}\\}, \\forall k \\in K\\),\nC\u2082:  \\(f_k \\in (0, f_s), \\forall k \\in K\\),\n\\(C_3: \\sum_{k=1}^{K} f_k \\le f_s,\\)\n\\(C_4: P_{k,MS}^U \\le p_{max}, P_{k,ES}^U \\le p_{max}, \\forall k \\in K,\\)\n\\(C_5: \\sum_{k=1}^{K} P_{k,MS}^U \\le p_{MS}^{max}, \\sum_{k=1}^{K} P_{k,ES}^U \\le p_{ES}^{max},\\)\n(13)\nwhere constraint C\u2081 limits selection of cut layer for each client, and  \\(I_{max}\\) denotes the largest cut layer available to client  \\(k\\) according to its storage. C\u2082 and C\u2083 indicate the limitation of the MS's computing frequency allocation and  \\(f_s\\) denotes the MS's total computing frequency. C\u2084 and C\u2085 guarantee the transmit power constraints of clients and servers.\nObviously,  \\(\\bf{P}\\) is a stochastic non-convex optimization prob-lem. Stochastic refers to the need for decisions to take into account the temporal dynamics of client computing abil-ity and channel conditions throughout the training process. non-convex refers to the non-convex objective and constraints, making the problem NP-hard."}, {"title": "B. Problem Solution", "content": "Note that solving  \\(\\bf{P}\\) directly is difficult, we propose an algorithm that simplifies the problem by decomposing it into manageable subproblems. Specifically, consider that the client's storage resource constraints, which indicates how large a client-side model the client can store, is stable during a training task, so the choice of cut layer  \\(\\bf{l}\\) should be fixed for the entire training process, representing a long-timescale variable. In contrast, the allocation of computational and transmission resources,  \\(\\bf{F}\\) and  \\(\\bf{P}\\), should be adjusted in each training round as short-timescale variables due to the dynamic computing capability of clients and channel conditions. In other words, we divide  \\(\\bf{P}\\) into long-timescale subproblem  \\(\\bf{P_L}\\) aiming to minimize the total training latency and short-timescale subproblem  \\(\\bf{P_S}\\) focusing on optimizing the single-round training latency, which are solved separately. The optimization algorithm flow is shown in Fig. 2.\nAt the beginning of training process, we solve the long-timescale subproblem  \\(\\bf{P_L}\\) which is given by\n\\(\\bf{P_L}:\\min_{\\bf{l}} T_a (\\bf{l}, \\bf{F}, \\bf{P})\\)\n\\(s. t.\\)\nC\u2081-C\u2085,\n(14)\nWhen optimal cut layer selection  \\(\\bf{l^*}\\) is obtained, the variables  \\(\\bf{F}\\) and  \\(\\bf{P}\\) in the  \\(\\bf{P_S}\\) are still coupled, jointly influencing the single-round training latency, as shown in Eq. (12). This presents a considerable challenge in optimizing them together. Therefore, we divide  \\(\\bf{P_S}\\) into  \\(\\bf{P_{S-1}}\\) to optimize  \\(\\bf{F}\\) and  \\(\\bf{P_{S-2}}\\) to optimize  \\(\\bf{P}\\) in each training round, which are given by\n\\(\\bf{P_{S-F}}:\\min_{\\bf{F}} T_a (\\bf{l^*}, \\bf{F}, \\bf{P})\\)\n\\(s. t.\\)\nC\u2082, C\u2083,\n\\(\\bf{P_{S-P}}:\\min_{\\bf{P}} T_a (\\bf{l^*}, \\bf{F}, \\bf{P})\\)\n\\(s. t.\\)\nC\u2084, C\u2085.\n(15)\n(16)"}, {"title": "1) Long-timescale Variables Solution", "content": "In  \\(\\bf{P_L}\\), the chal-lenges lies in how to reasonably integrate the dynamically changing client computing capabilities  \\(f\\) and channel condi-tions  \\(\\bf{G}\\) with the optimization variables to estimate the total latency. Additionally, HSFL allows clients to choose differ-ent cut layers, resulting in an exponentially large solution space, which becomes particularly significant when there are many choices. To tackle these challenges, we propose a GA algorithm based on SAA, where SAA leverages historical data samples of  \\(f\\) and  \\(\\bf{G}\\) to calculate the averaged single-round latency  \\(\\overline{T(G, f)}\\) as an approximation of  \\(T_a\\). Then GA efficiently searches for  \\(\\bf{l}\\) which minimize the  \\(\\overline{T(G, f)}\\). Thus, the objective of  \\(\\bf{P_L}\\) can be expressed by\n\\(\\sum_{\\alpha \\in A}T_a \\approx A\\overline{T (G, f)}.\\)\n(17)\nThe core steps of our SAA&GA-based algorithm are as fol-lows: First, assume  \\(\\bf{G}\\) and  \\(f\\) follow Gaussian distribution with their respective means and variances and we can optimize  \\(\\bf{F}\\) and  \\(\\bf{P}\\) with a given  \\(\\bf{l}\\),  \\(\\bf{G}\\) and  \\(f\\) with solving  \\(\\bf{P_S}\\), then we take  \\(S\\) historical samples of  \\(\\bf{G}\\) and  \\(f\\) to compute the single-round latencies on these samples and use their mean to approximate the expected value  \\(\\overline{T(G, f)}\\), which can be described by\n\\(\\overline{T(G, f)} \\sim \\sum_{s=1}^{S} T(\\bf{l}, \\bf{F^*}, \\bf{P^*}; \\bf{G^s}, f^s),\\)\n(18)\nwhere  \\(T(\\bf{l}, \\bf{F}, \\bf{P}; \\bf{G}, f)\\) denotes the optimized single-round training latency on historical sample  \\(s\\). Finally, we can find the  \\(\\bf{l^*}\\) by GA procedure.\nIn other words, our proposed algorithm sets the fitness of the standard GA as the  \\(\\overline{T(G, f)}\\). Since GA generally optimize towards individuals with higher fitness, we need set the fitness negative to indicate that we are aiming for minimum latency. More specifically, we initialize a population of  \\(P\\) individuals, where the fitness for individual  \\(l_p\\) is defined by\n\\(Fit(l_p) = -\\overline{T(G, f)}.\\)\n(19)\nThen we iterate over generations until the fitness of the best individual, which minimizes  \\(\\overline{T(G, f)}\\), shows no significant improvement for  \\(g\\) consecutive generations, yielding the  \\(\\bf{l^*}\\)."}, {"title": "2) Short-Timescale Variables Solution", "content": "For  \\(\\bf{P_{S-F}}\\), accord-ing to Eq. (9), (12), given  \\(\\bf{l^*}\\) and corresponding  \\(\\bf{P}\\), current  \\(f\\) and  \\(\\bf{G}\\), only  \\(T_{k,s}^s\\) in latency remains to be optimized. So the objective of  \\(\\bf{P_{S-F}}\\) can be expressed as\n\\(\\bf{P_{S-F}}: \\min_{\\bf{F}} \\max_{k \\in K} \\{\\frac{N}{\\kappa_s} (\\frac{b_{s}^{fp} (l_k) + b_{s}^{bp} (l_k)}{f_k}) + m_k\\},\\)\n\\(s. t.\\)\nC\u2082, C\u2083,\n(20)\nwhere  \\(m_k = N(T_{k,MS}^U + T_{k,ES}^U + T_{k,ES}^D) + T_0\\) and  \\(n_k = (b_{s}^{fp} (l_k) + b_{s}^{bp} (l_k)) /\\kappa_s\\) represent the values that has no relationship with  \\(\\bf{F}\\) and can be directly calculated. For this min-max problem with inequality constraints,"}, {"title": "V. SIMULATION AND PERFORMANCE EVALUATION", "content": "We deploy the ResNet-18 [13] network on HAM10000 [14], MNIST [15], and CIFAR-10 [16] datasets. The mini-batch size is set as 256 and the learning rate is set as 0.001. The system operates with a total bandwidth of 10MHz which are evenly distributed across 10 subchannels. The channel gain follows the standard norm distribution with 10-3W noise power. The transmit power of each client ranges from [1, 10]W, and the transmit powers of MS and ES are both 100W. The computing capability of each client is uniformly distributed within [0,10] \u00d7 1010cycles/s, and the computing capability of the MS is 100 \u00d7 1010cycles/s."}, {"title": "B. Performance Evaluation of HSFL", "content": "To demonstrate the performance improvement, the pro-posed framework is compared with baselines SplitFed [8], FL [2], SL [4], and CL [5]. I shows the model accuracy of those algorithms across different datasets. It is obvious that HSFL has the better performance than vanilla FL and parallel SL but slightly lower than CL. This is because our framework employs adaptive cut layers for resource-constrained device to conduct more local training. Additionally, we also compare the training convergences of these algorithms in Fig. 3. We can see that the convergence of HSFL is close to CL but faster than other baselines. This can also be explained by the fact that HSFL achieves more local update in the same training time."}, {"title": "C. Performance Evaluation of the Proposed Resource Opti-mization Algorithm", "content": "We compare the single-round latency among the proposed resource optimization algorithm and baselines: Rand Cut Layer Selections (RCLS): each client randomly selects its cut layer on feasible set. Shallowest Cut Layer Selections (SCLS) [8]: system adopts the same cut layer which is minimum of the maximum cut layers for all users. Even Computing Frequency Allocation (ECFA): the computing frequency of MS is allocated to clients evenly. Greedy-based Transmis-sion Resource Allocation (GTRA): the system allocate the"}, {"title": "VI. CONCLUSION", "content": "This paper proposes a parallel SL framework, HSFL, which allows different cut layers across all clients through additional transmission between servers. We analyse the workflow and latency of HSFL and then formulate an optimization prob-lem to minimize training latency, improving its efficiency in wireless networks. Simulation results show that HSFL outperforms existing parallel frameworks on accuracy, with a convergence speed close to CL. Additionally, our proposed"}]}