{"title": "Enhancing Chess Reinforcement Learning with Graph Representation", "authors": ["Tomas Rigaux", "Hisashi Kashima"], "abstract": "Mastering games is a hard task, as games can be extremely complex, and still fundamentally different in structure from one another. While the AlphaZero algorithm has demonstrated an impressive ability to learn the rules and strategy of a large variety of games, ranging from Go and Chess, to Atari games, its reliance on extensive computational resources and rigid Convolutional Neural Network (CNN) architecture limits its adaptability and scalability. A model trained to play on a 19 \u00d7 19 Go board cannot be used to play on a smaller 13 \u00d7 13 board, despite the similarity between the two Go variants. In this paper, we focus on Chess, and explore using a more generic Graph-based Representation of a game state, rather than a grid-based one, to introduce a more general architecture based on Graph Neural Networks (GNN). We also expand the classical Graph Attention Network (GAT) layer to incorporate edge-features, to naturally provide a generic policy output format. Our experiments, performed on smaller networks than the initial AlphaZero paper, show that this new architecture outperforms previous architectures with a similar number of parameters, being able to increase playing strength an order of magnitude faster. We also show that the model, when trained on a smaller 5\u00d75 variant of chess, is able to be quickly fine-tuned to play on regular 8 \u00d7 8 chess, suggesting that this approach yields promising generalization abilities. Our code is available at https://github.com/akulen/AlphaGateau.", "sections": [{"title": "1 Introduction", "content": "In the past decade, combining Reinforcement Learning (RL) with Deep Neural Networks (DNNs) has proven to be a powerful way to design game agents for a wide range of games. Notable achievements include AlphaGo's dominance in Go [17], AlphaZero's human-like style of play in Chess and Shogi [18], and MuZero's proficiency across various Atari games [15]. They use self-play and Monte Carlo Tree Search (MCTS) [6] to iteratively improve their performance, mirroring the way humans learn through experience, or intuition, and game-tree exploration.\nPrevious attempts to make RL-based chess engines were unsuccessful as the MCTS exploration requires a precise position heuristic to guide its exploration. Handcrafted heuristics such as the ones used in traditional minimax exhaustive tree searches were too simplistic, and lacked the degree of sophistication that the random tree explorations of MCTS expects to be able to more accurately evaluate a complex chess position. By combining the advances in computing powers with the progress of the field of Deep Learning, AlphaZero was able to provide an adequate heuristic in the form of a Deep Neural Network that was able to learn in symbiosis with the MCTS algorithm to iteratively improve itself."}, {"title": "2 Related Work", "content": "However, these approaches rely on rigid, game-specific neural network architectures, often representing games states using grid-based data structures, and process them with Convolutional Neural Networks (CNNs), which limits their flexibility and generalization capabilities. For example, a model trained on a standard 19 \u00d7 19 Go board cannot easily adapt to play on a smaller 13 \u00d7 13 board without significant changes to its internal structure, manual parameter transfer, and retraining, despite the underlying similarity of the game dynamics. This inflexibility is further compounded by the extensive computational resources required for training these large-scale models from scratch for each specific game or board configuration. If it was possible to make a single model train of various variants of a game, and on various games at the same time, it would be possible to speed up the training by starting to learn the fundamental rules on a simplified and smaller variant of a game, before presenting the model with the more complex version. Similarly, if a model learned all the rules of chess, it could serve as a strong starting point to learn the rules of Shogi, for example.\nIt could be possible to design a more general architecture for games such as Go, where moves can be mapped one-to-one with the board grid, so that a model could still use CNN layers and handle differently sized boards simultaneously, but this solution is no longer feasible when the moves become more complex, including having to move pieces between squares, or even dropping captured pieces back onto the board in Shogi.\nThose moves evoke a graph-like structure, where pieces, initially positioned on squares, are moved to different new squares, following an edge between those two squares, or nodes. As such, it is natural to consider basing an improved model on a graph representation, instead of a grid representation. We explore replacing CNN layers with GNN layers to implement that idea, and more specifically consider in this paper attention-based GNN layers, reflecting how chess players usually remember the structures that the pieces form, and how they interact with each other, instead of remembering where each individual piece is placed, when thinking about a position.\nRepresenting moves as edges in a graph also introduces the possibility to link the output policy size with the number of edges, to make the model able to handle different game variants with different move structures simultaneously. To do so, it becomes important to have edge features as well as node features, as they will be used to compute for each edge the equivalent move logit. As the classical attention GNN layer, the Graph Attention Network [19] (GAT) only defines and updates node-features, we propose a novel modification of the GAT layer, that we call Graph Attention neTwork with Edge features from Attention weight Updates (GATEAU), to introduce edge-features. We also describe the full model architecture integrating the GATEAU layer that can handle differently sized input graphs as AlphaGateau.\nOur experimental results demonstrate that this new architecture, when implemented with smaller networks compared to the original AlphaZero, outperforms previous architectures with a similar number of parameters. AlphaGateau exhibits significantly faster learning, achieving a substantial increase in playing strength in a fraction of the training time. Additionally, our approach shows promising generalization capabilities: a model trained on a smaller 5 \u00d7 5 variant of chess can be quickly fine-tuned to play on the standard 8 \u00d7 8 chessboard, achieving competitive performance with much less computational effort.\nReinforcement Learning. AlphaGo [17], AlphaZero [18], MuZero [15], and others have introduced a powerful framework to exploit Reinforcement Learning techniques in order to generate self-play data used to train from scratch a neural network to play a game.\nHowever, those frameworks use rigid neural networks, that have to be specialized for one specific game. As such, the training process requires a lot of computation resources. It is also not possible to reuse the training on one type of game to train for another one, or to start the training on a smaller and simpler variant of the game, before introducing more complexity.\nScalable AlphaZero. In the research of Ben-Assayag and El-Yaniv [2], using Graph Neural Networks has been investigated as a way to solve those issues. Using a GNN-based model, it becomes possible to feed as input differently-sized samples, such as Othello boards of size between 5 and 16, enabling the model to learn how to play in a simpler version of the game."}, {"title": "3 Setting", "content": "Our architecture is based on the AlphaZero framework, which employs a neural network $f_\\theta$ with parameters $\\theta$ that is used as an oracle for a Monte-Carlo Tree Search (MCTS) algorithm to generate self-play games. When given a board state $s$, the neural network predicts a (value, policy) pair $(v(s),\\pi(s,\\cdot)) = f_\\theta(s)$, where the value $v \\in [-1,1]$ is the expected outcome of the game, and the policy $\\pi$ is a probability distribution over the moves.\nWe utilize Algorithm 1 to train the models in this paper, with modifications to incorporate Gumbel MuZero [7] with a gumbel scale of 1.0 as our MCTS variant."}, {"title": "4 Proposed Models", "content": "This research focuses on implementing this idea in the context of chess. This requires to answer two questions: how to represent a chess position as a graph, and how to output a policy vector that is edge-based, and not node-based.\nThe architecture presented in this paper is based on GNNs, but using node features to evaluate the value head, and edge features to evaluate the policy head. As such, a GNN layer that is able to handle"}, {"title": "4.1 Motivation: Representing the Game State as a Graph", "content": "Many games, including chess, are not best represented as a grid. For example, chess moves are analogous to edges in a grid graph, and games like Risk naturally form planar graphs based on the map. As such, it makes natural sense to encode more information through graphs in the neural network layers that are part of the model."}, {"title": "4.2 Graph Design", "content": "In AlphaZero, a chess position is encoded as an\u00d7n\u00d7119 matrix, where each square on the n\u00d7n chess board is associated to a feature vector of size 119, containing information about the corresponding square for the current position, as well as the last 7 positions, as described in Table 1.\nWe will instead represent the board state as a graph G(V, E), with the n \u00d7 n squares being nodes V, and the edges E being moves, based on the action design of AlphaZero. Each AlphaZero action is a pair (source square, move), with n \u00d7 n possible source squares. In 8 \u00d7 8 chess, the 73 moves (resp. 49 in 5 \u00d7 5 chess) are divided into 56 queen moves (resp. 32), 8 knight moves (resp. 8), and 9 underpromotions (resp. 9) for a total of 4672 actions (resp. 1225). The edge associated with an action is oriented from the node corresponding to the source square, to the destination square of the associated move. In 8 \u00d7 8 chess, castling is represented with the action going from the king's starting square going laterally 2 squares. As this action encoding is a little too large, containing moves ending outside of the board that do not correspond to real edges, the constructed graph only contains 1858 edges (resp. 455), corresponding only to valid moves.\nThe node and edge features, of initial size 119 and 15, are detailed in Tables 1 and 2, respectively. Node features are based on AlphaZero's features, including piece type, game state information, and historical move data. Edge features encode move legality, direction, potential promotions, and piece-specific move capabilities. In the case of 5\u00d75 chess, we include the unused castling information, in order to have the same vector size of the 8 \u00d7 8 models. It would be possible to preprocess the node and edge features differently for different games or variants, but for simplicity we didn't do it.\nThe starting positions for all games played in our experiments were either the classical board setup in 8 \u00d7 8 chess, or the Gardner setup for 5 \u00d7 5 chess, illustrated in Figure 1."}, {"title": "4.3 GATEAU: A New GNN Layer, with Edge Features", "content": "The Graph Attention Network layer introduced by Veli\u010dkovi\u0107 et al. [19] updates the node features by averaging the features of the neighboring nodes, weighted by some attention coefficient. To be more precise, given the node features $h \\in \\mathbb{R}^{N \\times K}$, attention coefficients are defined as\n$e_{ij} = W_u h_i + W_v h_j$ (1)\nwith parameters $W_u, W_v \\in \\mathbb{R}^{K \\times K'}$. In the original paper, $W_u = W_v$, but as we are working with a directed graph, we differentiate them to treat the source and destination node asymmetrically. Then we can compute attention weights a, and use them to update the node features:\n$\\alpha_{ij} = softmax_j(LeakyReLU(e_{ij})) = \\frac{expLeakyReLU(e_{ij})}{\\Sigma_k expLeakyReLU(e_{ik})}$, (2)\n$h'_i = \\Sigma_{j \\in N_i} \\alpha_{ij} W h_j$ (3)\nwith parameters $W \\in \\mathbb{R}^{K \\times K''}$ and $a \\in \\mathbb{R}^{K'}$.\nThe main observation motivating GATEAU is that in this process, the attention coefficients $e_{ij}$ serve a role similar to node features, being a vector encoding some information between nodes i and j. As such, we propose to introduce edge features in place of those attention coefficients.\nOur proposed layer, called Graph Attention neTwork with Edge features from Attention weight Updates (GATEAU) takes the node features $h \\in \\mathbb{R}^{N \\times K}$ and edge features $g_{i,j} \\in \\mathbb{R}^{N \\times N \\times K'}$ as inputs. We start by simply updating the edge features similarly to Eq. 1:\n$g'_{ij} = W_u h_i + W_e g_{i,j} + W_v h_j$ (4)\nwith parameters $W_u, W_v \\in \\mathbb{R}^{K \\times K'}$ and $W_e \\in \\mathbb{R}^{K' \\times K'}$. Then the attention weights are obtained as in Eq. 2, by substituting the attention coefficients with our new edge features:\n$\\alpha_{ij} = softmax_j(LeakyReLU(a^T g'_{ij}))$ (5)\nwith parameter $a \\in \\mathbb{R}^{K'}$. Finally, we update the node features as in Eq. 3:\n$h'_i = W_o h_i + \\Sigma_{j \\in N_i} \\alpha_{ij} (W_h h_j + W_g g'_{ij})$ (6)\nwith parameters $W_o, W_h \\in \\mathbb{R}^{K \\times K''}$ and $W_g \\in \\mathbb{R}^{K' \\times K''}$. We add the self-edges manually as it is inconvenient for the policy head if they are included in the graph, and we mix back the values of the edge features back in the node features."}, {"title": "4.4 AlphaGateau: A Full Model Architecture Based on AlphaZero and GATEAU", "content": "Following the structure of the AlphaZero neural network, we introduce AlphaGateau, combining AlphaZero with GATEAU instead of CNN layers, and redesign the value and policy head to be able to exploit node and edge features respectively to handle arbitrarily sized inputs with the same number of parameters.\nWe define the following layers, which are used to describe AlphaGateau in Figure 2.\nAttention Pooling. In order to compute a value for a given graph, we need to pool the features together. Node features seem to be the more closely related to positional information, so we pool them instead of edge features. For this, we use an attention-based pooling layer, similar to the one described in Eq. 7 by Li et al. [14], which, for node features $h \\in \\mathbb{R}^{N \\times K}$ and a parameter vector $a \\in \\mathbb{R}^{K}$, outputs\n$\\alpha_i = softmax_i(LeakyReLU(a^T h_i))$,\n$H = \\Sigma_i \\alpha_i h_i,$ (7)"}, {"title": "5 Experiments", "content": "We evaluate AlphaGateau's performance in learning regular 8 \u00d7 8 chess from scratch and generalizing from a 5 x 5 variant to the standard 8 \u00d7 8 chessboard. The metric used to evaluate the models is the Elo rating, calculated through games played against other models (or players) with similar ratings. Due to computational constraints, we couldn't replicate the full 40 residual layers used in the initial AlphaZero paper, and experimented with 5 and 6 layer models. We also started exploring 8 layers, but these models required to generate a lot more data, which would make the experiment run an order of magnitude longer. Our results indicate that AlphaGateau learns significantly faster than a traditional CNN-based model with similar structure and depth, and can be efficiently fine-tuned from 5 \u00d7 5 to 8 \u00d7 8 chess, achieving competitive performance with fewer training iterations. 2 All models used in these experiments are trained with the Adam optimizer [12] with a learning rate of 0.001. All feature vectors have an embedding dimension of 128. The loss function is the same as for the original AlphaZero, which is, for $f_\\theta(s) = \\tilde{\\pi}, \\tilde{v}$,\n$L(\\pi,v, \\tilde{\\pi}, \\tilde{v}) = -\\pi^T log(\\tilde{\\pi}) + (v - \\tilde{v})^2$. (10)"}, {"title": "5.1 Implementation", "content": "Jax and PGX. As the MCTS algorithm requires a lot of model calls weaved throughout the tree exploration, it is essential to have optimized GPU code running both the model calls, and the MCTS search. In order to leverage the MCTX [8] implementations of Gumbel MuZero, all our models and experiments were implemented in Jax [3] and its ecosystem [11] [9]. PGX [13] was used for the chess implementation, and we based our AlphZero implementation on the PGX implementation of AZNet. We used Aim [1] to log all our experiments.\nTo estimate the Elo ratings, we use the statsmodels package [16] implementation of Weighted Least Squares (WLS).\nHardware. All our models were trained using multiple Nvidia RTX A5000 GPUs (Learning speed used 8 and Fine-tuning used 6), and their Elo ratings were estimated using 6 of those GPUs."}, {"title": "5.2 Evaluation", "content": "As each training and evaluation lasted a little under a week, we were not able to train each model configuration several times so far. As such, each model presented in the results was trained only once, and the confidence intervals that we include are on the Elo rating that we estimated for each of them, as described in the following.\nDuring training, at regular intervals (each 2, 5, or 10 iterations), the model parameters were saved, and we used this dataset of parameters to evaluate Elo ratings. In this section, we will call a pair (model, parameters) a player, and compute a rating for every player.\nWe initially chose 10 players, and simulated 60 games between each pair of players, to get initial match data M. For each pair of players that played a match, we store the number of wins $W_{ij}$, draws $d_{ij}$, and losses $l_{ij}$: $M_{ij} = (W_{ij}, d_{ij}, l_{ij})$. Using this data, we can roughly estimate the ratings $r \\in \\mathbb{R}^{N_{players}}$ of the players present in M using a linear regression on the following set of equations:\n$\\frac{r_j - r_i}{400} = \\frac{log(\\frac{W_{ij} + d_{ij} + l_{ij} + 1}{W_{ij} + \\frac{d_{ij}}{2} + 1})}{log(10)} $ for $i \\in M, j \\in M_i,$\n$\\Sigma_{i \\in M} r_i = |M| \\times 1000.$ (11)\nWe artificially add one draw to avoid extreme cases where there are only wins for one player and no losses, in which case the rating difference would theoretically be infinite. This is equivalent to a Jeffreys prior. The last equation fixes the average rating to 1000, as the Elo ratings are collectively invariant by translation.\nWe then ran Algorithm 2 to generate a densely connected match data graph, where each player played against at least 5 other players of similar playing strength. Finally, we used this dataset to fit a linear regression model (Weighted Least Squares) to get Elo ratings that we used in the results figures for the experiments. The confidence intervals were estimated by assuming that the normalized match outcomes followed a Gaussian distribution. If $p_{ij} = \\frac{e^{\\frac{r_i-r_j}{400}}}{1+e^{\\frac{r_i-r_j}{400}}}$ is the estimated probability that player i beats player j, we approximate the distribution that pij follows as a Gaussian, and using the delta method, we derive that $r_j - r_j$ asymptotically follows a Gaussian distribution of mean $\\frac{400}{log(10)}log(\\frac{W_{ij}+d_{ij}+1}{W_{ij}+d_{ij}+l_{ij}+1})$ and variance $(\\frac{400}{log(10)})^2\\frac{p_{ij}(1-p_{ij})}{(W_{ij}+d_{ij}+l_{ij})}$. The proof is detailed in Appendix A.2. Using the WLS linear model of statsmodels [16], we get Elo ratings for every player, as well as their standard deviations, which we use in the following to derive 2-sigma confidence intervals."}, {"title": "5.3 Experiments", "content": "Learning Speed. Our first experiment compares the baseline ability of AlphaGateau to learn how to play 8 \u00d7 8 chess from scratch, and compares it with a scaled down AlphaZero model. The AlphaZero model has 5 residual layers (containing 10 CNN layers) and a total of 2.2M parameters, and the AlphaGateau model also has 5 ResGATEAU layers (containing 10 GATEAU layers) and a total of 1.0M parameters, as it doesn't need a $N_{nodes} \\times h_s \\times N_{actions}$ fully connected layer in the policy head, and the GATEAU layers use 2/3 of the parameters a 3 \u00d7 3 CNN does."}, {"title": "5.4 Impact of the Frame Window and the Number of Self-play Games", "content": "In order to train deeper networks, we experimented with the number of self-play games generated at each generation, and with the size of the frame window. if seems from our results in Figure 7 that having more newly generated data helps the model learn faster. However, the time taken for each iteration scales linearly with the number of generated games, and the model is still able to improve using older data. As such, keeping a portion of the frame window from previous iterations makes for a good compromise. There are however options to improve our frame window selection:"}, {"title": "6 Conclusion", "content": "In this paper, we introduce AlphaGateau, a variant on the AlphaZero model design that represents a chess game state as a graph, in order to improve performance and enable it to better generalize to variants of a game. This change from grid-based CNN to graph-based GNN yields impressive increase in performance, and seems promising to enable more research on reinforcement-learning based game-playing agent research, as it reduces the resources required to train one.\nWe also introduce a variant of GAT, GATEAU, that we designed in order to handle edge features in a simple manner performed well, and efficiently.\nFuture Work. As our models were relatively shallow when compared to the initial AlphaZero, it would be important to confirm that AlphaGateau still outperforms AlphaZero when both are trained with a full 40-deep architecture. This will require a lot more computing time and resources.\nAs discussed in Section 5.4, our design of the frame window is a little unsatisfactory, and a future improvement would be to define an efficiently computable similarity metric between chess positions, that helps the neural network generalize.\nWe focused on chess for this paper, but there are other games that could benefit from this new approach. The first one would be shogi, as it has similar rules to chess, and the promising generalization results from AlphaGateau could be used to either train a model on one game, and fine-tune it on the other, or to jointly train it on both games, to have a more generalized game-playing agent. As alluded to in the Graph Design 4.2, more features engineering would be required to have node and edge features compatibility between chess graphs, and shogi graphs. It could also be possible to change the model architecture to handle games with more challenging properties, such as the game Risk, which has more than 2 players, randomness, hidden information, and varying maps, but is even more suited to being represented as a graph."}, {"title": "A Appendix / supplemental material", "content": "The Elo rating system was initially introduced as a way to assign a value to the playing strength of chess players. The rating of each player is supposed to be dynamic and be adjusted after each game they play to follow their evolution.\nThe Elo ratings are defined to respect the following property: If two players with Elo rating RA and RB played a game, the probability that player A wins is\n$E_A = \\frac{1}{1 + 10^{\\frac{R_B-R_A}{400}}}$ (12)"}, {"title": "A.2 Variance of estimated Elo rating difference", "content": "We assume that the outcome of a game between two players of Elo RA and RB is a bernouilli trial, with a probability that player A win being given by the central Elo equation 12. If we want to estimate the Elo of both players, we need to estimate that probability p. To do so, we can make the two players play n games, and sum the wins of player A, as well as half his draws, to get x = w + . From this, we can estimate the value of p using the estimator p = 2. In the case that one of the two players is significantly stronger than the other, \u00ee\u00ea\u00ee could be close to 0 or 1, in which case this estimator is wildly inacurrate. To remedy this, we will instead rely on a Jeffreys prior, to get the estimator $P_{Jeffreys} = \\frac{x+1/2}{n+1}$. We will note this estimator \u00eep in the following.\nFrom our assumptions, we have that x follows a binomial distribution B(n,p), and we will approximate the distribution that \u00ea\u00ee follows by a normal distribution $\\hat{p} \\sim \\mathcal{N}(p, \\frac{p(1-p)}{n})$.\nBy inverting the Elo equation 12, we can get the rating difference from the probability that A wins as\n$R_B - R_A = \\frac{400}{log(10)}log(\\frac{1}{p} - 1)$. (13)\nTherefore, posing $g(y) = \\frac{400}{log(10)}log(\\frac{1}{y} - 1)$, which is differentiable, we can use the delta method to get that g(p) is asymptotically Gaussian. The derivative of g is\n$g'(y) = \\frac{400}{log(10)} (\\frac{-1}{y^2} - \\frac{1}{y}) \\newline = \\frac{400}{log(10)} (\\frac{-1}{y^2} \\frac{y}{y-1}) \\newline = \\frac{400}{log(10)} \\frac{1}{y(y - 1)}$ (14)\nwhich gives\n$\\hat{R_B} - R_A = g(\\hat{p}) \\sim \\mathcal{N} (g(p), \\frac{\\hat{p}(1-\\hat{p})}{n} (g'(\\hat{p}))^2) \\newline \\sim \\mathcal{N} (\\frac{400}{log(10)} log(\\frac{1}{p}-1), \\frac{p(1-p)}{n} (\\frac{400}{log(10)} \\frac{1}{p(p - 1)})^2) \\newline \\sim \\mathcal{N} (\\frac{400}{log(10)} log(\\frac{1}{p}-1), \\frac{p(1-p)}{n} (\\frac{400}{log(10)})^2 \\frac{1}{(p(p - 1))^2})$ (15)"}]}