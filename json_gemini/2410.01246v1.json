{"title": "AHP-Powered LLM Reasoning for Multi-Criteria Evaluation of Open-Ended Responses", "authors": ["Xiaotian Lu", "Jiyi Li", "Koh Takeuchi", "Hisashi Kashima"], "abstract": "Question answering (QA) tasks have been extensively studied in the field of natural language processing (NLP). Answers to open-ended questions are highly diverse and difficult to quantify, and cannot be simply evaluated as correct or incorrect, unlike close-ended questions with definitive answers. While large language models (LLMs) have demonstrated strong capabilities across various tasks, they exhibit relatively weaker performance in evaluating answers to open-ended questions. In this study, we propose a method that leverages LLMs and the analytic hierarchy process (AHP) to assess answers to open-ended questions. We utilized LLMs to generate multiple evaluation criteria for a question. Subsequently, answers were subjected to pairwise comparisons under each criterion with LLMs, and scores for each answer were calculated in the AHP. We conducted experiments on four datasets using both ChatGPT-3.5-turbo and GPT-4. Our results indicate that our approach more closely aligns with human judgment compared to the four baselines. Additionally, we explored the impact of the number of criteria, variations in models, and differences in datasets on the results.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have shown remarkable capabilities in a wide range of tasks, such as natural language generation (Karanikolas et al., 2023; Ko et al., 2024), summarization (Ahmed and Devanbu, 2022; Laban et al., 2023), translation (Huang et al., 2023) and text classification (M\u00f8ller et al., 2024). Question Answering (QA) tasks have been widely studied and can be utilized to assess the breadth of knowledge and logical comprehension abilities of LLMs. These tasks are designed to probe the models not only for their factual accuracy but also for their ability to infer answers from complex queries, thereby reflecting their understanding of both context and content.\nOpen-ended questions are essential for generating new ideas, encouraging creativity, and facilitating deeper understanding, but require detailed and multi-faceted consideration for evaluation. The superiority or inferiority of an answer depends on which evaluation criteria are used, and an overall decision must be made by synthesizing evaluations based on different evaluation criteria, which can be a very complex decision-making task.\nIn this study, we consider the use of LLMs to evaluate responses to open-ended questions. In order to have LLMs evaluate answers from multiple perspectives, we focus on the Analytic Hierarchy Process (AHP) (Saaty, 1987, 2004), which is known as a mathematical decision-making tool in operations research, and execute the decision-making procedure using LLMs. AHP evaluates candidates according to multiple evaluation criteria and makes a final decision by weighting them according to the relative importance of the evaluation criteria. By having LLMs perform the AHP process, we expect to enhance the ability of LLMs to evaluate multiple aspects objectively. Specifically, our proposed method could be divided into two phases: the criteria generation phase and the evaluation phase. We first have an LLM generate multiple evaluation criteria for answers to a question and determine the relative importance of these criteria through pairwise comparisons in the criteria generation phase. We then perform pairwise comparisons of candidate answers for each criterion in the evaluation phase. Finally, these results are integrated to arrive at a final decision.\nOur results demonstrate that multiple criteria can effectively evaluate answers to open-ended questions. We conducted experiments on 4 datasets on ChatGPT-3.5-turbo and GPT-4. Compared to baseline methods, our approach performs better on quantitative indicators, concordance index, and soft concordance index. The contributions of this study are summarized as follows:\n\u2022 We propose a new method that leverages LLMs to evaluate answers to open-ended questions. We conducted experiments using two different LLMs across four datasets.\n\u2022 Our findings indicate that our proposed method outperforms four baseline models. Notably, we observed that pairwise comparison plays a crucial role in assessing answers to open-ended questions.\n\u2022 We explored the impact of the number of evaluation criteria on the results. Our results demonstrate that using multiple criteria yields better performance."}, {"title": "2 Related Works", "content": "Reja et al. (2003) examined human responses to the same question presented in two formats: open-ended and close-ended questions. The findings revealed that open-ended questions elicited a more diverse range of answers from participants. This suggests that open-ended questions play a more active role in exploring new ideas, requiring not just background knowledge and logical reasoning, but also a higher degree of intelligence and creativity.\nBelay et al. (2022) proposed using the AHP to employ multicriteria analysis on the success factors of the Ethiopian construction industry, aiming to enhance decision-making capabilities in the construction sector.\nSvoboda and Lande (2024) utilized the AHP and GPT-4 to generate answers for open-ended questions as an automated decision-making process. However, the proposed method lacked validation and quantitative evaluation metrics. Given the inherent uncertainty of open-ended questions, quantitative evaluation poses significant challenges. Our research focuses on developing and implementing quantitative evaluation metrics for our proposed methodology and evaluating the effectiveness of the combined use of AHP and GPT-4 in answering open-ended questions.\ndel Gobbo et al. (2023) summarized deep learning approaches for the automated scoring of answers of students to open-ended questions. Those methods utilize deep learning to extract representations of answers and employ supervised learning with ground truth scores. However, such approaches have two main drawbacks. First, they require training or at least fine-tuning the network for different questions. Second, they rely on the availability of ground truth labels.\nSimilarly, Uto and Uchida (2020) proposed the use of LSTM networks for scoring answers of students to open-ended questions. Such supervised learning methodology could properly assess the ability of students but might not facilitate the discovery of new knowledge or identify better answers to open-ended questions without prior knowledge. The presence of ground truth labels implies that the superior answers are predetermined, limiting the potential for innovation in question answering. Our proposed method can be directly applied to LLMs without the need for training or ground truth labels. By quantitatively assessing the responses to open-ended questions, our approach allows for the discovery of new insights."}, {"title": "3 Methodology", "content": "3.1 Preliminaries: Analytic Hierarchy Process\nAHP is a decision-making technique through evaluating possible candidates under multiple evaluation criteria (Saaty, 1987, 2004) and is extensively applied across various fields to tackle complex decision-making problems (Liu et al., 2020; Bruno et al., 2012; Podvezko et al., 2009; Sari et al., 2017).\nFor example, when choosing a restaurant for dinner, we might consider several criteria such as the quality of the food, service, and price. To prioritize these criteria, we use the pairwise comparison method, which involves comparing them against each other. We might ask, \"Which is more important: the quality of the food or the service?\" to determine the relative importance of each criterion. Then, using the same pairwise comparison approach, we assess the restaurants under each criterion by asking questions such as \"Does Restaurant A have better food than Restaurant B?\".\nThe overall scores for the restaurants are calculated by performing a matrix multiplication of the scores for each criterion with the corresponding weights of these criteria.\n3.2 AHP-Powered LLM Reasoning for Evaluating Open-Ended Responses\nOur proposed method which has LLMs automatically evaluate existing human open-ended responses, is divided into two distinct phases: the criteria generation phase and the evaluation phase. In the criteria generation phase, we generate multiple criteria to evaluate answers, tailored to capture the various dimensions of answer quality. In the subsequent evaluation phase, the answers are subjected to pairwise comparisons using the previously generated criteria. \nCriteria Generation Phase\nWe first randomly extract a small set, which contains m responses, from each dataset, resulting in $mP2$ ordered pairs. For each pair, we ask LLMs to explain why the first answer in the pair is better than the second and to summarize 2 or 3 reasons. These reasons are subsequently used as the criteria for later evaluations. After querying the LLMs about all $mP2$ pairs, we typically gather around $(mP2)$ reasons. Many of these reasons are repetitive. We input all collected reasons back into the LLMs, which then rank and output the top k reasons according to their importance and frequency of occurrence. We use these top k reasons as k evaluation criteria.\nEvaluation Phase\nWe utilize the previously prepared k evaluation criteria to conduct pairwise comparisons on all answers under each criterion. For a given answer pair i and j, we provide five options for the LLMs to select from: \"answer i is better than answer j\", \"answer i is slightly better than answer j\", \"almost the same\", \"answer j is slightly better than answer i\", \"answer j is better than answer i\". Under each evaluation criterion, we can construct a $k \\times n \\times n$ tensor A, which are the outcomes of pairwise comparisons, where n is the number of responses. We have simplified the original nine choices of the AHP to only five options for the LLMs to choose from. The definition of tensor A is as follows:\n$A_{kij} = \\begin{cases}\n5 & \\text{if } i \\gg j \\text{ under criteria } k, \\\\\n3 & \\text{if } i > j \\text{ under criteria } k, \\\\\n1 & \\text{if } i = j \\text{ under criteria } k, \\\\\n\\frac{1}{3} & \\text{if } j> i \\text{ under criteria } k, \\\\\n\\frac{1}{5} & \\text{if } j \\gg i \\text{ under criteria } k,\\end{cases}$ \nwhere $\\gg$ refers to 'better than', $>$ refers to 'slightly better than' and $i = j$ refers to 'almost the same', respectively. Numbers '1', '3', and '5' are an example of specified numbers in the AHP, which can be different numbers in the other cases.\nNext, we calculate the weights for each criterion by first constructing a preference $k \\times k$ matrix W:\n$W_{pq}=\\begin{cases}\n3 & \\text{if } p<q, \\\\\n1 & \\text{if } p=q, \\\\\n\\frac{1}{3} & \\text{if } p>q.\\end{cases}$\nSince we have already asked the LLMs to rank the k criteria by importance during their generation, we assume that earlier criteria are more important than later ones. Differing from typical AHP, we did not perform pairwise comparisons between criteria as LLMs tend to produce output \"almost the same\" in the importance comparisons of criteria.\nNext, we calculate the k-dimensional vector $w = \\sigma^{\\dagger}(W)$, where $\\sigma^{\\dagger}(.)$ returns the eigenvector corresponding to the largest eigenvalue of the input matrix. We use the normalized vector w as the weights for the criteria, where\n$w_i = \\frac{\\omega_i}{\\sum_{j=1}^k \\omega_j}$\nThe scores for each answer under each criteria $k \\times n$ matrix S can be calculated as\n$S_k = \\sigma^T(A_k)$,\nand normalized scores under each criteria S can be calculated as\n$S_{ki} = \\frac{S_{ki}}{\\sum_i S_{ki}}$\nFinally, we combine all the criteria to calculate the final score for each answer,\n$s = S^T \\overrightarrow{\\omega}$."}, {"title": "4 Experiments", "content": "We conducted experiments to answer the following three research questions:\nRQ1 Could our proposed method effectively evaluate open-ended responses?\nRQ2 What is the impact of multiple criteria on the results?\nRQ3 What are the findings in experiments with different LLMs?\n4.1 Experiment Settings\nWe utilized four datasets, Part-time job and Smoking are from The International Corpus Network of Asian Learners of English (ICNALE) (Ishikawa, 2018), while Meeting and Cheat are focused on proposing ideas for practical issues (Baba et al., 2020; Li, 2022; Zhang et al., 2022). We randomly selected 80 human responses to open-ended questions from each dataset.\nIn the Part-time job and Smoking datasets, there are English essays written by students at four different CEFR levels, with 20 individuals per level. We use the English CEFR level as the ground truth, where individuals with higher CEFR levels should receive higher scores.\n4.2 Baselines\nTo quantitatively compare our proposed method, our experiments include four baselines, summarized as follows:\n\u2022 Pairwise Comparison: Unlike our proposed AHP-powered multicriteria evaluation, this baseline involves having LLMs perform direct pairwise comparisons, deciding which answer is better without any criteria.\n\u2022 Scoring: We have LLMs score answers on a scale from 0 to 100.\n\u2022 Few-shot In-context Learning: We ask LLMs to assign a level to each answer based on two given examples. In the Part-time job and Smoking dataset, we select two answers from each of the four levels as examples. In the Meeting and Cheat dataset, we extract the two best answers, two answers from the top 33%, two from the top 66%, and the two worst answers to serve as examples for four levels.\n\u2022 CEFR Level: We instruct LLMs to evaluate answers based on CEFR definitions, such as for CEFR B2 writing level, which is defined as \"Can write clear, detailed texts on different subjects. Can use information and arguments from other sources in their writing.\" This baseline is used only in the Part-time job and Smoking dataset.\nOur proposed method, compared to other baselines, requires more LLM queries. We performed 108079/2 = 31600 comparisons for each dataset. Each comparison contains roughly 300 tokens, therefore the total input for each dataset is about 9.4 million tokens. The price for ChatGPT-3.5-turbo API was USD 3 per million input tokens and USD 5 per million input tokens for GPT-40. We believe that the cost is relatively affordable and expect that future developments for saving LLM costs will make the use of the proposed method easier.\nWe use the concordance index to measure the discrepancy between the scores given by the evaluation method for each answer and the scores provided in the ground truth. The concordance index is defined as follows,\n$CI(f, g) = \\frac{\\sum_i\\sum_j (f(x_i)>f(x_j))I(g(x_i)>g(x_j))}{\\sum_i\\sum_j (g(x)>g(x_j))}$,\nwhere x is the dataset, f is the evaluation method, g is the ground truth and I is the indicator function. When using the concordance index, some answers with minor differences may also be included in the calculation, therefore, we introduce soft as another metric that only takes answers with large differences into consideration and is defined as follows,\n$sCI(f, g) = \\frac{\\sum_i\\sum_j (f(x_i)>f(x_j))I(g(x_i)\\gg g(x_j))}{\\sum_i\\sum_j (g(x_i)\\gg g(x_j))}$,\nwhere $\\gg$ represents a significant difference between two answers. In the Part-time job and Smoking dataset, It indicates that the level difference is greater than or equal to 2. In the Meeting and Cheat dataset, it indicates that rankings differ by at least 20 positions within the dataset consisting of a total of 80 responses.\n4.3 RQ1: Could our proposed method effectively evaluate open-ended responses? \u2013 Yes.\nTables 3 and 4 show the results of the experiments. They indicate that even without multiple criteria, pairwise comparison significantly outperforms other baselines. This demonstrates that pairwise comparison is crucial for complex open-ended questions, as other baselines fail to elicit sufficiently effective information from LLMs. Figures 3, 4, and 5 demonstrate that without pairwise comparisons, LLMs are unable to effectively and appropriately evaluate the quality of open-ended responses. Our proposed AHP-powered multicriteria evaluation outperforms all baselines except for the Meeting dataset with ChatGPT-3.5 while the performance of our proposed method is slightly below that of pairwise comparison.\n4.4 RQ2: What is the impact of multiple criteria on the results? \u2013 Multiple criteria are helpful.\nFigure 6 illustrates the impact of multiple criteria. As the number of criteria increases, the average performance progressively improves. The leftmost of Figure 6 represents the performance when only one criterion is selected. We can observe a significant performance gap between the best and the worst criteria. The worst criteria often perform worse than direct pairwise comparisons without any criteria; while the average performance of all single criteria often performs worse than the all 10 criteria combined in the AHP. Proper criteria are assigned higher importance and weight. Even if a good response scores low under less important criteria, it can still achieve high overall scores if it scores high on important criteria. Therefore, after combining all criteria, the performance of our proposed multiple criteria method is better than pairwise comparisons without criteria.\n4.5 RQ3: What are the findings in experiments with different LLMs? \u2013 The performance depends on the prompt.\nTables 3 and 4 indicate that GPT-4 performs better in AHP and pairwise comparisons than ChatGPT-3.5-turbo. Table 5 shows the responses of GPT-3.5-turbo and GPT-4 to five select options. ChatGPT-3.5-turbo rarely selects options containing \u2018slightly' or 'almost the same', whereas GPT-4 frequently chooses options with \u2018slightly,' demonstrating better flexibility of evaluative scales. However, GPT-4 also rarely selects 'almost the same,' despite a large number of responses at the same level in the dataset.\nNevertheless, GPT-4 does not exhibit stronger capabilities than ChatGPT-3.5-turbo in Scoring, Few-shot, and CERF Level evaluations significantly as shown in Tables 3 and 4. Figures 3, 4, and 5 show that the diversity of answers is not better than ChatGPT-3.5-turbo. For example, in the CERF Level evaluation for the Part-time jobs dataset, GPT-4 assigns almost all responses to Level 3. This suggests that GPT-4 does not have a better understanding of all prompts compared to ChatGPT-3.5-turbo."}, {"title": "5 Conclusion", "content": "In this study, we proposed an evaluation approach to open-ended responses by AHP-Powered LLM reasoning to evaluate open-ended responses based on multiple different criteria by LLMs. We conducted experiments on four datasets and with two types of LLMs, and we used quantitative indicators, concordance index, and soft concordance index, to demonstrate that our method outperforms the baselines empirically. Three takeaways can be summarized as follows based on the results.\n\u2022 In the absence of pairwise comparisons with different answers, LLMs perform poorly in directly evaluating open-ended responses, tending to assign mid-to-high-level scores to all responses. This leads to both good and poor responses receiving similar scores, resulting in a lack of differentiation.\n\u2022 AHP-Powered multiple criteria perform better compared to pairwise comparison without criteria. Multiple criteria can effectively enhance the performance of LLMs.\n\u2022 GPT-4 does not necessarily perform better than ChatGPT-3.5-turbo, and choosing the appropriate prompt method is crucial for relatively difficult tasks.\nLimitations\nThis study involved experiments across two task types, four datasets, and two large models. Due to time and financial constraints, our experiments do not extend to more tasks currently. Compared to simpler baseline methods, our approach requires additional computations within LLMs, which in turn increases both time and financial costs."}]}