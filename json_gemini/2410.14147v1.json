{"title": "Leveraging Large Language Models for Enhancing Public Transit Services", "authors": ["Jiahao Wang", "Amer Shalaby"], "abstract": "Public transit systems play a crucial role in providing efficient and sustainable trans-\nportation options in urban areas. However, these systems face various challenges in\nmeeting commuters' needs. On the other hand, despite the rapid development of\nLarge Language Models (LLMs) worldwide, their integration into transit systems\nremains relatively unexplored.\n\nThe objective of this paper is to explore the utilization of LLMs in the public\ntransit system, with a specific focus on improving the customers' experience and\ntransit staff performance. We present a general framework for developing LLM ap-\nplications in transit systems, wherein the LLM serves as the intermediary for infor-\nmation communication between natural language content and the resources within\nthe database. In this context, the LLM serves a multifaceted role, including under-\nstanding users' requirements, retrieving data from the dataset in response to user\nqueries, and tailoring the information to align with the users' specific needs. Three\ntransit LLM applications are presented: Tweet Writer, Trip Advisor, and Policy\nNavigator. Tweet Writer automates updates to the transit system alerts on social\nmedia, Trip Advisor offers customized transit trip suggestions, and Policy Navigator\nprovides clear and personalized answers to policy queries. Leveraging LLMs in these\napplications enhances seamless communication with their capabilities of understand-\ning and generating human-like languages. With the help of these three LLM transit\napplications, transit system media personnel can provide system updates more effi-\nciently, and customers can access travel information and policy answers in a more\nuser-friendly manner.", "sections": [{"title": "1. Introduction", "content": "Does this scenario sound familiar? Picture this: After an exhausting day at work, you\nfind yourself at a bus stop, waiting for the bus to take you home. Predictably, yet still\nfrustratingly, the bus is late. You resort to checking the transit agency's social media\nfor real-time updates, which they claim to provide for your convenience. However, as\nis often the case, there's no update about your bus route. In response, you leave a\ncomment criticizing not just the tardy bus, but also the lack of timely information.\nLater, back home and working on your weekend plans, you still consider using public\ntransit despite the day's letdown. But as you start mapping out your journey, the"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Overview of Large Language Models", "content": "LLMs are advanced DL models utilized in Natural Language Processing (NLP) tasks,\nto comprehend and generate human-like texts. LLMs belong to the category of genera-\ntive artificial intelligence (AI) models, capable of not only understanding and analyzing\ndata but also generating novel outputs based on their training data. In this section, we\nprovide a general overview of LLMs, including their key characteristics and different\ntypes."}, {"title": "2.1.1. Key Characteristics", "content": "Transformer: Transformer (Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,\nKaiser, and Polosukhin (2017)) is the foundation of LLMs. According to (Vaswani\net al. (2017)), there are multiple advantages of using a Transformer as the base model\nfor LLMs, compared to other model structures that are widely used in Language Mod-\nels, such as Recurrent Neural Networks (RNNs) and Convolutional Neural Networks\n(CNNs). First, Transformer has better performance in capturing long-range depen-\ndencies within input sequences. Transformer is easier to learn information not only in\nthe sentence-dimensional, but also paragraph-dimensional, or even article-dimensional.\nSecond, Transformer does not rely on the sequence of input series, which makes the\nmodel capable of parallel computing and can highly improve the model's training and\ncomputing efficiency. Last but not least, the structure of the Transformer, built up\nwith Encoders and Decoders, is easy to modify, including the depth and width of the\nmodel. These features of Transformer make LLMs easy to have a deep model structure\nfor long-text learning tasks and can be trained with relatively less time and computing\nresources (Bietti, Cabannes, Bouchacourt, Jegou, and Bottou (2023)).\nLarge: Within the scope of Large Language Models (LLMs), the term 'Large' encap-\nsulates two significant aspects: the expansive volume of training data and the massive\nscale of model parameters. Training an LLM is not solely reliant on a single book\ndataset; it harnesses the power of multifaceted resources ranging from online forums,\nWikipedia, and news articles to code repositories. This diverse pool of training data\nprovides LLMs with a holistic and macroscopic understanding of various linguistic\nstyles, themes, and contexts, thereby enabling them to tackle a broader range of tasks\n(Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu (2019); Zhao, Zhou,\nLi, Tang, Wang, Hou, Min, Zhang, Zhang, Dong, et al. (2023)). Meanwhile, we ob-\nserve a growing trend in the size of the model architecture, with parameter counts\nincreasing into the billions, motivated by the requirement to optimally learn from in-\ncreasing data sizes (Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan,\nShyam, Sastry, Askell, et al. (2020)). The growth in both the volume of training data\nand the number of model parameters endows LLMs with emergent capabilities, which\nare not shown in the smaller language models, such as in-context learning, instruction\nfollowing, and step-by-step reasoning (Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud,\nYogatama, Bosma, Zhou, Metzler, et al. (2022a); Zhao et al. (2023)).\nTraining Process: To address the challenge of training with extensive volumes of\ndata, the LLM training pipeline is generally divided into two primary phases: Pre-\ntraining and Fine-tuning. The pre-training phase is the more time-consuming step,\nwhich involves self-supervised learning from large-scale datasets in a distributed man-\nner. This phase enables the model to acquire a broad, general-purpose problem-solving"}, {"title": "2.1.2. Types of Large Language Models", "content": "Large Language Models (LLMs) primarily fall into two categories: Base Language\nModels and Instruction-Fine-tuned Language Models. Base Language Models, such as\nT5 (Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu (2020)), GPT-\n3 (Brown et al. (2020)), and PaLM (Chowdhery, Narang, Devlin, Bosma, Mishra,\nRoberts, Barham, Chung, Sutton, Gehrmann, et al. (2022)), are fundamental LLMs\ntrained exclusively on large-scale text corpora without undergoing any fine-tuning pro-\ncesses for performance enhancement (Korbak, Shi, Chen, Bhalerao, Buckley, Phang,\nBowman, and Perez (2023)). Trained to predict subsequent words in a given context,\nthese models are not explicitly tailored for specific tasks but rather aim for the under-\nstanding of language structures and patterns. As for Instruction-Fine-tuned Language\nModels, such as T0 (Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai, Chaffin, Stiegler,\nScao, Raja, et al. (2021)), GPT-3.5/4 (Ouyang, Wu, Jiang, Almeida, Wainwright,\nMishkin, Zhang, Agarwal, Slama, Ray, et al. (2022)), and ChatGPT (Roumeliotis and\nTselikas (2023)), undergo an additional step of optimization based on base Language\nModels. Through fine-tuning with specific instructions, downstream tasks, or human\nfeedback, these models gain the ability to deal with specific language-related tasks,\nincluding question answering, text classification, summarizing, writing, keyword ex-\ntraction, etc (Lou, Zhang, and Yin (2023)). This category of LLMs offers specialized\nperformance, aiming for more task-focused applications of language models."}, {"title": "2.2. Large Language Model Applications", "content": ""}, {"title": "2.2.1. LLM Applications to Transportation Problems", "content": "Researchers are increasingly incorporating LLMs into the development of solutions\nfor conventional transportation tasks. For instance, in Ding, Abdel-Aty, Wang, Wang,\nZheng, Barbour, and Yuan (2023), LLMs were utilized to extract attributes from raw\ncrash reports and organize the extracted data according to a pre-set format. Addition-\nally, LLMs have shown promise in generating code for transportation-related software\nand data mining tasks within the transportation field. A demonstration of this ap-\nplication can be seen in Fu, Han, Su, and Fan (2023), where an LLM was used to\ninterpret urban information, generating code for ArcGIS and street view recognition\nprograms. Furthermore, LLMs have been employed in sentiment analysis for analyzing\ncitizens' behaviors. Beyond traditional information extraction and classification tasks\nthat rely on text input, LLMs can also be used for cross-modal applications. These\ninvolve answering questions based on image or audio inputs. This type of application\ncan be achieved by combining LLMs with other tools such as BLIP-2 (Li, Li, Savarese,\nand Hoi (2023b)) for vision-to-language encoding, and Whisper (Radford, Kim, Xu,\nBrockman, McLeavey, and Sutskever (2023)) for audio recognition. One application of\nthis approach, as demonstrated in Zheng, Abdel-Aty, Wang, Wang, and Ding (2023),"}, {"title": "2.2.2. Applications in Non-transportation Fields", "content": "On the other hand, LLMs have demonstrated their features in diverse non-\ntransportation fields, showing superior natural language understanding, generation,\nand manipulation capabilities. The adaptability of LLMs positions them as valuable\ntools with potential for adoption or adaptation in specific transportation contexts.\nHere, we explore how LLMs developed in different fields could find applications in\ntransportation-related areas.\nPlanning: The diverse applications of LLMs in fields such as optimizing construc-\ntion timetable (Prieto, Mengiste, and Garc\u00eda de Soto (2023)), supply chain decision-\nmaking (Frederico (2023)), and marketing strategies (Rivas and Zhao (2023)) sug-\ngest promising contributions to transportation planning. LLMs could optimize tran-\nsit driver schedules, enhance supply chain logistics for transportation infrastructure\nprojects, and craft effective communication strategies for public transportation initia-\ntives.\nManagement: In Liu, Zhong, Li, Yang, Ju, Wu, Ma, Shu, Chen, Kim, et al.\n(2023b); Singhal, Azizi, Tu, Mahdavi, Wei, Chung, Scales, Tanwani, Cole-Lewis, Pfohl,\net al. (2022), LLMs were used to answer radiology image-related queries after being\nfine-tuned with structured datasets containing patients' imaging findings and cor-\nresponding interpretations. This offers potential applications in traffic management\nwhere we can give management suggestions for the given structured traffic situation\ninformation. Fine-tuning LLMs with structured data from on-/off-road sensors and\ncorresponding traffic management strategies may empower them to analyze and re-\nspond to real-time traffic conditions, improving the efficiency of traffic flow and reduc-\ning congestion. The application of LLMs in multi-modal understanding and generation\n(Wu, Fei, Qu, Ji, and Chua (2023b)) could further enrich the data sources, including\ntraffic camera pictures, for fine-tuning LLMs in traffic management, and broaden the\napplication scenarios.\nEducation and Assessment: LLMs' proficiency in educational settings, as seen\nin the evaluation of ChatGPT by Kung, Cheatham, Medenilla, Sillos, De Leon,\nElepa\u00f1o, Madriaga, Aggabao, Diaz-Candido, Maningo, et al. (2023), suggests ap-\nplications in transportation education and assessment. Leveraging Chain-of-Thought\n(Li\u00e9vin, Hother, and Winther (2022)) or fine-tuning with domain-specific knowledge\n(Liu, He, Liu, Liu, and Zhai (2023a)) could enhance LLMs' performance in assessing\nthe knowledge and competence of transportation professionals. Additionally, utiliz-\ning LLMs for teaching assistance tasks, such as text and exam content generation\n(Kova\u010devi\u0107 (2023)) and feedback giving (Rudolph, Tan, and Tan (2023)), opens av-\nenues for educational initiatives in the transportation sector.\nHuman-Machine Interaction: The integration of LLMs into robotics suggests\npotential applications in human-autonomous vehicle interaction. Exemplified by Li,\nSousa, Mahadevan, Wang, Aoyagui, Yu, Yang, Balakrishnan, Tang, and Grossman\n(2023a), a GPT-3 model was deployed to process human audio instructions, enabling\nthe robot to move based on explicit movement directions and user preferences. In Chen,\nXia, Ichter, Rao, Gopalakrishnan, Ryoo, Stone, and Kappler (2023), an LLM was em-\nployed to generate a single comprehensive plan for robot control. Moreover, in Wu,\nAntonova, Kan, Lepert, Zeng, Song, Bohg, Rusinkiewicz, and Funkhouser (2023a), the\nPaLM model provides summarization capabilities, helping a household robot under-"}, {"title": "2.3. Discussion", "content": "While reviewing the state-of-the-art LLM-based applications in the transportation\nfield, we have noticed several limitations. First, these applications tend to focus more\non demonstrating specific functions of the LLM rather than integrating it into real-\nworld scenarios. Second, most of these applications are simple Q&A systems, where\nthere is only one-way communication with the LLM providing answers to user ques-\ntions. Lastly, these applications lack sufficient decision-making resources as they heav-\nily rely on user inputs without utilizing other supported resources for more reliable\nanswers.\nTo address these limitations and specifically focus on the area of public transit,\nwe introduce initial attempts in the following sections. We first introduce the basic\nmethodologies, including Chain-of-Thought and vector database. Then, we propose\nthe general structure for LLM applications in the transit domain. We further explore\nthree guiding applications related to customer experience in the public transit system.\nEach application pertains to a type of information communication scenario that may\narise. By showing the three guiding applications, we hope to give readers a better\nunderstanding of the technical requirements for building up similar transit digital\nassistants."}, {"title": "3. Leveraging LLMs for Public Transit System Services", "content": "Our objective in utilizing LLMs in the public transit system is to develop applica-\ntions that enhance the customer experience by delivering real-time and personalized"}, {"title": "3.1. Methodologies", "content": "In building our applications, we employ a range of methodologies in leveraging LLMs\nfor enhancing the public transit system experience, such as the Chain-of-Thought\napproach and Vector Database. Each of these methodologies contributes to the overall\nframework that underpins the three transit applications: Tweet Writer, Trip Advisor,\nand Policy Navigator. In the following subsections, we will delve into the specific\ndetails of each application, illustrating how these methodologies are applied to meet\nthe technical requirements and drive the functionality of the three applications."}, {"title": "3.1.1. Chain-of-Thought", "content": "While LLMs are capable of generating answers to users' queries, it is important to\nnote that the accuracy and consistency of these results is not guaranteed. According\nto a study by Kojima, Gu, Reid, Matsuo, and Iwasawa (2022), the accuracy of PaLM\ntested on MultiArith without any prompts (Zero-Shot) is significantly lower compared\nto cases using prompts. One particularly effective method for prompt engineering is\nthe Chain-of-Thought (CoT) approach, initially proposed by Wei, Wang, Schuurmans,\nBosma, Xia, Chi, Le, Zhou, et al. (2022b). The core idea behind CoT is to encourage\nthe LLM to demonstrate the thought process it undergoes to arrive at the final answer,\nrather than simply providing a direct response.\nThere are two main styles of CoT: Few-Shot CoT (Wei et al. (2022b)) and Zero-Shot\nCoT (Kojima et al. (2022)). In Few-Shot CoT, the LLM is provided with examples as\nreferences to guide the thought chain and derive the final result. Conversely, in Zero-\nShot CoT, a simple yet effective approach involves adding the sentence \"Let's think\nstep by step\" at the end of the original prompt. The experiment conducted in (Kojima\net al. (2022)) demonstrates the performance improvement achieved through Zero-Shot\nCoT."}, {"title": "3.1.2. Vector Database", "content": "Another technology utilized in our application development is a vector database. This\ntype of database is employed to address the requirements of querying (unstructured)\nresources with natural language input. Unlike traditional queries executed on struc-\ntured databases, such as relational databases, it is not feasible to directly translate\nuser queries into a specific database query language.\nThe concept of the vector database was initially introduced in Stata, Bharat, and\nMaghoul (2000), where the fundamental workflow for storage and querying on the vec-\ntor database is illustrated in Figure 2. The primary function of the vector database lies\nin the embedding process for the context and user query. By utilizing vector databases,\nit becomes feasible to perform similarity searches, such as KNN, in high-dimensional\nvector spaces. This is achieved by converting the unstructured user query and context"}, {"title": "3.1.3. General Application Structure", "content": "Before presenting the proposed application structure, let's begin by discussing the tra-\nditional information system structure. As depicted in Figure 3a, data, encompassing\nstructured information like system alerts and unstructured data such as policy infor-\nmation or feedback from agents, resides within the back-end data container. These\ndata sets undergo processing by human agents before being disseminated to the front\nend. Alternatively, the data can be directly transmitted to the front-end application,\nwhere it is accessible to customers.\nHowever, during this process, human agents are confronted with a substantial vol-\nlume of data, making it challenging to ensure the efficient publication of information.\nConversely, providing all information without tailoring, such as policy details, can\nprolong the time it takes for customers to access valuable information and potentially\ndiminish user satisfaction.\nTo optimize the information transformation process and deliver personalized infor-\nmation to transit users, we've integrated an Information System enhanced with Large"}, {"title": "3.2. Three LLM Applications to Transit", "content": "In this subsection, we present the three transit applications utilizing LLMs:\n\u2022 Tweet Writer: to help the staff in the media department in transit systems to\nefficiently disseminate transit system alerts on social media platforms;\n\u2022 Trip Advisor: to provide users with personalized trip recommendations;\n\u2022 Policy Navigator: to offer clear and tailored responses to policy-related user\nqueries.\nAll three applications require the LLM to provide accurate and consistent responses,\nadhering to specific formatting guidelines. These responses are constructed using in-\nformation sourced from both structured and unstructured databases. By delving into"}, {"title": "3.2.1. Tweet Writer", "content": "Contemporary transit agencies employ various forms of service alerts, encompassing\non-vehicle or on-station displays (CleverVision 4), official websites (TTC 5) or mobile\napplications (Transit 6), and text/SMS or email notifications (Peterborough Transit 7).\nAdditionally, there is an observed trend where transit agencies increasingly use social"}, {"title": "3.2.2. Trip Advisor", "content": "Transit agencies typically provide a web page for customers to plan their public transit\njourneys. In Ontario, the two main trip planner service providers are Google Maps and\nTripLinx 8, the official planner for the Greater Toronto and Hamilton Area (GTHA).\nUsers input their origin, destination, and departure time. TripLinx offers additional\noptions like walking distance and service provider preferences, but the form requires\nusers to input all details at once, potentially complicating the user experience. Notably,\nthe planner does not directly provide system alerts related to customers' trips, limiting\nits functionality.\nThe \"Trip Advisor\" application is designed to provide a more personalized expe-\nrience for users when planning their trips. Compared with classical trip planners, as\nmentioned previously, our application allows users to engage in a more natural interac-\ntion without worrying about the form of the query. Additionally, the application takes\ninto account individual user preferences and limitations, resulting in personalized trip\noptions that prioritize a user-centric experience.\nThis application is an example application that shows how the application can\nquery a structured database with unstructured input and generate output in natural\nlanguage. The developed application is currently designed for desktop use; however, it\ncan be readily adapted to function as a mobile application.\nIn the Trip Advisor application, we leverage the CoT framework to guide the LLM in\ndetermining the relevance of transit system alerts to individual passengers, as shown in\nFigure 10. The LLM systematically follows predefined steps to assess the relationship\nbetween the system alert and the passenger's specific situation. It checks for any\noverlap or alignment between the alert content and the passenger's needs before making\npersonalized trip suggestions.\nThe key inputs for this application include Origin-Destination (OD) information,\ntravel time preferences, and any specific requirements or special needs the user may\nhave. These inputs are provided through direct user interaction, typically through con-\nversational dialogue or specific user queries. Based on the user inputs, the application\nprovides two main outputs: customized trip suggestions and customized trip alerts.\nThe trip suggestions are tailored to the individual's preferences and the trip alerts"}, {"title": "3.2.3. Policy Navigator", "content": "The \"Policy Navigator\" application utilizes LLM to provide personalized policy advice\nin response to user queries. First, we examine the Virtual Assistant\u00ba deployed on the\nGO Transit website as an example of current tools that provide information to users.\nAs shown in Figure 14, the GO's virtual assistant requires users to navigate through\na series of steps, select a question class, and choose from a list of candidate questions\nunder that selected class. The retrieved information often includes irrelevant details."}, {"title": "4. Discussion", "content": ""}, {"title": "4.1. Limitations and Challenges Using LLM", "content": "Although LLMs are powerful tools for information communication between comput-\ners and humans, they have certain limitations. One of the main limitations is their\nlack of understanding of that which they do not know. This includes their knowledge\nboundaries, areas of expertise, the accuracy of their answers, and how to correct their\nmistakes. These limitations can result in incorrect answers that may be biased, fab-\nricated, or contain reasoning errors. Additionally, LLMs may have limitations due to\noutdated knowledge in their training datasets.\nThese limitations pose several challenges when developing applications for the pub-\nlic transit system. Firstly, ensuring the reliability of the system becomes difficult,\nincluding the trustworthiness and stability of the results. Since the models are not\nspecifically trained on domain-specific knowledge, their professionalism in handling\ncomplex decision-making tasks is questionable. Moreover, evaluating the output from"}, {"title": "4.2. Best Practices Using LLM in the Public Transit System", "content": "Several best practices can be employed to address the aforementioned challenges.\nFirstly, by using appropriate prompts one can enhance the reliability of the LLM's\noutput. By providing specific instructions in the prompts, such as the desired problem-\nsolving approach, format, tone, and references, we can guide the LLM toward gener-\nating more reliable answers. Open-source tools like LangChain Harrison Chase et al.\n(2023) can facilitate the development of effective prompts. Furthermore, there are also\nAI tools10,11 that can help generate proper prompts, if the user can specify their tasks.\nTo improve professionalism, fine-tuning techniques can be utilized to tailor the LLM\nwith domain-specific knowledge. This approach allows us to enhance the LLM's ex-\npertise in a particular area, leveraging relatively fewer training resources and time.\nFor evaluation purposes, a higher-version LLM model, such as GPT-4, can be em-\nployed as a critic to assess the quality of the application's output or the similarity\nbetween the generated results and the expected answers.\nHowever, during the development of the three transit applications described earlier,\nwe realized the importance of treating the LLM as a tool with limited intelligence.\nThe public transit domain demands a high level of reliability, as unreliable answers\ncan erode customer trust. While the LLM serves as a valuable tool for information\nconversion, it is crucial to regularly check its results using computer-aided functions\nor human supervision to ensure the accuracy of the application's output. For instance,\nin Tweets Writer, the tweets generated by the AI should undergo a human review\nwithin the transit agency before being published on social media. This process ensures\nthat human workers have access to both the generated tweets and the raw informa-\ntion generated by the system, maintaining the accuracy and appropriateness of the\ncontent shared with the public. Similarly, in the workflow of Trip Advisor, the AI is\nprogrammed to request basic information from users, such as origin, destination, and\npreferred time. The flexibility of the LLM allows for variations in when and how these\nquestions are posed. However, to ensure the integrity of the LLM's output, the ques-\ntions must be either explicitly asked or inferred from the customer's chat history. This\ncareful approach guarantees the reliability and relevance of the information provided\nby the LLM in generating personalized transit trip suggestions, even though it may\nrestrict the working space of LLMs and require additional coding or human effort."}, {"title": "5. Conclusions and Future Work", "content": "The technical guidance offered in this study is not solely tailored to the proposed three\napplications; rather, it serves as a comprehensive roadmap for transit researchers and\npractitioners. Through the exploration of the three showcased applications, we have\naddressed diverse information transformation processes prevalent in transit systems:\nthe conversion from structured information to neural language information, from neu-\nral language information to structured information, and the transformation between\nneural language information types. This study elucidates how these transformations\ncan be effectively executed using LLM.\nTransit agencies can benefit from the insights garnered in this research. Firstly, the\nproposed framework offers a systematic approach to incorporating LLMs into various\naspects of transit operations. By embracing this methodology, transit agencies can\nstreamline communication processes, automate service alerts, and deliver tailored re-\nsponses to customer inquiries. The showcased applications serve as practical models,\nillustrating how LLMs can be leveraged to improve the efficiency of system updates,\npersonalized trip planning, and policy-related interactions. While these applications\nserve as early-stage showcases, they lay the groundwork for further refinement and\nadaptation by real-world transit agencies. The demonstrated potential for improve-\nment underscores the scalability and practicality of integrating LLMs into transit\noperations, marking a transformative stride towards more efficient and responsive cus-\ntomer service systems in the public transit domain.\nThere are several promising avenues for future research and development in leverag-\ning LLMs in public transit systems. These directions aim to enhance customer service,\nimprove the efficiency of automatic reporting systems, and advance the capabilities\nof transit control assistant systems. Firstly, further refinement of the chatbot applica-\ntions discussed in this paper is needed to provide even more accurate and contextually\nappropriate responses to customers. Secondly, improving the working experience of\ntransit system staff can be achieved through the implementation of an automatic re-\nporting system. Lastly, enhancing transit control assistant systems is an important\narea of future development. By leveraging Cross-Modal LLMs, real-time transit situa-\ntions can be understood and analyzed, enabling the generation of control suggestions\nfor transit control centers. Fine-tuning LLMs with domain-specific information, such\nas historical control records, control strategies, or guidelines, can further enhance the\neffectiveness and efficiency of control recommendations."}]}