{"title": "Self-Attentive Spatio-Temporal Calibration for Precise Intermediate Layer Matching in ANN-to-SNN Distillation", "authors": ["Di Hong", "Yueming Wang"], "abstract": "Spiking Neural Networks (SNNs) are promising for low-power computation due to their event-driven mechanism but often suffer from lower accuracy compared to Artificial Neural Networks (ANNs). ANN-to-SNN knowledge distillation can improve SNN performance, but previous methods either focus solely on label information, missing valuable intermediate layer features, or use a layer-wise approach that neglects spatial and temporal semantic inconsistencies, leading to performance degradation. To address these limitations, we propose a novel method called self-attentive spatio-temporal calibration (SASTC). SASTC uses self-attention to identify semantically aligned layer pairs between ANN and SNN, both spatially and temporally. This enables the autonomous transfer of relevant semantic information. Extensive experiments show that SASTC outperforms existing methods, effectively solving the mismatching problem. Superior accuracy results include 95.12% on CIFAR-10, 79.40% on CIFAR-100 with 2 time steps, and 68.69% on ImageNet with 4 time steps for static datasets, and 97.92% on DVS-Gesture and 83.60% on DVS-CIFAR10 for neuromorphic datasets. This marks the first time SNNs have outperformed ANNs on both CIFAR-10 and CIFAR-100, shedding the new light on the potential applications of SNNs.", "sections": [{"title": "Introduction", "content": "Spiking Neural Networks (SNNs), considered the third generation of neural networks (Maass 1997), offer a promising advancement in low-power computing. Unlike artificial neural networks (ANNs), which use continuous-valued activations, SNNs emulate the brain's discrete, spike-based information transmission, making them ideal for event-driven and energy-efficient neuromorphic hardware (Akopyan et al. 2015). Two main approaches have emerged for developing supervised deep SNNs: 1) direct training from scratch using surrogate gradients to approximate the discontinuous derivatives of spiking neurons, and 2) ANN-to-SNN conversion, which aligns ANN neuron functions with spiking neurons. Despite progress, a performance gap persists between ANNs and SNNs. To address this, ANN-to-SNN knowledge distillation has been employed to transfer relevant knowledge from ANNs to SNNs.\nHowever, previous distillation methods have either failed to transfer sufficient knowledge or have faced spatial and temporal disparities in semantic information, resulting in degraded performance. This paper introduces a Self-Attentive mechanism to address the semantic mismatch problem by autonomously identifying the most relevant semantic layer patterns across spatial and temporal dimensions and allocating attention based on semantic relevance. The key contributions of this work are summarized as follows:\n1. We propose a self-attention mechanism to address semantic mismatching during ANN-to-SNN knowledge distillation by autonomously aligning the most relevant layer patterns between ANN and SNN both spatially and temporally.\n2. Through extensive experiments across various settings and prevalent network architectures, our method significantly boosts SNN performance in ANN-to-SNN distillation, surpassing current benchmarks across various datasets, including both static and neuromorphic ones.\n3. Our analysis demonstrates that SASTC successfully achieves semantic matching in ANN-to-SNN distillation, advancing its applications in robust representation."}, {"title": "Related Work", "content": ""}, {"title": "Direct Training from Scratch", "content": "We briefly summarize some significant achievements in direct training. Lee et al. directly train SNNs in terms of spikes by regarding the membrane potential as the combination of differentiable signals and discontinuous noisy (Lee, Delbruck, and Pfeiffer 2016). Wu et al. use an approximate derivative to construct an iterative LIF neuron model and propose a spatio-temporal backpropagation (STBP) method to train SNNs from scratch (Wu et al. 2018). Zheng et al. propose a threshold-dependent batch normalization (tdBN) method for tuning the loss function (Zheng et al. 2021). Rathi et al. propose to optimize the leakage and threshold in the LIF neuron model. Furthermore, many direct training method have been proposed based on designing various surrogate gradients and coding schemes to achieve SNNs with low latency and high performance (Wu et al. 2019)."}, {"title": "ANN-to-SNN Conversion", "content": "P\u00e9rez-Carrasco et al. first map sigmoid neuron model of ANNs into LIF neuron model by utilizing scaling factor, which is determined according to neuron parameters and modified manually (P\u00e9rez-Carrasco et al. 2013). Diehl et al. propose to regulate firing rates of SNNs through weight normalization (Diehl et al. 2015). Cao et al. adopt only one hyperparamter, which is the firing threshold of spiking neurons, to approximate the rectified linear unit (ReLU) function of ANNs (Cao, Chen, and Khosla 2015). Based on the great success achieved in previous conversion schemes, many subsequent studies are devoted to minimizing various errors in conversion process (Sengupta et al. 2019)."}, {"title": "ANN-to-SNN Distillation", "content": "Typically, SNNs employ the spike frequency of the output layer or the average membrane potential increment as inference indicators. Analogous to ANN distillation, the conventional ANN-to-SNN knowledge distillation minimizes the Kullback-Leibler (KL) divergence between these SNN inference indicators and the predictive class probability distributions of ANNs (Lee et al. 2021). Recent efforts explore the transfer of enriched information from feature maps to enhance performance (Hong et al. 2023)."}, {"title": "Self-Attentive Spatio-Temporal Calibration", "content": ""}, {"title": "Notations and Background", "content": "In this section, we provide a concise overview of fundamental concepts and establish necessary notations for subsequent illustration or clarity, the term \"teacher model\u201d denotes the ANN model, while the \"student model\" refers to the SNN model unless explicitly specified. Let \\(X = \\{x_i, y_i\\}\\) represent the training dataset consisting of n instances and N categories, with \\(x_i\\) as the input vector and \\(y_i\\) as the corresponding target in the form of a one-hot encoding vector. The number of output channels and spatial dimensions represented as c, h and w, respectively. For a mini-batch data of size b, the output of each SNN layer \\(s_l\\) at time step t is denoted as \\(f_t^{s_l} \\in \\mathbb{R}^{b \\times c_{s_l} \\times h_{s_l} \\times w_{s_l}}\\, where the superscript t signifies the index of the current time step, and T represents the total number of time steps. Simultaneously, the output of each ANN (teacher) layer \\(a_l\\) is denoted as \\(f_{a_l}^a \\in \\mathbb{R}^{b \\times c_{a_l} \\times h_{a_l} \\times w_{a_l}}\\. The layer indices \\(s_l\\) and \\(a_l\\) traverse from 1 to \\(s_L\\) and \\(a_L\\), respectively. Notably, \\(s_L\\) and \\(a_L\\) typically differ due to the intrinsic heterogeneity inherent in the teacher and student models. The output representations at the penultimate layer of the teacher and student models are labeled as \\(f_{a_L-1}^a\\) and \\(f_{s_L}^{s}\\). Furthermore, we define the feature pattern \\(\\mathcal{F}\\) as the set of outputs from intermediate feature layers. \\(\\mathcal{F}^t\\) represents the feature pattern of the student model at time step t, \\(\\mathcal{F}^t = \\{f_l^t | \\forall l \\in [1, ..., L]\\}\\), while \\(\\mathcal{F}^a\\) denotes the feature pattern of the teacher model, \\(\\mathcal{F}^a = \\{f_{a_l}^a | \\forall l \\in [1, ..., L]\\}\\). It is crucial to note that this collection is a permutation rather than a combination. In other words, multiple collections with the same intermediate layers but in different orders signify distinct feature patterns.\nConcerning the student model, the outputs of the final layer \\(f_{end}(f_t^l[i])\\) are represented as the averaged membrane potentials over all time steps, \\(O_i^t = \\sum_{t=1}^T f_{end}(f_t^l[i]) \\in \\mathbb{R}^N\\), where the notation i refers to the i-th input instance. We have added this clarification in the revised version. Predicted probabilities are derived through a softmax layer built on these outputs O, denoted as \\(P^t = \\sigma(O_i^t/\\alpha)\\). Similarly, the logits of the teacher model are designated as \\(O_i^a = f_{end}(f_{a_l}^a[i]) \\in \\mathbb{R}^N\\), and the corresponding predicted probabilities are denoted as \\(P^a = \\sigma(O_i^a/\\alpha)\\), commonly referred to as soft targets. In both the student and teacher models, the hyperparameter \\(\\alpha\\) is typically set to 1."}, {"title": "Spatio-Temporal Mismatch Problem on Existing ANN-to-SNN Knowledge Distillation", "content": "Prior studies have assumed that: 1) the distributions of semantic information embedded in ANNs and SNNs are similar (spatially matched), and 2) this distribution within SNNs remains constant across different time steps (temporally matched). We introduce a metric named Spatio-Temporal Mismatch Score (STM score) to assess the extent of semantic disparity between associated ANN-SNN layer pairs over time steps. STM score is computed as the Average Euclidean Distance between the generated similarity matrices of each corresponding ANN-SNN feature map pair, as expressed in Equation (1):\n\\[STMscore = \\frac{1}{T}\\sum_{t=1}^T\\frac{1}{|C^t|}\\sum_{l} MSE(A_{s_l}^t, A_{a_l}). \\quad(1)\\]\nwhere T represents the number of time steps, \\(|C^t|\\) denotes the number of candidate layer pairs, and \\(A_{s_l}^t\\) and \\(A_{a_l}\\) are the similarity matrices of ANN layer \\(a_l\\) and SNN layer \\(s_l\\), respectively. MSE measures the extent of semantic mismatches between the student SNN and the teacher ANN. A lower STM score signifies fewer mismatched association semantics, equating to superior model performance. Practically, we calculate the STM score (log-scale) values for each approach across training epochs and average them over the last 10 epochs, where they remain nearly unchanged.\nContrary to previous assumption, we find that existing ANN-to-SNN knowledge distillation methods either achieve very small improvements or result in degradation effects on STM scores, as shown in Table 1. In other words, spatio-temporal mismatch of semantic information results in the loss of valuable knowledge during the knowledge distillation process. Our proposed approach diverges from the traditional paradigm by introducing self-attentive calibration. This innovative method aims to effectively transfer spatio-temporal semantic information by dynamically selecting suitable layer associations at each time step, departing from dependence on fixed teacher-student feature patterns."}, {"title": "Formulation of Self-Attentive Calibration", "content": "In our methodology, each student layer at every time step seamlessly aligns itself with semantically matched target layers through attention allocation, as depicted in Figure. 1 (a) and (b). The training process, guided by calibrated associations, prompts the student model to adeptly gather and integrate information from multiple layers at each time step, fostering a more tailored regularization. Furthermore, SASTC is versatile and can be applied in scenarios where the number of candidate layers differs between the teacher and student models. The ensemble of acquired feature patterns at time step t in SASTC is denoted as \\(\\mathcal{C}^t = \\{(f_l^t, f_{a_l}^a) | \\forall f_l^t \\in \\mathcal{F}, f_{a_l}^a \\in \\mathcal{F}^a\\}\\), with the corresponding weight satisfying \\(\\sum_{a_l=1}^{a_L} \\eta(f_l^t, f_{a_l}^a) = 1, \\forall f_l^t \\in \\mathcal{F}\\) at each time step. This weight \\(\\eta(f_l^t, f_{a_l}^a) \\in \\mathbb{R}^{b \\times T}\\) signifies the degree to which the target layer \\(f_{a_l}^a\\) is considered in the calibration of spatio-temporal semantic differences during ANN-to-SNN distillation. A more detailed exploration of these self-attentive weights will be provided subsequently. The feature maps from each time step of the student model are transformed into \\(a_L\\) distinct forms, aligning with the spatial dimensions of each target layer for subsequent distance calculations, as indicated by\n\\[f_{s_l}^{s} = Proj(f_l^t \\in \\mathbb{R}^{b \\times c_{s_l} \\times h_{s_l} \\times w_{s_l}}, a_l), \\\\ f_l^t \\in \\mathcal{F}, a_l \\in [1, ..., a_L],\\quad(2)\\]\nwith \\(f_{s_l}^{s} = f_{a_l}^{t,a_l} \\in \\mathbb{R}^{b \\times c_{a_l} \\times h_{a_l} \\times w_{a_l}}\\. Each function \\(Proj(\\cdot, \\cdot)\\) comprises a stack of two convolution layers with 3 \u00d7 3 and 1 \u00d71 to fulfill the requirement for an efficient transformation. This design choice is guided by previous research, which has illustrated the remarkable effectiveness of a 3 x 3 convolution layer and the pyramid architecture (Han, Kim, and Kim 2017)."}, {"title": "Allocation with Self-attention", "content": "Previous research suggests that the abstraction of feature representations is closely associated with the layer depth (Bengio, Courville, and Vincent 2013). The semantic levels of these intermediates can vary between teacher and student architectures with differing capacities. Furthermore, we observe variations in the semantic level among the feature patterns of SNNs at different time steps, resulting in spatio-temporal information loss during distillation. To address these spatio-temporal differences in the student model and enhance the performance of feature transfer during distillation, each layer of the student model should be associated with the most semantically relevant target layer to derive its own regularization. Simple approaches such as random selection or forcing feature maps from the same layer depths to align may be inadequate due to the adverse effects resulting from semantic mismatched pairs.\nInspired by the layer associations facilitated by attention mechanisms in ANNs (Vaswani et al. 2017), we expand the self-attentive scheme from spatial calibration to spatio-temporal calibration. This extension presents a potentially viable solution for addressing the semantic mismatch problem and enhancing the overall distillation performance. Given that feature maps in SNNs, generated by similar instances, tend to cluster at distinct granularities across different time steps and layers, and similarly, in ANNs, these feature maps cluster based on their depth, the proximity of pairwise similarity matrices serves as a meaningful measure of inherent semantic similarity. These similarity matrices are computed as follows\n\\[A_{s_l}^t = R(f_l^t) \\cdot R(f_l^t)', f_l^t \\in \\mathcal{F}, \\\\A_{a_l} = R(f_{a_l}^a) \\cdot R(f_{a_l}^a)', f_{a_l}^a \\in \\mathcal{F}^a,\\quad(3)\\]\nwhere \\(R(\\cdot) : \\mathbb{R}^{b \\times c \\times h \\times w} \\rightarrow \\mathbb{R}^{b \\times chw}\\) represents a reshaping operation, and the symbol ' denotes the transpose operation. Consequently, \\(A_{s_l}^t\\) and \\(A_{a_l}\\) yield \\(b \\times b\\) matrices. More importantly, incorporating similarity matrices significantly mitigates the memory cost associated with large spatio-temporal dimensions (\\(T\\cdot C_{s_l/a_l} \\cdot h_{s_l/a_l}\\cdot w_{s_l/a_l} \\gg b\\)).\nBuilding upon the self-attention framework (Vaswani et al. 2017), we independently project the pairwise similarity matrices of each student layer, originating from individual time steps, and each target layer, into two subspaces using a Multi-Layer Perceptron (MLP). This procedure endeavors to alleviate the influence of noise and sparsity, with the resulting vectors identified as query and key. To expound further, for the i-th instance, the formulation can be articulated as follows:\n\\[Q^t[i] = MLP_Q(A_{s_l}^t[i]), K_{a_l}[i] = MLP_K(A_{a_l}[i]).\\quad(4)\\]\nThe parameters in \\(MLP_Q(\\cdot)\\) and \\(MLP_K(\\cdot)\\) are acquired through training to produce query and key vectors, shared across all instances. Subsequently, the calibrated weight \\(\\eta_{a_l}^{t,i}\\) for the i-th instance is computed as follows:\n\\[\\eta_{a_l}^{t,i} = \\frac{Q_{s_l}^t [i]'K_{a_l}[i]}{\\sum_{j=1}^{a_L} Q_{s_l}^t[i]'K_{a_j}[i]}.\\quad(5)\\]\nThe allocation based on attention offers a viable approach to alleviate the adverse effects stemming from spatio-temporal differences (mismatch in student-teacher layer pairs) and amalgamate beneficial guidance from multiple target layers. The complete training procedure, incorporating the proposed semantic calibration formulation, is succinctly outlined in Algorithm 1."}, {"title": "Loss Function", "content": "In a mini-batch of size b, the student model generates multiple feature patterns spanning various time steps (\\(\\mathcal{F}_1^t,..., \\mathcal{F}_{s_L}^t\\)). Following dimensional projections and self-attentive calibration, we employ Mean-Square-Error (MSE) to align the raw pairwise similarity matrices of the teacher and student (referred to as the loss of SASTC),\n\\[\\mathcal{L}_{SASTC} = \\sum_{t} \\sum_{s_l=1}^{s_L} \\sum_{a_l}^{a_L} \\eta(f_l^t, f_{a_l}^a) Dist(f_{a_l}^a, Proj (f_l^t, a_l)) \\quad(6)\\]\n\\[= \\sum_{i=1}^b\\sum_{t=1}^T \\sum_{a_l=1}^{a_L} \\sum_{s_l=1}^{sq=1} \\eta_{a_l}^{t,i} MSE(f_{a_l}^a[i], f_{s_l,a_l}^t [i]),\\]\nas it demonstrated superior empirical performance. In this process, each feature map from the student model \\(f_l^t\\) undergoes transformation via a projection function \\(Transs = Proj(\\cdot, \\cdot)\\), while the target layers remain unchanged through identity transformation \\(Transa(\\cdot) = I(\\cdot)\\). Multiplying the outcomes by the learned self-attentive distributions, the total loss is computed through a weighted summation of individual distances among feature maps from candidate teacher-student layer pairs at each time step. Consequently, the total loss of SASTC is expressed as:\n\\[L_{total} = L_{KD} + \\beta L_{SASTC}.\\quad(7)\\]\n\\[L_{KD} = L_{CE}(y_i, \\sigma(O_i^t)) + \\alpha^2L_{KL}(\\sigma(O_i^a), \\sigma(O_i^t)).\\quad(8)\\]\nwhere \\(L_{CE}\\) represents the standard cross-entropy loss (CE) between the predicted probabilities of the student model and the one-hot target, and \\(L_{KL}\\) denotes the KL divergence between \\(P_s\\) and the soft targets of the teacher model \\(P_a\\)."}, {"title": "Neuron Model and Surrogate Gradient", "content": ""}, {"title": "LIF Neuron", "content": "We employ the LIF neuron model, which in discrete time, is described by:\n\\[u^t = \\lambda u^{t-1} + I^t, o^t = \\Theta(u^t - V_{th}),\\quad(9)\\]\nwhere \\(u^t\\) signifies the membrane potential, \\(I^t\\) denotes presynaptic inputs, \\(\\lambda (< 1)\\) represents the constant leaky factor in the membrane potential, \\(V_{th}\\) signifies the threshold membrane potential, stands for the Heaviside step function, \\(o^t\\) denotes the spike output propagating to the next layer, and the superscript t indicates the time step. Following the emission of the spike output, the reset operation is delineated in\n\\[u^t = u^t \\cdot (1 - o^t).\\quad(10)\\]\nTo minimize trainable parameters, all neurons share identical leak values \\(\\lambda\\) and threshold potentials \\(V_{th}\\). For consistency across experiments, we set the initial membrane potential \\(u^0\\) to 0, the threshold \\(V_{th}\\) to 1, and the leaky factor \\(\\lambda\\) to 0.5."}, {"title": "Triangle Shape Surrogate Gradient", "content": "In this study, prioritizing a balance between accuracy and computational efficiency, we choose the triangular surrogate gradient, as established in prior research (Deng et al. 2022). The mathematical expression for the triangular surrogate gradient is as follows:\n\\[\\frac{\\partial o^t}{\\partial u^t} = \\frac{1}{2} max(0, \\gamma - |u^t - V_{th}|),\\quad(11)\\]\nwith \\(\\gamma\\) set to 0.3 based on previous works (Deng et al. 2022)."}, {"title": "Experiments", "content": "To demonstrate the effectiveness of our proposed self-attentive spatio-temporal calibration in ANN-to-SNN knowledge distillation, we conduct comprehensive experiments. We evaluate various ANN-SNN combinations using popular network architectures like VGG (Simonyan and Zisserman 2014), ResNet (He et al. 2016), PyramidNet (Han, Kim, and Kim 2017), and WRN (Zagoruyko and Komodakis 2016) on static datasets. Meticulously designed experiments and ablation studies validate the effectiveness of SASTC in providing proper regularization for student models. We apply SASTC to neuromorphic datasets like DVS-Gesture and DVS-CIFAR10, demonstrating its robust generalization in noisy-label learning. Additionally, we offer a visual analysis of SASTC's success.\nFurther analyses of temporal information dynamics are provided in Appendix 1, with demonstrations of robust generalization of few-shot learning in Appendix 2. Details on batch size, sensitivity, computational efficiency, running time, and memory consumption are summarized in Appendix 3. The experimental setup details are available in Appendix 4."}, {"title": "Mechanism Analysis and Ablation Study", "content": "In this section, we delve into an experimental exploration of the negative regularization effect induced by manually specified layer associations across time steps. Furthermore, we provide evidence of the success of SASTC, supported by the proposed criterion and visual evidence."}, {"title": "SASTC Improves Negative Regularization Effects", "content": "We conduct experiments on the CIFAR-10 dataset by training the student model exclusively with a specified teacher-student layer pair in various settings, and observe negative regularization effects that feature-pattern-based distillation with specific layer associations across time steps performs worse than vanilla ANN-to-SNN KD. The network architectures involved \"VGG-11& ResNet-19\", \"ResNet-18 & Pyramidnet-20\", and \"WRN-16-2 & WRN-28-4\". The numbers of candidate target layers and student layers for each case are (4, 5), (3, 4), and (4, 4), respectively.\nThe outcomes of student models with 20, 12 and 16 ANN-SNN layer combinations under the three settings on CIFAR-10 are illustrated in Fig. 2. Notably, all layer associations of SNN layer-2 and layer-3 in Fig. 2 (a) obtain extremely poor performance, likely due to highly sparse semantic information contained in layer-2 of VGG-11 student model. In addition, the performance of a student model significantly diminishes for certain layer associations across time steps (i.e., negative regularization effect), including SNN layer-1 to layer-4 in Fig. 2 (a) and (c), most like due to the substantial semantic mismatch. Notably, it is observed that the one-to-one layer matching scheme is non-optimal because better results can be obtained by leveraging information from a target layer with different depth, such as \"SNN layer-5 & ANN-layer3\" in Fig. 2 (a), \"SNN layer-4 & ANN-layer3\" in Fig. 2 (b) and \"SNN layer-2 & ANN-layer3\" in Fig. 2 (c).\nAlthough training with specific hand-crafted layer associations may outperform SASTC in isolated cases like \"SNN layer-5 & ANN-layer3\" in Fig. 2 (a) and \"SNN layer-4 & ANN-layer3\" in Fig. 2 (b), SASTC consistently performs well across a large number of associations. It is particularly noteworthy considering that the knowledge of the best layer association for each network combination is not available in advance. Furthermore, instances where training with SASTC is inferior to the best layer association suggest potential refinements in our association strategy."}, {"title": "SASTC Achieves Semantic Matching during Knowledge Distillation", "content": "The results in Table 5 indicate that SASTC consistently attains the lowest spatio-temporal mismatch score throughout the training process compared to other approaches owing to our spatio-temporal calibration mechanism on both CIFAR-100 and ImageNet datasets.\nMoreover, we provide evidence in Appendix 1 that SASTC optimizes the temporal information dynamics of SNNs, further illustrating the success and mechanism of our proposed method."}, {"title": "Extension, Application and Visual Analysis", "content": "Extension to Neuromorphic Datasets Previous works focus on utilizing complex architectures to tackle the challenging DVS-CIFAR10 task. However, these sophisticated models have failed to achieve satisfied performance and are susceptible to overfitting. Recently, a temporal efficient training approach has achieved the state-of-the-art accuracy with the streamlined VGGSNN architecture. In this paper, we conduct experiments on the DVS-Gesture and DVS-CIFAR10 datasets. As shown in Table 6, our proposed method outperform the contemporary best-performing methods, the accuracy of SASTC increase to 97.92% on DVS-Gesture and 83.60% on DVS-CIFAR10 through the calibration of spatio-temporal semantic mismatches during ANN-to-SNN distillation. Consequently, our SASTC method significantly improves the processing temporal information ability of SNNs on neuromorphic tasks."}, {"title": "Application to Robust Representation", "content": "In addition to evaluating the clean test set performance, we introduce noisy-label learning datasets by randomly perturbing 10%, 20%, 30%, 40%, and 50% of labels in training images. As shown in Table 7, training the lightweight SNN with SASTC extremely enhances its robustness compared to other ANN-to-SNN knowledge distillation approaches and the teacher ANN counterpart."}, {"title": "Visualization Analysis of SASTC", "content": "To visually elucidate the advantages of SASTC, we randomly selected several images from ImageNet, and highlight regions deemed crucial for predicting the respective labels by utilizing Spike Activation Map (SAM) (Kim and Panda 2021). As depicted in Figure 3, SASTC consistently centralizes class-discriminative regions and excels in capturing more semantically related information, resembling the teacher model, while the compared methods scatter them in the surroundings."}, {"title": "Conclusion", "content": "This study focuses on mitigating performance degradation due to spatio-temporal semantic mismatches and negative regularization in conventional ANN-to-SNN knowledge distillation methods. We propose a self-attentive mechanism to learn layer association weights across different time steps, enabling semantically aligned knowledge transfer. Qualitative and quantitative evidence validate SASTC's spatio-temporal calibration capability. Extensive experiments demonstrate that SASTC consistently outperforms various SNN training approaches and distillation schemes. SASTC also shows strong generalization across tasks and network architectures, excelling in robust representation."}, {"title": "Appendix 1", "content": ""}, {"title": "SASTC Optimizes Temporal Information Dynamics of SNN", "content": "To quantify the Fisher information and further analysis its distribution of different ANN-to-SNN distillation methods, we introduce the empirical Fisher Information Matrix trace, denoted as \\(FIM^t\\) in Equation (1):\n\\[FIM^t = \\frac{1}{n}\\sum_{j=1}^n ||\\nabla_W logP_W(y_t|x_j)||^2, \\quad(1)\\]\nwhere k denotes the index of time step, and \\(P_W(y|x)\\) denotes approximate posterior distribution of a network with weight parameters W, input instance x from data distribution X and output y.\nIn Fig. 1, we visualize temporal Fisher information of three different ANN-to-SNN distillation methods using the \"VGG-11 & ResNet-19\" combination on the CIFAR-10 dataset, along with the baseline as a reference. The Fisher information decreases notably with ANN-to-SNN distillation, and with training progression, the Fisher information with SASTC decreases more rapidly towards the final stable state compared to other distillation methods. In terms of temporal information distribution, ANN-to-SNN distillation can effectively supervise the optimization of the weight parameters, and our proposed SASTC is more effective than other ANN-to-SNN distillation methods."}, {"title": "Application to Few-Shot Learning", "content": "Furthermore, we assess the performance of SASTC in few-shot learning and noisy-label learning scenarios. These experiments consistently demonstrate that SASTC effectively leverages training data and exhibits relative robustness to noisy perturbations.\nWe randomly select 25%, 50%, and 75% of training images from each class in CIFAR-100 to create several few-shot learning datasets. As depicted in Table 1, SASTC consistently outperforms the compared ANN-to-SNN distillation approaches and conventional training methods in all settings. The improvement becomes more significant as the number of available training images decreases. Notably, SASTC surpasses the teacher ANN counterpart and when trained with only 25% and 50% of the original dataset, which further prove the advantages of our SASTC."}, {"title": "Additional Analysis", "content": ""}, {"title": "Batch Size Analysis", "content": "We conduct experiments to analysis the influence of choosing different batch size in our proposed method, since our calculation of similarity matrices and self-attentive calibration is based on batch-level. To consider eliminating the impact of other factors, we introduce the vanilla ANN-to-SNN KD method for comparison as well as directly trained SNNs as the baseline. Results are summarized in Table 2. Although batch size is a crucial hyperparameter that influences training performance with the SGD optimizer (Keskar et al. 2016), the SNN obtained through SASTC consistently outperforms other SNNs in all cases."}, {"title": "Sensitivity Analysis", "content": "We evaluate the impact of the hyperparameter beta on the performance of SASTC across CIFAR-10 and CIFAR-100 datasets. Combinations of \"VGG-11 & ResNet-19\" with 3 time steps and \"VGG-11 & ResNet-32x4\" with 3 time steps are adopted on CIFAR-10 and CIFAR-100 respectively. Comparisons are made between SASTC with varying beta values, the vanilla ANN-to-SNN knowledge distillation approach (KD), and baseline SNNs. The hyperparameter \\(\\beta\\) for SASTC ranges from 100 to 1500 at intervals of 100, while it remains 0 for KD, resulting in two horizontal lines in Fig. 2.\nOn the CIFAR-100 dataset, except for the \\(\\beta\\) setting of 100, SASTC consistently outperforms both KD and the baseline in all other cases. Particularly, at the optimal hyperparameter setting (\\(\\beta\\) = 700), SASTC exceeds KD and the baseline by 1.62 and 6.18 absolute accuracy, respectively. Similarly, on CIFAR-10, Fig. 2 demonstrates that SASTC consistently outperforms KD and the baseline across different \\(\\beta\\) settings. At the optimal beta setting of 1300, our method surpasses KD and the baseline by 1.12 and 1.24 absolute accuracy, respectively. These findings demonstrate the effectiveness of our proposed method across a wide range of the \\(\\beta\\) hyperparameter search space."}, {"title": "Comparison of Computation Efficiency", "content": "For computation energy analysis, we count the total spiking operations (SOPs) to estimate the energy consumption of different distillation SNNs compared with their ANN counterparts (Akopyan et al. 2015). Specifically, we adopt \"VGG-11 & ResNet-32x4\" combination, then sample 1024 samples and calculate the mean value of SOPs. Early stopping is also used to alleviate overfitting with a tolerance of 20 epochs. We also introduce the direct trained SNN (baseline) for comparison. In conservative estimation, SNNs consume 0.9pJ energy per accumulation (AC) and 4.6pJ energy per multiplication and accumulation (MAC) as measured in 45nm CMOS technology (Rathi and Roy 2021). However, further study proves that the energy consumption of SNNs on specified hardware design can be reduced by 12x to 77 fJ/SOP (Rathi and Roy 2021). SNNs in this paper have multiplication operations in the first layer, while the other layers only have addition operations.\nTable 3 summaries the results of various SNN training methods with different time steps. Notably, the SNN obtained through SASTC has 79.40% Top-1 test accuracy and 614.1 uJ energy consumption with 2 time steps, which achieves both the higher performance and higher efficiency compared to directly trained (baseline), vanilla distillation (KD) and feature distillation (feature KD) SNNs with 7 time steps. In addition, the energy is significantly efficient when MAC degrades to AC. For instance, our SAST"}]}