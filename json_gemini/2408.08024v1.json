{"title": "Adaptive User Journeys in Pharma E-Commerce with Reinforcement Learning: Insights from SwipeRx", "authors": ["Ana Fern\u00e1ndez del R\u00edo", "Michael Brennan Leong", "Paulo Saraiva", "Ivan Nazarov", "Aditya Rastogi", "Moiz Hassan", "Dexian Tang", "\u00c1frica Peri\u00e1\u00f1ez"], "abstract": "This paper introduces a reinforcement learning (RL) platform that enhances end-to-end user journeys in healthcare digital tools through personalization. We explore a case study with SwipeRx, the most popular all-in-one app for pharmacists in Southeast Asia, demonstrating how the platform can be used to personalize and adapt user experiences. Our RL framework is tested through a series of experiments with product recommendations tailored to each pharmacy based on real-time information on their purchasing history and in-app engagement, showing a significant increase in basket size. By integrating adaptive interventions into existing mobile health solutions and enriching user journeys, our platform offers a scalable solution to improve pharmaceutical supply chain management, health worker capacity building, and clinical decision and patient care, ultimately contributing to better healthcare outcomes.", "sections": [{"title": "1 INTRODUCTION", "content": "Personalization and adaptability are crucial for enhancing user experience and customer journeys in digital tools. Leveraging detailed user in-app behavior data, reinforcement learning (RL) provides adaptive journeys that improve user experience. This is particularly relevant for mobile health solutions, especially for healthcare workers in low- and middle-income countries (LMICs), where such tools can help mitigate the lack of resources. Pharmacies play a critical role in healthcare systems everywhere. They are often the only primary healthcare contact in LMICs for many patients while facing barriers like a shortage of trained professionals, medication, and suboptimal supply chains [23, 26, 42]. Supporting pharmacists with inventory management, easy-to-access clinical guides, and connection to their peers and the rest of the health system can significantly impact their communities."}, {"title": "1.1 The Reinforcement Learning Platform", "content": "We propose an artificial intelligence (AI) data-centric platform that can integrate into already existing healthcare-related digital tools to enhance them with an adaptive user journey through reinforcement learning (RL). By enabling the system to learn and adapt its behavior based on the feedback it receives, RL is used to optimize the user experience, personalizing it and making it more efficient. The end-to-end machine learning (ML) platform can integrate into different healthcare related software and leverages the behavioral and clinical logs from these solutions, together with other contextual information sources, to deliver adaptive interventions directly to their recipients through these tools in the form of personalized recommendations, reminders, incentives, or in-app content and workflows, providing an adaptive user experience and journey. We will illustrate the framework and technologies proposed by focusing on a pharmacist-facing tool, particularly its bussiness-to-bussines (B2B) e-commerce component.\nFigure 1 depicts the platform's architecture, which consists of a Software Developer Kit (SDK) which is embedded into the digital tools to track labeled data and deliver interventions, a frontend interface with analytics, model management, intervention and experimentation functionalities, and the backend, which takes care of log ingestion, data transformation, scheduled dispatch of nudges (message or content interventions) and hosts the algorithmic machine learning (ML) engine. The platform has also been described in [47] and has been designed to integrate with mobile health solutions beyond the pharma supply chain, including patient management [48] and clinical decision support, facility logistics, patient support, and patient management. A shorter version of this paper is also available [18]."}, {"title": "1.1.1 SDK", "content": "One of the platform's core functionalities is data tracking, organizing, and labeling. Ensuring data collection and its quality is arguably the most critical step in moving towards a solid AI approach to personalization in digital health. The standardized design of event logging enables the platform to be a uniform interface for transformation and aggregation across different application domains, e.g., supply chain and e-commerce marketplace, medication tracking, and patient communication. The SDK is a well-defined library and data structure at the heart of the platform's data side. It provides the tools for collecting relevant events on an edge device (e.g., mobile, wearable monitor, or smart device) and the messaging service that delivers the interventions."}, {"title": "1.1.2 Backend", "content": "The incoming logs through the SDK are processed by the tailor-made data pipeline and categorized into dynamic and static traits, which can aggregate through arbitrary time resolution. These traits represent insights about the users and their interactions with different in-app content and other subjects (pharmacies, patients, drugs), and once derived, they become available throughout the platform. They can be used to track behavior and outcomes, to group subjects or content (e.g., drugs or CPD modules) into meaningful cohorts (e.g., for pharmacy segmentation), as features/covariates for statistical and predictive modeling, or to make up the contexts and rewards for bandit-based decision algorithms. The backend orchestrates the storage and processing of domain-knowledge analytical traits, the computation of features and predictions derived from traits using advanced machine learning, and, finally, issues personalized nudges back to SDK instances."}, {"title": "1.1.3 Algorithmic engine", "content": "The machine learning component hosted in the backend is composed of analytic and predictive modeling, an ML recommendation engine, and an algorithmic decision-making service. These modules configure, train, host, and manage models for statistical analysis, time series forecasting, deep and ensemble survival analysis, item recommendations, and sequential decisions via reinforcement learning (see Section 2 for more technical details). These services expose their functionality via JSON request-response API to the backend."}, {"title": "1.1.4 Frontend", "content": "The frontend provides an intuitive user interface for configuring models and interventions and an analytics dashboard for monitoring ongoing ones. The configuration UI guides the user from the subject cohort and sample definition through algorithm selection to feature selection and target specification, including nudge alternatives. The dashboard represents various traits, predictions, and metrics in a convenient format for easier interpretation of the results."}, {"title": "1.2 SwipeRx", "content": "SwipeRx, the largest network of pharmacies in Southeast Asia, connects over 235,000 professionals across more than 45,000 pharmacies, revolutionizing the industry with its community-driven commerce model in Indonesia, the Philippines, Malaysia, Thailand, Cambodia, and Vietnam. The platform provides comprehensive services, including online education, centralized purchasing, logistics and financing, news, drug directory, adverse event reporting, and more, addressing the unique needs of this vital public health sector. Figure 2 shows some screenshots of the app.\nThe network is especially crucial in the region, where there's a heavy reliance on pharmacies due to a scarcity of professional physicians (with patients visiting pharmacies ten times more than doctors every year). Through its app, SwipeRx fosters knowledge sharing and collaboration among pharmacists, enhancing the quality of patient care in local communities. Over 80,000 pharmacy professionals have already been educated through its digital professional education.\nIn Southeast Asia, more than 80% of the pharmacies are independent, facing challenges of quality, availability, and affordability of essential medicines. SwipeRx is helping to solve this by empowering pharmacy professionals with the technologies and tools they need and addressing the operational challenges these predominantly small, family-owned pharmacies face, such as financing and supply chain management. By facilitating bulk purchases and analyzing drug consumption data, SwipeRx enables these pharmacies to procure medicines at competitive prices, ensuring they can meet patient needs effectively."}, {"title": "1.3 Adaptive User Journey", "content": "We propose a user-centric approach where each user's journey is tailored to their evolving needs, preferences, and the exact stage of their relationship with the service or tool to ensure that every user's unique requirements are met. This is made possible by the RL platform's adaptive algorithms (see Section 2.3) ability to systematically use the information from all users and continually adapt to improve user experience.\nIt begins with a personalized onboarding process by adjusting the level of support and introduction to app functionalities according to each user's requirements. An adaptive algorithm can determine the complexity of tips offered or whether to provide tips at all, based on user data such as location, experience, gender, job title, app usage patterns, and previous onboarding interactions. This ensures that tech-savvy users can navigate the app unhurdled by unnecessary popups, while less experienced users receive step-by-step guidance. An adaptive onboarding experience can also adjust workflow complexity based on the user's expertise. For example, community health workers (CHWs) struggling with technology are given a simplified patient screening workflow to keep them engaged without overwhelming them while collecting essential information. As they gain experience, the workflow introduces more complex options, allowing experienced users to provide detailed patient data, referrals, clinical history, or preventive education provided.\nBeyond onboarding, the personalized journey continues with tailored in-app content and workflows and with messages (via popups, notifications, WhatsApp or SMS) with personalized motivational prompts, tips, discounts, and incentives, to foster long-term user engagement. In tools such as SwipeRx, messages with personalized product recommendations (like those discussed in Section 3) and in-app ML-ordered product lists help increase or maintain purchasing engagement. The adaptive delivery mechanisms, in this case, can also use the outputs of predictive models to be more efficient, such restocking recommendations based on demand prediction that can also help reduce stockouts of essential drugs. Churn prediction models also play a significant role, as they can be used to precision target users at high risk with specific interventions, such as special discounts. Specific adaptive algorithms (see Section 2.3.3) able to allocate limited resources such as discounts or even a call from a sales representative efficiently."}, {"title": "2 METHODOLOGY", "content": "The following subsections describe in some technical detailthe platform's algorithmic server's (Section 1.1.3) capabilities."}, {"title": "2.1 Predictive Modeling", "content": "Predictive modeling can help to define who should be targeted with interventions and the right way and moment to do so. Note that while predictions are not used in the first set of interventions described in Section 3, we include this subsection for completeness as different predictions are readily available in the platform.\nThe platform includes different families of predictive models suitable for different use cases. Through its frontend, platform users can easily create models from these families by designating the traits to be used as targets and features and the specific algorithm with its parameter values. The model lifecycle can also be managed through the platform, including training schedules and performance monitoring. All resulting outputs of the models, including estimations of uncertainty and feature importance, are available to create additional traits that can be used to characterize subjects and content throughout the rest of the platform.\nSurvival analysis approaches [21, 32, 45, 58] offer a way of characterizing user (pharmacy, patient, distribution center) behavior by modeling the evolution with time of the probability of occurrence of different events of interest. Formally, the methods learn a function $g: X \\times [0, +\\infty) \\rightarrow [0, 1]$ from the observed data $(x_j, t_j, c_j)_{1}^{n}$ with the input features $x_j \\in X$, censored time-to-event $t_j \\in (0, M]$, and the censoring indicator $c_j \\in \\{0, 1\\}$, so that $g(x,.)$ approximates the conditional survival function $P(T\\geq | X = x)$ of the duration until the event of interest conditional of the input features $x \\in X$. They can be used, for example, to predict the risk of non-adherence to treatment, of pharmacy churn in an e-commerce platform, or of a pharmacist failing to complete a CPD module, which would be indicating adequate subjects to target with specific interventions (e.g., reminders to support treatment adherence, promotions to discourage churn, motivational prompts and incentives to encourage learning).\nDemand predictions are crucial in e-commerce, supply chain, and inventory optimization. Multivariate forecasting methods that learn either point or probabilistic forecasts from co-dependent time series are viable solutions for predicting the demand for different products at different sites [5]. These approaches include autoregression- and graph-regularized factor analysis [62] and latent state forecasting [54], state space modeling with LSTM and Gaussian copula for probabilistic forecasts [52], recurrent networks for multivariate count data with non-uniform scale [53], and attention-based deep models for multi-horizon interval predictions [36]."}, {"title": "2.2 Recommendation Algorithms", "content": "As will be described in section 2.3, the main mechanism in place to learn and exploit user preferences for recommendation and personalization will be that of adaptive delivery, i.e., bandit-based to a large extent. Other recommendation algorithms, however, might provide valuable insights that can be leveraged by the reinforcement learning algorithms and used for content recommendations.\nThe RL platform includes a neural embedding based item-to-item recommender [4, 16, 22], in which items are recommended based on their similarity to other items. This similarity is based both on the fixed known information about each item and on the interaction history of users with them. Transformers are used to convert available features written in natural language (e.g., name, description, manufacturer, ingredients...) into numerical vectors. Principal component analysis (PCA) is then applied to reduce their dimensionality. The PCA-transformed natural language vectors together with the interaction history (e.g., purchase orders item content) are fed into a fully connected neural network with three layers. The output of this system are the L2-normalized embedding characterizing each of the items, whose similarity to other items can be then computed as the Euclidean distance between their embeddings. However, the set of interventions discussed in this paper relies on the simpler rule-based algorithm described below."}, {"title": "2.2.1 Item-pair recommendation", "content": "The underlying idea is to find products that the pharmacy is very likely already purchasing, albeit elsewhere, and make them aware that these are also part of SwipeRx's catalog. This is achieved by recommending pairs of products which are typically purchased together by the user population to pharmacies that are only ordering one of the items frequently.\nFormally, consider the pair $p_k = (i, j)$, where $i$ and $j$ represent products. Define $d_{uit}$ as at time $t$, the number of days since user $u$ purchased product $i$. Let $d_{uit}^T$ be at time $t$ the average number of days between purchases in the last $T$ months. For the initial intervention with SwipeRx, $T$ was chosen to be three months. If $i$ has never been purchased by $u$, we adopt the convention that $d_{uit} = -1$. The recommendation follows a two-step process. In the first step, the list of candidate pairs is generated. Namely, the top 100 pairs purchased together are selected by ranking them according to either the number of times they have been purchased together or the generated revenue in the past $T$ months. For the initial intervention with SwipeRx, revenue was chosen. Keep in the list only pairs in which both items are in stock. In the second step, user-specific filtering is made. First, for user $u$, retain the list of $p_k = (i, j)$'s such that, without loss of generality, $i$ has been purchased recently, defined as $d_{uit}/d_{uit}^T \\in (0, 1)$. For user $u$, pick $j$ such that $d_{ujt} = -1$ or $d_{ujt}/d_{ujt}^T \\notin (0, 1)$. Retain the pairs such that $d_{ujt} = -1$. If no such $j$ exists for user $u$, retain pairs with the highest values for $|d_{uit}/d_{uit}^T \u2013 d_{ujt}/d_{ujt}^T|$, i.e., a large difference in expected recency between $i$ and $j$. Finally, from the ultimately finalized list of pairs, one is selected at random."}, {"title": "2.3 Bandits and Adaptive Intervention Delivery", "content": "The proposed framework aims to leverage data to intervene and nudge behavior. As such, the algorithmic piece deciding what interventions to use (or not), and when for each user is central. The intervention decision problem can be modeled as a Markov Decision Process (MDP), and reinforcement learning is the appropriate paradigm to drive personalization. It allows balancing between optimization and knowledge extraction depending on the use case and continually adapts to the evolving needs and preferences of the intervention subjects.\nStochastic contextual and restless multiarmed bandits (MABs) are good frameworks for instances of personalization and resource allocation where the long-term state evolution plays a minor role, such as those applications where we are concerned mainly with its immediate impact, or where simple, useful approximations to state dynamics can be found. Problems that need to robustly reconcile short and long-term goals through sequential decisions that optimize for the multi-step problem, such as decisions associated with medical treatment of certain conditions, require frameworks that model the full MDP [38, 63], in the spirit of collaborative, interactive recommendation systems [11, 25, 35, 37], or at least address the problems of long-term credit assignment, when learning the intervention policy.\nBandit algorithms can be thought of as online optimization methods with built-in model identification from partial feedback: the algorithm must make sequential decisions based on the observed interaction history with the ultimate goal of eliminating suboptimal choices [31]. During the course of $T$ repeated interactions, the algorithm observes a context $x_t$, then, given the history of its past contexts, interactions, and their outcomes $H_{<t} = \\{x_s, a_s, r_s\\}_{s<t}$, it picks an action $a_t$ from a finite set. It concludes the interaction by receiving a response-reward $r_t$. In this context, the action $a_t$ represents the intervention decision at time $t$, and both the context $x_t$ and reward $r_t$ are chosen from within the static and dynamic traits described in section 1.1.2. The (contextless) MABs are a special case, wherein the context is empty, i.e., an interaction-independent constant [31].\nDuring its rollout, a bandit algorithm gradually hones in on a set of optimal actions with a guaranteed confidence level for the outcome, allowing it to eventually abandon sub-optimal actions. This allows it to eventually minimize the gap between the taken and the unknown optimal action (the regret).\nIt is assumed that the law $r_t \\sim p(r | a_t, x_t)$ governing the interaction outcome is unknown but fixed and that current actions have no effect on the future contexts and interactions [31]. This is in contrast to the full MDP reinforcement learning setting, where the object being interacted with is a persistent state, which evolves due to interactions under a Markov decision process [56].\nAn important distinction from supervised or active learning is that bandit (and reinforcement learning) methods observe only partial feedback, meaning that the opportunity costs when learning an optimal policy are taken into account. As such, these algorithms need to strike a balance between exploration, that is, reliable identification of the action-reward feedback, and exploitation, i.e., leveraging the learned model to action policy that achieves the objective optimization goals.\nThere are extensions of the bandit setting to very large action spaces encountered in recommender systems [39], to settings in which the optimization aspect is not as crucial as the best-arm identification capabilities within a strict budget [9], and situations with budgeted interactions with a batch of concurrent Markov processes, which will be described in Section 2.3.3.\nThe following subsections cover the different algorithms available in the RL-platform described in 1.1 for operational sequential decision making."}, {"title": "2.3.1 Linear Bandits", "content": "A k-armed linear bandit assumes a linear model of the reward conditional on a context-action pair: $r_t = x_t^T \\theta_{a_t} + \\epsilon_t$, where $x_t \\in R^d$ is the feature vector, $(\\theta_k)_{k=1}^{K} \\in R^d$ are the coefficients of the k-the arm reward regression, and $\\epsilon$ is a conditionally independent subgaussian noise [12, 34].\nOriginally [34] used Upper Confidence Bound (UCB) for picking actions, which selects $a_t \\leftarrow arg max_k x_t^T \\theta_k + ucb_k (x_t; H_{<t})$, where $\\theta_k$ is the current k-th arm's reward model's coefficient estimate based on $H_{<t}$, and the second term represents the arm's optimism in face of uncertainty, computed from the best admissible linear model in a highly probable region determined from the interaction history [31, ch. 19]. The intervention described in Section 3 is delivered as proposed in [1]. Namely, a Bayesian approximation approach with the Normal likelihood with Gaussian-Gamma conjugate prior is used, which allows for the assignment of better-defined probabilities for the selected actions using Thompson sampling: $a_t \\sim P_{\\theta \\sim \\beta_t}(a_t \\in arg max_k x_t^T \\theta_k)$, where $\\beta_t$ is the current posterior belief $p(\\theta | H_{\\leq t})$. Thompson Sampling is implemented as a two-step sampling procedure. Still, it is worth noting that since the affine transformation $u \\rightarrow x_t^Tu$ of the conditional Gaussian part of the Gaussian-Gamma distribution yields a Normal-Gamma mixture, it is possible to sample in a single step from a location-scale student-t.\nThe linearity assumption equips linear bandits with regret guarantees that are sublinear in $T$, which stem from subgaussian concentration inequalities. They present an efficient learning alternative for use cases where the optimal action is to be selected based on a small collection of variables, for example, whether to send reminders to take medication and/or the importance of treatment adherence depending on self-reported adherence (or lack thereof) and interaction with received reminders in the previous few timesteps. It is also the approach used to send the item-pair recommendations introduced in Section 2.2.1 with the setup and results discussed in Section 3. They facilitate causal inference (i.e., statistical reasoning on under which circumstances the reminders work best) and are thus very useful for adaptive experimentation (see sSction 2.4)."}, {"title": "2.3.2 Beyond Linear Bandits", "content": "Linear bandits present substantial limitations in their representational power [51]. Deeper feature extractors can be used, i.e., replacing the reward model $x_t^T \\theta_{a_t}$ with $\\phi(x_t, a)$ (or other nonlinear models) and learning the relevant feature representation $\\phi$ along with the interaction. Increasing the complexity of the representations obscures statistical reasoning but can be the best alternative for intervention decisions for which optimality is expected to depend in complex ways on a variety of interrelated variables. This is the case, for example, of suggesting products to order with the goal of minimizing stockouts. The best recommendations will depend in complicated ways on the evolution of the demand and availability for multiple related products on different sites, the user's in-app activity, ordering behavior, and responses to previous suggestions, as well as on seasonal and environmental factors (disease outbreaks, weather, pollen levels,...).\nOne of the most used approaches to developing deep neural bandits is stacked neural-linear bandit: a linear bandit [1, 34] is grafted atop a deep feature extractor, [60, 65].\nFor example, [43] adopts the stacked neural-linear bandit approach and utilizes a small-capacity experience replay queue [17], and Gaussian-Gamma conjugate prior in the linear head to mitigate catastrophic forgetting. In particular, the method used updates a lower-level deep feature extractor $\\phi$ on the experience buffer (most recent $H_{<t}$), then in an empirical Bayes fashion, re-fits a prior for the bandit's head using reward variance matching based on the updated representations, and, finally, applies the Bayesian update to recompute the posterior.\nThe bandit problem is approached as a linearized Gaussian state-space model for the context-action-reward data in [14]. Specifically, the observation equation for the exogenous context $x$ and arm $a$ is modeled by a neural network $g(x, a; \\theta)$, with its parameters $\\theta$ being the unobserved state, which follows stochastic constant dynamics. The authors approximated the posterior of $\\theta$ using multivariate Gaussian distribution and suggested updating it with partial feedback observations using the extended Kalman Filter [13, 55]. To reduce the space complexity of the posterior, they re-parameterized $\\theta$ as a point in a low-dimensional linear subspace, either randomly, or inferred from the parameters' trace during a pre-training step."}, {"title": "2.3.3 Restless bandits", "content": "In contrast to multi-armed bandits, the restless bandit setup (RMAB) is closer to the full MDP RL formulation in that the present actions (or inactions) affect the future, making planning a more pressing concern. Subjects have a simple internal state (often a binary switch, e.g., is the patient adhering to treatment or is the drug in stock for the pharmacy) with evolution following simple intervention dynamics that can be subject-specific. The number of available interventions (e.g. follow-up call to assess adherence and remind the patient of its importance or drug stocks available for distribution) is limited, and the goal is to maximize the number of subjects with the desired internal state (in the examples, treatment adherence across patients or well stocked pharmacies of an essential drug).\nThe RMAB setting explicitly considers an incomplete and imperfect information situation involving overseeing a finite number of concurrent Markov Decision Processes with the express goal of optimizing cumulative reward by budgeting interventions among the processes at each decision point. Note the contrast with the MAB setting, wherein it is tacitly assumed that the unknown state and arm-conditional reward distributions are stationary. Additionally, the arms in RMAB refer to the overseen processes rather than action alternatives since the most commonly considered scenario is whether to intervene or skip.\nIn most RMAB applications, the Markovian dynamics of the processes looked after are unknown, which adds the exploration and estimation dimension to this concurrent control problem. Processes' model identification thus deals with counterfactual estimation since each MDP may have different state transition probability $p$, yet for each action and passive inaction, we observe only the partial states and rewards obeying $p(s', r' | s, a = 1)$ and $p(s', r' | s, a = 0)$, respectively.\nComputing the optimal policy in RMAB is a hard combinatorial optimization problem, and most applications resort to a polynomial-time heuristic policy known as the Whittle index policy [57]. The heuristic ranks the supervised processes according to the optimal reward subsidy, which tells how much to increase the passive reward of the MDP to make it just as rewarding as intervening. The ranking is used for the greedy selection of the arms to play. The existence of such policy requires the so-called indexability property to be satisfied for the Markov kernel and rewards of each process, which may be challenging to ensure in practice. It guarantees that at any given state, the optimal action does not switch from skipping back to intervention while the subsidy increases.\nEquitable RMABs (ERMABs) are those with policies that also simultaneously maximize some equity function on the relative differences between rewards allocated across different pre-defined groups of arms. Sequential resource allocation with equitable objectives is essential when the decisions are of sensitive nature, as are many in the realm of health care, such as the distribution of limited stocks of essential drugs. Efficient algorithms for RMAB satisfying the maximin reward (maximization of the minimum prospective reward per group) and the maximum Nash welfare (maximization of the product of all group rewards) conditions are introduced in [27].\nAs a resource allocation approach to budget-aware planning in concurrent processes, restless bandits found their application in many areas, from addressing exploration and exploitation trade-off in protecting endangered wildlife from poaching [50] to radio spectrum sensing [3]. In healthcare applications, the most notable examples include allocating call-center resources for targeted maternal health information messaging [7, 41, 44], optimal carcinoma screening in resource-constrained setting [33], healthcare worker visitation scheduling on regional and local level [6], optimization of timing of costly behavioral health interventions [2], tuberculosis treatment adherence [28], and mobile health clinics deployment [46]."}, {"title": "2.4 Experimentation", "content": "The RL-platform allows to perform experiments in order to measure the impact of interventions. Different experimental designs are available. Assignment to control and the intervention strategies on trial can be fully random or adaptive, the latter using stochastic MABs (see section 2.4) [10, 15, 59, 61]. For repeated interventions (e.g., weekly order suggestions), intra-subject assignment (i.e., assignment multiple times of the same subject throughout the experiment) is also an option to increase effective sample sizes when the impact of the intervention is expected to concentrate immediately after it is delivered, in the so-called micro-randomized trial design [29, 49, 64] when assignment is fully randomized."}, {"title": "2.5 Intervention Impact Analysis", "content": "The following subsections explore methods to assess the effectiveness of interventions and measure their impact. Some methods are suitable only for randomized or adaptive designs, but all are relevant in setups using a linear bandit for adaptive interventions, which function as adaptive experiments. In these experiments a portion of the user cohort is left out as a pure control\u00b9 to evaluate the intervention's overall effect. This approach is used in item-pair recommendations, discussed in Section 3."}, {"title": "2.5.1 T-tests", "content": "Whenever the response variables considered are quantitative and normally distributed, the hypothesis test to assess whether the mean behaviour in the intervention and pure"}, {"title": "2.5.2 Linear mixed-effects models", "content": "A linear mixed effects model (LMM) extends the classical linear model by incorporating both fixed effects, which are the same across individuals, and random effects, which vary between individuals. In RCT with repeated measurements for the same subjects over time, LMMs model the within-subject correlation over time. Fixed effects are the systematic influences shared across all subjects, and random effects capture subject-specific variability [20, 30]. The general form of a linear mixed effects model is $y_{ij} = \\beta_o + \\beta_1X_{ij} + b_i + e_{ij}$, where $y_{ij}$ is the response variable for subject $i$ at time $j$, $\\beta_o$ and $\\beta_1$ are fixed effect coefficients, $X_{ij}$ is the predictor variable for subject $i$ at time $j$, $b_i$ is the random intercept for subject $i$, assumed to be normally distributed with mean 0 and variance $\\sigma_u^2$, and $e_{ij}$ is the residual error term, assumed to be normally distributed with mean 0 and variance $\\sigma^2$.\nIn the context of experiments with digital interventions for an adaptive user journey, LMMs can be used for baseline value and other covariate adjustements and to model the effect with time. For example, in a setup where the interventions are messages that are sent adaptively to improve user engagement, we might specify $Y_{it} = \\beta_o+\\beta_1Y_{io} + \\beta_2X_i + \\beta_3M_{it} + \\beta_4T_t+\\beta_5 (X \\times T)_{it} + u_i + \\epsilon_{it}$ where $Y_{it}$ is the outcome for subject $i$ at time $t$ and $y_{io}$ its baseline value, $X_i$ is 1 if the user is part of the intervention and 0 if in pure control, $M_{it}$ indicates whether the subject was nudged at at time $t$, $\\beta_o$ is the intercept, $\\beta_1$ the baseline adjustment, $\\beta_2$ is the fixed effect of the treatment, $\\beta_3$ is the fixed effect o a nudge, $\\beta_4$ is the fixed effect of time, $\\beta_5$ is the fixed effect of the interaction between treatment and time, $u_i$ is the random effect for subject $i$, and $\\epsilon_{it}$ is the residual error term. The random effects $u_i$ are assumed to be normally distributed with mean zero and variance $\\sigma_u^2$, and the residual errors $\\epsilon_{it}$ are assumed to be normally distributed with mean zero and variance $\\sigma^2$. The time varying effect at time $t$ is $\\beta_2+\\beta_5T_t$ for users not nudged at time $t$ and $\\beta_2 + \\beta_3 + \\beta_5T_t$ for user who received a message at time $t$. For the analysis presented in the results described in Section 3.2, we use this model specification, and iteratively remove terms with non-significant parameters. Significance is assessed using the Wald test, which evaluates the null hypothesis of an estimated parameter or set of parameters being equal to zero."}, {"title": "2.5.3 Bandit assignment and sensitivity to context", "content": "The analysis of data from experiments with adaptive designs needs less stress on statistical inference as the learning on whether there is evidence of impact for different values of the context is carried out by the bandit algorithm as the experiment runs. Within the adaptive intervention, we track the evolution of the average reward per arm and of the fraction assigned to each arm.\nTo understand the role of the context we estimate how large the change in the probability of picking each of the arms would be, and in which direction, when the trait increases or decreases. These sensitivities are computed as the Jacobian of the arm probability based on softmax Thompson sampling approximation. For each trait, we compute the standard deviation of the derivative across the sample and use it to soft-threshold the sensitivity before averaging it, such that the sensitivity of arm a with respect to contextual trait b is given by $s_{ab} = 1/\\eta \\sum_j S_{\\lambda\\sigma_{a,b}} (Z_{ab}(x_j))$ with $\\lambda = 1/5where S_\\lambda(x)$ the soft-thresholding operator $S_\\lambda(v) = v max\\{1 \u2013 \\lambda/|v|, 0\\}$ with $\\lambda \\geq 0$. These sensitivities are then normalized and transformed into one of the 7 categories: negligible, and small, medium or large negative or positive sensitivity for ease of interpretation. To further understand whether the context as a whole has been adequately selected, we use t-Distributed Stochastic Neighbor Embedding (t-SNE) visualizations in the contextual trait space of both the best arm and its confidence (i.e., the difference between the assignment probabilities of the best and second best arms). T-SNE is a nonlinear dimensionality reduction technique used to visualize high-dimensional data by converting similarities between data points into joint probabilities and minimizing the Kullback-Leibler divergence between these probabilities in a low-dimensional space [24, 40]."}, {"title": "2.5.4 Recommendation success analysis", "content": "For all message-based nudges we also track how the users interacted with them, i.e., whether the messages were opened (by tapping on them), closed (clicking on the x symbol on their top right hand corner) or ignored (neither opened nor discarded). When the messages contain specific recommendations, we also track how many of these are then realized by the users (the recommended product purchased in this particular case), and how this depends on whether the message was opened, discarded or ignored."}, {"title": "3 ADAPTIVE ITEM PAIR RECOMMENDATIONS", "content": "The adaptive item pair recommendations that are being used in SwipeRx and for which several experiments have already been run are used to illustrate the potential of the framework described in this paper."}, {"title": "3.1 Intervention Setup and Experiments", "content": "The intervention involves sending weekly in-app messages with item-pair recommendations (see Section 2.2.1)", "state": "Pharmacies in your area typically purchase A and B. Click here to order now!\" Top 20% spenders were excluded from the initial"}]}