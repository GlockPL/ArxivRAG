{"title": "Spatial Transformer Network YOLO Model for Agricultural Object Detection", "authors": ["Yash Vivek Zambre", "Ekdev Rajkitkul", "Akshatha Mohan", "Joshua Peeples"], "abstract": "Object detection plays a crucial role in the field of computer vision by autonomously identifying and locating objects of interest. The You Only Look Once (YOLO) model is an effective single-shot detector. However, YOLO faces challenges in cluttered or partially occluded scenes and can struggle with small, low-contrast objects. We propose a new method that integrates spatial transformer networks (STNs) into YOLO to improve performance. The proposed STN-YOLO aims to enhance the model's effectiveness by focusing on important areas of the image and improving the spatial invariance of the model before the detection process. Our proposed method improved object detection performance both qualitatively and quantitatively. We explore the impact of different localization networks within the STN module as well as the robustness of the model across different spatial transformations. We apply the STN-YOLO on benchmark datasets for Agricultural object detection as well as a new dataset from a state-of-the-art plant phenotyping greenhouse facility. Our code and dataset are publicly available.", "sections": [{"title": "I. INTRODUCTION", "content": "Plant phenotyping is critical for crop improvement [1], yield optimization [2], and sustainable practices [3]. Artificial intelligence (AI), particularly object detection algorithms, has transformed plant phenotyping, enhancing efficiency and performance [4]. The You Only Look Once (YOLO) has been used effectively in various Agricultural applications such as pest detection [5], crop disease detection [6], and crop harvesting [7]. Despite the vast number of use cases, YOLO has some limitations due to various spatial transformations [8]. Spatial transformer networks [9] are an approach to improve an artificial neural network's robustness to spatial transformations. We propose to integrate STNs with the YOLO model to incorporate spatial invariance. The STN applies a learnable affine transformations to the images that will help with object detection. The STN-YOLO model demonstrates spatial invariance and outperforms the baseline YOLO model on several Agricultural benchmark datasets.\nAdditionally, we present a new, high quality, annotated dataset of various plants to advance the study of agriculture plant detection and phenotyping. The new Plant Growth and Phenotyping (PGP) dataset differs from other available image datasets in the following aspects: (1) multi-spectral images captured across varying heights; (2) challenging images of multiple crops across varying illumination conditions; (3) precise annotations assisted with the use of Segment Anything model [10], (4) large size and shape variations of plants. The features of the PGP dataset present new challenges for agricultural object detection. We perform extensive experiments on the proposed database and new model (STN-YOLO) for object detection. The key contributions of our work are the following:\n\u2022\tIntegration of STN within YOLO for improved object detection performance\n\u2022\tNew benchmark dataset for plant object detection."}, {"title": "II. RELATED WORK", "content": "A. Object Detection Models\nObject detection is a prominent challenge in computer vision, where researchers use deep learning to boost performance [11]\u2013[13]. Two-stage detectors, such as Faster R-CNN [14], involve distinct stages with a region proposal network (RPN) and region of interest (ROI) pooling for candidate bounding boxes [11]. In contrast, single-stage detectors such as single-shot detectors (SSD) predict bounding boxes directly by utilizing grid boxes and anchors, as described in [15]. Notably, YOLO has emerged as a prominent example of a single-stage detector. The advancements of CNNs architectures introduces innovations such as anchor boxes to enhance object detection [13]. Key advancements include R-CNN, integrating region proposals with CNN using support vector machine (SVM) classification and bounding-box regression [16]. However, R- CNN incurs substantial computational costs and information loss. Fast R-CNN extends this with ROI pooling and proposal refinement, improving test speed and algorithm precision [17]. Faster R-CNN further reduces computational requirements by introducing the RPN for regional proposals, selecting anchors based on specific criteria for outstanding recognition precision [14]. Ongoing advancements focus on speed optimization, including anchor-free detection [18].\nB. Spatial Transformer Network (STN)\nSTN is a module with differentiable properties that performs learnable spatial transformation on input feature maps [9]. The transformation is dependent on the specific input, generating a singular output feature map. In the case of multi-channel inputs, an identical transformation is applied on each channel. The STN module is comprised of three components, as illustrated in the Figure 1. The first component is a localization network, which passes input feature maps through several hidden layers to generate the parameters for a learnable affine transformation that is applied to each feature map. Subsequently, the predicted transformation parameters \u03b8 are used to construct a sampling grid\u2014a set of points indicating where the input feature maps should be sampled to generate the transformed output. Finally, the feature maps and the sampling grid serve as inputs to the sampler that generates the output map sampled from the input at the specified grid points.\nLocalization Networks The localization network (f_loc) takes the input feature maps X \u2208 R^{H\u00d7W\u00d7C} with width W, height H, and C channels. The features extracted by the localization network are then passed into a fully connected layer that outputs \u03b8, the parameters of the transformation T_\u03b8 to be applied to each feature map: \u03b8 = f_loc(X). The size of \u03b8 can vary depending on the transformation type that is parameterized [9]. The localization network function f_loc() can take any form, such as a fully-connected network or a CNN such as ResNet18 [19], but should include a final regression layer (i.e., fully connected layer) to produce the transformation parameters \u03b8 (e.g., affine transformation have six parameters).\nParameterized Sampling Grid To perform a transformation on the input feature map, each resulting \u201cpixel\u201d is determined by applying a sampling kernel centered at a specific location in the input feature map. The term \u201cpixel\u201d in this context refers to an element of a general feature map, not necessarily an image. In a general context, the resulting \u201cpixels", "1": "n\n[\\begin{bmatrix} x_i^s \\\\ y_i^s \\end{bmatrix} = T_\\theta(G_i) = A_\\theta \\begin{bmatrix} x_i^t \\\\ y_i^t \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\end{bmatrix} \\begin{bmatrix} x_i^t \\\\ y_i^t \\\\ 1 \\end{bmatrix} \\tag{1}\n\nIn this scenario, (x^t_i, y^t_i) denotes the target coordinates of the regular grid in the output feature map, and (x^s_i, y^s_i) represents the source coordinates in the input feature map that define the sample points. The relationship between these coordinates is determined by the affine transformation matrix A_\u03b8 [9]. The STN is self-contained module that is versatile and can be seamlessly integrated at any point in an artificial neural network architecture. The inclusion of an STN at the beginning of a CNN enables the network to learn how to dynamically transform input images to account for spatial variations."}, {"title": "III. METHOD", "content": "A. Plant Growth and Phenotyping Dataset\nThe Plant Growth and Phenotyping (PGP) dataset is a publicly available, high-resolution (512 \u00d7 512 pixels), multi- spectral collection of plant images captured from a state- of-the-art greenhouse. PGP consists of 1137 images from different crops: 442 corn, 246 cotton, and 449 rice plants. The dataset includes images taken from various heights of the same plants. The plant phenotyping facility consisted of a camera mounted on a robotic arm, capturing side and top views of the plants (only side view images are used in this work). The Multispectral Imaging System (MSIS-AGRI-1-A), integrating an MSC2-AGRI-1-A snapshot multispectral camera and a 4- channel LED illuminator, was used to collect images across four spectral bands: 580 nm (Green), 660 nm (Red), 730 nm (Red Edge), and 820 nm (Near Infrared).\nFor this work, the Red, Red Edge, and Green bands were combined to create pseudo \"RGB\u201d images compatible with pre-trained machine learning models.\nB. STN-YOLO MODEL\nFor the proposed STN-YOLO model, the STN module is first integrated in the beginning of the YOLO model to account for spatial transformations in the input images. After the images passes through STN module, the YOLO model is then used to perform the task of object detection. As stated in Section II-B, the STN has can use any backbone for the localization networks. The localization network within the STN module comprises various convolutional layers, linear layers, and activation functions.\nLocalization in STN model The STN models have the localization network which takes the input feature maps with width W and height H and C channels and outputs \u03b8 which are the transformation parameters. The localization network is selected by the user and can vary from a shallow or deeper net- work. The performances of different localization network are compared to evaluate if a shallow or deep, pre-trained model will lead to better object detection performance quantitatively (e.g., accuracy, precision, and recall) and computationally. The shallow network is comprised of one convolution layer with max pooling and ReLU activation function was used in contrast to the deep pretrained models. Following the convolution layer in the shallow network, an adaptive average pooling layer was used to aggregate the spatial resolution of the feature maps to a desired size (e.g., 28 \u00d7 28).\nYOLO Backbone The output of the STN module is then given to the YOLO model where, P1, P2, P3, P4, and P5 typically refer to different levels or stages in the feature pyramid and these are the different convolutional layers. Output features are obtained by extracting features at different scales from the input image to detect objects of various sizes and capture both fine-grained details and global context. P1 corresponds to the finest level with the highest resolution, while P5 is the coarsest level with lower resolution but capturing more context. The head in the YOLO model usually refers to the part of the network responsible for making predictions. The head takes features from multiple pyramid levels and produces predictions for object classes, bounding box coordinates, and other relevant information.\nLoss in YOLOv8 A two term loss function is used during training. The objective function consists of two components: classification loss (CLS), which penalizes errors in predicting object classes, and bounding box (BBOX) loss, which penal- izes errors in predicting the coordinates of the bounding boxes around objects. Combining these losses helps train the model to simultaneously improve both classification and localization aspects of object detection. Class imbalance in single-class detection refers to the discrepancy between detection target objects and the background, typically considered the negative class. Addressing this imbalance is crucial for improving detection performance. Bounding box loss in YOLOv8 uses the complete intersection over union (CIOU) loss [21], which considers box overlap and aspect ratio differences, enhancing regression performance. Distributed focal loss (DFL) [22] effectively tackles class discrepancy by dynamically adjusting the loss for each class during training to counteract biases caused by class imbalances."}, {"title": "IV. EXPERIMENTAL RESULTS AND DISCUSSION", "content": "A. Experimental Setup\nThree experimental runs of random initialization were used for each model and the YOLO backbones were pretrained on the COCO dataset. We used the AdamW optimizer with an initial learning rate of 0.002. The optimizer was configured with distinct parameter groups, each with specific weight decay values assigned to all parameters. The batch size for all experiments was set to 16. We used 100 epochs with early stopping (patience of 50) for each model. All the experiments were performed using the Ultralytics [23] framework and performed on an NVIDIA A100 GPU.\nIn our study, we used various benchmark datasets to assess the effectiveness of our proposed plant detection models. The GlobalWheat2020 dataset contains 4,000 images, with 3,000 for training and 1,000 for testing, focusing on wheat detection. The PlantDoc dataset comprises 2,569 images, with 2,330 for training and 239 for testing, addressing plant identifica- tion across varying environments. The MelonFlower dataset includes 288 images, with 193 for training and 95 for test- ing, specifically targeting melon flower identification. These datasets collectively provide a robust evaluation framework for assessing model generalization across diverse plant species and environments.\nB. Ablation Studies using PGP Dataset\nImpact of Localization Networks In Table I, we outline the performance metrics of various localization networks, empha- sizing the comparable quantitative results achieved by both the shallow localization network and deep pre-trained models. Both the shallow and deep networks used a global average pooling layer after the convolutional layers. Based on these findings, we opted for the shallow network as the localization network in the STN-YOLO model. This decision stemmed from the observed effectiveness of the shallow network in extracting features suitable for learning affine transformations comparable to the capabilities of the pre-trained models (based on similar quantitative measures). The shallow network and the pretrained networks were finetuned with the baseline YOLO model. The number of learnable parameters for the deeper networks, VGG16 and ResNet18 (approximately 82 million and 49 million respectively), were greater than the shallow network (approximately 5 million) for marginal improvements in performance hence the shallow network was chosen for the remaining experiments.\nImpact of Spatial Information To assess the impact of retaining more spatial information in the STN-YOLO model, modifications were applied to the last layer of the shallow localization network. Specifically, the adaptive average pooling layer, was evaluated for various spatial sizes of 1 \u00d7 1, 7 \u00d7 7, and 28 \u00d7 28 as shown in Table II. This investigation aims to evaluate the model's sensitivity to variations in spatial information preservation and identify the optimal configuration for improved performance in object detection tasks. We wanted to look at no spatial information (1 \u00d7 1) and retaining some spatial information (7 \u00d7 7, 28 \u00d7 28). The results demonstrated that retaining the most amount of spatial information (28 \u00d7 28) resulted in the best object detection performance. If the feature maps have more spatial information, then there is more context that the model will be able to use for maximizing performance. However, as the spatial resolution is increased, so does the computational cost as the number of parameters will increase.\nModel Analysis using Explainable AI The proposed model is better on average at reducing the detection of false positives, as the precision of the STN-YOLO model is better than the baseline YOLO model as shown in Table III. The STN- YOLO model also exhibits improved recall and mean average precision (mAP), which demonstrates a better object detection model as illustrated in Figure (4b-4c), where the baseline YOLO model fails to eliminate the bottom right vase area in comparison to the proposed STN-YOLO model. In addition to these metrics from Table III, we also provide insights from Eigen class activation maps (EignenCAMs) [24] as seen in Figure (4d-4e). These EigenCAMs offer valuable visualization for model explainability, as they highlight significant aspects of the images that contribute to the model's predictions. The results support the hypothesis that incorporating the spatial invariance in the input image can result in better object detection both qualitatively and quantitatively as the STN- YOLO emphasizes more of the plant area.\nAugmentations Testing One of the most important set of experiments performed was the testing of models on the unseen augmented data as shown in Table III. To do this, no data augmentation was added to the training dataset, but data augmentations were performed on the test dataset to evaluate the robustness of the STN-YOLO model across various transformations. The augmentations used on the test dataset were random cropping (15% zoom), shear (\u00b1 10\u00b0 horizontal and \u00b1 10\u00b0 vertical), and rotation (between \u00b1 10\u00b0) to simulate image conditions that may happen in the facility. We observed that the proposed STN-YOLO model gave better quantitative results in case of rotation and shear for PGP dataset compared to the baseline YOLO model. This result is intuitive as rotation and shear are affine transformations that can be learned by the STN-YOLO model. However, in the cases where cropping is used, the performance is comparable to the baseline YOLO method as cropping is not an affine transformation. Furthermore, for the combinations of the different augmentations (e.g., rotation with shear, rotation with crop), we observed that for most of the data augmentation cases (six out of eight), the STN-YOLO model performs better on average than the YOLO baseline for the precision values indicating that the STN-YOLO was effective at reducing the number of false positives by incorporating the added spatial invariance properties of the STNs.\nC. Benchmark Datasets Results\nGlobal Wheat2020 Dataset The GlobalWheat2020 dataset [25] is a comprehensive Agricultural dataset containing images of wheat heads sourced from various countries. From the results in Table IV, the STN-YOLO model has a higher average value for most metrics (except accuracy), but the baseline YOLO model is comparable to the average value when you take the standard deviation in comparison. We can also qualitatively look at this observation from Figure 5a where the number of detections by YOLO in the GlobalWheat2020 dataset are less as compared than the detections of STN- YOLO. For the GlobalWheat2020 dataset, the images con- sisted of \"zoomed\"-in views of the wheatheads. In this case, the spatial invariance would not impact the object detection results and this is indicated by the slight improvements in the average object detection metrics (except for accuracy) shown in Table IV.\nPlantDoc Dataset The PlantDoc dataset [26] is derived from the Plant Village dataset (PVD) [27]. The dataset was created using an automated system leveraging GoogleNet [28] and AlexNet [29]. Interesting observations can also be seen from the Table IV such as unlike the GlobalWheat2020 dataset, there is an increase in all the object detection metrics in the STN-YOLO model as compared to the baseline model. Particularly, the precision for STN-YOLO model is higher than the baseline model indicating the model reduced the number of false positives detected as noted for the PGP dataset. Qualitatively, we can see that in Figure 5b where the YOLO detects the leaf correctly but wrongly classifies the leaf as tomato early blight leaf. However, the STN-YOLO detects and correctly classifies the leaf as Tomato Septoria. In the PlantDoc dataset, the emphasis remains on various plants and zoomed-in images. However, spatial invariance remains insignificant due to the absence of pre-translated or rotated input images. Despite this, spatial invariance could potentially offer some benefits with this dataset, particularly as certain images provide a more \u201czoomed-out"}, {"title": "V. CONCLUSION", "content": "This work focused on integrating STN with YOLO, cre- ating the STN-YOLO model to address spatial invariance challenges. The model aimed to enhance plant image detection quality and can be used for downstream applications such as phenotypic feature extraction. Results demonstrated that STN improved model robustness and reduced number of false posi- tives in the datasets as indicated by the higher precision scores. The STN-YOLO model improved performance on benchmark datasets, showcasing potential in handling real-world spatial transformations. Future work includes integrating STNs into other object detection models (e.g., future versions of YOLO [31]), developing new objective functions to improve the learning of the model with STNs, expanding our PGP dataset to include more images across multiple crops with different image conditions (e.g., illumination), and incorporating the near infrared channel."}]}