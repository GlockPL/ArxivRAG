{"title": "LLM Assistance for Pediatric Depression", "authors": ["Mariia Ignashina", "Paulina Bondaronek", "Dan Santel", "John Pestian", "Julia Ive"], "abstract": "Depression is a complex mental health condition, particularly prevalent among young people aged 10-24, a group experiencing a sharp rise in cases. Traditional screening methods, such as the PHQ-9, are particularly challenging for children in pediatric primary care due to practical limitations. AI has potential to help but the scarcity of annotated datasets in mental health, combined with the computational costs of training, highlights the need for efficient, zero-shot approaches. Large Language Models (LLMs) offer a promising, computationally affordable zero-shot solution by extracting relevant text segments from electronic patient notes to support clinicians in identifying depressive symptoms, as well as for downstream AI. In this work, we investigate the feasibility of state-of-the-art LLMs for depressive symptom extraction in pediatric settings (ages 6-24). This approach aims to complement traditional screening and minimize diagnostic errors.\nMethods: We examined free text of the EHRs of pediatric patients with the diagnosis of depression or related mood disorders (age groups 6-24, 1.8K patients) from at the Cincinnati Children's Hospital Medical Center. We noticed drastic inconsistencies in the application and documentation of PHQ-9 screening", "sections": [{"title": "1 Introduction", "content": "Depression is a complex mental health condition that affects 4,7% [1] across various age groups, including children, adolescents, and young adults. It is typically characterized by persistent sadness, loss of interest or pleasure (anhedonia), and a broad range of emotional, cognitive, and physical symptoms. Unlike temporary emotional fluctuations, depressive disorders are marked by their severity, functional impairment, and resistance to relief from everyday positive experiences or social interactions [2].\nYoung people (ages 10-24) are particularly vulnerable to depression, as it coincides with critical stages of social, emotional, and cognitive development. In recent years, there has been a sharp rise in depression within this age group, especially among females. Many young individuals also exhibit \u201csubthreshold depression\" [3], where depressive symptoms are present but do not meet the full diagnostic criteria for major depressive disorder (MDD).\nA commonly used tool for assessing depression is The Patient Health Questionnaire (PHQ-9). The PHQ-9, developed by Kroenke et al. [4], is a validated screening instrument based on the Diagnostic and Statistical Manual of Mental Disorders (DSM-IV) criteria for major depressive disorder. It includes nine questions where individuals rate the frequency of depressive symptoms over the past two weeks on a scale of 0 to 3. The total score indicates the severity of depression, with higher scores reflecting greater impairment.\nHowever, screening is difficult to implement in the pediatric primary care setting, with traditional approaches that rely on multiple informants often producing discrepant findings [5]. Beyond this, clinicians in the primary care setting vary in how they consider risk factors and interpret symptoms [6], and clinicians may struggle to disentangle overlapping symptoms [7].\nMost research on detecting depression using machine learning focuses on analyzing social media data [8], where challenges like metaphorical language, sarcasm, and data scarcity make the task particularly complex. Inherent data sparsity problem had led to semi-automatic approaches to data collection via identifying self-expressions of mental illness diagnoses [9].\nEarly studies, such as Yazdavar et al. [10], address the depression detection problem by leveraging vocabulary matches and traditional models like Latent Dirichlet Allocation (LDA) to identify depression-related topics based on PHQ-9 criteria. Building on this foundation, [11] explores the complexities of figurative language, such as metaphors and sarcasm, in depressive social media posts using fine-tuned Transformer-based BERT models to handle this language. More recent studies such as [12] emphasise that the detection of subtle, figurative expressions of depression remains a significant hurdle even for the powerful pre-trained models.\nLarge Language Models (LLMs) have demonstrated significant potential in mental health care due to their advanced linguistic capabilities and generalization ability [13]. A key strength of LLMs is their capacity for zero-shot analysis, where they can process and extract insights without requiring annotated training data. This capability is especially valuable in mental health, where annotated datasets are limited due to ethical and privacy concerns. In healthcare applications, smaller models like FLAN-T5 have shown promising results in zero-shot evidence extraction, identifying cases such as postpartum hemorrhage more effectively than traditional methods [14]. Similarly, in social science research, FLAN-T5 has outperformed human crowdworkers in producing high-quality free-form explanations, highlighting its potential for zero-shot data annotation [15].\nDespite these promising capabilities, significant challenges remain when using LLMs directly in mental health contexts. Key concerns include hallucinated content, biased outputs, and random guesses when models attempt to classify data without adequate context [16]. These issues undermine their reliability as standalone diagnostic tools, even when human oversight is involved. To address these limitations, a safer approach is to leverage LLMs for evidence extraction rather than classification. By focusing on extracting text-based insights for clinical interpretation, this method reduces the risks associated with hallucinations and inaccuracies while preserving the benefits of automated data processing.\nIn general domains, LLMs have proven effective for retrieving supporting evidence in question-answering tasks [17]. These models excel at extracting relevant information from large text corpora, which can be further analyzed by professionals to support clinical decision-making. This application of LLMs as linguistic search tools presents a practical and low-risk use case in healthcare settings.\nRecent studies have applied LLMs to annotate symptoms of mental health conditions, such as depression, from online posts and psychological interviews [18-21]. These symptom annotations can serve as valuable inputs for downstream analytical models, enhancing the overall robustness of mental health screening. To optimize computational costs, pre-filtering techniques are commonly employed before applying LLMs, ensuring efficient and cost-effective processing.\nAI, particularly LLMs, has the potential to significantly enhance mental health screening procedures, especially given the challenges posed by the lack of comprehensive data and the nuanced variability in depressive symptom presentations. LLMs, with their advanced linguistic capabilities, become essential in this context, as they can efficiently extract and interpret relevant information from unstructured text."}, {"title": "Study Aim and Objectives", "content": "In this work, we focus on assessing the feasibility of using state-of-the-art large language models (LLMs) to assist clinicians in identifying depressive symptoms from free-form text in pediatric electronic health records (EHRs). Rather than relying on LLMs for diagnostic decisions, the models are used to extract potentially relevant text segments, ensuring that clinicians maintain full control over interpretation and decision-making. This approach adheres to ethical principles by preserving human judgment in the diagnostic process, offering a supportive tool for mental health screening in the challenging pediatric setting (ages 6-24).\nThis study aimed to evaluate the feasibility of using state-of-the-art LLMs to support mental health diagnostics in pediatric primary care by extracting relevant text segments from EHRs for depressive symptom detection based on PHQ-9 criteria.\nThe key objectives were to: 1) Assess the feasibility of zero-shot LLMs in identifying depressive symptoms in free-text clinical notes, with a specific focus on pediatric populations, 2) Compare and benchmark the zero-shot performance of three leading LLM models in extracting PHQ-9 symptom-related information, using manually labeled EHR records as ground truth, 3) Demonstrate the potential clinical value of LLM-driven evidence extraction to improve mental health screening and reduce data sparsity issues using an example of an interpretable AI-based depression screening tool."}, {"title": "2 Methods", "content": "This study involved a retrospective cohort analysis of pediatric electronic health records (EHRS) collected from a major pediatric institution over a 10-year period. We apply a range of state-of-the-art NLP methods to these data.\nWe used a foundational database comprising electronic health record (EHR) data from the Cincinnati Children's Hospital Medical Center (CCHMC) Epic Link, an online platform that provides real-time access to patient charts and clinical results. This database includes approximately 1.3 million unique pediatric patients treated at CCHMC between January 1, 2009, and March 31, 2022, with a total of 63 million clinical notes.\nFor this study, we focused on clinical notes recorded between January 2011 and December 2021. PHQ-9 scores were identified by searching for the pattern \"PHQ-9 Total Score:\" within the notes, resulting in 2% of the notes containing such scores."}, {"title": "2.1 Symptom Annotation Process", "content": "The annotation process was primarily guided by the 21 questions from Beck's Depression Inventory (BDI) [22], which were further augmented with additional symptoms from PHQ-9. Domain experts considered the BDI's comprehensive scope better suited for identifying depression in pediatric patients. Additionally, PHQ-9's semantics posed interpretability challenges for large language models (LLMs), reinforcing the need to use BDI as the primary framework.\nA total of 85 clinical notes from Cincinnati Children's Hospital Medical Center were manually annotated. These notes were sourced from departments including Adolescent Medicine, Endocrinology, Primary Care, Pediatrics, and Gastroenterology, covering 22 patients (13 females and 9 males) aged 15-17 years. The average note length was 425 words, and the dataset maintained a balanced class distribution, with 50% of the notes containing at least one symptom.\nAnnotations were performed at the sentence level, explicitly marking the presence or absence of depressive symptoms. This process resulted in binary labels (positive or negative) for each symptom across the entire note. For example, a note was labeled as positive for \"Irritability\" if any sentence within the note contained evidence of irritability."}, {"title": "2.2 Challenges in Annotation", "content": "Several challenges were encountered during the annotation process: 1) The descriptions of symptoms varied widely across patients, making consistent annotation difficult. For instance, while some notes explicitly mentioned concrete symptoms such as \"loss of appetite,' others used ambiguous terms like \"weight concerns\" without specifying whether the change was related to increased or decreased appetite, 2) Pediatric patients often struggled with self-awareness and articulation of certain symptoms, particularly those requiring introspection, such as guilt or self-loathing. These symptoms were less frequently documented, as they require direct elicitation from clinicians during consultations; 3) Notes written by different clinicians showed diverse styles and depth of reporting. For example, nurse notes frequently captured nuanced behaviors and subtle symptoms that might not have been documented in more formal physician notes. Behavior-based symptoms, such as \"neglecting activities,\" were described in varying contexts, such as \"the patient quit drama club\" or \"the guardian needs to encourage the patient to do drama.\"\nIn contrast to social media texts, where somatic signs are usually absent [12], our clinical data explicitly mention patient general physical states and behaviors. This allowed us to capture a broader range of symptoms, including physical manifestations, which are often overlooked in digital text analysis.\nWe identified the following symptom categories from the clinical notes (all examples are paraphrased):\n1.  Not going to school: Interruptions in on-site schooling, e.g., \"The patient hasn't been to school this month.\u201d Related to BDI items 4 (\u201cI am dissatisfied or bored with everything.\u201d) and 15 (\u201cI can't do any work at all.", "activities": "Interruptions in exercising or extracurricular activities, or absence of physical activities, e.g., \"The patient didn't roll into any after-school activities or clubs this year.\" Related to BDI items 4 and 15.\n3.  No motivation: Lack of motivation in completing day-to-day tasks, such as brushing teeth, e.g., \"The patient has stopped following their diet plan.\" Related to BDI items 4 and 15.\n4.  Feeling depressed: Experiencing sadness or depression, e.g., \"The patient has been feeling sad since their relative passed away.\" Related to BDI items 1 (\"I am so sad and unhappy that I can't stand it.\u201d) and 2 (\u201cI feel the future is hopeless and that things cannot improve.", "anxious": "Feeling anxious, overwhelmed, or stressed, e.g., \"The patient is reporting elevated levels of stress caused by recent personal events.\u201d Related to BDI item 20 (\u201cI am so worried about my physical problems that I cannot think of anything else.", "down": "Experiencing low mood, e.g., \"The patient's mood has recently lowered.\" Related to BDI items 1 and 2.\n7.  Irritability: Anger or frustration directed at guardians or clinical staff, e.g., \"The patient seemed very annoyed by our questions.\u201d Related to BDI item 11 (\u201cI feel irritated all the time.\u201d).\n8.  Mental health concerns: General or unspecified mental health concerns, e.g., \"Patient's school counselor noted concerns regarding the patient's mental health state.\" Not directly related to a BDI item, but associated with PHQ-2 (\u201cFeeling down, depressed, or hopeless?\u201d).\n9.  Sleep problems: Issues with sleep, e.g., \"The patient complains about having difficulty falling asleep at night and waking up in the morning.", "I wake up several hours earlier than I used to and cannot get back to sleep.": "and 17 (", "appetite": "Increased appetite or overeating, e.g., \u201cThe patient has gained 10+ pounds since their visit two months ago.", "Poor appetite or overeating?": "."}, {"appetite": "Reduced appetite or weight loss, e.g., \u201cPatient's guardian reports that it is a daily struggle to ensure that the patient consumes their meals.", "I have no appetite at all anymore.": "and 19 (\u201cI have lost more than fifteen pounds.", "change": "Uncontrolled weight changes, e.g., \"One of the patient's concerns is weight change.", "Poor appetite or overeating?": ".", "energy": "Persistent fatigue or tiredness without a clear cause, e.g., \"Patient's guardian reports that the patient is always complaining about feeling tired.", "I am too tired to do anything.": ".", "Self-loathing": "Negative self-image or self-worth issues, e.g., \"The patient mentioned feeling worthless from time to time.", "I hate myself.\") and 14 (\u201cI believe that I look ugly.\").\n15. Abnormal behavior: Disoriented behavior or unusual speech patterns, e.g., \\\"The patient looked disoriented at times, and their movements were erratic.\" Not directly related to a BDI item, but associated with PHQ-8 (\u201cMoving or speaking so slowly that other people could have noticed? Or being so fidgety or restless that you have been moving around a lot more than usual?": ".", "thoughts": "Suicidal ideation, e.g., \"The patient has been referred to counseling due to suicidal tendencies.", "I would kill myself if I had the chance.": "."}, {"title": "2.3 Natural Language Processing (NLP) Models", "content": "Following previous research [14, 15], we used the FLAN T5 (google/flan-t5-small) [23] model from HuggingFace in its default configuration. We also used the quantized Llama 3-70B (neuralmagic/Meta-Llama-3-70B-Instruct-quantized.w8a16) [24] and Phi 3.5 mini [25] (microsoft/Phi-3.5-mini-instruct) models in their default configuration to extract symptom mentions from text. Considering the constraints of handling private clinical data and limited computational resources, we selected these models for their ability to be deployed locally, minimizing external exposure.\nDue to token limitations in the Llama and Phi models, we restricted the input size to 6,000 characters, resulting in truncation for 18% of the notes. Entire notes were fed one by one into each LLM, and we asked whether each text contained evidence of specific symptoms (binary classification). We describe the prompts we used for each model in Tables 3, 4 and 5 below.\nAccording to best practices in the domain, we used few-shot prompting for Phi and Llama, using three in-context examples. These examples were designed based on insights gained from observing the data. For Flan prompts, we did not use few-shot examples, again adhering to standard practices.\nAs a baseline, we implemented a rigid word-level mapping approach using key parts of the questions. For instance, for the question \"Does it contain evidence that the patient is not going to school?\" we matched using the words \"going\" and \"school.\""}, {"title": "3 Results", "content": "We assess the quality of LLM extracts by calculating F1, precision, and recall at the note level for the positive class. Precision quantifies the proportion of sentences identified as relevant by the model (positive predictions) that are actually relevant. Recall indicates the proportion of relevant sentences in the dataset that were successfully identified by the model. The F1 score is a metric that combines precision and recall into a single measure, representing the harmonic mean of the two, and is used to evaluate a model's balance between false positives and false negatives.\nWe report results in Tables 6, 7 and 8 for the three metrics respectively.\nOur overall observation suggests that all the LLMs outperform the naive word-match baseline (0.55 average F1 over LLMs vs 0.22 average F1 of our baseline). Flan leads in recall with an average of 0.90, significantly higher than Phi (0.60) and Flan (0.55). Llama's high recall can be attributed to its tendency to overgeneralize when depressive symptoms are present, assuming that all symptoms are applicable across various depression-related queries. For example, if a general feeling of sadness is mentioned in the notes, Llama may incorrectly infer that the patient is overeating, due to the correlation between the two symptoms. Also, in one of the notes the mention of anxiety led Llama to infer irritability. Phi ranks as the second-best model for both precision and recall."}, {"title": "Validation of the Utility of LLM Extracts for Depression Screening", "content": "Our Large Language Model (LLM) annotations aim to provide meaningful insights that could potentially aid in clinical screening. However, directly evaluating their utility with real-life doctors is challenging due to the complexity and variability of clinical workflows. As a practical proxy for human judgment, we assess the usefulness of these annotations by using them as features in a range of traditional machine learning (ML) algorithms designed for clinical decision-making. This approach allows us to evaluate whether the annotations meaningfully contribute to distinguishing between case and control patients, which is an indirect measure of their potential impact in real-world clinical settings.\nFor this experiment, we focused on the 15-17-year-old cohort from Table 1. We extracted 3,000 clinical notes recorded within two weeks of completing the PHQ-9 questionnaire for 462 case patients. We matched by age and gender, ensuring they are born within 30 days of the corresponding case patient (462 controls with 3,000 notes). Additionally, controls had no history of a depression diagnosis up to the time of the case patient's diagnosis and had a clinical visit within the same 18-month window. This process resulted in a dataset comprising 924 patients (50/50 cases and controls)\nBuilding on our extraction quality annotation experiment, we used Flan to generate a symptom vector for each patient. This vector consisted of 16 dimensions (one dimension per question). Each dimension indicated the proportion of clinical notes of a patient containing specific symptoms as detected by Flan. These symptom vectors served as features for a range of traditional classifiers, including Random Forest, Logistic Regression, Support Vector Machine, Multi-layer Perceptron, and Decision Tree models. The goal was to determine whether the features derived by symptom detection could be distinctive enough to separate case and control patients.\nAs a baseline method, we employed the Big Bird Transformers model [26] with a 512-character limit and 5-fold cross-validation to classify raw clinical note texts merged per patient without using any feature extraction method. The 5-fold cross-validation has allowed us to produce predictions over the entire dataset without explicitly allocating any data for training.\nillustrates the number of clinical notes associated with each symptom detected by Flan. The most frequently detected symptoms were mental health (MH) concerns (433 notes), followed by feeling down (411 notes), and sleep problems (396 notes). The overrepresentation of certain symptoms, such as sleep-related issues, may result from the mandatory documentation of these topics in clinical notes even in the absense of any particular issues. These results suggest that general mental health issues and mood-related concerns are better documented in clinical notes than we have seen in our initial annotation sample. Conversely, symptoms like high appetite (57 notes), abnormal behavior (88 notes), and no motivation (68 notes) were least frequently detected. The table highlights significant differences in the prevalence of symptoms between cases and controls, with an overall average of 72% of symptoms documented in cases and 28% in controls. Symptoms such as Feeling depressed (86%), Feeling down (83%), Self-loathing (88%) and Suicidal thoughts (85%) are mainly concentrated in case notes confirming mental health issues. Symptoms like Sleep problems (66% Case, 34% Control) and MH concerns (63% Case, 37% Control) show a more balanced distribution due to the documentation practices mentioned above.\nOverall, we observed very good quality of the extracts provided by Flan. For instance, the system identified \"Not going to school\" as being linked to 25 absences due to chronic illness flare-ups. Similarly, symptoms like low appetite were flagged describing urgent referrals to dietitians and psychologists: \"Patient's guardian is highly concerned with their weight and eating habits. They consulted a dietitian who urgently referred them to a psychologist. The patient denies feeling anorexic or having body image problems\u201d (paraphrased). Lastly, the detection of suicidal thoughts highlights provided some good insights as well: \"Patient's guardian called stating that the patient is having thoughts about self-harm. They believe that the patient is safe with them at the moment, but requires an urgent appointment.\"\nThe observed errors in symptom detection by Flan often align with interpretations of due to figurative language. For example, when a note mentioned that a patient had a \"good appetite\" as part of their recovery from an illness, Flan incorrectly detected overeating."}, {"title": "4 Discussion", "content": "This study explored the potential of using Large Language Models (LLMs) to identify depressive symptoms within pediatric electronic health records (EHRs). By benchmarking the zero-shot performance of three state-of-the-art LLMS-FLAN-T5, Phi, and Llama 3-70B\u2014on extracting PHQ-9-related symptoms from free-form text, we demonstrate the potential of these models to improve the consistency of depression screening in the challenging pediatric context.\nOur key findings showed that LLMs, particularly FLAN-T5 achieved high precision (up to 80% across 16 symptom categories) in identifying depressive symptoms. These results suggest that LLMs can support automated mental health screening by providing accurate symptom extraction, which can be useful for automated mental health screening.\nThis study builds on prior research by applying large language models (LLMs) to annotate depressive symptoms directly from clinical notes [18-21].\nThis study supports the utility of zero-shot LLMs like FLAN-T5 for extracting symptom-related evidence [14, 15], a method previously underexplored in paediatric mental health contexts.\nThe efficacy and safety of such tools in real-world clinical settings remain to be thoroughly investigated [27]. This study highlights the advantage of utilizing real-world data to assess these tools, particularly when applied to depression. While majority of studies suggest that LLMs cannot replace human therapists [28, 29], they can serve as supplementary tools. These tools can integrate human clinical insights to improve therapeutic processes.\nHowever, challenges remain, including issues related to data bias and the necessity of human oversight. These challenges emphasize the need for careful integration of LLMs into existing healthcare frameworks [30, 31]. It is crucial to augment rather than replace traditional diagnostic and treatment practices to ensure the effective and safe use of these tools in clinical settings, especially for treating depression. And we have proposed an LLM-based approach to detect PHQ-9 symptoms from unstructured data, such as doctor observations and patient replies, to support traditional practices."}, {"title": "Strengths and Limitations", "content": "The strengths of this work lie in its novel application of LLMs for text-based evidence extraction in a domain where annotated data is particularly sparse, and texts are very heterogeneous (mental health clinical notes). We also use the LLM extracts as features to train an automated screening system as a proxy to human screening, which outperforms a strong baseline not using our feature extracts.\nBy leveraging the generalization capabilities of LLMs in a zero-shot setting, this approach avoids the need for costly and time-consuming manual annotation of training datasets. Furthermore, the ethical framework employed ensures that the models are used solely for evidence extraction, maintaining clinician agency and control over the diagnostic process. Also, the best-precision model FLAN-T5, being a relatively small and computationally efficient model, is well-suited for deployment in clinical settings where resources are limited.\nThe study has some limitations. First, while the zero-shot framework eliminates the need for annotated training data, it may lead to variations in performance across different clinical settings or note structures. This requires further validation. Second, the study focuses on extracting PHQ-9-relevant information, which, while validated, does not encompass the full spectrum of depressive symptoms or contextual factors. Additionally, our results are constrained to the pediatric age group (6-24 years), and further research is needed to generalize these findings to broader populations or other mental health conditions."}, {"title": "Implications", "content": "The implications of this study are significant for the field of mental health diagnostics, particularly in pediatric care. By addressing the challenges of data scarcity and heterogeneity, LLM-based tools can enhance the efficiency and consistency of depression screening, particularly in resource-limited settings. Moreover, the interpretability of extracted text segments allows for greater transparency and facilitates clinicians' trust in AI-assisted tools. This approach also provides a foundation for future research into using LLMs for feature extraction for downstream AI across mental health conditions."}, {"title": "4.1 Conclusion", "content": "In conclusion, this work highlights the potential of LLMs to serve as ethical, efficient, and scalable tools for evidence extraction in pediatric mental health diagnostics. While challenges remain in ensuring reliability and generalizability, the findings underscore the promise of integrating LLMs into clinical workflows to enhance the detection and management of depression in young patients. Further research should focus on refining these models, addressing their limitations, and validating their performance in real-world clinical settings."}]}