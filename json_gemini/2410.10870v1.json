{"title": "PORTLLM: PERSONALIZING EVOLVING LARGE LANGUAGE MODELS WITH TRAINING-FREE AND PORTABLE MODEL PATCHES", "authors": ["Rana Muhammad Shahroz Khan", "Pingzhi Li", "Sukwon Yun", "Zhenyu Wang", "Shahriar Nirjon", "Chau-Wai Wong", "Tianlong Chen"], "abstract": "As large language models (LLMs) increasingly shape the AI landscape, fine-tuning pretrained models has become more popular than in the pre-LLM era for achieving optimal performance in domain-specific tasks. However, pretrained LLMs such as ChatGPT are periodically evolved (i.e., model parameters are frequently updated), making it challenging for downstream users with limited resources to keep up with fine-tuning the newest LLMs for their domain application. Even though fine-tuning costs have nowadays been reduced thanks to the innovations of parameter-efficient fine-tuning such as LoRA, not all downstream users have adequate computing for frequent personalization. Moreover, access to fine-tuning datasets, particularly in sensitive domains such as healthcare, could be time-restrictive, making it crucial to retain the knowledge encoded in earlier fine-tuned rounds for future adaptation. In this paper, we present PORTLLM, a training-free framework that (i) creates an initial lightweight model update patch to capture domain-specific knowledge, and (ii) allows a subsequent seamless plugging for the continual personalization of evolved LLM at minimal cost. Our extensive experiments cover seven representative datasets, from easier question-answering tasks {BoolQ, SST2} to harder reasoning tasks {WinoGrande, GSM8K}, and models including {Mistral-7B, Llama2, Llama3.1, and Gemma2}, validating the portability of our designed model patches and showcasing the effectiveness of our proposed framework. For instance, PORTLLM achieves comparable performance to LoRA fine-tuning with reductions of up to 12.2\u00d7 in GPU memory usage. Finally, we provide theoretical justifications to understand the portability of our model update patches, which offers new insights into the theoretical dimension of LLMs' personalization.", "sections": [{"title": "INTRODUCTION", "content": "The rise of large pretrained language models has marked a significant shift in natural language processing (NLP), enabling remarkable performance across a wide array of tasks, including natural language understanding and generation. These models, such as GPT-4 (Achiam et al., 2023), have achieved state-of-the-art performance by leveraging vast amounts of pretraining data and learning rich semantic representations that encode broad linguistic and world knowledge, which in turn allows them to generalize very well across domains (Antoniades et al., 2024). Despite their powerful general ability, pretrained LLMs often fall short in specialized domains where a nuanced understanding of technical language or context-specific knowledge is required (Wang et al., 2022a; Bommasani et al., 2021; Qiu et al., 2020). Hence, while pretraining enables LLMs to capture general knowledge, fine-tuning (e.g., personalization) is essential for adapting these models to specific domains. Fine-tuning bridges this gap by adapting pretrained models to specific tasks, enhancing their performance in domains such as healthcare, legal analysis, or scientific research (Min et al., 2021; Wei et al., 2021; Ouyang et al., 2022; Wang et al., 2022b; Liu et al., 2022; Raffel et al., 2020; Chen et al., 2024; Gao et al., 2024b). For instance, fine-tuned models can more effectively recognize domain-specific terminology, reason about complex relationships, and deliver more accurate and contextual appropriate responses.\nMuch effort has been devoted to developing fine-tuning methods. Traditionally, fine-tuning would normally involve updating all the parameters of a model. For example, in the case of Mistral 7B (Jiang et al., 2023), it would involve updating all 7 billion parameters. Typically LLMs have billions of parameters, and so this process poses significant challenges in computational and memory requirements. To alleviate these constraints, many Parameter Efficient Fine-Tuning (PEFT) methods (Houlsby et al., 2019) have been proposed. One popular method is Low-Rank Adaptation (LORA) (Hu et al., 2021), which aims to estimate an updated matrix \\( AW \\) using the product of two low-rank matrices A and B. However, although LoRA lowers the training complexity, it still requires fine-tuning a large number of trainable parameters to reach a satisfactory performance. For example, LORA fine-tuning Llama 2 13B (Touvron et al., 2023) variant would require up to eight A6000 GPUs with 48 GB VRAM each, for a very small batch size, hence imposing a considerable memory and computational burden.\nFurthermore, as cloud-hosted LLMs like ChatGPT (OpenAI, 2022) and Gemini (Team et al., 2023) undergo periodic (bi-annually or shorter) updates with newer data, their improved performance often renders previous versions outdated. For downstream users who have already invested in fine-tuning these models for domain-specific tasks, repeatedly fine-tuning or performing personalization at every new update is highly impractical, as the process is not only computationally expensive but also time-consuming.\nBeyond the computational and logistical hurdles, continual updates present another challenge for the downstream user: the lack of availability of the fine-tuning dataset. In domains such as healthcare or finance, data access is regulated by privacy laws and potentially time-sensitive. For example, fine-tuning medical models on patient data requires strict adherence to ethical and legal guidelines, making it difficult to repeatedly fine-tune with every model update. As a result, repeatedly fine-tuning models on newer LLM releases is not a viable long-term strategy for many users, hindering the downstream users from harnessing the performance gains from the evolving nature of LLMs. In response to these challenges, a nature research question comes:\n(Q) How to leverage the personalized knowledge captured in the first fine-tuned model to update any evolving LLMs?\nTo address this question, we introduce PORTLLM, training-free framework that enables seamless transfer of domain-specific knowledge across evolving models. PORTLLM leverages model patches derived from LoRA, allowing users to port fine-tuned knowledge from one model iteration to another while preserving or even enhancing the performance of a downstream task. We show that our model patches are portable across model updates. If the downstream user fine-tunes a version of the model that has long become obsolete, they can simply add the model patches to the newer model, maintaining or boosting their performance on the downstream task. PORTLLM eliminates the need for costly periodic fine-tuning, offering a scalable solution for maintaining task-specific performance across different model versions. Our contributions can be summarized as follows:\n\u2022 We introduce PORTLLM, a training-free framework designed to transfer knowledge between different versions of an evolving LLM. Given two model versions, PORTLLM leverages task-specific model patches extracted from a pretrained LLM and seamlessly applies them to the updated version of the pretrained LLM. This process allows the updated model to achieve comparable, and in some cases improved, performance on downstream tasks \u2013 without any need for fine-tuning.\nWhy do our model patches work? We address this question through both theoretical analysis and empirical investigation. Our findings reveal that certain terms in the model patch are effectively negligible, enabling us to create a simplified version of the patch that requires no training. Consequently, adding the simplified model patches alone is sufficient to achieve improved performance on the downstream task. Furthermore, we examine the impact of the pretraining dataset on downstream tasks, demonstrating that our framework can harness the benefits of continued pretraining across different model updates, across different pretraining datasets including {OpenOrca, SlimOrca, OpenPlatypus, AlpacaGPT4}."}, {"title": "RELATED WORKS", "content": "Large Language Models (LLMs). LLMs have transformed natural language processing, enabling models to perform complex tasks with remarkable accuracy and generalization. Models like GPT-3 (Brown et al., 2020), BERT (Devlin et al., 2019), and T5 (Raffel et al., 2023) have set benchmarks across a range of NLP tasks, from translation and summarization to question answering and text generation (Vaswani et al., 2023; Zhang et al., 2020; Rajpurkar et al., 2016). More recently, models like Llama (Touvron et al., 2023; Dubey et al., 2024), Mistral (Jiang et al., 2023), and Gemma (Team et al., 2024) have pushed the boundaries further by optimizing both performance and computational efficiency. Llama was particularly notable for achieving state-of-the-art results with smaller model size, emphasizing that scaling down can still maintain performance. Mistral improves upon this by offering more efficient training methods, allowing faster training times without sacrificing model quality. Similarly, Gemma leverages architectural innovations to enhance model training and inference, focusing on reducing resource requirements while maintaining strong performance. Despite these advances, the computational demands for pretraining LLMs remain extremely high. Training large models requires weeks of computation on high-performance hardware, and continuous pre-training\u2013to keep models updated with the latest data-adds further complexity and cost (Brown et al., 2020). Even after pretraining, the process of fine-tuning these models for specific downstream tasks remains resource-intensive, making the deployment of LLMs at scale a significant challenge (Raffel et al., 2023). This cost barrier continues to limit the accessibility and widespread adoption of LLMs across diverse applications. In this paper, we propose a training-free solution that enables the seamless transfer of personalized knowledge across evolving LLMs, reducing the need for costly fine-tuning and enhancing accessibility.\nParameter Efficient Fine-tuning (PEFT). The rapid growth in the size of pretrained LLMs has posed significant challenges for efficiently fine-tuning LLMs to specific downstream tasks. To address this, numerous PEFT methods have been developed, aiming to balance efficiency and accuracy. Early approaches focused on inserting trainable adapters-feed-forward networks placed between the layers of the pretrained model (Houlsby et al., 2019; Lin et al., 2020). Recent advancements have led to more sophisticated adapter-based PEFT methods (Mahabadi et al., 2021; Pfeiffer et al., 2020; Luo et al., 2023). A notable example is LoRA (Hu et al., 2021), which introduces trainable low-rank weight perturbations to the pretrained model, significantly reducing the number of parameters required for fine-tuning. LoRA's key innovation lies in its use of the product of two low-rank matrices to approximate weight changes. Building upon this concept, Q-LoRA (Dettmers et al., 2023) proposes to propagate gradients through LoRA using a 4-bit quantized pretrained model, further enhancing efficiency. Concurrently, prompt-based learning methods have demonstrated effectiveness across various NLP tasks. Methods such as prompt-tuning (Lester et al., 2021) and prefix-tuning (Li & Liang, 2021) incorporate learnable continuous embeddings into the model's hidden states. They condition the frozen model to adapt to specific tasks without modifying the underlying architecture. Despite these advances, fine-tuning each updated LLM with PEFT to equip personalized knowledge remains highly costly, and how PEFT can bridge the gap in personalized settings within this evolving environment in a portable manner is yet to be fully explored. To this end, we develop in this paper the theory behind portable model patches that can be plugged into an updated model to carry over the personalized knowledge from the first fine-tuned model."}, {"title": "METHODOLOGY", "content": "3.1 PRELIMINARIES\nTransformers. Transformer models (Vaswani et al., 2023) is an architecture that has revolutionized NLP and other sequence-based tasks. It consists of two key components: (1) Multi-Head Self-Attention and (2) Feed-Forward Neural Network. The Multi-Head Self-Attention mechanism is the core innovation of transformers. It computes the weighted representation of the input sequence where each token attends to every other token. Suppose we are given an input \\( X \\in \\mathbb{R}^{n \\times d} \\) where n refers to the sequence length and d is the hidden dimension of the transformer model. Then for any given attention head i with a total of H heads, we define the following three matrices: query matrix \\( W_{q_i} \\in \\mathbb{R}^{d \\times d_H} \\), key matrix \\( W_{k_i} \\in \\mathbb{R}^{d \\times d_H} \\) and value matrix \\( W_{v_i} \\in \\mathbb{R}^{d \\times d_H} \\), where \\( d_H = d/H \\). Given these, we can compute the self-attention for the ith head as follows\n\\begin{equation}\nh_i = \\text{Softmax}\\left( \\frac{X W_{q_i} (X W_{k_i})^T}{\\sqrt{d_H}} \\right) X W_{v_i}, \\quad i = 1, ..., H.\n\\end{equation}\nThe outputs of the H attention heads are then concatenated as follows:\n\\begin{equation}\n\\text{Multi-head Self-Attention}(X) = \\text{Concatenate}(h_1, h_2, ..., h_H)W_o,\n\\end{equation}\nwhere \\( W_o \\in \\mathbb{R}^{d \\times d} \\) is a projection matrix that combines the outputs of the different heads back into the model's hidden dimension d. After the Muli-head Self-Attention, the output is passed through a position-wise feed-forward network that consists of two linear transformations and a nonlinear activation function (like ReLU or GELU). Given the weight matrix of the first linear layer \\( W_{\\text{up}} \\in \\mathbb{R}^{d \\times d_m} \\), the weight matrix of the second layer \\( W_{\\text{down}} \\in \\mathbb{R}^{d_m \\times d} \\), bias terms \\( b_1 \\in \\mathbb{R}^{d_m} \\), \\( b_2 \\in \\mathbb{R}^{d} \\), and a non-linear activation function \\( \\sigma(\\cdot) \\) where dm is the hidden dimension, Feed-Forward Network is applied independently to each position in the sequence as follows\n\\begin{equation}\n\\text{Feed-Forward Network}(X) = \\sigma(X W_{\\text{up}} + b_1)W_{\\text{down}} + b_2 .\n\\end{equation}\nFurthermore, these two layers are wrapped with residual connections and layer normalization."}, {"title": "Low-Rank Adaptation (LoRA)", "content": "Consider a transformer model where \\( W_o \\in \\mathbb{R}^{d \\times d} \\) is the pretrained weight matrix, which could be a weight matrix for any layer in the transformer. In a typical fine-tuning setup, the weights Wo are updated during training to adapt the model to a specific task. This update requires storing and computing the entire matrix Wo during training, which becomes computationally expensive for large models. Instead of updating the full weight matrix Wo, LORA (Hu et al., 2021) assumes that the weight update \\( \\Delta W \\in \\mathbb{R}^{d \\times d} \\), essentially the difference between pretrained weights Wo and the hypothetical fine-tuned weight, can be approximated by a low-rank decomposition:\n\\begin{equation}\n\\Delta W = B A,\n\\end{equation}\nwhere \\( B \\in \\mathbb{R}^{d \\times r} \\) and \\( A \\in \\mathbb{R}^{r \\times d} \\) are trainable matrices, while r is the rank of the decomposition with \\( r < d \\). In this setup, the full weight update matrix \\( \\Delta W \\) is replaced by the product of two smaller matrices, B and A, drastically reducing the number of trainable parameters from d\u00b2 to 2rd. Hence, the fine-tuned model weights we can be trained as follows:\n\\begin{equation}\nW_{\\text{new}} = W_o + \\Delta W = W_o + B A.\n\\end{equation}\nBy doing this, LoRA reduces the number of parameters that need to be trained while still allowing the model to adapt to new tasks."}, {"title": "PROPOSED TRAINING-FREE FRAMEWORK: PORTLLM", "content": "3.2\nNotations and Assumptions. We refer to the pretrained LLM as the first version of the model, denoted by \u03b8, and the updated continued pretrained model as \u03b8', as illustrated in Figure 2. We also assume that to get to \u03b8', the provider does continued pretraining using LoRA, where \\( \\Delta \\theta \\) denotes this adapter, however empirical experiments in Section 4.4 show that even if the provider does full weight continued pretraining on the newer dataset, our method still holds. We denote this full weight updated model as \\( \\Phi \\). Similarly, for any downstream user i, we have a corresponding dataset denoted di. The base model fine-tuned on this dataset di is denoted by \\( \\theta_i \\). We can also rewrite \\( \\theta_i \\) in terms of its LoRA update as \\( \\theta_i = \\theta + \\Delta \\theta_i \\). Consequently, if we fine-tune updated model d' for ith downstream task, we have \\( \\theta_i' = \\theta' + \\Delta \\theta_i' \\). We further assume that a personalization adaptor \\( \\Delta \\theta_i \\) or \\( \\Delta \\theta_i' \\) has a rank that is much smaller than that of a continued pretrained adaptor \\( \\Delta \\theta \\) or \\( \\Delta \\theta' \\).\nProposed Method of PORTLLM. PORTLLM aims to approximate the fine-tuned updated model \\( \\theta_i' \\) by applying the older personalization adaptor/model patch \\( \\Delta \\theta_i \\) to the continued pretrained model \u03b8'. It will be shown in Section 3.3 that the older model patch \\( \\Delta \\theta_i \\) may be used in lieu of the newer model patch \\( \\Delta \\theta_i' \\), namely,\n\\begin{equation}\n\\theta_i' = \\theta' + \\Delta \\theta_i' \\approx \\theta' + \\Delta \\theta_i.\n\\end{equation}\nIn other words, one can readily add the extra knowledge \\( \\Delta \\theta_i \\) from the previous fine-tuning process to the newest LLM \u03b8'. Throughout our experimental section, we perform experiments with this approximated patching process."}, {"title": "ANALYSIS OF OUR PROPOSED PORTABILITY", "content": "3.3\nTheoretical Justification. The fine-tuned updated model \\( \\theta_i' \\) can be decomposed into a naive update term and a residual matrix C as follows:\n\\begin{equation}\n\\begin{aligned}\n\\theta_i' &= \\theta' + \\Delta \\theta_i' \\\\\n&= (\\theta' + \\Delta \\theta_i) + (\\Delta \\theta_i' - \\Delta \\theta_i) .\n\\end{aligned}\n\\end{equation}\nNaive Update\nResidual Matrix C\nLemma: We claim that the residual matrix is negligible compared to the naive update. Proof: First, recall the assumption in Section 3.2 that \\( \\Delta \\theta \\) and \\( \\Delta \\theta_i \\) have ranks that are much smaller than the rank of \\( \\Delta \\theta \\). Second, \\( \\theta' = \\theta + \\Delta \\theta \\) ensures that rank(0') \u2265 rank(0). We therefore conclude that the rank of the residual matrix is approximately much smaller than that of the naive update matrix."}, {"title": "EXPERIMENTS", "content": "Datasets and Architecture. We evaluate our framework on a diverse set of datasets to demonstrate PORTLLM universality and effectiveness across various downstream tasks and domains. Specifically, we leverage datasets from the GLUE (Wang, 2018), SuperGLUE (Wang et al., 2019), Wino-Grande (Sakaguchi et al., 2021) and GSM8K (Cobbe et al., 2021) benchmarks commonly used for such evaluation in literature. For Question Answering tasks, we utilize BoolQ (from SuperGLUE) and SST-2 (from GLUE); for Similarity and Paraphrase Tasks, the MRPC (from GLUE) dataset; for Inference Tasks, the RTE and WNLI (both from GLUE) datasets; and lastly for Reasoning Tasks, we employ WinoGrande and GSM8K. This broad spectrum of tasks enables a comprehensive evaluation of our model's performance across diverse downstream applications. For continued pretraining datasets, we use the following: OpenOrca (Lian et al., 2023a), SlimOrca (Lian et al., 2023b), Open-Platypus (Lee et al., 2023) and AlpacaGPT4 (Peng et al., 2023). Additionally, we conduct extensive experiments across multiple model architectures to demonstrate the generalizablity of our framework. Specifically, we test our method on Mistral-7B (Jiang et al., 2023), Llama2-7B (Touvron et al., 2023), Llama3.1-8B (Dubey et al., 2024), and Gemma2-9B (Team et al., 2024), showcasing the robustness and adaptability of our approach across different LLMs.\nTraining Details. To simulate the progression of time, we employ continued pretraining, where we transition from 0 to \u03b8' by taking a pretrained model and further pretraining it on a specific dataset using LORA (Hu et al., 2021). In all continued pretraining scenarios, we maintain a constant rank r = 64 and a = 128 with a learning rate of 0.0001 and 4 epochs. For downstream tasks, we also use LoRA, but in this case, we set the rank r = 8 consistently to ensure a fair comparison across tasks. Furthermore, for all LoRA applications we optimize all the attention layers (Key, Value, Query, Projection), and all Feed Forward Network layers (Up Projection, Down Projection and Gate Projection, where applicable). For each downstream fine-tuning, we use a constant learning rate of 0.0004 while the number of epochs for each dataset {BoolQ, SST-2, MRPC, RTE, WinoGrande, WNLI, GSM8K} is as follows (5,5,5,5, 3, 5, 1}. Lastly, for fine-tuning on specific downstream dataset mentioned, we solely use the train split, and evaluate on test split.\nEvaluation Metrics. We use the Language Model Evaluation Harness (Gao et al., 2024a) by EleutherAI to assess the performance of all trained and fine-tuned models across the datasets in our experiments. All evaluations are conducted in a zero-shot setting rather than a few-shot setting. For datasets {BoolQ, SST-2, RTE, WinoGrande, WNLI}, we employ accuracy as the primary evaluation metric. In the case of {MRPC}, we utilize both accuracy and F1 score to provide a more comprehensive evaluation. For {GSM8K}, we had two evaluation options: (1) Flexible Match Accuracy or (2) Exact Match Accuracy. Due to poor zero-shot performance of the models on GSM8K for exact matches, we opted for Flexible Match Accuracy."}, {"title": "SUPERIORITY OF PORTLLM FRAMEWORK", "content": "4.1\nIn this section, we compare the performance of our model patches against several baseline models, including the pretrained LLM 0, the updated model using continued pretraining \u03b8', the fine-tuned model \u03b8i, and the updated fine-tuned model \\( \\theta_i' \\). For consistency, all models are variations of Mistral-7B, while continued pretraining dataset is OpenOrca. Importantly, the performance is evaluated under zero-shot setting across all the datasets."}, {"title": "CONSISTENT RESULTS ACROSS DIFFERENT PRETRAINING DATASETS", "content": "4.2\nIn this section, we investigate the impact of different pretraining datasets on our model patches \u0394\u03b8i and assess whether our method can effectively leverage updates obtained through continued pretraining. We conduct a comparative analysis across four downstream datasets \u2013 {BoolQ, MRPC, WNLI, WinoGrande} \u2013 alongside four distinct continued pretraining datasets: OpenOrca, SlimOrca, OpenPLatypus, and AlpacaGPT4. Consistent with our previous section, all experiments utilize the Mistral-7B model. The results of this analysis are summarized in Table 3."}, {"title": "CONSISTENT RESULTS ACROSS DIFFERENT MODEL ARCHITECTURES", "content": "4.3\nThis section provides a comprehensive analysis of our model patches across various architectures to evaluate their performance. We examine four different model architectures: {Mistral-7B, Llama2-7B, Llama3.1-8B, Gemma2-9B}, assessing their effectiveness on four distinct downstream tasks {BoolQ, MPRC, WNLI, WinoGrande}. Performance is evaluated under four settings: (1) pretrained model, (2) continued pretrained model or Updated Model, (3) fine-tuned model, and (4) our model patches ported to the updated model. The OpenOrca dataset is used for our continued pretraining in this analysis. The results are summarized in Table 4"}, {"title": "PORTLLM ALSO WORKS WITH FULL WEIGHT CONTINUED PRETRAINING", "content": "4.4\nFor our theoretical analysis, we initially assumed that continued pretraining was conducted using LORA. However, we aim to investigate whether our method is effective across model evolution when the updates occur through full weight continued pretraining. Such a model is denoted \\( \\phi \\). To evaluate this, we utilize Mistral-7B, which has undergone full weight continued pretraining on the OpenOrca dataset, and incorporate our model patches for various downstream tasks. We then compare the performance of these patched models against the zero-shot performance of \\( \\phi \\) to assess the improvements attributable to our model patches. The results across various downstream tasks are summarized in Table 5.\nWe find that our model patches can be effectively applied to a continued pretrained model utilizing full weight updates, rather than relying solely on LoRA. Across all evaluated datasets \u2013 {BoolQ, SST-2, MRPC, RTE, WinoGrande, WNLI, GSM8K} \u2013 we observe significant performance improvements of {3.27%, 1.72%, 10.04%, 15.52%, 12.31%, 16.90%, 20.69%}, respectively, compared to the zero-shot performance of the Updated Model."}, {"title": "COMPUTING EFFICIENCY COMPARISON OF PORTLLM", "content": "4.5\nThis section evaluates the performance of our method from an efficiency perspective. We employ the following metrics for comparison: (1) Number of Trainable Parameters, (2) GPU Memory Utilization, and (3) GPU Hours. We analyze the merging of our model patches in relation to model fine-tuning using LoRA to achieve comparable performance. For LoRA fine-tuning calculations, we have the following settings: Downstream task SST-2 for Mistral-7B, with local Batch Size of 4 and 5 Epochs. The results are summarized in Table 6.\nCompared to downstream fine-tuning using LoRA, our methods offers plug-and-play solution with no trainable parameters, resulting in a reduction of nearly 20 million parameters that need to be trained. This training-free paradigm not only conserves resources but also saves up to 12.2\u00d7 GPU Memory, reducing the requirement from 350 GB for LoRA to just 28.7 GB during the merge operation of model patches. Additionally, the merge operation can be executed in mere seconds, in start contrast to the hours required for fine-tuning. This opens many other doors for applications of our model patches. PORTLLM demonstrates the potential for on-device, training-free models for various down-stream tasks without the need for fine-tuning. Furthermore, it reduces the need for expensive cloud infrastructure, especially in large-scale fine-tuning."}, {"title": "CONCLUSION", "content": "5\nIn this paper, we propose PORTLLM, a framework aimed at addressing the challenges faced by downstream users of pretrained LLMs when adapting to frequent model evolutions over time. By leveraging lightweight model patches, PORTLLM offers a training-free, cost-effective solution to seamlessly transfer domain-specific knowledge between different iterations of LLMs. This enables users to maintain, and sometimes even enhance, their models' performance on specialized tasks without the need for repeated fine-tuning or extensive computational resources. Through extensive empirical evaluations across a set of tasks and models, we demonstrate that our method not only preserves performance but can also leverage the continual updates in pretraining LLMs, offering substantial gains in-task specific performance. Moreover, we provide theoretical insights into the portability of these model patches, highlighting the underlying factors that make them effective across evolving model versions. Looking forward, PORTLLM paves the way for more robust and adaptable solutions in the evolving landscape of LLM personalization, offering another avenue for training-free adaptation. Furthermore, future endeavours will aim at developing such methods that work across different model architecture, including using techniques from model merging."}]}