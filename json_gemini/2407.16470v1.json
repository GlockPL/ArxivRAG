{"title": "Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models", "authors": ["Kenza Benkirane", "Laura Gongas", "Shahar Pelles", "Naomi Fuchs", "Joshua Darmon", "Pontus Stenetorp", "David Ifeoluwa Adelani", "Eduardo Sanchez"], "abstract": "Recent advancements in massively multilingual machine translation systems have significantly enhanced translation accuracy; however, even the best performing systems still generate hallucinations, severely impacting user trust. Detecting hallucinations in Machine Translation (MT) remains a critical challenge, particularly since existing methods excel with High-Resource Languages (HRLs) but exhibit substantial limitations when applied to Low-Resource Languages (LRLs). This paper evaluates hallucination detection approaches using Large Language Models (LLMs) and semantic similarity within massively multilingual embeddings. Our study spans 16 language directions, covering HRLs, LRLs, with diverse scripts. We find that the choice of model is essential for performance. On average, for HRLs, Llama3-70B outperforms the previous state of the art by as much as 0.16 MCC (Matthews Correlation Coefficient). However, for LRLs we observe that Claude Sonnet outperforms other LLMS on average by 0.03 MCC. The key takeaway from our study is that LLMs can achieve performance comparable or even better than previously proposed models, despite not being explicitly trained for any machine translation task. However, their advantage is less significant for LRLS.", "sections": [{"title": "1 Introduction", "content": "Text generation models have drastically improved in recent years especially with the capabilities of LLMs in producing realistic and fluent output. However, hallucination continues to undermine user trust, as it generates and propagates misinformation and sometimes nonsensical outputs (Agarwal et al., 2018; Xu et al., 2023a; Guerreiro et al., 2023b).\nOne practical way of reducing hallucination in MT is by building more robust models, especially for LRL which tend to exhibit significantly higher hallucination rates. There are several efforts on scaling MT models to LRLs, such as M2M-100 (Fan et al., 2020), NLLB-200 (Team et al., 2022), MADLAD-400 (Kudugunta et al., 2023) etc.\nDespite initiatives to minimize hallucinations during the MT process, issues still persists. Therefore, detecting hallucinations post-translation remains a critical alternative approach to ensure the reliability and trustworthiness of the translated content.\nPrevious work on post-translation evaluation has mainly focused on a single English-centric (EN) to a HRL direction, while studies including LRL remain limited (Raunak et al., 2022; Xu et al., 2023b). Recently, Dale et al. (2023) introduced HalOmi- a benchmark dataset for detecting hallucination in MT that includes EN HRLs (ten directions) and ENLRLs (six directions), as well as two non-English directions HRL\u2192LRL, including different scripts. BLASER-QE (Communication et al., 2023), the state-of-the-art (SOTA) hallucination detector, is reported as the top performer on the HalOmi benchmark. It calculates a translation quality score by evaluating the similarity between encoded source texts and machine-translated texts within the SONAR embedding space (Duquenne et al., 2023).\nIn this paper, we evaluate the performance of LLMs and embedding based methods as hallucination detectors, aiming to enhance performance in both HRLs and LRLs. To this end, we use the HalOmi benchmark dataset with a binary hallucination detection approach. For our evaluation, we include 14 methods: eight LLMs with different prompt variations, and four embedding spaces by computing the cosine similarity between source and translated texts.\nWe find that LLMs are highly effective for hallucination detection across both high and low resource languages, although the optimal model selection depends on specific contexts. For HRLs, on average across directions, the Llama3-70B model significantly surpasses the previous SOTA method, BLASER-QE, by 16 points. Moreover, embedding-based methods have also demonstrated superior performance over the current SOTA in high resource contexts. However, for LRLs, Claude Sonnet is the best performing model, improving previous methods by a smaller difference. More precisely, LLMs outperformed BLASER-QE in five out of eight LRL translation directions, including the non-English-centric ones.\nFinally, our research makes the following primary contributions: First, we evaluate a wide range of LLMs for MT hallucination detection and establish that LLMs, despite not being explicitly trained for the task, are competitive and greatly outperform even the previous SOTA for HRLs. Second, large multilingual embedding spaces improve upon previously proposed methods and show that they remain competitive for HRLs, but struggle for LRLs. Third, we establish a new SOTA for 13 of the 16 languages that we evaluate on, including high and low resource languages. Surpassing the previous SOTA, which was explicitly trained for the task, on average by 2 MCC points."}, {"title": "2 Experimental setup", "content": "We evaluated our methods on the Halomi dataset. A first dataset filtration involved selecting only natural translations, without perturbations, as findings from perturbed data may not be applicable to the detection of natural hallucinations (Dale et al., 2023). The validation and test split was decided based on the translation direction. For the validation set, we selected the two translation directions DE\u2192EN, which encompasses 301 sentences. This choice was made as extensive resources and established benchmarks are available for this language pair (Guerreiro et al., 2023a), with the expectation that the models would exhibit generalizability to less frequently used language pairs. For the test set, the other 16 pairs were used: more precisely, it includes four pairs with English and a HRL (EN\u2194AR, EN ZH, EN\u2194RU, EN\u2194ES), three pairs with English and a LRL (EN\u2194KS, EN\u2194MN, and EN\u2194YO), and one non-English HRL-LRL pair (ES\u2194YO). The test set includes 2,558 sentence pairs. This test set excludes six sentence pairs that were removed due to sensitive content flagged and filtered out by LLMs. A more detailed description of the dataset is available in Appendix B."}, {"title": "2.2 Hallucination detection setting", "content": "We consider two settings: (1) Severity ranking introduced by the authors of HalOmi. (2) Binary detection-a new setting we added due to data imbalance and ease of evaluation.\nSeverity ranking the classification of hallucinations was based on four severity levels: No Hallucination, Small Hallucination, Partial Hallucination, and Full Hallucination. This fine-grained categorization aimed to capture the nuances in the extent and impact of hallucinations on the translated output. We use this setting only as ablation study in Appendix C., both for consistency with the HalOmi benchmark, but also to assess the relevance of our binary detection approach.\nBinary detection In this setting, all three instances of hallucinations were labelled as Hallucination, regardless of their severity. We also change the way the evaluation was done in HalOmi, with an appropriate prompt (Appendix D), and threshold calculation for binary classification for embeddings cosine similarity, see subsection 2.4. The primary reason for choosing this setting is the significant class imbalance in HalOmi, largely due to the scarcity of hallucinations across different severity levels. Some translation directions have particularly imbalanced data, for example EN\u2192RU, with the following distribution: out of 148 sentence pairs, we have 141 No Hallucination (96.6%), 1 Small (0.68%), 2 Partial (1.4%), and 4 Full (2.8%). High class imbalance can affect the ability of model to perform well (Prusa et al., 2016; Sordo and Zeng, 2005; Fern\u00e1ndez et al., 2013)."}, {"title": "2.3 LLMs for hallucination detection", "content": "We assessed the performances of eight LLMs, mixing capabilities models across LLMs families. We evaluate OpenAI's GPT4-turbo and GPT40; Cohere's Command R and Command R+; Mistral's Mistral-8x22b; Anthropic's Claude Sonnet and Claude Opus and Meta's Llama3-70B.\u00b9 More details about the selection are in subsection E.2.\nFirst, we built our prompt design by differentiated system and user prompts for better results (Kong et al., 2024). The system prompt contained the task description, and optionally, the inclusion of Chain-of-Thought (CoT), while the user prompt contained, for each sentence pair, the source text and MT text, as well as a direct hallucination classification question.\nWe derived the task description prompts from the Evaluate Hallucination and Evaluate Coherence in the Summarization Task prompts in G-Eval (Liu et al., 2023). The CoT prompts were inspired by Evaluation Steps from G-Eval, and by the human annotation guidelines and severity level definitions from HalOmi. All prompts are available Appendix D. More details about the chosen hyperparameters with LLMs can be found in Appendix E.\nWe determined the optimal prompts for each model using the DEEN validation set, evaluating three prompts and two CoT proposals for binary detection. The best prompt for each model was selected based on the average MCC across both translation directions. The MCC was chosen as the primary metric for binary detection due to its superiority in providing a single, easily interpretable value between -1 and +1. This value encapsulates the model's performance for the confusion matrix scores, making it more robust to class imbalance."}, {"title": "2.4 Embeddings", "content": "We assessed the performance of three LLM-related embedding spaces: OpenAI's text-embedding-3-large, Cohere's Embed v3, and Mistral's mistral-embed. Additionally, we included SONAR, the multilingual embedding space used as the base for BLASER-QE. Specifically, we calculated the cosine distance between embeddings of the source text and the machine-translated text. This approach draws on previous studies showing that hallucinated translations tend to have embeddings that are significantly distanced from those of the source text (Dale et al., 2022).\nWe binarised the cosine similarity scores of embeddings using an optimal threshold value determined from the validation set. This threshold, established by maximizing the F1-score from the precision-recall curve, was then applied to the test set for binary hallucination detection across all language pairs. Each embedding space was independently processed to maintain the integrity of the evaluation."}, {"title": "3 Results", "content": "LLMs are the new SOTA for hallucination detection The results in Figure 2 demonstrate that LLMs have the best overall performance across languages for binary hallucination detection. Specifically, Llama3-70B surpasses the previous best performing model, BLASER-QE, by +5 points, with an MCC of 0.43. For HRLs, 10 out of 12 evaluated methods outperform BLASER-QE (0.46), with Llama3-70B greatly improving over the baseline by 16 points (0.63). Notably, the results show that the choice of LLM should rely on the resource level; as for LRLs, Claude Sonnet achieves the highest average MCC. However, GPT40 was the more robust LLM across all languages, with the lowest standard deviation. Finally, for 13 out of the 16 evaluated translation directions, the evaluated methods outperform BLASER-QE, with the exception of KS\u2192EN, YO\u2192EN and EN\u2192MNI. Our findings on LLMs' superior hallucination detection capabilities align with prior research on their effectiveness in MT quality assessment (Kocmi and Federmann, 2023).\nEmbedding-based hallucination detectors remain competitive for HRLs For HRLs, simple embedding-based methods display competitive capabilities, outperforming more sophisticated models in five out of eight translation directions. For instance, although BLASER-QE is a more advanced model based on SONAR, SONAR exhibits comparable or superior performances in most HRLs directions. This suggests that the effectiveness of these methods may be highly sensitive on their training data, and hence to the resource level, as we observe SOTA performances for HRLs and suboptimal results for LRLs. Additionally, the embeddings' performance may be highly dependent on the threshold chosen using the EN DE validation set, generalizing well for HRLs but not for LRLs.\nLLMs' contrastive performances across LRLS First, while Llama3-70B obtains the best performance overall, it was outperformed in most translation directions, especially in LRL. This result reveals a HRLs-centric approach of the model but also concludes that there is not one-LLM fits all resource levels. Secondly, for LRLs, models such as Sonnet, Opus, GPT40, and Mistral \u2014in order of decreasing performances, achieve higher scores, supporting the feasibility of employing LLMs in settings encompassing a wide range of languages. These results should be contrasted with a wide difference of hallucination distribution across resource levels, for example with the MN\u2192EN direction which only has 28% No hallucination sentence pairs. More precisely, Sonnet and BLASER-QE perform on par for LRL, with the particularity that BLASER-QE has a significantly higher rate of false negatives, while Sonnet maintains a more balanced ratio of false positives to negatives. Moreover, BLASER-QE performs well in translations from English and comparably to Sonnet in translations to English, but falls short in non-English-centric translations, which follows the same trends as previously reported models in (Dale et al., 2023). Figure 14 provides a more detailed view of these performance metrics.\nEmbeddings are high performers for non-Latin scripts, while LLMs can generalise to non-English centric translations For HRLs\u2192EN directions with source scripts different than Latin (AR, RU, ZH), embeddings are the best performers, suggesting high capabilities with cross-script transfer learning. These observations align with the findings of Hada et al. (2023), who report decreased performance for non-Latin scripts in LLM-based evaluators. In the two non-English centric translation directions (ES\u2194YO), Opus outperforms by far both BLASER-QE (0.11) and the best embedding Mistral (0.12), with a score of 0.28. Unlike the overall LRLs trends, Opus outperforms Sonnet for this direction pair: this can suggest that the advanced analytical capabilities of LLMs can generate improved results even in scenarios with limited relevant training data. Remarkably, in the YO ES translation direction, six out of our fourteen methods and BLASER-QE exhibit scores close to random guessing (within the [\u22121, +1] range). This observation underscores the pressing need for enhanced capabilities in detecting hallucinations in non-English-centric translation settings. Figure 1 presents two examples that highlight the challenges faced by LLMs when dealing with non-Latin scripts, with the exception of Llama3-70B. Additionally, it illustrates how embeddings may struggle with reasoning capabilities in non-English centric contexts."}, {"title": "4 Conclusion", "content": "In this work, we demonstrates that LLMs and embedding semantic similarity are highly effective for hallucination detection in machine translation, with LLMs establishing a new state-of-the-art performance across both high and low-resource languages. Our findings suggest that the optimal model selection depends on specific contexts, such as resource level, script, and translation direction. Our study highlight the need for further research to enhance hallucination detection capabilities, particularly in low-resource and non-English-centric translation settings."}, {"title": "Limitations", "content": "Despite the promising results obtained by LLMs and embedding-based methods in our evaluation, there are certain limitations that should be noted.\nFirst, the dataset shows distribution imbalance across translation directions, with different trends for high and low resource languages, even after binarisation (see Appendix B): The HRLs show a pronounced data imbalance towards No Hallucination labels, with distribution between 79% and 94%. Moreover, for LRLs, there's a broader interval, from 28% to 85%. This imbalance often results in models that classify translations as No hallucination being more frequently correct for HRLs than for LRLs, thereby introducing a bias into the binary evaluation. Moreover, the translation direction display a qualitative bias, as shown subsection B.3: HRLS and LRLs don't have the same selection distribution which display a potential bias towards hallucination. Future dataset improvements should prioritize larger, more diverse samples, non-Latin scripts, and non-English centric translations. Using consistent source text across languages and balancing hallucination severity levels would enable more sophisticated methods, improve generalizability, and allow for a fair evaluation of models' hallucination detection capabilities.\nThe validation set used to identify the optimal threshold for non-LLM methods and the best prompt for LLMs only included EN DE translations. To improve parameter optimization and generalization across various translation directions, especially for low-resource languages (LRLs), cross-validation is recommended for future research, as suggested by Dale et al. (2023) and initially planned for our study. However, financial constraints associated with benchmarking non-open source models prevented the implementation of this approach. Future work should focus on developing novel approaches that perform well on well-studied high-resource languages (HRLs) while generalizing effectively to LRLs, assessing robustness, or exploring alternative methods to address this challenge within the limitations of dataset size.\nFinally, for benchmarking purposes, only the previous state-of-the-art (SOTA) was included for comparison against the newly evaluated methods. Therefore, for a more comprehensive analysis, it is recommended to include additional methods previously evaluated by HalOmi."}, {"title": "A Related Work", "content": "Significant advancements have been made in automatic machine translation evaluation, but these have predominantly focused on general translation errors. As a result, hallucinations are often overlooked, and evaluation scores may not reflect their impact due to their relatively low frequency compared to less severe errors like omissions (Guerreiro et al., 2023a).\nPrevious studies have demonstrated that sentence similarity measures between source and translated texts, using cross-lingual embeddings such as LASER (Heffernan et al., 2022) and LaBSE (Feng et al., 2022), can effectively identify severe hallucinations (Dale et al., 2022). However, the recently introduced Halomi dataset, A Manually Annotated Benchmark for Multilingual Hallucination and Omission Detection in Machine Translation (Dale et al., 2023), which expands to include LRLs and non-English-centric translation directions, reveals the limitations of embedding semantic similarity methods primarily with LRLs. Conversely, the BLASER model (Communication et al., 2023)-utilizing the SONAR embedding space (Duquenne et al., 2023)\u2014demonstrates greater robustness across language resources, establishing it as the latest state-of-the-art. This model notably improves performance in LRLs compared to previous methods, yet it still shows deficiencies in some non-English-centric directions.\nRecent works have underlined the capabilities of LLMs in multilingual MT evaluation, demonstrating strong performances across various languages, although discrepancies are noted in LRLs (Zhu et al., 2023; Xu et al., 2023b). G-Eval (Liu et al., 2023) introduces a robust prompting framework for hallucination detection and demonstrates that LLMs can be used as automatic metrics to generate a single quality score. Furthermore, Kocmi and Federmann (2023) showed that LLMs, when appropriately prompted, can assess the quality of machine-generated translations, achieving state-of-the-art performance in system-level quality evaluation. Moreover, Fernandes et al. (2023) pioneered the evaluation of LLMs for MT tasks in LRLs using a new prompting technique, although its focus is primarily on broader translation errors rather than specifically on hallucination detection."}, {"title": "B Dataset description", "content": "The languages acronyms follow this mapping throughout the paper: Arabic (AR), Chinese (ZH), English (EN), German (DE), Kashmiri (KA), Manipuri (MN), Russian (RU), Spanish (ES), and Yoruba (YO)."}, {"title": "B.2 Hallucination distribution", "content": "B.2.1 Distribution of Hallucination in the severity ranking framework"}, {"title": "C Ablation study", "content": "The ablation study focus on hallucination severity ranking. We present results for comparability with Dale et al. (2023), which assesses the methods' abilities to accurately rank hallucinations by severity (e.g.,full hallucinations ranked higher than partial ones, and any hallucinations ranked above non-hallucinations). The employed metric is an adaptation of the ROC AUC for multiclass tasks, which calculates the percentage of incorrectly ranked sentence pairs with different labels and subtracts this value from the perfect score of 1. We compute these metrics separately for each translation direction to assess the detector's performance across different language pairs."}, {"title": "C.1 LLMs for severity ranking", "content": "C.1.1 Prompt design\nWe designed tailored prompts for this approach, just as for our main binary approach, this time to generate multiclass predictions. For severity ranking, each prompt has a different assigned CoT."}, {"title": "D Prompts", "content": "We used two types of CoTs: One based on the human guidelines for hallucination detection, and the other based on the severity level definition, that was readapted to each case. For binary detection, two CoTs were tested for three prompts."}, {"title": "ELLMs experiments", "content": "E.1 LLMs hyperparameters\nFor the evaluation of LLMs, we used LangChain to ensure reproducibility of results, except for Llama3-70B that was ran locally. We set the TEMPERATURE to 0 for minimum randomness and the MAX_OUTPUT_TOKEN to 15 to avoid verbose.All the experiments were zero-shot, with an exhaustive label (for example, ['Hallucination', 'No Hallucination'] for binary detection). These choices showed the highest performances in previous research (Kocmi and Federmann, 2023) (Wei et al., 2022)."}, {"title": "F Binary detection results", "content": "Table 6 provides MCC scores per LLM for each of the prompts and CoT variations evaluated on the validation set. The most robust LLMs across prompt variations in the validation set, specifically Sonnet, GPT40, and Llama3-70B, exhibit superior performance across language resource settings in the test set. This suggests that extensive prompt engineering might not be required for these models in the current task, as the performance using the optimal prompt from the validation set aligns with high performance on the test set."}, {"title": "F.2 Test results", "content": "Figure 14 displays the performances of evaluated methods on the test set grouped by translation directions and resource setting. The results indicate that the highest scores for HRLs are achieved in translations to English, whereas for LRLs, the highest scores are from translations originating in English or Spanish. Additionally, these findings underscore that no single model uniformly excels across all translation directions."}]}