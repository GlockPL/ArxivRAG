{"title": "Kolmogorov-Arnold Networks (KAN) for Time Series Classification and Robust Analysis", "authors": ["Chang Dong", "Liangwei Zheng", "Weitong Chen"], "abstract": "Kolmogorov-Arnold Networks (KAN) has recently attracted significant attention as a promising alternative to traditional Multi-Layer Perceptrons (MLP). Despite their theoretical appeal, KAN require validation on large-scale benchmark datasets. Time series data, which has become increasingly prevalent in recent years, especially univariate time series are naturally suited for validating KAN. Therefore, we conducted a fair comparison among KAN, MLP, and mixed structures. The results indicate that KAN can achieve performance comparable to, or even slightly better than, MLP across 128 time series datasets. We also performed an ablation study on KAN, revealing that the output is primarily determined by the base component instead of b-spline function. Furthermore, we assessed the robustness of these models and found that KAN and the hybrid structure MLP_KAN exhibit significant robustness advantages, attributed to their lower Lipschitz constants. This suggests that KAN and KAN layers hold strong potential to be robust models or to improve the adversarial robustness of other models.", "sections": [{"title": "1 Introduction", "content": "In recent years, time-series analysis has become increasingly prevalent in various fields, including healthcare[5], human activity recognition [15], remote sensing[16] etc. Among these, time series classification (TSC) is one of the key challenges in time series analysis. Due to rapid advancements in machine learning research, many TSC algorithms have been proposed. Before the prevalence of deep learning, numerous effective algorithms existed. For instance, NN-DTW [1] is based on measuring the general similarity of whole sequences using different distance metrics. Other methods included identifying repeated shapelets in sequences as features [6], discriminating time series by the frequency of repetition of some sub-series [17], and using ensemble methods [12]. However, these approaches often faced limitations, as they were either difficult to generalize to various scenarios or had high time complexity."}, {"title": "2 Background", "content": "KAN is inspired by Kolmogorov-Arnold representation theory (KAT). It states that any multivariate continuous function defined in a bounded domain can be represented as a finite composition of continuous functions of a single variable and the binary operation of addition. Specifically, if f is a continuous function on a bounded domain $D \\subset R^n$, then there exist continuous functions $\\phi_{ij}$ and $\\psi_i$ such that:\n\n$\\displaystyle f(x_1, x_2,...,x_n) = \\sum_{i=1}^{2n+1} \\psi_i(\\sum_{j=1}^{n} \\phi_{ij}(x_j)) $,\n\nwhere $\\phi_{ij}: [0,1] \\rightarrow R$ and $\\psi_i : R \\rightarrow R$. It transforms the task of learning a multivariable function into learning a finite number of univariable functions. Compared to MLP, it explicitly provides the number of one-dimensional functions needed for fitting. However, these univariable functions could be non-smooth or even fractal, making it theoretically feasible but practically useless. Nevertheless, Liu et al. [13] found that, by analogy to MLP in neural networks, KAN need not be limited to two layers and finite width to fit all non-linearities. Furthermore, most natural functions tend to be smooth and have sparse structures. These insights suggest that an scalable KAN could become a strong competitor to MLP."}, {"title": "2.2 Adversarial Attack", "content": "Adversarial attacks involve applying carefully crafted small perturbations $r \\in R^d$ to input data $x \\in R^d$, leading to significant changes in a model's output, such as fooling a classifier $f: x \\rightarrow R^m$ with the goal of altering the predicted label.\n\n$\\displaystyle \\begin{aligned}  &\\underset{x}{\\operatorname{argmax}} \\{f(x)\\} \\neq \\underset{x}{\\operatorname{argmax}} \\{f(x_{adv})\\}, \\\\  &x_{adv} = x + r, s.t. ||r||_2 < ||x||_2.  \\end{aligned}$\n\nhere, the perturbation r is small in magnitude relative to x as indicated by their norms. We normally apply these perturbations to test whether the model can be fooled, thus assessing its robustness. \u03a4o consider the worst-case scenario, we typically implement the gradient attacks which require knowledge of all the information about the model and the data. Among them, the most widely used"}, {"title": "2.3 Local Lipschitz Constant", "content": "A function $f: R^m \\rightarrow R^n$ is defined to be $l_f$-locally Lipschitz continuous at radius r if for each $i = 1, ..., n$, and $\\forall ||x_1 - x_2|| \\leq r$, the following holds:\n\n$||f(x_1) - f(x_2)|| \\leq l_f||x_1 - x_2||\n\nwhere $l_f$ is the local Lipschitz constant. Hereafter, we will refer to it simply as the Lipschitz constant. The Lipschitz constant is directly linked to perturbation stability, which in turn relates to robustness[18]."}, {"title": "3 Methodology", "content": "Assume there is a data distribution $D \\subseteq R^d \\times R^m$. Our objective is to learn a function $f: x \\in R^d \\rightarrow y \\in R^m$ such that the following risk is minimized as $R(f) =  \\frac{1}{N} \\sum_{i=1}^{N} ||y_i - f(x_i)||$. The purpose of the KAN is to learn such a representation of f, thereby minimizing the objective loss. The original KAN used a two-layer structure, while Liu, et al. [13] extended to arbitrary width and depth. In contrast to MLP, the activation function are placed on edges instead of the neurons, KAN use 3rd-order B-spline (k = 3) functions for fitting, which allows learning sophisticated activation function by controlling the weight of each basis. In this case, the neuron q in the layer 1 + 1 can be represented as:\n\n$\\displaystyle x_{l+1,q}^{spline} = \\sum_{p=1}^{n} w_{l,q,p}^{spline}.\\Phi_{l,q,p}(x_{l,p}) =  \\sum_{p=1}^{n} w_{l,q,p}^{spline}.\\sum_{i=1}^{k+G} W_{i} B_{i} (x_{l,p})$,\n\nwhere $x_{l,p}$ is the input from an arbitrary neuron p in the previous layer l. The input from all n neurons in the previous layer l undergoes a nonlinear transformation produced by a learnable B-spline combination, where G is the grid size which determines the number of B-spline bases (k + G). This is followed by a weighted summation to obtain the $q^{th}$ output of $x_{l+1,q}^{spline}$. Additionally, KAN"}, {"title": "3.1 Kolmogorov-Arnold Networks (KAN)", "content": "introduce a base function similar to residual connections, using weighted silu, to stabilize optimization, which can be represented as:\n\n$\\displaystyle x_{l+1,q}^{base} = \\sum_{p=1}^{n} w_{l,q,p}^{base}. silu(x_{l,p}) =  \\sum_{p=1}^{n} w_{p,q}^{abase} \\frac{x_{l,p}}{1 + e^{-x_{l,p}}}$.\n\nTherefore, the output of the $q^{th}$ neuron in layer 1 + 1 can be represented as:\n\n$x_{l+1,q} = x_{l+1,q}^{spline} + x_{l+1,q}^{base}$\n\nFor a multi-layer KAN, the final output can be represented as a nested structure of layers:\n\n$f(x) = f(x_1,x_2, ..., x_n) = \\Psi_L \\circ \\Psi_{L-1}.\u2026\u2026 \\circ \\Psi_1 \\circ x$\n\nwhere $\\Psi_l$ denotes the $l^{th}$ layer, which includes the combination of the above two operations: a spline transformation and a base activation silu. Fig. 1 illustrates a three-layer KAN structure with the architecture [3-5-3-1], clearly depicting how KAN operate."}, {"title": "3.2 \u039a\u0391N for time series classification", "content": "We constructed classifiers using KAN, similar to the structure shown in fig. 1. Due to the setting of the B-spline fitting interval being [-1,1], the data dis-tribution outside this interval will not achieve an effective fitting. Instead of directly adopting the method proposed by Liu et al., which suggested updating"}, {"title": "4 Experiment Settings", "content": "Dataset: We applied the UCR2018 datasets [2] to evaluate these models. The UCR Time Series Archive encompasses 128 datasets, which are all univariate. These datasets span a diverse range of real-world domains, including healthcare, human activity recognition, remote sensing and more. Each dataset comprises a distinct number of samples, all of which have been pre-partitioned into training and testing sets. Reflecting the intricacies of real-world data, the archive includes datasets with missing values, imbalances, and those with limited training sam-ples.\n\nEvaluation Metrics: We used the accuracy and the F1 score to assess the performance of all models in tab. 1. During adversarial attacks, we evaluate the robustness of the models using the Attack Success Rate (ASR).\n\nExperiment setup: Our experiments were executed on a server equipped with Nvidia RTX 4090 GPUs, 64 GB RAM, and an AMD EPYC 7320 processor."}, {"title": "5 Result", "content": "Fig. 2 (a) and (b) show the accuracy and F1 distribution across 128 UCR datasets for the five models respectively. We observe that these five models achieve rel-atively similar performance across the 128 datasets, both in terms of F1 score and accuracy. However, KAN performs slightly better overall. This conclusion is also supported by the results shown from the critical diagrams in Fig. 3, where only KAN and MLP_L in the same parameters exhibit statistically significant differences. In the critical diagram, KAN ranks the highest, indicating its strong fitting capability and demonstrating that it can achieve performance compara-ble to, or even better than, traditional neural networks on the benchmark time series datasets."}, {"title": "5.1 Performance Comparison", "content": ""}, {"title": "5.2 Ablation Study of KAN", "content": "KAN have a more complex structure compared to MLP, due to the combinations of base and spline functions. The different grid sizes of spline functions have varying impacts on performance. To evaluate their influences, we investigated three configurations of KAN as follows:\n\n1. Complete KAN with different grid sizes: 1, 5, 50\n2. KAN with only the wpline component, with different grid sizes: 1, 5, 50\n3. KAN with only the base component"}, {"title": "5.3 Evaluation and Analysis of Adversarial Robustness", "content": "We also found that KAN demonstrate better robustness compared to MLP. We performed PGD untargeted attacks on the aforementioned five models, with e ranging from 0.05 to 0.5. The results consistently show that KAN significantly"}, {"title": "6 Conclusion", "content": "In this paper, we applied the KAN in Time Series Classification and conducted a fair comparison among KAN, MLP, and mixed structures. We found that KAN can achieve comparable performance to MLP. Additionally, we analyzed the importance of each component of KAN and discovered that a larger grid size can be difficult to optimize, leading to lower performance. Furthermore, we evaluated the adversarial robustness of KAN and these models, finding that KAN exhibited remarkable robustness. This robustness is attributed to KAN's lower Lipschitz constant. Moreover, we found that KAN with a larger grid size have a greater Lipschitz constant but exhibit stronger robustness. We provided an explanation for this phenomenon, although it requires further verification and broader experiments in our future work."}]}