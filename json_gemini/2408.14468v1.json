{"title": "K-Sort Arena: Efficient and Reliable Benchmarking for Generative Models via K-wise Human Preferences", "authors": ["Zhikai Li", "Xuewen Liu", "Dongrong Fu", "Jianquan Li", "Qingyi Gu", "Kurt Keutzer", "Zhen Dong"], "abstract": "The rapid advancement of visual generative models necessitates efficient and reliable evaluation methods. Arena platform, which gathers user votes on model comparisons, can rank models with human preferences. However, traditional Arena methods, while established, require an excessive number of comparisons for ranking to converge and are vulnerable to preference noise in voting, suggesting the need for better approaches tailored to contemporary evaluation challenges. In this paper, we introduce K-Sort Arena, an efficient and reliable platform based on a key insight: images and videos possess higher perceptual intuitiveness than texts, enabling rapid evaluation of multiple samples simultaneously. Consequently, K-Sort Arena employs K-wise comparisons, allowing K models to engage in free-for-all competitions, which yield much richer information than pairwise comparisons. To enhance the robustness of the system, we leverage probabilistic modeling and Bayesian updating techniques. We propose an exploration-exploitation-based matchmaking strategy to facilitate more informative comparisons. In our experiments, K-Sort Arena exhibits 16.3\u00d7 faster convergence compared to the widely used ELO algorithm. To further validate the superiority and obtain a comprehensive leaderboard, we collect human feedback via crowdsourced evaluations of numerous cutting-edge text-to-image and text-to-video models. Thanks to its high efficiency, K-Sort Arena can continuously incorporate emerging models and update the leaderboard with minimal votes. Our project has undergone several months of internal testing and is now available at K-Sort Arena.", "sections": [{"title": "1. Introduction", "content": "Visual generation models have made significant advancements, excelling in tasks such as text-to-image [5, 40, 43, 53] and text-to-video [11, 13, 16, 54] generation. Such great progress has attracted more and more researchers, leading to a continuous proliferation of new models. Therefore, an efficient and reliable evaluation of the models' capabilities is urgently desired. However, traditional evaluation metrics, such as IS [44], FID [19], FVD [48], etc., fall short in providing a fair and comprehensive evaluation, especially not reflecting human preferences in the real world.\nTo this end, Chatbot Arena [10] is proposed as a platform developed for evaluating large language models (LLMs). It constructs randomized, anonymous pairwise comparisons of models and collects user judgments of their outputs, thereby forming an overall ranking of models' capabilities. Despite the great progress, Chatbot Arena still faces challenges regarding efficiency and accuracy: (i) The inefficiency of Chatbot Arena stems primarily from two inherent mechanisms: pairwise comparisons and randomized matching. By allowing only two models to be compared at a time and potentially matching models of vastly different ranks, the system often yields minimal information per comparison. This inefficiency necessitates an excessive number of comparisons to achieve a stable ranking, resulting in a significant waste of valuable human effort in voting. More importantly, as a massive number of new models continueuously emerge, this inefficiency prevents the rapid evaluation of new models' capabilities and the timely updating of the leaderboard, causing a lagged response to the latest advances. (ii) In user voting, preference noise and subjective bias are inherent, leading to occasional unjustified ratings. Pairwise comparisons are sensitive to this issue, which could introduce bias into the relative rankings. This is especially problematic when the leaderboard is updated frequently and the number of votes is small.\nTo address the above issues, we propose K-Sort Arena, a novel benchmarking platform for visual generation models. K-Sort Arena offers better efficiency and reliability. Specifically, K-Sort Arena employs K-wise comparisons (K>2), allowing K models to participate in free-for-all battles, which provides greater information benefits than pairwise comparisons, as shown in Figure 1. This approach is based on a practical biological principle: images and videos have higher perceptual intuitiveness compared to texts, enabling rapid evaluation of multiple samples at once. To ensure the robustness of ranking, we introduce probabilistic modeling of the models' capabilities, as well as a Bayesian score updating strategy applied after free-for-all battles among K models, which can dilute the adverse effects of preference noise. Furthermore, we propose an exploration-exploitation-based model matching strategy, which facilitates matchmaking among models with comparable strength while also incorporating under-explored models, thereby maximizing the expected benefit of each comparison. The overview is presented in Figure 2.\nTo demonstrate the superiority of K-Sort Arena, in Section 4, we design experiments to simulate the scenarios of model comparisons and user voting. Encouragingly, K-Sort Arena shows 16.3\u00d7 faster ranking convergence than the ELO system in Chatbot Arena and exhibits greater robustness to preference noise. On one hand, it can update the leaderboard accurately and quickly with a minimum number of votes, which can effectively cope with model proliferation; on the other hand, it is more stable and reliable in long-term evaluations, obtaining more trustworthy evaluations with the same number of votes.\nK-Sort Arena has served to evaluate dozens of state-of-the-art visual generation models, including both text-to-image and text-to-video models. By statistically organizing user feedback from crowdsourced questions, K-Sort Arena effectively builds comprehensive model leaderboards. To aptly reflect the diverse real-world applications, users are free to choose prompts sampled from open-source datasets or to create fresh prompts. Moreover, K-Sort Arena supports multiple voting modes and user interactions. Users can either select the best output from a free-for-all comparison or rank the K outputs instead. This flexibility ensures a faster, more user-friendly, and versatile evaluation process. Overall, our contributions can be summarized as follows:\n\u2022 We introduce K-Sort Arena, an efficient and reliable platform for evaluating visual generation models. It can continuously monitor new models and quickly update the leaderboard with minimal votes.\n\u2022 We propose K-wise comparisons to obtain richer feedback information and save human efforts in evaluation.\n\u2022 We devise an exploration-exploitation-based matchmaking strategy with probabilistic capability modeling and Bayesian updating mechanisms.\n\u2022 Ablation study shows that compared to traditional ELO algorithms, K-Sort Arena can achieve 16.3\u00d7 faster convergence and greater robustness against preference noise."}, {"title": "2. Related Work", "content": "To address the limitations of static metrics, DynaBench [24] suggests implementing a live benchmark system that integrates a human-in-the-loop approach, thus allowing for more dynamic and adaptive evaluation. Building on this idea, Chatbot Arena [10] is developed as a platform specifically for LLMs. It constructs model arenas that allow LLMs to make randomized, anonymous pairwise comparisons. Users are required to judge and score the outputs of two models to continuously calibrate the capability scores of each model, resulting in an overall ranking of model capabilities. It also inspires WildVision's efforts to rank vision-language models [36]. However, these Arena algorithms require excessive comparisons to achieve a stable ranking and are susceptible to preference noise in voting. As our concurrent work, GenAI Arena [23] replicates the above workflow to visual generative models and thus has the same issues. Consequently, the coverage of the leaderboard is limited to a few models. In contrast, K-Sort Arena capitalizes on the intuitive advantage of visual information over texts, incorporating more robust modeling methods and more effective matchmaking strategies, which shows great potential in large-scale model evaluations."}, {"title": "3. Methodology", "content": "In this section, we describe how to perform robust probabilistic modeling and Bayesian updating of model capabilities in free-for-all comparisons of K models, and how to schedule matches to accelerate ranking convergence."}, {"title": "3.1. K-wise Comparison", "content": "The pairwise comparison employed by Chatbot Arena evaluates only two models per round and is inefficient. In contrast, K-Sort Arena evaluates K models (K>2) simultaneously, which naturally provides more information and thus improves the efficiency of the overall ranking. In coordination with K-wise comparisons, the modeling and updating of model capabilities are detailed below.\nIndividual numerical modeling, as in the ELO system [12], provides only a certain value of the estimate and thus cannot ensure reliability. Instead, by using probability distributions to represent capabilities, it is possible to capture and quantify the inherent uncertainty and hence become more flexible and adaptive. This idea can be seen in popular ranking systems such as Glicko [14] and TrueSkill [17]. Our approach, while inspired by them, incorporates further improvements to enhance efficiency and reliability. Formally, we represent the capability \u03b8 of each model as a normal distribution:\n$\\theta_i \\sim \\mathcal{N}(\\mu_i, \\sigma_i^2)$   (1)\nwhere $ \\mu_i $ and $ \\sigma_i $ denote the i-th model's expected score and uncertainty, respectively. Here, i = 1, 2,\u2026\u2026\u2026, N, and N are the total number of models. As previously mentioned, user voting inevitably has preference noise, which is orthogonal to the uncertainty $ \\sigma $ of the model's performance. Therefore, we introduce an additional stochastic variable $ \\beta $ over the model's capability $ \\theta $ such that the model's actual performance judged by human evaluation is:\n$X_i \\sim \\mathcal{N}(\\theta_i, \\beta^2)$  (2)\nTo build a leaderboard, we use the conservative score [39] to estimate the model's capability, as defined below:\n$S_i^{(n)} = \\mu_i^{(n)} - \\eta \\cdot \\sigma_i^{(n)}$  (3)\nwhere $ \\eta $ is a coefficient with a typical value of 3.0, $ S_i^{(n)} $, $ \\mu_i^{(n)} $ and $ \\sigma_i^{(n)} $ are the values after n comparisons and updates. For each update of $ \\mu $ and $ \\sigma $, j = 1,2,\u2026, N, we follow Eq. 9 and Eq. 10 specified as follows.\nBased on probabilistic modeling, we implement the updating process using Bayesian inference with observed match results. We begin by discussing the case of two models and then generalize to the free-for-all comparison of K models. Assuming in the current comparison there are two models $ M_1 $ and $ M_2 $, the likelihood estimate of observation D that $ M_1 $ wins $ M_2 $ is:\n$P(D|\\theta_1, \\theta_2) = P(X_1 > X_2) = \\Phi \\left(\\frac{\\theta_1 - \\theta_2}{\\sqrt{\\beta^2 + \\beta^2}}\\right)$ (4)\nHere, \u03a6(x) is the cumulative distribution function of standard normal distribution, i.e., $ \\Phi(x) = \\int_{-\\infty}^x \\phi(u)du $, and $ \\phi(x) $ is the probability density function of standard normal distribution, i.e., $ \\phi(x) = \\frac{e^{-x^2/2}}{\\sqrt{2\\pi}} $. Then, based on Bayes' theorem, we can derive the joint posterior density of $ (\\theta_1, \\theta_2) $ given observation D as follows:\n$P(\\theta_1,\\theta_2|D) \\propto P(\\theta_1)P(\\theta_2)P(D|\\theta_1,\\theta_2)$  \n$= \\mathcal{N}(\\frac{\\theta_1 - \\mu_1}{\\sigma_1}) \\mathcal{N}(\\frac{\\theta_2 - \\mu_2}{\\sigma_2}) \\Phi(\\frac{\\theta_1 - \\theta_2}{\\sqrt{\\beta_1^2 + \\beta_2^2}})$ (5)\nThe marginal posterior density of $ \\theta_1 $ can be subsequently obtained by the following equation:\n$P(\\theta_1|D) = \\int_{-\\infty}^{\\infty} P(\\theta_1,\\theta_2|D) d\\theta_2$  \n$\\propto  \\int_{-\\infty}^{\\infty} \\mathcal{N}(\\frac{\\theta_1 - \\mu_1}{\\sigma_1}) \\mathcal{N}(\\frac{\\theta_2 - \\mu_2}{\\sigma_2}) \\Phi(\\frac{\\theta_1 - \\theta_2}{\\sqrt{\\beta_1^2 + \\beta_2^2}}) d\\theta_2$ (6)\nWith the marginal posterior density, the posterior mean of $ \\theta_1 $ can be calculated as follows:\n$\\mu_1 = E[\\theta_1|D] = \\frac{\\int_{-\\infty}^{\\infty} \\theta_1 P(\\theta_1|D) d\\theta_1}{\\int_{-\\infty}^{\\infty} P(\\theta_1|D) d\\theta_1}$  \n$= \\mu_1 + \\frac{\\sigma_1^2}{\\sqrt{\\Sigma(\\beta_i^2 + \\sigma_i^2)}} V(\\frac{\\mu_1 - \\mu_2}{\\sqrt{\\Sigma(\\beta_i^2 + \\sigma_i^2)}})$  \n$= \\mu_1 + \\frac{\\sigma_1}{\\mathcal{C}_{12}} V(\\frac{\\mu_1 - \\mu_2}{\\mathcal{C}_{12}})$ (7)\nwhere $ V(x) = \\phi(x)/\\Phi(x) $ and $ \\mathcal{C}_{12} = \\Sigma(\\beta_i^2 + \\sigma_i^2) $. Here, $ \\mu_1 $ is the updated mean $ \\mu_1 $ value. Similarly, the updating process of the variance of is given by the following equation:\n$\\sigma_1^2 = Var[\\theta_1|D] = E[\\theta_1^2|D] - (E[\\theta_1|D])^2$  \n$= \\sigma_1^2(1 - \\frac{\\sigma_1^2}{\\Sigma(\\beta_i^2 + \\sigma_i^2)} W(\\frac{\\mu_1 - \\mu_2}{\\sqrt{\\Sigma(\\beta_i^2 + \\sigma_i^2)}}))$  \n$= \\sigma_1^2(1 - \\frac{\\sigma_1^2}{\\mathcal{C}_{12}} W(\\frac{\\mu_1 - \\mu_2}{\\mathcal{C}_{12}}))$ (8)\nwhere $ W(x) = V(x)(V(x) + x) $.\nThe above procedure accomplishes Bayesian updating of the two models after comparing them, and as the number of comparisons increases, $ \\mu $ gets closer to the true value and $ \\sigma $ tightens up, resulting in a high-confidence capacity estimate [4]. We generalize it to a free-for-all comparison of K models, and the capacity updating formulas for the i-th model are as follows:\n$\\mu_i = \\mu_i + \\sigma_i^2 \\cdot \\Sigma_{q: r_i > r_q} \\frac{1}{\\mathcal{C}_{iq}} V(\\frac{\\mu_i - \\mu_q}{\\mathcal{C}_{iq}})$   \n$+ \\Sigma_{q: r_i < r_q} \\frac{1}{\\mathcal{C}_{iq}} V(\\frac{\\mu_i - \\mu_q}{\\mathcal{C}_{iq}})$  (9)\n$\\sigma_i^2 = \\sigma_i^2(1 -  \\Sigma_{q: r_i > r_q} \\frac{1}{\\mathcal{C}_{iq}} W(\\frac{\\mu_i - \\mu_q}{\\mathcal{C}_{iq}})$   \n$+ \\Sigma_{q: r_i < r_q} \\frac{1}{\\mathcal{C}_{iq}} W(\\frac{\\mu_i - \\mu_q}{\\mathcal{C}_{iq}})$ (10)\nThanks to the probabilistic modeling and Bayesian updating employed in K-wise comparison, the model's capabilities can be represented with high robustness, thereby facilitating stable and accurate ranking. Additionally, it is important to note that K-wise comparison offers an inherent advantage in terms of efficiency. Generally speaking, a K-wise comparison can be viewed as $ C_K^2 = \\frac{K(K-1)}{2} $ pairwise comparisons. Assuming that each pairwise comparison provides a certain ranking benefit, and this benefit is additive, we can claim that the total number of comparisons required is significantly less than that for pairwise comparisons."}, {"title": "3.2. Exploration-Exploitation Based Matchmaking", "content": "Effective model matchmaking significantly impacts the efficiency of ranking convergence. Here we first examine matchmaking methods used in notable ranking systems. For instance, the ELO system [12], employed by traditional Arena, uses completely random matching. This can result in pairing the lowest-ranked player with the highest-ranked one, even after numerous comparisons when rankings are nearly stable. Such matchups provide minimal new information, often leading to inefficient use of evaluation resources and slower ranking convergence. To address the above issue, TrueSkill system [17] focuses on matching players whose strengths are as equal as possible. However, it is only effective for assessing the ability of individual players, because each player's opponents are limited to a small, localized group of candidates. This limitation means that it lacks a comprehensive understanding of the overall pool of players, making it less useful for the overall ranking of a large number of players.\nTo this end, we propose an exploration-exploitation-based matchmaking strategy, which promotes valuable comparisons and thus achieves efficient model ranking with fewer comparisons. Specifically, we model the selection of players as a multi-armed bandit problem, where each pair of players is viewed as an arm. The objective is to maximize the overall benefit after n comparisons, i.e., to provide the most information for the overall ranking after n comparisons. Notably, our approach emphasizes maximizing global gains, offering a broader perspective compared to TrueSkill, which focuses on short-term benefits from an individual player's viewpoint. To solve the multi-armed bandit problem, we apply the Upper Confidence Bound (UCB) algorithm. The UCB algorithm performs exploration with the most optimistic attitude given the current exploitation, which is formulated as follows:\n$U^{(n)}(X_i, X_q) = |S_i^{(n)} - S_q^{(n)}| + \\alpha \\cdot \\sqrt{\\frac{ln n}{N_{iq}}}$ (11)\nwhere $ |S_i^{(n)} - S_q^{(n)}| $ indicates the absolute difference in scores between the i-th model and q-th model after n comparisons, $ n_{iq} $ denotes the number of comparisons that have been made between the two models, and $ \\alpha $ is a balancing coefficient with a typical value of 1.0. Eq. 11 realizes the trade-off between exploration and exploitation, where the first part is exploitation and the second part is exploration. In exploitation phase, we prioritize selecting players of similar skill levels to create valuable comparisons, while in the exploration phase, we encourage players who have not been sufficiently evaluated to participate in matches to ensure a comprehensive assessment. We theoretically prove its advantage over random selection.\nConsequently, for a pre-specified player $ X_i $, designated as the pivot, we can achieve grouping by greedily selecting its K-1 opponents based on their Upper Confidence Bound (UCB) scores, as follows::\n$\\left\\{ X_{x_1^*},..., X_{x_{K-1}^*}\\right\\} = \\bigcup_{k=1}^{K-1} \\left\\{ arg \\mathop{max}_{X_q \\in \\mathcal{X}_k} U(X_i, X_q)\\right\\}  $ (12)\nwhere $ \\mathcal{X}_k = \\mathcal{X}_{k-1} - \\left\\{ X_{x_{k-1}^*}\\right\\} $, \n$\\mathcal{X}_0 = \\left\\{ X_q \\right\\}_{q=1}^{N}, X_i \\in \\mathcal{X}$,(13)\nThe above procedure accomplishes the effective selection of its opponents after specifying the pivot player $ X_i $. In our algorithm, instead of random selection, we specify $ X_i $ under the guidance of equalizing the number of comparisons to promote balanced participation in comparisons by each player, which is formulated as follows:\n$X_i = arg \\mathop{min}_{X_i \\in \\mathcal{X}_0}  \\Sigma_{q=1, q\\neq i}^{N} N_{iq}$  (14)\nIn the following, we present the advantages of the proposed specification policy of the pivot player $ X_i $ in a scenario-by-scenario manner.\n\u2022 Scenario 1: Ranking many models from scratch. In each round of comparisons, we select the model with the fewest comparisons as the pivot. This promotes balanced participation across all models, preventing insufficient or excessive evaluation of certain models. Such equalization from a global perspective is also an important factor in promoting rapid convergence of the overall ranking.\n\u2022 Scenario 2: Adding new models to an existing ranking. Our algorithm facilitates new models to participate in comparisons as pivots frequently in the early rounds so that they can quickly catch up with the number of comparisons of old models. Hence, with an effective matchmaking strategy, we can efficiently evaluate new models' capabilities, allowing us to showcase the latest progress in the leaderboard in real time."}, {"title": "4. Experiments", "content": "In this section, we design experiments that simulate user voting and different ranking scenarios, to verify the validity of each proposed component in K-Sort Arena .\nIn Sections 4.2, 4.3, and 4.4, we conduct experiments to rank 50 models from scratch. In Section 4.5, we perform experiments by adding a new model to an existing ranking of 50 models. To simulate user voting on model comparisons, we assign a preset out-of-order label to each model to indicate its ground-truth ability. The result of a specific comparison depends on the performance of models, which are determined by their ground-truth abilities and the preference noise. Note that this preset label is used solely for evaluating the comparison results and is not involved in any other part of the ranking process, such as model capability modeling and updating. Finally, we calculate the Mean Squared Error (MSE) of the ranked positions against the preset labels to evaluate the convergence speed and accuracy."}, {"title": "4.1. K-Sort Arena vs. ELO-based Arena", "content": "Table 1 shows the number of comparisons required for ELO system and K-Sort Arena to reach convergence, i.e., MSE becomes consistently zero. Encouragingly, with the advanced modeling method and matchmaking strategy, K-Sort Arena is 16.3 times more efficient than ELO system, dramatically reducing the number of user votes required. Below we will verify the advantages of each component."}, {"title": "4.2. Probabilistic vs. Numerical Modeling", "content": "We begin by verifying the advantages of probabilistic modeling over numerical modeling employed in ELO systems, as shown in Figure 3a. Since the ELO system is designed for pairwise comparisons, we fix K in K-Sort Arena to 2 in this experiment for fairness. Remarkably, numerical modeling exhibits violent oscillation and fails to converge even after 3000 comparisons. This outcome highlights the unreliability of the existing Arena platform, despite the large number of votes that have been collected. On the contrary, our probabilistic modeling provides rapid convergence after about 1500 comparisons.\nFigure 3b illustrates the case of voting with preference noise. In Figure 3a, comparison results are directly determined by the preset labels, whereas in Figure 3b we introduce a 5% chance of inconsistency between the comparison results and the labels. As observed, numerical modeling still fails to converge, while probabilistic modeling, despite converging slightly slower due to noise effects, manages to converge after approximately 2000 comparisons. This fully demonstrates the high robustness of probabilistic modeling, offering a strong assurance of the reliability of evaluations."}, {"title": "4.3. K-wise vs. Pairwise Comparison", "content": "Next, we verify the effect of different K values (K\u2208[2,4,6]) on ranking convergence. All three sets of experiments adopt UCB matchmaking strategy, and the experimental results are shown in Figure 4. When K is increased to 4, multiple models engage in free-for-all comparisons in each round, which yields richer information than the case of K=2, resulting in faster convergence (approximately twice as fast). For K=6, while MSE decreases more rapidly in the early stages, small fluctuations occur in the later stages before final convergence, resulting in less pronounced efficiency gains. Therefore, K=4 is considered as a trade-off choice."}, {"title": "4.4. UCB vs. Traditional Matchmaking", "content": "In this section, we demonstrate the advantages of the proposed UCB matchmaking strategy. The comparison methods include random matchmaking in the ELO system and skill-based matchmaking in the TrueSkill system. The experimental results are presented in Figure 5. Since random matching can potentially result in low-information comparisons, such as pairing the highest-ranked player with the lowest-ranked one, it continues to oscillate after 3,000 comparisons. The goal of skill-based matchmaking is that the skills of players in the comparison are as equal as possible. This may promote interesting matches for an individual player, but it ignores exploration and thus fails to ensure convergence and stability of the overall ranking from a global perspective. Fortunately, our UCB matchmaking strategy addresses this issue by balancing exploitation and exploration, achieving ranking convergence with minimal comparisons."}, {"title": "4.5. Specified vs. Random Pivot", "content": "Here, we focus on the case of adding a new model to an existing ranking and verify the effectiveness of the proposed pivot specification method. We initialize the new model's ranking at 51 and set its actual label to 31. The experimental results are presented in Figure 6. When both the pivot and its opponents are selected randomly, the new model are less likely to be selected. When the pivot is chosen randomly and UCB is used for matching opponents, the efficiency improves. This improvement is due to the exploration term in Eq. 11, the new model's small niq increases its probability of being selected as an opponent. Furthermore, when employing our balance-guided specification method, since the new model naturally participates in the minimal number of comparisons, it is always selected as the pivot in the initial period. Notably, only roughly 30 comparisons are needed to determine the new model's ranking, which provides a prerequisite for rapid leaderboard updating."}, {"title": "5. K-Sort Arena Platform", "content": "In this section, we build an open and live evaluation platform with human-computer interactions in Huggingface Space, which integrates the proposed algorithms to improve efficiency and reliability. On this platform, users can input a prompt and receive outputs from K anonymous generative models. Users then cast a ranked vote for these models based on their preferred responses, and these votes are saved for updating the leaderboard. K-Sort Arena platform has the following highlights:\n\u2022 Open-source platform: K-Sort Arena platform is open-source, open-access, and non-profit, fostering collaboration and sharing in the community.\n\u2022 Extensive model coverage: It covers a comprehensive range of models, including numerous open-source and closed-source models across various types and versions.\n\u2022 Real-time update: It continuously adds new models, completes its evaluation with minimal votes, and updates the leaderboard in real-time.\n\u2022 Robust evaluation: Bayesian modeling and anonymous comparisons reduce preference noise and model prejudice, making the leaderboard reliable and authoritative.\n\u2022 User-friendly interaction: It supports various prompt input modes, voting modes, and user interaction styles, offering users a high degree of flexibility."}, {"title": "5.1. Covered Tasks and Models", "content": "K-Sort Arena is dedicated to evaluating visual generation tasks with human preferences, with a particular focus on text-to-image and text-to-video tasks. To ensure a comprehensive and thorough evaluation, we strive to cover as many mainstream models as possible, including both open-source and closed-source models, as well as multiple versions of a single model, if available. Currently, K-Sort Arena has served to evaluate dozens of state-of-the-art models. A detailed list of models is presented in Appendix C."}, {"title": "5.2. Platform Construction", "content": "K-Sort Arena platform is designed using Gradio and hosted in Huggface Space. Model inference is performed on ZeroGPU Cloud or Replicate API calls.\nInterface Overview The interface features two main functionalities: leaderboard display and user voting for model battles. When participating in voting, after the user enters the prompt, the interface can display 4 generated images or videos from the anonymous models, i.e., K = 4 is taken as default. The interface layout is illustrated in Appendix E.\nTo aptly reflect diverse real-world applications, K-Sort Arena supports two prompt input modes.\n\u2022 Ready-made prompts: Users have the option to randomly extract pre-designed prompts from our extensive data pool for input into the models. This feature eliminates the need for users to spend time creating their own prompts, thereby significantly improving the efficiency of their interactions. At present, the data pool contains 5000 representative prompts, which are sampled from popular datasets such as MS COCO [32] and WebVid [3].\n\u2022 Custom prompts: Users are also free to create fresh input prompts, allowing them to tailor and customize the generated content to meet their specific needs.\nK-Sort Arena supports two voting modes for K-wise free-for-all comparisons, called Best Mode and Rank Mode. In Best Mode, users compare the outputs of K models and vote for the most preferred answer. For users who are unsure, a tie option is also available. In Rank Mode, users can rank the outputs of K models, providing a more fine-grained comparison (tie is also available).\n\u2022 Best Mode: In this mode, the user only needs to select the best model, making one K-wise comparison theoretically equivalent to K \u2013 1 pairwise comparisons. Since it requires only one mouse click, as in pairwise comparisons, it is K-1 times more efficient.\n\u2022 Rank Mode: In this mode, the user provides feedback by ranking the K models. One K-wise comparison is theoretically equivalent to $ \\frac{K(K-1)}{2} $ pairwise comparisons. Since it requires clicking on the rank of each model, i.e., K clicks, it is $ \\frac{K-1}{2} $ times more efficient."}, {"title": "5.3. Leaderboard Building", "content": "Our project has been undergoing internal testing for several months, during which we have collected over 1,000 high-quality votes. All voters are professors and graduate students in the field of visual generation. To ensure high quality and mitigate preference noise, we organize pre-voting training and provide evaluation guidelines. Specifically, for text-to-image models, the evaluation criteria consist of Alignment (50%) and Aesthetics (50%). Alignment encompasses entity, style, and other matching aspects, while Aesthetics includes photorealism, light and shadow rendering, and the absence of artifacts. Text-to-video models are similarly evaluated based on Alignment (50%) and Aesthetics (50%). Alignment is broken down into video content matching, movement matching, and inter-frame consistency. Aesthetics comprises photorealism, physical correctness, and the absence of artifacts."}, {"title": "6. Conclusion", "content": "In this paper, we introduce K-Sort Arena, a benchmarking platform for visual generation models. K-Sort Arena employs K-wise comparisons (K>2), allowing K models to play free-for-all games, along with probabilistic modeling and Bayesian updating to improve efficiency and robustness. Furthermore, an exploration-exploitation based matchmaking strategy is proposed to facilitate valuable comparisons, which further accelerates convergence. We validate the superiority of the proposed algorithms via multiple simulated experiments. To date, K-Sort Arena has collected extensive high-quality votes to build comprehensive leaderboards for image and video generation."}]}