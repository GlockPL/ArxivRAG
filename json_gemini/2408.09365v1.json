{"title": "Concept Distillation from Strong to Weak Models via Hypotheses-to-Theories Prompting", "authors": ["Emmanuel Aboah Boateng", "Cassiano O. Becker", "Nabiha Asghar", "Kabir Walia", "Ashwin Srinivasan", "Ehi Nosakhare", "Victor Dibia", "Soundar Srinivasan"], "abstract": "Hand-crafting high quality prompts to optimize the performance of language models is a complicated and labor-intensive process. Furthermore, when migrating to newer, smaller, or weaker models (possibly due to latency or cost gains), prompts need to be updated to re-optimize the task performance. We propose Concept Distillation (CD), an automatic prompt optimization technique for enhancing weaker models on complex tasks. CD involves: (1) collecting mistakes made by weak models with a base prompt (initialization), (2) using a strong model to generate reasons for these mistakes and create rules/concepts for weak models (induction), and (3) filtering these rules based on validation set performance and integrating them into the base prompt (deduction/verification). We evaluated CD on NL2Code and mathematical reasoning tasks, observing significant performance boosts for small and weaker language models. Notably, Mistral-7B's accuracy on Multi-Arith increased by 20%, and Phi-3-mini-3.8B's accuracy on HumanEval rose by 34%. Compared to other automated methods, CD offers an effective, cost-efficient strategy for improving weak models' performance on complex tasks and enables seamless workload migration across different language models without compromising performance.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have shown remarkable capabilities for various downstream tasks. An inexpensive alternative to training and finetuning, Prompt Engineering has emerged as a powerful method to control and optimize the outputs from LLMs. Prompt engineering is enabled by the in-context learning (ICL) capability of LLMs (Dong et al., 2022), which enables us to apply LLMs to new tasks by providing them with a suitable input prompt that contains relevant information and instructions (Xie et al., 2021).\nAs such, crafting high-quality prompts can be a challenging and labor-intensive process. Finding the right instructions can require several rounds of trial-and-error experimentation. Furthermore, the same prompt may not work for different tasks, models or domains (Lu et al., 2023; Rubin et al., 2021). Importantly, weak models such as GPT-3.5 or Mistral-7B often lack the same reasoning capabilities as strong models such as GPT-40 and as a result, struggle with complex and high-reasoning tasks (Edwards and Camacho-Collados, 2024; Liang et al., 2023). This leads to significant performance gaps between stronger and weaker models for such tasks. Conversely, practical reasons (e.g., lower runtime latency, cost, and memory footprint) may still motivate and impose the use of weak models in practical applications (Xia et al., 2023; Hadi et al., 2023). While fine-tuning can close the gap, it is an expensive and technically involved process. This results in the repeated cycle of manually writing new prompts or adapting previous prompts to boost the performance of the weaker models.\nAnother key area this work addresses is the efficient adaptation of prompts for various models. A primary challenge is transitioning prompts from an existing model, such as GPT-4, to a newly released variant like GPT-40. It is essential to recognize that different models may respond uniquely to the same prompts. As such, there is the need for strategies that effectively modify and tailor existing prompts to maintain aligmnet with new or evolving models.\nIn this paper, we introduce concept distillation (CD), an automated prompt optimization technique. CD improves the performance of weak/small language models on complex tasks by distilling key rules, concepts, or examples from a strong/large model via hypotheses-to-theories prompting. These distilled concepts are then verified and used to guide a weak model, enabling it to produce more accurate responses, all without the need for fine-tuning. The structured approach within the CD framework ensures that these distilled concepts are sufficiently general to be transferable across various other language models. Fig. 1 shows a high-level illustration of the concept distillation for prompt optimization approach. Overall, this paper makes the following contributions:\n\u2022 We introduce the notion of concept distillation, in which a strong model is used to derive new concepts (i.e., specific prompt instructions) to help a weak model improve its performance on complex tasks, thereby enabling greater adaptability of the weak model in various applications (see Fig. 3).\n\u2022 Building on time-tested principles of scientific discovery, we propose the hypotheses-to-theories prompt optimization framework, which leverages the strong model's ability to perform inductive and deductive reasoning over the weak model's deficiencies (see Sections 3 and 4).\n\u2022 We demonstrate that the prompt optimization framework enables efficient adaptation of prompts across different language models (LMs). The distilled concepts are transferable, allowing for quick and effective updates in response to new model releases or changes,"}, {"title": "2 Related Work", "content": "Given the significance and broad-scale effectiveness of prompt engineering, there have been several efforts to perform automated prompt optimization and generation. These methods typically involve an iterative algorithm consisting of several steps an initially generated prompt, scoring of the prompt, and regeneration of the prompt using the score as an improvement signal, till a stopping criteria is met (Zhou et al., 2022a; Hu et al., 2023; Pryzant et al., 2023a; Ye et al., 2023; Wang et al., 2023; Deng et al., 2023; Guo et al., 2023). We propose an approach that augments this framework for prompt optimization through the distillation of concepts, and introduces an explicit verification step to demonstrate relative performance improvements for a small model.\nOur method is inspired by several recent works. APE (Zhou et al., 2022a) deduces an initial prompt from training samples, and then uses an LLM to refine and generate new semantically similar prompt candidates. However, prompts are simply paraphrased during the refinement process, which is akin to random search in the prompt space. Evoke (Hu et al., 2023) uses the same LLM to review and score the quality of a prompt, as well as to refine the prompt based on the reviewer feedback. (Zhu et al., 2023) first uses an LLM to induce a rule library from a set of training examples, which are later sampled for dynamic prompt construction. This is followed by a deduction phase where these rules are evaluated based on their coverage and confidence. (Zhang et al., 2024) generates high and low-level concepts from mistakes using an LLM, and later uses the same LLM for solving tasks. There is no deduction phase to filter out the generated concepts. PE2 (Ye et al., 2023) explores meta-prompt variants to guide LLMs to"}, {"title": "3 Background", "content": "In this section, we explore the foundational concepts and terminologies central to this paper. This technique draws inspiration from human cognitive processes (Hunt, 2003; Cherukunnath and Singh, 2022), particularly in how we acquire, refine, and apply knowledge and concepts across various domains.\nConcept Distillation: distinction from Knowledge Distillation. The core of our technique is encapsulated in the process of 'concept distillation'. This process involves the transfer of concepts from a stronger LM (referred to as the 'teacher') to a weaker LM (referred to as the 'student'). The differentiation between knowledge and concept distillation is critical. Unlike traditional knowledge distillation (Phuong and Lampert, 2019), which focuses on the explicit transfer/update of learned weights and biases through intensive training or fine-tuning procedures, concept distillation emphasizes the induction of general concepts, rules, examples, or key ideas from the teacher model, applying them to the student model solely via in-context learning (ICL), without necessitating extensive training or fine-tuning. Figure 3 depicts the distinction between knowledge and concept distillation.\nHypotheses, Theories, and Reasoning: frameworks for conceptual transfer. Our approach is deeply rooted in the scientific methodologies of hypothesis generation, experimental validation, and theory (Scerbo et al., 2019). A hypothesis, in this context, is a proposition based on limited evidence, serving as a foundation for further investigation that could culminate in a theory, i.e., a well-substantiated explanation of a phenomenon. This framework is critical in concept distillation, where hypotheses derived from observations are validated through experimental evidence to form theories that explain the underlying principles or phenomena.\nThe transformation from hypotheses to theories is facilitated by two modes of reasoning: inductive and deductive reasoning. Inductive reasoning involves deriving general rules from specific observed facts, whereas deductive reasoning entails deriving new facts from established facts and rules. Deductive reasoning allows us to apply general principles to specific cases to derive accurate conclusions. These modes of reasoning allow the extrapolation of concepts from inductive reasoning and the application of these concepts to new, unseen instances.\nDrawing parallels to the human process of scientific discovery (Bradford and Hamer, 2022), our technique mirrors the iterative cycle of observation, hypothesis formulation, experimentation, and"}, {"title": "4 Concept Distillation Framework", "content": "In this section, we describe the main steps and components of our technique, which consists of three main phases: initialization, induction, and deduction from verification.\nInitialization phase. Phase 1 starts with a base prompt template, which can either be an existing prompt we aim to modify (for a strong, large model we aim to replace), a generated prompt using an off-the-shelf algorithm, or one manually crafted by domain experts, serving as a foundation for subsequent refinement. In this phase, we assess the strengths and weaknesses (mistakes) of the weaker model concerning the intended task. The primary goal here is to pinpoint areas and examples where the weaker model struggles, enabling us to induce concepts that aid in reasoning in these specific areas. It's important to focus on the model's weaknesses, avoiding unnecessary adjustments in areas where the model already performs well.\nInduction phase. Phase 2 involves the induction of concepts from a strong model, such as GPT-4,\ntailored to address the identified weaknesses and mistakes of the weaker model. The aim is to enhance the weaker model's performance by equipping it with these newly induced concepts. During this process, we use the strong model to reason through the facts or questions presented to the weak model, the incorrect responses it generated, and the correct answers, in order to generate general concepts that can overcome the weak model's mistakes.\nDeduction from verification phase. Phase 3 is the deduction-from-verification process. The assumption is that not all induced rules/concepts or examples qualify as useful distilled concepts. This phase uses a deduction process to verify the induced rules and examples. The rules which qualify as having broad coverage and high prediction confidence are accepted as distilled concepts. Consequently, they are added to the initial prompt template that we started with to form an improved, updated prompt. After adding the induced concepts to the base prompt template, a verification process is applied to filter the concepts. Either the strong model can be used to generate test examples similar to the weaknesses identified earlier or similar examples from a validation set can be used for the verification. The weaker model is required to accurately address all validation examples with a level of certainty or probability that meets or exceeds a specific predefined threshold before we accept the induced concepts as distilled concepts and integrate them into its prompt. This ensures that the final prompt effectively addresses the weak model's shortcomings, leading to improved performance. Detailed description of the concept distillation process with a walk-through practical example is provided in Appendix C."}, {"title": "5 Experiments", "content": "In this section, we outline the experimental setup used to evaluate the effectiveness of the proposed Concept Distillation framework.", "subsections": [{"title": "5.1 Task and Datasets", "content": "We focus on three benchmark datasets: converting natural language to structure queries (NL2Code) HumanEval (Chen et al., 2021), and reasoning over mathematical problems \u2013 Multi-Arith (Roy and Roth, 2015) and GSM8K (Cobbe et al., 2021). HumanEval consists of natural language prompts that require the model to generate corresponding code snippets. It is widely used to benchmark the performance of language models in code generation tasks. Multi-Arith is designed to evaluate the arithmetic reasoning capabilities of language models. It includes a variety of arithmetic problems that require step-by-step reasoning to arrive at the correct answer. And finally, the GSM8K dataset is designed to evaluate the mathematical reasoning capabilities of language models. It includes a variety of math problems that require step-by-step reasoning to arrive at the correct answer."}, {"title": "5.2 Implementation Details", "content": "To comprehensively evaluate the effectiveness of the CD framework, we conducted experiments using multiple models, including GPT-3.5 Turbo, Claude 2.1, Phi-3-mini-3.8B, Mixtral-8x7B* (a new flagship Nous Research model trained over the Mixtral 8x7B LLM), and Mistral-7B, with GPT-40 serving as the strong model for concept distillation. Our experimental setup was designed to address the two main hypotheses and problem categories outlined in this paper:\n\u2022 Boosting the performance of weak models on complex and structured tasks via concept distillation from a strong model. We distilled concepts from GPT-40 based on the deficiencies of the weak models to improve their performance on training data, and then evaluated them on test data.\n\u2022 Adapting prompts to new models: We tested the transferability of the distilled concepts by applying the optimized prompts, initially designed for GPT-3.5 Turbo, to other models such as Claude 2.1, Phi-3-mini-3.8B, Mixtral-8x7B*, and Mistral-7B. This allowed us to evaluate the effectiveness of the distilled concepts across different language models.\nFor each dataset, we split the data into training and test sets. The training set was utilized for prompt optimization through CD, and the test set was employed for evaluation. We performed a comprehensive comparison of our work with several state-of-the-art methods."}]}, {"title": "6 Results and Analyses", "content": "In Table 1 we summarize the performance of various models on the HumanEval test set, using only the base prompt and after CD (using the updated prompt with concepts). Notably, with base prompt alone, the strong model GPT-4o achieved a perfect score (100%); in comparison, the weak models performed poorly. However, when using the updated\nprompt with concepts distilled using the CD technique, we observe significant performance boosts for the weak models.\nFirstly, we observe a performance increase of 11% for the GPT-3.5 Turbo model, raising its accuracy from 0.85 to 0.96. Claude 2.1 nearly achieved a perfect score, improving from 0.89 to 0.99, an increase of 10%, indicating that CD is effective in optimizing prompts even for models that initially perform well. The most notable performance gain was observed with the smallest model, Phi-3-mini-3.8B, which saw a substantial improvement of 32%, from 0.48 to 0.82. Across all models evaluated, there was a significant performance increase compared to the base prompt evaluation, with an average performance increase of 13%.\nIn Table 2 we summarize the results on the Multi-Arith dataset. We observe a 6% performance gain for the GPT-3.5 Turbo model, a significantly larger gain for the Claude 2.1 model with a 29% increase in accuracy from 0.62 to 0.91, and a similarly large 20% accuracy gain for the Mistral-7B model. On average, weak models observed a performance lift of 15% on the Multi-Arith mathematical reasoning task.\nThe results in Table 1 and 2 provide evidence that Concept Distillation enhances the capabilities of weaker and smaller models, helping them overcome mistakes, and boosting their performance on complex, structured tasks like code generation and mathematical reasoning.\nTable 3 presents a comparative analysis of accuracy on the HumanEval dataset among three different methods: APE, CoT, and our work (CD). The results demonstrate that CD consistently outperforms both APE and CoT across multiple models. For instance, GPT-3.5 shows an increase in accuracy from 0.93 with APE, 0.45 with CoT, but it observes the greatest lift to 0.96 with CD. Similarly, Claude-2 achieves near-perfect accuracy with CD at 0.99, compared to 0.96 with APE and 0.82 with CoT.\nThe results also highlight significant improvements for Mixtral-8x7B* and Mistral-7B, where CD boosts their accuracies to 0.95 and 0.96, respectively, compared to lower accuracies with APE (0.73 and 0.71) and CoT (0.88 and 0.87). Notably, Phi-3-mini-3.8B's accuracy slightly decreases with CD compared to CoT due to its initial weaknesses during training, which resulted in a lower baseline accuracy of 38% on the training set. As a result, the extensive concept distillation required to address these weaknesses introduced slight confusion in some edge cases. Despite this, Phi-3-mini-3.8B still maintains competitive performance.\nTable 4 provides a comprehensive comparison of different models and methods, including APE, CoT, Iterative APE, APO, PE2, and CD, across the various models on the Multi-Arith dataset. The results demonstrate that CD consistently outperforms other methods across most models. Particularly, GPT-3.5 achieves the highest accuracy with CD at 0.95, compared to 0.63 with APE and 0.71 with CoT. Similarly, Claude-2 shows a substantial improvement with CD, reaching an accuracy of 0.91, while other methods like APE and CoT achieve lower accuracies of 0.43 and 0.48, respectively. Mixtral-8x7B* also benefits significantly from CD, achieving an accuracy of 0.88, compared to 0.84 with APE and 0.85 with CoT. However, Mistral-7B's performance slightly decreases with CD, achieving an accuracy of 0.67, compared to 0.72 with CoT. Similar to Phi-3-mini-3.8B in the previous section, we observed that the introduced concepts led to confusion for the Mistral-7B model on certain edge cases. Overall, Table 4 highlights the effectiveness of the CD framework, demonstrating its superior performance in enhancing model accuracy compared to other prompt optimization"}, {"title": "6.1 Transferability of distilled concepts", "content": "We tested how well the optimized prompts, originally designed for GPT-3.5 Turbo, work on other models like Claude 2.1, Phi-3-mini-3.8B, Mixtral-8x7B*, Mistral-7B, and GPT-4. This helped us see if the distilled concepts are effective across different language models.\nTable 5 provides compelling evidence for our hypothesis that distilled concepts from CD are transferable and generalizable across different models. In this experiment, GPT-3.5 Turbo served as the base model for distilling concepts using a strong model (GPT-40), and the optimized prompts were then transferred to other models for evaluation. We observe significant performance improvements across all models. Notably, Claude 2.1 achieved a perfect score of 100%, demonstrating an 11% improvement. The smallest model, Phi-3-mini-3.8B, exhibited the most remarkable improvement, with a performance boost of 34%, increasing its accuracy from 0.45 to 0.79. This result further validates the observation that smaller models gain substantial benefits from the CD process. Overall, the results show an average performance increase, confirming that the distilled concepts are not only effective for the base model but also enhance the performance of other models significantly.\nTable 6 provides a comparative analysis of the accuracy improvements achieved through distilled concepts transfer from GPT-3.5 Turbo prompt optimized using CD to both smaller and larger models, compared to APE on the HumanEval dataset. The CD method significantly outperforms APE, with notable improvements in models such as Mistral-7B, which saw a substantial increase of 25% (from 0.71 to 0.96). Mixtral-8x7B* also benefited greatly, with a 14% boost in accuracy (from 0.73 to 0.87). These results show the superior performance of the CD approach in enhancing model performance by distilling and transferring essential concepts from stronger to weaker models.\nSimilarly, on the GSM8K dataset shown in the Appendix (see Appendix A.2, CD continues to demonstrate its effectiveness, particularly with the GPT-3.5 model, which improved from 0.67 with APE and 0.76 with CD. While Claude 2.1 experienced a slight decrease in performance likely due to the comprehensiveness of the GSM8K dataset and potential prompt overload, GPT-4 saw a significant improvement from 0.84 to 0.90. These findings validate the effectiveness and robustness of the CD framework in improving the accuracy of weaker models on structured and more complex tasks."}, {"title": "7 Conclusion", "content": "In conclusion, our study demonstrates the robustness of Concept Distillation (CD) in significantly enhancing the performance of weaker language models across various tasks, as evidenced by substantial accuracy improvements on the HumanEval, Multi-Arith, and GSM8K datasets. By distilling and transferring essential concepts from stronger models, CD not only boosts the capabilities of smaller models but also ensures the transferability of these improvements across different models. Our extensive experiments show that CD consistently outperforms various state-of-the-art prompt optimization methods. This robust framework, therefore, addresses critical challenges in prompt engineering, offering a scalable and resource-efficient solution that advances the state-of-the-art in prompt optimization for language models."}, {"title": "A Case Study: Natural Language to Cypher Translation", "content": "In this study, we also employed the concept distillation approach on a dataset designed for Natural Language to Cypher (NL2Cypher) query translation, aiming to leverage the generative capabilities of LLMs for producing syntactically correct Cypher codes from natural language queries. The dataset encompassed various subsets, including queries pertaining to calendars (e.g., \"when is my next meeting with person\"), files, and people, structured according to a specific schema.\nOur observations highlighted that the GPT-4 model demonstrated superior performance across all dataset subsets during validation, with its lowest accuracy approximately 80% occurring in NL2Cypher query translations concerning people. Conversely, the GPT-3.5 Turbo model, utilizing identical prompts to GPT-4, exhibited markedly lower performance across these subsets. Notably, it failed entirely to translate queries related to files and people within an organization, resulting in zero accuracy for these categories. Figure 4 shows the accuracy comparison between GPT-3.5-Turbo (with baseline prompt), GPT-3.5 Turbo model (with optimized prompt) and GPT-4 model (with baseline prompt).\nSubsequent to the application of concept distillation from GPT-4 into the prompt optimization process for GPT-3.5 Turbo the performance of the latter model saw substantial improvements across all validation datasets. In particular, for queries related to the calendar category, the GPT-3.5 Turbo model not only improved but also exceeded GPT-4's performance, achieving an accuracy rate of 95.65%. Moreover, in scenarios involving people-related queries, where the GPT-3.5 Turbo model"}, {"title": "B Additional results and discussion", "content": "Table 7 presents the accuracy comparison on the GSM8K dataset between CD and APE. The results demonstrate that CD consistently outperforms APE across multiple models. For instance, the GPT-3.5 model shows a significant improvement in accuracy from 0.67 with APE to 0.76 with CD. Similarly, GPT-4's accuracy increases from 0.84 with APE to 0.90 with CD, highlighting the effectiveness of CD in enhancing model performance on mathematical reasoning tasks.\nDespite these improvements, the Claude 2.1 model experienced a slight decrease in performance, dropping from 0.86 with APE to 0.84 with CD. This suggests that while CD is generally effective, it may introduce prompt overload that can sometimes negatively impact certain models, particularly in scenarios involving highly comprehensive datasets like GSM8K. Future work will explore methods to encourage the consolidation of distilled concepts or the development of a hierarchical structure of concepts to enhance their effectiveness."}, {"title": "C Hypothetical Example: Detailed walk-through of the Approach", "content": "To illustrate our proposed method, we employ a hypothetical example, guiding you through the three phases of the concept distillation process shown in Fig. 2.\nThe example involves a chatbot designed to translate natural language into Cypher query commands. Cypher is a declarative graph query language used for querying and managing data in graph databases, such as Neo4j. It enables users to efficiently and intuitively query, update, and manage graph data by expressing patterns in the graph structure through a readable syntax. This chatbot utilizes an LLM, specifically GPT-3.5, to interpret a user's natural language query and generate a corresponding Cypher query based on a predefined graph schema. This example will demonstrate how our technique optimizes the prompt of the assumed weak model in question (GPT-3.5). Figure 5 depicts the hypothetical natural language to Cypher query translator utilized for the purpose of explaining the method."}, {"title": "C.1 Initialization", "content": "In this initial phase, we set up essential components for our technique. This includes defining the task (natural language to Cypher query translation in this case), preparing a 'golden dataset' which contains pairs of natural language queries and their corresponding Cypher queries (serving as the task's ground truth), and creating a prompt template with basic information and instructions for the task. This template might include a few examples, and specify the input-output format. The golden dataset represents the training dataset for the method. Depending on the size of the training dataset set, we cluster it into various entities and then use stratified sampling technique to split the dataset into train and validation sets The initial task-specific prompt used for this phase could be generated by an off-the-shelf algorithm, manually crafted, or an already existing prompt being used by a different LM.\nWe then evaluate the weak model, in this case, GPT-3.5, using this golden dataset of NL-Cypher pairs, as illustrated in Fig. 6. We start by selecting a pair of natural language and Cypher queries from the dataset and feeding them to the weak model using the prompt template. We then observe the output of the weak model and compare it to the ground truth. If the output is correct, we move on to the next pair. If the output is wrong, we record the error and proceed to the next step. In our hypothetical example, the first data point is deemed a strength of GPT-3.5 as it correctly generates the expected Cypher query. However, the second data point reveals a weakness, with the model failing to generate the correct Cypher query in response to the natural language query \"who are the devs I am meeting in 1:1s.\u201d"}, {"title": "C.2 Induction", "content": "In this phase, we use the strong model to induce key concepts and rules from the given task and dataset, by prompting it to reason through the facts. Here, we start constructing the prompt for the strong model by going through the following steps:\n\u2022 First, we define the persona of the strong model, for example, \"you are an expert in generating and reasoning over natural language to Cypher queries translation...\u201c\n\u2022 Next, we present to the strong model the accurate NL-Cypher pair specifically, the one that the weak model failed to predict correctly. Along with this, we include in the strong model's prompt the incorrect Cypher query generated by the weak model, as well as the original prompt template that was used for the weak model.\n\u2022 Following this, we request the strong model to analyze and identify the reasons behind the weak model's incorrect response. This analysis is based on all the information and facts that have been included in the prompt.\n\u2022 The strong model then reasons through the facts presented and tries to provide a sense of meaning into why the weak model is struggling with the input query, which we are considering in this case as \u201cwho are the devs I am meeting in 1:1s.\" Here, we ask the strong model to explain why the response of the weak model is wrong, and what are the missing or incorrect concepts or rules that the weak model should have used.\n\u2022 We then follow up with another turn of discussion, in this case, we prompt the strong model to induce some concepts (concepts here could be rules, examples, etc. depending on the application) to guide the weak model in explicit reasoning, in such a way that it is able to answer all similar questions correctly.\n\u2022 The strong model finally induces these concepts based on the presented facts and its reasoning over the cause of the weak model's inability to generate the correct response."}, {"title": "C.3 Deduction from Verification", "content": "The final phase, Deduction from Verification, employs deductive reasoning to validate the concepts induced in the previous phase. This involves using the strong model to generate test cases that are similar to the incorrectly predicted input in questions (as \"who are the devs I am meeting in 1:1s.\"). The generated test cases mimic the initial failure but"}]}