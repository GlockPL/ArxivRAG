{"title": "Concept Distillation from Strong to Weak Models via Hypotheses-to-Theories Prompting", "authors": ["Emmanuel Aboah Boateng", "Cassiano O. Becker", "Nabiha Asghar", "Kabir Walia", "Ashwin Srinivasan", "Ehi Nosakhare", "Victor Dibia", "Soundar Srinivasan"], "abstract": "Hand-crafting high quality prompts to optimize the performance of language models is a complicated and labor-intensive process. Furthermore, when migrating to newer, smaller, or weaker models (possibly due to latency or cost gains), prompts need to be updated to re-optimize the task performance. We propose Concept Distillation (CD), an automatic prompt optimization technique for enhancing weaker models on complex tasks. CD involves: (1) collecting mistakes made by weak models with a base prompt (initialization), (2) using a strong model to generate reasons for these mistakes and create rules/concepts for weak models (induction), and (3) filtering these rules based on validation set performance and integrating them into the base prompt (deduction/verification). We evaluated CD on NL2Code and mathematical reasoning tasks, observing significant performance boosts for small and weaker language models. Notably, Mistral-7B's accuracy on Multi-Arith increased by 20%, and Phi-3-mini-3.8B's accuracy on HumanEval rose by 34%. Compared to other automated methods, CD offers an effective, cost-efficient strategy for improving weak models' performance on complex tasks and enables seamless workload migration across different language models without compromising performance.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have shown remarkable capabilities for various downstream tasks. An inexpensive alternative to training and finetuning, Prompt Engineering has emerged as a powerful method to control and optimize the outputs from LLMs. Prompt engineering is enabled by the in-context learning (ICL) capability of LLMs (Dong et al., 2022), which enables us to apply LLMs to new tasks by providing them with a suitable input prompt that contains relevant information and instructions (Xie et al., 2021).\nAs such, crafting high-quality prompts can be a challenging and labor-intensive process. Finding the right instructions can require several rounds of trial-and-error experimentation. Furthermore, the same prompt may not work for different tasks, models or domains (Lu et al., 2023; Rubin et al., 2021). Importantly, weak models such as GPT-3.5 or Mistral-7B often lack the same reasoning capabilities as strong models such as GPT-40 and as a result, struggle with complex and high-reasoning tasks (Edwards and Camacho-Collados, 2024; Liang et al., 2023). This leads to significant performance gaps between stronger and weaker models for such tasks. Conversely, practical reasons (e.g., lower runtime latency, cost, and memory footprint) may still motivate and impose the use of weak models in practical applications (Xia et al., 2023; Hadi et al., 2023). While fine-tuning can close the gap, it is an expensive and technically involved process. This results in the repeated cy-"}, {"title": "Related Work", "content": "Given the significance and broad-scale effectiveness of prompt engineering, there have been several efforts to perform automated prompt optimization and generation. These methods typically involve an iterative algorithm consisting of several steps an initially generated prompt, scoring of the prompt, and regeneration of the prompt using the score as an improvement signal, till a stopping criteria is met (Zhou et al., 2022a; Hu et al., 2023; Pryzant et al., 2023a; Ye et al., 2023; Wang et al., 2023; Deng et al., 2023; Guo et al., 2023). We propose an approach that augments this framework for prompt optimization through the distillation of concepts, and introduces an explicit verification step to demonstrate relative performance improvements for a small model.\nOur method is inspired by several recent works. APE (Zhou et al., 2022a) deduces an initial prompt from training samples, and then uses an LLM to refine and generate new semantically similar prompt candidates. However, prompts are simply paraphrased during the refinement process, which is akin to random search in the prompt space. Evoke (Hu et al., 2023) uses the same LLM to review and score the quality of a prompt, as well as to refine the prompt based on the reviewer feedback. (Zhu et al., 2023) first uses an LLM to induce a rule library from a set of training examples, which are later sampled for dynamic prompt construction. This is followed by a deduction phase where these rules are evaluated based on their coverage and confidence. (Zhang et al., 2024) generates high and low-level concepts from mistakes using an LLM, and later uses the same LLM for solving tasks. There is no deduction phase to filter out the generated concepts. PE2 (Ye et al., 2023) explores meta-prompt variants to guide LLMs to"}, {"title": "Background", "content": "In this section, we explore the foundational concepts and terminologies central to this paper. This technique draws inspiration from human cognitive processes (Hunt, 2003; Cherukunnath and Singh, 2022), particularly in how we acquire, refine, and apply knowledge and concepts across various domains.\nConcept Distillation: distinction from Knowledge Distillation. The core of our technique is encapsulated in the process of 'concept distillation'. This process involves the transfer of concepts from a stronger LM (referred to as the 'teacher') to a weaker LM (referred to as the 'student'). The differentiation between knowledge and concept distilla-"}, {"title": "Concept Distillation Framework", "content": "In this section, we describe the main steps and components of our technique, which consists of three main phases: initialization, induction, and deduction from verification.\nInitialization phase. Phase 1 starts with a base prompt template, which can either be an existing prompt we aim to modify (for a strong, large model we aim to replace), a generated prompt using an off-the-shelf algorithm, or one manually crafted by domain experts, serving as a foundation for subsequent refinement. In this phase, we assess the strengths and weaknesses (mistakes) of the weaker model concerning the intended task. The primary goal here is to pinpoint areas and examples where the weaker model struggles, enabling us to induce concepts that aid in reasoning in these specific areas. It's important to focus on the model's weaknesses, avoiding unnecessary adjustments in areas where the model already performs well.\nInduction phase. Phase 2 involves the induction of concepts from a strong model, such as GPT-4,"}, {"title": "Experiments", "content": "In this section, we outline the experimental setup used to evaluate the effectiveness of the proposed Concept Distillation framework.\n5.1 Task and Datasets\nWe focus on three benchmark datasets: converting natural language to structure queries (NL2Code) HumanEval (Chen et al., 2021), and reasoning over mathematical problems \u2013 Multi-Arith (Roy and Roth, 2015) and GSM8K (Cobbe et al., 2021). HumanEval consists of natural language prompts that require the model to generate corresponding code snippets. It is widely used to benchmark the performance of language models in code generation tasks. Multi-Arith is designed to evaluate the arithmetic reasoning capabilities of language models. It includes a variety of arithmetic problems that require step-by-step reasoning to arrive at the correct answer. And finally, the GSM8K dataset is designed to evaluate the mathematical reasoning capabilities of language models. It includes a variety of math problems that require step-by-step reasoning to arrive at the correct answer."}, {"title": "Implementation Details", "content": "To comprehensively evaluate the effectiveness of the CD framework, we conducted experiments using multiple models, including GPT-3.5 Turbo, Claude 2.1, Phi-3-mini-3.8B, Mixtral-8x7B* (a new flagship Nous Research model trained over the Mixtral 8x7B LLM), and Mistral-7B, with GPT-40 serving as the strong model for concept distillation. Our experimental setup was designed to address the two main hypotheses and problem categories outlined in this paper:\n\u2022 Boosting the performance of weak models on complex and structured tasks via concept distillation from a strong model. We distilled concepts from GPT-40 based on the deficiencies of the weak models to improve their performance on training data, and then evaluated them on test data.\n\u2022 Adapting prompts to new models: We tested the transferability of the distilled concepts by applying the optimized prompts, initially designed for GPT-3.5 Turbo, to other models such as Claude 2.1, Phi-3-mini-3.8B, Mixtral-8x7B*, and Mistral-7B. This allowed us to evaluate the effectiveness of the distilled concepts across different language models.\nFor each dataset, we split the data into training and test sets. The training set was utilized for prompt optimization through CD, and the test set was employed for evaluation. We performed a comprehensive comparison of our work with several state-of-the-art methods."}, {"title": "Results and Analyses", "content": "In Table 1 we summarize the performance of various models on the HumanEval test set, using only the base prompt and after CD (using the updated prompt with concepts). Notably, with base prompt alone, the strong model GPT-4o achieved a perfect score (100%); in comparison, the weak models performed poorly. However, when using the updated"}, {"title": "Transferability of distilled concepts", "content": "We tested how well the optimized prompts, originally designed for GPT-3.5 Turbo, work on other models like Claude 2.1, Phi-3-mini-3.8B, Mixtral-8x7B*, Mistral-7B, and GPT-4. This helped us see if the distilled concepts are effective across different language models.\nTable 5 provides compelling evidence for our hypothesis that distilled concepts from CD are transferable and generalizable across different models. In this experiment, GPT-3.5 Turbo served as the base model for distilling concepts using a strong model (GPT-40), and the optimized prompts were then transferred to other models for evaluation. We observe significant performance improvements across all models. Notably, Claude 2.1 achieved a perfect score of 100%, demonstrating an 11% improvement. The smallest model, Phi-3-mini-3.8B, exhibited the most remarkable improvement, with a performance boost of 34%, increasing its accuracy from 0.45 to 0.79. This result further validates the observation that smaller models gain substantial benefits from the CD process. Overall, the results show an average performance increase, confirming that the distilled concepts are not only effective for the base model but also enhance the performance of other models significantly.\nTable 6 provides a comparative analysis of the accuracy improvements achieved through distilled concepts transfer from GPT-3.5 Turbo prompt optimized using CD to both smaller and larger models, compared to APE on the HumanEval dataset. The"}, {"title": "Conclusion", "content": "In conclusion, our study demonstrates the robustness of Concept Distillation (CD) in significantly enhancing the performance of weaker language models across various tasks, as evidenced by substantial accuracy improvements on the HumanEval, Multi-Arith, and GSM8K datasets. By distilling and transferring essential concepts from stronger models, CD not only boosts the capabilities of smaller models but also ensures the transferability of these improvements across different models. Our extensive experiments show that CD consistently outperforms various state-of-the-art prompt optimization methods. This robust framework, therefore, addresses critical challenges in prompt engineering, offering a scalable and resource-efficient solution that advances the state-of-the-art in prompt optimization for language models."}, {"title": "Case Study: Natural Language to Cypher Translation", "content": "In this study, we also employed the concept distillation approach on a dataset designed for Natural Language to Cypher (NL2Cypher) query translation, aiming to leverage the generative capabilities of LLMs for producing syntactically correct Cypher codes from natural language queries. The dataset encompassed various subsets, including queries pertaining to calendars (e.g., \"when is my next meeting with person\"), files, and people, structured according to a specific schema.\nOur observations highlighted that the GPT-4 model demonstrated superior performance across all dataset subsets during validation, with its lowest accuracy-approximately 80%-occurring in NL2Cypher query translations concerning people. Conversely, the GPT-3.5 Turbo model, utilizing identical prompts to GPT-4, exhibited markedly lower performance across these subsets. Notably, it failed entirely to translate queries related to files and people within an organization, resulting in zero accuracy for these categories. Figure 4 shows the accuracy comparison between GPT-3.5-Turbo (with baseline prompt), GPT-3.5 Turbo model (with optimized prompt) and GPT-4 model (with baseline prompt).\nSubsequent to the application of concept distillation from GPT-4 into the prompt optimization process for GPT-3.5 Turbo-the performance of the latter model saw substantial improvements across all validation datasets. In particular, for queries related to the calendar category, the GPT-3.5 Turbo model not only improved but also exceeded GPT-4's performance, achieving an accuracy rate of 95.65%. Moreover, in scenarios involving people-related queries, where the GPT-3.5 Turbo model"}, {"title": "Additional results and discussion", "content": "Table 7 presents the accuracy comparison on the GSM8K dataset between CD and APE. The results demonstrate that CD consistently outperforms APE across multiple models. For instance, the GPT-3.5 model shows a significant improvement in accuracy from 0.67 with APE to 0.76 with CD. Similarly, GPT-4's accuracy increases from 0.84 with APE to 0.90 with CD, highlighting the effectiveness of CD in enhancing model performance on mathematical reasoning tasks.\nDespite these improvements, the Claude 2.1 model experienced a slight decrease in performance, dropping from 0.86 with APE to 0.84 with CD. This suggests that while CD is generally effective, it may introduce prompt overload that can sometimes negatively impact certain models, particularly in scenarios involving highly comprehensive"}, {"title": "Hypothetical Example: Detailed walk-through of the Approach", "content": "To illustrate our proposed method, we employ a hypothetical example, guiding you through the three phases of the concept distillation process shown in Fig. 2.\nThe example involves a chatbot designed to translate natural language into Cypher query commands. Cypher is a declarative graph query language used for querying and managing data in graph databases, such as Neo4j. It enables users to efficiently and intuitively query, update, and manage graph data by expressing patterns in the graph structure through a readable syntax. This chatbot utilizes an LLM, specifically GPT-3.5, to interpret a user's natural language query and generate a corresponding Cypher query based on a predefined graph schema. This example will demonstrate how our technique optimizes the prompt of the assumed weak model in question (GPT-3.5). Figure 5 depicts the hypothetical natural language to Cypher query translator utilized for the purpose of explaining the method."}, {"title": "Initialization", "content": "In this initial phase, we set up essential components for our technique. This includes defining the task (natural language to Cypher query translation in this case), preparing a 'golden dataset' which contains pairs of natural language queries and their corresponding Cypher queries (serving as the task's ground truth), and creating a prompt template with basic information and instructions for the task. This template might include a few examples, and specify the input-output format. The golden dataset represents the training dataset for the method. Depending on the size of the training dataset set, we cluster it into various entities and then use stratified sampling technique to split the dataset into train and validation sets The initial task-specific prompt used for this phase could be generated by an off-the-shelf algorithm, manually crafted, or an already existing prompt being used by a different LM.\nWe then evaluate the weak model, in this case, GPT-3.5, using this golden dataset of NL-Cypher pairs, as illustrated in Fig. 6. We start by selecting a pair of natural language and Cypher queries from the dataset and feeding them to the weak model using the prompt template. We then observe the output of the weak model and compare it to the ground truth. If the output is correct, we move on to the next pair. If the output is wrong, we record the error and proceed to the next step. In our hypothetical example, the first data point is deemed a strength of GPT-3.5 as it correctly generates the expected Cypher query. However, the second data point reveals a weakness, with the model failing to generate the correct Cypher query in response to the natural language query \"who are the devs I am meeting in 1:1s.\u201d"}, {"title": "Induction", "content": "In this phase, we use the strong model to induce key concepts and rules from the given task and dataset, by prompting it to reason through the facts. Here, we start constructing the prompt for the strong model by going through the following steps:\n\u2022 First, we define the persona of the strong model, for example, \"you are an expert in generating and reasoning over natural language to Cypher queries translation...\u201c\n\u2022 Next, we present to the strong model the accurate NL-Cypher pair - specifically, the one"}, {"title": "Deduction from Verification", "content": "The final phase, Deduction from Verification, employs deductive reasoning to validate the concepts induced in the previous phase. This involves using the strong model to generate test cases that are similar to the incorrectly predicted input in questions (as \"who are the devs I am meeting in 1:1s.\"). The generated test cases mimic the initial failure but"}]}