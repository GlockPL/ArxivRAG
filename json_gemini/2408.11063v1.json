{"title": "Tabular Transfer Learning via Prompting LLMs", "authors": ["Jaehyun Nam", "Woomin Song", "Seong Hyeon Park", "Jihoon Tack", "Sukmin Yun", "Jaehyung Kim", "Kyu Hwan Oh", "Jinwoo Shin"], "abstract": "Learning with a limited number of labeled data is a central problem in real-world applications of machine learning, as it is often expensive to obtain annotations. To deal with the scarcity of labeled data, transfer learning is a conventional approach; it suggests to learn a transferable knowledge by training a neural network from multiple other sources. In this paper, we investigate transfer learning of tabular tasks, which has been less studied and successful in the literature, compared to other domains, e.g., vision and language. This is because tables are inherently heterogeneous, i.e., they contain different columns and feature spaces, making transfer learning difficult. On the other hand, recent advances in natural language processing suggest that the label scarcity issue can be mitigated by utilizing in-context learning capability of large language models (LLMs). Inspired by this and the fact that LLMs can also process tables within a unified language space, we ask whether LLMs can be effective for tabular transfer learning, in particular, under the scenarios where the source and target datasets are of different format. As a positive answer, we propose a novel tabular transfer learning framework, coined Prompt to Transfer (P2T), that utilizes unlabeled (or heterogeneous) source data with LLMs. Specifically, P2T identifies a column feature in a source dataset that is strongly correlated with a target task feature to create examples relevant to the target task, thus creating pseudo-demonstrations for prompts. Experimental results demonstrate that P2T outperforms previous methods on various tabular learning benchmarks, showing good promise for the important, yet underexplored tabular transfer learning problem.", "sections": [{"title": "Introduction", "content": "Learning with a limited number of labeled samples is often a critical requirement in real-world machine learning applications. This limited data problem is particularly important in the tabular domain; tabular datasets often require substantial annotation efforts (e.g., credit risk assessment; Clements et al., 2020), or it is hard to obtain new samples for emerging tasks (e.g., identifying patients with new diseases such as COVID-19; Peplow, 2016; Zhou et al., 2020). To address this problem, various methods have been thoroughly studied in domains such as vision (Assran et al., 2021; Pham et al., 2021) and language (Chen et al., 2021; Min et al., 2022). However, the research on tabular data has only recently begun to gain traction (Yoon et al., 2020; Nam et al., 2023), despite its wide-ranging impact across a variety of industries (Guo et al., 2017; Ulmer et al., 2020; Zhang et al., 2020).\nLearning transferable knowledge by training a neural network from various sources (Chen et al., 2020; Perez et al., 2021; Lee & Shin, 2022) is a common way to address this limited data problem in other domains. However, such transfer learning is challenging in the tabular domain, because the source and target datasets are often very heterogeneous (i.e., they have different columns and feature spaces; Zhu et al., 2023; Wang & Sun, 2022; Yan et al., 2024). For example, using the collected features to predict diabetes to predict whether another"}, {"title": "Related work", "content": "Tabular transfer learning. Researchers have developed a number of methods to train transferable representations for tabular data. One major stream of work exploits unlabeled datasets. In this context, Yoon et al. (2020) and Ucar et al. (2021) introduced various pre-text task losses (e.g., reconstruction loss) with the goal of self-supervised learning on tabular datasets. In addition, Nam et al. (2023) proposed an unsupervised meta-learning framework to address few-shot tabular learning problems. Another stream of work exploits multiple heterogeneous datasets. Zhu et al. (2023) used a federated learning approach with separate encoders for each dataset to aggregate the parameters into a single transformer. Wang & Sun (2022) and Yan et al. (2024) used tokenizers to map the heterogeneous structure of tables into a unified language space and then fine-tuned the language model. We propose a transfer learning framework that leverages both unlabeled and heterogeneous sources while applying in-context transfer to enable immediate predictions (i.e., training-free).\nTabular learning with large language models. Recent advances in LLMs have provided an impetus to explore their potential for tabular learning. Dinh et al. (2022) investigated the performance of fine-tuned GPT-3 models (Brown et al., 2020) on tabular data. Extending this line of research, Hegselmann et al. (2023) conducted a comprehensive analysis using the TO model (Sanh et al., 2022), leveraging the language prior in LLMs. Their analysis is extended to sample efficiency, even conducting zero-shot experiments. Recently, Manikandan et al. (2023) developed a boosting framework over prompting LLMs that uses LLMs as weak learners in tabular prediction tasks. Inspired by these preceding studies, our work proposes a method for the effective exploitation of various transfer sources - an aspect overlooked in prior research. By integrating the utilization of unlabeled (or heterogeneous) data with LLMs, we aim to enhance performance in transfer learning scenarios.\nIn-context learning. As model and dataset sizes increase (Brown et al., 2020), LLMs have exhibited the capability for ICL (or prompting), where they draw knowledge from a handful of contextual examples. For example, Wei et al. (2022) have illustrated the competency of LLMs in solving mathematical reasoning problems via ICL. The ICL process begins by employing a small number of examples to establish a contextual framework, typically constructed using natural language templates. Following this, a query question and a contextual demonstration are combined to form a prompt, which is subsequently fed to the LLMs for prediction. Notably, ICL does not necessitate parameter updates and directly carries out predictions using LLMs, enabling easy implementation for real-world applications. In our work, we delve deeper into the potential of ICL by examining its performance on transfer learning, using source data for creating effective demonstrations."}, {"title": "P2T: Prompt to Transfer", "content": "In this section, we propose a simple yet effective tabular transfer learning that leverages ICL, the capability of LLMs that adapt to a new task using the context provided in a prompt without updating model parameters. In a nutshell, our framework creates pseudo-demonstrations from transfer sources that act as proxies for causal relationships between input features and labels, and then prompt the LLM to make predictions. We first briefly describe preliminaries (Section 3.1), and then the core component coined Prompt to Transfer (P2T), which creates effective pseudo-demonstrations for ICL (Section 3.2)."}, {"title": "Preliminaries", "content": "Problem setup: tabular transfer learning. We first describe the problem setup of our interest. A labeled target dataset \\(D^t = \\{(x, y)\\}_{i=1}^{N_t} \\subseteq \\mathcal{X} \\times \\mathcal{Y}\\) and a source dataset \\(D^s = \\{x^s\\}_{i=1}^{N_s} \\subseteq \\mathcal{X}_s\\) are given, where \\(x^t\\) is \\(d_t\\)-dimensional, and \\(x^s\\) is \\(d_s\\)-dimensional feature which correspond to the value in the respective table columns. Also, we assume that column name sets \\(F = \\{f_1,\\ldots, f_d\\}\\) are given as well (e.g., 'x\u2081 : Male' is feature values of the 'f1 : sex' column), and \\(f_{target}\\) is the column name of the labels y of the target dataset. Labels \\(y \\in \\mathcal{Y}\\) are provided in the form of natural language annotations (e.g., 'Non Diabetic' and"}, {"title": "In-context tabular transfer learning with P2T", "content": "We now present P2T, a novel approach to improve tabular prediction performance by creating additional pseudo-demonstrations from the source data, which serve as proxies for few-shot labeled demonstrations. This is achieved by (i) first identifying the column feature in the source dataset that exhibits the highest correlation with the target \\(f_{target}\\) and (ii) then constructing pseudo-demonstrations from the transfer source. The constructed prompt \\(P_{P2T}\\), which integrates pseudo-demonstrations with conventional prompting, is then fed into the LLM M to generate the desired prediction output.\nCorrelation identification. P2T begins by identifying the feature \\(f_k \\in F^s\\) (the column name set of the source data) that holds the most significant correlation with the task column \\(f_{target}\\) in the target data \\(D^t\\). To achieve this, we ask LLM M which feature among \\(F^s\\) is most important for predicting \\(f_{target}\\). Formally, an in-put prompt designed to identify correlations consists of two main components: (i) few-shot labeled demonstrations \\(P_{label}(D^t)\\) and (ii) identi-fication instructions \\(P_{cor}(F^s, f_{target})\\); see Figure 2. Finally, we prompt LLM M to obtain \\(f_k\\):\n\\(f_k = M(P_{label}(D^t) \\oplus P_{cor}(F^s, f_{target}))\\).\nOur choice of using LLMs over conventional methods (Chen & Guestrin, 2016; Prokhorenkova et al., 2018) is driven by the capabilities of LLMs to interpret linguistic context; their ability to leverage both the semantics of column names and the associated numeric values offers a distinct advantage. For instance, considering a column like 'Age,' conventional algorithms would focus solely on the numeric values. Conversely, LLMs can utilize the semantic understanding of \u2018Age,' incorporating additional contextual information.\nPseudo-demonstrations from the source tables. Our main idea is to create pseudo-demonstrations from the unlabeled (or heterogeneous) source tables using highly correlated column feature \\(f_k\\) as a new target feature. The rationale behind using \\(f_k\\) stems from the intuition that predicting the most correlated column feature from the remaining features would resemble the original task of predicting the label from all the features; therefore, these pseudo-demonstrations from source data include the useful knowledge to predict the original target task. For instance, predicting \u2018Diabetes' from \u2018BMI' and 'Age' is similar to predicting 'Insulin' using the same features (Nam et al., 2023). Thus, we create pseudo-demonstrations that predict the value of \\(f_k\\) from remaining features, which we refer to as"}, {"title": "Experiments", "content": "In this section, we validate the effectiveness of our proposed method for transfer learning scenarios on a variety of tabular datasets from the OpenML repositories (Vanschoren et al., 2014) and Kaggle. First, in Section 4.1, we verify that P2T improves zero-shot prediction performance by leveraging different types of transfer sources (i.e., unlabeled and heterogeneous datasets). Note that zero-shot prediction is one of the big benefits of using LLMs that is not possible with traditional methods like CatBoost (Prokhorenkova et al., 2018). Then, in Section 4.2, we verify that our method outperforms other tabular learning methods, including unsupervised meta-learning methods (Nam et al., 2023), in the few-shot learning scenario by extracting effective knowledge from the transfer sources via prompting. Finally, in Section 4.3, we validate the effectiveness of our proposed pseudo-demonstration and that better performance can be achieved with a more advanced LLM (i.e., GPT-4; OpenAI, 2023).\nCommon setup and baselines. When using unlabeled data as the training source, we use 20% of the labeled dataset for test samples and convert the remaining 80% to unlabeled, except for a limited number of labeled samples. Following Nam et al. (2023), we one-hot encode categorical features for the baseline and then min-max scaling. To validate P2T, we consider supervised learning baselines such as CatBoost (Prokhorenkova et al., 2018), logistic regression (LR), and nearest neighbor classifier (kNN) that do not utilize unlabeled (or heterogeneous) data. We also consider VIME (Yoon et al., 2020), a self-supervised learning baseline where the model is initially pre-trained and then evaluated using labeled samples via logistic regression. Note that we intentionally exclude methods that require careful hyperparameter tuning, e.g., MT (Tarvainen & Valpola, 2017), MPL (Pham et al., 2021), ICT (Verma et al., 2022), due to sensitivity to hyperparameters and overfitting issues; our problem settings characterized by limited labeled data are not suitable for hyperparameter tuning in real-world scenarios due to the lack of labeled validation sets. We also consider"}, {"title": "Zero-shot prediction", "content": "One of the distinct advantages of using LLMs is that they can easily obtain the desired answer in a zero-shot manner. In fact, Hegselmann et al. (2023) have shown that LLMs can perform zero-shot prediction tasks even in tabular domains. In this section, we investigate whether the proposed P2T framework can improve the performance of zero-shot prediction by transferring knowledge from unlabeled and heterogeneous datasets, respectively. We emphasize that P2T can be seen as a zero-shot transfer module because it is based on the ICL.\nAs shown in Table 1, using a transfer source improves the zero-shot prediction performance. We first found that LLM (GPT-3.5 in our case) often fails to predict with zero-shot. For example, on the Credit-g dataset, which is a binary classification, LLM scores behind random guessing (see Table 1). However, using heterogeneous sources (i.e., Credit-A) or unlabeled samples of the Credit-g dataset, P2T improves zero-shot prediction accuracy by 16.0% and 24.0%, respectively (note that we randomly select 30 samples from the transfer sources due"}, {"title": "Few-shot prediction", "content": "For few-shot tabular prediction, we evaluate the performance when one and five labeled samples are available per class, respectively. Firstly, we found that taking few-shot labeled tabular data, serializing it into text and prompting it to LLMs (i.e., LIFT-ICL in Table 2) outperforms existing state-of-the-art methods for few-shot tabular prediction (e.g., STUNT; Nam et al., 2023). Thus, using LLMs as prediction models is indeed a promising direction for tabular learning, and the P2T further extends this line of work by using LLMs as transfer modules via prompting. In this section, we provide few-shot tabular prediction results using (i) unlabeled data and (ii) heterogeneous data as the transfer source, respectively.\nTransfer learning utilizing unlabeled dataset. As demonstrated in Table 2, P2T significantly and consistently improves the few-shot prediction performance on 12 tabular datasets by utilizing unlabeled data of the same dataset as transfer source (here, we use 30 unlabeled samples that are closest to each labeled sample in terms of Euclidean distance). Note that this improvement is achieved without model updates. To provide a specific example, P2T sig-nificantly outperforms LIFT-ICL in 1-shot classification, raising the average performance from 58.10% to 64.92%. Additionally, P2T consistently achieves superior results, yielding the highest score in all 12 datasets in the 1-shot classification problem, and in 11 out of the 12 datasets in the 5-shot scenario. These results represent an improvement of approximately 6.8% and 6.6% over the best performing baselines, respectively. The significant improvement is achieved by constructing effective pseudo-demonstrations from the unlabeled dataset, therefore extracting valuable information in an in-context manner.\nTransfer learning utilizing heterogeneous dataset. We next demonstrate the effect of introducing training samples from heterogeneous data sources. First, we found that merg-ing distinct column sets from diverse sources in the tabular domain demands a heuristic process to create a unified feature set. Such an approach may not generalize well, and"}, {"title": "Ablation studies", "content": "In this section, we perform further analysis of the proposed P2T framework. (i) First, we use the column feature in the source dataset that is most relevant to the target task label as useful target for creating pseudo-demonstrations. Here, we ask whether the created pseudo-demonstrations are really similar to the real task, i.e., if we create pseudo-demonstrations with an arbitrary column feature of the source data as a target, the created demonstrations may not be similar to the real task and hence the LLM may not convey knowledge well. (ii) Second, a variety of LLMs have emerged in recent years. This raises the question of whether P2T can achieve better performance with more advanced models.\nEffectiveness of identifying target for pseudo-demonstration construction. In Figure 3, we validate the effectiveness of using the identified column feature as an useful target for creating pseudo-demonstrations. Here, we consider a transfer learning scenario utiliz-ing an unlabeled dataset for 1-shot prediction. As shown in Figure 3, using the identified target that is highly correlated with the target task consistently outperforms us-ing random targets (e.g., using the 'Age' column as the target for the pseudo-demonstrations in Figure 1). Interestingly, using randomly se-lected column features as targets can perform worse than not leveraging unlabeled data (see the Haberman dataset results in Figure 3). This is because, for example, in the Haberman dataset, 'patient's year of operation' may not have any correlation to the target task 'patient's survival status.' This highlights that carefully constructing pseudo-demonstrations designed to be highly relevant to the target task is a key factor in enabling transfer learning via prompting.\nQualitative analysis on selected column. In this part, we use the Breast dataset as an example to qualitatively analyze which column feature is most relevant to breast cancer. For the Breast dataset, 'deg-malig' is selected as the most relevant, making it a useful target when creating pseudo-demonstrations. On the other hand, the other feature \u2018tumor-size' is"}, {"title": "Robustness to missing values", "content": "In practice, tabular data often contains missing values for various reasons. For instance, biopsy results may not be collected for all patients due to the risks and complications involved in the data collection process (Yoon et al., 2018). Conventionally, missing values are managed using imputation algorithms (Yoon et al., 2018;"}, {"title": "P2T without language descriptions", "content": "Indeed, LLMs require explicit column descriptions to effectively exploit the language prior, but infor-mative descriptions are often absent in tabular datasets (Asuncion & Newman, 2007). Con-sequently, researchers are forced to use the generic indicator, a prompt that is used to sub-stitute (or pretend as) actual column names, for instance, 'feature' (Dinh et al., 2022). In Figure 4, we present the 1-shot test results of generic model where the language descrip-tions are removed. Specifically, we employ generic indicator \u2018feature' and 'output y' for the column names. Even in this case, lever-aging unlabeled data with P2T significantly improves performance, demonstrating P2T's potential to handle all types of tabular data."}, {"title": "Conclusion", "content": "In this paper, we introduce P2T, a novel framework for transfer tabular learning. Our approach utilizes prompting to effectively transfer knowledge from the transfer source. Through extensive experiments, we show the efficacy of P2T across various datasets. It is noteworthy that P2T does not necessitate expert-level knowledge in machine learning. This allows practitioners to adopt our framework effortlessly. We hope that our work holds promising potential in broadening the use of LLMs for tabular learning, particularly in conjunction with readily available resources such as unlabeled data."}, {"title": "Baseline details", "content": "In this section, we provide brief explanations of the chosen baselines. For CatBoost (Prokhorenkova et al., 2018) and logistic regression, we employ the default hyperparameters as provided by the CatBoost library and the Scikit-learn library, respectively. For VIME (Yoon et al., 2020) pre-training, we adopt the hyperparameters recommended in the original paper, utilizing the Adam optimizer with a learning rate of 1e \u2013 3 and weight decay of 1e \u2013 4. When implementing STUNT (Nam et al., 2023), we follow the unsupervised validation scheme proposed in the original paper for hyperparameter search and early stopping. For LIFT-ICL (Dinh et al., 2022), we provide prompt example used on the Customers dataset in Listing 1."}, {"title": "Prompt examples used in P2T", "content": "In this section, we provide examples of prompts used in P2T, specifically on the Customers dataset in scenario of transferring knowledge from the unlabeled dataset. In particular, we illustrate the prompts employed to identify the column feature with the highest correlation (see Listing 2), along with the prompts used during the final inference stage (see Listing 3). For the sake of brevity and due to constraints on paper length, the prompts we provide consist of merely two unlabeled samples along with a single labeled sample per class. We also provide prompt used for zero-shot prediction (see Table 1) on the Adult dataset, where the source dataset is Electricity, in Listing 4. Note that prompt used in Table 3 just requires additional labeled samples of the target dataset."}, {"title": "Future work and limitations", "content": "Despite its superiority in transfer tabular learning, our method is constrained by the prompt size limit of LLMs. For example, tabular data with a large number of columns may not fully benefit from P2T, as the result of serialized tabular data can be excessively lengthy. Nonetheless, we hope that future advances in LLMs, GPT-4 (OpenAI, 2023) for instance, will mitigate this issue by accommodating larger prompt sizes. Another potential strategy could involve utilizing LLMs to subsample critical features, thereby enabling more efficient integration of serialized tables into LLMs. Building on this, we hope that our work will inspire researchers to further investigate the relatively under-explored problems associated with tabular learning via LLMs."}, {"title": "Broader impacts", "content": "Tabular data often include privacy-sensitive or confidential features, such as social security numbers. As such, it is crucial to handle this data with care. However, P2T is also effective for managing anonymized features. For instance, our experiments indicate that even when categorical features are replaced with random alphabetical symbols, and generic indicators are used instead of actual column names, P2T still shows competitive performance (see Figure 4). Therefore, despite potential privacy concerns related to tabular classification, P2T shows promise for widespread use alongside privacy-preserving techniques."}]}