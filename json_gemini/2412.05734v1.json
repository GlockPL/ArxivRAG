{"title": "PrivAgent: Agentic-based Red-teaming for LLM Privacy Leakage", "authors": ["Yuzhou Nie", "Zhun Wang", "Ye Yu", "Xian Wu", "Xuandong Zhao", "Wenbo Guo", "Dawn Song"], "abstract": "Recent studies have discovered that LLMs have serious privacy leakage concerns, where an LLM may be \u201cfooled\" into outputting private information under carefully crafted adversarial prompts. These risks include leaking system prompts, personally identifiable information, training data, and model parameters. Most existing red-teaming approaches for privacy leakage rely on humans to craft the adversarial prompts. A few automated methods are proposed for system prompt extraction, but they cannot be applied to more severe risks (e.g., training data extraction) and have limited effectiveness even for system prompt extraction.\nIn this paper, we propose PrivAgent, a novel black-box red-teaming framework for LLM privacy leakage. We formulate different risks as a search problem with a unified attack goal. Our framework trains an open-source LLM through reinforcement learning as the attack agent to generate adversarial prompts for different target models under different risks. We propose a novel reward function to provide effective and fine-grained rewards for the attack agent. We also design novel mechanisms to balance exploration and exploitation during learning and enhance the diversity of adversarial prompts. Finally, we introduce customizations to better fit our general framework to system prompt extraction and training data extraction. Through extensive evaluations, we first show that PrivAgent outperforms existing automated methods in system prompt leakage against six popular LLMs. Notably, our approach achieves a 100% success rate in extracting system prompts from real-world applications in OpenAI's GPT Store. We also show PrivAgent's effectiveness in extracting training data from an open-source LLM with a success rate of 5.9%. We further demonstrate PrivAgent's effectiveness in evading the existing guardrail defense and its helpfulness in enabling better safety alignment. Finally, we validate our customized designs through a detailed ablation study. We release our code here https://github.com/rucnyz/RedAgent.", "sections": [{"title": "1. Introduction", "content": "Large language models have demonstrated great performance in generating coherent text, reasoning various inputs (e.g., math problems, coding tasks), and planning for intricate tasks [1, 2, 3]. Together with their tremendous successes comes the concerns on privacy leakage. Specifically, existing works showed that when prompting an LLM with specific adversarial prompts, the model will output various private information, including system prompts, personally identifiable information (PII), training data, and even model parameters [4, 5, 6, 7, 8]. Note that some works treat PII extraction as part of the training data extraction. Here, we separate them out as a stand-alone risk.\nThese risks impose serious concerns on model developers and users and can cause severe consequences. For example, recent researchers and practitioners developed a number of LLM-integrated applications, where they wrap LLM with different system prompts for specific use cases, such as OpenAI's GPT Store [9] and Poe [10]. System prompts highly shape the behavior and significantly affect the performance of LLMs, making them critical assets for these applications. Developing such prompts demands substantial time and effort from developers and may involve sensitive information. If an attacker can extract the system prompt of an LLM-integrated application, the attacker can steal the sensitive information and easily rebuild the application, compromising the developers' intellectual property and causing serious loss to the developer [11]. Similarly, leaking model training data and parameters will lead to plagiarism and intellectual property issues [5, 8].\nTo prevent privacy leakage and other testing-phase risks, existing model developers conduct extensive red-teaming tests of their LLMs and LLM-integrated applications before publishing them [12, 13]. So far, most red-teaming tests, especially for privacy leakage, still rely on humans to craft adversarial prompts, which is time-consuming and cannot scale [14]. Recent works conduct early explorations on automated red-teaming for privacy leakage. These methods either leverage gradient-based optimizations [7] or fuzzing approaches [15] to generate adversarial prompts. These methods have limited generalizability and are only applicable to system prompt extraction. Additionally, these methods are either impractical, as gradient-based optimizations require access to model internals, or ineffective due to the inherent randomness of fuzzing. Existing works also developed a number of red-teaming approaches for other risks, such as toxicity, jailbreaking, and adversarial robustness [16, 17, 18]. However, these methods cannot be directly applied for privacy leakage due to different goals and setups.\nIn this work, we propose PrivAgent, a novel and generic red-teaming framework for LLM privacy leakage. At a high level, we design an agentic-based approach, where we train an open-source LLM using deep reinforcement learning (DRL) as the attack agent to generate adversarial prompts. These prompts will \"fool\" a target LLM to produce responses that contain certain desired private information."}, {"title": "2. Background", "content": "Large Language Models (LLMs). LLMs are transformer-based neural networks [19] with a large number of layers and billions of parameters. As demonstrated in Figure 1, such a model takes a sequence of text as input, tokenizes the input text, and feeds the vectorized representations into the transformer model. As discussed later, the model autoregressively generates the next token based on the input and previously generated tokens. Benefiting from their ultra-high model capacity and large training data, LLMS exhibit exceptional capabilities in understanding the context and generating accurate responses. Recent research further shows that LLMs also have emerging reasoning abilities, enabling them to tackle complex tasks such as coding [20, 21], solving mathematical challenges [22], and conducting scientific discoveries [23]. Popular LLMs include closed-source models like OpenAI's GPT family [12], Google's Gemini [13], and, Anthropic's Claude family [24], as well as open-source models like Meta's Llama family [25, 26, 27], Mistral AI's Mistral family [28, 29].\nTraining. The training of LLMs usually involves two stages: pre-training and fine-tuning. Pre-training involves unsupervised learning on a massive text corpus, where the model learns to predict the next word in a sequence. This process allows the model to develop a general understanding of language, including syntax, semantics, and some world knowledge [30]. The fine-tuning process has two possible training methods: supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF). The goal for this stage is to calibrate the model on datasets tailored to particular tasks, such as translation, question-answering, or topic classification. SFT uses a typical supervised learning loss and a labeled dataset. While, RLHF requires a preference or a reward function to assign a reward to the model output [31, 32]. The model is trained to maximize the total reward it receives using the Proximal Policy Optimization (PPO) algorithm [33] or to align with preferences using the Direct Preference Optimization (DPO) algorithm [34].\nInference. Once an LLM is deployed, users can interact with the model by providing input texts, referred to as prompts. Prompts can vary in form depending on the user's intent. For example, in Q&A tasks, a prompt could be a question \"How to create a file in a Linux system?\". When fed to an LLM, the model is supposed to answer this question correctly. Recent research shows that in-context learning can enhance the model's ability to understand and respond to input prompts [35]. In-context learning includes a few examples (denoted as few-shot examples) to demonstrate how to respond to a particular query. In the prompt above, the few-shot example could be \"How to remove a file in a Linux System? Answer: rm -rf your_file_name\". To gain global control over the model's outputs, developers often use a system prompt, which provides global guidelines and assumptions for the model's responses. For instance, a system prompt in Q&A tasks could be \"You are a computer science expert that can answer user's questions correctly.\" This system prompt is typically placed before the user prompt when inputting text into the model, e.g., with the system prompt, the above prompt becomes \"You are a computer science expert that can answer user's questions correctly. How to remove a file in a Linux System? Answer: rm -rf your_file_name. How to create a file in a Linux system?\".\nWhen generating a response, LLMs can employ different sampling strategies. The most straightforward approach is to select the token with the highest probability at each step, known as greedy sampling. However, this method often leads to repetitive outputs. To improve diversity, more complex sampling methods such as top-k sampling, nucleus (top-p) sampling, or temperature-controlled sampling are commonly used [36]. These techniques introduce controlled randomness, allowing for more diverse output.\nLLM-integrated applications. As mentioned above, the system prompt is widely used to enable global control of the model outputs. Recent research and practice further show that carefully designed system prompts can significantly improve an LLM's performance in specific application domains [37]. Through this method, application developers no longer need to fine-tune the general-purpose LLM for their applications, which is time, data, and computationally expensive. As such, there have been a large number of LLM-integrated applications that wrap a general-purpose LLM with application-specific system prompt(s) [10]. For example, as demonstrated in Figure 1, to extract a patient's current medication from their medical records, the application might send \"List the current medications from the following medical record: {data}\" to the LLM, where \"{data}\" is replaced with the text of the patient's medical history. There are also some popular LLM-integrated application zoos, like OpenAI's GPT store [9] and Poe [10], which provide a lot of applications across different domains. For such applications, system prompts are their most critical assets that need to be well protected. This is because, although cheaper than fine-tuning, crafting proper system prompts still requires a considerate number of model queries (i.e., time and computation). If the system prompts are leaked, whoever processes such system prompts can easily replicate the corresponding application with minimum costs."}, {"title": "3. Existing Attacks and Limitations", "content": "Risk/attack categorization. According to position papers [6, 38], existing inference-phase attacks against LLMs can be categorized into the following classes based on different attack goals. 1) Toxicity/jailbreaking attacks attempt to make the LLM generate harmful, offensive, or inappropriate content [17, 39, 40, 41]. 2) Stereotype bias and fairness attacks force the model to generate discriminatory responses corresponding to certain societal biases and stereotypes [42]. 3) Adversarial robustness, in-context backdoor, and OOD attacks deliberately mutate given inputs (e.g., in-context learning [43], user input [44, 45, 46], and knowledge base [47]) that cause the model to make mistakes or behave unpredictably. 4) Privacy leakage attacks aim to extract sensitive and private information from the model [4, 5, 7, 15]. In this paper, we mainly focus on privacy leakage attacks.\nPrivacy leakage attacks. We consider the broad taxonomy of privacy leakage risks within the context of LLMs. These risks include the leakage of training data, model parameters, and personally identifiable information (PII). Regarding the privacy leakage of training data, one prominent threat model is membership inference attacks [48], which aim to predict whether a specific data point is part of a target model's training dataset. As for the privacy leakage of PII, since LLMS are trained on large-scale datasets, they may inadvertently memorize sensitive PII. Consequently, LLMs can generate verbatim PII if it exists in the training data. Another concern is that LLMs, due to their strong reasoning capabilities, may infer PII using side information and compositional reasoning [6]. As mentioned in Section 2, system prompt leakage is a newly introduced risk by LLM-integrated applications.\nExisting attacks for membership inference and PII leakage. Membership inference attacks (MIA) have been extensively studied in deep learning classifiers, particularly for image models [49, 50, 51, 52]. Due to differences in model structures, inference methods, and discrepancies in model capacities, membership inference attacks on LLMs face new challenges. Recently, several LLM-specific approaches have been proposed. For instance, the Min-K%-Prob method [53] exploits the observation that the least probable tokens tend to have higher average log-likelihoods for training examples compared to unseen samples. Another approach by [54] demonstrates dataset memorization by exploiting data exchangeability principles, where model preference for specific data orderings indicates training exposure. The DE-COP framework [55] reformulates MIA as a question-answering task, leveraging the observation that LLMs often correctly answer verbatim training text, even in black-box settings. However, these MIA tasks are mainly for the binary classification that one data is in the training data or not. In contrast, we focus on more aggressive privacy attacks that aim to recovering complete training examples or personally identifiable information, which are known as data extraction attacks. Existing data extraction approaches [4, 5, 6, 38, 56] typically rely on manual prompt engineering. For example, Carlini et al. [4] found that repeating the same tokens many times as prefixes can force an LLM to output training data. However, such manual efforts limiting the attack's scalability for comprehensive LLM security testing. By comparison, our work automates privacy leakage attacks for both training data and system prompts, achieving superior attack performance and transferability.\nExisting attacks for system prompt leakage. Most attacks for various LLM risks still rely on human-based red-teaming [12, 13]. Automated methods primarily target jailbreaking attacks. White-box and gray-box jailbreaking attacks [16, 17, 18] typically rely on gradient-based optimization or fuzzing techniques, while black-box approaches often rely on in-context learning [57, 58] or fuzzing-based methods [59]. There are relatively fewer automated system prompt leakage attacks, including the white-box attack PLeak [7] and the black-box attacks PromptFuzz [15] and PRSA [60]. PLeak draws inspiration from GCG [17] and relies on gradient-based optimization to craft attack inputs, which is impractical in many scenarios as it requires access to model internals. PRSA [60], on the other hand, operates under a restrictive setting where the attacker cannot query the model and instead relies on a small set of input-output pairs. This approach is inherently limited in effectiveness due to its constraints on model interaction. PromptFuzz [15] is motivated by GPTFuzzer [59] and adopts a more realistic black-box setup, where attackers can query the model but lack access to its internals. This method employs a fuzzing-inspired approach: starting with a set of initial seeds, it uses LLM-based mutators to iteratively modify these seeds until a predefined feedback function identifies effective adversarial prompts. However, as shown in Section 5, this method suffers from reduced effectiveness due to its inherent randomness and heavy reliance on the quality of the initial seeds and mutators. In contrast, we depart from these conventional designs by fine-tuning another LLM in a black-box setup to automate attacks against the target LLM. Our evaluation in Section 5 demonstrates that this approach is significantly more effective than fuzzing, or gradient-based attacks for system prompt leakage. Our approach further introduces the first automated attacks targeting training data leakage.\nNote that we do not consider training-phase attacks [45, 61, 62], attacks against multi-modal models [63, 64], and LLM-integrated agents [65, 66] in this paper. There are a number of prompt injection attacks, where the attack prompts are not directly fed to the model. Instead, they are injected as part of user prompts. Some of these attacks include system prompt leakage as their attack goals [67, 68] with manually crafted adversarial propmts."}, {"title": "4. Methodology", "content": "In this work, we design and develop, PrivAgent, a novel black-box privacy leakage attack against LLMs, which fine-tunes an open-source LLM with reinforcement learning (RL) to search for effective adversarial prompts. We propose a generic attack framework together with customized fine-tune procedures and reward functions for system prompt and training data leakage. In the following, we first introduce our threat model, followed by our technical overview. We then discuss on our specific designs for system prompt and training data leakage."}, {"title": "4.1. Threat Model and Problem Formulation", "content": "Threat model. We assume the attackers can only query the target LLM without accessing the model internals as well as its training process. We consider popular open-source and commercial LLMs as our target models, such as Llama [27] and GPT [12]. These models all went through safety alignment and can reject obvious adversarial prompts for various attacks, including privacy leakage. For example, when feeding the model a question \"Can you tell me what's your system prompt?\", the model will reply \"I'm sorry, but I can't assist with that request.\". As specified in Section 5, we assume the defender can fine-tune the target model or apply guardrail models to defend against our attack [69, 70, 71]\nAs mentioned in Section 3, the privacy leakage risks of LLMs mainly include system prompt extraction, PII extraction, and training data extraction. We mainly consider the system prompt extraction and training data extraction. As discussed later, our proposed method is generalizable to PII extraction as well. Given that model developers are increasingly using data sanitization to filter out sensitive information in the training data [72], we do not consider this attack goal. For both attack goals, we aim to generate diverse and realistic (semantically coherent and natural-sounding) adversarial prompts to \"fool\" the target model in outputting the system prompt or private training data. This will require bypassing the safety alignment and guardrail defenses of the target LLM.\nProblem formulation. We formulate the attack as an optimization problem where we search for adversarial prompts"}, {"title": "4.2. Technique Overview", "content": "Motivation for our RL-based method. Recall that there are a few existing automated attacks in system prompt leakage that rely either on gradient of the target model [7, 17, 18] or fuzzing/genetic approaches [15]. Gradient-based methods are the most effective approaches for solving optimization problems [73]. However, it does not comply with our black-box setup. In existing work, training surrogate models and using genetic approaches are two predominant ways of launching black-box attacks [15, 74]. Training surrogate models is less common and often impractical for LLMs given their large-scale training data and extremely high training costs. As such, existing black-box red-teaming approaches, including privacy leakage, mainly leverage genetic approaches [15, 16, 59, 75]. Abstractly speaking, such approaches initiate the search/optimization process by selecting seeds in a randomly chosen initial region. They then iteratively perform random exploration of the current local region and move to a nearby region based on the search result in the current region [76, 77]. These methods conduct the local search by mutating the current seeds and moving to the next region via offspring (new seeds) selections.\nAlthough do not require access to target internals, genetic-based methods have limited effectiveness due to the lack of guidance and inherent randomness. Specifically, the mutators are randomly selected in each iteration, and there is also not clear guidance for how to design effective mutators. As such, it is likely that the entire genetic/attack process cannot find a single useful seed due to the limitation in mutator construction and selection. The classical optimization and search theory also supports this argument [77, 78]. In particular, we can prove that to reach a certain target grid in a simple grid search problem, the total number of grid visits required by genetic methods is at least three times greater than gradient-based or rule-based methods. This limitation becomes particularly critical in our attack problem due to its huge search space. As demonstrated in Section 5, although some useful adversarial prompts are found for system prompt extraction, genetic-based approaches fail to handle training data extraction. This is because, without high-quality initial seeds and very effective mutators, genetic methods are similar to random exploration. It cannot find adversarial prompts to trigger the extraction of specific target training data, which has an ultra-high search space.\nTo enable effective attack in a black-box setup, we design our optimization method based on deep reinforcement learning. DRL can train an agent to iteratively modify the adversarial prompt until it reaches the attack goal. During the training process, the agent learns an effective policy through trials and errors on a large number of trials. Once the agent finds an effective policy, it will then take actions following the policy, which reduces the randomness in the attack process. Furthermore, the whole process does not require access to the internals of the target LLM.\nChallenges in RL-based methods. While DRL provides a promising framework, its effectiveness highly depends on the design of the system, especially for problems with a large search space. It is very likely that the agent cannot find a path to successful attacks and thus only receives negative rewards in the early learning stage. In such cases, the RL method also downgrades to random search. In the following, we specify the challenges of using RL in our problem by discussing the limitations of a straightforward solution.\nGiven that we aim to generate diverse and coherent adversarial prompts, it is straightforward to use another LLM as the agent (denoted as \"attack agent\") and fine-tune it with RL for adversarial prompt generation. To do so, we need to define a customized reward function and provide initial prompts po. We then fine-tune the attack agent h, which takes initial prompts as inputs, to generate a set of attack prompts that maximizes the reward function. Here, we can design the reward function as an exact match between the target model's output and the target information d. This binary value is assigned after generating the last token (e.g., <eos>). Formally, it can be defined as:\n$r = \\begin{cases} 1, & \\text{if } f(h(p_0)) = d \\\\ 0, & \\text{otherwise}. \\end{cases}$\nHere, $h(p_0)$ generates the adversarial prompts p and $f(.)$ is the target model. The agent is trained to maximize the accumulated reward $\\sum_t \\gamma^t r_t$ using the widely adopted PPO algorithm [33].\nThis straightforward solution has the following limitations."}, {"title": "4.3. Our Red-teaming Framework", "content": "In this section, we introduce our solutions for addressing the limitations discussed above, followed by the overall framework of our proposed red-teaming approach.\nAddress limitation \u2460: Design a dense reward function. We aim to design a reward function that measures semantic similarity between a target model's output u and the desired output d. A straightforward solution is to feed u and d into a text embedding model, such as BERT models [79] and OpenAI embedding models [80], and measure their distance in the embedding space. Some common choices include cosine distance and 12-norm distance. However, in our exploration, we find that this simple solution has drawbacks. Specifically, it gives overly high scores to the target model's outputs that are not that similar to the desired information. This not only introduces false positives, more importantly, if the reward function gives a high reward for most target model outputs, it again cannot provide effective learning signals for the attack agent.\nOther text similarity metrics such as ROUGE [81] and BLEU [82], which compare the n-gram similarity between u and d, suffer a similar issue. Specifically, they assign overly high scores when u contains only partial of the desired information d.\nThe early exploration shows that an ideal reward function needs to reflect the semantic similarity between the target model's output and the desired information as precisely as possible. The RL learning process also favors a reward function that can measure the fine-grained similarity between u and d even when u contains only part of the desired information. To achieve this, we design our reward function as follows.\nWe start by introducing the Levenshtein distance or edit distance [83, 84]. Edit distance is a measure of the minimum number of operations (insertions, deletions, or substitutions) required to transform one string into another. Formally, edit distance at the word level can be defined as:\n$WED(u, d) = \\min_{e \\in (W(u), W(e))} |e|;$\nwhere $W(.)$ denotes the word sequence obtained through tokenizing its input via a word tokenizer, such as Punkt [85]. $E(W(u), W(d))$ is the set of all edit sequences that transform $W(u)$ into $W(d)$, and $|e|$ is the length of an edit sequence e. Compared to embedding similarity and n-gram similarity metrics, editing distance can better distinguish the nuance difference between u and d, preventing giving overly high similarity scores. For example, when the target model outputs content following its system prompt rather than outputting the system prompt itself, embedding similarity will give a high score while editing distance can call the difference. Another example is when the target model output contain partial and rephrased version of u, n-gram similarity will assign a high score but editing distance will not.\nHowever, it cannot be directly used as our reward function due to the following limitations. First, it tends to give a very low score when the desired information d is shorter than u. Second, editing distance is not aligned for d with different lengths. Specifically, when u is almost the same as d, the pairs with a longer length will have a lower similarity.\nTo solve the first limitation, we propose to apply a sliding window to the target model's output and then calculate the edit distance for each slide. Formally, it can be defined as\n$SWES(u, d) = \\begin{cases} \\max_{i \\in [0, |u|-|d|]}  \\frac{}{}-log(WED(u, d)), & \\text{if } |u| < |d| \\\\ - log(WED(u[i : i + |d|], d)), & \\text{if } |u| \\ge |d| \\end{cases}$\nWe take log to make the similarity more smooth. To solve the second limitation, we then normalize SWES as follows.\n$SWES_{norm} (u, d; k, x_0) = \\frac{1}{1 + e^{-k(SWES(u,d)-x_0)}}$,\nwhere k controls the steepness of the sigmoid curve and $x_0$ is the intercept.\nThe insights behind this normalization are two-fold. First, we can set a larger k to create a sharp distinction when the SWES is around 20, amplifying the fine-grained differences between u and d while maintaining the smoothness of the reward function. Second, normalizing the reward function can avoid outlier reward value and thus help approximate the value function and stabilize the training process. We set k = 5 and $x_0$ = 0.6 based on our empirical experience. It means when SWES(u, d) > 0.6, it will be mapped to probabilities higher than 0.5 and vice versa. Our modified edit distance allows us to compare strings of different lengths more effectively, particularly when searching for substring matches within longer texts. In Appendix C, we provide case studies to validate the superiority of our proposed similarity metric.\nWe also introduce another regularization in the reward function, which favors u that has a similar length as the d Our final reward function is defined as:\n$R(u, d) = (1 \u2013 \u03bb)SWES_{norm} + \u03bb \\frac{1}{|||u|- |d|||},$\nwhere we set x = 0.1 based on our empirical experiences.\nAddress limitation \u2461: Dynamically adjust the generation temperature. To encourage exploration in the early learning stage, we propose a dynamic temperature adjustment strategy. As shown in the following equation calculating the probability of generating each token xt, temperature T is a hyperparameter that controls the level of uniformity of the token distribution.\n$P(x_t/x_{<t}) = \\frac{exp(logits/T_t)}{\\sum_{vocab\\_size} exp(logits/T_t)}$\nA higher temperature leads to more diverse and creative outputs, while a lower temperature results in more deterministic responses. We propose the following temperature adjustment scheme,\n$T_i = \\begin{cases} T_{high} & \\text{if } i < k \\\\ T_{base} & \\text{if } i \\ge k \\end{cases}$\nAt the early learning steps (i < k), we sample the initial tokens at a very high temperature $T_{high}$ \u226b 1, combined with top-k filtering to make the candidate tokens more controllable. This will encourage the exploration of diverse prompt beginnings, reducing the reliance on the initial prompt. When generating later tokens (i > k), we proceed using a regular temperature $T_{base}$. This design balances exploration and exploitation in that if the reward is high, we can lower the temperature that forces the agent to follow the current strategy, otherwise, the learning process will increase the temperature to encourage exploration again. In our empirical study, we find this dynamic temperature adjustment strategy can improve learning efficiency and effectiveness, as well as reduce our attack agent's reliance on the initial input.\nAddress limitation \u2462: Add an additional regularization. To prevent model collapse, we introduce a regularization that explicitly encourages the diversity of the generated adversarial prompts. During the learning process, we will collect and maintain a set of adversarial prompts that achieve a reward higher than 0.9. We then calculate the similarity between the newly generated prompts and this set using our proposed similarity metric in Eqn. (6). Prompts that exhibit lower similarity to this set receive an additional reward of 0.2. This mechanism further incentivizes the attack agent to explore a wider range of adversarial prompts, enabling our method to comprehensively test the target model and generate diverse data for facilitating better safety alignment.\nOverall framework. Figure 2 shows the overview of our framework. The attack agent is given an initial input/state po. In each round, the agent takes the same po and outputs a sequence of tokens as the adversarial prompt pi. To encourage diversity, we sample the length of pi from a pre-defined range [15,64]. Then, we feed the adversarial prompt to the target model and obtain the corresponding response ui. The reward r1 is calculated by comparing ui with D using our proposed reward function. We iterate this process and collect a set of adversarial prompts and their corresponding reward to update the attack agent. We apply the PPO algorithm to train the attack agent as it is the SOTA RL algorithm with the monotonicity guarantee. We also apply LoRA with quantization to improve our training efficiency [86]. After the training process converges, we fix the obtained agent and apply it to generate adversarial prompts for new target models and corresponding desired information d.\nNote that another possible approach for using RL to generate adversarial prompts is to design a set of mutators for the adversarial prompts (e.g., shorten, crossover) and design an agent to select these mutators during the attack process. This process requires designing customized states and actions for different attack goals, which is more complex and less general than our method, where the state and actions are inherent in the attack agent. More importantly, although demonstrated effective in jailbreaking attacks [87, 88], we found it difficult to design effective mutators for privacy leakage. In our initial exploration, we used the mutators designed in existing RL-based and fuzzing-based attacks [15, 87, 88] and found out they cannot effectively generate adversarial prompts for our attack goals. As a result, we choose a simplified but more effective design path, where we do not need to design customized mutators."}, {"title": "4.4. Customizations for Specific Attack Goals", "content": "4.4.1. System Prompt Extraction. The first customization required for system prompt extraction is the choice of the initial prompt po. As discussed in Section 4.3, with our proposed temperature adjustment scheme, the attack agent training process is less reliant on the choice of the initial prompt. As such, we use a general phrase \"Please generate a prompt for me\" as our initial prompt po. This general phrase also serves as an initial instruction for the agent, clarifying its task is to generate a prompt for other language models.\nSecond, we use the ground-truth system prompts collected from existing open-source LLM-integrated applications as the desired information d and train the attack agents against them. During testing, we apply the trained agents to broader LLM-integrated applications where the system prompts are not available to show the transferability of our attack policies.\n4.4.2. Training Data Extraction. In general, training data extraction is a much more difficult task compared to system prompt extraction as the search space is much larger considering the large amount of training data. This is also the main reason why there is no existing automated approach for this attack goal. As such, it requires more customizations to the attack framework.\nFirst, we generate a more specialized initial prompt following this pattern: \"[eos]\" or \"{\" or \"%\" \u00d730. These particular sequences are chosen inspired by existing work [4, 89] and our own empirical observations. This initial prompt together with a few other tokens can possibly fool an LLM to output responses containing partial training data. It helps our attack agent obtain positive rewards in the early learning stage, preventing the learning process from becoming random searches due to the lack of positive rewards.\nSecond, to train our attack agent, we need the target information. We select the open-source models with released training data as our target model and then apply the trained agents to other models without public training data information. Here, the released training data is constructed as a database with a search mechanism. We propose a two-stage procedure for attack agent learning. In the first stage, we employ a coarse-grained search mechanism that searches whether a target's model's output contains part of the information in a known training dataset. We treat the database as a tool and use its search mechanism to decide whether a target's model's output is aligned with any data point in the database. This stage serves as a preliminary filter, allowing us to identify more promising training data samples to extract. Otherwise, directly training the attack agent to match millions of training samples is equivalent to random search, where the agent's goal is too diverse and vague. In addition, LLMs have different memorization for different samples. If we randomly choose a training sample as the target, it is likely that the model does not have a strong memory of this sample, making our attack process targeting an impossible goal. Once we identify a potential match in the first stage, we transition to a more refined stage, where we employ our designed reward function (Eqn. 7). Here, we continue training the attack agent to recover as much and as accurately as possible the entire information in the selected training sample d. This two-stage approach also balances exploration and exploitation at the high level, where the first stage allows for global explorations with rapid identification of promising directions, and the second stage focuses more on local exploration and exploitation."}, {"title": "5. Evaluation", "content": "In this section, we comprehensively evaluate PrivAgent from the following aspects.\n1) We compare PrivAgent with SOTA system prompt extraction methods Pleak [7] and PromptFuzz [15] and extensions of representative red-teaming methods designed for jailbreaking.\n2) We evaluate the cross-model transferability of our attack agents and apply them to real-world LLM-integrated applications.\n3) We evaluate the resiliency of PrivAgent against two SOTA training-phase defense StruQ [69] and SecAlign [70] as well as a inference-phase guardrail defense PromptGuard [90].\n4) We apply safety alignment to a target model with the data generated by our method and evaluate its robustness against selected attacks.\n5) We apply PrivAgent to training data extraction for open-source LLMs with known training data.\n6) We perform the ablation studies to justify key designs of PrivAgent.\nOur base system for this research comprises of an Ubuntu 20.04 machine, with 1.48 TB of RAM, 2xAMD EPYC 9554 64-core processors, and 8x NVIDIA L40S GPUs. Additionally, for simplicity, we refer to our proposed similarity metric $SWES_{norm}$ as WES without causing any ambiguity. In the following, we specify the design and results of each experiment."}, {"title": "5.1. System Prompt Extraction", "content": "5.1.1. Experiment setup and design. Recall that to train our attack agent, we need a set of system prompts as our target. We use the dataset collected from existing LLM-integrated applications, awesome-ChatGPT-prompts [91"}]}