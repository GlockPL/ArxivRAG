{"title": "How will advanced AI systems impact democracy?", "authors": ["Christopher Summerfield", "Lisa Argyle", "Michiel Bakker", "Teddy Collins", "Esin Durmus", "Tyna Eloundou", "Jason Gabriel", "Deep Ganguli", "Kobi Hackenburg", "Gillian Hadfield", "Luke Hewitt", "Saffron Huang", "Helene Landemore", "Nahema Marchal", "Aviv Ovadya", "Ariel Procaccia", "Mathias Risse", "Bruce Schneier", "Elizabeth Seger", "Divya Siddarth", "Henrik Skaug S\u00e6tra", "MH Tessler", "Matthew Botvinick"], "abstract": "Advanced AI systems capable of generating humanlike text and multimodal content are now widely available. In this paper, we discuss the impacts that generative artificial intelligence may have on democratic processes. We consider the consequences of AI for citizens' ability to make informed choices about political representatives and issues (epistemic impacts). We ask how AI might be used to destabilise or support democratic mechanisms like elections (material impacts). Finally, we discuss whether AI will strengthen or weaken democratic principles (foundational impacts). It is widely acknowledged that new AI systems could pose significant challenges for democracy. However, it has also been argued that generative AI offers new opportunities to educate and learn from citizens, strengthen public discourse, help people find common ground, and to reimagine how democracies might work better.", "sections": [{"title": "Introduction", "content": "In 2024, half the world \u2013 including India, the US and several other of the world's richest and most populous nations \u2013 go to the polls. In the short intervening time since these voters last cast their ballot, there has been a step change in the development of advanced AI systems. Large generative models can now produce text, images, audio, and audio-visual outputs that closely resemble those produced by humans. In November 2022, OpenAI launched a publicly available large language model (LLM) on its ChatGPT website, which is now the 20th most visited internet page globally, flanked by rival systems from Google (originally Bard; now Gemini), Anthropic (Claude) and similar open-source versions. Over recent months, the impact that these powerful, publicly available AI systems may have on the political process has been widely debated in the media, often with a focus on the potential of AI to disrupt or corrode democracy. Here, we situate this discussion in a growing academic literature across both AI research and social science1-7.\nDemocracy is a system of government in which the people, rather than monarchs or oligarchs, hold political power. In modern liberal democracies this power is mainly exercised through the act of voting for political representatives, although some democracies also empower mass decision-making through referenda. Different conceptions of democracy emphasise the struggle among leaders to gain the popular vote, how interest groups and organisations seek to influence the political process or the processes by which political decisions are made (e.g., voting, negotiation, or deliberation) 10. Language plays an indispensable role in each of these conceptions of democracy. It allows information about candidates and policies to be shared with to voters, lawmakers to create legislation, and citizens and representatives to collectively discuss, deliberate and decide on which course of action to pursue. In liberal democracies, elected leaders use oratory to explain and justify their decisions and actions to the larger public, interest groups use persuasive messaging to lobby for their preferred policies, and the general public engages in debate, either informally or through organised events such as town halls and citizens' assemblies11,12. Given the primacy of linguistic exchange in the political process, the arrival of conversational machines \u2013 such as ChatGPT, which is already generating more than 100 billion words per day13 \u2013 has the potential for far-reaching impact on democracy worldwide.\nWe propose that AI creates three classes of potential challenge for democracy, but argue that each is tempered by corresponding potential opportunities. First, we consider epistemic impacts \u2013 those that impact citizens' ability to make informed choices about both representatives and policies. There is widespread concern that LLMs will spread misinformation at scale, or be used to craft highly persuasive political messages that undermine voters' ability to make autonomous decisions in their own interest. However, there is also considerable scope for AI to improve the epistemic health of our democracies, by providing voters with accurate and balanced information about political events, policies or leaders, by automating fact-checking, or helping people deliberate and find common ground over principles and issues.\nSecondly, we turn to material impacts. AI could be misused to attack the infrastructure that supports democratic procedures, for example by overwhelming electoral processes or unfairly disenfranchising voters. However, it can also be deployed to improve the efficacy of governance processes, by helping policymakers make better use of data, or providing citizens with accurate information about their rights.\nFinally, we discuss foundational impacts, by which AI may weaken or strengthen the very principles on which democracy is based, or affect its opportunity to flourish worldwide. Foundational impacts, whilst potentially mediated by epistemic or material impacts, have more diffuse, systemic, and long-lasting effect. Threats to the foundations of democracy could arise if"}, {"title": "Epistemic impacts", "content": "Even before powerful LLMs became available, algorithms were responsible for shaping the flow of information and misinformation on digital platforms19. Algorithm design has often been blamed for the erosion of public discourse on social media, and for growing polarisation and partisanship in political debate20. However, the advent of LLMs presages new challenges and opportunities for global epistemic health. We consider how democracies may be weakened by political bias in AI systems, automated persuasion, polarisation from personalised content, or the scaling misinformation; but also how they may be strengthened by AI systems that allow fact-checking, increased mutual intelligibility, deliberative upskilling, and automated tooling for political participants to find common ground."}, {"title": "Political bias", "content": "Publicly available LLMs already have wide user bases, thought to collectively exceed 100 million monthly users. If citizens are using LLMs such as ChatGPT, Gemini or Claude to obtain information about current affairs, political controversies and electoral choices, then even weak biases in their outputs could significantly impact the distribution of political beliefs in this population. Several studies have attempted to quantify the degree of LLM political bias, typically by administering multiple choice survey questions (such as the Political Compass test\u00b9) to LLMs, and measuring the relative output probability associated with each candidate answer (e.g. option A vs. option B). These studies have shown that models are broadly calibrated to the distribution of political views in their training data, so that after pre-training on large datasets, LLMs reflect a wide spectrum of opinions, encompassing both more conservative and more liberal perspectives. However, when the multiple choice approach is applied to models that have undergone certain forms of fine-tuning, designed to minimise toxic or illegal outputs in the models21, models have been reported to prefer options that tend in a more libertarian (e.g., favouring deregulation) and progressive (e.g., supporting civil rights) direction22\u20132425. However, at the same time, LLMs are highly malleable, and when prompted to play the role of characters with different political opinions and worldviews, they are quick to adopt political opinions of both Republicans and Democrats26,27. Moreover, subsequent research has revealed that this stylised evaluation method, whereby the model is forced to choose a candidate response to an issue-based question, yields results that are unrepresentative of everyday user interactions with an LLM, because multiple choice items do not offer respondents the opportunity to voice balanced or equivocal replies. In fact, when responding freely to user queries in everyday settings, models like ChatGPT typically preface replies with reminders that they do not hold political opinions, and give scrupulously balanced answers to direct enquiries about the relative merits of political representatives or policies. They also remind the user about the limits of their knowledge, and refer them to sources on the internet for the most up-to-date information. In fact, under normal usage conditions, the models may be much less opinionated than previously argued 28. Major"}, {"title": "Persuasive messaging and dialogue", "content": "During an election campaign, candidates, parties and interest groups attempt to shape voters' beliefs through advertisements, media engagement, public events, door-to-door canvassing, and other activities. Several recent studies, focussed on the US and UK electorate, have aimed to directly measure the impact of LLM-generated messaging on political attitudes through randomised controlled trials.\nFirst, studies have consistently found that LLMs are able to write messages that persuade on political issues. For example, messages crafted by GPT-3 increased support among a representative sample of US voters for a ban on smoking, or a tightening of gun control policy, by about 2-4% on average32. However, when comparing LLMs against human-written messages, research findings have been more mixed. In one study, messages generated by GPT-4 were significantly more persuasive than those written by experts such as political consultants 33, whereas another found that messages generated by Claude 3 Opus were no more persuasive than those written by laypeople34. These findings suggest that, at present, LLMs' greatest potential impact is to cheaply and rapidly produce persuasive content at a roughly human level35, rather than to substantially improve upon the impact of campaigns' messaging itself. Given the pace of research in the field, however, this picture may well change with the release of new models, or the development of new prompting approaches.\nNext, we consider another potentially significant capacity of LLMs: their use in political microtargeting. This is the practice of tailoring political messaging to a specific individual, based on such features as their demographic data or social media activity36,37, which became notorious after Cambridge Analytica scraped data from 50 million Facebook users to target political ads during the 2016 US presidential election and UK Brexit referendum. On the one hand, microtargeting could bring benefits to the political process. By tailoring messages to voters' concerns, it may increase participation, or heighten interest in topics relevant to minority voters39. However, there is concern that LLMs could distort campaigning by mass-producing highly tailored messages with minimal human intervention. Studies have already shown that LLMs can infer political preferences from a small snapshot of user data, such as a single tweet40, and LLM-generated messages are viewed as more persuasive when tailored to participants' personality traits37,41. However, as shown in Fig. 1, none of the three studies which directly measured the effect of targeted messaging on participant's attitudes showed a significant difference between the impact of targeted and untargeted LLM messages42. These findings align with existing work suggesting that microtargeted messages are rarely more effective than the single most persuasive message across the entire population43, and suggest that at present, the use of LLMs for tailored political messaging may be less transformative than has been feared."}, {"title": "Political polarisation", "content": "In many modern democracies, opinions have become highly polarised, with politics dominated by opposing groups who reject each other's views and values outright. This takes the form of both issue polarisation (highly divergent political perspectives) and affective polarisation (animosity between people with different political affiliations). Polarisation is often blamed on algorithms designed to maximise engagement with digital content that trap users in \u201cfilter bubbles\" (where their prejudices are constantly reinforced) or \u201cecho chambers\u201d (where they are insulated from the discomfort of contrary views)49,50. If LLMs are personalized they may exacerbate this issue, by generating replies that flatter the user's preconceptions, or isolate them from ideologically opposing views. To date, most LLMs are generic (rather than personalised to suit the tastes of individual users) and after fine-tuning mostly provide neutral or diplomatic replies designed to have broad appeal. Nevertheless, there is evidence that even generic LLMs tend to be \"sycophantic\", or to preferentially express views that may be shared by the user, even if these are untrue. This occurs because human feedback provided during the feedback process tends to reward LLM replies that echo user sentiments51. Moreover, some models (such as Replika and Pi) already explicitly tailor outputs based on users' demographics, interests and tastes to make AI more appealing. OpenAI is currently testing a version of the model that remembers user preferences from past conversations52.\nHowever, it is unclear whether AI personalisation will heighten political polarisation53. The view that filter bubbles and echo chambers increases partisanship has come into question 54,55. Instead, affective polarisation may occur because social media algorithms often encourage the most divisive content to be viewed, attended to and shared54. By comparison, publicly available LMs, when fine-tuned to give equanimous perspectives on issues of debate, offer the opportunity to expose users to a spectrum of legitimate opinions, and could nourish public discourse in ways that social media platforms have systematically failed to do56.\nAn alternative explanation for heightened partisanship on digital platforms is that voters are allowed to form highly stereotyped perceptions of their political opponents (for example, in the US, Republicans believe that 32% of Democrats identify as LGBTQ, and Democrats believe that 38% of Republicans earn in excess of $250K per year, where in reality the figures are 6% and 2%)57. Without careful prompting, LLMs tend to generate caricatured outputs that may exaggerate stereotypical features in exactly this way58, and thus risk contributing to partisanship by erasing the nuance in the way that people see each other."}, {"title": "Deliberation and consensus", "content": "In a healthy democracy, people can express diverse opinions, and deliberate in an atmosphere of mutual tolerance and respect. Whilst some fear that Al may be used to weaken or suppress political discussion59, there is also hope that Al could be used to create healthier spaces for deliberation among citizens. Machine learning tools are already used to moderate content, by identifying insulting, profane or explicit messages online but LLMs may allow us to go a step further, by intercepting uncivil messages and proposing that they are voluntarily withdrawn or rephrased (potentially faster and more reliably than human moderators can). In one study, LLMs were prompted to intervene in political discussions between US voters with opposing views on gun control, by proposing less adversarial message rephrasing. Discussants accepted the proposed wording about two thirds of the time, and when they did so, improvements in perceived conversation quality and democratic reciprocity (the extent to which political opponents report respecting each other's right to hold contrary views) were observed 61. Another possibility is that LLMs intervention might help amplify voices that are at risk of being marginalised in a discussion. For example, inserting LLMs into mixed gender groups of Afghani citizens discussing contentious political issues was shown to improve the range of ideas contributed by female group members 62. LLMs also offer new opportunities for improving interactions among citizens in social media or debate platforms, by summarising opinions and optimising the routing of comments"}, {"title": "Information and misinformation", "content": "LLMs are prone to generate factually unreliable content (or confabulate; this is usually called \"hallucination\u201d by Al researchers). Whilst safety fine-tuning pipelines and retrieval techniques are increasingly effective at steering the model towards more accurate statements, model replies can still be poorly sourced, untruthful, or over-confident70. Moreover, LLMs are already being deliberately misused to generate misleading content or propaganda. Recent breakthroughs in multimodal generative Al have greatly expanded opportunities for malicious actors to create and manipulate digital content. Some recently deployed models allow users to generate highly realistic audio and video from simple text descriptions, or to alter media in misleading ways. In a political context, this means manipulating multimodal content to portray political rivals in compromising or defamatory ways, producing deceptive campaign videos, and even counterfeiting entire news websites. Already in 2024, deepfake videos have been deployed with obvious intent to shift the electoral calculus in India, Indonesia, Mexico, Pakistan, Slovakia, the US and Taiwan\u00b2. For example, in Pakistan Al was used to generate a fake video of prime ministerial candidate Imran Khan giving a victory speech from prison, and in Taiwan an Al-generated fake video was released on election day in which a candidate was supposedly endorsed by a former rival \u2013 with each of these items receiving hundreds of thousands of views. Whilst there is scant evidence that electoral outcomes were materially affected in these cases, the arrival of hyper-realistic generative content could threaten to rob news media of its \"epistemic backstop\" - the decisive authority that previously provided by a video or audio recording of a news event. LLMs may also be used as \u201csocial bots\" on digital platforms, and tasked with spreading false or hyper-partisan content rapidly through networks whilst disguising its Al-based origin\u00b9\u00b9. Unfortunately, evidence suggests that. at least in an experimental setting, Al systems may be more convincing when the content they produce is deceptive34.\nAl-generated media is becoming harder to spot. Recent work suggests that Al-generated audio and video72, images of human faces73, and tweets74 may now sometimes be indistinguishable from non-synthetic content. Developers are working on machine learning methods for"}, {"title": "Material Impacts", "content": "Democracy is an idealised principle of governance, but in modern societies its material realisation relies partly on technology, including digital technology 4,90. As well as influencing how citizens consume and digest political information, technology shapes how individuals and groups can participate in collective decision-making in a democracy, by debating, protesting, lobbying, polling, funding, or voting. It determines how elected representatives communicate policies and principles with citizens, and how policy is implemented by the bureaucratic machinery of state. Al is the transformative technology of the 21st century, and so it naturally has an impact on the materiality of democracy \u2013 the infrastructure that supports the democratic process in society.\nIn 2024, a year in which so many countries go to the polls, there has been an uptick of concern that Al could be deployed to disrupt elections. There is the worry that malicious actors, including"}, {"title": "Election-related misuse", "content": "The material practice of democracy relies on elections and referenda being free and fair - eligible voters should be able to cast their vote unhindered. As voters turn to Al with questions about elections, developers need to ensure that LLMs provide accurate, up-to-date information about eligibility and voter registration, polling station access, voter ID, and other election rules. At present, this is not always the case. For example, one study tested the accuracy of leading proprietary and open source models on practical queries about electoral participation in the US, finding that over half of replies were inaccurate. The problem may be particularly acute for those models (like the free-to-use version of ChatGPT) which do not use real-time internet queries to obtain up-to-date information for replies, and thus risk providing outdated advice (although deployed models are increasingly fine-tuned to encourage users to seek information from authoritative sources).\nUnfortunately, attempts have been made to misuse Al to influence voter turnout.. In one well-publicised example, generative Al was used to synthesise an automated telephone message (or robocall) which appeared to feature President Joe Biden discouraging voters from participating in the 2024 New Hampshire primary. Given the ease with which such deepfake materials can be generated - using a short snippet of genuine audio and a few dollars \u2013 efforts to use generative Al to sow confusion among voters and officials could grow. In the near future, heightened personalisation of messages to individuals could exacerbate this risk, for example with robocalls that feature tailored disinformation about eligibility to vote (e.g. based on past felony convictions). Tracking and disabling tools that allow these malicious activities is becoming increasingly difficult.\nAnother vulnerability is voter registration, which is already a battleground issue in many US states. According to recent reports a tool called EagleAl, which purports to identify fraudulent voter activity, is being deployed by activists to query or reject legitimate registrations (especially from minorities in contested wards) on the basis of unreliable evidence. EagleAl has been approved for voter roll maintenance in at least one Georgia county, potentially giving it the power to arbitrate over thousands of registration challenges95. A related risk is that Al's ability to generate content at scale is used to deliberately overwhelm electoral infrastructure, undermining the credibility of the democratic process or suppressing voter participation en masse. In the US, many states have seen a huge surge in voter records requests (sometimes running to millions of documents) made under freedom of information laws, in an apparent attempt to disrupt legitimate election audit processes. Al can be used to accelerate this sort of disruptive activity. For example, EagleAl also allows partisan groups to file mass voter challenges (attempts to strip large numbers of registrants of their vote) on the basis of limited evidence. By"}, {"title": "Augmenting political decision making", "content": "Democratic representatives are empowered to make choices on behalf of citizens, but to do so they need to access and process relevant information. Currently, politicians rely heavily on experts to brief them on relevant issues (such as the Congressional Research Service in the US, and other stakeholders and advocates). It has been proposed that LLMs might support human political decision-making, helping politicians summarise vast bodies of data, brainstorming policy initiatives, or writing draft legislation101. This could allow legislatures to write, debate, and pass more effective bills, or aid in the insertion of \u201cmicro-legislation,\" minor and subtle text that changes the effect of laws 102, as well as aiding in the detection of loopholes. Al may even start to draft entire pieces of legislation (even if based on human desiderata) \u2013 in November 2023 the legislature of Porto Alegre, Brazil, passed the first law written entirely by an LLM103. If Al can help politicians respond better to citizen's needs, this could bolster their perceived legitimacy as democratic representatives.\nAl systems can also potentially enhance conduits of communication between legislators, public servants, and the electorate. For example, LLMs can produce well-structured texts or oratory which could help politicians communicate ideas more clearly to their constituents. In turn, Al may open new avenues for people to feed back their views to government. LLMs are already being used for more effective election polling, harnessing social media data to make microscale predictions about voting intentions that match or exceed those from statistical models used by professional pundits 104, 105. Al systems may also facilitate civic education, by helping voters inform themselves about the issues that most concern them, and which parties best represent their interests. LLMs may also be used to empower citizens by providing easier routes to learn about their rights, or to help them navigate state bureaucracy and legal processes.\nMuch more is possible. Al systems have already been shown to provide balanced summaries of the opinions expressed by small groups of people66,67, but in newer LLMs with longer context lengths (the number of input tokens on which they can condition their output) this automated opinion digest could potentially be scaled to groups of thousands or more, providing a new, LLM-based mechanism for governments to ascertain what citizens think and want. The summarisation process could even be conditioned on demographics, allowing insight into how both majority and minority groups may respond to a political decision 106. In theory, if participants' beliefs are modelled accurately, it could be possible to hold an \"election\" for every decision, in which Al agents vote on behalf of stakeholders."}, {"title": "Foundational Impacts", "content": "Democracy is based on a set of shared values and principles that undergird democratic institutions and offset the burden of democratic participation 110. As a number of political analysts have noted, if these norms are eroded, democracies may \u2018backslide', or gravitate toward authoritarianism\u00b9\u00b9\u00b9. Aside from the epistemic and material impacts discussed above, Al could either corrode or bolster the foundations of democracy \u2013 to either accelerate, or guard against, democratic backsliding.\nOne major concern is that Al will serve to concentrate excessive power in the hands of political leaders or parties. According to one view, democracy flourished in the 20th century because the technological landscape favoured decentralised economies and polities, where power is distributed across diverse groups and individuals112. However, Al could be used to dramatically streamline 21st century state bureaucracies towards centralized governance, and potentially strengthening authoritarian forms of governance. Commentators have speculated that Al intrinsically favours \u2018turn-key authoritarianism'113 or even 'tyranny'114.\nFaith in democratic institutions can be undermined by a perception that the political process is rigged to create winners and losers. For example, many elected governments are perceived as being unresponsive to the demands of the majority, and catering selectively to the few. Populist parties with anti-democratic agendas are poised to exploit these grievances for their own political advantage. There are fears that Al will accelerate this trend by increasing inequality, especially in developed nations115. For example, new capabilities exhibited by LLMs could lead to the displacement of some sectors within the labour market, including administrative and creative industry jobs that were previously thought likely to be spared automation116. However, the precise impact that Al will have on the economy and composition of the workforce remains uncertain, with some forecasting new opportunities for job creation117,118. For example, emerging studies show that equipping workers with LLMs tends to bring the skill levels of lower skilled workers in line with their more highly trained counterparts119,120, which could imply that Al will help create a more level playing field in the workforce. A related concern is that without appropriate governance, the wealth generated by this technological revolution could become concentrated in the hands of a few multinational corporations, in a handful of countries, who are building Al and distributing its services 121. Moreover, governments may struggle to keep up with the pace and complexity of private technology development and so critical choices about the way Al shapes societies may be made by corporations instead of democratic polities122. If so, this could reinforce the perception that democracy's cherished liberal principles have evolved to serve elites rather than society as a whole.\nIn a democracy, representatives and public officials need to be accountable for their actions. If politicians fail to deliver, they can be voted out of office. Al could undermine this principle by blurring lines of accountability when policies fail - because it is unclear whether human or machine made the final decision123. As Al systems become embedded in the machinery of government, this creates new opportunities for blame to be deflected and"}, {"title": "Conclusions and outlook", "content": "Al is the most significant technology of our times. However, as its impact expands, it is becoming clear that it will increasingly interface with another of humanity's most important inventions: democratic self-governance. In this brief review, we have surveyed the possible implications of this encounter for the future of democracy.\nAl will present specific challenges to democracy at multiple levels: epistemic, material and foundational. However, Al also holds out potential affirmative opportunities. Our analysis suggests that neither exuberant optimism nor despairing pessimism is an appropriate stance. Instead, what is called for is clear-eyed and persistent efforts to shape both the design of Al technology and the design of democratic institutions so that they fit together well, yielding democratic benefits from Al while preventing democratic harms. If we plan carefully, we should be able to assure-and even enhance-our democratic future."}]}