{"title": "LASP: Surveying the State-of-the-Art in Large Language Model-Assisted AI Planning", "authors": ["Haoming Li", "Zhaoliang Chen", "Jonathan Zhang", "Fei Liu"], "abstract": "Effective planning is essential for the success of any task, from organizing a vacation to routing autonomous vehicles and developing corporate strategies. It involves setting goals, formulating plans, and allocating resources to achieve them. LLMs are particularly well-suited for automated planning due to their strong capabilities in commonsense reasoning. They can deduce a sequence of actions needed to achieve a goal from a given state and identify an effective course of action. However, it is frequently observed that plans generated through direct prompting often fail upon execution. Our survey aims to highlight the existing challenges in planning with language models, focusing on key areas such as embodied environments, optimal scheduling, competitive and cooperative games, task decomposition, reasoning, and planning. Through this study, we explore how LLMs transform AI planning and provide unique insights into the future of LM-assisted planning.", "sections": [{"title": "1 Introduction", "content": "Integrating LLMs into AI planning and decision-making systems is increasingly pertinent. However, many current methods are limited to constrained domains, such as household tasks performed by robots (Shridhar et al., 2020b; Valmeekam et al., 2023), puzzles (Yao et al., 2023a), maze navigation (Silver et al., 2023; Lehnert et al., 2024), and autonomous vehicle navigation (Pan et al., 2024). The complex nature of real-world scenarios, such as trip planning and corporate strategic planning, calls for new frameworks that leverage LLMs' extensive world knowledge and commonsense reasoning. To address this gap, our survey critically examines existing planning benchmark datasets and methods. We seek to highlight promising directions and identify significant obstacles, offering a potential roadmap for harnessing LLMs' capabilities to tackle real-world planning challenges.\nPlanning involves generating a sequence of actions to achieve specific goals (Russell and Norvig, 1995). As illustrated in ALFWorld (Shridhar et al., 2020b), a household robot performs a series of actions, e.g., 'goto the stove' and 'take the pan from the stove' to achieve the goal of \u2018put a pan on the dining table.' A key tool in this area is the Planning Domain Definition Language (PDDL; Ghallab et al. 1998), which defines the necessary conditions before actions can take place and the effects of those actions afterwards. PDDL's symbolic representation of states and actions also resonates with formal semantics in computational linguistics (Banarescu et al., 2013; O'Gorman et al., 2018). Our paper reviews key studies using PDDL in planning systems and discusses emerging alternatives that are suitable for handling open-domain tasks.\nThe quality and variety of benchmarks significantly influence the development of planning systems. We categorize existing benchmarks into three groups: (a) embodied environments, where agents perform household tasks or navigate mazes, with algorithms designed to find the most efficient route through a grid (Gupta et al., 2010; Shridhar et al., 2020b; Lehnert et al., 2024); (b) puzzle challenges, e.g., the Game of 24, graph coloring, and Towers of Hanoi, which increase in complexity as the problem size grows (Valmeekam et al., 2023; Yao et al., 2023a); and (c) natural language planning that focuses on optimal scheduling, travel planning, and task decomposition, which require advanced reasoning capabilities from LLMs (Xie et al., 2024; Zheng et al., 2024). It should be noted that our survey does not include planning problems that require significant visual processing, such as autonomous driving (Hu et al., 2023), as these are beyond the scope of this study.\nThe ability to plan is crucial for embodied agents and robotic systems because these systems must break down high-level goals into a sequence of low-level, admissible actions to function effectively. Classical algorithms, such as Fast-Downward (Helmert, 2006), can handle problems defined in PDDL, but many real-world applications are less ideal. Several issues arise, including goals that may be ambiguously expressed in natural language, environments that may only be partially observable, and state transition functions that may not be well-defined and are non-deterministic. LLMS offer a promising solution to some of these challenges, thanks to their flexibility and intrinsic understanding of the world.\nOur survey addresses a crucial gap by examining the state-of-the-art planning methods and benchmarks. LLMs demonstrate text planning capabilities, as they implicitly perform content determination and structuring during text generation (Hovy, 1988; Stent et al., 2004; Moryossef et al., 2019). Nevertheless, real-world planning problems are versatile, spanning a range of contexts and may require an understanding of constraints.\nThis survey is intended for NLP researchers interested in exploring AI planning problems. Junior researchers may find it beneficial to learn about these benchmarks and contribute new findings. Senior researchers will likely appreciate this survey, as it summarizes successful cases of integrating LLMs into planning frameworks, identifies opportunities for improvement, and encourages further contributions to the field of LLM-assisted planning (LASP). The following sections provide a mathematical formulation of planning problems, discuss the Planning Domain Definition Language (PDDL), and survey popular benchmarks and methods."}, {"title": "2 Classic Planning", "content": "Problem Formulation. In classic planning, an agent operates within a fully observable environment is modeled as a Markov Decision Process (MDP). The planning task involves a series of states and actions $(s_0, a_0, s_1,..., a_{T-1}, s_T)$, beginning at an initial state $s_0$ and finally reach the goal state $s_T$. The environment is described by a set of states $S$, and a set of actions $A$ available to the agent. The state transition is modeled as $p_o(s_{t+1}|s_t, a_t)$, predicting the next state $s_{t+1}$ after an action $a_t$ is taken. This state transition modeling is sometimes known as a world model, which is a representation of the environment that large language models can simulate. A reward function $S \u00d7 A \u2192 R$ assigns a scalar reward $r_o(r_t|s_t, a_t)$ after an action is taken from a given state. The ultimate goal of an MDP is to develop a policy, denoted as $a_t = \\rho(a|s_t)$, focuses on identifying the optimal action $a_t$ given the current state $s_t$, that maximizes the total expected reward over time.\nDomain Description. The Planning Domain Definition Language (PDDL; Ghallab et al. 1998) is a representation used to define planning problems and domains using the BNF syntax. In PDDL, there are three main parts: a domain file, a problem file, and a plan. The domain file describes the actions and predicates that can be used in various problems, the problem file defines specific initial conditions and goals for a particular situation, and the plan lists the actions needed to achieve the goals under those conditions. The following is an example of a PDDL for the Blocksworld domain. pick-up and put-down are two example actions. The preconditions and the post effects of these actions are specified with :prediction and :effect, which is a conjunction of predicates, indicating conditions can be true or false.\nPDDL offers the advantage of verifying whether plans produced by LLMs can actually be executed, as it can identify when preconditions, such as those in (unload b1 b2), are not met. However, its inflexibility and the need for creating domain and problem files pose significant drawbacks. Integrating PDDL with LLMs often involves converting these files and the plans into natural language, which can complicate the process."}, {"title": "3 Planning Benchmarks", "content": "3.1 Planning in Embodied Environments\nEmbodied environments used to evaluate planning systems typically employ discrete action spaces and are narrowly limited to home tasks.\nShridhar et al. (2020a) introduced ALFRED (Action Learning From Realistic Environments and Directives), a dataset that pairs natural language instructions with egocentric visual inputs to guide action sequences for household tasks. ALFRED comprises 25,743 English instructions covering 8,055 expert demonstrations, each with a high-level goal and an average of 50 steps, markedly expanding the scope of prior datasets. The dataset is split into training (21,023 annotations), validation (1,641 annotations), and testing (3,062 annotations) sections. The validation and testing segments are further divided into 'seen' and 'unseen' categories to evaluate how well models perform in new environments and with unfamiliar object types.\nIn evaluating models, ALFRED uses two metrics: Task Success and Goal-Condition Success. Task Success is binary, assigned a 1 if an object ends up in the right location and state to meet the task's requirements, and a 0 otherwise. Goal-Condition Success calculates the percentage of completed requirements at the end of a task relative to what was necessary for completion.\nIn ALFWorld (Shridhar et al., 2020b), a model named BUTLER has been developed for evaluation on the dataset. This model comprises three components: brain, vision, and body. The brain element processes textual data, constructing a text-based conceptualization of the physical world using the TextWorld engine. Simultaneously, the vision component translates each visual scene from the physical environment into textual descriptions, employing a pre-trained Mask R-CNN detector at each step. The body component then translates high-level textual instructions into specific actions within the physical domain. In addition to BUTLER, a Seq2Seq model serves as the baseline for comparison.\nThe success rate is used to measure model performance across six task types, with BUTLER performing better than the baseline model. While BUTLER has perfect state estimation, object detection, and navigation, it does not utilize this advanced capability to improve its success rate and actually shows a decrease in performance. Nevertheless, when assessed with human-annotated goals, BUTLER achieves a notable success rate that may warrant further investigation. The best performance in this benchmark is achieved by ITCMA (Zhang et al., 2024), which attains completion rates of 100% on the seen set and 98% on the unseen set. The experiment uses 3,553 training task instances spanning six categories, along with 140 in-distribution (seen set) and 34 out-of-distribution (unseen set) evaluation task instances.\nThe VirtualHome Activity Dataset (Puig et al., 2018b) is a collection of video simulations showcasing household tasks in eight different household scenes. Each entry in this dataset consists of a natural language description of an activity and its corresponding symbolic representation, called \"programs,\" which outline the steps involved. The dataset contains two subsets of programs: those written by humans and those generated synthetically by the simulator. The human-written programs include 2,821 examples, with an average of 11.6 steps each. The synthetic programs, on the other hand, total 5,193 and average 9.6 steps per program. The content for each program is diversely generated, considering different homes, agents, camera perspectives, and the arrangement of some objects within the home.\nTo evaluate the program, the LCS between the predicted program and the ground truth is calculated. This metric allows for gaps in the matched steps as long as they appear in the correct order. Accuracy is assessed by dividing the length of the correct subsequence by the maximum length of the programs being compared. Notably, these metrics do not account for whether the predicted program can be executed, which is why the percentage of executable programs is also reported. The state-of-the-art model, ToolkenGPT (Hao et al., 2024), shows a 68% success rate and 82% executability, slightly exceeding the 66.66% human-evaluated correctness reported in the 2018 dataset."}, {"content": "3.2 Planning for Optimal Scheduling\nPlanning is necessary for optimal scheduling, as it ensures that time and resources are properly managed, tools are used as needed, and intended goals are achieved within set constraints. Datasets have been developed to help with trip planning, meeting scheduling, calendar management, auction bidding, and logistics coordination (Valmeekam et al., 2023; Xie et al., 2024; Chen et al., 2024; Zheng et al., 2024). These tasks typically involve constraints related to time, budget, and resource allocation, e.g., adhering to specific time frames, assigning meeting spaces, transportation, and accommodations. Geographic factors, including distance and accessibility, are also important.\nTravelPlanner (Xie et al., 2024) focuses on developing language agents that can search and generate trip plans in response to user queries. These agents can navigate through approximately 4 million online entries using six specialized tools: CitySearch, AttractionSearch, FlightSearch, DistanceMatrix, RestaurantSearch, and AccommodationSearch. The benchmark also includes 1,225 user queries accompanied by human-annotated reference plans. An example user query is: \u201cI'm traveling from Seattle to California from November 6 to 10, 2023. I have a budget of $6,000. For lodging, I prefer an entire room, and the accommodations must be pet-friendly.\u201d LLM agents are designed to create travel plans that not only meet user needs but also adhere to commonsense constraints."}, {"title": "3.3 Competitive and Cooperative Games", "content": "Collaborative and competitive games serve as testing grounds for evaluating LLMs' abilities in strategic planning, resource allocation, risk management, and multi-agent behaviors as they work toward specific goals. Examples of such games include Rock-Paper-Scissors, Tower of Hanoi, Minecraft, Public Goods, Guess 2/3 of the Average, Auction, Bargaining and more (Wu et al., 2024; Huang et al., 2024; Chen et al., 2024). Games are commonly used as benchmarks because they have well-defined objectives and quantifiable outcomes.\nSmartPlay (Wu et al., 2024) is comprised of six games: Rock-Paper-Scissors, Tower of Hanoi, Two-armed Bandits, Messenger, Crafter (Hafner, 2022), and Minecraft (Fan et al., 2022), all of which are accompanied by language descriptors. These games have been selected to challenge LLMs on essential capabilities such as reasoning with object dependencies, long-term planning, spatial reasoning, learning from history, and understanding randomness.\nAUCARENA (Chen et al., 2024) is an evaluation suite designed to simulate multi-round auctions for a list of items with varying values. Items have a starting bid (e.g., $1,000) and a true resale value (e.g., $2,000). Each bidder operates with a budget (e.g., $20,000) and tasked with maximizing their profits (their strategies can vary, such as acquiring specific items or securing as many items as possible.) In each round, agents bid on an item transparently, and the highest bidder in the final round wins the item. To succeed, bidder agents must operate with a fixed budget and make strategic, long-term decisions across several rounds.\nGAMA-Bench (Huang et al., 2024) features eight games divided into three categories: collaborative (Guess 2/3 of the Average, El Farol Bar, Divide the Dollar), betrayal (Public Goods Game, Diner's Dilemma, Sealed-Bid Auction), and sequential (Battle Royale, Pirate Game). These games have been studied in Game Theory literature. For example, in the Public Goods Game, each player receives an amount of money and can choose how much to contribute to a common pot. The total amount in the pot is then multiplied (usually doubled) and distributed equally among all participants, regardless of individual contribution. The Nash equilibrium in this game is for all players to contribute nothing, as each player hopes to free-ride on the contributions of others. An LLM agent is then evaluated by how its contributions nearly sum to zero while interacting with others."}, {"title": "3.4 Task Decomposition", "content": "Task decomposition benefits planning by enabling efficient and reliable execution. Breaking down a task into subtasks facilitates the creation of a task-specific taxonomy. As a result, tasks can often be executed more effectively when provided with a concrete plan that includes actionable steps. It also becomes easier to recover from interruptions (Yuan et al., 2023a,b; Ou et al., 2024).\nTaskLAMA (Yuan et al., 2023a) has developed a dataset comprising 1,612 annotated complex tasks, which includes 711 tasks from the MSComplex-Tasks dataset (Zhang et al., 2021) and 901 tasks derived from how-to search queries. Examples of these tasks are \u201ccook lobster tails at home (Grilled)\" and \"plan a wedding (In Italy).\u201d Human annotators are tasked with: 1) writing their assumptions to contextualize the task (denoted in parentheses); 2) outlining the necessary steps for completing the tasks within this context; 3) detailing the temporal dependencies among these steps. The resulting task is structured into a directed acyclic graph known as a Task Graph, where each node represents a step, and the edges indicate the temporal dependencies between these steps.\nWORLDAPIS (Ou et al., 2024) uses a top-down strategy to derive APIs (actions) from wikiHow's step-by-step instructions for everyday tasks. For example, the task \u201cHow to Melt Chocolate in Microwave\" can be broken down into steps such as \"Chop the chocolate\u201d and \u201cPlace the chocolate,\" ending with \"Allow the chocolate to cool.\" Starting with an initial set of APIs, WORLDAPIS employs the LLM to iteratively generate Python programs for these tasks. When existing APIs cannot cover a step, the program 'hallucinates' new APIs, which are then added to the pool. This method has expanded the action space to over 300 APIs necessary for tasks in the physical world. In contrast, existing simulators support only a fraction (9 of the top 50) of these induced APIs.\nYuan et al. (2023b) introduce the task of constrained language planning and present CoScript, a dataset comprising 55,000 goal-oriented scripts. Each script is a sequence of the necessary steps to achieve a specific goal. For instance, to achieve the goal of 'make a cake,' one may follow steps such as gathering ingredients and preheating the oven. Additionally, constrained language planning imposes various constraints on planning goals. E.g., a cake can be made using different ingredients (e.g., chocolate or vanilla), various tools (a microwave or an oven), or for distinct purposes (a wedding or a birthday party). The authors use an \u201cover-generate-then-filter\" approach to select high-quality scripts from multiple LLM-generated candidates. A good planner is expected to generate steps that respect these constraints.\""}, {"title": "3.5 Reasoning and Planning", "content": "Reasoning and planning are distinct in terms of their focus. Reasoning involves integrating multiple pieces of information and making inferences to address complex problems. E.g., \u2018What musical instruments do Minnesota-born Nobel Prize winners play?' is considered a multi-hop reasoning problem as it involves inferential chaining to generate an answer. In contrast, planing is not only about achieving goals but doing so optimally, e.g., with minimal cost or shortest path. Planning involves considering various constraints and predicting future states that result from actions. E.g., 'booking the cheapest flight' involves generating a sequence of actions (search, compare, book) around a clear objective with temporal and cost constraints.\nDespite the distinction, reasoning can still play a secondary role in planning. For example, PRONTOQA (Saparov and He, 2023) is a synthetic QA dataset designed to assess LLMs' reasoning capability. This dataset is based on synthetic world models represented in first-order logic. Each sentence in the chain-of-thought is parsed into a formal representation to reconstruct proof steps, which are evaluated against a gold-standard proof. Their study suggests that while LLMs can effectively perform individual deduction steps, they struggle with planning. Particularly, LLMs face challenges in selecting the correct proof step where multiple viable options are available, often leading to incomplete proofs and incorrect answers.\nAGENTBENCH (Liu et al., 2023b) evaluates LLMs' reasoning and decision-making in a multi-turn, open-ended context. It is designed for text-only LLMs acting as autonomous agents, and features 8 distinct environments, including operating system, database, knowledge graph, digital card game, lateral thinking puzzles, housekeeping, and web shopping and browsing. The benchmark focuses on LLMs' core skills such as instruction following, knowledge acquisition, logical reasoning, and commonsense grounding. Results suggest that training LLMs on code and high-quality, multi-turn alignment data enhances agent performance.\nSWE-BENCH (Jimenez et al., 2024) is an evaluation framework that includes 2,294 software engineering problems collected from 12 Python GitHub repositories. It presents a codebase and issue description for an LLM to solve. Solutions are then evaluated using the repository's existing testing framework. Resolving issues in SWE-BENCH requires LLMs to modify various functions, classes, or files, challenging their ability for processing long contexts and complex reasoning."}, {"title": "4 LLM-Assisted Planning Methods", "content": "We provide a modularized view of LLM-assisted planning algorithms, which includes the plan generator, environment interpreter, and enhanced feedback provider. Our goal is not to survey all of the planning methods. Instead, we focus on undersanding LLMs' role in assisting the development of SOTA planning systems and to solve new planning problems. We broadly categorize the methods into LLM-as-Planner and LLM-as-Facilitator. The first category explicitly uses LLM's inherit reasoning abilities to generate plans, while the latter relies on other planning algorithms for plan generation and LLMs only serve to facilitate the process, such as as a simulator for the world model, or action planner to estimate the future actions from the current state, or using LLMs to redefine the action space with world APIs. Note that we provide an in-depth discussion on whether LLMs can plan as its intrinsic ability in the discussion section.\nPlan generator is the core of the entire operation, which predicts one future action, multiple future actions, or even multiple threads of future actions. Many recent works used LLMs as the plan generator, exploiting its versatility to handle scenarios and problems across different domains. However, some researchers argued that LLMs in their current states are fundamentally weak at planning. So, many have also proposed using algorithms such as Fast-Downward (Helmert, 2006) and even separately trained models to perform planning (Liu et al., 2024). We argue that the exploration of plan generators, especially non-LLM based planners, are far from an end. Recently, for example, (Lehnert et al., 2024) proposed Searchformer, a transformer based planning algorithm that was both performant and incredibly efficient."}, {"title": "4.1 Using LLM as an Action Planner", "content": "Using LLM as planner relies heavily on the prompt design and LLM's inherent ability to generate and refine plans. The key advantage of using LLMs to generate plans lies in their ability to understand and generate natural language, allowing them to process problem descriptions and generate plans without the need for extensive domain-specific knowledge or specialized training. One main disadvantage, however, is that LLMs are inherently non-deterministic and its behavior is much harder to predict compared to using tools such as symbolic planners."}, {"title": "4.1.1 Dynamic Plan Updates with Feedback", "content": "Reflexion (Shinn et al., 2023) employs an iterative reinforcement process where agents generate actions, receive evaluations, and produce reflective feedback to improve subsequent actions. In particular, this system features a memory module where short-term memory contains the recent actions and their outcomes, while long-term memory stores linguistic reflections which guide the future behavior. Compared to sparse reward signals, linguistic self-reflection provides richer information and context to help agent to navigate complex planning problems. ReAct (Yao et al., 2023b) integrates reasoning and acting in LLMs to enhance decision-making in language and interactive tasks. By interleaving reasoning traces and actions, the system can dynamically update plans and interact with external sources to refine its actions, which helps mitigate issues like hallucination and error propagation seen in traditional models.\nHuang et al. (2022a) proposes a method called Translated (LM), which employs a multi-step planning process where large language models first generate free-form action plans, which are then semantically translated into admissible actions using a pre-trained BERT-style model. The plans are generated autoregressively, with each step corrected and conditioned on previously translated actions. Dynamic example selection further refines the initial prompts, ensuring relevant contextual examples guide the model, similar to in-context learning setup.\nLLM-Planner (Song et al., 2023) employs large language models to perform few-shot grounded high-level planning for embodied agents. It generates high-level plans from natural language instructions and dynamically re-plans based on environmental observations. G-PlanET (Lin et al., 2023) employs a method where LMs take a high-level task description and an object table from a realistic environment as inputs, generating a step-by-step plan using an iterative decoding strategy. This approach involves flattening object tables into token sequences and integrating them with seq2seq learning frameworks. This approach does not use a decoder-only language model such as GPT, but rather an encoder-decoder model known as BART.\nInner Monologue (Huang et al., 2022b) uses a continuous feedback loop where an LLM processes various types of textual feedback from the environment - including success detecftion, passive scene description, and active scene description \u2013 to enhance planning and reasoning in robotic tasks. This method allows for real-time adjustments and improvements in action sequences based on success detection, scene descriptions, and human interactions. ISR-LLM (Zhou et al., 2023) improves LLM-based long-horizon planning by using an iterative self refinement process. Initially, natural language instructions are translated into PDDL files, which are then used to generate an action plan. This plan is validated and refined iteratively based on feedback. The PDDL translation and plan generation are both performed by LLMs, and validation phase is done with either an LLM or an external mechanism.\nSelf-Refine (Madaan et al., 2023) employs an iterative process where an LLM generates an initial output, provides self-feedback, and refines the output based on this feedback. Here, LLM serves as plan generator, refiner, and feedback provider. The feedback provider is prompted to generate actionable and specific feedback. Zhao et al. (2023) argues that during inferring, planning will be able to find more useful facts which could lead to success reasoning. Planning-based reasoning systems are easier to interpret and therefore tend to be more useful in user-centred and safety-critical scenarios.\nSELFGOAL (Yang et al., 2024) aims to break down high-level, non-executable goals such as \u201cmaximizing profit\" actionable subgoals. Their method was tested in four games requiring multiple agents, including the Public Goods,Guess 2/3 of the Average, First-price Auction, and Bargaining. In these games, agents may deviate from their goals without tangible subgoals or if these subgoals are not grounded in the environment.\nIt has three main modules: Search, Decompose, and Act, all powered by a LLM. In the Decompose stage, the system uses the LLM's prior knowledge to break down a high-level goal into a tree of practical subgoals. It then selects the K most effective subgoals that contribute towards achieving the main objective. In the Act stage, these chosen subgoals guide the LLM in generating specific actions based on the current scenario. This method seeks to balance prior task decomposition and post-hoc experience summarization to effectively realize high-level objectives.\nThe Tree of Thoughts (ToT) (Yao et al., 2023a) framework enhances LLMs' problem-solving capabilities by structuring the task as a tree search, where each node represents a partial solution or thought. This method enables the LLM to explore multiple reasoning paths, evaluate them heuristically, and apply systematic search algorithms like BFS or DFS. Reasoning via Planning (RAP) (Hao et al., 2023) employs a strategy where the LLM is repurposed as both a world model and a reasoning agent, incorporating Monte Carlo Tree Search (MCTS) to guide strategic exploration. The process involves incrementally building a reasoning tree under the guidance of the world model and rewards, effectively balancing exploration and exploitation to find high-reward reasoning paths. This process maintains a grounded and coherent inference by simulating future states, making it more reliable for complex tasks.\nSayCanPay (Hazra et al., 2024) integrates LLMs' generative capabilities with heuristic search to produce feasible and cost-effective plans. The method involves three steps: generating candidate actions (Say), evaluating their feasibility (Can), and estimating their payoff (Pay). The approach employs both Greedy-Action and Beam-Action decoding strategies to optimize the planning process. SayCanPay's main advantage is its ability to combine LLM-generated actions with heuristic evaluation, leading to more grounded and efficient plans. However, the method requires extensive domain-specific training data and may struggle with out-of-distribution generalization.\""}, {"title": "4.1.2 Code-based Planning and Prompting", "content": "PROGPROMPT (Singh et al., 2022) leverages a programming language-inspired prompt structure to guide LLMs in generating robot task plans that are executable and contextually appropriate. By incorporating assert statements and recovery actions, the method ensures that the plans are grounded in the current state of the environment and robot capabilities.\nRaman et al. (2022) also proposes a prompting-based method that leverages precondition errors to improve the generation of executable and semantically correct plans from LLMs. In particular, upon detecting a precondition error, the method re-prompts the LLM with information about the error to extract a corrective action. The types of information provided includes the notion of error, inference of error, and cause of error. However, they may not be always available, depending on the environment.\nSilver et al. (2023) proposed using LLMs as generalized planners. In this framework, GPT-4 is prompted to first summarize the PDDL domain, propose a non-search-based strategy, and then implement this strategy in Python which returns a list of actions. Automated debugging iteratively improves the code based on various types of feedback, including Python exceptions, timeouts, plan syntax, and plan semantics.\nAdaPlanner (Sun et al., 2023) employs an adaptive closed-loop approach where an LLM agent generates and refines plans in response to environmental feedback, using Pythonic code prompts to minimize ambiguity. This framework performs refinement both using failure and success experiences. For in-plan feedback, where the environment is in line with the predicted plan, AdaPlanner extracts useful information from this success experience for upcoming actions. For Out-of-plan Feedback where the observation deciates from the predicted feedback, AdaPlanner refines the plan based on the previous checkpoint."}, {"title": "4.1.3 Using Separately Trained Components", "content": "The LID framework (Li et al., 2022) uses pretrained LMs to initialize a policy network, which receives goals, observations, and histories as inputs and outputs the prediction of the next action. The policy network was trained through imitation learning on expert demonstrations. However, expert data is not always available. To address this limitation, they also introduced Active Data Gathering (ADG), where the model explores the environment and learns from experience.\nDEPS (Wang et al., 2023b) employs a cyclic process of describing the current state, explaining errors, re-planning, and selecting the most feasible sub-goals. Planner and explainer are both LLMs, describer is a vision-language model (VLM) to handle multi-modality inputs, and selector is a separately trained module. This method allows for iterative improvement of plans, addressing the inefficiencies of static planning approaches."}, {"title": "4.1.4 Planning with Constraints", "content": "In a planning problem, the implicit constraints include preconditions for a given action, the range of possible actions, and etc. However, sometimes constraints are explicit in the goal. Yuan et al. (2023b) introduced a method targeted to generate scripts that adhere to specific constraints (such as making a cake for people with diabetes). The paper proposed an \"over-generate-then-filter approach\u201d, which initially generates multiple script options and then filtering these to select the most suitable ones. This approach not only enhances the relevance of the scripts to the specified goals but also significantly improves the faithfulness to the constraints, a noted improvement over traditional models that often fail to consider finer details in constraints."}, {"title": "4.2 LLM-as-Facilitator", "content": "While using LLMs to generate plans offers remarkable flexibility, many have noted that LLMs often fail to produce feasible and optimal plans for complex, multi-step problems that involve understanding and manipulating the state of the world (Liu et al., 2023a). Therefore, some only use LLMs as a facilitator to power other algorithms to generate the plan."}, {"title": "4.2.1 Using Symbolic Planners", "content": "LLM+P (Liu et al.", "models": 1}]}