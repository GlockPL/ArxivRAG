{"title": "SOLAMI: Social Vision-Language-Action Modeling\nfor Immersive Interaction with 3D Autonomous Characters", "authors": ["Jianping Jiang", "Weiye Xiao", "Zhengyu Lin", "Huaizhong Zhang", "Tianxiang Ren", "Yang Gao", "Zhiqian Lin", "Zhongang Cai", "Lei Yang", "Ziwei Liu"], "abstract": "Human beings are social animals. How to equip 3D au-\ntonomous characters with similar social intelligence that\ncan perceive, understand and interact with humans re-\nmains an open yet foundamental problem. In this pa-\nper, we introduce SOLAMI, the first end-to-end Social\nvision-Language-Action (VLA) Modeling framework for\nImmersive interaction with 3D autonomous characters.\nSpecifically, SOLAMI builds 3D autonomous characters\nfrom three aspects: 1) Social VLA Architecture: We pro-\npose a unified social VLA framework to generate multi-\nmodal response (speech and motion) based on the user's\nmultimodal input to drive the character for social interac-\ntion. 2) Interactive Multimodal Data: We present Syn-\nMSI, a synthetic multimodal social interaction dataset gen-\nerated by an automatic pipeline using only existing motion\ndatasets to address the issue of data scarcity. 3) Immersive\nVR Interface: We develop a VR interface that enables users\nto immersively interact with these characters driven by var-\nious architectures. Extensive quantitative experiments and\nuser study demonstrate that our framework leads to more\nprecise and natural character responses (in both speech\nand motion) that align with user expectations with lower\nlatency.", "sections": [{"title": "1. Introduction", "content": "Have you ever imagined having an immersive face-to-face\nconversation with a character you deeply admire? Not\nmerely through speech dialogue, but through an interaction\nwhere you can observe its subtle facial expressions, nat-\nural body language, and even fleeting emotional changes.\nPsychology research [23, 34, 62, 66] shows that in so-\ncial interactions, the greater the level of immersion, the\nbetter the human experience. However, current character\nagents [2, 4, 64] are still limited to text or voice interac-\ntions. This limitation prompts us to build 3D autonomous\ncharacters with richer modalities.\nDeveloping an autonomous 3D character requires effec-\ntively modeling its behavior system, which involves two\nmajor challenges: 1) The 3D character needs to accurately\nobserve and understand the information conveyed by the\nuser, and respond appropriately based on the context and\nits character setting through speech, body motion, and fa-\ncial expression, etc. This goes beyond previous singular\nhuman-related tasks, such as motion generation [83], mo-\ntion understanding [37], and audio-to-motion [8]. 2) Data\nfor multimodal interactions between users and 3D charac-\nters is extremely scarce due to the prohibitive cost of the\ncomprehensive setup.\nPrevious work [17] is primarily based on the LLM-Agent\nframework, using text to link various sub-modules (such\nas motion captioning and text-to-motion). While this ap-\nproach performs well in high-level tasks like planning and\nmemory, it tends to fall short in tasks such as understanding\nuser behaviors and providing timely body motion responses.\nThis limitation arises because using text as the intermediary\nbetween modules conveys high-level information but often\nomits subtle nuances. And the sub-modules (motion cap-\ntioning, speech recognition, etc.) in the complex engineer-\ning framework incur substantial latency that undermines the\ntimeliness of a natural communication [67].\nInterestingly, research on robotics shows the similar con-\nclusion. The LLM-Agent framework can handle planning\ntasks [35], but for low-level manipulation tasks, end-to-end\nVision-Language-Action (VLA) models built upon LLMs\nshow superior performance [14, 33, 39, 90]. We argue that\ndigital avatars are essentially robots with virtual humanoid\nembodiment. Therefore, building a VLA model for social\ninteractions with users is a promising direction.\nIn this paper, we implement an end-to-end social VLA\nmodel, SOLAMI. Our model, built upon a decoder-only\nLLM backbone, processes the inputs of user speech and mo-\ntion into discrete representations, and generate the respon-\nsive speech and motion tokens, which are then decoded to\nthe character's speech and motion. This modeling approach\ncan effectively learn character behavior patterns across mo-\ntion and speech modalities and offer low latency.\nAlthough there are numerous datasets related to human"}, {"title": "2. Related Work", "content": "Existing role-play agents primarily rely on text [64],\nspeech [2, 4, 52], or video [9] as interactive media, but\nbuilding 3D autonomous characters means modeling 3D\nembodied behavior, especially body language. Unlike\nsingle-purpose human motion tasks [8, 16, 31, 44, 65,\n76, 83], we expect this 3D character not only to compre-\nhend user speech and body language but also to respond\naccording to its profile setting. Large language models\n(LLMs) [52, 69], with their remarkable emergent abili-\nties [72], provide promising solutions for this direction.\nOne approach [26, 38, 86, 88] tries to integrate LLMs and\nhuman motion in an end-to-end fashion, enabling a sin-\ngle model to perform multiple motion-related tasks. How-\never, the goal of these methods is not to generate respon-\nsive motion based on the input motion according to the\ncharacter setting. Another approach [17, 45] utilizes the\nLLM as a versatile brain center that controls various sub-\nmodules (e.g., motion understanding, text-to-motion gener-\nation) with text or code instructions. However, this mod-\nular approach inherently introduces information loss and\nengineering-related time latency. Therefore, how to create\nfeasible autonomous 3D characters for immersive interac-\ntion remains an open challenge."}, {"title": "3. Social Vision-Language-Action Modeling", "content": "Our framework is an end-to-end social VLA model that\ntakes the user's speech and motion as input and generates\nthe character's responsive speech and motion as output. In\nthis process, speech and motion are added as new languages\nto the LLM text vocabulary. First, the user's speech and mo-\ntion are converted into discrete motion tokens and speech\ntokens via a motion tokenizer and a speech tokenizer, re-\nspectively. A decoder-only LLM backbone then predicts\nthe character's output motion and speech tokens based on\nthe user's input tokens and the character setting. The gen-\nerated tokens are subsequently decoded into corresponding\nmotion and speech by their respective decoders.\nMotion Representation. To take advantage of SMPL-\nX's [56] compatibility with industry animation workflow,\nwe directly model human poses as SMPL-X joint rotations\ninstead of keypoint positions [38, 49] to facilitate animation\nof characters down the stream.\nMotion Tokenizer. Our motion tokenizer employs a Vector\nQuantized Variational Autoencoders (VQ-VAE) structure as\nIt learns discrete representations of motion, en-\nabling the LLM to understand the text-motion connection.\nWe design separate VQVAEs (Q6, Qh, Qt) for the body mo-\ntion mb, hand motion mh, and inter-character relative trans-\nform (rotation and translation) mt for higher reconstruction\naccuracy [49]. The quantization process for motion mu is\nformulated as\nm = Qu (m\u00b2) = arg min ||m\u00b2 - Zi||2, (1)\nZi EZu\nL\nwhere Zu is the codebook of motion part u \u2208 {b,h,t},\nand my is the corresponding motion tokens. The VQ-\nVAEs of body and hand, Qb and Qh, apply 1D convolu-\ntions on motion features along the temporal dimension to\nget LM sequential tokens Mb = [m, m,..., m\u2122] and\nMh = [mh, mh,..., mh], and the VQVAE Qt uses MLPS\nto get the transform token mt of a sequence.\nSpeech Tokenizer. Research [12, 82] on speech discretiza-\ntion mainly utilizes the RVQ-VAE structure [80]. In this\nwork, we utilize the SpeechTokenizer [84] that disentangles\nsemantic and acoustic information within speech S. This\nallows us to use the semantic tokens S = [1, 2, ..., $15]\nfrom the first layer as input to the LLM, reducing inference\ncosts (Ls is the sequence length of semantic tokens). Si-\nmultaneously, a short sample of the character's voice (4 to 6\nseconds) can be used as a prompt to achieve instance voice\ncloning when decoding through SoundStorm [13].\nMulti-modal Multi-round Interaction. User-character in-\nteraction is formulated as a multi-round conversation fash-\nion of common LLMs [52, 69]. When the user sends speech\nand motion to SOLAMI, the model auto-regressively gen-\nerates speech and motion responses based on previous dia-\nlogue contents and the character setting. To facilitate train-\ning, we use special tokens to mark the start and end of each\nmodality sequence as [38, 40]. The interaction process can\nbe formulated as follow:"}, {"title": "3.2. Training", "content": "The training of SOLAMI adopts a three-stage strategy.\nStage 1: Tokenizer Training. The training approach for\nthe motion tokenizer uses the fashion of [38]. The train loss\nis\nLm = ArLr + de\u00a3e + Ac\u00a3c + \u03bb\u03c5\u00a3v, (2)\nwhere Lr means reconstruction loss, Le embedding loss,\nLe commitment loss, Lu velocity loss, and A\u2217 are manu-\nally adjusted weights. For the speech tokenizer, we use the\npre-trained checkpoint from AnyGPT [81]. We freeze the\ntokenizers' weights after this stage.\nStage 2: Multi-task Pre-training for Modality Align-\nment. As shown in Fig. 2, the second stage is multi-task\npre-training, achieving modality alignment between mo-\ntion and text, as well as between speech and text. It\nis necessary because motion data is scarce, and direct\ntraining on multimodal interaction data results in sub-\noptimal models, as demonstrated in subsequent ablation\nstudies. For motion and text alignment, we use 46 K\nmotion-text pairs for text-to-motion generation and mo-\ntion captioning tasks, and 11 K interactive motion pairs"}, {"title": "4. SynMSI Dataset", "content": "Social interaction between users and virtual characters is in-\nherently unique, which makes collecting such multimodal\ninteraction data particularly challenging. Currently, avail-\nable public datasets [29, 51, 76] are incomplete for our task.\nTo address this issue, we propose a data synthesis pipeline\nthat leverages existing motion-text datasets, text-based role-\nplay models, and speech synthesis methods and generates a\nlarge-scale multimodal dialogue dataset, SynMSI.\nWe collect motion-text data for two purposes: first, to\nachieve alignment between motion and text during pre-\ntraining, and second, to generate multimodal data for in-\nstruction tuning. Since our work focuses on modeling so-\ncial interactions, we select existing datasets that contain\nrich social actions: HumanML3D [29] with 24 K motion-\ntext pairs, Inter-X [76] with 20 K motion-text pairs and 10\nK two-person motion pairs, and DLP-MoCap [17] with 2\nK motion-text pairs. Since the Inter-X [76] dataset con-\ntains only text descriptions of two-person interactive motion\nwithout descriptions of individual motion, we used GPT-\n4o [52] to decompose the two-person action descriptions\ninto single-person motion-text pairs. Additionally, we used\nGPT-40 [52] to synthesize comprehensive text descriptions"}, {"title": "5. VR Interface", "content": "To demonstrate our method directly, we developed a VR in-\nterface with an Oculus Quest 3 frontend and a backend ser-\nvice, as shown in Fig. 4. The frontend enables immersive\ninteraction between users and 3D autonomous characters,\nwhile the backend, powered by 2 H800 GPUs, supports the\ncomputation of various baselines in our experiments. Dur-\ning usage, the VR headset captures the user's speech and\nbody motion, and sends them to the backend computation\nnodes. For motion capture, we use the Quest's full-body\ntracking system [73] to obtain pose parameters, which are\nthen retargeted onto an SMPL-X model [56]. The computa-\ntion nodes generate the body motion parameters and speech\nresponses of the character based on the multimodal input."}, {"title": "6. Experiments", "content": "In our experiment, we selected the AnyGPT-base\nmodel [81] (based on LLaMA2-7B [69]) as the back-\nbone for SOLAMI, because it is an open-source model\navailable at our experimental time that supports end-to-end\nspeech processing. During the pre-training stage, we utilize\n32 V100 GPUs to train the model for 3 K steps (batch size\n256, learning rate 4e-5). For instruction tuning, we train\nthe SOLAMI for 800 steps using 16 V100 GPUs (batch\nsize 48, learning rate 2e-5). For LoRA fine-tuning [32], we\nset the rank as 8 and alpha as 16. We split the synthesized\nmultimodal data into training and test sets with a 9:1 ratio.\nWe use DeepSpeed [61] to accelerate the training. During\ntesting, we evaluate each round of the character's response.\nBaselines. To validate the performance improvement in\nsocial interaction brought by incorporating 3D modalities\n(such as body motion), we compared SOLAMI with the\nLLM+Speech and the AnyGPT (fine-tune) approach. For\nthe LLM+Speech framework, the user's speech is first tran-\nscribed into text using ASR techniques, which is then fed to\na LLM to generate the character's response in text, and sub-\nsequently converted into speech using TTS. For fairness, we\nuse LLaMA2-7B-Chat [69] as the LLM backbone, Whisper\nlarge-v3 [60] for ASR, and XTTS_v2 [19] for voice cloning.\nFor the AnyGPT (fine-tune) framework, we use the speech\ndata of SynMSI to train the AnyGPT-base model [81] with\nthe same parameter settings. To compare the effectiveness\nof the LLM-Agent architecture with the social VLA frame-\nwork, we used DLP [17] as a baseline method. In DLP\nframework, the user's speech and body motion are sepa-\nrately processed as text descriptions by ASR and motion\ncaptioning modules. Based on the input text descriptions,\nLLM generates the character's text instructions of speech\nand motion, which are transferred into speech and body mo-\ntion by TTS and motion generation module. The speech\ncomponent of DLP follows the LLM+Speech method. Con-\nsidering that MoMat-MoGen module in DLP is too slow\nfor user interaction (over 5 seconds latency), we use Mo-\ntionGPT [38] for motion captioning and motion generation.\nTo ensure a fair comparison, we used the same motion data\nas the pre-training stage of SOLAMI to train MotionGPT.\nAdditionally, we conducted ablation experiments on the ef-\nfect of the pre-training stage, marked as (w/o pretrain). For\nthe ablation study of the motion tokenizer, please refer to\nthe supplementary materials. We use vLLM [41] to accel-\nerate the LLM backbones for low latency interaction."}, {"title": "6.2. Quantitative Evaluation", "content": "We conducted quantitative evaluation for our method and\nall the baselines mentioned in Sec. 6.1.\nEvaluation Metrics. For motion, we evaluate the model re-\nsponses using metrics including FID, diversity, PAMPJPE\n(mm), and angle error [31]. Following Duolando [65],\nwe obtain FID and diversity using motion features from\nAIST++ [43]. For speech, we use VC similarity [77] to\nevaluate the voice similarity with the character. To eval-\nuate the content quality of speech, we first use Whisper-\nlarge-v3 [60] to transcribe the speech into text. Then fol-\nlowing [64, 70], we employ GPT-40 [52] as the judge to\nassess Context Relevance and Character Consistency on a\nLikert scale ranging from 1 to 5. Context Relevance indi-\ncates whether the speech content aligns with the topic and\ncontext of the conversation, while Character Consistency\nassesses whether the content adheres to the character set-\ntings. For inference latency (seconds), we deploy all the\nmodels on 2 H800 GPUs with vLLM [41] framework and\nasynchronous mechanisms to improve performance while\nmaintaining fairness."}, {"title": "6.3. VR Interface User Study", "content": "Quantitative evaluation of a single modality alone cannot\nfully compare the 3D autonomous characters built on dif-\nferent frameworks. To address this, we conducted user\nstudy with a VR interface. As shown in Fig. 4, we em-\nploy LLM+Speech, DLP (MotionGPT), and SOLAMI as\nmethod options of the server backend and the same VR\nfrontend across different methods. This implies that the\nvariable in the experiment is the driving method in the\nserver. Users are asked to engage in more than five rounds\nof dialogue with the VR character before completing our\nquestionnaire."}, {"title": "7. Conclusion", "content": "In this paper, we propose SOLAMI, an approach for build-\ning 3D autonomous characters. This approach includes\nthree key components: 1) Architecture: A novel social\nVLA modeling framework enabling multimodal social in-\nteraction; 2) Multimodal Data Synthesizing: A pipeline for\nautomatically generating multimodal interaction data from\nexisting incomplete datasets; 3) VR Interface: A VR en-\ngineering framework that facilitates immersive interactions\nbetween users and various characters. Together, these mod-\nules contribute to an enhanced user interaction experience."}, {"title": "A. Future Work", "content": "Our work, SOLAMI, represents a preliminary exploration\nfor building 3D autonomous characters. While it has per-\nformed well in comparative experiments, there remains sig-\nnificant room for improvement on aspects as follows:\n\u2022 Input Modality: For dyadic social interaction, using\nthe user's body motion and speech as input is suffi-\ncient. However, when considering multi-person interac-\ntion or interaction involving the environment and objects,\nvideo [24, 90] or dynamic 3D scenes [57] might be a bet-\nter choice;\n\u2022 Data Collection: Our synthetic dataset, SynMSI, enables\nsatisfactory user evaluation results. However, collecting\nreal-time data of actual dyadic interaction could enable\nour model to generate more precise and natural body lan-\nguage and speech, while also supporting duplex stream-\ning conversations, similar to [9, 87]. Compared to text\nand video modalities, the collection of embodied 3D data\nis undoubtedly challenging. Potential solutions include:\ncapturing [16] or learning human behavioral data [11]\nfrom existing video datasets, building immersive interac-\ntion platforms [63] to gather data on human interactions,\nand using surrogate control to collect data from human\ninteractions with 3D characters [21];\n\u2022 Cross Embodiment: Using a unified SMPL-X [56]\nmodel to represent characters' motion inevitably intro-\nduces challenges in cross-embodiment for different char-\nacters. While some degree of error and misalignment may\nnot hinder information exchange in social language in-\nteraction, such representations clearly lack generalizabil-\nity for fine-grained tasks (e.g., handshaking, object ma-\nnipulation). The challenges of retargeting in 3D human-\nrelated tasks and cross-embodiment in robotics [90] share\nsimilarities, providing opportunities for mutual inspira-\ntion and methodological exchange;\n\u2022 Long-Short Term Design: Although SOLAMI demon-\nstrates effective modeling for real-time interactions, its\narchitecture encounters challenges such as computational\nredundancy, forgetting, and training difficulties during ex-\ntended social interactions. A promising direction [17, 22]\nto explore is integrating long-term memory, knowledge,\nand skills with short-term real-time interaction. This ap-\nproach could ensure interaction quality while reducing\ncomputational overhead and simplifying the training pro-\ncess;\n\u2022 Efficient Learning Method: Although our dataset, Syn-\nMSI, tries to collect large-scale motion data, the inher-"}, {"title": "B. More Details of Architecture Design", "content": "In this section, we discuss the input and output modalities of\nSOLAMI in Appendix B.1, compare the motion represen-\ntation in Appendix B.2, and introduce details of our motion\ntokenizer and pre-training design in Appendix B.3."}, {"title": "B.1. Input and Output Modalities", "content": "Our ultimate goal is to establish a unified behavioral model-\ning system for any character, where input modalities include\na wide range of sensory observations, including vision, au-\ndio, and haptics etc., and output modalities represent ac-\ntions in the finest possible granularity. However, currently,\nwe need to balance the ideal with the constraints of exist-\ning data and devices to develop a model that provides an\noptimal user experience.\nRegarding devices, we employ VR headsets instead of\nmobile phones or computers because VR headset enables\na more immersive interactive experience by capturing and\npresenting richer information.\nIn terms of input modalities, while 3D scenes or videos\ncould serve as input and have some foundational mod-\nels [42, 57], collecting corresponding social interaction data\nis challenging. For instance, datasets like Ego4D [27] and\nEgo-Exo4D [28] capture first-person videos and motion\ndata but include very limited social interaction content and\nno data involving character interaction. Within VR envi-\nronments, the majority of incremental information a char-\nacter can observe comes from user's behaviors that VR de-\nvices can capture. Consequently, we chose user motion and\nspeech as the primary input for SOLAMI.\nSimilarly, for easy synthetic data generation and model\ntraining, we maintain the same types of output modalities\nfor the character as for the user's input. This symmetry en-"}, {"title": "B.2. Motion Representation Comparison", "content": "Common representations of human motion are often based\non 3D keypoints [29, 38, 49], which provide higher preci-\nsion compared to methods based on joint rotations. How-\never, this approach is inconsistent with the driving mech-\nanism of 3D engines such as Unity Engine. When the\nmodel generates 3D keypoints, retargeting is necessary to\nderive the relative rotation of each joint with respect to its\nparent joint. Considering human motion priors, a typical\napproach [55] involves fitting an SMPL-X [56] model to\nthe 3D keypoints using optimization strategies, and subse-\nquently retargeting the fitted SMPL-X model to the charac-\nter. However, this process has two main drawbacks:\n1. Time-Consuming Fitting Process: The fitting step is\ncomputationally intensive. With optimized methods like\nSMPLify [55], achieving an adequate result requires\nabout 1 second of iteration on a V100 GPU.\n2. Fitting Artifacts and Distortion: Inevitable fitting er-\nrors can lead to biologically implausible joint rotations,\nsignificantly degrading visual quality.\nIn our experiments, we observed that while human mo-\ntion representation based on 3D keypoints performs well in\nterms of motion metrics, as shown in Tab. 3 and Tab. 4, its\nvisual fidelity is inferior to representation based on joint ro-\ntations. To address this, we adopted a cont6d representation\nfor joint rotations, achieving improved visual outcomes."}, {"title": "B.3. Motion Tokenizer and Pre-training", "content": "After processing as described in Appendix B.2, we obtained\na 315-dimensional motion representation. When convert-\ning this motion representation into tokens via the tokeniz-\ners, several issues need to be discussed. Should body and\nhand motion features be represented separately? If so, how\nshould their tokens be handled? Should the tokens for the\nbody and hand motions be interleaved, or should they be\ninput as independent sequences in the pre-training stage?\nConsidering our computational cost, we conducted abla-\ntion experiments on the text-to-motion task using the GPT-\n2 backbone as the baseline model. Finally, we com-\npared the models under the same settings using Llama2-\n7B as the backbone.\nAs shown in Tab. 4 and Tab. 3, compared to unified rep-\nresentations of hand and body motion (marked as \u201cbind", "separate\") achieves\nbetter performance, particularly with higher precision on\nthe text-to-motion task (t2m). However, the trade-off is\nthat the probability of GPT-2 producing outputs that\nconform to the expected format decreases. We think\nthis improvement is due to the differences in the language\nmodels: GPT-2, the relatively smaller language model, has\nweaker comprehension of textual instructions. In contrast,\nLlama2, trained on extensive corpora, demonstrates signifi-\ncantly stronger text understanding capabilities. Moreover,\ncompared to interleaved tokens (\u201cYes\u201d for \\\"Token Inter-\nleaved\"), separate sequence representations (\u201cNo\u201d for \u201cTo-\nken Interleaved\") achieve better motion metrics. We hy-\npothesize that this is because learning separate sequences\nreduces the overall complexity of the motion pre-training\ntask, thereby improving performance.\nBased on the above experimental evaluations, we ulti-\nmately select Llama2-7B for its strong text comprehen-\nsion capabilities as the LLM backbone. For processing mo-\ntion representation, we employ separate motion tokenizers\nthat convert the motion representation into noninterleaved\ntoken sequences. This configuration is used for the final\ninstruction fine-tuning stage.\"\n    },\n    {\n      \"title\": \"C. More Details of Data Generation\",\n      \"content\": \"In this section, we first discuss several methods for col-\nlecting multimodal social interaction data in Appendix C.1.\nThen, we introduce the technical details of SynMSI gener-\nation pipeline in Appendix C.2.\"\n    },\n    {\n      \"title\": \"C.1. Comparison of Data Collection Methods\",\n      \"content\": \"From the perspective of data sources, we discuss three\nsources: internet videos, Immersive VR platform, and exist-\ning incomplete motion capture datasets, as shown in Tab. 5.\nCollecting from Internet Videos. The development of mo-\nbile devices has led to an explosion of video content, and re-\nsearchers naturally expect the model to learn knowledge and\ncapabilities from internet videos. Many works aim to im-\nplicitly learn human capabilities from videos [20, 74], but\nfor our task, we anticipate obtaining explicit multi-modal\ninteractive data through various tools [16, 52]. Human mo-\ntions can be captured through video motion capture, but\ncurrent video motion capture [16] faces challenges such as\"\n    },\n    {\n      \"title\": \"C.2. Details of SynMSI Generation Pipeline\",\n      \"content\": \"Motion Post-process Existing motion-text datasets [17, 76]\nprimarily provide semantic-level text annotations, often\noverlooking behavioral details (such as sitting versus stand-\ning positions, orientations, etc.). Considering GPT-4o": "ca-\npability [52] in understanding human behaviors in videos,\nas shown in Tab. 5, one approach would be to render all mo-\ntions into videos and then use VLM for annotation. How-\never, for a small research team, the cost of VLM API calls\nis relatively high. We propose a compromise strategy: com-\nbining multiple text annotations for a single motion and us-\ning GPT-40 [52] to generate a comprehensive, detailed de-\nscription. In practice, we find this method to be quite effec-\ntive.\nTopics Collection. Without topic guidance, conversations\nwith LLMs often converge to simple, generic content rather\nthan character-specific, in-depth content [17, 64]. Using\nprompts to guide conversation is a common strategy. We\ncollected topics from the following perspectives:\n1. Character-related topics: These topics are difficult to\ncollect in bulk from the internet and were generated\nthrough GPT-40 [52] brainstorming;\n2. News-related topics: Google Trends [3] has compiled\nmany news topics that people care about in daily life;\n3. Daily life topics: Some community websites, such as\nJike, specifically curate such topic content;\n4. Topics people are curious about: Common Q&A web-\nsites (such as Quora, Zhihu [6]) specifically organize\nthese topics."}, {"title": "E. Acknowledgments", "content": "We extend our sincere gratitude to Fei Xia, Huazhe Xu,\nTao Kong, Jiangyong Huang for their insights from the\nembodied intelligence field. We thank Mingyuan Zhang,\nFangzhou Hong, and Xinying Guo for discussions on mo-\ntion generation, Bo Li and Yuanhan Zhang for advice on\nmultimodal model training. We would also like to acknowl-\nedge Han Du, Fanzhou Wang, Jiaqi Li, Liang Pan, Peng\nGao, and Yukun Wei for insightful discussions on the topic."}, {"title": "D. More Details of Experiments", "content": "We chose Llama2-7B because at the time of our exper-\niments, end-to-end models with speech pre-training were\nscarce, with AnyGPT being one of the few that per-\nformed well. Thus we selected the Llama2 series as the\nbackbone for fair comparison in subsequent experiments.\nReaders aiming to achieve the best results can certainly\nchoose state-of-the-art models as the backbone."}, {"title": "Text-to-speech", "content": "We aim to achieve the best voice\ncloning effect in near real-time conditions. For this purpose,\nwe compare these software and algorithms: Instant Voice\nCloning, ChatTTS + OpenVoice, XTTS_v2, MARS5, and Bark. Among them,\nMARS5 uses a diffusion framework and is rela-\ntively slow; Instant Voice produces the best results but has\nhigh API costs and tends to generate speech at a faster pace.\nXTTS_v2 is a more suitable option, and can achieve a\ngood balance between speed and quality."}]}