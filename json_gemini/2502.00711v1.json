{"title": "VIKSER: Visual Knowledge-Driven Self-Reinforcing Reasoning Framework", "authors": ["Chunbai Zhang", "Chao Wang", "Yang Zhou", "Yan Peng"], "abstract": "Visual reasoning refers to the task of solving ques- tions about visual information. Current visual reasoning methods typically employ pre-trained vision-language model (VLM) strategies or deep neural network approaches. However, existing efforts are constrained by limited reasoning inter- pretability, while hindering by the phenomenon of underspecification in the question text. Addition- ally, the absence of fine-grained visual knowl- edge limits the precise understanding of sub- ject behavior in visual reasoning tasks. To ad- dress these issues, we propose VIKSER (Visual Knowledge-Driven Self-Reinforcing Reasoning Framework). Specifically, VIKSER, trained us- ing knowledge distilled from large language mod- els, extracts fine-grained visual knowledge with the assistance of visual relationship detection techniques. Subsequently, VIKSER utilizes fine- grained visual knowledge to paraphrase the ques- tion with underspecification. Additionally, we design a novel prompting method called Chain- of-Evidence (CoE), which leverages the power of \"evidence for reasoning\" to endow VIKSER with interpretable reasoning capabilities. Mean- while, the integration of self-reflection technol- ogy empowers VIKSER with the ability to learn and improve from its mistakes. Experiments con- ducted on widely used datasets demonstrate that VIKSER achieves new state-of-the-art (SOTA) results in relevant tasks.", "sections": [{"title": "1. Introduction", "content": "Endowing machines with robust logical reasoning capa- bilities has been a long-standing goal of vision-language models (VLMs) (Steiner et al., 2024; Liu et al., 2023b; Kamath et al., 2021). A critical step toward realizing the goal lies in enhancing the model's visual reasoning capabil- ities (Wu et al., 2024; Ray et al., 2024). Visual reasoning involves solving questions about visual information (Gupta & Kembhavi, 2023; He et al., 2021), a task that necessitates precise alignment between visual and textual features, along with advanced logical reasoning skills (Chang et al., 2024; Prasad et al., 2024; Zhang et al., 2023). Figure 1 illustrates a typical example of visual reasoning, where an agent must accurately align the image with the question and infer the in- tent through multiple steps of logical reasoning. The notable features of visual reasoning not only drive advancements in cross-modal learning (Deitke et al., 2024; Bazi et al., 2023) but also contribute to enhancing machines' logical reasoning abilities (Prasad et al., 2024; Marcu et al., 2024), underscoring its substantial research significance.\nExisting visual reasoning methods typically employ pre- trained VLM strategies or deep neural network frame- works (Steiner et al., 2024; Deitke et al., 2024; Zakari et al., 2022). However, the reasoning capabilities of current efforts still exhibit limited interpretability (i.e., Issue 1). Addition- ally, underspecification is a common phenomenon in visual reasoning tasks (Prasad et al., 2024; Pezzelle, 2023), where a ambiguous description of the subject in the question text can hinder the alignment between textual and visual features, leading to multiple incorrect visual interpretations and un- dermining reasoning reliability (i.e., Issue 2). As shown in Figure 1, the subject \"he\" in the question is ambiguously described, which may be confused with other individuals in the image, presenting a risk of incorrect reasoning. On the other hand, visual knowledge extraction (VKE) tech- niques, which provide enriched visual information, have the potential to significantly assist in addressing visual reason- ing tasks (Cui et al., 2024; Yang et al., 2024a). However, existing VKE methods are insufficient in extracting latent and fine-grained visual knowledge (i.e., Issue 3). The is- sue limits the accurate understanding of subject behavior in complex questions, thereby hindering the interpretability and precision required for visual reasoning tasks.\nTo address these issues, we design VIKSER (Visual Knowledge-Driven Self-Reinforcing Reasoning Frame- work). The core components of VIKSER are the fine- grained visual knowledge extraction (F-VKE) module and the self-reinforcing reasoning (S-RR) module. For Issue 1, we deploy the S-RR module, which offers enhanced in- terpretability in reasoning and integrates self-reinforcement capabilities. Specifically, the module integrates a novel prompting technique, Chain-of-Evidence (CoE), which we propose to enhance interpretability by facilitating step-by- step reasoning grounded in factual evidence. Meanwhile, a self-reflection mechanism, which utilizes insights from past failures to refine future reasoning, is introduced to facil- itate adaptive reinforcement. Additionally, intending to ad- dress Issue 2, a specification paraphrase method is explored for the S-RR module. Concretely, we collect object-level information from fine-grained visual knowledge to refine ambiguous descriptions of the subject in question, thereby improving the question's clarity and completeness.\nTo address Issue 3, we deploy the F-VKE module to provide fine-grained visual knowledge to VIKSER. To be specific, the F-VKE module first detects the detailed visual relation- ships among key entities in an input image. Subsequently, we train the F-VKE module using knowledge distilled from large language models (LLMs) to uncover causal relation- ships between entities' behaviors and their inferred out- comes. For instance, as shown in Figure 3, LLMs can infer that a man depicted squatting on the right side of the train door may disembark at the next station. Following this, the trained F-VKE module leverages visual and causal relation- ships to generate fine-grained visual knowledge. A specific case that validates the fine-grained nature and richness of the visual knowledge generated by the F-VKE module is pro- vided in Section 4.4. We conduct extensive experiments on diverse widely-recognized datasets to validate VIKSER'S visual knowledge extraction and reasoning capabilities. The results demonstrate that VIKSER outperforms the latest research across all datasets, achieving new SOTA results.\nWe summarize our contributions as follows:\n\u2022 We propose VIKSER, a novel framework for visual reasoning tasks, which extracts fine-grained and en- riched visual knowledge while performing highly in- terpretable and self-reinforcing reasoning."}, {"title": "2", "content": "\u2022 A specification paraphrase method is designed to help VIKSER mitigate underspecification, while a novel CoE prompting technique and a self-reflection mecha- nism are introduced to assist VIKSER in performing highly interpretable and self-reinforcing reasoning.\n\u2022 Extensive experimental results demonstrate VIKSER'S improvements over advanced baselines across diverse public datasets, achieving new SOTA results."}, {"title": "2. Related Work", "content": "Visual Knowledge Extraction Existing VKE methods either employ a holistic image captioning strategy or rely on fixed knowledge formats to extract visual information (Cui et al., 2023; Liu et al., 2023b). Specifically, deep learning has played a crucial role in advancing image captioning approaches (Jiang et al., 2018; Yao et al., 2017). With the development of VLMs, an increasing number of stud- ies leverage pre-trained multimodal large language mod- els (MLLMs) to understand both visual and textual infor- mation in a unified framework (Hu et al., 2023; Fang et al., 2022). On the other hand, the image captioning methods provide a broad understanding of the image content, yet they encounter challenges in conveying fine-grained details. In contrast, some studies utilize fixed knowledge graphs to map visual features to predefined semantic categories (Yang et al., 2024a; Li et al., 2023b), providing a structured form of visual knowledge (Li et al., 2024; Zareian et al., 2020). While these methods ensure consistency and interpretability, they fall short in providing enriched visual knowledge.\nLanguage Model Reasoning Recently, large VLMs have demonstrated remarkable success in reasoning and infer- ence, particularly in facilitating few-shot (Lang et al., 2022; Gao et al., 2021) and zero-shot (Yu et al., 2024; Kojima et al., 2022) learning. Meanwhile, the potential of prompt- based reasoning has been extensively explored to tackle diverse cross-modal visual reasoning tasks, including vi- sual question answering (VQA) (Prasad et al., 2024; Wu et al., 2024), visual entailment (VE) (Kayser et al., 2021), and visual commonsense reasoning (VCR) (Chang et al., 2024; Sammani et al., 2022). By leveraging the substantial information embedded within VLMs, novel insights are gen- erated to advance reasoning and interpretability research."}, {"title": "3. Method", "content": "In this section, we provide detailed descriptions of VIKSER'S F-VKE module and S-RR module. Specifi- cally, the F-VKE module consists of two agents: the visual relationship detector (Ag-VRD) and the visual knowledge enRicher (Ag-VKR), where Ag-VKR consists of a causal re- lationship analyzer Ga and an image caption generator Ge."}, {"title": "3.1. Fine-Grained Visual Knowledge Extraction", "content": "Guided by the principles of multi-agent collaboration, the functionality of the F-VKE module in extracting fine- grained visual knowledge is distributed across two agents: the visual relationship detector Ag-VRD and the visual knowledge enRicher Ag-VKR. The architectural design of Ag-VRD and the Ag-VKR is elaborated as follows."}, {"title": "Visual Relationship Detector", "content": "Accurate detection of enti- ties in images and their relationships form the foundation for effective VKE techniques. With the aim of endowing the F-VKE module with efficient visual relationship detec- tion capabilities, we design the Ag-VRD based on existing VRD methods. Concretely, given an input image I, the Ag-VRD is prompted to extract all entities within the image. Each entity is then assigned a validity score $S_e$, reflecting its relevance in understanding the image's content. Notably, when the entity's score exceeds a predefined threshold $\\theta_e$, it is classified as a key entity $K_e$. Following this, with the aid of CoVLM (Li et al., 2024), we extract all relevant vi- sual relationships of $K_e$. CoVLM is an exceptional VRD method that facilitates the collaboration between visual de- tection networks and LLMs, utilizing specially designed communication tokens. The advantage of CoVLM lies in its ability to precisely identify the target entity $E_B$ correspond- ing to a given relationship based on the input entity $E_A$, by leveraging the specially crafted communication tokens. However, due to the fixed structure of its communication tokens, CoVLM is limited in offering fine-grained visual"}, {"title": "3", "content": "knowledge. Therefore, we utilize CoVLM primarily for refining the detected visual relationships.\nTo be specific, we first employ the detection network of CoVLM to identify the regions of interest corresponding to each $K_e$ in I. Based on these regions and their associated entities, the Ag-VRD is then prompted to detect all potential relationships associated with $K_e$. Subsequently, we employ CoVLM to detect the relevant entities corresponding to these potential relationships. These potential relationships are then merged with their associated entities to form the visual relationship r. Following this, the Ag-VRD is employed to assess a relationship validity score $S_r$ for each r based on its effectiveness in contributing to the understanding of I. To determine which visual relationships associated with the key entities are truly effective in capturing the semantics of the input image, we propose a joint entity-relation validity evaluation algorithm to compute the entity-relation joint validity score $S_e$:\n$S = S_eW_r$, $W_r = 1 + \\gamma(\\alpha \u2013 N_e)$,(1)\nwhere $S_e$ denotes the entity validity score and $S_r$ denotes the relationship validity score. w represents the weight as- sociated with the current visual relationship r. $N_e$ denotes the number of r held by the key entity $K_e$. $\\gamma$ and $\\alpha$ are hyperparameters, where $\\gamma$ controls the influence of the num- ber of visual relationships on the weight, constrained within the range (0.05, 0.2). And $\\alpha$ limits the number of visual relationships. For example, as $N_e$ increases, $W_r$ decreases to amplify the constraint of $S_e$ on the visual relationship. Conversely, $W_r$ increases when the number of relationships is smaller. Additionally, in case $S_e$ exceeds a threshold $\\theta$, the corresponding visual relationship under the current key entity is labeled as a key visual relationship $K_r$. After identifying $K_e$ and $K_r$, we generate the preliminary image description D by progressively aligning each $K_e$ with its corresponding $K_r$.\nVisual Knowledge Enricher Existing VRD methods pri- marily focus on detecting surface-level, concept-based re- lational information within an image (Li et al., 2024; Yang et al., 2024b). This nature makes it challenging for current VRD methods to conduct in-depth, object-level analysis of the image content. However, such analyses are essential for a comprehensive understanding of the image. For this rea- son, we further deploy Ag-VKR, which consists of a causal relationship analyzer $G_a$ and an image caption generator $G_e$, to enhance the depth of knowledge in image analysis. Specifically, $G_a$ first interprets the preliminary image de- scription D concerning the input image I. Next, $G_a$ derives an analysis report A that uncovers the causal relationships between the behaviors of key entities in D and their inferred outcomes. Subsequently, $G_e$ enriches D by incorporating the visual knowledge from I and A, resulting in a detailed image caption C for I."}, {"title": "3.2. Self-Reinforcing Reasoning", "content": "We leverage the detailed visual knowledge in the image caption C, which is generated by the F-VKE module, to enhance the performance of VIKSER in visual reasoning tasks, such as VQA and VE. To achieve this, we propose an S-RR module, consisting of a specification paraphraser Ag- SPR and a self-refining reasoner Ag-SR, to endow VIKSER with advanced reasoning capabilities. Specifically, starting with an image and a question, Ag-SPR extracts and analyzes the fine-grained visual knowledge in C, to refine ambiguous descriptions in the question and paraphrase it. Subsequently, Ag-SR resolves the paraphrased question by leveraging the input image and the detailed image caption through a self- reinforcing reasoning paradigm. It is important to empha- size that we aim to develop a reasoning approach charac- terized by high generalizability and a gradient-free, in light of the diversity of visual reasoning tasks.. For this reason, we propose employing a flexible and plug-and-play reason- ing prompt paradigm to prompt pre-trained visual language models (PVLMs) to perform the functions of Ag-SPR and Ag-SR. Detailed prompts are discussed in Appendix A.2. The architectural design of Ag-SPR and the Ag-SR is elabo- rated as follows."}, {"title": "Specification Paraphraser", "content": "Starting with the input image I, the original question Q, and the detailed image caption C generated by the F-VKE module, Ag-SPR leverages the detailed visual information in C to paraphrase Q, thereby refining any ambiguous descriptions within Q. We employ a PVLM as the core of Ag-SPR to facilitate this process. Specifically, we prompt the PVLM in Ag-SPR to first iden- tify the main subject in Q, and then query C for relevant"}, {"title": "5", "content": "textual information. Meanwhile, the PVLM is prompted to extract the interaction between the main subject and the image scene from C, providing a complementary descrip- tion of the main subject's context. Subsequently, the above textual information is paraphrased in natural language and integrated into Q to form the paraphrased question $Q_r$. The underlying logic of this process is that the entities mentioned inQ intuitively provide key information relevant to the ex- pected answer intuitively. For this reason, providing more detailed descriptions of the subject aids the S-RR module in resolving the visual reasoning task.\nSelf-Refining Reasoner As a key component of the S-RR module, the cognitive and reasoning capabilities of Ag-SR significantly influence the overall problem-solving perfor- mance of the system. To enhance Ag-SR's capabilities, we introduce a prompting technique called CoE, which guides Ag-SR to reason incrementally and provide highly inter- pretable steps toward the expected answer of $Q_r$. Further- more, we present a self-reflection mechanism that allows Ag-SR to perform self-correction, thereby improving its ro- bustness in reasoning. Detailed descriptions are as follows.\nHighly Interpretable Reasoning: We prompt a PVLM to serve as the core of Ag-SR. Given that conventional reason- ing prompts, such as \"think step-by-step\" (Wei et al., 2022), often induce hallucinations due to insufficiently grounded rationale, we propose CoE, a tailored prompting technique, to enhance the PVLM's reasoning ability. Concretely, the PVLM first analyzes fine-grained visual knowledge pro- vided by the F-VKE module, extracting factual informa- tion to serve as evidence for reasoning. Subsequently, CoE guides the PVLM through a structured, step-by-step rea- soning process based on the extracted evidence, ultimately generating a predicted answer $\\tilde{a}$. This reasoning framework enables CoE to enhance interpretability while improving reasoning accuracy. Further details on CoE prompting are provided in Appendix A.2.\nSelf-Reflection-Based Reinforcement Mechanism: Existing research has shown that even highly intelligent agents are susceptible to generating low-quality answers, which sig- nificantly diminishes the accuracy of their reasoning and decision-making processes. To address this, we propose a self-reflection mechanism that builds upon the method pro- posed by Shinn et al (Shinn et al., 2024). The mechanism enables Ag-SR to manage instances of low-quality answers by leveraging past experiences, thereby reducing the likeli- hood of similar errors occurring in the future. Specifically, after deriving $\\tilde{a}$, Ag-SR utilize an exact matching mecha- nism to map complex natural language information in $\\tilde{a}$ to a two-dimensional discrete binary reward score $S_{ref}$. If $S_{ref}$ is positive, the predicted answer $\\tilde{a}$ is directly used as the final answer a. In contrast, when $S_{ref}$ is represented as negative, Ag-SR is prompted to analyze $Q_r$ and a reasoning"}, {"title": "VIKSER", "content": "trajectory t from the previous trial. Subsequently, Ag-SR reflects on the cause of failure and generates a detailed ver- bal reflection, denoted as $V_{ref}$. Following this, $V_{ref}$ serves as experiential knowledge to guide Ag-SR in generating a more accurate answer in subsequent trials. It is important to note that, compared to the reward score $S_{ref}$, $V_{ref}$ en- compasses more comprehensive experiential information. For instance, while $S_{ref}$ merely indicates success or failure, $V_{ref}$ provides insight into which aspects of the previous reasoning trajectory were incorrectly applied, resulting in previous failure. Furthermore, $V_{ref}$ can provide guidance on how to avoid similar mistakes in future trials."}, {"title": "4. Experiments", "content": "In this section, we conduct extensive experiments on pub- licly available datasets to evaluate VIKSER's performance."}, {"title": "4.1. Experimental Setup", "content": "In our experiments, we prompt GPT-40 mini (OpenAI, 2024) as the LLM foundation for Ag-VRD of the F-VKE module in VIKSER. Additionally, we employ LLaVa-1.5-7B (Liu et al., 2023b) as the PVLM foundation for the S-RR module of VIKSER. It is important to note that in Section 3.1, the LLM used for knowledge distillation is ChatGPT-4 (Achiam et al., 2023), while Ga and Ge are obtained through train- ing LLaVa-1.5-7B. On the other hand, a summary of the experimental setup is provided, which includes the datasets, baseline methods, and evaluation metrics employed for per- formance assessment.\nDatasets: To evaluate the reasoning abilities of VIKSER, we utilize widely-used datasets for visual reasoning tasks, in- cluding VQAv2 (Goyal et al., 2017), A-OKVQA (Schwenk et al., 2022), VizWiz (Gurari et al., 2018), e-SNLI- VE (Kayser et al., 2021), COLA (Ray et al., 2024), and CREPE (Ma et al., 2023).\nBaselines: We compare VIKSER with 12 competitive base- lines, including SMOLA-PaLI-X (Wu et al., 2024), LLaVa- 1.5-7B (Liu et al., 2023b), REPARE (Prasad et al., 2024), CoVLM (Li et al., 2024), Molmo-72B (Deitke et al., 2024), PaLi-X (Chen et al., 2022), PaliGemma 2 (Steiner et al., 2024), NLX-GPT (Sammani et al., 2022), Rapper (Chang et al., 2024), e-UG (Kayser et al., 2021), OFX-X (Pl\u00fcster et al., 2022), MosaiCLIP (Singh et al., 2023), CLIP+ MM- Pred (Ray et al., 2024), and CLIP+ Linear (Ray et al., 2024).\nEvaluation Metrics: Based on previous work (Ray et al., 2024; Ma et al., 2023; Prasad et al., 2024; Chang et al., 2024), we adopt appropriate dataset-specific metrics for evaluation. Specifically, (1) For the VQAv2 dataset, we evaluate accuracy separately for the yes/no (Y/N), num- ber (Num.), and other (Other) types of questions, and ad- ditionally evaluate overall accuracy (Overall). (2) For the"}, {"title": "6", "content": "A-OKVQA dataset, we evaluate both Direct Answer accu- racy (DA.) and Multiple Choice accuracy (MC.). (3) For the VizWiz dataset, we evaluate overall accuracy (Overall). (4) For the Cola, CREPE, and e-SNLI-VE datasets, we as- sess Accuracy (Acc.). Notably, for the CREPE dataset, we further evaluate Recall@1 (R@1)."}, {"title": "4.2. Main Results", "content": "We conduct extensive experiments to validate VIKSER'S visual reasoning capabilities, with the main experimental results summarized as follows.\nVIKSER performs outstandingly in reasoning and an- swering visual questions. To validate the superiority of VIKSER's visual reasoning capabilities, we compare its per- formance with eight competitive baselines on the VQAv2, A-OKVQA, and VizWiz datasets, as presented in Table 1. To be specific, VIKSER achieves new SOTA results across all datasets, with the exception of the Num. metric on the VQAv2 dataset. Notably, when LLaVa-1.5-7B is used as the PVLM foundation for the S-RR module, VIKSER significantly outperforms LLaVa-1.5-7B across all metrics. In particular, on the MC. metric of the A-OKVQA dataset, VIKSER achieves an almost 30% improvement over LLaVa- 1.5-7B. Meanwhile, VIKSER, employing a gradient-free reasoning paradigm, continues to outperform task-specific methods pre-trained on the datasets. These substantial ad- vantages demonstrate the superiority and generalizability of VIKSER's reasoning paradigm. Moreover, VIKSER, with fewer parameters, outperforms baselines with more parame- ters, such as Molmo-72B and PaliGemma-2 (10B), across all metrics except for Num. The advancements highlight VIKSER's potential in visual reasoning tasks.\nOn the other hand, we analyze the reasons for VIKSER's slight underperformance on the Num. metric of the VQAv2 dataset as follows: (1)VIKSER has fewer model parameters, larger models, such as Molmo-72B, benefit from a larger pa- rameter size; (2) VIKSER prioritizes extracting information from key entities in the image, which may lead to a poten- tial risk of overlooking a holistic analysis of the image. For these reasons, VIKSER performs slightly worse on visual reasoning tasks involving numerical questions compared to more powerful LLMs, such as Molmo-72B. Nonetheless, we believe there is still room for improvement in VIKSER'S numerical analysis performance.\nVIKSER performs excellently in reasoning and under- standing visual information. To further evaluate the rea- soning performance of VIKSER, we compare it with nine competitive baselines on the Cola, CREPE, and e-SNLI-VE dataset, as shown in Table 2 and Figure 4. Specifically, VIKSER outperforms existing methods, achieving a new SOTA result. It is worth noting that both the Cola and CREPE datasets require a model to precisely match images"}, {"title": "7", "content": "with captions. Therefore, the success of VIKSER highlights its capacity to accurately interpret image content and effec- tively extract information from key entities, enabling it to perform visual reasoning tasks with high precision."}, {"title": "4.3. Ablation Studies", "content": "We conduct various ablation experiments to evaluate the performance of each component of VIKSER. Specifically, on the VQAv2 dataset, we systematically ablate the Ag-SPR"}, {"title": "8", "content": "On the other hand, to evaluate the capability of the F-VKE module, we conduct a qualitative analysis of the fine-grained visual knowledge extracted from two images. As shown in Figure 5, the F-VKE module precisely captures the fine- grained details within the images. Specifically, in the above image, F-VKE describes the boy's appearance (e.g., \"blonde hair\" and \"brown shirt\") and his physical actions (e.g., \"standing\u201d and \u201cholding\u201d). These valuable details contribute to a deeper comprehension of the image, facilitating the interpretation of its content by intelligent agents. Addition- ally, F-VKE further uncovers causal relationships embedded within the fine-grained details of the image (e.g., \"he just took a bite of the pastry\" and \"the scene is taking place in colder weather\"). The discovery suggests that F-VKE effec- tively learns the causal relationship uncovering capabilities of LLMs through the knowledge distillation process. More- over, F-VKE also addresses additional aspects, such as the boy's expressions (\u201ccurious\u201d and \u201cfocused\u201d), indicating that the boy is likely exploring the taste of the pastry. By linking these details to the action \"took a bite of the pastry\", F-VKE constructs a more coherent causal pathway. Similarly, in the lower image, F-VKE accurately extracts fine-grained visual knowledge, including the man's attire and physical actions, and further uncovers the underlying causal relationships (e.g., \"he might disembark at the next station\"). The fine- grained visual knowledge extracted by VIKSER'S F-VKE module includes detailed entity information as well as the causal relationships between entity behaviors and inferred outcomes, thereby offering significant assistance in solving related visual reasoning tasks."}, {"title": "4.4. Case Studies", "content": "To further validate the effectiveness of VIKSER, we conduct a qualitative analysis of its performance on two practical visual reasoning cases. As shown in Figure 6, in case (a), VIKSER identifies that the motorcyclist has a lit cigarette in his mouth while riding, by generating an image caption that contains fine-grained visual knowledge. This discovery serves as the foundation for solving the case (a). Subse-"}, {"title": "5. Conclusion", "content": "In this paper, we introduce VIKSER, a new framework for visual reasoning tasks, which integrates the extraction of fine-grained and enriched visual knowledge with high interpretability and self-reinforcing reasoning capabilities. VIKSER comprises F-VKE and S-RR modules, as well as integrates advanced mechanisms to achieve superior perfor- mance. We conduct extensive experiments on diverse public datasets. The results demonstrate that VIKSER outperforms the latest research across public datasets, achieving new SOTA results. Our future work will focus on: 1) Exploring methods that extract finer-grained visual knowledge while performing more comprehensive visual reasoning based on overall image features. 2) Further integrating the capabili- ties of world models into VIKSER."}, {"title": "A. Appendix", "content": "A.1. Training the Image Caption Generator\nAfter obtaining the analysis reports KA,, the image caption generator G\u025b enriches the preliminary image description D by integrating the visual knowledge from the input image I and KA to generate a detailed image caption C for I. As discussed in Section 3.1, we utilize profound-ground-truth data generated by the LLM to train the Ge. Specifically, we extract pseudo-ground-truth image caption Cp from LLM with a task-specific set of few-shot demonstrations as follows:\n$K_{Cp} = \\{C_p | C_p ~ P_{LLM}(I, D, A_{a})\\}$,(6)\nwhere I denotes the input image, D represents the ground-truth preliminary image description of I, and A', \u0454 \u041a\u0410 represents the pseudo-ground-truth analysis report. PLLM denotes the LLM operating autoregressively. Cp denotes the the pseudo-ground-truth image caption sampled from PLLM, and KCp represents the set of all Cp.\nUnfortunately, KCp may contain noise and errors, adversely affecting the training of Ge. To address this, we similarly apply a post-processing mechanism to filter KAp into KA,. Specifically, for each Cp in KCp, we use F (the pre-trained MLLM) to assess its validity score Sc, based on whether Cp correctly introduces the image. If Sc, exceeds the predetermined threshold 7, the corresponding Cp is retained. The process of collecting KC, is formalized as follows:\n$K_{Cp} = \\{C_p | S_{C_p} > \\tau\\}$, $S_{C_p} = F (C_p, (I, D, A_{p}))$,(7)\nwhere Cp denotes the pseudo-ground-truth image caption. I denotes the input image, D represents the ground-truth preliminary image description. A represents the pseudo-ground-truth analysis report. 7 denotes the predetermined threshold. F denotes the pre-trained MLLM. With KC, serving as pseudo-ground-truth data, we are able to train Ge with the distillation loss LG, as formalized below:\n$L_{G_e} = \\sum_{t=1}^{T} log (P_{G_e} (C_{p,t} | C_{p,t-1}, (I, D, A'_{a})))$,(8)\nwhere $C \\in K_C, A' \\in K_{A}'$, and T = |C|. Finally, the trained Ge and Ga collaborate to generate a detailed image caption C for the input image I."}, {"title": "A.2. Generating Reasoning from PVLMs", "content": "As discussed in Section 3.2, given the diversity of visual reasoning tasks, we propose a generalized, plug-and-play reasoning prompt paradigm to prompt PVLMs to activate the capacities of the S-RR module. In this section, we provide a detailed discussion of the design methodology for each prompt."}, {"title": "Specification Paraphrase", "content": "After acquiring fine-grained visual knowledge from the F-VKE module, the S-RR module prompts a PVLM to paraphrase the question text that exhibits underspecification. The prompt for specification paraphrasing is shown in Figure 7."}]}