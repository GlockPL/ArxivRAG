{"title": "License Plate Detection and Character Recognition Using Deep Learning and Font Evaluation", "authors": ["Zahra Ebrahimi Vargoorani", "Ching Yee Suen"], "abstract": "License plate detection (LPD) is essential for traffic management, vehicle tracking, and law enforcement but faces challenges like variable lighting and diverse font types, impacting accuracy. Traditionally reliant on image processing and machine learning, the field is now shifting towards deep learning for its robust performance in various conditions. Current methods, however, often require tailoring to specific regional datasets. This paper proposes a dual deep learning strategy using a Faster R-CNN for detection and a CNN-RNN model with Connectionist Temporal Classification (CTC) loss and a MobileNet V3 backbone for recognition. This approach aims to improve model performance using datasets from Ontario, Quebec, California, and New York State, achieving a recall rate of 92% on the Centre for Pattern Recognition and Machine Intelligence (CENPARMI) dataset and 90% on the UFPR-ALPR dataset. It includes a detailed error analysis to identify the causes of false positives. Additionally, the research examines the role of font features in license plate (LP) recognition, analyzing fonts like Driver Gothic, Dreadnought, California Clarendon, and Zurich Extra Condensed with the OpenALPR system. It discovers significant performance discrepancies influenced by font characteristics, offering insights for future LPD system enhancements.", "sections": [{"title": "1 Introduction", "content": "Automatic License Plate Recognition (ALPR) systems face challenges due to environmental factors like light conditions, rain and dust, high speeds, varied angles, and poor image quality [1]. The font style on license plates also significantly affects recognition [2]. ALPR is crucial for traffic control, security enhancement, and law enforcement. Despite various techniques, including image processing and machine learning, there is still potential to improve accuracy in ALPR systems."}, {"title": "2 Related Work", "content": "In reference [7], researchers used a Convolutional Neural Network (CNN) to extract relevant features from license plates, identifying distinct visual patterns through convolutional layers. These features were then processed by a Recurrent Neural Network (RNN) with 36 hidden units to sequence the detected characters and accurately reconstruct the license plate information. By combining the spatial pattern recognition capabilities of CNNs with the sequential learning abilities of RNNs, they achieved a 76% accuracy rate for recognizing all characters on a license plate and a 95.1% accuracy rate per individual character. This approach highlights the complementary strengths of CNNs and RNNs: CNNs capture intricate visual details, while RNNs process sequences of characters in the correct order.\nIn reference [8], the Single Shot Detection (SSD) algorithm, coupled with Residual Network (ResNet) instead of the typical VGG Network, was used for license plate detection. ResNet helped prevent gradient vanishing, enhancing performance and achieving an 85.5% accuracy rate, better than VGG's 83.6%. This study emphasizes the importance of refining algorithms to improve effectiveness in computer vision tasks.\nReference [9] described modifications to the FAST-YOLO network for detecting vehicles and their license plates simultaneously, adapting YOLO for character detection and integrating heuristic strategies. This resulted in a 63.18% accuracy rate in detecting and recognizing license plates, surpassing the 55.47% accuracy of the Sighthound method. This improvement underscores the benefits of customizing neural network architectures and using heuristic techniques to enhance precision in computer vision tasks.\nThe research in [10] highlighted the impact of font characteristics on word recognition and reading comprehension in digital publishing. It introduced a new shape descriptor for recognizing ancient characters with high accuracy, accommodating variations like translation, rotation, and scaling. This study underlined the relationship between typography and readability, showing how design choices enhance digital reading experiences.\nThe study in [11] outlined the development of Sitka, a new serif typeface designed to enhance font legibility on digital displays. This typeface was part of a broader initiative aimed at facilitating legibility studies and iterative design processes, emphasizing trade-offs in design, especially regarding x-height, which affects the legibility of letters with ascenders and descenders. This research advances typeface design, highlighting challenges in optimizing legibility for narrow letters."}, {"title": "3 Proposed Methodology", "content": "While this work leverages established approaches in the field of license plate detection and recognition, it enhances them through strategic combinations and novel applications to achieve improved performance. The methodological framework consists of the following key components:"}, {"title": "3.1 License Plate Detection", "content": "The license plate detection is performed using a Faster R-CNN [3] (Region-based Convolutional Neural Network) architecture, which is widely recognized for its robustness and high detection accuracy, even in challenging conditions such as varying lighting, occlusions, and diverse plate designs. Our approach utilizes a ResNet-50 backbone pre-trained on ImageNet to balance accuracy and computational efficiency while extracting high-level features from input images. The Region Proposal Network (RPN) generates region proposals likely to contain license plates, refining and filtering these proposals to reduce false positives. These proposals undergo ROI (Region of Interest) pooling and are passed through fully connected layers for precise bounding box regression and classification."}, {"title": "3.2 License Plate Recognition", "content": "For the OCR task, we developed a CNN-RNN model integrated with Connectionist Temporal Classification (CTC) loss [5]. This combination is particularly effective for sequence-based recognition tasks such as reading the characters on a license plate. The CNN feature extractor, inspired by MobileNetV3, is lightweight yet powerful, designed to run efficiently on edge devices while extracting spatial features from the detected license plate images. The RNN sequence modeler, using a bidirectional LSTM (Long Short-Term Memory) network, processes the sequential features extracted by the CNN, capturing contextual information across the character sequence. The Connectionist Temporal Classification (CTC) loss allows the model to be trained end-to-end for sequence prediction without requiring pre-segmented training data, crucial for handling varying lengths and unaligned character sequences typical of license plates."}, {"title": "3.3 Model Training and Optimization", "content": "The recognition model underwent 100 iterations or epochs during the training phase to ensure optimal convergence and proficiency. We chose to use 100 iterations for training the model based on empirical observations. This number of iterations allowed for optimal convergence and proficiency without overfitting the model, balancing training time and performance. The learning rate was set to 0.01, determining the step size at which the stochastic gradient descent optimizer adjusted the model's parameters during the back-propagation process. This choice of learning rate was informed by empirical observations and experimentation, balancing the need for stable convergence and fine-grained parameter updates. To train the model's ROI (Region of Interest) heads, we utilized a batch size of 64. This batch size indicates the number of ROI samples per image used in the training process. By carefully selecting this batch size, we aimed to balance computational efficiency and the model's ability to generalize to varying regions of interest within license plate images.\nIn our study, OpenALPR serves as a baseline for comparison. This widely recognized system, developed by OpenALPR Technology Inc. and launched in May"}, {"title": "3.4 Data Augmentation and Synthetic Data Generation", "content": "To enhance the robustness of the recognition model, various data augmentation techniques were employed. These included rotation, perspective transform, color channel transform, and adding noise. Rotation was applied in the range of -15 to 15 degrees to simulate different viewing angles. Perspective transforms were used to mimic various distortions. Color channel transformations involved adjustments to hue (hsv_h: 0.015), saturation (hsv_s: 0.7), and value (hsv_v: 0.4) to account for different lighting conditions. Additionally, translations (translate: 0.1) and scaling (scale: 0.5) were applied to simulate different image perspectives, and a mosaic effect (mosaic: 1.0) was added to increase variability. These techniques significantly increased the dataset's variability, allowing the model to learn from a broader range of examples. The data augmentation and synthetic data generation contributed to improved model performance, particularly in handling challenging conditions such as occlusions, different lighting, and varied plate designs."}, {"title": "3.5 Font Type Evaluation Aspects", "content": "Our research evaluates font types based on several key characteristics from reference [2]: similar apexes, defined as the junction of a stem; positioning of crossbars, which are horizontal strokes connecting parts of a letter, with varied positions enhancing legibility; similar top and bottom counters, the open spaces within a letter, with identical bottom counters increasing the likelihood of confusion; similar bowls, the rounded parts of letters; identical spurs, small projections at the end of a curved stroke; identical horizontal strokes at the bottom or top of letters; unclear tails, short downward strokes that can be mistaken for other elements when not clear; and similar diagonal strokes, where identical placement and design can lead to confusion. These criteria were chosen because they"}, {"title": "4 Results and Discussion", "content": "For the training process of detection and recognition models, we leveraged the power of the NVIDIA Tesla T4 GPU. The NVIDIA Tesla T4 has 16GB of GPU memory, offering substantial computational resources and parallel processing capabilities. This GPU enabled us to accelerate the training and optimization of our models, effectively handling large-scale datasets."}, {"title": "4.1 Evaluation Metrics", "content": "The performance of our models on the test set was evaluated using various metrics. The average precision (AP) at different IOU thresholds, areas, and maximum detections provide insights into the model's object detection capabilities. For the recognition model, we used recognition rate, character error rate (CER) and recall ratio as evaluation metrics. Additionally, confusion matrices were computed to analyze the types of errors made by the recognition model."}, {"title": "4.2 Detection Results", "content": "The detection model maintained a high precision at a higher IoU threshold of 0.75. When considering different object sizes, the model exhibited varying performance. For small objects, the average precision needed to be computed (-1.000), indicating a lack of reliable detection. However, for medium-sized objects, the model achieved a reliable average precision. The model excelled in detecting large objects, achieving an Impressive average precision. These results demonstrate the effectiveness of our Faster R-CNN [3] model in accurately detecting license plates in various scenarios, highlighting its potential for real-world license plate recognition applications(see Fig. 3)."}, {"title": "4.3 Recognition Results", "content": "The recognition model was evaluated on two datasets: the UFPR-ALPR [6] dataset and our CENPARMI dataset. The results indicate a significant improvement in performance when using the CENPARMI dataset, attributed to the extensive data augmentation and synthetic data generation techniques employed during training. The results are summarized in the Table 2.\nThe higher recognition rate and lower CER on the CENPARMI dataset suggest that the model generalizes well to the varied conditions and plate designs encountered in real-world scenarios, and the effectiveness of our advanced training techniques. This improvement is particularly noteworthy given the diversity of the CENPARMI dataset, which includes images from different states with varying plate designs and used fonts, lighting conditions, and environmental factors.\nOne of the key innovations in our approach is the use of a lightweight backbone inspired by MobileNetV3 for the recognition model. This design choice ensures that the model can run efficiently on edge devices with limited computational resources. The lightweight architecture reduces the model's complexity and computational cost without compromising accuracy, making it suitable for real-time applications on devices such as smartphones, tablets, and embedded systems."}, {"title": "4.4 State-Wise Recognition Performance", "content": "The recognition model's performance was further analyzed on datasets from four different datasets in Canada and United States of America. The results for each dataset are detailed in the Table 3."}, {"title": "4.5 Font Analysis Results", "content": "In this section, we present five distinct sets of outcomes from evaluating fonts based on the recognition results of individual letters (see Fig. 8). From our confusion matrices and detailed analysis of the task of recognizing license plates, we can draw several conclusions:\nCENPARMI Dataset: Our CENPARMI dataset includes 1600 license plates from diverse California, New York, Ontario, and Quebec environments. We conducted license plate detection and recognition in each province separately because each dataset has a unique font.\n1. Quebec Dataset: The characters 'Q' and '0' often need clarification due to their similar shapes, with the 'Q's tail sometimes indistinct. The resemblance between '6' and 'G' stems from their similar curves and inner spaces. Confusion between 'Q' and 'D' can occur if the 'Q's tail blends with its bowl, resembling a 'D,' especially if both have a similar spur. 'W' and 'M' are also confusing due to their similar diagonal strokes and internal structure. Additionally, '6' and '4' can be mistaken for each other due to similarities in their upper structures and angles.\n2. Ontario Dataset: The characters 'Q' and '0' might be mistaken for each other due to their rounded bowls, especially if the 'Q' has a subtle tail. Similarly, '5' and '2' could be confused because of their upper curves and strokes, while '6' and 'G' might look alike due to their shape and internal space. Characters like '1' and 'I', as well as '4' and 'A', can be indistinguishable when depicted as simple vertical lines. 'V' and 'Y' can also appear similar if the 'Y' has a short or blended tail.\n3. California Dataset: The characters 'O' and '0' can easily be confused due to their similar rounded shapes. Similarly, 'F' and 'E' might be mistaken for each other because of their horizontal strokes, making it hard to notice the"}, {"title": "4. New York Dataset:", "content": "'O' and '0' can look very similar due to their tightly rounded shapes. The letters 'J,' 'I,' and 'T' might be mistaken for one another due to similar vertical strokes, especially when 'J' has a subtle curve and 'T' has a short horizontal stroke. 'V' and 'U' can also be confused if 'V' has a sharp vertex resembling a 'U' without the middle crossbar. 'X' and 'L' may appear similar if 'X's diagonal strokes blend together, looking like intersecting 'L's. In a condensed font, 'M' and 'W' might look like mirror images with unclear middle peaks and valleys."}, {"title": "UFPR-ALPR Dataset:", "content": "The confusion matrix highlights common character misidentifications due to their similar appearances(see Fig. 9). 'O' and '0' are often confused as both have round, unmarked shapes. 'I', '1', and 'T' can be mistaken for each other when depicted as straight lines, especially if 'T' has a"}, {"title": "5 Conclusion and Future Work", "content": "In summary, this research concentrates on detecting license plates and recognizing their characters through a dual-phase approach that utilizes a Region-based Convolutional Neural Network (R-CNN) for detection and a Connectionist Temporal Classification(CTC) network for character recognition. It also examines how font selection affects ALPR system performance. The paper introduces a novel dataset and underscores the importance of precise LPD systems. Future work could include enlarging the dataset, employing additional deep learning methods, expanding the system to encompass license plate recognition and generation, and selecting a more legible font for license plates. This research contributes to advancing intelligent transportation systems, supporting improved traffic management and security."}]}