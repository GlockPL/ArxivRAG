{"title": "DIVIDE AND CONQUER: LEARNING CHAOTIC DYNAMICAL\nSYSTEMS WITH MULTISTEP PENALTY NEURAL ORDINARY\nDIFFERENTIAL EQUATIONS", "authors": ["Dibyajyoti Chakraborty", "Seung Whan Chung", "Romit Maulik"], "abstract": "Forecasting high-dimensional dynamical systems is a fundamental challenge in various fields, such as\nthe geosciences and engineering. Neural Ordinary Differential Equations (NODEs), which combine\nthe power of neural networks and numerical solvers, have emerged as a promising algorithm for\nforecasting complex nonlinear dynamical systems. However, classical techniques used for NODE\ntraining are ineffective for learning chaotic dynamical systems. In this work, we propose a novel\nNODE-training approach that allows for robust learning of chaotic dynamical systems. Our method\naddresses the challenges of non-convexity and exploding gradients associated with underlying chaotic\ndynamics. Training data trajectories from such systems are split into multiple, non-overlapping\ntime windows. In addition to the deviation from the training data, the optimization loss term further\npenalizes the discontinuities of the predicted trajectory between the time windows. The window\nsize is selected based on the fastest Lyapunov time scale of the system. Multi-step penalty(MP)\nmethod is first demonstrated on Lorenz equation, to illustrate how it improves the loss landscape\nand thereby accelerating the optimization convergence. MP method can optimize chaotic systems\nin a manner similar to least-squares shadowing with significantly lower computational costs. Our\nproposed algorithm, denoted the Multistep Penalty NODE, is applied to chaotic systems such as\nthe Kuramoto-Sivashinsky equation and the two-dimensional Kolmogorov flow. It is observed that\nMP-NODE provide viable performance for such chaotic systems, not only for short-term trajectory\npredictions but also for invariant statistics that are hallmarks of the chaotic nature of these dynamics.", "sections": [{"title": "1 Introduction", "content": "Dynamical systems are ubiquitous with examples such as weather, fluid flows, chemical reactions, etc. Ordinary\ndifferential equations are commonly used to describe the time evoluation of deterministic dynamical systems. Many\ncommon dynamical systems show chaotic behavior (for example the behavior of the atmosphere), where the time\nevolution of their trajectories is extremely sensitive to initial conditions. For such systems, small perturbations in initial\nconditions are amplified leading to significant deviations over time. It must be noted that these deviations are physical"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Neural Ordinary Differential Equations", "content": "While interest in Neural Differential Equations has grown recently, its roots lie in substantially old literature. Chu et\nal.(1991) used artificial neural networks (ANNs) to predict the dynamics of a continuous time-evolving system [27].\nRico et al. (1992) used ANNs to predict short-term dynamics of experimental chemical reaction data [28]. To the best\nof our knowledge, they were the first to use and state the limitations of NODE in predicting dynamical systems. They\nalso used NODE to predict latent dynamics [29] (i.e., dynamics in a reduced-dimensional space) as an extension to their\nprevious work and further solved stiff ODE systems as well [30]."}, {"title": "2.2 Stable Data-Driven Prediction of Chaotic Systems", "content": "Learning long term stable dynamics of chaotic systems has been a challenge for data-driven modeling of dynamical\nsystems. It is well-known that capturing long term dynamics through gradient descent is difficult [56, 57]. Standard\ntime-series learning methods include recurrent neural networks (RNN) [58]. RNNs suffer from vanishing or exploding\ngradients [8] which motivated the development of LSTM [59], gated recurrent units (GRU)[60], and other derivative\nmethods that enforce constraints on the recurrence matrix of the RNN [61, 62, 63]. Other techniques popularly used\nfor prediction of chaotic systems are reservoir computing [64], teacher forcing [65], and multiple shooting [66]. An\ninteresting observation in all of the above works is that autoregressive models tend to overfit on short term dynamics and\ndiverge in longer rollouts. A solution to this is to use longer rollouts in training [67]. However, the gradients become\nless useful when backpropagating through longer rollouts as it is theoretically proven that gradients of RNN-type\nmethods always diverge for chaotic systems [26]. Recent developments have used loss functions based on invariant\nstatistics for stable prediction of chaotic systems [68]. However, these tend to rely on matching empirical statistics of"}, {"title": "2.3 Sensitivity Analysis for Chaotic systems", "content": "We make the claim that the solution to training data-driven models for learning chaotic dynamics may be found by\nfollowing in the footsteps of rich literature devoted to sensitivity analysis for chaotic systems modeled as differential\nequations. Kolmogorov et al.(1962) introduced the adjoint method for computing sensitivities (or gradients) of an\nobjective function with respect to parameters related to state variables by a differential equation [69]. Although it\nhas been explored extensively for optimization, the limitations of the adjoint method for obtaining sensitivity for\nchaotic systems is well recognized [18]. Wang et al. [70] proposed the Least Square Shadowing (LSS) method to\ncompute the gradient by computing the shadowing trajectory. Recently, several improvements to vanilla LSS have been\nproposed [20, 22, 20, 19]. Ni et al. (2019) improved upon standard adjoint sensitivity and proposed the Non-Intrusive\nLeast Square Adjoint Shadowing (NILSAS) technique for chaotic dynamical systems [21]. Chandramoorthy et al.\n(2020) proposed a procedure to use automatic differentiation for computing shadowing sensitivity [71]. However, an\nefficient method with low computational and memory requirement for obtaining sensitivity of chaotic systems is still an\nactive area of research."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Neural Ordinary Differential Equations", "content": "A Neural Ordinary Differential Equation [1] is defined as\n$\\frac{dq(t)}{dt} = R(t, q(t), \\Theta)$\n$q(0) = q_0,$\nwhere $R : R \\times \\mathbb{R}^{d_1\\times d_2...\\times d_n} \\times \\mathbb{R}^{d_{\\Theta}} \\rightarrow \\mathbb{R}^{d_1\\times d_2Xdn}$ is generally a simple feed-forward network, $\\Theta$ is a vector of\nlearnable parameters, and $q : \\mathbb{R} \\rightarrow \\mathbb{R}^{d_1\\times d_2...\\times d_n}$ is the solution to a differential equation. NODE are considered\nthe continuous limit of Residual Networks (ResNets) [72]. Since the evolution of hidden states in a Resnet is similar\nto Euler approximation of an ODE, the advantage of NODE is to use more efficient and accurate ODE solvers [73].\nSecondly, most advanced deep learning techniques like GRUs [60] and LSTMs [59] use autoregressive procedures that\nare very similar to discretized differential equations. Another advantage to using NODE is the ability to use the adjoint\nsensitivity method which can compute gradients with far-reduced memory requirements [74]."}, {"title": "3.2 Standard gradient computation for ODE-constrained optimization", "content": "Standard NODEs are effective for approximating non-chaotic dynamical systems. However, standard gradient-based\ntechniques to train NODEs are certain to lead to poor learning, since gradient computation in chaotic systems is ill-posed.\nEvidence for this was provided in Chung et al.(2022) [7] where it was demonstrated that standard gradient-based\noptimization fails for several chaotic systems. To further explain the specifics of this research, we start by defining a\nstandard ODE-constrained optimization problem defined as\n$\\underset{q, \\Theta}{\\text{minimize }} J[q, \\Theta] \\in \\mathbb{R} \\text{ such that } N[q; \\Theta] = 0,$\nwhere q and $\\Theta$ are constrained by an ODE,\n$N[q; \\Theta] = \\frac{dq}{dt} - R[q; \\Theta] = 0,$\nwith initial condition\n$q(t_i) = q_i.$"}, {"title": "3.3 Least Squares Shadowing", "content": "For chaotic systems, the tangent variable $v(t; \\Theta)$ in Equation 8 grows at an exponential rate (given by the Lyapunov\nExponent), failing the standard method to compute useful gradient for optimization. Ergodic quantities of the chaotic\ndynamics are proven to be differentiable, though the actual evaluation of their gradients is challenging [14, 15, 16, 17].\nThe Least Square Shadowing method exploits the Shadowing Lemma [78], which states that there exists a trajectory that\nis always close to the original trajectory for a small perturbation of the parameter. LSS obtains a shadowing trajectory $\\tilde{q}$\nwith respect to the reference solution $q$, for an arbitrary initial condition and a time transformation $\\tau(t)$ defined in\n$t_i = 0$ to $t_f = T$, by optimizing\n$\\underset{T,\\tilde{q}}{\\text{minimize }} \\frac{1}{T} \\int_0^T ||\\tilde{q}(\\tau(t)) - q(t)||^2dt,$\nsuch that $\\frac{d\\tilde{q}}{d\\tau} = \\tilde{R} (\\tilde{q}, \\Theta + d \\Theta) .$\nA modified version of Equation 9 which has a better condition number on solving is\n$\\underset{T,\\tilde{q}}{\\text{minimize }} \\frac{1}{T} \\int_0^T (||\\tau'(t)||^2 + \\alpha^2(\\tau'')^2)dt,$\nsuch that $\\frac{d\\tilde{q}}{d\\tau} = \\tilde{R} (\\tilde{q}, \\Theta + d \\Theta).$\nThe tangent form of Equation 10 is given as\n$\\underset{\\eta,\\nu}{\\text{minimize }} \\frac{1}{T} \\int_0^T (||v||^2 + \\alpha^2 \\eta^2)dt,$\nsuch that $\\frac{dv}{dt} = \\frac{d\\tilde{f}}{d\\tilde{q}}\\delta\\tilde{q} + \\frac{\\partial \\tilde{R}}{\\partial \\Theta}(\\tilde{q}, \\Theta),$\nwhich determines the tangent variables for $q$ and $\\frac{d \\tilde{q}}{d \\tau}$, respectively,\n$v(t) = \\frac{d}{d\\Theta} (q(\\tau(t; \\Theta); \\Theta)), \\eta(t) = \\frac{d \\tau'(t; \\Theta)}{d \\Theta}.$\nThe sensitivity of the objective can be computed using\n$\\frac{dT}{d\\Theta} = \\frac{2}{T} \\int_0^T (v+\\alpha^2 \\eta) \\frac{dT}{d \\Theta} dt, \\text{ where } J = \\frac{1}{T} \\int_0^T Idt.$\nThe optimization problem 11 can be solved by formulating the Lagrangian,\n$L = \\int_0^T (vv + \\alpha^2 \\eta^2 - 2w(\\frac{dv}{dt} - \\frac{\\partial R}{\\partial q}v - \\frac{\\partial R}{\\partial \\Theta} d \\Theta)) dt$"}, {"title": "3.4 Multistep Penalty Method", "content": "To address the issues with standard gradient-based optimization for chaotic dynamical systems we turn to a multi-step\npenalty method inspired by Chung et al.(2022)[7]. In this work, the authors proposed the introduction of intermediate\ninitial conditions ($q_{k+}$) such that for $t_k = t_i + kT$,\n$\\frac{dq}{dt} = R[q; \\Theta] = 0 \\text{ for } t \\in [t_k,t_{k+1})$\n$q(t_k) = q_{k} \\text{ for } k = 0, . . ., N - 1.$\nThe time domain is split based on the Lyapunov time of the system. This prevents the exploding gradient and the\nextremely non-convex objective functional. The intermediate constraints are additionally imposed,\n$\\Delta q_k = q_{k}^{+} - q(t_k) = 0 \\text{ for } k = 1, ..., N - 1,$\nwhere,\n$q(t_k) = q_{k-1} + \\int_{t_{k-1}}^{t_k} R[q; \\Theta]dt.$\nNow, a penalty loss $P$, which penalizes the intermediate constraints with a regularizing constant $\\mu$, is added to the\nobjective to form an augmented objective function $J_a$\n$Ja[q, \\Theta; \\{q\\}, \\mu] = I[q, \\Theta] + P[\\{\\Delta q_k\\}, \\mu],$\nWith $\\{q\\} = (q_1^{+}, q_2^{+},\\dots, q_{N-1}^{+})$ and $\\{\\Delta q_k\\} = (\\Delta q_1, \\Delta q_2, \\dots, \\Delta q_{N-1})$, the subproblem is formulated as,\n$\\{q\\} = argmin_{\\{q\\},\\Theta} Ja[q, \\Theta; \\{q\\}, \\mu],$\nAdjoint sensitivity or automatic differentiation can be used to compute the gradients of Equation 19. The multi-step\npenalty (MP) method can solve the problems of exploding gradients in chaotic optimization as shown further in the\npaper. The computational cost of this method is of the same order as the cost of computing gradients in the classical\nway using automatic differentiation."}, {"title": "3.5 Multi-step Penalty Neural Ordinary Differential Equations", "content": "The requirement of an efficient algorithm for backpropagating through long rollouts is necessary to learn invariant\nstatistics of chaotic systems. Consequently, we propose a multistep penalty optimization to train NODE which has\nsimilar computational cost as vanilla NODE. A schematic for comparison between a vanilla NODE and MP-NODE is\nshown in figure 1. The multistep penalty optimization can be extended to NODE by using a neural network as the RHS\nof Equation 16 and changing the loss function I to a Mean Squared Error(MSE) loss between the prediction and the\nground truth. We propose to use the Multi-step Penalty Neural ODE (MP-NODE) defined as,\n$\\frac{dq(t)}{dt} = R(q(t), t, \\Theta) = 0 \\text{ for } t \\in [t_k,t_{k+1})$\n$q(t_k) = q_{k}^{+} \\text{ for } k = 0, . . ., n - 1.$"}, {"title": "4 Results", "content": ""}, {"title": "4.1 Lorenz-63 System", "content": ""}, {"title": "4.1.1 Problem 1", "content": "We take the same problem as mentioned in the original LSS [70] paper for a proof of concept demonstration of the\nproposed method. It introduces a controlled Lorenz-63 system,\n$\\frac{dx}{dt} = \\sigma(y - x)$\n$\\frac{dy}{dt} = x(\\rho - z) - y$\n$\\frac{dz}{dt} = xy - \\beta z,$\nwhere $\\rho$ is the control parameter. The objective function for control is given by\n$J = \\underset{T \\rightarrow \\infty}{\\text{lim }} \\frac{1}{T} \\int_0^T z dt.$"}, {"title": "4.1.2 Problem 2", "content": "To illustrate how the MP optimization improves performance further, we explore the loss landscape of both vanilla and\nMP objective functions for a benchmark problem introduced in Chung et al.(2022) [7].\n$\\frac{dx}{dt} = \\sigma(y - x)$\n$\\frac{dy}{dt} = x(\\rho - z) - y$\n$\\frac{dz}{dt} = xy - \\beta z + f(t),$\nThe system is integrated from t = 0 to 20 with 2000 timesteps. The control $f(t)$ is taken as a vector of 2000 parameters\ndefined at each timesteps. The objective function for optimization is defined as\n$\\mathscr{I} = \\frac{1}{t_f-t_i} \\int_{t_i}^{t_f} \\mathscr{I}[q]dt,$\n$\\mathscr{I}[q] = \\begin{cases}\n\\frac{(2x+y)^2}{5} & \\text{2x + y  \\geq 0}\n\\text{0} & \\text{otherwise.}\n\\end{cases}.$\nChung et al.(2022) showed that MP optimization can reduce the loss function to 99.9% as compared to vanilla\noptimization which can reduce it to only 44.6% [7]. Figure 3 shows the comparison of loss landscape for vanilla and\nMP objective function varying two arbitrary parameters (from the 2000) for the time-varying control $f(t)$. It clearly\nshows that vanilla loss function is highly non-convex which hinders convergence during optimization."}, {"title": "4.2 Kuramoto Sivashinsky equation", "content": "The Kuramoto Sivashinsky(KS) equation is given by a fourth-order non-linear partial differential equation,\n$\\frac{\\partial q}{\\partial t} = -q\\frac{\\partial q}{\\partial x} - \\frac{\\partial^2 q}{\\partial x^2} - \\frac{\\partial^4 q}{\\partial x^4}.$\nSolutions to the KS equation show chaotic dynamics which eventually converge onto a low-dimensional inertial\nmanifold over long periods [82]. Predicting the long term trajectories of such chaotic systems is very challenging\nas small errors have large effects of the roll-outs after multiple Lyapunov time lengths. So, we aim to have a model"}, {"title": "4.3 Kolmogorov Flow", "content": "In this section, we study the performance of the proposed framework for two-dimensional homogeneous isotropic\nturbulence with Kolmogorov forcing governed by the incompressible Navier-Stokes equations. The purpose of these\nexperiments is to assess the capabilities of MP-NODE for significantly high-dimensional state as well as a large number\nof parameters. Forced two-dimensional turbulence exhibits the classical characteristics of chaotic dynamics and has\nbecome a popular benchmark for ML techniques used for prediction of dynamical systems [84, 68, 85]. The 2D\nNavier-Stokes Equation is given by\n$\\frac{\\partial u}{\\partial t} + \\nabla. (uu) = -\\frac{1}{\\rho} \\nabla p + \\frac{1}{R_e} \\nu \\nabla^2 u + f$\n$\\nabla . u = 0,$\nwhere $u = (u, v)$ is the velocity vector, $p$ is the pressure, $\\rho$ is the density, $R_e$ is the Reynolds number, and $f$ represents a\nforcing function given by\n$f = A sin(ky) \\hat{e_{x}} - ru$\nparameterized by the amplitude $A = 1$, wavenumber $k = 4$, linear drag $r = 0.1$, and the Reynolds number $Re = 1000$\nchosen for this study [85]. $\\hat{e_{x}}$ denotes the x-direction unit vector. The initial condition is taken as a random divergence\nfree velocity field [86]. The ground truth datasets are obtained from direct numerical simulation(DNS) [87] of the\ngoverning equations in a doubly periodic square domain with L = $2\\pi$ discretized in a uniform grid (512x512) and\nfiltered it to a coarser grid (64x64). Details for constructing this dataset can be found in the work by Shankar et al.(2023)\n[85]. The trajectories are temporally sampled after flow reaches the chaotic regime with a gap of T = 256\u2206tDNS to\nensure sufficient distinction between two snapshots empirically."}, {"title": "5 Discussion", "content": ""}, {"title": "5.1 Limitation and Future Works", "content": "In this section, we discuss some limitations of the proposed methodology. The primary issue is the increased requirement\nof memory for the longer rollouts. One approach to address this issue is to learn dynamics on a low-dimensional\nmanifold - however this may cause the loss of small scale information on the flow field. This is also observed in\nour Kolmogorov flow experiments as evident in Figures 7 and 8, where finer structures are smoothed. For systems\nwhere this is undesirable, future work can investigate memory-efficient adjoint sensitivity methods [90, 91, 92, 93]. A\nsecond limitation is the computation of stochastic gradients within the MP-NODE framework. The proposed method"}, {"title": "5.2 Conclusion", "content": "The advent of neural differential equations has opened new avenues by combining the flexibility of neural networks with\nthe accuracy of numerical solvers, allowing for improved predictability of dynamical systems. However, the challenge of\nlong term stable prediction for chaotic systems has mostly remained unsolved. Learning chaotic dynamics is challenging\nas the optimization problem becomes extremely non-convex, making the standard gradient useless. Existing methods\nlike least-squares shadowing, while attractive, suffer from very high computational costs for high-dimensional systems\nas well as overparameterized models. The algorithm proposed in this work solves this issue by dividing the time domain\ninto multiple steps and allowing a penalized discontinuity to calculate sensible gradients with linear computational\ncomplexity. The benefit of MP-NODE is that it can be extended to several time-series forecasting techniques that rely\non multistep loss calculations and backpropagation through time. Moreover, this advancement has the potential to\nimprove understanding and predictability of chaotic systems and has broader implications for several scientific domains."}, {"title": "6 Appendix", "content": ""}, {"title": "6.1 Algorithms", "content": ""}]}