{"title": "DIVIDE AND CONQUER: LEARNING CHAOTIC DYNAMICAL SYSTEMS WITH MULTISTEP PENALTY NEURAL ORDINARY DIFFERENTIAL EQUATIONS", "authors": ["Dibyajyoti Chakraborty", "Seung Whan Chung", "Romit Maulik"], "abstract": "Forecasting high-dimensional dynamical systems is a fundamental challenge in various fields, such as the geosciences and engineering. Neural Ordinary Differential Equations (NODEs), which combine the power of neural networks and numerical solvers, have emerged as a promising algorithm for forecasting complex nonlinear dynamical systems. However, classical techniques used for NODE training are ineffective for learning chaotic dynamical systems. In this work, we propose a novel NODE-training approach that allows for robust learning of chaotic dynamical systems. Our method addresses the challenges of non-convexity and exploding gradients associated with underlying chaotic dynamics. Training data trajectories from such systems are split into multiple, non-overlapping time windows. In addition to the deviation from the training data, the optimization loss term further penalizes the discontinuities of the predicted trajectory between the time windows. The window size is selected based on the fastest Lyapunov time scale of the system. Multi-step penalty(MP) method is first demonstrated on Lorenz equation, to illustrate how it improves the loss landscape and thereby accelerating the optimization convergence. MP method can optimize chaotic systems in a manner similar to least-squares shadowing with significantly lower computational costs. Our proposed algorithm, denoted the Multistep Penalty NODE, is applied to chaotic systems such as the Kuramoto-Sivashinsky equation and the two-dimensional Kolmogorov flow. It is observed that MP-NODE provide viable performance for such chaotic systems, not only for short-term trajectory predictions but also for invariant statistics that are hallmarks of the chaotic nature of these dynamics.", "sections": [{"title": "1 Introduction", "content": "Dynamical systems are ubiquitous with examples such as weather, fluid flows, chemical reactions, etc. Ordinary differential equations are commonly used to describe the time evoluation of deterministic dynamical systems. Many common dynamical systems show chaotic behavior (for example the behavior of the atmosphere), where the time evolution of their trajectories is extremely sensitive to initial conditions. For such systems, small perturbations in initial conditions are amplified leading to significant deviations over time. It must be noted that these deviations are physical"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Neural Ordinary Differential Equations", "content": "While interest in Neural Differential Equations has grown recently, its roots lie in substantially old literature. Chu et al.(1991) used artificial neural networks (ANNs) to predict the dynamics of a continuous time-evolving system [27]. Rico et al. (1992) used ANNs to predict short-term dynamics of experimental chemical reaction data [28]. To the best of our knowledge, they were the first to use and state the limitations of NODE in predicting dynamical systems. They also used NODE to predict latent dynamics [29] (i.e., dynamics in a reduced-dimensional space) as an extension to their previous work and further solved stiff ODE systems as well [30].\nRecently, NODE were popularized in the era of data-intensive machine learning by the work of Chen et al. [1]. We denote their formulation the 'vanilla' NODE. Various improvements and modifications have been proposed for the vanilla NODE. Since NODE preserves the topology of the input space, which limits the approximation of a family of functions, Dupont et al.(2019) [6] developed an augmented NODE that is more stable and generalizable. They proposed to augment the input space that can avoid intersections of ODE trajectories by providing additional dimensions to lift the points. Norcliffe et al.(2020) [31] went beyond the augmented NODE to explicitly incorporate the second-order behavior of dynamical systems. The vanilla NODE just evolves the dynamics based on initial condition and has no mechanism to adjust its trajectory based on subsequent observations. Neural Controlled Differential Equations (NCDE) tackles this issue by encompassing incoming data using Riemann\u2013Stieltjes integral in the training of a NODE to improve its performance for irregular time series [32]. NCDEs can be interepreted as an approximation for Recurrent Neural Networks (RNNs) with infinite depth. Ghosh et al. (2020) proposed the temporal regularization of NODE for improving performance and reducing training time [33]. Kim et al. (2021) showed that modified ODE solvers like Implicit-Explicit methods can boost the capability of NODE to solve stiff dynamical systems [5]. In our past work, Linot et al. (2023) added a stability-promoting linear term to the right-hand side of Equation 1, which improved its capability to predict the long-term dynamics of chaotic systems [34]. Yildiz et al. (2019) presented a generative model based on NODE using Variational Auto-Encoders[35]. Zhang et al. (2019) used a coupled ODE architecture for evolving the weight and activation functions of a NODE [36]. They demonstrated significant gains in accuracy with the same memory and computational time cost. Poli et al. (2019) used a graph neural network to represent the function on the right hand side of a NODE [37]. Liu et al. (2019) [38] used stochastic noise to stabilize NODE. Massaroli et al. (2020) used orthogonal basis functions to represent the parameters of a NODE and improved its interpretability and generalizability [39]. Chang et al. (2023) developed an architecture to generate complete time series using incomplete data using Generative Adversarial Networks (GANs) [40].\nNODE has been heavily studied for applications to predicting various dynamical systems. Maulik et al. (2020) compared NODE and long short-term memory (LSTM) for predicting the viscous burgers equation[41]. It has also been used to perform the time series modeling of turbulent kinetic energy [42], stochastic differential equations [43], reduced order modeling for dynamics of the flow past a cylinder [44], modeling the chaotic dynamics in Kuramoto-Sivashinsky equations [45], prediction of battery degradation [46], climate modeling [3], continuous emotion prediction [2], link prediction in graphs [47], improving the adversarial robustness of image classifiers [48], efficient solving of chemical kinetics [49], and chaotic complex Ginsburg Landau equation [50]. Among these studies, we specifically point out the framework of the multiple-shooting NODE [51] which uses the concept of introducing local discontinuities in the training data trajectories. This is found to be beneficial for purposes such as training with irregularly sampled trajectories [52], improving efficiency and scalability [53], learning from sparse observations [54], and forecasting with limited training data [55]. However, the connection to learning the invariant statistics of chaotic dynamics has not been explored."}, {"title": "2.2 Stable Data-Driven Prediction of Chaotic Systems", "content": "Learning long term stable dynamics of chaotic systems has been a challenge for data-driven modeling of dynamical systems. It is well-known that capturing long term dynamics through gradient descent is difficult [56, 57]. Standard time-series learning methods include recurrent neural networks (RNN) [58]. RNNs suffer from vanishing or exploding gradients [8] which motivated the development of LSTM [59], gated recurrent units (GRU)[60], and other derivative methods that enforce constraints on the recurrence matrix of the RNN [61, 62, 63]. Other techniques popularly used for prediction of chaotic systems are reservoir computing [64], teacher forcing [65], and multiple shooting [66]. An interesting observation in all of the above works is that autoregressive models tend to overfit on short term dynamics and diverge in longer rollouts. A solution to this is to use longer rollouts in training [67]. However, the gradients become less useful when backpropagating through longer rollouts as it is theoretically proven that gradients of RNN-type methods always diverge for chaotic systems [26]. Recent developments have used loss functions based on invariant statistics for stable prediction of chaotic systems [68]. However, these tend to rely on matching empirical statistics of"}, {"title": "2.3 Sensitivity Analysis for Chaotic systems", "content": "We make the claim that the solution to training data-driven models for learning chaotic dynamics may be found by following in the footsteps of rich literature devoted to sensitivity analysis for chaotic systems modeled as differential equations. Kolmogorov et al.(1962) introduced the adjoint method for computing sensitivities (or gradients) of an objective function with respect to parameters related to state variables by a differential equation [69]. Although it has been explored extensively for optimization, the limitations of the adjoint method for obtaining sensitivity for chaotic systems is well recognized [18]. Wang et al. [70] proposed the Least Square Shadowing (LSS) method to compute the gradient by computing the shadowing trajectory. Recently, several improvements to vanilla LSS have been proposed [20, 22, 20, 19]. Ni et al. (2019) improved upon standard adjoint sensitivity and proposed the Non-Intrusive Least Square Adjoint Shadowing (NILSAS) technique for chaotic dynamical systems [21]. Chandramoorthy et al. (2020) proposed a procedure to use automatic differentiation for computing shadowing sensitivity [71]. However, an efficient method with low computational and memory requirement for obtaining sensitivity of chaotic systems is still an active area of research."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Neural Ordinary Differential Equations", "content": "A Neural Ordinary Differential Equation [1] is defined as\n$\\frac{dq(t)}{dt} = R(t, q(t), \\Theta)$\n$q(0) = q_0,$\nwhere $R : \\mathbb{R} \\times \\mathbb{R}^{d_1\\times d_2...\\times d_n} \\times \\mathbb{R}^{d_{\\Theta}} \\rightarrow \\mathbb{R}^{d_1\\times d_2 X dn}$ is generally a simple feed-forward network, $\\Theta$ is a vector of learnable parameters, and $q : \\mathbb{R} \\rightarrow \\mathbb{R}^{d_1\\times d_2\\ldots \\times d_n}$ is the solution to a differential equation. NODE are considered the continuous limit of Residual Networks (ResNets) [72]. Since the evolution of hidden states in a Resnet is similar to Euler approximation of an ODE, the advantage of NODE is to use more efficient and accurate ODE solvers [73]. Secondly, most advanced deep learning techniques like GRUs [60] and LSTMs [59] use autoregressive procedures that are very similar to discretized differential equations. Another advantage to using NODE is the ability to use the adjoint sensitivity method which can compute gradients with far-reduced memory requirements [74].\nNODE can achieve the same accuracy as ResNet [75] for the MNIST [76] dataset with one-third number of parameters due to its low memory cost [1] (i.e., it uses a fixed amount of memory that is constant with the depth of neural network). If memory is not a bottleneck, current automatic differentiation libraries like Jax [77] can efficiently use backpropagation to compute gradients for training NODE as well. Regardless of the mode of gradient computation, vanilla NODE are unable to learning chaotic dynamics [34] if trained using a simple mean-squared-error minimization. This motivates our algorithmic development described in the following sections."}, {"title": "3.2 Standard gradient computation for ODE-constrained optimization", "content": "Standard NODEs are effective for approximating non-chaotic dynamical systems. However, standard gradient-based techniques to train NODEs are certain to lead to poor learning, since gradient computation in chaotic systems is ill-posed. Evidence for this was provided in Chung et al.(2022) [7] where it was demonstrated that standard gradient-based optimization fails for several chaotic systems. To further explain the specifics of this research, we start by defining a standard ODE-constrained optimization problem defined as\n$\\text{minimize } J[q, \\Theta] \\in \\mathbb{R} \\text{ such that } N[q; \\Theta] = 0,$\nwhere q and $\\Theta$ are constrained by an ODE,\n$N[q; \\Theta] = \\frac{dq}{dt} - R[q; \\Theta] = 0,$\nwith initial condition\n$q(t_i) = q_i.$"}, {"title": "3.3 Least Squares Shadowing", "content": "For chaotic systems, the tangent variable $v(t; \\Theta)$ in Equation 8 grows at an exponential rate (given by the Lyapunov Exponent), failing the standard method to compute useful gradient for optimization. Ergodic quantities of the chaotic dynamics are proven to be differentiable, though the actual evaluation of their gradients is challenging [14, 15, 16, 17]. The Least Square Shadowing method exploits the Shadowing Lemma [78], which states that there exists a trajectory that is always close to the original trajectory for a small perturbation of the parameter. LSS obtains a shadowing trajectory $\\tilde{q}$ with respect to the reference solution $q$, for an arbitrary initial condition and a time transformation $\\tau(t)$ defined in $t_i = 0$ to $t_f = T$, by optimizing\n$\\underset{T,\\tilde{q}}{\\text{minimize}} \\frac{1}{T} \\int_0^T ||\\tilde{q}(\\tau(t)) - q(t)||^2dt,$\nsuch that $\\frac{d\\tilde{q}}{d\\tau} = R (\\tilde{q}, \\Theta + d\\Theta)$.\nA modified version of Equation 9 which has a better condition number on solving is\n$\\underset{T,\\tilde{q}}{\\text{minimize}} \\frac{1}{T} \\int_0^T (||\\tilde{q}(\\tau(t)) - q(t)||^2 + \\alpha^2(\\frac{d\\tau}{dt})^2)dt,$\nsuch that $\\frac{d\\tilde{q}}{d\\tau} = R (\\tilde{q}, \\Theta + d\\Theta)$.\nThe tangent form of Equation 10 is given as\n$\\underset{\\eta, v}{\\text{minimize}} \\frac{1}{T} \\int_0^T (||v||^2 + \\alpha^2\\eta^2)dt,$\nsuch that $\\frac{dv}{dt} = \\frac{\\partial f}{\\partial q} v + \\eta \\frac{\\partial R}{\\partial \\Theta}(q, \\Theta),$\nwhich determines the tangent variables for q and $\\tilde{\\tau}$, respectively,\n$v(t) = \\frac{d}{d \\Theta} \\left(q(\\tilde{\\tau}(t;\\Theta); \\Theta)\\right), \\eta(t) = \\frac{d\\tilde{\\tau}(t; \\Theta)}{d \\Theta}.$\nThe sensitivity of the objective can be computed using\n$\\frac{dT}{d\\Theta} = \\frac{1}{T} \\int_0^T (v + \\eta(I - J))dt, \\text{where } J = \\frac{1}{T} \\int_0^T Idt.$\nThe optimization problem 11 can be solved by formulating the Lagrangian,\n$L = \\int_0^T (v^{\\top}v + \\alpha^2\\eta^2 + 2w^{\\top}(\\frac{dv}{dt} - \\frac{\\partial f}{\\partial q}v - \\eta \\frac{\\partial R}{\\partial \\Theta})) dt$"}, {"title": "3.4 Multistep Penalty Method", "content": "To address the issues with standard gradient-based optimization for chaotic dynamical systems we turn to a multi-step penalty method inspired by Chung et al. (2022)[7]. In this work, the authors proposed the introduction of intermediate initial conditions $(q_k^+)$ such that for $t_k = t_i + kT,$\n$\\frac{dq}{dt} = R[q; \\Theta] = 0 \\text{ for } t \\in [t_k,t_{k+1}),$\n$q(t_k) = q_k^+ \\text{ for } k = 0, ..., N - 1.$\nThe time domain is split based on the Lyapunov time of the system. This prevents the exploding gradient and the extremely non-convex objective functional. The intermediate constraints are additionally imposed,\n$\\Delta q_k^+ = q_k^+ - q(t_k) = 0 \\text{ for } k = 1, ..., N - 1,$\nwhere,\n$q(t_k) = q_{k-1}^+ + \\int_{t_{k-1}}^{t_k} R[q; \\Theta]dt.$\nNow, a penalty loss $\\mathcal{P}$, which penalizes the intermediate constraints with a regularizing constant $\\mu$, is added to the objective to form an augmented objective function $J_a$\n$J_a[q, \\Theta; \\{q_k^+\\}, \\mu] = I[q, \\Theta] + \\mathcal{P}[\\{\\Delta q_k^+\\}, \\mu],$\nWith $\\{q_k^+\\} = (q_1^+, q_2^+, ..., q_{N-1}^+)$ and $\\{\\Delta q_k^+\\} = (\\Delta q_1, \\Delta q_2, ..., \\Delta q_{N-1})$, the subproblem is formulated as,\n$(\\{q_k^+\\}^*, \\Theta^*) = \\underset{\\{q_k^+\\},\\Theta}{\\text{argmin}} J_a[q, \\Theta; \\{q_k^+\\}, \\mu],$\nAdjoint sensitivity or automatic differentiation can be used to compute the gradients of Equation 19. The multi-step penalty (MP) method can solve the problems of exploding gradients in chaotic optimization as shown further in the paper. The computational cost of this method is of the same order as the cost of computing gradients in the classical way using automatic differentiation."}, {"title": "3.5 Multi-step Penalty Neural Ordinary Differential Equations", "content": "The requirement of an efficient algorithm for backpropagating through long rollouts is necessary to learn invariant statistics of chaotic systems. Consequently, we propose a multistep penalty optimization to train NODE which has similar computational cost as vanilla NODE. A schematic for comparison between a vanilla NODE and MP-NODE is shown in figure 1. The multistep penalty optimization can be extended to NODE by using a neural network as the RHS of Equation 16 and changing the loss function $\\mathcal{I}$ to a Mean Squared Error(MSE) loss between the prediction and the ground truth. We propose to use the Multi-step Penalty Neural ODE (MP-NODE) defined as,\n$\\frac{dq(t)}{dt} = R(q(t), t, \\Theta) = 0 \\text{ for } t \\in [t_k,t_{k+1})$\n$q(t_k) = q_k^+ \\text{ for } k = 0, ..., n - 1.$"}, {"title": "4 Results", "content": ""}, {"title": "4.1 Lorenz-63 System", "content": ""}, {"title": "4.1.1 Problem 1", "content": "We take the same problem as mentioned in the original LSS [70] paper for a proof of concept demonstration of the proposed method. It introduces a controlled Lorenz-63 system,\n$\\frac{dx}{dt} = \\sigma(y - x)$\n$\\frac{dy}{dt} = x(\\rho - z) - y$\n$\\frac{dz}{dt} = xy - \\beta z,$\nwhere $\\rho$ is the control parameter. The objective function for control is given by\n$J = \\lim_{\\tau \\rightarrow \\infty} \\frac{1}{T} \\int_0^T z dt.$"}, {"title": "4.1.2 Problem 2", "content": "To illustrate how the MP optimization improves performance further, we explore the loss landscape of both vanilla and MP objective functions for a benchmark problem introduced in Chung et al.(2022) [7].\n$\\frac{dx}{dt} = \\sigma(y - x)$\n$\\frac{dy}{dt} = x(\\rho - z) - y$\n$\\frac{dz}{dt} = xy - \\beta z + f(t),$\nThe system is integrated from t = 0 to 20 with 2000 timesteps. The control $f(t)$ is taken as a vector of 2000 parameters defined at each timesteps. The objective function for optimization is defined as\n$I = \\frac{1}{t_f - t_i} \\int_{t_i}^{t_f} I[q]dt,$\n$I[q] = \\begin{cases}\n\\frac{1}{\\pi(2x + y)^2} \\text{ if } 2x + y \\geq 0\\\\\n5 \\text{ otherwise}.\n\\end{cases}$"}, {"title": "4.2 Kuramoto Sivashinsky equation", "content": "The Kuramoto Sivashinsky(KS) equation is given by a fourth-order non-linear partial differential equation,\n$\\frac{\\partial q}{\\partial t} = -q\\frac{\\partial q}{\\partial x} - \\frac{\\partial^2 q}{\\partial x^2} - \\frac{\\partial^4 q}{\\partial x^4}.$\nSolutions to the KS equation show chaotic dynamics which eventually converge onto a low-dimensional inertial manifold over long periods [82]. Predicting the long term trajectories of such chaotic systems is very challenging as small errors have large effects of the roll-outs after multiple Lyapunov time lengths. So, we aim to have a model"}, {"title": "4.3 Kolmogorov Flow", "content": "In this section, we study the performance of the proposed framework for two-dimensional homogeneous isotropic turbulence with Kolmogorov forcing governed by the incompressible Navier-Stokes equations. The purpose of these experiments is to assess the capabilities of MP-NODE for significantly high-dimensional state as well as a large number of parameters. Forced two-dimensional turbulence exhibits the classical characteristics of chaotic dynamics and has become a popular benchmark for ML techniques used for prediction of dynamical systems [84, 68, 85]. The 2D Navier-Stokes Equation is given by\n$\\frac{\\partial u}{\\partial t} + \\nabla \\cdot (u \\otimes u) = -\\frac{1}{\\rho} \\nabla p + \\frac{1}{Re} \\nabla^2 u + f$\n$\\nabla \\cdot u = 0,$\nwhere $u = (u, v)$ is the velocity vector, $p$ is the pressure, $\\rho$ is the density, $Re$ is the Reynolds number, and $f$ represents a forcing function given by\n$f = A\\text{sin}(ky)\\hat{e}_{ru}$\nparameterized by the amplitude $A = 1$, wavenumber $k = 4$, linear drag $r = 0.1$, and the Reynolds number $Re = 1000$ chosen for this study [85]. $\\hat{e}$ denotes the x-direction unit vector. The initial condition is taken as a random divergence free velocity field [86]. The ground truth datasets are obtained from direct numerical simulation(DNS) [87] of the governing equations in a doubly periodic square domain with $L = 2\\pi$ discretized in a uniform grid (512x512) and filtered it to a coarser grid (64x64). Details for constructing this dataset can be found in the work by Shankar et al.(2023) [85]. The trajectories are temporally sampled after flow reaches the chaotic regime with a gap of $T = 256\\Delta t_{DNS}$ to ensure sufficient distinction between two snapshots empirically.\nFor the ML framework to predict these dynamics, we utilize an Encoder-NODE-Decoder architecture is used as shown in figure 6. The input is an initial condition with 2 channels (u, v) of 64x64 grid. The encoder layer has 3 Convolutional Neural Network (CNN) layers with varying kernel sizes (7,5,2) and number of channels (8,16,4). The latent initial condition $z_o$ has 4 channels which are augmented [6] with 2 extra channels to allow the NODE to learn complex dynamics in a low-dimensional latent space. The NODE has 7 CNN layers with kernel size 3 and 16 number of channels (except output layer). It also has dilated kernels [84] with varying dilation (1,2,3,4,3,2,1) to capture long range interactions. The dilated CNN is similar to standard CNN but has kernels with holes that can skip pixels, thereby increasing the receptive field without amplifying the number of parameters. The latent-space NODE outputs are"}, {"title": "5 Discussion", "content": ""}, {"title": "5.1 Limitation and Future Works", "content": "In this section, we discuss some limitations of the proposed methodology. The primary issue is the increased requirement of memory for the longer rollouts. One approach to address this issue is to learn dynamics on a low-dimensional manifold - however this may cause the loss of small scale information on the flow field. This is also observed in our Kolmogorov flow experiments as evident in Figures 7 and 8, where finer structures are smoothed. For systems where this is undesirable, future work can investigate memory-efficient adjoint sensitivity methods [90, 91, 92, 93]. A second limitation is the computation of stochastic gradients within the MP-NODE framework. The proposed method"}, {"title": "5.2 Conclusion", "content": "The advent of neural differential equations has opened new avenues by combining the flexibility of neural networks with the accuracy of numerical solvers, allowing for improved predictability of dynamical systems. However, the challenge of long term stable prediction for chaotic systems has mostly remained unsolved. Learning chaotic dynamics is challenging as the optimization problem becomes extremely non-convex, making the standard gradient useless. Existing methods like least-squares shadowing, while attractive, suffer from very high computational costs for high-dimensional systems as well as overparameterized models. The algorithm proposed in this work solves this issue by dividing the time domain into multiple steps and allowing a penalized discontinuity to calculate sensible gradients with linear computational complexity. The benefit of MP-NODE is that it can be extended to several time-series forecasting techniques that rely on multistep loss calculations and backpropagation through time. Moreover, this advancement has the potential to improve understanding and predictability of chaotic systems and has broader implications for several scientific domains."}, {"title": "6 Appendix", "content": ""}, {"title": "6.1 Algorithms", "content": ""}]}