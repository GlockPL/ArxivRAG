{"title": "UNIDOOR: A Universal Framework for Action-Level Backdoor Attacks in Deep Reinforcement Learning", "authors": ["Oubo Ma", "Linkang Du", "Yang Dai", "Chunyi Zhou", "Qingming Li", "Yuwen Pu", "Shouling Ji"], "abstract": "Deep reinforcement learning (DRL) is widely applied to safety-critical decision-making scenarios. However, DRL is vulnerable to backdoor attacks, especially action-level backdoors, which pose significant threats through precise manipulation and flexible activation, risking outcomes like vehicle collisions or drone crashes. The key distinction of action-level backdoors lies in the utilization of the backdoor reward function to associate triggers with target actions. Nevertheless, existing studies typically rely on backdoor reward functions with fixed values or conditional flipping, which lack universality across diverse DRL tasks and backdoor designs, resulting in fluctuations or even failure in practice.\nThis paper proposes the first universal action-level backdoor attack framework, called UNIDOOR, which enables adaptive exploration of backdoor reward functions through performance monitoring, eliminating the reliance on expert knowledge and grid search. We highlight that action tampering serves as a crucial component of action-level backdoor attacks in continuous action scenarios, as it addresses attack failures caused by low-frequency target actions. Extensive evaluations demonstrate that UNIDOOR significantly enhances the attack performance of action-level backdoors, showcasing its universality across diverse attack scenarios, including single/multiple agents, single/multiple backdoors, discrete/continuous action spaces, and sparse/dense reward signals. Furthermore, visualization results encompassing state distribution, neuron activation, and animations demonstrate the stealthiness of UNIDOOR. The source code of UNIDOOR can be found at https://github.com/maoubo/UNIDOOR.", "sections": [{"title": "1 Introduction", "content": "Deep reinforcement learning (DRL) has achieved significant milestones, including superhuman-level Go AI [50], protein structure prediction [24], matrix multiplication discovery [17], and safety alignment for large language models [1]. However, DRL faces substantial threats, such as backdoor attacks, where malicious backdoors are injected into the victim agent's policy during training and activated via triggers during deployment.\nCurrent backdoor attacks against DRL are categorized into two types: policy-level and action-level. Policy-level backdoors [19, 57, 61] manipulate the victim agent's long-term objectives, with each trigger corresponding to a target policy. However, they lack precise control over the actions and require retraining when the attack objective evolves. In contrast, action-level backdoors [2, 11, 13, 25, 46] emphasize precise control over the victim agent's step-by-step actions. This enables the adversary to flexibly activate the backdoor to align with various attack objectives, posing a heightened threat to safety-critical applications. We conduct a comprehensive comparison of the two types of backdoors in Section 2.2, accompanied by examples.\nExisting action-level backdoor attacks predominantly draw on the poisoning paradigm from deep learning (DL) [6,36,49]. The key distinction lies in the adversary targeting transitions (state, action, reward triplets) in DRL rather than sample-label pairs in DL. In this case, a trigger is embedded into the state and bound to a target action via the backdoor reward function. However, these studies prioritize trigger design and target action selection while defining backdoor reward functions in simplistic forms, such as fixed values [25] or conditional flipping [2], predetermined by the adversary on a case-by-case basis. Through an empirical study, we reveal that the case-by-case approach limits the universality of current action-level backdoor attacks in practical applications, resulting in performance fluctuations or outright failure.\nEmpirical Study. We take TrojDRL [25] as an example, where the backdoor reward is defined as a fixed value, to investigate how variations in backdoor reward and task settings affect attack performance. We select 6 DRL tasks from OpenAI Gym [41] and design 38 action-level backdoor tasks (indices 0-37 in Table 7). The results in Figure 1 indicate that: (1) The attack performance exhibits fluctuations or outright failure in response to changes in the backdoor reward. (2) It is infeasible to predetermine a static backdoor reward that"}, {"title": "2 Background", "content": "2.1 Deep Reinforcement Learning\nDRL is a machine learning paradigm in which an agent learns to make optimal sequential decisions within an environment by maximizing cumulative rewards through trial and error. This process is modeled as a MDP, which represented as M = (S,A,R,P, \u03b3), where S is the state space. A is the action space. R: S \u00d7 A \u2192 R is the reward function, indicating the immediate reward that the agent receives from the environment for taking action a \u2208 A in state s \u2208 S. P : S \u00d7 A \u2192 A(S) is the state transition function, indicating the probability that taking action a \u2208 A in state s \u2208 S results in a transition to s' \u2208 S. \u03b3\u2208 [0, 1) is the discount rate, which determines the present value of future rewards.\nThe agent makes decisions based on the deep neural network policy \\(\u03c0_\\theta\\), where \\(\u03c0_\\theta\\) : S \u2192 \u2206(A) maps state s \u2208 S to a"}, {"title": "2.2 DRL Backdoor Attacks", "content": "DRL backdoor attacks aim to force the victim agent to execute a specific policy or action, and they can be categorized into the following two types.\nPolicy-Level Backdoor. This type of backdoor focuses on coarse-grained manipulation of the victim agent, with each trigger mapping to a target policy. For instance, the adversary activates the backdoor to force an autonomous vehicle to redirect its destination from a school to a hospital, regardless of the specific route taken. Yang et al. [61] proposed an environment switching framework, while Wang et al. [57] introduced a policy combination approach guided by behavior cloning to inject policy-level backdoors. Gong et al. [19] proposed generating poisoned trajectories from a backdoor policy to implement policy-level backdoor attacks in offline RL.\nAction-Level Backdoor. This type of backdoor focuses on fine-grained manipulation of the victim agent, with each trigger mapping to a target action. For instance, an adversary might activate the backdoor at a critical moment to force an autonomous driving agent to execute a sharp turn, potentially causing traffic congestion or collisions. Kiourti et al. [25] proposed a transition poisoning paradigm for injecting action-level backdoors. Building on this approach, Ashcraft et al. [2] enhanced stealthiness using in-distribution triggers, while Cui et al. [13] improved effectiveness through state-based trigger optimization. These methods employ predefined static backdoor rewards, such as constants, conditional flipping, or the"}, {"title": "3 Threat Model", "content": "In this paper, the attack scenario involves two parties: the victim and the adversary. The victim employs a DRL algorithm to train a policy \\(\u03c0_\\theta\\) for a benign task M = (S, A, R., P, \u03b3). The adversary aims to inject action-level backdoors into the policy \\(\u03c0_\\theta\\). The backdoor task is defined by the tuple (T,S\u2020, A\u2020, Fs, Fa, R\u2020), where T is the trigger space. S\u2020 \u2286 S is the subset of states containing embedded triggers. A\u2020 \u2286 A is the target action space. Fs : S \u00d7 T \u2192 S\u2020 is the trigger-state mapping function, defining how a state is transformed when a"}, {"title": "4 Methodology", "content": "This section first introduces the framework of UNIDOOR and then details the design of each module. The key insight behind UNIDOOR is to conceptualize action-level backdoor injection as a multi-task learning problem, distinct from conventional DRL multi-task learning [54, 56], as the benign and backdoor tasks share a single policy network.\n4.1 Framework\nUNIDOOR consists of four main modules: Performance Monitoring, Initial Freezing, Transition Poisoning, and Adaptive Exploration (see Figure 3), activated sequentially over time.\nStep 1: Performance Monitoring (Section 4.2). The adversary first conducts Performance Monitoring to estimate the victim agent's performance on both benign and backdoor tasks by analyzing trajectories and transitions. These estimations are processed with exponential weighted averaging (EWA) and normalization to derive benign task performance (BTP) and attack success rate (ASR). This task-agnostic approach is pivotal in mitigating Task Discrepancy (Challenge 1). This module functions throughout the entire attack process, with the monitored performance serving as the operational basis for Initial Freezing and Adaptive Exploration.\nStep 2: Initial Freezing (Section 4.3). The adversary then conducts Initial Freezing to delay the backdoor injection and mitigates Distraction Dilemma (Challenge 2). The intuition stems from the fact that backdoor tasks typically exhibit lower complexity compared to benign tasks (e.g., |S\u2020| < |S|), rendering them more likely to dominate the learning process during the early stages. The lift signal is determined by either the number of episodes the victim agent interacts with the environment or the monitored BTP. This module operates only once and will not be reactivated after termination.\nStep 3: Transition Poisoning (Section 4.4). After ending the freezing phase, the adversary activates Transition Poisoning to manipulate the transitions in the victim agent's replay buffer to inject action-level backdoors. This module involves three components: state tampering, action tampering, and reward hacking. The backdoor reward function in reward hacking is provided by Adaptive Exploration, enabling adaptive adjustments to diverse benign and backdoor tasks.\nStep 4: Adaptive Exploration (Section 4.5). Simultaneously with Transition Poisoning, the adversary executes Adaptive Exploration, using the monitored BTP and ASR to track the victim agent's training status and adaptively adjust the backdoor reward function accordingly. The intuition is that BTP is negatively correlated with the backdoor reward, while ASR is positively correlated. Through conservative estimation, the frequency of backdoor reward adjustments is minimized, addressing Limited Trial-and-Error Search (Challenge 3)."}, {"title": "4.2 Performance Monitoring", "content": "The adversary leverages the trajectories and transitions stored in the victim agent's replay buffer to monitor the performance of both benign and backdoor tasks.\nEpisode-Wise BTP. A trajectory \u03c4 = {\u03c4\u03bf, \u03c41, ..., \u03c4\u03c4} encapsulates the complete transition information over a single episode of interaction between the victim agent and the environment, extending from the initial state to the terminal state. Therefore, we propose aggregating the rewards within the trajectory,\nP = \\sum_{i=0}^{T}r_i,\nwhere P denotes the unprocessed BTP of the victim agent for a specific episode. Non-stationarity during DRL training"}, {"title": "4.3 Initial Freezing", "content": "The purpose of Initial Freezing is to delay the adversary's backdoor injection, thereby mitigating the Distraction Dilemma. During the freezing phase, the adversary only observes the victim agent's trajectory data and calculates BTP. This intuition arises from a task complexity analysis across three aspects, which generally makes action-level backdoor tasks less complex than benign tasks.\nGoal Specificity. An action-level backdoor task is associated with the mapping of a trigger to a target action, where Fa : T \u2192 A\u207a is a bijection. This one-to-one goal structure operates without necessitating optimal sequential decision-making or the balancing of trade-offs across diverse states and actions.\nState Space Complexity. Fs : S \u00d7 T \u2192 S\u2020 indicates that S\u2020 is generated through the joint interaction of S and T. The goal specificity enables a small number of triggers to suffice for the attack requirements, leading to |T| < |S|. Furthermore, S\u2020CS implies that S\u2020 covers only a minimal portion of S. This focused mapping compresses the potential state space, typically resulting in |S\u2020| < |S|.\nAction Space Complexity. A\u2020 C A, and the difference between |A| and |A\u2020 | becomes more pronounced as the cardinality of A grows. This is because the adversary is only interested in a small subset of actions that are sufficient to carry out the action-level backdoor attack. When A is a continuous space, this typically results in |A\u2020| < |A|.\nTherefore, the backdoor task is more likely to dominate during the initial stages of policy training, suggesting that backdoor injection should be delayed. The adversary is recommended to lift the freezing when the number of trajectories in the victim agent's replay buffer surpasses the trajectory threshold of or the BTP reaches the performance threshold op. The former approach is suitable for benign tasks with lower complexity, while the latter is recommended when the benign task suffers from cold-start issues [29], such as BTP failing to rise during the initial stages of policy training due to sparse rewards and infinite-horizon episodes."}, {"title": "4.4 Transition Poisoning", "content": "Once the Initial Freezing is lifted, the adversary initiates Transition Poisoning, commencing the injection of the predefined action-level backdoor into the victim agent. Transition Poisoning is carried out either with a predefined probability or at fixed intervals, with each execution modifying a single transition \u03c4\u2081 = (st, at, rt). It involves three components: state tampering, action tampering, and reward hacking.\nState Tampering. The adversary selects a trigger \u03b4\u2208 T and applies the trigger-state mapping function to substitute st with st, where st = Fs(st, \u03b4) and st \u2208 S\u2020.\nAction Tampering. The adversary employs the trigger-action mapping function to substitute a\u2081 with \u00e3\u2081, where \u00e3\u2081 = Fa(\u03b4) and \u00e3\u2081 \u2208 A\u207a. If A is a continuous space, the adversary enhances exploration by adding random noise sampled from a uniform distribution [42], i.e., \u00e3\u2081 = Fa(8) + \u00e2, where \u00e2 ~ U(-p,p) and p is the perturbation radius.\nAction tampering is performed at a fixed frequency to prevent poisoned transitions from exclusively containing positive samples, which could hinder the victim agent from effectively learning the correct decision boundary for the target action.\nReward Hacking. The adversary substitutes rt with t based on the discrepancy between the current action in the transition and the target action. When A is a discrete space, it is assigned as follows: if the current action is identical to the target action, then \u00ee\u2081 = r; otherwise, \u00ee\u2081 = \u2212r. When A is a continuous space, \u00eet is assigned as follows: if the distance between the current action and the target action satisfies the norm constraint, \u0159\u2081 = r; otherwise, \u00ee\u2081 = \u2212r."}, {"title": "4.5 Adaptive Exploration", "content": "The empirical analysis in Section 1 highlights the necessity of tailoring the backdoor reward function to accommodate variations in benign and backdoor tasks. Consequently, UNIDOOR proposes an adaptive exploration of the backdoor reward to achieve cross-task universality, marking a substantial advancement over prior studies.\nThe core idea is to decrease r when P\u2081 falls below expectations and increase it when Pt falls below expectations.\nConservative Expectation. The adversary adopts a conservative strategy to estimate the victim agent's expected performance on both benign and backdoor tasks, aiming to reduce the frequency of backdoor reward adjustments and mitigate"}, {"title": "5 Experimental Setup", "content": "Environments and Tasks. We select 3 commonly used environments for evaluation: Gym [41], MPE [35], and PyBullet [12]. Gym, developed by OpenAI, serves as a comprehensive platform for DRL experimentation, offering a wide range of tasks for diverse applications. We select 6 tasks from it: CartPole, Acrobot, Lunar Lander, MountainCar, Pendulum, and Bipedal Walker. MPE, developed by OpenAI, is designed explicitly for multi-agent reinforcement learning (MARL), supporting cooperative, competitive, and mixed-agent tasks. We select 2 tasks from it: Predator-prey and WorldCom. PyBullet, developed by Facebook AI, is a high-quality physics simulation engine designed for robotics and DRL. It supports continuous action spaces and real-time simulations. We select 3 tasks from it: Hopper, Reacher, and Half Cheetah."}, {"title": "6 Attack Evaluation", "content": "In this section, we first evaluate UNIDOOR's attack performance in single and multiple backdoor scenarios, considering both training from scratch and post-training setups. Next, we analyze the devastating impact of activated action-level backdoors on the victim agent's benign task performance. Furthermore, we examine the stealthiness of UNIDOOR from three perspectives.\n6.1 Single Backdoor Scenarios\nThe adversary aims to inject a single action-level backdoor, involving the backdoor task indices {0-20, 38-41, 44-49} (see Table 7), with the attack commencing after the victim agent initializes its DRL policy.\n6.2 Multiple Backdoor Scenarios\nThe adversary aims to inject multiple action-level backdoors, involving the backdoor task indices {21-37, 42-43, 50-52} (see Table 7), with the attack commencing after the victim agent initializes its DRL policy. All methods employ a cross-poisoning approach, where transitions are poisoned according to the sequence of backdoor tasks during each iteration.\n6.3 Post-Training Scenarios\nThe adversary obtains a well-trained DRL policy from a policy-sharing platform and aims to inject single or multiple action-level backdoors during the post-training phase, involving backdoor task indices {0-37} (see Table 7).\n6.4 Activation Strategies\nIn the deployment phase, the adversary can activate the action-level backdoor based on an activation strategy to achieve the intended attack objective. In this section, we demonstrate the detrimental impact of action-level backdoors by forcing the victim agent to fail in 6 benign tasks in Gym (as shown in"}, {"title": "6.5 Visualization", "content": "We further evaluate the stealthiness of UNIDOOR by comparing the benign and backdoored policies through visualizations across three aspects. (1) We collect interactions of the benign and backdoored (inactive) policies with the environment over 10,000 time steps and observe that the state distributions are nearly identical (see Figure 7). (2) We collect the activations of the neurons and visualize them using t-SNE. Figure 8 shows that the neuron activations of the benign and inactive backdoored policies are indistinguishable. (3) We generate animations of the interactions between the benign and inac-"}, {"title": "7 Ablation Study", "content": "This section conducts ablation studies from the perspectives of modules and additional factors to thoroughly investigate the impact of various factors on the performance of UNIDOOR.\n7.1 Module Ablation\nWe evaluate the impact of removing each module of UNIDOOR on its performance across different environments. Specifically, Performance Monitoring and Transition Poisoning cannot be entirely removed, so we substitute them with the removal of EWA and action tampering, respectively.\n7.2 Additional Factors\nWe comprehensively evaluate the impact of additional factors such as poisoning interval, bias in bound estimation, perturbation radius, norm constraint, conservative expectation, and exploration step size on UNIDOOR. The evaluation environment chosen for this part is Gym."}, {"title": "8 Discussion", "content": "Potential Defenses. Inspired by backdoor defense work in DL, the paradigm for DRL backdoor defense consists of two main components: trigger restoration and policy retraining [10, 21]. Trigger restoration aims to reverse potential triggers by identifying the combination of states and perturbations that maximize the agent's total poisoned rewards. Suppose a detected agent is suspected of containing a backdoor. In that case, the defender can mitigate the potential threat by employing policy retraining, typically achieved using methods such as fine-tuning or machine unlearning [5].\nWe advise defenders to exercise caution when transferring the retraining methods from DL to DRL, as performance fluctuations during training are more frequent and intense in DRL."}, {"title": "9 Related Work", "content": "In recent years, there has been a surge in research utilizing DRL to address various security challenges [39, 60, 63]. However, despite the widespread adoption of security applications, the inherent security threats within DRL remain largely unexplored. This section outlines DRL's primary threats and highlights typically related works.\nAdversarial Perturbations. Inspired by adversarial examples [7], the most widely adopted type of attack by adversaries involves adding adversarial perturbations to the environment or the victim's observations [4, 22, 51, 55], disrupting the victim's sequential decision-making. Furthermore, researchers have investigated adding perturbations to the victim's action outputs. For instance, Lee et al. [30] introduced two action manipulation attacks: the myopic action-space attack injects action perturbations based on current observations, while the look-ahead action-space attack considers future steps to maximize the attack's impact. However, directly manipulating the victim's actions is impractical; thus, adversarial policies have been introduced to overcome this limitation.\nAdversarial Policies. Gleave et al. [18] first introduced the concept of adversarial policies in zero-sum games, later termed Victim-play, in which the adversary gains control over the opponent and manipulates its actions to guide the victim into making suboptimal decisions. Guo et al. [20] extended the concept to general-sum games, while Wu et al. [59] integrated explainable AI techniques to enhance the stealthiness of adversarial policies. Wang et al. [58] delved into adversarial policies in discrete action scenarios and successfully beat superhuman-level Go AIs, showcasing that near-Nash or e-equilibrium policies are exploitable. Furthermore, Ma et al. [38] and Liu et al. [33] demonstrated that even when adversaries only have partial observation privileges over the victim or partial control over the opponent, adversarial policies still pose a significant threat to DRL.\nPoisoning Attacks. Poisoning the environment and reward function in DRL is a well-discussed area of research. This is because the reward function characterizes the long-term objectives of a DRL task and guides the policy updates. Existing works [31, 40, 45] have demonstrated that adversaries can poison the reward function to deviate from the intended objectives, and this attack strategy has been extended to the safety alignment in RLHF [3,43]. Furthermore, as described in Section 2.2, transition poisoning [2, 11, 13, 25, 46] is the primary method for implementing action-level backdoors in DRL, as it forces the binding of triggers and target actions.\nCopyright Protection. With the widespread application of DRL, copyright protection has gained attention, with a focus on protecting policies, trajectories, and environments. Chen et al. [8] introduced a temporal-based watermarking scheme that verifies the copyright of DRL policies through action probability distributions, which is algorithm-agnostic. Du et al. [16] proposed a trajectory-level dataset auditing mechanism for offline RL, using the cumulative reward as an intrinsic and stable fingerprint of the dataset. Ye et al. [62] proposed reinforcement unlearning, a method that selectively forgets the learned knowledge of the training environment from the agent's memory, to mitigate the risk of exposing the privacy of the environment owner."}, {"title": "10 Conclusion", "content": "This paper proposes UNIDOOR, the first action-level backdoor attack framework that achieves universality across various attack scenarios, eliminating the reliance on expert knowledge or grid search. The key insight of UNIDOOR lies in framing action-level backdoor attacks within a multi-task learning paradigm, adapting backdoor rewards based on performance monitoring. In contrast to previous works, we highlight that action tampering is a crucial component for backdoor injection in continuous action scenarios. Extensive evaluations demonstrate that UNIDOOR significantly enhances the effectiveness of backdoors while maintaining stealthiness."}, {"title": "A Attack Scenarios", "content": "As a universal framework for action-level backdoor attacks, UNIDOOR is applicable to a range of scenarios, including but not limited to the following four.\nAgent Provider. The adversary is the provider of the agent (e.g., drones and autonomous vehicles), injecting action-level backdoors into the DRL policy during releases or updates. In this scenario, the adversary has full training privileges and complete knowledge of the victim agent.\nInternal Adversary. The adversary is an internal employee who injects action-level backdoors into the victim agent released by its employer. In this scenario, the adversary can manipulate the state, modify transitions, and has knowledge of the training schedule, algorithm, model structure, and hyperparameter settings.\nThird-Party Outsourcing. The victim seeks assistance from third-party outsourcing for agent training due to limited DRL expertise or computational resources. The third-party outsourcing, with malicious intent, aims to inject action-level backdoors into the victim agent. In this scenario, the adversary can manipulate the state and modify transitions, but not"}, {"title": "B Bound Estimation", "content": "This section presents methods for estimating the upper and lower bounds of BTP across different attack scenarios. Furthermore, we experimentally demonstrate in Section 7.2 that an estimation bias within a specific range only slightly impacts the attack performance of UNIDOOR.\nUpper Bound. In the simplest case, the adversary, possessing prior knowledge of the benign task, can directly calculate the maximum cumulative reward for a single episode and employ it as Pu. If the attack occurs in a post-training scenario, the adversary can directly use the victim agent's average test performance on the benign task as Pu. Another common scenario is that the benign task designer provides and publishes the training objectives, which can be directly used as Pu. Otherwise, the adversary should adopt a conservative strategy by collecting additional trajectories before launching the attack and using the maximum cumulative reward as Pu\nLower Bound. In the simplest case, the adversary, possessing prior knowledge of the benign task, can directly calculate the minimum cumulative reward for a single episode and employ it as Pl. If the adversary has interaction access to the environment, it can initialize a random policy and use its tested average performance as Pl. If the adversary obtains trajectory data from the early stages of the victim agent's training, it can use the average cumulative reward of these trajectories as Pl. Otherwise, the adversary should adopt a conservative strategy by collecting additional trajectories before launching the attack and using the minimum cumulative reward as Pl."}, {"title": "C Additional Advantages of Initial Freezing", "content": "Initial Freezing is a one-time process, meaning that once it is lifted, it will not be reinstated. This ensures the stability of the victim agent's training. It is task-agnostic, meaning it does not affect the universality of UNIDOOR. Initial Freezing also serves as an information-gathering tool for the adversary and can be seamlessly integrated with other modules within the framework. For instance, it aligns with Performance Monitoring, providing the adversary with a convenient means to observe benign trajectories. This facilitates estimating the upper"}, {"title": "D Poisoning Paradigm", "content": "Rathbun et al. [46] categorize the poisoning paradigms in action-level backdoor attacks into inner-loop and outer-loop based on the threat model.\nInner-Loop Paradigm. The adversary acts as a man-in-the-middle between the environment and the victim agent, per-"}, {"title": "E Space Initialization", "content": "The adversary aggregates the rewards from all transitions collected during Initial Freezing into a set RIF = {r1,r2,..., rn}, and initializes rl = min(RIF), ru = max(RIF). The following three scenarios are then considered:\n* If rl < ru and rl, ru \u2208 Z+, then r = [(rl+ru)/2];\n* If rl < ru and either rl \u2208 R+ \\ Z+ or ru \u2208 R+ \\ Z+, then r = (rl+ru)/2;\n* If rl = ru, then r = rl and ru = max(RIF) + \u03c9.\nIn the above statement, Z+ denotes the set of positive integers, R+ denotes the set of positive real numbers, and \u03c9 is the exploration step size."}, {"title": "Algorithm 1 Performance Monitoring (BTP).", "content": "\nInput: Lower bound Pl, upper bound Pu and the latest smoothed BTP Pt\u22121, smoothing factor \u03b2.\nCollect a trajectory \u03c4 = {\u03c40, \u03c41, ...\u03c4T}.\nif the adversary decides to update the BTP then\nP = \u2211i=0Tri\nPt \u2190 \u03b2 \u00b7 Pt\u22121 + (1 \u2212 \u03b2) \u00b7 P\nPt \u2190 (Pt \u2212 Pl)/(Pu \u2212 Pl)\nend if\nOutput: Pt, Pt"}, {"title": "Algorithm 2 Performance Monitoring (ASR).", "content": "\nInput: Action space A, the selected trigger \u03b4 \u2208 T, trigger-state mapping function FS, trigger-action mapping function FA, norm constraint \u03b5, smoothing factor \u03b2, and the latest ASR Pt\u22121.\nObserve the current state st of the environment.\nif the adversary decides to update the ASR then\ns' \u2190 FS(st, \u03b4)\nObserve the action output at of the victim agent.\nif A is a discrete space then\nP\u2020 \u2190 1[at = FA(\u03b4)]\nend if\nif A is a continuous space then\nP\u2020 \u2190 1[||at \u2212 FA(\u03b4)||2 \u2264 \u03b5]\nend if\nPt \u2190 \u03b2 \u00b7 Pt\u22121 + (1 \u2212 \u03b2) \u00b7 P\u2020\nend if\nOutput: Pt"}, {"title": "Algorithm 3 Initial Freezing.", "content": "\nInput: The modeled MDP of the benign task M, monitored BTP Pt, trajectory threshold \u03c6t, and performance threshold \u03c6p.\nInitialize the freezing tag Ff \u2190 1\nwhile Ff = 1 do\nObserve the interaction between the victim agent and the environment, as well as its replay buffer B. |B| denotes the number of trajectories in the replay buffer.\nif M exhibits low complexity and |B| > \u03c6t then\nFf \u2190 0\nend if\nif M exhibits high complexity and Pt > \u03c6p then\nFf \u2190 0\nend if\nend while\nOutput: Ff"}, {"title": "Algorithm 4 Transition Poisoning.", "content": "\nInput: Timer t, poisoning interval Ip, freezing tag Ff, action space A, the selected trigger \u03b4 \u2208 T, trigger-state mapping function FS, trigger-action mapping function FA, action tampering frequency f, uniform distribution U(\u2212\u03c1, \u03c1), the backdoor reward at the current time step r+\nInitialize the timer t \u2190 0\nwhile the victim agent continuously interacts with the environment do\nt \u2190 t + 1\nObserve the current transition \u03c4t = (st, at, rt).\nif t mod Ip = 0 and Ff = 0 then\nst \u2190 FS(st, \u03b4)\nif A is a discrete space then\n{ FA(\u03b4) if t mod f = 0\nat \u2190 at otherwise\nt \u2190 { r+ if at = FA(\u03b4)\n\u2212r+ otherwise\nelse\n{ FA(\u03b4) + a if t mod f = 0\nat \u2190 at otherwise\nt \u2190 { r+ if ||at \u2212 FA(\u03b4)||2 \u2264 \u03b5\n\u2212r+ otherwise\nend if\n\u03a4t \u2190 (st, a't, r't)\nend if\nend while\nOutput: \u03c4t, t"}, {"title": "Algorithm 5 Adaptive Exploration.", "content": "\nInput: Exploration interval Ie, phase tag Fp, the latest backdoor reward r+\u22121, upper bound ru, lower bound rl, exploration step szie \u03c9.\nLoad Pt, Pt\u22121, P+t, and P+t\u22121\nCalculate Etl and Etb based on Equation 11 and Equation 12.\nif t mod Ie = 0 and Fp = Expansion then\nif the step-wise ASR converges then\nFp \u2190 Contraction\nend if\nif (Pb > Etl and P+t < Etb) or (P+t \u2212 P+t\u22121) > (Pt\u22121 \u2212 Pt\u22121) then\nrl \u2190 r+\u22121 + \u03c9\nru \u2190 2 \u00b7 r \u2212 rl\nend if\nend if\nif t mod Ie = 0 and Fp = Contraction then\nif Pt < Etl and Pt < Pt\u22121 then\nru \u2190 r\nr \u2190 [(ru + rl)/2] if rl, ru \u2208 Z+ else (ru + rl)/2\nend if\nif P+t < Etb and P+t < P+t\u22121 then\nrl \u2190 r\nr \u2190 [(ru + rl)/2] if rl, ru \u2208 Z+ else (ru + rl)/2\nend if\nend if\nOutput: rl, ru, r"}]}