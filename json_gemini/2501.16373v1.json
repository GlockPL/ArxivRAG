{"title": "Unveiling Discrete Clues: Superior Healthcare Predictions for Rare Diseases", "authors": ["Chuang Zhao", "Hui Tang", "Jiheng Zhang", "Xiaomeng Li"], "abstract": "Accurate healthcare prediction is essential for improving patient outcomes. Existing work primarily leverages advanced frameworks like attention or graph networks to capture the intricate collaborative (CO) signals in electronic health records. However, prediction for rare diseases remains challenging due to limited co-occurrence and inadequately tailored approaches. To address this issue, this paper proposes UDC, a novel method that unveils discrete clues to bridge consistent textual knowledge and CO signals within a unified semantic space, thereby enriching the representation semantics of rare diseases. Specifically, we focus on addressing two key sub-problems: (1) acquiring distinguishable discrete encodings for precise disease representation and (2) achieving semantic alignment between textual knowledge and the CO signals at the code level. For the first sub-problem, we refine the standard vector quantized process to include condition awareness. Additionally, we develop an advanced contrastive approach in the decoding stage, leveraging synthetic and mixed-domain targets as hard negatives to enrich the perceptibility of the reconstructed representation for downstream tasks. For the second sub-problem, we introduce a novel codebook update strategy using co-teacher distillation. This approach facilitates bidirectional supervision between textual knowledge and CO signals, thereby aligning semantically equivalent information in a shared discrete latent space. Extensive experiments on three datasets demonstrate our superiority.", "sections": [{"title": "1 Introduction", "content": "Healthcare predictions, such as medication recommendations, are critically important as they directly influence the efficacy of medical treatments [13, 33]. Accurate medication recommendations can enhance patient recovery rates by up to 30% and reduce adverse drug reactions by 25%, demonstrating their significant positive impact [40, 44].\nCurrent research in healthcare prediction can be broadly cate-gorized into three genres [1, 35, 54]: rule-based, graph-based, and sequence-based approaches. Rule-based systems [9, 41] typically rely on expert-defined rules to guide predictions, offering effective solutions but often facing limitations in scalability and potential con-flicts among rules. In contrast, graph-based methods [3, 5] leverage graph neural networks to model electronic health records (EHRs) as homogeneous or heterogeneous graphs, enhancing predictive performance through the exploration of intricate collaborative (CO) signals within the data. Sequence-based methods [55, 62] represent a shift from static approaches by focusing on the sequential patterns inherent in longitudinal EHRs, capturing temporal dependencies that static models might overlook. While these methods are effec-tive, they tend to emphasize maximizing overall accuracy [53, 63], which can lead to performance degradation for specific diseases. This issue arises from the highly skewed data distribution in EHRs. As depicted in Figure 1(a), datasets such as MIMIC-III [18], MIMIC-IV [17], and eICU [37] exhibit a pronounced imbalance in data distribution. In MIMIC-IV dataset, the commonest diseases (top 20%) account for approximately 95% of interactions in EHRs, while the rarest diseases (tail 20%) represent only about 0.2%. Meanwhile, as shown in Figure 1(b), we observe that existing advanced methods demonstrate superior performance in diagnosing common diseases. However, their effectiveness diminishes significantly when applied to rare diseases. This disparity is a key factor contributing to over-all predictive shortcomings and may lead to health inequalities in diagnosis [63]. It underscores the need for more effective strategies. Recently, several studies have demonstrated distinct distribu-tions of long-tail and head objects [61]. This observation motivates us to treat rare diseases and common diseases as different feature domains and find a way to align rare diseases (CO space) with com-mon diseases (CO space) to leverage the established knowledge, e.g., disease-medication relationships derived from rich EHRs as-sociated with common diseases. However, as depicted in Figure 1, limited data impedes the establishment of a robust CO space for rare diseases. Textual knowledge (Text), shared across all diseases and recognized as a consistent and reliable semantic resource [19, 48], serves as a bridge to facilitate alignment between these two spaces. Consequently, our aim is to align CO signals with textual knowledge within a unified discrete space, followed by executing a high-quality Text\u2192CO mapping for rare diseases to enrich representation se-mantics. The discrete space, derived from VQ-VAE [39], employs a vector quantized (VQ) process to facilitate code-level mappings between textual knowledge and CO signals. This aligns with the multi-symptom nature of the disease and demands fewer compu-tations compared to continuous modeling [10, 22]. To develop our approach, we highlight two key aspects.\n\u2022 How to acquire distinguishable discrete encodings for precise disease representation? 1) In clinical documentation, even minor variations in symptoms can necessitate different medical codes, despite similar text descriptions. For instance, Type 1 and Type 2 diabetes, though both may present as \"diabetes without complications,\" diverge significantly in their pathophysi-ology and management, with Type 1 typically requiring lifelong insulin therapy and Type 2 often managed through lifestyle modifications and oral medications. This necessitates that the model be adept at discerning subtle yet significant differences in clinical context, despite relatively similar text descriptions. 2) While VQ-VAE is effective at reconstructing data and learn-ing broad patterns, its approach to feature extraction and re-construction may not always align with the specific, detailed requirements of downstream predictive tasks, resulting in po-tential limitations in predictive accuracy. For example, while the reconstructed text representation provides a coherent overview, it might lack critical details like specific symptom patterns or treatment adherence levels. Similarly, reconstructed CO signals might miss key interactions or subtle patterns that are crucial for precise medication recommendation or diagnosis prediction.\n\u2022 How to perform effective semantic alignment between CO signals and textual knowledge? Text and CO signals typically reside in distinct semantic spaces, with text represented in natural languages and CO signals in interaction embeddings. This domain gap is an obstacle that hinders the Text\u2192CO signal mapping. Furthermore, as both representations of disease are mapped into a discrete space-where each code embodies unique symptom semantics-aligning at the code level is crucial for mitigating the domain gap and facilitating knowledge transfer.\nTo tackle these challenges, we introduce UDC, a tailored VQ-VAE framework for healthcare that utilizes textual knowledge and CO signals for alignment and reconstruction, enhancing the representa-tion semantics of rare diseases during discrete representation learn-ing (DRL). To ensure the distinguishability of disease encodings, we upgrade the original VQ process to incorporate condition-aware calibration. We specifically include medical entities that co-occur during the same visit for a particular disease as contextual condi-tions. This adjustment allows the model to produce distinct recon-structions based on varying contexts, even when the text appears similar. For instance, in a medical scenario, the distinction between Type 1 and Type 2 diabetes could be identified by examining com-plications such as diabetic ketoacidosis (more common in Type 1) or by specific laboratory findings in EHRs, thereby enhancing the granularity of representations. Furthermore, to guarantee task rele-vance in the reconstructed representations, we devise a contrastive task-aware calibration. Leveraging mixed-domain and synthetic target representations as hard negatives, we boost the model's abil-ity to discern distinct features and facilitate the reciprocal transfer of knowledge between CO signals and textual information. This empowers the reconstructed representations to react adaptively in accordance with the particular downstream tasks at hand. To achieve better semantic alignment of Text-CO signals, we intro-duce a novel codebook update strategy using co-teacher distillation. In this approach, the text and the CO signal, both featuring en-coded diseases, act as mutual reconstruction labels, facilitating the aggregation of quantized vectors encoded from two signals with equivalent semantics into a unified latent space.\nTo sum up, our key contributions are as follows.\n\u2022 To our knowledge, UDC has significantly enriched the seman-tics of rare diseases, thereby improving healthcare prediction performance. Our framework can be seamlessly integrated into various advanced healthcare prediction models.\n\u2022 We tailor the VQ process for healthcare, incorporate condition-aware and task-aware calibration, and devise a novel codebook update mechanism. These enhancements notably improve re-construction performance and adaptability to downstream tasks.\n\u2022 Our algorithm demonstrates superior performance across two healthcare prediction tasks on three datasets, effectively han-dling both common and rare diseases. We have made the code available on Github \u00b9 to ensure reproducibility."}, {"title": "2 Related Work", "content": "We review related work, emphasizing connections and distinctions."}, {"title": "2.1 Healthcare Prediction", "content": "Healthcare prediction employs advanced data-driven models to forecast clinical outcomes and disease progression [54]. This prac-tice significantly impacts personalized treatment by facilitating early intervention and optimizing clinical decisions."}, {"title": "2.2 Generative Retrieval", "content": "Generative retrieval is a key technique in modern systems, enabling the direct generation of candidate items rather than selecting from a fixed set, as in discriminative genres [21]. This is critical for delivering context-aware retrievals in domains with limited data.\nGenerative retrieval [24, 42] can be broadly categorized into three genres: autoregressive-based [27, 51], GAN-based [4, 16], and autoencoder-based models [2, 50, 64]. Autoregressive mod-els [27], such as those utilizing Transformer architectures, generate sequences by predicting the next item based on previous context, making them well-suited for tasks requiring a sequential under-standing. However, they are often computationally intensive and may suffer from exposure bias. GAN-based models [4] generate re-alistic candidate items through a generator that creates samples and a discriminator that evaluates their authenticity. While GANs [16] excel in producing high-quality outputs, they are challenging to train and may experience instability issues. Autoencoder-based models, including approaches like VAE [38, 60], use an encoder to map inputs to a latent space and a decoder to reconstruct them. These models effectively capture complex data distributions and facilitate structured, interpretable generation. VQ-VAE [39], in par-ticular, leverages discrete latent variables, balancing the strengths of both autoregressive and autoencoder-based approaches while offering robustness in handling diverse distributions.\nOur method aligns with the last genre, specifically extending VQ-VAE to healthcare. We focus on enhancing the representation of rare diseases by introducing condition-aware and task-aware calibration."}, {"title": "3 Proposed Method", "content": "Preliminary. Each patient's medical history is recorded as a sequence of visits, represented by $U^{(k)} = (u_1^{(k)}, u_2^{(k)},..., u_{T_k}^{(k)})$, where k identifies the patient within the patient set N, and $T_k$ is the total number of visits. Each visit $u^{(k)}$ is defined as a triplet $u^{(k)} = (d^{(k)}, p^{(k)}, m^{(k)})$, corresponding to the diagnoses (d), procedures (p), and medications (m) associated with that visit, respectively. These components are encoded as multi-hot vectors: $d^{(k)} \u2208 {0,1}^{|D|}$, $p^{(k)} \u2208 {0,1}^{|P|}$, and $m^{(k)} \u2208 {0,1}^{|M|}$, where D, P, and M represent the sets of all possible diagnoses, procedures, and medications, and $|\u00b7|$ denotes the cardinality of these sets. For instance, the vector d = [1, 0, 1, 0] suggests that the patient has diseases 1 and 3, assuming |D| = 4. Additionally, each medical entity * is associated with a corresponding text description denoted as T(*). For clarity, k is omitted in the following content.\nTask formulation. Following [57, 59, 62], we outline the defini-tions of the two common healthcare prediction tasks.\n\u2022 Diagnosis Prediction (Diag Pred) entails a multi-label classifi-cation challenge that centers on anticipating forthcoming risks. This task revolves around scrutinizing [u\u2081, ..., ut] to forecast the diagnosis set $d_{t+1}$ at time t + 1, where target $y_{[u_{t+1}]} \u2208 R^{1x|D|}$.\n\u2022 Medication Recommendation (Med Rec) involves a multi-label classification task dedicated to pinpointing the most suit-able medications for the patient's present state. This process entails scrutinizing [u1, ..., ut], alongside $(d_{t+1}, p_{t+1})$, to antici-pate mt+1 at time t + 1, where target $y_{[u_{t+1}]} \u2208 R^{1\u00d7|M|}$.\nSolution Overview. Our solution for enhancing healthcare pre-diction, particularly for rare diseases, unfolds through a structured three-step process. First, we develop a robust healthcare prediction model $F_{co}(\u00b7)$ by training on the entire dataset, which acts as the pre-trained collaborative model (PCM). However, this alone proves insufficient, as the resulting representations En often fail to capture the nuances of rare diseases due to sparse co-occurrence. To address this, we choose a pre-trained language model (PLM), i.e. $F_{te} (\u00b7)$, and introduce a discrete representation learning (DRL) framework in the second stage, where we reconstruct these representations to ensure Text-CO signals alignment. Our key innovations lie in this phase, where we employ condition injection, contrastive learning, and co-teacher distillation to ensure that the discretized representations, incorporating both textual and collaborative signals, are distinct, task-aware, and aligned at the code level. Finally, in the fine-tuning & inference stage, we freeze DRL to produce \u00ca\u00d0 that substitute the original embeddings En and fine-tune $F_{co}(\u00b7)$, thereby significantly improving the model's capability to handle the challenging rare cases. The comprehensive framework is illustrated in Figure 2."}, {"title": "3.1 Discrete Disease Representation", "content": "We employ discrete modeling to map disease representations onto discretized code vectors for reconstruction. Contrasted with VAES, VQ [22] process excels in compression and offers interpretability. Formally,\n$c_l = arg min_{c_i \u2208 C_l}||r_{l-1} - \u03c6(e_d) - c_i||_2,$\n$r_l = r_{l-1} - c_l,$"}, {"title": "3.2 Condition-aware Calibration", "content": "Vanilla RQ-VAE typically proceeds to decode once the latent vector $z_d$ is obtained. However, their efficacy in reconstructing samples with similar descriptions is limited. Mechanistically, vanilla RQ-VAE uses MSE loss to minimize overall reconstruction error, leading to identical \"average\" representations for similar text, sacrificing individual specificity [14]. This constraint significantly hampers their utility in healthcare scenes, where medical entities frequently share analogous descriptions yet possess distinct semantic nuances. For instance, Type 1 and Type 2 diabetes may both be described as \"diabetes without complications,... \"(similar text) but they differ sig-nificantly in pathophysiology, warranting distinct representations in reconstruction. However, vanilla RQ-VAE produces similar repre-sentations for them due to overall MSE and similar text [14, 26]. To address this deficiency, we propose integrating external conditions, specifically diverse types of medical entities within the same visit, to modulate the quantization vector via normalization. This strat-egy aims to embed condition variations into the index map, thereby stimulating the decoder to produce a broader array of reconstructed representations. Formally,\n$f_d = MHA_p(e_p, e_p, e_p) + \u041c\u041d\u0410_\u043c(e_\u0442, e_\u0442, e_\u0442),$\nwhere MHA() denotes the multi-head attention from Appendix A and $f_d$ is the condition representation. $e_p \u2208 E_p$ and $e_m \u2208 E_m$ de-note the entities for disease d at the same visit. Then, we incorporate it in normalized form. Formally, for the CO branch,\n$z_d = \u03c6_\u03b3 \\frac{\u03c6(z_d^{old}) - \u00b5(f_d)}{\u03c3(f_d)} + \u03c6_\u03b2(z_d^{old}),$\nwhere $z_d^{old}$, as defined in Eq. 5, is labeled as \"old\" for clarity. \u00b5 and \u03c3 denotes the mean and variation. $\u03c6_\u03b3$ and $\u03c6_\u03b2$ signify the transfor-mation matrix. This normalizing ensures that f's values fall within a similar range, which helps maintain consistency in the scale of the input features, thereby aiding in training stability and con-vergence without escalating the model's complexity. Likewise, we could obtain $\u017e_d$ using $e_d$ and $e_m$"}, {"title": "3.3 Task-aware Calibration", "content": "While incorporating conditions can enhance the semantics of $z_d$ for decoding, there remains a crucial gap: the model lacks awareness of downstream tasks. This awareness can help optimize model per-formance by guiding the learning process towards features that are most relevant to the healthcare task, leading to improved accuracy. In other words, we necessitate that the reconstructed representa-tion not only mirrors the original one but also closely aligns with the target Sd in the subsequent visit (Sq \u2208 D for Diag Pred and Sa e M for Med Rec); otherwise, it remains distant. To achieve this objective, beyond conventional intra-domain (Text/CO signal) contrastive learning [28, 58], we devise two distinct hard negative sampling to augment the contrastive training approach. Formally, using CO signal $z_d$ as an example,\n$L_{intra} = - \\frac{1}{|D|}\u2211_{d=1}^{|D|} log[\\frac{exp(s_dWz_d)}{exp(s_d'Wz_d) + \u2211_{j\u2260d} exp(s_jWz_d)}],$\n$L_{inter} = - \\frac{1}{|D|}\u2211_{d=1}^{|D|} log[\\frac{exp(s_dWz_d)}{exp(s_{d\u02dc}Wz_d) + \u2211_{j\u2260d} exp(s_jWz_d)}],$\nwhere sa denotes d's next-visit target representation, i.e., $S_d = \u2211_{d\u2208Sd} \u03c6_{te}(e_d)$. $s_d'$ denotes the synthetic disease representation acquired by randomly substituting the medical entities associated with the target Sq. Likewise, we define $s_d = \u2211_{d\u2208Sa} \u03c6_{te}(e_d)$. For-mally, we advance from both collaborative and textual standpoints,\n$L_{con} = L_{intra} + L_{inter} + L_{intra} + L_{inter},$"}, {"title": "3.4 Co-teacher Distillation", "content": "In the preceding sections, we transform both the CO and textual signals into discrete representations. However, this pipeline does not ensure semantic alignment between the two at the code level, leading to a domain gap that significantly impedes the subsequent Text\u2192CO mapping. To address this constraint, we introduce a co-teacher distillation that iteratively refines the same code by leveraging both text and CO signals. Specifically, for each code $c_i$, we first retrieve the related diseases set N and N in the collabora-tive and textual domain at the l-th level codebook. Subsequently, we combine their representations to obtain a holistic view o\u00b9. For clarity, we omit the superscript 1. Formally, for t-th iteration,\n$o^{(t)} = \u043a\u043e^{(t-1)} + (1 \u2212 \u03ba)[ \\frac{\u2211_{d\u2208\\hat{N}^{(t)}} z_d}{den^{(t)}} + \u2211_{d\u2208N^{(t)}} \\frac{z_d}{2} ],$\n$b_d^{(t)} = MHA(z_d, z_d, z_d), b_{d\u02dc}^{(t)} = MHA(z_d, z_d, z_d),$\nwhere k refers to the decay rate and b extract the relationship be-tween two views. Then, we employ an exponential moving average method to update $c_i$. Formally,\n$c^{(t)} = o^{(t)}/n^{(t)},$\n$n^{(t)} = x^{(t-1)} + (1 \u2212 \u03ba)[\u2211_{d\u2208\\hat{N}^{(t)}} + \u2211_{d\u2208N^{(t)}}],$"}, {"title": "3.5 Training & Fine-tuning Strategy", "content": "We outline the training objectives of the DRL and fine-tuning stages.\nTraining Strategy. Our final optimization objective for DRL com-prises reconstruction loss and the two preceding parts. Formally,\n$L_{total} = ||e_d \u2013 Y_{co}(z_d)||^2 + ||\u0113_d \u2013 Y_{te}(\u017ed)||^2 +L_{con} + L_{com},$\nwhere Y denotes the MLP decoder for reconstruction. Once DRL is trained, it can be used as a mapping function to transform textual space into collaborative space. At this stage, we exclusively leverage data related to common diseases $D_{com}$, as collaborative signals from rare diseases $D_{rar}$ are considered unreliable. Dcom and Drar are splited according to Section 4.1.\nFine-tuning & Inference. Upon DRL alignment training comple-tion, DRL can transform textual signals into collaborative signals. This enables us to utilize the textual description of rare diseases to supplant their original inferior collaborative signals. Formally,\n$\u00ea_d = \n\\{\n Y_{co} [\u03c6(\u03c6_{te}(e_d); e_m, e_m)], if d\u2208 D_{rar}\n Y_{co} [\u03c6(\u03c6_{co}(e_d); e_m, e_m)], if d\u2208 D_{com}$\nFollowing this, we freeze DRL and En, and fine-tune $F_{co}(\u00b7)$ to capture updated interaction patterns using Eq. 2. This step is crucial, as evidenced in Appendix B, since the prior $F_{co}(\u00b7)$ may not fully grasp interaction patterns with other medical entities owing to the data scarcity on rare diseases. For a new representation, it necessitates re-learning to enhance its effectiveness. Finally, we can integrate Fco() and DRL for the estimation \u0177. Formally,\n$\u0177 = F_{co} (\u00ea_d, e_p, e_\u0442, T_k; 0).$\nOverall, through the three-step process, we can effectively map rare diseases onto the feature space of common diseases using textual knowledge as a bridge, thereby enhancing their semantic richness. A concise algorithm flow can be seen in Appendix C."}, {"title": "4 Experiments", "content": "We first outline the necessary setup and then present the analysis."}, {"title": "4.1 Experimental Setup", "content": "Datasets & Baselines. Our experiments are conducted on three popular healthcare datasets: MIMIC-III [18], MIMIC-IV [17], and eICU [37]. Detailed statistics for these datasets are summarized in Appendix D. Textual knowledge is extracted by parsing EHR entities according to the internationally recognized ICD and ATC systems [12] to obtain corresponding textual descriptions. We retain patients with more than one visit in MIMIC-III and eICU, while for MIMIC-IV, we include patients with two or more visits.\nWe select advanced baselines for comparison. Specifically, for both tasks, we include Transformer [43], MICRON [49], RETAIN [6], GRAM [5], StageNet [11], SHAPE [30], StratMed [25], HAR [46], GraphCare [15], and RAREMed [63]. For Diag Pred, we further in-corporate HITANet [31], Deepr [36], Dipole [34], MedPath [59], Se-qCare [53] as specialized baselines. In Med Rec, additional baselines such as SafeDrug [56], GAMENet [41], COGNet [55], VITA [20], MoleRec [57], and DEPOT [62], are included, given their distinctive designs and strong performance. Transformer, RETAIN, HITANet, Deepr, StageNet, RAREMed, and SHAPE are sequence-based ap-proaches, while GRAM, GAMENet, MoleRec, MICRON, DEPOT, StratMed, COGNet, and VITA further integrate EHR graphs to en-hance representation. MedPath, HAR, SeqCare, and GraphCare leverage external knowledge to improve performance. RAREMed and SeqCare incorporate tailored reconstruction tasks and denois-ing techniques specifically designed for rare diseases.\nImplementation Details & Evaluations. To ensure fairness, fol-lowing [15, 62], all algorithms use an embedding dimension of 128. We employ the AdamW optimizer with a learning rate of 1e-3 for Diag Pred and 2e-4 for Med Rec. The batch size is set to 16. The epochs for the DRL and fine-tuning stages are set at 50 and 50, respectively. Following RQ-VAE, we configure the code layer L = 4, meaning each disease is represented by four codes. The codebook size C\u2081 and commitment weight a, which are crucial hyperparam-eters, are set to 64 and 0.25, respectively. Their effects are evaluated in Appendix E. Following the Pareto principle and previous re-search [61], we classify diseases appearing in 20% or more cases as common Dcom, with all others considered rare Drar. The impact of varying thresholds \u03b7 is further explored in Appendix E.\nFor data partitioning, we follow established practices [46, 53, 62] by dividing the datasets into training, validation, and test sets in a 6:2:2 ratio. For Diag Pred, we use Acc@K, Pres@K, AUPRC, and AUROC for evaluation. Here K=20, different values are discussed in Section E.1. For Med Rec, we assess using Jaccard, F1-score, PRAUC, and AUROC. These metrics are selected for their significant clinical relevance and comprehensive assessment [1, 15]."}, {"title": "4.2 Overall Performance", "content": "As depicted in Tables 1-2, our proposed UDC achieves the best performance across all scenarios, despite only utilizing the rela-tively weak Transformer as the PCM. Regarding the baselines, we observe that the sequence-based methods, such as SHAPE and DE-POT significantly outperform GRAM, underscoring the importance of capturing temporal patterns. COGNet and VITA are Transformer variants that leverage medical priors, like EHR graphs, resulting in notable enhancements over pure Transformer. GraphCare, Med-Path, and SeqCare distinguish themselves by leveraging external knowledge graphs to enrich the inherent entity semantics. Never-theless, the absence of adequate denoising measures hinders their effectiveness. While RAREMed introduces pre-trained tasks to ad-dress the cold-start issue, its overall predictive capacity remains relatively modest. Observations suggest a potential decline for com-mon disease prediction, as detailed in Section 4.3.2.\nConcerning the tasks, Diag Pred is more challenging than Med Rec, as the former requires recalling and ranking a broader range of medical entities. UDC, GraphCare, and SeqCare demonstrate greater robustness, as they not only rely on CO signals but also leverage semantic associations between items from the external knowledge. The broader Diag Pred benefits more from the external knowledge effects in the sampling process, leading to a 3% Acc@K improvement in MIMIC-IV. Our observations indicate that eICU demonstrates enhanced performance in Diag Pred, likely due to the smaller disease size, which results in greater similarity among diseases across consecutive periods. MICRON's performance on MIMIC-III and eICU is constrained in both tasks due to its require-ment for at least two visit lengths, which limits the available data. StratMed does not reproduce its success from Med Rec on Diag Pred. This disparity could stem from the drug interaction graph it introduced not being suitable for the Diag Pred.\nConsidering the datasets, MIMIC-IV is the most challenging, as it exhibits more complex entity interactions, reflected in the larger data volume and higher sparsity. Additionally, the MIMIC-IV data presents a more imbalanced distribution, as shown in Figure 1. Most algorithms, such as StratMed, Dipole, and DEPOT, experience noticeable performance degradation on this dataset. Despite incor-porating external knowledge, as seen in GraphCare and HAR, their approaches overlook the domain gap between this knowledge and the CO signal, potentially leading to negative transfer. Meanwhile, the lack of standard EHR coding in the eICU dataset leads to sig-nificant gaps in external knowledge, diminishing the advantages of these baselines. Conversely, UDC directly leverages the text of eICU records and aligns CO signals with textual knowledge without requiring additional indexing, effectively alleviating this issue."}, {"title": "4.3 Model Analysis and Robust Testings", "content": "Without loss of generalization, we conduct various robustness ex-periments on MIMIC-III to validate our efficacy."}, {"title": "4.3.1 Ablation Study", "content": "We conduct ablation experiments to vali-date the efficacy of sub-modules. As shown in Table 3, UDC-CO, which lacks the condition-aware modeling between the disease and visit components, is the limited-effective configuration, with a sub-stantial 3% drop in Diag Pred. This absence causes disease, akin to textual descriptions, to be challenging for the model to differentiate, thereby leading to a blurred decision boundary. While UDC-NT has little impact on the reconstruction ability, it fails to impose effec-tive constraints on the representation space. Directly applying this representation to downstream tasks proves challenging, necessitat-ing additional training during the fine-tuning phase, yet achieving equivalent performance remains elusive. When contrasted with UDC-NT, both UDC-NM and UDC-NS exhibit enhanced perfor-mance, attributed to their capability to enhance the model's individ-ual discernment by integrating hard negative instances. UDC-NCD, akin to RQ-VAE in codebook update, experiences a 2% degradation due to domain gaps between text and CO spaces. This disparity could result in a significant negative transfer. Overall, the results validate the essential contributions of the key sub-modules."}, {"title": "4.3.2 Group Analysis", "content": "To examine the model's performance on rare diseases, we conduct a group-level analysis. Specifically, in Diag Pred, diseases are categorized into five prevalence groups: 0-20% (G1), 20-40%(G2), 40-60%(G3), 60-80% (G4), and 80-100%(G5), where G1 is the rarest disease group. As shown in Figure 3(a), the model's efficacy in Diag Pred generally exhibits a positive correlation with the sparsity of the disease groups, with G2-G5 significantly outper-forming G1. However, the performance of the G5 is not optimal, likely due to the low clinical significance of high-frequency diseases in Diag Pred; for instance, fever can indicate multiple underlying health risks. While RAREMed surpasses other baselines in G1 and G3, it compromises accuracy for common diseases. UDC exhibits the most notable boost in G1-G4, showcasing that our innovations excel at enhancing performance for rare diseases.\nFor the Med Rec, we further analyze the predictive performance for patient groups with various rare diseases. More precisely, we identify the rarest disease for each patient and allocate them to the corresponding group based on that rarity. Figure 3(b) indicates that recommendation performance for G1-G3 is limited, as fewer medications co-occur with their disease entities, leading to weaker disease-medication CO signals. Both SHAPE and RAREMed suffer from this issue. While GraphCare attempts to mitigate this problem by leveraging external knowledge, it fails to fully bridge the domain gap during the knowledge fusion and suffers from the potential knowledge noise. In contrast, UDC explicitly optimizes code-level alignment in DRL, facilitating bidirectional alignment of CO signals and textual knowledge, which leads to remarkable improvements.\nIn general, group-level analyses confirm that UDC significantly outperforms other baselines in managing rare diseases, essential for effective clinical decision support."}, {"title": "4.3.3 Plug-in Application", "content": "We examine the extensibility of UDC.\nDiverse PCM. For the PCM, we select three modern methods-GRU, Transformer, and Multi-head Attention-due to their widespread use in sequence-based healthcare baselines [34, 41, 57]. As shown in Figure 4, RAREMed has larger fluctuations, likely due to its ex-plorations of three CO signals, maximizing its advantage from PCM. UDC demonstrates robust performance with various sophisticated PCM. The improvement in Multi-head Attention variants results from their significant CO advancements and convergence toward a more precise subspace during DRL alignment. This superior con-vergence contributes to an overall boost in model performance.\nDiverse PLM. Similarly, for the PLM, we evaluate the integration of both BioGPT [32] and Clinical-BERT [45]. Understanding the textual semantics encoded in clinical notes is another crucial aspect of the DRL, as it can capture similarities between entities that may not be evident from the EHRs alone. Compared to the Sap-BERT and Clinical-BERT, the BioGPT, which is fine-tuned on larger medical-domain corpora, possesses more semantic representations. Furthermore, the larger parameter capacity of BioGPT enables it to obtain an even more robust alignment of the DRL module, leading to notable performance gains when integrated into UDC."}, {"title": "4.3.4 Case Study", "content": "Real Prediction\nTo intuitively demonstrate the superiority of UDC, we present the medication recommendations for a randomly selected patient. Specifically, UDC achieves a significantly higher Jaccard compared to the other baselines. This indicates that UDC can generate diagnos-tic and treatment suggestions that are much closer to the clinically validated outcomes and better distinguish between positive and negative samples. Furthermore, F1-score generated by our model is also higher compared to RAREMed. This finding suggests that instead of relying on broad recommendations to enhance perfor-mance metrics, our framework offers improved recommendations that effectively balance sensitivity and specificity [7]."}, {"title": "5 Conclusion", "content": "In this paper, we introduce UDC, an innovative framework aimed at enhancing the representation semantics of rare diseases. UDC uti-lizes discrete representation learning to connect textual knowledge and CO signals, enabling both signals to be in the same seman-tic space. The framework incorporates condition-aware and task-aware calibration, along with co-teacher distillation tailored for healthcare applications. These advancements significantly enhance the distinguishability and task awareness of encoded representa-tions, as well as the code-level alignment between textual and CO signals. Extensive experiments validate the efficacy of our approach. However, our model has limitations, including the need to integrate modalities beyond text, which will be explored in future work."}, {"title": "A Diverse Condition Encoder", "content": "As evidenced in Table 4, we find that the choice of condition en-coders (MHA) has a minor impact, while Eq. 7 plays a crucial role. In Eq. 7, this normalizing ensures that f's values fall within a sim-ilar range, which helps maintain consistency in the scale of the input features, thereby aiding in training stability and convergence without escalating the model's complexity [52]."}, {"title": "B Diverse Training Methods", "content": "We also experiment with various training methods, such as joint training (0, 0) and inference without fine-tuning, as depicted in Figure 7. Formally, UDC-JT trains PCM and DRL simultaneously, and we observe that this model initially focuses on learning col-laborative signals, leading to DRL training collapse. In contrast, UDC-IF skips fine-tuning and directly performs inference. How-ever, since Fco() does not fully capture the interaction patterns between rare and common diseases, improvements stem primarily from the integration of textual semantic information. From UDC, it is evident that learning these interaction patterns plays a critical role in enhancing the model's overall performance."}, {"title": "C Algorithm", "content": "The algorithm flow is shown in Algorithm 1."}, {"title": "D Dataset Statistics", "content": "MIMIC-III is a widely utilized dataset containing EHRs from over 40,000 patients in critical care. MIMIC-IV, the successor to MIM"}]}