{"title": "Korean Aspect-Based Sentiment Analysis via Implicit-Feature Alignment with Corpus Filtering", "authors": ["Kibeom Nam"], "abstract": "Investigations into Aspect-Based Sentiment Analysis (ABSA) for Korean restaurant reviews are notably lacking in the existing literature. Our research proposes an intuitive and effective framework for ABSA in low-resource languages such as Korean. It optimizes prediction labels by integrating translated benchmark and unlabeled Korean data. Using a model fine-tuned on translated data, we pseudo-labeled the actual Korean NLI set. Subsequently, we applied LaBSE and MSP-based filtering to this pseudo-NLI set as implicit feature, enhancing Aspect Category Detection and Polarity determination through additional training. Incorporating dual filtering, this model bridged dataset gaps, achieving positive results in Korean ABSA with minimal resources. Through additional data injection pipelines, our approach aims to utilize high-resource data and construct effective models within communities, whether corporate or individual, in low-resource language countries. Compared to English ABSA, our framework showed an approximately 3% difference in F1 scores and accuracy. We release the dataset and our code for Korean ABSA, at this link\u00b9.", "sections": [{"title": "Introduction", "content": "In low-resource downstream tasks such as Korean ABSA, constraints exist in constructing ABSA systems that are socially and industrially beneficial (e.g., obtaining accurate labels and high-quality training data, building a efficient serving model). Addressing this challenge is fundamentally crucial for the practical implementation of multilingual ABSA leveraging the advantages of language models (Zhang et al., 2021; Lin et al., 2023). On the other hand, ABSA utilizing Large Language Models like ChatGPT can perform labeling through prompt tuning. However, it still has limitations compared to small-scale models in terms of classifier metrics and resource for training and inference (Wang et al., 2023; Wu et al., 2023).\nIn this study, we derive pseudo-labels for real Korean reviews using machine-translated English ABSA data, inspired by the past research (Balahur and Turchi, 2012; Hoshino et al., 2024). Moreover, we employ Dual filtering on the actual Korean corpus converted to implicit NLI task (Hendrycks and Gimpel, 2017; Sun et al., 2019; Feng et al., 2022), thereby constructing an effective framework coined as Korean ABSA using Pseudo-Classifier with Corpus Filtering (KPC-CF) for implicit-feature alignment. Through this process, we assess the impact of our constructed classifier on the practical alignment of actual reviews. We validate that the pseudo-classifier, generated through the sentence-pair approach, outperforms the single approach in translation task. Furthermore, using the model that predicts the translated dataset most effectively as a baseline, we generate pseudo-labels for actual data and conduct real-world testing of Korean ABSA. This involves subsequent fine-tuning the filtered corpus based on language-agnostic embedding similarity for review and aspect sentence pairs, along"}, {"title": "Two phase of Pseudo-Classifier", "content": "The goal of this research is to propose a framework for achieving the best ABSA on actual data with Korean nuances through high-resource languages. Past research by Balahur and Turchi (2012) has shown that Machine Translation (MT) systems can obtain training data for languages other than English in general sentiment classification. However, existing research using translation data for alignment and alignment-free methods (Li et al., 2020; Zhang et al., 2021) inadequately address the challenge of universal knowledge transfer for linguistic subgroups like Korean. Also, although it was a different domain at Zhou et al. (2021), we found it necessary to investigate whether the concept of pseudo-labels could help bridge the gap of feature between translated source data \\(D_s\\) and actual target language data \\(D_T\\). Therefore, we attempted the following two phases to assess the impact of the generated pseudo-classifier, fine-tuned using translated datasets from the ABSA benchmark and pseudo-labeled actual review data, on Korean ABSA. Fig. 2 shows the two-phase pseudo-classifiers we will employ. In the first phase, similar to the findings of Hoshino et al. (2024), the optimal baseline model for Korean reasoning is identified from the pool of models trained and assessed utilizing the translation dataset. In Phase 2, we fine-tune the baseline model \\(\\Psi_{pre}(\\Phi(D_s);\\theta_{D_s})\\), which was effective in training on \\(D_s\\), by additionally incorporating pseudo-labeled actual \\(D_T\\). Employing the tuned model \\(\\Psi_{post}(\\Phi(D_T);\\theta_{D_s \\rightarrow D_T})\\), we conduct predictions and evaluations on manually labeled actual Korean reviews. Throughout this process, LaBSE and confidence score filtering are performed to enhance implicit features (\\(D_T\\)). Detailed description of our Language Adaptation for aligned ABSA task is provided in Appendix B."}, {"title": "LaBSE based Filtering", "content": "In this approach, we aim to extract good-quality sentences-pair from the pseudo-NLI corpus. Language Agnostic BERT Sentence Embedding model (Feng et al., 2022) is a multilingual embedding"}, {"title": "Confidence score Filtering", "content": "Meanwhile, we need to develop a classifier \\(\\Psi_{post}\\) capable of optimal predictions on the \\(D_T\\) test, which can be considered as out-of-distribution data separate from the \\(D_s\\). Drawing on previous research (Arora et al., 2021), we expect that language shifts (\\(D_s \\rightarrow D_T\\)) embody both Background and Semantic shift characteristics. To ensure robust learning in both aspect detection and sentiment classification, we introduce additional thresholding on Maximum Softmax Probability (MSP; Hendrycks and Gimpel 2017) after LaBSE-based filtering on the \\(D_T\\) train set. When considering an input \\(x = (x_s, x_a) \\in \\mathcal{X}\\) and its corresponding pseudo-label \\(y \\in \\mathcal{Y}\\), the score \\(s(x)\\) for MSP is expressed as:\n\\[S_{MSP}(x) = max_{key} P_{model}(y = k | x).\\]\nThrough this, we intended a dual scoring and filtering process to ensure that our \\(\\Psi_{post}\\) does not retrain on misplaced confidence or subpar prediction outcomes for out-of-distribution data. The algorithm for calculate scores and filter with the target \\(D_T\\) batch set can be found in Algorithm 1."}, {"title": "Experiment", "content": "Based on the results from Kor-SemEval, we observed analogous patterns between mBERT and XLM-RBase, notwithstanding their distinct properties (see Appendix D). Accordingly, We opted for the Baseline-NLI approach (i.e., Baseline_{mBERT, XLM-R}), which demonstrated the best performance, as the base model for Phase 2.\nTo investigate the effect of features (\\(D_T\\)) for each corpus, we conduct baseline tuning comparisons between the PL and the PL-CF (see Tab. 1, 2 for details). The variants of our tuning framework includes:\n\u2022 Baseline+PL (Pseudo-Labeled data): Fine-tuning Baseline-NLI with pseudo-KR3.\n\u2022 Baseline+PL-CF (Corpus Filtering): Fine-tuning Baseline-NLI with the data obtained by truncating instance from pseudo-KR3, where the threshold of MSP (Hendrycks and Gimpel, 2017) is less than 0.5 and the cosine similarity between LaBSE embeddings is less than 0.15.\n\u2022 Baseline+TR (TRanslated data)+PL: Fine-tuning Baseline-NLI (pre-tuned or jointly fine-tuned with Kor-SemEval) using pseudo-KR3.\n\u2022 KPC-CF (Baseline+TR+PL-CF): Fine-tuning Baseline-NLI (pre-tuned or jointly fine-tuned with Kor-SemEval) using PL-CF.\nResults on the KR3 test set are presented in Tab. 2 and Fig. 3. We find that the KPC-CF approach"}, {"title": "Discussion", "content": "In Phase 1, XLM-R, known for its proficiency in capturing cross-lingual representations, exhibits an underfitting tendency concerning the contextual disparities in aspect vocabulary within a single task. This can be attributed to data scarcity relative to model availability for each classifier or viewed as a limitation in single task using SPM in low-resource Korean ABSA (Son et al., 2023). Nevertheless, in the NLI task, it showcases potential by outperforming mBERT, guided by \"aspect\". Conversely, mBERT demonstrates stable results in both single and NLI tasks, exhibiting an overall accuracy increase, particularly in the NLI task (Appendix D).\nFurthermore, Phase 2 reveals that the combination of the NLI approach and translation impacts the metrics of detection in aspects. Pseudo-labels in this phase contribute to enhancing the determination of sentiment, resulting in improved classifier performance. Notably, PL-CF, unlike a mere addition to translated data, play a crucial role in maintaining and enhancing accuracy and F1 score, even with fewer samples. Essentially, the filtered pseudo-NLI set alleviates the bias of pre-trained model parameters during training and enhances"}, {"title": "Conclusion", "content": "Aspect-Based Sentiment Analysis (ABSA) has been recognized as one of the most attractive subareas in text analytics and NLP. However, obtaining high-quality or ample-size labeled data has been one of the most essential issues hindering the development of ABSA. In this paper, we addressed the language gap in ABSA by building a pseudo-classifier. This involved aligning implicit features through dual filtering, further fine-tuning Korean NLI pairs with optimal pseudo-labels from a model trained on translated data. Additionally, we presented Kor-SemEval and KR3 train (pseudo labeled & filtered), testset (Gold) composed of Korean fine-grained set. We invite the community to extend Korean ABSA by providing new datasets, trained models, evaluation results, and metrics."}, {"title": "Appendix", "content": "Multilingual BERT is a BERT trained for multilingual tasks. It was trained on monolingual Wikipedia articles in 104 different languages. It is intended to enable mBERT finetuned in one language to make predictions for another. Azhar and Khodra (2020) and Jafarian et al. (2021) show that mbert performs effectively in a variety of multilingual Aspect-based sentiment analysis. It is also actively used as a base model in other tasks of Korean NLP (Lee et al., 2021; Park et al., 2021), but is rarely confirmed in Korean ABSA tasks. Thus, our study used the pre-trained mBERT base model with 12 layers and 12 heads (i.e., 12 transfomer encoders). This model generates a 768-dimensional vector for each word. We used the 768-dimensional vector of the Extract layer to represent the comment. Like the English language subtasks, a single Dense layer was used as the classification model.\n\nXLM-R XLM-ROBERTa (Conneau et al., 2020) is a cross-lingual model that aims to tackle the curse-of-multilingualism problem of cross-lingual models. It is inspired by RoBERTa (Liu et al., 2019), trained in up to 100 languages, and outperforms mBERT in multiple cross-lingual ABSA benchmarks (Zhang et al., 2021; Phan et al., 2021; Szo\u0142omicka and Kocon, 2022). However, like mBERT, Korean ABSA has yet to be actively evaluated, so we used it as a base model. We use the base version (XLM-RBase) coupled with an attention head classifier, the same optimizer as mBERT."}, {"title": "Task description", "content": "In ABSA, Sun et al. (2019) set the task as equivalent to learning subtasks 3 (Aspect Category Detection) and subtask 4 (Aspect Category Polarity) of SemEval-2014 Task 4 simultaneously. A similar approach was adopted for Korean ABSA in restaurant reviews, aiming to develop a task-specific model through comparison of two PLMs, differing only in tokenization, vocabulary size, and model parameters (Conneau et al., 2020). Defining a unified-serving model using multi-label-multi-class classification from a task-oriented perspective was considered impractical due to challenges in modifying pre-training set and the ongoing injection of additional data, rendering implicit mapping unattainable (Sun et al., 2020; Ahmed et al., 2022; Qin and Joty, 2022). Consequently, the problem has been"}, {"title": "Classification approach", "content": "BERT for single-sentence classification tasks. For ABSA, we fine-tune the pre-trained BERT model to train na (i.\u0435., number of aspect categories) classifiers for all aspects and then summarize the results. The input representation of the BERT can explicitly represent a pair of text sentences in a sequence of tokens. In the case of a single task, only one review text is tokenized and inputted as a sequence of tokens. A given token's input representation is constructed by summing the corresponding token, segment, and position embeddings. For classification tasks, the first word of each sequence is a unique classification embedding [CLS]. Segment embeddings in single sentence classification use one.\nBased on the auxiliary sentence constructed as aspect word text, we use the sentence-pair classification approach to solve ABSA. The input representation is typically the same with the single-sentence approach. The difference is that we have to add two separator tokens [SEP], the first placed between the last token of the first sentence and the first token of the second sentence. The other is placed at the end of the second sentence after its last token. This process uses both segment embeddings (In the case of XLM-ROBERTa, an additional one is placed in the first position, resulting in a total of three segment embeddings). For the training phase in the sentence-pair classification approach, we only need to train one classifier to perform both aspect categorization and sentiment classification. Add one classification layer to the Transformer output and apply the softmax activation function.\nMeanwhile, we additionally use a voting-based ensemble, a typical ensemble method. The ensemble can confirm generalized performance based on similarity of model results in NLI task (Xu et al., 2020). So, we add separate power-mean ensemble result to identify a metric that amplifies probabilities based on the Pre-trained Language Models (PLMs). we reported the ensemble results of the top-performing models trained on NLI tasks for each PLM."}, {"title": "Evaluation of Language Adaptation", "content": "In this paper, we focus on the langauge adaptation of ABSA. The input data includes a set of labeled sentences from a translated source data Ds = \\(\\{(x_s^i, x_a^i, y^i)\\}_{i=1}^N\\) and a set of unlabeled sentences from a target language data Dr = \\(\\{(x_s^i, x_a^i)\\}_{i=1}^N\\) Our goal is to validate our proposed filtering technique across three types of tasks: (1) training only pseudo-labeled DT, (2) joint training using both Ds and DT, (3) transfer learning from Ds to Dr. Namely, unlike Chen et al. (2023), we fixed token embeddings and fine-tuned the transformer body & head. Contrary to Gururangan et al. (2020), we trained with Ds = \\(\\{(x_s^i, x_a^i, y^i)\\}_{i=1}^N\\) and then reinitialized the classifier layer for further fine-tuning. This approach examines if the universal encoder and params 0 aligned with Ds enhance implicit alignment via pseudo-labels of refined Dr. The joint \\(((D_s \\bar{} D_T); O_{D_SUD_T})\\) model was also evaluated, despite its inefficiency in terms of memory and computation, for data validation purposes."}, {"title": "Experimental Setup", "content": "We translate the SemEval-2014 Task 4 (Pontiki et al., 2014) dataset\u00b3. Moreover, it is evaluated for Korean aspect-based sentiment analysis. The training data was machine-translated (by Google Translate), and Test data was corrected manually only for fewer than 10 instances where abnormal translations occurred after machine translation. Each sentence contains a list of aspect xa with the sentiment polarity y. Ultimately, given a"}, {"title": "Hyperparameter", "content": "All experiments are conducted on two pre-trained cross-lingual models. The XLM-ROBERTa-base and BERT-base Multilingual-Cased model are fine-tuned. The number of Transformer blocks is 12, the hidden layer size is 768, the number of self-attention heads is 12, and the total number of parameters for the XLM-RoBERTa-base model is 270M, and BERT base Multilingual-Cased is 110M. When fine-tuning, we keep the dropout probability at 0.1 and the initial learning rate is 2e-5. In the Preliminary Experiment (Appendix D), we set the batch size to 3, this setting was applied to both single and NLI tasks. The max length was set to 512, and for epochs beyond 3, no significant performance improvement was observed, so the results from epoch 2 were noted. Subsequently, in Main Experiment (Sec. 3.1), we fine-tuned with a batch size of 16, following the pattern of previous experiments (Karimi et al., 2021). However, no significant performance improvement was observed within 10 epochs, so we reported the results from epoch 4. Each reported metric is the average of runs with four different random seeds to mitigate the effects of random variation in the results."}, {"title": "Preliminary Experiment", "content": "We conducted evaluations for each of the mBERT-single, XLM-RBase-single, mBERT-NLI, XLM-RBase-NLI, and NLI-ensemble models. We included the results from the previous SemEval14 research and Kor-SemEval to compare and evaluate the performance in Korean.\nResults on Kor-SemEval are presented in Tab. 5 and Tab. 6. Similar to the SemEval results, it was confirmed that tasks converted to NLI tasks tend to be better than single tasks, with mBERT achieving better results in single and XLM-RBase in NLI. The XLM-RBase-NLI model performs best, excluding precision for aspect category detection."}, {"title": "Limitations and Future Work", "content": "Acknowledging limitations is crucial before delving into future work. While mBERT exhibits somewhat inadequate transfer effects, which can be attributed to its smaller model size. Due to pretraining solely on translation data, the scores are densely populated, resulting in limited performance improvement through additional training. Nevertheless, unlike jointly with Target None, the regression line of maximum probability for Target Polarity still trends upward according to LaBSE score (see Tab 1, Figure 5), indicating that ongoing evaluation of threshold points for the two scores can be done amidst more diverse and precise patterns. Meanwhile, we conducted direct inference on individual NLI inputs through a demo5. Upon detecting various aspects within the reviews, we observed a slight predominance towards the \"None\" class within the class-wise similar probability distributions per sentence in TR training, while a more concentrated prediction tendency towards the binary class was observed in PL training. Furthermore, a tendency for one class to dominate over two or more aspect classes within review sentences was identified. This appears to stem from the class imbalance in TR and contextual differences (Jin et al., 2024) within all review data (i.e., TR & PL).\nOur analysis of the PL-CF thresholding reveals that while it reduces the number of predicted Target None classes resulting from extensive training on None class in TR, it also qualitatively explains why"}, {"title": "Computational Resources", "content": "All experiments in this study were conducted using high-performance computing resources from a leading cloud platform. The computational infrastructure included instances with 50GB or more of RAM, featuring NVIDIA A100 and V100 GPUs, chosen for advanced parallel processing capabilities essential for our LMs."}]}