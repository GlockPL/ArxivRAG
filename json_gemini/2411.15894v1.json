{"title": "Navigating the Effect of Parametrization for Dimensionality Reduction", "authors": ["Haiyang Huang", "Yingfan Wang", "Cynthia Rudin"], "abstract": "Parametric dimensionality reduction methods have gained prominence for their ability to generalize to unseen datasets, an advantage that traditional approaches typically lack. Despite their growing popularity, there remains a prevalent misconception among practitioners about the equivalence in performance between parametric and non-parametric methods. Here, we show that these methods are not equivalent \u2013 parametric methods retain global structure but lose significant local details. To explain this, we provide evidence that parameterized approaches lack the ability to repulse negative pairs, and the choice of loss function also has an impact. Addressing these issues, we developed a new parametric method, ParamRepulsor, that incorporates Hard Negative Mining and a loss function that applies a strong repulsive force. This new method achieves state-of-the-art performance on local structure preservation for parametric methods without sacrificing the fidelity of global structural representation. Our code is available at https://github.com/hyhuang00/ParamRepulsor.", "sections": [{"title": "Introduction", "content": "Dimension reduction (DR) methods are incredibly useful for data analysis. They provide a bird's eye view of a dataset that shows clusters and their relationships. These algorithms have been used for examining and processing images [1], text documents [2, 3], and biological datasets [4\u20138]. The successes of modern DR methods can mostly be attributed to neighborhood embedding (NE), which is the basis for modern DR methods [9] including t-SNE, LargeVis, UMAP, and PaCMAP [10\u201313]. These algorithms aim to optimize the low-dimensional layout of the data, such that the high dimensional local structure (i.e., neighborhoods) are preserved.\nA major weakness of existing NE algorithms is that they struggle with adaptability to large, incrementally updated datasets. These algorithms depend on a K-Nearest Neighbor graph, encompassing the entire dataset, to generate the embedding. Consequently, the introduction of new data necessitates a complete re-computation of the embedding, leading to significant time and computational resource demands for large datasets. Although recent adaptations have been developed to optimize only the additional data [14, 13], these modifications potentially alter the original algorithm's objective function, thereby compromising the embedding's quality.\nAddressing these challenges, recent developments in combining neural networks with NE algorithms have shown promise. These algorithms maintain the same objectives as traditional NE methods but leverage neural networks to optimize the projection of high-dimensional data into lower-dimensional spaces [16-18]. The integration of neural networks allows these NE algorithms to be effectively trained on large datasets and generalize to unseen data. Throughout this paper, we refer to this class of algorithms as parametric algorithms. However, as shown in Fig. 1, despite the similarity in loss"}, {"title": "Fundamentals of Neighborhood Embedding Algorithms and Contrastive Learning", "content": "We provide essential background on Neighborhood Embedding (NE) methods and notation. We notate the high dimensional data as $X = {X_1 ... X_n} \\in R^D$, where n is the number of data points, and D is the dimension. NE algorithms aim to preserve predefined high-dimensional similarities within a low-dimensional embedding to reveal the local and global structure of X. Specifically,"}, {"title": "NCE/InfoNCE-based: t-SNE and NCVis", "content": "Both NCE- and InfoNCE-based approaches assume the high-dimensional data similarities (the $s_{ij}$'s) follow an underlying data similarity pattern, represented by an unknown distribution p. These methods learn a function $f_\\theta$ that generates a similar low-dimensional similarity pattern, described by a distribution q, aiming to match p. q decreases as the pairwise distances in the low-dimensional space increase, though their exact relationship can vary. Since q represents a probability distribution, it must be normalized to ensure all possibilities sum up to 1. The only difference is that NCE uses a logistic loss, whereas InfoNCE uses a cross-entropy loss for the data distribution match.\nApproximating p as a Bernoulli distribution [24] with value 1 for NN pairs and 0 for FP pairs. Assuming that for each step in the optimization, we optimize a batch that contains one NN pair and m FP pairs, q should minimize:\n$L_{NCE} = -E_{ij\\in NN,ik_c=1...m\\notin NN} log(\\frac{q_{ij}}{q_{ij} + \\sum_{c=1...m} q_{ik_c}})$ (2)\n$L_{InfoNCE} = -E_{ij\\in NN,ik_1...m\\notin NN} (log q_{ij} - m log(\\sum_{c=1...m} q_{ik_c}) )$ (3)\nt-SNE, the most popular NE algorithm, utilizes a loss defined over the full data set. The raw t-SNE loss is usually written as the KL-divergence between high-dimensional and low-dimensional conditional probability distributions p and q. Here, we separate the loss following [25, 13]. [18] notes that the exact values of the $p_{ij}$'s have limited impact and can be treated as binary weights without impacting outcomes. To simplify the calculation and allow for mini-batch stochastic gradient descent, [18] rewrote this loss as an InfoNCE loss [22]. Denoting $d^2(i, j) = ||f_\\theta(x_i) \u2013 f_\\theta(x_j)||^2 + 1 = ||y_i - y_j|| + 1$, the t-SNE loss function can be rewritten as an InfoNCE loss:\n$L_{t-SNE}(\\theta) = -E_{ij\\in NN,ik_1...m\\notin NN} log(\\frac{\\frac{1}{d^2(i, j)}}{\\frac{1}{d^2(i, j)} + \\sum_{c=1...m} \\frac{1}{d^2(i, k_c)}})$ (4)"}, {"title": "NEG-based: UMAP", "content": "Negative Sampling (NEG) [23] simplifies the modeling process. Define $q_\\theta$ to be a similarity function in the low dimensional space:\n$L_{NEG}(\\theta) = -E_{ij\\in NN} log(\\frac{q_{ij}}{1+q_{ij}})- mE_{ij \\notin NN} log(\\frac{1}{1+q_{ij}})$ (6)\nUMAP [12] is a DR algorithm that utilizes the NEG loss [18]. Its loss function is\n$L^{UMAP}(\\theta) = -E_{ij \\in NN} log(\\frac{q^{UMAP}(i, j)}{1+ q^{UMAP}(i, j)}) - mE_{ij \\notin NN} log(\\frac{1}{1 + q^{UMAP}(i, j)})$ (7)\nwhich is NEG with the similarity kernel $q^{UMAP}(i, j) = d(i,j)^{-1}$."}, {"title": "PaC\u041c\u0410\u0420", "content": "PaCMAP [13] is another recent DR algorithm that achieves high-quality data visualization. Compared to other NE algorithms, PaCMAP's loss function is designed to follow several mathematical design principles, but does not have a probabilistic explanation. The loss function (omitting the mid-near pairs term, as it is only relevant during the initial epochs, see Appendix D) is defined as follows:\n$L_{ij \\in NN} = W_{NN}\\frac{d^2(i, j)}{d^2(i, j) + C_1}$, $L_{ikeFP} = W_{FP}\\frac{1}{d^2(i, j) + C_2}$\nin which the W weights change based on the epoch, and $C_1$ and $C_2$ are set to 10 and 1, respectively.\nTo study the effect of parametrization on DR algorithms, we extend the PaCMAP framework to incorporate an MLP to map the high-dimensional input to the low-dimensional embedding. We refer to the resulting parametric algorithm as ParamPaCMAP. Implementation details are in Appendix F.."}, {"title": "Effect of Parametrization on DR Results", "content": "Machine learning practitioners have long believed that parametric NE algorithms behave similarly to their non-parametric counterparts [17, 18]. In this section, we investigate the performance of the aforementioned parametric and non-parametric versions of these algorithms. To understand the effect of parametrization more thoroughly, for each parametric DR method, we additionally implemented three new versions, using a neural network with 0 hidden layers (i.e., a linear model), 1 hidden layer, or 2 hidden layers as a projector. We fix the number of neurons for each layer to 100.\nObservation 1: Parametric NE algorithms typically lead to worse visual effects as well as worse local structure preservation. Our results indicate that parametric NE algorithms often fail to produce embeddings of the same quality as their non-parametric counterparts, even on simple datasets such as MNIST [15]. The non-parametric methods in the rightmost column of Fig. 2 are able to separate the clusters fairly well, but from the first four columns of Fig. 2, we see that all four parametric algorithms generate clusters that are densely packed with indistinct boundaries, despite the fact that clusters in MNIST are actually separated. These blurred boundaries result in poorer preservation of local structure, with the possible exception of Parametric PaCMAP."}, {"title": "ParamRepulsor", "content": "While our ParamPaCMAP algorithm preserves better local structure, the embedding space remains suboptimal, with some clusters that should be distinct still merged together. To solve the problem from its root cause, we propose ParamRepulsor, a novel parametric algorithm built upon our ParamPaCMAP. Pseudocode for ParamRepulsor is found in Alg. 1 and detailed in Alg. 2 in App. F.\nThere are several major differences of ParamRepulsor from other methods, the major one being the use of Hard Negative Mining in the repulsive terms. Our goal is to learn from Hard Negative (HN) Samples - pairs whose DR projections are close but should be far apart. Efficiently sampling HNs could be challenging. Existing approaches either rely on ground truth labels that are not applicable in the unsupervised DR setting [28, 29], or rely on the InfoNCE loss [30] which is less useful for NEG losses. We select mid-near (MN) pairs for HN sampling (for the opposite purpose they are used in PaCMAP, where they exert attractive forces). A MN point for point i is identified through the following process: 1) sample h ~ Uniform{1, n} points from the high-dimensional data, and 2) select the second closest point from the sampled set. Here, we use h = 6. We justify the use of MN pairs as HN samples based on two key observations.\nObservation 3: Using MN for HN sampling reduces the probability for false negatives. Existing DR algorithms sample the negative pairs from an (approximately) uniform distribution over all possible pairs. While this approach enhances computational efficiency, it often results in false negatives, which is known to be problematic for contrastive learning [31, 30]. We show that MN pairs are ideal candidates for negatives as they rarely become false negatives."}, {"title": "Experiments", "content": "Here, we evaluate the performance of our ParamPaCMAP and ParamRespulsor algorithms empirically. To contextualize our findings, we juxtapose our results against those obtained from other contemporary parametric DR algorithms. Visualization for the embeddings generated by all algorithms can be found in App. \u0421.\nDatasets. We use a wide-ranging collection of datasets across various disciplines. For image analysis, we analyzed the MNIST [15] and Fashion-MNIST (F-MNIST) [32] datasets, along with COIL-20 [33] and COIL-100 [34]. In the domain of computational biology, our assessment leveraged single-cell RNA-sequencing (scRNA-seq) datasets from studies by [35], [36], [37], [38]. Further diversifying our dataset selection, the 20 Newsgroups (20NG) [39] text dataset was included for textual data analysis. The preprocessing of scRNA-seq datasets adhered to the methodology outlined by [40]. Additionally, simulated datasets featuring predefined known structures \u2013 such as Circle, Mammoth [41, 42], Gaussian Lineage, and Gaussian Hierarchical [43] \u2013 were integrated into our analysis. See Section G.1 for more details. This multifaceted dataset compilation enables a thorough examination of the DR algorithms' performance across a spectrum of datasets.\nAlgorithms. Besides the two algorithms we proposed in this work, ParamPaCMAP (P-PaCMAP) and ParamRepulsor (P-Rep), we also perform experiments on other recent Parametric NE algorithms: Parametric UMAP (P-UMAP) [17], Parametric Info-NC-t-SNE (P-ItSNE) [18], Parametric Neg-t-SNE (P-NtSNE), and Parametric NCVis (P-NCVis) [24, 18]. Besides NE algorithms, we also compare against Geometric Autoencoder (GeoAE) [44], an autoencoder-based DR algorithm. While we note that there are many other parametric DR algorithms, they either aim to serve as an intermediate representation for downstream tasks (i.e., not visualization) [45, 46], or focus only on image dataset only [1]. We refer readers to Section 6 for more details. We compare these algorithms on local and global structure preservation. For each algorithm, we use the hyperparameter settings and the network structure suggested in their implementation. Coincidentally, all the parametric algorithms in our experiment (except for GeoAE) are equipped with a 3-layer 100-neuron fully-connected neural network as their parametric projector $f_\\theta$.\nOther setup. For each experiment, we ran each DR algorithm 10 times using different random seeds to obtain 10 embeddings. We report the average metric measured across these 10 embeddings, highlighting the highest value in bold. An independent t-test with a significance level of p = 0.05 was conducted to assess significant differences between methods. Metrics not significantly different from the highest value are in italics."}, {"title": "Local Structure Evaluation", "content": "We first look into the local structure of the embedding, which examines DR algorithms' ability to discover the cluster structure. Following previous works [47, 48, 13, 43], we evaluate local structure using three approaches, with results below. All visualizations can be found in App. C. We achieve state-of-the-art performance in local structure preservation.\nLocal Structure 1: k-NN Accuracy. Here, DR is performed and the labels are revealed afterwards. A k-NN model then classifies points in the DR projection, with its accuracy as the metric of interest. We perform leave-one-out cross validation, and utilize a k-NN classifier to predict the label of the point. For embedding data with good local structure, points that belong to the same class should be close to each other, which would yield a higher k-NN accuracy. In this study, we use k = 10. Table 1 presents the 10-NN accuracy of each DR algorithm. ParamRepulsor achieves the highest accuracy on 10 out of 14 datasets and comes close to the highest accuracy on the remaining datasets, demonstrating its strong performance in preserving local structure."}, {"title": "Global Structure Evaluation", "content": "We evaluate global structure by evaluating the preservation of cluster-level triplet relationships. Cluster-level triplet relationship preservation is important, particularly for computational biologists performing lineage analysis. Following [43], the metric for this is a Spearman (rank correlation). To compute it, we take one cluster centroid c and rank all other centroids based on low-dimensional distance to c. We also repeat this for all C cluster centroids and place these rankings in a single vector. We repeat the process for the high-dimensional space and place these rankings in another vector. The Spearman correlation between these two vectors is the result, shown in Table 4 in App. H.3. Out of the 14 datasets, ParamPaCMAP achieves the highest correlation on 5 of them, whereas ParamRepulsor achieves the highest on 4. These results suggest our ideas are powerful for global structure preservation."}, {"title": "Related Work", "content": "The evolution of DR algorithms can be broadly categorized into two distinct phases. In the initial phase, the focus was on the development of methods that preserved only global structure. Key techniques in this category include Principal Components Analysis (PCA) [49], Multidimensional Scaling [50], and Non-negative Matrix Factorization [51]. While these methods effectively maintain the global layout of the data, their primary limitation is that they often fail to retain the inherent neighborhoods and clusters of the data.\nSubsequent DR methods were developed to address the shortcomings by emphasizing the preservation of local structure, specifically focusing on preserving k nearest neighbor relationships in the original dataset. These local methods, such as Isomap [52], Local Linear Embedding (LLE) [53], Laplacian Eigenmap [54], and more recent Neighborhood Embedding (NE) algorithms like t-SNE [10] and UMAP [12], are particularly adept at maintaining cluster structure. However, they may not adequately preserve the overall spatial layout of clusters. NE methods are more frequently used because they show clusters and manifolds in the high-dimensional space that are difficult to see any other way.\nNE methods are typically non-parametric, creating a low-dimensional embedding that maps each data point to a location in 2D, but there does not exist a function that maps from the original (high-dimensional) space to the embedding space. To map new points to the low dimensional space, one"}, {"title": "Discussion and Limitations", "content": "Parameterization of DR methods has major practical advantages. It allows for new data to be mapped directly from the high-dimensional space to the low-dimensional space by a function. We introduced a new method called ParamRepulsor, which demonstrates enhanced preservation of local structure without compromising global structure metrics, making it applicable across a broad spectrum of scientific inquiry.\nWe note that our method also exhibit limitations. Although ParamRepulsor outperforms Parametric UMAP in terms of speed, it requires more computational time than Parametric Info-NC-t-SNE. Other open questions that are not resolved by this work include the design of evaluation metrics that better reflect performance, and choosing the optimal architecture for both preservation and generalization."}, {"title": "Code and data availability", "content": "Implementations of ParamRepulsor/ParamPaCMAP discussed in this paper, along with the code for the experiments, are available at https://github.com/hyhuang00/ParamRepulsor. The datasets used in our study are publicly accessible from their original publications."}, {"title": "Proof that PaCMAP's loss follows NEG", "content": "Recall that the NEG loss follows the form\n$L_{NEG}(\\theta) = -E_{ij\\in NN} log(\\frac{q_{\\theta} (y_i, y_j)}{1+ q_{\\theta} (y_i, y_j)}) - mE_{ij \\notin NN} log(\\frac{1}{1 + q_{\\theta} (y_i, y_j)})$ (10)\nwhere m is the number of negative samples of each batch and $q_\\theta$ is the similarity function defined in the low dimensional space.\nNow, we consider the PaCMAP loss. While the PaCMAP loss optimization process involves three stages with different emphasis on the loss terms, the first two stages are essentially equivalent to the early exaggeration used in t-SNE and UMAP [13]. Therefore we consider only the last stage of the PaCMAP optimization process that involves only the NN and FP losses:\n$L^{PACMAP}(\\theta) = L_{ij\\in NN} + L_{ij\\in FP} = \\sum_{ij\\in NN} \\frac{1}{d^2(i, j) + 10} + \\sum_{ij\\in Fp} \\frac{1}{d^2(i, j) +1}$ (11)\nWhile PaCMAP samples the repulsion using a pre-defined set of further pairs, the further pairs themselves are uniformly sampled from all the points that are not nearest neighbors. The number of neighbors are usually tiny compared to the size of the dataset. In our experiments, adding the nearest neighbors back to the further pairs candidate set does not generate any major impact to the datasets. Therefore, here we consider it to be essentially the same as sampling from ij \u2208 E.\nSince the purpose of the loss function is to find $\\theta$ that minimizes it, applying any affine transformation will not affect the optimum. Recall that in PaCMAP, the set FP is m times of the size of NN. Thus, we have:\n$[L^{PaCMAP}(\\theta) = #NN. \\frac{d^2(i, j)}{d^2 (i, j) + 10} +  \\sum_{ij\\in Fp} \\frac{1}{d^2(i, j) +1} ] $ (12)\n$= #NN. ( E_{ij\\in NN} \\frac{d^2(i, j)}{d^2(i, j) + 10}+ mE_{ij\\in E} \\frac{1}{d^2(i, j) + 1} ) $ (13)\n$\\propto E_{ij\\in NN} \\frac{d^2(i, j)}{d^2(i, j) + 10} + mE_{ij\\in E} \\frac{1}{d^2(i, j) + 1}$ (14)\n$\\propto -E_{ij\\in NN} \\frac{10}{d^2(i, j) + 10}- mE_{ije} \\frac{d^2(i, j)}{d^2(i, j) + 1}$ (15)\n= -E_{ij\\in NN} log exp(\\frac{d^2(i, j)}{d^2(i, j) + 10}) - mE_{ije} log exp(\\frac{1}{d^2(i, j) + 1}).$ (16)\nDue to the different choices of normalizing constant in the NN loss and the FP loss, the PaCMAP actually utilizes a different kernel to model the similarity between the NN and FPs. Solving for the functions $q^{NN}_{\\theta}$ and $q^{FP}_{\\theta}$, we have the result:\n$q^{NN}_{\\theta}(y_i, y_j) = \\frac{exp(\\frac{d^2(i,j)}{d^2(i,j)+10})}{1 - exp(\\frac{d^2(i,j)}{d^2(i,j)+10})}$ (17)\n$q^{FP}_{\\theta}(y_i, y_j) = \\frac{1}{exp(d^2(i,j)+1) - exp(d^2(i,j))}$ (18)"}, {"title": "Proof that the Mid-Near Hard Negative False Negative Rate converges to 0 quadratically", "content": "For simplicity, we consider a dataset of size n + 1, so that each point will sample from a pool of n points. For each point, we consider its k = 10 nearest neighbors as its positive points, and the rest of the points as negative points.\nNow, we consider the mid-near sample process. Recall that the mid-near point samples the second closest point from a pool of 6 points. Denote the event that a mid-near point being a false negative"}, {"title": "Discussion on the Depth of of Neural Network Projector", "content": "As a supplement to Fig. 2, we extend the number of layers beyond three for Into-NC-t-SNE, UMAP and PaCMAP. Here, the local metric represents 10-NN accuracy, while the global metric denotes the random triplet preservation. Results show that further increasing the number of layers beyond increasing the number of layers beyond three yields only diminishing and negligible improvements in local structure on all three methods."}, {"title": "Discussion on the Hyperparameter settings of Parametric DR algorithm", "content": "Fig. 6 illustrates the effect of varying the number of nearest neighbors (NN) in P-ItSNE, P-UMAP, and P-PaCMAP. Adjusting the number of NNs during NN-graph construction is commonly regarded as a key mechanism for controlling the local-global structure trade-off in nonparametric DR algorithms [58-60]. Nevertheless, in the parametric setting, modifying the number of NNs had minimal influence on the resulting embeddings. Notably, increasing the number of NNs beyond the typical range (e.g., to 60) can severely disrupt the structure, as observed in the P-ItSNE case."}, {"title": "Visualizations from all DR methods", "content": "In this section we provide visualizations for the output of all DR methods. Notably, ParamRepulsor performs well on all datasets and achieves state-of-the-art on both local and global structure preservation. On MNIST, ParamRepulsor is the only parametric algorithm that separates the clusters with clear boundaries. Compared to nonparametric algorithms, ParamRepulsor has better global structure preservation, as it is able to keep the structure of the mammoth on Mammoth dataset, and keep the structure of the hierarchy on the Hierarchy dataset."}, {"title": "Computation Platforms", "content": "All experiments are conducted with an Exxact TensorEX 2U Server with 2 Intel Xeon Ice Lake Gold 5317 Processors @ 3.0GHz. We limit the RAM usage to be 32GB. Parallel computation are performed over a single Nvidia RTX A5000 GPU."}, {"title": "Additional Experiments, Tables and Figures", "content": "Fig. 22 provides a comprehensive analysis over distances between different kinds of pairs, generated by multiple DR algorithms. We can see that all parametric methods generate a shorter FP distance compared against their non-parametric counterpart. MN pairs, though should be classified as FPs, tend to be harder to optimize, resulting in a shorter distance on average."}, {"title": "Computational Speed Evaluation", "content": "As datasets grows larger, scalability becomes more important. We evaluate the time consumed by ParamInfo-NC-t-SNE, ParamRepulsor and ParamUMAP on two extremely large datasets, from [65] and [4]. The dataset sizes are 1, 306, 127 and 2,058, 652, respectively. We can see that ParamRepulsor outperforms ParamUMAP in terms of scalability. The computational efficiency of ParamRepulsor can be further improved."}, {"title": "Additional Tables", "content": "Table 2, 3, 5, 4 measure the SVM accuracy, k-nearest neighbor preservation ratio, Triplet preservation ratio, and cluster centroid distance correlation. We note that GeoAE performs particularly well on Triplet preservation. This is expected: as an autoencoder-based method, GeoAE aims to preserve the geographical distance information in the high-dimensional space, usually at the cost of the local structure. As a result, its local structure performance is particularly low. However, our method, ParamPaCMAP and ParamRepulsor, achieve comparable result on this metric."}]}