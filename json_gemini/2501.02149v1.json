{"title": "Attribute-Based Robotic Grasping with Data-Efficient Adaptation", "authors": ["Yang Yang", "Houjian Yu", "Xibai Lou", "Yuanhao Liu", "Changhyun Choi"], "abstract": "Robotic grasping is one of the most fundamental robotic manipulation tasks and has been the subject of extensive research. However, swiftly teaching a robot to grasp a novel target object in clutter remains challenging. This paper attempts to address the challenge by leveraging object attributes that facilitate recognition, grasping, and rapid adaptation to new domains. In this work, we present an end-to-end encoder-decoder network to learn attribute-based robotic grasping with data-efficient adaptation capability. We first pre-train the end-to-end model with a variety of basic objects to learn generic attribute representation for recognition and grasping. Our approach fuses the embeddings of a workspace image and a query text using a gated-attention mechanism and learns to predict instance grasping affordances. To train the joint embedding space of visual and textual attributes, the robot utilizes object persistence before and after grasping. Our model is self-supervised in a simulation that only uses basic objects of various colors and shapes but generalizes to novel objects in new environments. To further facilitate generalization, we propose two adaptation methods, adversarial adaption and one-grasp adaptation. Adversarial adaptation regulates the image encoder using augmented data of unlabeled images, whereas one-grasp adaptation updates the overall end-to-end model using augmented data from one grasp trial. Both adaptation methods are data-efficient and considerably improve instance grasping performance. Experimental results in both simulation and the real world demonstrate that our approach achieves over 81% instance grasping success rate on unknown objects, which outperforms several baselines by large margins.", "sections": [{"title": "I. INTRODUCTION", "content": "OBJECT attributes are generalizable properties in object manipulation. Imagine how we describe a novel object when asking someone to fetch it, \u201cPlease give me the apple, a red sphere.\u201d, we intuitively characterize the target by its appearance attributes (see Fig. 1). If an assistive robot can be similarly commanded utilizing such object attributes (e.g., color, shape, and category name, etc.), it would allow better generalization capability for novel objects than using a discrete set of pre-defined category labels. Moreover, individuals learn to recognize and grasp an unknown object through rapid interactions; hence, it would be advantageous if a grasping pipeline is capable of adapting with minimal adaptation data. These factors motivate the development of attribute-based robotic grasping with data-efficient adaptation capability.\nRecognizing and grasping a target object in clutter is crucial for an autonomous robot to perform daily-life tasks in the real world. Over the past years, the robotics community has made substantial progress in target-driven robotic grasping by combining off-the-shelf object recognition modules with data-driven grasping models [2], [3]. However, these recognition-based approaches presume a unique ID for each category and are likely to experience limited generalization when applied to novel objects. In contrast, we propose an attribute-based robotic grasping approach that enables a robot to grasp an attributes-specified target object. The intuition of using attributes for grasping is that the grounded attributes can help transfer object recognition and grasping capabilities across different environments.\nSuffering from domain shift [4], a machine learning model trained with the data in one domain is subject to limited generalization when tested in another domain. In robotic grasping, the source of domain shifts includes novel objects, new environments, perception noises, etc. To mitigate the domain shift, domain adaptation methods [5] are widely used for model transfer. These adaptation methods, on the other hand, typically require the collection of a large adaption dataset, which is costly, inefficient, and time-consuming. To efficiently transfer our pre-trained attribute-based grasping model, we present two tailored adaptation methods. Both the two proposed adaptation methods are data-efficient, requiring minimal data collecting and labeling.\nCompared to recognition-based robotic grasping (i.e., employing pre-trained recognition modules), the challenges of attribute-based grasping are 1) mapping from workspace images and query text of the target to robot motions, 2) associating abstract attributes with raw pixels, 3) data labeling in target-driven grasping, and 4) data-efficient adaptation to unknown objects and new scenes. In this paper, we design an architecture that consists of a multimodal encoder (i.e., encoding both visual and textual data) and an affordances decoder (i.e., predicting instance grasping affordances [6]). The key aspects of our system are:\n\u2022 We design the deep grasping neural networks that represent 3-DOF grasp poses. After encoding and fusing visual-textual representations, the networks rotate the fused features to account for different grasping angles, and then predict pixel-wise instance grasping affordances.\n\u2022 To learn a multimodal metric space, we employ the equation of object persistence before and after grasping; the visual embedding of a grasped object should be equal to the textual embedding of that object.\n\u2022 Our model learns object attributes that generalize to new objects and scenes by only using basic objects (of various colors and shapes) in simulation.\n\u2022 With the pre-trained attribute representations, our model supports efficient adaptation with minimal data. Adversarial adaptation regulates the image encoder with augmented data of unlabeled images, whereas one-grasp adaptation updates the end-to-end model with augmented data requiring only one successful grasp trial. Both adaption approaches are data-efficient, and they can be employed independently or in combination to improve instance grasping performance.\nThe deep grasping model in our approach is fully self-supervised through the interactions between the robot and objects. Fig. 1 presents an example of attribute-based robotic grasping, wherein our approach successfully grounds object attributes and accurately predicts grasping affordances for an attributes-specified target object.\nIn our prior work [1], we proposed 1) an end-to-end architecture for learning text-commanded robotic manipulation and 2) a method of self-supervising multimodal attribute embeddings through object grasping to facilitate quick adaptation. As an evolved paper, this article presents an in-depth study of adaptation in robotic manipulation and strives to improve the autonomy of robots by achieving self-supervision and self-adaptation. The pre-trained model is self-supervised in a simulation that only uses basic objects of various colors and shapes. In our adaptation framework, we make use of autonomous robots to collect raw data for adaptation. We present three core technical contributions as follows:\n1) A sequential adaptation scheme. We propose a robotic grasping adaptation framework that comprises two stack-able and data-efficient adaptation methods. The adversarial adaptation and one-grasp adaptation methods aim to comprehensively adapt the model for object recognition and grasping. Through data-efficient adaptation, the robot adeptly grasps challenging objects, eliminating the need for extensive data collection.\n2) Data-efficient augmentation methods. We design data augmentation methods that only require unlabeled images of candidate objects for adversarial adaptation and one-grasp data of a target object for one-grasp adaptation.\n3) Evaluation and analysis of robot grasping. We evaluate the grasping model in simulated and real-world scenes with various testing objects and domain gaps, which verifies the effectiveness of our grasping model. Furthermore, the ablative analysis of the data augmentation methods shows the efficiency of our approach.\nWith observations from an RGB-D camera, our robot system is designed to grasp a target object following the user command containing object attributes. To our best knowledge, this is the first work that explores object attributes to improve the generalization and adaptation of deep robotic grasping models. We believe that the adaptation framework not only enhances the overall performance but also opens up new possibilities for solving the problem in target-driven robotic manipulation."}, {"title": "II. RELATED WORK", "content": "Though there are different taxonomies, the existing work of robotic grasping can be roughly divided according to approaches and tasks: 1) model-driven [7] and data-driven [8] approaches; 2) indiscriminate [9] [10] and instance grasping [2] tasks. Our approach is data-driven and focuses on instance grasping. Typical instance grasping pipelines assume a pre-trained object recognition module (e.g., detection [2], segmentation [3] [11], template matching [12], and object representation [13], etc.), limiting the generalization for unknown objects and the scalability of grasping pipelines. Our model is end-to-end and exploits object attributes for generalization. Some recent research also proposes end-to-end learning methods for instance robotic grasping. [14] learns to predict the grasp configuration for an object class with a learning framework composed of object detection, classification, and grasp planning. In [15], CCAN, an attention-based network, learns to locate the target object and predict the corresponding grasp affordances given a query image. Compared to these methods, the main features of our work are two-fold. First, we collect a much smaller dataset of synthetic basic objects to learn generic attribute-based grasping. Moreover, our generic grasping model is capable of further adapting to new objects and domains. Second, our approach takes a description text of target attributes as a query command, which is more flexible when grasping a novel object."}, {"title": "B. Attribute-Based Methods", "content": "Object attributes are middle-level abstractions of object properties and generalizable across object categories [16]. Learning object attributes has been widely studied in the tasks"}, {"title": "III. PROBLEM FORMULATION", "content": "The attribute-based robotic grasping problem in this paper is formulated as follows:\nDefinition 1. Given a query text for a target object, the goal for the robot is to grasp the corresponding object that is placed in the cluttered workspace.\nTo handle the natural language that is diverse and unconstrained, we assume a language attribute parser, such as [35], and make the following assumption:\nAssumption 1. The query text is parsed into the keywords of object attributes as an input to the robotic grasping model.\nWe consider color, shape, and category name attributes in this paper, while the proposed approach is extensible to other attributes (e.g., texture, pose, and functionality, etc.). In order to make object recognition tractable, we have the following assumption regarding object placement:\nAssumption 2. The objects are stably placed within the workspace, and there is no stacking between objects.\nWhile we show robotic grasping as a manipulation example in this paper, the proposed attribute-based learning methods should be, in principle, extensible to other robotic manipulation skills, such as suction, pushing, and placing."}, {"title": "IV. LEARNING ATTRIBUTE-BASED GRASPING", "content": "Object attributes are semantically meaningful features and serve as an intermediate representation for object recognition and manipulation. In this section, we propose an end-to-end neural network for attribute-based robotic grasping. The proposed model takes as input an image of visual observation and a text of target description to predict pixel-wise instance grasping affordances. To acquire a rich dataset for training, we build a simulation environment that allows domain randomization with diverse objects. In simulation, the model is pre-trained to learn instance grasping and object attributes simultaneously."}, {"title": "A. Learning Grasping Affordances", "content": "We formulate attribute-based grasping as a mapping from pairs of workspace images and query text to target grasping affordances. The proposed visual-textual manipulation architecture assumes no prior linguistic or perceptual knowledge. It consists of two modules, a multimodal encoder and an affordances decoder, as illustrated in Fig. 2.\nMultimodal Encoder: As shown in Fig. 1a, our robot system uses an overhead RGB-D camera to capture the workspace. The RGB-D image is projected into a 3-D point cloud and then orthographically back-projected in the gravity direction to construct a heightmap image Upre of RGB and depth channel. To specify an object in the image as the grasping target, we give a text command t composed of color and/or shape attributes, e.g., \u201cred cuboid\u201d. The workspace image Upre and query text t are the input to visual spatial encoder \u03c6u, spa and text encoder $t respectively. We use the ImageNet-pretrained [36] ResNet-18 [37] backbone as our image encoder \u03c6u, spa. We replace the first convolutional layer of the ResNet backbone with a 4-channel convolutional layer to match the RGB-D heightmap input. The encoder encodes the RGB and depth observation into 3D visual matrix \u03c6\u03c5, spa \u2208 RH\u00d7W\u00d7512. The text encoder $t is a deep averaging network [38] represented by three fully-connected layers and interleaved ReLU [39] activation functions. We first map each token in a sentence text to an embeddings vector of 128 dimension. The mean token embeddings (i.e., continuous bag-of-words [40] model) of the text are input to the 3-layer MLP text encoder to produce a text vector \u03c6t \u2208 R512. The visual matrix Ov, spa and the text vector \u03c6t are then fused by the gated-attention mechanism [41]: each element of \u03c6t is repeated and expanded to an H \u00d7 W matrix to match the dimension of \u03c6\u03c5, spa. The expanded matrix is multiplied element-wise with \u03c6\u03c5, spa to produce a fusion matrix Fatt. The gated-attention unit is designed to gate certain pixels in the visual feature matrix matching to the text vector, resulting in the fusion matrix containing the visual features selected by the query text. By this means, we can detect different attributes of the objects in the image, such as color and shape.\nAffordances Decoder: Grasping affordances decoder og is a fully-convolutional residual network [37], [42] interleaved with spatial bilinear 4\u00d7 upsampling and ended with the sigmoid function. The decoder takes as input the fusion matrix Fatt and outputs a unit-ranged map Qg with the same size and resolution as the input image Vpre. Each value of a pixel qi \u2208 Q9 represents the predicted score of target grasping success when executing a top-down grasp at the corresponding 3D location with a parallel-jaw gripper oriented horizontally concerning the map Qg. The grasping primitive is parameterized by a 3-D location and an angle. To examine different grasping angles, we rotate the input Fatt by N = 6 (multiples of 30\u00b0) orientations before feeding into the decoder, which predicts pixel-wise scores of horizontal grasps within the rotated heightmaps. The pixel with the highest score among all the N maps determines the parameters (i.e., location and angle) for the grasping primitive to be executed. As in Fig. 2, our model predicts accurate target grasping location and valid (e.g., the selected angles for the red cuboid) target grasping angle.\nThe motion loss Lgrasp, which supervises the entire encoder-decoder networks, is the error from predictions of grasping affordances:\n$L_{grasp} = \\frac{1}{N_s} \\sum_{i \\in M} (q_e - \\hat{q_e})^2 + \\lambda_M \\sum q_i^2$ (1)\nwhere Ns is the size of the dataset that is collected in simulation, qe is the grasping score in Qg at the executed location, and \u011fe is the ground-truth label (see Sec. IV-C). The second term ensures lower grasping scores for the pixels in background mask M (obtained from the depth image) with weight \u03bbM [43], and qi is the grasping score of a background pixel."}, {"title": "B. Learning Multimodal Attributes", "content": "To learn generic object attributes, we perform multimodal attributes learning, where visual or textual embedding vectors corresponding to similar attributes are encouraged to be closer in the latent space. Inspired by [13], we take advantage of the object persistence: the embedding difference of the scene before and after grasping is enforced closer to the representation of the grasped object. During data collection, we record image-text data (Upre, Upost, t), where Upre and post are the workspace image before and after grasping respectively, and t is the query text that describes attributes of the grasped object.\nWe add one layer of global average pooling (GAP) [44], [45] at the end of the encoder \u00d8v,spa and denote the network as visual vector encoder \u03c6u,vec. The output from \u03c6u,vec is a visual embedding vector that represents the average of scene features and has the same dimension of $t(t). We express the logic of the object persistence as an arithmetic constraint on visual and textual vectors such that (\u03c6\u03c5,vec (Upre) \u2013 \u03a6\u03c5, vec (Upost)) is close to $t(t). We use the triplet loss [46] to approximate the constraint, and the set of triplets T is defined as\nT = {(fi, fit, fi ) | s(asz, aft) > s (afi af)} (2)\nwhere fi, fit and fi are random samples from the pool of vectors (u,vec(Upre) \u2013 \u03a6\u03c5,vec(Upost)) and $t(t), and af is an n-dimensional attribute label vector corresponding to the feature vector f (e.g., color, shape, and category name, etc.). Function s(,) is an attribute similarity function that evaluates the similarity between two attribute label vectors:\n$s(a_1, a_2) = \\frac{1}{n} \\sum_{i=1}^{n} \\delta(a_1^i, a_2^i)$ (3)\n$\\delta(a_1, a_2) = \\begin{cases} 1 \\quad \\text{if } a_1 = a_2 \\neq 0 \\\\ 0 \\quad \\text{otherwise} \\end{cases}$ (4)\nwhere a denotes the i-th element of the label vector a, and the indicator function 1(,) evaluates the element-wise similarity. Note that 0 indicates null attribute meaning no attribute is specified in the label. As an example, suppose we have the dictionary dict = {\"eos\": 0, \"red\" : 1, \u201cblack\u201d : 2, \"yellow\": 3, \"cylinder\": 4, \u201ccube\u201d : 5}; then, \u201cred cylinder\u201d can be represented as a label vector afo = [1, 4], and \u201cred cube\u201d can be represented as af\u2081 = [1,5]. The similarity between the two label vectors is computed using (3) such that s(afo, af\u2081) = s([1,4], [1,5]) = 0.5. Additionally, when \u201cred\u201d and \u201cblack\u201d are used without any additional attribute description, they are mapped to the vectors [1,0] and [2,0], respectively. In this case, the similarity between the two labels is derived as s([1,0], [2,0]) = 0. With the triplets of embedding vectors, multimodal metric loss Lattr is defined as\n$L_{attr}(T) = \\sum_{i=1}^{|T|} \\max(||f_i - f_{i^+}||^2 - ||f_i - f_{i^-}||^2 + \\alpha, 0)$ (5)\nwhere \u03b1 is a hyperparameter that controls the margin between positive and negative pairs. By encoding workspace images and query text into a joint metric space and supervising the embeddings through the equation of object persistence (as shown in Fig. 3), we learn generic attributes that are consistent across object categories, as discussed in Sec. VII-A."}, {"title": "C. Data Collection and Training", "content": "To achieve self-supervision, we create a simulation environment in which objects are identified and grasped based on a description of semantic attributes. We collect training data in simulation with the following procedure, as summarized in Algorithm 1. Several objects are randomly dropped into the workspace in front of the robot. Given a workspace image and a query text, the robot learns to grasp a target under e-greedy exploration [47] (\u20ac = 0 during testing, i.e., an argmax policy). We save the workspace images, query text, background masks, executed actions, and results into a bounded buffer. The ground-truth labels are automatically generated for learning grasping affordances. The label \u011fe in (1) is assigned as the attribute similarity in (3) between the query text and the grasped object (0 if no object grasped). We also save the workspace image after a successful grasping"}, {"title": "V. DATA-EFFICIENT ADAPTATION", "content": "Due to the high cost of collecting data on real robots, we often choose to train robotic models in a simulator. However, the domain gap between the source domain (e.g., simulation, trained objects) and the target domain (e.g., the real world, novel objects) frequently leads to the failure of the learned models. We propose to, in addition to randomizing the source domain in Sec. IV-C, adapt our learned model using data from the target domain to further alleviate the domain shifts. One typical adaptation approach is fine-tuning the pre-trained model. However, the fine-tuning methods remain expensive in terms of data usage. In this section, as shown in Fig. 6, we propose two data-efficient adaptation methods: 1) adversarial adaptation, which adapts the image encoder using unlabeled images, and 2) one-grasp adaptation, which updates the end-to-end model using one grasp trial. The two adaptation methods can be either used independently or in combination for performance improvement."}, {"title": "A. Adversarial Adaptation", "content": "Despite that our generic model trained using the simulated basic objects shows good generalization (see Sec. VII-B), the visual feature shifts (e.g., objects, lighting conditions, and scene configurations, etc.) are inevitable. As a result, the image encoder is likely to produce out-of-distribution visual embeddings, leading to the failure of the grasping model. To reduce the influence of the domain shifts, we propose to use adversarial adaptation [28] to learn domain-invariant visual features that are transferable across different domains. In our problem setup, the simulated basic objects constitute the source domain, and our goal is to transfer the learned model to a target domain that is prone to domain shifts.\nAs shown in Fig. 6a, adversarial adaptation regularizes the weights of the image encoder or by enforcing a two-player game similar to the generative adversarial network (GAN) [49]. A domain classifier (i.e., discriminator) learns to distinguish between two domains, while the image encoder learns to fool the domain classifier by learning domain-invariant features. To achieve adversarial training, we connect the encoder and the discriminator via a gradient reversal layer (GRL) [50] that has reverse forward and back-propagation schemes. The GRL R is an identity mapping during forward-propagation but reverses the sign of the gradients during back-propagation:\nR(x) = x (7)\n$\\frac{dR}{dx} = -\\lambda_I I$ (8)\nwhere I is an identity matrix, and Ar is a positive constant."}, {"title": "B. One-Grasp Adaptation", "content": "By learning domain-invariant features, the adversarial adaptation technique in Sec. V-A improves model generalization using unlabeled images of the target domain. However, the adversarial loss uses unlabeled images to only update the image encoder and leave the text encoder and the grasping affordance decoder unadapted. When deploying in a new domain, end-to-end model fine-tuning is often necessary, but this comes at the cost of a large dataset covering all potential testing object configurations. To further adapt to novel objects and new scenes in a data-efficient manner, we present a one-grasp adaptation scheme (see Fig. 6b) that only requires one successful grasp of a novel object. The inductive bias of object attributes in Sec. IV-B is the key to adaptation in this limited-data regime. If similar objects are enforced closer in the embedding space, the adaptation distance for a novel object is likely to be shorter [51].\nThe proposed one-grasp adaptation method improves the model performance on a novel target object at the cost of only one grasp. The adaptation data is collected with the following one-grasp data augmentation (OneGraspAug) procedure. We place the object solely in the workspace and run the generic model to collect one successful grasp. The setting of a sole object facilitates grasping and avoids combinatorial object arrangements. Because convolutional neural networks are not rotation-invariant by design, we also augment the grasp data by rotating with various orientations to achieve rotation-invariance [52], [53], i.e., the ability to recognize and grasp an object regardless of its orientation. As shown in Fig. 8, we rotate the collected image and action execution to have rotated versions of the collected data.\nIn the adaptation stage, we add the category name of the object as an additional token to the query text, e.g., \u201capple, red sphere\" for the testing object apple. The token embedding of the object name is initialized properly to keep the embedding vector of the query text unchanged. The addition of the object name allows for a more specific grasping instruction and distinguishing from similar objects via adaptation. By optimizing over motion loss Lgrasp in (1), we jointly fine-tune the recognition and grasping of our model for unknown objects and scenes. As delineated in Sec. VII-C, the adapted model outputs higher affordances on the target objects that are not seen and grasped before."}, {"title": "VI. SYSTEM IMPLEMENTATION", "content": "We use CoppeliaSim [54] to build our simulation environ-ment. The simulation setup includes a UR5 robot arm and"}, {"title": "VII. EXPERIMENTS", "content": "We propose training with simulated basic objects first to have a generic model and then adapting it to novel objects and real-world scenes. In the experiments, we first analyze the structured metric space of our generic model and show the consistency between attention and grasping maps. Next, we evaluate the instance grasping performance of the generic model and show its modest generalization even before adaptation. Then, we adapt the model using the proposed adversarial and one-grasp adaptation methods and test the grasping models after adaptation. Finally, we run a series of ablation studies to investigate the two adaptation methods. The goals of the experiments are four-fold:\n1) to show the effectiveness of multimodal attribute learning for instance robotic grasping,\n2) to evaluate our attribute-based grasping system in both simulated and real-world settings,\n3) to evaluate the proposed adversarial and one-grasp adaptation methods, and\n4) to show the importance of the proposed data augmentation methods for grasping adaptation."}, {"title": "A. Multimodal Attention Analysis", "content": "By embedding workspace images and query text into a joint metric space, the multimodal encoder ( and t), supervised by metric loss Lattr and motion loss Lgrasp, learns attending to text-correlated visual features. We visualize what our model \"sees\" by computing the dot product of text vector t with each pixel of the visual matrix Ov,spa. This computation obtains an attention heatmap over the image, which refers to the similarity between the query text and each pixel's receptive field (see Fig. 9a). We quantitate the attention of our model and report its attention localization performance in Table I (see Ours-Attention). Evaluation metrics: An attention localization is considered correct only if the maximum value in the attention heatmap lies on the target object.\nOurs-Attention (in Table I) performs target localization at a 74.5% accuracy on simulated novel objects and a 70.2% accuracy on real-world objects, without any localization supervision provided. In summary, our multimodal embeddings demonstrate a consistent pattern across object categories and scenes. Though the localization results are not directly used for grasping, the consistent embeddings facilitate learning, generalization, and adaptation of our grasping model, as shown in Fig. 9 and discussed in the following subsections."}, {"title": "B. Generic Instance Grasping", "content": "We compare the instance grasping performance of our generic model with the following baselines:\n1) Indiscriminate is an indiscriminate grasping version of our approach and composed of a visual spatial encoder \u03a6\u03c5, spa and a grasping affordances decoder $g. We collect a dataset of binary indiscriminate grasping labels and train Indiscriminate using Lgrasp in (1).\n2) ClassIndis extends Indiscriminate with an attributes classifier that is trained to predict color and shape attributes on cropped object images. We filter the grasping maps from Indiscriminate using the mask of a target recognized by the classifier.\n3) EncoderIndis is similar to [21] and is another extension of Indiscriminate, which leverages a multimodal encoder (u,vec and $t in Sec. IV-A) for text template matching. The encoder is trained using Lattr in (5) to evaluate the similarity between each cropped object image and query text. During training, we also include attributes classification as an axillary task.\n4) AttrID is for an ablation study of our text encoder $t. The only difference between AttrID and the proposed method is that AttrID takes the attribute shape and color ID one-hot encoding as the system input, but our generic model uses Word2Vec continuous bag-of-words (CBOW) model to convert the texts into vector inputs. During training, we use both the motion loss and the metric loss to update the model.\n5) NoMetric is for an ablation study of multimodal metric loss. We simply remove the metric loss on the basis of our approach during its training.\nEvaluation metrics: These methods have different target recognition schemes: ClassIndis and EncoderIndis recognize a target by classification and text template matching respectively; NoMetric, AttrID and Ours are end-to-end. We report their target recognition performance (in addition to instance grasping performance, as in the next paragraph). A target localization is correct only if the predicted grasping location lies on the target object. The instance grasping success rate is # of successful grasps on correct target. In each testing scene,\ndefined as # of total grasps we only execute grasping once.\nWe evaluate the methods on both simulated basic (sim basic) and simulated novel (sim novel) objects in simulation, where there are 1200 test cases for the basic objects (Fig. 4) and 3400 test cases for the 34 novel objects (Fig. 5a, mostly from the YCB dataset [55]). We assume the objects are placed right-side up to be stable while their 4D pose (3D position and a yaw angle) can vary arbitrarily. For each testing object, we pre-choose a query text that best describes its color and/or shape. In each test case, four objects are randomly sampled and placed in the workspace, except avoiding any two objects with the same attributes. The robot is required to grasp the target queried by an attribute text. We report the results of target recognition in Table I and the results of instance grasping in Table II."}, {"title": "C. Adapted Instance Grasping", "content": "The generic model in Sec. VII-B infers the object closest to the query text as the target. Overall, our generic model demonstrates good generalization despite the gaps in the testing scenes. Specifically, these gaps are 1) RGB values of the testing objects deviate from training ranges, 2) some testing objects are multi-colored, 3) shape and size differences between the testing objects and the training objects, and 4) depth noises in the real world causing imperfect object shapes. To account for the gaps, we further adapt our generic model to increase instance recognition and grasping performance.\nWe first collect one successful grasp of a solely placed target object and then augment the collected data by rotating with additional N - 1 angles, as discussed in Sec. V-B and shown in Fig. 8. The compared methods that are adapted with the same adaptation data are as follows:\n1) ClassIndis updates its attributes classifier for a better recognition accuracy on the adaptation data.\n2) EncoderIndis minimizes the latent distance between cropped target images and query text to improve text template matching."}, {"title": "E. Ablative Analysis of Adaptation", "content": "The proposed adaptation approaches efficiently improve the instance grasping performance of our model. As discussed in Sec. VII-C, the finding that the approaches have complementary adaptation focuses leads to one of the major features: the two adaptation methods can be employed individually or in combination, depending on the availability of adaptation data. To investigate their independent and combinative performance, we conduct an ablation study to compare the grasping models as follows:\n1) Generic is the baseline generic model obtained in Sec. VII-B before any adaptation.\n2) Adversarial adapts the generic model to learn domain-invariant features using a large augmented data, as discussed in Sec. V-A.\n3) One-Grasp adapts the generic model using one grasping trial of the target object, as discussed in Sec. V-B.\n4) Adversarial+One-Grasp applies the two adaptation approaches on the generic model sequentially to improve the recognition and grasping.\nAs shown in Fig. 14, the adaptation methods are compared in four testing environments with an increasing extent of domain shifts: 1) sim basic jitter\u2014simulated basic objects with visual jitter (see Sec. VI-A) applied on color and depth channels as well as background, 2) sim novel-simulated novel objects, 3) sim novel jitter\u2014simulated novel objects with visual jitter, and 4) real novel-real novel objects. As the training environment uses simulated basic objects, the testing environments include the domain shifts caused by novel objects, visual jitter, and real-world noises.\nWe report the experimental results of the instance grasping success rate in Fig. 13. Overall, all adaptation methods improve the grasping performance across the testing environments. It is not surprising that adaptation is likely to be more effective (i.e., leading to more performance increments) if the domain shifts are severer. For example, the adaptation gain in the environment of sim basic jitter is less than 4%, while the real environment witnesses an adaptation gain of over 18%. Moreover, when encountering complex novel objects that are more challenging (e.g., drill) than the basic training objects (e.g., red cuboid), the One-Grasp method provides more adaptation power than the Adversarial method. In Adversarial adaptation, we use unlabeled data from the target domain to update the image encoder, while the text encoder and grasping decoder remain unadapted. As a result, the Adversarial adapted model is more prone to encountering difficulties with challenging novel objects (e.g., drill and spatula). On the other hand, One-Grasp adaptation adapts the entire model and demonstrates better performance on these challenging objects, but it requires additional labeled data (at least one grasp) of the objects. Another observation is that we can combine the two adaptation methods to achieve an even higher adaptation performance. Through various tests, the combined method Adversarial+One-Grasp adaptation consistently shows the best performance in all the testing environments. This suggests that the two adaptation methods adapt our model complementarily for accumulative"}, {"title": "F. Data Augmentation for Adaptation", "content": "The quality of adaptation data is critical for grasping adaptation. The two data augmentation methods, ObjectAug and OneGraspAug, are proposed for the two adaptation methods respectively. We execute the ablation studies in Table IV and Table V to examine the augmentation methods. The results of adapted instance grasping are presented in the tables. In the ablations for object-level augmentation, the compared approaches are\n1) Objects uses the raw data (images of single objects) as the adaptation data.\n2) ObjectOverlay overlays the randomly sampled objects on the background image to synthesize a large dataset covering possible object combinations and locations.\n3) ObjectAug is our object-level data augmetation method discussed in Sec. V-A, where much richer object configurations (i.e., orientations and scales) are covered in the synthesized dataset.\nFor the above augmentation methods, we keep the dataset size constant and use each augmentation data in (9), accordingly. Even though the data is unlabeled, the adaptation data quality has a direct impact on grasping performance, as seen in Table IV. The performance difference between Objects and ObjectAug, for example, is up to 4% on real novel objects, despite the fact that they nominally contain the identical objects. This finding demonstrates that the suggested object-level data augmentation successfully reduces domain shift by supplying rich unlabeled data.\nIn the ablations for one-grasp augmentation, the comparable approaches include\n1) OneGrasp uses the raw data of one successful grasp trial, including an RGB-D image and the corresponding grasping action.\n2) OneGraspRpt simply repeats the one-grasp data N times without rotating the data, where N = 6 is the angle discretion parameter.\n3) OneGraspAug is our one-grasp data augmetation method discussed in Sec. V-B, where we augment the one-grasp data by rotating for N orientations to enrich possible orientations of objects and robot grasping. As shown in Table V, OneGraspAug outperforms the compared methods by over 4% grasping success rate on real robot experiments, which demonstrates how angle-augmented data can be used to make the grasping model rotation-invariant for object recognition and grasping."}, {"title": "VIII. CONCLUSION", "content": "In this work, we presented a novel attribute-based robotic grasping system. An end-to-end architecture was proposed to learn object attributes and manipulation jointly. Workspace images and query text were encoded into a joint metric space, which was further supervised by object persistence before and after grasping. Our model was self-supervised in a simulation only using basic objects but showed good generalization. To further adapt to novel objects and real-world scenes, we proposed two data-efficient adaptation methods, adversarial adaptation and one-"}]}