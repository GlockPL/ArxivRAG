{"title": "DR-MPC: Deep Residual Model Predictive Control\nfor Real-world Social Navigation", "authors": ["James R. Han", "Hugues Thomas", "Jian Zhang", "Nicholas Rhinehart", "Timothy D. Barfoot"], "abstract": "Abstract-How can a robot safely navigate around people\nexhibiting complex motion patterns? Reinforcement Learning\n(RL) or Deep RL (DRL) in simulation holds some promise,\nalthough much prior work relies on simulators that fail to\nprecisely capture the nuances of real human motion. To address\nthis gap, we propose Deep Residual Model Predictive Control\n(DR-MPC), a method to enable robots to quickly and safely per-\nform DRL from real-world crowd navigation data. By blending\nMPC with model-free DRL, DR-MPC overcomes the traditional\nDRL challenges of large data requirements and unsafe initial\nbehavior. DR-MPC is initialized with MPC-based path tracking,\nand gradually learns to interact more effectively with humans.\nTo further accelerate learning, a safety component estimates\nwhen the robot encounters out-of-distribution states and guides\nit away from likely collisions. In simulation, we show that DR-\nMPC substantially outperforms prior work, including traditional\nDRL and residual DRL models. Real-world experiments show\nour approach successfully enables a robot to navigate a variety\nof crowded situations with few errors using less than 4 hours of\ntraining data (video: https://youtu.be/GUZIGBk60uY, code: upon\nacceptance).\nIndex Terms-Social HRI, Reinforcement Learning, Model\nPredictive Control, Real-world Robotics, Motion Planning.", "sections": [{"title": "I. INTRODUCTION", "content": "ACHIEVING reliable robotic navigation remains one of\nthe main challenges in integrating mobile robots into\neveryday environments. Applications such as food service,\nwarehouse logistics, and equipment transport could see sub-\nstantial cost and time savings but require robots to navigate\ndense human environments with static obstacles.\nSocial robot navigation research focuses on enabling robots\nto move safely and efficiently around humans. Deep Reinforce-\nment Learning (DRL) has emerged as a promising alterna-\ntive to traditional learning-free approaches including reactive,\ndecoupled, and coupled planning. Reactive methods model\nhumans as static objects [1], decoupled planning approaches\nforecast future human trajectories to generate a cost map for\nnavigation [2], and coupled planning approaches model human\nmotion and solve a joint optimization problem [3]. These\nmethods have notable shortcomings. Reactive approaches are\nshortsighted, resulting in intrusive behaviour [2]. Decoupled\napproaches neglect the robot's impact on humans, which can\ncause the robot to 'freeze' when the robot's plan conflicts with\nforecasted human trajectories [4]. Lastly, coupled methods\nfalter when the human model is inaccurate [3], and accurately\nmodeling human motion is challenging.\nDRL-based social navigation offers a compelling value\nproposition: learning efficient and safe robot behaviour without\nrelying on an accurate human behaviour model. Most DRL\nsocial navigation research centers around simulation due to\nthe dangers posed by randomly initialized DRL agents and the\nextensive data required for training [5]. However, these sim-\nulators contain human models that are inevitably mismatched\nwith the true dynamics of human motion. Thus, agents trained\nin these simulators are unfit for deployment. For instance,\nthe popular CrowdNav simulator assumes a cooperative and\ndeterministic human policy [6]: Optimal Reciprocal Collision\nAvoidance (ORCA) [7]. This assumption leads DRL agents\nto learn overly aggressive behaviours [8]. Additionally, many\nsimulators such as [6] neglect the presence of static obstacles,\nfurther exacerbating the sim-to-real gap.\nIn this paper, we train a DRL-based social navigation\nmodel directly in the real world. To handle environments with\nstatic obstacles, we modify the typical DRL social naviga-\ntion Markov Decision Process (MDP) into human avoidance\nwith path tracking within virtual corridors. We introduce a\nnovel approach, Deep Residual Model Predictive Control (DR-\nMPC), that integrates Model Predictive Control (MPC) path\ntracking to significantly accelerate the learning process. Lastly,\nwe design a pipeline with out-of-distribution (OOD) state\ndetection and a heuristic policy to guide the DRL agent\ninto higher-reward regions. Our approach enabled real-world"}, {"title": "II. RELATED WORKS", "content": "Several 2D and 3D simulators have been developed and\ncontinue to be developed for social navigation. These simula-\ntors offer a way to safely test and experiment with different\napproaches. Simulators also offer the possibility, in theory, of\ndeveloping 'sim-to-real' approaches, which can substantially\nreduce the amount of real-world learning (and mistakes) that\nrobots make. A popular simulator for social navigation is the\n2D CrowdNav simulator [6], designed for waypoint navigation\nin open spaces with the human policy defined by ORCA\n[7]. While CrowdNav has facilitated substantial DRL model\ndevelopment [8]-[11], the cooperative nature of the ORCA\npolicy creates a large sim-to-real gap. When the robot is\nvisible to humans, the learned DRL policy is dangerously\naggressive: the robot pushes humans out of its way to reach its\ngoal [8]. Consequently, the invisible testbed-when the robot\nis invisible to the humans was adopted as the benchmark\nstandard. Although the invisible testbed reduces the DRL\nagent's aggression, the problem becomes equivalent to decou-\npled planning where the DRL agent does not learn how its\naction will influence humans [8].\nLooking beyond CrowdNav, other simulators use alternative\nhuman motion models, such as variations of the Social Forces\nModel (SFM) or behaviour graphs [12]-[14]. Unfortunately,\neven the latest simulators struggle with human realism and of-\nten exhibit unsmooth human motions and enter deadlock sce-\nnarios. These deficiencies create a significant sim-to-real gap,\nwhere models trained in simulation often perform differently--\n-and usually worse when applied in the real world [15].\nWhile these simulators have been essential tools for advancing\nDRL model architectures, the most accurate data for DRL\nsocial navigation is real-world data. By reducing the amount\nof training data required, we enable the direct training of DR-\nMPC in the real world, avoiding unnecessary inductive biases\nintroduced by simulators."}, {"title": "B. RL Social Navigation Models", "content": "Crowd navigation, a major sub-field of DRL social naviga-\ntion, focuses on enabling a robot to navigate among humans\nin open areas. Significant progress has been made in incor-\nporating machine learning advancements to enhance model\narchitectures that enable reasoning about crowd dynamics in\nboth spatial and temporal dimensions.\nEarly models analyzed the scene by considering humans\nindividually [16], [17]. Then, approaches incorporated atten-\ntion mechanisms to reason about the crowd as a whole,\nusing learned attention scores to generate a crowd embedding\nfor decision making [6], [18], [19]. Subsequent advances\nanalyzed both human-robot and human-human interactions\nthrough graph neural networks (GNNs) [9], [10], [19]. Most\nrecently, models include temporal reasoning using spatio-\ntemporal graphs, multihead attention mechanisms, and trans-\nformers [8], [11], enabling reasoning on the trajectory level.\nThis paper does not focus on developing a novel architecture\nfor traditional waypoint-based crowd navigation. Instead, we\nleverage state-of-the-art (SOTA) architectures for processing\nhuman-related inputs and incorporate them into DR-MPC."}, {"title": "C. Residual DRL", "content": "Residual DRL integrates two policies together: one is user-\nsupplied, another is learned [20]. DRL learns corrective actions\non top of this base controller. The learning speedup occurs\nbecause on initialization, the DRL agent will have an expected\naction equal to the base controller, which, assuming a suitable\nuser-supplied controller, can result in much better performance\nthan a random policy [20].\nTo the best of our knowledge, residual DRL has not\nbeen applied to social navigation, but some related works\nhave attempted to incorporate classical control. For example,\nK\u00e4stner et al. [21] learn a DRL policy that switches between"}, {"title": "III. DECISION PROCESS FORMULATION", "content": "We modify the social navigation MDP in [6] to combine\nhuman avoidance and path tracking within virtual corridors,\nwhere these corridors ensure safety from static obstacles. To\nsimplify the problem, we assume a constant corridor width.\n1) State Space: We construct our state $S = {SPT, SHA}$,\nwhere $S_{PT}$ is the state information for path tracking and $S_{HA}$\nis the state information for human avoidance.\nAs in [8], we exclude human velocities due to the difficulty\nof estimating these quantities in the real world and assume\na constant human radius. If at time t there are $n_t$ visible\nhumans: $SHA = {v_{t-H:t-1}^1, r_{t-H:t}^1, q_{t-H:t}^1, ..., v_{t-H:t-1}^{n_t}, r_{t-H:t}^{n_t}, q_{t-H:t}^{n_t}}$,\nwhere $v_{t-H:t-1}$ is the robot's past velocities from time t - H\nto t\u22121, $r_{t-H:t}$ is the robot's past positions in the current robot\nframe from time t - H to t, and $q_{t-H:t} = [p_{t-H}...p_t]$ is\nthe positional trajectory of the ith visible human in the current\nrobot frame with a look-back of $H_i$, capped at length H.\nWe select a local path representation as in [24] to generalize\nto any path length. Given that $p_c$ is the closest localized node\nto the robot, $SPT = [p_{c-L}...p_c...p_{c+F}]$, where L is the\nlook-back and F is the look-forward parameter. These path\nnodes are also transformed into the current robot frame.\n2) Actions: Our DRL agent interacts at the level of velocity.\nOur action space is a linear and angular velocity: $a = [\u03c5 \u03c9]$.\n3) Rewards: Our reward function considers path advance-\nment, path deviation, goal reaching, corridor collisions, small\nspeeds, human collisions, and human disturbance:\n$r = r_{pa}^* + r_{dev}^* + r_{goal}^* + r_{cor-col}^*+r_{act}^* + r_{hum-col}^* + r_{dist}$, (1)\nwhere rewards marked with an asterisk are terminal rewards;\non all termination conditions, the robot stops moving. All\nthe following $\\kappa$ are tuning parameters where $\\kappa > 0$. Path\nadvancement: $r_{pa} = \\kappa_{pa}( - Si)$ where si and si are the\narclength positions along the path at S and S'. Deviation:\n$r_{dev} = \\kappa_{dev}dxy + \\kappa_{dev}|do|$ where dxy is the Euclidean distance\nand do is the angular offset from the closest point on the path.\nGoal: $r_{goal} = -\\kappa_{goal}$ if the heading difference exceeds a thresh-\nold and 0 otherwise. Corridor collision: $r_{cor-col}^* = -\\kappa_{cor-col}$ for\ncolliding with the corridor. Minimal actuation: $r_{act}^* = -\\kappa_{act}$ if\nthe sum of the robot's past H speeds falls below a threshold.\nFor human avoidance, we align our rewards with the\ntwo most important principles of human-robot interaction\n(HRI): safety and comfort [25]. For safety, human collision:\n$r_{hum-col}^* = -\\kappa_{hum-col}$. For comfort, disturbance penalty: $r_{dist} =$\n$\\Sigma_{i=1}^{n_t} (\\kappa_{\u03bd} \\Delta \u03bd + |\\Delta \u03b8|)$, which penalizes the robot for\ncausing changes in a human's velocity (A) and heading\ndirection ($\\Delta \u03b8$). These changes are computed relative to the\nhuman's velocity and heading at the previous time step.\nFinally, we add two safety layers: safety-human and safety-\ncorridor raises, which are conservative versions of the human-\ncollision and corridor-collision penalties. While a safety viola-\ntion does not guarantee a collision, it is likely. So, we slightly\nreduce the theoretical performance limit for safety."}, {"title": "IV. DR-MPC POLICY ARCHITECTURE", "content": "Our DR-MPC policy architecture (Figure 3) consists of two\nmain components: individual processing of SPT and SHA, and\nthe information fusion to generate a single action.\nFor processing Spr, we employ a Multi-layer Perceptron\n(MLP) to generate a latent embedding of the path information.\nAlso, an MPC optimization guided by [23] generates aMPC.\nTo process SHA, we modify [10] to handle varying-\nlength human trajectories for off-policy learning. We refer to\nthis adapted architecture as the Human Avoidance Network\n(HAN); further details are provided in the appendix. HAN is\na modular component of our pipeline and can be replaced with\nother crowd navigation work.\nThe output of HAN is a 'crowd embedding' that is used to\ngenerate six candidate human avoidance actions (aHA), where\neach aha is positioned in a different cell of the action space.\nThe human-avoidance action space is partitioned into a grid\nof six cells defined by two linear velocity bins [Umin, Umiddle],\n[Umiddle, Vmax], and three angular velocity bins [Wmin, Wlower],\n[Wlower, Wupper], [Wupper, Wmax]. The inclusion of multiple actions\nis motivated by the fact that, in many cases, several viable\nactions may exist for human avoidance. We found empirically\nthat this design reduces learning time compared to having the\nmodel learn the diversity. The output of the MLP following\nHAN is the mean action within each cell. Using a predeter-\nmined variance that decays over time, we sample a Gaussian\nto get aha; this standard formulation comes from [26].\nThe key innovation of our model lies in how we combine\nthese individual components. The path tracking embedding,\nthe crowd embedding, aMPC, and aha are combined to output\n$\\overline{a} = [\\overline{a}_1...\\overline{a}_6]$, where each $\\overline{a}_i \\in [0,1]$, and a\ncategorical distribution $p = [p^1 . . . p^6]$. Note, as in Soft Actor-\nCritic (SAC), the model learns the mean and log-standard\ndeviation to generate $\\overline{a}$, which is then passed through a tanh\nfunction to squash it, followed by scaling and shifting [27].\nWe then compute $a_i^\\flat = clip(\\overline{a}_i + tanh(\u03b2), 0, 1)$, where \u03b2 is\na learned parameter. Each aHa corresponds to $a_i^\\flat$ and $p^i$ of the\nsame index. After sampling the index j from p, the final action\nis constructed as a weighted sum: $a = a_j^\\flat a_{HA}^j + (1 -a_j^\\flat)a_{MPC}$.\nUnlike residual DRL, which follows the base controller in\nexpectation, DR-MPC begins with near-MPC behaviour by\nbiasing actions toward MPC, initializing \u03b2 = -0.8 to suppress\n$a^\\flat$. Note that initializing \u03b2 too low yields little qualitative\ndifference and requires a lot of model updates to raise \u03b2\ntowards 0. We observe the model naturally learns to adjust\n\u03b2 towards 0 to be able to take non-MPC actions.\nAlthough DR-MPC's effective action space after fusion is\nsmaller than regular DRL, it contains the best actions for\npath tracking and human avoidance. While DRL with the full\naction space might theoretically achieve higher performance\ngiven infinite training, it faces practical challenges in balancing\nexploration and exploitation. Despite having a reduced action\nspace, we can still achieve a high-performing policy but with\nsignificantly faster learning times.\nThe DRL algorithm we employ to train our model is Dis-\ncrete Soft Actor-Critic (DSAC) with double average clipped Q-\nlearning with Q-clip to properly backpropagate through p [28]."}, {"title": "V. THE PIPELINE", "content": "The exploration-exploitation problem in DRL does not have\nto be addressed solely by the actor network. To perform\nbetter than random exploration, we guide the selection of\naha using heuristics to find promising directions for human\navoidance. The sooner these valuable experiences are stored\nin the replay buffer, the sooner the critic can correct these\nQ-values. This concept of early intervention in uncertain or\nhigh-risk states has proven effective in interactive imitation\nlearning and sample-efficient DRL [29], [30].\nTo implement this, we perform OOD state detection to\nassess the DRL model's familiarity with a state. If the state is\nin-distribution, we execute DR-MPC's action. Otherwise, we\neither validate the model's action as safe using a heuristic\nsafety check or override it using a heuristic policy. This\nprocess steers the DRL agent toward collision-free areas.\nOnce a state is in-distribution, the DRL agent will continue\nto explore and may still encounter collisions. The agent will\nquickly learn to focus on known good actions and avoid poor\nreward regions, thereby accelerating learning speed. Note that\nthe OOD state detection, heuristic safety check, and heuristic\npolicy are modular components and can be swapped out or\nfine-tuned independently of everything else in the pipeline.\n1) OOD State Detection: There are numerous methods for\nOOD state detection: we select a K-nearest neighbours (KNN)\nalgorithm in the latent space of the model [31]. This method\nis relatively interpretable and can be implemented in real-time\nusing Facebook's Faiss package [32]. We extract the latent\nembedding from our model at the layer immediately following\nthe fusion of path tracking and human avoidance information.\n2) Heuristic Safety Check and Policy: Upon detecting an\nOOD state, we assess whether the DRL model's proposed\naction is safe. We assume a constant velocity motion model\n(CVMM) for both the human and the robot. We roll out the\nproposed action for the robot and the estimated human velocity\nbased on the last two timesteps for a predefined number\nof timesteps. We then evaluate if the safety-human raise is\ntriggered at any point during this rollout; if it is, the action\nis deemed unsafe. For the safety-corridor condition, we only\ncheck the one-timestep rollout result.\nIf the proposed DRL action is both OOD and fails the\nCVMM safety check, we evaluate a predefined set of alterna-\ntive actions for safety. We select the safe action that is closest\nto the MPC action, thereby guiding our DRL agent into regions\nthat are both safe and conducive to path advancement."}, {"title": "A. Soft Resets", "content": "In real-world applications, episode resets are inefficient.\nThus, unlike traditional episodic RL, we avoid 'hard resets'"}, {"title": "VI. SIMULATION EXPERIMENTS", "content": "We augment the CrowdNav simulator from [10], originally\ndeveloped by [6]. In a modularized manner, we replace\nwaypoint navigation with path tracking. Our training schema\ncycles through four scenarios, as illustrated in Figure 5. Once\na path is completed, the next one begins. We run our simulator\nwith nt = 6; these 6 humans are modeled using ORCA and\ncontinuously move to random goals in the arena. Our robot\nhas an action space of v \u2208 [0,1] and w \u2208 [-1,1]. The MDP\nhas a timestep of 0.25s, and we train our models with a limited\namount of data: 37,500 steps-around 2.5 hours of data."}, {"title": "B. Model Baselines", "content": "We evaluate four models. The first applies the model of [10],\nwhich shares a similar backbone to DR-MPC. The DRL model\ncombines the 'PT Embedding' and 'HA Embedding' with an\nMLP to generate the agent's action; we refer to this model as\nthe na\u00efve DRL model. The second model is a residual DRL\nmodel guided by [20], which outputs a corrective action: the\naction executed in the environment is the sum of the MPC and\nDRL corrective action, which is then truncated into the feasible\naction space. Since path tracking and human avoidance may\nrequire distinct actions, we set the DRL corrective action to\nhave v \u2208 [-1,1] and w \u2208 [\u22122, 2], ensuring that the final action\ncan cover the entire action space. The residual DRL model\narchitecture is the same as the na\u00efve DRL model but also has\naMPC inputted into the final MLP. Finally, we evaluate two\nversions of DR-MPC: one without the OOD state detection\nand CVMM modules, and one with them."}, {"title": "C. Results and Discussion", "content": "The main results are depicted in Figure 4. The plotted lines\nare the mean values across 12 trials, while the shaded areas\nindicate the variance observed during training. For all metrics,\nthe values in these plots are averaged over 500 environment\nsteps. We do not use the reward-per-episode metric because\ndifferent path lengths inherently yield different maximum\ncumulative rewards, which would skew comparisons.\nFrom the cumulative reward plot, it is clear that both of our\nDR-MPC models significantly outperform the na\u00efve DRL and\nresidual DRL models. While all models can learn a high path-\nadvancement reward and navigate paths effectively, interacting\nwith humans remains a greater challenge. DR-MPC, however,\nexcels in task switching and optimizing both path tracking\nand human avoidance using the a parameter. It is important\nto note that the DRL and residual DRL models were verified\nfor correctness and, given sufficient training, could eventually\nreach similar reward levels as DR-MPC.\nAs designed, both DR-MPC models start with a high base\nreward due to their initialization with near-MPC path tracking\nbehaviour, which is better than the residual DRL model, which\nfollows the MPC action only in expectation, resulting in slower\nconvergence and lower initial rewards."}, {"title": "VII. HARDWARE EXPERIMENTS", "content": "1) Robot: We use a Clearpath Robotics Jackal equipped\nwith an Ouster OSO-128 LiDAR. The OSO sensor has a 90\u00b0\nvertical field-of-view (FOV) and a 360\u00b0 horizontal FOV, cap-\nturing range and reflectivity data. This data can be represented\nas equirectangular images-an example of the reflectivity\nimage is shown in Figure 2. These images are comparable to\nlow-resolution cameras, enabling the application of computer-\nvision models. By fusing the 2D computer-vision results with\nthe LiDAR's depth information, we can produce 3D outputs.\n2) Mapping, Localization, Path Tracking: We use the open-\nsource Teach and Repeat (T&R) codebase [34]. Specifically, in\nLiDAR T&R, after manually driving the robot through a new\nenvironment once, the robot can subsequently localize itself\nto this previously driven path and track it using MPC. T&R\nenables rapid deployment of our pipeline in new environments.\n3) Human Tracking: We detect humans by running a pre-\ntrained YOLOX model from mmDetection on the reflectivity\nimage and its 180\u00b0 shifted version to account for humans at\nthe wrap-around point [35]. By combining depth information\nfrom the range image with the localization result from T&R,\nwe recover the 3D world positions of humans in the scene and\ntrack them using the Norfair package [36]."}, {"title": "B. Experimental Setup", "content": "During training, between zero and four humans interact with\nthe robot. The MDP timestep is 0.2s, aligned with incoming\nLiDAR data every 0.1s. Inference is performed on a ThinkPad\nP16 Gen 1 with an Intel i7-12800HX Processor and NVIDIA\nRTX A4500 GPU, while real-time training occurs on another\ncomputer. Inference model weights are periodically updated.\nAlthough we trained on 3.75 hours of experience transitions,\nthe entire process took about 15 hours. After every 500\nexperience tuples, we paused data collection to allow the\nmodel to update, performing twice as many training updates as\nenvironment samples. While off-policy learning can leverage\nolder data, keeping the inference model synchronized with the\ntraining model allows Q-values to be corrected as state-action\ndistributions shift. Additionally, outliers caused by processing\ndelays from resource allocation variance were filtered to\nreduce data noise. Also, soft resets, along with charging the\nlaptop and robot batteries, contributed to the additional time.\nWe periodically evaluate the model on fixed but diverse\nscenarios (Figure 8). The scenario on the left involves a\nstationary human, a human walking toward the robot, free\ndriving, and a crowd crossing. The loop on the right tests\nthe robot's weaving ability between two stationary humans,\nfollowed by free driving, a human crossing the robot's path,\nand a differently configured crowd crossing. Each model\nundergoes 6 runs-3 per scenario.\nWe benchmark real-world DR-MPC against three models:\n(1) DR-MPC trained in simulation without modification ('Sim\nModel Raw'), (2) DR-MPC trained in simulation with real-\nworld adjustments, such as observation delays, acceleration\nconstraints, and distance-based detection limits ('Sim Model\nAdjusted'), and (3) a heuristic policy, which executes aMPC if\nit passes the CVMM safety check and switches to the heuristic\npolicy used to guide the DR-MPC model if unsafe.\nThe real world introduces challenges absent in simulation,\nsuch as perception errors (missed detections, false positives,\nmisidentifications) and variable timing delays from tasks (lo-\ncalization, human detection, action generation). Also, human\nmovements are probabilistic, and the robot faces acceleration\nconstraints based on battery charge. These challenges highlight\na key advantage of DRL: it maximizes expected cumulative\nreward, allowing effective learning in noisy environments."}, {"title": "C. Results and Discussion", "content": "'Sim Model Raw' performs poorly, oversteering and collid-\ning with virtual boundaries due to unaccounted state delays.\nAdjusting the simulator ('Sim Model Adjusted') improves\nperformance but still falls short of DR-MPC trained in the real\nworld. This underscores the advantage of real-world training\nin handling noisy data and developing robust policies."}, {"title": "VIII. CONCLUSION", "content": "We introduced DR-MPC, a novel integration of MPC path\ntracking with DRL, and demonstrated its effectiveness and\nsuperiority to prior work in both simulation and real-world\nscenarios. Training a DRL agent directly in the real world by-\npasses the sim-to-real gap, addressing the inevitable mismatch\nbetween the dynamics of modeled humans and real humans.\nWhile simulation is crucial for model development, the ulti-\nmate goal is deploying DRL agents that perform effectively in\nreal-world conditions, where a plethora of challenges remain."}, {"title": "APPENDIX A\nHUMAN AVOIDANCE NETWORK", "content": "Compared to [10], we use the humans' past trajectories\nrather than their forecasted trajectories. This way, DRL di-\nrectly learns from the sensor noise embedded in the state.\nFor each human trajectory $q_{t-H:t} = [p_{t-H}...p_t]$, we\nsequentially process it from time t - Hi to t using a GRU\nto generate an embedded trajectory $e_{traj}^{traj}$. This step handles\nhuman trajectories of varying lengths, embedding them into a\nuniform latent space. Next, with $E_{traj} = [e_{traj}^1...e_{traj}^{n_t}]$, we use\nthree MLPs to generate the queries $Q_{traj}$, keys $K_{traj}$, and values\n$V_{traj}$. We then apply multi-head attention using the scaled dot-\nproduct attention: $MultiHead(Q_{traj}, K_{traj}, V_{traj})$ (see [37]). The\noutput of this module is $E_{HH}$, a $n_t \\times d_{HH}$ tensor, where $d_{HH}$\nis the dimension of the human-human embeddings.\nNext, we process the robot trajectory $r_{t-H:t}$ by embedding\nit with an MLP into $e_{robot}^{robot}$. We then compute the robot-human\nattention. Here, the keys $K_{robot}$ are generated from $e_{robot}^{robot}$ and\ntraj\nthe queries $Q_{HH}$ and the values $V_{HH}$ from $E_{HH}$. The result of\nthis multi-head attention network is the embedding $E_{RH}$.\nFinally, we concatenate $e_{RH}$ with $v_{t-H:t-1}$ and pass this\ntensor through one last MLP to obtain the crowd embedding\n$e_{HA}$, which is then used to generate the 6 mean actions for\nhuman avoidance."}]}