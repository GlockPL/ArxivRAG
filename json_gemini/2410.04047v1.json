{"title": "BEYOND FORECASTING: COMPOSITIONAL TIME SERIES REASONING FOR END-TO-END TASK EXECUTION", "authors": ["Wen Ye", "Yizhou Zhang", "Wei Yang", "Lumingyuan Tang", "Defu Cao", "Jie Cai", "Yan Liu"], "abstract": "In recent decades, there have been substantial advances in time series models and benchmarks across various individual tasks, such as time series forecasting, classification, and anomaly detection. Meanwhile, compositional reasoning in time series prevalent in real-world applications (e.g., decision-making and compositional question answering) is in great demand. Unlike simple tasks that primarily focus on predictive accuracy, compositional reasoning emphasizes the synthesis of diverse information from both time series data and various domain knowledge, making it distinct and extremely more challenging. In this paper, we introduce Compositional Time Series Reasoning, a new task of handling intricate multistep reasoning tasks from time series data. Specifically, this new task focuses on various question instances requiring structural and compositional reasoning abilities on time series data, such as decision-making and compositional question answering. As an initial attempt to tackle this novel task, we developed TS-Reasoner, a program-aided approach that utilizes large language model (LLM) to decompose a complex task into steps of programs that leverage existing time series models and numerical subroutines. Unlike existing reasoning work which only calls off-the-shelf modules, TS-Reasoner allows for the creation of custom modules and provides greater flexibility to incorporate domain knowledge as well as user-specified constraints. We demonstrate the effectiveness of our method through a comprehensive set of experiments. These promising results indicate potential opportunities in the new task of time series reasoning and highlight the need for further research.", "sections": [{"title": "1 INTRODUCTION", "content": "Over the past few decades, research in time series analysis has heavily focused on improving the performance of individual tasks such as time series forecasting, anomaly detection, and time series classification (De Gooijer & Hyndman, 2006; Kirchg\u00e4ssner et al., 2012; Zong et al., 2018; Dau et al., 2019; Hamilton, 2020; Jin et al., 2024). These results have benefited various areas such as risk assessment in finance, disease diagnose in healthcare, pandemic modeling in public health and event detection in natural and social science (Tsay, 2005; Cao et al., 2022; 2023c; Kamra et al., 2021; Team & Murray, 2020; Penfold & Zhang, 2013; Cheng et al., 2021; Sharma et al., 2021; Zhang et al., 2021).\nHowever, most real-world applications demand multi-step reasoning, where well-established tasks should serve as intermediate steps. A typical example is forecasting future energy supply (Zheng et al., 2022), in which scientists must integrate domain knowledge with statistical analysis. The process begins with the examination of time series data to forecast future signals with statistical methods, following by formulating constraints based on domain expertise to refine predictions. Another typical example is analyzing climate time series (Mudelsee, 2010), where it is not only necessary to predict what happens next but also crucial for experts to comprehend the fundamental physical laws governing such data. For instance, a climate scientist studying the impact of greenhouse gas"}, {"title": "2 RELEVANT WORKS", "content": "Time Series Analysis Tasks and Models Classical time series analysis encompasses several key tasks that leverage patterns and trends in data over time. These tasks include forecasting, imputation (filling in missing data points to create a complete dataset), classification (categorizing time series data into predefined classes), and anomaly detection (identifying unusual patterns or outliers that deviate from expected behavior). Each of these tasks serves unique purposes across various application domains, highlighting the significance of time series analysis. However, recent researches reveal that it is extremely difficult to evaluate the performance of time series models on one specific task with a unified metrics for all applications since the results could affect the final outcomes of multi-step reasoning tasks in a different way from the metrics, such MAPE, RMSE and so on. This challenge motivates the exploration on end-to-end execution of multi-step time series tasks.\nTo tackle time series analysis, researchers have made significant contribution over the years. Early methods were mainly task-specific, where each individual task-such as forecasting, imputation, classification, or anomaly detection-was addressed by a dedicated model optimized for its specific purpose, resulting in a fragmented approach. Recently, inspired by the emergence of large language models, there has been a shift toward general-purpose Large Time Series Models. Notable contributions include work by (Gruver et al., 2024), who simply encoded time series as strings, and (Jin et al., 2023), who converted time series into language representations through alignment. (Cao et al., 2023b) and (Pan et al., 2024) incorporated decomposition techniques and prompt design, enabling generalization to unseen data and multimodal scenarios. (Zhou et al., 2023) adapted GPT-2 as a general-purpose time series analysis model, extending it to various tasks. Additionally, (Talukder et al., 2024) utilized VQVAE as a tokenizer for transformers, while (Ansari et al., 2024) employed scaling and quantization techniques for embedding time series. These models are designed to handle multiple preset tasks and are jointly pre-trained on diverse datasets. However they still operate under predefined task modes, which limits their ability to perform complex and compositional reasoning. As a result, while they can handle multiple tasks, they lack the flexibility to adapt to more intricate scenarios that require a deeper understanding on task instructions and composition of different tasks or concepts.\nComplex and Compositional Reasoning with Pre-trained Foundation Models Large Language Models (LLMs) have demonstrated significant capabilities in managing complex reasoning tasks by emulating human cognitive processes, especially when incorporated with appropriate in-context samples (Huang & Chang, 2022; Qiao et al., 2022; Ahn et al., 2024; Qu et al., 2024). The Chain of Thought (CoT) prompting method (Wei et al., 2022) is a prime example, encouraging models to articulate intermediate reasoning steps (i.e. rationales) before reaching a conclusion. This method improves performance in multi-step logical deductions by transparently demonstrating the thought"}, {"title": "3 TASK DEFINITION", "content": "In this section, we first define compositional time series reasoning, which involves the synthesis of information from time series data in conjunction with task-specific instructions and contextual external knowledge.\nDefinition 3.1 (Compositional Time Series Reasoning). Let x denote a time series, which is a sequence of data points indexed in time order. Let C represent the context, which encompasses the task instruction and additional external information. The primary objective of time series reasoning is to derive a set of rationales R = (r1,r2,...,rn) that are conditional on the inputs x and C in an step-by-step manner. Each ri addresses a single sub-task related to the time series, e.g. r1 tackle missing value imputation, r2 tackle forecasting, and r3 tackle numerical reasoning and optimization. Then we can generate the final answer y to the task based on both rationales and the input, mathematically expressed as:\nri+1 = f(r1, ..., ri, x, C)\n\u21d2\ny = g(R, x, C) = g(r1, ..., rn, x, C)\nHere, g is a function that maps the generated rationales R along with the inputs x and C to the final response y, and g. The rationales R serve as intermediary results or conclusions that facilitate the reasoning process and may exhibit various probabilistic structural dependencies. The most common one is the sequential dependency, on which we define Chain of Thought (CoT)(Feng et al., 2023):\nri ~ P\u03b8(ri|r1, r2, ..., ri\u22121, x, C)\nwhere p\u03b8 is a Large Language Model (LLM), and every rationale is generated fully by the LLM based on auto-regressive decoding. In this paper, our model will apply an alternative paradigm,"}, {"title": "4 DATASET AND TASKS", "content": "Our dataset\u00b2 is primarily built for three major categories of tasks: decision making on financial data, compositional question answering about finance market and energy usage, and causal mining on synthetic dataset (Denis et al., 2003; Gonzalez-Vidal et al., 2019; Cao et al., 2023a). Among these tasks, decision making present unique challenges for evaluation, as they cannot be easily assessed through simple comparisons with ground truth answers like traditional question answering tasks. To address this, we innovatively proposed an instruction/program pool to abstract the evaluation process for these tasks, shown in Fig. 2. Specifically, we designed a set of appropriate instructions for each application domain, in which each instruction is paired with an evaluation configuration that outlines the criteria for success. Given a response, our unified evaluation program (shown in Algorithm 1) determines whether the answer is successful based on the evaluation configuration, reporting both the success rate and end task specific performance metrics. For instance, in the context of financial decision making, we assess whether the given decision is compliant with the given budget and evaluate the corresponding outcomes such as the total profit.\nThis comprehensive approach allows us to effectively evaluate performance across diverse tasks that extend beyond conventional time series analysis, providing a clear picture of both success rates and overall effectiveness."}, {"title": "4.1 DECISION MAKING", "content": "In our decision-making task, we focus on investment portfolio decisions within the financial market, which requires the ability to synthesize information from multiple areas such as trend recognition, risk assessment, and numerical optimization based on human expertise (Bonaparte et al., 2014). For each test sample, historical stock prices of interest are provided alongside immediate future data. The historical data includes natural language questions articulating investment goals such as maximizing profit\u2014as well as constraints, including budget limitations, expected profit ratios, and acceptable loss ratios. The question is generated according to the following template:"}, {"title": "4.2 COMPOSITIONAL QUESTION ANSWERING", "content": "In our compositional question answering task, we primarily focus on financial markets and load-related issues in the energy sector. Specifically, each test sample provides the model with a natural language question and relevant time series historical data, such as stock prices and energy supply data. The questions are generated by the following templates:"}, {"title": "4.3 CAUSAL MINING", "content": "For causal mining, we synthesize a set of data grounded in domain knowledge related to climate science, finance, and economics. Specifically, we generate a series of multivariate time series data based on established causal relationships and meteorological principles. Each test sample consists of a time series dataset of various variables, accompanied by a natural language instruction that asks the model to uncover the causal dependencies between the given time series. For details on data generation process, please refer to section C.4. The reasoning model must infer dependencies based on the given data and instructions. The evaluation framework then measures the model's performance by comparing its inferred causal relationships with the ground truth. The questions are generated by the following template:"}, {"title": "5 PROGRAM-BASED TIME SERIES REASONING", "content": "When handling time series data, methods that rely solely on large language models (LLMs) for reasoning, such as the Chain-of-Thought (CoT) approach, often struggle with understanding numerical information (Zhang et al., 2024). These models, while powerful in generating logical inferences, are prone to making errors in calculations or failing to adhere to numerical constraints that are crucial in tasks involving time series analysis. These shortcomings underscore the necessity for programmatic assistance in reasoning processes. To mitigate such errors, we propose a framework that supplements LLM-based reasoning with program-based decomposition and leverages the in-context learning ability of LLMs.\nTask Decomposer The proposed framework handles time series data by integrating program-based decomposition and task-specific models. As illustrated in Fig.3, the core idea revolves around a programmatic task decomposition engine, which we refer to as the \"Problem Decomposer\u201d. This component is responsible for disassembling complex tasks into a series of smaller, manageable subtasks, each described in a programmatic manner. These subtasks are subsequently addressed by distinct processing modules, enabling the framework to provide robust, step-wise solutions to time series-related problems. In TS-Reasoner, We use ChatGPT-3.5-turbo as our task decomposer. We leverage the in-context learning ability of pretrained language model and construct question-program pairs as in-context examples. The in context examples are carefully constructed so that the samples questions are equally distributed across the four question types (Financial Investment Strategy, Future Stock Characteristic Prediction, Energy Load Perdiction with known knowledge, Causal Relation). As shown in Fig. 3, every in-context sample is a question program pair where the question is described in natural language and the program is pseudo-code like. Please refer to section C.1 for prompts given to task decomposer.\nThe decomposition of tasks allows for targeted processing through three types of modules, each specialized for different aspects of the reasoning process:\nTime Series Model Modules: These modules are grounded in foundation time series models and are primarily responsible for handling standard operations such as forecasting, anomaly detection, trend analysis, and other predictive or diagnostic tasks. Their purpose is to leverage established models in the field to process data-driven subtasks with high precision and efficiency.\nNumerical Method Modules: A second class of modules focuses on numerical and statistical methods. These modules are particularly adept at performing quantitative manipulations on the data, such as extracting trends, computing ranges, and conducting basic arithmetic or statistical analyses. The application of numerical techniques allows for a clearer interpretation of time series dynamics, particularly in tasks where precise quantitative reasoning is required.\nCustom Module Generation via Large Language Models (LLMs): The third type of module addresses a significant challenge in time series reasoning: the handling of external knowledge and"}, {"title": "6 EXPERIMENTS", "content": "In this section, we conducted a series of comparative experiments to assess the performance of various models across our defined tasks of decision making, compositional question answering, and multi-domain causal mining. Our baseline models included the Chain of Thought (CoT) approach and CoT + code approach. For the most competitive result, we used ChatGPT-4-turbo. In CoT prompting, we outline the steps for the model to think about and directly return result. In CoT + code setting, we provide CoT prompts that outline steps to take and additionally allows the model the generate code that we execute to obtain result. Fore more details on the prompts, please refer to section C.3. Through these experiments, we aimed to measure not only the performance of the outputs but also the models' ability to adhere to constraints and optimize outcomes within the complex frameworks of financial markets and energy usage. By systematically analyzing the strengths and weaknesses of each approach, we seek to elucidate the most effective strategies for leveraging large language models in practical decision-making, compositional question answering and causal inference tasks."}, {"title": "6.1 DECISION MAKING", "content": "Evaluation Protocol In decision making, the overall objective and specific user requirements may be different. For this reason, we respectively report the performance of models on each kind of instances. The user's main objective is to maximize the total profit/ minimize the loss. The customized requirements can be generally divided to: Profit Percent Guarantee (the decision needs to guarantee the minimum profit percent that the user expected), Risk Tolerance (the volatility of the investment portfolio must be within an expected range), and Budget Allocation (control the budget for a specific stock). In evaluation, we focus on two types of metrics: success rate (SR), absolute average profit (AAP) and relative average profit (RAP). The strict success rate is defined as the percentage of test samples that did not violate any constraint and requirements. The average absolute profit is the profit that the model made on all successful instances. The relative average profit is defined as the relative profit gain over the vanilla investment strategy that do not consider the requirements in the instructions. In Profit Percent and Budget Allocation task, we aim at improving the profit over the vanilla strategy. In Risk Tolerance, the model is required to first ensure the risk and minimize the profit reduction over the vanilla strategy."}, {"title": "6.2 COMPOSITIONAL QUESTION ANSWERING", "content": "Evaluation Protocol In compositional QA, the various questions may lead to different reasoning steps. For this reason, we respectively report the performance of models on each kind of instances. Specifically, we applied the data from finance (stock price) and energy power supply. For finance, we mainly tackle the prediction on price, volatility, which are relatively simple. For energy supply, we consider the energy power supply forecast with external requirement attached such as the max and min load regularization for the system or load ramp rate and variability limit. Given such requirements, TS-Reasoner needs to additionally refine the results based on the specified external knowledge. In evaluation, we focus on two types of metrics: success rate (SR) and Mean Absolute Percentage Error (MAPE). The success rate is defined as the percentage of test samples in which the model successfully execute the tasks and did not violate any constraint in the instructions (e.g. Energy Power load constraint/Requirement).\nOverall Performance Table 2 shows the performance of TS-Reasoner and baseline reasoning approaches. It is evident that TS-Reasoner generally outperforms CoT and CoT + code in terms of both success rate (SR) and MAPE. We can observe that as reasoning steps increase, TS-Reasoner shows a clear advantage over CoT and CoT + code. For simpler tasks with 1-2 steps, the performance across all models is relatively similar. However, as tasks become more complex, TS-Reasoner consistently outperforms both baselines, with significantly higher success rates (SR) and better MAPE."}, {"title": "6.3 CAUSAL RELATIONSHIP RECOGNITION", "content": "Evaluation Protocol In causal relationship recognition, TS-Reasoner is given multi-variable time series, a description to the data, and expert knowledge of percentage of true relationships. The model need to incorporate the expert knowledge and the causal discovery tools based on directed acyclic graph to infer the probable causal relationship across multiple variables. In evaluation, we focus on three types of metrics: success rate (SR), causal relationship accuracy (CRA) and causal graph accuracy (CGA). The success rate is defined as the percentage of test samples in which the model successfully execute the task and did not violate any constraint in the instructions (e.g. the percentage of the causal relationships). The causal relationship accuracy is defined as the accuracy of classifying each pair of variable as causally related or not. The causal graph accuracy is defined as the percentage of test samples of which all causal relationships are correctly classified.\nOverall Performance In the causal relation recognition task, TS-Reasoner outperform the CoT and CoT + code on all metrics, as shown in Fig. 5. It is also noticeable that the performance of all methods on CGA, which is the hardest evaluation metric, are not satisfactory. Specifically, both CoT-based methods acquired 0.0 accuracy, which means that for any given test instance, none of these approaches can correctly infer all pairs causal relationships within it. Although TS-Reasoner slightly outperforms the baselines, the result is still very modest, opening opportunities for future works on addressing challenges under this setting."}, {"title": "7 CONCLUSION", "content": "In this work, we proposed complex and compositional time series reasoning task. To address this task, we developed a dedicated dataset collected from multiple domains and an evaluation framework that facilitate assessment of reasoning capabilities in time series analysis. Based on this framework and dataset, we developed a novel model, TS-Reasoner, which integrates program-based decomposition with large language models (LLMs) and task-specific modules. Our approach combines time series models, numerical methods, and LLM-generated custom modules to ensure both numerical accuracy and flexibility in handling a wide range of tasks, including personalized decision-making scenarios. This contribution offers a structured method that overcomes the limitations of purely LLM-driven reasoning-such as numerical errors encountered in Chain-of-Thought approaches-by enhancing it with programmatic task decomposition. For future work, we aim to"}, {"title": "A ANALYSIS FOR DIFFERENT IN-CONTEXT SAMPLES.", "content": "In Fig. 6, we analyze how the in-context samples impact the correctness of the program generated by the model. Specifically, we report how the percentage of the program that are correct varies with the number of in-context samples. As we can see, as the number increases, the correctness of generated program improves. When 16 samples are provided, the model is able to always generate correct programs."}, {"title": "B ERROR EXAMPLES", "content": "In this section, we present the example errors (shown in Fig. 7 and 8) from TS-Reasoner, CoT, and CoT + code approaches."}, {"title": "C PROMPT AND TOOL", "content": "C.1 PROMPT FOR TS-REASONER\nReturn only programs, using the specified operation functions. Do not return any results like dictionaries or lists. You must accurately learn the relationship between the question and the required operations. You must choose correct operations for each question. Do not use other irrelevant oper-ations."}, {"title": "C.2 AVAILABLE MODULES IN TOOLBOX", "content": "The tasks defined in this paper, along with their corresponding input-output formats and the tools utilized, are summarized in Table 3 below.\nSpecifically, the detailed description of the Predefined Tools is as follows:"}, {"title": "C.3 COT PROMPT", "content": "(1) Stock Future Price Prediction CoT\nYou are an experienced data scientist specializing in time series forecasting. I will provide you with a list of multiple time series. You must generate only predictions for the following questions, returning only the predictions without any codes, markdown formatting or extra characters. question\nChain of Thought: Step 1: Understand and parse the input data from text. Must clarify the number of all stocks. Step 2: Choose an appropriate model: You will select an appropriate time series forecast-ing model (e.g., ARIMA, LSTM, etc.) based on the input data. Make sure the model is suitable for forecasting the next n steps. Step 3: Apply the model: You will apply the chosen model to each time series (each column in the input data) and generate predictions for the next n steps. Step 4: Return the prediction results: Output the future predictions directly as 'predictions=List([List(),List(),...,])'.\nRequirement: Do not return any codes, just the final results 'predictions=List([List(),List(),...,])'. Please ensure that the output number of stocks is correct and the predicted length is accurate. Simply output the future predictions as a list. The predictions should be stored in a variable called 'predictions' and output the list directly.\n(2) Stock Future Price Prediction CoT with Code\nYou are an expert in time series forecasting. You need to perform a forecasting tasks. Generate only Python code, no markdown or extra characters, for the task below: question\nInstructions: 1. Input is a 2D numpy array 'data' of shape [L, C], where L is the historical data length and C is the number of time series. 'n' is the future length to predict. 2. Define a function that predicts n steps for each time series using models like ARIMA or LSTM. Ensure the model outputs all n steps of predictions. 3. Store your output in the variable called 'predictions'. 'predictions' should be a nested list: 'predictions = [[step1, step2, ..., stepn] for each time series]', Your prediction should be a 2d array of shape [n,C].\nRequirements: - Define data = np.array([]) as placeholders and do not include any hardcoded data in the data variable. - Ensure the code is fully executable and 'n' is set from the prompt.\n(3) Stock Future Volatility Prediction CoT\nYou are an experienced data scientist specializing in time series forecasting. I will provide you with a list of multiple time series. You must generate only predictions for the following questions, returning only the predictions without any codes, markdown formatting or extra characters. question\nChain of Thought: Step 1: Understand and parse the input data from text. Must clarify the number of all stocks. Step 2: Choose an appropriate model: You will select an appropriate time series forecasting model (e.g., ARIMA, LSTM, etc.) based on the input data. Make sure the model is"}, {"title": "C.4 CASUAL MINING DATA GENERATION", "content": "Now you are a Time series data scientist, please help me to write the code to generate some synthetic data in real world Time series domain, you should save the data into \"*/data.csv\":\nNow suggesting you should construct a series data based on a relation matrix and the correlation ratio for different influence factor, you should notice the following points, for time step I want you to generate 500 time steps:\n1. data correlation: the multi variable should be correlated, sample: which A first influence B, then B have influence on C or D, there should be some time delay, as the influence on other staff needs time."}]}