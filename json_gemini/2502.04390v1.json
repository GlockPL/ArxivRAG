{"title": "In Praise of Stubbornness:\nThe Case for Cognitive-Dissonance-Aware Knowledge Updates in LLMs", "authors": ["Simone Clemente", "Zied Ben Houidi", "Alexis Huet", "Dario Rossi", "Giulio Franzese", "Pietro Michiardi"], "abstract": "Despite remarkable capabilities, large language\nmodels (LLMs) struggle to continually update\ntheir knowledge without catastrophic forgetting.\nIn contrast, humans effortlessly integrate new in-\nformation, detect conflicts with existing beliefs,\nand selectively update their mental models. This\npaper introduces a cognitive-inspired investiga-\ntion paradigm to study continual knowledge up-\ndating in LLMs. We implement two key com-\nponents inspired by human cognition: (1) Dis-\nsonance and Familiarity Awareness, analyzing\nmodel behavior to classify information as novel,\nfamiliar, or dissonant; and (2) Targeted Network\nUpdates, which track neural activity to identify\nfrequently used (stubborn) and rarely used (plas-\ntic) neurons. Through carefully designed experi-\nments in controlled settings, we uncover a number\nof empirical findings demonstrating the potential\nof this approach. First, dissonance detection is\nfeasible using simple activation and gradient fea-\ntures, suggesting potential for cognitive-inspired\ntraining. Second, we find that non-dissonant up-\ndates largely preserve prior knowledge regardless\nof targeting strategy, revealing inherent robustness\nin LLM knowledge integration. Most critically,\nwe discover that dissonant updates prove catas-\ntrophically destructive to the model's knowledge\nbase, indiscriminately affecting even information\nunrelated to the current updates. This suggests\nfundamental limitations in how neural networks\nhandle contradictions and motivates the need for\nnew approaches to knowledge updating that better\nmirror human cognitive mechanisms.", "sections": [{"title": "1. Introduction", "content": "Humans effortlessly update their knowledge as they experi-\nence the world. They seamlessly integrate new information,\nignore redundant stimuli, and actively resolve conflicts with\nexisting beliefs before updating their mental models. This\ncognitive flexibility stems from several key abilities. Hu-\nmans exhibit (1) selective attention, focusing on novel or\nrelevant information while filtering out irrelevant or familiar\nstimuli (Posner et al., 1990; Petersen & Posner, 2012; Desi-\nmone et al., 1995; Ranganath & Rainer, 2003). They readily\n(2) detect conflicts (Croyle & Cooper, 1983) between new\ninformation and existing knowledge and actively engage in\nresolving them, a process known in psychology as cognitive-\ndissonance (Festinger, 1957; Van Veen et al., 2009). More-\nover, their brains exhibit a form of (3) adaptive plasticity,\nallowing for updates to neural networks that can incorporate\nnew information while often preserving existing knowledge.\nWhile the exact mechanisms are still being investigated, this\nprocess seems to balance the stability of well-established\nknowledge with flexibility in the face of new or uncertain\ninformation (McClelland et al., 1995; Behrens et al., 2007).\nDespite demonstrating remarkable capabilities, Large Lan-\nguage Models (LLMs) are still far from such learning abili-\nties. Current LLMs face significant challenges in real-world\ndeployment and long-term utility due to their static nature\nand training paradigms. They suffer from catastrophic for-\ngetting (Kirkpatrick et al., 2017a; Kemker et al., 2018; Li\net al., 2022; Luo et al., 2024; Kotha et al., 2024), where\nincorporating new information often leads to the erasure of\npreviously learned knowledge. Furthermore, LLMs engage\nduring training in indiscriminate learning, passively accept-\ning all training data, even when it contradicts what they\nalready learned. Despite emergent sparsity (Jaiswal et al.,\n2023; Mirzadeh et al., 2024), knowledge in LLMs follows\nbackpropagation and the objective function, with no explicit\nmechanism for targeted knowledge storage or retrieval. This\nresults in a situation where all weights are potential candi-\ndates for storing knowledge, necessitating comprehensive\nretraining to properly incorporate new information.\nIn this work, we embark on a systematic empirical investi-\ngation of how LLMs handle knowledge updates, drawing\ninspiration from human cognitive traits. Through carefully"}, {"title": "2. Dissonance-aware Targeted Updates", "content": "Our cognitively-inspired investigation begins with a striking\nempirical discovery. Fig. 2 reveals a fundamental distinction\nin how language models handle different types of updates:"}, {"title": "2.1. Extraction of historical activations and gradients", "content": "We maintain an aggregate profile of neuronal activity by\naccumulating activations and gradients for each neuron at\nevery training step. Specifically, for each neuron n in the\nTransformer blocks including feed-forward (MLP) layers\nand attention projections (Key, Query, Value matrices)-we\ncompute $\\hat{H}_n$, the cumulative historical gradient magni-\ntude over time, and $\\hat{H}_n$, the cumulative historical acti-\nvation magnitude over time. To mitigate scale differences"}, {"title": "2.2. Dissonance and Novelty Awareness", "content": "We cast our classification problem on three classes: for a\ngiven input sequence X, decide if it is Novel (e.g. could be\nintegrated), Familiar (e.g. can be ignored), or Dissonant\n(likely requiring proper resolution)."}, {"title": "2.3. Targeted Neuron Updates", "content": "Building upon the historical tracking of neural activity, we\nimplement targeted network updates to study the impact\nof knowledge placement location on new knowledge inges-\ntion and past knowledge retention. We design four main\ntypes of targeted updates, which we experimentally evaluate.\nDuring training on new information, we perform standard\nforward and backward passes to compute the loss and gradi-\nents. Before the optimizer step, we modify the gradients to\nfreeze certain neurons. Specifically, given the gradients for\nall parameters of a given layer, we zero-out those that do\nnot belong to the selected set of neuron and corresponding\nweights, defined as plastic, stubborn, candidate and specific,\nas described below. This process effectively freezes the\nweights of non-selected neurons, allowing for targeted up-\ndates to specific parts of the model. By varying the choice\nof selected neurons, we control how new information is inte-\ngrated into the model while managing its impact on existing\nknowledge. Next, we introduce strategies to select which\nneurons and weights to update: Figure 3 illustrates the con-\nceptual relationship between the various neuron updates\nstrategies within the model's parameter space."}, {"title": "3. Experimental evaluation", "content": "We discuss our experimental setup, to evaluate dissonance-\nawareness, as well as continual knowledge update."}, {"title": "3.1. Experimental setup", "content": "Dataset. We use the COUNTERFACT dataset (Meng et al.,\n2022b) as our primary data source as it contains both facts\nand counterfacts. This dataset, with approximately 17,000\nfacts, allows us to test models' handling of conflicting\nknowledge and addition of potentially known information,\ntwo key aspects of our dissonance-aware approach. To ad-\ndress the lack of truly novel facts in COUNTERFACT, we\ngenerate additional data using GPT-3.5. We transform ex-\nisting statements into plausible yet fictitious information,\nmaintaining structural similarity while introducing novel\ncontent. For example, \"Danielle Darrieux's mother tongue\nis French\" becomes \"Sylvan Myrthil's mother tongue is\nSylvan\" (see Appendix C.1 for details)."}, {"title": "3.2. Dissonance awareness", "content": "Settings. Our first goal is to evaluate the ability to discrimi-\nnate familiar, novel, and conflicting information using the\nreadily-available simple features we extract from the mod-\nels during the forward and backward passes. As schematized\nin Fig. 1 (left), we do so by relying on simple classifiers (ran-\ndom forests and SVMs), contrasting two scenarios for the\ninput features: (1) a GPT-2 model fine-tuned on 1000 facts\n(the knowns), and (2) a GPT-2 pre-trained model (using its\n600 extracted known facts as known class samples)."}, {"title": "3.3. Non-dissonant updates", "content": "Settings. We now investigate how LLMs handle non-\ndissonant updates using our different strategies as exper-\nimental tools. In our experiments, schematized in Fig. 1\n(middle), we evaluate the incorporation of non-conflicting"}, {"title": "3.4. Dissonant Updates", "content": "Settings. We examine how LLMs handle conflicting infor-\nmation as shown in Fig. 1 (right): after training on 2,000 old\nfacts and 1,000 new facts, we introduce 1,000 conflicting\nupdates contradicting previously learned new facts, apply-\ning our various strategies to study their impact on learning\nconflicting facts and preserving unrelated old knowledge."}, {"title": "4. Discussion and conclusions", "content": null}, {"title": "4.1. Lessons learned", "content": "Fundamental Properties of Knowledge Updates: Our re-\nsults reveal a striking pattern: while non-dissonant updates\nshow remarkable robustness, dissonant updates trigger se-\nvere corruption of unrelated knowledge - dropping accuracy\nbelow 60% even when modifying just 10-100 facts. This\neffect persists across our tested model scales and update\nstrategies, suggesting a deeper challenge in how current\nneural architectures handle contradictory information.\nFeasibility of Dissonance Detection: LLMs encode clear\nsignatures distinguishing between novel, familiar, and dis-\nsonant information. In our controlled experiments, simple\nclassifiers achieve 95% accuracy with pre-trained models\nand 99% with finetuned models using either activation/gra-\ndient features or output probabilities. This suggests the\npotential for developing mechanisms to identify potentially\nproblematic updates before they occur."}, {"title": "4.2. Limitations and Future Directions", "content": "Experimental Control vs. Scale: While our controlled\nexperiments reveal fundamental properties of knowledge\nupdating, investigating these phenomena in much larger\nmodels presents challenges as it is not straightforward to\ntrack the impact on their broader knowledge.\nDataset Limitations: Our current findings rely on\nCounterFact-derived data with simple factual statements.\nDeveloping larger, more diverse datasets is essential for\nunderstanding how these properties generalize to more com-\nplex forms of knowledge and conflicts.\nNeuron Classification Metrics: While our analysis of\nneural plasticity uses gradient magnitudes effectively, fu-\nture work could explore richer metrics incorporating activa-\ntion patterns and network connectivity to better understand\nknowledge distribution and update mechanisms.\nBeyond Binary Dissonance: Our current investigation\ntreats dissonance as binary, while real-world knowledge\nupdates often involve varying degrees of conflict and differ-\nent types of knowledge. Understanding how these nuances\naffect knowledge integration remains a challenge.\nTowards Human-Inspired Updates: The catastrophic na-\nture of dissonant updates suggests we may need fundamen-\ntally different approaches to LLM training. Rather than\nattempting to overwrite existing knowledge, future work\nmight explore mechanisms for maintaining and contextu-\nalizing potentially conflicting information - similar to how\nhumans maintain both historical and updated knowledge\nwith appropriate contexts."}, {"title": "Impact Statement", "content": "Our work reveals a fundamental and concerning limitation\nof current Al systems: when LLMs encounter contradictory\ninformation, they suffer catastrophic corruption of unrelated\nknowledge, even in large-scale models. This vulnerability\ncontrasts sharply with human cognition's remarkable flexi-\nbility in handling contradictory information. Interestingly,\nthis distinction might hint at why human cognition seems\nto favor accumulation and contextualization of conflicting\nknowledge (e.g., \"before\" vs \"after\"), despite the tension\nit creates, rather than direct overwriting. This cognitive\npattern might suggest that direct knowledge editing poses\ninherent risks that even evolutionary processes had to work\naround.\nThis finding has critical implications for Al deployment:\nAny AI system operating in the real world will inevitably\nencounter contradictory information, whether through natu-\nral knowledge evolution (e.g., medical guidelines updates)\nor potential adversarial attacks (e.g., coordinated misinfor-\nmation campaigns). Our work demonstrates that such con-\ntradictions don't just affect related knowledge - they can cor-\nrupt the system's broader knowledge base in unpredictable\nways. Another question is whether this also applies for\nvalue alignment in case of attempts to update an AI sys-\ntem's learned values or ethical principles.\nThese findings motivate our development of dissonance\ndetection capabilities as a crucial safety mechanism for de-\nployed Al systems. More broadly, they suggest we may\nneed to fundamentally rethink AI architectures to develop\nsystems that, like humans, maintain and contextualize po-\ntentially conflicting information rather than attempting to\noverwrite it. This might require moving away from current\napproaches that try to \"edit\" neural networks toward archi-\ntectures that can accumulate and contextualize knowledge\nwhile maintaining multiple, temporally-organized versions\nof truth."}, {"title": "B.1. Preliminary notation", "content": "We focus on the historical tracking of gradients of the outputs (grad_outs) and activations for four key matrices within each\nblock of the transformer model: $Attnc\\_attn$, $Attnc\\_proj$, $MLPc\\_fc$, and $MLPc\\_proj$.\nGiven an input sequence $X \\in \\mathbb{R}^{B \\times N \\times d_{model}}$, where B is the batch size, N is the sequence length, and $d_{model}$ is the model\ndimension, the transformer block is defined as follows:\nAttention Layer: The attention mechanism computes query Q, key K, and value V matrices:\n$Q = XW_Q$, $K=XW_K$, $V = XW_V$\nwhere $W_Q \\in \\mathbb{R}^{d_{model} \\times d_{key}}$, $W_K \\in \\mathbb{R}^{d_{model} \\times d_{key}}$, and $W_V \\in \\mathbb{R}^{d_{model} \\times d_{value}}$ are trainable projection matrices.\nThe concatenated matrix $Attnc\\_attn$ is:\n$Attnc\\_attn = [Q, K, V] = XW_{attn}$\nwhere $W_{attn} = [W_Q, W_K, W_V] \\in \\mathbb{R}^{d_{model} \\times (2d_{key}+d_{value})}$.\nThe attention context $Attn_{context}$ is computed as:\n$Attn_{context} \\triangleq softmax(\\frac{QK^T}{\\sqrt{d_{key}}})V$\nThe projected attention output $Attnc\\_proj$ is:\n$Attnc\\_proj = Attn_{context} W_{proj}$\nwhere $W_{proj} \\in \\mathbb{R}^{d_{value}\\times d_{model}}$\nMLP Layer: The MLP layer consists of two linear transformations with an activation function $\\sigma$:\n$MLPc\\_fc = \\sigma(XW_{fc} + b_{fc})$\nwhere $W_{fc} \\in \\mathbb{R}^{d_{model} \\times d_{ff}}$ and $b_{fc} \\in \\mathbb{R}^{d_{ff}}$.\nThe projected MLP output $MLP_proj$ is:\n$MLPc\\_proj = MLPc\\_fc W_{proj} + b_{proj}$\nwhere $W_{proj} \\in  ][\\mathbb{R}^{d_{ff} \\times d_{model}}$ and $b_{proj} \\in \\mathbb{R}^{d_{model}}.$"}, {"title": "B.2. Historical gradient and activation collection", "content": "Collecting a profile of neuron activity during training or simulation of training is needed as (i) input feature to know if a fact\nis dissonant, novel or known, and (ii) as means to identify where to locate targeted updates.\nDuring training, we collect and cumulate the gradients of the outputs (grad_outs) and activations for the matrices $Attnc\\_attn$,\n$Attnc\\_proj$, $MLPc\\_fc$, and $MLPc\\_proj$. Let t denote the training step. We collect activations at step t:\n$Attnc\\_attn(t)$, $Attnc\\_proj(t)$, $MLPc\\_fc (t)$, $MLPc\\_proj (t)$\nas well as Gradient of the Outputs (grad_outs) at step t :\n$\\nabla L(Attnc\\_attn(t))$, $\\nabla L(Attnc\\_proj(t))$, $\\nabla L(MLPc\\_fc(t))$, $\\nabla L(MLPc\\_proj(t))$\nIn the remainder, we denote these, regardless of their provenance matrix, as:\n$A'(t)$, $G'(t) \\in \\mathbb{R}^{BxNxdout}$\nwhere $l$ denotes the layer, B is the batch size, N is the sequence length, and $d_{out}^l$ is the output dimension of layer $l$.\nWhen needed, we standardize these metrics for each layer $l$ as follows:\n$\\hat{A}^l(t) = \\frac{A^l(t) - \\mu_A^l(t)}{\\sigma_A^l(t)}$, $\\hat{G}^l(t) = \\frac{G^l(t) - \\mu_G^l(t)}{\\sigma_G^l(t)}$\nwhere $\\mu$ and $\\sigma$ are the mean and standard deviation computed over all dimensions of the respective tensor.\nWe then sum over the batch dimension:\n$S_A^l(t)_{n,i} = \\sum_{b=1}^{B} \\hat{A}_{b,n,i}^l(t)$, $S_G^l(t)_{n,i} = \\sum_{b=1}^{B} \\hat{G}_{b,n,i}^l (t)$\nOptionally, we can sum over the token dimension:\n$S_A^l(t) = \\sum_{n=1}^{N}S_A^l(t)_{n,i}$, $S_G^l(t) = \\sum_{n=1}^{N}S_G^l(t)_{n,i}$\nThe standardized and summed metrics are then accumulated across the training steps:\n$H \\hat{A}^l = \\sum_{t=1}^{T} \\hat{S}_A^l(t)$, $H \\hat{G}^l = \\sum_{t=1}^{T} \\hat{S}_G^l(t)$;\nwhere $T$ is the total number of training steps.\nThese historical activations $H \\hat{A}^l$ and gradients $H \\hat{G}^l$ provide cumulative measures of neuron activity over the training\nprocess. They help identify neurons that are heavily utilized (stubborn neurons) and those that are underutilized (plastic\nneurons), which is crucial for our targeted updates."}, {"title": "C.1. Augmenting the COUNTERFACT Dataset with Novel facts", "content": "To generate unknown facts to augment the Counterfact dataset, we used GPT-3.5 with a prompt as follows:"}]}