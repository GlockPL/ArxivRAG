{"title": "DATA-DRIVEN PSEUDO-SPECTRAL FULL WAVEFORM INVERSION VIA DEEP NEURAL NETWORKS", "authors": ["Christopher Zerafa", "Pauline Galea", "Cristiana Sebu"], "abstract": "FWI seeks to achieve a high-resolution model of the subsurface through the application of multi-variate optimization to the seismic inverse problem. Although now a mature technology, FWI has limitations related to the choice of the appropriate solver for the forward problem in challenging environments requiring complex assumptions, and very wide angle and multi-azimuth data necessary for full reconstruction are often not available.\nDeep Learning techniques have emerged as excellent optimization frameworks. These exist between data and theory-guided methods. Data-driven methods do not impose a wave propagation model and are not exposed to modelling errors. On the contrary, deterministic models are governed by the laws of physics.\nApplication of seismic FWI has recently started to be investigated within Deep Learning. This has focussed on the time-domain approach, while the pseudo-spectral domain has not been yet explored. However, classical FWI experienced major breakthroughs when pseudo-spectral approaches were employed. This work addresses the lacuna that exists in incorporating the pseudo-spectral approach within Deep Learning. This has been done by re-formulating the pseudo-spectral FWI problem as a Deep Learning algorithm for a data-driven pseudo-spectral approach. A novel DNN framework is proposed. This is formulated theoretically, qualitatively assessed on synthetic data, applied to a two-dimensional Marmousi dataset and evaluated against deterministic and time-based approaches. Inversion of data-driven pseudo-spectral DNN was found to outperform classical FWI for deeper and over-thrust areas. This is due to the global approximator nature of the technique and hence not bound by forward-modelling physical constraints from ray-tracing.", "sections": [{"title": "Introduction", "content": "Full waveform inversion (FWI) seeks to achieve a high-resolution model of the subsurface through the application of multivariate optimization to the seismic inverse problem [1]. Optimization theory is fundamental to FWI since the parameters of the system under investigation are reconstructed from indirect observations that are subject to a forward modelling process [2]. Choice of the forward problem will impact the accuracy of the FWI result. Challenging environments require more complex assumptions to try and better explain the physical link between data and observations, with not necessarily improved levels of accuracies [3]. Secondly, the data being used to reconstruct the mapping of"}, {"title": "Theoretical Background", "content": "The forward problem in FWI is based on the wave equation describing particle wave motion. It is a second order, partial differential equation involving both time and space derivatives. The particle motion for an isotropic medium is given by:\n$\\frac{1}{c(m)^{2}} \\frac{\\partial^{2} p(m, t)}{\\partial t^{2}} - \\nabla^{2} p(m, t) = s(m, t)$,\nwhere $p(m, t)$ is the pressure wave-field, $c(m)$ is the acoustic p-wave velocity and $s(m, t)$ is the source [11]. To solve the wave equation numerically, it can be expressed as a linear operator.\nA common technique employed within the forward modelling stage is to perform modelling in pseudo-spectral domain rather than the time domain. The most common domain is the Fourier domain [11] and computational implementation is generally achieved via the Fast Fourier Transform (FFT) [12].\nAfter forward modelling the data in pseudo-spectral domain, the objective is to seek to minimize the difference between the observed data and the modelled data. The misfit between the two datasets is known as the objective- or cost-function J. The most common cost function is given by the $L^{2}$-norm of the data residuals:\n$J(m) = \\frac{1}{2} \\sum_{i=1}^{D} [||d - F(m)||^{2} + \\lambda ||m||^{2}]$,\nwhere $D$ indicates the data domain given by $n_{s}$ sources and $n_{r}$ receivers, $M$ is the model domain, and $\\lambda$ is a regularization parameter to introduce well-posedness. The misfit function $J$ can be minimized with respect to the model parameters $d$ if the gradient is zero, namely:\n$\\nabla J = \\frac{\\partial J}{\\partial d} = 0$,\nAt each iteration $k$, assuming small enough model perturbation and using Taylor Expansion up to second orders, the misfit function $J(m_{k})$ is calculated from the previous iteration model $m_{k-1}$ as:\n$J(m_{k}) = J(m_{k-1}) + \\frac{\\partial J}{\\partial m_{k-1}} \\delta m_{k} + \\frac{1}{2} \\delta m_{k-1}^{2T} \\frac{\\partial^{2} J}{\\partial m_{k-1}^{2}} \\delta m_{k-1}$"}, {"title": "Proposed FWI as a Data-Driven DNN", "content": "Neural networks are a subset of tools in artificial intelligence which when applied to inverse problems can approximate the non-linear function of the inverse problem $F^{-1}: D \\rightarrow M$. That is, using a neural network, a non-linear mapping can be learned to minimize:\n$||m - g_{\\Theta} (d)||^{2}$,\nwhere $\\Theta$ the large data set of pairs (m, d) used for the learning process [5].\nThe most elementary component in a neural network is a neuron. This receives input and sums the result to produce an output. For a given artificial neuron, consider $n$ inputs with signals $m$ and weights $w$. The output $d_{k}$ of the $k^{th}$ neuron from all input signals is given by:\n$d_{k} = \\sigma (b + \\sum_{j=0}^{m} w_{kj}m_{j})$,\nwhere $\\sigma$ is the activation function and $b$ is a bias term enabling the activation functions to shift about the origin. When multiple neurons are combined together, they form a neural network. The architecture of a neural network refers to the number of neurons, their arrangement and their connectivity [13]. The output of the unit in each layer is the result of the weighted sum of the input units, followed by a non-linear element-wise function. The weights between each units are learned as a result of a training procedure.\nWhen training a DNN, the forward propagation through the hidden layers from input m to output d needs to be measured for its misfit. The most commonly used cost function is the Sum of Squared Errors, defined as:\n$J(m) = \\frac{1}{2} \\sum_{i=1}^{J} (m - g_{\\Theta} (d^{(i)}))^{2}$,\nwhere $d$ is the labelled true dataset, $d^{(i)}$ is the output from the $i^{th}$ forward pass through the network and the summation is across all neurons in the network. The objective is to minimize the function $J$ with respect to the weights $w$ of the neurons in the neural network. Employing the Chain Rule, the error signals for all neurons in the network can be recursively calculated throughout the network and the derivative of the cost function with respect to all the weights $w$ can be calculated. Training of the DNN is then achieved via a Gradient Descent algorithm, referred to as back-propagation training algorithm [15]."}, {"title": "Experimental Results", "content": null}, {"title": "Train-Test Data Split", "content": "Learning the inversion from time to pseudo-spectral domain requires a training dataset which maps time to pseudo-spectral components and their respective velocity profile. A data generator was designed to create synthetic data for a 2000ms time window. The steps involved in the data generator are:\n1. Randomly create velocity profile $v_{p}$ for a 2000ms distance, with value ranging from $1450ms^{-1}$ and $4500ms^{-1}$.\n2. Estimate the density $p$ using Gardner's equation $p = \\alpha v_{p}^{\\beta}$ where $\\alpha = 0.31$ and $\\beta = 0.25$ are empirically derived constants [16].\n3. At each interface, calculate the Reflection Coefficient $R = \\frac{\\rho_{2}v_{p_{2}}-\\rho_{1}v_{p_{1}}}{\\rho_{2}v_{p_{2}}+\\rho_{1}v_{p_{1}}}$ where $\\rho_{i}$ is density of medium $i$ and $v_{p}$ is the p-wave velocity of medium $i$.\n4. Define a wavelet $W$. This was selected to be a Ricker wavelet at 10Hz [17].\n5. The reflection coefficient time series and wavelet are convolved to produce the seismic trace $T$."}, {"title": "DNN Framework", "content": "illustrates the DNN framework used to first invert for the Fourier coefficients from the time domain and then invert for velocity profile. The complete workflow has five modules, with each module consisting of a neural network with 5 fully connected hidden layers. The layer distributions is shown within each fiure. This hour-glass design can be considered representative of multi-scale FWI [18] since at each hidden layer, the NN learns an abstracted frequency component of the data at a different scale. This is synonymous with modern DNN approaches such as encoder-decoders and U-Net [19] and how they extract data representations. The final concatenate network learns the optimal way for combining the outputs. In the case of the CWT, we designed a similar framework, except that there is an additional dimension to be able to create the CWT."}, {"title": "Pre-Processing", "content": "In classical DNN approaches, it is best practice to normalise or standardize the dataset. An experiment was executed to assess what would happen with and without normalisation of the data for the Magnitude component architecture for 10,000 training and 1,000 validation traces. The scaling approaches considered are the Standard Scaler and Min-Max Scaler, defined as:\nMin-Max Scaler : $x_{MM} = \\frac{x - x_{MIN}}{x_{MAX} - x_{MIN}}$\nStandard Scaler: $x_{SS} = \\frac{x - \\mu}{\\sigma}$\nwhere $x_{MIN}$, $x_{MAX}$ are the minimum and maximum values of the data, $\\mu$ is the mean and $\\sigma$ is the standard deviation of the training samples.\nThe impact of the pre-processing on the inverted velocity profiles is shown in Figure 3. Min-Max scaling was the worst performant, only able to reconstruct the first layer at 500ms. Standard Scaling was considered as a potential, however, considering the third velocity profile, we can see how without scaling, more of the second layer is being reconstructed. Furthermore, having to scale data would include an additional compute overhead. For these reasons, it was shown that normalization should not be applied for data-driven FWI inversion."}, {"title": "Architecture Comparison", "content": "We proceed trying to identify the ideal configuration in terms of architecture, loss, time-to-train and over-fitting. The conversion from time trace to pseudo-spectral representation was fixed for all 1D and 2D networks such that the comparison was on the architectures.\nThe networks were trained for the same number of epochs without early stopping. The training and validation data consisted of 1,000,000 and 100,000 generated traces. The loss was fixed to be the Mean Squared Error (MSE) The architectures chosen for this comparison are a Multi-Layer Perceptron (MLP), Convolutional neural network with 1D filters (Conv1D) and 2D filters (Conv2D), VGG [20] and ResNet [21]. The loss optimizers were Adagrad [22], Adadelta [23], RMSprop [24] and Adam.\nEvaluation for all the loss curves is summarised in Table 1. The criteria below are ranked from 1-20, with 20 being the best result:\n$\\bullet$ Duration (d): Duration of training. The shorter the duration, the better the performance.\n$\\bullet$ Train (t): Lowest MSE within training.\n$\\bullet$ Validation (v): Qualitative assess of under-fitting/over-fitting and learning rate performance.\n$\\bullet$ Inversion (i): RMSE of 100,000 validation velocities as compared to true velocity.\nThe score was calculated as\nScore = d + t + v + 2i.\nThis formula is arbitrarily chosen and linear in nature, making ideal for interpretation and understanding. The additional weight of 2 for the inversion rank emphasizes the inversion is the most important criteria. The overall rank criteria"}, {"title": "Marmousi Model Experiment", "content": "The Marmousi-2 model [25] was used to evaluate data-driven DNN on an industry standard dataset. This was filtered with a 150m vertical median filter and the number of layers in each velocity profile was analytically calculated to range between 20 to 50 layers."}, {"title": "DNN FWI Generator", "content": "A generator was constructed to be able to invert for our modified Marmousi model."}, {"title": "DNN Training", "content": "The network was trained for 30 epochs, at 1,000,000 traces and 100,000 traces per training and testing dataset respectively. The network was a slightly modified version of the ideal network identified in the previous section due to the longer time length of trace."}, {"title": "Classical FWI", "content": "3.5Hz FWI with Sobolev space norm regularization [26] was used to compare against data-driven FWI. This results in a minimum update resolution of 414m, and the iterative update process started from frequency 1Hz and iteratively updated by a factor of 1.2. The optimization algorithm was L-BFGS-B, with 50 iterations per frequency band in each update. Forward shot modelling was done every 100m, starting from 100m offset, and receivers spaced every 100m.\ncompares classical FWI and data-driven DNN FWI. Off the start, it is clearly evident how the DNN approach is producing a lot more uplift than the standard approach. There is improved imaging in the sediment layers, with distinct layers being reconstructed which would otherwise be missed with classical FWI \u2013 Zoom 1 The middle section, with the heavily over-trusted layers shown in Zoom 2, the velocity layers are also being reconstructed to good levels and the small sedimentary pockets at the pinch of the over-thrust are being to be imaged as well. These are being missed completely in Classical FWI. Sub-salt in Zoom 3, DNN is once again producing much better imaging up to the salt and below the salt. Indeed, sub-salt, we are starting to image partially some of the layer coming up into the salt.\nThe velocity profiles confirm that our DNN approach inverted more of the signal than classical FWI. Upon closer investigation, we are seeing small spikes on the velocity on the salt section of Xlines 2000, 4000 and 6000. Further training would potentially mitigate this. From the velocity profiles, we see how FWI is able to update the shallow sections up to 1400m really well, potentially better than DNN as it is able to identify a velocity inversion at depth 500m on Xline 8000 and a pronounced segment layer at depth 800m on Xline 12000. However, beyond 1400m depth, the geometry and forward-modelling physical constraints from ray-tracing come into play and FWI is unable to provide more uplift at deeper velocity packages."}, {"title": "Discussion", "content": null}, {"title": "Inversion Paradigm", "content": "Within the DNN approach, the inversion component is data-driven and the generator is based on geophysics. If data-generators are to include information that might not be deterministically available, the inversion process could invert for this additional information. These types of data-driven models could be pre-cursors for deterministic models [27].\nshows inversion for classical FWI without and with DNN as a priori model. The latter approach produces more layer continuity and better imaging at depth. This comes with relatively no extra overhead cost since the DNN would be a pre-trained network."}, {"title": "Data Generators for Real Data", "content": "The DNN requires a global model for a real world problem. This is not always available in real world problems, however a collection of pre-trained DNN could be used as precursors of classical FWI and enable better parameterization. Consider as example a data generator trained on data that is limited to 3 layers, namely Conv1D-Adadelta DNN trained for 30 epochs for a maximum 3 layers for velocity ranging from 1450ms-1 to 5000ms-1. If inversion is to be performed on more than 3 layers, the inversion process will start degrading as shown in Figure 6a. Figure 6b illustrates inversion for a large velocity of 6000ms-\u00b9 and velocity inversion respectively. Given that these models were not included in the development dataset, the inversion process would never be able to invert for these velocity types correctly."}, {"title": "Conclusion", "content": "In this manuscript elements within a classical FWI framework were shown to be substitutable with DNN components. The base architecture for the network was set to be an hour-glass neuron design, representative of multi-scale FWI and modern DNN approaches. This was tested for normalization and concluded not applicable. MLP, 1D and 2D convolutions, VGG and ResNet type architectures for Adagrad, Adadelta, RMSprop and Adam optimizer werequantitatively evaluated for training duration, performance, validation and learning rate performance, and inversion. The best performing architecture-loss combination was identified as Conv1D-Adadelta.\nThe Conv1D-Adadelta network was trained for inversion of a modified Marmousi model by using a 20-50 layer generator, with velocities ranging from 1450ms-1 to 5000 ms\u00af\u00b9. 3.5Hz classical FWI was compared to this data-driven DNN inversion. Inversion performance in shallow sections was equally good for either classical FWI or DNN approach. DNN framework performs better for deeper and over-thrust areas since DNNs are not bound by forward-modelling physical constraints from ray-tracing.\nData-driven FWI should be considered within global approximation approaches and has potential to be used as a priori to deterministic FWI. The DNN model generators were shown to work within the boundaries of their parameter. Application of pre-trained networks is relatively easy and thus different geophysical model hypothesis could be assessed quickly. Pseudo-spectral DNN FWI is robust to noise and future work would involve implementation as a de-noising techniques."}]}