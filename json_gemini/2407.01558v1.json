{"title": "Graphical user interface agents optimization for visual instruction grounding using multi-modal artificial intelligence systems", "authors": ["Tassnim Dardouri", "Laura Minkova", "Jessica L\u00f3pez Espejel", "Walid Dahhane", "El Hassane Ettifouri"], "abstract": "Most instance perception and image understanding solutions focus mainly on natural images. However, applications for synthetic images, and more specifically, images of Graphical User Interfaces (GUI) remain limited. This hinders the development of autonomous computer-vision-powered Artificial Intelligence (AI) agents. In this work, we present Search Instruction Coordinates or SIC, a multi-modal solution for object identification in a GUI. More precisely, given a natural language instruction and a screenshot of a GUI, SIC locates the coordinates of the component on the screen where the instruction would be executed. To this end, we develop two methods. The first method is a three-part architecture that relies on a combination of a Large Language Model (LLM) and an object detection model. The second approach uses a multi-modal foundation model.", "sections": [{"title": "1. Introduction", "content": "Repetitive tasks refer to activities that require little critical thinking and are regularly repeated within organizations, such as businesses and public administrations. In many cases, these tasks are considered tedious and un-rewarding for the employees who perform them and can lead to monotony and professional exhaustion. For this reason, the design of an autonomous AI-powered Graphical User Interface (GUI) agent to automate these tasks becomes necessary. A key step towards building this agent is making it capa-ble of interacting with the GUI environment. In this context, many solutions use structured data which can be lengthy (e.g., HTML), impractical and even inaccessible (e.g., on desktops). To alleviate this issue, it is necessary to design an Artificial Intelligence (AI) agent that only relies on screenshots for interactions with its GUI environment. Recent development of Large Language Models (LLMs) has opened countless possibilities in automating numerous AI tasks. Transformer-based models such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2018), T5 (Raffel et al., 2019), and GPT-3 (Generative Pre-trained Transformers) (Brown et al., 2020) had great success for Natural Language Processing (NLP) ap-plications namely text generation (Jiang et al., 2023; Touvron et al., 2023), translation (Moslem et al., 2023; Waldendorf et al., 2024; Wu et al., 2024a), summarization (Lewis et al., 2019; Zhang et al., 2020; Takeshita et al., 2022), and so forth.\nSubsequently, the use of transformers was later extended to the com-puter vision domain where they revolutionized multiple applications such as image classification (Tan and Le, 2019; Kolesnikov et al., 2019; Dosovitskiy"}, {"title": null, "content": "et al., 2020; Bao et al., 2021; Radford et al., 2021; Graham et al., 2021;\nLiu et al., 2021; Jaegle et al., 2021; Liu et al., 2022; Oquab et al., 2024),\nobject detection (Carion et al., 2020; Zhu et al., 2020; Meng et al., 2021;\nSmock et al., 2021; Fang et al., 2021; Ouyang-Zhang et al., 2022) and ob-ject understanding (Yang et al., 2019; Ye et al., 2019; Zhu et al., 2022; Yang\net al., 2021) through multi-modal foundation models. An interesting sub-category of object understanding for developing vision capabilities for GUI\nagents is Referring Expression Grounding (REG) where the key challenge is\nassociating natural language expressions with specific objects or regions in vi-sual data. REG involves understanding and identifying the referred objects\nor regions based on the contextual information provided by the language,\nwhich can include visual attributes, spatial relationships, and interactions\nwith the surrounding environment. This task is pivotal in Natural Language\nUnderstanding (NLU) and GUI environment understanding. It bridges the\nsemantic gap between linguistic descriptions and the visual world, enabling\nmachines to comprehend and interact with the environment more effectively.\nResearch conducted by Mao et al. (2015) demonstrated the efficiency of\nREG in image captioning tasks. Furthermore, Anderson et al. (2017) illus-trated its significance in visual question answering while Ye et al. (2019),\nYang et al. (2021) and Zhu et al. (2022) leveraged cross-modal learning to\nsegments out an object referred to by a natural language expression from\nan image. Furthermore, Anderson et al. (2017) illustrated its significance in\nvisual question answering, while Ye et al. (2019); Yang et al. (2021), and Zhu\net al. (2022) leveraged cross-modal learning to segment out an object referred\nto by a natural language expression from an image. Despite the numerous"}, {"title": null, "content": "research works in the context of referring to expression grounding, the ma-jority focuses on natural images. Few works studied the REG in the context\nof GUI instruction grounding. Indeed, research works like Li et al. (2020);\nRozanova et al. (2021); Zhang et al. (2023c); Venkatesh et al. (2023), and\nCheng et al. (2024) propose solutions allowing the localisation of screen ele-ments based on natural language instructions. This task is especially useful\nfor building autonomous visual GUI agents designed to automate complex\ntasks on digital devices.\nIn this work we propose to achieve natural language instruction grounding\nusing two methods. The first method uses an OCR-powered instruction\ngrounding module that we name Search Instruction Coordinates with OCR\nor SICocr. It relies on the finetuning of YoloV8 (Reis et al., 2023), the use\nof ChatGPT (OpenAI et al., 2024) and an Optical Character Recognition\n(OCR) module. The second method is more straightforward called Search\nInstruction Coordinates direct or SICdirect and it relies on the finetuning\nof the model aimed at universal instance perception proposed in Yan et al.\n(2023b) for the specific task of GUI instruction grounding."}, {"title": "2. Related works", "content": "Autonomous GUI Navigation : Several earlier research works investi-gated GUI automation tasks for web applications (Shi et al., 2017; Liu et al.,\n2018; Gur et al., 2018) and mobile UI applications (Li et al., 2020; Burns\net al., 2022; Li and Li, 2023). More recently, the significant advancements\nin the NLP field, propelled by the emergence of Large Language Models\n(LLMS) (Touvron et al., 2023; Xu et al., 2024; OpenAI et al., 2024; Sun"}, {"title": null, "content": "et al., 2024; Wu et al., 2024b), have shifted the focus toward developing LLM-powered GUI agents. This has become a primary area of interest for many\ncontemporary studies. Indeed, some works proposed to tackle this challenge\nwith a focus on prompt engineering of ChatGPT and ChatGPT-4 for web\ntasks using in context-learning (Zheng et al., 2024b) and self-refining (Kim\net al., 2023). Other research works proposed to train LLMs for more specific\ntasks. In this context, Deng et al. (2023) introduced a two-stage method\ndesigned to identify target elements in a given HTML file, while Gur et al.\n(2024) used programming to allow interactions with websites. Despite these\nadvancements, LLMs are still limited because of their ability to process text\nonly. To overcome this, recent works proposed a vision-based GUI naviga-tion (Shaw et al., 2023; Zhang and Zhang, 2024; Hong et al., 2023) using\nGPT-4V (Yan et al., 2023a; Gao et al., 2024). However, they take metadata\nas input and do not rely on visual data (Zhang et al., 2023a; Zheng et al.,\n2024a).\nLarge Vision-Language Models : The research interest in process-ing images and text data simultaneously has been getting more and more\nattention. In this respect, recent research has seen significant efforts in\nconstructing Large Vision-Language Models (LVLMs) capable of process-ing both images and text jointly (Liu et al., 2023; Zhu et al., 2023; Ye et al.,\n2023; Li et al., 2023). Cross-modal vision-language tasks have become pos-sible to perform with the integration of vision encoders with LLMs. Indeed,\nthrough contrastive learning and masked data modeling on large-scale image-text pairs techniques, models like CLIP (Radford et al., 2021), ALIGN (Jia\net al., 2021), Florence (Yuan et al., 2021), BEIT3 (Wang et al., 2022), and"}, {"title": null, "content": "Flamingo (Alayrac et al., 2022) demonstrate impressive zero-shot capabili-ties on vision-language challenges. Other works like DALL-E (Ramesh et al.,\n2021, 2022), and Stable Diffusion (Rombach et al., 2021) rely on image- c\u0430\u0440-tion pairs training for the generation of image content given textual infor-mation. More recently, research works propose to rely on reference language\ninput as well as visual input to perform a multitude of tasks including Refer-ring Expression Comprehension (REC) (Yu et al., 2016; Zhang et al., 2021;\nZou et al., 2023), Referring Expression Segmentation (RES) (Yuan et al.,\n2021; Zhang et al., 2023b, 2021), and Referring Video Object Segmentation\n(R-VOS) (Bommasani et al., 2021; Touvron et al., 2023; Xu et al., 2016),\nwhere the goal is to match objects with given language expressions. Studies\nhave also focused on grounding tasks with LVLMs (Wang et al., 2023; Bai\net al., 2023; Chen et al., 2024), such as providing bounding boxes for objects\nwhen generating responses (Chen et al., 2023; Peng et al., 2023). However,\nthese efforts primarily addressed natural images and did not explore GUI con-texts. In this paper, we investigate natural language instruction grounding\nin GUI environment and explore its potential for autonomous GUI agents."}, {"title": "3. Proposed approaches", "content": null}, {"title": "3.1. SICocr approach", "content": "YoloV8 model (Reis et al., 2023) demonstrated cutting edge performance\nin object detection and tracking, instance segmentation, image classification\nand pose estimation tasks (Ken, 2023). Motivated by its success and the\nrecent advancements in LLMs, we propose to leverage YoloV8's object de-tection capabilities coupled with an Optical Character Recognition (OCR)"}, {"title": null, "content": "module and OpenAI's LLM to create an 3-step approach for GUI instruc-tion grounding able to locate a component on the screen given a natural\nlanguage instruction from the user and a screenshot of the current screen.\nFigure 3 showcases the proposed SICocr architecture. Below, we offer a com-prehensive breakdown of each stage within the SICocr instruction grounding\nmethodology."}, {"title": "3.1.1. List all components in the GUI", "content": "The first step of the approach consists in listing all the elements present\nin the GUI (i.e tab, button, text field, list, etc.). To this end, we propose\nto deploy YoloV8 (Reis et al., 2023) for object detection and fine-tune it for\nGUI objects."}, {"title": null, "content": "In spite of the good accuracy of the object detection module, it still ex-hibits some limitations due to its inability to read text and correctly identify\neach component in an image. For example, it wouldn't be able to distinguish\nbetween the \u201cCancel\u201d button and \u201cSubmit\u201d button and would simply iden-tify them as \u201cButton\u201d. For this reason, we propose to add an OCR module\nto help identify each component on the GUI screen. To this end, we design\nthe OCR module to first \u201cread\u201d all the text present on the screen and return\nthe coordinates of each text. We then match the coordinates of each object\non the screen with the corresponding text. This process allows us to identify\nfor each component its type and role and it finally returns a list containing\nthe Id, type, role, and coordinates of each component in the GUI."}, {"title": "3.1.2. Component name and type extraction", "content": "The second step of the architecture is dedicated to extract information\nabout the target component on the screen from the input user instruction\nwritten in natural language. More specifically, we aim to know the type\nand role of the component we're looking for on the screen. For example,\ngiven the following instruction: \u201cPlease type in john in the name field\u201d, we\nwould have a type corresponding to \u201ctext field\u201d and a role corresponding to\n\"name\". To this end, we resort to prompt engineering using OpenAI's GPT4\nLLM (OpenAI et al., 2024)."}, {"title": "3.1.3. Component-text matching engine", "content": "As described in 3.1.1 and 3.1.2, the two first steps produce the components\ninformation list and information about the target component, respectively.\nConsequently, using the results from the first two steps, the third and last"}, {"title": null, "content": "step is designed to return the coordinates of the component we're looking\nfor. In this regard, we also deploy the capabilities of OpenAI's LLM through\nprompt engineering with the goal of finding the component description in the\nlist that best matches the target component. Once it is identified, it returns\nits Id, type, role, and coordinates on the GUI screen.\nFollowing this 3-step strategy, we are able to achieve instruction ground-ing for the GUI environment. Despite the good performance of this approach,\nit shows some limitations. For instance, it heavily relies on text information\nfor component identification and wouldn't be able to accurately identify im-age components and icons. Furthermore, it calls for the use of OpenAI's\nGPT4 LLM which can present a few issues. Namely data security concerns\nsince it's not an LLM that can be deployed locally as well as potential fi-nancial concerns. To address these issues, we propose in what follows a local\nand more straightforward approach."}, {"title": "3.2. SICdirect", "content": null}, {"title": "3.2.1. Motivation", "content": "Yan et al. (2023b) introduces UNINEXT, a universal instance perception\nmodel demonstrating good performance for different types of perception tasks\nincluding object detection, instance segmentation, multiple object tracking,\nreferring expression comprehension, and referring expression grounding. Fol-lowing their success in instance perception for natural images, an intuitive\nsolution for our challenge is the finetuning of the pre-trained model to fit the\nGUI instruction grounding task."}, {"title": "3.2.2. Global approach description", "content": "Yan et al. (2023b) formulate the instance perception task as a prompt-guided object discovery and retrieval problem, aiming to unify diverse in-stance perception tasks into a singular model architecture and learning parad-igm. The tasks are categorized into three types based on their input prompts:\ncategory names, language expressions, and reference annotations. This cate-gorization facilitates the understanding that all tasks essentially aim to find\nobjects specified by prompts, motivating a unified approach. The model calls\nfor a 3-step architecture: prompt generation, image-prompt feature fusion,\nand object discovery and retrieval. Figure 4 shows the global architecture of\nthe approach."}, {"title": "Prompt Generation :", "content": "This step transforms various types of input\nprompts computer vision into a unified format. It employs language and"}, {"title": null, "content": "reference visual encoders to handle language-related and annotation-guided\ntasks, respectively, generating prompt embeddings that guide the object dis-covery process.\nImage-Prompt Feature Fusion : Concurrently with prompt genera-tion, the current image undergoes processing through a visual encoder to\nobtain hierarchical visual features. An early fusion module then enhances\nthese features with prompt embeddings to enable deep information exchange\nand produce highly discriminative representations for instance prediction.\nObject Discovery and Retrieval : Utilizing enhanced visual and\nprompt representations, this phase employs a Transformer-based architec-ture to generate instance proposals, which are then filtered based on prompt-instance matching scores to retrieve the final object instances. The model\narchitecture facilitates flexibility in handling different types of instances by\nsimply changing the input prompts.\nThis approach allows UNINEXT to train on diverse tasks and domains\nwithout needing task-specific heads, achieving superior performance on var-ious instance perception tasks with a single model. While this universal\nmethodology allows the model better flexibility and visibility, in our work\nwe're interested in the visual grounding task. In this context, we propose to\ntrain the model on GUI data with a focus on language expressions prompting."}, {"title": "4. Experimental settings", "content": null}, {"title": "4.1. SICocr approach", "content": null}, {"title": "4.1.1. Dataset", "content": "We propose to build a training dataset composed of annotated GUI\nscreenshots of multiple websites. We devise the following list of the most rele-vant and frequently occurring targets for object detection training: \u201cButton\u201d,\n\u201cText field\u201d,\u201cText area\u201d, \u201cCheckbox\u201d, \u201cRadio button\u201d, \u201cText\u201d, \u201cLink\u201d.\n\u201cList\u201d, \u201cTab\u201d, \u201cDialog box\u201d, \u201cImage\u201d, \u201cProgress bar\u201d, \u201cToolbar\u201d, and \u201cMenu\nbar\". The final dataset is composed of 1264 images and contains a total of\n102760 training examples. We also adopt an 80-10-10 split for dividing the\ndataset into training, validation, and testing sets, respectively."}, {"title": "4.1.2. Training details", "content": "To optimize object detection performance, the fine-tuning process of YoloV8\nis meticulously undertaken, with a particular focus on hyper-parameter set-tings, including the batch size, learning rate and weight decay. By opting\nfor a batch size of 16, we aim to enhance both computational efficiency\nand the model's learning capabilities. Furthermore, a learning rate of 0.01\nand a weight decay of 0.0005 is chosen to facilitate effective model opti-mization and accelerate the convergence process. This careful selection of\nhyper-parameters is found to not only speed up the learning process but also\nensure ongoing refinement throughout the training phase. We conduct the\ntraining on a A6000 GPU equipped with a 48 GB of GDDR6 memory for a\ntotal of 400 epochs."}, {"title": "4.2. SICdirect approach", "content": null}, {"title": "4.2.1. Dataset", "content": "Regarding the training dataset, we create an extensive web data dataset\ncontaining image-expressions pairs. The dataset comprises a total of 51, 433\npairs. Each pair corresponds to a visual component and a list of expressions\nassociated with it. For example, a submit button could be paired with the\nfollowing expressions list: [\"submit button\u201d, \u201csubmit\u201d, \u201cbutton to submit\u201d].\nWe also divide the dataset into training, validation, and testing sets using\nan 80-15-5 split, respectively. Figure 5 presents an example of an image-expressions pair of the dataset."}, {"title": null, "content": "We categorize the components into 7 categories: Button, Tab, Link, Text\nfield, Checkbox, Radio button, and List. We aimed to achieve a balanced\ndistribution as much as possible, but due to a lack of certain elements, the"}, {"title": null, "content": "dataset remains imperfectly balanced."}, {"title": "4.2.2. Training details", "content": "We conduct the fine-tuning of the model with special care and atten-tion to the configuration of the hyper-parameters, namely the batch size\nand learning rate to achieve optimal performance for expression grounding.\nIndeed, we choose a batch size of 32 to maximize the computational and\nlearning efficiency. Moreover, we select a learning rate of $2e^{-4}$ to ensure\nmodel optimization while boosting convergence speed. We find that this\nhyper-parameter configuration can guarantee not only rapid learning but\nalso continuous refinement during training."}, {"title": "5. Experiments and Results", "content": "In this section, we outline the experimental results obtained for the state-of-the-art method SeeClick (Cheng et al., 2024) as well as SICocr and SICdi-"}, {"title": null, "content": "rect, detailing their performance metrics and comparing their effectiveness\nin our study. SeeClick is a recent instruction grounding approach introduced\nin (Cheng et al., 2024) and it focuses on interest point coordinates predic-tion given a natural language instruction in desktop, web and mobile images.\nModel weights could be found on the Huggingface plateform\u00b9"}, {"title": "5.1. Evaluation dataset", "content": "We propose to evaluate the models using the test split of our dataset. It is\ncomposed of 2464 (image,expression) pairs and each pair can be divided into\n8 categories: 'Button', 'Text field', 'Text area', 'Checkbox', 'Radio button',\n'Link', 'List', and 'Tab'. The table below 2 shows the category distribution\nof the test dataset."}, {"title": "5.2. Metrics", "content": "We propose to evaluate the approaches using two metrics. The first metric\nis Intersection over Union (IoU) (Zhou et al., 2019) that measures the overlap\nbetween the predicted bounding box and the ground truth bounding box.\nThe IoU score ranges from 0 to 1, where 0 means there is no overlap i.e. false\nprediction and 1 indicates perfect overlap i.e. perfect prediction.\nFurthermore, we introduce a new metric that we call Central Point Valida-tion or CPV for short. The design we propose allows us to determine whether\nthe center of the predicted bounding box is inside the ground truth bounding\nbox. This metric is especially relevant for task automation with GUI agents.\nIndeed, any given instruction (click, type, etc..) would be performed on the\nGUI screen by the agent using a virtual keyboard and mouse. Therefore, as\nthe center of the prediction bounding box of the screen component is where\nthe agent would be directed to execute any given instruction (i.e. click on\nthe center), it would be more interesting to verify if this center corresponds\nto a point within the original area of interest i.e. the ground-truth bounding\nbox of the screen component. As the metric returns a boolean value, we are\ninterested to see the percentage of accurate prediction across the test dataset."}, {"title": "5.3. Results and discussion", "content": "In this section we present our experimental results. We first propose to\nmeasure the category-wise performance of each proposed method using the\ntwo previously mentioned metrics IoU and CPV. We then show a comparison\nbetween the evaluation results of SICdirect and SICocr.\nIndeed, Table 3 depicts the category-wise evaluation results of the state-of-the-art method SeeClick (Cheng et al., 2024) as well as the proposed SIC-"}, {"title": null, "content": "ocr and SICdirect approaches. As we can see, SeeClick performs best for\nthe Link category with a 44.69% CPV rate, followed by Button, List, Text\nfield, and Text area categories with 26.01%,15.22%,14.35%, and 12.5 CPV\nrates, respectively. However, it does not perform as well for the rest of the\ncategories. One possible explanation for this disparity is a more pronounced\npresence of text information for the Button, List, Text field, and Text area\ncategories which may help the coordinates prediction accuracy. It is worth\npointing out that we only evaluate SeeClick in terms of the CPV metric due\nto the fact that the method outputs the coordinates of the point of interest\nand not a bounding box. In this case, the IoU metric becomes irrelevant and\nthe CPV metric is determined by verifying whether the predicted point is\ninside the area of the GUI screen component in question.\nFor SICocr, we can see that the model has the highest CPV values for\nthe following categories: text area, tab, link, and list, with a CPV score\nranging from 45.65% to 87.5%. For the mIoU score, the top ranking cate-gories are text area, tab, list, and text field with values between 0.24 and\n0.67, respectively. However, we don't see very good performance for other\ncategories like checkbox, radio button, and button. This could be due to\nthe approach's heavy dependence on text information for grounding which\nmay be lacking for these object types. This limitation leads to a less optimal\noverall performance (0.21 mIoU and 33.93% CPV).\nRegarding the SICdirect, for the case of radio button and checkbox we\nnotice relatively low performance compared to the other categories. Our the-ory is that this is due to the visual similarity between checkboxes or radio\nbuttons that are listed together. In fact, a human would be able to select the"}, {"title": null, "content": "right box by reading the text next to it. So for a model to be able to select\nthe desired check box, it would have to have the ability to read text in an im-age. This is a challenging task that would require a large dataset with images\ncontaining textual information. Overall, we can see that for the CPV metric,\ntab, list, and text area have almost perfect scores ranging from 97.04% to\n100% while the rest of the categories except for radio button and checkbox\nhave very good performance with scores between 71% and 91%. This results\nin a relatively high overall CPV score of 79.3%. Similarly, we observe good\nperformance in terms of mIoU where, with the exception of Radio button and\ncheckbox, the scores range from 0.65 to 0.93 leading to a global mIoU of 0.67.\nOverall, it is clear that SICocr outperforms SeeClick in terms of the CPV\nmetric accross all categories with margins ranging from 1,56% to 75%. How-ever, we can see more significant performance improvements, also across all\ncategories, with the SICdirect approach compared to both SeeClick and SIC-ocr. Indeed, we observe CPV margins between 12.5% and 62.81% compared\nto SICocr. However, compared to SeeClick, the margins become even more\npronounced and range from 26.43% to 87.5%. These evaluation results high-light the effectiveness of our proposed methods SICocr and more significantly\nSICdirect even when compared to the most recent state of the art methods.\nFinally, Table 4 shows the overall evaluation results of both proposed\napproaches as well as the SeeClick (Cheng et al., 2024) approach. As we\ncan clearly see, SICocr outperforms the state of the art method SeeClick on\nthe CPV metric with a 10.55% difference. But more importantly, SICdi-rect shows superior performance to SICocr on both metrics. Specifically,"}, {"title": "6. Conclusion", "content": "This paper focuses on the challenge of making an AI agent interact with\na GUI environment solely based on screenshots. In this respect, we introduce\ntwo novel approaches: SICocr leveraging object detection coupled with Op-tical Character Recognition (OCR), and SICdirect that employs a universal\ninstance perception model for GUI instruction grounding.\nThe experimental results demonstrate the superior performance of SICdi-rect over SICocr in both mean Intersection over Union (mIoU) and the ac-curacy of locating object centers within bounding boxes. Thus showcasing\nits potential for enhancing the capabilities of autonomous GUI agents.\nBy addressing the limitations of current GUI agents that rely on struc-tured data or text-only inputs, this work makes significant strides towards\ncreating more effective and versatile AI-powered GUI agents capable of un-derstanding and interacting with GUIs based on visual data. The success\nof these methods opens new avenues for automating tasks and improving\nefficiency in businesses and public administrations, marking a step forward\nin the field of computer vision, NLP and AI."}]}