{"title": "Graphical user interface agents optimization for visual instruction grounding using multi-modal artificial intelligence systems", "authors": ["Tassnim Dardouri", "Laura Minkova", "Jessica L\u00f3pez Espejel", "Walid Dahhane", "El Hassane Ettifouri"], "abstract": "Most instance perception and image understanding solutions focus mainly on natural images. However, applications for synthetic images, and more specifically, images of Graphical User Interfaces (GUI) remain limited. This hinders the development of autonomous computer-vision-powered Artificial Intelligence (AI) agents. In this work, we present Search Instruction Coordinates or SIC, a multi-modal solution for object identification in a GUI. More precisely, given a natural language instruction and a screenshot of a GUI, SIC locates the coordinates of the component on the screen where the instruction would be executed. To this end, we develop two methods. The first method is a three-part architecture that relies on a combination of a Large Language Model (LLM) and an object detection model. The second approach uses a multi-modal foundation model.", "sections": [{"title": "1. Introduction", "content": "Repetitive tasks refer to activities that require little critical thinking and are regularly repeated within organizations, such as businesses and public administrations. In many cases, these tasks are considered tedious and un-rewarding for the employees who perform them and can lead to monotony and professional exhaustion. For this reason, the design of an autonomous AI-powered Graphical User Interface (GUI) agent to automate these tasks becomes necessary. A key step towards building this agent is making it capable of interacting with the GUI environment. In this context, many solutions use structured data which can be lengthy (e.g., HTML), impractical and even inaccessible (e.g., on desktops). To alleviate this issue, it is necessary to design an Artificial Intelligence (AI) agent that only relies on screenshots for interactions with its GUI environment. Recent development of Large Language Models (LLMs) has opened countless possibilities in automating numerous AI tasks. Transformer-based models such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2018), T5 (Raffel et al., 2019), and GPT-3 (Generative Pre-trained Transformers) (Brown et al., 2020) had great success for Natural Language Processing (NLP) applications namely text generation (Jiang et al., 2023; Touvron et al., 2023), translation (Moslem et al., 2023; Waldendorf et al., 2024; Wu et al., 2024a), summarization (Lewis et al., 2019; Zhang et al., 2020; Takeshita et al., 2022), and so forth.\nSubsequently, the use of transformers was later extended to the computer vision domain where they revolutionized multiple applications such as image classification (Tan and Le, 2019; Kolesnikov et al., 2019; Dosovitskiy et al., 2020; Bao et al., 2021; Radford et al., 2021; Graham et al., 2021; Liu et al., 2021; Jaegle et al., 2021; Liu et al., 2022; Oquab et al., 2024), object detection (Carion et al., 2020; Zhu et al., 2020; Meng et al., 2021; Smock et al., 2021; Fang et al., 2021; Ouyang-Zhang et al., 2022) and object understanding (Yang et al., 2019; Ye et al., 2019; Zhu et al., 2022; Yang et al., 2021) through multi-modal foundation models. An interesting sub-category of object understanding for developing vision capabilities for GUI agents is Referring Expression Grounding (REG) where the key challenge is associating natural language expressions with specific objects or regions in visual data. REG involves understanding and identifying the referred objects or regions based on the contextual information provided by the language, which can include visual attributes, spatial relationships, and interactions with the surrounding environment. This task is pivotal in Natural Language Understanding (NLU) and GUI environment understanding. It bridges the semantic gap between linguistic descriptions and the visual world, enabling machines to comprehend and interact with the environment more effectively. Research conducted by Mao et al. (2015) demonstrated the efficiency of REG in image captioning tasks. Furthermore, Anderson et al. (2017) illustrated its significance in visual question answering while Ye et al. (2019), Yang et al. (2021) and Zhu et al. (2022) leveraged cross-modal learning to segments out an object referred to by a natural language expression from an image. Furthermore, Anderson et al. (2017) illustrated its significance in visual question answering, while Ye et al. (2019); Yang et al. (2021), and Zhu et al. (2022) leveraged cross-modal learning to segment out an object referred to by a natural language expression from an image. Despite the numerous research works in the context of referring to expression grounding, the majority focuses on natural images. Few works studied the REG in the context of GUI instruction grounding. Indeed, research works like Li et al. (2020); Rozanova et al. (2021); Zhang et al. (2023c); Venkatesh et al. (2023), and Cheng et al. (2024) propose solutions allowing the localisation of screen elements based on natural language instructions. This task is especially useful for building autonomous visual GUI agents designed to automate complex tasks on digital devices.\nIn this work we propose to achieve natural language instruction grounding using two methods. The first method uses an OCR-powered instruction grounding module that we name Search Instruction Coordinates with OCR or SICocr. It relies on the finetuning of YoloV8 (Reis et al., 2023), the use of ChatGPT (OpenAI et al., 2024) and an Optical Character Recognition (OCR) module. The second method is more straightforward called Search Instruction Coordinates direct or SICdirect and it relies on the finetuning of the model aimed at universal instance perception proposed in Yan et al. (2023b) for the specific task of GUI instruction grounding."}, {"title": "2. Related works", "content": "Autonomous GUI Navigation : Several earlier research works investigated GUI automation tasks for web applications (Shi et al., 2017; Liu et al., 2018; Gur et al., 2018) and mobile UI applications (Li et al., 2020; Burns et al., 2022; Li and Li, 2023). More recently, the significant advancements in the NLP field, propelled by the emergence of Large Language Models (LLMS) (Touvron et al., 2023; Xu et al., 2024; OpenAI et al., 2024; Sun et al., 2024; Wu et al., 2024b), have shifted the focus toward developing LLM-powered GUI agents. This has become a primary area of interest for many contemporary studies. Indeed, some works proposed to tackle this challenge with a focus on prompt engineering of ChatGPT and ChatGPT-4 for web tasks using in context-learning (Zheng et al., 2024b) and self-refining (Kim et al., 2023). Other research works proposed to train LLMs for more specific tasks. In this context, Deng et al. (2023) introduced a two-stage method designed to identify target elements in a given HTML file, while Gur et al. (2024) used programming to allow interactions with websites. Despite these advancements, LLMs are still limited because of their ability to process text only. To overcome this, recent works proposed a vision-based GUI navigation (Shaw et al., 2023; Zhang and Zhang, 2024; Hong et al., 2023) using GPT-4V (Yan et al., 2023a; Gao et al., 2024). However, they take metadata as input and do not rely on visual data (Zhang et al., 2023a; Zheng et al., 2024a).\nLarge Vision-Language Models : The research interest in processing images and text data simultaneously has been getting more and more attention. In this respect, recent research has seen significant efforts in constructing Large Vision-Language Models (LVLMs) capable of processing both images and text jointly (Liu et al., 2023; Zhu et al., 2023; Ye et al., 2023; Li et al., 2023). Cross-modal vision-language tasks have become possible to perform with the integration of vision encoders with LLMs. Indeed, through contrastive learning and masked data modeling on large-scale image-text pairs techniques, models like CLIP (Radford et al., 2021), ALIGN (Jia et al., 2021), Florence (Yuan et al., 2021), BEIT3 (Wang et al., 2022), and Flamingo (Alayrac et al., 2022) demonstrate impressive zero-shot capabilities on vision-language challenges. Other works like DALL-E (Ramesh et al., 2021, 2022), and Stable Diffusion (Rombach et al., 2021) rely on image- caption pairs training for the generation of image content given textual information. More recently, research works propose to rely on reference language input as well as visual input to perform a multitude of tasks including Referring Expression Comprehension (REC) (Yu et al., 2016; Zhang et al., 2021; Zou et al., 2023), Referring Expression Segmentation (RES) (Yuan et al., 2021; Zhang et al., 2023b, 2021), and Referring Video Object Segmentation (R-VOS) (Bommasani et al., 2021; Touvron et al., 2023; Xu et al., 2016), where the goal is to match objects with given language expressions. Studies have also focused on grounding tasks with LVLMs (Wang et al., 2023; Bai et al., 2023; Chen et al., 2024), such as providing bounding boxes for objects when generating responses (Chen et al., 2023; Peng et al., 2023). However, these efforts primarily addressed natural images and did not explore GUI contexts. In this paper, we investigate natural language instruction grounding in GUI environment and explore its potential for autonomous GUI agents."}, {"title": "3. Proposed approaches", "content": "3.1. SICocr approach\nYoloV8 model (Reis et al., 2023) demonstrated cutting edge performance in object detection and tracking, instance segmentation, image classification and pose estimation tasks (Ken, 2023). Motivated by its success and the recent advancements in LLMs, we propose to leverage YoloV8's object detection capabilities coupled with an Optical Character Recognition (OCR) module and OpenAI's LLM to create an 3-step approach for GUI instruction grounding able to locate a component on the screen given a natural language instruction from the user and a screenshot of the current screen. Below, we offer a comprehensive breakdown of each stage within the SICocr instruction grounding methodology.\n3.1.1. List all components in the GUI\nThe first step of the approach consists in listing all the elements present in the GUI (i.e tab, button, text field, list, etc.). To this end, we propose to deploy YoloV8 (Reis et al., 2023) for object detection and fine-tune it for GUI objects.\nIn spite of the good accuracy of the object detection module, it still exhibits some limitations due to its inability to read text and correctly identify each component in an image. For example, it wouldn't be able to distinguish between the \u201cCancel\u201d button and \u201cSubmit\u201d button and would simply identify them as \u201cButton\u201d. For this reason, we propose to add an OCR module to help identify each component on the GUI screen. To this end, we design the OCR module to first \u201cread\u201d all the text present on the screen and return the coordinates of each text. We then match the coordinates of each object on the screen with the corresponding text. This process allows us to identify for each component its type and role and it finally returns a list containing the Id, type, role, and coordinates of each component in the GUI.\n3.1.2. Component name and type extraction\nThe second step of the architecture is dedicated to extract information about the target component on the screen from the input user instruction written in natural language. More specifically, we aim to know the type and role of the component we're looking for on the screen. For example, given the following instruction: \u201cPlease type in john in the name field\u201d, we would have a type corresponding to \u201ctext field\u201d and a role corresponding to \"name\". To this end, we resort to prompt engineering using OpenAI's GPT4 LLM (OpenAI et al., 2024).\n3.1.3. Component-text matching engine\nAs described in 3.1.1 and 3.1.2, the two first steps produce the components information list and information about the target component, respectively. Consequently, using the results from the first two steps, the third and last step is designed to return the coordinates of the component we're looking for. In this regard, we also deploy the capabilities of OpenAI's LLM through prompt engineering with the goal of finding the component description in the list that best matches the target component. Once it is identified, it returns its Id, type, role, and coordinates on the GUI screen.\nFollowing this 3-step strategy, we are able to achieve instruction grounding for the GUI environment. Despite the good performance of this approach, it shows some limitations. For instance, it heavily relies on text information for component identification and wouldn't be able to accurately identify image components and icons. Furthermore, it calls for the use of OpenAI's GPT4 LLM which can present a few issues. Namely data security concerns since it's not an LLM that can be deployed locally as well as potential financial concerns. To address these issues, we propose in what follows a local and more straightforward approach.\n3.2. SICdirect\n3.2.1. Motivation\nYan et al. (2023b) introduces UNINEXT, a universal instance perception model demonstrating good performance for different types of perception tasks including object detection, instance segmentation, multiple object tracking, referring expression comprehension, and referring expression grounding. Following their success in instance perception for natural images, an intuitive solution for our challenge is the finetuning of the pre-trained model to fit the GUI instruction grounding task.\n3.2.2. Global approach description\nYan et al. (2023b) formulate the instance perception task as a prompt-guided object discovery and retrieval problem, aiming to unify diverse instance perception tasks into a singular model architecture and learning paradigm. The tasks are categorized into three types based on their input prompts: category names, language expressions, and reference annotations. This categorization facilitates the understanding that all tasks essentially aim to find objects specified by prompts, motivating a unified approach. The model calls for a 3-step architecture: prompt generation, image-prompt feature fusion, and object discovery and retrieval.\nPrompt Generation : This step transforms various types of input prompts computer vision into a unified format. It employs language and reference visual encoders to handle language-related and annotation-guided tasks, respectively, generating prompt embeddings that guide the object discovery process.\nImage-Prompt Feature Fusion : Concurrently with prompt generation, the current image undergoes processing through a visual encoder to obtain hierarchical visual features. An early fusion module then enhances these features with prompt embeddings to enable deep information exchange and produce highly discriminative representations for instance prediction.\nObject Discovery and Retrieval : Utilizing enhanced visual and prompt representations, this phase employs a Transformer-based architecture to generate instance proposals, which are then filtered based on prompt-instance matching scores to retrieve the final object instances. The model architecture facilitates flexibility in handling different types of instances by simply changing the input prompts.\nThis approach allows UNINEXT to train on diverse tasks and domains without needing task-specific heads, achieving superior performance on various instance perception tasks with a single model. While this universal methodology allows the model better flexibility and visibility, in our work we're interested in the visual grounding task. In this context, we propose to train the model on GUI data with a focus on language expressions prompting."}, {"title": "4. Experimental settings", "content": "4.1. SICocr approach\n4.1.1. Dataset\nWe propose to build a training dataset composed of annotated GUI screenshots of multiple websites. We devise the following list of the most relevant and frequently occurring targets for object detection training: \u201cButton\u201d, \u201cText field\u201d,\u201cText area\u201d, \u201cCheckbox\u201d, \u201cRadio button\u201d, \u201cText\u201d, \u201cLink\u201d. \u201cList\u201d, \u201cTab\u201d, \u201cDialog box\u201d, \u201cImage\u201d, \u201cProgress bar\u201d, \u201cToolbar\u201d, and \u201cMenu bar\". The final dataset is composed of 1264 images and contains a total of 102760 training examples. We also adopt an 80-10-10 split for dividing the dataset into training, validation, and testing sets, respectively.\n4.1.2. Training details\nTo optimize object detection performance, the fine-tuning process of YoloV8 is meticulously undertaken, with a particular focus on hyper-parameter settings, including the batch size, learning rate and weight decay. By opting for a batch size of 16, we aim to enhance both computational efficiency and the model's learning capabilities. Furthermore, a learning rate of 0.01 and a weight decay of 0.0005 is chosen to facilitate effective model optimization and accelerate the convergence process. This careful selection of hyper-parameters is found to not only speed up the learning process but also ensure ongoing refinement throughout the training phase. We conduct the training on a A6000 GPU equipped with a 48 GB of GDDR6 memory for a total of 400 epochs.\n4.2. SICdirect approach\n4.2.1. Dataset\nRegarding the training dataset, we create an extensive web data dataset containing image-expressions pairs. The dataset comprises a total of 51, 433 pairs. Each pair corresponds to a visual component and a list of expressions associated with it. For example, a submit button could be paired with the following expressions list: [\"submit button\u201d, \u201csubmit\u201d, \u201cbutton to submit\"]. We also divide the dataset into training, validation, and testing sets using an 80-15-5 split, respectively.\n4.2.2. Training details\nWe conduct the fine-tuning of the model with special care and attention to the configuration of the hyper-parameters, namely the batch size and learning rate to achieve optimal performance for expression grounding. Indeed, we choose a batch size of 32 to maximize the computational and learning efficiency. Moreover, we select a learning rate of 2e 4 to ensure model optimization while boosting convergence speed. We find that this hyper-parameter configuration can guarantee not only rapid learning but also continuous refinement during training."}, {"title": "5. Experiments and Results", "content": "In this section, we outline the experimental results obtained for the state-of-the-art method SeeClick (Cheng et al., 2024) as well as SICocr and SICdirect, detailing their performance metrics and comparing their effectiveness in our study. SeeClick is a recent instruction grounding approach introduced in (Cheng et al., 2024) and it focuses on interest point coordinates prediction given a natural language instruction in desktop, web and mobile images. Model weights could be found on the Huggingface plateform\u00b9\n5.1. Evaluation dataset\nWe propose to evaluate the models using the test split of our dataset. It is composed of 2464 (image,expression) pairs and each pair can be divided into 8 categories: 'Button', 'Text field', 'Text area', 'Checkbox', 'Radio button', 'Link', 'List', and 'Tab'.\n5.2. Metrics\nWe propose to evaluate the approaches using two metrics. The first metric is Intersection over Union (IoU) (Zhou et al., 2019) that measures the overlap between the predicted bounding box and the ground truth bounding box. The IoU score ranges from 0 to 1, where 0 means there is no overlap i.e. false prediction and 1 indicates perfect overlap i.e. perfect prediction.\nFurthermore, we introduce a new metric that we call Central Point Validation or CPV for short. The design we propose allows us to determine whether the center of the predicted bounding box is inside the ground truth bounding box. This metric is especially relevant for task automation with GUI agents. Indeed, any given instruction (click, type, etc..) would be performed on the GUI screen by the agent using a virtual keyboard and mouse. Therefore, as the center of the prediction bounding box of the screen component is where the agent would be directed to execute any given instruction (i.e. click on the center), it would be more interesting to verify if this center corresponds to a point within the original area of interest i.e. the ground-truth bounding box of the screen component. As the metric returns a boolean value, we are interested to see the percentage of accurate prediction across the test dataset.\n5.3. Results and discussion\nIn this section we present our experimental results. We first propose to measure the category-wise performance of each proposed method using the two previously mentioned metrics IoU and CPV. We then show a comparison between the evaluation results of SICdirect and SICocr.\nIndeed, it is clear that SICocr outperforms SeeClick in terms of the CPV metric accross all categories with margins ranging from 1,56% to 75%. However, we can see more significant performance improvements, also across all categories, with the SICdirect approach compared to both SeeClick and SIC-ocr. Indeed, we observe CPV margins between 12.5% and 62.81% compared to SICocr. However, compared to SeeClick, the margins become even more pronounced and range from 26.43% to 87.5%. These evaluation results highlight the effectiveness of our proposed methods SICocr and more significantly SICdirect even when compared to the most recent state of the art methods.\nFinally, SICdirect achieves a mIoU of 0.67 compared to SICocr's 0.21. Furthermore, SICdirect demonstrates superior performance in accurately locating the center of objects within the predicted bounding boxes, with 79.3% of object centers correctly identified compared to only 33.93% by SICocr."}, {"title": "6. Conclusion", "content": "This paper focuses on the challenge of making an AI agent interact with a GUI environment solely based on screenshots. In this respect, we introduce two novel approaches: SICocr leveraging object detection coupled with Optical Character Recognition (OCR), and SICdirect that employs a universal instance perception model for GUI instruction grounding.\nThe experimental results demonstrate the superior performance of SICdirect over SICocr in both mean Intersection over Union (mIoU) and the accuracy of locating object centers within bounding boxes. Thus showcasing its potential for enhancing the capabilities of autonomous GUI agents.\nBy addressing the limitations of current GUI agents that rely on structured data or text-only inputs, this work makes significant strides towards creating more effective and versatile AI-powered GUI agents capable of understanding and interacting with GUIs based on visual data. The success of these methods opens new avenues for automating tasks and improving efficiency in businesses and public administrations, marking a step forward in the field of computer vision, NLP and AI."}]}