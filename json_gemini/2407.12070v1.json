{"title": "Co-Designing Binarized Transformer and Hardware Accelerator for Efficient End-to-End Edge Deployment", "authors": ["Yuhao Ji", "Chao Fang", "Shaobo Ma", "Haikuo Shao", "Zhongfeng Wang"], "abstract": "Transformer models have revolutionized AI tasks, but their large size hinders real-world deployment on resource-constrained and latency-critical edge devices. While binarized Transformers offer a promising solution by significantly reducing model size, existing approaches suffer from algorithm-hardware mismatches with limited co-design exploration, leading to suboptimal performance on edge devices. Hence, we propose a co-design method for efficient end-to-end edge deployment of Transformers from three aspects: algorithm, hardware, and joint optimization. First, we propose BMT, a novel hardware-friendly binarized Transformer with optimized quantization methods and components, and we further enhance its model accuracy by leveraging the weighted ternary weight splitting training technique. Second, we develop a streaming processor mixed binarized Transformer accelerator, namely BAT, which is equipped with specialized units and scheduling pipelines for efficient inference of binarized Transformers. Finally, we co-optimize the algorithm and hardware through a design space exploration approach to achieve a global trade-off between accuracy, latency, and robustness for real-world deployments. Experimental results show our co-design achieves up to 2.14~49.37\u00d7 throughput gains and 3.72~88.53\u00d7 better energy efficiency over state-of-the-art Transformer accelerators, enabling efficient end-to-end edge deployment.", "sections": [{"title": "1 INTRODUCTION", "content": "Transformer-based neural networks have experienced a remarkable surge in recent years, from BERT [6], to ViT [7, 31], further to the large language models (LLMs) [2, 42], demonstrating exceptional performance across a diverse range of tasks. However, the dramatic increase in model size and computational complexity has imposed significant constraints, particularly in latency-critical and resource-constrained end-to-end edge scenarios.\nQuantization [17, 19, 30, 40, 43] emerges as a promising solution to alleviate the challenge of Transformer edge deployment, which reduces model size and computational complexity by lowering the bit width of weights or activations. Among them, binarized Transformer [1, 21, 26, 32] stands out, which reduces the bit width of weights to 1-bit and transforms computations to bit-wise operations, minimizing both parameter storage and computational complexity. However, challenges remain on both algorithm and hardware levels for efficient end-to-end edge deployment of binarized Transformers.\nOn the algorithm level, existing binarized Transformers struggle for efficient edge deployment due to hardware-unaware model architectures, theoretical-practical performance gap, and neglected model robustness. 1) Existing binarized Transformers often lack consideration for hardware implementation, posing challenges to the deployment on edge devices with limited resources. For instance, the use of complex activation functions like GELU in BinaryBERT [1] demands significant hardware resources and sophisticated design [37], increasing implementation costs. 2) Binarized Transformers often prioritize theoretical compression ratios, neglecting the impact on hardware performance. For instance, existing works [1, 21, 32] only evaluate the theoretical reduction in model size and FLOPs of their binarized Transformers without validating the actual speedup performance. Consequently, these models may not translate theoretical compression benefits into real-world performance improvements on edge devices. 3) While accuracy has improved, the robustness of binarized Transformers remains hardly explored, which is crucial for ensuring reliable performance in diverse and challenging environments, especially at the edge where data quality may be various and unpredictable.\nOn the hardware level, current accelerator designs [9-11, 18, 20, 23, 25, 29, 36, 39] fall short in fully enabling efficient end-to-end edge deployment of binarized Transformers due to limitations in constrained computational support, architectural inefficiencies, and suboptimal hardware configurations. 1) Existing accelerators primarily focus on high-bit quantized models, neglecting the unique requirements of binarized Transformers, resulting in a lack of support for binarized acceleration. Furthermore, the crucial hardware implementation of the quantization process itself remains unexplored, preventing an end-to-end acceleration. 2) Existing accelerator architectures fall into two categories: streaming-like [11, 16, 18, 24, 34, 36] and processor-like [10, 14, 20, 23, 25, 27, 33, 39, 41], but neither offers an ideal solution for edge deployment with binarized Transformers. Streaming-like architectures, optimized for efficiency with large batches, face underutilization issues when dealing with the smaller batch sizes typically encountered at the edge. Conversely, processor-like architectures, designed for versatility, introduce overhead with complex datapaths and scheduling for intricate Transformer operations. 3) Additionally, the hardware configurations of existing accelerators often rely on empirical values rather than a systematic exploration, which contributes to the suboptimal performance observed in practice. Consequently, the resulting accelerators fail to operate at their full potential, hindering efficient edge deployment.\nTo facilitate efficient end-to-end edge deployment, we propose a co-design approach within binarized Transformers and hardware accelerator that addresses the above challenges from three aspects. From algorithm aspects, we introduce BMT, a novel binarized Transformer model that incorporates hardware-aware quantization methods and computational components, specifically designed for efficient execution on edge devices. Additionally, we develop weighted ternary weight splitting (WTWS) to optimize the training process of BMT. From hardware aspects, to fully exploit the practical acceleration of binarized Transformer, we design BAT, a novel streaming-processor-mixed binarized Transformer accelerator, equipped with highly optimized computational units and scheduling pipelines to enable an efficient end-to-end inference for binarized Transformers. From joint optimization aspects, we further push the performance boundaries by co-optimizing the binarized Transformers and BAT through a design space exploration (DSE) approach, allowing us to find a global trade-off between accuracy, latency, and robustness for real-world deployments.\nThe contributions of this work are summarized as follows:\n\u2022 A hardware-friendly binarized Transformer, BMT, which yields a substantial compression ratio compared to the full precision baseline with comparable accuracy, exhibiting significant potential for edge deployment. (Section 3)\n\u2022 A streaming-processor-mixed binarized Transformer accelerator, BAT, enabling high efficiency and low power end-to-end acceleration of binarized Transformers. (Section 4)\n\u2022 Dataflow optimizations adaptable for edge scenarios to improve the hardware efficiency, and a DSE approach to jointly optimize both algorithmic and hardware parameters, striking a fine balance between accuracy, robustness, and latency. (Section 5)\n\u2022 Comprehensive experiments showing our co-design achieves up to 2.14~49.37\u00d7 and 3.72~88.53\u00d7 improvement on throughput and energy efficiency over SOTA Transformer accelerators and improves energy efficiency by 213.82\u00d7 and 174.01\u00d7 compared to the CPU and GPU implementations. (Section 6)"}, {"title": "2 BACKGROUND", "content": "Fig. 1 illustrates the architecture of the encoder-based Transformer network, where each encoder consists of a multi-head attention module (MHA) and a feed-forward network (FFN). Residual addition and layer normalization (LN) are used before and after FFN. Within each MHA, the input (A) is projected to query (Q), key (K) and value (V) matrices through three different linear layers ($\\mathbf{A}_{W}$). The query matrix is then multiplied with $\\mathbf{K}^{T}$, and the scaled product ($\\mathbf{Q} \\mathbf{K}^{T}$) is processed by a softmax module to produce the score matrix (S). This matrix S is then multiplied with V and the product ($\\mathbf{S} \\mathbf{V}$) is passed through an additional linear layer ($\\mathbf{B}_{W}$) to generate the final output of the MHA. In FFN, the input is first projected into an intermediate matrix (R) through a linear layer ($\\mathbf{C}_{W}$), followed by the activation function. The activated result ($R_{1}$) is then subjected to another linear transformation ($R_{1}W$) to obtain the output of FFN. Compared to the vanilla Transformer, the quantized Transformer incorporates quantization processes, which quantizes weights and activations to the designed bit-width, and matrix multiplications are transformed to quantized matrix multiplications (QMMs).\nBinarization, which reduces the bit width to 1-bit, has been studied for a long time [1, 15, 21, 22, 26, 28, 32, 38]. A notable work, Binary-Weight-Network (BWN) [15], binarizes full precision weight $W_{R}$ with a scaling parameter $\\alpha$ according to the following equation:\n$\\text{BWN}(W_R) = \\alpha W_B, \\qquad(1)$\nwhere $W_B = \\text{Sign}(W_R)$, $\\alpha = \\frac{||W_R||_1}{N}$, N is the total number of elements of $W_R$."}, {"title": "3 ALGORITHM OPTIMIZATION", "content": "Limited resources in edge devices require hardware-aware design for binarized models. Existing models [1, 21, 26, 32] lack such considerations, hindering efficient deployment. Our binarized Transformer, namely BMT, addresses this with hardware-optimized techniques.\nElastic Activation Quantization. Existing activation quantization methods often incur high hardware resource and latency overhead due to extra scaling factor computations [5] during implementation on dedicated accelerators. To overcome this issue, we incorporate hardware-friendly elastic quantization [21] method for activations when designing our BMT, making it exhibit outstanding performance in the extremely low-bit scenario. The equation can be formulated as follows:\n$X_{\\text{INT}} = \\alpha \\left[ \\text{clip}\\left( \\frac{X_R + \\beta}{\\alpha}, Q_n, Q_p \\right) \\right], \\qquad(2)$\nwhere [] represents the rounding function. $X_R$ is a full precision tensor and $\\alpha$ is the full precision scaling factor. $Q_n/Q_p$ is the min/max value of the quantization range.\nBMT benefits from several aspects by using hardware-friendly quantization. First, both coefficients $\\alpha$ and bias $\\beta$ of BMT are learnable parameters, but fixed before deployment, reducing computational burden during inference. Second, the pre-computed reciprocal of $\\alpha$ and the standardized values of $Q_n$ and $Q_p$ in BMT further streamline online inference by eliminating complex operations and simplifying the clip function implementation.\nPrioritizing hardware efficiency, BMT employs the simple ReLU activation function instead of the computationally expensive GELU function commonly used in binarized Transformers. ReLU requires minimal hardware resources, as it can be implemented with a single multiplexer, making it ideal for resource-constrained edge devices."}, {"title": "3.2 Weighted Ternary Weight Splitting", "content": "To compensate for potential accuracy loss from hardware-oriented design choices, BMT introduces a novel technique called weighted ternary weight splitting (WTWS). Unlike prior techniques like TWS in BinaryBERT [1], WTWS leverages trainable coefficients during the splitting process. These coefficients capture the importance of each splitted weight, leading to higher accuracy.\nAs shown in Fig. 3, TWS starts from training a half-sized ternary Transformer model, and then applies ternary weight splitting operator on its linear layers to obtain the full-sized latent binarized Transformer model, which quantizes weights to {-1, 1}. The equations are as follows:\n$\\begin{aligned} \\hat{W} &= \\begin{cases} a \\cdot W, & \\text{if } W \\neq 0, \\\\ -b + W, & \\text{if } W = 0, W > 0, \\\\ b, & \\text{otherwise}, \\end{cases} \\qquad(3) \\end{aligned}$\nwhere $a$ and $b$ are defined as follows:\n$a = \\frac{\\sum_{i \\in I} |W| - \\sum_{j \\in J} W - \\sum_{k \\in K} W}{2 \\sum_{i \\in I} |W|}, b = \\frac{\\sum_{i \\in I} |W| - \\sum_{j \\in J} W - \\sum_{k \\in K} W}{2(|J| + |K|)}, \\qquad(4)$\nwhere $I = \\{i | W \\neq 0\\}$, $J = \\{j | W = 0 \\text{ and } W > 0\\}$, $K = \\{k | W = 0 \\text{ and } W < 0\\}$. $|\\cdot|$ denotes the cardinality of the set. The equation above ensures the result of the linear layer remains the same, i.e. $W_t = \\hat{W}_p + \\hat{W}_b$.\nHowever, simply adding the split weights in TWS may discard valuable information. For example, when $\\hat{w}_p >> \\hat{w}_b$, $\\hat{w}_t \\approx \\hat{w}_p$ and the information of $\\hat{w}_b$ may be lost. To alleviate this problem, WTWS introduces trainable coefficients $\\sigma_1$ and $\\sigma_2$ to learn the significance of $\\hat{W}_p$ and $\\hat{W}_b$ respectively, reformulating the equation to $Y = X(\\hat{W}_p \\cdot \\sigma_1 + \\hat{W}_b \\cdot \\sigma_2)$. As shown in Fig. 3, WTWS introduces an additional layer highlighted in orange to facilitate this process. Both $\\sigma_1$ and $\\sigma_2$ are initialized as all-ones vectors and are column-wise trainable."}, {"title": "4 HARDWARE ACCELERATOR", "content": "Considering the disadvantages of primitive streaming or processor-like accelerators, our proposed binarized Transformer accelerator, namely BAT, is characterized by a streaming-processor-mixed architecture. On the one hand, the MHA and FFN are executed separately in a pipelined manner following a streaming-like design. On the other hand, the architectures within MHA and FFN modules are designed as a processor-like paradigm."}, {"title": "4.1 Architecture Overview", "content": "Fig. 4 shows the architecture of BAT, which is composed of a MHA module, a FFN module, a direct memory access (DMA) engine, an external memory, and several on-chip buffers. Both the MHA and FFN modules are equipped with a QMM engine, a vector unit (VU), a elastic quantization unit (QU) and a layer normalization unit. The MHA module includes a softmax unit while the FFN module features a ReLU activation unit. The individual design of MHA and FFN modules significantly simplifies the control logic and datapath compared to a simple processor-like architecture where the MHA and FFN are processed within a unified architecture [4, 10, 23].\nIn both MHA and FFN modules, the dominant operations, QMMs, are performed by dot-product-based QMM engine which can flexibly adapt to different matrix multiplication sizes with high efficiency. VU handles operations with low computational density including dequantization and residual addition with vectorized inputs, and QU is designed for elastic quantization. To ease the on-chip memory consumption and avoid the overhead associated with pattern matching between the outputs of QU and the inputs of QMM engine, these intermediate results are temporarily transferred back to the external memory. Despite increasing the bandwidth requirement, it is a strategic trade-off that conserves valuable on-chip resources. Besides, ping-pong buffer technique is also implemented for all the on-chip buffers to overlap the time between data transfer and computation."}, {"title": "4.2 Quantized Matrix Multiplication engine", "content": "Processor-like architectures hinge on the core computation engine for overall performance, necessitating a QMM engine designed for high throughput and flexibility to support diverse QMMs. Fig. 4 shows the hardware architecture of QMM engine, which consists of $P_{dpu}$ number of dot product units (DPUs) with controllers including a finite state machine (FSM) and an address generator.\nAs mentioned in Section 2.2, there are two types of QMMs in the quantized Transformer: (a) activation\u00d7weight and (b) activation\u00d7 activation. Fig. 5 (a) and (b) illustrates the unified data access pattern of QMM engine for these two QMMs: (a) The weight matrix are partitioned into $p_{dpu}$ tiles along the column direction, with each tile being fed into the corresponding DPU. Concurrently, the activation matrix is multicast to each DPU. (b) QMM follows the multi-head mechanism, both activation matrices are partitioned into numhead tiles along the column direction and are then unicast to their respective DPU. When numhead and $P_{dpu}$ are not exactly matched, the numhead tiles are batched (numhead > $P_{dpu}$) or divided along the row dimension (numhead < $P_{dpu}$). Notably, the MHA's QMM engine is designed to incorporate both patterns while the FFN's QMM engine only needs pattern (a), as the FFN contains only the activation \u00d7 weight QMMs.\nFig. 5 (c) elaborates on the structure of DPU. Each DPU is composed of ppe-parallel processing element units (PEs), followed by a compressor tree loop. For high throughput, the DPU executes vector dot products in an unfolding factor of ppe, processing ppe elements per vector simultaneously. The compressor tree loop accumulates PE outputs in each iteration. The compressor tree is constructed based on 4:2 compressor, which mitigates the delay of carry chain propagation and thus optimizes the combinational delay."}, {"title": "4.3 Processing Element Unit", "content": "Fig. 6 (a) depicts the detailed architecture of PE for a multiplication $x \\times y$. TABLE 2 summarizes the operands assigned to PE for different QMMs in binarized Transformer. To improve hardware utilization, PE is implemented with a bit-serial design. Each cycle, PE generates an N-bit \u00d7 1-bit partial product. Consequently, it accomplishes N-bit activation \u00d7 binarized weight and N-bit \u00d7 N-bit activation multiplications with 1 cycle and N cycles, respectively, maximizing the hardware utilization.\nFormally, the decimal value of binary encoded y can be obtained through $\\sum_{i=0}^{N_y-1} y_i 2^i$, where the bit value of $y_i$ is either 1, 0, \u22121. Based on this, we develop a bit decoder to decode $y_i$ into its real bit value (1: 2'b01, 0: 2'b00, -1: 2'b11) according to its data config, which can be implemented as a 2-bit output look-up table (LUT).\nFig. 7 further showcases the $x \\times y = x \\times \\{\\dots Y_2 Y_1 Y_0\\}$ computation process of PE with SBE when $N_x$ = 4. First cycle: $y_0$ = 0, the value of partial product $x \\times y_0$ is (0000)$_2$ and sn is 1. Therefore, $p$ = (10000)$_2$ after SBE, which is fed into the adder together with the initial product 1. The partial sum (psum) is then loaded into the right shift register. Second cycle: $y_1$ = 1, the value of $x \\times y_1$ is x. Following the signed top case or the unsigned case, $p_i$ is selected through the multiplexers. Third cycle: $y_2$ = -1, the partial product is -x, which can be reformulated to $\\overline{x} + 1$. Term $\\overline{x}$ undergos the SBE transformation and the bit value 1 is directed to the cin port of the adder. Through SBE, PE unifies signed and unsigned QMM operations. Also, since there is no sign extension involved, PE is implemented as a right-shifting sequential multiplier, which requires a smaller adder and prevents long carry propagation compared to a left-shifting design, thereby saving the hardware resource and reducing the path delay [3]."}, {"title": "4.4 Elastic Quantization Unit", "content": "Fig. 8 presents our fully pipelined elastic quantization unit. It consists of a floating-point bias adder, a floating-point coefficient multiplier, a converter from floating-point to integer, and a clip unit. For specific clipping ranges like signed or unsigned numbers, the clip unit logic can be significantly simplified, as shown in Fig. 8, using a combination of bit-wise operations and selection mechanisms. For instance, consider clipping an 16-bit signed activation to a 4-bit signed integer. The first step is to check if the most 13 significant bits are identical. Identical bits indicate that the input falls within the representable range of a 4-bit signed number and can be directly output. Conversely, non-identical high bits suggest the input is outside this range. In this case, the sign bit is checked and the output is set to 2$^{(4-1)}$ - 1 if the sign bit is 0, or -2$^{(4-1)}$ if the sign bit is 1. The logic for clipping unsigned integers is similar."}, {"title": "5 SCHEDULING AND CO-OPTIMIZATION", "content": "We propose two dataflow optimization methods, inter-layer pipeline, and intra-layer pipeline, for the streaming-like and processor-like design in BAT, respectively, enhancing the processing efficiency."}, {"title": "5.1 Dataflow Optimization", "content": "Inter-Layer Pipeline. We propose an inter-layer pipeline to fully utilize the benefits of the streaming-like design with individual computational modules of MHA and FFN. As shown in Fig. 9 (a), in the first stage, the MHA module loads input data and performs computations for sample 1. Once this process is finished, the MHA module proceeds to compute the next sample in the second stage, while the FFN module handles the computations for the output results generated by the MHA module in the first stage. In the third stage, the FFN module sends the output activations to the MHA module for the computations of layer 2 in sample 1. This alternating process enables the MHA and FFN modules to perform computations for multiple layers in both samples without encountering pipeline bubbles. Besides, as we set the batch size to 2 in this case, it is suitable for the mini-batch processing in edge scenarios. The overall latency can be given by:\n$T_{total} = T_{MHA} + (L - 1) \\times \\text{max}(T_{MHA}, T_{FFN}) + T_{FFN}, \\qquad(5)$\nwhere $L$ is the number of model layers.\nIntra-Layer Pipeline. Given that both processor-like MHA and FFN modules operate with a quantization-dequantization scheme, we introduce an intra-layer pipeline in a row-by-row execution manner to maximize computation overlap. For example, the operations within QMM and softmax units are conducted in integer and floating-point arithmetic, respectively. As shown in Fig. 9(b), once the softmax unit completes the computation of the first row in an input tensor, the results are immediately fed to the quantization unit. By employing our proposed intra-layer pipeline, BAT achieves a noticeable reduction in overall execution latency compared to the non-pipelined implementation with the elimination of waiting time for continuous processing. Additionally, this method allows for buffering data in certain rows instead of the entire intermediate data, leading to efficient utilization of storage resources."}, {"title": "5.2 Algorithm and Hardware Co-Optimization", "content": "The overall design space of our end-to-end system is formed by the hyperparameters of binarized Transformers and BAT, consisting of algorithmic parameters and hardware parameters in MHA and FFN modules, as presented in TABLE 3. Since the softmax operation constitutes a relatively small fraction of the total computational process, we set the parallelism of the softmax module to 4 without further exploration.\nTo achieve a globally optimized design, we assess the network deployment performance for each design variable, including accuracy, robustness, latency, and resource consumption. Specifically, the accuracy is determined by training and evaluating the binarized Transformer. In terms of robustness, we introduce perturbation $\\Delta x$ in the embedding layer using Gaussian noise, with a variance set to 0.01 and magnitude set to 10% of the magnitude of its original output, similar to steps in [32] and [13]. After repeating the inference 20 times, we obtain an accuracy array A. We define the robustness as $\\frac{A_0}{\\sqrt{\\frac{1}{20} \\sum_{i=1}^{20} (A_i - A_0)^2}}$, where $A_0$ represents the reported best accuracy without perturbation. The end-to-end latency is derived from a performance model developed for our BAT, which is cross-validated through RTL simulation results, taking into account memory accesses to external memory. Hardware resource consumption is obtained after synthesis to ensure the accelerator can fit within the target FPGA device. Finally, we employ an exhaustive grid search to explore all the design points under specific constraints, such as the minimal accuracy demand, and identify the Pareto-optimal set."}, {"title": "6 EVALUATION", "content": "We follow the experimental setting of BinaryBERT [1] and BiT [21] and use the pre-trained BERT-base as our full precision baseline. The algorithmic performance is evaluated on the development set of GLUE [35], a widely adopted benchmark across a diverse set of language understanding tasks.\nWe train and test our BMT using PyTorch v1.10.1. Following BinaryBERT [1], we obtain the half-sized full precision models from DynaBERT [12]. We adopt LSQ quantization [8] for the half-sized ternary model and elastic quantization for the full-sized binarized model. Nvidia RTX 3090 GPU is used for training. Following BinaryBERT, the hidden dimension size and intermediate dimension size are set to 384 and 1536, respectively for our BMT, termed full-size.\nWe implement our BAT hardware accelerator using SystemVerilog on Xilinx ZCU102 FPGA board and evaluate it for BinaryBERT [1], BiT [21] and our BMT on the MRPC task in the GLUE benchmark to obtain the end-to-end latency. The running frequency is set to 200 MHz, and Xilinx Vivado 2022.2 is used for synthesis and implementation. Power consumption values are obtained using the Xilinx Power Estimator (XPE) tool. We use 16-bit half-precision floating-point (FP16) numbers to process the full precision operations in the quantization-dequantization computational flow of BMT to ensure model accuracy."}, {"title": "6.2 Algorithmic Performance", "content": "We compare our BMT with five existing quantized Transformer models: Q-BERT [30], Q2BERT [40], TernaryBERT [43], Binary-BERT [1], and BiT [21], as well as the full precision pre-trained BERT as the baseline. TABLE 4 presents the main results on the GLUE benchmark. Compared with SOTA binarized Transformers, our proposed BMT improves the average performance by 1.0% and 1.1% in W1A4 and W1A2 quantization configuration, respectively. Notably, compared to the full-precision baseline, our W1A4 BMT retains competitive performance with a significant reduction of model size and computation up to 25\u00d7 and 15\u00d7, respectively, demonstrating significant acceleration potential for edge deployment."}, {"title": "6.3 Effectiveness of Co-optimization", "content": "We then evaluate the effectiveness of our co-design method in finding the Pareto-optimal configurations in both algorithm and hardware aspects. The MRPC dataset is used for demonstration purposes. The design space is presented in Table 3, and we constrain the total hardware resource consumption to be less than 80% of the available FPGA resources to facilitate implementation.\nFig. 10 illustrates the trade-off between accuracy, robustness, and latency. The red circles indicate the Pareto optimal points. The brown dashed line represents the accuracy constraint, set at a maximum loss of 1% compared to the full-precision BERT-base model. We also set the robustness constraint of exceeding the upper quartile. Among Pareto optimal points satisfying the constraints, we select the one with the lowest latency. The results demonstrate that our co-designed solution achieves up to 10% higher accuracy than other design points within the same latency range and is 2.1x faster than those with similar accuracy, all while maintaining comparable robustness. The resulting optimal configurations for the MRPC dataset are:\n\u2022 <model type, $d_{hid}$, $d_{inter}$, $b_{act}$> = <BMT, 384, 1536, 4>;\n\u2022 <$P_{dpu}$, $P_{quan}$, $\\Phi_{vu}$, $P_{ln}$> = <16, 128, 32, 8> for MHA module;\n\u2022 <$P_{dpu}$, $P_{quan}$, $\\Phi_{vu}$, $P_{ln}$> = <16, 128, 96, 8> for FFN module.\nThese configurations will be used for the evaluation in the remaining sections unless otherwise specified.\nWhile the selected design point is approximately 10.1\u00d7 slower than the one with the absolute lowest latency, it adheres to the crucial constraints of accuracy and robustness, which are essential for real-world deployments. This global balance ensures that our system not only performs efficiently but also maintains high standards of reliability and accuracy."}, {"title": "6.4 Hardware Evaluation", "content": "To explore the energy efficiency benefits of binarized Transformers, we choose 10 commonly quantized network configurations and deploy their corresponding activation \u00d7 weight QMM on our QMM Engine. The size of QMM is set to (128,768) \u00d7 (768, 768). As shown in Fig. 11, the binarized Transformer configuration, where weights are quantized to 1-bit, achieves significantly lower energy consumption compared to other configurations by 2~8\u00d7 under the same activation precision."}, {"title": "6.4.2 Hardware Consumption", "content": "TABLE 6 presents the FPGA resource consumption and power breakdown of BAT. Due to its bit-wise operations, the QMM Engine is not efficiently mapped to DSP units and primarily relies on LUT resources. Notably, the QMM Engine consumes a modest 10.90% of the total power consumption, highlighting the effectiveness of our optimization for matrix multiplication. The DSP usage within the Quantization Unit stems from the floating-point bias adders and coefficient multipliers. Other components, mainly consisting of floating-point operations and on-chip buffer, consume 69.19% of the total power consumption, indicating a potential bottleneck for further energy optimization."}, {"title": "6.4.3 Overall System Evaluation", "content": "TABLE 5 compares our BAT with SOTA FPGA-based end-to-end Transformer accelerators, as well as commercial CPU and GPU products, in terms of throughput and energy consumption. To adapt to the mini-batch feature at the edge, the batch size on CPU and GPU is set to 2. The quantization-dequantization scheme is adopted on GPU, where the QMM is implemented in high-performance CUDA kernel. Our proposed BAT significantly outperforms other FPGA-based accelerators, achieving 2.14~49.37\u00d7 improvement in throughput and 3.72~88.53\u00d7 improvement in energy efficiency. Compared to CPU and GPU implementations, BAT demonstrates up to 7.54x and 2.72\u00d7 speedup, and 213.82x and 174.01\u00d7 improvement in energy efficiency, respectively."}, {"title": "6.5 Ablation Study", "content": "Fig. 12 evaluates the contributions of our algorithm and hardware design via an ablation study. We first evaluate both BERT-base and BMT under three configurations, i.e. half-size, full-size, and double-size on the NVIDIA RTX 3090 GPU to validate the performance improvement brought by our algorithm. The results demonstrate a 1.29~1.62\u00d7 speedup and energy savings. We further evaluate the BMT model on our BAT accelerator under the same configuration. Compared to the optimized GPU implementation, BAT achieves a 2.58~2.80\u00d7 speedup and a 166.76~180.83\u00d7 energy reduction. These results demonstrate the effectiveness of our co-design approach."}, {"title": "7 CONCLUSION", "content": "The paper proposes the end-to-end acceleration of binarized Transformer via algorithm-hardware co-design enabling efficient edge deployment. From algorithm aspects, we propose BMT, a hardware-friendly binarized Transformer model that achieves a substantial compression ratio while maintaining high accuracy compared to the full precision baseline. From hardware aspects, we propose BAT, a streaming-processor-mixed binarized Transformer accelerator equipped with highly optimized computational units and dataflows, which enables efficient end-to-end inference for binarized Transformers. Moreover, algorithm and hardware design parameters are jointly optimized to push the performance boundaries under real-world constraints from accuracy, latency, and robustness. Experimental results show our co-design yields up to 2.14~49.37\u00d7 3.72~88.53\u00d7 improvement on throughput and energy efficiency, respectively, over the state-of-the-art Transformer accelerators."}]}