{"title": "Symmetry Breaking in Neural Network Optimization: Insights from Input Dimension Expansion", "authors": ["Jun-Jie Zhang", "Nan Cheng", "Fu-Peng Li", "Xiu-Cheng Wang", "Jian-Nan Chen", "Long-Gang Pang", "Deyu Meng"], "abstract": "Understanding the mechanisms behind neural network optimization is crucial for improving network design and performance. While various optimization techniques have been developed, a comprehensive understanding of the underlying principles that govern these techniques remains elusive. Specifically, the role of symmetry breaking, a fundamental concept in physics, has not been fully explored in neural network optimization. This gap in knowledge limits our ability to design networks that are both efficient and effective. Here, we propose the symmetry breaking hypothesis to elucidate the significance of symmetry breaking in enhancing neural network optimization. We demonstrate that a simple input expansion can significantly improve network performance across various tasks, and we show that this improvement can be attributed to the underlying symmetry breaking mechanism. We further develop a metric to quantify the degree of symmetry breaking in neural networks, providing a practical approach to evaluate and guide network design. Our findings confirm that symmetry breaking is a fundamental principle that underpins various optimization techniques, including dropout, batch normalization, and equivariance. By quantifying the degree of symmetry breaking, our work offers a practical technique for performance enhancement and a metric to guide network design without the need for complete datasets and extensive training processes.", "sections": [{"title": "1 Introduction", "content": "Artificial intelligence (AI) has seen remarkable advancements over the past decade, particularly in neural networks, which have revolutionized fields such as computer vision, natural language processing, and autonomous systems [1-5]. These models have also made significant strides in scientific computing and creative domains, enhancing areas like healthcare diagnostics, weather forecasting, and art generation [6-10].\nDespite these successes, the underlying mechanisms of these complex models remain poorly understood [11]. This lack of interpretability often forces researchers to rely on trial-and-error methods to improve model performance [12]. As neural networks grow in size and complexity, they become increasingly difficult to analyze and understand. The high dimensionality and intricate architectures of these networks pose significant challenges for theoretical analysis. With the continuous growth in data and network parameters, achieving a deeper understanding of these models becomes both more difficult and more crucial [13].\nGiven the complexity and expense of directly modeling and analyzing neural networks, it is worth considering more efficient approaches inspired by first principles in scientific fields. One effective strategy is principle-based modeling, which leverages foundational principles from various disciplines to guide the development and optimization of neural networks [14].\nA particularly promising principle is symmetry breaking-a concept deeply rooted in physics that describes how a system that is initially symmetric becomes asymmetric under certain conditions, leading to new and often more stable configurations [15, 16]. In the context of neural networks, symmetry breaking [17, 18] can be thought of as a mechanism that helps the model escape from local minima and saddle points in the loss landscape [19, 20], thereby facilitating better optimization and generalization."}, {"title": "2 Observation: Input Expansion Brings Performance Enhancement", "content": "In this study, we propose treating symmetry breaking as a fundamental principle and describe its application in neural network optimization. Our investigation began with an intriguing empirical observation: expanding the dimensionality of input pixels and filling the expanded dimensions with constant values significantly improves model performance across various datasets and tasks, including image classification, Physics-Informed Neural Networks (PINNs) [21], image coloring, and sentiment analysis.\nTo understand this phenomenon, we propose the hypothesis of symmetry breaking and claim it to be a fundamental principle in neural network optimization. Our findings indicate that augmenting the input space with additional dimensions of constant values mimics a symmetry breaking process. This augmentation proves advantageous for network training, as it facilitates smoother transitions in parameter values during gradient descent.\nTo further test and validate the symmetry breaking hypothesis, we conducted a series of experiments comparing the loss landscapes of models trained with different techniques, including equivariance [22], dropout [23], and batch normalization [24]. Our findings suggest that these techniques all adhere to the symmetry breaking principle in neural network optimization. Moreover, we discovered that embedding equivariance into the network design is a more effective approach than merely scaling up the network, highlighting a trend that deviates from the current focus on scaling laws.\nFinally, inspired by the \"Parasi\" scheme, which is used as a metric to measure the \"replica symmetry\" of spin-glass systems [25, 26], we developed a procedure to measure the \"degree of symmetry breaking\" for the neural network without the need for complete datasets and extensive training processes. This measurement could serve as a metric to evaluate neural network designs.\nOur work advances the theoretical understanding of neural network optimization and provides practical tools and methodologies that can be widely applied to improve AI systems across various domains."}, {"title": "2.1 Impact of Dimension Expansion on Image Classification", "content": "We begin by analyzing image classification performance across various datasets, focusing on models primarily based on Convolutional Neural Network (CNN) architectures. Specifically, we compared the performance of models when fed with original data versus data that had undergone dimension expansion. The datasets used in our experiments included CIFAR-10, CIFAR-100\u00b9, and ImageNet-R, ImageNet-100\u00b2. To ensure a comprehensive evaluation, we employed a range of network architectures, including ResNet-18 and ResNet-50 [2] from the classic residual networks, DenseNet-121 [27] for deeper network analysis, and EfficientNet [28] and MobileNet-v3 [29] to assess the impact on lightweight quantized networks. All network architectures adhered to"}, {"title": "2.2 Impact of Dimension Expansion on AI4Science", "content": "To demonstrate the effects of dimension expansion in the intersection field of AI and scientific discovery, known as \"AI4Science\" and \"Science4AI\", we performed examples based on PINNS a hybrid approach that combines data-driven and physics-driven methods.\nData-Driven task Equation of State in Quantum Chromodynamics. Quantum Chromodynamics (QCD) [35, 36] is the fundamental theory that describes the strong interaction between quarks and gluons. One of the key challenges in QCD is to understand the equation of state (EoS) of the hot and dense QCD matter, which is crucial for explaining phenomena such as the early universe and the heavy-ion collisions [37-39]. Presently, for the hot and dense quark-gluon matter, the QCD EOS can only be obtained by Lattice QCD [40, 41] calculations based on first principles, which requires huge computational effort.\nHere, we developed a quasi-particle model to reproduce the QCD EoS at zero baryon chemical potential. Our approach utilizes three neural networks to represent the temperature-dependent masses of quasi-quarks and quasi-gluons.\nThe neural networks take temperature T as input and masses as outputs. Given these masses, the physical quantities of pressure and energy density can be obtained. Limited by the huge computational cost, we only have 50 points of pressure and energy density from Lattice QCD, hence the task is a date-driven task with 50 Lattice QCD points.\nFig. 3 shows the results for both expanded and unexpanded input dimensions of the neural network. Here, \"unexpanded\" refers to using T as the input, while \"expanded\" refers to using (T,Tc) as the input, with Tc = 0.155 GeV. We can see that the"}, {"title": "2.3 Impact of Dimension Expansion on Other Tasks", "content": "Image Coloring. In the task of image coloring, where the goal is to colorize black-and-white images, we used a generative model based on conditional diffusion to restore grayscale images to color RGB maps with a human-face-based CelebA dataset [43]. By both expanding the input dimension of the noisy image, which is used to denoise as a color image, and the grayscale image, which is used as the prompt of the diffusion, we observed a modest improvement in performance. Specifically, the mean absolute error (MAE) of the colorization model was reduced from 8.7 \u00d7 10\u22123 to 8.3 \u00d7 10-3.\nSentiment Analysis. For sentiment analysis tasks using the IMDB dataset [44], we explored the impact of expanding input dimensions on model performance. We modified a BERT-based model [45] by doubling the embedding dimension through an additional linear layer, then mapping it back to the original size to maintain compatibility with the Transformer architecture. This approach allowed us to test the effect of increased input dimensions without altering the overall model structure.\nThe results showed a modest improvement in accuracy, increasing from 83.40% to 83.67%, highlighting the potential benefits of input dimension expansion in enhancing the performance of sentiment analysis models."}, {"title": "3 Mechanism: Exploring the Symmetry Breaking Hypothesis in Neural Networks", "content": ""}, {"title": "3.1 Symmetry Breaking Mechanism in the Ising Model", "content": "In physics, symmetry breaking [46] is a fundamental concept that elucidates how systems initially exhibiting symmetry can lose this symmetry due to changes in external conditions or internal interactions. Here, we delve into the two-dimensional Ising model [47], a widely utilized stochastic process model in physics, primarily employed to study phase transitions and spin-spin interactions. The Ising model is defined on a two-dimensional periodic lattice, typically arranged in a square grid, as depicted in Fig. 5 (A). Each lattice point is assigned a spin variable, which can be either spin-up (denoted by +1) or spin-down (denoted by -1).\nIn the absence of an external magnetic field, the Ising model exhibits what is termed \"energetic symmetry\". This implies that different configurations of spins (i.e., various arrangements of +1 and -1 spins on the lattice) can result in the same total energy for the system. This is because the system's energy E of a configuration is determined by the interactions between neighboring spins, mathematically expressed as:\n$E = -J\\Sigma_{(i,j)} S_iS_j,$\nwhere J is the interaction strength between neighboring spins, $s_i$ and $s_j$ are the spin values at lattice points i and j, and the sum is over all pairs of neighboring spins. In this equation, since the value of $s_is_j$ remains unchanged when all spins are flipped (changing +1 to -1 and vice versa), the system's total energy remains constant, reflecting the system's symmetry.\nHowever, when an external magnetic field h is introduced, this energy symmetry is disrupted. The interaction between the field and the spins adds a new term to the energy equation:\n$E = -J\\Sigma_{(i,j)} s_is_j-h \\Sigma_i s_i$"}, {"title": "3.2 Symmetry Breaking in Neural Networks", "content": "Analogous to the Ising model, consider a neural network with input x, weight w, and activation function \u03c3. The output of a two-layer network can be simplified as:\n$\\sigma(\\sigma(W_{i,j}x_i)W'_{j,1}) ~ W_{i,j}x_iW'_{j,1} + O(higher order terms),$"}, {"title": "4 Evidence and Metric: Symmetry Breaking Techniques and Its Measurement", "content": ""}, {"title": "4.1 Symmetry Breaking Effects of Equivariance, Dropout, and Batch Normalization", "content": "To investigate the symmetry-breaking effects of equivariance [22], dropout [23], and batch normalization [24] in neural networks, we conducted a series of controlled experiments using a simple CNN architecture. We compared the performance of five different network configurations: a baseline CNN, a CNN with dropout, a CNN with batch normalization, a CNN with equivariance constraints, and a CNN with incorrect symmetry embedded in the structure (wrong equivariance).\nWe constructed a synthetic dataset consisting of 12 binary images, each of size 2\u00d72 pixels, with corresponding binary labels. The images were designed to exhibit simple patterns with pixel values of -1, 0, or 1, and are invariant under 180-degree rotations and flips but not under 90-degree rotations a property crucial for evaluating the effects of equivariance in neural networks. The labels were assigned based on the presence of specific patterns in the images, creating a binary classification task.\nWe generated all possible combinations of weights for the convolutional and fully connected layers, with each weight taking values of either -1 or 1. This exhaustive search allowed us to evaluate the loss landscape comprehensively.\nTo assess the symmetry-breaking effects, we compared the loss values across the different network configurations. The results of our experiments are summarized in Fig. 7. The figure shows the sorted loss values for each network configuration, highlighting the impact of dropout, batch normalization, and equivariance on the loss landscape.\nOur findings indicate that there are two primary approaches to break such symmetry: splitting the degenerated states as seen in dropout, batch normalization, or reducing the number of parameter combinations, as seen in the equivariance cases.\nAmong these methods, equivariance stands out as a more efficient approach compared with dropout and batch normalization. By embedding symmetry constraints directly into the network architecture, equivariance reduces the complexity of the parameter space, leading to a more streamlined and effective training process. This observation suggests that incorporating symmetry principles into network design could be a promising direction for future research, potentially offering advantages over solely applying various regularization techniques or increasing network size."}, {"title": "4.2 Measuring the Degree of Symmetry Breaking", "content": "To quantify the degree of symmetry breaking in neural networks, we developed a metric called the \"Replica Distance (RD) Metric\". This metric evaluates the Wasserstein distance between network outputs across different weight configurations (i.e., replicas), providing insights into the extent of symmetry breaking. We applied the RD Metric to the CIFAR-10 dataset using five distinct neural network architectures: SimpleCNN, DropoutCNN, BatchNormCNN, FlipEquivarianceCNN, and Rotation EquivarianceCNN (180 degrees), analogous to that in Fig. 7. Here, it is"}, {"title": "5 Discussion and Conclusion", "content": "In this study, we explored the principle of symmetry breaking in neural network optimization through three key findings: input dimension expansion, unified optimization techniques, and the development of a symmetry breaking metric. Here, we summarize our key findings, discuss their implications, and outline future research directions."}, {"title": "5.1 Key Findings", "content": "Input Dimension Expansion. Our experiments demonstrated that expanding input dimensions with constant values significantly improves performance across various tasks, including image classification, Physics-Informed Neural Networks (PINNs), image coloring, and sentiment analysis. This technique leverages symmetry breaking to facilitate smoother optimization and better generalization by providing additional degrees of freedom for the model to explore during training. However, it is crucial to recognize that input expansion is not a universal solution and has its limitations. Excessively increasing input dimensions can lead to inefficiencies, and the effectiveness of input expansion can vary depending on the initial network design and the specific task.\nSymmetry Breaking as a fundamental optimization principle. We drew an analogy between the symmetry breaking mechanism in the Ising model and neural networks. By introducing additional input dimensions, we break the inherent symmetries in the network, leading to a reduction in the number of degenerate states and a smoother training trajectory. This symmetry breaking mechanism helps the network escape local minima and saddle points in the loss landscape, thereby facilitating better optimization.\nSymmetry Breaking Techniques and Measurement. We investigated the symmetry-breaking effects of equivariance, dropout, and batch normalization in neural networks. Our findings indicate that these techniques effectively break the symmetry in the loss landscape, leading to more robust optimization. Among these methods, embedding equivariance constraints directly into the network architecture proved to be particularly effective. Additionally, we developed a metric to quantify the degree of symmetry breaking in neural networks. By analyzing the Wasserstein distance between weight distributions, we can measure the extent of symmetry breaking."}, {"title": "5.2 Challenges and Future Directions", "content": "While our results are promising, further research is essential to validate and refine the symmetry breaking metric across different datasets and neural network architectures. Extending the analysis to a broader range of datasets, including CIFAR-100, ImageNet, and other domain-specific datasets, will help us understand how the symmetry metric performs across various types of data and tasks, providing a more comprehensive evaluation of its effectiveness. In fact, our experiments have shown that the worse the original network performs, the more significant the improvement brought by input dimension expansion. Therefore, for some tasks where the network design is already very good, the effect of input dimension expansion is not obvious, such as in sentiment analysis and image coloring tasks.\nRefining the symmetry metric itself is also a critical area for future work. Our proposed metric relies on the choice of several parameters and could be unstable depending on the random seeds. Hence, exploring alternative and comprehensive formulations of the metric will capture more aspects of the loss landscape, offering more nuanced insights into symmetry breaking. By developing a more sophisticated and accurate metric, we can better quantify the degree of symmetry breaking in neural networks and understand its impact on optimization and performance.\nMoreover, investigating the practical applications of the symmetry metric is beneficial for translating our theoretical findings into real-world benefits. By using the symmetry metric to guide the design of neural network architectures and optimization strategies, we can develop more efficient and effective models. This approach can help identify the most appropriate symmetries to embed in network designs, leading to improved performance and generalization.\nAnother promising direction is the use of neural networks themselves to identify equivariance within the data. By training networks to recognize and exploit these properties, we can develop models that are inherently more efficient and effective. This approach could be particularly valuable for scientific and engineering applications, where the tasks often involve complex and domain-specific equivariance. By identifying and embedding the equivariance, we can improve the performance of neural networks even in scenarios with limited data and computational resources."}, {"title": "5.3 Conclusion", "content": "In conclusion, our exploration of the principle of symmetry breaking in neural network optimization has yielded insights and practical techniques for enhancing model performance. By understanding and leveraging symmetry breaking, we can develop more efficient and effective AI systems, thereby contributing to the advancement of both AI and its interdisciplinary applications. Continued research in this area holds the potential to further improve our understanding and optimization of neural networks, paving the way for more interpretable AI models.\nOur findings highlight the value of interdisciplinary approaches, demonstrating how principles from physics can inform and advance AI research."}, {"title": "6 Methods", "content": ""}, {"title": "6.1 Methodology for Expanding Input Dimensions in Image Classification", "content": "In identify the symmetry breaking principle, we coincidentally found a method that enhances image classification performance by expanding the spatial dimensions of input images. This approach involves filling the expanded positions with a fixed value, followed by a careful design of the neural network architecture to manage the increased dimensionality. For instance, if the original image is N \u00d7 N pixels, we expand it by a factor of K as an image with NK \u00d7 NK pixels. In this NK \u00d7 NK image, for example, the original pixels of [0,0], [0, 1], [0, 2], \u00b7 \u00b7 \u00b7, [1, 0], [1, 1], \u00b7 \u00b7 \u00b7, [N, N] are positioned at coordinates as [0,0], [0, K], [0, 2K], \u00b7\u00b7\u00b7, [K, 0], [K, 2K], \u00b7\u00b7\u00b7, [N(K \u2212 1), N(K \u2212 1)], while the remaining positions are filled with the value 0.5 (an example is shown in Fig. 9). This method ensures that the spatial expansion maintains the relative positions of the original pixels while introducing new elements with a uniform value to fill the increased space.\nIn our experiments, we applied this expansion method with a factor of K = 2 for all datasets and neural network models. This expansion technique allows us to maintain a consistent number of neural network parameters by using a pooling method on the feature map extracted by the last convolutional layer. Consequently, the size of the feature map output depends only on the number of channels, not the input data size. During training, we employed the Stochastic Gradient Descendent optimizer with an"}, {"title": "6.2 Methodology for Expanding Input Dimensions in AI4Science", "content": ""}, {"title": "6.2.1 Quantum Chromodynamics", "content": "In QCD, the partition function ln Z(T) is fundamental for calculating the pressure P(T) and energy density \u20ac(T) using statistical dynamics formulae,\n$ln Z(T) = ln Z_g(T) + ln Z_{u/d}(T) + ln Z_s(T),$\n$P(T) = T \\frac{\\partial ln Z(T)}{\\partial V},$\n$\\epsilon(T) = \\frac{T^2}{V} (\\frac{\\partial ln Z(T)}{\\partial T})_V,$\nwhere ln $Z_g(T)$, ln $Z_{u/d}(T)$, and ln $Z_s(T)$ are the partition functions for gluons, u/d quarks, and s quarks, respectively. These partition functions can be computed from the momentum integration,\n$ln Z_g(T) = \\frac{d_g V}{2\\pi^2}\\int_0^{\\infty} p^2dp ln [1 - exp(-\\frac{1}{T} \\sqrt{p^2 + m_g^2(T)})],$\n$ln Z_{q_i}(T) = +\\frac{d_{q_i} V}{2\\pi^2}\\int_0^{\\infty} p^2dp ln [1+ exp(-\\frac{1}{T} \\sqrt{p^2 + m_g^2(T)})],$\nHere, $d_g = 16$, $d_{qs} = 12$, and $d_{qu/d} = 24$ are the degrees of freedom for gluons and quarks. These integrals can be numerically evaluated using a 50-point Gaussian quadrature method [48]. With the calculated P(T) and \u2208(T), the Eos can be obtained, linking the entropy density s with P(T) and e(T) as,\n$S = \\frac{\\epsilon + P}{T}$"}, {"title": "6.2.2 Solving Partial Differential Equations", "content": "Experimental Setup. We utilized a simple feedforward neural network (FNN) architecture with five fully connected layers, each containing 100 nodes. The PDEs were selected from the PINNacle benchmark, which includes a diverse range of PDEs such as Burgers' equation, Poisson's equation, heat equation, Navier-Stokes equations, wave equations, and chaotic PDEs like the Gray-Scott and Kuramoto-Sivashinsky equations. The experiments were conducted using the following setup:\nPDE List. The list of PDEs included Burgers 1D, Burgers 2D,\nPoisson 2D, Poisson Boltzmann 2D, Poisson 3D with Complex Geometry,\nPoisson 2D of Many Area, Heat2D with Varying Coefficient, Multiscale Heat2D,\nHeat2D with ComplexGeometry, Long Time Heat2D, NS2D with LidDriven,\nNS2D with BackStep, NS2D with LongTime, Wave 1D, Heterogeneous Wave 2D,\nLong Time Wave2D, Kuramoto Sivashinsky Equation, GrayScott Equation,\nPoisson ND, and Heat ND.\nTraining Configuration. The neural network was configured with 5 hidden layers each of 100 nodes and a learning rate of 10-3. The training was conducted for 20,000 iterations. The experiments were repeated 30 times with different random seeds ranging from 0 to 29 to ensure robustness.\nImplementation. The experiments were implemented using the DeepXDE [49] library with PyTorch backend. The training process used Adam optimizer. The models were trained using a combination of PDE residuals, boundary conditions, and available data losses exactly the same as in PINNacle.\nAnalysis. To analyze the performance of the input expansion method, we focused on the distribution of the loss values for both the raw and expanded input methods. The loss values were calculated as the MSE between the predicted and true solutions of the PDEs."}, {"title": "6.3 Methodology for Expanding Input Dimensions in Other Tasks", "content": ""}, {"title": "6.3.1 Image Coloring", "content": "In the image coloring task, we use the CelebA dataset, where a generative model based on conditional diffusion is employed to restore grayscale images to color RGB maps, utilizing a U-Net architecture [50] as the primary backbone. During training, we began with a three-channel color map and, similar to the DDPM approach [51], progressively added Gaussian random variables, diffusing the image into nearly complete random noise. In the recoloring process, a random matrix is first sampled, and then the grayscale map is input into the U-Net as a condition. The denoising network then"}, {"title": "6.3.2 Sentiment Analysis", "content": "We describe the method used to evaluate the effect of expanding input dimensions on sentiment analysis tasks using the IMDB dataset. The goal was to assess whether adding constant values to the input dimensions could improve the performance of a BERT-based model for binary classification.\nData Preparation. We utilized the IMDB dataset, which contains movie reviews labeled as either positive or negative. The dataset was divided into training and testing sets, each containing reviews stored in separate directories for positive and negative sentiments.\nData Preprocessing. We used the DistilBertTokenizer to tokenize the text data. The tokenizer was configured to truncate and pad the sequences to a maximum length of 256 tokens, ensuring that all input sequences are of uniform length, which is necessary for batch processing in neural networks.\nModel Architecture. We modified the BERT model to include an expanded embedding layer. The embedding layer was replaced with a custom FixedBertEmbedding class that allowed for the expansion of input dimensions. In the baseline model, we used the standard BERT embedding layer. This model served as the control to compare the effects of input dimension expansion. The expanded embedding model included a linear layer to map the expanded input dimensions back to the original embedding size. This modification aimed to introduce additional features into the input space, potentially aiding the model in capturing more complex patterns in the data to break the proposed symmetry.\nThe FixedBertEmbedding class was designed to freeze the original BERT embedding layer, preventing updates during training. The original embeddings were expanded by interleaving them with zeros, effectively doubling their dimensions. A linear layer was then used to map these expanded dimensions back to the desired embedding size."}, {"title": "6.4 Analyzing the Loss Landscape in Random Number Classification", "content": "To investigate the impact of input dimension expansion on the loss landscape of neural networks, we conducted experiments using a simple feedforward neural network for binary classification of random numbers.\nInput Data. We used a random number between 0 and 1 as input to the network. The number was labeled as 0 if it was less than or equal to 0.5, and as 1 otherwise.\nModel Architectures. To compare cases for raw (the usual input) and expanded (expanded input), we employed two network configurations:\nA feedforward neural network with an input layer of size 1, followed by three hidden layers of sizes 3, 3, and 2, and an output layer of size 1. The activation function used was the hyperbolic tangent (tanh).\nA similar network but with an expanded input layer of size 2. In this configuration, the original input was augmented with an additional constant value of 0.5. For example, if the original input was 0.7, the expanded input would be the vector [0.7, 0.5].\nLoss Landscape Analysis. To evaluate the loss landscapes, we varied the weights of the network layers, allowing each weight to take values of -1 or 1. For each combination of weights, we computed the network's output and the corresponding loss using the MSE loss function.\nTo explicitly demonstrate the effect of the additional dimension, we considered two scenarios in our analysis. One did not include bias terms in the network layers, so the loss was computed solely based on the weights and the input data; and another included bias terms in the first layer of the network."}, {"title": "6.5 Analyzing the Loss Landscape of Equivariance, Dropout, and Batch Normalization", "content": "To systematically investigate the effects of equivariance, dropout, and batch normalization on the loss landscape of neural networks, we designed a series of controlled experiments using a simple CNN architecture and a synthetic dataset. The following outlines the methodology employed in our study.\nDataset Generation. A synthetic dataset was constructed comprising 12 binary images, each of size 2\u00d72 pixels, with corresponding binary labels (Fig. 10). The images were designed to represent simple patterns, such as adjacent pixel pairs with values of -1, 0, or 1. Labels were assigned based on the presence of specific patterns within the images, forming a binary classification task."}, {"title": "6.6 Measuring the Degree of Symmetry Breaking", "content": "Measuring the degree of symmetry in a network is primarily achieved by comparing the correlations of the same network under different weight configurations (also called different replicas). We use different seeds to train the network models with identical hyper-parameters, and by measuring the symmetry (i.e., correlation) between replicas, we can equivalently measure the symmetry of the network structure itself. We applied this metric to the CIFAR-10 dataset using five distinct neural network architectures: SimpleCNN, DropoutCNN, BatchNormCNN, FlipEquivarianceCNN, and Rotation EquivarianceCNN.\nModel Architectures. Five distinct network configurations were defined to isolate the effects of equivariance, dropout, and batch normalization. The detailed structures of these five networks are as follows:\nSimpleCNN: Consists of two convolutional layers followed by ReLU activations and max pooling, then a fully connected layer and an output layer.\nDropoutCNN: Similar to SimpleCNN but includes a dropout layer with a dropout rate of 0.3 between the fully connected layer and the output layer.\nBatchNormCNN: Similar to SimpleCNN but includes batch normalization layers after each convolutional layer.\nFlipEquivarianceCNN: Similar to SimpleCNN but includes a data augmentation step in the forward pass that horizontally flips half of the images in the batch.\nRotationEquivarianceCNN: Similar to SimpleCNN but includes a data augmentation step in the forward pass that rotates half of the images in the batch by 180 degrees.\nTraining Different Networks.\nData Preparation: The CIFAR-10 dataset was preprocessed by normalizing the images and splitting it into training and testing sets. Data augmentation techniques such as random horizontal flips and rotations were applied for FlipEquivarianceCNN and RotationEquivarianceCNN, respectively."}]}