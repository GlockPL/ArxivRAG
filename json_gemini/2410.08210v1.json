{"title": "POINTOBB-V2: TOWARDS SIMPLER, FASTER, AND\nSTRONGER SINGLE POINT SUPERVISED ORIENTED\nOBJECT DETECTION", "authors": ["Botao Ren", "Xue Yang", "Yi Yu", "Junwei Luo", "Zhidong Deng"], "abstract": "Single point supervised oriented object detection has gained attention and made\ninitial progress within the community. Diverse from those approaches relying\non one-shot samples or powerful pretrained models (e.g. SAM), PointOBB has\nshown promise due to its prior-free feature. In this paper, we propose PointOBB-\nv2, a simpler, faster, and stronger method to generate pseudo rotated boxes from\npoints without relying on any other prior. Specifically, we first generate a Class\nProbability Map (CPM) by training the network with non-uniform positive and\nnegative sampling. We show that the CPM is able to learn the approximate object\nregions and their contours. Then, Principal Component Analysis (PCA) is applied\nto accurately estimate the orientation and the boundary of objects. By further\nincorporating a separation mechanism, we resolve the confusion caused by the\noverlapping on the CPM, enabling its operation in high-density scenarios. Exten-\nsive comparisons demonstrate that our method achieves a training speed 15.58\u00d7\nfaster and an accuracy improvement of 11.60%/25.15%/21.19% on the DOTA-\nv1.0/v1.5/v2.0 datasets compared to the previous state-of-the-art, PointOBB. This\nsignificantly advances the cutting edge of single point supervised oriented detec-\ntion in the modular track.", "sections": [{"title": "INTRODUCTION", "content": "Oriented object detection is essential for accurately labeling small and densely packed objects, es-\npecially in scenarios like remote sensing imagery, retail analysis, and scene text detection, where\nOriented Bounding Boxes (OBBs) provide precise annotations. However, annotating OBBs is labor-\nintensive and costly. Therefore, numerous weakly supervised methods have emerged in recent years,\nincluding horizontal bounding box supervision and point supervision. Representative methods for\nhorizontal bounding box supervision include H2RBox (Yang et al., 2023a) and H2RBox-v2 (Yu\net al., 2023). In addition, point supervision, which only requires labeling the point and category\nfor each object, significantly reduces the annotation cost. Notable point-supervised methods include\nP2RBox (Cao et al., 2024), Point2RBox (Yu et al., 2024), and PointOBB (Luo et al., 2024).\nAs illustrated in Fig. 1, existing point-supervised oriented object detection methods can be broadly\ncategorized into three paradigms: (a) SAM-based methods (Cao et al., 2024; Zhang et al., 2024)\nrely on the powerful SAM (Kirillov et al., 2023b) model, which, although effective in natural im-\nages, struggles with cross-domain tasks like aerial imagery, particularly in small object and densely\npacked scenarios. Additionally, SAM-based methods are slow and memory-intensive due to post-\nprocessing; (b) Prior-based Weakly-supervised Oriented Object Detection (WOOD) methods, such\nas Point2RBox (Yu et al., 2024), integrate human priors which reduce generalizability since different\ndatasets require distinct prior knowledge. Further, the end-to-end setup limits flexibility, prevent-\ning these methods from leveraging more powerful detectors and benefiting from their performance\nimprovements; (c) Modular WOOD methods (Luo et al., 2024) do not rely on manually designed"}, {"title": "2 RELATED WORK", "content": "In addition to horizontal detection (Zhao et al., 2019; Liu et al., 2020), oriented object detection\n(Yang et al., 2018; Wen et al., 2023) has received extensive attention. In this section, we first intro-\nduce oriented detection supervised by rotated boxes. Then, approaches to point-supervised oriented\ndetection and other weakly-supervised settings are discussed."}, {"title": "2.1 RBOX-SUPERVISED ORIENTED DETECTION", "content": "Representative works include anchor-based detector Rotated RetinaNet (Lin et al., 2020), anchor-\nfree detector Rotated FCOS (Tian et al., 2019), and two-stage solutions, e.g. Rol Transformer (Ding\net al., 2019), Oriented R-CNN (Xie et al., 2021), and ReDet (Han et al., 2021). Some research en-\nhances the detector by exploiting alignment features, e.g. R\u00b3Det (Yang et al., 2021b) and S2A-Net\n(Han et al., 2022). The angle regression may face boundary discontinuity and remedies are devel-\noped, including modulated losses (Yang et al., 2019a; 2022; Qian et al., 2021) that alleviate loss\njumps, angle coders (Yang & Yan, 2020; Yang et al., 2021a; Yang & Yan, 2022; Yu & Da, 2023) that\nconvert the angle into boundary-free coded data, and Gaussian-based losses (Yang et al., 2021c;d;\n2023b;c; Murrugarra-Llerena et al., 2024) transforming rotated bounding boxes into Gaussian dis-\ntributions. RepPoint-based methods (Yang et al., 2019b; Hou et al., 2023; Li et al., 2022) provide\nalternatives that predict a set of sample points that bounds the spatial extent of an object."}, {"title": "2.2 POINT-SUPERVISED ORIENTED DETECTION", "content": "Recently, several methods for point-supervised oriented detection have been proposed: 1) P2RBox\n(Cao et al., 2024), PMHO (Zhang et al., 2024) and PointSAM (Liu et al., 2024) propose oriented\nobject detection with point prompts by employing the zero-shot Point-to-Mask ability of SAM (Kir-\nillov et al., 2023a). 2) Point2RBox (Yu et al., 2024) has introduced a novel end-to-end approach\nbased on knowledge combination in this domain. 3) PointOBB (Luo et al., 2024) achieves point\nannotation based RBox generation method for oriented object detection through scale-sensitive con-\nsistency and multiple instance learning.\nAmong these methods, P2RBox, PMHO and PointSAM require SAM model pre-trained on massive\namounts of labeled data, whereas Point2RBox requires one-shot samples (i.e. human priors) for\neach category. Although achieving better accuracy, they are not as general as PointOBB. Hence, we\nchoose PointOBB as our baseline to develop a simpler, faster, and stronger method, PointOBB-v2."}, {"title": "2.3 OTHER WEAKLY-SUPERVISED SETTINGS", "content": "Compared to the Point-to-RBox, some other weakly-supervised settings have been better studied.\nThese methods are potentially applicable to our Point-to-RBox task setting by using a cascade\npipeline, such as Point-to-HBox-to-RBox and Point-to-Mask-to-RBox. In our experiment, cascade\npipelines powered by state-of-the-art weakly-supervised approaches are also adopted for compari-\nson. Here, several representative work are introduced.\nHBox-to-RBox. The seminal work H2RBox (Yang et al., 2023a) circumvents the segmentation\nstep and achieves RBox detection directly from HBox annotation. With HBox annotations for the\nsame object in various orientations, the geometric constraint limits the object to a few candidate\nangles. Supplemented with a self-supervised branch eliminating the undesired results, an HBox-to-\nRBox paradigm is established. An enhanced version H2RBox-v2 (Yu et al., 2023) is proposed to\nleverage the reflection symmetry of objects to estimate their angle, further boosting the HBox-to-\nRBox performance. EIE-Det (Wang et al., 2024) uses an explicit equivariance branch for learning"}, {"title": "3 METHOD", "content": "Our task focuses on oriented object detection with single point supervision. We first utilize point\nannotations for each object in the training dataset to generate pseudo labels, which are then used\nto train an existing detector. As shown in Fig. 2, the model first generates a Class Probability\nMap (CPM) based on the point annotations. Specifically, during training, we devise a positive and\nnegative sample assignment strategy, where the resulting CPM outlines the rough object contours,\nwith higher probabilities concentrated around the point and along the object axes.\nWe generate pseudo oriented bounding boxes according to CPM. We perform non-uniform sampling\naround the point annotation of each object, guided by the probability distribution within the CPM.\nWe convert the sampling process into a weighted probability approach, which maintains the same\nexpected result while eliminating the variance introduced by random sampling. By applying Prin-\ncipal Component Analysis (PCA) to the weighted grid points, we can infer the object's orientation.\nWe then determine the object boundaries by combining the thresholded CPM with the inferred ori-\nentation. Furthermore, to address densely populated object scenarios, we introduce a mechanism for\ndifferentiating between closely situated objects, ensuring effective separation and accurate detection."}, {"title": "3.1 CLASS PROBABILITY MAP GENERATION", "content": "The Class Probability Map (CPM) represents the per-class probability for each point on the feature\nmap, with values ranging between [0, 1]. To generate the CPM, our model first takes an image I\nof dimension (C, H, W) as input and processes it through a ResNet-50 (He et al., 2016) backbone\nwith an FPN (Lin et al., 2017) structure. The final class probability map is derived from the highest-\nresolution feature map of the FPN, which is then projected through a projection layer. The output is\na map of size (Nclass, Ho, Wo). Formally, it is defined as:\n$CPM = Proj(f(I)o)$,                                                              (1)\nwhere CPM is the class probability map, Proj(\u00b7) represents the projection layer, and f(I)o is the\nhighest-resolution feature map from the ResNet-50 + FPN."}, {"title": "3.2 LABEL ASSIGNMENT", "content": "A key component of our approach is the design of a robust sample assignment strategy for both pos-\nitive and negative samples. This strategy is essential for building an accurate CPM, which outlines\nrough object contours, concentrating higher probabilities around object centers and along their axes.\nTo ensure reliable separation of objects, especially in densely populated scenarios, our method ad-\ndresses the challenge of closely situated objects by introducing additional mechanisms for effective\ndifferentiation. We illustrate this label assignment in the upper-right part of Fig. 2. The specific\ndetails of the sample assignment process are outlined below:\nPositive Label Assignment. For positive samples, we select all points within a fixed radius b\u2081 (set\nto 6 in our model) around each point. If a point lies within multiple such radii, it is assigned to the\nclosest center. The condition for positive samples is as follows:\n$\\exists GT_i \\in GT_{1~N}, (d(p, GT_i) < b_1) \\land (d(p, GT_i) = min(d(p, GT_{1~N}))) \\\\ \\Rightarrow p \\text{ is positive}, cls(p) = cls(GT_i).$                                                                                                                                                                     (2)\nNegative Label Assignment. Given N ground truth objects (GT), for each GTi, we identify its\nnearest neighboring object GTj based on the Euclidean distance. This gives us a vector distmin\nwith dimension [N], where each element dist\u2081 represents the minimum distance between GTi and\nits closest neighbor. We then draw a circle with radius a \u00d7 dist; around GTi, where a (set to 1 in\nour model) is a fixed proportional constant. Points outside all such circles are designated as negative\nsamples. The negative sample condition is formulated as:\n$\\forall GT_i \\in GT_{1~N},d(p, GT_i) > a \\times dist_i \\Rightarrow p \\text{ is negative}.$                                                                                                              (3)\nIn addition to the above defined negative labels, we also set the middle region between objects\nas negative to make the boundaries clearer between densely packed objects (denoted as \"Neg./M\"\nin ablation Tab. 4). For each GT\u017c, we identify its nearest neighbor GTj that belongs to the same\nclass. A circle is drawn with a radius b2 (set to 4 in our model) and centered at the midpoint of the\nline connecting GTi and GTj, and points within this circle are assigned as negative samples. The\ncondition is defined as:\n$\\forall GT_i \\in GT_{1~N},\\exists GT_j \\in GT_{1\\sim n}, d(GT_i, GT_j) = min(d(GT_i, GT_{1~N})) \\\\ \\text{s.t.} \\cls(GT_i) = cls(GT_j) \\land d(p, (GT_i + GT_j)/2) < b_2 \\Rightarrow p \\text{ is negative}.$                                                                                                                     (4)"}, {"title": "Robustness.", "content": "While we do not explicitly define positive and negative samples based on precise object\ncontours or oriented bounding boxes, which may result in some inaccuracies during label assignment\n(i.e. incorrectly assigning a small portion of positive or negative samples during training), this does\nnot significantly hinder our method's ability to learn accurate object contours. These minor label\nassignment inaccuracies, particularly in densely populated regions or for objects with extreme aspect\nratios, do not affect the overall robustness and effectiveness of the approach. As demonstrated in\nFig. 3, our strategy is capable of learning the correct contours, even for objects with large aspect\nratios and in densely packed scenarios."}, {"title": "3.3 ORIENTATION AND BOUNDARY ESTIMATION VIA PCA", "content": "After obtaining the CPM, we sample points around each ground truth based on the class probabil-\nities, and then apply Principal Component Analysis (PCA) on the sampled points to determine the\nobject's orientation. As shown in bottom part of Fig. 2, we sample points around the GT based on\nthe probabilities in the CPM for the corresponding object class. We choose a 7 \u00d7 7 grid centered at\nthe GT with the coordinates of the 49 integer points zi\u22481~49 as:\n$(x, y), x \\in [-3,3], y \\in [-3, 3].$                                                                        (5)\nFor each grid point zi, we can compute the CPM probability pi and decide whether to sample the\npoint based on this probability. Once we have the sampled points, we apply PCA to find the primary\ndirection of the point set, which represents the object's orientation. While PCA provides the correct\nprimary direction in expectation, the randomness introduced by sampling can cause variance in the\nresult from a single pass. Although averaging over multiple sampling runs can mitigate this variance,\nit also increases computational cost.\nTo address this, we propose an equivalent method that transforms probabilistic sampling into a\nweighted coordinate transformation. Instead of sampling points probabilistically, we assign a weight\nof pi to each point zi, ensuring the same expected outcome while eliminating the variance caused\nby random sampling. The covariance matrix is then defined as:\n$C_z = \\sum_{i=1}^N p_i(z_i - \\mu_z)(z_i - \\mu_z)^T.$                                                                                                       (6)\nWe then perform eigenvalue decomposition on Cz:\n$C_zv_i = \\lambda_iv_i.$                                                                                                        (7)\nThe eigenvector v\u2081 corresponding to the largest eigenvalue \u5165\u2081 is chosen as the primary direction.\nSince Cz is a real symmetric matrix, the secondary direction is guaranteed to be orthogonal to the\nprimary direction. This orthogonality corresponds to the perpendicular relationship between the two\nadjacent sides of an oriented bounding box.\nAfter identifying the primary and secondary directions, we determine the object boundaries along\nthese directions. Starting from the center, we move along each direction and stop when the value at\na position falls below a threshold, indicating the object boundary."}, {"title": "3.4 OBJECT DIFFERENTIATION IN DENSE SCENARIOS", "content": "In dense scenarios, objects can be difficult to distinguish on the CPM. This can affect the PCA's\nability to determine the object orientation and the boundary identification. To address this, we\ndesign a \"Vector Constraint Suppression\" method to resolve boundary ambiguity.\nVector Constraint Suppression. Even after determining the correct orientation in dense scenarios,\nthe object boundaries may still be unclear, making it difficult to precisely locate them using the\nprobability threshold described in Sec. 3.3. In most cases, simply distinguishing between two closely\npositioned objects is sufficient to define the object's boundary.\nWe propose a simple constraint: For each GTi, we first find its nearest same-class neighbor GTj\nand compute the vector u = \u3008GTi, GTj\u3009 between GTi and GTj. If the angle between this vector\nand the primary or secondary direction is smaller than a threshold a (set to \u03c0/6 in our model),"}, {"title": "4 EXPERIMENT", "content": ""}, {"title": "4.1 DATASETS", "content": "DOTA (Xia et al., 2018) is a large-scale dataset designed for object detection in aerial images,\ncovering various object categories and complexities. DOTA has three versions:\nDOTA-v1.0 has 2,806 images with 188,282 instances across 15 categories. The images range from\n800\u00d7800 to 4,000\u00d74,000 pixels and exhibit significant variation in scale and orientation.\nDOTA-v1.5 extends DOTA-v1.0 by adding annotations for extremely small objects (less than 10\npixels) and introducing a new category, Container Crane (CC). It includes a total of 403,318\ninstances while retaining the same image count and dataset split as DOTA-v1.0.\nDOTA-v2.0 further expands the dataset to 11,268 images and 1,793,658 instances, covering 18 cat-\negories. Two additional categories, Airport (AP) and Helipad (HP), are introduced, providing\na more diverse and challenging set of aerial images."}, {"title": "4.2 EXPERIMENTAL SETTINGS", "content": "Our implementation is based on the MMRotate library (Zhou et al., 2022). In the pseudo-label\ngeneration stage, we train the model for 6 epochs using momentum SGD as the optimizer. We set\nthe weight decay to 1e-4, with an initial learning rate of 0.005, which decays by a factor of 10 after\nthe 4th epoch. The batch size for training is set to 2. For the detector training phase using pseudo-\nlabels, we use the same detector configurations as the default settings in MMRotate. Throughout the\nentire training process, random flipping is employed as the only data augmentation technique. Our\nexperiments were accelerated using two GeForce RTX 3090 GPUs."}, {"title": "4.3 MAIN RESULTS", "content": "Results on DOTA-v1.0. As shown in Tab. 1, our method achieves state-of-the-art performance\ncompared to the previous leading approaches, i.e. PointOBB and Point2RBox. Specifically, under\nthree different detectors, our method attains mAP50 scores of 41.68%, 41.64%, and 44.85%, repre-\nsenting improvements of 11.60%, 8.33%, and 10.90% over PointOBB, respectively. Additionally,\nwhen compared to Point2RBox-RC, which does not incorporate human prior knowledge, our ap-\nproach achieves a substantial gain of 10.78%. Even when compared with Point2RBox-SK, which\nleverages manual sketches to assist in boundary determination, our method still outperforms it by"}, {"title": "4.4 ABLATION STUDIES", "content": "Label Assignment. Tab. 4 demonstrates the impact of our three label assignment strategies on\nmodel performance. In these experiments, different label assignment strategies were used to train\nand generate the CPM. When a specific strategy was applied to define positive and negative samples,\nthe remaining points were ignored during training. We observed that using a simple circular strategy\nto determine positive samples resulted in only 23.62%. However, by incorporating a more compre-\nhensive strategy to identify negative samples, the performance increased significantly to 44.75%.\nFurthermore, assigning middle region between objects to negative (denoted as \"Neg./M\") yielded a"}, {"title": "Robustness.", "content": "While we do not explicitly define positive and negative samples based on precise object\ncontours or oriented bounding boxes, which may result in some inaccuracies during label assignment\n(i.e. incorrectly assigning a small portion of positive or negative samples during training), this does\nnot significantly hinder our method's ability to learn accurate object contours. These minor label\nassignment inaccuracies, particularly in densely populated regions or for objects with extreme aspect\nratios, do not affect the overall robustness and effectiveness of the approach. As demonstrated in\nFig. 3, our strategy is capable of learning the correct contours, even for objects with large aspect\nratios and in densely packed scenarios."}, {"title": "PCA Sampling Strategy and Size.", "content": "We conducted ablation experiments on the PCA sampling strat-\negy and the range of sampling sizes. As shown in Tab. 5, our weighted method for PCA calculation\nimproves accuracy by 3.45% compared to the probabilistic method. We also found that this improve-\nment primarily benefits classes with larger aspect ratios, such as large vehicles and harbors. This\nis because CPM in elongated objects exhibit significant probability variation along the short axis of\ntheir oriented bounding boxes, and probabilistic sample method introduces considerable instability.\nAdditionally, we evaluated the impact of the PCA sampling size. As shown in Tab. 7, our method\nachieves the best performance when the sampling size is set to 7."}, {"title": "Vector Constraint.", "content": "As shown in Tab. 6, applying the vector constraint significantly improves de-\ntection performance. Through further analysis, we found that the improvement is primarily con-\ncentrated in dense object categories, such as small vehicles, large vehicles, and ships. In contrast,\nsparse categories like harbors and swimming pools are almost unaffected. This observation aligns\nwith the motivation behind the design of this module, which especially addresses densely packed\nobject scenarios."}, {"title": "4.5 ANALYSIS", "content": "Label Accuracy. Recognizing the potential inaccuracies in human annotations, where the center\npoint might not be perfectly labeled, we conducted experiments by adding noise to the center points\nto evaluate the robustness of our model. We selected different thresholds and calculated the\nobject's scale as S = \u221awh. The center points were randomly shifted along a uniformly sampled\ndirection, with the offset distance drawn from a uniform distribution over the range [-6S, 6S].\nWe observed a slight performance decrease as the center points were perturbed. As Tab. 8 shows,"}, {"title": "5 CONCLUSION", "content": "In this paper, we introduced PointOBB-v2, a simpler, faster, and stronger approach for single point-\nsupervised oriented object detection. By employing class probability maps and Principal Compo-\nnent Analysis (PCA) for object orientation and boundary estimation, our method improves detection\naccuracy while discarding the traditional time- and memory-heavy teacher-student structure. Exper-\nimental results demonstrate that PointOBB-v2 consistently outperforms the previous state-of-the-\nart across multiple datasets, achieving a training speed 15.58\u00d7 faster and accuracy improvements\nof 11.60%/25.15%/21.19% on the DOTA-v1.0/v1.5/v2.0 datasets, with notable gains in small and\ndensely packed object scenarios. Our method achieves a substantial speedup and accuracy boost\nwhile using less memory, showcasing its effectiveness for real-world applications."}]}