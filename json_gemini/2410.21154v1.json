{"title": "Trajectory Flow Matching with Applications to Clinical Time Series Modeling", "authors": ["Xi Zhang", "Yuan Pu", "Yuki Kawamura", "Andrew Loza", "Yoshua Bengio", "Dennis L. Shung", "Alexander Tong"], "abstract": "Modeling stochastic and irregularly sampled time series is a challenging problem found in a wide range of applications, especially in medicine. Neural stochastic differential equations (Neural SDEs) are an attractive modeling technique for this problem, which parameterize the drift and diffusion terms of an SDE with neural networks. However, current algorithms for training Neural SDEs require backpropagation through the SDE dynamics, greatly limiting their scalability and stability. To address this, we propose Trajectory Flow Matching (TFM), which trains a Neural SDE in a simulation-free manner, bypassing backpropagation through the dynamics. TFM leverages the flow matching technique from generative modeling to model time series. In this work we first establish necessary conditions for TFM to learn time series data. Next, we present a reparameterization trick which improves training stability. Finally, we adapt TFM to the clinical time series setting, demonstrating improved performance on three clinical time series datasets both in terms of absolute performance and uncertainty prediction, a crucial parameter in this setting.", "sections": [{"title": "1 Introduction", "content": "Real world problems often involve systems that evolve continuously over time, yet these systems are usually noisy and irregularly sampled. In addition, real-world time series often relate to other covariates, leading to complex patterns such as intersecting trajectories. For instance, in the context of clinical trajectories in healthcare, patients' vital sign evolution can follow drastically different, crossing paths even if the initial measurements are similar, due to the influence of the covariates such as medication intervention and underlying health conditions. These covariates can be time-varying or static, and often sparse.\nDifferential equation-based dynamical models are proficient at learning continuous variables with-out imputations [Chen et al., 2018, Rubanova et al., 2019, Kidger et al., 2021b]. Nevertheless, systems governed by ordinary differential equations (ODEs) or stochastic differential equations (SDEs) are unable to accommodate intersecting trajectories, and thus requires modifications such as augmentation or modelling higher-order derivatives [Dupont et al., 2019]. While ODEs model deterministic systems, SDEs contain a diffusion term and can better represent the inherent uncertainty and fluctuations present in many real world systems. However, fitting stochastic equations to real life"}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.1 Notation", "content": "We consider the setting of a distribution of trajectories over \\( \\mathbb{R}^d \\) denoted \\( X := \\{x^1, x^2, ..., x^n\\} \\) where each \\( x^i \\) consists of a set of trajectories of length \\( T \\) i.e. \\( x^i := \\{x_1^i, x_2^i,..., x_{T_i}^i \\} \\) with associated times \\( t^i := \\{t_1^i, t_2^i, ..., t_{T_i}^i \\} \\). Let \\( x_{[t-h,t-1]}^i \\) denote a vector of the last \\( h \\) observed time points. We denote a (Lipschitz smooth) time dependent vector field conditioned on arbitrary conditions \\( c \\in \\mathbb{R}^e \\)\n\\( v(t, x_t, X_{[t-h,t-1]}, c) \\rightarrow \\mathbb{R}^d: ([0, 1], \\mathbb{R}^d, \\mathbb{R}^{h\\times d}, \\mathbb{R}^e) \\rightarrow \\mathbb{R}^d \\) with flow \\( \\phi_t(v) \\) which induces the time-dependent density \\( p_t = \\phi_t(v)_{\\#}(p_0) \\) for any density \\( p_0 : \\mathbb{R}^d \\rightarrow \\mathbb{R}_+ \\) with \\( \\int_{\\mathbb{R}^d} p_0 = 1 \\). We also consider the coupling \\( \\pi(x_0, x_1) \\) which operates on the product space of marginal distributions \\( p_0, p_1 \\)."}, {"title": "2.2 Neural Stochastic Differential Equations", "content": "A stochastic differential equation (SDE) can be expressed in terms of a smooth drift \\( f : [0, T] \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^d \\) and diffusion \\( g : [0, T] \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^{d^2} \\) in the Ito sense as:\n\\( dx_t = f dt + g dW_t \\)\nwhere \\( W_t : [0, T] \\rightarrow \\mathbb{R}^d \\) is the d-dimensional Wiener process. A density \\( p_0(x_0) \\) evolved according to an SDE induces a collection of marginal distributions \\( p_t(x_t) \\) viewed as a function \\( p : [0, T] \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}_+ \\). In a Neural SDE [Li et al., 2020, Kidger et al., 2021a,b] the drift and diffusion terms are parameterized with neural networks \\( f_\\theta(t, x_t) \\) and \\( g_\\theta(t, x_t) \\).\n\\( dx_t = f_\\theta(t, x_t)dt + g_\\theta(t, x_t)dW_t \\)  (1)\nwhere the goal is to select \\( \\theta \\) to enforce \\( x_T \\sim X_{\\text{true}} \\) for some distributional notion of similarity such as the Wasserstein distance [Kidger et al., 2021b] or Kullback-Leibler divergence [Li et al., 2020]. However, these objectives are simulation-based, requiring a backpropagation through an SDE solver, which suffers from severe speed and stability issues. While some issues such as memory and numerical truncation can be ameliorated using the adjoint state method and advanced numerical solvers [Kidger et al., 2021b], optimization of Neural SDEs is still a significant issue.\nWe note that in the special case of zero-diffusion (i.e. \\( g_\\theta(t, x_t) = 0 \\)) this reduces to a neural ordinary differential equation (Neural ODE) [Chen et al., 2018], which is easier to optimize than SDEs, but still presents challenges to scalability."}, {"title": "2.3 Matching algorithms", "content": "Matching algorithms are a simulation-free class of training algorithms which are able to bypass backpropagation through the solver during training by constructing the marginal distribution as a mixture of tractable conditional probability paths.\nThe marginal density \\( p_t \\) induced by eq. 1 evolves according to the Fokker-Plank equation (FPE):\n\\( \\partial_t p_t = -\\nabla \\cdot (p_t f_t) + \\frac{g^2}{2} \\Delta p_t \\)  (2)\nwhere \\( \\Delta p_t = \\nabla \\cdot (\\nabla p_t) \\) denotes the Laplacian of \\( p_t \\) and gradients are taken with respect to \\( x_t \\).\nMatching algorithms first construct a factorization of \\( p_t \\) into conditional densities \\( p_t(x_t|z) \\) such that \\( p_t = E_{q(z)} [p_t(x_t|z)] \\) and where \\( p_t(x_t|z) \\) is generated by an SDE \\( dx_t = v_t(x_t|z)dt + \\sigma_t(x_t|z)dW_t \\). Given this construction it can be shown that the minimizer of\n\\( \\mathcal{L}_{\\text{match}}(\\theta) := E_{t,q(z),p_t(x/z)}[||f_\\theta(t, x_t) - v_t(x_t|z)||^2 + \\lambda^2 ||g_\\theta(t, x_t) - \\sigma_t(x_t|z)||^2] \\)  (3)\nsatisfies the FPE of the marginal \\( p_t \\). This is especially useful in the generative modeling setting where \\( q_0 \\) is samplable noise (e.g. \\( \\mathcal{N}(0, 1) \\)) and \\( q_1 \\) is the data distribution. Then we can define \\( z := (x_0, x_1) \\) as a tuple of noise and data with \\( q(z) := q_0(x_0) q_1(x_1) \\). This makes eq. 3 optimize a model which will draw new samples according to the data distribution \\( q_1(x_1) \\) using\n\\( x_0 \\sim q_0; \\ \\ x_1 = \\int_0^1 f_\\theta(t, x_t)dt + g_\\theta(t, x_t)dW_t \\)  (4)\nwith the integration computed numerically using any off-the-shelf SDE solver. While this is guaran-teed to preserve the distribution over time, it is not guaranteed to preserve the coupling of \\( q_0 \\) and \\( q_1 \\) (if given).\nPaired bridge matching In generative modeling random pairings [Liu et al., 2023c, Albergo and Vanden-Eijnden, 2023, Albergo et al., 2023] or optimal transport [Tong et al., 2024, Pooladian et al., 2023] pairings are constructed for the conditional distribution \\( q(z) \\). However, in some problems we would like to match pairs of points as is the case in image-to-image translation [Isola et al., 2017, Liu et al., 2023a, Somnath et al., 2023]. In this case, training data comes as pairs \\( (x_0, x_1) \\). In this case we set \\( q(z) := q(x_0, x_1) \\) to be samples from these known pairs, and optimize eq. 3. While empirically, these models perform well, there are no guarantees that the coupling will be preserved outside of the special case when data comes from the (entropic) optimal transport coupling \\( \\pi_{\\epsilon}^*(q_0, q_1) \\) and defined as:\n\\( \\pi_{\\epsilon}^*(q_0, q_1) = \\underset{\\pi \\in \\mathcal{U}(q_0, q_1)}{\\text{arg min}} \\int d(x_0, x_1)^2 d\\pi(x_0, x_1) + \\epsilon KL(\\pi || q_0 \\otimes q_1), \\)  (5)"}, {"title": "3 Trajectory Flow Matching", "content": "We now describe our simulation-free method to learn SDEs on time-series data using trajectory flow matching as summarized in Alg. 1. In the case of time series we need to ensure that trajectory couplings are preserved. We first set out a general algorithm for flow matching on vector fields in \u00a73.1 then present a numerical reparameterization which we find stabilizes training in \u00a73.2, a next observation prediction for irregularly sampled time series in \u00a73.3, and finally present how to learn the noise in \u00a73.4."}, {"title": "3.1 Preserving Couplings", "content": "In this section, we assume access to fully observed and evenly spaced trajectories \\( X = (x^1, x^2, ..., x^n) \\) with \\( x^i := (x_1^i, x_2^i,..., x_{T_i}^i) \\) for clarity and notational simplicty. We note that our method is easily extensible to the more general setting of irregularly sampled trajectories. In this simplified case we let\n\\( z := (x_1, x_2, ..., x_T) \\)  (6)\n\\( q(z) := U(X) \\)  (7)\n\\( p_t(x|z) := \\mathcal{N}(([t] - t)x_{[t]} + (t - [t])x_{[t]}, \\sigma^2([t] - t)(t - [t])I) \\)  (8)\n\\( u_t(x|z) := \\frac{x_{[t]} - x_{[t]}}{[t]-t} \\)  (9)\nwhere \\( U(X) \\) is the uniform empirical distribution over \\( X \\), \\( [.], \\lceil \\cdot \\rceil \\) are the ceiling and floor functions, and \\( \\mathcal{N}(\\cdot, \\cdot) \\) is the multivariate normal distribution. This is a valid regression in the sense that a function minimized with Alg. 1 will return a stochastic process that will match the observed marginal distributions over time as shown in the following lemma."}, {"title": "3.2 Target prediction reparameterization", "content": "While flow matching generally predicts the flow, there is a target predicting equivalent namely given \\( v_\\theta(t,x) := \\frac{x_{[t]} - x_{[t]}}{[t]-t} \\) and \\( u_t(x|z) := \\frac{x_1 - x_0}{t_1 - t_0} \\) which is equivalent to \\( x_1 - x_0 \\) when \\( x_t := tx_1 + (1 - t)x_0 \\), then it is easy to show that the target predicting loss is equivalent to a time-weighted flow-matching loss. Specifically let the target predicting loss be\n\\( \\mathcal{L}_{\\text{target}}(\\theta) = E_{t,q(z),p_t(x/z)} ||\\hat{x}(t, x) - x_{[t]}||^2 \\)  (10)\nthen it is easy to show that"}, {"title": "3.3 Irregularly sampled trajectories", "content": "We next consider irregularly sampled time series of the form \\( x^i := ((x_1^i, t_1), (x_2^i, t_2), ..., (x_{T_i}^i, t_{T_i})) \\) with \\( t_1 < t_2 < ... < t_{T_i} \\) with \\( t_{\\text{next}} \\) denoting the next timepoint observed after time t. In this case, when combined with the target predicting reparameterization in \u00a73.2, we can predict the time till next observation. We therefore parameterize an auxiliary model \\( h_\\theta(t, x_t) : [0, T] \\times \\mathbb{R}^d \\rightarrow [0, T] \\) which predicts the next observation time. This is useful numerically, but also, perhaps more importantly, is useful in a clinical setting, where the spacing between measurements can be as informative as the measurements themselves [Allam et al., 2021]. \\( h_\\theta \\) is trained to predict the time till the next observation with the time predictive loss:\n\\( L_{ip}(\\theta) = \\sum_{t \\in T^i} ||h_\\theta(t, x_t) - (t_{\\text{next}} - t)||^2 \\)  (11)\nwhere \\( t_{\\text{next}} \\) is the time of the next measurement. This can be used in conjunction with the \\( x_{\\text{next}} \\) predictor to calculate the flow at time t as\n\\( v_\\theta(t, x_t) := \\frac{\\hat{x}(t, x_t) - x_t}{h_\\theta(t, x_t) - t} \\)  (12)\nwhich can be used for inference on new trajectories."}, {"title": "3.4 Uncertainty prediction", "content": "Finally, we consider uncertainty prediction. till now we have defined conditional probability paths using a fixed noise parameter \\( \\sigma \\). However, this does not have to be fixed. Instead, we consider a learned \\( \\sigma_\\theta(t, x_t) \\) which can be learned iteratively with the loss:\n\\( \\mathcal{L}_{\\text{uncertainty}}(\\theta, x) = \\sum_{t \\in T} ||\\sigma_\\theta(t, x_t) - || \\frac{\\sigma_\\theta(t, x_t) - x_{\\text{next}} ||^2}{2} ||^2 \\)  (13)\nwhich learns to predict the error in the estimate of \\( x_t \\). This loss can be interpreted as training an epistemic uncertainty predictor which is similar to that proposed in direct epistemic uncertainty prediction (DEUP) [Lahlou et al., 2023]."}, {"title": "A Proof of theorems", "content": "We first prove a Lemma which shows TFM learns valid flows between distributions with the target prediction reparameterization trick."}, {"title": "Experimental Details", "content": ""}, {"title": "B.1 1D Oscillators", "content": "The three oscillation trajectories correspond to c = 0.25 (the red trajectory in Figure 2), c = 2 (blue), and c = 3.75 (green). Before used as an input, t was scaled to between 0 and 1 by dividing by 10."}, {"title": "B.2 Clinical Datasets", "content": ""}, {"title": "B.2.1 Clinical Data Characteristics", "content": "In order to accurately model the perturbations in the physiologic signals (mean arterial pressure and heart rate) of the underlying patient states, we need to learn beyond the general trend of the data.\nWhile the physiologic measurements themselves reflect patient status and drive clinical decision making, the degree of variation holds information that goes beyond the snapshot at a single time point. Our approach models the data distribution and stochasticity rather than just fitting the average trajectory. Other time-varying such as treatment conditions and non-time-varying covariates such as underlying disease states may also hold information that may impact the underlying state generating the physiologic signals. Our approach also incorporates this information to inform the trajectory modeling."}, {"title": "B.2.2 Clinical Data Preprocessing", "content": "For each clinical dataset, we modeled patient trajectories formed with heart rate and blood pressure measurements during the first 24 hours following admission. The timeline for each trajectory, originally in minutes, was scaled to a range between 0 and 1 by dividing by 1440. Additionally, heart rate and blood pressure values were z-score normalized to standardize the data.\nIntensive Care Unit Sepsis (ICU Sepsis) Dataset The eICU Collaborative Research Database v2.0 [Pollard et al., 2019] is a database including deidentified information collected from over 200,000 patients in multiple intensive care units (ICUs) in the United States from 2014 to 2015. The ICU Sepsis Dataset was created by subsetting the eICU Database for 3362 patients with sepsis as the"}]}