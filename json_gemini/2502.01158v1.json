{"title": "MIND: Modality-Informed Knowledge Distillation Framework\nfor Multimodal Clinical Prediction Tasks", "authors": ["Alejandro Guerra-Manzanares", "Farah E. Shamout"], "abstract": "Multimodal fusion leverages information across modalities to learn better feature representa-\ntions with the goal of improving performance in fusion-based tasks. However, multimodal\ndatasets, especially in medical settings, are typically smaller than their unimodal counterparts,\nwhich can impede the performance of multimodal models. Additionally, the increase in the\nnumber of modalities is often associated with an overall increase in the size of the multimodal\nnetwork, which may be undesirable in medical use cases. Utilizing smaller unimodal encoders\nmay lead to sub-optimal performance, particularly when dealing with high-dimensional clini-\ncal data. In this paper, we propose the Modality-INformed knowledge Distillation (MIND)\nframework, a multimodal model compression approach based on knowledge distillation that\ntransfers knowledge from ensembles of pre-trained deep neural networks of varying sizes into\na smaller multimodal student. The teacher models consist of unimodal networks, allowing the\nstudent to learn from diverse representations. MIND employs multi-head joint fusion models,\nas opposed to single-head models, enabling the utilization of unimodal encoders in the case of\nunimodal samples without requiring imputation or masking of absent modalities. As a result,\nMIND generates an optimized multimodal model, enhancing both multimodal and unimodal\nrepresentations. It can also be leveraged to balance multimodal learning during training.\nWe evaluate MIND on binary classification and multilabel clinical prediction tasks using\nclinical time series data and chest X-ray images extracted from publicly available datasets.\nAdditionally, we assess the generalizability of the MIND framework on three non-medical\nmultimodal multiclass benchmark datasets. The experimental results demonstrate that\nMIND enhances the performance of the smaller multimodal network across all five tasks, as\nwell as various fusion methods and multimodal network architectures, compared to several\nstate-of-the-art baselines.", "sections": [{"title": "Introduction", "content": "Healthcare decision-making, like most human-based active reasoning (Smith & Gasser, 2005), is inherently\nmultimodal. This means that clinical decisions are typically informed by multiple sources of information or\nmodalities, such as diagnostic imaging, clinical time-series data, or medical history, combined with clinical\nexperience and expertise (Bate et al., 2012; Pauker & Kassirer, 1980; Pines et al., 2023). As a result,\nrecent research has demonstrated the potential of multimodal learning in modeling various clinical prediction\ntasks (Huang et al., 2020), particularly in information fusion to enhance downstream classification performance\ncompared to relying solely on a unimodal network (Hayat et al., 2022; Yang et al., 2022; Wang et al., 2020a).\nNote that the term unimodal model refers to a modality-specific neural network, i.e., a model that processes\n1"}, {"title": "Background", "content": "samples consisting of a single data modality (e.g., image), while the term multimodal model refers to a\nnetwork that processes samples consisting of multiple data modalities.\nDespite recent advancements, the nature of the learning process in multimodal clinical prediction tasks\nremains unclear. Most research primarily focuses on achieving optimal fusion performance (Huang et al.,\n2020), often overlooking additional challenges of multimodal training, such as optimizing modality utilization\nand modeling cross-modal interactions for effective training (Huang et al., 2022b; Wang et al., 2020b; Wu\net al., 2022b). For instance, recent findings indicate that multimodal deep neural networks are prone to\noverfitting due to their larger capacity (Wang et al., 2020b). They tend to rely primarily on one of the\navailable input modalities, specifically the one that is fastest to learn from (Wu et al., 2022b). Since different\nmodalities may generalize and overfit at different rates, naive joint training often leads to sub-optimal results\n(Wang et al., 2020b).\nThe methodological challenges of multimodal learning are coupled with other challenges when applied in\nhealthcare. This includes the limited availability of large-scale multimodal clinical datasets, as well as\nthe sparse and heterogeneous nature of the input modalities, such as when fusing medical images with\nclinical time-series data. The increase in the number and heterogeneity of modalities entails the use of\nmodality-specific encoders, such as convolutional neural networks for images and recurrent neural networks for\ntime-series data (Hayat et al., 2022). This requirement increases the overall size of the multimodal network,\nwhich may hinder deployment in clinically relevant use cases such as privacy-preserving training (Baruch\net al., 2022), secure inference (Lou et al., 2021), and applications on resource-constrained devices (Zhao et al.,\n2018).\nOur objective in this work is to improve the compression of multimodal networks, both in terms of size and\npredictive performance when dealing with small multimodal datasets. We study this within the context of joint\nfusion, where two input data modalities have their encoded representations aggregated before applying a fusion\nlayer. To achieve this objective, we propose a training framework based on knowledge distillation that incurs\nminimal modifications to the joint fusion architecture and learning objective, while significantly enhancing\npredictive performance and reducing overall size. We refer to this training framework as Modality-INformed\nknowledge Distillation (MIND). Specifically, we make the following contributions:\nWe propose integrating modality-specific supervision signals into the joint fusion architecture. This\napproach enhances the representations of unimodal encoders by reducing the influence of the global\nmodel, allowing for a stronger focus on each unimodal encoder. As a result, this leads to enhanced\nperformance in multimodal fusion.\nWe propose pre-training unimodal teachers to distill knowledge into modality-specific signals, com-\npressing the knowledge from an ensemble of larger unimodal models and enhancing the representations\nlearned by the modality encoders of the student model. Our results demonstrate that this approach\nsignificantly improves fusion performance while utilizing a more compact student network.\nWe introduce additional weighting hyper-parameters to emphasize modality-specific learning. Our\nempirical results demonstrate that this approach improves both unimodal and fusion (multimodal)\nperformance, while also helping to balance modality learning during multimodal training."}, {"title": "Methods", "content": "Preliminaries. In a simple multimodal fusion task, we assume the presence of two input modalities\nrepresented by XA and XB. The goal of the multimodal fusion task is to jointly learn from both modalities to\npredict a set of ground-truth labels denoted by y. Due to heterogeneity, each modality is first processed by\nmodality-specific encoders, such that Za = fA(XA) and ZB = fB(XB), where za and Z\u00df are latent feature\nrepresentations. These latent representations are concatenated and processed by a fusion classification layer,\nsuch that \u0177AB = 9AB(ZA,ZB). We then apply the binary cross-entropy (BCE) loss, denoted by LSAB(\u0423,\u04f0AB),\nand optimize fA, B, and gAB jointly, as shown in Figure 1 A.1.\nMultimodal loss. We incorporate two additional loss terms by introducing classification modules for each\nmodality, such that \u0177A = 9A(ZA) and \u0177B = 9B(ZB). Hence, the overall supervised learning loss becomes:\n$L_S = L_{SAB} (Y, \\hat{y}_{fusion}) + L_{SA}(Y, \\hat{y}_A) + L_{SB} (y, \\hat{y}_B)$   (2)\nThis modification allows for the independent use of unimodal classification heads in the event of unimodal\nsamples at inference time, as illustrated in Figure 1 A.2. Note that the term unimodal sample refers a data\nsample that contains only one modality (e.g., A or B), whereas a multimodal sample involves the presence of\nmore than one modality (e.g., A and B).\n4"}, {"title": "Methods", "content": "Unimodal teachers. We use KD to compress the knowledge of an ensemble of unimodal teacher models into\na single, typically smaller student model. KD was originally conceived for multiclass classification problems,\nwhere different values of 7 are used to generate the soft targets. In our work, we propose to use KD for a wider\nrange of predictive tasks including binary and multilabel classification tasks. For multilabel classification, we\nmodify the LKD term in Equation 1 to:\n$L_{K D}(\\hat{y} s, \\hat{y} t)=\\frac{1}{N} \\sum_{i=1}^{N} B C E(\\hat{y} s^{i}, \\hat{y} t^{i})$   (3)\nwhere L is the set of possible labels in a multilabel classification problem, N is the sample size, and s and t\ndenote the student and teacher networks, respectively.\nGiven an ensemble T of K teachers, we define a new average ensemble prediction for an i-th label (omitting i\nwithout loss of generality):\n$\\hat{y}^{T}=\\frac{1}{K} \\sum_{k=1}^{K} \\hat{y}^{k},$   (4)\nsuch that the Ensemble Knowledge Distillation (EKD) loss is now denoted as LEKD(\u0177,\u0177T).\nIn a comprehensive multimodal setting, the goal of the classification model is to accurately classify both\nmultimodal and unimodal samples, which is particularly relevant in healthcare applications where some\nmodalities may be absent in a sample. To improve the representations of multimodal and unimodal encoders,\nwe introduce LEKDU, which consists of an ensemble of unimodal teachers trained with sub-components of\nEquation 2 for each modality, namely LSA and LSB. Hence, we introduce a new learning objective term\nfor each input modality to distill knowledge from unimodal teachers to the modality encoders of a smaller\nmultimodal student. In the case of two modalities (A and B), two terms are introduced:\nWe note that goal of the multimodal student network is to learn the distribution P(Y|XA, XB). To enhance\nencoder representations and improve the learning of the multimodal distribution, we introduce the unimodal\nEKD components, consisting of LEKDU and LEKDU, which represent the distributions P(Y|XA) and\nP(Y|XB), respectively, as learned by the unimodal teachers.\nBalancing multimodal training. We introduce weighting hyper-parameters WA and WB to adjust the\nfocus on the distillation components. This enhances encoder representation and modality learning, resulting\nin the overall loss defined as follows:\n$L_{M I N D}=L_{S A B}+L_{S A}+L_{S B}+w_{A} \\times L_{E K D U_{A}}+w_{B} \\times L_{E K D U_{B}}$  (6)\nA visual representation of the proposed MIND framework is provided in Figure 1 B. In particular, setting\nWA, WB > 1 minimizes both loss components (supervision signals and EKD), emphasizing distillation for\nunimodal encoder learning. Specifically, WA,WB \u226b 1 prioritize knowledge distillation, while WA,WB = 1\ntreat all loss components equally. When WA \u00bb WB, the learning focuses on modality A, whereas WA\u0391 \u00ab\u03c9\u0392\nprioritizes modality B. This independent weighting can be leveraged to balance multimodal learning by\nemphasizing the less utilized modality with a larger weight. Setting WA, WB 0 disables distillation, reverting\nthe learning objective to Equation 2.\nOverall, the MIND framework produces a smaller, optimized version of the original multimodal model. This\ncompressed model can make predictions using both multimodal (when both modalities are present) and\n5"}, {"title": "Experiments", "content": "Datasets. To evaluate our approach, we focus on two clinical prediction tasks: predicting clinical conditions\n(L = 25, Equation 3), and predicting in-hospital mortality after a 48-hour ICU stay (L = 1, Equation 3),\nusing Chest X-Ray (CXR) images and clinical time-series data extracted from the patient's Electronic Health\nRecords (EHR). We use chest X-ray images from MIMIC-CXR (Johnson et al., 2019b), where each image\n(xx) is replicated across three channels, yielding X\u0104 \u2208 R224\u00d7224\u00d73. Associated clinical time-series data (XB)\nis obtained from MIMIC-IV (Johnson et al., 2023), with dimensions XB \u2208 Rt\u00d776, where t represents the\nnumber of time-steps based on the patient's ICU stay duration, and 76 denotes the number of pre-processed\nfeatures per time-step. We follow the dataset splits and multimodal architecture from the work of Hayat et al.\n(2022), where fa is parameterized by a ResNet-34, fB is parameterized as a two-layer LSTM and the fusion\nmodel gAB is parameterized as an LSTM layer, referred to as MedFuse. We use the multimodal dataset for\ntraining the MIND framework and baselines, containing samples with both modalities present. For the clinical\nconditions task, the dataset comprises 7,728 training, 877 validation, and 2,161 test samples. For in-hospital\nmortality prediction, it consists of 4,885 training, 540 validation, and 1,373 test samples. When training\nunimodal models, we utilize all available data for each modality. Specifically, the CXR dataset comprises\n124,671 training, 15,282 validation, and 36,625 test samples, while the EHR dataset contains 42,628 training,\n4,802 validation, and 11,914 test samples.\nEnsembles. In the MIND framework, we first pre-train the unimodal teachers using the available unimodal\ndatasets. To ensure diversity, we train multiple models for each modality, following the architectures outlined\nin previous work (Hayat et al., 2022). For chest X-ray images, we train ResNet-34, ResNet-18, and ResNet-10\n6"}, {"title": "Experiments", "content": "models. Similarly, for clinical time-series data, we train 2-layer, 3-layer, and 4-layer LSTM networks. The\ntop three performing models from each modality are then combined to create ensembles. These ensembles\nof unimodal teachers are utilized to distill knowledge, as described in Equation 6, into a smaller randomly\ninitialized multimodal student model. This student model consists of a ResNet-10 and a 2-layer LSTM,\nserving as encoders for chest X-ray and time-series data, respectively.\nBaselines. In this work, we focus on joint fusion, such that both encoders are trained from scratch (randomly\ninitialized), although previous work primarily focuses on fine-tuning pre-trained unimodal encoders (Huang\net al., 2020). We believe that learning with randomly initialized encoders is more appropriate in our\nproposed setup for several reasons. First, randomly initialized encoders establish a consistent baseline for\nfair comparisons across models, ensuring that any performance improvements reflect the architecture and\ntraining methods rather than pre-trained weights. Additionally, starting with random initialization helps\nisolate the specific learning dynamics of our setup, avoiding the potential of skewing the performance as a\nresult of using fine-tuned pre-trained encoders. Our approach also introduces unique elements that may not\nalign with assumptions of previous work, such as avoiding the need for pre-training the student model, so we\ncan assess how well the model learns from scratch to understand its full capabilities. Finally, using randomly\ninitialized encoders eliminates biases from original training data associated with fine-tuning, allowing for a\nmore objective evaluation of our method.\nWe compare the performance of MIND with the original multimodal model, MedFuse. The MIND model is\nthree times smaller (in terms of learnable parameters) than MedFuse, as shown in Table A1. Further, we\nmodify the original MedFuse architecture to adopt the loss introduced in Equation 2, denoted as MedFuse-3H.\nWe also compare MIND to other related KD baselines, including TS (Wang et al., 2020a), MKE (Xue et al.,\n2021), and UME (Du et al., 2023). For MKE, we evaluate two models: MKE-CXR and MKE-EHR. Further\ndetails about the baseline models can be found in Appendix A.4. All MIND and baseline models trained for\neach specific task are all trained using the same dataset sizes, as described in Appendix A.2 and Appendix\nC.1.\nHyperparameter Tuning. All models, including baselines, are trained for a maximum of 300 epochs with\nearly stopping after 40 epochs. We use a batch size of 16 and the Adam optimizer across all experiments.\nHyperparameter tuning involves optimizing the learning rate and weighting parameters for MIND and\nall baselines. Further details on hyperparameter tuning are provided in Appendix A. We select the best\nmodels based on the checkpoint yielding the highest Area Under the Receiving Operating Characteristic\n(AUROC) on the validation set and present the results on the test set in terms of AUROC and Area Under\nthe Precision-Recall Curve (AUPRC). We also report 95% confidence intervals with 1,000 iterations using the\nbootstrapping method (Efron & Tibshirani, 1994). For reproducibility, our code is included in Appendix A.5\nas we aim to make it publicly available, following the pseudo-code implementation of the MIND framework\nin Algorithm 1.\nWe conduct our experiments on a shared high-performance computing cluster equipped with Nvidia A100\nGPUs. For the clinical conditions task, training the models takes less than 20 hours (120-150 epochs), while\nfor in-hospital mortality prediction, the models are trained in under two hours."}, {"title": "Results", "content": "Table 1 compares the performance of multimodal baselines with our proposed model on the test set. The\nMIND framework achieves the highest AUROC and AUPRC across both tasks, outperforming all baselines by\nover 1.4% AUROC and 2.3% AUPRC for clinical conditions prediction, and 1.6% AUROC and 1.4% AUPRC\nfor in-hospital mortality prediction. Notably, MIND surpasses MedFuse and MedFuse-3H by over 3.4%\nAUROC and 5.4% AUPRC for clinical conditions, and by over 2.4% AUROC and 3.7% AUPRC for in-hospital\nmortality. Validation set results are in Appendix B.1. The results indicate that the MIND framework excels\nat compressing knowledge from unimodal teacher models while enabling unimodal predictions for unimodal\nsamples an advantage not present in the compared frameworks."}, {"title": "Quality of unimodal representations", "content": "We also evaluate the quality of the unimodal representations. The test set results are summarized in Table 2\nand Table 3 for the clinical conditions and in-hospital mortality tasks, respectively. Additional results on the\nvalidation set are provided in Appendix B.3. For clinical conditions prediction using X-ray images, the MIND\nmodel achieves a superior AUROC (0.709 vs. 0.663) and AUPRC (0.409 vs. 0.349) compared to MedFuse-3H.\nSimilarly, for time-series data, the MIND model outperforms MedFuse-3H with an AUROC of 0.746 vs. 0.715\nand an AUPRC of 0.451 vs. 0.404. In the in-hospital mortality task, the MIND model improves chest X-ray\nmodel performance by over 10% compared to MedFuse-3H while maintaining similar performance for clinical\ntime series. These results demonstrate that the MIND model not only enhances multimodal performance but\nalso ensures unimodal performance is on par with unimodal models trained on much larger datasets (124,671\nfor CXR and 42,628 for EHR vs. 7,728 paired CXR-EHR samples)."}, {"title": "Ablation study I: sensitivity analysis", "content": "We conduct ablation studies to understand the impact of each component in Equation 6. To evaluate the\nbenefits of ensembling, we experiment with both single-teacher KD and EKD, using three models per modality\nensemble. Specifically, we consider six settings:\nSupervised learning as in previous work (Hayat et al., 2022), without knowledge distillation.\nSupervised learning with modified loss (Equation 2), without knowledge distillation.\nSupervised learning with unweighted unimodal single-teacher KD. Specifically, we set WA, WB 1 in\nEquation 6 and use a single unimodal model as the modality teacher.\nSupervised learning with weighted unimodal single-teacher KD. We apply Equation 6 using a single\nunimodal model as the modality teacher."}, {"title": "Ablation study II: balancing multimodal learning with WA and WB", "content": "We conduct a hyperparameter sensitivity analysis for the weighting parameters WA and WB introduced in\nEquation 6. These parameters help the model focus on unimodal representation learning, thereby improving\nthe quality of unimodal predictions, as shown in Section 5.2. We evaluate their impact on enhancing the\nbalance of multimodal learning using the conditional utilization rate per modality (Wu et al., 2022b), which\nwe adopt to characterize modality usage in the multimodal deep neural network. Specifically, we modify u\nfor joint fusion multimodal neural networks as follows:\n$u_{A}=\\frac{A(\\hat{y}_{A B})-A(\\hat{y}_{B})}{A(\\hat{y}_{A B})}, u_{B}=\\frac{A(\\hat{y}_{A B})-A(\\hat{y}_{A})}{A(\\hat{y}_{A B})}$\nwhere A and B are two modalities without loss of generality, A(\u00b7) denotes a classification accuracy metric,\nUA computes the conditional utilization rate for modality A, and up for modality B. The conditional\nutilization rate measures the marginal contribution of each modality to the fusion model. Following Wu\net al. (2022b), dutil is defined as the difference between conditional utilization rates: dutil = UA \u2013 UB. This\nallows for the assessment of imbalanced modality usage within the multimodal fusion model. Specifically,\ndutil \u2208 R s.t. \u2013 1 < dutil < 1, with extreme values indicating imbalanced modality usage.\nWe use the validation AUROC as the accuracy metric and test six settings: (i) Wear = Wehr = 0, (ii)\nWc\u00e6r < Wehr, (iii) Wc\u00e6r < Wehr, (iv) Wcxr = Wehr, (v) Wcxr > Wehr, and (vi) wcar > Wehr. Figure 3 shows the\nconditional utilization rates and their differences for the clinical conditions task. Table B7 details the specific\nvalues of the hyper-parameters. In the setting where wear = Wehr = 0, the fusion model tends to overfit the\nEHR modality, with utilization rate differences reaching up to 10%. Assigning significantly different weights\nto the modalities, as seen in the Wcxr > Wehr and Wcxr < Wehr settings, increases the conditional utilization\nrate of the modality with the larger weight while decreases it for the other, thereby affecting the balance of\nutilization rates during training. This effect can be leveraged to achieve more balanced multimodal learning,"}, {"title": "Additional Results on Multimodal Benchmark Datasets", "content": "The results in Section 5.1 demonstrate the utility of the MIND framework in clinical settings, which is the\nprimary motivation for our study, particularly for multilabel and binary clinical tasks, using a state-of-the-art\narchitecture as the base model. To evaluate the generalization and applicability of the MIND framework\nacross diverse multimodal datasets, tasks, and architectures, we conducted additional validation on three\nmultimodal, multiclass benchmark datasets: CREMA-D (Cao et al., 2014), S-MNIST (Khacef et al., 2019),\nand LUMA (Bezirganyan et al., 2024). We report the best accuracy results for each model in Table 5. For\nthese experiments, the feature encoders consist of ResNet-3/6 models, with the output from each encoder\nconcatenated and used as input for the fusion model, which is a linear layer. Additional implementation\ndetails, such as dataset descriptions, the multimodal architectures employed, and their sizes, can be found in\nAppendix C.\nAs shown in Table 5, the MIND model consistently outperforms all multimodal baselines across all datasets.\nAdditionally, the performance of the unimodal encoders either surpasses or is comparable to that of the\nbaselines. Note that UME does not involve any multimodal training; it simply averages the predictions of\nthe unimodal models to compute the multimodal prediction. We observe that the MIND model improves\nmultimodal prediction by an average of 1.5% and boosts the performance of the weaker modality by\napproximately 5.2% across tasks."}, {"title": "Discussion", "content": "Recent multimodal learning research highlights the challenges of learning from multiple modalities (Wu et al.,\n2022b; Wang et al., 2020b). While increasing model size is sometimes feasible, it is not always desirable.\nAdditionally, multimodal models often overfit to one modality, under-utilizing others. In this work, we\npropose MIND, a simple yet effective KD framework that distills knowledge from pre-trained unimodal\nteachers to a smaller multimodal student, enhancing both multimodal and unimodal predictive power. Our\nexperimental results demonstrate significant benefits from this approach, likely due to unimodal encoders\nleveraging larger training datasets and learning better representations than those trained on the comparably\nsmaller multimodal clinical datasets. To further improve ensemble impact, we hypothesize that stronger\nteachers and potentially a larger number of teachers are needed (Hinton et al., 2015). Notably, our framework\nis architecture-agnostic and can be easily adapted to similar settings in other application domains.\nWhile Bucilu\u0103 et al. (2006), Fukuda et al. (2017), Asif et al. (2020), Li et al. (2021), and Wu et al. (2022a)\ncombine ensemble learning and offline response-based knowledge distillation to train compact networks, their\napproaches primarily focus on the unimodal networks. In contrast, we are concerned with the particularities\nof multimodal joint fusion networks, both in performance and size, thus we propose incorporating and\nweighting modality-specific ensembles during student knowledge distillation as a distinctive feature. Despite\nmethodological differences, MIND similarly avoids the need for a large ensemble of classifiers, which demands\nsignificant resources during inference. In addition, the MIND framework can be used to address imbalanced\nmultimodal learning during training and leverages modality encoders to handle unimodal samples, achieving\npredictive performance for unimodal samples that is on par with powerful unimodal models.\nThe introduction of the new learning objective (Equation 6) in our proposed approach does not compromise\nits applicability to real-world scenarios, particularly regarding its scalability as the number of modalities, M,\nincreases. For two modalities, as defined in Equation 6, the loss consists of five components: three supervised\nlearning losses and two weighted unimodal ensemble knowledge distillation losses. For M = 3, the proposed\nloss would contain seven components: four supervised losses and three weighted unimodal ensemble knowledge\ndistillation losses, incorporating one additional supervised loss and one knowledge distillation loss for the\nnew modality. Generalizing to M modalities, the proposed loss would have 2M + 1 loss terms, i.e., O(M),\nmaking our approach linearly scalable for practical settings."}, {"title": "Broader Impact Statement", "content": "Potential Risks of Malicious Usage of Medical Models. The deployment of machine learning models\nin healthcare carries significant risks if misused. Malicious actors could potentially exploit these models to\nmanipulate medical diagnoses, treatment plans, or patient data. For instance, altering a model's predictions\ncould lead to incorrect diagnoses or inappropriate treatments, posing severe health risks to patients. In\naddition, the unauthorized access and manipulation of sensitive clinical data could lead to privacy breaches\nand identity theft.\nImplicit Biases Learned from Knowledge Distillation. Machine learning models trained on clinical\ndatasets are susceptible to inheriting and amplifying existing biases present in the training data. Knowledge\ndistillation, a model training process where a smaller model learns from larger, pre-trained models, can\npropagate these biases. This can result in unequal treatment outcomes for different demographic groups,\nexacerbating health disparities. For instance, if the training data contains biases against certain ethnicities or\ngenders, the distilled model may continue to exhibit these biases, leading to unfair treatment recommendations.\nPrivacy Concerns with MIMIC-IV and MIMIC-CXR data. The clinical datasets used in this\nresearch are MIMIC-IV (Johnson et al., 2023) and MIMIC-CXR (Johnson et al., 2019b). The authors would\nlike to note that there are no privacy concerns related to these datasets. In particular, the MIMIC-IV\nand MIMIC-CXR datasets are de-identified and adhere to strict privacy regulations, ensuring that patient\nconfidentiality is maintained (Johnson et al., 2024; 2019a). This allows researchers to utilize real-world clinical\ndata for developing and testing machine learning models without compromising patient privacy.\nWhile machine learning holds great promise for advancing healthcare delivery, it is critical to consider and\naddress these broader impact concerns. Ensuring robust security measures, mitigating biases, and maintaining\npatient privacy are essential steps towards the responsible and ethical use of machine learning in healthcare."}, {"title": "Conclusion", "content": "Multimodal learning offers potential enhancements for clinical prediction tasks by leveraging cross-modality\ninteractions. However, integrating multiple modalities can lead to increased network complexity and size,\nposing challenges for resource-constrained applications. Additionally, multimodal clinical data present hurdles\nsuch as modality heterogeneity and missing data. Overall, our work addresses these challenges through\nweighted ensemble knowledge distillation, providing a promising approach to enhance fusion networks for\nreal-world multimodal clinical data. Finally, we demonstrate the generalizability of our proposed approach by\nvalidating it across three multimodal benchmark datasets, as well as two clinical tasks, two fusion methods,\nand various multimodal network architectures."}, {"title": "Implementation details", "content": "A.1 Model size\nTable A1 shows the sizes of the different model architectures. MedFuse (Hayat et al., 2022) serves as the base\narchitecture for our work and is provided as a reference. While it was originally proposed using ResNet-34\nand 2-layer LSTM encoders, we adopt the same configuration for MIND (ResNet-10 and 2-layer LSTM) and\nall baselines for the sake of comparison. Notably, the MIND model is three times smaller than the original\narchitecture while providing unimodal classification capabilities.\nA.2 Clinical dataset\nIn our experimental setup, we use two publicly available datasets for two clinical prediction tasks: (i)\nmultilabel clinical conditions prediction and (ii) in-hospital mortality prediction. For modality A, we use\nchest X-ray images (CXR) extracted from the MIMIC-CXR dataset (Johnson et al., 2019b). For modality B,\nwe use electronic health record (EHR) time-series data extracted from the MIMIC-IV dataset (Johnson et al.,\n2023). We link the modalities following Hayat et al. (2022). The dataset comprises all clinical time-series\nthat have an associated chest X-ray image, ensuring that both modalities are present for each sample. We\nsummarize the dataset in Table A2.\nA.3 MIND architecture & implementation\nWe use MedFuse (Hayat et al., 2022) as the multimodal baseline for our experimental setup. The original\nimplementation of MedFuse employs an LSTM-based fusion module that processes a sequence of modality\nrepresentations provided by a ResNet-34 encoder for the chest X-ray images and a 2-layer LSTM encoder for\nthe clinical time-series data. In the MIND framework, we reduce the size of the image encoder to a ResNet-10\nmodel to achieve model compression. In our experiments, we adhere to the hyperparameters used in the\noriginal MedFuse implementation for the randomly initialized multimodal fusion network."}, {"title": "Implementation details", "content": "For the MIND model and all baselines", "encoders": "a\nResNet-34 for CXR images and a 2-layer LSTM for time-series data. The outputs from the encoders\nare concatenated and passed through an LSTM layer before the final multimodal classification head. To\nincorporate their methodology", "https": "github.com/nyuad-\ncai/MedFuse. In our experiments, we perform learning rate tuning while adopting the default settings for the\nother hyper-parameters.\nMedFuse-3H. Like most multimodal networks, the original MedFuse architecture does not include a\nclassification head on the modality encoders, resulting in an exclusively multimodal output. In our MIND\nframework, the first step is to modify the multimodal network architecture to leverage the encoders for\nsamples with absent modalities, thus avoiding the need of masking or imputation. We extend the original\nMedFuse architecture to include three classification heads (3H), naming it MedFuse-3H, and use it as a\nbaseline for our approach. In our experiments, We perform learning rate tuning while adopting the default\nMedFuse settings for the other hyper-parameters.\nTS (Wang et al., 2020a) proposes a response-based knowledge distillation framework where teacher models,\ntrained with large datasets that include samples with missing modalities, transfer knowledge via soft\nlabels to a multimodal student model. The multimodal student is trained using the soft labels from\nthe unimodal teachers along with the supervision loss, as shown in Table A4. Originally proposed for\nmulticlass classification, we adapt it for our multilabel classification framework. We perform random hyper-\nparameter tuning of the learning rate and the \u03b1, \u03b2 distillation coefficients. Following Wang et al. (2020a),\n\u03b1"}]}