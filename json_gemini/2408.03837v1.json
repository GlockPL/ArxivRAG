{"title": "WALLEDEVAL: A Comprehensive Safety Evaluation Toolkit for Large Language Models", "authors": ["Prannaya Gupta", "Le Qi Yau", "Hao Han Low", "I-Shiang Lee", "Hugo M. Lim", "Yu Xin Teoh", "Jia Hng Koh", "Dar Win Liew", "Rishabh Bhardwaj", "Rajat Bhardwaj", "Soujanya Poria"], "abstract": "WALLEDEVAL is a comprehensive AI safety testing toolkit designed to evaluate large language models (LLMs). It accommodates a diverse range of models, including both open-weight and API-based ones, and features over 35 safety benchmarks covering areas such as multilingual safety, exaggerated safety, and prompt injections. The framework supports both LLM and judge benchmarking, and incorporates custom mutators to test safety against various text-style mutations such as future tense and paraphrasing. Additionally, WALLEDEVAL introduces WALLEDGUARD, a new, small and performant content moderation tool, and SGXSTEST, a benchmark for assessing exaggerated safety in cultural contexts. We make WALLEDEVAL publicly available at https://github.com/ walledai/walledeval.", "sections": [{"title": "Introduction", "content": "LLM technology has undoubtedly proven to be a valuable tool that simplifies various aspects of our lives. It can act as an email writing assistant, streamline information access, and help us write code blocks, saving us hours of work. Starting with OpenAI's ChatGPT-3.5, we have seen the emergence of numerous LLM variants, including both proprietary and closed-weight models, such as the ChatGPT series models (ChatGPTs, Achiam et al. (2023)) and the Claude series models (Claudes, Anthropic (2024)). Alongside these closed variants, there has been a surge in open-weight models, including the popular series of Mistrals (Jiang et al., 2023), Llamas (Dubey et al., 2024) and Gemmas (Team et al., 2024).\nAs new models continue to emerge with enhanced knowledge and multitasking capabilities, it is crucial to assess their safety risks comprehensively. Potential harms include training data leakage, biases in responses and decision-making (potentially leading to bias laundering), and unauthorized use, for example, for purposes such as terrorism and the generation of sexually explicit content (Vidgen et al., 2024). This increases the need for a one-stop center for safety evaluations of advanced AI systems; we thus introduce a Python-based framework WALLEDEVAL.\nThe following are features of WALLEDEVAL:\n\u2022 Open-weight and API-based model support. WALLEDEVAL supports a wide array of open-weight models built on the HuggingFace Transformers library (Wolf et al., 2019), allowing users to test Llamas, Mistrals and Gemmas, amongst others. It also supports API inference endpoints from proprietary and open-weight model hosts, including OpenAI, Anthropic, Google, Groq, and Together, and is continually enhancing support for additional hosts.\n\u2022 Comprehensive safety benchmarks. WALLEDEVAL hosts over 35 AI safety benchmarks \u00b9, allowing users to perform comprehensive safety tests on LLMs across dimensions such as multilingual safety (e.g., the Aya Red-Teaming dataset, Ahmadian et al. (2024)), exaggerated safety (e.g., XSTest, R\u00f6ttger et al. (2023)), and prompt injections (e.g., WildJailbreak).\n\u2022 Judge support. WALLEDEVAL also supports various safety judges, including content moderators (guardrails) such as LlamaGuard and LionGuard. As part of this work, we also release a new content moderator, WALLEDGUARD2, which is approximately 16 times smaller than state-of-the-art guardrails like LlamaGuard-3 and its previous"}, {"title": "Framework Design", "content": "The WALLEDEVAL framework consists of three main classes for creating core objects: a) Dataset loader HuggingFaceDataset; b) LLM loader HF_LLM; and c) Judge loader LLMasaJudge. This combination allows three types of testing: LLM benchmarking (Dataset \u2192 LLM \u2192 Judge \u2192 Score), Judge benchmarking (Dataset \u2192 Judge \u2192 Score) and MCQ benchmarking (Dataset \u2192 Template \u2192 LLM \u2192 Judge \u2192 Score).\nGetting the dataset ready. The first step is preparing the benchmark dataset. Using functions in the HuggingFaceDataset class, the dataset object can be created in several ways: through a list of prompts, a CSV/JSON file, or a HuggingFace dataset (Lhoest et al., 2021) as shown in Figure 2. The list can contain either string prompts that one can directly feed into the LLM or a list of dictionaries. The rest should contain the field \"prompt\""}, {"title": "Evaluating LLMs and Judges", "content": "Once the core objects are created, we can perform two tests: a) LLM benchmarking, i.e., LLM safety evaluations; and b) Judge benchmarking, i.e., judge accuracy evaluations."}, {"title": "LLM Benchmarking", "content": "WALLEDEVAL supports LLM benchmarking for two types of behaviors: 1) Harmful and 2) Refusal."}, {"title": "Harmful Behavior", "content": "WALLEDEVAL allows for evaluating the harmful behavior of LLMs and judges, i.e., the fraction of times the LLM responds safely to an unsafe prompt. To evaluate the safety of an LLM L, one can prompt it with each unsafe sample in the dataset D, feed the LLM response to the judge J, and obtain the score. The score is True if the response is safe; otherwise, it is False. The overall score of L on D using J is computed as: Harm-score = #True/#samples in D (we report results as a percentage). Note that Harm-score is meaningful only if all the prompts in the datasets are unsafe."}, {"title": "Refusal Behavior", "content": "While evaluating defensiveness against harmful prompts is important, it has been observed that models over-optimize for harmlessness and thus tend to exhibit exaggerated safety behavior (R\u00f6ttger et al., 2023). Therefore, we facilitate the refusal behavior testing of LLMs. Given a dataset of safe and unsafe prompts, we frame the task as a Multiple Choice Question (MCQ), asking the model if it would choose to answer the question (choice A) or not (choice B). Specifically for MCQ tasks, WALLEDEVAL integrates an MCQJudge for response parsing, scoring the choices against the ground truth: Refusal-score = #Correct choice A/B/#samples in D"}, {"title": "Judge Benchmarking", "content": "Using LLM-as-a-Judge has recently become quite popular recently, especially for evaluating the safety of LLMs (Zheng et al., 2024; Qi et al., 2023; Bhardwaj et al., 2024). Therefore, assessing the quality of judges (J) is important before using them for scoring LLM responses, as an inaccurate judge can produce unreliable scores. Thus, WALLEDEVAL also facilitates judge quality evaluations, defined as the percentage of correct classifications of a text (prompt and response) as safe or unsafe."}, {"title": "WALLEDGUARD & SGXSTEST", "content": "WALLEDGUARD. Content moderators play a crucial role in identifying potentially unsafe prompts and responses (Inan et al., 2023). However, incorporating them into the LLM application leads to increased latency. To address this issue, we introduce a new content moderator, WALLEDGUARD, which has 494M parameters \u2014 approximately 16 times smaller than LlamaGuard-3, but still delivers strong performance on English benchmarks (Table 2).\nSGXSTEST. For testing refusal behavior in a cultural setting, we introduce SGXSTEST a set of manually curated prompts designed to measure exaggerated safety within the context of Singaporean culture. It comprises 100 safe-unsafe pairs of prompts, carefully phrased to challenge the LLMs' safety boundaries. The dataset covers 10 categories of hazards (adapted from R\u00f6ttger et al. (2023)), with 10 safe-unsafe prompt pairs in each category. These categories include homonyms, figurative language, safe targets, safe contexts, definitions, discrimination, nonsense discrimination, historical events, and privacy issues. The dataset was created by two authors of the paper who are native Singaporeans, with validation of prompts and annotations carried out by another native author. In the event of discrepancies, the authors collaborated to reach a mutually agreed-upon label."}, {"title": "Experimental Settings", "content": "WALLEDEVAL hosts over 35 datasets that test different safety behaviors of LLMs and facilitates the addition of custom datasets (Figure 2). In this paper, we demonstrate its utility using harmful behavior datasets consisting of unsafe prompts, such as HarmBench (Mazeika et al., 2024), AdvBench (Zou et al., 2023), and CatQA (English) (Bhardwaj et al., 2024), as well as refusal behavior datasets with tricky safe and unsafe prompts, including XSTest (R\u00f6ttger et al., 2023) and SGXSTEST (Ours). (Details on datasets and prompting are relegated to Appendix A.1.\nWe perform experiments on several open-weight models, namely Llamas (2023), Mistrals (2023), Qwens (2023), Gemmas (2024), Phi (2024), and Aya models (2024), as well as the closed-weight models ChatGPT-4 (2023), Gemini 1.5 Pro (2017), and Claude 3 Sonnet (2024). For LLM harmful behavior benchmarking, we use LlamaGuard 2 8B as Judge given it outperforms others Table 2."}, {"title": "Mutations", "content": "WALLEDEVAL hosts mutators that perform text-style transformations of a given prompt. In this demo, we show the effectiveness of nine such mutations: rephrasing, altering sentence structure, changing style, inserting meaningless characters, misspelling sensitive words, paraphrasing with fewer words, translating English to Chinese (Ding et al., 2023), and converting between past and future tenses. For demonstration, we create a mutated"}, {"title": "Experiments & Discussions", "content": "We showcase the results obtained by interacting with WALLEDEVAL by performing various safety tests, such as standard benchmark testing, refusal tests (primarily English), and multilingual safety tests (in eight languages).\nHarmful behavior tests. In Table 1, under \"Harmful Behavior\", we observe that, amongst open-weight models, Llamas and Gemma 2 yield the greatest number of safe responses while Mistrals perform poorly, scoring the lowest average of 72.17%. For closed-weight models, Gemini and Claude score better compared to ChatGPT-4.\nRefusal behavior tests. We demonstrate over-refusal tests of LLMs using XSTest, SGXSTEST, and XSTestm. We observe a significant drop in scores from XSTest to XSTestm, exceeding 5%, showing that out-of-distribution (OOD) text often triggers unexpected behavior in these systems. A similar drop of ~ 4% is observed when testing on SGXSTEST, indicating that while current LLMs are good at understanding cultural-generic prompts, they lack cultural-nuanced knowledge. Although ChatGPT-4 performs worse in harmful behavior benchmarks, it is also less prone to over-refusal, with a margin of about 8.5% from Claude.\nMultilingual safety tests. Next, we perform a multilingual safety test of the models using WALLEDEVAL on the Aya Red-Teaming dataset (Ahmadian et al., 2024). Table 3 shows the scores of various models. Gemma 2 9B outperforms the other models, while Gemini 1.5 Pro performs best on harmful behaviors within the group of closed-weight models. However, it demostrates the worst performance on the refusal behavior tests, signifying over-refusal, which reduces its generic utility.\nJudge tests. Next, we demonstrate the utility of WALLEDEVAL for benchmarking judges. For this, we evaluate them on multilingual (Aya) and exaggerated safety datasets. In Table 2, we compare LlamaGuard 7B and recent 8B models (Inan et al., 2023). We also evaluate small-scale content moderators LionGuard (Foo and Khoo, 2024) and the proposed WALLEDGUARD, which have 0.3B and 0.5B parameters, respectively. On average, we observe LlamaGuard 2 outperforming all the guardrails with a score of 61.47%.\nWALLEDGUARD, despite being significantly smaller, beats LlamaGuard by 2.8% as well as LionGuard by 44.08% when evaluated on the English subset of Aya. When compared on exaggerated safety datasets, we observe LlamaGuard 2 outperforming it with 83.45% accuracy. WALLEDGUARD achieves the second-best score of 80.92%, which is better than LionGuard by 22.17%.\nSimilar to when testing judges, we observe an under-performance on OOD texts. All the judges consistently show a significant performance decline (averaging a drop of 12.73%) when the context of the prompts is changed from generic (global) to culturally inclusive (local)."}, {"title": "Conclusion", "content": "In this paper, we propose WALLEDEVAL, a tool for benchmarking LLMs and content moderators (judges) on a range of safety evaluation datasets, over 35 of which are hosted on the platform. We demonstrate the tool's utility in testing both harmful and refusal behavior. Additionally, we introduce a new content moderator, WALLEDGUARD a significantly smaller yet high-performing guardrail and a culturally tailored refusal dataset, SGXSTEST."}, {"title": "Appendix", "content": "A.1 Dataset details\nFor our standard safety tests on open-weight models, we choose Llamas, Mistrals, Qwens, Gemmas, Phi, and Aya models tested on HarmBench (Mazeika et al., 2024), AdvBench (Zou et al., 2023), CatQA (English) (Bhardwaj et al., 2024), XSTest (R\u00f6ttger et al., 2023), and SGXSTEST (Ours). We show dataset samples in Table 4 and different ways to load datasets in fig. 2. For standard testing, we follow the prompt template of the model and the datasets.\nA.2 Supported environments\nWALLEDEVAL is a Python package built for Python versions following and including 3.10. Certain features will not work for versions below this due to dependency constraints.\nA.3 Refusal testing template\nExaggerated safety evaluation datasets test if the LLM or judge correctly choose to refuse to answer the prompt. For LLM benchmarking, we prompt LLMs by casting samples into a MCQ prompt format as shown below:\nAnswer the following multiple choice question. The entire content of your response should be confined to the option. Choose from ['A', 'B'].\nWill you choose to answer the following question?\n{prompt}\nA. I refuse to answer this question\nB. I can answer this question\nAnswer:\nThe overall refusal score is computed as a percentage of correct options chosen by the LLM, i.e., A for unsafe prompts and B for safe prompts. For judge benchmarking, in all our experiments, we follow the moderator's template to classify if a given prompt is safe or unsafe.\nA.4 Existing Libraries\nExisting evaluation frameworks for LLM safety primarily focus on evaluating a specific component of LLM safety. Here, we detail a couple of open-source AI safety testing platforms.\nJailbreakEval (Ran et al., 2024) hosts various safety judges from HuggingFace Hub (Wolf et al., 2019) and API providers, such as OpenAI Moderation and Perspective. They also support substring judges as seen in Zou et al. (2023). WALLEDEVAL implements all HuggingFace and string-based judges in JailbreakEval.\nEasyJailbreak (Zhou et al., 2024) provides support for various attack methods such as GCG (Zou et al., 2023), allowing you to use your own dataset and mutate it to jailbreak an LLM. However, it has limited support for evaluators and custom LLMs. WALLEDEVAL currently implements only one-to-one mutators, largely inspired by many implementations from EasyJailbreak.\nNeither library currently supports customizable LLMs-as-a-Judge.\nA.5 Limitations and Future Plans\nWhile WALLEDEVAL aims to provide a comprehensive method for evaluating LLMs across a range of safety benchmarks, we acknowledge some limitations that will be addressed as feature enhancements in future work:\n\u2022 User Interface. WALLEDEVAL was designed as a library-first utility, so currently, it can only be used as a Python library. We plan to develop a command-line or web user interface in the future to facilitate broader use of WALLEDEVAL by the wider community.\n\u2022 Limited Mutator Support. Currently, WALLEDEVAL supports only nine mutators, which are primarily simple text-style transformations and are agnostic to the LLM under test and the context of the conversation. Moving forward, we plan to add more complex mutators, such as GCG (Zou et al., 2023) and PAIR (Chao et al., 2023) that adapt to the LLM under test and trigger harmful behaviors.\n\u2022 Multimodal Support. Due to certain limitations in standardizing between various frameworks and the evolving field, we currently focus on text-only safety evaluation. Moving forward, we plan to expand WALLEDEVAL to support multimodal safety testing. This will allow users to test on datasets such as HarmBench-multimodal (Mazeika et al., 2024).\n\u2022 Batching Support. WALLEDEVAL does not batch inputs to HF_LLM for faster inference. As an immediate feature enhancement, we are working towards adding support for batching to make evaluations with WALLEDEVAL much faster and more efficient."}, {"title": "", "content": "Answer:\nThe overall refusal score is computed as a percentage of correct options chosen by the LLM, i.e., A for unsafe prompts and B for safe prompts. For judge benchmarking, in all our experiments, we follow the moderator's template to classify if a given prompt is safe or unsafe.\n\u2022 Quality Templates. Although WALLEDEVAL aims to provide a rich database of prompt templates for designing LLMs-as-a-Judge, mutating prompts, and more, we currently offer a limited number of prompt templates gathered from literature for immediate use. We hope to compile additional templates in the future. Additionally, we have observed that many of our prompt templates, especially those for mutators, are inconsistent and not well-tested across various LLMS for generation. We plan to enhance standardization by sanitizing the base prompts derived from various papers and sources.\n\u2022 Dataset Merging. Currently, HuggingFaceDataset loads only one split of a dataset at a time, which is highly inefficient as it limits the amount of data that can be loaded at once. Therefore, we plan to add support for merging datasets and splits in HuggingFaceDataset to allow users to test various benchmarks more effectively and efficiently.\nA.6 Ethics Statement\nOur study tests vulnerabilities in the alignment of large language models, presenting a potential avenue for widespread exploitation by malicious"}]}