{"title": "TABGEN-ICL: Residual-Aware In-Context Example Selection for Tabular\nData Generation", "authors": ["Liancheng Fang", "Aiwei Liu", "Hengrui Zhang", "Henry Peng Zou", "Weizhi Zhang", "Philip S. Yu"], "abstract": "Large Language models (LLMs) have achieved\nencouraging results in tabular data genera-\ntion. However, existing approaches require fine-\ntuning, which is computationally expensive.\nThis paper explores an alternative: prompt-\ning a fixed LLM with in-context examples.\nWe observe that using randomly selected in-\ncontext examples hampers the LLM's perfor-\nmance, resulting in sub-optimal generation\nquality. To address this, we propose a novel\nin-context learning framework: TABGEN-ICL,\nto enhance the in-context learning ability of\nLLMs for tabular data generation. TABGEN-\nICL operates iteratively, retrieving a subset\nof real samples that represent the residual be-\ntween currently generated samples and true\ndata distributions. This approach serves two\npurposes: locally, it provides more effective\nin-context learning examples for the LLM in\neach iteration; globally, it progressively nar-\nrows the gap between generated and real data.\nExtensive experiments on five real-world tab-\nular datasets demonstrate that TABGEN-ICL\nsignificantly outperforms the random selection\nstrategy. Specifically, it reduces the error rate\nby a margin of 3.5% - 42.2% on fidelity met-\nrics. We demonstrate for the first time that\nprompting a fixed LLM can yield high-quality\nsynthetic tabular data. The code is provided in\nthe link.", "sections": [{"title": "Introduction", "content": "Tabular data, despite being one of the most preva-\nlent data modalities in real-world applications, often encounters several is-\nsues in practical use. These include imbalanced\ndata categories, privacy concerns\n(as many tabular datasets con-\ntain sensitive personal information that cannot be\ndirectly shared), insufficient data quality, and high data collection costs. Tabular generation is an important\nmeans to address these problems. Classic tabu-\nlar generation methods such as GANs, VAEs and diffusion mod-\nels have two main\nlimitations. First, they require large amounts of tab-\nular data for training, which leads to a noticeable\ndecline in performance in low-resource scenarios.\nThis is particularly problematic considering that\nmost real-world situations requiring tabular genera-\ntion lack abundant data. Second, they need special\npreprocessing to handle heterogeneous data types,\nmaking them less flexible.\nThe rapid development of large language models\n(LLMs) brings new possibilities for solving table\ndata generation problems with their powerful se-\nmantic understanding, reasoning, and generation\ncapabilities. LLMs can understand and process var-\nious data types and structures without complicated\ndata preprocessing, offering more flexible and prin-\ncipled solutions. Moreover, LLMs' few-shot learn-\ning ability may alleviate data scarcity issues, en-\nabling excellent performance in low-resource sce-\nnarios. Previous works resort to fine-tuning\ngeneral-purpose LLMs on target tables. While ef-\nfective, fine-tuning requires substantial computa-\ntional resources, making it inapplicable in resource-\nscarce scenarios.\nIn-context learning effectively solves such prob-\nlems. By adding examples to the context, distri-\nbution characteristics can be provided to LLMs,\nguiding them to generate data that conforms to\nthe target distribution without specific fine-tuning\n. However, simple in-context\nlearning strategies still face challenges. Figure 1(a)\nshows that even without in-context examples (see\nthe full prompt at Appendix A.2), LLMs can gen-\nerate reasonable distributions, reflecting the influ-"}, {"title": "Related works", "content": "Deep generative models for synthetic tabular\ndata generation Generative models for tabu-\nlar data have become increasingly important and\nhave widespread applications. For example, CTGAN and TAVE deal with mixed-type tabular data gen-\neration using the basic GAN and VAE frame-\nwork. GOGGLE incorporates"}, {"title": "Preliminaries", "content": "Notation. Tabular dataset refers to data organized\nin a tabular format with N row and D columns,\nwhere each row denotes a data record or sample,\nand each column denotes an attribute or feature.\nEach attribute can be either discrete (e.g. categor-\nical) or continuous (e.g. real number R). We use\nP(x) to denote the probability distribution of x.\nData Setup. We have access to a training dataset\nof N samples: Dtrain = {xi}=1, each sample xi\nis i.i.d. drawn from an unknown distribution P(x).\nN\n=1\nObjective. The goal is to generate a new dataset\nDsynt = {xt}t=1 such that is i.i.d. sampled from\nP(x). Direct copy of training data is not allowed.\nSerialization. As LLMs primarily process\ntext input, it is necessary to convert tabular data\ninto a suitable textual format. There are many\nserialization formats for tabular data, such as\nJSON, Markdown , Sentences, etc.\nNotably, the JSON format is widely supported\nby LLMs, with models like GPT-40 capable of\ngenerating structured outputs in JSON format\nthrough constrained decoding ."}, {"title": "TABGEN-ICL", "content": "This section presents TABGEN-ICL framework for\ntabular data generation. TABGEN-ICL retrieves\na subset of samples from the training dataset that\nsatisfy two properties: 1) Local: at each prompting\niteration, the LLMs can effectively extract patterns\nfrom the in-context examples; 2) Global: after\nenough iterations, the overall generated samples\nmimic the distribution of the real samples. In the\nfollowing, we will introduce each component of\nTABGEN-ICL in detail."}, {"title": "LLM Generation with In-context\nExamples", "content": "Our key observation is LLMs have strong prior\ndistribution, and LLMs tend to generate samples\nfollowing their prior distribution, neglecting the in-\ncontext examples, see Figure 1. Formally, given in-\ncontext examples, we assume the LLMs generate\nsamples following a mixture distribution:\nDefinition 1 (LLM Generation Distribution).\nGiven the empirical distribution of in-context ex-\nample: Pic. We define the LLMs generation distri-\nbution to be the following mixture of distributions:\n$\\mathbb{P}_{gen} := \\lambda \\mathbb{P}_{llm} + (1 - \\lambda) \\mathbb{P}_{ic}$       (1)\nwhere Plum is the prior distribution of LLMs, \u03bb \u2208\n[0, 1]. To sample from Pgen, we first sample an\nindex z from a categorical distribution over {0,1}\nwith parameter \u03bb, then sample from the correspond-\ning distribution:\n$\\mathbb{P}_{gen}(x) = \\begin{cases} \\mathbb{P}_{llm}(x) & \\text{if } z = 1 \\\\ \\mathbb{P}_{ic}(x) & \\text{if } z = 0 \\end{cases}$\nDefinition 1 quantifies how the in-context exam-\nples steer the LLMs' generation from its own prior"}, {"title": "In-context Examples Selection", "content": "Recall our goal is to let LLMs generate samples that\nfollow the same distribution as the training table,\ni.e. Pgen \u2248 Ptrain. It is tempting to choose the in-\ncontext examples by sampling from the empirical\ndistribution of the training table, i.e. Pic = Ptrain\n. However, as the LLMs' gen-\neration is affected by the prior distribution Plum,\nthe actual output distribution of LLMs would be\nPgen = \u03bbPlum + (1 \u2013 \u03bb)Ptrain, which is not our\ntarget distribution Ptrain. Instead, a more plausi-\nble way is to select in-context examples s.t., when\ncombined with \u03bb proportion of data generated from\nPlum, the resulting distribution is close to Ptrain. In\nother words, the in-context examples can be under-\nstood as the residual of Ptrain w.r.t. Plum. Formally,\nwe introduce the definition of residual as follows:\nDefinition 2 (Residual). Let X be a set of N i.i.d.\nsamples from a data distribution P(x), and let Y\nbe an arbitrary set of samples with the same dimen-\nsion as X. We define the residual (abbrev. RES) of\nX w.r.t. Y as a subset of n samples of X such that,\nwhen concatenated with Y, the empirical distribu-\ntion of the concatenated samples is most similar to"}, {"title": "Compute Residual", "content": "We propose to use a simple heuristic to shrink the\nsearch space. Specifically, we first randomly se-\nlect a column, then we group the real samples X\nbased on the value of the selected column. Each\ngroup of samples is then concatenated with the gen-\nerated samples Y. Finally, we select the group"}, {"title": "Table Generation by TABGEN-ICL", "content": "TABGEN-ICL can be easily integrated with LLMs\nto generate high-quality synthetic tabular data. See\nFig. 2 for an overview of the procedure. Here are\nthe concrete steps involved in this procedure:\n1. In-context Prompting: For the first iteration,\nwe randomly select n samples from the real\ndataset X as the initial set of in-context ex-\namples. Otherwise, we plug the residual sam-\nples computed in the previous iteration into the\nprompt template to prompt LLMs. We append\nthe generated samples into Y.\n2. Residual Computation: We then compute the\nresidual of X w.r.t. Y: RES(X, Y, n). Specifi-\ncally, if the current iteration is an even number,\nwe instantiate d as JSD, otherwise, we instanti-\nate d as KSD.\n3. Iterative Refinement: Repeat the above steps\nuntil enough synthetic samples are generated."}, {"title": "Experiments", "content": "We validate the performance of TABGEN-ICL\nthrough extensive experiments. In particular, we\ninvestigate the following questions:\n\u2022 Can TABGEN-ICL improve generation quality\ncompared to previous LLM-based methods?\n\u2022 How does TABGEN-ICL perform, compared to\ntraining-based deep generative models, under a\ndata-scarcity setting?\n\u2022 Does TABGEN-ICL generate new synthetic data\ninstead of copying the training dataset?\nDatasets. We select five real-world tabular\ndatasets containing both numerical and categori-\ncal attributes: Adult, Default, Shoppers, Magic\nand California. The statistics of the datasets are\nsummarized in Table 5 in Appendix A.5.\nBaselines. To comprehensively assess TABGEN-\nICL's performance, we conduct comparisons\nagainst a wide range of traditional deep genera-\ntive models and LLM-based methods, which we\ncategorize into the following two groups:\n\u2022 Deep generative models: 1) VAE-based method\nTVAE, 2) GAN-based method\nCTGAN, 3) Diffusion-based\nmethod TabSyn, TabDDPM,\nCoDi, STaSy , 4) Autoregressive\nmethod TabMT ,RealTabformer.\n\u2022 LLM-based methods: 1) with fine-tuning:\nGReaT, 2) without fine-\ntuning: CLLM . CLLM was\noriginally employed with GPT-3.5 and GPT-4, to\nensure a fair comparison to CLLM, we employ\nCLLM with stronger models: GPT-40-mini and\nGPT-40, and we keep all the other experimental\nsettings the same as ours.\nTo the best of our knowledge, CLLM is the only previous work that is training-free\nand solely based on in-context learning (excluding\nthe curation step). TABGEN-ICL falls in this set-\nting.\nImplementation details. Our main experiments\nemploy GPT-40-mini and GPT-40 as the LLMs.\nFor all LLMs, we set the temperature to 1.0. We\ngenerate 3000 samples (N= 3000) for each\ndataset. Each experiment is conducted 5 times\nand the average results are reported.\nEvaluation metrics. We evaluate the synthetic\ntabular data from three distinct dimensions: Fi-\ndelity - if the synthetic data faithfully recovers the\nground-truth data distribution. We evaluate fidelity\nby 5 metrics: 1) Marginal distribution through\nKolmogorov-Sirnov Test, 2) Pair-wise column cor-\nrelation (Corr.) by computing Pearson Correlation,\n3) Classifier Two Sample Test (C2ST) 4) Preci-\nsion and Recall, 5) Jensen-Shannon Divergence\n(JSD). Utility - the utility of the synthetic data\nwhen used to train downstream models, we use\nthe Train-on-Synthetic-then-Test (TSTR) protocol\nto evaluate the AUC score of XGBoost model on\npredicting the target column of each dataset. Privacy - if the synthetic data is not copied from\nthe real records, we employ the Distance to Closest"}, {"title": "TABGEN-ICL outperforms LLM-based\nbaseline methods", "content": "As shown in Table 1, TABGEN-ICL consistently\noutperforms current LLM-based approaches on\nfidelity metrics, including both the training-free\nmethod CLLM and the fine-tuning-based method\nGReaT. Specifically, when using GPT-4o-mini,\nTABGEN-ICL improves fidelity scores by a mar-\ngin of 3.5% 42.2%, and when using GPT-4o, the\nimprovements range from 1.6% to 34.1% across\nvarious metrics. Notably, the highest gains are\nobserved in Recall: 42.2% improvement with GPT-\n40-mini and 34.1% with GPT-40. Recall measures\nwhether the synthetic data adequately covers the\ndiverse spectrum of the real data; thus, improved\nRecall signifies enhanced diversity in the synthe-\nsized samples. This significant improvement is\nattributable to TABGEN-ICL 's strategy of com-\nputing residual samples at each prompt iteration.\nThese residual samples target underrepresented re-\ngions of the data distribution, thereby enriching the\noverall diversity of the synthetic data. This observa-\ntion further validates the effectiveness of TABGEN-\nICL 's residual-based iterative refinement mecha-\nnism."}, {"title": "TABGEN-ICL outperforms deep\ngenerative models under data-scarcity", "content": "One important application of tabular data synthe-\nsis is addressing data scarcity. In many cases, we\nhave access to only a limited number of real data\npoints, yet we require a much larger dataset to ade-\nquately train our downstream models. To generate\nsufficient training data, generative models can be\nemployed. In our experiments, we evaluate the\nperformance of TABGEN-ICL in comparison with\nother deep generative models under data-scarce\nconditions. To simulate such scenarios, we cre-\nated training sets by randomly sampling 100, 500,\n1000, 2000, and 3000 rows from the Default dataset.\nThe generative models were then trained on these\nsubsets, and the synthesized data's quality was eval-\nuated using the original full training set of 30,000\nrows. As shown in Figure 3, deep generative mod-\nels like TVAE, CTGAN, and TabDDPM exhibit a\nsignificant drop in performance when trained on\nlimited data. In contrast, TABGEN-ICL and CLLM"}, {"title": "TABGEN-ICL does not copy training data", "content": "In Figure 4, we illustrate the distributions of the L2\ndistances between the synthetic data and both the\ntraining and holdout datasets for CLLM, GReaT,\nREaLTabFormer, and TABGEN-ICL. No-\ntably, TABGEN-ICL exhibits nearly identical dis-\ntributions for the training and holdout sets, suggest-\ning it is less prone to copying the training data. In\ncontrast, CLLM and GReaT show more disparate\ndistributions, indicating a higher likelihood of rely-\ning on the training data."}, {"title": "Ablation Study", "content": "Effect of d. We examine the effect of the distribu-\ntion distance metric d used for quantifying residual\nin Equation 2. We test TABGEN-ICL (w. GPT-40-\nmini) with only KSD or JSD metric and compare\nit with our alternating strategy (KSD+JSD) on the\nCalifornia dataset. As shown in Table 3, the alter-\nnating strategy achieves the best performance."}, {"title": "Effect of Large Language Models.", "content": "In this sec-\ntion, we investigate the impact of large language\nmodel (LLM) capabilities on TABGEN-ICL's per-\nformance. We evaluate TABGEN-ICL using LLMs\nof varying parameter sizes, including Gemini-1.5-\nFlash, Gemini-1.5-Pro , Claude-\n3-Haiku, Claude-3-Sonnet, LLaMA-3.1 8B,\nLLaMA-3.1 405B , and Qwen2\n. We assess the marginal metric\non the California dataset, with results presented in\nTable 4. Our findings reveal a correlation between\nLLM capacity and synthetic data generation qual-\nity. As the LLMs' capacity increases, the quality of\ngenerated synthetic data improves. We hypothesize\nthat this improvement stems from larger models'\nenhanced ability to capture and reproduce complex\npatterns within the data, resulting in more realistic\nsynthetic outputs. This relationship underscores\nthe importance of model capacity in generating\nhigh-quality synthetic data."}, {"title": "Conclusion", "content": "This work proposes TABGEN-ICL, an ICL frame-\nwork for tabular data generation with LLMs. It\nsteer the LLM's prior distribution towards real data\ndistribution by iteratively retrieving the most under-\nrepresented regions. Extensive experiments vali-\ndate the effectiveness of our approach, demonstrat-\ning its potential to enhance LLM-based synthetic\ndata generation across various domains."}, {"title": "Limitations", "content": "TABGEN-ICL relies on a simple heuristic search al-\ngorithm to compute residual samples, under which\nthe optimality is not guaranteed. In future, we will\nexplore more principled approaches to compute\nresiduals."}, {"title": "Appendix", "content": "Prompts used for generating tabular data\nThis prompt template is used in Section 4 to generate realistic data that follows the same distribution as\nthe given real data.\nYou are a synthetic data generator tasked with creating new tabular data samples\nthat closely mirror the distribution and characteristics of the original dataset\nInstruction\nAnalyze the provided real samples carefully.\nGenerate synthetic data that maintains the statistical properties of the real\ndata.\nEnsure all attributes cover their full expected ranges, including less common or\nextreme values.\nMaintain the relationships and correlations between different attributes.\nPreserve the overall distribution of the real data while introducing realistic\nvariations.\nKey points to consider\nReplicate the data types of each column (e.g., numerical, categorical).\nMatch the range and distribution of numerical attributes.\nMaintain the frequency distribution of categorical attributes.\nReflect any patterns or trends present in the original data.\nIntroduce realistic variability to avoid exact duplication.\nReal samples\n{data}\nOutput format:\nPlease present the generated data in a JSON format, structured as a list of objects,\nwhere each object represents a single data point with all attributes."}, {"title": "Dummy Prompt", "content": "The following prompt only contains the column names, but not any actual data in it. It is used to produce\nthe results in Fig.1 (a).\nYou are a synthetic data generator tasked with creating new tabular data samples\nthat closely mirror the distribution and characteristics of the original dataset\nGenerate 50 samples of synthetic data.\nEach sample should include the following attributes:\n{attributes_list}\nMake sure that the numbers make sense for each attribute.\nOutput Format:\nPresent the generated data in a JSON format, structured as a list of objects, where\neach object represents a single data point with all attributes."}, {"title": "JSON Schema", "content": "The following code define the JSON data class for the structured output function of GPT-40 and GPT-40-\nmini.\ndef create_json_model (df: pd. DataFrame, dataname=None) -> BaseModel:\nfields = {}\nfor column in df.columns:\nif df [column].dtype == 'object':\nfields [column] = (str, ...)\nelif df [column].dtype == 'int64':\nfields [column] = (int, ...)\nelif df [column].dtype == 'float64':\nfields [column] = (float, ...)\nelif df [column].dtype == 'bool':\nfields [column] = (bool, ...)\nelse:\nraise TypeError (f\"Unexpected dtype for column {column}: {df[column].\ndtype}\")\nJSONModel = create_model (dataname, fields)\nclass JSONListModel (BaseModel):\nJSON: List [JSONModel]\nreturn JSONListModel"}, {"title": "Heuristic for computing residual", "content": "In this section, we provide the pseudo-code of our heuristic strategy for computing the residual.\nAlgorithm 1 Compute residual\nRequire: current dataset X, target dataset Y, distribution distance d.\nRandomly select a column index j\nif column j is categorical then\nLet C; be the number of categories in column j\nGroup samples in Y into Cj number of subsets based on its category on column j, denote the set\nof subsets by (Y)1\nelse\nQuantize column i into 50 bins\nC; \u2190 50\nGroup samples in Y into Cj number of subsets based on its bin index on column j, denote the set\nof subsets by (Y)1\nend if\nfor i = 1 to Cj do\nCompute distance between YUX and Y : d\u2081 = d(Y} \u222a X, Y)\nend for\nreturn subset Y that attains the minimal distance."}, {"title": "Datasets", "content": "We use five real-world datasets of varying scales, and all of them are available at Kaggle2 or the UCI\nMachine Learning repository\u00b3. We consider five datasets containing both numerical and catergorical\nattributes: California, Magic5, Adult, Default7, Shoppers. The statistics of these datasets are presented\nin Table 5."}, {"title": "Evaluation Metrics", "content": "Fidelity To evaluate if the generated data can faithfully recover the ground-truth data distribution, we\nemploy the following metrics: 1) Marginal distribution: The Marginal metric evaluates if each column's\nmarginal distribution is faithfully recovered by the synthetic data. We use Kolmogorov-Sirnov Test for\ncontinuous data and Total Variation Distance for discrete data. 2) Pair-wise column correlation: This\nmetric evaluates if the correlation between every two columns in the real data is captured by the synthetic\ndata. We compute the Pearson Correlation between all pairs of columns then take average. In addition, we\npresent joint density plots for the Longitude and Latitude features in the California Housing data set in\nFigure 5. 3) Classifier Two Sample Test (C2ST): This metric evaluates how difficult it is to distinguish\nreal data from synthetic data. Specifically, we create an augmented table that has all the rows of real data\nand all the rows of synthetic data. Add an extra column to keep track of whether each original row is\nreal or synthetic. Then we train a Logistic Regression classifier to distinguish real and synthetic rows. 4)\nPrecision and Recall: Precision measures the quality of generated samples. High precision means the\ngenerated samples are realistic and similar to the true data distribution. Recall measures how much of the\ntrue data distribution is covered by the generated distribution. High recall means the model captures most\nmodes/variations present in the true data. 5) Jensen-Shannon Divergence (JSD): This metric evaluates\nthe Jensen-Shannon divergence between the distributions of real data and synthetic data.\nUtility We evaluate the utility of the generated data by assessing their performance in Machine Learning\nEfficiency (MLE). Following the previous works, we first split a real table into a real\ntraining and a real testing set. The generative models are trained on the real training set, from which a syn-\nthetic set of equivalent size is sampled. This synthetic data is then used to train a classification/regression\nmodel (XGBoost Classifier and XGBoost Regressor, which will be evaluated\nusing the real testing set. The performance of MLE is measured by the AUC score for classification tasks\nand RMSE for regression tasks.\nPrivacy A high-quality synthetic dataset should accurately reflect the underlying distribution of the\noriginal data, rather than merely replicating it. To assess this, we employ the Distance to Closest Record\n(DCR) metric. We begin by splitting the real data into two equal parts: a training set and a holdout set.\nUsing the training set, we generate a synthetic dataset. We then measure the distances between each\nsynthetic data point and its nearest neighbor in both the training and holdout sets. In theory, if both sets\nare drawn from the same distribution, and if the synthetic data effectively captures this distribution, we\nshould observe an equal proportion (around 50%) of synthetic samples closer to each set. However, if\nthe synthetic data simply copies the training set, a significantly higher percentage would be closer to the\ntraining set, well exceeding the expected 50%."}, {"title": "Scalability of TABGEN-ICL", "content": "To evaluate the scalability of TABGEN-ICL, we compare TABGEN-ICL with CLLM on a large-scale\ndataset: Covertype dataset. This dataset consists of 581,012 instances and 54 features. TABGEN-ICL and\nCLLM use iterative in-context learning to generate samples, thus the running time of these two methods\nare agnostic to the size of the training dataset, making them scalable to large datasets. In the following\ntable, we compare TABGEN-ICL with CLLM, employed with both GPT-40 mini and GPT-40.\nThe results demonstrate that TABGEN-ICL outperforms CLLM on most of the metrics. Notably, the\nRecall metric again shows the greatest improvement: 34.85% on GPT-40 and 26.25% on GPT-40 mini.\nThis observation is consistent with our original findings in Sec. 5.2. We believe these results strongly\nsupport TABGEN-ICL's scalability to larger, more complex datasets."}]}