{"title": "Hierarchical Universal Value Function Approximators", "authors": ["Rushiv Arora"], "abstract": "There have been key advancements to building universal approximators for multi-goal collections of reinforcement learning value functions-key elements in estimating long-term returns of states in a parameterized manner. We extend this to hierarchical reinforcement learning, using the options framework, by introducing hierarchical universal value function approximators (H-UVFAs). This allows us to leverage the added benefits of scaling, planning, and generalization expected in temporal abstraction settings. We develop supervised and reinforcement learning methods for learning embeddings of the states, goals, options, and actions in the two hierarchical value functions: $Q(s, g, 0; \\theta)$ and $Q(s, g, o, a; \\theta)$. Finally we demonstrate generalization of the HUVFAs and show they outperform corresponding UVFAS.", "sections": [{"title": "1. Introduction", "content": "Value functions (V(s) and Q(s, a)) are key to reinforcement learning algorithms (Sutton & Barto, 1998). They represent how important it is for the agent to be in a certain state and take certain actions in a given state. Using an optimal value function for a particular goal allows us to recreate the optimal policy the agent must follow to maximize the discounted return for that goal. Hierarchical reinforcement learning extends this to temporal abstraction (Sutton et al., 1999; Precup, 2000) by using a hierarchy of two or more levels of value functions to create a collection of useful skills or behaviors (Silva et al., 2012) that solve independent tasks or subgoals. A meta-value function selects the appropriate skill in the given state while a lower-level value function drives the policy that executes the actions the agent takes when exploiting the chosen skill. This is beneficial for scaling and planning.\nUniversal value functions (Schaul et al., 2015) (V(s, g) and Q(s, a, g)) and general value functions (Sutton et al., 2011) (Vg(s) and Qg(s, a)) are key when extending this to multitask domains. They incorporate information of the goal into the value function itself, making more expressive agents that can act differently in the same state as appropriate for the current goal. A collection of universal and general value functions over a diverse set of goals is key in building a single unified expression for generating policies and building agents with multiple capabilities (multi-task capabilities).\nComplex environments require parameterized representations of the value functions (V(s; \\theta) or Q(s, a; \\theta)), often with large neural networks. These parameterized representations exploit the structure of the state space to learn a mapping to the value functions and generalize behavior to unseen states. Universal value function approximators (Schaul et al., 2015) demonstrated that one can learn parameterized representations of universal value functions V(s, g; \\theta) that exploit the underlying structure of the goal and state space to generalize to unseen goals and states. They exploit the universal function approximation capabilities of neural networks to learn a single value function that represents states and goals.\nWe extend UVFAs to hierarchical reinforcement learning, or temporal abstraction, using the options framework. Since there are two or more levels of value functions in a hierarchy, we must learn representations (both parameterized and nonparameterized) of all the levels of the hierarchy to results in a final state of value functions equal in number to the hierarchy levels. The challenge lies in extending the concept of UVFAs from two or three dimensions (for the state and action value functions respectively) to n-dimensions. Our method is novel because it shows that there is underlying structure in the states, goals, options and actions that results in a universal representation of the hierarchy. This universal representation results in universal temporally abstract behaviors or skills that has zero-shot generalization to unseen goals. We show that this representation outperforms corresponding UVFAs."}, {"title": "2. Background", "content": "A Markov Decision Process (MDP) (S, A, T, R, \u03b3) consists of a set of states S, a set of actions A, a transition function T(s, a, s') := P(St+1 = s'|St = s, At = a), a reward function R : S\u00d7A \u2192 R and a reward discount factor \u03b3\u2208 [0, 1). Since we consider a multi-task setting, each goal has its own reward function Rg and discount factors Yg. A Markovian policy \u03c0 : S \u00d7 A \u2192 [0,1] := P(At = a|St = s) is a probability distribution of all the actions conditioned over the states. The agent's goal is to construct a policy that maximises the state value function V\u2081(s) and action-value function Q(s, a) defined as\n$V_{\\pi}(s) = E_{\\pi}[\\sum_{t=0}^{\\infty} \\gamma^t R_{t+1} \\vert s_0 = s]$\n$Q_{\\pi}(s, a) = E_{\\pi}[\\sum_{t=0}^{\\infty} \\gamma^t R_{t+1} \\vert s_0 = s, a_0 = a]$\nSolving for the optimal action-value function results allows one to deduce a greedy optimal policy. For discrete MDPS there is at least one optimal policy that is greedy with respect to its action-value function.\nUniversal Value Functions (Schaul et al., 2015) extend value functions to include the goals and produce the universal state value function V\u03c0(s, g) and universal action value function Q\u03c0(s, a, g). These universal value functions are defined in the context of multi-task learning over multiple goals G and are constructed using multiple individual value functions Vg,\u03c0(8) and Q9,\u03c0(s, a) defined for each goal g\u2208 G. Universal value functions can be learned using function approximators to become universal value function approximators (UVFAs). Given UVFAs that are trained on a diverse set of goals and parameterized with sufficient capacity, we learn an underlying structure of the states and goals that can be used to construct value functions that extend to unseen states or goals.\nThe Options Framework (Sutton et al., 1999; Precup, 2000) formalizes the concept of temporal abstractions in reinforcement learning, or hierarchical reinforcement learning, by using semi-MDPs. A Markovian option w \u2208 \u03a9 is a tuple (Iw, \u03c0\u03c9, \u03b2\u03c9) where Iw \u2208 S is its initiation set, \u03c0\u03c9 is its intra-option policy, and \u03b2\u03c9 : S \u2192 [0, 1] is its termination function. We assume that \u2200s \u2208 S, \u2200w \u2208 \u03a9: s \u2208 Iw-all options are available everywhere. An option w is picked from the initiation set Iw using the policy-over-options \u03c0\u03a9(s, o) and follows its intra-option policy \u03c0\u03c9(s, a) until it terminates by \u03b2w(s) which determines the probability of a given option terminating in a given state.\nThis is a call-and-return model where the option initiation is determined by the policy-over-options (meta-policy) $\\pi_{\\Omega}(\\omega|s)$ that gives us the probability of selecting option w in state s and is driven by the option-value function Qn(s,w) : S\u00d7\u03a9 \u2192 R. Once the option is selected, it performs actions using the intra-option policy \u03c0\u03c9(a|s,w) which is driven by its intra-option value function Qu (s, o, a) : S\u00d7\u03a9\u00d7A \u2192 R. This continues until the option terminates, upon which the process restarts in the then-current state. The two sets of value functions are related as\n$Q_{\\Omega}(s,\\omega) = \\sum_{a} \\pi_{\\omega,s}(a | s) Q_U(s,\\omega,a), \\qquad (1)$\n$Q_U(s,\\omega, a) = r(s, a) + \\gamma\\sum_{s'}P (s' \\vert s, a) U (\\omega, s').\\qquad (2)$\nwhere\n$U(\\omega, s') = (1 - \\beta_{\\omega,v}(s'))Q_{\\Omega}(s',\\omega) + \\beta_{\\omega,v}(s')V_{\\Omega}(s')$ is the value of executing w upon entering a state s'."}, {"title": "3. Hierarchical Universal Value Function Approximators", "content": "Hierarchical universal value function approximators (H-UVFAs) extend UVFAs to hierarchical reinforcement learning to give zero-shot generalization to unseen goals by exploiting underlying structure in the states, goals, options and actions that results in a universal representation of the hierarchy. We aim to construct H-UVFAs over every level in the hierarchy (in our case, we use a two level hierarchy). Particularly, we aim to construct $Q_{\\Omega}(s, o, g; \\theta)$ and $Q_U (s, o, a, g; \\eta)$ where the H-UVFA meta-policy is parameterised by \u03b8 \u2208 Rm and the H-UVFA intra-option policy is parameterized by \u03b7 \u2208 Rn. Note that m and n don't necessarily have to be equal. These H-UVFAs draw upon hierarchical general value functions ($Q_{\\Omega}(s, o, g; \\theta) \\approx Q_{\\hbar,g}(s, o)$ and $Q_U (s, o, a, g; \\eta) \\approx Q_{\\v,g}(s, o, a)$) which, to the best of our knowledge, haven't been defined before and will be created in the process of building H-UVFAs.\nLike UVFAs, we propose learning by using a multi-stream architecture as seen in Figure 1. Each component of the value function gets its own stream of embeddings.\n\u2022 The meta-H-UVFA $Q_{\\Omega}(s, o, g; \\theta)$ can be split into three sets of embeddings: \u03a6: S \u2192 RM, \u03a8 : G \u2192 RM and X : \u03a9 \u2192 Rm\n\u2022 The intra-option H-UVFA $Q_U (s, o, a, g; \\eta)$ can be split into three sets of embeddings: \u00a2 : S \u2192 R\u2033, \u03c8 : G \u2192 Rn, x: \u03a9 \u2192 R\", and d : A \u2192 Rn\""}, {"title": "4. Supervised Learning of H-UVFAs", "content": "The supervised learning approach to building H-UVFAs is best applicable when it is possible to iterate over the states, goals, options, and actions to construct the matrix that we can decompose into embeddings.\nThe two-step multi-stream process described in the previous section is achieved using the following steps to learn the parameters \u03b8 and \u03b7:\n\u2022 Iterate over all the states, goals, actions, and options and organize their Q-values into two multi-dimensional tensors. The first tensor represents Qn(s, o, g) and is a three dimensional tensor with one dimension each for the states, goals, and options. The second tensor is the four dimensional Qu(s, o, a, g) with one dimension each for the states, goals, options, and actions. The tensors are of dimensions |S| \u00d7 \u03a9 \u00d7 G and S x \u03a9 \u00d7 |A| x |G| respectively.\n\u2022 Decompose the two multi-dimensional tensors into a set of embedding vectors corresponding to each dimension using either PARAFAC (Kolda & Bader, 2009) or Tucker decompositions (Tucker, 1966). As discussed before, the embedding vectors correspond as Qn: (s,g,o) \u2192 (\u03a6, \u03a8, X) and Qu: (s,g,o,a) \u2192 (\u03c6, \u03c8, \u03c7, \u03b4).\n\u2022 Train a function approximator to approximate the embeddings from the embedding vectors to learn the parameters \u03b8 = {\u03a6, \u03a8, \u03a7} and \u03b7 = {\u03c6, \u03c8, \u03c7, \u03b4}. This now results in $Q_{\\Omega}(s, o, g; \\theta)$ and $Q_U (s, o, a, g; \\eta)$.\nThis method minimizes the mean-squared-errors (MSE) $MSE_{Q_{\\Omega}} = [Q_{\\Omega,g}(s,o) \u2013 Q_{\\Omega}(s, o, g; \\theta)]^2$ and $MSE_{Q_U} =$"}, {"title": "5. Reinforcement Learning of H-UVFAs", "content": "We now present results in the more practical setting of reinforcement learning. This method is applicable to continuous domains where it is not possible to access the value functions for every possible combination of states, goals, options, and actions. We find that it is possible to construct H-UVFAS simply using the observed values of the value functions seen during episodes. We will continue to build on the multistream architecture we used in supervised learning settings. Like most hierarchical reinforcement learning approaches, we assume a finite set of options."}, {"title": "5.1. Method", "content": "We build upon the method presented by Schaul et al. (2015) to construct two methods of developing H-UVFAs: using a Horde (Sutton et al., 2011) of general value functions, and directly using bootstrapping.\nThe first method using a Horde is depicted in Algorithm 1. We learn multiple policies on a diverse set of goals-a Horde and construct hierarchical general value functions Qn,g and QU,g. Using these hierarchical general value functions, we build two data matrices that we factorize into the respective state, goal, option, and action embeddings that the HUVFAs approximate. To the best of our knowledge, no other work introduces hierarchical general value functions (H-GVFs) that we use in constructing our H-UVFAs."}, {"title": "5.2. Reinforcement Learning Experiments", "content": "The first set of reinforcement learning experiments construct H-UVFAs using a target of Hordes as described in Algorithm 1. We use a target of 15 diverse Hordes, consisting of 15 goals spread out over three of the four rooms. The"}, {"title": "6. Comparison with UVFAS", "content": "Learning UVFAs, instead of H-UVFAs, would result in two two-dimensional matrices that are factored as\n$Q_{\\Omega}(s, o, g; \\theta) = h(\\Phi(s, o), \\Psi(g))$\n$Q_U (s, o, a, g; \\eta) = h(\\phi(s, o, a), \\psi(g))$\nLearning $Q_{\\Omega}(s, o, g; \\theta)$ is akin to learning UVFAs with options instead of actions; however, learning $Q_U (s, o, a, g; \\eta)$ involves compressing a lot of information about the state, option, and action into a single embedding. This compression of a large amount of information into the single embedding leads to the poor performance and high variance of UVFAs as the policies/value functions of different options are not interchangeable or comparable by a similarity measure.\nAs such, we draw the following comparisons between HUVFAs and UVFAs in hierarchical settings:\n\u2022 H-UVFAs achieve better performance and generalize better since they split all the embeddings into their respective dimensions instead of compressing a large amount of information into a single embedding. This is further applicable in the reinforcement learning methods of training H-UVFAs\n\u2022 H-UVFAs require less data and are less computationally intense. In the reinforcement learning method one would require a large amount of data to explore more option-action pairs."}, {"title": "7. Ongoing Work", "content": "We are currently working on extending the results to a more complex domain in the Arcade Learning Environment (Bellemare et al., 2013), and will update the paper with further results."}, {"title": "8. Discussion and Future Directions", "content": "We have introduced Hierarchical Universal Value Function Approximators (H-UVFAs), a universal approximator for goal-directed hierarchical tasks. We discuss supervised learning and reinforcement learning based approaches to learn H-UVFAs and provide experiments for the same. We show generalization of learned H-UVFAs to unseen tasks, and show that H-UVFAs have better performance and generalization to the baseline of UVFAS.\nAs a future direction, we propose a work that extends the universal approximator to also include the termination function used in hierarchical reinforcement learning."}, {"title": "C. Explaining UVFA Performance: Options as Specialized Behaviors", "content": "Figure 10 plots the meta-policy and intra-option policies when the goal is located right below the right corridor.\nWe can see that the options specialize to a subset of the rooms and do not learn a policy in other rooms. The policy of the option in a room it doesn't initialize in leads to sub-optimal or even incorrect behavior. For instance, option 0 doesn't initialize in the bottom-left room, option 1 doesn't initialize in the bottom-right room and option 3 doesn't initialize in the top-right room. The policies of the options in these rooms is non-existent and even moves the agent away from the goal.\nHence, these options learn specialized skills and behaviors pertaining to a \"subgoal\u201d construct. In the reinforcement learning of UVFAs, the history might only include transitions pertaining to a given option in a given area of the map, however, that option might be specialized to another room for another goal and hence the relevant information is not learned by the UVFA. Using H-UVFAs gives us the granular control needed to select the correct options in the necessary rooms."}]}