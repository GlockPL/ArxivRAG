{"title": "CCIS-DIFF: A GENERATIVE MODEL WITH STABLE DIFFUSION PRIOR FOR CONTROLLED COLONOSCOPY IMAGE SYNTHESIS", "authors": ["Yifan Xie", "Jingge Wang", "Tao Feng", "Fei Ma", "Yang Li"], "abstract": "Colonoscopy is crucial for identifying adenomatous polyps and preventing colorectal cancer. However, developing robust models for polyp detection is challenging by the limited size and accessibility of existing colonoscopy datasets. While previous efforts have attempted to synthesize colonoscopy images, current methods suffer from instability and insufficient data diversity. Moreover, these approaches lack precise control over the generation process, resulting in images that fail to meet clinical quality standards. To address these challenges, we propose CCIS-DIFF, a Controlled generative model for high-quality Colonoscopy Image Synthesis based on a Diffusion architecture. Our method offers precise control over both the spatial attributes (polyp location and shape) and clinical characteristics of polyps that align with clinical descriptions. Specifically, we introduce a blur mask weighting strategy to seamlessly blend synthesized polyps with the colonic mucosa, and a text-aware attention mechanism to guide the generated images to reflect clinical characteristics. Notably, to achieve this, we construct a new multi-modal colonoscopy dataset that integrates images, mask annotations, and corresponding clinical text descriptions. Experimental results demonstrate that our method generates high-quality, diverse colonoscopy images with fine control over both spatial constraints and clinical consistency, offering valuable support for downstream segmentation and diagnostic tasks.", "sections": [{"title": "1. INTRODUCTION", "content": "Colonoscopy is an essential tool for detecting adenomatous polyps and reducing rectal cancer mortality rates [1, ?, ?]. However, training models for automatic polyp detection is challenging due to the small scale of available colonoscopy datasets, making it difficult to have sufficient robustness and generalization that meet real-world clinical demands.\nTo address this problem, previous methods [4, 5, 2, 6] primarily relied on generative adversarial networks or diffusion models to synthesize more colonoscopy images. Although these efforts aim to address the data scarcity problem, they struggle to generate a sufficiently diverse and high-quality image, and the generation process lacks adequate control, leading to images that fail to meet clinical requirements for practical use. As illustrated in Fig. 1, ArSDM [2] only utilizes the mask to synthesize the colonoscopy image and the generated image is of poor quality and contains noise. Meanwhile, large-scale text-to-image (T2I) diffusion models such as Stable Diffusion [7] and DALL\u00b7E [8] have demonstrated remarkable capabilities in generating images from various prompts.\nThis raises an important question: Can colonoscopy images be generated in a controlled manner using a pre-trained large-scale T2I model? In response, we present an innovative generative method to synthesize high-quality colonoscopy images in a controlled manner. The main contributions of this paper can be summarized as follows:\n\u2022 We propose a novel generative model, named CCIS-DIFF, which offers fine control over both the spatial attributes and clinical characteristics of polyps, enabling more clinically consistent image synthesis for practical use.\n\u2022 We introduce the blur mask weighting strategy to ensure seamless integration of synthesized polyps with the colonic mucosa, along with a text-aware attention mechanism that incorporates textual information into the generation process, to enable customization of polyp images."}, {"title": "2. METHOD", "content": "2.1. Multi-Modal Colonoscopy Dataset\nA key challenge for current diffusion models in colonoscopy image synthesis [2, 6] is the absence of a dataset with consistent spatial characteristics of polyps and their corresponding clinical textual descriptions. Existing datasets, such as EndoScene [9], CVC-ClinicDB [10], and Kvasir [11], primarily contain image-mask pairs that focus on polyp regions. However, these datasets do not include detailed clinical text descriptions, limiting their usefulness for training models that aim to generate clinically accurate and diverse synthetic images. To address this limitation, we construct a novel multi-modal colonoscopy dataset that integrates three essential components: colonoscopy images, segmentation masks, and clinical text descriptions. Such a dataset is vital for fine-tuning pretrained diffusion models, allowing them to adapt effectively to controlled colonoscopy image synthesis.\nAn overview of our dataset construction process is shown in Fig. 2 (b). To generate accurate captions that accurately reflect both spatial constraints and relevant clinical characteristics, we construct the open-source LLaMA [12] large language model (LLM) agent. This agent takes a textual prompt and an image-mask colonoscopy pair to create captions for both the foreground and background. To increase textual diversity, the LLM incorporates different aspects like color, shape, texture, and swelling. As a result, we develop a multi-modal colonoscopy dataset consisting of triplets of colonoscopy images, mask images, and their corresponding textual descriptions which provides the necessary foundation for generating images with improved realism and variability.\n2.2. CCIS-DIFF Architecture\n2.2.1. Overview\nThe overview architecture of our CCIS-DIFF is presented in Fig. 2 (a). Using our multi-modal colonoscopy dataset, we provide the original colonoscopy image I, the mask image M, and the corresponding text description T. Each of these components passes through its respective encoder, noting that the text encoder is frozen. Additionally, we develop a blur mask to ensure that the generated polyp integrates seamlessly with the background, the detailed description will be illustrated in Sec. 2.2.2.\nBased on ControlNet [3], we adopt the trainable diffusion branch and implement the zero convolution strategy to protect this branch by eliminating random noise as gradients in the initial training steps. This structure, when applied to large models like Stable Diffusion [7], enables the frozen parameters to preserve the integrity of the production-ready model that has been trained on billions of images. Meanwhile, the trainable diffusion branch leverages this large-scale pre-trained model to establish a robust backbone for managing multi-modal input conditions. Furthermore, to address the issue of neglecting text prompt and effectively incorporating textual information into the generation process, we incorporate a text-aware attention mechanism (Sec. 2.2.3) in the trainable diffusion branch."}, {"title": "2.2.2. Blur Mask Weighting Strategy", "content": "The purpose of the blur mask weighting strategy is to ensure that the generated polyp is seamlessly integrated with the background. To achieve this, we apply a Gaussian blur operation o to the mask image M, softening the transition between the polyp mask region and the background. We utilize two separate mask encoders to extract features from the mask and the blurred mask, each with non-shared parameters. Subsequently, a weighting matrix Mw is introduced to balance the two mask branches, and Mw is learned using a three-layer MLP. Thus the blur mask embedding Fr can be constructed as:\n$Fb = Mw \\cdot Em(\\sigma(\u039c)),$ (1)\nwhere is the Hadamard product and Em denotes the mask encoder."}, {"title": "2.2.3. Text-Aware Attention Mechanism", "content": "In our experiments, we observed that existing methods, such as ControlNet [3], often overlook the text prompt and rely more heavily on the mask image. We hypothesize that this visual dominance over the text prompt arises from the text-free nature of the self-attention layers. To address this issue, we propose a text-aware attention mechanism that leverages the cross-attention matrix to regulate the output of the self-attention.\nGiven the noise embedding Fn, it is first passed through a ResNet block, as referenced in ControlNet [3]. Following this, we obtain the attention input tensor Fa \u2208 R(hxw)\u00d7d, which is processed through projection layers to derive the queries Qs, keys Ks, and values Vs, as well as the attention QSKT\nmap Ms = \n\u2208 Rhwxhw. To reduce the strong influence of the mask image, we adjust the attention scores of the prompt text. Specifically, we begin by constructing the cross-attention similarity matrix:\n$Mc = SoftMax(QcK^T/\\sqrt{d}),$ (2)\nwhere Qc \u2208 R(hxw)\u00d7d, Kc \u2208 [Rl\u00d7d represent the query and key tensors from their corresponding cross-attention layers, and I denotes the number of tokens in the text prompt. For each pixel j, we define its similarity to the text prompt by summing its similarity scores with the embedding indexed by Mc. We then apply a clamp operation to normalize the scores s; within the range [0, 1]:\n$s_j = Norm(Sum_i(M_c)).$ (3)\nBy calculating the scores for each pixel, we can obtain the final text-aware map S. Subsequently, we utilize S to compute the updated attention map and refine the self-attention process.\nFinally, the output of the cross-attention mechanism interacts once more with the blur mask embedding, generating the final output embedding through a ResNet block."}, {"title": "2.2.4. Training and Inference", "content": "During the training phase, the diffusion models start with an original image I and progressively add noise to create a noisy embedding It, where t indicates the number of times noise is added. The models learn a network ee to predict the noise added to the noisy image It, based on a set of conditions that include the time step t, a text prompt T, and a mask image M. The overall loss function L for the entire diffusion model is represented as:\n$L = E_{1,t,T,M,\\epsilon~N(0,1)} [||\\epsilon \u2013 \\epsilon_{\\theta} (I_t,t,T, M)||^2].$ (4)\nAdditionally, we randomly replace half of the text prompts with empty strings. This strategy enhances the model's ability to directly recognize the semantics of the input mask image, serving as a substitute for the text prompt.\nDuring the inference phase, the diffusion models automatically generate noise and produce the final colonoscopy images based on a text prompt and a mask image. Simultaneously, we utilize Classifier-Free Guidance (CFG) [13] to enhance the sampling process."}, {"title": "3. EXPERIMENT AND RESULTS", "content": "3.1. Implementation Details\nAll methods were implemented in PyTorch on a single NVIDIA A100 40G GPU. We utilized the pre-trained Stable Diffusion v1.5 [7] model, following ControlNet [3], to replicate its UNet encoder as the trainable copy encoder. The batch size was configured to 4, and the learning rate was set to le-5. For inference, we used a default CFG scale of 7.0. The DDIM sampler was employed, using 20 steps to sample each image.\n3.2. Baseline Algorithms and Evaluation Metrics\nFor the colonoscopy image synthesis task, we compared our method with ControlNet [3] and Uni-ControlNet [14], all of which were trained on our multi-modal colonoscopy dataset. Following [3], we assessed performance using FID, CLIP-score, and CLIP-image (measuring the similarity between the generated image and the reference image).\nFor the downstream polyp segmentation task, we utilized PraNet [1] and Polyp-PVT [15] as baseline segmentation"}, {"title": "3.3. Results", "content": "The quantitative results of the colonoscopy image synthesis are illustrated in Table 1. Our method outperforms others in both visual quality and text alignment. Furthermore, we present a comparison of our method with others in Fig. 3. The results show that the polyp region generated by our method is more consistent with the mask image, and the overall information aligns more closely with the text prompt.\nTo conduct a more comprehensive evaluation, we implemented a user study questionnaire. We selected 30 groups of images for comparison and invited 15 clinical experts from the First Affiliated Hospital of Sun Yat-sen University to participate in the survey. Participants were required to rate the synthesized images based on three criteria: image fidelity, mask accuracy, and text accuracy. The average scores for each"}, {"title": "4. CONCLUSION", "content": "In this paper, we present CCIS-DIFF, a generative model that leverages a Stable Diffusion prior for the controlled colonoscopy image synthesis. We begin by developing a blur mask weighting strategy to seamlessly integrate the generated polyp with the colonic mucosa, along with a text-aware attention mechanism to address the issue of neglecting text prompt. Additionally, we introduce a multi-modal colonoscopy dataset created by large language models. Extensive experiments across various settings demonstrate the superior performance of CCIS-DIFF."}, {"title": "5. COMPLIANCE WITH ETHICAL STANDARDS", "content": "This research study was conducted retrospectively using human subject data made available in open access. Ethical approval was not required as confirmed by the license attached with the open access data."}]}