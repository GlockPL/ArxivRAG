{"title": "BatStyler: Advancing Multi-category Style\nGeneration for Source-free Domain Generalization", "authors": ["Xiusheng Xu", "Lei Qi", "Jingyang Zhou", "Xin Geng"], "abstract": "Abstract-Source-Free Domain Generalization (SFDG) aims to\ndevelop a model that performs on unseen domains without relying\non any source domains. However, the implementation remains\nconstrained due to the unavailability of training data. Research\non SFDG focus on knowledge transfer of multi-modal models\nand style synthesis based on joint space of multiple modalities,\nthus eliminating the dependency on source domain images.\nHowever, existing works primarily work for multi-domain and\nless-category configuration, but performance on multi-domain\nand multi-category configuration is relatively poor. In addition,\nthe efficiency of style synthesis also deteriorates in multi-category\nscenarios. How to efficiently synthesize sufficiently diverse data\nand apply it to multi-category configuration is a direction with\ngreater practical value. In this paper, we propose a method called\nBatStyler, which is utilized to improve the capability of style\nsynthesis in multi-category scenarios. BatStyler consists of two\nmodules: Coarse Semantic Generation and Uniform Style Gener-\nation modules. The Coarse Semantic Generation module extracts\ncoarse-grained semantics to prevent the compression of space for\nstyle diversity learning in multi-category configuration, while the\nUniform Style Generation module provides a template of styles\nthat are uniformly distributed in space and implements parallel\ntraining. Extensive experiments demonstrate that our method\nexhibits comparable performance on less-category datasets, while\nsurpassing state-of-the-art methods on multi-category datasets.\nCode is available here.", "sections": [{"title": "I. INTRODUCTION", "content": "EEP neural networks (DNNs) [1, 2, 3, 4, 5, 6] have\nachieved significant progress for various tasks, such as\nimage classification [7, 8], object detection [9, 10], and other\nvisual tasks. However, it is well-known that DNNs tend to\nperform poorly on out-of-distribution test data [11]. To tackle\nthis problem, Domain Adaption (DA) techniques [12, 13] have\nbeen extensively explored to learn domain-invariant features\nusing source and unlabeled target data. In practical scenarios,\nobtaining unlabeled target data is not always feasible yet.\nDomain Generalization (DG) [14, 15, 16, 17] is developed to\nmitigate this limitation, it aims to improve the model's general-\nization ability by using one or more source domains. However,\ndue to the arbitrary nature of domain shifts, limited source\ndomains will easily lead to model overfitting. Furthermore, it\nis costly and infeasible to collect and annotate multiple source\ndomains data for training.\nLimited by the scale and diversity of DG datasets, it\nis difficult for existing methods to generalize to arbitrary\ndomains in open-world scenarios. Therefore, Source-Free Do-\nmain Generalization (SFDG) [19, 20, 21] task is necessary and\nchallenging in real world scenarios. It assumes no access to\nimages during the training and relies solely on the definition of\ntarget task, including category names. Thanks to the research\nof vision-language models, existing SFDG methods typically\nadapt a pre-trained model to target domains [22, 23, 24], but\nthese methods suffers from both unsatisfactory accuracy due\nto limited use of target domains.\nRecently, Cho et al. [18] propose a prompt-driven style gen-\neration method called PromptStyler, which utilizes a textual\nprompt to represent the corresponding image feature within a\njoint space of vision-language models (e.g. CLIP [25]), thus\neliminating the dependence on images. Nonetheless, there still\nexist two issues: 1) PromptStyler ensures semantic consistency\nthrough all downstream category names. As shown in Fig. 1(a),\nwith the number of categories increases, semantic consistency\nwill hinder the learning of style diversity. The cosine sim-\nilarity between styles are continuously rising, which means\nthat increasingly poorer diversity. 2) The one-by-one training\nstrategy is too slow, limiting the training efficiency when the\nnumber of styles and category names are very large, as shown\nin Fig. 2(a). To address these two issues, we propose BatStyler\nto further unlock the space of diversity and provide styles that\nare more uniformly distributed. Leveraging the characteristic\nof uniform distribution, we achieve parallel training of styles.\nIn this paper, we introduce a novel method: BatStyler,\nwhich is utilized to enhance the style diversity in multi-\ncategory scenarios. We design our framework from the fol-\nlowing two perspectives: 1) How to weaken the overly strong\nconstraints of semantic consistency on style diversity? Experi-\nmental results and theoretical evidence reveal that the semantic\nconsistency compress the space of style diversity, which leads\nto the obstruction in diversity learning, especially in the case\nof a large number of categories. The core issue is to weaken\nthe overly strong constraints of semantic consistency while\nretaining sufficient semantics to prevent styles from learning\ncompletely at random, we model this issue as an optimization\nproblem, with the objective of maximizing style diversity and\nthe regularization term being semantic consistency. For more\ndetails, please refer to Sec. III-A. We consider that when the\nnumber of categories is very large, some categories have a\nhigh similarity (e.g. 'tabby cat' and 'tiger cat'). Therefore,\nwe filter out redundant fine-grained semantic constraints to\nenhance style diversity. Before training, we extract coarse-\ngrained semantics from all categories in task through LLM,\nresulting in Coarse Semantic Generation (CSG). 2) How to\ngenerate style uniformly distributed in joint space? In [18],\nstyle diversity is trained by orthogonality, which means there\nmust be a large portion of entire space that remains uncovered\n(e.g. 80 orthogonal vectors can not cover a 1024-dimension\nspace according to schmidt orthogonalization). To address\nthe deficiency, we propose a module called Uniform Style\nGeneration (USG) to initialize a set of uniformly distributed\nvector templates through neural collapse, ensuring that the\ngenerated styles are evenly distributed throughout the entire\nspace. We present this set of templates in the form of a clas-\nsifier, which conveniently enables parallel training of styles,\nthereby improving training efficiency.\nIn summary, our main contributions can be listed as:\n\u2022\nWe propose an innovative approach for SFDG in multi-\ncategory scenarios that enhance the style diversity of\nsynthetic data by weakening the strength of semantic\nconstraints and providing a style template that uniformly\ndistributed in the joint space.\n\u2022\nWe integrate style templates into a fixed classifier to\nachieve parallel training of styles.\n\u2022\nExtensive experiments demonstrate the comparable per-\nformance on less-category datasets and superior perfor-\nmance on multi-category datasets compared with state-\nof-the-art methods."}, {"title": "II. RELATED WORK", "content": "In this section, we will review the related approaches to\nour method, including domain generalization, vision-language\nmodels and neural collapse. The following part presents a\ndetailed investigation."}, {"title": "A. Domain Generalization", "content": "There has been a surge of interest in developing meth-\nods to tackle domain generalization problem in recent years\n[26, 27, 28, 29]. Prior works aim to learn domain-invariant\nfeatures by data augmentation [30, 31], feature alignment\nacross domains [32, 33] and feature normalization [34, 35, 36].\nHowever, most of DG methods expect labelled data from one\nor more source domains to prevent overfitting, which limits the\napplicability to important application areas (e.g. autonomous),\nwhere labelled data is scarce. Limited by the cost of collection\nand annotation, existing methods is difficult to apply to the real\nworld scenarios. Therefore, source-free domain generalization\nis necessary and urgent.\nUnlike traditional DG, source-free domain generalization\n[18, 20] has no access to any images during training. Thus,\nSFDG is much more challenging than previous DG routes,\nbut more in line with real-world applications. To realize this,\nprevious works mostly achieve generalization through data\naugmentation via synthetic data [37, 38, 39]. Recently, Cho\net.al [18] propose a prompt-driven style generation method\ncalled PromptStyler, which uses textual features to represent\nimage features in a joint space. Specifically, the method preset\na prompt (e.g. \"a S style of a\"), where the S is a pseudo-word\nthat can be trainable. Using style diversity loss to ensure that\nall encoded style features are perpendicular to each other and\ncontent consistency loss to put complete prompt (e.g. \"aS\nstyle of a [class]\") and category name (e.g. \"[class]\") together\nto be parallel. Then, a linear classifier is trained using text\nfeatures and inferred using image features. However, its style\ndiversity and training speed will decline with the increase of\nstyle vectors and category names. We improve PromptStyler\nby introducing a coarse semantic generation module to weaken\nthe constraints on style diversity and providing a style template\nthat uniformly distributed in joint space, which enhance the\nstyle diversity in multi-category scenarios."}, {"title": "B. Vision-Language models", "content": "Vision-Language models [40, 41, 25, 42, 43] play a key\nrole in incredible strides of computer vision. Most visual\ntasks rely heavily on a classifier head in DNNs, and only can\nbe applied to sole task, leading to poor transferability and\nlack of flexibility. To tackle these problems, vision-language\nmodels (VLMs) are investigated extensively, which use image-\ntext pairs as training samples and text as supervision, instead\nof hand-crafted labels. They can learn rich vision-language"}, {"title": "C. Neural Collapse", "content": "Recently, a new phenomenon called Neural Collapse (NC)\n[45] has been discovered, which describe an elegant geometric\nalignment between last-layer features and classifier in a well-\ntrained model. DNNs enter the terminal phase of training\nand tend to exhibit intriguing neural collapse properties when\ntraining error reaches zero. NC essentially clarifies a state at\nwhich the within-class variability of last-layer outputs is in-\nfinitesimally and their class means form a simplex Equiangular\nTight Frame (ETF), which refers to a matrix that composed\nof N maximally-equiangular and equal-l2 norm P-dimension\nvectors. E=[e1,.\u2026\u2026,en] \u2208 \\mathbb{R}^{P\u00d7N} and satisfies formula E =\nVNU(IN \u2212 1/N 1N1^T), where IN \u2208 \\mathbb{R}^{N\u00d7N} is the identity\nmatrix, 1n \u2208 \\mathbb{R}^{N} is an all-ones vector, and U\u2208 \\mathbb{R}^{P\u00d7N} is\na partial orthogonal matrix that allows a rotation and satisfies\nU^TU = IN. All vectors in E have equal l2-norm 1 and equal-\nmaximal equiangular angle - 1/(N-1) [46].\nAlthough this theory is relatively new, there has already\nbeen a lot of works using NC in imbalanced learning [47, 46],\nweakly-supervised learning [48, 49], class-incremental learn-\ning [50, 51], VLM tuning [52, 53] and other visual tasks.\nTo address the issue of non-uniform style distribution, we\ninitialize style vector templates for pseudo-styles uniformly\ndistributed in joint space through neural collapse. Then we\nfurther adjust them using semantic consistency loss. At the\nsame time, since it initializes pseudo-styles with largest mar-\ngins between them all at once, we are able to achieve parallel\ntraining, improving the training efficiency significantly."}, {"title": "III. THE PROPOSED METHOD", "content": "In this section, we will present our proposed framework\nBatStyler, which consists of two encoders, including a text\nencoder and an image encoder, a fixed classifier used to pro-\nvide template for pseudo-styles, an coarse semantic generation\nmodule to extract coarse-grained categories and a learnable\nlinear classifier used to classify images. Note that there is\nno correlation between the two classifiers mentioned above.\nWe employ CLIP as vision-language model, all parameters\nin CLIP are frozen in entire framework. The overview of\nproposed method is displayed in Fig. 3. The training process\nis divided into two stages: style generation and linear classi-\nfier training. The Pseudo-style words is the only learnable"}, {"title": "A. Coarse Semantic Generation", "content": "In PromptStyler, the model necessitates the semantic con-\nsistency loss with all categories and demonstrates remarkable\nperformance in less-category datasets. However, we observe\nthat with the increase of categories, the performance tends to\ndeteriorate. In multi-category scenarios, many semantics have\na high similarity, and redundant semantic information greatly\ncompresses the learning space for style diversity, limiting the\ndiversity visible to model.\nThe objective is to enhance the style diversity, which we\nmodel it as an optimization problem, as follows:\nmin(L(\u03b8) + \u03bbR(\u03b8)),\nwhere L(\u03b8) is our objective function: style diversity loss, \u03b8\nis parameter of learnable pseudo-style word embeddings, \u03bb is\nregularization coefficient, and R(\u03b8) is regularization: semantic\nconsistency loss. According to PromptStyler, we can define\nL(\u03b8) and R(\u03b8) as:\nL(\u03b8) = \\sum_{i=1}^{K} \\sum_{j=1}^{K-1} |cos < f_{style}(\u03b8_i), f_{style}(\u03b8_j) > |,\nR(\u03b8) = \\sum_{i=1}^{K} \\sum_{c=1}^{N} -cos < f_{content}(c), f_{style-content} (\u03b8_i, c) >,\nwhere the K is number of pseudo-styles and N is the number\nof categories. Note that three kinds of prompts we used: a\nstyle prompt (\u201ca S\u00a1 style of a", "[class]c\"),\nand a style-content prompt (\u201ca S\u00a1 style of a [class]c\"), and\n\u03b8\u00a1 is parameter of i-th pseudo-style word embedding. fstyle,\nfcontent and fstyle-content is style prompt feature, content\nprompt feature and style-content feature respectively encoded\nand normalized by CLIP's text encoder. The cos < \u00b7 > stands\nfor the cosine calculation operation. From Eq. 3, it can be\nseen that the strength of semantic consistency constraint is\nproportional to the number of categories N. Thus, the core\nissue to enhance style diversity lies in reducing N, but directly\nreducing N would lead to a corresponding weakness of seman-\ntic consistency, causing styles to learn completely at random.\nTherefore, it is necessary to retain semantic consistency as\nmuch as possible while reducing N.\nWhen the number of categories N is very large, there\nwill be a high similarity among some categories (e.g. 'tabby\ncat' and 'tiger cat'), which implies the existence of redun-\ndant semantics. We cluster similar semantic information into\npiles and then extract coarse-grained semantics from each\npile to eliminate redundant information, achieving the goal\nof reducing N, thereby enhancing style diversity. Next, we\nwill introduce proposed module Coarse Semantic Generation\n(CSG) to address this issue.\nBased on above observation, we design an automatic ex-\ntraction module to extract common semantics. Specifically, we\nencode categories by text encoder to derive the text features.\nThen, we apply KMeans++ [54] to cluster these text features,\nand the metric of clustering we used is silhouette coefficient,\nwhich can determine the optimal number of clusters without\nmanual intervention. Text within each cluster can be regarded\nas sharing some coarse-grained semantics, as shown in Fig. 4.\nNext, we extract the common coarse-grained semantics from\neach cluster.\nInspired by GPT's powerful capabilities, we utilize GPT-\n4 [55] to extract coarse-grained semantics. Specifically, for a\ncluster, we query GPT-4 with:\n\\\"Q: Tell me\nlist of category names under a cluster} have in\ncommon with three words. If not, it can be\nnothing.\\\".\nAfter extraction, we further check if generated coarse-grained\nsemantics contains \\\"nothing\\\", it means that no common infor-\nmation. If so, we perform binary clustering for corresponding\ncluster and query again. It should be noted that although\nwe considered the scenario where no shared semantics exist,\nin actual execution, we have never encountered \u201cnothing": "nWe define this coarse semantic set as css. Based on this\nprocess, when dealing with numerous categories, we can per-\nform semantic consistency loss using few common semantic\n(e.g. [\"tabby cat\", \"tiger cat\"] = \"cat\"). We leverage coarse-\ngrained consistency loss Lsc to replace semantic consistency,\nas formulated below:\nL_{sc} = \\sum_{i=1}^{K} \\sum_{c\u2208css} -cos < f_{content}(C), f_{style-content} (\u03b8_i, c) >,\nwhere c is from css, instead of original categories. Using\nfew coarse-grained category names to guarantee semantic\nconsistency, we effectively avoid disturbing the diversity of"}, {"title": "B. Uniform Style Generation", "content": "In [18], the style diversity is trained by orthogonality,\nwhich means there must be a large portion of joint space\nthat remains uncovered if the number of pseudo-style K is\nsmall than the dimension of joint space (e.g. 80 orthogonal\nvectors can not cover a 1024-dimension space according to\nschmidt orthogonalization). The core issue is that style diver-\nsity is learned merely based on orthogonality. Therefore, we\npropose a module: Uniform Style Generation (USG), which\nuses a uniformly distributed template to learn more diverse\nstyles. Inspired by the characteristic of neural collapse, it can\ninitialize a set of vectors with equal and maximum margin,\nwhich we use to initialize the templates. We employ these\nvectors initialized by neural collapse as template to enhance\nthe style diversity. In addition, we present this set of templates\nin the form of a classifier, which conveniently enables parallel\ntraining of styles, thereby improving training efficiency.\nSpecifically, we pre-define K vectors by neural collapse,\nwhere the K is number of pseudo-styles, which is used as the\ninitial template for pseudo-styles. Each pseudo-style has its\nown unique template vector, which is trained through semantic\nconsistency and template. We assume that \u0174 = [W1,...,WK]\n\u2208 \\mathbb{R}^{P\u00d7K} as the vectors initialized by neural collapse, where\nPis dimension of vision-language model joint space. Each\npair within \u0174 satisfies Eq. 5:\n\\frac{w_1^T w_{n_2}}{\\lVert w_1\\rVert \\lVert w_{n_2}\\rVert} = \\begin{cases} \\frac{1}{K-1}, & \\text{if } n_1 = n_2 \\\\ \\frac{-1}{K-1}, & \\text{otherwise} \\end{cases}, \\forall n_1, n_2 \u2208 [1, K],\nwhere \u03b4_{n_1,n_2}=1 when n\u2081 = n2 and 0 otherwise. Therefore, they\nhave equal and maximal margin between any pairwise vectors.\nMost importantly, they are uniformly distributed throughout\nthe entire joint space.\nIn order to improve the efficiency, we combine these vectors\ninto a fixed classifier, which allows for parallel training.\nSimultaneously, to integrate the training of style diversity, we\nassign labels to each template in sequence from 1 to K. With\nthis fixed classifier and the labels we manually assigned, we\ncan achieve diversity training through a simple cross-entropy\nloss LCE. Not only does it discard complex loss, but it also\nallows for more uniform diversity."}, {"title": "C. Training and Inference", "content": "In training phase, the whole training process is divided into\ntwo stages. The first stage is the style generation, which is\nused to generate data for training of next stage. After the style\ngeneration, we take fstyle-content as training data and train a\nlinear classifier FC to obtain the final class prediction. Notably,\nthis classifier differs from the fixed classifier in Sec. III-B. The\ntraining of first stage can be trained with:\nL_{1-stage} = L_{CE} + L_{SC}.\nSimilar to [18], we employ ArcFace Loss [56] as our classifi-\ncation loss Le in second training stage. Crucially, we employ\ncoarse-grained semantics to guarantee semantic consistency\nin first training stage, while in second stage, we utilize fine-\ngrained semantics to train classifier.\nIn the inference stage, we simply use image encoder to\nreplace text encoder. Given an image, the image encoder\nencode image to feature in joint vision-language space, and\nthen used for classification."}, {"title": "D. Discussion", "content": "Our method is designed to enhance the diversity of synthe-\nsized styles in multi-category scenarios. In Sec. III-A, we have\nformulated it as an optimization problem, as formulated in Eq\n1. It can be seen that there are two approaches to weakening\nsemantic consistency constraints: reducing N and lowering \u03bb.\nIn proposed method, we adopt the strategy of reducing N. In\nthis section, we discuss why we do not directly lower \u03bb. \u03a4\u03bf\nevaluate, we make modification to the loss of [18], as follows:\nL_{prompt} = L_{style} + \u03bbL_{content},\nwhere \u03bb is between 0 and 1. As seen in Tab. VII, with the\nincrease of \u03bb, style diversity has indeed rise (the lower the\nSD, the better style diversity), but semantic consistency is\nalso continuing to drop (the higher the SC, the better the se-\nmantic consistency), which means increasingly poor semantic\nconsistency. Therefore, we conclude that it is not possible to\ndirectly enhance style diversity while maintaining semantic\ninformation by simply reducing the semantic consistency loss\ncoefficient \u5165. This leads to the development of our method: by\nextracting coarse-grained semantics to reduce N, we weaken\nthe overly strong constraints of semantic consistency on style\ndiversity, thereby expanding the space of style diversity."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we begin by presenting the datasets in Sec-\ntion IV-A and implementation details in Sec. IV-B. Following\nthat, we evaluate our proposed method against the current\nstate-of-the-art domain generalization source-free image clas-\nsification techniques in Section IV-C. To verify the impact of\ndifferent modules in our framework, we carry out ablation\nstudies in Section IV-D. Finally, we delve deeper into the\nproperties of our approach in Section IV-E."}, {"title": "A. Evaluation Datasets", "content": "To evaluate the effectiveness of our method, we conduct\nexperiments on six domain generalization benchmark datasets:\nPACS [69], VLCS [70], OfficeHome [71], ImageNet-R [72],\nDomainNet [33] and ImageNet-S [73]. The PACS dataset\nconsists of 7 classes captured from four domains: art, cartoon,\nphoto and sketch, and the validation set has 9991 images.\nThe VLCS dataset consists of 5 classes from four domains:\nVOC2007, LabelMe, SUN09 and Caltech101, totaling ap-\nproximately 25K images. The OfficeHome dataset has 65\nclasses from four domains: art, clipart, product and real-world,\naccumulating to a total of 15,500 images. The ImageNet-\nR dataset has 200 classes from many domains (e.g. sculp-\ntures, art), which has about 30K images. The DomainNet\ndatasets has 345 classes from six different domains, which\nhas about 600K images. Finally, the ImageNet-S dataset has\n1000 classes from many domains in ImageNet, which has\nabout 1.2 million images for evaluation. According to the task\ndefinition of source-free domain generalization, we do not use\nany images for training, hence, all images are reserved for\nevaluation. Note that we define PACS, VLCS and OfficeHome\nas less-category datasets, and ImageNet-R, DomainNet and\nImageNet-S as multi-category datasets with more categories."}, {"title": "B. Implementation Details", "content": "BatStyler maintains consistent implementation and training\nwith identical configurations across all evaluation datasets. The\nmodels are trained on a single RTX3090 GPU. Further detailed\ncomparisons are detailed as follows:\nArchitecture. We employ CLIP [25] as vision-language\nmodel, and implement with ResNet-50, ViT-B/16 and ViT-\nL/14 for evaluation. Notably, the image encoder and text\nencoder are all frozen during training. The dimension of their\njoint space 1024, 512, 768 respectively, and the dimension of\nembedding are 512, 512, 768 respectively.\nCoarse Semantic Generation. Before training, we employ\nsilhouette coefficient as clustering metric, the number of\nclusters is set to the value where the silhouette coefficient\nreaches its peak value. Additionally, we employ GPT-4 to\nextract 3 coarse-grained semantics for each cluster. It should\nbe noted that this module is function before training.\nStyle Generation. We integrate vectors initialized by neural\ncollapse as a fixed classifier, and generate pseudo-styles in\nparallel. We train the styles for 300 epochs using SGD\noptimizer with a learning rate 0.2, a momentum 0.9, a batch"}, {"title": "C. Evaluations", "content": "Comparison with state-of-the-art methods. We compare\nour method with image-free state-of-the-art (SOTA) methods,\nincluding Zero-shot CLIP [25], WaffleCLIP [63], PromptStyler\n[18], PromptTA [65], DPStyler [66], Cp-CLIP [68], DCLIP\n[67] and SFADA [61]. In addition, we also compare our\nmethod with other methods, noting that these methods require\ntraining with source domain images, including MSAM [58],\nNormAUG [59], IPCL [60] and NAMI [62]. The CLIP utilize\nweb scale image-text pairs to train a vision-language model\nwith contrastive learning. We only evaluate zero-shot perfor-\nmance on six datasets above with two prompts: C (e.g. \"dog\")\nand PC (e.g. \"a photo of dog\"). WaffleCLIP observe that\nthe average of many prompts is the main driver of enhanced\nperformance, instead of fine-grained semantics. Therefore, it\nproposed to initialize many different prompts with random\nwords, which is a non-training method. The PromptStyler\nproposed to employ textual prompt to represent corresponding\nimage feature, and train a linear classifier with text feature,\nwhich is a source-free method and do not need any images\nfor training. The PromptTA and DPStyler are improvements\nover PromptStyler, both are source-free domain generalization\nmethods. For zero-shot CLIP, we only evaluate performance\non benchmark datasets. For WaffleCLIP, we use the same\nbackbone with official configuration to perform. In Prompt-\nStyler, PromptTA and DPStyler, we employ same configura-\ntion with BatStyler to conduct the comparison. Compared to\nmethods that are trained with source domain images, under\nthe same model architecture, the performance improvement\nof our method is also quite significant. Results can be seen\nin Tab. I. The Avg is average result across all datasets,\nand M-Avg is average result across multi-category datasets.\nRemakably, BatStyler outperforms other methods across multi-\ncategory datasets, and achieves comparable performance on\nless-category datasets. Besides, although our method is slightly\npoorer than other methods on PACS, VLCS and OfficeHome\n(less-category datasets), the average result of our method is\nbest in all downstream tasks.\nDomain-independent analysis. In Tab. II, we conduct an\nindependent analysis of the performance on each domain. The\ndataset used is DomainNet, and all are conducted on ResNet-\n50. It is evident that the performance of CLIP-based models is\nhighly related to CLIP itself. The performance of CLIP on the\nQuickdraw domain is relatively poor, and other CLIP-based\nmodels also perform relatively poorly in this domain. It can\nalso be seen that the performance of BatStyler is better than\nPromptStyler, especially in Quickdraw and Sketch.\nEvaluation on computation resources. As shown in Tab.\nIII, we provide detailed resource usage comparison between\nzero-shot CLIP [25], PromptStyler [18] and BatStyler (Ours)\non multi-category datasets, encompassing both training and\ninference resources. From the table, it is noticeable that during\ntraining stage with USG, BatStyler's training time can be\nreduced to 10% of PromptStyler in style generation. Due to\nthe parallel training, the GPU memory is higher than other\nmethods. It should be noted that we primarily focus on training\nstage 1: style generation.\nEvaluations of t-SNE visualization results. We utilize\nstyle-content prompt to visualize style-content features, the\ndifferent colors represent different categories. Due to the vast\nnumber of categories of ImageNet-S, it is impractical to\nvisualize all categories. Therefore, we randomly select four\ncategories in ImageNet-S for visualization. The four randomly\nselected categories are mower, megalith, wing and airedale.\nFrom Fig. 5, in multi-category scenarios, BatStyler exhibits\nbetter style diversity within a category, and preserves the\nintervals between different semantics.."}, {"title": "D. Ablation Study", "content": "Main Ablation on modules. We conduct ablation exper-\niments to evaluate the effectiveness of each module in our\nproposed method. The results are presented in Tab. IV, where\nUSG represents Uniform Style Generation and CSG represents\nCoarse Semantic Generation, respectively. From the table,\nwhen using USG or CSG individually, there is a significant\nperformance improvement on multi-category datasets. The\ngreater improvement with CSG suggests that the overly strong\nconstraints imposed by semantic consistency are the main\nreason hindering the model's generalization to multi-category\nscenarios. Finally, the forth line, \u201cUSG + CSG\" module\nachieves the best average results in all tasks except PACS,\nconfirming the effectiveness of the BatStyler.\nComparison under different classifier initialization. We\ncompare neural collapse with random initialization but not\ntrainable, and a learnable classifier in Tab. V. According to\nthe comparison, we find that the proposed NC-initialization\ncan obtain better performance than the random classifier and\ntrainable classifier, which thanks to the fact that the neural"}, {"title": "E. Further Analysis", "content": "Analysis of Coarse Semantic Generation. In this section,\nwe investigate the impact of different numbers of coarse-\ngrained semantics of each cluster C, as shown in Tab. VI.\nDue to the comparable performance, we only evaluate the\nimpact on multi-category datasets. As observed in figure,\nin ImageNet-R, ImageNet-S and DomainNet, after coarse-\ngrained extraction, the performance is fluctuating obviously,\nespecially in ImageNet-R. The motivation of proposed method\nis to reduce the number of categories N in multi-category\nscenarios to enhance style diversity. If too many semantics are\nextracted, it will again lead to poor style diversity. Therefore, it\nis necessary to limit the number of semantics within an appro-\npriate range. It can be observed that when C is relatively large,\nthe performance significantly declines. When extracting one\ncoarse-grained semantic, the extracted semantic may not be\nsufficiently informative to fully represent the cluster it stands\nfor, hence the need to extract multiple semantics. Additionally,\nbetween 3 and 5, the model shows no significant fluctuations,\nso we set C to 3, which achieves good performance with fewer\nsemantic costs.\nAnalysis of Text-to-Image synthesis. In Fig. 6, we select 6\ndifferent style-content prompts randomly and retrieve images"}, {"title": "V. CONCLUSION", "content": "In this paper, we have proposed an effective strategy to\nsynthesize pseudo-styles, called BatStyler, which utilized to\nenhance the style diversity in multi-category scenarios without\ncompromising the semantic consistency, and it realizes parallel\ntraining. Although we have achieved good performance, since\nwe build upon the CLIP, the performance heavily relies on\nCLIP's joint space of vision and language, which implies if\nthe vision and language is not well aligned, the performance\nmay also suffer from deterioration."}]}