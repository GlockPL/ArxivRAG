{"title": "What is lost in Normalization? Exploring Pitfalls in\nMultilingual ASR Model Evaluations", "authors": ["Kavya Manohar", "Leena G Pillai"], "abstract": "This paper explores the pitfalls in evaluating multilingual automatic speech recognition (ASR) models, with a particular focus on Indic language scripts. We investigate the text normalization routine employed by leading ASR models, including OpenAI Whisper, Meta's MMS, Seamless, and Assembly AI's Conformer, and their unintended consequences on performance metrics. Our research reveals that current text normalization practices, while aiming to standardize ASR outputs for fair comparison, by removing inconsistencies such as variations in spelling, punctuation, and special characters, are fundamentally flawed when applied to Indic scripts. Through empirical analysis using text similarity scores and in-depth linguistic examination, we demonstrate that these flaws lead to artificially inflated performance metrics for Indic languages. We conclude by proposing a shift towards developing normalization routines that leverage native linguistic expertise, ensuring more robust and accurate evaluations of multilingual ASR models.", "sections": [{"title": "1 Introduction", "content": "Automatic speech recognition (ASR) systems have become increasingly relevant in various applications, ranging from voice assistants and transcription services to accessibility tools for the disabled population. The performance and usability of ASR models are evaluated in terms of their error rates. Recent advancements in open ASR models pretrained in self-supervised (Schneider et al., 2019; Babu et al., 2022; Chung et al., 2021) manner or weakly supervised (Radford et al., 2023) manner are capable of handling various languages and scripts. These models can be fine-tuned for improved performance in domains or languages of interest. This capability has revolutionized speech recognition in ultra low resource languages and scenarios (Rouditchenko et al., 2023). Many of these models have brought down state of the art (SOTA) word error rates (WERs) on popular benchmarks.\nEvaluation of the performance of ASR models are often affected by the prediction differing from the ground truth in letter casing, punctuation, spelling variants etc. leading to inflated WERs. To mitigate this, a text normalization routine is employed (Deviyani and Black, 2022; Zhang et al., 2021). A proper text normalization routine is required to minimize penalization of non-semantic differences by aligning the predicted output more closely with the ground truth.\nThe study presented in this paper examines the pitfalls in the current normalizations routines employed in the latest ASR models on the banchmarking of non-English languages, specifically on many Asian languages that use Indic scripts. Our empirical analysis reveals that the current normalization practices can result in significant errors, particularly in many low-resource languages, by boosting the model performance on many benchmarks and misleading the research community. We propose for the development of linguistically informed normalization routines that account for the unique characteristics of each language, ensuring a fair and reasonable evaluation and benchmarking process for multilingual ASRs."}, {"title": "2 Background and Related Works", "content": "Prior to the introduction of OpenAI's Whisper model (Radford et al., 2023), most ASR systems were trained on normalized text transcripts and produced output without punctu-"}, {"title": "3 Methodology", "content": "In this study, we present two complementary empirical evaluations to assess the impact of Whisper's normalization routine on different languages. First, we conduct an intrinsic evaluation by comparing the similarity of example sentences from various languages before and after normalization, using the METEOR score as a text similarity metric. Second, we perform an extrinsic evaluation by measuring the WER on a multilingual benchmark dataset for the same set of languages, with and without the application of Whisper's normalization. For our case study, we used both the baseline and fine-tuned Whisper ASR because their outputs include punctuation, unlike other ASR models in the literature. This allowed us to demonstrate the impact of normalization on ASR outputs with punctuation. All the datasets and the models used in this experiments are available under permissive licenses in Huggingface repositories and listed in Appendix A. All the evaluations were run on a single NVIDIA A100 GPU."}, {"title": "3.1 Analysis of Text Similarity after\nWhisper Normalization", "content": "To empirically assess the impact of Whisper's normalization routine on different languages, we conducted a comparative analysis of example sentences from languages that employ various script systems. Specifically, we selected languages that use Latin script (English and Finnish), Indic scripts (Hindi, Tamil, and Malayalam), and South East Asian scripts (Thai). For each language, we prepared a set of example sentences that were identical in meaning but differed in their script and formatting as presented in Table 1. The METEOR score (Banerjee and Lavie, 2005) was employed to quantify the similarity between the original and normalized sentences. It is a text similarity metric that considers the precision, recall, and F-score of the machine-translated text, providing a comprehensive measure of its similarity to the reference text, while also placing importance on the order of words in the text.\nThe similarity scores we obtained demonstrate the varying impact of Whisper's normalization routine on different languages. The high similarity scores for English (0.97) and Finnish (0.95) indicate that the normalization process preserves the linguistic structure and meaning of these languages very well. The diacritic marks in Finnish are retained without any distortion as indicated in the Table 1. This is because the normalization routine ensures the diacritic marks gets converted to letter class of characters using NFKC compatibility composition rules of Unicode\u00b3, before mark class of characters are replaced by space.\nIn contrast, as illustrated in Table 1, the normalization process severely distorts the text in languages other than English and Finnish. The replacement of Unicode characters in the mark class, including vowel signs and virama symbols, by spaces after Whisper normalization significantly alters the linguistic structure of these languages. While Hindi, with a METEOR score of 0.38, is less affected due to its analytic typology, Malayalam and Tamil are severely impacted (Kumar et al., 2007; Manohar et al., 2020) by the splitting of morphologically complex words at every occurrence of vowel signs and virama symbols, leading to similarity scores of 0. Thai, which typically does not use spaces between words, is also affected by the removal of important vowel signs, resulting in a text that is unusable due to excessive spacing and a similarity score of 0."}, {"title": "3.2 Impact of Whisper Normalization\non WER", "content": "To empirically analyze the impact of normalization on the WER, we present the results of evaluating the original Whisper-small model, referred to as the baseline model, with and without the application of Whisper's normalization on the test split of Google FLEURS (Conneau et al., 2022) multilingual speech dataset.\nThe left side bar graph in Figure 1 shows that the WER of the baseline model is significantly high for languages other than English and Finnish, with values of 86.9% for Hindi, 93.3% for Tamil, and 287.4% for Malayalam. The baseline ASR model exhibits a WER exceeding 100% for Malayalam due to a high number of insertion errors, leading to the combined total of substitutions, deletions, and insertions surpassing the total word count in the reference transcript. While the application of Whisper's normalization results in modest WER improvements for English and Finnish, with an absolute reduction of 5.1% and 3.2% respectively, Indic languages experience suspicious absolute WER reductions: 21.9% for Hindi, 41.5% for Tamil, and a substantial 152.2% for Malayalam.\nDue to the poor performance of the baseline model on many Indian languages, we conducted a further comparison of WER with and without Whisper's normalization on publicly available models that have been derived from the baseline model after language-specific finetuning. The fine-tuned models used in these evaluations are listed in Appendix A. Fine-tuning has significantly improved the performance of the Hindi, Tamil, and Malayalam models.\nFine-tuned models of English and Finnish exhibit a reasonable absolute reduction of 4.5% and 3.2% on WER respectively. In contrast, Indic languages exhibit a substantial absolute reduction in WER, with decreases of 10.7% for Hindi, 21.3% for Tamil, and 34.1% for Malayalam. Notably, the languages that showed the worst similarity scores exhibit the maximum improvement in WER after normalization. This suggests that the normalization process, which breaks most words into a series of consonants and adds spaces, artificially increases the number of words in the reference, thereby improving the WER."}, {"title": "4 Recommendations", "content": "Findings from our empirical evaluation underscore the importance of language-specific normalization routines to ensure accurate text representation and reliable performance evaluation in many underrepresented Indic languages. Building up on our findings, we propose a collaborative approach, leveraging the collective efforts of native speakers and linguistic experts to develop effective normalization routines for diverse linguistic contexts."}, {"title": "5 Conclusions", "content": "The empirical evaluation conducted in this study highlights that the current practice of normalization severely affects the text representation across languages, resulting in artificially boosted WER and SOTA performance. By adopting a more tailored approach to evaluations, we can enhance the reliability of multilingual ASR models, making them truly inclusive and effective across diverse linguistic landscapes."}, {"title": "6 Limitations", "content": "1. Being a position paper, this study highlights only the limitations of existing normalization techniques, but does not propose new normalization algorithms.\n2. The results are based on specific datasets and publicly available models used for evaluating WER. Variability in datasets (e.g., different accents, dialects, or recording conditions) might influence the reported values.\n3. The primary metric discussed is WER. Other evaluation metrics (e.g., phoneme error rate, semantic error rate, match error rate) might provide additional insights into the impacts of text normalization.\n4. We used the raw transcription field of the FLEURS corpus, which could be a reason for the difference from the WER values reported in Radford et al. (2023).\n5. While the paper focuses on text normalization on Indian languages there could be other languages which gets affected by the normalization differently.\n6. We omitted Thai from WER comparison charts because for languages where space is not a word delimiter, character error rate is the metric reported in Radford et al. (2023)."}, {"title": "A Resources", "content": "We have used the following publicly available models and datasets for our experiments.\nASR Models\n1. The baseline model:\nhttps://huggingface.co/openai/\nwhisper-small\n2. The Fine-tuned English:\nhttps://huggingface.co/openai/\nwhisper-small.en\n3. The Fine-tuned Finnish:\nRASMUS/whisper-small-fi-15k_sample\n4. The Fine-tuned Hindi:\nhttps://huggingface.co/vasista22/\nwhisper-hindi-small\n5. The Fine-tuned Tamil:\nhttps://huggingface.co/vasista22/\nwhisper-tamil-small\n6. The Fine-tuned Malayalam:\nhttps://huggingface.co/vrclc/\nWhisper_small_malayalam\nSpeech Dataset\n1. Google FLUERS:\nhttps://huggingface.co/datasets/\ngoogle/fleurs"}]}