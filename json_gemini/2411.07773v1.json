{"title": "Likelihood as a Performance Gauge for Retrieval-Augmented Generation", "authors": ["Tianyu Liu", "Jirui Qi", "Paul Hett", "Arianna Bisazza", "Mrinmaya Sachan", "Ryan Cotterell"], "abstract": "Recent work finds that retrieval-augmented generation with large language models is prone to be influenced by the order of retrieved documents in the context. However, the lack of in-depth analysis limits the use of this phenomenon for prompt engineering in practice. In this study, we posit that likelihoods serve as an effective gauge for language model performance. Through experiments on two question-answering datasets with a variety of state-of-the-art language models, we reveal correlations between answer accuracy and the likelihood of the question at both the corpus level and the instance level. In addition, we find that question likelihood can also indicate the position of the task-relevant information in the context. Based on these findings, we propose two methods that use question likelihood as a gauge for selecting and constructing prompts that lead to better performance. We demonstrate their effectiveness with experiments. In addition, our likelihood-based methods are efficient, as they only need to compute the likelihood of the input, requiring much fewer language model passes than heuristic prompt engineering methods that require generating responses. Our analysis deepens our understanding of how input prompts affect model performance and provides a promising direction for efficient prompt optimization.", "sections": [{"title": "1 Introduction", "content": "Prompt designing is crucial for large language models (LMs) when tackling downstream tasks with retrieval-augmented generation (RAG, Lewis et al., 2020). Well-designed prompts can boost LMs' performance and lead them to generate responses that better meet users' expectations (Gao et al., 2021; Izacard et al., 2024; Liu et al., 2024; Schulhoff et al., 2024; Ma et al., 2024, inter alia).\nTypically, under the RAG framework, a prompt consists of three major components\u2014an instruction defining a task and providing general guidance, a specific question of the task, and a context comprising a set of documents retrieved by retrievers from some external source (Karpukhin et al., 2020; Ni et al., 2022). Much previous work has explored empirical approaches of prompt engineering, such as manually designing prompts that mimic human reasoning (Wei et al., 2023; Yao et al., 2023). Recently, Liu et al. (2024) have shown that LM performance is substantially affected by the order of the retrieved documents in the context:\nFormally, in an input prompt, we refer to the segment that directly conveys the question or query expected to be solved by the LM's output as a question. In our experiments, this contains the entire question sentence, including the punctuation."}, {"title": "2 Related Work", "content": "Prompt engineering is important for making the best use of LMs in real-world applications (Giray, 2023; Ekin, 2023; Gonen et al., 2023). The most straightforward prompt engineering method is to manually design prompts using heuristics,\nEncoding, also known as prefill phase or prompt phase, requires significantly fewer LM passes than decoding. Previous work (Kwon et al., 2023; Zhong et al., 2024) reports that the throughput of processing input prompts, measured by the number of tokens processed per second, can be up to three orders of magnitude larger than that of completion generation."}, {"title": "2.1 Prompt Engineering", "content": "which requires human experts to design prompts based on domain-specific knowledge and select the prompts that lead to better performance on downstream tasks (Zhou et al., 2023; Marvin et al., 2023). Meanwhile, another line of work explores automatic approaches for prompts engineering (Gao et al., 2021; Pryzant et al., 2023). However, they both require decoding for outputs from LMs to evaluate the quality of prompts, thus incurring high computational costs."}, {"title": "2.2 Retrieval-Augmented Generation", "content": "Retrieval-augmented generation is a promising technique for improving LMs' ability to solve knowledge-intensive tasks (Lewis et al., 2020; Asai et al., 2021; Borgeaud et al., 2022). In the RAG framework, a set of documents relevant to a user query is retrieved from an external source and inserted into prompts as a context, to provide additional information to the LM and improve response quality (Petroni et al., 2020; Lewis et al., 2020). RAG tasks can be divided into two types: short-form and long-form, depending on the topic of the questions and the format of the expected answers. Short-form QA (Izacard and Grave, 2021; Liu et al., 2024) usually concerns factual questions about real-world facts. The expected answers are often unambiguous and concrete words or short phrases. Long-form QA (Fan et al., 2019; Gao et al., 2023) involves how, why, and what questions that seek more comprehensive responses."}, {"title": "2.3 Effect of Document Order on Answer Accuracy", "content": "Liu et al. (2024) finds that LMs perform better when the document with relevant information is positioned at the beginning or the end of the prompt using under RAG framework. Specifically, when moving the task-relevant information from the beginning to the end of the document sequence, answer accuracy exhibits a U-shaped trend on a multi-document QA task and a synthetic key-value retrieval task, both using RAG pipelines. However, Liu et al. (2024) mainly focuses on an empirical study with less in-depth analysis, resulting in a gap between the phenomenon and its practical implications. In this work, we attempt to bridge this gap.\nIn Liu et al.'s (2024) experimental settings, the gold document is unique in a prompt for each question."}, {"title": "3 Experimental Setup", "content": ""}, {"title": "3.1 Datasets", "content": "We first experiment on the NQ-Open dataset following Liu et al. (2024). This dataset covers 2655 factual questions curated from the Natural Questions dataset (Kwiatkowski et al., 2019; Lee et al., 2019) under CC-BY-SA-3.0 license. Each question is accompanied by k documents retrieved from Wikipedia, among which exactly one contains the answer to the question, namely the gold document. The remaining k - 1 documents are termed distractors, which are relevant to the topic of the question but do not contain any ground truth answers, retrieved using Contriever (Izacard et al., 2022). In our experiments, the total number of documents k is taken to be {10, 20, 30}.\nTo validate the generality of our findings, we also experiment on an open-ended non-factual QA dataset ELI5 (Fan et al., 2019) with BSD license. ELI5 consists of questions beginning with how, why or what curated from the Reddit forum \u201cExplain Like I'm Five\u201d, where the answers are expected to be more comprehensive and diverse. Each question is accompanied by k documents retrieved from Sphere (Piktus et al., 2021)\u2014a filtered version of Common Crawl, where k is taken to be {5, 10, 20} to avoid truncation due to the long questions and LMs responses for the long-form QA task. In contrast to NQ-Open, ELI5 does not provide the annotations of gold documents, which aligns with real-world RAG application scenarios, making it a more practical and challenging dataset (Nakano et al., 2021; Menick et al., 2022; Liu et al., 2023).\nWe remark that NQ-Open was specifically synthesized to examine how answer accuracy is affected by changing the position of relevant information. In real-world applications, the retrieved documents for one question may contain multiple gold documents or none. Nevertheless, it mimics the RAG setup underlying many commercial generative search and QA systems."}, {"title": "3.2 Answer Accuracy Metrics", "content": "NQ-Open and ELI5 apply different evaluation metrics to the LM responses. Examples are illustrated in Appendix A.\nOn NQ-Open, the ground truth answer for each question is either a word or a short phrase. The accuracy is 1 when the LM response contains the golden answer as a sub-string, otherwise the accuracy is 0. Following Liu et al. (2024), we compute the model's average accuracy over the\nhttps://www.reddit.com/r/explainlikeimfive/\nhttps://commoncrawl.org"}, {"title": "3.3 Language Model Settings", "content": "Most state-of-the-art closed LMs, e.g., OpenAI's ChatGPT, GPT-4 and Anthropic's Claude, do not provide direct access to the likelihood of either input or output tokens. Thus, we select the state-of-the-art open LMs for our experiments, including three families of open LMs, namely LLAMA-2, LLAMA-3 (Touvron et al., 2023), and Mistral-v0.3 (Jiang et al., 2023). Besides, we also evaluate MPT on NQ-Open. Following the settings of Liu et al. (2024), we adopt greedy decoding for all models when generating responses. The maximum number of decoded tokens is set to\nIn our preliminary experiments, MPT fails to generate adequately long responses on ELI5, resulting in incomparable performance to other LMs."}, {"title": "4 Question Likelihood Correlates with Model Performance", "content": "As introduced in Section 1, we look into question likelihood as the position of the gold document changes. Intuitively, it is reasonable to expect a strong correlation between question likelihood and answer likelihood. I.e., a high question likelihood often signifies the answer to it having a high likelihood as they tend to co-occur in the same sentence during training. Meanwhile, answer likelihood is expected to correlate with task performance. Thereby, question likelihood correlates with task performance."}, {"title": "4.1 Likelihood Measurement", "content": "In the prompt templates used in our experiments, question is located after the retrieved documents, i.e., the context, in the input prompt (see the ex-"}, {"title": "4.2 Corpus-Level Correlation", "content": "Following previous works (Gao et al., 2023; Liu et al., 2024), the answer accuracy metrics we use in our tasks (cf. Section 3.2) produce fixed discrete values from {0,1} or {0,0.33,0.67,1} on NQ-Open and ELI5, respectively. Thus, commonly-used metrics such as Pearson's and Spearman's correlation coefficients (Pearson, 1895; Spearman, 1904) between answer accuracy and question likelihood of these instances may not be adequately informative. Therefore, for each LM, we group the instances by the three tertiles (denoted by lowest, medium, highest) based on their log p(question) and compute the average answer accuracy for each group. Results in Figure 3 show that LMs tend to perform better on the prompts with a higher log p(question) compared to those with lower log p(question), demonstrating that the likelihood of question is indicative of answer accuracy at the corpus level."}, {"title": "4.3 Instance-Level Correlation", "content": "We further analyze the instance-level correlation between log p(question) and answer accuracy by varying context while keeping the question fixed."}, {"title": "4.3.1 Revisiting Positional Bias on NQ-Open", "content": "To begin with, we experiment on NQ-Open by varying the position of the gold document in the context. The set of retrieved documents and the order of non-gold documents remain the same. As the gold document moves across different positions within the context, both log p(question) and answer accuracy fluctuate. Using the same experimental setup, Liu et al. (2024) observed a drop in answer accuracy when the gold document is positioned within the middle of the context, an LM positional bias termed \u201clost in the middle\". We\nRecall that exactly one retrieved document is marked as the gold document for each question in NQ-Open.\""}, {"title": "5 Improving Accuracy via Document Reordering", "content": "In Section 4.3, we find that log p(question) strongly correlates with model performance at the instance level. I.e., given the same question, prompts with higher log p(question) are likely to lead to higher answer accuracy on QA tasks. Therefore, we propose two methods for improving LM's performance through document reordering-na\u00efve likelihood-based selection and gold document reordering. The experiment is conducted on the ELI5 dataset and a subset of 500 questions from NQ-Open, using Mistral-7B-Inst-v0.3, LLAMA-3.1-8B, and LLaMA-3.1-8B-Inst. Each question is associated with 10 and 20 retrieved documents, respectively, on ELI5 and NQ-Open."}, {"title": "5.1 Method 1: Na\u00efve Likelihood-Based Reordering", "content": "Section 4.3 has shown that for each instance, the prompt that achieves the highest question likelihood during document shuffling likely leads to superior answer accuracy on QA tasks. We select the prompt that achieves the highest question likelihood during document shuffling as the LM input for each question. We consider this the"}, {"title": "5.2 Method 2: Gold Document Reordering", "content": "As observed in Figure 4, answer accuracy is the highest when the gold document is placed at the beginning of the document sequence. This suggests that answer accuracy can be improved by moving the gold documents towards the beginning of the context. To this end, we propose to measure each document's relevance in answering the question through question likelihood. Using such means, we aim to detect the most relevant document and manually place it at the beginning of the document sequence to get an \"optimal prompt\".\nAlthough the performance also rises when the gold document is placed close to the end of context on LMs such as Llama-3-8B, it is nevertheless inferior to when the gold document is placed at the beginning. Besides, placing the gold document closed to the end does not necessarily lead to increased performance on many instruction-finetuned LMs (see Figure 4b). This finding is consistent with previous work (Liu et al., 2024; Chen et al., 2024)."}, {"title": "5.2.1 Gold vs. Non-Gold Document", "content": "We have found in Section 4.3 that question likelihoods tend to be higher when the gold document is placed at the beginning or the end of the document sequence. When the gold document is placed in the middle of the context with non-gold documents at the beginning or end, question likelihood drops."}, {"title": "5.2.2 Reordering via Relevance Identification", "content": "Based on the findings in Section 5.2.1, we propose ConvexScore to measure the relevance of each document with the question, defined as\n$ConvexScored = InitScored + Convexd$,\nwhere InitScored is the sum of question log-likelihoods when placing the document d at the beginning and ending position of the document sequence with the rest documents placed in random order. Convexd measures the curvature of the question likelihoods when the document d is placed at each position, defined as below\n$Convexd = \u2211(ld,i+1 (ld+1 - ld,i) - (ld,i - ld,i-1)$.\nThe document with the highest ConvexScore is moved to the beginning of the context to construct an optimized prompt for eliciting better answers."}, {"title": "5.3 Results and Analysis", "content": "Shown in Table 3, both na\u00efve likelihood-based selection and gold document reordering can boost answer accuracy. On NQ-Open, where only one document in the sequence is relevant to the question, gold document reordering significantly improves the answer accuracy and narrows the gap to the upper bound. Furthermore, on the more challenging and practical QA benchmark ELI5, we also observe a modest improvement in answer accuracy, indicating that improving question likelihoods via document reordering can effectively obtain better LM responses.\nRegarding efficiency, our proposed methods are mildly time-dependent thanks to the parallelizable"}, {"title": "6 Discussion", "content": ""}, {"title": "6.1 How does document order affect answer accuracy?", "content": "The objective of language modeling is to estimate a distribution over the sequence of tokens in a train-"}, {"title": "6.2 Instruction-Finetuned vs. Base Models.", "content": "In our analysis, we find base LMs, e.g., LLAMA-3-8B, tend to be more vulnerable to the changes in the position of the gold document. Their log p(question), log p(answer), and answer accuracy drop significantly when the gold document is placed in the middle of the document sequence. On the other hand, the performance of instruction-tuning models is more robust to the positional bias caused by the changes in gold document positions as shown in Figure 4. Specifically, the log p(question) still appears in a U-shaped curve, but the drop in answer accuracy and log p(answer) is less significant for the instruction-finetuned model when the gold document is positioned at the middle."}, {"title": "6.3 Context Likelihood Analysis", "content": "This work explores the correlation between question likelihood and answer accuracy when documents are in various orders. However, the likelihood for the document sequence, namely log p(context), may also vary during the shuffling. To alleviate the concern that context likelihoods may affect answer accuracy, we compute the log p(context) for questions in NQ-Open, when placing the gold document at different positions. To our observation, the average log p(context) is approximately 1/10 compared with average log p(question) per token, indicating that the context likelihood has a marginal effect on the likelihood at the input side. We investigate context likelihood in Appendix D for a comprehensive study."}, {"title": "7 Conclusion", "content": "In this work, we analyze the relationship between likelihood and language model's accuracy in solving tasks under the retrieval-augmented generation framework. Through experiments, we demonstrate that the question likelihood is affected by the order of documents in the input context. We reveal the correlation between question likelihood and answer accuracy at both the corpus level and instance level. Our findings show that it is possible to use likelihood to gauge language model performance and improve the quality of input prompts. We propose two practical methods for prompt optimization based on these findings. Experimental results show that both effectively and efficiently improve LM's accuracy on QA tasks, demonstrating that using logp(question) as a gauge for optimizing prompts is a promising direction. We leave other prompt modification choices, beyond document reordering, for future study."}, {"title": "Limitations", "content": "One major limitation of our work is that only open-source LMs are studied in this work since we need full access to the question likelihoods. One possible way to extend our findings to closed LMs is to encode the prompts with open-source LMs belong-ing to the same LM family. For example, it is pos-"}]}