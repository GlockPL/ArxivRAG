{"title": "Efficient PINNs: Multi-Head Unimodular Regularization of the Solutions Space", "authors": ["Pedro Taranc\u00f3n-\u00c1lvarez", "Pablo Tejerina-P\u00e9rez", "Raul Jimenez", "Pavlos Protopapas"], "abstract": "We present a machine learning framework to facilitate the solution of nonlinear multiscale differential equations and, especially, inverse problems using Physics-Informed Neural Networks (PINNs). This framework is based on what is called multihead (MH) training, which involves training the network to learn a general space of all solutions for a given set of equations with certain variability, rather than learning a specific solution of the system. This setup is used with a second novel technique that we call Unimodular Regularization (UR) of the latent space of solutions. We show that the multihead approach, combined with the regularization, significantly improves the efficiency of PINNs by facilitating the transfer learning process thereby enabling the finding of solutions for nonlinear, coupled, and multiscale differential equations.", "sections": [{"title": "1 Introduction", "content": "Non-linear differential equations describe complex systems in nature, such as fluid dynamics, climate modeling, and general relativity, to mention a few. While linear equations are widely applicable, non-linear equations often arise in systems where interactions or feedback loops create more intricate and unpredictable behaviors. Yet, in most cases, analytic solutions are either impossible or highly complex, necessitating the use of numerical methods to provide solutions [1]. With the advent of deep neu-ral networks, particularly Physics-Informed Neural Networks (PINNs), it has become increasingly feasible to solve complex differential equations more efficiently. Extensive literature now exists on how to use PINNs to solve a wide variety of nonlinear, multiscale differential equations including those in complex, real-world systems where traditional numerical methods may struggle. An added layer of complexity arises when the goal is to solve inverse problems, where the objective is to infer unknown parameters or functions within a system based on available data, such as initial or boundary conditions [2].\nHere, we present two efficient algorithms to use in PINNs that are aimed at significantly improving the solution of inverse problems for non-linear, coupled and multi-scale differential equations (DEs). The first technique is called multi-head training, where we train the network to learn the general space of all solutions (or latent space) to different variations of the same DEs, from which we can perform transfer learning to different regimes [3-8]. The second one is a novel techinique that we call unimodular regularization of the latent space of solutions. It is based on the computation of the metric tensor, a concept from differential geometry, induced on the latent-space-hypersurface. This second technique facilitates (or even unlocks) the transfer learning process to extreme regimes by ensuring a more controlled response of the latent space to changes in the DEs [9].\nThere are previous works where concepts of differential geometry have been used in the context of NNs. In [10] the authors explore and characterize the behavior of NNs during training through the theory of Riemannian geometry and curvature - they relate the output of the NN with the input transformations through the curvature equations. An analysis and comparison between the convergence properties of the metric of NNs (used for classification) and the metric under the Ricci-DeTurck flow is done in [11]. In [12, 13], the authors remove edges of low importance (prune) the computational graph using the definition of Ricci curvature. To the best of our knowledge, differential geometry techniques have not been used in order to increase the performance of PINNs.\nUsing PINNs to solve DEs offers several advantages over traditional methods. For instance, a PINN can be trained on a specific range of boundary conditions (BCs) or initial conditions (ICs). The specific solution(s) to a DE can only be determined if the corresponding ICs or BCs are specified as part of the problem. The existence and uniqueness theorem [14] ensures that there is a unique solution for a set of DEs once specific ICs or BCs are chosen. In traditional numerical methods, numerical integration is performed for each set of initial and boundary value conditions (IC-BV). If multiple IC-BVs are given, the numerical integration must be run separately for each one. However, for PINNs, we can train the NN on a specific set of DEs in a given range of IC-BVs referred to as a bundle, which must be sampled [15]. The training process may"}, {"title": "2 Methodology", "content": "Physics-Informed Neural Networks (PINNs), first introduced in the works of Dissanayake and Phan-Thien [23] and Lagaris et al. [24], have emerged as a powerful framework for solving partial and ordinary differential equations (PDEs/ODEs). Raissi et al. [21] popularized this approach by demonstrating its effectiveness on a variety of challenging physical problems, while other researchers have made significant advance-ments in applying neural networks to PDEs and ODEs, including notable examples such as [25-27]. In this paradigm, one typically trains a distinct neural network for each unknown function in the governing equations. The training process involves minimiz-ing a loss function that encodes the squared residuals of these differential equations, thereby embedding the physics directly into the learning objective.\nWhile traditional numerical methods often outperform PINNs in terms of com-putational efficiency and accuracy, PINNs offer unique advantages. One key benefit"}, {"title": "2.1 Single-head", "content": "The single-head approach involves using the latent space components Hi(x4) and passing them through a linear layer or another NN that produces the desired output dimension for the specific solution (in the case of a simple ODE, this would be a scalar output, i.e., the solution to the system). The structure responsible for processing Hi(x4) is called a head, which projects the latent space into the specific solution:\n$\\psi(x) = \\text{head}_W [H_i (x^4)]$\nwhere W represents the weights and biases of the head, as shown in Fig. 1\nIn this simple case with a single head, we essentially have the standard setup of a NN with one output as the solution to an ODE problem. The parameters of the head and the body are initialized randomly and optimized by minimizing the loss using the Adam optimizer. The loss function consists of the squared residuals of all DEs, averaged over the number of equations, sampled points along the independent variable, different ICs/BCs, parameters, and forcing functions. We can express the loss function for our PINN as follows:"}, {"title": "2.2 Multi-head", "content": "We are now ready to understand how multiple heads enable the NN to learn the latent space of solutions across a whole family of DEs.\nThe goal is to find a specific solution \u03c8NN (x) for each problem by projecting H into a reduced space, non-linearly combining its components through a head with parameters Wa, where the index a labels different variations of the DE family. Each head Wa correspond to a specific DE variation and produces a loss La as described in Eq. 3. We then define the total loss at each iteration as follows:\n$\\mathcal{L}_{tot} = \\sum_{\\alpha=1}^{\\#heads} \\mathcal{L}_\\alpha$\nand we perform gradient descent on the total loss. Thus, the MH PINN simultaneously solves all specified DE problems.\nThis approach uses one NN for each problem, with the body shared across all NNs. Thus, the body captures the global properties of the solutions, ideally representing the entire family of equations."}, {"title": "2.3 Training and transferring", "content": "The body H is trained using standard gradient descent techniques, with the loss func-tion defined in Eqs. (3) and (4). As shown in the previous section and in Fig. 2, each head specializes in a specific variation within the family of DEs. If we continue train-ing the body and heads with sufficiently diverse set DE elements, we expect the latent space H to converge to a stable set of function values as the loss flattens.\nOnce the latent space H stabilizes, we freeze the parameters of the body, so they no longer change, even as the model continues training. We denote the frozen body as Hf.\nThe pre-training of the latent space H can be applied to solve any new variations of the DE family by simply attaching a new head that takes Hf as input. This process is referred as transfer learning (TL). The only part that requires training is the set of parameters W for the new head, which is much smaller and computationally less expensive (see Fig. 3):\n$\\mathcal{N}_{new} = \\text{head}_W (H_f).$"}, {"title": "2.4 Unimodular regularization", "content": "In this subsection, we introduce Unimodular Regularization (UR), a novel technique developed as part of the MH setup. This method involves imposing specific constraints on the values of the latent space and its derivatives, enabling the MH framework to gen-eralize effectively across variations within the family of DEs. By regularizing the latent space in this manner, we enhance its adaptability and ensure robust transfer learning capabilities. The method's theoretical foundations and technical implementation are detailed below.\nAs we have previously discussed, the body NN in a MH setup is a critical compo-nent. Its primary function is to learn a generalized latent space of solutions, which serves as a functional basis for the problem. This latent space captures the underlying structure of the solution space of a family of DEs, and enables the model to repre-sent a wide range of behaviors. The head of the network then nonlinearly combines this latent space for a specific element of the DEs family to produce the correspond-ing solution. This separation between the generalized latent representation (body) and specific parametrization (head) ensures both flexibility and efficiency in solving the DEs. We denote the latent space functions as Hi, where i ranges from 1 to d, with"}, {"title": "3 Results", "content": "In this section, we apply the proposed methods to three different ODEs of increasing complexity. First, we address the flame equation, a first-order, non-linear ODE that presents a challenge due to its rapid growth for specific initial conditions. Next, we tackle the van der Pol oscillator, a second-order, non-linear ODE that is reformulated as a system of two first-order ODEs. Finally, we will apply the method to an inverse problem: recovering a free function and solving a system of coupled, first-order, non-linear ODEs with different BC for the unknown functions. This problem, motivated by holography, involves solving the Einstein Field Equations (EFE) with a scalar field.\nWe demonstrate that the proposed method successfully handles all three problems. Moreover, by imposing UR during the training of the body, the solutions obtained can be effectively transferred to more challenging regimes.\nThe structure of the following subsections will be consistent across the cases dis-cussed. First, we train the body of the MH setup using different heads, each projecting the latent space toward the desired solutions for specific values of a given parameter or IC/BC (specific realizations of an element within the same family of ODEs). This training process will be performed twice: once with UR and once without it. Notably, the solutions obtained during this initial training phase appear identical in both cases.\nNext, we will conduct TL by freezing the parameters of the body and training a new head for values of the parameter/IC/BC where the solutions are stiffer. In some cases, a proper solution through TL is only achievable when using a modified latent space geometry, which we refer to as the \"regularized metric\". To highlight this, we will compare the performance of the standard MH setup (no UR) with the modified \"UR MH\" setup. This comparison shows that, under conditions of extreme stiffness, the MH wit UR setup facilitates TL much more effectively or even exclusively when the standard MH approach fails to converge to a solution. By incorporating UR, we further enhance the stability and generalization of the MH approach.\nThe results shown in this section were obtained by training the models on a single NVIDIA A30 graphic card. The total process of training the body and then learning a new head to transfer learn took few days in the most computationally demanding case (EFE)."}, {"title": "3.1 Flame equation", "content": "The flame equation describes the growth of a flame ball with radius y as a function of the time t, starting from a fixed initial value d. The amount of consumed oxygen during combustion inside the ball scales with the volume ~ y\u00b3 and the flux of incoming oxygen scales with the surface area ~ y\u00b2. Different values of d imply different realizations of the ODE as an element within the family of flame equations. This equation and the proper IC can be written in the following form\n$\\frac{dy}{dt} = \\rho (y^2 - y^3); y(0) = \\delta : 0 \\leq t \\leq \\frac{2}{\\delta\\rho}$ \nwhere we have introduced an additional parameter p that will artificially increase the stiffness of the problem. In our case we are choosing p to be 300. As we have mentioned, the solution to this equation will present a rapid growth on a time scale 2/\u03b4\u03c1. This growth will be faster as we decrease \u03b4. Thus, the problem will be stiffer for smaller values of \u03b4.\nThe strategy for applying our method is as follows. We will first train two different bodies using four heads for \u03b4 \u2208 [0.02, 0.04], for 1.5 106 epochs. In one of them, we will impose UR as an additional loss. The results obtained for the solutions and the determinant of the metric are shown in figure 4.\nNext, we will freeze the body in both cases and perform TL by training a new head from scratch for a value of 8 = 0.015. The results of this TL are shown in figure 5, along with the relative error between the numerical and the NN solutions, computed as\n$RE[\\%] = \\frac{|y_{RK4}(t) \u2013 y_{NN} (t)|}{1+|y_{RK4}(t)|} .100$\nwhere we have added the +1 factor in the denominator to avoid dividing by zero.\nThe model architecture and training details are summarized here. The body is a Fully-Connected Neural Network (FCNN) with two input units, 64 output units, and three layers of 64 neurons each. Each head is an FCNN with 64 input units, one output, and two layers of 32 neurons each, using Tanh as the activation function. For MH body training, we used adam as the optimizer with an initial learning rate of 1\u00b710-3, reduced by 5 every 15,000 epochs using a step scheduler. TL followed the same parameters for both bodies but used a higher learning rate (5.10-3), reduced by 2.5% every 2,500 epochs.\nFor training, we sampled 100 points in the t -direction between the time bounds in (10), adding small random noise at each epoch to prevent overfitting. When training the MH body, we used d = 0.02 (the smallest value in the range) to ensure the widest time intervals. For UR, we set X = 5\u00b710-7 and imposed the additional loss term every 100 epochs."}, {"title": "3.2 Van der Pol oscillator", "content": "The van der Pol oscillator represents a specific instance of a non-linear, damped harmonic oscillator. It can be mathematically characterized by the subsequent"}, {"title": "3.3 Einstein Field Equations in AdS background", "content": "Before going into details of this case, it is important to note that the level of difficulty here is significantly higher than in the previous examples. The ODEs considered earlier served mainly as toy models to illustrate both the MH setup and UR technique, and their solutions and behaviors are well understood. However, the example presented in this section has only been solved for not very stiff regimes [22].\nMoreover, its resolution involves solving a system of seven ordinary highly non-linear ODEs for approximately 70 BCs, and recovering a free-form function appearing within the equations that is uniquely determined by these BCs. Consequently, this stands out as one of the most challenging setups for testing the algorithms introduced thus far.\nAs previously indicated, our system comprises seven first-order, non-linear ODEs (14a)-(14g), with six being linearly independent. At first glance, one can observe that the first three equations are simply redefinitions of the derivatives of the variables \u017d, \u00c3 y 6. This arises because the Einstein Field Equations (EFE) are second-order differential equations, which have been rewritten as first-order equations. For a detailed discussion on the physical motivation behind these redefinitions, the reader is referred to [22] and references therein.\n$\\Sigma \\Sigma' = 0,$\n$\\nu_A - \\tilde{A}' = 0,$\n$\\nu_\\phi - \\tilde{\\phi}' = 0,$\n$\\frac{2}{\\Sigma} + \\nu_A' = 0,$\n$8\\nu^2 \\Sigma\\nu_A + \\tilde{V}(\\phi) \\Sigma + \\nu_A (3 \\nu \\phi^2 \\nu_\\phi \u2013 5 \\phi \\tilde{A}^2) + \\tilde{A} (8 \\phi^2 \u2013 6 \\nu_\\phi) = 0,$\n$\\frac{dV}{d\\phi} + \\tilde{V}  (-3 u \\tilde{A} \\nu_\\phi + \\nu^2 \\Sigma\\nu_A + 3 \\nu^2 \\nu_\\phi \\tilde{A}) = 0,$\n$(\\nu_\\phi \u2013 \\Sigma)  (\\nu^2\\tilde{A}\\nu_A +2\\nu^2\\tilde{A} \\nu_\\phi \u2013 4u\\tilde{A}\\nu) \u2013 \\frac{\\nu^2}{2} (u^2\\tilde{A}\\nu \u2013 2V(\\phi)) = 0.$\nThe independent variable is u. The six functions (dependent variables) are \u2193(u) = (\u2211(u), \u0100(u), \u03c6(u), \u03bd\u03b5 (u), va (u), v(u)). Looking at Eqs. (14e)-(14g), we can see a free function V($) and its derivative. Evidently, for different V($), one would get different equations, and thus different solutions (u).\nThe BCs are given by:"}, {"title": "4 Conclusions", "content": "We have presented new techniques to solve nonlinear, multiscale differential equations (DEs) with Physics-Informed Neural Networks (PINNs), placing special emphasis on inverse problems. These techniques, which we term multi-head (MH) and Unimodular Regularization (UR), have proved effective in various examples of nonlinear, multiscale DEs. We have shown that, by using a single NN structure (the body) shared across"}]}