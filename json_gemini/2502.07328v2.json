{"title": "Music for All: Representational Bias and Cross-Cultural Adaptability of Music Generation Models", "authors": ["Atharva Mehta", "Shivam Chauhan", "Amirbek Djanibekov", "Atharva Kulkarni", "Gus Xia", "Monojit Choudhury"], "abstract": "The advent of Music-Language Models has greatly enhanced the automatic music generation capability of AI systems, but they are also limited in their coverage of the musical genres and cultures of the world. We present a study of the datasets and research papers for music generation and quantify the bias and under-representation of genres. We find that only 5.7% of the total hours of existing music datasets come from non-Western genres, which naturally leads to disparate performance of the models across genres. We then investigate the efficacy of Parameter-Efficient Fine-Tuning (PEFT) techniques in mitigating this bias. Our experiments with two popular models - MusicGen and Mustango, for two underrepresented non-Western music traditions - Hindustani Classical and Turkish Makam music, highlight the promises as well as the non-triviality of cross-genre adaptation of music through small datasets, implying the need for more equitable baseline music-language models that are designed for cross-cultural transfer learning. The code for the paper is available at our Github Repository and the model adapters are available at Huggingface.", "sections": [{"title": "Introduction", "content": "Music, as a powerful expression of cultural identity, is deeply embedded in traditions (Swain, 1995; Chung, 2006). Recent advancements in AI, powered by deep learning models (Schneider et al., 2024; Copet et al., 2023; Tal et al., 2024), have led to significant improvements in automatic music generation technologies. This progress has led to several music generation playgrounds such as Jukebox (Radford et al., 2020), Suno\u00b9, and Udio2 offering users the ability to generate music according to their specifications. However, these models often reflect biases, particularly towards Western musical traditions (Tao et al., 2024; Copet et al., 2023), in their training data.\nThis lack of diversity in datasets, as outlined by Copet et al. (2023); Melechovsky et al. (2024); Radford et al. (2020), is also evident in the disparate performance of the music generation models across genres. More specifically, the models tend to rely on Western tonal and rhythmic structures when generating music for non-Western genres, such as Indian or Middle Eastern music. The situation is comparable to the lack of cultural and linguistic diversity (Joshi et al., 2020; Bender and Friedman, 2018; Bender et al., 2021) in NLP research.\nIn order to quantify the severity of this problem in music generation research landscape, we conduct a comprehensive analysis of existing music datasets and music generation papers, which reveals a stark disparity in the representation of non-Western music. Particularly noteworthy is the scarcity of non-Western music data, with merely 5.7% of the total hours of the available datasets. This finding highlights the need for more diverse musical datasets and methods to adapt state-of-the-art models to low-resource genres.\nHowever, it remains unclear whether cross-genre music adaptation, similar to cross-lingual adaptation, can be effectively achieved using lightweight computational techniques such as parameter-efficient fine-tuning (PEFT) (Houlsby et al., 2019). In this paper, we explore this question by adapting two open-source models, MusicGen (Copet et al., 2023) and Mustango (Melechovsky et al., 2024) for two low-resource non-Western genres - Hindustani Classical 3 music of India and Makamat 4 music of the Middle East."}, {"title": "The Disparity in Music Generation Research", "content": "Al music generation has evolved rapidly with techniques such as autoregressive (Agostinelli et al., 2023; Copet et al., 2023; Ziv et al., 2024), diffusion-based (Schneider et al., 2024; Huang et al., 2023; Li et al., 2024) and GAN-based (Dong et al., 2018; Li and Sung, 2021) producing high-quality music. Some of the works include adapter-based settings which proved effective for music editing and inpainting (Lin et al., 2024; Zhang et al., 2024). Moreover, Lan et al. (2024) used adapters for rhythm and chord conditioning. Tan et al. (2020) showcased how visual emotions from images can be effectively translated into music using deep learning techniques.\nDrawing inspiration Joshi et al. (2020), which systematically analyzes the under-representation of languages spoken by the global majority, we conduct a survey of the datasets and research papers on music generation."}, {"title": "Data Collection", "content": "To get our initial pool of papers, we implemented an efficient, automated data collection method.\nWe employed a multi-stage, keyword-based selection method, leveraging the Scholarly package Cholewiak et al. (2021) to gather approximately 5000 papers. This included up to 1000 papers per query, using broad search terms such as \"music,\" \"music generation,\u201d \u201cnon-Western music,\" \"MIDI,\" and \"symbolic music.\" We then refined our selection by focusing on papers presented at 10 major conferences including IJCAI, AAAI, ICML, EURASIP, EUSIPCO, ISMIR, NeurIPS, SMC, NIME and ICASSP, chosen based on their popularity and prestige in the area of computational processing of music, narrowing our pool to around 800 papers. Conferences such as ISMIR and NIME specialize in music information retrieval and musical expression, frequently showcasing work related to generative AI. Additionally, conferences like ICASSP, AAAI, and NeurIPS are known for their focus on cutting-edge AI technologies, such as GANs and transformers, which are crucial for music generation.\""}, {"title": "Dataset Papers", "content": "To identify papers proposing datasets, we read through the title and abstract of each paper. This led to a set of 152 papers proposing new datasets with a total of 1 million+ hours of music. These datasets were manually annotated for the region and genres covered, total hours of music data, and whether the dataset is annotated with other details (such as, instruments, genre, and style). Papers that directly provided details of the distribution of data points across genres and regions, were analyzed with the already available statistics. Unfortunately, several datasets did not offer substantial details necessary for our study. If such a dataset had more than 10,000 hours of audio data, we analyzed each sound file's metadata to collect genre and region information. However, when the genre and region were not explicitly mentioned in either the paper or the metadata, we did not make any assumptions; thus, 7.9% of the datasets totaling 5,772 hours were excluded from our analysis."}, {"title": "Findings", "content": "Our findings are summarized in Figure 1. The results reveal an almost complete omission of musical genres from non-Western countries, especially those from the Global South. Approximately 94% of the total hours in available datasets are dedicated to music from the Western world, while only 5.7% are devoted to South Asian, Middle Eastern, Oceanian, Central Asian, Latin American, and African music combined. This imbalance is likely to cause poor-quality music generation for genres from the Global South. For detailed analysis, please refer to Appendix A."}, {"title": "Genre Adaptation: Data, Models and Experiments", "content": "For our genre-adaptation experiments, we selected two distinct non-Western genres Hindustani Classical (Jairazbhoy, 1971) and Turkish Makam (Signell, 2008) \u2013 both significantly underrepresented in music generation research and datasets, and two open source models \u2013 MusicGen (Copet et al., 2023) and Mustango (Melechovsky et al., 2024). We begin by describing the dataset creation, followed by prompt generation, the models, adapter architectures and finally, the training process."}, {"title": "Dataset Creation", "content": "Our study necessitated diverse corpus of non-Western music with detailed metadata. The Dunya (Porter et al., 2013) which is part of the CompMusic project (Serra, 2014), emerged as the ideal choice, offering an extensive collection of over 1,300 hours of music across multiple non-Western genres. This corpus includes Carnatic, Hindustani, Turkish Makam, Beijing opera, and Arab Andalusian music, providing a broad spectrum of cultural music. We focused specifically on Hindustani Classical and Turkish Makam genres as both genres possess complex culturally specific melodic and rhythmic structures different from Western music & we had easier access to listeners familiar with Indian and Turkish music. For Hindustani Classical, we chose the MTG Saraga (Srinivasamurthy et al., 2021) annotated dataset which is built on CompMusic offering 50 hours of audio. For Turkish Makam, we use the Dunya dataset API for accessing the metadata and audio samples leading to 405 hours of audio.\nTo ensure consistency and manage computational resources effectively, we implemented several pre-processing steps. We standardized the audio sample length by truncating longer recordings to 30 seconds. We utilized the accompanying metadata from the Dunya corpus without modification. These descriptions, rich in genre-specific details, served as valuable inputs for creating prompt templates. Finally, to accommodate the differing requirements of our chosen models, we performed audio resampling. Specifically, for MusicGen we resampled the audio to a 32 kHz sampling rate and for Mustango 16 kHz sampling rate.\nThe metadata from the dataset provides genre-specific information for each audio clip, including three key details critical to our study: melodic line, rhythmic pattern, and instrumentation. For the melodic line, we extracted the raga (a melodic framework in Hindustani Classical music) and Makam (a system of melodic modes in Turkish music). For rhythmic patterns, we identified laya (tempo) in Indian music and usul (a sequence of rhythmic strokes) in Turkish music. Additionally, we extracted the meta-data for the instruments (including voice) played in each audio sample. Details of the dataset can be found in Appendix F.\nAfter pre-processing, we collected a total of 23.24 hours of audio for Hindustani Classical music and 121.16 hours for Turkish Makam music."}, {"title": "Prompt Generation", "content": "To create effective prompts for model training, we created three distinct templates that describe each musical piece based on sample metadata from the selected genres.\nFor each audio sample, we randomly selected one of the three templates and populated it with relevant metadata attributes as shown in Table 1. This process ensures that each prompt captures the unique musical elements of the sample. By maintaining this metadata-specific structure across prompts, we help the model learn to identify and respond to key attributes within each genre, enabling it to generate more accurate and culturally informed outputs during training."}, {"title": "Models", "content": "We utilize two state-of-the-art models, Music-Gen (Copet et al., 2023) and Mustango (Melechovsky et al., 2024), to explore cross-genre adaptation. MusicGen is a transformer-based model,"}, {"title": "MusicGen", "content": "In MusicGen, we enhance the model with an additional 2 million parameters by integrating Bottleneck Residual Adapter after the transformer decoder within the MusicGen architecture after thorough experimentation with other placements. The total parameter count of MusicGen is 2 billion, making the adapter only 0.1% of the total size. The adapter, as shown in Figure 2, consists of a linear layer that compresses the embedding to a very small dimension, followed by a non-linear activation and projection back to the original size.\nMusicGen leverages the Encodec (D\u00e9fossez et al., 2023) framework, which compresses audio into latent representations. These latent representations are processed through a transformer model, which generates new music based on input prompts. By placing adapters at the end of the decoder, we achieve a lightweight adaptation mechanism that enhances the model's ability to generate music in specific styles or regions, such as Hindustani Classical music and Makam, without modifying the fundamental Encodec structure."}, {"title": "Mustango", "content": "In Mustango, we enhance the model with an additional 2 million parameters, which represents only 0.1% of the model's total parameter count, by integrating a Bottleneck Residual Adapter.\nWhile Mustango supports chord and beat embeddings, we opted not to use them here due to the distinct focus of Hindustani Classical and Turkish Makam on melodic lines rather than harmonic progressions. Unlike Western classical music, these genres feature complex, rhythms with accents often within a single beat, making fixed beat and chord embeddings difficult to apply.\nThe adaptation process in Mustango begins with the FLAN-T5 (Chung et al., 2024) model, which converts the input text into embeddings. These embeddings are then incorporated into the UNet architecture (Ronneberger et al., 2015) through a cross-attention mechanism, aligning the text and audio components. To refine this process, a Bottleneck Residual Adapter with convolution layers is incorporated into the up-sampling, middle, and down-sampling blocks of the UNet, positioned immediately after the cross-attention block at the end of each stage (Figure 2). The adapters reduce channel dimensions by a factor of 8, using a kernel size of 1 and GeLU activation after the down-projection layers to introduce non-linearity. Various adapter configurations and placements were explored to preserve the musical structure while adapting stylistic elements, with this setup yielding the best output quality. This design facilitates cultural adaptation while preserving computational efficiency."}, {"title": "Training settings", "content": "For MusicGen fine-tuning, we used two RTX A6000 GPUs over a period of around 10 hours. The adapter block was fine-tuned, using the AdamW (Loshchilov and Hutter, 2017) optimizer with a learning rate of 5e-5 and a weight decay of 0.05 using MSE based Reconstruction Loss. The training spanned 20 epochs, with a patience threshold of 5 epochs for early stopping based on validation loss. We utilized a batch size of 4 and applied gradient clipping with a maximum norm of 1.0. The training data was split into 90% for training and 10% for validation.\nFor Mustango model fine-tuning, we used one RTX A6000 GPU over a period of 12 hours. The adapter block was fine-tuned, using the AdamW optimizer with a learning rate of 4.5e-5 and a weight decay of 0.01 using MSE based Reconstruction Loss. The training spanned 25 epochs for both genres, with a patience threshold of 5 epochs for early stopping based on validation loss. The training data was split into 80% for training and 20% for validation."}, {"title": "Results", "content": "We evaluated four models, Mustango Baseline (MTB), Mustangto Fine-tuned (MTF), MusicGen Baseline (MGB), and MusicGen Finetuned (MGF), on two genres using both objective metrics and human evaluation, providing both objective and subjective insights into model performance."}, {"title": "Automatic Metrics", "content": "We sample 400 audio samples from the test set to form our test prompt corpus. For capturing the distance between generated audio and the test corpus we compute Fr\u00e9chet Audio Distance(FAD), Fr\u00e9chet Distance(FD) and Kullback-Leibler(KL) with Sigmoid activation. We utilize the AudioLDM (Liu et al., 2023) toolkit for implementation of FAD, FD, and KL, with distributions computed using PANN-CNN14 (Kong et al., 2020) as the backbone model for extracting features for each audio sample."}, {"title": "Human Evaluation", "content": "To complement our objective metrics, we designed a rigorous human evaluation process, recognizing the crucial role of human perception in assessing music quality and authenticity. We begin by generating prompts for drawing audio inferences from the models based on Bloom's taxonomy criteria. Then we present the outputs to human judges to compare them in an arena setup (Chiang et al., 2024; mrfakename et al., 2024).\nWe divided our process into two phases. In first phase, two annotators independently judged a portion of the same set of data points. This allowed us to compute inter-annotator agreement, a crucial measure of evaluation reliability. Disagreements were systematically discussed and resolved, refining our evaluation criteria. In phase two, annotators transitioned to single annotations per data point continuing evaluation of the rest audios. Finally, we compute ELO ratings of the models based on second phase annotations."}, {"title": "Material", "content": "We introduce novel evaluation criteria based on Bloom's Taxonomy to assess a model's understanding of musical elements in text and their alignment with the generated audio using arena-style evaluation.\nWe evaluate the models under three conditions: recall, analysis, and creativity, by manually generating 10 prompts in each category (Table 1).\n\u2022 Recall: Tests the model's ability to reproduce combinations of melody, instrument, and rhythm from the fine-tuning data, testing effective memorization and recall.\n\u2022 Analysis, We create novel combinations by substituting melodies, rhythms, or instruments, testing the model's adaptability beyond the training data.\n\u2022 Creativity, We combine genres, blending melodies, rhythms, and instruments across styles to test the model's integration of underrepresented and over-represented genres.\nFor each case, we generate model responses from all models, creating 120 total music samples. Since Mustango generates 10-second inferences at 16kHz, we process MusicGen outputs by clipping them to 10 seconds and downsampling to 16kHz to ensure uniform evaluation conditions."}, {"title": "Method", "content": "We decided to go for a comparative evaluation of pieces instead of absolute judgments of pieces in isolation to control the subjectivity so that, the shorter, lower-sampling-rate music clips, are more effectively evaluated through comparison. For each comparison, the user receives a reference prompt and two anonymous audio samples (with the comparisons ordered randomly), followed by five comparative evaluation questions comparing the two audio generations on each criterion: Overall Aesthetic(OA), Instrument Accuracy(Inst.), Rhythm Capture(RC), Melody Capture(MC), and Creativity(CR) since we provide these entities in the prompt and we are trying to assess the alignment of the text to the audio generated. For each criteria, we provide the annotator with 7 options: A \u00bb B, A > B, A = B, A < B, A \u226a B, None, and Not Applicable (NA). Please refer to Appendix B for questions.\nIn first phase, we conduct four types of comparisons: baseline vs. baseline, baseline vs. finetuned (for both models), and fine-tuned vs. finetuned. We request two avid listeners of each genre, who are aware of the nuances but not themselves professional musicians(demography details in Appendix I), to annotate these samples. The annotation process begins by evaluating 36 comparisons for each genre-9 generations from each model per genre-compared across all models based on the five evaluation criteria. After the completion of first phase we compute the Inter-Annotator Agreement(IAA), using distance and direction-based kappa scores. The distance-based Kappa quantifies the absolute differences in annotations by both the annotators whereas direction-based Kappa assesses consistency in preference order rather than the extent of preference. Detailed kappa-score calculation methods are provided in the Appendix E."}, {"title": "ELO Ratings", "content": "After comparing the model outputs, we compute ELO ratings (usually used to calculate the relative skill levels of players in a two-player game) for each model across all query types for each evaluation criterion. For each criterion, we consider a single annotation as a single match between the models. If the annotator marks it as NA, then we omit it from the calculation, if A=B or None is marked we consider it as a draw and A\u226bB, A> is considered a win for A and vice-versa for the remaining cases. The details of computing ELO ratings are given in Appendix G."}, {"title": "Conclusion", "content": "In this paper, for the first time, we systematically explored and established the skewed distribution of musical genres from around the world in datasets used for training Music-Language Models. Non-Western musical traditions are severely underrerp-resented which naturally leads to disparate performance across genres in these models. We also demonstrate that PEFT-based techniques vary in effectiveness across different genres and models, further aggravating the challenges of overcoming the data scarcity problem.\nAs generative models continue to gain traction in the field of music generation and are expected to be used even more widely in the coming years, the misrepresentation and under-representation of the musical genres of the \u201cglobal majority\" poses a significant threat to the inclusion of musical cultures from around the world. The skewed distribution in datasets, reflected in model outputs, can lead to several issues, including cultural homogenization, reinforcement of Western culture dominance (Crawford, 2016), misrepresentation of musical styles, and most importantly, gradual decline leading to the disappearance of many musical genres (Tan, 2021; Lund, 2019; Team, 2023). Therefore, it is critically important to prioritize the creation of inclusive music datasets and models, with an emphasis on under-represented musical genres."}, {"title": "Limitations", "content": "Our work relies on adapter-based techniques for cross-cultural adaptation but there is a need to explore additional architectural configurations to further optimize low-resource fine-tuning such as LORA (Hu et al., 2021) or Compacter (Davison, 2021) approaches.\nAdditionally, our approach only focused on a few genres, and future work should aim to incorporate a broader range of musical styles. Our investigation involves only Hindustani classical and Turkish Makam traditions, leaving other genres from the Dunya dataset unexplored. This narrow focus stems not from a lack of curiosity, but from our limited cultural expertise - a constraint we acknowledge upfront.\nWe also trained separate models for Hindustani Classical and Turkish Makam music; combining these into a single model could offer greater generalization across genres.\nAnother limitation lies in the evaluation process. Human evaluations were conducted on a limited number of samples with a duration of 10 seconds, and more genre-specific assessments are necessary. We also believe that computing objective metrics for underrepresented genres may obscure the full picture because the backbone models used to compute these metrics may not have been trained on various underrepresented genres, resulting in an erroneous portrayal of genres."}, {"title": "Annotation Details", "content": "We asked annotators to choose between two audio samples, based on their preference, to select which better represents the prompted culture in both the inter-annotator agreement scenario and human evaluation. For both inter-annotator agreement and human evaluation, we relied on the same set of questions outlined below.\n\u2022 Overall, which piece do you like more?\n\u2022 Which piece captures the instrument (if mentioned the prompt) better?\n\u2022 Which piece captures the melodic line/scale (if mentioned the prompt) better?\n\u2022 Which piece captures the rhythm/tempo (if mentioned the prompt) better?\n\u2022 Which piece is more creative (ignore audio quality while answering this question)?"}, {"title": "Evaluation of Inter Annotator Agreement Results", "content": "The inter-annotator agreement (IAA) results, measured using Cohen's Kappa, reveal interesting patterns across genres, metrics, and query types.\nIn Table 5 Turkish Makam consistently showed higher agreement (0.57-0.67) than Hindustani Classical (0.40-0.60), suggesting potentially clearer structural elements. This trend is particularly pronounced in Rhythm (RC) annotations, where Turkish Makam exhibits substantially higher agreement (0.58-0.76) compared to Hindustani Classical (0.19-0.21).\nInstrument identification (Inst.) showed high agreement across both genres (0.57-0.89), with Hindustani Classical scoring particularly well (0.89 for direction). Creativity (CR) exhibited the lowest overall agreement (0.02-0.55), reflecting the inherent subjectivity in assessing creativity.\nExamining query types in Table 6 reveals that Recall queries generally yielded higher agreement, particularly in Turkish Makam (0.74-0.75). This indicates strong consistency in factual recall tasks. Analysis queries showed mixed results, with some categories in Hindustani Classical even showing negative agreement, pointing to potential confusion or divergent interpretations in analytical tasks. Interestingly, Creativity queries showed perfect agreement (1.0) in Melody for both genres, suggesting a strong consensus in perceiving creative aspects of melody."}, {"title": "Evaluation of Human Evaluation Results", "content": "The human evaluation results in Table 7 and Table 8, measured using ELO ratings, also reveal intriguing patterns across genres, models, and query types. Comparing the two genres, we observe distinct performance profiles for each model. In Hindustani Classical Music, the Mustango finetuned model emerges as the clear leader (OA: 1577), outperforming other models across most categories, particularly excelling in Melodic Contour (MC: 1623). This suggests a strong grasp of the melodic structures specific to Hindustani music. Conversely, for Turkish Makam, the MusicGen finetuned model takes the lead (OA: 1597), with both MusicGen and Mustango baseline models also performing well.\nThe MusicGen Baseline shows remarkable consistency across both genres, often scoring above 1500 in various categories. This suggests a robust general understanding of musical elements that transcends genre boundaries. The Mustango Baseline, while competitive, generally scores lower than MusicGen Baseline, especially in Hindustani Classical Music.\nFinetuning yields mixed results across the two models. For Mustango, it significantly improves performance in Hindustani Classical but drastically reduces its effectiveness in Turkish Makam. Conversely, MusicGen's finetuning slightly lowers its performance in Hindustani Classical but enhances it in Turkish Makam. This divergence underscores the complexity of adapting models to specific musical traditions without losing generalizability.\nExamining performance across query types reveals further insights. In Recall queries for Hindustani Classical, Mustango finetuned significantly outperforms other models (OA: 1630), particularly in Melody (1668). For Turkish Makam, MusicGen Baseline leads in Recall queries (OA: 1512), with MusicGen finetuned close behind (OA: 1530). This suggests that finetuning can enhance a model's ability to accurately reproduce genre-specific musical elements.\nCreativity queries yield particularly interesting results, with baseline models outperforming their finetuned counterparts in both genres. In Hindustani Classical, Mustango Baseline leads (OA: 1553), while in Turkish Makam, it shares the top position with MusicGen Baseline (OA: 1508 and 1566 respectively). This suggests that finetuning, while beneficial for recall and analysis, might constrain the model's creative capabilities."}, {"title": "Kappa Score Computation", "content": "The distance-based Kappa quantifies the absolute differences in annotations by both the annotators. Each option(except None & NA) is assigned a value between 2 to -2 in order; A \u226b B, A > B, A = B, A < B, A \u226a B. After assigning values we calculate absolute distances between annotator preferences while excluding all cases which are annotated None or NA by annotators. The distance values are clipped to a maximum of 3, with agreement computed as follows :\np\n_0 = 1-d\n_i\n/3\n\n3"}, {"title": "Direction-based Computation Matrix", "content": "The direction-based Kappa assesses consistency in preference order rather than the extent of preference. A disagreement is defined as only when the preference orders are reversed between the two annotators (i.e., when one annotator chooses A<B or A\u226aB and the other annotator chooses B0 = 1-d"}, {"title": "ELO Ratings Computation", "content": "For phase I, the total number of evaluations are 36 by each annotator and we consider each annotation as a single match. In Phase II, 63 additional annotations are conducted making a total of 135 matches for computing the ELO ratings. For every match the new rating:\nR_i = R_i + K * (S_i \u2013 E_i)\nR_i: Player's current Elo rating.\nK: Weighting factor that determines how much a single game affects the rating.\nS_i: Outcome of the game for the player: 1 for a win, 0.5 for a draw, and 0 for a loss.\nE_i =\n1\n(1 + 10^(R_j-R_i)/400))\n, E_j = \n1\n(1 + 10^(R_i-R_j)/400))\nR_i: Player- 1 current Elo rating.\nR_j: Player- 2 current Elo rating.\nE_i: Expected Elo rating of Player-1.\nWe use a K value of 15 for calculations due to the limited number of matches; a higher K would disproportionately weight each match and skew the ELO ratings."}]}