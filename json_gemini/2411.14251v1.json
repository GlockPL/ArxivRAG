{"title": "NATURAL LANGUAGE REINFORCEMENT LEARNING", "authors": ["Xidong Feng", "Ziyu Wan", "Haotian Fu", "Bo Liu", "Mengyue Yang", "Girish A. Koushik", "Zhiyuan Hu", "Ying Wen", "Jun Wang"], "abstract": "Reinforcement Learning (RL) mathematically formulates decision-making with Markov Decision Process (MDP). With MDPs, researchers have achieved remarkable breakthroughs across various domains, including games, robotics, and language models. This paper seeks a new possibility, Natural Language Reinforcement Learning (NLRL), by extending traditional MDP to natural language-based representation space. Specifically, NLRL innovatively redefines RL principles, including task objectives, policy, value function, Bellman equation, and policy iteration, into their language counterparts. With recent advancements in large language models (LLMs), NLRL can be practically implemented to achieve RL-like policy and value improvement by either pure prompting or gradient-based training. Experiments over Maze, Breakthrough, and Tic-Tac-Toe games demonstrate the effectiveness, efficiency, and interpretability of the NLRL framework among diverse use cases. Our code will be released at https://github.com/waterhorse1/Natural-language-RL.", "sections": [{"title": "1 INTRODUCTION", "content": "Reinforcement Learning (RL) (Sutton & Barto, 2018) provides a rigorous framework \u2013 Markov Decision Process (MDP) for solving general decision-making problems. It transforms the policy learning problem into a mathematical optimization task. While RL has achieved breakthroughs across various domains, several challenges remain. For example, traditional RL algorithms generally lack task-specific prior knowledge, requiring extensive sampling to approximate environment dynamics. RL policy also lacks interpretability. Even in superhuman-performing models like AlphaZero (Silver et al., 2017), strategic reasoning remains elusive, even to professional players. RL training is also unstable (Zheng et al., 2023; Andrychowicz et al., 2020) due to its reliance on scalar rewards as the sole supervision signal. This one-dimensional feedback is particularly limiting in real-world scenarios where richer, multi-modal signals are naturally available, such as textual feedback (Bai et al., 2022; Madaan et al., 2024), visual demonstrations (Bousmalis et al., 2023; Xu et al., 2024), or other sensory inputs (Qiao et al., 2024).\nTo tackle these challenges, we seek a new RL paradigm shift, inspired by language-centric decision-making. Unlike traditional RL, which relies heavily on formalized mathematical modeling, humans can leverage natural language to interpret tasks, devise strategies, and communicate their reasoning. This language-driven approach enables rapid generalization using text-based prior knowledge, enhances interpretability through explicit reasoning, and provides access to rich, informative signals from linguistic data. Thus, natural language represents a largely untapped resource for improving the efficiency, stability, and interoperability of RL systems. The recent success of language-based transformers (Vaswani, 2017) further opens new avenues for integrating language into the RL framework. Large language models (LLMs) have demonstrated their unprecedented proficiency in generating, understanding, and processing complex language-based information.\nBuilding upon language-centric decision-making and advancement of LLMs, we introduce Natural Language Reinforcement Learning (NLRL), a novel RL paradigm that combines RL's mathematical rigor with the representational richness of natural language. In NLRL, core RL components such as task objectives, policies, value functions, and the Bellman equation\u2014are reinterpreted as language-based constructs. The medium of natural language largely facilitates the integration of prior knowledge stored in LLMs, and effectively translates decision-making processes into a form that is both intuitive and interpretable. NLRL also provides a systematic solution for leveraging rich textual feedback in sequential decision-making tasks, enabling stable training.\nBuilding on this new paradigm, we can efficiently implement RL algorithms in language representation space, resulting in various LLM applications. Leveraging the unsupervised nature of RL, all training and improvements can be achieved through environment interaction without the need for labeled data from humans or advanced models. In particular, Sec 4.2 illustrates how NLRL algorithms can enhance an LLM agent's critique and planning abilities using pure prompting. Sec 4.3 details the training of a natural language value function to serve as an evaluator or validator, providing reliable assessments for any given state. A comprehensive NLRL training pipeline, covering iterative training of both language policies and critics, is outlined in Sec 4.4. Similar to the traditional actor-critic framework (Barto et al., 1983; Sutton et al., 1999), this pipeline enables the LLM to learn purely from textual environmental feedback, generating language-based Chain-of-Thought (Wei et al., 2022b) enhanced policies and critiques. We empirically validate these use cases in environments such as the Maze game, Breakthrough board game, and Tic-Tac-Toe, demonstrating the effectiveness and superiority of the NLRL framework."}, {"title": "2 PRELIMINARY OF REINFORCEMENT LEARNING", "content": "Reinforcement Learning models the decision-making problem as a Markov Decision Process (MDP), defined by the state space S, action space A, probabilistic transition function $P : S\\times A\\times S \\rightarrow [0, 1]$, discount factor $\\gamma\\in [0,1)$ and reward function $r : S\\times A \\rightarrow [-R_{max}, R_{max}]$. The goal of RL aims to learn a policy $\\pi : S \\times A \\rightarrow [0,1]$, which measures the action a's probability given the state s: $\\pi(a|s) = Pr(A_t = a | S_t = s)$. In decision-making tasks, the optimal policy tends to maximize the expected discounted cumulative reward: $\\pi^*(a|s) = arg \\max \\mathbb{E}_{\\pi} [\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)]$. The state-action and state value functions are two key concepts that evaluate states or state-action pairs by measuring the cumulative re-"}, {"title": "3 NATURAL LANGUAGE REINFORCEMENT LEARNING", "content": "In contrast to the precise statistical models used in traditional RL, NLRL frames all elements-including task objectives, value evaluations, and strategic policies-within the form of natural language. This section aims to navigate decision-making tasks using natural language, aligning it with traditional RL concepts, definitions, and equations. Due to the inherent ambiguity of natural language, the equations presented here are not strictly derived from mathematical definitions. Instead, they are analogical and based on empirical insights into original RL concepts. We leave rigorous theoretical definition and analysis for future work."}, {"title": "3.1 ANALOGIES", "content": "We start with analogies in traditional RL to model NLRL. We provide Fig. 2 to illustrate most concepts discussed in this section."}, {"title": "Text-based MDP:", "content": "To conduct RL in natural language space, we convert traditional MDP to the text-based one, which leverages text descriptions to represent MDP's basic concepts, including state s, action a, and environment feedback (state transitions P and reward r)."}, {"title": "Language Task instruction:", "content": "For decision-making tasks, NLRL defines a natural language task instruction $T_L$, like \"reaching the goal\" or \"opening the door\". Then, we denote a metric by F that measures the completeness of the task instruction given the trajectory description $D_L(\\tau_{\\pi})$, where $D_L$ is a language descriptor that can transform the trajectory distribution $\\tau_{\\pi}$ into its corresponding language description $D_L(\\tau_{\\pi})$. The objective of NLRL is reformulated as\n$\\max_{\\pi} F(D_L(\\tau_{\\pi}), T_L)$\nThat is, NLRL is trying to optimize the policy so that the language description of the trajectory distribution $\\tau_{\\pi}$ can show high completeness of the task instruction."}, {"title": "Language Policy:", "content": "Instead of directly modeling action probability, NLRL determines the action with chain-of-thought process (Wei et al., 2022b), including strategic thoughts, logical reasoning, and planning. Thus, we represent the policy on language as $\\pi(a, c|s) = \\pi_L(C|S)\\pi(a|c, s)$, which will first generate such thought process $\\pi_L(C|s)$, then output the final action probability $\\pi(a|c, s)$."}, {"title": "Language Value Function:", "content": "Similar to the definition of Q and V in traditional RL, NLRL leverages language value function, relying on natural language evaluation to assess the policy effectiveness. The language state value function V and language state-action value function Q are defined as:\n$Q(s_t, a_t) = D ((s,a)_{t+1:\\infty} \\sim P_{\\pi} | s_t, a_t, T_L), V (s_t) = D (a_t, (s, a)_{t+1:\\infty} \\sim P_{\\pi} | s_t,T_L)$\nGiven the current state $s_t$ or state-action $(s_t, a_t)$, $Q^L$ and $V^L$ leverage language descriptions instead of scalar value to demonstrate the effectiveness of policy for achieving the task objective $T_L$. The language value functions are intuitively rich in the information of values and enhance interpretability rather than the traditional scalar-based value. It can represent the evaluation results from different perspectives, consisting of the underlying logic/thoughts, prediction/analysis of future outcomes, comparison among different actions, etc."}, {"title": "Language Bellman Equation:", "content": "In the traditional Bellman equation (Equ. 1), the state evaluation value $V_{\\pi}(s_t)$, can be decomposed into two parts: : (1) the intermediate transition, which include immediate $a_t$, reward $r_t$, and next state $s_{t+1}$. (2) the next state evaluation $V_{\\pi}(s_{t+1})$. Based on such decomposition intuition, we argue that the language value function $V^L$ should analogically satisfy the language Bellman equation Equ.6.\n$V(S_t) = \\mathbb{G}_{a_t,S_{t+1}\\sim P} (\\mathbb{G}_2 (d (a_t, r (S_t, a_t), S_{t+1}), V (S_{t+1}))), \\forall st \\in S,$\nwhere $d (a_t, r (s_t, a_t), S_{t+1}))$ depicts the language description of intermediate transition, while $\\mathbb{G}_1$ and $\\mathbb{G}_2$ serves as two information aggregation functions. By drawing an analogy to Equ. 1, $\\mathbb{G}_2$ mimics the add the summation operation '+' in the original Bellman equation, aggregating information from intermediate transition's description d and future evaluation $V^L(s_{t+1})$. Meanwhile, $\\mathbb{G}_1$ serves the role of the expectation operator E, aggregating information accross different $(a_t, S_{t+1})$ pairs by sampling from the transition distribution P."}, {"title": "3.2 LANGUAGE GENERALIZED POLICY ITERATION", "content": "In this part, we introduce how language GPI is conducted. Similar to traditional GPI, language GPI also consists of two procedures-language policy evaluation and language policy improvement."}, {"title": "3.2.1 LANGUAGE POLICY EVALUATION", "content": "Language policy evaluation aims to estimate language value function $V^L$ and $Q^L$ for each state. We present how two classical estimations: MC and TD estimate work in language policy evaluation."}, {"title": "Language Monte-Carlo Estimate.", "content": "Starting from the state $s_t$, MC estimate is conducted over text rollouts (i.e. K full trajectories {$a_t, (s, a)_{t+1:\\infty}$}) given the policy $\\pi$. Since we cannot take the average operation in language space, we instead leverage language aggregator $\\mathbb{G}_1$ to aggregate information over finite trajectories, approximating the expected evaluation:\n$V(s_t) \\approx \\mathbb{G}_1 (\\{a, (s, a)_{t+1:\\infty}^n\\}_{n=1}^{K})$"}, {"title": "Language Temporal-Difference Estimate.", "content": "Language TD estimate mainly relies on the one-step language Bellman equation illustrated in Equ. 6. Similar to the language MC estimate, we aggregate K one-step samples to approximate the expected evaluation:\n$V(s_t) \\approx \\mathbb{G}_1(\\mathbb{\\{G}_2(d(s_t, a, (s_t, a), r), (2+1))\\}_{1=1}^{K}), \\forall st \\in S,$\nwhere d, $\\mathbb{G}_1$ and $\\mathbb{G}_2$ has the same meaning for that in Equ.6.\nCheck Appendix D for more about how language MC/TD corresponds to the traditional MC/TD.\nLanguage MC estimate is free from estimation \"bias\" * as it directly utilizes samples from complete trajectories. However, the MC method is prone to high \u201cvariance\u201d considering the significant number of variations (Large K) in the long-term future steps. Such variability poses a challenge for the language aggregator $\\mathbb{G}_1$ in Equ. 7 to effectively extract crucial information from diverse trajectories. On the contrary, while the inaccuracy of the next state evaluation (st+1) can bring estimation \u201cbias\u201d to TD estimate, they effectively reduce \u201cvariance\u201d by discarding future variations. $\\mathbb{G}_1$ and $\\mathbb{G}_2$ are only required to conduct simple one-step information aggregation with limited variations."}, {"title": "3.2.2 LANGUAGE POLICY IMPROVEMENT", "content": "Similar to traditional policy improvement, the motivation of language policy improvement also aims to select actions that maximize the task completeness function F:\n$\\pi_{new}(s) = arg \\max_{a | \\pi(s)\\in\\mathbb{P}(A)} F(Q_{\\pi_{old}}^L (s, a), T_L), \\forall s \\in S$\nThe task completeness F is difficult to quantify for general language-based tasks, as it largely relies on human textual prior knowledge. Given this complexity, instead of mathematically optimizing F, we also refer to the chain-of-thought process-NLRL leverages a language analysis process I that can generate the thought process c to guide policy optimization and action selection.\n$\\pi_{new}(\\cdot | s), c = I(Q_{\\pi_{old}}^L(s, a), T_L), \\pi(\\cdot | s) \\in \\mathbb{P}(A), \\forall s \\in S.$\nLanguage policy improvement conducts strategic analysis c to determine the most promising action for task completion as the new policy $\\pi_{new}(\\cdot | s)$. Ideally, this analysis is mainly based on the correlation judgment between the language evaluation $Q_{\\pi_{old}}^L(s, a)$ and task objective TL."}, {"title": "4 PRACTICAL NLRL IMPLEMENTATION WITH LLMS", "content": "Section 3 outlines the core philosophy of NLRL, which involves translating key concepts from RL into their human natural language counterparts. To practically implement these key concepts, a model capable of understanding, processing, and generating language is essential. Large language models (LLMs), trained on vast corpora of human language and knowledge, emerge as a natural choice to emulate human behaviour and implement language-based RL components. In the following sections, we demonstrate how LLMs can function in various roles within the NLRL framework."}, {"title": "4.1 COMPONENTS AND PROCEDURES", "content": "\u2460LLMs as language policy ($\\pi$). Many works adopted LLMs as the decision-making agent (Wang et al., 2023a; Feng et al., 2023a; Christianos et al., 2023; Yao et al., 2022) with Chain-of-thought process (Wei et al., 2022b). By setting proper instructions, LLMs can leverage natural language to describe their underlying thought for determining the action, akin to human strategic thinking.\n\u2461LLMs as language value function approximator ($Q^L, V^L$). The original idea of value function approximation (Sutton et al., 1999) is to use a parameterized one-dimensional output function to replace the big value table and serve as a value function approximator. Such design largely helps RL to handle high-dimensional and large-scale decision-making problems. Similarly, in NLRL, we can naturally leverage (multi-modal) LLMs, to evaluate the state s or state-action pair (s, a), and serve as the language value function approximator for $Q^L, V^L$. This exactly corresponds to what (multi-modal) LLMs are capable of - they are designed to take in the features from the task state, such as low-dimension statistics, text, or images, and output the corresponding language understanding. By further prompting or fine-tuning over evaluation dataset, LLMs can generate language assessment.\n\u2462LLMs as language Monte-Carlo and TD operator ($\\mathbb{G}_1, \\mathbb{G}_2$). As mentioned in Sec 3.2.1, we can estimate the language value function by language MC or TD estimate. The key issue is implementing language aggregator $\\mathbb{G}_1, \\mathbb{G}_2$. To achieve this, we can leverage the inherent strengths of LLMs in aggregating and synthesizing diverse linguistic inputs. Specifically, LLMs can be powerful information summarizers (Zhang et al., 2023), extractors (Xu et al., 2023), and aggregators to help us fuse intermediate transitions and future language evaluations. LLMs can serve as $\\mathbb{G}_1, \\mathbb{G}_2$ by prompting them to summarize and aggregate multiple rollout trajectories (language MC), or multiple few-step transitions and future states' evaluation (language TD).\nDistilling language value estimation into language value function approximator. \u2462 provides an unsupervised and scalable way to generate language evaluation data through environment interaction, which can be leveraged to train our language value function approximator in \u2461. This corresponds to traditional critic training for value-based RL (e.g, DQN (Mnih et al., 2015)) or actor-critic algorithms (e.g, PPO (Schulman et al., 2017)), but happens in natural language representation space. The only difference is the training objective \u2013 the scalar-based value function regresses the value mean with L2 mean squared error while the language value function tries to imitate the language aggregation result with supervised-finetuning's cross-entropy loss.\n\u2464LLMs as policy improvement operator (I). With the chain-of-thought process and prior knowledge about the world, LLMs are better to determine the most promising action $\\pi_{new}(\\cdot | s)$ by taking language analysis c over the correlation of language evaluation $Q_{\\pi_{old}}^L (s, a)$ and task objective TL. The underlying idea also aligns with some recent works (Kwon et al., 2023a; Rocamonde et al., 2023) that leverage LLMs or Vision-language models as the reward-they can accurately model the correlation. Specifically, for a given state s, we prompt the LLM with several action candidates and their corresponding language evaluations $Q^L$, by which we can obtain the improved action with a chain-of-thought process analyzing different actions' evaluation results.\nLanguage policy training by distilling improved policy. Similar to \u2463, we can train our language policy LLM in \u2460 by supervised-finetuning over the improved chain-of-thought based language policy data from \u2464. This corresponds to the traditional policy training in policy-based RL (e.g, REINFORCE (Williams, 1992)) or actor-critic algorithms (e.g, PPO (Schulman et al., 2017)). However, NLRL relies on the more stable supervised loss instead of the policy gradient loss.\nWe also provide example visualizations in Fig 1 to illustrate these components and procedures. Building upon these components and procedures, we can create various NLRL applications. Here we illustrate three use cases, though many more possibilities exist beyond these examples."}, {"title": "4.2 LANGUAGE GPI BOOSTS LLM'S CRITIC AND POLICY BY PROMPTING (1, 2, 3, 5)", "content": "Our first case utilizes language GPI to enhance LLM's critic and policy solely through prompting, which can be particularly beneficial for improving proprietary models such as GPT-4 (OpenAI, 2023) or Gemini (Team et al., 2023). Specifically, we first combine \u2460, \u2461, \u2462 to build a language policy evaluation pipeline. Take language TD shown in Equ. 8 as an example. We prompt LLMs to (1) evaluate the subsequent state's value $V^L (s_{t+1})$ (\u2461) (2) serve as the TD operator $\\mathbb{G}_1, \\mathbb{G}_2$ (3). By performing a one-step look-ahead with $a_t \\sim \\pi$ (1) and leveraging $\\mathbb{G}_1, \\mathbb{G}_2$ to aggregate information"}, {"title": "4.3 TRAINING NATURAL LANGUAGE VALUE FUNCTION FOR A GIVEN POLICY (2, 3, 4", "content": "Our second case aims to train an LLM critic capable of evaluating any given state with natural language explanations, similar to a chess annotator who provides insightful commentary on boards, moves, and strategies. For example, we can build an iterative language TD pipeline by combining \u2461, \u2462, and 4. First, we leverage a tunable LLM A and prompt it to become a language value function approximator 2. Combined with the look-ahead transitions $(a_t, S_{t+1}) \\sim P$ by taking rollouts with policy $\\pi$, and the subsequent state evaluation $V^L (s_{t+1})$ generated by A, we prompt LLM B for the language TD estimate (3), similar to Sec 4.2. The model A is further finetuned by such language TD estimates (\u2463) and will be plugged back for $V^L (s_{t+1})_{new}$ in a new iteration. Iteratively, we can obtain the final converged natural language value function. Check Algorithm 2 for more details."}, {"title": "4.4 NATURAL LANGUAGE ACTOR-CRITIC LEARNING (1, 2, 3, 4, 5, 6)", "content": "Last, we combine all these procedures and build the full natural language actor-critic pipeline. Similar to traditional actor-critic (Barto et al., 1983; Sutton et al., 1999), our natural language actor-critic framework simultaneously learns both a language policy and a language critic through unsupervised environment interactions. For each iteration, we first use language policy to take rollouts in the environment. With new trajectories, we update our language value model \u2461 by language MC or TD \u2462 and train it with supervised-finetuning loss (\u2463). For language policy improvement, we query our updated language value to evaluate action candidates for states extracted from the rollout trajectories. Further LLM-based improvement operator \u2464 brings us a stronger policy, which will be used to train our language policy (\u2465). Check Algorithm 3 for more details."}, {"title": "5 EXPERIMENTS", "content": ""}, {"title": "5.1 LANGUAGE GPI BY PROMPTING (SEC 4.2)", "content": "Our first experiments explore Sec 4.2 and leverage language GPI to improve LLM capability with pure prompting. Specifically, we choose the maze games of LMRL (Abdulhai et al., 2023), aiming to validate that Language TD Estimate and Language Policy Improvement can benefit the evaluation and further improve policy. We use the original settings of LMRL Gym Maze, where the agent is required to navigate to a goal position in a \u201cDouble T\u201d or \u201cMedium\u201d maze. We consider the fully-observable setting, where the agent's observation (described by text) includes the agent's current position in the maze, the agent's action history, the walls' position around the agent (if any), and the goal position. The action space is discrete, including moving up / down / right / left. We evaluate the performance on 30 different initial positions, each with 3 random seeds.\nFor language TD estimate in Equ. 8, we prompt gpt-40-mini-2024-07-18 as the language aggregator $\\mathbb{G}_1$, language state-action value aggregator $\\mathbb{G}_2$, and language state value function $V^L$ respectively. Specifically, given an environment state, for each candidate action, we use a fully random policy to rollout N steps into the future and use the language state value function $V^L$ to evaluate the look-ahead state. For each state we repeat this process for K times and use $\\mathbb{G}_2$ to aggregate into a state-action value estimation. For language policy improvement in Equ. 10, the same GPI-40-mini is leveraged as the improvement operator I.\nWe compare Language GPI with a few different baselines, including prompt-based language policy $\\pi^L$ (\u2460), as well as prompt-based language value function $Q^L(s, a)$ + language policy improvement I."}, {"title": "5.2 TRAINING NATURAL LANGUAGE VALUE FUNCTION WITH LANGUAGE TD (SEC 4.3)", "content": "Our second experiment is an implementation of Sec 4.3. Specifically, we aim to train a language value function with language TD in the 5x5 breakthrough board game (Fig 4). Breakthrough is a turn-based board game for two players, white and black. Each player has a few lines of pawns. Black moves first and in each turn, the player chooses one piece to move one square forward (straight or diagonal) onto empty squares or capture diagonally. The goal is to break through the opponent's line and reach their home row.\nAs mentioned in Sec 4.3, this value function can serve as a reliable board evaluator/annotator. However, evaluating board states in this game is challenging for LLMs. First, the 5x5 breakthrough variant is niche, leading to sparse pre-training data on board evaluations. Most LLMs have minimal knowledge of it. Second, despite the small board size, its state space complexity can reach $10^8$ (Saffidine et al., 2012), making natural language assessments significantly more difficult."}, {"title": "5.2.1 ENVIRONMENT, DATASET, MODEL, AND EVALUATION", "content": "Text-Breakthrough. As mentioned in the text-based MDP, we textualize the game, including board representation, pieces, moves, positions, and information about capturing or termination.\nPolicy $\\pi$in $V^L(s)$. Our first task is to determine the $\\pi$in $V^L(s)$ since $V^L(s)$ measures the value for a specific policy, according to Equ 5. Given that our ultimate goal is to train a reliable board annotator, it makes sense to select a strong or near-optimal policy-analogous to the way we place greater trust in a grandmaster's annotations when evaluating a position. In our experiment, our $\\pi$ is the Monte-Carlo Tree-Search (MCTS) algorithm (Kocsis & Szepesv\u00e1ri, 2006) from OpenSpiel (Lanctot et al., 2019), with a high number of simulations and rollouts to ensure an extensive search.\nBuilding State Dataset $s \\sim P_{\\pi}(s)$. Since $V^L(s)$ is over state s, our second step is to build a state dataset D-the distribution $P_{\\pi}(s)$ that our $V^L$ works on. To ensure that $V^L$ can assess positions across different levels of gameplay, we build a mixed state dataset. Specifically, we maintain a pool of MCTs policies, where each policy variant is characterized by a distinct number of simulations and rollouts. The mixed state dataset is then created by pairwise combining policies from this pool, running rollouts for each policy pair, and merging the resulting data from all combinations. Then we can easily split Ds to build training state set $D_{train}$ and test state set $D_{test}$.\nBuilding TD training dataset. The TD training dataset is built by conducting look-ahead expansion with rollout policy $\\pi$, from state training set $D_{train}$. For each state $s_t$, we conduct a few times l-step look-ahead rollout and deduplicate to K distinct variations: {{(s + i, at + i, r + i, St + i + 1 ) = 0} K\nModels. We use two LLMs for the full language-TD pipelines. For the language TD operator ($\\mathbb{G}_1, \\mathbb{G}_2$), we prompt the large LLaMA-3.1-70B-Instruct (Dubey et al., 2024) model since it preserves stronger aggregation and reasoning ability. Because of the limitation of computing resources, the language value function $V^L (s)$ is trained from the small LLaMA-3.1-8B-Instruct model."}, {"title": "5.3 NATURAL LANGUAGE ACTOR-CRITIC LEARNING (SEC 4.4)", "content": "For validating Sec 4.4, we implement the natural language actor-critic pipeline in a model-free setting (Sutton & Barto, 2018) in the Tic-Tac-Toe game (Ju, 2024), where the system learns purely from sampled trajectories without access to environment dynamics. As a supplement to language TD in Sec. 5.1 and 5.2, we utilize the language MC (Equ. 7) for the language policy evaluation."}, {"title": "5.3.1 STABLIZING LANGUAGE VALUE FUNCTION", "content": "Despite the soundness of Sec 4.4, we identify training instability resulting from the language value function, mainly from two perspectives. First, we observed that the language value function $V^L$ can"}, {"title": "5.3.2 EXPERIMENT SETUP AND METHODS", "content": "Text-TicTacToe. Similar to text-breakthrough, we textualize all information in the Tictactoe game.\nModel Architecture. Our implementation uses three language models: one LLaMA-3.1-70B-Instruct and two LLaMA-3.1-8B-Instruct models. The 70B model implements the language aggregator ($\\mathbb{G}_1$) and policy improvement operator I - notably, we restrict this model to use only its general language processing and information aggregation capabilities, rather than any game-specific knowledge it may have learned during pre-training. This restriction is enforced through carefully designed prompts (see Appendix C.3 for details). The two 8B models implement our trainable components: one serves as the language policy $\\pi_L$, generating actions through chain-of-thought reasoning, while the other implements the language value function $Q^L$, evaluating state-action pairs.\nDataset Construction. For each iteration, we collect 512 complete game trajectories with $\\pi_t$. For Monte Carlo value estimation, we collect KMC 5 complete trajectories for each state-action pair. For policy improvement, $\\pi_t$ proposes Nsample = 10 candidate actions per state and we select the top-m (m = 10) actions from the candidate action list.\nEvaluation Protocol. To thoroughly evaluate our approach, we test against two different types of opponents. The first is a deterministic opponent that always takes the first available action, while the second is a stochastic opponent that plays randomly, presenting a more challenging case for policy evaluation. We measure performance through metrics including win/loss/tie rates.\nBaselines. We compare our algorithm with prompting-based method using GPT-40-08-06, LLaMA-3.1-8B/70B-Instruct with step-by-step reasoning. We also include a traditional RL baseline by fine-tuning LLaMA-3.1-8B-Instruct with standard PPO (Schulman et al., 2017)."}, {"title": "5.3.3 EXPERIMENTAL RESULTS", "content": "Performance against baselines with different opponents The learning curves in Fig. 6 demonstrate our system's superiority against all other baselines in different settings. Our algorithm out-"}, {"title": "6 RELATED WORK", "content": "Language Model Based Autonomous Agent. Inspired by the strong emergent capabilities of LLMs in complex reasoning and planning scenarios (Brown et al., 2020; Wei et al., 2022a; Anil et al., 2023; OpenAI, 2023; Feng et al., 2023b). The field of language model-based autonomous agents (Feng et al., 2023a; Christianos et al., 2023; Zhang et al., 2024) has witnessed a growing trend of leveraging LLMs for high-level planning purposes. For text agents, ReAct(Yao et al., 2022) leverages chain-of-thought planning about the intermediate goals and task-specific generation via few-shot prompting. Reflexion (Shinn et al., 2023) is built upon ReAct (Yao et al., 2022) with self-reflection, named verbal reinforcement learning, to generate tips given online feedback. Such tips strengthen agent's capability through in-context learning. (Zhong et al., 2024) proposes to distill such reflection ability from the large model to train a small policy feedback model that can guide the policy. Their underlying ideas share similarities with NLRL's language Monte-Carlo estimate, while NLRL formulates this process more formally and incorporates it into training-we extract core information/concepts by sampling multiple trajectories and leverage them to train our natural language value function. Another line of work like Zhang et al. (2023); Xu et al. (2023) leverages large language models to summarize and extract information to help fuse intermediate changes. They are related to the \"aggregation\" idea of language value function training.\nInterpretable Reinforcement Learning. One of the major purposes of interpretable RL is to automatically seek explanations from non-AI experts. For instance, methods of concept-based explanations for sequential decision-making tasks. Ji et al. (2023) provide a concept-based explanation for 3D action recognition CovNets by clustering learned human interpretable features. Sreedharan et al. (2020) formulates concept-based explanations upon state preconditions and action costs, representing any factual statement a user associates with a given state. Similarly, Hayes & Shah (2017) uses logical formulas to summarize policies. Additionally, Das et al. (2023) trains a joint embedding model for state-action pairs and concept-based explanations."}, {"title": "Learning from Language Feedback.", "content": "Our work is also related to research on learning from language feedback. While Cheng et al. (2023) focuses on benchmarking algorithms, we aim to propose a new algorithmic framework. Studies such as (Yang et al., 2023; Yuksekgonul et al., 2024; Cheng et al., 2024) introduce an LLM-based optimization paradigm that leverages natural language to represent optimization operators like gradients and backpropagation, achieving end-to-end generative optimization. NLRL, on the other hand, represents a parallel approach, offering a generative framework specifically designed for RL optimization problems.\nLLMs as evaluation function. Our language value function aligns with recent efforts in NLP"}]}