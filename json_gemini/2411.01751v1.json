{"title": "RAGVIZ: Diagnose and Visualize Retrieval-Augmented Generation", "authors": ["Tevin Wang", "Jingyuan He", "Chenyan Xiong"], "abstract": "Retrieval-augmented generation (RAG) combines knowledge from domain-specific sources into large language models to ground answer generation. Current RAG systems lack customizable visibility on the context documents and the model's attentiveness towards such documents. We propose RAGViz, a RAG diagnosis tool that visualizes the attentiveness of the generated tokens in retrieved documents. With a built-in user interface, retrieval index, and Large Language Model (LLM) backbone, RAGViz provides two main functionalities: (1) token and document-level attention visualization, and (2) generation comparison upon context document addition and removal. As an open-source toolkit, RAGViz can be easily hosted with a custom embedding model and HuggingFace-supported LLM backbone. Using a hybrid ANN (Approximate Nearest Neighbor) index, memory-efficient LLM inference tool, and custom context snippet method, RAGViz operates efficiently with a median query time of about 5 seconds on a moderate GPU node.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs), such as GPT-4 (ope, 2024), have revolutionized the field of artificial intelligence with their impressive language understanding and generation capabilities developed through extensive pretraining on large-scale textual data.\n\nA key limitation of using pretrained LLMs for zero-shot answer generation is their lack of access to domain-specific knowledge, as these models rely solely on parametric memory. The fixed knowledge derived from parametric memory often leads to hallucinations. To address this issue, Lewis et al. (2020) introduces retrieval-augmented generation (RAG), a technique that leverages retrieval mechanisms to incorporate non-parametric memory, typically derived from documents retrieved from domain-specific data stores.\n\nVarious systems have been developed to deliver RAG services. For instance, OpenAI Assistants (OpenAI, 2024) and Pinecone Assistant (Cordeiro et al., 2024) are \"chat-with-your-files\" products that use retrieved documents as context for a chatbot. While these RAG systems offer state-of-the-art performance in grounded answer generation, they lack explainability regarding the efficacy of the context documents they use to produce those answers.\n\nSome existing tools have been developed to improve language model explainability, such as BertViz (Vig, 2019), an open-source Python tool that provides attention visualizations for transformer models. Although such tools effectively analyze input token importance, they lack a customizable approach for analyzing the interaction between retrieved context documents and language generation.\n\nIn this paper, we propose RAGViz, a diagnostic tool designed to analyze LLM attention mechanisms on the retrieved documents that provide context to ground LLM answer generation. RAGViz's novelty lies in its focus on the interaction between the retrieval pipeline and the language model. RAGViz offers attention visualizations based on different levels of scoring: both cumulative attention scores on documents and individual token attention scores selected by the user. Along with document toggling, RAGViz enables users to qualitatively assess the effectiveness of retrieved documents and determine whether they contribute to hallucinations.\n\nRAGViz's system primarily relies on CPU nodes, with the exception of a GPU node that hosts the LLM inference server. The system entry point is a web node that hosts the frontend as static content"}, {"title": "RAGViz Features and Use Cases", "content": "This section first examines the innovative features of RAGViz and outlines its key benefits. Then, a few potential use cases are explored to demonstrate how RAGViz can be valuable to researchers and domain experts."}, {"title": "Features", "content": "RAGViz's system includes a few key features. One is the attention visualization on retrieved documents. RAGViz uses token highlighting to visualize the attentiveness of any generated token sequence to input tokens, as shown in Figure 1b. The level of attentiveness is measured by the attention score across all layers of the LLM and visualized by color magnitude. A cumulative document-level attention score is displayed to showcase the attentiveness of the generation output to each retrieved passage.\n\nRAGViz also offers a drag-to-select user interface. By simply dragging and selecting, users can easily inspect the cumulative attention of any token sequence, as demonstrated in Figure 1a.\n\nIn addition to attention visualization, RAGViz provides document toggling functionality. By toggling, users can select tokens and documents to omit when constructing the answer generation context. The newly generated answer will be shown side-by-side with the original answer to provide a comparative analysis of how adding or removing tokens and documents affects the LLM output. An"}, {"title": "Benefits", "content": "Through the features described, RAGViz provides several key advantages.\n\nFirstly, RAGViz enables precise document efficacy diagnosis through attention-based visualizations. By examining how LLMs allocate attention across different retrieved context documents during generation, users can assess the quality and relevance of the retrieval process. This helps identify which document contributes meaningfully to the generated output and which may lead to irrelevant or hallucinated information.\n\nSecondly, the system's multi-level attention visualizations offers flexibility for users to inspect attentiveness at various levels of granularity. With its intuitive drag-to-select interface, users can analyze attention not only at the token level but also at the phrase or sentence level. This allows for a deeper exploration of how specific sections of the text influence the model's output.\n\nAnother significant advantage of RAGViz is its ability to support iterative experimentation with document context. Through its document toggling functionality, users can modify the input context by adding or removing specific documents, and then compare the resulting generation side-by-side. This iterative approach helps in understanding how changes to the context impact the final output, using attention scores as a heuristic for evaluation.\n\nIn addition, RAGViz simplifies comparative analysis by displaying original and modified outputs alongside their corresponding attention scores. This side-by-side visualization allows users to observe how variations in input documents affect the generation, providing valuable insights into the interaction between retrieval and generation.\n\nRAGViz enhances retrieval precision testing by allowing users to adjust the number of documents retrieved for a query. This feature enables diagnostic testing to determine whether fewer or more documents are necessary for the model to generate accurate and well-grounded responses.\n\nRAGViz is also private and secure. Its basic API key authentication functionality restricts access and ensures that datasets and models are protected."}, {"title": "Example Use Cases", "content": "RAGViz presents several use cases for researchers and developers working with RAG pipelines. We highlight a few of these use cases.\n\nOne use case is to analyze the interpretability of attention mechanisms within large language models. A key need in RAG systems is to understand how context is leveraged to produce grounded results. RAGViz provides a novel tool that enables researchers to explore the distribution of attention across different parts of the retrieved snippets, offering insight into how context documents influence the generation process.\n\nAnother application is to design and evaluate new retrieval mechanisms tailored to RAG. The ability to visualize attention on documents in RAGViz provides researchers with a powerful method to iterate and refine the retrieval process, facilitating the development of more effective retrieval strategies to better support LLMs in grounding their outputs.\n\nRAGViz serves as a valuable tool for debugging RAG pipelines, particularly in diagnosing the sources of hallucinations. RAGViz can help differentiate between hallucinations caused by the retrieved documents or those stemming from the LLM's internal parameters. For instance, if a hallucination occurs when the model shows a high concentration of attention on specific context documents, it is likely that the source of the error lies within the retrieved data. Conversely, if the attention is not focused on any particular document, the issue may originate from the model's own parametric memory.\n\nAdditionally, RAGViz enables domain experts to assess the effectiveness of various data stores for RAG-based systems. By visualizing the attention levels on documents retrieved from different data stores, users can evaluate which data stores are most suitable for addressing domain-specific queries, offering critical insights into the alignment between the data store and the model's generation."}, {"title": "Examples", "content": "In this section, we showcase how RAGViz can help debug RAG pipelines by identifying hallucinations from parametric and non-parametric memory."}, {"title": "System Architecture", "content": "This section introduces RAGViz's system architecture and its query pipeline. The system has four main components: the ANN (Approximate Nearest Neighbor) index for dense retrieval, the backend server, the LLM inference server, and the frontend user interface. These components are implemented separately to allow for configurability. RAGViz's system is originally designed for use with a job scheduler like SLURM (Yoo et al., 2003)."}, {"title": "Dense Retrieval", "content": "In dense retrieval, queries and documents are encoded into high-dimensional feature vectors, also known as embeddings. A similarity search using metrics like cosine similarity or inner product is then performed to determine the nearest neighbors of a particular query vector. Significant research efforts have focused on various Approximate Nearest Neighbor Search (ANNS) indexing algorithms (Liu et al., 2004), which reduce search time by approximating the exact K-Nearest Neighbor search (KNNS).\n\nFor large-scale datasets, storing the embeddings and hosting an index for ANNS is often unfeasible on a single machine. RAGViz solves this by using a distributed system, where partitions of the set of embeddings are individually indexed and stored on the SSDs of separate nodes, represented in Figure 4 as worker CPU nodes 1 through i. The worker nodes each hosts a REST API that accepts query embeddings and returns the approximated top-k nearest neighbors in the form of dataset indices."}, {"title": "Context Builder", "content": "These REST API servers receive requests from the context-building backend server, which handles all the logic for constructing the language model context. Its responsibilities include loading the embedding model, managing backend logic, and storing the full corpus. This context builder is represented in Figure 4 as the main CPU node. Once queries are received and processed by authentication middleware, they are encoded into embeddings and routed to all worker nodes to perform ANNS. The top documents retrieved from the index at each worker CPU node are then reranked to return the final top k nearest neighbors of the query in the whole dataset.\n\nOnce these documents are retrieved, a snippeting technique is applied to extract the portion of the document relevant to the query. RAGViz provides two document snippeting methods: naive first and sliding window. The naive first method represents a document by its first 128 tokens. The sliding window method embeds windows of 128 tokens from the document into vectors and uses the window whose encoded vector has the highest similarity with the query to represent the corresponding document. Figure 5 shows a diagram of the sliding window method. This method increases latency in exchange for better document representation, based on the assumption that embedding similarity is correlated with relevance. After snippeting, the document context is routed to an LLM inference server."}, {"title": "Generation and Attention Output", "content": "RAGViz's system requires a node with access to GPUs, represented in Figure 4 in green, to run LLM inference tasks. As a first prototype, RAGViz's system uses two model libraries. vLLM (Kwon et al., 2023) is a library for fast LLM inference. VLLM is used in RAGViz to efficiently generate text from a prompt created by combining the doc-"}, {"title": "Frontend User Interface", "content": "The frontend user interface is adapted from Search with Lepton (Jia et al., 2024) and uses the Next.JS framework (Rauch, 2017). The frontend is built and exported as static files, which are hosted on an Apache web server (Fielding and Kaiser, 1997). The frontend utilizes a form to collect query information and other parameters to route to the main backend node.\n\nOnce the attention scores are received from the backend, they are stored in React states for use in the attention visualization. As users drag to select output tokens, the system stores a React state that lists the selected token indices. For every output token, the frontend sums the corresponding document token attentions and highlights the relevant, high-attention tokens in the document. The frontend also provides buttons for toggling document inclusion and routes new queries with updated sets of documents to a rewrite endpoint."}, {"title": "Experiment", "content": "This section introduces the chosen configurations of RAGViz's system demonstration and presents efficiency evaluations."}, {"title": "Datasets and Settings", "content": "RAGViz's demonstration is configured with the following systems:\n\nDataset: RAGViz has been tested with ClueWeb22 (Overwijk et al., 2022) and The Pile (Gao et al., 2020). ClueWeb22 is a 10-billion-document dataset collected from information-rich webpages. RAGViz uses the 80 million English documents in Category B, which includes the most frequently visited webpages. The Pile is a dataset primarily used for language model training. RAGViz uses the Pile CC training split, which includes filtered HTML pages from the Common Crawl (Foundation, 2007). The Pile is used for the demonstration of RAGViz because of its open-source flexibility.\n\nEmbedding model: We experimented with Anchor-DR (Xie et al., 2023), an embedding model trained on a contrastive learning task that matches anchor text (text referencing information from linked pages) to those linked pages.\n\nANNS system: RAGViz uses DiskANN (Jayaram Subramanya et al., 2019), an efficient graph-based memory-SSD (Solid State Drive) hybrid indexing ANNS system that maintains state-of-the-art performance in terms of latency and recall. DiskANN allows RAGViz's worker nodes to utilize SSDs to reduce memory consumption when serving the index.\n\nLanguage model: RAGViz uses Llama-2-7b (Touvron et al., 2023), an open-source language model developed by Meta. Llama-2-7b is lightweight and is supported by both vLLM and HuggingFace. The output token limit is set to 100 tokens for faster performance.\n\nThe system demonstration was hosted and evaluated with the hardware listed in Table 3."}, {"title": "Efficiency Evaluation", "content": "We benchmarked the overall efficiency of RAGViz, comparing the two snippeting techniques it offers. Table 1 shows that the system provides reasonable query latency when using the naive first snippeting method, with most of the latency stemming from LLM generation and the forward pass.\n\nThe sliding window technique offers a slight improvement in context relevance, as measured by the inner product. However, it leads to a significant increase in latency, as shown in Table 2. The minor relevance improvement makes it difficult to justify the substantial tradeoff in latency."}, {"title": "Conclusion", "content": "RAGViz is a powerful diagnostic tool for analyzing and improving RAG pipelines by providing detailed visualizations of attention mechanisms at various levels. Its attention-driven insights help users better understand the relationship between retrieved documents and language model outputs, making it invaluable for identifying hallucinations and enhancing retrieval efficacy.\n\nAs an open-source tool under the MIT license, RAGViz is available for research and development. We plan to support custom models in the future, allowing users to evaluate their own language models within the RAG pipeline. Additionally, we aim to improve usability by containerizing services for more efficient deployment and resource management. We will also unify the LLM inference process to use one inference library, leading to further improvements in speed and resource utilization."}, {"title": "Limitations", "content": "While RAGViz provides valuable visualizations of attention scores between generated and retrieved tokens, it assumes that higher attention scores indicate greater relevance and influence during generation. Further research is needed to evaluate the relationship between attention scores and model interpretability to fully determine RAGViz's effectiveness in improving RAG system explainability.\n\nCurrently, RAGViz supports only a single language model for generation tasks, limiting its ability to offer comparative insights across models. Adding support for multiple models could offer a more controlled framework for comparative analysis, enhancing the tool's diagnostic capabilities."}]}