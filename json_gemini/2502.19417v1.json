{"title": "Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models", "authors": ["Lucy Xiaoyang Shi", "Brian Ichter", "Michael Equi", "Liyiming Ke", "Karl Pertsch", "Quan Vuong", "James Tanner", "Anna Walling", "Haohuan Wang", "Niccolo Fusai", "Adrian Li-Bell", "Danny Driess", "Lachy Groom", "Sergey Levine", "Chelsea Finn"], "abstract": "Generalist robots that can perform a range of different tasks in open-world settings must be able to not only reason about the steps needed to accomplish their goals, but also process complex instructions, prompts, and even feedback during task execution. Intricate instructions (e.g., \u201cCould you make me a vegetarian sandwich?\u201d or \"I don't like that one\") require not just the ability to physically perform the individual steps, but the ability to situate complex commands and feedback in the physical world. In this work, we describe a system that uses vision-language models in a hierarchical structure, first reasoning over complex prompts and user feedback to deduce the most appropriate next step to fulfill the task, and then performing that step with low-level actions. In contrast to direct instruction following methods that can fulfill simple commands (\u201cpick up the cup\u201d), our system can reason through complex prompts and incorporate situated feedback during task execution (\"that's not trash\"). We evaluate our system across three robotic platforms, including single-arm, dual-arm, and dual-arm mobile robots, demonstrating its ability to handle tasks such as cleaning messy tables, making sandwiches, and grocery shopping.", "sections": [{"title": "1. Introduction", "content": "A defining feature of intelligence is its flexibility: people not only excel at complex tasks but also adapt to new situations, modify behaviors in real time, and respond to diverse inputs, corrections, and feedback. Achieving this kind of flexibility is essential for robots in open-ended, humancentric environments. For instance, consider a robot tasked with tidying up a table after a meal: instead of rigidly following a single predefined set of steps, the robot would need to interpret dynamic prompts like \"only take away someone's dishes if they are done eating,\" respond to corrections like \"leave it alone,\" and adapt when faced with unfamiliar challenges, such as a delicate object that requires special handling. This paper aims to advance robotic intelligence by enabling robots to interpret and act on diverse natural language commands, feedback, and corrections\u2014a step towards creating agents that reason through tasks, integrate human feedback seamlessly, and operate with humanlike adaptability. If we can enable a robot to process and engage with complex natural language interaction, we can unlock not only better instruction following, but also the ability for users to guide a robot through new tasks and correct the robot in real time.\nAchieving this level of flexibility and steerability in robotic systems is challenging. While standard languageconditioned imitation learning can follow simple, atomic instructions such as \u201cpick up the coke can\u201d (Brohan et al., 2022), real-world tasks are rarely so straightforward. Imagine a more realistic prompt, such as: \u201cCould you make me a vegetarian sandwich? I\u2019d prefer it without tomatoes. Also, if you have ham or roast beef, could you make a separate sandwich with one of those for my friend?\u201d This requires not only understanding the language, but also the ability to situate commands within the current context and compose existing skills (e.g., picking up the roast beef) to solve a new task. If the robot further receives corrections and feedback (\u201cthat\u2019s not how you do it, you have to get lower, otherwise you\u2019ll keep missing\u201d), these must also be integrated dynamically into task execution. This challenge resembles the distinction between Kahneman's \u201cSystem 1\u201d and \u201cSystem 2\u201d cognitive processes (Kahneman, 2011). The \u201cautomatic\u201d System 1 corresponds to a policy capable of executing straightforward commands by triggering pre-learned skills, while the more deliberative"}, {"title": "2. Related Work", "content": "Our work relates to research on VLMs for robotic control, which we can categorize into two groups: directly training VLMs for robotic control and using VLMs out-of-the-box with pre-defined robot skills. In the former category, methods fine-tune VLMs to output robotic controls based on input images and language commands (Brohan et al., 2023a;\nWen et al., 2024; Kim et al., 2024; Black et al., 2024; Liu"}, {"title": "3. Preliminaries and Problem Statement", "content": "A learned policy controls a robot by processing observation inputs, which we denote ot, and producing one or more actions At = [at, at+1, ..., at+H\u22121], where we use At to denote an action chunk consisting of the next H actions to execute (Zhao et al., 2023). Our system takes as input the images from multiple cameras I, ..., Ir, the robot's configuration (i.e., joint and gripper positions) qt, and a language prompt lt. Thus, we have ot = [I], ..., Ir, lt, qt], and the policy represents the distribution p(At|ot). Prior works have proposed various methods for representing and training such policies (Zhao et al., 2023; Chi et al., 2023; Octo Model Team et al., 2024; Pertsch et al., 2025).\nSince our focus will be specifically on complex, multi-stage tasks that require parsing intricate prompts and even dynamic user feedback, we need our policies to be able to interpret complex language and ground it via observations of the environment. A particularly powerful approach for handling such complex semantics is provided by visionlanguage-action (VLA) models (Black et al., 2024; Brohan et al., 2023a; Kim et al., 2024; Wen et al., 2024), which use vision-language model (VLM) pre-training to initialize the policy p(At|ot). A VLM is a language model that has also been trained to process image inputs, and represents a distribution p(l'|I, l) the probability of a language suffix l' (e.g., an answer to a question) in response to an image-language prefix consisting of an image I and a prompt l (e.g., a visual question). The most commonly used VLMs represent p(l'|I, l) via an autoregressive decoder-only Transformer model, factorizing the distribution into a product of autoregressive token probabilities p(xt+1|x1, ..., xt, I), where xt denotes the tth token (not to be confused with a physical time step), and we have l = [x1,..., xt] and l' = [xtp+1,..., Xtp+ts], with tp the length of the prefix and ts the length of the suffix (Beyer et al., 2024). We also use such Transformer-based VLMs, but since we do not modify their architecture and their autoregressive structure is therefore not relevant to our discussion, we will use the more concise p(l'|I, l) notation to represent a standard VLM.\nA standard VLA is produced by fine-tuning the VLM p(l'|I, l) such that the actions At are represented by tokens in the suffix l', typically by tokenizing the actions via discretization. We build on the \u03c0\u03bf VLA (Black et al., 2024),"}, {"title": "4. Hi Robot", "content": "We provide an overview of our method in Figure 2. Our approach decomposes the policy p(At|ot) into a low-level and high-level inference process, where the low-level policy consists of a VLA that produces the action chunk At in response to a simpler, low-level language command, and the high-level policy consists of a VLM that processes the open-ended task prompt, and outputs these low-level language commands for the low-level inference process. The two processes run at different rates: the low-level process produces action chunks at a high frequency, while the high-level process is invoked less often, either after a set time or upon receiving new language feedback. Thus, the highlevel process essentially \u201ctalks\u201d to the low-level process, breaking down complex prompts and interactions into bitesized commands that can be converted into actions."}, {"title": "4.1. Hierarchical Inference with VLAs", "content": "Formally, the high-level policy \u03c6hi(lt|I1, ..., Ir, lt) takes in the image observations and an open-ended prompt lt, and produces an intermediate language command lt. The lowlevel policy plo(At|I1, ..., Ir, lt, qt) takes in the same type of observation as the standard VLA described in Section 3,"}, {"title": "4.2. Incorporating User Interaction", "content": "The user can intervene at any point during policy execution and provide additional information and feedback, or even change the task entirely. In our prototype, these interventions take the form of text commands or spoken language (which is then transcribed into text). When the system receives a user intervention, the high-level inference is triggered immediately to recompute lt. The high-level policy has the option to include a verbal utterance ut in the command lt, which can be confirmations or clarifications from the robot. When ut is included, we use a text to speech system to play the utterance to the user, and remove it from lt before passing it into the low-level policy.\nWhen an interjection (\u201cleave it alone\u201d) has been fulfilled, the user can signal to the robot that it may switch back to the previous command and continue the task execution. Notably, the responses of the high-level policy are contextual, because it observes not only the prompt lt, but also the current image observations. Therefore, it can correctly ground feedback like \u201cthat\u2019s not trash,\u201d which is not possi"}, {"title": "4.3. Data Collection and Training Hi Robot", "content": "To train Hi Robot in a scalable manner, we employ both human-labeled and synthetically generated interaction data, as illustrated in Figure 3. First, we collect robot demonstration data Ddemo via teleoperation. This yields trajectories with coarse language annotations of the overall goal (e.g., make a sandwich). We then segment these full demonstration episodes into short skills, lt, such as pick up one piece of lettuce, which generally last between one and three seconds. We also heuristically extract basic movement primitives (e.g., small corrective motions) such as move the right arm to the left from the raw robot actions. The resulting dataset Dlabeled contains a set of (lt, I, ..., Ir) tuples that describe robot skills.\nNext, we use a large vision-language model (VLM) pgen to produce synthetic user prompts and interjections lt, and corresponding robot utterance ut. Given Dlabeled, we prompt pgen with both the visual context I, ..., In and the skill label lt (e.g., pick up the lettuce). pgen then imagines an appropriate interaction that might have led to lt in a real user interaction: it generates possible user prompts lt (e.g., \"Can you add some lettuce for me?\") along with the robot's verbal responses and clarifications ut. We detail the generation of the synethetic dataset Dsyn in Appendix A.\nWe train the high-level policy \u03c6hi(lt|I1, ..., Ir, lt) on Dsyn \u222a Dlabeled using the cross-entropy loss for nexttoken prediction. To train the low-level policy plo(At|I1, ..., Ir, lt, qt), we use Dlabeled \u222a Ddemo using a flow-matching objective, following Black et al. (2024)."}, {"title": "4.4. Model Architecture and Implementation", "content": "In our implementation, the low-level and high-level policies use the same base VLM as a starting point, namely the PaliGemma-3B VLM (Beyer et al., 2024). The lowlevel policy is the \u03c0\u03bf VLA (Black et al., 2024), which is trained by finetuning PaliGemma-3B with an additional flow matching \"action expert\" to produce continuous actions, while the high-level policy is fine-tuned on the image-language tuples described in Section 4.3 to predict commands. While we employ \u03c0\u03bf for our experiments, our framework is inherently modular, allowing for the integration of alternative language-conditioned policies as needed."}, {"title": "5. Experiments", "content": "In our experimental evaluation, we study a range of problems that combine challenging physical interactions with complex user interaction, including multi-stage instructions, live user feedback in the middle of the task, and prompts that describe novel task variations. We compare our full method to prior approaches and to alternative designs that use other high-level policy training methods. The aims of our experiments are:\n1. Evaluate the ability of our method to follow a variety of complex textual prompts and live user feedback.\n2. Compare our full method to prior approaches that train a flat instruction-following VLA policy or that use foundation models out-of-the-box for high-level reasoning.\n3. Evaluate the importance of synthetic data and hierarchy for task performance and language following."}, {"title": "5.1. Tasks and Baseline Methods", "content": "We use three complex problem domains in our experiments, as shown in Figure 4.\nTable bussing involves cleaning up a table, placing dishes and utensils into a bussing bin and trash items into the trash. The training data consists of full table cleaning episodes. This task is physically challenging because some items require nuanced grasping strategies (e.g., grasping a plate by the edge), the robot must pick up and singulate different objects, and in some cases might even manipulate some objects using others (e.g., picking up a plate with trash on it and tilting the plate to dump the trash into the trash bin). In our evaluation, the robot receives prompts that substantively alter the goal of the task, such as \u201ccan you clean up only the trash, but not dishes?\u201d, \u201ccan you clean up only the"}, {"title": "5.2. Metrics and Evaluation Protocol", "content": "We report two complementary metrics, measured by a human evaluator who is blind to the method being run. Each evaluation consists of 20 trials per task per method.\nInstruction Accuracy (IA). This score measures how well the high-level policy's predicted instruction aligns with human intent, requiring multi-modal understanding of the current environment and prompt. If the prediction from the high-level model is consistent with both the user's command and the current observation, the evaluator marks it as a correct prediction; otherwise, it is labeled as incorrect. The Instruction Accuracy for a trial is then computed as the proportion of correct predictions out of the total number of predictions. For flat baselines, which lack interpretable language predictions, scoring is based on the evaluator's interpretation of the intent of the policy behavior.\nTask Progress (TP). Since all tasks we evaluate are com"}, {"title": "5.3. Core Results", "content": "We present results for our system and two key baselines: a GPT-40 policy and a flat VLA method. Quantitative and qualitative results are in Figure 5 and Figure 6, and we summarize our findings below.\n(1) Hi Robot excels at open-ended instruction following. Across all tasks, Hi Robot exhibits substantially higher Instruction Accuracy and Task Progress, compared to GPT40 and the flat baseline. It properly identifies, picks up, and places the correct items even when prompted to handle only certain objects or omit ingredients (e.g., \"I'm allergic to pickles\"). In contrast, GPT-40 frequently loses context once physical interaction begins, issuing nonsensical commands (e.g., \u201cpick up bermuda triangle\u201d) or sometimes labeling everything as \u201cplate\u201d or \u201cspoon,\u201d which disrupts long-horizon planning.\n(2) Hi Robot shows strong situated reasoning and adaptation to feedback. When users modify requests mid-task (e.g., \u201cleave the rest,\u201d \u201cI also want a KitKat\u201d), Hi Robot updates low-level commands accordingly. GPT-40, however, often fails to maintain a coherent internal state, leading to commands like picking up new objects when the gripper is still occupied or prematurely switching tasks. The flat baseline, on the other hand, does not react to real-time feedback.\n(3) Hi Robot is effective across diverse tasks, robots, and user constraints. On single-arm, dual-arm, and mobile bimanual platforms, Hi Robot is able to handle distinct objects (from fragile cheese slices to tall bottles) while respecting dynamic constraints (e.g., \"bus only yellowish"}, {"title": "5.4. Ablation Studies", "content": "We conduct two key ablations to isolate the contributions of (1) synthetic data for high-level reasoning, and (2) hierarchical decomposition vs. a single \"flat\" policy.\n(A) Synthetic data is critical for open-ended instruction following. Comparing Hi Robot (trained on human-labeled + synthetic data) to a variant trained solely on humanlabeled data shows that synthetic interactions significantly boost language flexibility (Figure 7). Without them, the ablated model ignores clarifications (e.g., \"this is not trash\") or includes forbidden items (e.g., pickles), while Hi Robot smoothly adapts to such feedback, due to the broader coverage of compositional language in synthetic data.\n(B) Hierarchical structure outperforms a flat policy. We next compare Hi Robot to a flat policy trained on the same synthetic data but without a separate reasoning step (Figure 8). The flat model often reverts to clearing all items or fails to handle partial instructions (\"bus only the yellowish things\"), whereas Hi Robot re-checks the prompt at each high-level step and responds coherently to mid-task up"}, {"title": "6. Discussion and Future Work", "content": "We presented Hi Robot, a system that uses vision-language models (VLMs) in a hierarchical structure, first reasoning over complex prompts, user feedback, and language interaction to deduce the most appropriate next step to fulfill the task, and then performing that step by directly outputting low-level action commands. Our system can be thought of as a VLM-based instantiation of the \"System 1\" and \"System 2\" architecture (Kahneman, 2011). The deliberative \"System 2\" layer takes the form of a high-level VLM policy, which leverages semantic and visual knowledge from web-scale pre-training to reason through complex prompts and user interactions. The physical, reactive \"System 1\""}, {"title": "A. Synthetic Data Generation", "content": ""}, {"title": "A.1. Scenario and Response Categorization", "content": "To ensure the quality and diversity of the synthetic data, we incorporate structured scenario classification and response categorization into the prompt design for pgen, following (Stephan et al., 2024). Specifically, we classify interactions into different scenario types, such as negative task (where the user instructs the robot what not to do), situated correction (where the user adjusts an earlier command based on the evolving task state), and specific constraint (where the user specifies particular constraints, such as dietary preferences). In addition, we categorize the robot's responses into types such as simple confirmations, clarifications, and error handling. These classifications guide the generation process to ensure a broad range of user-robot interactions."}, {"title": "A.2. Prompt Construction for Contextual Grounding", "content": "In prompt P, we include a detailed description of the task (e.g., bussing a table, making a sandwich, grocery shopping) and instruct the model to ground responses in visual observations and prior context. A key advantage of leveraging large pretrained VLMs is their ability to incorporate world knowledge when generating interactions. For instance, the model can infer dietary constraints when generating prompts for sandwich-making, producing user commands such as \u201cCan you make a sandwich for me? I'm lactose intolerant\u201d and an appropriate robot response like \u201cSure, I won't put cheese on it.\u201d Similarly, it can reason over ambiguous or implicit requests, such as inferring that \u201cI want something sweet\u201d in a grocery shopping scenario should lead to suggestions like chocolate or candy.\nTo maintain consistency in multi-step tasks, we condition pgen on prior skill labels within an episode 10, ..., lt\u22121, allowing it to generate coherent user commands that account for past actions. For instance, if the robot has already placed lettuce and tomato on a sandwich, the generated user prompt might request additional ingredients that logically follow. This ensures that the synthetic interactions reflect realistic task progression rather than isolated commands. As such, we leverage pgen(lt, ut|I1, ..., Ir, l0, ..., lt\u22121, lt, 1 lt-1, lt, P) to produce a richer, more diverse synthetic dataset Dsyn that provides meaningful supervision for training our high-level policy.\nWhile in this work we generate a separate Dsyn and train a separate high-level policy for each task (e.g., sandwich making vs. table cleaning) for clarity and ease of benchmarking, the architecture is readily amenable to a unified multi-task formulation. In principle, the same hierarchical approach could be used to train a single high-level policy across a multitude of tasks, facilitating knowledge transfer"}, {"title": "B. System and Robot Overview", "content": "Our system integrates speech-based interactions and realtime robotic control. Below, we detail the components of our system, including audio processing, GPU-based inference, and the robot configurations."}, {"title": "B.1. Perception and Language Processing", "content": "For speech-based interaction, we use a consumer-grade lavalier microphone for audio input. Speech-to-text transcription is handled locally using Whisper large-v2 (Radford et al., 2023). For text-to-speech synthesis, we employ the Cartetia API to generate natural and expressive speech outputs."}, {"title": "B.2. Inference Hardware", "content": "To support real-time inference, we utilize one to two NVIDIA GeForce RTX 4090 consumer-grade GPUs."}, {"title": "B.3. Robot System Details", "content": "We employ three different robot configurations with various manipulation and mobility capabilities.\nUR5e. This setup features a 6-DoF robotic arm equipped with a parallel jaw gripper. It includes two cameras: a wrist-mounted camera and an over-the-shoulder camera. The system operates within a 7-dimensional configuration and action space.\nBimanual ARX. This configuration consists of two 6DoF ARX arms. The system is equipped with three cameras: two wrist-mounted cameras and one base camera. The combined system has a 14-dimensional configuration and action space, enabling dextrous bimanual manipulation tasks.\nMobile ARX. Built on the Mobile ALOHA (Fu et al., 2024) platform, this system integrates two 6-DoF ARX robotic arms mounted on a mobile base. The nonholonomic base introduces two additional action dimensions, resulting in a 14-dimensional configuration space and a 16dimensional action space. Similar to the bimanual setup, it includes two wrist-mounted cameras and a base camera, providing robust visual feedback for navigation and manipulation."}]}