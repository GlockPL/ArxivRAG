{"title": "Policy-as-Prompt: Rethinking Content Moderation in the Age of Large Language Models", "authors": ["KONSTANTINA PALLA", "JOS\u00c9 LUIS REDONDO GARC\u00cdA", "CLAUDIA HAUFF", "FRANCESCO FABBRI", "HENRIK LINDSTR\u00d6M", "DANIEL R. TABER", "ANDREAS DAMIANOU", "MOUNIA LALMAS"], "abstract": "Content moderation plays a critical role in shaping safe and inclusive online environments, balancing platform standards, user expectations, and regulatory frameworks. Traditionally, this process involves operationalising policies into guidelines, which are then used by downstream human moderators for enforcement, or to further annotate datasets for training machine learning moderation models. However, recent advancements in large language models (LLMs) are transforming this landscape. These models can now interpret policies directly as textual inputs, eliminating the need for extensive data curation. This approach offers unprecedented flexibility, as moderation can be dynamically adjusted through natural language interactions. This paradigm shift raises important questions about how policies are operationalized and the implications for content moderation practices. In this paper, we formalise the emerging policy-as-prompt framework and identify five key challenges across four domains: Technical Implementation (1. translating policy to prompts, 2. sensitivity to prompt structure and formatting), Sociotechnical (3. the risk of technological determinism in policy formation), Organisational (4. evolving roles between policy and machine learning teams), and Governance (5. model governance and accountability). Through analysing these challenges across technical, sociotechnical, organisational, and governance dimensions, we discuss potential mitigation approaches. This research provides actionable insights for practitioners and lays the groundwork for future exploration of scalable and adaptive content moderation systems in digital ecosystems.", "sections": [{"title": "1 Introduction", "content": "Content moderation involves the systematic monitoring and regulation of user-generated content in online platforms. This critical practice fosters safe, inclusive, and respectful digital spaces while aligning with the values of hosting organisations and adhering to regulatory requirements. As digital ecosystems grow and content creation surges, the demand for scalable and adaptive moderation systems has become increasingly urgent. Online platforms now face the dual challenge of managing vast, dynamic content streams and meeting evolving societal and regulatory expectations. With the internet continuing to transform how we access information, entertainment, and services, content moderation has become essential for effectively managing online platforms, bridging local and global perspectives to address societal needs and foster community trust.\nThe architecture of moderation pipelines varies significantly across digital services, reflecting their unique operational needs. For instance, video streaming platforms often prioritise automated detection of copyrighted or harmful content, while community forums like Reddit [50] emphasise moderating user interactions and discussions, often requiring human"}, {"title": "2 The Evolution of Content Moderation Technology", "content": "The evolution of content moderation, particularly within the text domain, has been closely tied to the exponential growth of user-generated content, driven by the rapid advancement of algorithmic capabilities. Early moderation efforts relied on basic automation, functioning like a digital \"find and replace\u201d tool. Methods such as keyword filters and hash-matching were used to identify and remove prohibited content [14]. While these first-generation tools were effective at detecting exact matches, they were easily circumvented by minor alterations, underscoring their limitations in adapting to the dynamic nature of online expression [49].\nThe rapid increase in user-generated content demanded more robust and scalable solutions, driving a shift towards machine learning-based approaches [48]. Early word embedding models like Word2Vec [36] and GloVe [45] enabled moderation approaches to capture semantic relationships between words, enhancing their ability to detect variations in harmful phrases and toxic language. This marked a pivotal step in moving beyond simple pattern matching to understanding the meaning behind online content.\nHowever, the true breakthrough came with the introduction of contextual embedding models [47] and the transformative Transformer architecture [63]. Models like BERT [12] enhanced content moderation by analysing content in context, enabling systems to improve detection of nuances such as sarcasm, coded language, and implicit bias that were previously undetectable. This advancement significantly improved the ability to understand the intent and impact of online communication, specifically in the realm of textual content [7].\nToday, advanced language models like GPT series [41], LLaMA series [61] and Claude series [3] are redefining the boundaries of content moderation. These models depart from traditional supervised learning-where systems learn from training examples- and are able to interpret desired behaviour directly from textual instructions. Their natural language understanding and reasoning capabilities allow them to go beyond simply identifying harmful content-they can assess its context and potential impact. They showcase capabilities such as applying complex policy guidelines, engaging in nuanced dialogue with users, and adapting to evolving language and online trends [1, 5, 11, 25, 26, 39, 43]. Rather than"}, {"title": "2.2 Policy-as-prompt", "content": "While the annotated dataset approach has long been the industry standard in moderation pipelines with an algorithmic component, the emergence of LLMs has reimagined how guidelines can be integrated into the moderation process. LLMs build their knowledge through extensive pretraining on web-based text, leveraging the transformer architecture [63]. This process employs self-supervised learning, where models predict subsequent words in incomplete sentences, enabling them to associate words and phrases with their typical contexts. As a result, LLMs acquire an internal knowledge of language that enables them to generate coherent and contextually relevant responses across diverse scenarios. Interaction with LLMs relies on prompts-input text that guides the model's response. LLMs encode the prompt into a high-dimensional vector space, preserving semantic relationships between words and phrases. These representations inform the model's output, drawing on patterns learned during pretraining.\nThe quality of an LLM's response is profoundly influenced by the prompt. A well-crafted prompt ensures that the model's output aligns with the intended outcome, highlighting the prompt as a pivotal element in effective LLM interaction. Factors such as phrasing, specificity, and context within the prompt play a critical role in shaping the response. As a result, prompt engineering has emerged as a systematic practice to extract knowledge from LLMs. This involves designing precise, task-specific instructions, that guide models toward accurate, relevant, and coherent outputs without retraining or altering the model's parameters (weights) [4]. Techniques range from simple instruction-based prompts to advanced methods like in-context learning, where illustrative examples are included in the prompt to steer the model towards the desired output [52, 54]. Other sophisticated techniques, such as Chain-of-Thought prompting (CoT), encourage the model to break down complex reasoning into step-by-step deductions [66], while Reason and Act (ReAct) prompts guide the model to interact with external tools or information sources to enhance its responses [69].\nIn content moderation, prompts enable quick adaptation of LLMs to changing policies or criteria, offering a flexible approach that avoids the need for retraining (see Figure 1b). By simply re-engineering the prompt with updated policy guidelines, LLMs can swiftly respond to evolving societal norms, emerging risks, or platform-specific needs. In this setting, policy guidelines become an integral part of the prompt, allowing for seamless updates and adjustments to content moderation practices.\nLLMs in content moderation currently operate in hybrid setups where human moderators collaborate with and oversee LLM-driven enforcement, but they have the potential to transform content moderation if given greater autonomy. To ensure this does not lead to harm, we need research to understand the complexities of this future. As organizations adopt this paradigm, they must navigate challenges spanning technical, sociotechnical, organisational, and governance dimensions as we show in Table 1. Technical implementation challenges emerge from the fundamental change in how policy guidelines are operationalized-moving from trained models based on annotated datasets to dynamic prompts that directly guide LLM behaviour. This shift introduces concerns around accurately converting policy to prompts and the sensitivity of LLMs to variations in prompt structure and formatting. At the sociotechnical level, challenges stem from the limitations of LLMs, which can constrain human judgment and social adaptability. Embedding policies directly into LLM prompts risks reinforcing technological determinism, where policies are rigidly interpreted based on the model's design, and potentially overlooking the nuanced interplay between technical systems and the human contexts in which they operate. On an organisational front, this shift demands changes to workflows and expertise. Traditional content moderation relied on distinct roles between policy formulation and operationalisation, where collaboration was often limited. Policy experts defined guidelines, and machine learning teams operationalised"}, {"title": "3 Technical Implementation Challenges", "content": "Traditionally, policy writing has been a human-centric endeavour, where subject matter experts meticulously crafted guidelines designed for clarity and human interpretation. These guidelines were refined through iterative feedback from human reviewers, including annotators, moderators, and domain experts, who brought contextual understanding, nuanced judgment, and collective experience to the process. Ambiguities were addressed through deliberation and testing. For instance, annotators would extensively test policies by creating hypothetical scenarios, exploring edge cases, and providing qualitative feedback, enhancing the robustness of the guidelines. The primary metric of success was the policy's clarity and actionability for human practitioners.\nIn the policy-as-prompt paradigm, the policy prompt becomes a distinct artifact. Policy writing now serves a dual purpose: it must remain accessible to human understanding, while also be supplemented with representations optimized for machine interpretation. This requires translating a policy's intent and rules into a format that is optimised for machine processing while faithfully preserving its meaning. However, verifying whether a prompt accurately conveys the policy's intent to the model is challenging. Such verification often relies on other signals, such as comparing LLM moderation decisions against human-labelled ground truth or expert judgments, to assess whether the model applies the policy as intended."}, {"title": "3.2 Prompt Structure and Format Sensitivity", "content": "In LLM-based content moderation systems, the structure and format of policy guidelines embedded within prompts are critical factors influencing performance. The presentation and organisation of policy text can directly affect moderation outcomes. Recent empirical studies highlight the significant, yet often overlooked, impact of prompt structure on LLM performance. For instance, Levy et al. [29] found that increasing input length can negatively impact performance, with significant drops occurring well before reaching the models' maximum input capacity. Similarly, [32] showed that performance drops significantly when relevant information is repositioned, highlighting a lack of robustness in long-context processing. He et al. [21] demonstrated that seemingly minor formatting choices-such as using bullet points instead of paragraphs-can substantially influence model performance across tasks. Perhaps most concerning, Sclar et al. [55] showed that LLMs exhibit unexpected sensitivities to superficial formatting elements like capitalisation and whitespacing, challenging common assumptions about their robustness.\nThe precise details of why certain prompt structures work better than others is still an active area of research in AI. Related works have observed that these sensitivities likely emerge from fundamental aspects of LLM architecture and pretraining. The models process text through self-attention mechanisms that compute relationships between all tokens in the input sequence [63]. While theoretically capable of handling arbitrary input structures, the effectiveness of these attention patterns is heavily influenced by how information is organized in the prompt. Zhang et al. [71] and Li et al. [30] demonstrated that different prompt structures lead to distinctly different attention patterns, affecting how information flows through the model's layers. Additionally, Kazemnejad et al. [24] showed that the positional encoding schemes used in transformer architectures can create inherent biases in how models process information at different positions in the sequence, potentially explaining the observed sensitivity to information positioning.\nThese findings underscore that the structural choices in presenting information to LLMs are far from superficial. Instead, they represent fundamental parameters that can significantly impact model behavior and reliability. This sensitivity has critical implications for applications like content moderation, where consistent and reliable performance is essential to ensure fairness, accuracy, and trust in automated decisions. Further, this structural sensitivity parallels broader concerns in machine learning regarding how seemingly minor design choices can produce significant downstream effects. Recent research shows that variations in machine learning pipeline components, even when they do not"}, {"title": "4 Sociotechnical Challenge: Technological Determinism in Policy Formation", "content": "Another challenge we identify in the policy-as-prompt approach is that of technological determinism. Technology significantly influences societal values and structures. The concept of technological determinism posits that technology, along with its design and inherent capabilities, can shape social structures, cultural norms, and even political systems [67].\nAlgorithmic systems, particularly those used in content moderation, have long been susceptible to technological determinism due to limitations such as model bias and undertraining [19]. Traditionally, human oversight in content moderation served as a vital feedback loop. Human annotators played a key role in shaping training data, ensuring that machine learning models were informed by social contexts and values. This interaction mitigated the influence of technology, enabling the development of nuanced rules that accounted for context and intent, incorporated ethical considerations, and provided mechanisms for human appeals to challenge automated decisions [51].\nThe advent of LLMs, particularly proprietary and opaque third-party models, is now shifting this balance. By enabling the direct use of policy guidelines in moderation workflows, thus bypassing human annotation, LLMs amplify the risks of technological determinism. As discussed in Section 3, LLMs excel at processing structured, algorithmic-friendly guidelines. However, this capability may inadvertently push experts to prioritize machine clarity over the nuance and flexibility required for context-dependent rules. Structured formulations might be favoured even when they do not fully align with underlying policy objectives, creating an environment where complex social issues are reduced to simplistic, binary judgments. This echoes Lessig's \"code as law\" principle, where technical architecture becomes a primary regulator of online behaviour [28]. Similarly, Yeung [70] describes \"algorithmic regulation\" as the process by which designing and deploying algorithms transforms into a form of governance, shaping both individual and collective behaviour. Finally, van Dijck [62] further argues that platform governance is increasingly dictated by technical affordances rather than democratic deliberation. Consequently, there is a growing risk that content moderation will be driven primarily by what LLMs can efficiently process, rather than by what best serves the diverse needs of online communities.\nA broader concern is that LLM capabilities could become the dominant force shaping policy decisions, overshadowing thoughtful considerations of platform values, user needs, and societal impact. The difficulty of encoding complex ethical considerations into LLM-interpretable rules may push platforms towards simpler, more easily enforceable policies, resulting in the homogenisation of content moderation practices. If LLMs struggle with highly nuanced or context-sensitive rules, platforms may opt for standardized policies to ensure consistent enforcement. This trend recalls the Television Code era of the 1950s and 1960s, when standardized broadcast guidelines homogenized content across local stations, forcing diverse communities to conform to mainstream cultural norms rather than addressing their unique need [6, 33]. Similarly, LLM-driven content moderation risks prioritizing technological efficiency over the"}, {"title": "5 Organisational Challenge: Converging Roles, Policy Authors and ML Practitioners", "content": "The integration of promptable LLMs into content moderation workflows has introduced a significant shift in roles and skill sets for both policy experts and machine learning practitioners. As LLMs are used to operationalise guidelines, policy authors are increasingly required to broaden their expertise beyond traditional policy development to include a working knowledge of machine learning principles, such as prompt engineering and the nuances of algorithmic behaviour, including bias, accuracy, and explainability [8]. This expanded skill set enables policy experts to anticipate potential issues, assess LLM performance, and contribute to the creation of more robust and fair moderation systems.\nConversely, machine learning practitioners are stepping into domains traditionally governed by policy experts. By designing and refining the prompts that guide LLMs, they play a pivotal role in shaping how policies are interpreted and enforced. However, without formal training in policy development or regulatory frameworks, their contributions may inadvertently introduce systemic biases or misinterpretations that diverge from the policy's original intent and potentially overlook critical ethical considerations.\nThese emerging workflows are not simply technical translations of policy but complex \"contested spaces\" where different professional epistemologies and interpretive frameworks converge [20]. This intersection highlights the growing need for cross-disciplinary collaboration, where machine learning practitioners develop a deeper understanding of policy nuances, and policy experts acquire foundational machine learning literacy. In this evolving paradigm, content moderation would no longer rely on distinct, sequential contributions from policy experts and machine learning practitioners. Instead, it would require hybrid expertise and continuous collaboration.\nA notable example of this shift is the adoption of red teaming in LLM evaluation [15, 46]. Traditional evaluation methods, which rely on bespoke testing procedures, often fall short in capturing the full spectrum of behaviours and risks associated with increasingly autonomous and capable LLMs. Red teaming addresses this challenge by borrowing concepts from security practices, where deliberate attack strategies are employed to stress-test systems and uncover vulnerabilities. This approach requires not only technical expertise to understand and exploit the model's intricacies but also insights from fields such as social science, ethics, and policy to identify risks beyond technical performance, such as misuse or harmful societal impacts.\nThis shift underscores the need for hybrid knowledge systems that blend technical, ethical, and policy expertise. In the future, content moderation may give rise to roles such as \"AI policy translators\"-professionals skilled in bridging the gap between technical and policy teams. These individuals would play a pivotal role in ensuring that automated systems align with policy goals while leveraging the capabilities of LLMs. By fostering cross-disciplinary collaboration, they would contribute to the development of more robust, adaptable, and ethically sound moderation practices."}, {"title": "6 Governance Challenges", "content": "The rapid adoption of LLMs in automated decision-making systems has intensified discussions around model governance, focusing on issues such as transparency, accountability, and fairness in the deployment of AI-driven systems [9, 16, 40]. Rather than revisiting existing discussions on governance for automated decision-making systems, we specifically examine the implications unique to the policy-as-prompt configuration.\nDecision attribution poses an significant accountability challenge in the policy-as-prompt setup. The lack of clear system boundaries create confusion when unwanted outcomes occur, making it difficult to pinpoint their exact cause-whether they stem from the policy language in the prompt, the LLM's interpretation of that language, or the complex interaction between the two. This ambiguity directly undermines auditability \u2013 reconstructing the decision-making process and documenting the influence of various components becomes incredibly difficult. The involvement of multiple stakeholders, including policy writers, prompt engineers, and model providers, further complicates attribution by obscuring how their contributions influence these outcomes [13].\nA further challenge is determining which prompt adjustments warrant formal documentation. As discussed in Section 3, changes to enforcement can be made by modifying prompts, often with the semantic meaning of the underlying policies remaining unchanged. However, the sensitivity to prompt structure can lead to significant shifts in model behaviour, making it difficult to track how enforcement evolves over time. If every minor change requires extensive documentation, it adds unnecessary overhead. Yet, if changes are not sufficiently documented, it becomes increasingly difficult to monitor and understand the impact of these modifications on model performance."}, {"title": "7 Recommendations", "content": "In this section, we build on the systematic breakdown of challenges presented earlier (see Table 1) to propose strategic directions for addressing them. These strategies are informed by the insights gained from analysing the technical, sociotechnical, organisational and governance-related aspects of the challenges. Rather than offering a definitive set of solutions, this section emphasizes high-level, actionable pathways that can guide future research and practical efforts. By framing these strategies within the context of our earlier analysis, we aim to provide a foundational perspective for tackling the complexities associated with this technology."}, {"title": "7.1 Enhanced Evaluation", "content": "A key mitigation strategy across several challenges is to do rigorous evaluation of the policy-as-prompt implementation. This evaluation must extend beyond traditional accuracy metrics to assess performance across critical dimensions.\nTo address technical sensitivity to prompt structure and formatting, the evaluation process should include comprehensive sensitivity analysis during the prompt development phase. This would involve studying the impact of formatting, phrasing, and structural changes on model outputs. Stress testing with diverse policy prompts ensures that small adjustments, such as punctuation or formatting changes, do not unintentionally lead to misinterpretation by the model. Sclar et al. [55] recommend reporting performance accross a range of plausible prompt formats and styles rather that focusing solely on the performance of a single prompt format. Sensitivity analysis should also address subtle inconsistencies, such as those revealed by predictive multiplicity. For instance, Rashomon sets can identify cases where semantically similar policy phrasing elicit divergent model responses, such as inconsistent handling of edge cases, despite achieving comparable accuracy metrics [18, 35]. By analysing these variations, developers can identify policy formulations that are both effective and robust.\nAt the sociotechnical level, evaluation should move beyond conventional accuracy metrics to include measures of societal readiness and adaptability. Metrics like demographic fairness [37, 64] can help identifying whether specific prompt formulations create disparities across demographic groups, ensuring equitable policy application. Additionally, case libraries can serve as a valuable tool for addressing potential simplifying tendencies of LLMs. These libraries, containing nuanced, real-world examples of moderation edge cases-such as region-specific cultural references or satire-can test how well a policy-as-prompt system manages societal complexity. Specifically for the prompted guidelines,"}, {"title": "7.2 Collaborative Prompt Engineering", "content": "In Section 3.1 we discussed the complexity of converting policy guidelines to prompt, requiring careful attention to the ambiguities that can arise due to the inherent differences between human and machine interpretability of the policy prompt. To minimize machine misinterpretations, strategies must focus on crafting prompts that address multi-faceted aspects of content and encourage diverse perspectives. Already techniques like chain-of-thought reasoning [66] have shown promise in crafting prompts that encourage step-by-step reasoning, allowing models to consider multiple facets of a problem before arriving at a conclusion.\nTechniques like meta-prompting can dynamically integrate contextual ethical reasoning [60], and multi-persona prompting [65] can embed varied policy perspectives, enabling models to better navigate nuanced contexts. Additionally, fostering collaborative environments where LLMs-trained on diverse data and architectures-contribute to policy interpretation helps mitigate biases and refine prompt designs. These LLMs can collaboratively suggest rewrites and identify gaps creating a feedback loop for continuous improvement [31]. By treating prompt writing as an iterative, collaborative process of continuous refinement, practitioners can develop more nuanced and adaptive approaches to capturing the complex contextual understanding required in content moderation."}, {"title": "7.3 Traceability of Prompt Modifications", "content": "At the model governance level accountability concerns can be addressed by implementing mechanisms that enhance the transparency and traceability of prompt modifications. A \"prompt genealogy\" could record changes to prompt structures, formatting, and phrasing, along with the rationale behind each modification. Similar to version control systems used in software development [59] and drawing inspiration from tools like DVC (Data Version Control) [23] and Pachyderm [44], which provide versioning and lineage tracking for datasets and machine learning pipelines, a prompt genealogy would extend these principles to prompt engineering. For instance, practitioners could generate contextual logs or metadata documenting key aspects of the system's behaviour, such as the inputs provided, the policy components referenced in the prompts, and the resulting outputs. While this approach does not reveal the internal workings of how the LLM processes or interprets prompts internally-a subject of ongoing research [57]-it offers a structured way to analyse how prompt changes or input variations correlate with differences in outcomes.\nThis transparent record would also provide organisations with a comprehensible audit trail, supporting accountability requirements and reproducibility. It would also allow organisations to demonstrate alignment with intended policy goals and, where necessary, facilitate external reviews or assessments. Finally, in addition to this tool being useful for auditing, it can complement the sensitivity analysis idea mentioned in Section 7.1, by providing data for offline tests."}, {"title": "7.4 Bridging Organisational Silos", "content": "As discussed in Section 5, integrating policy authors and machine learning practitioners into a cohesive workflow is essential. However, the transition may encounter practical challenges, similar to historical precedents in other technological paradigm shifts. For example, in the early 2000s the rise of data science in business required close collaboration between domain experts and data specialists [10]. Similarly, the emergence of bioinformatics in the"}, {"title": "8 Conclusions", "content": "In this paper, we explored the complexities of the policy-as-prompt paradigm in content moderation through four dimensions: technical, sociotechnical, organizational, and governance. By presenting a structured overview of the challenges practitioners (would) face when transitioning from \u201ctraditional\u201d machine learning or rule-based systems to the policy-as-prompt paradigm, we aim to support decision-making and underscore the importance of holistically considering the interplay between the technical capabilities of large language models, social and organizational contexts, and governance frameworks. Additionally, we propose potential strategies to mitigate some of these challenges, while acknowledging that many open questions remain regarding the implementation of effective content moderation systems under this paradigm.\nOur discussion highlights challenges across these four dimensions without claiming to be exhaustive, and readers may identify additional issues beyond those covered in this paper. Furthermore, while we emphasize the transformative potential of the policy-as-prompt paradigm, it is important to note that such systems are not currently implemented in a fully autonomous manner. Instead, they typically operate within hybrid setups, where human moderators work alongside and oversee LLM-driven enforcement mechanisms. However, even within this hybrid framework, the increasing adoption of the policy-as-prompt configuration introduces considerable complexities and potential risks that warrant further investigation. Addressing these challenges proactively will be critical to developing more effective hybrid moderation systems that thoughtfully integrate LLM capabilities with human oversight.\nEmpirically, we focused on one specific issue-model sensitivity to prompt structure-to demonstrate the brittleness of current LLMs are. We recognize, however, that model performance is influenced by a broader set of parameters beyond prompt structure, including the extent of pre-training, the choice of fine-tuning techniques, model architecture and size, and other factors that were fixed in our experiments. A more comprehensive analysis that examines how these parameters interact would yield deeper insights into their collective impact on model behaviour and reliability in content moderation tasks.\nGoing forward, as LLM capabilities continue to improve, the policy-as-prompt has the potential to overcome many of its current limitations, enabling organizations to dynamically align enforcement with nuanced and evolving policy objectives. Sustained exploration of these advancements will be essential in shaping the future of content moderation systems."}, {"title": "A Experiments", "content": "We observe that model performance, as measured by the F-beta score, fluctuates across textual prompt variations (see Figure 7 for an example of the template variations and Figure 8a for the results). Categories such as Not Violating and Self-Harm maintain relatively high and stable scores, while more contentious categories like Jailbreak, Harm, and Illegal Goods display marked sensitivity. This disparity highlights that model performance is prone to degradation when subjected to structural changes in the policy prompt. This directly supports the thesis of Section 3.2, highlighting the role of prompt design in shaping moderation outcomes. The variations in F-beta scores (Figure 8a) and the high standard deviation (Figure 8b) demonstrate predictive multiplicity in action. Even though the overall performance across prompt variations may appear statistically comparable, the conflicting behaviours observed in key categories (e.g., Jailbreak, Harm) point to a deeper underlying issue: different prompt structures lead to divergent model predictions for the same input samples.\nTo examine the role of policy specificity, we created temporal snapshots of policy descriptions, starting with minimal drafts (one-line descriptions) and progressively adding detail to arrive at comprehensive guidelines (as seen in Table 2 reversely).\nTo gain insight how models with different size respond to variations of the prompt, we ran experiments using models with different size from different providers. In Figure 9a we plot the performance distribution and use 'P#' to indicate"}, {"title": "1. Punctuated", "content": "Prompts that incite violence or hatred towards a, person or group, of people based on race religion gender identity or expression sex, ethnicity, nationality sexual orientation veteran, status age disability or other characteristics associa ed with systemic discrimination or marginalization includes but may not be limited to: prai sing supporting, or calling for violence or exclusion, against a person or group of people based on the characteristics lis ted above, dehumanizing statements..."}, {"title": "2. Structured", "content": "- Prompts that incite violence or hatred towards individuals or groups are prohibited. - These prompts target characteristics such as:\nrace\nreligion\ngender identity or expression\nsex\nethnicity\nnationality\nsexual orientation\nveteran status\nage\ndisability\nother characteristics linked with systemic discrimination or marginalization\n- Prohibited actions include, but are not limited to:\nPraising, supporting, or calling for violence or exclusion against individuals or groups based on the aforementioned characteristics.\nMaking dehumanizing statements..."}, {"title": "3. Concise", "content": "Policy that prohibits content inciting violence or hatred on grounds of race, religion, gender identity, sex, ethnicity, nationality, sexual orientation, veteran status, age, disability, or other marginalized traits includes, but is not limited to: endorsing or advocating violence or exclusion, dehumanizing statements..."}, {"title": "4. Verbose", "content": "The policy is dedicated to addressing and regulating prompts that have the potential to incite or provoke acts of violence or expressions of hatred directed toward an individual or a group of individuals. This regulation specifically targets instances where such behavior is based on categorical aspects such as race, religion, gender identity or expression, sex, ethnicity, nationality, sexual orientation, veteran status, age, disability, or other attributes which are commonly linked to systemic discrimination or marginalization. The policy encompasses, but is not strictly limited to, behaviors such as: endorsing, supporting, or advocating for acts of violence or exclusion toward an individual or groups of individuals based on any of the aforementioned characteristics, making statements that dehumanize an..."}, {"title": "5. Annotator", "content": "If a message encourages violence or hatred towards someone because of their race, religion, gender identity, sex, ethnicity, nationality, sexual orientation, veteran status, age, disability, or other similar traits, it's not allowed. This includes actions like praising, supporting, or demanding violence or exclusion of these individuals. It also involves saying dehumanizing things..."}]}