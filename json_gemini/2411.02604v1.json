{"title": "Computing critical exponents in 3D Ising model via pattern recognition/deep learning approach", "authors": ["TIMOTHY A. BURT"], "abstract": "In this study, we computed three critical exponents (\u03b1, \u03b2, \u03b3) for the 3D Ising model with Metropolis Algorithm using Finite-Size Scaling Analysis on six cube length scales (L=20,30,40,60,80,90), and performed a supervised Deep Learning (DL) approach (3D Convolutional Neural Network or CNN) to train a neural network on specific conformations of spin states. We find one can effectively reduce the information in thermodynamic ensemble-averaged quantities vs. reduced temperature t (magnetization per spin < m > (t), specific heat per spin < c > (t), magnetic susceptibility per spin < x > (t)) to six latent classes. We also demonstrate our CNN on a subset of L=20 conformations and achieve a train/test accuracy of 0.92 and 0.6875, respectively. However, more work remains to be done to quantify the feasibility of computing critical exponents from the output class labels (binned m, c, x) from this approach and interpreting the results from DL models trained on systems in Condensed Matter Physics in general.", "sections": [{"title": "Introduction", "content": "In terms of dimensions, the three-dimensional Ising square lattice model is a simple extension of the two-dimensional Ising model, which Onsager solved analytically in 1944. However, to this day, no exact solution exists. Critical exponents are typically found using simulation results.\nThis project was primarily motivated by two papers: a computational physics simulation study and a pattern recognition study using DL on a 2D Ising model.\nSonsin et al. [1] simulated the 3D Ising system using a Monte Carlo method/Metropolis algorithm and found that for three system sizes of L=150,200,250, the Metropolis approach (local spin update) can reach equilibrium in a feasible amount of time (10000 Monte Carlo Sweeps (MCS)) and found the critical temperature T to be \u2248 4.512 kBT/J from Binder's Cumulant $U_4 = 1 - \\frac{<m^4>}{3<m^2>^2}$. Their approach contrasts the standard global spin update methods typically used for large system sizes of L (e.g., Wolff, Swendsen-Wang algorithms), which avoid the critical slowing down near Tc.\nHolzbeck et al. [2] investigated the phase transition of the 2D Ising model using a DL ap- proach to recognize patterns in spin conformations from equilibrated Monte Carlo Metropolis algorithm simulations. They found the trained DL model could distinguish between confor- mations with an inner region coupling constant Jinner = 2 from conformations with an outer region coupling constant Jouter = 1.\nThe two separate but primary goals for this project were to:\n1. Compute three critical exponents for the 3D Ising model using finite-size scaling anal- ysis (FSSA) on simulation results, replicating the model and parameters in [1].\n2. Design, implement, and evaluate a supervised Deep Learning (DL) approach to rec- ognize specific realizations of spin states, each categorized by a unique pattern or montage, extending the work from [2].\nThis paper was initially written as a final project for the PHYS 7350 Advanced Computa- tional Physics class at the University of Houston (UH) in Spring 2020."}, {"title": "Methods", "content": "For this study, we design a method for determining critical exponents from specific state realizations labeled with a class corresponding to three phases of matter using a supervised DL approach. We outline each step in our scheme below:\n1. Run 3D Ising model (ferromagnetic square lattice PBC, Metropolis algorithm) simu- lations\n\u2022 Input\nParameters: T=[3.5,5.5], (kB & J=1), \u2206T = 0.05, Total Monte Carlo Sweeps (MCS): 10000, L=20,30,40,60,80,90, H=0, 2 independent runs (different seed, both all spins up initially) at each L,T value\nRandom number seeds used: [2,9]1\nThe Hamiltonian used is shown in Equation 1\n\u2022 Output\n\u2022 Trajectories: MC sweep (MCS), E(t), M(t) (and running averages of each)\nConformations: output every 25 MCS (only use MCS \u2265 6500 frames, based on magnetization equilibration / correlation time < Tm > + < Tm >\u03bc\nNumber of conformations saved: 2822 conforms available at each L,T value\n\u2022 Train/test ratio: 70/30\n\u2022 Using that ratio gives 19,740 realizations for training and 8,460 for testing, ran- domly drawn from 6 predefined latent classes (over L,T) evenly (to prevent over- fitting/bias)\n2. Calculate correlation (or equilibration times) averaged over both runs for all L,T\n3. Calculate thermodynamic averaged c(T), m(T), x(T) values\n\u2022 Bin each of these plots into six latent classes by manually identifying three ranges for each of the three plots (only 6 out of 27 possible classes had non-zero bins)\n4. Run Finite-Size Scaling Analysis (FSSA) to compute critical exponents c,m,x on two sets of L ranges, L=20,40,80 & L=30,60,90. This step develops intuition about simu- lation accuracy by comparing these values to the literature.\n5. Preprocess data and prepare it for DL algorithm training\n\u2022 Loading conformation sparse .txt files and compressing/cutting unused frames\n\u2022 Train/test split over six classes (evenly)\n6. 3D CNN DL training/testing\u00b3\n\u2022 Input: Spin conformation only\n\u2022 Output: Latent class (6 categories)\n7. Run FSSA again to compute critical exponents c,m,x on train/test sets (mixed L's) in- dependently. The final accuracy of the DL algorithm is based on the percent difference between the two sets."}, {"title": "Results/Discussion", "content": "3.1 Simulation and finite-size scaling analysis\n3.1.1 Equilibration times\nFigure 1 shows magnetization per spin < tm > and energy per spin < te > equilibration times vs. temperature T for L=20. Since L=20 near Tc had the largest overall L values, only conformations with MCS \u2265 6500 were used as training/testing data (came from < Tm > +\u03c3 < Tm >\u03bc).\nNote that throughout the paper, the ensemble average (<\u00b7 >) is defined over the two independent simulation runs.\n3.1.2 < c> (T), < m > (T), < x > (T) results\nFigure 2 shows ensemble averaged < c > (T), < m > (T), < x > (T) values for L=20 (other L results similar). The statistics are sufficiently sampled to estimate the corresponding\n3.1.3 Critical exponents \u03b1, \u03b2, \u03b3 results from FSSA\nFigure 3 shows the uncollapsed and the corresponding data collapse of the scaling functions forc, m, X vs. reduced temperature on the L values L=20,40,80 (L=30,60,90 similar, not shown). The Python package pyfssa [3] was used to perform automated parameter tuning to find the best fits for the critical exponents \u03b1, \u03b2. Let us introduce the scaling function conventions used (from [4]).\n$c = L^{\\alpha/v}c(L^{t/v})$\n$m = L^{-\\beta/v}m(L^{t/v})$\n$x = L^{\\gamma/vx}(L^{t/v})$\nSince the exponents are related and only two are independent, we can reduce these fit values in the following way:\n1. Obtain \u03b1, \u03b4\u03b1, \u03bd, \u03b4\u03bd directly from pyfssa on c (using a logarithmic power law In(L) near Tc)."}, {"title": "Using Deep Learning on spin states to predict different phases of matter", "content": "Since a standard CNN was implemented in this paper, it was intended for classification tasks (i.e., supervised learning). Classification ML models only output latent class labels with no uncertainty, while regression ML models can output continuous values (i.e., unsupervised learning). This difference is due to a typical CNN algorithm's standard dropout layer after the fully connected layers. Therefore, we must assign categories to ranges of the continuous quantities we want to predict.\nTo devise a methodology for choosing enough bins to give accurate distributions of m, c, x while ensuring they are physical and well-defined is a trial and error process. We could also consider this as course-graining the physics.\n3.2.1 Class label definitions and interpretation\nLet us define three constant ranges (bin edges) on the reduced temperature t. Since there are three quantities we aim to predict (m, c, x), there are three ranges for each of the three quantities (Figure 4).\n\u2022 t < -0.1: Ferromagnetic region\n\u2022 -0.1 < t < 0.01: Transition region\n\u2022 t \u2265 0.01: Paramagnetic region\nCombining all of the possibilities results in 27 possible latent classes. The Python package pandas was used to analyze & bin the data. After performing standard techniques from statistics, we find there are only six non-empty categories (i.e., for each L,T value, only 6 combinations of the 27 possible bins exist). With the amount of data available, we should be able to adequately train our DL model to detect which class a given conformation belongs to using six classes.\n3.2.2 DL implementation and results\nDue to time constraints, we trained/tested on a small subset of L=20 conformations (defined as the miniset). For a detailed description of the DL model, please see Appendix.\nThe input size to the CNN is 256x256x40 (width, height,depth), respectively. This input size can handle any L size from our simulations, which can be rotated to fit inside those dimensions. Extra space not filled with a conformation, e.g., L=20, is padded with zeros, left unscaled, and the conformation repositioned if needed to fit inside.\nTo reduce the chance of overfitting the model, we split each of the six classes into their own train/test set using pandas DataFrame.shuffle method. Given the conformation data, the RandomState seed passed was 6; one can use this same value to reproduce our exact train/test sets.\nWe also shuffled each batch of training conformations randomly each time so that the neural network (NN) saw the different classes as out of order. This choice increases the robustness of the trained model for distinguishing between different conformation classes.\nThe model is trained on conformations for 20 epochs (20 passes over 5 batches of 25 real- izations, with accuracy computed every epoch (after 125 realizations, the size of the L=20 miniset). The training batch size is 125, which defines one epoch (when the neural network weights are updated with probability 0.5).\nAt this point, the accuracy of the final training is computed. After training, the test accuracy was measured on a separate miniset of 16 realizations that had not been used during training. A batch's train/test accuracy is defined as #correct/batch size (sometimes called the test sensitivity).\nWe were able to successfully train our model using this subset of L=20 conformations (125/8093 \u2248 1.5% of total available (Table 3)). Figure 5 shows a plot of training accu- racy vs. epochs."}, {"title": "Conclusions", "content": "We have partially demonstrated a method based on intuition for training a 3D CNN to distinguish between 6 latent classes of spin realizations on a miniset of L=20 spin states. We also computed three critical exponents \u03b1, \u03b2, \u03b3 for the 3D Ising model from Monte Carlo Metropolis simulations using FSSA on two sets of L values separately and provide a pipeline for measuring and determining the accuracy of critical exponents output by the trained DL model.\nWe computed the error between our measured critical exponents and literature and found systematic biases to be why our reported errors were too small. We then give a procedure that should correctly account for both random and systematic errors. Two ways to improve our critical exponent values would be to use larger system sizes, spanning a range of 1-2 orders of magnitude (10-100x), along with checking the literature for empirical formulas for corrections to m, c, x for small L values to better collapse the data.\nWhile the test accuracy was lower than expected, we identified not using a batch size with equally distributed class conformations as the likely culprit. Since both train and test accu-"}, {"title": "Appendix", "content": "CNN architecture\nOutlined here are the exact layers in the implemented 3D DL architecture. It is a vanilla 3D CNN implemented serially using TensorFlow v1 in Python. The training step uses the Adam Optimizer to minimize the cross entropy with a constant learning rate of 1 \u00d7 10-3. Specific hyperparameters for the L=20 miniset used are given in Figure 5. The layers are described below.\n\u2022 First Convolutional Layer: 32 features for each 5x5 patch (1 feature - 32 features)\n\u2022 First Max Pool Layer: max pooling over 2x2 blocks (output image: 14x14x32)\n\u2022 Second Convolutional Layer: 64 features for each 5x5 patch (32 features - 64 features)\n\u2022 Second Max Pool Layer: max pooling over 2x2 blocks (output image: 7x7x64)\n\u2022 First Fully Connected (Densely Connected) Layer: 1024 neurons to process the entire image (output: 1024)\n\u2022 Second Fully Connected (Readout) Layer: output size 6 (one per class), e.g., [1 0 0 0 0 0] would be class 0 output\nA depiction of the features each layer of a 2D implementation of our CNN finds on a Class 0 (Far Tc paramagnetic phase) spin state axial slice is shown in Figure 7."}, {"title": "Acknowledgements", "content": "I want to thank Prof. Kevin E. Bassler for mentoring the class project and instructing the PHYS 7350 course at UH. I also thank Prof. Ioannis A. Kakadiaris for providing feedback on the paper for a presentation at TSAPS 2020."}]}