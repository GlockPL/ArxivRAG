{"title": "PREDICT: Preference Reasoning by Evaluating Decomposed preferences Inferred from Candidate Trajectories", "authors": ["St\u00e9phane Aroca-Ouellette", "Natalie Mackraz", "Barry-John Theobald", "Katherine Metcalf"], "abstract": "Accommodating human preferences is essential for creating AI agents that deliver personalized and effective interactions. Recent work has shown the potential for LLMs to infer preferences from user interactions, but they often produce broad and generic preferences, failing to capture the unique and individualized nature of human preferences. This paper introduces PREDICT, a method designed to enhance the precision and adaptability of inferring preferences. PREDICT incorporates three key elements: (1) iterative refinement of inferred preferences, (2) decomposition of preferences into constituent components, and (3) validation of preferences across multiple trajectories. We evaluate PREDICT on two distinct environments: a gridworld setting and a new text-domain environment (PLUME). PREDICT more accurately infers nuanced human preferences improving over existing baselines by 66.2% (gridworld environment) and 41.0% (PLUME).", "sections": [{"title": "1 Introduction", "content": "A fundamental component of effective interaction is understanding the preferences of those with whom we engage. Successfully recognizing and accommodating these preferences leads to more pleasant and efficient experiences (Felfernig et al., 2006). While such preferences can be verbalized explicitly, they can also be inferred implicitly from past interactions. Ideally, an AI agent should be able to both use explicit feedback and learn from implicit cues. As human directions are commonly expressed in natural language, creating a mapping from implicit cues to natural language could enable a natural-language conditioned agent to seamlessly integrate both implicitly and explicitly defined preferences. This work focuses on this gap by proposing a method to infer natural language preferences from a user's actions.\nLarge Language Models (LLMs) possess strong priors about human behavior (Brown et al., 2020). Previous work has demonstrated that these priors can provide the basis to infer user preferences in domains such as robotic manipulation (Peng et al., 2024) and collaborative authoring (Gao et al., 2024). However, current methods infer preferences without reflection nor refinement, resulting"}, {"title": "2 Related Work", "content": "Natural Language Conditioned Agents Language is the most natural way for humans to commu-nicate and express themselves. As a result, considerable research has focused on natural language-conditioned agents across a variety of domains. BabyAI (Chevalier-Boisvert et al., 2019) introduces an environment for natural language conditioned gridworld agents. Further advancements, such as gSCAN (Qiu et al., 2021), investigate how gridworld agents handle compositionality, while Zhong et al. (2020) explore the ability of agents to learn environment dynamics from text. Misra et al. (2018) proposes LingUNet as way to fuse language and vision in a simulated 3D world. Blukis et al. (2018, 2020) extend this for continuous drone control. In room-to-room navigation, works such as CLIPNav (Du et al., 2023) and Embodied CLIP (Khandelwal et al., 2022) use CLIP embeddings (Radford et al., 2021) to condition agents on visual-language aligned representations.\nIn robotic arm manipulation, Lynch & Sermanet (2021) condition trajectories on both goal images and natural language, demonstrating successful task completion with limited language labelling. Jang et al. (2021) builds upon this and use videos as goal contexts. For pick-and-place tasks, Shridhar et al. (2022) uses a CLIP-based two-stream architecture, and Mees et al. (2022, 2023) demonstrate long-horizon task completion via hierarchical approaches.\nIn natural language generation, prompting (Radford et al., 2019) and in-context learning (Brown et al., 2020) have proven effective methods for controlling the generation of text, especially in a preference-driven context (Sun et al., 2023, 2024).\nPersonalization Some prior approaches of adapting models to user preferences involve RLHF (Stiennon et al., 2020) and fine-tuning (Tan et al., 2024; Zhuang et al., 2024), which can be compute-intensive and inaccessible to some practitioners without the budget or scale of needed data. With the rise of LLMs with strong instruction-following capabilities, methods like prompting to adapt a user's profile have become more popular (Shen et al., 2024; Salemi et al., 2024), however these approaches often rely on explicit feedback provided from the user (Lin et al., 2024). PREDICT circumvents these issues by learning from implicit user signals, breaking down preferences into sub-components to generate tailored user-preferences, all without the need of fine-tuning.\nPreference-Conditioned Agents Combined preference inference and conditioning has recently gained traction, with the following two works being most aligned with our approach.\nPeng et al. (2024) explores preference learning in quadrupedal mobile manipulation using an object detection module to map image observations to text. An LLM then infers preferences by comparing pairs of trajectories. These preferences are in turn used to improve task alignment with user preferences. Gao et al. (2024) propose the PRELUDE environment, where an LLM learns writing style preferences in a collaborative authoring task. We discuss this work in detail in Section 4.3.\nThese methods rely on a single inference step, whereas our approach uses iterative refinement for more precise preferences, and validation across several user examples for robustness."}, {"title": "3 PREDICT", "content": "We now outline PREDICT's key contributions to preference inference: (1) iteratively refining preferences, (2) breaking down inferred preferences into their constituent components, and (3) validating the inferred preferences against relevant user examples. The prompts for each step and each evaluation task are shown in Appendix F.1 (Fig. 7 and 8), and the algorithm is provided in Appendix A2.\nIterative Refinement using Candidate Trajectories A preference-inferring agent requires a preference-conditioned agent that can act according to the inferred preferences. There has been sig-nificant work in the area of natural language conditioned agents (Szot et al., 2024; Ichter et al., 2022; Chevalier-Boisvert et al., 2019; Mees et al., 2022, 2023; Lynch & Sermanet, 2021; Sun et al., 2024) demonstrating the viability of natural language preference-conditioned agents. Motivated by how (Peng et al., 2024) and Gao et al. (2024) use comparative examples to infer preferences, PREDICT uses the preference-conditioned agent to iteratively create tailored candidate trajectories using the most recently inferred preferences for comparison. By comparing these contrasting trajectories to the user examples, the inferring agent can more clearly pin point differences that indicate missing preferences, in turn enabling the inferring agent to extract more from a single user example.\nSpecifically, for each task instance, the preference-conditioned agent in PREDICT completes the task with the current set of inferred preferences\u2014if no user examples have been seen before, an empty preference set is used. This provides an initial candidate trajectory. If the candidate trajectory matches the user's examples then the current inferred preferences are sufficient to explain the user's behavior and no further learning is required. However, if the trajectory differs, we prompt the LLM to refine the inferred preferences to explain the difference between the candidate trajectory and the user's example. We then create a new candidate trajectory using the newly refined preferences, and repeat this process until we match the user's example, or a maximum number of iteration steps is reached. In all experiments, we use a maximum of three refinement steps per user example.\nBreaking Down Preferences The second novel contribution is to break preferences down into their base components. For example, write as if the events could happen with emojis interspersed could be broken down into [write using conditional expressions, use emojis]. This provides several advantages. Preference components provide greater coverage of the preference space with less data, e.g., three components can be combined to create nine profiles. Further, preference components make it easier to refine preference sets by adding, removing, or modi-fying single components of a preference rather than modifying full preferences. Lastly, preference components remove ambiguity when validating preferences. If we validate a compound preference and only a single component is incorrect, then the entire compound preference including any useful components may be discarded.\nValidating Preferences The third contribution of PREDICT is motivated by the insight that prefer-ences should be observed consistently\u00b3, i.e., preferences should be noticeable in several user examples. The most similar work of Gao et al. (2024) retrieves related user examples to create the starting preferences for a new task instance. PREDICT adds an additional stage that uses these examples to validate inferred preferences and filter incorrectly inferred preferences.\nWe test each preference component of the inferred preference set against each user example by prompting an LLM to determine whether the user's examples strongly confirm the preference, some-what confirm the preference, is neutral toward the preference, somewhat contradict the preference, or strongly contradict the preference. Each answer is mapped to a score from -2 (strongly contradicts)"}, {"title": "4 Experimental Set Up", "content": "All of our experiments consist of three phases per task. First, the user completes the task using their true preferences. Second, the agent attempts to complete the task using its currently inferred preferences (if any). Finally, the agent compares its attempt at task completion with the user's example to infer new preferences to use going forward.\nAll agents are evaluated along two dimensions: preference quality that measures similarity between the true and inferred preference sets, and action quality that evaluates an agent's task completion against the user's true preferences. Note that the first task completion will always be conditioned on an empty preference set and that we evaluate the preference set used to solve the task. Thus, the first step is equivalent across all agents, and we omit its results.\nThe agent learns from 4 10 users (depending on the task) with five examples per user, and performance is reported as the mean across all examples, users, and across five seeds (standard deviation is reported over these seeds). The user preferences for the assistive writing tasks are in Appendix D (Table 4), whereas the PICK UP task has a rule-based user preference construction procedure described in Section 4.2. For all experiments, we use GPT-40 as the inferring agent except when we compare LLMs of different sizes and quality (shown in Fig. 2) 4. For the assistive writing tasks, GPT-40 is used as a synthetic human. The synthetic human prompts can be found in Appendix F.2.\n4.1 Research Questions\nWe pose the following research questions:\nRQ1: Does iteratively creating candidate comparison trajectories improve the quality of inferred preferences? To explore this, we consider three variants of PREDICT: (1) PREDICT1NC (1NC=1 inference step, no candidate) uses no example comparisons and prompts the LLM a single step to infer the preference given only the user's example; PREDICT1sc (1SC=1 inference step, single candidate) is PREDICT with a single inference step and a single candidate trajectory; and (3) PREDICTSC (\u22643 inference steps, single candidate) is PREDICT with a single candidate used for all inference steps. Comparing PREDICT1NC and PREDICTsc measures the effect of comparing candidate examples to the user's examples when inferring preferences. The differences between the PREDICT1sc and PREDICTse variants quantifies the role of increasing the number of inference steps, while comparing PREDICTsc and the full PREDICT algorithm clarifies the effects of explicitly providing the LLM with the outcomes of its predictions.\nRQ2: Does breaking down preferences into components improve the performance and consis-tency of the preference inferring methods? To answer this question we compare the full PREDICT algorithm with a variant that does not breakdown preferences PREDICTCP (CP=compound prefer-ences). We hypothesize that PREDICT improves performance and reduces variance between seeds relative to PREDICTCP.\nRQ3: Does filtering preferences by validating them across multiple examples lead to fewer errors? While the preference inferring step can add missing preferences, we hypothesize that validating preferences provides a strong mechanism to remove incorrectly inferred preferences. To test this hypothesis, we evaluate a variant, PREDICTNV (NV=no validation), that does not validate preferences. Our hypothesis is that PREDICT's attempts to complete the tasks will be of a higher quality than attempts using PREDICTNV."}, {"title": "4.2 Environment 1: PICK UP", "content": "We develop the PICK UP (Policy Imitation by Comprehending Key User Preferences) task in a gridworld environment populated with various objects of different shapes and colors. Users in the environment navigate to pick up objects with attributes (i.e., shape/color) they like, while avoiding objects with attributes they dislike, before navigating to an end goal location. When an object is collected, a reward of +1 is awarded for each liked attribute and a reward of -1 is awarded for each disliked attribute. For example, an object whose shape and color are both liked would have a reward of +2, while an object whose shape is liked and color disliked has a reward of 0. Note the reward function is used only for evaluation purposes and does not play a role in preference learning.\nFor PICK UP, we automatically transform trajectories into a structured language description. Fig. 4 (Appendix B) shows a visual and natural language representation of the environment and its trajecto-ries. The objective of a preference inferring agent in this environment is to be able to collect the same objects that the user's would. To accomplish this, they must first identify the user's likes and dislikes, and then navigate the world to collect the appropriate objects. We include the presence of neutral objects in the environment, which adds ambiguity to the system as neutral objects are only picked up if they are along the shortest path between desirable objects or the goal, which is not identifiable from the text representation of the user's example. Thus, from the perspective of an inferring LLM, the environment is only partially observable. This design is intentional; motion is inherently difficult to encode in language, so many tasks will be partially observable to an LLM. Due to the partial observability, we require three validations to discard a preference in PICK UP.\nIn this environment, each task instance is defined by a user identifier and an environment layout containing seven random objects placed at random locations in a 5x5 grid. The user identifier maps to a unique and private set of preferences. Each user's preference set contains exactly one liked shape, one liked color, one disliked shape, and one disliked color, however this information is not provided to the inferring agent. These are all specified in the structured format: <likes/dislikes><attribute>. Users are neutral toward all the remaining attributes. The well-defined structure of the preferences in PICK UP allows us to map a preference set to a set of positive reward objects and negative reward objects. We then use this mapping to condition an A* agent that collects all the positive objects while avoiding negative objects.\nThe preference structure also enables direct comparison of preferences. To this end, we report the Intersection over Union (IoU) between the inferred and true preference sets as the preference quality metric. A downside of the rigid preference structure is that it requires us to decompose preferences, which prevents us from addressing RQ2 in this environment. For the action quality metric, we measure the cumulative reward, or return, of the agent's trajectories. Each liked/disliked attribute (shape or color) in the set of collected objects adds +1/-1 to the score respectively. For all experiments, we use 10 distinct users (N = 10)."}, {"title": "4.3 Environment 2: Assistive Writing", "content": "PRELUDE: Gao et al. (2024) propose PRELUDE (PREference Learning from User's Direct Edits) as an environment to evaluate preference inferring algorithms. PRELUDE consists of two tasks: summarizing articles and writing emails from notes. Each task has a set of users with each user having a distinct set of preferences. Each user additionally writes their summaries/emails on different topics, with each topic corresponding to a different source of articles/notes (e.g., chat forums, paper abstracts, encyclopedia articles). The summarization and email writing tasks have five and four users respectively.\nFor each task instance, the agent must write a summary or email using the article / notes and any inferred preferences it has learned up to that point. The user is then asked if the agent's output is satisfactory based on their true preferences. If the agent's output is satisfactory, the cost to the agent is zero. If the agent's output is not satisfactory, the user edits the agent's output according to their preferences, and a cost based on the extent of the edit is incurred.\nPLUME: The objective of the PRELUDE environments is to evaluate how well a model infers a user's preferences and the cost of incorrectly inferred preferences. Therefore, it is vital that the measure of inferred preference quality is highly correlated with the cost function. We analyze PRELUDE (see below) and find that the chosen metrics, the editing process, and the sets of preferences used are key"}, {"title": "5 Results and Discussion", "content": "We present our main results comparing baselines and various PREDICT ablations in Table 1. Results on PRELUDE can be found in Appendix C.1. To compare tasks on action quality with metrics on different scales, we use a percentile score, where 0% corresponds to the no-preference (NP) baseline and 100% to the oracle preference baseline. All percentage improvements are reported as the difference in scores on this scale. Overall, PREDICTFull outperforms PREDICTBase by 9.3%, BC by 66.2%, and CIPHER by 41.0%.\nRQ1. In our first question, we set out to verify whether generating iterative candidate trajectories is beneficial to inferring preferences. Comparing PREDICT to its ablated versions on the action/gener-ation quality metric (PPCM), shows each component of the iterative refinement process improves"}, {"title": "5.1 Limitations and Future Work", "content": "While the methods proposed in this work provide a number of significant improvements, their limitations and challenges provide interesting avenues for future work."}, {"title": "6 Conclusion", "content": "In this paper, we propose three novel contributions to guide an LLM to better infer preferences from user examples and introduce a new environment for evaluation. First, we iteratively refine preferences by using a preference conditioned agent to test inferred preferences. Second, we break preferences down into their constituent components. Third, we validate preferences against other user examples. We demonstrate on both navigation and writing environments that the proposed method improves performance by as much as 66.2% and 58.8%."}, {"title": "7 Acknowledgements", "content": "We would like to thank Miguel Sarabia, Charlotte Magister, Maartje ter Hoeve, and Walter Talbott for their helpful feedback on the manuscript drafts. We would like additionaly thank Charlotte for her help with the LLM personalization literature review. We would like to thank Martin Klissarov for his discussions during ideation and project framing, and Yizhe Zhang for discussions during prompt tuning. Finally a big thank Lindsay Hislop for help coordinating dataset and model use approvals."}, {"title": "A Algorithm", "content": "Algorithm 1 Preference-Conditioned Agent Task Completion\n1: Require:\n2:\ntask_instance \u25b7 Task instance\n3: Initialize empty preference set all_preferences \u2190 \u00d8\n4: Retrieve relevant examples related_examples \u2190 get_relevant_examples(task_instance.context)\n5: for each example in related_examples do\n6:\nall_preferences \u2190 all_preferences U example.learned_preferences\n7: Coalesce and condense preferences preferences_to_use \u2190 LLM.coalesce(all_preferences)\n8: Generate agent trajectory agent_trajectory \u2190 agent.solve_task(preferences_to_use)\n9: Output: Completed task trajectory agent_trajectory and final preferences\npreferences_to_use\nAlgorithm 2 PREDICT: Preference Refinement and Inference\n1: Require:\n2:\n3:\n4:\ntask_instance \u25b7 Task instance\nagent_trajectory \u25b7 Agent trajectory\nuser_example \u25b7 User example\n5: Initialize inferred_preference_set \u2190 preferences_to_use\n6: Set candidate trajectory candidate_trajectory \u2190 agent_trajectory\n7: for each refinement step (up to 3 steps) do\n8:\n9:\n10:\n11:\n12:\n13:\nif candidate_trajectory = user_example then\n\u2610 Stop refinement\nelse\nRefine preferences\ncompound_preference \u2190 LLM.Refine(inferred_preference_set,\nuser_example,\ncandidate_trajectory)\nDecompose preference\ninferred_preference_set \u2190 LLM.Breakdown(compound_preference)\nGenerate new candidate trajectory\ncandidate_trajectory \u2190 agent.solve_task(inferred_preference_set)\n14: Initialize empty validation score list validation_scores \u2190[]\n15: for each preference_component in in ferred_preference_set do\n16:\nfor each example in related_examples do\nnew_score \u2190 LLM.validate(preference_component, example)\n17:\nValidate preference against trajectory\nvalidation_scores \u2190 validation_scores + [new_score]\n18:\n19:\n20:\nif mean(validation_scores) < threshold then\nDiscard preference_component\n21: Add task_instance and inferred_preference_set to list of examples for future learning"}, {"title": "B PICK UP Objects Visualization", "content": "A rendering of the PICK UP Objects task is provided in Appendix Fig. 4.\nThe following objects are available:\na red pentagon, a red square, a yellow triangle,\nblue circle, and a green pentagon.\nThe user picks up the yellow triangle, the blue\ncircle, and the green pentagon\nReturn: 2\nUser Preferences:\n\"likes yellow\", \"likes circles\", \"dislikes red\u201d, \u201cdislikes squares\u201d"}, {"title": "C Extended Results", "content": "Additional results tables and figures discussed in the main body of the paper.\nC.1 PRELUDE Results\nResults on PRELUDE (Gao et al., 2024) for PREDICT and baselines: a No-Learning baseline (NPC), an Oracle preference baseline, in-context learning (ICL), CIPHER-1, and CIPHER-5 Gao et al. (2024) (Table 2). To directly evaluate the ability to infer preferences, we provide all models with ground-truth knowledge of the source of the documents. On the summarization task, PREDICT outperforms all baselines on action/generation quality. On the email writing task, PREDICT outperforms all baselines on the PPCM metric, but slightly underperforms CIPHER-1 on the poorly correlated Levenshtein distance metric (see Section 4.3-Metric Correlation for issues with Levenshtein distance).\nResults in this table further support issues with the current preference-quality metrics. In the email writing task, the no-learning baseline (which always uses an empty preference), has a higher accuracy than any learning method, which may be due to the significant overlap between preference sets in the task. Further, in both tasks, the highest preference-quality scores do not lead to the highest action-quality scores. We encourage future work to look into alternative preference-quality metrics.\nWe lastly note that PRELUDE has substantially smaller range between the no-learning (NPC) and oracle preference baselines relative to PLUME. On PPCM, PRELUDE has a range 2.45 and 0.62 for summarization and email writing respectively, while PLUME has ranges of 3.17 and 2.91 for the two tasks. This further supports PLUME as the primary evaluation environment."}, {"title": "C.2 Metric Correlation Results", "content": "The metric correlation results for the assistive writing tasks both across the summary versus email\nwriting sub-tasks and by sub-task (Table 3)."}, {"title": "C.3 Preference Inference and Conditioning Performance by Number of User Samples", "content": "In Fig. 5 we show the impact of the number of samples for a given user according to the measures for\ninferred-preference and action/generation quality metrics."}, {"title": "D PRELUDE vs. PLUME Preference Sets", "content": "The preference sets used for each document source and environment (PRELUDE vs. PLUME) are\ngiven in Appendix Table 4."}, {"title": "E Illustrative Examples of Issues with PRELUDE", "content": "E.1 Levenshtein Distance and Multiple Generations\nWe present here an inherent issue with Levenshtein distance. For all three generations, we prompt gpt-40 to summarize the encyclopedia article on hotdogs. The first two generations (A. and B.) use identical prompts, in which we ask it to use the preference: \u201cuse a haiku\u201d. In the third generation, we ask it to use the preference \u201cuse a single short sentence\". We then calculate the Levenshtein distance between each pair, reported in Table 5. Notably, the Levenshtein distance and length normalized Levenshtein distance, in which a lower score represents higher similarity, would both indicate that generation A and C are more similar to generation A and B, even though generation A and B share the same exact same prompt while generation C uses a entirely different preference. BERTScore does a better job of identifying the most similar texts, however the PPCM scores, which compares writing directly to the true preferences, clearly labels both Generation A. and Generation B. as \"strongly exhibiting\" the preference \u201cuse a haiku\", and Generation C. as \u201cstrongly contradicting\" the preference.\nGeneration A.\nA sausage in bun,\nToppings vary near and far,\nHot dog, rich in roots.\nGeneration B.\nGrilled or steamed sausage,\nIn a bun with cheese or spice,\nStreet food's warm embrace.\nGeneration C.\nA hot dog is a sausage in a bun with various toppings such as mustard, ketchup,\nand relish"}, {"title": "E.2 Editing Influences the User", "content": "In this next example, we demonstrate how the editing procedure can influence the output of the user. We again ask an LLM to summarize the encyclopedia article on hot dog. Let's assume, an inferring agent has incorrectly inferred the preference \"question answering style\". Using this preference, the preference-conditioned agent writes:\nWhat is a hot dog?\nA hot dog is a dish made of a sausage (typically a wiener or frankfurter) served in\na partially sliced bun. It can refer to the sausage itself, which is grilled, steamed, or\nboiled.\nHow is a hot dog typically prepared?\nHot dog preparation varies, but common condiments include mustard, ketchup,\nrelish, and onions. Some toppings can also include cheese sauce, sauerkraut, chili,\nbacon, or jalape\u00f1os.\nWhere did hot dogs originate?\nThe sausages used in hot dogs were culturally imported from Germany, and the\ndish became popular in the U.S. as a working-class street food. Over time, hot\ndogs became closely associated with American culture, particularly baseball.\nWhere are hot dogs most popular?\nWhile they are especially associated with New York City, hot dogs became\nwidespread across the United States throughout the 20th century, becoming an\nintegral part of American cuisine.\nWe ask the user if the above is satisfactory for it's true preference of: \"style targeted to young children\". It does not, so we then ask the user to edit the above for it true preference \"style targeted to young children\", and the user writes:\nWhat is a hot dog?\nA hot dog is a yummy snack made of a sausage (like a wiener or frankfurter) inside\na soft bun. Sometimes people call the sausage itself a hot dog, and it can be cooked\nby grilling, steaming, or boiling.\nHow is a hot dog usually made?\nHot dogs can be made in different ways, but they often have yummy toppings like\nmustard, ketchup, relish, and onions. Some people like to add even more fun stuff\nlike cheese, sauerkraut, chili, bacon, or spicy jalape\u00f1os.\nWhere did hot dogs come from?\nThe sausages in hot dogs came from Germany, and a long time ago, they became\nsuper popular in the U.S. as a quick and tasty street food for busy people. Now, hot\ndogs are a big part of American culture, especially at baseball games!\nWhere are hot dogs the most popular?\nEven though hot dogs are popular all over, they are especially loved in New York.\nOver the years, hot dogs have become a favorite food all across the United States!\nWhile the above summary does correctly follow the true preference, it is still formatted in a question answering style, which is not part of the user's preferences. I.e., the preference-inferring agent's error has directly impacted the user's writing. This is another reason why PLUME removes the editing procedure.\nFor reference, if the user is directly asked to summmarize the encyclopedia article on hot dogs using it's true preference of \u201cstyle targeted to young children\", it outputs:\nA hot dog is a yummy food that's made with a sausage, like a wiener or frankfurter,\ninside a soft bun. You can cook the sausage in different ways, like grilling or\nboiling. People love to put tasty toppings on their hot dogs, such as ketchup,\nmustard, cheese, and even onions!\nHot dogs came from Germany, but they became super popular in the US, especially\nat baseball games. Now, you can find hot dogs all over the country, and in some\nplaces, like Chicago, people have special ways of making them!\""}, {"title": "F Prompts", "content": "F.1 Preference Inference and Preference-Conditioned Agent Prompts\nThe prompts used by PREDICT for candidate trajectory generation and task completion in the PLUME environment are in Appendix Fig. 6. The prompts used by PREDICT to infer the user preferences from user examples are provided in Appendix Fig. 8 for the PICK UP environment and in Appendix Fig. 7 for the PLUME environment."}, {"title": "F.2 Synthetic Human Prompts", "content": "The prompts used to have GPT-40 play the role of our synthetic human for PREDICT are given in Appendix Fig. 9. The \u201chuman\u201d is instructed to complete the task in the same way as the preference-conditioned agent when completing the writing tasks (see Appendix Fig. 6)."}, {"title": "F.3 Preference-Conditioned Agent Baseline Prompts", "content": "The prompts used in the no-preference baseline are in Appendix Fig. 10 and for the in-context learning baseline are in Appendix Fig. 11. For the in-context learning baseline, the number of examples l matches the number of examples used when coalescing prevoiusly inferred prompts (see Appendix Fig. 7)."}, {"title": "F.4 LLM-as-a-Judge Prompts", "content": "The prompts used by the LLM-as-a-Judge are shown in Fig. 12."}]}