{"title": "ACTIVATIONS THROUGH EXTENSIONS: A FRAMEWORK TO BOOST PERFORMANCE OF NEURAL NETWORKS", "authors": ["Chandramouli Kamanchi", "Sumanta Mukherjee", "Kameshwaran Sampath", "Pankaj Dayama", "Arindam Jati", "Vijay Ekambaram", "Dzung Phan"], "abstract": "Activation functions are non-linearities in neural networks that allow them to learn complex mapping between inputs and outputs. Typical choices for activation functions are ReLU, Tanh, Sigmoid etc., where the choice generally depends on the application domain. In this work, we propose a framework/strategy that unifies several works on activation functions and theoretically explains the performance benefits of these works. We also propose novel techniques that originate from the framework and allow us to obtain \"extensions\" (i.e. special generalizations of a given neural network) of neural networks through operations on activation functions. We theoretically and empirically show that \"extensions\" of neural networks have performance benefits compared to vanilla neural networks with insignificant space and time complexity costs on standard test functions. We also show the benefits of neural network \"extensions\" in the time-series domain on real-world datasets.", "sections": [{"title": "INTRODUCTION", "content": "Current literature on machine learning and artificial intelligence is filled with triumphs of neural networks over other alternatives. A significant portion of these successes could be attributed to the design of novel and innovative activation functions. Here we comment on articles that are relevant to our work.\nReLU, Sigmoid, and Tanh are arguably the most common activation functions of choice in artificial neural networks of which ReLU is more predominant for its simplicity and computational efficiency. Several enhancements and modifications for these activation functions are proposed in the literature. For example Maas et al. (2013) proposes Leaky ReLU and shows performance improvement over ReLU on benchmark tasks in the acoustics domain. In He et al. (2015), authors come up with a parametrization of ReLU (PReLU). It is shown that PReLU, along with a novel initialization procedure, surpasses human-level performance on the ImageNet classification dataset. Of late, Maniatopoulos & Mitianoudis (2021) introduces Learnable Leaky ReLU (LeLeLU), which is a further parameterized variant of Leaky ReLU."}, {"title": "BACKGROUND", "content": "In this section we introduce notation and review feedforward neural networks and statistical learning theory very briefly."}, {"title": "FEEDFORWARD NEURAL NETWORKS", "content": "A vanilla feedforward neural network Haykin (2009) (FNN) consists of L layers. Each layer has M\u2081,1 \u2264 l < L hidden nodes. Given an input vector x[1: t] = [x1, x2,\u00b7\u00b7\u00b7, xt]', the output \u0177 of FNN is obtained as follows. In the first layer, we construct pre-activation h\u00b9 = W1 * x[1: t] + b\u00b9 where the weight matrix W\u00b9 \u2208 RM1\u00d7t and the bias vector b\u00b9 \u2208 RM1. The pre-activation is transformed coordinate by coordinate via a differentiable nonlinear activation function a\u00b9 to obtain f\u00b9 = a\u00b9(h\u00b9). For every subsequent layer i, 2 \u2264 i \u2264 L \u2212 1, the output from the previous layer fl-1 is transformed to obtain the output of the current layer, fl = a\u00b9(W\u00b9 * fl\u22121 + b\u00b2), with W\u00b9 \u2208 IR MixMi\u22121 and b \u2208 RM\u0131. In the final layer, l = L, of the neural network the output \u0177 is obtained as \u0177 = aL(WL * fL\u22121 + bL) where WL \u2208 R1\u00d7M\u2081\u22121 and b \u2208 R. It is well-known that a single hidden layer neural network with an arbitrary number of hidden nodes is a universal approximator Haykin (2009). However, in practical scenarios, multi-layer neural networks are adopted. To learn a complex relationship between input and output, we search the space of weight matrices and biases for ideal parameters by optimizing a carefully chosen loss measure on a dataset."}, {"title": "SOME ELEMENTS OF STATISTICAL LEARNING THEORY", "content": "Statistical Learning Theory Vapnik (2013) is at the foundation of most machine learning algorithms. A key problem, the discipline addresses is the following. Given a parameterized family of functions H = {f(x, 0), 0 \u2208 \u04e8}, a joint distribution of random variables X, Y, denoted p(x, y) and a loss function L, the discipline explores the conditions that are necessary and sufficient to solve the optimization problem\nmin Ep [L (y, f (x, 0))]\n\u03b8\u0395\u0398\nthrough samples generated from the joint distribution p(x, y).\nIn many practical scenarios, one is typically interested in improving the state of the art model performance on benchmark datasets (i.e., to improve Ep[L]). A probable way to accomplish this task is to expand the search space (i.e. search in H' \u2283 H as min\u04a3, Ep [L (y, f(x,0))] \u2264 minh Ep [L (y, f(x, 0))]). Many works in the literature He et al. (2015); Maniatopoulos & Mitianoudis (2021); Biswas et al. (2021); Manessi & Rozza (2018) expand the search space through \"extentions\" albeit without the explicit mention of the same. In what follows, we formally define \"extentions\", state and prove their properties, point out some extensions in the literature and define a few novel extensions and demonstrate their benefits on synthetic as well as real-world datasets."}, {"title": "ANALYSIS", "content": "In this section, we define extensions and proceed to state and prove their properties. We start with the definition of an extension.\nDefinition 1. An extension of a given function f : D \u2192 R is a function F : D \u2192 R where D C D and F(x)|{x\u2208D} = f(x). Here f is called as a restriction of F."}, {"title": "PROPERTIES", "content": "In this subsection we state and prove properties of extensions and define our neural network extensions.\nLemma 1. Assume that F : D \u2192 R is an extension of a function f : D \u2192 R then\nmin F(x) < min f(x)\nxED\nXED\nProof. By definition F(x) = f(x) for x \u2208 D. As a consequence, minx\u2208D F(x) = minx\u2208D f(x).\nAgain by definition D C D and on a larger set minimum value can only decrease. So,\nminx\u2208D F(x) < minx\u2208D f(x).\nLemma 2. Suppose F: D \u2192 R is an extension of f : D \u2192 R and G : D IR is an extension of F. Then G is also an extension of f.\nProof. Since F is an extension of f and G is an extension of F, we have D C D and D \u2286 D. So\nD C D. Similarly, from the definition we have F(x)|x\u2208D = f(x) and G(x)|x\u2208D = F(x). As\nDCD, we have G(x)|x\u2208D = F(x) = f(x). So G is an extension of f. Moreover, the relation\nextension between two functions is transitive.\nLemma 3. Suppose F: D \u2192 R is an extension of f : D \u2192 R and f : D \u2192 R is an extension of F\nas well. Then f = F.\nProof. It is given that f is an extension of F, so D C D and f(x) = F(x) on D. It is also given that\nF is an extension of f, so D C D and F(x) = f(x) on D. So we have D = D and f(x) = F(x)\nand the relation extension is anti-symmetric.\nLemma 4. The relation extension forms a partial order on the space of functions.\nProof. From Lemma 2 and Lemma 3, the relation extension is anti-symmetric and transitive. A\nfunction f is, by definition, an extension of itself. So the relation extension is reflexive as well.\nHence the relation extension forms a partial order on the space of functions."}, {"title": "LEARNABLE ACTIVATIONS", "content": "We call the activations given by\ngi = Xa. (1)\ngi = a\u00b2 A\u2081\u03b1 + \u03bb\u03b1. (2)\nas Linear Learnable Activation (LLA) and Quadratic Learnable Activation (QLA) respectively\nRemarks 6. The choice of learnable activation (LLA or QLA) in a neural network is not hyper parameter optimization. We are not choosing the best activation functions for the neural network. The hyper parameter optimization in this context is the choice of the activation library S.\nRemarks 7. To ensure that the convergence properties of M are similar to N, constraints - i.e., IT TA\u2081 = 1 and li > are enforced.\nRemarks 8. As the extensions get complicated, the optimization process complexity and the number of parameters increase"}, {"title": "TIME AND SPACE COMPLEXITY", "content": "Given the library of activation functions S := {ReLU, GELU, Tanh, Sigmoid, ... } and per iteration computation cost, C, of N, the per iteration computation cost of M with LLA is O(|S|)C. Under the assumption that the number of parameters per layer in N is P, the number of parameters in M with LLA is P + |S|.\nSimilarly, in the case of M with QLA the per iteration computation cost is O(|S|)C + O(|S|2) and the number of parameters per layer in M with QLA is P + O(|S|2)"}, {"title": "EXPERIMENTS", "content": "We evaluate our extensions on eight test functions taken from http://www.sfu.ca/\n~ssurjano/index.html and defined in Table 1. All these test functions are rather complex and pose significant difficulty for the learning process especially for vanilla activation functions.\nOur experimental setting is as follows. For all these test functions our experimental evaluation is in dimension d = 2. We fixed the seed to be 13. We choose a simple feed-forward neural network with 2 input nodes, 2 hidden layers with 20 hidden nodes each and 1 output node with default initialization of weights and biases as described in https://pytorch.org/docs/stable/\ngenerated/torch.nn.Linear.html.\nFor LLA and QLA the corresponding parameters are initialized with Kaiming normal initialization as described in https://pytorch.org/docs/stable/nn.init.html#torch.nn.\ninit.kaiming_normal_and the library chosen is S = {ReLU, GELU, Tanh, Sigmoid}\nWe set the learning rate as 0.001, and ran the experiment for each test function for 500 epochs with a batch size of 256. Our loss function is Mean Squared Error. We have chosen the Adam optimizer for the learning process and a dataset of 15000 points for each test function is generated through a quasi montecarlo process that is described in https://docs.scipy.org/\ndoc/scipy/reference/generated/scipy.stats.qmc.Halton.html. We measure the performance in terms of Mean Absolute Error (MAE) and Mean Squared Error (MSE) and summarize in Table 2\nIt is easily seen that in most cases LLA and QLA are superior to individual activations and QLA is superior to LLA, in alignment with the analysis of Section 3. The differences in the learning process are noticed visually as well. For example, observe the differences between learned surfaces for Shubert function on a grid of test data in the case of ReLU vs QLA shown in the Figure 1."}, {"title": "EXPERIMENTS ON REAL-WORLD TIME SERIES DATASETS", "content": "We choose four real-world time series benchmark datasets ETTh1, ETTh2, ETTm1 and ETTm2 available at https://github.com/zhouhaoyi/ETDataset/tree/main/\nETT-small. ETTh1 and ETTh2 have measurements at hourly frequency, while ETTm1 and ETTm2 have measurements at 15 min frequency. Each dataset has timestamped measurements of 7 features of electricity transformers namely, HUFL (High UseFul Load), HULL (High UseLess Load), MUFL (Middle UseFul Load), MULL (Middle UseLess Load), LUFL(Low UseFul Load), LULL (Low UseLess Load) and OT (Oil Temperature).\nIn the case of each dataset, given 512 historical points of these features, the task is to forecast 96 points into the future for all the features. For the forecasting task, our network consists of 512 \u00d7 7 dimensional input layers. 96 \u00d7 7 dimensional output layer and 2 hidden layers of dimension 7 each. We choose QLA configuration for activations with S = {ReLU, GELU, Sine, Cosine} as the library. For the reproducibility of results we set the seed as 36 for all the experiments. Our initial-ization for weights and biases is default and for QLA parameters it is Kaiming normal as described in the earlier experiment for synthetic test functions. Each dataset spans over 2 years. We have chosen the last 4 months for the test dataset. The training and validation datasets comprise of first 16 months and 16 - 20 months of data and the dataset is normalised for the learning process. We chose MSE as the error metric. We compare QLA against vanilla ReLU activation for these datasets. The performance metrics are summarized in the Table 3 Based on the metrics in Table 3, it is easily seen that QLA outperforms or on par with ReLU on the chosen time series datasets. Also, as is evident from Figure 2, optimization of neural networks under LLA/QLA is extremely non-convex. Hence choice of initial point, stopping criterion etc., have a significant role in the optimization process. For example, we have observed it in the case of ETTm2 dataset, on introducing an early stopping criterion based on validation dataset, we observe that MSE of QLA drops to 0.197 surpassing ReLU.\nOne of the reasons for this improvement is the exploitation of periodicity in the datasets. Consider the plots shown in Figure 3 for a typical sample point of HULL in ETTh1 test dataset. Green plot is the ground truth, the last 96 points of the red plot are the predictions and the first 512 points form the history utilized for forecasting. It is evident that QLA models periodicity in the data much better than ReLU most likely due to the presence of sin(x) and cos(x) in the library."}, {"title": "CONCLUSION AND FUTURE WORK", "content": "We have shown that expansion of the search space through extensions is a framework that boosts the performance of neural networks. Based on the framework we proposed LLA and QLA that are extensions and analysed their performance on synthetic as well as real world datasets. Similar to LLA and QLA, exploring higher-order extensions like cubic learnable activation, node specific learnable activations, unlike layer specific activations in this work, are promising directions for future work. We noticed encouraging benefits in the case of cubic learnable activation, however, the optimization process faces convergence issues and potentially requires additional constraints on the elements of library S. Initialization strategies, based on the elements of the library S, for learnable activation parameters to avoid local optima and applications in relevant domains are other promising future directions."}]}