{"title": "ACTIVATIONS THROUGH EXTENSIONS: A FRAMEWORK TO BOOST PERFORMANCE OF NEURAL NETWORKS", "authors": ["Chandramouli Kamanchi", "Kameshwaran Sampath", "Arindam Jati", "Vijay Ekambaram", "Sumanta Mukherjee", "Pankaj Dayama", "Dzung Phan"], "abstract": "Activation functions are non-linearities in neural networks that allow them to learn complex mapping between inputs and outputs. Typical choices for activation functions are ReLU, Tanh, Sigmoid etc., where the choice generally depends on the application domain. In this work, we propose a framework/strategy that unifies several works on activation functions and theoretically explains the performance benefits of these works. We also propose novel techniques that originate from the framework and allow us to obtain \"extensions\" (i.e. special generalizations of a given neural network) of neural networks through operations on activation functions. We theoretically and empirically show that \"extensions\" of neural networks have performance benefits compared to vanilla neural networks with insignificant space and time complexity costs on standard test functions. We also show the benefits of neural network \"extensions\" in the time-series domain on real-world datasets.", "sections": [{"title": "INTRODUCTION", "content": "Current literature on machine learning and artificial intelligence is filled with triumphs of neural networks over other alternatives. A significant portion of these successes could be attributed to the design of novel and innovative activation functions. Here we comment on articles that are relevant to our work.\nReLU, Sigmoid, and Tanh are arguably the most common activation functions of choice in artificial neural networks of which ReLU is more predominant for its simplicity and computational efficiency. Several enhancements and modifications for these activation functions are proposed in the literature. For example Maas et al. (2013) proposes Leaky ReLU and shows performance improvement over ReLU on benchmark tasks in the acoustics domain. In He et al. (2015), authors come up with a parametrization of ReLU (PReLU). It is shown that PReLU, along with a novel initialization procedure, surpasses human-level performance on the ImageNet classification dataset. Of late, Maniatopoulos & Mitianoudis (2021) introduces Learnable Leaky ReLU (LeLeLU), which is a further parameterized variant of Leaky ReLU."}, {"title": "BACKGROUND", "content": "In this section we introduce notation and review feedforward neural networks and statistical learning theory very briefly."}, {"title": "FEEDFORWARD NEURAL NETWORKS", "content": "A vanilla feedforward neural network Haykin (2009) (FNN) consists of L layers. Each layer has $M_l, 1 \\leq l < L$ hidden nodes. Given an input vector $x[1: t] = [x_1, x_2,\\cdots, x_t]^\\prime$, the output $\\hat{y}$ of FNN is obtained as follows. In the first layer, we construct pre-activation $h^1 = W^1 * x[1: t] + b^1$ where the weight matrix $W^1 \\in \\mathbb{R}^{M_1 \\times t}$ and the bias vector $b^1 \\in \\mathbb{R}^{M_1}$. The pre-activation is transformed coordinate by coordinate via a differentiable nonlinear activation function $a^1$ to obtain $f^1 = a^1(h^1)$. For every subsequent layer $i, 2 \\leq i \\leq L - 1$, the output from the previous layer $f^{l-1}$ is transformed to obtain the output of the current layer, $f^l = a^l(W^l * f^{l-1} + b^l)$, with $W^l \\in \\mathbb{R}^{M_i \\times M_{i-1}}$ and $b^l \\in \\mathbb{R}^{M_l}$. In the final layer, $l = L$, of the neural network the output $\\hat{y}$ is obtained as $\\hat{y} = a^L(W^L * f^{L-1} + b^L)$ where $W^L \\in \\mathbb{R}^{1 \\times M_{L-1}}$ and $b^L \\in \\mathbb{R}$. It is well-known that a single hidden layer neural network with an arbitrary number of hidden nodes is a universal approximator Haykin (2009). However, in practical scenarios, multi-layer neural networks are adopted. To learn a complex relationship between input and output, we search the space of weight matrices and biases for ideal parameters by optimizing a carefully chosen loss measure on a dataset."}, {"title": "SOME ELEMENTS OF STATISTICAL LEARNING THEORY", "content": "Statistical Learning Theory Vapnik (2013) is at the foundation of most machine learning algorithms. A key problem, the discipline addresses is the following. Given a parameterized family of functions $H = \\{f(x, \\theta), \\theta \\in \\Theta\\}$, a joint distribution of random variables $X, Y$, denoted $p(x, y)$ and a loss function $L$, the discipline explores the conditions that are necessary and sufficient to solve the optimization problem\n$\\min_{\\theta \\in \\Theta} E_p[L (y, f (x, \\theta))]]$\nthrough samples generated from the joint distribution $p(x, y)$.\nIn many practical scenarios, one is typically interested in improving the state of the art model performance on benchmark datasets (i.e., to improve $E_p[L]$). A probable way to accomplish this task is to expand the search space (i.e. search in $H^\\prime \\supset H$ as $\\min_{H^\\prime} E_p[L (y, f(x,\\theta))] \\leq \\min_H E_p[L (y, f(x, \\theta))]]$). Many works in the literature He et al. (2015); Maniatopoulos & Mitianoudis (2021); Biswas et al. (2021); Manessi & Rozza (2018) expand the search space through \"extentions\" albeit without the explicit mention of the same. In what follows, we formally define \"extentions\", state and prove their properties, point out some extensions in the literature and define a few novel extensions and demonstrate their benefits on synthetic as well as real-world datasets."}, {"title": "ANALYSIS", "content": "In this section, we define extensions and proceed to state and prove their properties. We start with the definition of an extension."}, {"title": "PROPERTIES", "content": "In this subsection we state and prove properties of extensions and define our neural network extensions."}, {"title": "LEARNABLE ACTIVATIONS", "content": "We call the activations given by\n$g_i = \\lambda^T_i \\alpha$. (1)\n$g_i = \\alpha^T A_i \\alpha + \\lambda^T_i \\alpha$. (2)\nas Linear Learnable Activation (LLA) and Quadratic Learnable Activation (QLA) respectively\nRemarks 6. The choice of learnable activation (LLA or QLA) in a neural network is not hyper parameter optimization. We are not choosing the best activation functions for the neural network. The hyper parameter optimization in this context is the choice of the activation library S.\nRemarks 7. To ensure that the convergence properties of M are similar to N, constraints - i.e., $1^T 1A_i = 1$ and $\\lambda_i \\geq 0$ are enforced.\nRemarks 8. As the extensions get complicated, the optimization process complexity and the number of parameters increase"}, {"title": "TIME AND SPACE COMPLEXITY", "content": "Given the library of activation functions $S := \\{ReLU, GELU, Tanh, Sigmoid, ... \\}$ and per iteration computation cost, C, of N, the per iteration computation cost of M with LLA is $O(|S|)C$. Under the assumption that the number of parameters per layer in N is P, the number of parameters in M with LLA is P + |S|.\nSimilarly, in the case of M with QLA the per iteration computation cost is $O(|S|)C + O(|S|^2)$ and the number of parameters per layer in M with QLA is $P + O(|S|^2)$"}, {"title": "EXPERIMENTS", "content": "We evaluate our extensions on eight test functions taken from http://www.sfu.ca/ ~ssurjano/index.html and defined in Table 1. All these test functions are rather complex and pose significant difficulty for the learning process especially for vanilla activation functions.\nOur experimental setting is as follows. For all these test functions our experimental evaluation is in dimension d = 2. We fixed the seed to be 13. We choose a simple feed-forward neural network with 2 input nodes, 2 hidden layers with 20 hidden nodes each and 1 output node with default initialization of weights and biases as described in https://pytorch.org/docs/stable/ generated/torch.nn.Linear.html.\nFor LLA and QLA the corresponding parameters are initialized with Kaiming normal initialization as described in https://pytorch.org/docs/stable/nn.init.html#torch.nn. init.kaiming_normal_and the library chosen is S = {ReLU, GELU, Tanh, Sigmoid}\nWe set the learning rate as 0.001, and ran the experiment for each test function for 500 epochs with a batch size of 256. Our loss function is Mean Squared Error. We have chosen the Adam optimizer for the learning process and a dataset of 15000 points for each test function is generated through a quasi montecarlo process that is described in https://docs.scipy.org/ doc/scipy/reference/generated/scipy.stats.qmc.Halton.html. We measure the performance in terms of Mean Absolute Error (MAE) and Mean Squared Error (MSE).\nIt is easily seen that in most cases LLA and QLA are superior to individual activations and QLA is superior to LLA, in alignment with the analysis of Section 3. The differences in the learning process are noticed visually as well. For example, observe the differences between learned surfaces for Shubert function on a grid of test data in the case of ReLU vs QLA shown in the Figure 1."}, {"title": "EXPERIMENTS ON REAL-WORLD TIME SERIES DATASETS", "content": "We choose four real-world time series benchmark datasets ETTh1, ETTh2, ETTm1 and ETTm2 available at https://github.com/zhouhaoyi/ETDataset/tree/main/ ETT-small. ETTh1 and ETTh2 have measurements at hourly frequency, while ETTm1 and ETTm2 have measurements at 15 min frequency. Each dataset has timestamped measurements of 7 features of electricity transformers namely, HUFL (High UseFul Load), HULL (High UseLess Load), MUFL (Middle UseFul Load), MULL (Middle UseLess Load), LUFL(Low UseFul Load), LULL (Low UseLess Load) and OT (Oil Temperature).\nIn the case of each dataset, given 512 historical points of these features, the task is to forecast 96 points into the future for all the features. For the forecasting task, our network consists of 512 \u00d7 7 dimensional input layers, 96 \u00d7 7 dimensional output layer and 2 hidden layers of dimension 7 each. We choose QLA configuration for activations with S = {ReLU, GELU, Sine, Cosine} as the library. For the reproducibility of results we set the seed as 36 for all the experiments. Our initialization for weights and biases is default and for QLA parameters it is Kaiming normal as described in the earlier experiment for synthetic test functions. Each dataset spans over 2 years. We have chosen the last 4 months for the test dataset. The training and validation datasets comprise of first 16 months and 16 - 20 months of data and the dataset is normalised for the learning process. We chose"}, {"title": "CONCLUSION AND FUTURE WORK", "content": "We have shown that expansion of the search space through extensions is a framework that boosts the performance of neural networks. Based on the framework we proposed LLA and QLA that are extensions and analysed their performance on synthetic as well as real world datasets. Similar to LLA and QLA, exploring higher-order extensions like cubic learnable activation, node specific learnable activations, unlike layer specific activations in this work, are promising directions for future work. We noticed encouraging benefits in the case of cubic learnable activation, however, the optimization process faces convergence issues and potentially requires additional constraints on the elements of library S. Initialization strategies, based on the elements of the library S, for learnable activation parameters to avoid local optima and applications in relevant domains are other promising future directions."}]}