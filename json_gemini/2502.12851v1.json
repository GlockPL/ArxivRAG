{"title": "MeMo: Towards Language Models with Associative Memory Mechanisms", "authors": ["Fabio Massimo Zanzotto", "Elena Sofia Ruzzetti", "Giancarlo A. Xompero", "Leonardo Ranaldi", "Davide Venditti", "Federico Ranaldi", "Cristina Giannone", "Andrea Favalli", "Raniero Romagnoli"], "abstract": "Memorization is a fundamental ability of transformer-based Large Language Models, achieved through learning. In this paper, we propose a paradigm shift by designing an architecture to memorize text directly, bearing in mind the principle that memorization precedes learning. We introduce MeMo, a novel architecture for language modeling that explicitly memorizes sequences of tokens in layered associative memories. By design, MeMo offers transparency and the possibility of model editing, including forgetting texts. We experimented with the MeMo architecture, showing the memorization power of the one-layer and the multi-layer configurations.", "sections": [{"title": "1 Introduction", "content": "Transformer-based Large Language Models achieve unrivaled performance in language modeling by learning to capture and represent complex sequential dependencies from statistical patterns through extensive training phases that iteratively refine their weights to best approximate natural language. This has triggered significant interest in gaining a better understanding of the inner workings of these models, focusing on how these models generalize and capture structure between similar samples in terms of syntactic dependencies (Vig and Belinkov, 2019), compositional relations (Hupkes et al., 2020; Zanzotto et al., 2015) concerning the quantity (Reizinger et al., 2024) and quality (Yang et al., 2024) of training data. Besides generalization, a key component of Transformers' success is the ability to memorize data while learning (Ranaldi et al., 2023b,a). Indeed, earlier work investigated this other side of learning. While Carlini et al. (2023); Mahdavi et al. (2024) demonstrated evidence of memorization, Kharitonov et al. (2021); Mahdavi et al. (2024) studied how the internal components lead to memorization, and Kim et al. (2023) estimated the boundary between generalization and memorization, providing an estimation on their storage capacity. Memorization is not inherently a drawback in language models because it plays a crucial role in handling factual knowledge, which is important for question answering, summarization, or information retrieval. This factual recall relies on a delicate balance. While generalization helps capture patterns and unseen relationships in data, memorization ensures that models retain critical and exact information when required.\nRecent research has highlighted that memorization capability can be effectively harnessed using concepts rooted in associative memories (Kohonen, 1972; Anderson, 1972) - a system designed to link inputs to specific outputs and offers a structured and transparent way to store and retrieve information. By leveraging associative memory mechanisms, researchers have proposed strategies to post-edit LLMs (Meng et al., 2022, 2023a), enabling control over what is memorized, how it is stored, and how it is accessed, enhancing their reliability in fact-based tasks.\nIn this paper, we propose a paradigm shift by designing Language Models based on a different principle: memorization proceeds learning. By using associative memories, we build MeMo, a novel architecture for language modeling that explicitly memorizes sequences of tokens in layered associative memories. MeMo leverages correlation matrix memories (Kohonen, 1972; Anderson, 1972), the concept that tokens and sequences of tokens can be represented as random vectors (Plate, 1995; Sahlgren, 2005), and the Johnson-Lindestrauss Transform to embed larger vectors in smaller spaces by preserving their distances (Johnson and Lindenstrauss, 1984). By design, MeMo offers transparency and the possibility of model editing, including forgetting texts. We experimented MeMo, showing the memorization power of single and multi-layer architecture."}, {"title": "2 Preliminaries and Background", "content": "Representing words or tokens in small random vectors is the first important step in building language models with neural network architectures. Using random vectors is a standard technique. Indeed, random vectors are used in random indexing (Sahlgren, 2005) in information retrieval to reduce the document vector space and in distributed representations for neural networks as a convenient way to determine a set of vectors to represent sets of different tokens (Plate, 1995) or structures (Zanzotto and Dell'Arciprete, 2012; Zanzotto and Ferrone, 2017; Zanzotto et al., 2020). Moreover, random vectors are used to initialize weight matrices in any language-oriented application in neural networks, including the initialization of transformers (Vaswani et al., 2017) to build large language models from scratch.\nMultivariate Gaussian random vectors have the important property of being able to generate sets E of nearly orthogonal unitary vectors that can form an approximate base of the space $R^n$ in a smaller space $R^d$ (Johnson and Lindenstrauss, 1984). Each token t is then represented with a distinct vector in $t \\in E$, and the two following properties hold with a probability larger than 1 \u03b4:\n$$\n||a^T b|| < \\epsilon \\quad \\text{if } a \\neq b\n$$\n$$1 - \\epsilon < a^T b < 1 + \\epsilon \\quad \\text{if } a = b$$\nwhere a and b are tokens and a and b are vectors representing those tokens in the reduced space $R^d$. By using the Johnson-Lindestrauss Lemma (Johnson and Lindenstrauss, 1984), it is possible to find a lower bound of how large d should be in order to host n vectors given the approximation \\epsilon and the probability factor 8 (see Appendix A). In less precise equations, the two properties can be rewritten as:\n$$ a^T b \\approx \\begin{cases} 0 & \\text{if } a \\neq b \\\\ 1 & \\text{if } a = b \\end{cases} $$\nUsing these vectors with their properties, it is possible to represent a bag-of-tokens B in a single vector to offering the operation that approximately counts the number of times a token is in B. The vector $t_B$ is obtained by summing up vectors representing tokens in B and, then, the counting operation is:\n$$ a^T t_B \\approx k $$\nwhere k is the number of times a belongs to the bag B.\nCorrelation matrix memories (CMMs) (Kohonen, 1972; Anderson, 1972) are a powerful tool to store key-value $(k_i, v_i)$ pairs in distributed memories as the sum of outer products of the vectors representing the keys $k_i$ and vectors representing the values $v_i$:\n$$C = \\sum_{i=1}^n k_i v_i^T$$\nThese CMMs have been generally defined on one-hot representations (Hobson, 2011) and, eventually, reduced afterwards (Kohonen, 1972). Then, to retrieve the value associated with a key, the matrix C should be multiplied with k. As vectors $k_i$ are one-hot vectors, the following property holds:\n$$k^T_j C = v^T_j$$\nTo optimize the construction of these CMM matrices, we use the correlated form:\n$$C = K V^T = \\begin{bmatrix} k_1 & k_2 & ... & k_n \\end{bmatrix} \\begin{bmatrix} V_1^T \\\\ V_2^T \\\\ ... \\\\ V_n^T \\end{bmatrix}$$\nTo make CMMs practical, in MeMo, we use these memories along with the multivariate Gaussian vectors to represent keys and values. Hence, the generic property of this associative matrices is\n$$k_j^T C \\approx e_j^T V = V_j$$\nwhere $e_j$ is the onehot vector of the position j and $k_j$ and $V_j$ are multivariate Gaussian vectors to represent the key $k_j$ and the value $V_j$.\nThe idea behind correlation matrix memories has often been used to explain that feed-forward matrices are where Transformer architectures store most information (Meng et al., 2023b). In MeMo, CMMs become the cornerstone for defining a novel approach to building Language Models.\nJohnson-Lindestrauss Transform (Dasgupta and Gupta, 1999), derived by using the Johnson-Lindestrauss Lemma (JLL) (Johnson and Lindenstrauss, 1984), guarantees that it exists a linear transformation $T_{d\\times n}$ that transforms vectors in a bigger space $R^n$ in vectors in a smaller space $R^d$ by preserving their distance with an approximation \\epsilon. Then, given two vectors a and b in $R^n$, the following property is guaranteed:\n$$ ||a - b|| - \\epsilon < ||T a - T b|| < ||a - b|| + \\epsilon $$"}, {"title": "3 MeMo: Language Models with Multi-layer Correlation Matrix Memories", "content": "Building on Correlation Matrix Memories, on multi-variate Gaussian vectors to represent tokens and token sequences, and on Johnson-Lindestrauss Transforms, we present here MeMo\u00b9, a way to build language models that memorize texts in a clear, transparent way. We first present how to build a language model with a single CMM (Sec. 3.1). This single-layer CMM language model predicts the next tokens of sequences with a fixed length h. Then, we generalize MeMo to a multi-layer approach in order to increase the length of the sequences that can be memorized, retrieved, and forgotten (Sec. 3.2)."}, {"title": "3.1 Language Models with single Correlation Matrix Memories", "content": "Correlation matrix memories (CMMs) and multivariate Gaussian vectors with their properties offer an interesting opportunity to build simple language models.\nLanguage models can be seen as predictors of the next tokens given input sequences. From a symbolic perspective, a language model stores the associations between sequences and the next tokens along with the observed frequency in order to estimate the probability. Then, from a symbolic perspective, the base for a language model is a multi-set LM containing:\n$$LM = {([x_1,x_2, ..., x_h], y)} = {(s, y)}$$\nwhere $s = [x_1, x_2, ..., x_h]$ are the fixed length sequences of tokens and y are the next tokens implied by sequences s. Tokens are contained in a fixed vocabulary V of n tokens. These multisets are the sample sets where probabilities are estimated by counting.\nThe translation of these multi-sets LM in a CMM is straightforward: input sequences s are"}, {"title": "3.2 Multi-layer Correlation Matrix Memories", "content": "To increase the maximum length of the input window of language models, in line with what is done in transformers (Vaswani et al., 2017), we stack layers containing correlation matrix memories (see Fig. 2 for an example).\nThe driving idea is that CMMs of a generic MeMo layer store the encoding of sequences whose length is determined by the level of the layer. Hence, the generic MeMo layer contains key-value pairs where the key is the representation of the sequence elements, and the value is a vector representing the sequence as a whole. The representation of the sequence elements is done similarly to what is done for an LM based on a single CMM (as in Sec. 3.1). The last MeMo layer instead stores the relation between sequences of increasing length and the next token, and, thus, it is the layer devoted to the next token prediction.\nTo define MeMo, we need first to fix the notation: h is the number of heads or, also, the maximum number of input elements that are treated by the"}, {"title": "4 Experimental Investigation", "content": "In this section, we experiment the memorization capacity of MeMo with a single layer and with multiple layers."}, {"title": "4.1 Exploring Memorization Capabilities of Single-layer MeMo", "content": "Experimental set-up In the first experiment, we investigate the capacity of a single-layer MeMo to memorize the association between sequences of symbols and one output symbol. Hence, we created a generator of random sequences of h symbols $[X_1, X_2, ..., X_h]$ that are mapped to a random symbol y. To maximize the diversity, symbols are taken with a uniform random distribution from a vocabulary of 100,000 symbols. This guarantees that the mapping between sequences and symbols is unique. Therefore, we are testing the real capacity of memorization of the CMM. In the experiments, we used random vectors $x_i$ representing symbols $x_i$ with d dimensions with $dh \\in {16, 32, 64, 128, 256}$ and we experimented with sequences of increasing length with $h \\in {2,4,8,16,32}$. The output vectors y representing symbols y are instead random vectors with d in {512, 1024, 2048, 4096, 8192}. Therefore, experimental CMMs are matrices with $(h \\times dh, d)$ dimensions. Thus, the number of parameters of each CMM is $NoP = h\\cdot dh\\cdot d$.\nIn this experiment, batches $B_i$ of 1,000 pairs $\\{([X_1, X_2, ..., X_h], y)\\}$ are stored into the CMM matrix C for each step i and, then, the storing capacity is evaluated by computing the accuracy of reproducing the tokens of the batch $B_i$ and the first batch $B_0$. The accuracy $Acc(B_i, C)$ of the CMM C on the batch $B_i$ is computed as the percentage of correct emitted tokens y given sequences $[X_1, X_2, ..., X_h]$ with equation 4. The storing capacity of a CMM matrix C is computed as the number of pairs that can be stored that guarantee an"}, {"title": "4.2 Exploring Memorization Capabilities of Multi-layer MeMo", "content": "Experimental set-up In the second experiment, we investigate the capacity of MeMo to memorize complete texts. As we aim to investigate only the memorization capacity, we used randomly generated texts of a given chunk length. To really test the capacity of splitting the ability to store long sequences with a layered model, we produced a text generator that simulates the existence of repeated words long h tokens in the text. These repeated words decoy a memorizer with only h heads because the same decoy of h tokens should produce different next tokens according to the tokens preceding the decoy, which may be captured only if MeMo with more layers is memorizing sequences"}, {"title": "5 Conclusion and Future Work", "content": "Memorization is a key component of transformer-based Large Language Models. Hence, in this paper, we proposed to shift the paradigm by designing language models based on memorization. We then presented MeMo as a novel way to build language models using correlation matrix memories stacked in layers. Experimental evaluation has shown that MeMo-like architecture can memorize sequences of tokens.\nBy using memorization, MeMo-like architectures are transparent and editable by design and opens back the possibility to include explicit knowledge modeling in neural network language models. Indeed, MeMo can help leverage traditional linguistic studies in this era, where transformer-based large language models are obtaining unprecedented performance. With MeMo, we could control how linguistic knowledge is used to generalize examples, we could embed transformation rules, and we could represent knowledge graphs and linguistic ontologies. In other words, MeMo gives back control to knowledge experts, linguists, and NLP practitioners with the aim of reducing data hungriness of Large Language Models."}, {"title": "Limitations", "content": "The approach proposed in this paper is a paradigm shift, and then, the software implementing the model has some compatibility issues with the existing software ecosystem of transformers in Hugging Face. Hence, it has not been possible to experiment with the model using the current evaluation suites. Although this is a limit with respect to the comparability of MeMo with current transformer-based LLMs, it does not represent a major limit concerning the memorization capability of MeMo."}, {"title": "Ethical Statement", "content": "Making memorization more evident and being editable by design, MeMo may allow an easier control of the stored texts by mitigating leaks of sensible data and social biases."}, {"title": "Appendix A: Analyzing Storing Capacity of Random Vectors", "content": "This section explores theoretically how many nearly orthogonal unit vectors can be stored in a set NOV (\u03b5, \u03b8) in the space Rd, where \u025b is the approximation required and 1 \u2013 0 is the probability that this approximation is guaranteed. For two vectors a and b in NOV (\u03b5, \u03b8), the following should hold:\n$$ P[l_a, eb - \\epsilon < a^T b < la, eb + \\epsilon] = 1 \u2212 \u03b6 $$$$ Pl(a^T, b + \u03b6] \u2265 1 \u2212 \u03b8 $$\nIn other terms, if a and b are the same generalized sequence, $ab^T \\approx 1$, whereas, if if a and b are two different generalized sequences, $ab^T \\approx 0$. There is a long-lasting conjecture that postulates a relation between d and m for any given \u03b8 and \u025b (Hecht-Nielsen, 1994) but, to the best of our knowledge, a definitive demonstration does not still exist. By using the Johnson&Lindestrauss Lemma (Johnson and Lindenstrauss, 1984), we derived an upper-bound for d. Sets NOV (\u03b5, \u03b8) can potentially host\u00b2 m vectors with 0 = 2/m\u00b2 \u2013 1/m\u00b2 according to this relation:\n$$m < e^{8(e^2-4/3e^3)d}$$\nThus, there is an exponential relation between d and m. This is a positive result as spaces $R^d$ can host large sets of NOV (\u03b5, \u03b8).\nThus, definitely many substructures in S in real datasets can be represented with vectors in NOV (\u03b5, \u03b8).\nExisting results Our corollary stems from two results (Johnson and Lindenstrauss, 1984; Dasgupta and Gupta, 1999):\nTheorem .1 (Johnson-Lindenstrauss Lemma). For any 0 < \u0454 < 1 and any integer m. Let d be a positive integer such that\n$$d \u2265 4(\u20ac^2/2 \u2013 3/3)^{-1}1n m$$\nThen for any set V of m points in Rk, there is a map f : Rk \u2192 Rd such that for all u, v \u2208 V,\n$$(1 - \u20ac)||u - v|| \u2264 ||f(u) - f(v)||3 \u2264 (1 + \u20ac)||u - v||3.$$\n$$m\u2264e^{\\frac{(\\epsilon^2/2-3/3)d}{4}}$$\nThe theorem can be derived using the following lemma:\nLemma .2. For any \u0454 > 0, \u03c4 < 1/2 and positive integer d, there exists a distribution D over Rd\u00d7k for d = O(\u0454\u00af\u00b2 log 1/\u03c4) such that, for any x \u2208 Rk with ||x||2 = 1,\n$$ P(| ||Ax||3 \u2013 1| > \u0454) < \u03c4 $$\nby choosing \u03c4 = 1/m\u00b2 and by applying the union bound on the vectors (u \u2013 v)/||u \u2013 v||2 for all vectors u and v in V. It is possible to demonstrate that there is a probability strictly greater than 0 that a function f exists.\nOur Corollary Now we can demonstrate that the following lemma holds:\nCorollary .3. For any 0 < \u0454 < 1 and any integer m. Let d be a positive integer such that\n$$d \u2265 4(\u20ac^2/2 \u2013 \u20ac^3/3)^{-1} ln m$$\nThen given the standard basis E of Rm, there is a map f : Rm \u2192 Rd such that for all ei, ej \u2208 E,\n$$P(1 - \u20ac < || f(ei)||2 < 1 + \u20ac) > 1 -\u03c4=1-1/m\u00b2$$\nand\n$$P(|f (ei) \u00b7 f(ej)| < 2\u20ac) > (1 \u2013 \u03c4)\u00b2 = (1 \u2013 1/m\u00b2)\u00b2$$"}]}