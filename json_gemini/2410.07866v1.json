{"title": "System-2 Reasoning via Generality and Adaptation", "authors": ["Sejin Kim", "Sundong Kim"], "abstract": "While significant progress has been made in task-specific applications, current models struggle with deep reasoning, generality, and adaptation-key components of System-2 reasoning that are crucial for achieving Artificial General Intelligence (AGI). Despite the promise of approaches such as program synthesis, language models, and transformers, these methods often fail to generalize beyond their training data and to adapt to novel tasks, limiting their ability to perform human-like reasoning. This paper explores the limitations of existing approaches in achieving advanced System-2 reasoning and highlights the importance of generality and adaptation for AGI. Moreover, we propose four key research directions to address these gaps: (1) learning human intentions from action sequences, (2) combining symbolic and neural models, (3) meta-learning for unfamiliar environments, and (4) reinforcement learning to reason multi-step. Through these directions, we aim to advance the ability to generalize and adapt, bringing computational models closer to the reasoning capabilities required for AGI.", "sections": [{"title": "Introduction", "content": "Nowadays, Al has made significant strides in task-specific applications, mainly through the success of neural networks [15, 28, 41, 61]. Furthermore, AI models based on language models [1, 6] and reinforcement learning models [25, 51] are achieving human-level performance with surprising results on several tasks. However, achieving human-level deep and logical reasoning, referred to as System-2 reasoning [29], a concept central to human cognition and intelligence, remains an open challenge. System-2 reasoning is characterized by abstract thought, logical deduction, and the ability to adapt to novel and complex situations, critical components of AGI.\nThere is growing recognition that for AI to meet the reasoning requirements of AGI, it must excel in generality and adaptation [24], which are two fundamental capabilities that enable models to handle unseen tasks and unpredictable environments. Generality helps AI apply learned knowledge to new contexts, while adaptation enables flexible responses to changing scenarios [14]. These capabilities are also crucial for System-2 reasoning because they are fundamental to human-like intelligence [33].\nDespite recent advancements, current AI models are limited by their reliance on training data and task-specific optimization [57]. Neural networks such as Large Language Models (LLMs) [17, 23, 36], and Model-Based RL (MBRL) models [51], while highly capable in constrained settings, often fail to generalize effectively beyond their training distributions. This has led to a critical gap between existing AI models and the level of reasoning required for AGI. In particular, most models demonstrate System-1 reasoning, which refers to fast, pattern-based decision-making, while lacking the deliberative and adaptable qualities inherent in System-2 reasoning [5, 11].\nTo bridge this gap, it is essential to identify strategies that foster both generality and adaptation in AI models. The Abstraction and Reasoning Corpus (ARC) [14] has been proposed as a benchmark specifically designed to assess an AI model's ability to generalize and reason abstractly in unfamiliar"}, {"title": "Backgrounds", "content": ""}, {"title": "Two Modes of Reasoning: System-1 and System-2", "content": "The concepts of System-1 and System-2 reasoning, introduced in psychology, describe two distinct modes of thinking [29]. System-1 is fast and intuitive, relying on automatic decision-making based on pattern recognition. Humans use System-1 in everyday situations that require little cognitive effort, such as driving or performing simple arithmetic, where decisions are made quickly based on learned experiences. In AI, this can be emulated using neural networks that learn rapidly and, when combined with tree search, replicate fast decision-making [5]. However, System-1 models work well in familiar environments but struggle with new situations and complex tasks.\nOn the other hand, System-2 reasoning involves slower, more deliberate thought processes that require logical deduction and abstract thinking. In contrast to System-1, System-2 is needed when encountering novel tasks that demand reasoning and planning, rather than relying on pattern recognition. This type of reasoning is essential for solving new challenges and adapting to dynamic environments, as is often the case when humans develop strategies for unfamiliar tasks. Introducing System-2 reasoning into AI would address current AI models' limitations in handling more abstract, logic-driven tasks and enhance their ability to reason more like humans [11].\nSystem-1 and System-2 differ in their cognitive mechanisms. System-1 is fast, intuitive, and pattern-based, while System-2 is slow, deliberate, and relies on logical reasoning. System-1 operates efficiently in familiar situations but lacks the flexibility to adapt to new environments, whereas System-2 is crucial for handling more complex, unfamiliar tasks. In AGI research, the integration of both systems is essential for achieving human-like reasoning capabilities, enabling AI to not only replicate learned behaviors but also think abstractly and adapt to novel challenges."}, {"title": "Generality and Adaptation: Two Key Components of AGI Reasoning", "content": "While System-2 reasoning enables deep, logical thought processes for handling complex and novel tasks, two critical components are required to fully realize its potential in AI models: generality and adaptation. For AI to achieve human-like intelligence, it must not only excel in task-specific skills but also demonstrate the ability to generalize and adapt across a wide range of environments and tasks. These two capabilities, generality and adaptation, are fundamental components of Artificial General Intelligence (AGI). Generality enables AI models to apply learned knowledge to novel contexts, allowing them to operate in unfamiliar situations without extensive retraining. Adaptation refers to the ability of AI to modify its behavior in response to changing environments, ensuring that it can handle new tasks and challenges as they arise.\nGenerality in AI is crucial because it allows models to move beyond narrow tasks and extend their capabilities to a broader range of problems. Rather than relying solely on patterns learned from training data, generality enables the system to extract underlying principles and apply them across different domains. For example, a model trained to recognize objects in images can generalize its knowledge to understand new categories with minimal supervision. The generality of an AI model $G(M,T, K)$ can be mathematically defined as:"}, {"title": null, "content": "$G(M,T,K) = \\frac{1}{N}\\sum_{i=1}^{N}P(T_i|K)$"}, {"title": null, "content": "In Eq. 1, $G(M, T, K)$ represents the ability of a model M to generalize across a set of tasks $T = {T_1, T_2,\u2026, T_N}$, given the domain knowledge K. $P(T_i|K)$ refers to the probability that the model can successfully complete a novel task $T_i$ based on the knowledge it has acquired. This formulation highlights how well the model can apply its learned knowledge to new tasks and situations, a key factor in achieving AGI-level reasoning. Achieving this level of generality requires AI to reason abstractly and draw connections between seemingly unrelated tasks, much like how humans operate.\nAdaptation, on the other hand, ensures that AI models can adjust to new conditions or tasks without requiring extensive re-engineering or retraining. It reflects the model's ability to maintain performance despite changes in the environment. For instance, a robotic system deployed in different physical spaces must adapt its behavior to account for various layouts, obstacles, and interactions. Without adaptation, even the most sophisticated AI models would struggle to maintain relevance in constantly evolving real-world environments, thus limiting their long-term effectiveness in AGI applications. The adaptation of an AI model $A(M, T, K, E)$ can be described as:"}, {"title": null, "content": "$A(M,T,K,E) = \\frac{1}{N}\\sum_{i=1}^{N}P(T_i|K, E_i)$"}, {"title": null, "content": "In Eq. 2, $A(M, T, K, E)$ defines the model's ability to adapt across tasks T in varying environmental conditions $E = {E_1, E_2,\u2026, E_N}$, given the domain knowledge K. The term $P(T_i|K, E_i)$ represents the likelihood that the model will successfully complete the task $T_i$ given both global domain knowledge K and the specific environmental condition $E_i$. This formulation emphasizes how the ability to adapt depends not only on the model's knowledge but also on its flexibility to respond to changes in the environment.\nThe synergy between generality and adaptation is what ultimately defines AGI's reasoning capabilities. A truly intelligent system must generalize from previous experiences while adapting to new challenges, thus combining both qualities in its reasoning process. Research into enhancing generality and adaptation is therefore essential for developing AI models that can approach the reasoning capabilities of humans and handle tasks that go far beyond simple pattern recognition."}, {"title": "ARC: A Benchmark for Measuring Generality and Adaptation", "content": "The Abstraction and Reasoning Corpus (ARC) is a benchmark to evaluate an AI model's ability to reason and generalize in ways that mirror human cognitive processes [14]. ARC tasks involve pairs of input and output grids, to predict the correct output for a new, unseen input based solely on a few example pairs. What makes ARC particularly challenging is that it requires systems to infer underlying abstract rules from minimal data, without the support of large datasets or domain-specific information. Each task tests the model's capacity for abstraction, pattern recognition, and flexible reasoning, and the tasks are structured in such a way that they cannot be solved by brute force or by memorizing patterns.\nARC is specifically designed to assess two core components of AGI, generality and adaptation. For an AI model to succeed in ARC tasks, it must generalize from limited input-output examples and adapt its strategies to solve new, unseen tasks. Thus, ARC challenges AI models to demonstrate both the ability to apply learned knowledge in novel contexts (generality) and the capacity to adjust to varying conditions or rules (adaptation). ARC is not only a benchmark for generality and adaptation but also serves as a comprehensive test for System-2 reasoning in AI. The tasks in ARC require the type of generality and adaptation that are central to System-2 reasoning, making it an ideal tool for evaluating the research directions proposed in this paper. By succeeding in ARC tasks, AI models can demonstrate progress towards achieving System-2 reasoning, which is a crucial step towards AGI."}, {"title": "Analysis of Existing Approaches", "content": "In addressing ARC tasks, several approaches in AI have been explored, including program synthesis, LLMs, and transformers. In the view of System-2 reasoning, while these approaches have advanced generalization, they still face difficulties when adapting to novel and unfamiliar tasks. By reviewing these methods, we aim to highlight their successes and identify areas where further research is needed to improve both generality and adaptation."}, {"title": "Program Synthesis Is Bounded in Generality and Adaptation by Initial DSLs", "content": "Program synthesis allows AI models to generate programs to solve tasks based on given specifications or data. In the context of ARC, program synthesis facilitates abstract reasoning and interpretability. However, ARC tasks present significant challenges in terms of generality and adaptation, which are both critical for solving novel and dynamic problems. Although research has made significant strides in improving program synthesis through neural methods, object-centric reasoning, and symbolic or logic-based synthesis, these approaches are fundamentally limited by their reliance on an initial domain-specific language (DSL), which constrains both generality and adaptation.\nNeural program synthesis has made strides in improving program generation by using neural networks to guide the process [2\u20134, 8, 9, 20, 26, 37, 38, 50, 65]. For example, one approach focused on creating interpretable programs [2], while another used a bidirectional search for efficiency [4]. Similarly, object-centric program synthesis leverages object relationships to guide program generation, focusing on enhancing generality by manipulating object characteristics across tasks [20, 37]. On the other hand, symbolic and inductive logic program synthesis applies logical reasoning to decompose ARC tasks into simpler components, improving generality through rule-based reasoning [9, 50]. However, across all these methods, the reliance on predefined DSL limits both generality and adaptation, particularly when faced with tasks that fall outside the scope of the initial DSL. Despite their successes, these approaches face adaptation challenges when problems require flexibility beyond the predefined rules and primitives of the DSL.\nThese studies highlight a fundamental challenge in program synthesis, where the initial DSL provided for generating programs influences both generality and adaptation. As shown in Eq. 1 and Eq. 2, the domain knowledge (K) in the form of DSLs directly impacts the model's ability to generalize to tasks and adapt to unfamiliar environments. Thus, both generality and adaptation of program synthesis are inherently tied to the expressiveness of the DSL, and future research must explore ways to create more dynamic and adaptable DSLs that allow AI to operate across a broader range of domains."}, {"title": "LLMs Demonstrate Strong Generality but Face Adaptation Constraints", "content": "Large Language Models (LLMs) have been successfully applied to a variety of tasks, including abstract reasoning tasks within the Abstraction and Reasoning Corpus (ARC). These models are particularly effective at recognizing patterns and generating solutions based on large-scale pre-training. In the context of ARC, LLMs have been employed to solve tasks by leveraging inductive reasoning, symbolic conversions, and hypothesis refinement. Despite their ability to generalize from large datasets, LLMs still encounter challenges when adapting to novel or unseen problems that require deeper, more abstract reasoning.\nSeveral recent studies have explored different ways to apply LLMs to ARC tasks [13, 22, 27, 40, 48, 53, 59, 62, 63, 66]. For example, one study used LLMs for hypothesis refinement, allowing models to adjust their generated hypotheses based on feedback [48]. Another study focused on converting symbolic problems into natural language explanations, where LLMs could generalize across symbolic reasoning tasks, but faced limitations when dealing with more abstract challenges [53]. In addition, some models have been applied to pattern recognition tasks, utilizing LLMs to identify and generate patterns, though these models were constrained by their reliance on pre-training [40].\nWhile LLMs show significant strengths in generality when solving ARC tasks, this generality ultimately stems from the vast amounts of data they are trained on (K), making it inevitable that they exhibit weaknesses with untrained data. This inherent limitation of LLMs raises doubts that they are not truly reasoning but rather performing a form of interpolation based on the data they have been trained on [36]."}, {"title": "Transformers Also Enhance Generality but Struggle with Adaptation", "content": "Transformers have demonstrated remarkable capabilities in fields such as natural language processing and image recognition. However, when applied to tasks in ARC, transformers often face challenges in terms of generality and adaptation-critical components for solving novel and dynamic problems. Several studies have aimed to adapt transformer models for ARC tasks, leveraging their powerful pattern recognition and attention mechanisms to address complex reasoning tasks. Nevertheless, these approaches remain constrained by their ability to generalize beyond training data and adapt to unfamiliar scenarios.\nVarious transformer-based models have been proposed to handle ARC tasks more effectively. One approach introduced symmetry priors into the attention mechanisms, allowing the model to handle geometric transformations and generalize across tasks sharing geometric rules [7]. Another model focused on object-centric reasoning, enhancing its generality by leveraging object detection and relationships to solve ARC tasks involving object manipulation [46]. A third study aimed to improve the model's counting abilities, a crucial aspect of quantitative reasoning in ARC, by refining its recognition of numerical patterns [44]. While these models have demonstrated notable improvements in handling specific ARC tasks, they are still limited by their reliance on predefined structures and priors, which restrict their adaptation to novel or abstract tasks.\nTransformer-based approaches in ARC have shown strengths in enhancing generality, particularly in tasks involving geometric transformations, object relationships, or quantitative reasoning. This aligns with the prior knowledge [14], which defines these concepts as foundational components of intelligence. However, these models continue to struggle with adaptation, as they are often constrained by predefined structures like symmetry priors, object detection, or counting mechanisms. Future research must explore ways to make transformers more adaptable to unseen tasks, moving beyond predefined patterns and rules to better handle the dynamic and abstract nature of ARC."}, {"title": "Proposed Research Directions Toward System-2 Reasoning", "content": "In the previous section, we explored how generality and adaptation are critical components of System-2 reasoning. While existing AI models have shown mixed results in terms of generality, they consistently struggle with adaptation, which limits their ability to handle novel and complex tasks. To address these limitations, this section proposes four research directions aimed at advancing System-2 reasoning. AI must be able to extract abstract knowledge (K), such as human intentions, from data and combine this knowledge in a generalized way, as seen in neuro-symbolic approaches. Additionally, meta-learning plays a crucial role in enabling AI to adapt efficiently across diverse environments and tasks, building on the foundation of learned models. Finally, reinforcement learning serves as a framework that integrates these research directions, providing the necessary structure to improve both generality and adaptation."}, {"title": "Learning Human Intentions from Data Supports Logical Deduction", "content": "To achieve System-2 reasoning, it is essential to establish a foundation for logical deduction and abstract thinking by capturing human intentions. Humans inherently perform System-2 reasoning through logical and abstract processes, and human intentions contain these crucial elements. Therefore, learning from human intentions, rather than merely imitating human actions, can provide a strong basis for advancing System-2 reasoning. Benchmarks like ARC [14], mini-ARC [30], 1D-ARC [66], ConceptARC [42], LARC [2], and MC-ARC [53] enable the collection of human behavior data to train AI models. Platforms like ARCreate Playground [31], O2ARC [52], ARC-Interactive [56], and ARC-Game [12] are useful for collecting such data. Additionally, environments like ARCLE [34] provide settings where models can learn from the collected data to improve reasoning and adaptability.\nOnce data is gathered, methods for extracting human intentions must be applied to enhance AI's logical deduction capabilities. Several techniques, including Topic Modeling [10], Sequential Pattern Mining (SPM) [55], and Hidden Markov Models (HMM) [49], have been developed to infer intentions from human behavior."}, {"title": null, "content": "Topic Modeling [10] clusters sub-sequences based on common patterns. It enables AI to infer goals by identifying recurring action sequences. Models must account for temporal dependencies to ensure that relationships between actions are preserved. Frequently co-occurring actions reveal intentions associated with specific patterns, aiding logical deduction and decision-making."}, {"title": null, "content": "Sequential Pattern Mining (SPM) [55] identifies frequently occurring action sequences and links state transitions to goals. SPM enhances logical deduction by anticipating actions based on observed patterns and tying actions to outcomes."}, {"title": null, "content": "Hidden Markov Models (HMM) [49] use probabilistic methods to model transitions between actions and hidden states. HMMs are effective for modeling temporal dependencies, inferring goals, and linking short-term actions to long-term objectives."}, {"title": "Combining Symbolic and Neural Models Improves Abstraction and Generality", "content": "Achieving System-2 reasoning requires models to move beyond simple pattern recognition and incorporate logical, structured reasoning capabilities. One promising approach to achieving this is through hybrid models that combine neural networks with symbolic reasoning. Neural networks excel at learning patterns from data, while symbolic reasoning provides interpretable, rule-based frameworks for logical deduction. By integrating these two methodologies, it is possible to enhance the ability to reason abstractly and generalize to new, unseen tasks, which are essential for System-2 reasoning.\nSeveral methods have been developed to combine symbolic and neural models, enhancing the generality and reasoning abilities."}, {"title": null, "content": "Neuro-Symbolic Programming (NSP) [45] integrates the pattern recognition strengths of neural networks with the explicit logic of symbolic programming. In this approach, the neural network learns patterns from data, while the symbolic component generates interpretable code that can be applied to reasoning tasks. This hybrid method enables handling of complex tasks requiring both learning from examples and reasoning over abstract concepts, such as program synthesis and abstract reasoning."}, {"title": null, "content": "Differentiable Inductive Logic Programming (@ILP) [19] extends inductive logic programming by integrating it with neural network optimization techniques. This method allows the system to learn logical rules while maintaining the flexibility of neural architectures. ILP improves the model's ability to generalize by discovering dynamic rules that can be applied across various domains, enabling AI to adapt its reasoning to new, unfamiliar tasks with minimal supervision."}, {"title": null, "content": "Symbolic Regression with Neural Networks [60] combines the power of neural networks with symbolic reasoning to uncover functional relationships between variables. The neural network identifies patterns in the data, and symbolic regression translates these patterns into symbolic expressions. This approach is particularly effective in tasks that require reasoning about mathematical or functional relationships, enhancing the model's generality across different problem spaces."}, {"title": "Meta-Learning Facilitates Adaptation to Unfamiliar Environments", "content": "System-2 reasoning requires AI systems to adapt quickly to new and unfamiliar tasks, a process that can be supported by meta-learning. Unlike traditional learning methods that are focused on task-specific optimization, meta-learning trains models to \"learn how to learn,\" making them more adaptable to various environments. By facilitating faster adaptation and improved generalization, meta-learning methods enable Al systems to respond effectively to novel challenges, which is crucial for achieving System-2 reasoning in tasks that require flexible thinking and logical deduction.\nSeveral meta-learning techniques have been developed to enhance adaptability in AI systems, particularly in environments that demand reasoning about new and evolving tasks."}, {"title": null, "content": "Recurrent Neural Processes (RNPs) [47, 64] extend neural processes to handle sequential data, making them useful for tasks that require reasoning over time. In RNPs, models learn from time-dependent patterns, allowing them to predict future states based on past experiences. This capability is particularly relevant for multi-step reasoning tasks, enabling AI to adapt dynamically to evolving task requirements."}, {"title": null, "content": "Prototypical Networks [54] are designed for few-shot learning, where models classify new examples by comparing them to prototype representations. By learning these prototypes, Al systems can generalize across tasks with minimal data, making them efficient in environments that require structural reasoning. Prototypical networks are especially beneficial for System-2 reasoning, where recognizing and adapting to new patterns is key."}, {"title": null, "content": "Meta-Reinforcement Learning (Meta-RL) [21] combines meta-learning with reinforcement learning, creating a framework where agents not only adapt to new tasks but also optimize their reasoning over time. By incorporating a reward-based system, Meta-RL allows AI to balance reasoning about goals with adaptation in dynamic environments, strengthening its long-term planning and decision-making abilities."}, {"title": "Reinforcement Learning Strengthens Adaptation and Multi-Step Logical Reasoning", "content": "System-2 reasoning requires not only logical deduction but also adaptability in dynamic environments and the ability to execute multi-step reasoning. Reinforcement Learning (RL) is particularly suited for this, as it enables AI systems to learn through trial and error by optimizing actions based on rewards received from the environment. By continuously adjusting actions to maximize long-term rewards, RL enhances AI's ability to adapt to unfamiliar tasks and perform multi-step reasoning, which are both critical for System-2 reasoning.\nSeveral advanced RL techniques have been developed to strengthen adaptability and multi-step reasoning, providing AI systems with the capacity to plan, adapt, and reason across various contexts."}, {"title": null, "content": "Hierarchical Reinforcement Learning (HRL) [16] organizes decision-making into layers, where higher-level agents set overarching goals and lower-level agents execute actions to achieve those goals. This hierarchical structure allows for complex reasoning tasks to be broken down into manageable sub-tasks, improving both reasoning and adaptation in multi-step problems."}, {"title": null, "content": "Model-Based Reinforcement Learning (MBRL) [58] involves creating a model of the environment to predict the outcomes of different actions. By simulating various sequences of actions, MBRL enables AI to plan, reason about potential strategies, and adapt its decisions based on predicted outcomes. This approach is particularly effective for tasks that require long-term planning and reasoning about future states."}, {"title": null, "content": "Relational Reinforcement Learning (RRL) [18] focuses on learning relationships between objects and actions within a task. By understanding how different entities interact, RRL allows AI systems to reason about object relationships and adapt to new tasks that require spatial reasoning or object manipulation."}, {"title": "Conclusion", "content": "This paper explored the limitations of existing AI approaches, such as program synthesis, transformers, and large language models (LLMs), in solving ARC tasks, particularly in the context of generality and adaptation. While these methods have demonstrated notable improvements in generality, they consistently struggle with adaptation to novel and unfamiliar environments. Their reliance on predefined structures, patterns, or vast pre-training makes it challenging for current AI systems to achieve the deeper, more flexible reasoning associated with System-2 capabilities.\nTo address these challenges, we proposed four key research directions aimed at enhancing both generality and adaptation for System-2 reasoning: (1) learning human intentions from action sequences, (2) integrating symbolic and neural hybrid models, (3) employing meta-learning for adaptation, and (4) using reinforcement learning for multi-step logical reasoning. Each of these approaches targets specific limitations of current methods, helping models better handle abstract reasoning, generalize across various domains, and dynamically adapt to novel tasks.\nBy learning human intentions from action sequences, AI can better capture underlying goals, enabling stronger logical deduction and more human-like reasoning. Symbolic and neural hybrid models enhance abstract reasoning by bridging the gap between data-driven learning and rule-based logic, allowing AI to generalize beyond direct experience. Meta-learning facilitates faster adaptation to new environments by optimizing models to generalize their learning process itself, addressing the flexibility required for unfamiliar tasks. Finally, reinforcement learning strengthens adaptation and multi-step logical reasoning by providing a framework for sequential decision-making and long-term strategy development.\nAdvancing towards System-2 reasoning will require these approaches to work in synergy, allowing models to generalize across diverse tasks and adapt seamlessly to unforeseen scenarios. By pursuing these research directions, AI systems can evolve closer to AGI-level reasoning, demonstrating not only pattern recognition but true human-level reasoning, logical deduction, and abstract thinking."}]}