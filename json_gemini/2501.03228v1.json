{"title": "LightGNN: Simple Graph Neural Network for Recommendation", "authors": ["Guoxuan Chen", "Lianghao Xia", "Chao Huang"], "abstract": "Graph neural networks (GNNs) have demonstrated superior performance in collaborative recommendation through their ability to conduct high-order representation smoothing, effectively capturing structural information within users' interaction patterns. However, existing GNN paradigms face significant challenges in scalability and robustness when handling large-scale, noisy, and real-world datasets. To address these challenges, we present LightGNN, a lightweight and distillation-based GNN pruning framework designed to substantially reduce model complexity while preserving essential collaboration modeling capabilities. Our LightGNN framework introduces a computationally efficient pruning module that adaptively identifies and removes redundant edges and embedding entries for model compression. The framework is guided by a resource-friendly hierarchical knowledge distillation objective, whose intermediate layer augments the observed graph to maintain performance, particularly in high-rate compression scenarios. Extensive experiments on public datasets demonstrate LightGNN's effectiveness, significantly improving both computational efficiency and recommendation accuracy. Notably, LightGNN achieves an 80% reduction in edge count and 90% reduction in embedding entries while maintaining performance comparable to more complex state-of-the-art baselines. The implementation of our LightGNN model is available at the github repository: https://github.com/HKUDS/LightGNN.", "sections": [{"title": "1 Introduction", "content": "Recommender systems [7, 38] have become indispensable in modern online platforms, effectively addressing information overload and enhancing user engagement through personalized service delivery. At the core of these systems, Collaborative Filtering (CF) [14, 20] stands as a dominant paradigm, leveraging users' historical interactions to model latent preferences for behavior prediction.\nThe evolution of collaborative filtering has spawned diverse approaches, from classical matrix factorization methods (e.g. [13]) to sophisticated neural architectures (e.g. [9]). Among these developments, Graph Neural Networks (GNNs) have emerged as particularly powerful tools for CF-based recommendation, distinguished by their ability to capture complex, high-order interaction patterns through iterative embedding smoothing. Pioneering works include NGCF [25], which introduced graph convolutional networks (GCNs) to model user-item relationships, and LightGCN [8], which simplifies GCNs to their essential components for recommendation. To address the challenge of sparse interactions in GNN-based recommendation, researchers have developed innovative self-supervised learning (SSL) techniques, including SGL [27], NCL [15], and HCCF [30]. These approaches significantly enhance recommendation accuracy by leveraging self-augmented supervision signals.\nDespite significant advancements in GNNs, we would like to emphasize two inherent limitations that continue to challenge GNN-based CF models. i) Limited scalability of GNNs: Online recommendation services typically handle vast amounts of relational data (e.g., millions of interactions). This causes the size of user-item graphs to increase dramatically, resulting in a considerable number of information propagation operations within GNNs. Such scalability issues present challenges concerning storage, computational time, and memory requirements. Furthermore, GNN-based CF relies heavily on id-corresponding embeddings for user and item representation [8], with the complexity of these embeddings directly linked to the growing number of users and items, incurring significant memory costs. ii) Presence of pervasive noise in interaction graphs: Collaborative recommenders mainly utilize users' implicit feedback, such as clicks and purchases, because of its abundance. However, these interaction records often contain substantial noise that diverges from users' true preferences, including misclicks and popularity biases [23]. Although some existing methods address scalability through techniques like random dropping (e.g., PinSage [33]) or knowledge distillation (KD) (e.g., SimRec [29]), they remain susceptible to misinformation, which can result in inaccurate predictions from their compressed recommenders.\nTo address these limitations, this paper proposes pruning redundant and noisy components in GNNs, specifically targeting graph edges and embedding entries. We aim to enhance model scalability while preserving essential user preference features. However,\nachieving this objective presents non-trivial challenges, outlined as:\n\u2022 How to identify the graph edges and embedding entries that are genuinely redundant or noisy in the user-item interaction graph?\n\u2022 How to maintain the high performance of GNN-based CF when significant structural and node-specific information is removed?\nAs illustrated in Figure 1(a), a considerable proportion of items that users interact with fall into the same category, leading to redundant information about users' preferences. By identifying and removing this redundancy from both structures and parameters, we can significantly reduce the complexity of GNN-based CF. Additionally, many observed interactions represent noise linked to users' negative feedback, as revealed by the review text. This noise can disrupt the preference modeling of existing compressed CF methods, which often fail to explicitly identify such noisy information. Regarding the second challenge, depicted in Figure 1(b), traditional knowledge distillation approaches struggle to effectively maintain performance when compressing the GNN model at a high ratio due to the limited number of edges and parameters. In contrast, our innovative hierarchical KD offers enhanced preservation capabilities.\nFully aware of these challenges, we introduce a GNN pruning framework called LightGNN that facilitates efficient and denoised recommendations. LightGNN incorporates graph structure learning to explicitly assess the likelihood of redundancy or noise for each edge and embedding entry. This learning process is supervised in an end-to-end fashion, leveraging the downstream recommendation task alongside a hierarchical knowledge distillation paradigm. Inspired by the advantages of global relation learning in recommendation [30], our KD approach features an intermediate distillation layer that utilizes high-order relations to enhance candidate edges in the compressed model. This augmentation improves the model's capacity to maintain recommendation performance under high-rate compression. Through innovative importance distillation and prediction-level and embedding-level alignments, our hierarchical knowledge distillation enriches learnable pruning with abundant supervisory signals, boosting its compression capability.\nThe contributions of our LightGNN are summarized as follows:\n\u2022 We introduce a novel GNN pruning framework for recommendation, explicitly identifying and eliminating redundancy and noise in GNNs to enable efficient and denoised recommendations.\n\u2022 Our LightGNN framework integrates an innovative hierarchical knowledge distillation paradigm, seamlessly compressing GNNS at high ratios while preserving prediction accuracy.\n\u2022 We conduct extensive experiments to demonstrate the superiority of LightGNN in terms of recommendation accuracy, inference efficiency, model robustness, and interpretability."}, {"title": "2 GNN-based Collaborative Filtering", "content": "Graph neural network (GNN) has been shown a most effective solution to collaborative filtering (CF) [4, 28]. The CF task typically involves a user set U (|U| = I), an item set V (|V| = J), and a user-item interaction matrix A \u2208 RI\u00d7J. For a user ui \u2208 U and an item vj \u2208 V, the entry ai,j \u2208 A equals 1 if user ui has interacted with item vj, otherwise ai,j = 0. Common interactions include users' rating, views, and purchases. GNN-based CF methods construct the user-item graph based on the interaction matrix A. This graph can be denoted by G = (U,V,E), where U, V serve as the graph vertices, and & denotes the edge set. For each (ui, vj) that satisfies ai,j = 1, there exists bidirectional edges (ui, vj), (vj, ui) \u2208 E.\nBased on the user-item graph G, GNNs conduct information propagation to smooth user/item embeddings for better reflecting the interaction data. Specifically, it firstly assigns initial embeddings ei, ej \u2208 Rd to each user ui and item vj, respectively. Here d represents the hidden dimensionality. Then it iteratively propagates each node's embedding to its neighboring nodes for representation smoothing. Take the widely applied LightGCN [8] as an example, the embeddings for user ui and item vj in the l-th iteration are:\nei,l = \\frac{1}{\\sqrt{d_id_j}} \\sum_{(i,j) \\in E} e_{j,l-1}, ej,l = \\frac{1}{\\sqrt{d_id_j}} \\sum_{(i,j) \\in E} e_{i,l-1} (1)\nwhere ei,l, ei,l\u22121 \u2208 Rd denote the embedding vectors for ui in the l-th and the (l \u2212 1)-th iterations, and analogous notations are used in ej,l, ej,l\u22121. The 0-th embedding vectors ei,0, ej,0 uses the initial embeddings ei, ej. And di, dj represent the degrees of nodes ui, vj, for Lapalacian normalization. After a total L iterations, GNN-based CF aggregates the multi-order embeddings for final representations \\bar{e}_i, \\bar{e}_j \u2208 Rd and user-item relation predictions \\hat{y}_{i,j}, as follows:\n\\hat{y}_{i,j} = \\bar{e}_i^T \\bar{e}_j, \\quad \\bar{e}_i = \\sum_{l=0}^L e_{i,l} \\quad \\bar{e}_j = \\sum_{l=0}^L e_{j,l} (2)\nWith the prediction scores \\hat{y}_{i,j}, the GNN models are optimized by minimizing the BPR loss function [18] over all positive user-item pairs (ui, vj+) \u2208 &, and sampled negative pairs (ui, vj\u2212), as follows:\nL_{bpr} = \\sum_{(u_i,v_{j+},v_{j-})} - \\log \\sigma(\\hat{y}_{i, j+} - \\hat{y}_{i,j-}) (3)\nThough the above GNN framework achieves state-of-the-art performance in recommendation, its scalability is limited by the large-scale interaction graph and embedding table. In light of this, this paper proposes LightGNN aiming to effectively prune the GNN model for efficient graph neural collaborative filtering."}, {"title": "3 Methodology", "content": "This section goes through the proposed LightGNN to show the technical details. The overall framework is illustrated in Figure 2."}, {"title": "3.1 Graph Neural Network Pruning", "content": "Inspired by the lottery ticket hypothesis for GNNs [5, 6], we propose to use only a subset of GNN's parameters that maximally preserve the model functionality, to improve its efficiency. Specifically, the time complexity for a typical GNN model as aforementioned is O(Lx|8|xd), and the space complexity is correspondingly O(|8|+(I + J) \u00d7 d). Therefore by reducing the number of edges |8|, and the number of non-zero elements in the d embedding dimensions, our LightGNN is able to optimize both the computational efficiency and memory efficiency. To achieve this, it is essential to identify the noisy and redundant parts in the edges & and the embedding table E = {ei, ej|ui \u2208 U, vj \u2208 V}, to prevent performance degradation.\n3.1.1 Edge Pruning. To this end, LightGNN employs a sparse weight matrix W \u2208 RI\u00d7J for edge pruning. If an edge (ui, vj) is a candidate for pruning, the corresponding weight wij in W is a learnable parameter. Otherwise wij is set as 0 and is not optimized. With the weight matrix W, the graph information propagation process for the pruned GNN is conducted as follows:\nEu,l = D^{\\frac{1}{2}}(A \\odot W)D^{\\frac{1}{2}}E_{v,l-1} + E_{u,l-1} (4)\nwhere \\odot denotes the element-wise product operator which injects the learnable weights W into the information propagation process. Here Eu,l, Ev,l\u22121 \u2208 RI\u00d7d denote the user embedding table in the l-th and the (l \u2212 1)-th iteration, and Ev,l\u22121 \u2208 RJ\u00d7d denotes the embedding matrix for items in the (l \u2212 1)-th iteration. And Du \u2208 RIXI, Dv \u2208 RJXJ denote the degree matrices for users and items, respectively. The information propagation to obtain higher-order item embeddings Ev, is analogously using (A \\odot W)T.\nBased on the parametric information propagation, the weights W participate in the calculation for final user/item embeddings, which are then used for predictions and loss calculations. Through the back propagation, W is tuned to reflect the importance of edges, wherein larger |wi,j| denotes the edge (ui, vj) having a larger influence on producing better recommendation results. In light of this property, our LightGNN framework prunes the less important edges (noises or redundancies) after training, specifically by setting the p% candidate edges with the least importance to 0 (see 3.2.3), where \u03c1\u03b5 (0, 100) denotes the proportion to drop. The pruning algorithm follows an iterative manner with multiple runs. In each run, LightGNN first conducts parameter optimization for model training and pruning weight tuning, and then prunes the GNN by dropping edges and other parameters.\n3.1.2 Embedding and Layer Pruning. As indicated by the complexity analysis for GNNs, the parameters for representing users and items (i.e. embeddings E) also contribute significantly to the running time and the memory costs of GNNs. Therefore LightGNN follows the similar pruning algorithm for edges to prune the entries in the embedding matrix E. As the scalar parameters in E already reflect the importance of their corresponding entries, LightGNN does not employ extra pruning weights for embeddings. Analogously, LightGNN alternately conducts model training and parameter pruning with ratio p'% according to the absolute value |ei,d'|, where ei,d' represents the d'-th dimension in i's embedding vector.\nIn addition to the edges and embeddings, the time complexity of GNNs suggests that the number of graph propagation layers L also greatly impacts the computation time of GNNs. Moreover, in practice, L is also significant to influence the temporary memory costs for stacking the intermediate results. Thus our LightGNN further reduces the number of graph iterations L for efficiency, which also alleviates the over-smoothing effect of GNNs [30]."}, {"title": "3.2 Hierarchical Knowledge Distillation", "content": "3.2.1 Bilevel Alignment. Motivated by the strength of knowledge distillation (KD) in compressing the learned knowledge of advanced models into light-weight architectures [29], the proposed LightGNN develops a hierarchical knowledge distillation framework to maximally retain the original high performance in the pruned GNN model. Taking a well-trained GNN model (e.g. LightGCN [8]) as the teacher, LightGNN aligns the student model with pruned structures, embeddings, and GNN layers to the teacher model with respect to both hidden embeddings and final predictions. In the prediction level, the following loss function is applied:\nL_{p-kd} = \\sum_{v} - (\\sigma(\\hat{y}_v^t) \\cdot log(\\sigma(\\hat{y}_v^s)) + \\delta(\\sigma(\\hat{y}_v^t)) \\cdot log(\\delta(\\sigma(\\hat{y}_v^s)))) (5)\nwhere v = (ui, vj1, vj2), \u03b4(x) = 1 \u2212 \u03c3(x), \\epsilon = \\frac{1- \\sigma(x)}{\\epsilon} (5)\nHere (ui, vj1, vj2) denotes the randomly sampled training tuples analogous to the BPR loss, while vj1 and vj2 are not fixed to be positive or negative samples. \u03c3(\u00b7) denotes the sigmoid function to constrain the values to be within (0, 1). And \u03c4\u2208 R is known as the temperature coefficient [10]. We denote the predictions made by the student model using the superscript s, and denote the predictions made by the teacher model with the superscript t. With this training objective, our LightGNN framework encourages the pruned GNN model to mimic the predictions made by the complete GNN model with all the edges, embedding entries and propagation iterations, to obtain the teacher's prediction ability as much as possible.\nBesides the prediction-level alignment, our LightGNN aligns the teacher model and the student model by treating their learned embeddings as paired data views for contrastive learning. In specific,\nthe following infoNCE loss function [16] is applied:\nL_{e-kd} = - \\sum_{u_i \\in U} \\log \\text{softmax} (S_{u_i, u_i}) - \\sum_{v_j \\in V} \\log \\text{softmax} (S_{v_j, v_j})\n\\text{where } \\text{softmax}(S_{u, u_i}) = \\frac{\\exp S_{i',i}}{\\sum_{u_{i'} \\exp S_{i',i}}}\\quad S_{i',i} = \\cos(\\bar{e}_{u_{i'}}^t, \\bar{e}_{u_i}^s) (6)\nHere si',i \u2208 Su denotes the cosine similarity between the final embeddings \\bar{e}_{u_{i'}}, \\bar{e}_{u_i} for the users ui and ui, given by the student model and the teacher model, respectively. The item-side embedding-level KD is calculated analogously. With this embedding-level KD objective, our LightGNN can better guide the pruned GNN to preserve the essential graph structures and parameters in a deeper level.\n3.2.2 Intermediate KD Layer for Structure Augmentation. Due to the sparsity nature of the user-item interaction data, some key preference patterns are not reflected by the direct neighboring relations but preserved by the high-order relations. To facilitate the capturing of these high-order connections during our edge pruning, we augment the knowledge distillation of LightGNN with an intermediate KD layer model for edge augmentation.\nTo be specific, LightGNN conducts a two-stage distillation, firstly from the original GNN to an augmented GNN, and then from the augmented GNN to the final pruned GNN. The augmented GNN does not prune any edges or embedding entries, but instead includes the high-order connections as augmented edges. Formally, the augmented GNN has the same model architecture (Eq. 4) as the student but works over the following augmented interaction graph:\nG = (U,V,\\bar{E}), \\bar{E} = {(u_i, v_j), (v_j, u_i)|\\bar{a}_{i,j}^{(h)} \\neq 0} (7)\nwhere \\bar{a}_{i,j} denotes the entry for (ui, vj) in the h-th power of the symmetric adjacent matrix with self loop [25]. In other words, edge (ui, vj) exists in the augmented graph G if ui can be connected to vj via any path with its length shorter than or equal to h hops in the original graph. With this structure augmentation, the augmented GNN directly includes the high-order connections in the model parameters, to prevent losing the key high-order patterns in radical edge pruning. During the intermediate KD, the augmented GNN is supervised by the original GNN (no weights), not only to mimic its accurate predictions, but also to learn proper weights W\u00b9 for all the edges. The intermediate KD layer prevents the augmented larger graph from introducing noises using the supervision of the bilevel distillation from original GNN and the adaptive edge weights.\n3.2.3 Importance Distillation for Pruning. After the first knowledge distillation from the original GNN to the augmented GNN model, our LightGNN then distills its learned knowledge with structure augmentation to the final pruned GNN model. Apart from the aforementioned bilevel alignment, LightGNN further enhances this second KD with the importance distillation, which explicitly leverages the learned importance weights in the intermediate model to increase the precision of pruning weights in the final model. Specifically, the pruning weight matrix in the final pruned GNN is a compound variable whose entries are calculated as follows:\nWij = w_{ij}^s + \\beta_1 \\cdot w_{ij}^t + \\beta_2 (\\bar{e}_i^t \\bar{e}_j^t) \\quad \\text{for } (u_i, v_j) \\in E (8)\nwhere wij\u2208 R denotes the weight to decide if edge (ui, vj) should be pruned, and it is acquired using the independent edge weight wsij \u2208 Ws of the final student model, the tuned edge weight wtij \u2208 Wt of the intermediate GNN as the teacher model, and the edge prediction made by the intermediate GNN's final embeddings \\bar{e}_i^t, \\bar{e}_j^t \u2208 Rd. Here \u03b21, B2 denote two hyperparameters for weighting and we define the sparse decision matrix W\u00b3 = {wsij}IxJ.\nWith this importance distillation in the edge pruning, the pruning weights W\u00b3 in the final student model are not only trained in the end-to-end manner using the bilevel KD objectives, but also directly adjusted by the well-trained weights in the intermediate teacher model. Moreover, by utilizing the edge weights obtained in the augmented graph, the pruned GNN is injected with the high-order connectivity to facilitate edge dropping and global relation learning. It is worth noting that, apart from the edge pruning, the student's edge weights are also employed in the graph information propagation, to enrich the pruned GNN with less edges but compensatory, adaptive and informative edge importance."}, {"title": "3.3 Optimization with Uniformity Constraint", "content": "Inspired by the advantage of learning uniform embeddings in CF [22, 28], our LightGNN proposes to regularize the model optimization with an adaptive uniformity constraint based on contrastive learning. In specific, the constraint minimizes the pairwise inner-product between embeddings to enforce representation uniformity, while maximizing the embedding similarity between nodes with similar pruning masks. In this way, the positive relations are augmented by the learned pruning weights for enhancement. Formally, the adaptive uniformity constraint is as follows:\nL_{u-reg} = \\sum_{u_i \\in U} - \\log \\frac{\\sum_{u_l \\in S_i} \\exp ( \\frac{\\tau_i \\tau_l}{\\tau})}{\\sum_{u_j \\in U} \\exp (\\frac{\\tau_i \\tau_{j}}{\\tau})} + \\sum_{v_j \\in V} - \\log \\frac{\\sum_{v_l \\in S_j} \\exp ( \\frac{\\tau_j \\tau_l}{\\tau})}{\\sum_{v_j \\in V} \\exp (\\frac{\\tau_j \\tau_{l}}{\\tau})} (9)\nwhere Si and Sj denote the positive sets of user ui and item vj, respectively, which are determined by picking the users/items that share the highest similarity in embedding pruning. Take the user side as an example, the neighborhood set Si is acquired by:\nS_i = {u_{i1} | ||\\theta_{i} \\odot \\theta_{i1} ||_0 \\geq \\max (||e_i||_0, ||e_{i1}||_0) - \\delta} (10)\nwhere \u03b8i, \u03b8i1 \u2208 {0, 1}d denote binary pruning masks for the 0-th embedding vectors es and e, respectively. Operator \u2299 denotes the element-wise multiplication, and || \u2217 ||0 denotes the lo norm of vectors. d represents the threshold hyperparameter for similarity relaxation, which is selected according to the pruning ratio.\nWith the above contrastive loss using similarly-pruned embeddings as positive sets, LightGNN can learn uniformly-distributed embeddings while capturing the node-wise similarity during the pruning process. Combining it with the collaborative filtering loss Lbpr, the bilevel KD losses Lp-kd and Le\u2212kd, and a weight-decay regularization term over parameters \u0398, LightGNN applies the following multi-task training loss with hyperparameters \u03bb*:\nL = \u03bb0Lbpr + \u03bb1Lp\u2212kd + \u03bb2Le\u2212kd + \u03bb3Lu\u2212reg + \u03bb4||\u0398|| (11)"}, {"title": "4 Evaluation", "content": "We conduct extensive experiments on our LightGNN framework, aiming to answer the following research questions (RQs):\n\u2022 RQ1: How is the performance of LightGNN after the model pruning, compared to existing recommendation methods?\n\u2022 RQ2: How efficient is our pruned GNN, compared to baselines?\n\u2022 RQ3: How do the components of the proposed LightGNN impact the recommendation performance of the pruned GNN?\n\u2022 RQ4: How do the pruning ratios impact the recommendation performance and the efficiency of the pruned GNN?\n\u2022 RQ5: Can the proposed LightGNN framework alleviate the over-smoothing effect with its hierarchical knowledge distillation?\n\u2022 RQ6: Can our LightGNN effectively identify the redundant and noisy information in the user-item interaction graph?\n4.1 Experimental Settings\n4.1.1 Datasets. LightGNN is evaluated using three real-world datasets: Gowalla, Yelp, and Amazon. The Gowalla dataset contains user check-in records at geographical locations from January to June 2010, obtained from the Gowalla platform. Yelp dataset is obtained from Yelp platform and contains user ratings on venues from January to June 2018. The Amazon dataset contains people's ratings of books on the Amazon platform, during 2013. Following [29], we filter out users and items with less than three interactions, and splitting the original datasets into training, validation, and test sets by 70:5:25. Additionally, we convert ratings into binary implicit feedback, following [8]. The data statistics are listed in Table 1.\n4.1.2 Evaluation Protocols. We follow common evaluation protocols for recommendation [25, 35]. We rank all uninteracted items with the positive items from test set for each user, a method known as full-rank evaluation. We use two common metrics, Recall@N and NDCG@N [24, 27] with values of N = 20 and 40.\n4.1.3 Baselines. We compare LightGNN to 18 baselines from diverse categories, including factorization method (BiasMF [13]), deep neural CF methods (NCF [9], AutoR [19]), graph-based methods (GCMC [1], PinSage [33], STGCN [36], NGCF [25], GCCF [4], LightGCN [8], DGCF [26]), self-supervised recommenders (SLRec [32], SGL [27], NCL [15], SimGCL [34], HCCF [30]), and compressed CF approaches (GLT [5], UnKD [3], SimRec [29]).\n4.1.4 Hyperparameter Settings. We implement LightGNN with PyTorch, using Adam optimizer and Xavier initializer with default parameters. For all models, the training batch size is set to 4096 and the embedding size is 32 by default. For all GNN-based models, we set the layer number to 2. Weights A0, A1, A2 in LightGNNare tuned from {1e-k|k = 0, 1, ..., 4}. And 33 is tuned in a wider range which additionally contains {1e-5, 1e-6}. The weight 14 for weight-decay regularization is selected from {1e\u00afk|k = 3, 4, ..., 9}. All temperature coefficients are chosen from {1e-k, 3e-k, 5e-k|k = \u22121, 0, 1, 2}."}, {"title": "4.2 Performance Comparison (RQ1)", "content": "We first compare LightGNN to baselines on recommendation accuracy. The results are in Table 2. We make the following observations:\n\u2022 Superior performance of LightGNN: The proposed model LightGNN surpasses all baselines across different categories, including simple neural CF, graph-based recommenders, self-supervised methods, and compression methods. This superiority in performance demonstrates that our learnable pruning framework and hierarchical distillation paradigm not only maintain prediction accuracy after model compression but also enhance existing recommendation frameworks. The effective elimination of noise and redundancy in the interaction graph and embedding parameters contributes to these performance improvements.\n\u2022 Drawbacks of CF without model compression: When comparing the best-performing CF methods, such as self-supervised CF techniques like SGL, HCCF, and SimGCL, to compression methods like UnKD and SimRec, it is evident that CF methods without model compression fall short in terms of recommendation accuracy. This discrepancy can be attributed to the debiasing and anti-over-smoothing effects embedded in the knowledge distillation process of UnKD and SimRec. This suggests that model compression techniques, such as knowledge distillation, can go beyond improving model efficiency. They can also address adverse factors present in observed data and modeling frameworks, such as data bias, noise, and over-smoothing effects.\n\u2022 Importance of explicit noise elimination: While UnKD and SimRec refine the distilled model by addressing bias and over-smoothing effects in GNN-based CF, they rely solely on high-level supervision methods. In contrast, our LightGNN explicitly identifies and eliminates fine-grained noisy and redundant elements within the model, such as edges and embedding entries. This empowers our LightGNN with notable strength in recommender refinement, leading to significant performance superiority."}, {"title": "4.3 Efficiency Test (RQ2)", "content": "To assess the model efficiency, we evaluate the memory and computational costs of LightGNN and baselines. The compared baselines include NGCF, GCCF, HCCF, and existing GNN compression method UnKD. Our LightGNN is tested with different preservation ratios. In Figure 3, the results are presented relative to the performance of NGCF. We deduce the following observations:\n\u2022 Simplified GNNs. Despite simplifying the GNN architecture by removing transformations and activations, some GNN methods like GCCF fail to significantly reduce memory and time costs related to graph storage and information propagation. Consequently, the costs of GCCF remain comparable to those of NGCF. This demonstrates the limitation of architectural simplifications in improving efficiency for graph-based recommendation.\n\u2022 SSL-enhanced GNNs. SSL techniques have been utilized to enhance graph recommenders by generating self-supervision signals. However, it is important to note that these methods may introduce additional operations, leading to increased memory and time costs. This is exemplified by the performance of HCCF, where utilizing extra hypergraph propagation necessitates more FLOPs and yields a noticeable increase in computational time.\n\u2022 Existing compressed GNNs. UnKD has been successful in achieving efficiency improvements, particularly in terms of computational time. However, when comparing UnKD to LightGNN, a significant disadvantage becomes evident. This limitation arises from UnKD's lack of explicit identification and removal of redundancy and noise in the GNN model. As a result, UnKD is unable to prune a larger portion of the GNN to achieve superior efficiency improvements like our LightGNN framework does.\n\u2022 Efficiency of LightGNN. The results demonstrate a significant memory reduction of 70% in LightGNN, considering both the parameter number and storage size. Moreover, there is an impressive reduction of over 90% in FLOPs during forward propagation and an over 50% reduction in physical prediction time. These efficiency optimizations can be attributed to two key aspects. Firstly, the learnable GNN pruning paradigm accurately removes redundant and noisy information from the GNN. This facilitates efficient utilization of computational resources. Secondly, our learnable pruning mechanism is supervised by the hierarchical KD, which incorporates multi-dimensional alignment and high-order structure augmentation. This maximizes the retention of performance, allowing for more extensive pruning of parameters."}, {"title": "4.4 Ablation Study (RQ3)", "content": "We investigate the effectiveness of LightGNN's technical designs using Gowalla and Yelp data, with different pruning ratios. The results are shown in Table 3. We make the following observations.\nEffectiveness of the GNN pruning techniques.\n\u2022 ~EmbP, ~EdgeP, ~BothP: We replace the learnable pruning with random dropping. The three variants replace embedding pruning, edge pruning, and both, respectively. Significant performance drop can be observed under different pruning ratios, indicating the effectiveness of our learnable pruning in identifying the essential embedding entries and edges. Especially, when dropping with high ratios (e.g. preserving only 11% and 8% entries), the prediction ability of the random variants experiences a destructive (over 70%) decay, while LightGNN preserves most of its accuracy.\n\u2022 BnEdge: To study the effect of learned edge weights WS, BnEdge uses binary edge weights instead of Ws during GNN propagation. Though it maintains the learnable pruning process unchanged, a noticeable degradation can be observed. This suggests the crucial role of learned weights. They not only identify which edges to prune, but also effectively preserve the pruned information.\nEffectiveness of knowledge distillation.\n\u2022 -BiAln: To assess the significance of the KD constraints for effective pruning, we remove the bilevel alignment, including the prediction-level and embedding-level KD. The notable performance drop verifies the importance of aligning the teacher model with the pruned model, to effectively retain model performance.\n\u2022 -IntKD: This variant removes the intermediate KD layer in Light-GNN. As a result, its performance notably deteriorates, particularly on the Yelp dataset. The increased importance of this module for Yelp can be attributed to the higher sparsity of the dataset. In such cases, the intermediate KD layer is able to seek more edges from high-order relations to enrich the small edge set.\n\u2022 -ImpD: This variant removes the importance distillation, and the results confirm the benefits of incorporating learned edge weights and predictions from the intermediate KD layer model into the decision-making process of edge dropping."}, {"title": "4.5 Influence of Pruning Ratios (RQ4)", "content": "In this experiment, we investigate the impact of pruning ratios for edges and embedding entries on both model performance and efficiency. Figure 4 shows the evaluated model performance and computing FLOPs (floating point operations) during forward propagation across various preservation ratios. We present two pruning schemes: a mild pruning scheme that removes fewer graph edges in the GNN, and an aggressive pruning scheme that removes more edges. Based on the results, we draw the following observations:\n\u2022 Performance change. As we discard a larger number of embedding entries and graph edges, we observe a continuous decline in performance. However, it is noteworthy that even when a substantial portion of the GNN model is removed, our LightGNN consistently maintains a high level of recommendation performance compared to SimGCL and LightGCN. This resilience can be attributed to the hierarchical KD, which effectively aligns the predictions of the student model with those of the well-performing teacher model through bilevel alignment, and the importance distillation that gives the optimal dropping strategies. Additionally, the intermediate KD layer with structure augmentation further enhances the recommendation ability by incorporating more edges sampled from high-order relations. These features collectively contribute to the robust performance of LightGNN.\n\u2022 Efficiency change. As the pruning ratio increases, LightGNN exhibits a significant decrease in FLOPs. This confirms the effectiveness of enhancing GNN efficiency by pruning embeddings and structures. Specifically, our LightGNN achieves a FLOPs reduction of 90% during forward propagation while maintaining comparable performance to SimGCL, and a FLOPs reduction of 95% while performing similarly to LightGCN. These substantial reductions in FLOPs highlight the effectiveness of our learnable pruning strategy in minimizing computational operations.\n\u2022 Differences across datasets. Furthermore, it is worth mentioning that our LightGNN demonstrates better preservation of recommendation performance when pruning the same proportion of information on the Amazon dataset compared to the Gowalla dataset. This observation suggests the presence of more redundancy or noise in the Amazon data, which aligns with the larger number of edges and users/items present in the Amazon dataset."}, {"title": "4.6 Anti-Over-Smoothing Effect Study (RQ5)", "content": "To assess the ability of LightGNN to mitigate the over-smoothing effect of GNNs during the pruning process", "that": "i) The clustering effect observed in both 2-D plots and angle plots is notably stronger for LightGCN, demonstrating the severe over-smoothing effect resulting from the iterative embedding smoothing paradigm. ii) To address this issue, SGL and SimGCL incorporate contrastive learning to enhance the distribution uniformity of embeddings. Both methods exhibit higher uniformity in the estimated distribution compared to LightGCN, with SimGCL exhibiting some superiority due to its less-random augmentation design. iii) Compared to SimGCL, our LightGNN exhibits even fewer dark regions in the embedding distribution ring, indicating higher uniformity. This advantage becomes more apparent in the angle-based plot, where the low probabilities are much closer to the high ones in LightGNN. This observation strongly indicates a higher anti-over-smoothing ability of our LightGNN, which can be ascribed to the sparsification effect caused by embedding pruning, and the uniformity constraint in our LightGNN.\n\u2022 Mean Average Distance (MAD) Values. We further evaluate the MAD values [2, 29"}]}