{"title": "EDGE-ENHANCED DILATED RESIDUAL ATTENTION NETWORK FOR MULTIMODAL Medical Image FUSION", "authors": ["Meng Zhou", "Yuxuan Zhang", "Xiaolan Xu", "Jiayi Wang", "Farzad Khalvati"], "abstract": "Multimodal medical image fusion is a crucial task that combines complementary information from different imaging modalities into a unified representation, thereby enhancing diagnostic accuracy and treatment planning. While deep learning methods, particularly Convolutional Neural Networks (CNNs) and Transformers, have significantly advanced fusion performance, some of the existing CNN-based methods fall short in capturing fine-grained multiscale and edge features, leading to suboptimal feature integration. Transformer-based models, on the other hand, are computationally intensive in both the training and fusion stages, making them impractical for real-time clinical use. Moreover, the clinical application of fused images remains unexplored. In this paper, we propose a novel CNN-based architecture that addresses these limitations by introducing a Dilated Residual Attention Network Module for effective multiscale feature extraction, coupled with a gradient operator to enhance edge detail learning. To ensure fast and efficient fusion, we present a parameter-free fusion strategy based on the weighted nuclear norm of softmax, which requires no additional computations during training or inference. Extensive experiments, including a downstream brain tumor classification task, demonstrate that our approach outperforms various baseline methods in terms of visual quality, texture preservation, and fusion speed, making it a possible practical solution for real-world clinical applications.", "sections": [{"title": "1 INTRODUCTION", "content": "Medical imaging plays an increasingly prominent role in clinical diagnosis, it aims to aggregate common and complementary information from different image modalities as well as integrate the information to generate more clearer images (Xie et al., 2023). Medical image fusion can enhance crucial details of anatomy and tissue information from different image modalities and hence helps physicians and radiologists in accurate diagnosis of diseases, e.g., precise localization of tumor boundaries and tissues (Chen et al., 2024) and effective radiotherapy treatments (Safari et al., 2023; Xie et al., 2023).\nMultimodal medical images provide organizations and structures from various aspects. For instance, Magnetic Resonance Imaging (MRI) offers high-resolution soft-tissue anatomical details, while computerized tomography (CT) scans reveal high-density information like bone structures"}, {"title": "2 MATERIALS AND METHOD", "content": "The proposed fusion framework consists of three main components: a feature encoder, a fusion module, and a feature decoder. Specifically, we designed an asymmetric autoencoder comprising a deep feature encoder and a lightweight feature decoder. The autoencoder extracts multi-scale features from the input image into the latent space using the proposed DRAN and an edge enhancer, then reconstructs the image back to the original space, as illustrated in Figure 1. Since there is no golden ground truth for the fusion task, we disentangle the training process of the proposed fusion framework into two stages following recent works (Xie et al., 2023; Fu et al., 2021; Li et al., 2022). In the first stage, we train the autoencoder solely to extract multi-scale deep features and reconstruct images through a general reconstruction task. In the second stage depicted in Figure 2, we introduce a family of parameter-free fusion strategy utilizing softmax weights and the nuclear norm to fuse two feature maps, $f_a$ and $f_b$, into a single feature map $f_e$. Using the trained decoder, this fused feature map $f_e$ is then decoded back to the image space to obtain the final fused image. Below, we provide a detailed explanation of each component."}, {"title": "2.1 FEATURE EXTRACTION AND IMAGE RECONSTRUCTION", "content": "Feature Encoder. The core module in the feature encoder is the Dilated Residual Attention Network (DRAN), which is inspired by two state-of-the-art mechanisms: residual attention (Wang et al., 2017) and pyramid attention (Li et al., 2018). The residual attention mechanism includes a sequence of convolutional layers for feature processing and, in parallel, a downsample-upsample block with a sigmoid function for learning weights based on feature importance. The residual attention network enhances the feature expression capability through the attention mechanism and aids in faster model convergence by preventing gradient vanishing or explosion via the residual connection. However, residual attention alone cannot effectively extract and learn multi-scale semantic features. To address this, we incorporate an additional pyramid attention network (Li et al., 2018). The convolution block in pyramid attention usually contains multiple convolutional layers that capture features at different scales and receptive fields. We adopt this approach by replacing convolutions with larger kernel filters with a sequence of smaller kernel filters (Szegedy et al., 2016). Different from Fu et al. (2021), we leverage the {1,3,5}-dilated convolution (Yu & Koltun, 2015) on shallow features of the original input image to further enhance the learning ability of local multi-scale information and fine details without downsampling the feature map. The receptive field is expanded using three different dilated convolutions to improve the discriminative multi-scale feature extraction ability of the model. Once the multi-scale features are extracted, they are concatenated channel-wise. The residual-pyramid attention paradigm is then applied to further extract deep features. These deep features are the output of the feature extraction module and are used in both the fusion and reconstruction modules.\nEdge information in MRI and CT is also crucial for accurate feature representation. To precisely capture these fine-grained edge features, we introduce a learnable dense residual gradient operator (DRGO) to enhance edge feature representation. The proposed module aggregates learnable convolutional features with gradient magnitude information on shallow features, and these are directly added element-wise to the features after the DRAN block. Our DRGO module uses two convolutional layers with 3\u00d73 kernel sizes with residual connections to extract features. Additionally, a Sobel gradient operator (Kanopoulos et al., 1988), followed by a 1\u00d71 convolutional layer, is used to learn the gradient information. This combination enhances the edge features by integrating both convolutional and gradient-based information.\nAsymmetric Autoencoder. After we obtained the latent features of input images from the feature encoder, we passed them into a lightweight decoder which only contains three convolutional layers and leaky ReLU activation function. The rationale behind this asymmetric design is that if the latent feature maps contain a wealth of useful information, they can be easily reconstructed back to the original image. Therefore, we employ a lightweight decoder to maximize the encoder's capability to extract rich semantic information in the latent space. Note that the latent features will be directly used in the subsequent fusion process, making the informative latent features crucial to the quality of the overall fused image."}, {"title": "2.2 FEATURE FUSION", "content": "Softmax Weighted Fusion Strategy. The fusion strategy is used to fuse the extracted features of input images into a single feature map. In this work, we introduce a novel parameter-free fusion strategy termed Softmax Feature Weighted Strategy. First, we obtained two output feature maps $f_a$, $f_b$ from the extraction module for input images $I_a$, $I_b$, respectively. These feature maps can be used to generate the corresponding weight maps that indicate the amount of contribution of each pixel to the final fused feature map (Lahoud & S\u00fcsstrunk, 2019). then, to get the weight map, we take the channel-wise softmax operation to the feature map which can be realized by Equation equation 1, where $x_i$ is the i-th channel of the output feature map x.\n$S(x_i) = \\frac{exp(x_i)}{\\sum_i exp(x_i)}$\nAfter we obtained the softmax output, we computed the matrix nuclear norm ($|| . ||_*$), which is the summation of its singular values. Finally, we obtain the weights for the output feature map by taking the weighted average of the maximum value of the nuclear norm. The formula is given in Equation equation 2.\n$W_k = \\frac{\\phi(||S(x_i)_k||_*)}{\\sum_{k=1}^C \\phi(||S(x_i)_k||_*)}$\nWhere C is the number of multimodality images (C = 2 in our work), $S(x_i)_k$, $k \\in [a, b]$ is the weight map after softmax operation for the feature map $f_k$, $k \\in [a, b]$. As mentioned, we selected $\\phi(.)$ to be $max ()$ in this work, but it can also be $mean()$, $sum()$ or $identity()$. The final fused feature map is then given by $f = \\sum W_k * f_k$, $k \\in [a, b]$."}, {"title": "2.3 Loss FUNCTION", "content": "Since only the autoencoder in Stage 1 involves training and optimization, we use the reconstruction objective which combines pixel loss, image gradient loss (Ma et al., 2020; Fabbri et al., 2018), and perceptual loss (Johnson et al., 2016) to optimize the network. The gradient loss (Ma et al., 2020; Fabbri et al., 2018) is added to model the fine details of textures in the reconstructed image, and the perceptual loss (Johnson et al., 2016) is added to model the high-level semantic similarity between reconstructed and input images. In detail, our loss function is defined as follows:\n$L_{pixel} = ||x - \\hat{x}||_2, L_{grad} = ||\\nabla x - \\nabla \\hat{x}||_2,$\n$L_{perp} = \\sum_{k=1}^C ||\\pounds_k(x) - \\pounds_k(\\hat{x})||_2^2$\n$L(\\theta) = L_{pixel} + \\lambda_1 * L_{grad} + \\lambda_2 * L_{perp}$\nEquation equation 3 shows the three losses utilized. We use a standard L2 distance for the pixel loss, where M is the number of input images, Io is the output image. The image gradient loss is also realized by the L2 norm of the image gradient in both x and y-direction. Finally, the perceptual loss (Johnson et al., 2016), where $\\pounds(x)$ is the the k-th channel in i-th layer (with size $W_i \\times H_i$) from the pre-trained VGG16 network (Simonyan & Zisserman, 2014) with input image x, and C is the number of channels. We prefer i to be large, i.e., the deeper layer of the VGG network. The total loss function is given in Equation equation 4, $\\theta$ is the set of network weights to be optimized, $\\lambda_1, \\lambda_2$ are weight balancing factors of the gradient and perceptual loss, respectively. We empirically set $\\lambda_1, \\lambda_2$ equal to 0.5."}, {"title": "2.4 DATA AND PREPROCESSING", "content": "In this work, we use three datasets to validate the effectiveness of our proposed approach: MRI-CT (184 pairs) and MRI-SPECT (357 pairs) multi-modality fusion from The Harvard Whole Brain Atlas dataset, and additional FLAIR and T2 sequence data from the BraTS 2019 dataset (Bakas et al., 2017; 2018; Menze et al., 2014) (335 patients) to evaluate our method on the downstream brain tumor type classification task. For MRI-SPECT fusion, we converted SPECT images from the RGB color space to the YCbCr space following (Fu et al., 2021; Xie et al., 2023; Li et al., 2022), using only the Y-channel images to train the model. For MRI-CT fusion, we used the original images as they are all single-channel grayscale images. All MRI-CT and MRI-SPECT image pairs were co-registered and preprocessed beforehand so that each pixel intensity is in the range of [0,255]. We further normalized the pixel intensity to [0,1].\nFor the T2 and FLAIR sequence data from BraTS, we first obtained the ROIs by multiplying the images with masks, then reshaped the data from 240\u00d7240\u00d7155 to 128\u00d7128\u00d7128, and normalized"}, {"title": "3 EXPERIMENTS", "content": "All programs were implemented in PyTorch and were trained on Google Colab and Compute Canada. For both MRI-CT and MRI-SPECT pairs, we trained the autoencoder for 100 epochs with an initial learning rate of 0.0001 and cosine decay to 3e-7, a mini-batch size of 4, and with the Adam optimizer (Kingma & Ba, 2014). We randomly held out 30 image pairs from the MRI-CT dataset and 50 pairs from the MRI-SPECT dataset as the standalone test set. To ensure the robustness of our model, we repeated our experiments three times and ensured that we had different test sets in each run.\nTo assess the usability of our fusion framework, we conducted an ROI-based brain tumor classification task between LGG and HGG using the BraTS 2019 data as we discussed previously. First, we randomly held out 40 patients (20 LGG and 20 HGG patients, 1152 slices in total) as a standalone test set. The rest of the data is used to train our model. We trained our fusion autoencoder for 25 epochs with a constant learning rate of 0.0001, a mini-batch size of 4, and with the Adam optimizer. We used our proposed SFNN-max fusion strategy to fuse T2 and FLAIR images. For the classification model, we used ResNet-50 for all experiments and trained with focal loss (Lin et al., 2017) followed (Zhou & Khalvati, 2024). We trained the model for 50 epochs with a constant learning rate of 0.001, a mini-batch size of 8, and an Adam optimizer. We ran the classification experiment for three trials with different train-validation splits to ensure the robustness and reliability of our findings.\nBaseline Model & Comparison. For comparison of image fusion results on MRI-CT and MRI-SPECT data, we considered five open-sourced state-of-the-art methods: IFCNN (Zhang et al., 2020), MSRPAN (Fu et al., 2021), MSDRA (Li et al., 2022), SwinFusion (Ma et al., 2022) and MRSCFusion (Xie et al., 2023), where the first three are CNN-based methods and the last two are Transformer-based methods. We rerun all models except MRSCFusion due to memory limitations, and we used the pre-trained weights of MRSCFusion from their official repository. For quantitative comparisons, we select five commonly used metrics in previous works Peak signal-to-noise ratio (PSNR), Structural Similarity (SSIM) (Wang et al., 2004), Feature Mutual Information (Haghighat et al., 2011), Feature SSIM (FSIM) (Zhang et al., 2011), and Information Entropy (EN). The downstream classification performance is evaluated using AUC, F1-Score, and Accuracy."}, {"title": "4 RESULTS AND DISCUSSIONS", "content": "Main Results. In Figure 3, we compare the fusion results of different methods across three randomly selected MRI-CT and MRI-SPECT test pairs. For the MRI-CT fusion task, we focus on the area with more tissue information from MRI images and bone structures from CT images (highlighted and zoomed in red). We expect that the dense information (e.g., bone structures) in CT images and soft tissues in MRI images should be simultaneously retained in the fused images. We observed that the MRSCFusion had undesirable pixel intensities and some of the details from the MRI image were missing, such as the brain contour line. SwinFusion preserves tissue information from MRI images well but fails to maintain the boundary details for both CT and MRI images. IFCNN retains MRI tissue details well but slightly weakens dense structures from CT images. MSRPAN struggles to distinguish tissue boundaries between CT and MRI, losing fine-grained details resulting in overly sharp edges. MSDRA produces unclear boundaries and lacks significant intensity differences, making it difficult to differentiate between dense boundaries and soft tissues. In contrast, our proposed method provides a clear, bright brain contour from MRI and better edge contrast from CT. Our fused images offer superior contrast between edges and inner tissues, preserving more edge and fine-grained information from both modalities. Visually, our results appear more natural and the overall contrast is more promising.\nFor the MRI-SPECT fusion task, we focus on the area with more morphological (color) information from the SPECT and texture information from MRI images for a better comparison. MRSCFusion introduces noticeable intensity distortions and loses texture details from MRI images. SwinFusion achieves satisfactory results, but some color information blurs the MRI texture. IFCNN effectively represents the functional information from SPECT images but still misses some fine details from MRI images. MSRPAN and MSDRA preserve color information well but blur some MRI texture details. Our fused images retain the appropriate color information from SPECT while preserving more structure and tissue details from MRI.\nThe quantitative metrics, computed over three distinct test sets, are reported with mean values and standard deviations in Table 1. Our EH-DRAN method achieves the best performance in terms of PSNR, FMI, FSIM, and Information Entropy for the MRI-CT fusion task. The high FMI, FSIM, and Information Entropy scores indicate that our fused images maintain superior structural similarity and contain richer information. Although our method shows a slightly lower SSIM score compared to SwinFusion, this may be attributed to SwinFusion's tendency to emphasize MRI features. In contrast, our approach balances the contributions from both MRI and CT images.\nFor the MRI-SPECT fusion task, our method consistently surpasses baseline methods in PSNR, SSIM, FMI, and FSIM. Despite slightly lower Entropy values than SwinFusion, all other metrics demonstrate that our approach effectively preserves more functional and morphological information from MRI and SPECT images. This outcome aligns with the qualitative fusion results discussed earlier.\nSelection of Fusion Strategies. As discussed in Section 2.2, our proposed fusion strategy offers several variants, including $\\phi(\u00b7) = mean()$, $sum()$, and $max ()$. We conducted an extensive qualitative and quantitative analysis of these strategies for both the MRI-CT and MRI-SPECT fusion tasks, with the results summarized in Table 2. For comparison, we used FER (Fu et al., 2021) and FL1N (Li et al., 2022), two parameter-free fusion strategies from prior research, as baselines.\nOur results demonstrated that the proposed fusion strategy consistently outperformed the baseline methods across all quantitative metrics. Notably, the robustness of our approach was evident, as the performance varied only slightly across different $\\phi(\u00b7)$ functions. This robustness underscores the stability and reliability of our strategy.\nFusion Time. Next, we assess the model complexity by examining the total number of trainable parameters and the image fusion time for each image pair using the MRI-SPECT dataset, as detailed in Table 3. The MRI-SPECT dataset is selected due to its larger number of image pairs and its representation of a more complex task, which closely mirrors real-world scenarios. Compared to other baselines, our proposed method has a reasonable number of parameters and achieves image fusion in only 1 second. The proposed parameter-free fusion strategy suggests great potential for real-time fusion in clinical settings. Furthermore, we expect the fusion time for MRI-CT pairs to be even shorter than that for MRI-SPECT, primarily due to the absence of color space conversion between RGB and YCbCr.\nAblation Study. We performed our ablation studies in two folds. (1) to assess the effectiveness of the proposed Dense Residual Gradient Operator (DRGO) module to learn edge details, (2) to evaluate the impact of incorporating the gradient loss during model optimization. The ablation experiments were performed using both the MRI-CT and MRI-SPECT datasets. We hypothesize that (1) the edge enhancer aids the autoencoder in extracting meaningful edge features, thereby reinforcing edge details from both source images in the fused images, and (2) the gradient loss assists the model in learning and reproducing fine texture details in the reconstructed output. Table 4 presents the results of our ablation study. We began with a baseline model that excluded both the DRGO module and the gradient loss ($L_{grad}$). Adding $L_{grad}$ to the loss function led to an improvement across all evaluation metrics compared to the baseline, confirming the benefit of gradient loss for optimizing texture detail learning. When we further integrated the DRGO module into the model, we observed additional improvements across all metrics. The notable increase in SSIM and FSIM scores demonstrates the DRGO module's effectiveness in preserving edge information from both source images, thus validating its contribution to overall fusion quality."}, {"title": "4.2 CLASSIFICATION RESULTS", "content": "To validate the efficacy of the proposed fusion framework in clinical multi-modality brain tumor images, we trained a classification model to validate the efficacy of the proposed fusion framework in distinguishing between HGG and LGG brain tumor types. Following (Xie et al., 2024b), we used T2 and FLAIR images for the classification task. For comparison, we evaluated three approaches: single-modality images (using either T2 or FLAIR), dual-modality images (using a channel-wise concatenation of T2 and FLAIR), and T2-FLAIR fused images. The classification results, presented in Table 5, demonstrate that utilizing the fused image generated by our proposed fusion framework significantly improves performance in terms of AUC and F1-Score compared to the single-modality and dual-modality baselines. This improvement highlights our method's ability to enhance details in the brain tumor ROIs while preserving overall image contrast, corroborating the findings from the previous section. These results further suggest the potential of leveraging fused images for improved accuracy in real clinical diagnosis."}, {"title": "5 CONCLUSIONS", "content": "In this work, we proposed a novel asymmetric autoencoder architecture incorporating a Dilated Residual Attention Network (DRAN) for effective multi-scale feature extraction. Additionally, we integrated a Dense Residual Gradient Operator (DRGO) as an edge enhancer to capture fine-grained edge details. We introduced a family of parameter-free softmax-weighted fusion strategies for multimodal image fusion, designed to operate without requiring parameter computation during both training and inference phases, pushing us a step further to achieve real-time image fusion. Our extensive evaluations demonstrate that the proposed method outperforms several baseline approaches in subjective visual quality and objective fusion metrics. The improved performance in the downstream brain tumor classification task further highlights the effectiveness of our fusion framework. We envision our approach being applied to subsequent disease localization tasks, radiotherapy treatment planning, and surgical navigation in real-world clinical settings.\nIn the future, we plan to extend our method from 2D to 3D medical image fusion, as 3D data is more common in practical medical imaging. Furthermore, we aim to explore the integration of Transformers or Selective State Spaces Models (Gu & Dao, 2023) to enhance feature extraction and image fusion capabilities."}, {"title": "A MORE ON FUSION STRATEGIES", "content": "Qualitative of different fusion strategies: We selected two variants of our proposed SFNN methods, $sum()$ and $max()$, due to their superior visual quality and performance, as illustrated in Figure 4. For the MRI-CT fusion task, our method demonstrated clear advantages over the FER strategy (Fu et al., 2021), achieving enhanced fidelity with sharper delineation of inner tissue structures from MRI images and well-preserved boundaries from CT images. Furthermore, compared to the FL1N method (Li et al., 2022), our approach produced brighter and more defined edges, thereby improving the contrast between edges and tissues. In the case of MRI-SPECT fusion, our method retained more detailed features from the MRI image compared to the FER method. While the visual distinctions between our method and the FL1N strategy were subtle, the quantitative analysis further demonstrates the superiority of our approach."}]}