{"title": "MECG-E: Mamba-based ECG Enhancer for Baseline Wander Removal", "authors": ["Kuo-Hsuan Hung", "Kuan-Chen Wang", "Kai-Chun Liu", "Wei-Lun Chen", "Xugang Lu", "Yu Tsao", "Chii-Wann Lin"], "abstract": "Electrocardiogram (ECG) is an important non- invasive method for diagnosing cardiovascular disease. However, ECG signals are susceptible to noise contamination, such as elec- trical interference or signal wandering, which reduces diagnostic accuracy. Various ECG denoising methods have been proposed, but most existing methods yield suboptimal performance under very noisy conditions or require several steps during inference, leading to latency during online processing. In this paper, we propose a novel ECG denoising model, namely Mamba-based ECG Enhancer (MECG-E), which leverages the Mamba architecture known for its fast inference and outstanding nonlinear mapping capabilities. Experimental results indicate that MECG-E surpasses several well-known existing models across multiple metrics under different noise conditions. Additionally, MECG-E requires less inference time than state-of-the-art diffusion-based ECG denoisers, demonstrating the model's functionality and efficiency.", "sections": [{"title": "I. INTRODUCTION", "content": "Cardiovascular diseases are the leading cause of death worldwide\u00b9, necessitating the accurate and reliable diagnostic tools. Among these, the electrocardiogram (ECG) plays a crucial role as a non-invasive method for monitoring the heart's electrical activity [1]. However, ECG signal acquisition is frequently compromised by various types of noise and artifacts, which can significantly degrade signal quality and impact diagnostic accuracy [2].\nTo address the above issues, several signal processing techniques have been proposed. Commonly used filter-based methods include Finite Impulse Response (FIR) filters [3], [4], Infinite Impulse Response (IIR) filters [5], and adaptive Kalman filters [6]. Additionally, mode-decomposition-based approaches [7]-[9] has been employed to decompose noisy signals into intrinsic mode functions (IMFs), with the IMFs containing the most noise being removed. Statistical methods, such as Independent Component Analysis (ICA) [10], [11], have also been explored. Among transform-based methods, wavelet transform (WT) techniques [12]-[14] have been primarily developed and have shown promising results. Despite their widespread application, these traditional methods are typically effective under ideal conditions (e.g., low signal-to-noise ratio and single noise type), and often heavily rely on human expertise or trial-and-error. These limitations and challenges notably constrain their denoising ability and generalizability in challenging scenarios.\nIn recent years, the advent of deep learning (DL) has revolutionized the field of ECG signal processing. Various deep learning methods have been introduced to improve the quality of ECG signals by effectively eliminating noise and artifacts, such as deep recurrent neural networks (DRNN) [15] and fully convolutional networks (FCN) [16]. Additionally, DeepFilter [17] has introduced Multi-Kernel Linear And Non-Linear (MKLANL) filter modules for multi-scale feature denoising. The use of Generative Adversarial Networks-based model (GANs) [18], [19] to generate high-fidelity ECG signals from noisy inputs has also been explored.\nRecently, with the growing interest in diffusion models, score-based diffusion models [20], [21] have been applied and have notably outperformed other methods. However, diffusion models require numerous sampling steps to achieve the desired quality, resulting in considerably slower inference speeds.\nTo address the challenges of suboptimal performance and slow inference speeds in existing ECG denoising methods, we propose a novel model, Mamba-based ECG Enhancer (MECG- E). MECG-E incorporates the Mamba framework, which is known for its rapid inference and high performance [22], into an advanced ECG signal enhancement architecture. The key contributions of this paper are outlined as follows:\n\u2022\nMECG-E consistently outperforms existing models across various metrics under a wide range of noise levels. Com- pared to the state-of-the-art (SOTA) model, it significantly reduces the inference time, proving its efficiency and effectiveness in practical applications.\n\u2022\nTo the best of our knowledge, MECG-E is the first model to apply time-frequency (TF) domain features to the ECG denoising task. Additionally, we conduct an in-depth comparison of the performance in two input feature types: \"Complex\" and \"Mag.+Phase\u201d.\n\u2022\nA comprehensive analysis of the composite loss function is provided, evaluating the impact of different loss functions"}, {"title": "II. METHODOLOGY", "content": "Fig.1 illustrates the architecture of proposed MECG-E. The noisy ECG signal $x \\in R^{1\\times L}$ is first converted into the complex spectrum $X \\in R^{T\\times F\\times 2}$ by a short-time Fourier transform (STFT), where L, T and F denote the length of ECG signal, time dimensions and frequency dimensions, respectively. For the better prediction, the power-law compression is applied on the complex spectrum as follows:\n$X^c = |X|^c e^{jX_p} = X_m e^{jX_p} = X_r + jX_i$\nwhere $X_m$, $X_p$, $X_r$, and $X_i \\in R^{T\\times F}$ are the magnitude, wrapped phase, real, and imaginary components of the com-"}, {"title": "III. RELATED WORK", "content": "The proposed MECG-E is built on Mamba, an advanced variant of state space models (SSMs) [23], [24]. SSMs are effective for sequence modeling by transforming input sequences x(t) into output sequences y(t) through hidden states h(t), which can be formulated as follows:\n$h'(t) = Ah(t) + Bx(t)$,\n$y(t) = Ch(t)$\nWhere A, B, and C are the parameters used to define the sequence-to-sequence transformation in two stages. The \"continuous parameters\u201d (\u0394, A, B) are transformed to \"discrete parameters\" (A, B) by the zero-order hold (ZOH):\n$A = \\exp(\\Delta A)$,\n$B = (\\Delta A)^{-1}(\\exp(\\Delta A) \u2013 I)\\cdot \\Delta B$\nTraditional SSMs effectively capture temporal dependencies but are constrained by linear time-invariance, with fixed model parameters that lack adaptability to changing inputs. The Mamba model [22] overcomes these limitations through a selective state space mechanism that dynamically adjusts parameters based on input. This mechanism filters irrelevant information and retains crucial data, improving performance on tasks with complex and long-range dependencies. Additionally, Mamba incorporates structural optimizations for memory usage and processing speed, particularly for GPU deployment, enabling efficient handling of long sequences.\nRecently, the Mamba-based models have demonstrated success in various fields such as natural language processing [25], computer vision [26], [27], and speech signal processing [28]-[30].In [31], Mamab has been utilized as a fundamental architecture for ECG classification, showing promising perfor- mance; however, its application to ECG signal denoising, as a regression task, remains underexplored. This paper investigates the potential of the Mamba model for denoising ECG signals, leveraging its advanced framework to effectively capture the complex dynamics of physiological data."}, {"title": "A. Model Architecture", "content": "1) Encoder: The encoder comprises two convolutional blocks and a dilated DenseNet [32]. Each convolutional block includes a 2D convolutional layer, instance normalization [33], and a parametric rectified linear unit (PReLU) activation [34]. The first convolutional block increases the dimensions of the input feature. The dilated DenseNet, positioned between the convolutional blocks, extends the receptive field along the time axis through multiple convolutional layers and aggregates multi-level features using dense connections. The second convolutional block downsamples the feature by expanding the stride in the convolutional layer to reduce the computational complexity. Overall, the encoder transforms the input features into a higher-dimensional TF-domain representation with a reduced sampling rate.\n2) Time-Frequency Bi-direcional Mamba (TF-Bi-Mamba): As illustrated in Fig.1, the TF-Bi-Mamba model consists of two sequential Bidirectional Mamba (Bi-Mamba) blocks: the Time Bi-Mamba block captures temporal dependencies, and the Frequency Bi-Mamba block captures frequency dependencies. This structure allows the model to effectively process both time and frequency aspects of the input data.\nIn each Bi-Mamba block, the input is processed in forward and backward directions using two parallel Mamba layers. The input for the backward Mamba is flipped before processing and reverted afterward. The outputs from both layers are con- catenated, followed by a residual connection and a transposed convolution layer. This structure allows Bi-Mamba to effectively combine both current and historical information.\nFor the Mamba layer, we adopt the architecture outlined in [22], which combines the H3 block [35] with the widely- used MLP block. In comparison to the H3 block, the Mamba layer replaces the first multiplicative gating mechanism with an activation function. Additionally, unlike the standard MLP block, the Mamba layer integrates a State-Space Model (SSM) into its main computational branch, further improving its capacity for temporal modeling. The SiLU [36] activation function is employed to maintain a smooth and efficient transformation of input data."}, {"title": "3) Decoder", "content": "We adopt the decoder architecture in [37], which comprises two components: the Magnitude Mask De- coder and the Phase Decoder. The Magnitude Mask Decoder is designed to predict a magnitude mask M, which is then multiplied with the noisy magnitude spectrum to obtain the enhanced magnitude spectrum $\\hat{X_m}$:\n$\\hat{X_m} = (|X_m|^c \\odot M)$\nwhere $\\odot$ denotes element-wise multiplication and c is the compression exponent defined in Eq.3. The Phase Decoder consists of a dilated DenseNet, a deconvolutional block, and a parallel phase estimation architecture. This architecture employs two parallel 2D convolutional layers to produce the pseudo- real $X_p^{(r)}$ and pseudo-imaginary $X_p^{(i)}$ components. Depending on the input features, two distinct reconstruction methods are employed.\n\u2022 Complex Spectrum: The enhanced magnitude spectrum $\\hat{X_m}$ is initially combined with the input phase $X_p$ to form the pseudo-complex spectrum. This spectrum is then element-wise added to the pseudo-real and pseudo-imaginary components, resulting in the final complex spectrum.\n$X_r = \\hat{X_m} \\cos(X_p) + X_p^{(r)}$,\n$X_i = \\hat{X_m} \\sin(X_p) + X_p^{(i)}$\nThe complex spectrum is then processed with an inverse STFT, converting it back into the reconstructed ECG signal.\n\u2022 Magnitude and Phase Spectrum: The pseudo-real and pseudo-imaginary components are then combined using the two-argument arctangent (Arctan2) function to predict the clean wrapped phase spectrum $\\hat{X_p}$.\n$\\hat{X_p} = \\arctan2\\left( \\frac{X_p^{(i)}}{X_p^{(r)}} \\right) = \\frac{\\pi}{2} \\text{Sgn}^*\\left(\\frac{X_p^{(i)}}{X_p^{(r)}} \\right) \\cdot [\\text{Sgn}^*(\\frac{X_p^{(i)}}{X_p^{(r)}}) - 1]$\nWhere Sgn* (t) equals to 1 when t > 0 and -1 otherwise. Combining the enhanced magnitude spectrum $\\hat{X_m}$ with the wrapped phase spectrum $\\hat{X_p}$, the ECG signal is reconstructed."}, {"title": "B. Loss function", "content": "The loss function is a combination of three components: time loss $L_{time}$, complex loss $L_{cpx}$, and consistency loss $L_{con}$. The time and complex losses optimize the enhanced ECG in the time and TF domains, respectively. The consistency loss minimizes the inconsistency caused by inverse STFT and STFT. The loss function $L_{all}$ is depicted in the following equation:\n$L_{all} = \\gamma_1 L_{time} + \\gamma_2 L_{cpx} + \\gamma_3 L_{con.}$,\n$L_{time} = E_{y, \\hat{y}}[||y - \\hat{y}||_1]$,\n$L_{cpx} = E_{X_c, Y_c}[||Y_c - X_c||_2]$,\n$L_{con} = E_{X_c}[||X_c-(F_{stft}(x)||_2]$,\nwhere $\\gamma_1, \\gamma_2$, and $\\gamma_3$ are the weights for the losses, and $x, y, X_c,$ and $Y_c$ represent the enhanced signal, target signal, and their respective complex spectrums, respectively."}, {"title": "IV. EXPERIMENT", "content": "A. Dataset\nTo ensure consistency and comparability, we followed the dataset settings from previous studies [17], [20]. The clean ECG records from the QT Database\u00b2 [38] were corrupted with noise profiles from the MIT-BIH Noise Stress Test Database\u00b2 (NSTDB) [39]. The noise was normalized to the range of the corresponding ECG signals and rescaled by multiplying by a random factor between 0.2 and 2.0. This preprocessing pipeline ensures that the noise added to the ECG signals covers different levels of noise, allowing for a comprehensive evaluation of the denoising methods.\nB. Implementation Details\nThe MECG-E was trained using the AdamW optimizer [40] with a batch size of 96 and a learning rate of 1e-4 for 40 epochs. The weight decay was set to 1e-2, and an ExponentialLR scheduler [41] was employed. For the STFT, a Hamming window of 64 points and a hop length of 8 points were used. The compression exponent c in Eq. 3 was set to 0.3. The encoder's dilated DenseNet utilized four convolutional layers with dilation sizes of {1, 2, 4, 8}. We configured the model with four TF-Bi-Mamba modules, each having a dimension of 32. The coefficients $\\gamma_1$, $\\gamma_2$, and $\\gamma_3$ in Eq. 9 were given values of 0.5, 1, and 0.5, respectively. The implementations of MECG-E will be released upon the publication of this work.\nC. Baselines and Evaluation metrics\nWe compared our proposed MECG-E method with several ECG denoising methods, including FIR [3], IIR [3], DRNN"}, {"title": "Evaluation metrics", "content": "To quantitatively evaluate ECG signal distortion, we employed four distance-based metrics commonly used in previous studies [17], [20]:\n\u2022 Sum of the Squares of the Distances (SSD) [42]: SSD calculates the sum of squared differences between the original and processed signals, providing a detailed comparison of signal similarity over time.\n$SSD(x, y) = \\sum_{n=1}^{N} [y(n) \u2013 x(n)]^2.$\n\u2022 Absolute Maximum Absolute Distance (MAD) [42]: MAD measures the maximum absolute difference between the original and processed signals. It is a widely used metric for evaluating ECG signal quality, focusing on the largest deviation caused by processing.\n$MAD(x, y) = \\max|y(n) \u2013 x(n)|, 1\u2264 n \u2264 N.$\n\u2022 Percentage Root-Mean-Square Difference (PRD) [42]: PRD is a percentage-based metric that quantifies the distor- tion in the processed signal.\n$PRD(x, y) = \\sqrt{\\frac{\\sum_{n=1}^{N}[y(n) \u2013 x(n)]^2}{\\sum_{n=1}^{N}[y(n) \u2013 \\sum_{n=1}^{N}x(n)]}} \u00d7 100\\%$.\nCosine Similarity (CosSim): CosSim measures the angle between two vectors by calculating the normalized dot product based on their Euclidean L2 norms.\n$CosSim(x, y) = \\frac{(x, y)}{||x|| ||y||}$\nFor SSD, MAD, and PRD, lower values indicate better performance, while higher values are better for CosSim."}, {"title": "V. RESULTS", "content": "The existing methods primarily use time-domain signals as input. In this paper, the proposed MECG-E is applied to the TF-domain for the first time, utilizing the 'Complex' or 'Mag.+Phase' as input features. Table I provides the mean and standard deviation of the evaluated metrics, with the bold and underlined values, respectively, representing the best and second-best results for each input feature. Among time-domain methods, the diffusion-based DeScoD-ECG outperforms other baseline approaches, especially when results are averaged across multiple runs (denoted as 'n-shots'). In the TF-domain, aside from the SSD score for 'Complex', all other metrics surpass the baseline methods, and the 'Mag.+Phase' shows slightly better performance compared to 'Complex'. Conse- quently, we choosed 'Mag.+Phase' as the input feature in the subsequent ablation study.\nWe conducted an ablation study to assess the impact of loss functions, as shown in Table I. Similar patterns were observed for both 'Complex' and 'Mag. + Phase' inputs. First, combining all three loss functions resulted in either the best or second-best performance, except for the MAD score with the 'Complex' input. Removing the time loss ($L_{time}$) results in the most significant performance decrease, likely because the evaluation metrics are calculated in the time domain, making them highly sensitive to loss in the time domain. Furthermore, although adding consistency loss ($L_{con}$) slightly degrades the MAD score, it significantly improved the other three metrics, demonstrating its overall benefit. In summary, removing the time loss leads to substantial degradation across all metrics, removing the consistency loss mainly impacts CosSim, and removing the complex loss most affects SSD performance. Therefore, the combination of all three loss functions provides the best balance across the evaluation metrics.\nWe assessed the model's performance across various noise amplitudes, as illustrated in Fig.2, which also presents the results of DeScoD-ECG averaged over multiple runs (denoted as 'n-shots'). The results show that MECG-E (red line) consis- tently exceeds others with different amounts of contamination. Notably, the performance improvement of MECG-E over"}, {"title": "VI. CONCLUSION", "content": "In this paper, we proposes MECG-E, an ECG denoising model built on the Mamba architecture. Our findings reveal that MECG-E surpasses existing methods across all evalu- ated metrics, even under different SNR levels. Compared to SOTA diffusion-based methods, MECG-E demonstrates greater efficiency by requiring less inference time. We also examine different STFT settings and compression exponents to identify the optimal configurations for ECG processing. In the future, we will focus on integrating MECG-E into other downstream classification tasks to further investigate its effectiveness. Additionally, exploring the model's potential for real-time applications could provide valuable insights and enhance its practical utility."}]}