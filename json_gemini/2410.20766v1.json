{"title": "A Static and Dynamic Attention Framework for Multi Turn Dialogue Generation", "authors": ["WEINAN ZHANG", "YIMING CUI", "KAIYAN ZHANG", "YIFA WANG", "QINGFU ZHU", "LINGZHI LI", "TING LIU"], "abstract": "Recently, research on open domain dialogue systems have attracted extensive interests of academic and industrial researchers. The goal of an open domain dialogue system is to imitate humans in conversations. Previous works on single turn conversation generation have greatly promoted the research of open domain dialogue systems. However, understanding multiple single turn conversations is not equal to the understanding of multi turn dialogue due to the coherent and context dependent properties of human dialogue. Therefore, in open domain multi turn dialogue generation, it is essential to modeling the contextual semantics of the dialogue history, rather than only according to the last utterance. Previous research had verified the effectiveness of the hierarchical recurrent encoder-decoder framework on open domain multi turn dialogue generation. However, using RNN-based model to hierarchically encoding the utterances to obtain the representation of dialogue history still face the problem of a vanishing gradient. To address this issue, in this paper, we proposed a static and dynamic attention-based approach to model the dialogue history and then generate open domain multi turn dialogue responses. Experimental results on Ubuntu and Opensubtitles datasets verify the effectiveness of the proposed static and dynamic attention-based approach on automatic and human evaluation metrics in various experimental settings. Meanwhile, we also empirically verify the performance of combining the static and dynamic attentions on open domain multi turn dialogue generation.", "sections": [{"title": "1 INTRODUCTION", "content": "Since the question \u201cCan machines think?\u201d proposed by A.M. Turing [33] in 1950, passing the Turing test\u00b9 becomes a long term goal for the artificial intelligence research. As the Turing test is designed as an imitation game through human-machine conversation, the research of open domain conversations attracted wide attention of researchers. However, until now, training an open domain conversational system to passing the Turing test is still a non-trivial task. In the early work, an unsupervised clustering approach [18], a phrase-based statistical machine translation approach [19] and a vector space model (VSM)-based approach [3] are proposed to generate responses of open domain conversations. Recently, with the blooming of deep learning techniques, especially the neural sequence-to-sequence learning (Seq2Seq) models, the generation of open domain conversational responses follows the framework of an end-to-end encoding and decoding process [9, 10, 22, 24, 25, 30, 35, 37]. Previous works on short text conversation [24] (single turn conversation generation) successfully promote the research of generative approaches on open domain conversation generation. However, a session of human conversations usually consists of multiple utterances so that it is often called a multi turn dialogue. Therefore, in the imitation of human conversations, an open domain dialogue model should consider the historically utterances rather than the last utterance. Table 1 shows the impact of historically utterances(dialogue context) on response generation.\nAs shown in Table 1, when giving the same input message: \"What do you like best there?\", different responses are separately generated by different speakers according to the different dialogue context. However, an open domain conversation model, which does not consider the dialogue context, will generate a unique response according to the given message (the last utterance). It will lead to the generation of incoherent human-computer dialogues and thus seriously impact the users' experience. Therefore, recent works begin to model the coherence and context-aware generation [8, 11, 12, 21, 23, 26, 31, 36, 39, 44] of open domain conversations. An early context-sensitive dialogue generation model is the hierarchical recurrent encoder-decoder (HRED) [21]. Two RNNs are utilized to model each utterance in dialogue context and each word in an utterance in a unique framework. Due to the generation of generic responses, they [23] further proposed a variational model based on the HRED model, which is called variational HRED (vHRED). A stochastic latent variable is added at each utterance to improve the diversity of response generation. Furthermore, the CVAE model [43] is proposed by introducing a conditional variational autoencoder into the encoder-decoder framework to learn the diversity of dialogue context for conversation generation.\nBesides the introducing of stochastic latent variable to improve the diversity of generated responses, the attention mechanism is also utilized to model the dialogue context and generate responses. Two typical research works are the HRAN [36] and the WSI [31]. The former is proposed to hierarchically model the interactions between words and utterances. While, the latter models the dialogue context through a recurrent neural network in encoding step."}, {"title": "2 RELATED WORK", "content": "In the early work, a data-driven statistical machine translation (SMT)-based approach [19] had been proposed to generate responses in social media platform. It verified that the SMT-based generation model outperforms the IR-based model in response generation. It is also a representative work on open domain dialogue generation.\nRecently, the neural encoder-decoder framework for sequential generation are widely used to dialogue generation [13, 15, 21\u201325, 30, 37, 39, 48]. The first sequence-to-sequence (Seq2Seq)-based neural network [30] is proposed to improve the performance of neural machine translation. The same framework is then proposed to applying to the neural dialogue generation [35]. Based on the above Seq2Seq framework, the attention-based neural Seq2Seq model [2] is proposed to resolve the neural machine translation in the RNN-based encoder-decoder architecture. The use of soft alignment and weight calculation over each time step improve the performance of the vanilla Seq2Seq model on machine translation and further adapt to dialogue generation. Shang et al. (2015) [24] proposed a hybrid attentive mechanism to model the global and local relevance between input message and target response in short text conversation (single turn conversation). Shao et al. (2017) [25] introduced an attentive model by adding a self-attention in decoding phase to improve the coherence and diversity of generated responses. Yao et al. (2017) [37] proposed a two-stage approach which includes a cue words inference and a cue word guided response generation in an encoder-decoder framework. Zhu et"}, {"title": "3 THE PROPOSED APPROACH", "content": "A general Seq2Seq learning-based neural network for multi turn conversation generation in open domain usually consists of an encoder and a decoder. The encoder converts the dialogue utterances into a dense vector or a matrix to represent the semantics of the dialogue history in a multi turn dialogue session. While, taking the output of the encoder as an input, the decoder then generates a response word by word. In case of the generation of multi turn"}, {"title": "3.1 Preliminary", "content": "A general Seq2Seq learning-based neural network for multi turn conversation generation in open domain usually consists of an encoder and a decoder. The encoder converts the dialogue utterances into a dense vector or a matrix to represent the semantics of the dialogue history in a multi turn dialogue session. While, taking the output of the encoder as an input, the decoder then generates a response word by word. In case of the generation of multi turn"}, {"title": "3.2 The Proposed Model", "content": "The proposed attentive framework, which consists of a static and dynamic attention mechanism, for multi turn dialogue generation is based on the encoder-decoder architecture. To obtain the context representation, the encoder of the proposed framework hierarchically encode the representation of the utterances in a dialogue session. To obtain the utterance representation, we take the advantages of the three prevalent approaches to encoding the dialogue context information for the generation of multi turn dialogue responses [31, 36, 39]. We present two mechanisms to obtain the representation of an utterance. One is the GRU [5] model, which is recurrently modeling the embedding of words in each utterance. The other is the Transformer [34] encoding, which includes the multi-head self-attention and positional encoding. To obtain the context representation in upper layer of the proposed model, the proposed attentive framework, which consists of a dynamic and a static attention, measures the weight of each utterance in representing the meaning of a dialogue session. Figure 2 shows the proposed framework."}, {"title": "Static Attention", "content": "As shown in Figure 2, the static attention mechanism calculates the importance of each utterance as $e_i$ or $a_i (i \\in \\{1, ..., S\\})$.\n$e_i = V^T tanh(Wh_i + Uh_s)$                                                                                       (1)"}, {"title": "Dynamic Attention", "content": "Rather than the unchanged weights of each utterance in decoding phase, the dynamic attention maintains a weighting matrix and updates the weights of each utterance during the decoding process. The formal illustration of the dynamic attentive mechanism is as follows:\n$e_{i.t} = V^T tanh(Wh_i + Us_{t-1})$                                                                                    (5)\n$a_{i,t} = \\frac{exp(e_{i,t})}{\\sum_i exp(e_i)}$                                                                                  (6)\n$c_t = \\sum_i a_{i,t}h_i$                                                                                               (7)"}, {"title": "Hybrid Attention", "content": "The hybrid attention denotes the combination of the static and dynamic attentions. There are two natural ways for the combination. First is to concatenate the context vector c in static attention-based encoding and the context vector $c_t$ in dynamic attention-based encoding. Second is to sum up the context vector c in static attention-based encoding and the context vector $c_t$ in dynamic attention-based encoding. Equation (9) and (10) denote the above two types of combination respectively.\n$s_t = f(y_{t-1}, s_{t-1}, [c, c_t])$                                                                                             (9)\n$s_t = f (y_{t-1}, s_{t-1}, c + c_t)$                                                                                         (10)\nHere, besides the [c, ct] and c + ct, other notations are same to the above definition.\nIn addition, we also try another two ways to explore the advantages of the two attention mechanisms. First is the linear interpolation of the context vector c in static attention-based encoding and the context vector $c_t$ in dynamic attention-based encoding. Equation (11) shows the state st with linear interpolation.\n$s_t = f(y_{t-1}, s_{t-1}, \\alpha \\cdot c + \\beta \\cdot c_t)$                                                                            (11)\nThere are also three ways to fix the values of \u03b1 and \u03b2.\n\u2022 We can empirically let both \u03b1 and \u03b2 equals to 1, as shown in Equation (10)."}, {"title": "Token-level Information", "content": "As shown in Figure 2, the proposed static and dynamic attentions only model the utterance-level information for multi turn dialogue generation. To verify the impact of token-level information, we further introduce the token representations in the proposed static and dynamic attention models. Concretely, first, the original input representations of utterances are substituted by the token embeddings so that the model can take the token-level attention for decoding. And the rest parts of the model are unchanged. Second, for each utterance, we concatenate the original utterance representation and the representation obtained by token-level attention. The model then takes the concatenated representation as the input of decoder.\nNoticed that the main difference between the proposed model and the above three state-of-the-art models are three folds:\n\u2022 First, we propose two utterance-level attentions for weighting the importance of each utterance in dialogue context, which is more simple in structure and has less number of parameters than the hierarchical attention approach.\n\u2022 Second, in the proposed approach, the weights of utterance in dialogue context are learned by two attention mechanisms from the data, which is more reasonable and flexible than the heuristic based approach.\n\u2022 Third, other utterance-level representation learning approaches, such as self-attention based utterance represen-tation learning, can be integrated to the proposed attention-based dialogue encoding and response generation framework."}, {"title": "4 EXPERIMENTAL RESULTS", "content": "Datasets and Parameters: In this paper, two experimental datasets are chosen to verify the performance of multi turn dialogue generation. One is the Ubuntu dataset [14], which is extracted from the Ubuntu Internet Relayed Chat (IRC) channel and widely used for the generation of open domain dialogue responses [20, 21, 23]. We follow the data"}, {"title": "4.1 Experiment Settings", "content": "Datasets and Parameters: In this paper, two experimental datasets are chosen to verify the performance of multi turn dialogue generation. One is the Ubuntu dataset [14], which is extracted from the Ubuntu Internet Relayed Chat (IRC) channel and widely used for the generation of open domain dialogue responses [20, 21, 23]. We follow the data"}, {"title": "Pretraining Enhanced Results", "content": "In this section, we want to verify the impact of pre-trained language models on the proposed contextual dialogue generation framework.\nWe tried to integrate the GPT2 3 model into the proposed static and dynamic attention-based models. Thanks to the large-scale parameters and the Transformer-based model architecture, one of the strongest capabilities of the pretrained models is precisely representing the semantics of words. Therefore, in considering not to make fundamental changes to the proposed framework, we use a simple way to take the advantages of GPT2 to the proposed attention-based dialogue generation framework. In detail, we first use the tokenizer of the GPT2 to transfer the tokens in an input (a sequence of utterances, e.g., dialogue context) to the embeddings of the GPT2 model. Second, these token embeddings are encoded by the proposed static and dynamic attention-based models. At last, in decoding phase, the generated list of token IDs"}, {"title": "4.2 Evaluation and Results", "content": "The automatic evaluation metrics for the generated dialogue responses in open domain is still an open problem. The BLEU score [16] is a widely used in evaluating the performance of machine translation systems. However, due to the difference between machine translation and dialogue generation, BLEU is not suitable for dialogue generation as the meanings of the referred responses are various so that they may share less common words. Meanwhile, the perplexity is often used to evaluate the performance of language model, but is not performing well on evaluating the relevance between messages and responses in dialogue [11, 24].\nTo address the above issues, in this paper, we use the embedding metric that is proposed by [21] and also used in [23] for evaluating the relevance of the generated dialogue responses to the dialogue context. Rather than measuring the similarity between a generated responses \u00ee and the ground-truth responses r in token-level or n-gram level, the embedding metric measures their semantic similarity by matching the semantic representations. There are three aspects in the embedding metric, namely Average, Greedy and Extrema."}, {"title": "4.2.1 Automatic Evaluation", "content": "The automatic evaluation metrics for the generated dialogue responses in open domain is still an open problem. The BLEU score [16] is a widely used in evaluating the performance of machine translation systems. However, due to the difference between machine translation and dialogue generation, BLEU is not suitable for dialogue generation as the meanings of the referred responses are various so that they may share less common words. Meanwhile, the perplexity is often used to evaluate the performance of language model, but is not performing well on evaluating the relevance between messages and responses in dialogue [11, 24].\nTo address the above issues, in this paper, we use the embedding metric that is proposed by [21] and also used in [23] for evaluating the relevance of the generated dialogue responses to the dialogue context. Rather than measuring the similarity between a generated responses \u00ee and the ground-truth responses r in token-level or n-gram level, the embedding metric measures their semantic similarity by matching the semantic representations. There are three aspects in the embedding metric, namely Average, Greedy and Extrema."}, {"title": "Hybrid Attention Results", "content": "As described in Section 3.2 hybrid attention, there are 5 ways to combine the proposed static and dynamic attentions to a hybrid attention. We use concat and sum to denote the concatenation and directly sum of the static and dynamic attentions, as shown in Equation (9) and (10), respectively learnable and attention indicate the two ways to fix the parameters of linear combination of the static and dynamic attentions, as shown in Equation (11). max and mean denote the element-wised max and average pooling of the static and dynamic attentions, respectively.\nAs shown in Table 4 and 5, in tuning the experimental parameters, we found that the embedding metric and the distinct metric have no obvious positive correlation. When the embedding metric achieve the best performance, the distinct metric can not be simultaneously optimal, and vice versa. Therefore, to show the results more comprehensively, in the hybrid attention experiments, we separately show the best performance of the embedding metric and distinct metric. The unidirectional GRU is used for obtaining utterance representation. In the rest of this paper, we will use the same way to show the experimental results."}, {"title": "Token-level Information Results", "content": "Previous experiments of the proposed static and dynamic attention models are based on the utterance-level representations. In this section, we will further explore the performance of adding token-level information as an enhancement for multi turn dialogue generation. In experiments, we first replace each utterance"}, {"title": "Human Evaluation", "content": "For human evaluation, two evaluation metrics are chosen, namely Coherence and Natu-ralness. As the example shown in Table 1, in multi turn dialogue generation, a generated response should not only dependent on the last input utterance, but should also consider the dialogue context in a session. Coherence is used to measure the contextual relevance between a generated response and the context. The Coherence score is one of 0,1 or 2, where 0, 1, 2 denote incoherent, neutral and coherent, respectively.\nHowever, in some cases, a contextual relevant response generated by a model may not be as natural as human responded. For example, an input message, \"Can you tell me the way to the nearest bazaar?\", the generated response \"Yes, I can tell you the way to the nearest bazaar.\" is relevant to the input. But it is not a natural response. Another example of an input and response pair is \"I don't know what you are talking about!\u201d and \u201cI don't know what you are talking about!\u201d. From the above examples, we can see that besides the Coherence, the naturalness of a generated response is also an important measure. Therefore, we proposed a metric to evaluate the naturalness of a generated response. For human evaluation, given a dialogue context and a response generated by a model, Naturalness denotes whether the response can be an alternative to a human response. The Naturalness score is 1 or 0, which indicates that the generated response is from a human or a model.\nBesides the human evaluation metrics, we also automatically calculate the Diversity score of the generated responses by all the baselines and our model. Here, for a generated response, we first calculate the number of distinct tokens in the response and then divide to the total number of distinct tokens in its dialogue context, which includes the number of"}, {"title": "4.2.2 Human Evaluation", "content": "For human evaluation, two evaluation metrics are chosen, namely Coherence and Natu-ralness. As the example shown in Table 1, in multi turn dialogue generation, a generated response should not only dependent on the last input utterance, but should also consider the dialogue context in a session. Coherence is used to measure the contextual relevance between a generated response and the context. The Coherence score is one of 0,1 or 2, where 0, 1, 2 denote incoherent, neutral and coherent, respectively.\nHowever, in some cases, a contextual relevant response generated by a model may not be as natural as human responded. For example, an input message, \"Can you tell me the way to the nearest bazaar?\", the generated response \"Yes, I can tell you the way to the nearest bazaar.\" is relevant to the input. But it is not a natural response. Another example of an input and response pair is \"I don't know what you are talking about!\u201d and \u201cI don't know what you are talking about!\u201d. From the above examples, we can see that besides the Coherence, the naturalness of a generated response is also an important measure. Therefore, we proposed a metric to evaluate the naturalness of a generated response. For human evaluation, given a dialogue context and a response generated by a model, Naturalness denotes whether the response can be an alternative to a human response. The Naturalness score is 1 or 0, which indicates that the generated response is from a human or a model.\nBesides the human evaluation metrics, we also automatically calculate the Diversity score of the generated responses by all the baselines and our model. Here, for a generated response, we first calculate the number of distinct tokens in the response and then divide to the total number of distinct tokens in its dialogue context, which includes the number of"}, {"title": "\u2022 Results on Various Context Lengths", "content": "To verify the impact of context length on the performance of the proposed model for response generation, we vary the length of context to respectively train the proposed models, which are called various context models, on two datasets. Here, context length indicates the number of historical utterances that"}, {"title": "4.2.3 \u2022 Results on Various Context Lengths", "content": "To verify the impact of context length on the performance of the proposed model for response generation, we vary the length of context to respectively train the proposed models, which are called various context models, on two datasets. Here, context length indicates the number of historical utterances that"}, {"title": "4.2.4 Additional Analysis with SOTA model", "content": "For the analysis of the generated cases of the proposed models and the SOTA baseline (ReCoSa) in test set, we first present some token-level statistics to show the difference of the models in the generation of multi turn dialogues. We calculate the frequency distributions of the tokens of the generated dialogue responses by the three models over the vocabulary. Note that the stop words are removed from the vocabulary. Figure 4 shows the statistics of the token frequencies of the proposed static and dynamic attention-based models and the ReCoSa model.\nFrom Figure 4, we can see that the token frequency distributions of the three models are quite different. It demonstrates that given the same set of input messages, the generate responses are various. The reasons may be two-fold:\n\u2022 First, the most essential characteristic of the open domain dialogue generation is the one-to-many phenomenon, which denotes that given an input message, there may be many suitable and reasonable responses. Therefore,"}, {"title": "5 CONCLUSION AND FUTURE WORK", "content": "In this paper, we proposed a static and dynamic attention-based approach to the generation of open domain multi turn dialogues. The proposed static and dynamic attention-based model is an attention-based framework for multi-turn dialogue generation. We have verified that it is compatible to different granularities of input information, such as token-level and utterance-level information, different types of attentions, such as multi head attention and self-attention, and pretrained language model, such as GPT2, etc. Furthermore, the proposed static and dynamic attentions can be"}, {"title": "6 ACKNOWLEDGMENTS", "content": "This paper is supported by the Science and Technology Innovation 2030 Major Project of China (No. 2020AAA0108605) , National Natural Science Foundation of China (No. 62076081, No. 61772153 and No. 61936010) and Nature Scientific Foundation of Heilongjiang Province (No.YQ2021F006)."}]}