{"title": "InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption", "authors": ["Tiehan Fan", "Kepan Nan", "Rui Xie", "Penghao Zhou", "Zhenheng Yang", "Chaoyou Fu", "Xiang Li", "Jian Yang", "Ying Tai"], "abstract": "Text-to-video generation has evolved rapidly in recent years, delivering remarkable results. Training typically relies on video-caption paired data, which plays a crucial role in enhancing generation performance. However, current video captions often suffer from insufficient details, hallucinations and imprecise motion depiction, affecting the fidelity and consistency of generated videos. In this work, we propose a novel instance-aware structured caption framework, termed InstanceCap, to achieve instance-level and fine-grained video caption for the first time. Based on this scheme, we design an auxiliary models cluster to convert original video into instances to enhance instance fidelity. Video instances are further used to refine dense prompts into structured phrases, achieving concise yet precise descriptions. Furthermore, a 22K InstanceVid dataset is curated for training, and an enhancement pipeline that tailored to InstanceCap structure is proposed for inference. Experimental results demonstrate that our proposed InstanceCap significantly outperform previous models, ensuring high fidelity between captions and videos while reducing hallucinations.", "sections": [{"title": "1. Introduction", "content": "Recently, text-to-video (T2V) generation with advanced diffusion transformers (DiT) [2, 8, 10-12, 15, 18, 21, 23, 25, 32, 36] have attracted significant attention for the ability to generate realistic, long-duration videos based on text prompts. Video-caption paired data is typically used in training and plays a crucial role in enhancing generation performance. Current video recaption methods often incorporate multimodal large language models to produce detailed captions, which however usually suffer from hallucinations, leading to inconsistencies between captions and video content. Consequently, creating consistent video-caption pairs with accurate details and precise motion depiction for T2V generation remains a significant challenge.\nAs shown in Figure 1, current video recaption methods can be broadly categorized into three types: 1) Short captions, such as Panda-70M [4], lack sufficient coverage of video content, leading to low fidelity. 2) Dense captions, like ShareGPT4Video [3], enrich textual content but suffer from hallucination issues, often generating meaningless or inaccurate video content. 3) Coarse-level structured captions, exemplified by MiraData [9], improve video quality but provide coarse-level details. Moreover, the redundancy introduced by MLLM across structures diminishes its overall effectiveness. To this end, achieving accurate captions remains two crucial challenges: 1) High fidelity between caption and video: Retain as much of the original video's objects, textures, and motion information as possible. 2) Accurate content in caption: Enable MLLM model to generate precise content, minimizing hallucinations and repetition.\nTo address the challenges, we propose a novel instance-aware structured caption framework, termed InstanceCap, to achieve instance-level and fine-grained video caption for the first time. Our structure is specifically designed to incorporate instances, background, and camera movement. For each instance, we specify class, appearance, actions, motion, and position. To enhance the fidelity and accuracy of video captions, we focus on two key aspects: 1) From Global Video to Local Instances: For each instance, we propose an auxiliary models cluster (AMC) to isolate it from the original video and obtain the corresponding position and category information. This operation minimizes interference from unrelated regions while retaining as much of the original video's information as possible. 2) From Dense prompt to Structured Phrases: We leverage multimodal large language models (MLLMs) in an improved Chain-of-Thought (CoT) process to obtain concise yet accurate descriptions of textures, camera movement, actions and motion for each instance. This reduces the probability of hallucinations and irrelevant content produced by the lan-"}, {"title": "2. Related works", "content": "Video recaptioning. Advancements in text-to-video generation demand high-quality video-text datasets to build robust foundational video models for visual-language alignment. Current video recaption methods fall into two main categories: manual annotation [1, 24, 37] and end-to-end recaption using multimodal large language models (MLLM). Although manual annotation provides higher accuracy, scaling datasets to meet the needs of high-quality video generation models remains a substantial challenge. Recent"}, {"title": "3. Method", "content": "In Section 3.1, we first present the InstanceCap pipeline, as shown in Figure 2. Based on this pipeline, we recaption the carefully curated dataset InstanceVid in Section 3.2, enhancing T2V models' instance generation. Additionally, in Section 3.3, we introduce InstanceEnhancer to convert short prompts into our proposed instance-aware structured caption format during inference."}, {"title": "3.1. Instance\u0421\u0430\u0440", "content": "Video preprocessing with auxiliary model cluster. For continuous video processing, we implemented uniform sampling using decord\u00b2, following the methodology established in LLaVA-Video [34]. This approach enables us to extract essential temporal metadata, including duration, frame count, and timestamps, allowing MLLMs to better interpret temporal sequences in recaptioning tasks. Additionally, to enhance MLLMs' capabilities through structured guidance, we incorporate several auxiliary models to achieve accurate object detection, video instance segmentation and camera motion prediction, providing precise prior information to the MLLMs.\nGlobal description, background detail and camera movement. When describing video content, a high-quality global description should capture primary elements, environmental context, camera movements, angles, and tonal qualities. MLLMs excel in generating high-level video summaries using Chain-of-Thought methodology. By employing carefully designed prompts with CoT, we can guide MLLMs to produce detailed background descriptions while minimizing references to foreground elements.\nHowever, MLLMs' limitations in processing discrete frames rather than continuous video segments make it challenging to distinguish camera motion from instance action. To address this, we achieve camera annotations from Open-Sora [35] for basic movements (e.g., zoom, rotation) and rely on MLLMs to capture subtle motion attributes (e.g., intensity, speed). The integration of camera movement indicators with MLLM capabilities provides comprehensive annotations, as illustrated in Figure 10 (a).\nStructured Description on Instances. In this subsection, we introduce the details to achieve our instance-aware structured description. To address MLLMs' limitations in instance annotation and the suboptimal results of directly adapting weak visual prompts from images to videos [22, 31], we make full use of the auxiliary model cluster, including initial object detection [38], video instance segmentation with SAM2 [20], and blur non-instance regions to achieve blur background, resulting in better recaptioning outcomes compared to alternative visual prompt methods in video, as shown in Figure 11. To this end, we decompose the global videos into local instances.\nNext, we describe how to achieve detailed and accurate description of each instance (Figure 3). To maintain instance-level precision, we deliberately constrain the information accessible to MLLMs during instance annotation. Crucially, the global video remains invisible due to our designed blurred backgrounds, preventing MLLMs from confusing information across multiple instances. This approach allows InstanceCap to focus on local instances identified through auxiliary model cluster. Furthermore, to avoid the potential limitation of MLLMs seeing only isolated instances, which could lead to overlooking inter-instance interactions and subsequent misinterpretations, we incorporate the global description mentioned in previous subsection. Specifically, we inject the global description into the instance-annotation MLLMs. This strategic mitigates potential biases in instance action descriptions while maintaining instance-specific accuracy.\nTo enhance the capability of InstanceCap in capturing instance-level details, we introduce novel insights into the improved CoT process. Our analysis of current MLLM-"}, {"title": "3.2. Instance Vid", "content": "Data collection. InstanceVid is curated via refining a subset from the high-aesthetic, high-consistency videos from OpenVid-1M [16]. To showcase our method's high-fidelity labeling of instance details and motion, we selected video samples that included at least one instance exhibiting high motion intensity during dataset filtering.\nStatistical analysis of InstanceVid. Figure 4 illustrates the statistical characteristics of InstanceVid across two main dimensions: video scenes, and temporal durations. Our data collection emphasizes videos with distinct instances while ensuring a balanced representation of outdoor scenes to prevent biases from an overemphasis on"}, {"title": "3.3. Instance Enhancer", "content": "When the caption distribution of training data differs from that of inference text, it may result in poor instruction-following performance or even problematic outputs. This issue is evident in T2V generation, particularly when long captions are used for training but short captions for inference, leading to subpar results. Since users typically prefer short captions, it is essential to enhance short caption effectively to better align with our proposed instance-aware structured caption during training.\nAs shown in Figure 5, we introduce a tuning-free approach called InstanceEnhancer that achieves this by strictly limiting the generated formats to match the caption corresponding to the training input we used. Our method differs from existing tuning-free caption enhancement approaches, such as those presented in RPG [29]. Instead of directly enhancing short captions, which we found can introduce inconsistencies between multiple instances' actions and their environmental context in video generation, we employ a two-stage enhancement strategy. In Stage A, short prompts are expanded into detailed long prompts. Stage B(I)&(II) uses both expanded and original captions to segment and enhance specific instances, preserving contextual coherence while ensuring precise instance identification. Due to the space limitation, more details of our enhance pipeline can be found in the supplementary material."}, {"title": "4. Experiments", "content": "4.1. Experimental setup\nVideo reconstruction with recaptions. To comprehensively evaluate InstanceCap, we conducted a series of experiments, benchmarking against state-of-the-art methods including Panda-70M [4], ShareGPT4Video [3], and MiraData [9]. To this end, we carefully selected 100 video clips from OpenVid-1M [16] and Animal Kingdom [17]. For each video, we generated one caption using various caption models, which were then input into the advanced T2V model CogvideoX-5b [30] for video generation. We calculated the differences between the generated videos and the ground truth videos to evaluate each caption model's performance, where smaller visual differences indicate more accurate captions and higher fidelity.\nWe introduced several metrics to evaluate the video reconstruction performance: 1) 3DVAEscore: Using 3DVAE from CogVideoX [30] as the backbone, we extract hidden-space representations from both the original videos and their recaption-reconstructed counterparts. These representations quantify the perceptual distance between them. 2) CLIPSenbySen: To handle CLIP's 77-token processing limit, we segment long captions into individual sentences"}, {"title": "5. Conclusions and Limitations", "content": "In this paper, we introduce InstanceCap, the first instance-aware structured caption method for text-to-video generation. We design an Auxiliary Models Cluster (AMC) to convert global video into instances, enhancing instance fidelity. we also propose an improved CoT pipeline with MLLMs to refine dense prompts into structured phrases, achieving concise yet precise instance descriptions compared to the previous video caption models. Additionally, based on InstanceCap, we curated InstanceVid dataset for training and InstanceEnhancer during inference, significantly enhancing T2V models' generation capabilities on instance details and actions.\nLimitations. Since the precision of InstanceCap partly depends on object detection methods, requiring fine-tuning of the detection model for domain-specific instances, and its benefits decrease in instance-free scenes. Furthermore, the scale of InstanceVid limits its use as a large-scale pre-training dataset. Moving forward, we plan to apply InstanceCap to a larger video dataset and train more powerful T2V models to amplify its impact."}, {"title": "1. Positive/Negative Lexicon", "content": "To enhance the aesthetic quality of generated videos, we carefully collected prompts from various open-source model galleries, extracting adjectives to build a Positive Lexicon. Conversely, we manually constructed a Negative Lexicon, which was further enriched using the powerful LLM, GPT-40. Both lexicons were refined through meticulous manual screening. The detailed contents of the Positive/Negative Lexicons are shown in Figure S1."}, {"title": "2. Human-designed Class Hints", "content": "For the Human-designed Class Hints, we carefully crafted additional prompts for over eighty categories, each specifically tailored to its specific characteristics. Below, we present twenty of these categories. The full JSON-formatted hints for all classes, ready for direct use, will be provided in the code we plan to release later.\n\u2022 Person: \"Please focus primarily on the person's facial expressions, attire, age, gender, and race in the video and give description in detail. Please mention if there are any necklaces, watches, hat or other decoration; otherwise, there's no need to bring them up.\""}, {"title": "3. Prompt Design of Figure 3", "content": "System prompt. Referring to ShareGPT4Video [3], we divided the System prompt into three parts. Through extensive tests on challenging samples, including multi-instance, complex scenes, and high-intensity motion, we finalized the system prompt shown in Figure S5. Additionally, temporal metadata extracted using the code provided in Figure S6.\nPrompts of global description, background detail and camera movement. The global description is derived from a single prompt: \"Please describe this video in one sentence, no more than 20 words.\". To illustrate the acquisition of camera motion and background details, we provide an example of implementing camera hints with movement cues in Figure S7. A similar approach is used for extracting background details included in our code released later.\nPrompts of structured caption. In the structured caption section, we we use Actions and Motion as examples, with the CoT prompt shown in Figure S8. The acquisition of Appearance and the injection of Human-designed class hints follow a similar approach."}, {"title": "4. Design of InstanceEnhancer", "content": "In InstanceEnhancer, prompt alignment during inference is achieved through a two-stage process (Figure S2). To pro-"}, {"title": "5. Evaluation metrics for video reconstruction", "content": "3DVAE score (3DVAEscore). The LIPIPS score [33] which is widely used to evaluate image reconstruction quality, measures perceptual distance between ground truth (GT) and reconstructed images. We extent this concept for video data by using 3DVAE [30] to extract latent-space video representations from both GT videos and their caption-reconstructed versions. 3DVAEscore computes the distance between latent representations across spatial and temporal dimensions:\n$d(Z_{GT}, Z_{rec}) = \\frac{1}{\\Omega}  \\sum_{l}\\sum_{h,w} \\sum_{t} || \\omega_{l} (Z^{l}_{GT, hwt} - Z^{l}_{rec, hwt}) ||_{2}$    (1)\nwhere $Z^{l}_{GT,hwt}$ and $ze Z^{l}_{rec, hwt}$ represent the latent representations at layer l, spatial location (h, w), and temporal frame t, with $w_{l}$ as the layer-specific weight matrix. We set (h, w, t) = (224, 224, 8) for evaluation.\nTo ensure consistency, we use the same video generation model across all captioning methods. Following LIPIPS methodology, we validate the 3DVAE score by comparing GT videos against various distorted versions. As shown in Tab. S1, the results demonstrate that our score effectively captures perceptual similarities between GT and reconstructed videos.\nCLIP score sentence by sentence (CLIP SenbySen). While CLIP [19] is widely used for text-video similarity computation [6, 14], its 77-token limit restricts processing of long texts. To overcome this, we propose CLIP score sentence by sentence (SenbySen), which segments texts into individual sentences and computes CLIP similarity between each sentence and video frame."}, {"title": "6. Inseval", "content": "Inference prompts of Inseval. In implementing Inseval, we designed multiple prompts to test each dimension, as illustrated in Figure S3. To further evaluate the model's generative capabilities and instruction-following accuracy,"}, {"title": "7. Analysis on Commercial Products vs. Open-source Models", "content": "Prompt processing analysis. Commercial T2V products excel at processing complex input prompts, effectively handling long-form text in structured formats while preserving semantic coherence. They can seamlessly interpret detailed scene descriptions, character interactions, and sequential events within a single prompt, producing coherent visual narratives, have shown surprising results in many situations.\nOpen-source T2V models, however, are unable to directly process long-text structured prompts, requiring an additional alignment step (Figure S11). This preprocessing can lead to potential information loss and inconsistencies in the final output, restricting the ability to capture nuanced details from the original prompt.\nInformation retention capabilities. Different models exhibit notable differences in information retention (Figure S4). Commercial products (e.g., Hailuo AI) excel in maintaining fidelity between text and visual content, effectively preserving detailed instructions and translating multiple attributes into video sequences. This strength is particularly apparent when our caption contains complex scenes that demand temporal consistency and fine-grained details."}]}