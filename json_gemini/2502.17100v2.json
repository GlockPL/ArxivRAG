{"title": "Generative Models in Decision Making: A Survey", "authors": ["Yinchuan Li", "Xinyu Shao", "Jianping Zhang", "Haozhi Wang", "Leo Maxime Brunswic", "Kaiwen Zhou", "Jiqian Dong", "Kaiyang Guo", "Xiu Li", "Zhitang Chen", "Jun Wang", "Jianye Hao"], "abstract": "In recent years, the exceptional performance of generative models in generative tasks has sparked significant interest in their integration into decision-making processes. Due to their ability to handle complex data distributions and their strong model capacity, generative models can be effectively incorporated into decision-making systems by generating trajectories that guide agents toward high-reward state-action regions or intermediate sub-goals. This paper presents a comprehensive review of the application of generative models in decision-making tasks. We classify seven fundamental types of generative models: energy-based models, generative adversarial networks, variational autoencoders, normalizing flows, diffusion models, generative flow networks, and autoregressive models. Regarding their applications, we categorize their functions into three main roles: controllers, modelers and optimizers, and discuss how each role contributes to decision-making. Furthermore, we examine the deployment of these models across five critical real-world decision-making scenarios. Finally, we summarize the strengths and limitations of current approaches and propose three key directions for advancing next-generation generative directive models: high-performance algorithms, large-scale generalized decision-making models, and self-evolving and adaptive models.", "sections": [{"title": "INTRODUCTION", "content": "GENERATIVE models have become a hot topic in both academia and industry, primarily due to their ability to generate large quantities of synthetic data with high quality and diversity. From early systems like DALL-E [1] (for image generation) and GPT-3 [2] (for text generation) to more recent advancements such as DALL-E3 [3], ChatGPT and GPT-4 [4], generative models have rapidly advanced in both quality and scale of their outputs.\nContent generation aims to create coherent material that mimics training examples, while decision-making fo- cuses on producing action sequences for optimal outcomes. Unlike content generation, decision-making involves com- plex, dynamic environments and long-term decisions. Thus, despite generative models' success in content generation, applying them to decision-making poses challenges. These challenges include: 1) how to learn policies through interac- tion with the environment, rather than simply mimicking expert behavior. 2) how to generate new policies based on learned behaviors, transitioning from policy learning to policy generation. 3) how to establish a robust fundamental decision generating model that can adapt to various en- vironments with minimal tuning efforts. 4) how to build multi-step reasoning and long-term evolution capabilities of strategies. These challenges emphasize the need for gen- erative models to go beyond mere data generation.\nIn practice, decision-making is often referred to as se- quential decision making, where a decision maker makes a series of observations over time, and each decision in- fluencs subsequent choices. The goal is to identify a policy that optimizes expected rewards or minimizes costs across sequential actions. Classical algorithms such as Dynamic Programming (DP) and Reinforcement Learning (RL) are widely used to solve problems modeled as Markov Deci- sion Processes (MDPs). These methods optimize decision- making by updating policies based on observed rewards and state transitions rather than generating new ones. De- spite their many successful applications, these traditional approaches often rely on trial-and-error or predefined states and transitions, limiting exploration and potentially missing better solutions. Moreover, they require substantial compu- tation and optimization, which can be impractical for high- dimensional or large-scale problems. Traditional methods also need significant reconfiguration or retraining to new environment, reducing flexibility.\nOn the other hand, generative models are designed to model data distributions, rather than simply fitting labels. Once trained, they can generate new samples that resem- ble the original data, enabling the exploration of diverse scenarios and outcomes. This ability enables the discovery of novel strategies that may not be immediately evident with traditional methods. In complex or poorly labeled data scenarios, generative models offer a richer under- standing of possible decision paths, sometimes leading to strategies that better align with high rewards or desired goals. However, traditional methods like optimization or reinforcement learning remain effective in simpler, well-defined environments where the decision space is clearer, and goals are more straightforward. The choice between these approaches depends on the task complexity and the environment characteristics.\nRecognizing these advantages, recent years have seen substantial research efforts aimed at developing new gener- ative models and applying them to decision making. Fig. 1 illustrates the research trends in generative models and their"}, {"title": "2 PRELIMINARIES", "content": ""}, {"title": "2.1 Sequential Decision Making", "content": "Sequential decision-making involves making step-by-step decisions, where each choice depends on previous out- comes. At each step, the agent typically observes the current state, selects an optimal action based on its policy, and the environment updates its state while providing new rewards. The ultimate goal is to maximize the accumulated rewards.\nThe Sequential decision process is categorized accord- ing to the number of agents and the observation to the environment. In this paper, we only focus on the single agent. If the agent is able to fully observe the environment, it is considered a Markov Decision Process (MDP) [5]. An MDP is a framework for modeling discrete-time decision- making process, where outcomes depend on both random factors and the agent's decisions [5]. Formally, an MDP is defined as a tuple \\(M = (S,A,R, P, \\rho_0, \\gamma, H)\\) to represent MDP, where S is the state space, and \\(s \\in S\\) contains all the information perceived by the agent from the environment. A represents the action space, encompassing all possible actions a that the agent can execute when interacting with the environment. \\(r \\in \\mathbb{R} : S\\times A\\times S \\rightarrow \\mathbb{R}\\) denotes the reward function, which corresponds to the transition pair \\((s, a, s')\\). P: S \u00d7 A \u2192 \u0394(S) refers to the transition function. When the action a is applied to the state s, P(s'|s,a) generates a distribution over states. \\(\\rho_0 \\in \\Delta(S)\\) represents the initial state distribution. The discount factor \\(\\gamma\\in [0,1]\\) quantifies the long-term value of the current action, while H denotes the horizon.\nIf observations are limited, we can define the Partially Observed Markov Decision Process (POMDP) [6] as a tuple \\(M = (S, A, O,R, P, \\rho_0, E, \\gamma, H)\\). Here, O denotes the obser- vation space, and each observation \\(o_t \\in O\\) is obtained by the emission function \\(E(o_t|s_t)\\).\nThe goal of sequential decision making is to learn a policy \\(\\pi(a|s)\\) or \\(\\pi(a|o)\\) by optimizing the expected reward:\n\\[J(\\pi) = E_{\\tau \\sim p_{\\pi} (\\tau)} \\bigg[ \\sum_{t=0}^{H} \\gamma^t r (s_t,a_t) \\bigg] \\qquad(1)\\]\nwhere \\(p_{\\pi} (\\tau)\\) is the distribution over trajectories \u03c4 induced by policy \u03c0:\n\\[p_{\\pi}(\\tau) = \\rho_0(s_0) \\prod_{t=0}^{H} \\pi(a_t|s_t) P(s_{t+1}|s_t, a_t). \\qquad(2)\\]"}, {"title": "2.2 Related Methods", "content": "We introduce several traditional methods for solving se- quential decision making problems in this section."}, {"title": "2.2.1 Imitation Learning", "content": "Imitation learning aims to optimize a policy \\(\\pi_{\\theta}(s)\\) such that the action distribution is close to that of an expert policy \\(\\pi^*\\) for any state s, the objective function is given by\n\\[\\theta = \\underset{\\theta}{arg\\ min} \\ \\mathbb{E}_{s\\sim p(s|\\pi_{\\theta})} L(\\pi^*(s) - \\pi_{\\theta}(s)), \\qquad(3)\\]\nwhere \\(p(s|\\pi_{\\theta})\\) denotes the state distribution induced by \\(\\pi_{\\theta}\\), and L is a metric function."}, {"title": "2.2.2 Search based Methods", "content": "Policy search focuses on exploring the policy space directly, rather than computing the value function explicitly. For a given policy \u03c0, the expected discounted return is given by\n\\[U(\\pi) = \\sum_{s \\in S} \\rho_0(s)U^{\\pi}(s). \\qquad(4)\\]\nWith large state space, U(\u03c0) could be approximated by sampling trajectories consists of (S, A, R) pairs and U(\u03c0) can be reformulated as [7]:\n\\[U(\\pi) = \\mathbb{E}_{\\tau}[R(\\tau)] = \\int p_{\\pi}(\\tau)R(\\tau)d\\tau, \\qquad(5)\\]\nwhere R(\u03c4) denotes the discounted return related to \u03c4. Monte Carlo policy evaluation entails estimating the ex- pected utility of the policy \u3160by performing numerous rollouts starting from \\(s_0 \\sim \\rho_0(s)\\). Local search starts with a feasible solution and incrementally moves to the neighbor with better utility over the search space until convergence occurs, e.g. Hooke-Jeeves method [8]. Simulated anneal- ing [9] allows for occasional moves to feasible solutions with worse utility to approximate the global optimum. Genetic algorithms evaluate different simulations concur- rently based on the objective, recombine them and guide the population toward a global optimum."}, {"title": "2.2.3 Planning & Optimization", "content": "Planning is an optimization approach that relies on a pre- defined model to guide the search process. Let U(s) denote the action space for each state s, which represents the set of all actions that could be applied from s. For distinct s, s' e S, U(s) and U(s') are not necessarily disjoint. As part of the planning problem, a set \\(S_G \\in S\\) of goal states and the initial state \\(s_0\\) are defined.\nLet \\(\\pi_K\\) denote a K-step plan, which is a sequence of \\((a_1,a_2,..., a_K)\\) of K actions. Let F denote the final stage where F = K + 1. The cost functional is defined as\n\\[L(\\pi_K) = \\sum_{k=1}^{N} l(s_k, a_k) + l_F(s_F), \\qquad(6)\\]\nwhere l(sk,ak) is the cost term yielding a real value for every \\(s_k \\in S\\) and \\(a_k \\in U(s_k)\\). The final term is defined \\(l_F(s_F) = 0\\) if \\(s_F \\in S_G\\) and \\(l_F(s_F) = \\infty\\) otherwise."}, {"title": "2.2.4 Reinforcement Learning", "content": "The field of RL contains various methods that cater to dif- ferent decision-making tasks, but all standard RL algorithms generally follow the same learning principles. In particular, the agent interacts the current MDP M using some sort of behavior policy, which can be the present policy \\(\\pi(a|s)\\) or mix it with a random policy. Then the agent can receive the subsequent state \\(s_{t+1}\\) along with the reward function \\(r_t\\). After repeating this process for many steps, the agent can use the collected samples {s,a,r, s'} to update the policy.\nValue-based Methods. One classic approach is to di- rectly estimate the value function of the state of state-action so that a near-optimal policy can be obtained. Define the state-action value as\n\\[Q^{\\pi}(s_t, a_t) = E_{\\tau \\sim p(\\tau|s_t,a_t)} \\bigg[ \\sum_{t'=t}^{H} \\gamma^{t'-t} r (s_{t'},a_{t'}) \\bigg] \\qquad(7)\\]\nand the state value \\(V^{\\pi}(s_t)\\) as\n\\[V^{\\pi}(s_t) = E_{a_t \\sim \\pi(a_t|s_t)}[Q^{\\pi} (s_t, a_t)]. \\qquad(8)\\]\nThen the recursive representation of \\(Q^{\\pi}(s_t, a_t)\\) can be de- rived as [10]:\n\\[Q^{\\pi}(s_t, a_t) = r (s_t, a_t) + E_{s_{t+1} \\sim P(s_t a_t)} [V^{\\pi} (s_{t+1})], \\qquad(9)\\]\nwhich can also be expressed as the Bellman operator B\u03c0, i.e., \\(Q^{\\pi} = B^{\\pi}Q^{\\pi}\\). \\(B^{\\pi}\\) has the unique fixed point obtained by repeating iterations \\(Q^{i+1} = B^{\\pi}Q^i\\) [11]. Based on this property, we can derive the modern value iteration (VI) algorithms. Q-learning is a common VI method that ex- presses the policy as \\(\\pi(a_t|s_t) = \\delta(a_t = arg\\ max_{a_t} Q(s_t,a_t))\\) with Dirac function \\(\\delta(.)\\). The optimal Q-function can be approximated by substituting this policy into (9). To derive a learning algorithm, we can define a parametric Q-function estimator \\(Q_{\\phi}(s, a)\\), which can be optimized by minimizing the difference between LHS and RHS in Bellman equation, such as fitted Q-learning [12]. With the help of neural networks, existing deep Q-learning methods can achieve more accurate value-function estimation by minimizing the Bellman error objective [13].\nPolicy Gradients. Another classic approach is to directly estimate the gradient of (1). We define policy \\(\\pi_{\\theta}(a|s)\\) parameterized by \u03b8, which can be a neural network and output the logits of action a. Then, We can then write the gradient of (1) with respect to \u03b8 as:\n\\[\\nabla_{\\theta} J(\\pi_{\\theta}) = E_{\\tau \\sim \\pi_{\\theta} (\\tau)} \\bigg[ \\sum_{t=0}^{H} \\gamma^t \\nabla_{\\theta}log \\pi_{\\theta} (a_t|s_t) A(s_t, a_t) \\bigg] \\qquad(10)\\]\nwhere \\(\\hat{A}(s_t, a_t)\\) is the return estimator, written by\n\\[\\hat{A}(s_t, a_t) = \\sum_{t'=t}^{H} \\gamma^{t'-t}r (s_{t'}, a_{t'}) - b(s_t), \\qquad(10)\\]\nwhich can be calculated with Monte Carlo samples [14]. b(st) denotes the baseline, which can be approximated as the mean return of the collected trajectories or by the value function V(st) [15].\nBy introducing KL regularization and clipped surrogate objective, True Region Policy Optimization (TRPO) [16] and Proximal Policy Optimization (PPO) [17] constrain policy size updated at each iteration to stable the training pro- cess. These methods and their variants have achieved great success in areas like robot control [18], games [19] and generative model training [20].\nActor-Critic Methods. Taking advantage of both pol- icy gradient and value-based approaches, Actor-critic al- gorithms use a parameterized policy and value function. And the value function can provide a better estimation of \\(\\hat{A}(s_t, a_t)\\) for policy gradients, with calculating the average return over the sampled trajectories.\nUnlike Q-learning, the actor-critic method seeks to op- timize the Q-function associated with the current policy \u03c0\u03b8 rather than learning the optimal Q-function directly. Many variants are proposed based on this principle, such as on- policy and off-policy algorithms [21]. Deep Deterministic Policy Gradient (DDPG) introduces a deterministic policy under the Actor-Critic architecture, which can solve contin- uous control problems [21]. Soft actor-critic introduce the entropy regularization into the policy optimization process, so that the agent can make better exploration [22].\nModel-based RL. For model-based RL, the first step is to learn the environment model \\(P(\\cdot|s, a)\\) based on historical trajectories. According to the probabilistic transition model, a common approach is to reduce the KL divergence between the learned model \\(P(\\cdot|s, a)\\) and the true dynamics P(.ls, a) as [23]:\n\\[\\underset{\\theta}{min} \\mathbb{E}_{(s,a) \\sim p} [D_{KL} (P(. \\mid s,a), P_{\\theta}(s,a))], \\qquad(11)\\]\nwhere \u03b8 denotes the learned model's parameters. The model learning process can be transformed as a supervised learn- ing task, which can be solved effectively by some supervised learning technique. To reduce the error of model learning, several scholars propose the Lipschitz continuity constraints [24], distribution matching [25], and robust model learning methods [26] for further precise estimation.\nWhen the model is ready, the agent develops plans to improve its strategy to interact with the modeled world. Traditional methods for integrating planning into MBRL mainly include model predictive control, Monte Carlo tree search and Dyna-style methods [27]. Dyna-style methods utilize the learned model to generate more experience and then perform RL on the model-augmented dataset, which has been the mainstream method [28]. Similarly, these meth- ods employ value function estimation and policy gradient to improve the policy."}, {"title": "2.3 Generative Models", "content": "Generative Model [29], [30], [31], [32], [33], [34], [35], [36] is an important subdivision of artificial intelligence that employs techniques and models specifically designed for sampling unseen data from the underlying distribution of the existing dataset.\nIn this section, we explore generative models through three crucial dimensions: sample quality, sample diversity, and computational efficiency [37], [38]. These dimensions are essential for understanding how generative models per- form in decision-making, as they directly impact the accu- racy, robustness, and practical applicability of the generated outputs. Sample quality measures how well the generated samples align with the real data distribution, reflecting the realism and reliability of the outputs. High sample quality ensures that decisions based on the generated data are both accurate and trustworthy in real-world contexts. Sample diversity, on the other hand, evaluates how well the model can generate a broad range of distinct samples, capturing the full spectrum of potential outcomes. This is vital in decision-making tasks, where diverse options are necessary to adapt to varying conditions and avoid narrow, overfitted solutions. Finally, computational efficiency refers to the computational resources required for training and inference, making it a key consideration for the deployment of generative models in real-world applications. In con- texts such as autonomous driving or robotic control, where decisions must be made in real time and under resource constraints, computational efficiency is critical to ensure that the model remains practical and scalable. Together, these dimensions provide a comprehensive framework for evaluating generative models, guiding their application in complex, resource-sensitive decision-making environments.\nWe will examine each of the seven generative models through these three crucial dimensions, identifying their strengths and weaknesses. Generative models face chal- lenges in balancing sample quality, sample diversity, and computational efficiency. For example, Diffusion Models and Normalizing Flows offer strong sample diversity and stability but require high computational resources, limiting their suitability for real-time decision-making applications [39], [40], [41]. In contrast, models like VAEs and GANS provide faster training and better efficiency but may strug- gle with maintaining sample diversity, potentially leading to overly similar or overfitted outputs [34], [42], [43], [44]. Based on the references [43], [45], [46], [47], [48], [49], [50], [51], [52], [53], we compare the performance of these seven generative models across sample quality, diversity, and effi- ciency, as shown in Fig. 3.\nEnergy Based Models (EBMs). An EBM [54] assigns an energy function \\(E_{\\theta}(x)\\) to each data x [55], which is the unnormalized log probability of the data to measure the compatibility of x with the model. The density of x is\n\\[P_{\\theta}(x) = \\frac{exp(-E_{\\theta}(x))}{Z_{\\theta}}, \\qquad(12)\\]\nwhere \\(Z_{\\theta}\\) denotes the normalizing constant, which also known as partition function:\n\\[Z_{\\theta} = \\int exp(-E_{\\theta}(x))dx. \\qquad(13)\\]\nThe model is trained to reduce the energy of data point from the training set while increasing the energy of other, possibly unrealistic, data points [27]. The earliest EBM is the Boltzmann machine [33], which is a stochastic version of the Hopfield network [56]. Since the energy function has no re- strictions, it can be parameterized freely and can model any high-dimensional complex data [57], [58] like images [59] and natural languages [60]. However, the intractable nor- malizing constant poses training and sampling challenges. Typical solutions include 1) MCMC sampling [61]; 2) score matching [62] and 3) noise contrastive estimation [63].\nGenerative Adversarial Networks (GANs). GANs [34] are well-known due to their distinctive architecturewhich involves a generator G that creates synthetic samples and a discriminator D that evaluates them by distinguishing between real and generated data. The target loss of a typical GAN is given by\n\\[L(\\theta, \\phi) = \\underset{\\theta}{min} \\ \\underset{\\phi}{max} \\mathbb{E}_{x\\sim P_{data}} [log\\ D(x; \\phi)] + \\mathbb{E}_{x\\sim G(\\theta)} [log(1 - D(x; \\phi))]. \\qquad(14)\\]\nWhile GAN can generate realistic contents in image syn- thesis [34], image style transfer [35], and behavior [64], it suffers from training instability and mode collapse. Several variants of GANs have be proposed to solve the problem, like Wasserstein GANs [65], [66], CycleGANs [35], and Progressive GANs [67], which use different loss functions, architectures, or training techniques.\nVariational Autoencoders (VAEs). VAEs [43], known for their stable learning process and efficient computation, attract significant attention in recent years. VAEs [68] are probabilistic models that represent the input data in a com- pressed form, referred to as the latent space. New samples are generated by sampling from this latent space and then decoding them back into the original data domain. The goal of a VAE, presented in (15), aims to maximize the evidence lower bound (ELBO):\n\\[L(\\phi, \\theta) = E_{z\\sim p(z|x;\\phi)} [log\\ p(x | z; \\theta)] - KL(q(z)||p(z)). \\qquad(15)\\]\nIn (15), the first term represents the likelihood of the ob- served data x given latent variable z, and the expectation is taken over the distribution of z. The KL term measures the divergence between the variational distribution q(z), which approximates the posterior distribution of z, and the prior distribution p(z).\nVAEs are deployed to generate new images or videos [69], and augment images, such as image inpainting [70] and image colorization [71]. VAEs are used to compress data [72], [73] or provide a good way to detect anomalies or outliers in data [74]. However, VAEs may suffer from low generation quality and the independence assumption between the latent variables [72]. To address these limita- tions, several extensions of VAEs have been proposed, e.g., conditional VAE [75] and Gaussian mixture VAE [76].\nResearchers propose to combine VAEs with other tech- niques, including adversarial training [77], [78] and normal- izing flows [79] for improving disentanglement. Besides, [80] and [81] study the continual learning of VAE rep- resentations and [82] trains a response function of the hyperparameter \u03b2 in \u03b2-V AE to trade off the reconstruction error and the KL divergence.\nNormalizing Flows (NFs). Flow-based models [83] learn to map a simple random variable, such as a Gaussian distribution, to a more complex distribution capable of rep- resenting data. Normalizing Flow [84], [85] applys a specific type of invertible transformation to the simple distribution to preserve the tractability of the density function, as shown below:\n\\[x = f_k(f_{k-1} (... f_0(z_0))) ...) \\text{ with } z_0 \\sim N(0, I), f_0(z) = z + uh (w^Tz+b). \\qquad(16)\\]\nA primary strength of normalizing flows is their precision in capturing complex probability. Normalizing flows can also be used to generate new samples and to perform density es- timation, among other tasks. Despite their success, normal- izing flows still face some challenges, such as the restricted network structure and the rendered limited expressiveness. Nevertheless, Normalizing flows remain a dynamic field of study, with continuous advancements being made.\nFrom limited pre-neural attempts [86], [87], [88], spe- cific flow transformations [89] are proposed to reduce the computational cost, such as NICE [90], Real-NVP [91], and Glow [41]. Researchers also use autoregressive flows [92], [93], [94] to accelerate the sampling process. Many Normal- izing Flow variants also leverage the techniques of Neural ODE [95], [96], [97] and optimal transport [98], [99].\nDiffusion Models (DMs). Diffusion models, introduced by Sohl-Dickstein et al. in 2015 [36], learn to generate samples through a diffusion process. This process involves gradually introducing noise to the data, followed by the reversal of this noise addition during sampling to recover the original data (as illustrated in Fig. 4). The key strength of diffusion models lies in their ability to generate high-quality, realistic, and diverse outputs, surpassing earlier generative models such as GANs in various image synthesis tasks [100]. This capability has sparked significant interest in their application to decision-making scenarios, particularly in high-dimensional environments.\nThe key components of a diffusion model are the for- ward and the reverse process. In the forward phase, Gaus- sian noise is progressively introduced to the data. At time step t, the data point \\(x_{t-1}\\) is transformed into a noisy version \\(x_t\\) according to Equation 17:\n\\[q(x_t | x_{t-1}) = N (x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_tI). \\qquad(17)\\]\nHere, \\(\\beta_t\\) denotes a schedule of noise variance, which governs how quickly the noise is introduced at step t. The forward process effectively destroys the original data structure, making it increasingly difficult to recover the original input as the process progresses.\nThe sampling process works by reversing the diffusion (or noise addition) steps. The model iteratively refines a noisy sample to recover the original data by learning a reverse diffusion process:\n\\[P_{\\theta} (x_{t-1}|x_t) \\text{ for } t = T, T - 1, ..., 1. \\qquad(18)\\]\nThe reverse process can be computationally expensive, as it involves many time steps, each requiring careful sam- pling to accurately reconstruct the data. Despite this chal- lenge, diffusion models can generate high-quality samples, particularly in fields such as image generation, where they have outperformed GANs in terms of visual fidelity and diversity.\nIn the context of decision-making, diffusion models of- fer several advantages. They can model complex, high- dimensional decision spaces and generate solutions that explore a wide range of potential outcomes, making them suitable for applications in reinforcement learning (RL), robotics, and automated planning.\nFor instance, in robotic control, diffusion models can generate motion sequences that not only mimic the de- sired behavior but also explore alternative strategies that could lead to better performance in uncertain or dynamic environments. Similarly, in autonomous driving, they can generate realistic trajectories and responses to dynamic driv- ing scenarios, taking into account the uncertainty of the environment.\nMoreover, diffusion models can facilitate the exploration of decision spaces in complex environments where tradi- tional models might struggle to account for the full range of possibilities. This capability is particularly useful in multi- objective decision making, where a model needs to balance trade-offs between conflicting goals.\nDiffusion models demonstrate remarkable potential for producing high-quality, realistic, and novel data, and they continue to be an important tool in the field of genera- tive modeling. Nevertheless, diffusion models have a slow sampling process, which will limit its scalability to high-dimensional cases. Consistency models [102] are a potential solution that support fast one-step generation.\nGFlowNets/CFlowNets. GFlowNets is a new type of generative model, which [51], [103] are initially defined on finite directed acyclic graphs G = (S,E) with given initial and final states \\(s_0, s_f \\in S\\) and given reward function R(s\u2192 sf); an edge flow is simply a map F: E \u2192 R+. From such an edge flow, one can define a Markov chain (pt) on G setting \\(P(p_{t+1} = s'|p_t = s) = \\frac{F(s\\rightarrow s')}{\\sum_{s \\rightarrow s'' F(ss'')}}\\). The edge flow F is trained so that for all state \\(s\\in S \\setminus {s_0, s_f}\\)\n\\[\\sum_{s' \\rightarrow s}F(s' \\rightarrow ) = \\sum_{s \\rightarrow s'}F(s \\rightarrow s'), \\quad F(s \\rightarrow s_f) = R(s \\rightarrow s_f). \\qquad()\\]\nThe first equality is the flow-matching constraint, the second is the reward constraint. Assuming the reward R is non-negative and not identically zero, those two constraints en- sures that the Markov chains reaches sf at some finite time T and that the last non-terminal position \\(p_{T-1}\\) of the Markov chain follows the distribution obtained by normalizing R i.e. \\(P(p_{T-1} = s) = \\frac{R(s \\rightarrow s_f)}{\\sum_{s' \\in S} R(s' \\rightarrow s)}\\). The framework has been extended beyond graphs and acyclicity [104], [105], [106], with variation on losses and regularizations [105], [107], [108].\nAutoregressive Models (AMs). Autoregressive models are commonly used in natural language processing and generative modeling tasks. They are a key component of sequence-to-sequence models, such as the Transformer model [109], and are responsible for generating output sequences based on an input sequence or a given context.\nAutoregressive models operate by predicting each out- put element sequentially conditioned on the preceding ones. This means that the generation process is sequential, with the model generating the elements in a left-to-right fashion. At each step, the model takes into account the previously generated elements and the context information to make predictions.\nA popular method for autoregressive generation is using an autoregressive language model, such as GPT [110], which has proven successful in NLP tasks like text generation, machine translation, and dialogue systems.\nIn autoregressive models, the input to each step is typ- ically a combination of the previously generated elements and a context vector that captures the overall context or representation of the input sequence. The context vector can be obtained from an encoder network that processes the input sequence and produces a fixed-size representation, which is then used as input to the decoder.\nDuring training, autoregressive models are typically optimized using Maximum Likelihood Estimation (MLE) [111]. The model aims to optimize the likelihood of gen- erating target output sequence given the input sequence or context. This involves calculating the probability of each tar- get element based on the preceding elements and updating parameters to improve its predictions [112].\nOverall, autoregressive models are a powerful tool for sequence generation tasks, particularly in natural language processing [113]. They have contributed to significant ad- vancements in generative modeling and continue to be an active area of research in the field."}, {"title": "2.4 Difference with previous approaches", "content": "Reinforcement Learning (RL) and generative models are two distinct branches of machine learning that serve different purposes and tackle different challenges. The key difference lies in their primary objectives and the nature of the learning process [114], as show in Fig. 6.\nThe primary objective of RL is for an agent to learn an optimal policy that maximizes cumulative rewards through trial-and-error with the environment. The agent learns by taking actions, observing the resulting states, then adjusting current strategy according to the returns. On the other hand, generative models aims to understand and represent the latent data distribution in a given dataset [115]. These models are used to generate new samples resembling the training data, capturing inherent patterns and structures in the data.\nThe second one is the learning process. In RL, the agent learns through trial and error by exploring the environment, taking actions, and receiving rewards. As it gains experi- ence, the agent refines its policy to make more effective decisions, ultimately maximizing its rewards. Meanwhile, generative models learn from a given dataset and analyze the data, attempting to estimate the probability distribution that generates the data, such as VAEs and GANs.\nLast but not least, they differ in how they interact with environment [116]. The RL agent interacts with the envi- ronment iteratively, learning from feedback and adjusting its behavior without generating any data. In contrast, the generative model, once trained, is capable of producing entirely new data points resembling the original ones [117], thus actively generating rather than merely responding.\nIn summary, reinforcement learning focuses on learning optimal actions to achieve specific goals [118], whereas gen- erative models aim to model data distributions and generate new, similar samples. Both are crucial in machine learning but address different challenges."}, {"title": "3 TAXONOMY", "content": "In this section, we state our taxonomy to group the gen- erative approaches for solving sequential decision making problems. We categorize the methodologies into five key dimensions, outlined as follows. This taxonomy is further illustrated in Table 1.\nFamily. The first dimension is the family of the gener- ative models. We represent the family of the approach by the acronym of the category of the generative models. To be more specific, Energy Based Models (EBMs), Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), Normalizing Flow (NFs), Diffusion Models (DMs), GFlowNets (GFNs), and Autoregressive Models (AMs).\nFunction. The second dimension functions on the role of generative models in sequential decision making. Because they can model a diverse set of distributions"}]}