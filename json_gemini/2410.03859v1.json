{"title": "SWE-BENCH MULTIMODAL: DO AI SYSTEMS GENERALIZE TO VISUAL SOFTWARE DOMAINS?", "authors": ["John Yang", "Carlos E. Jimenez", "Alex L. Zhang", "Kilian Lieret", "Joyce Yang", "Xindi Wu", "Ori Press", "Niklas Muennighoff", "Gabriel Synnaeve", "Karthik R. Narasimhan", "Diyi Yang", "Sida I. Wang", "Ofir Press"], "abstract": "Autonomous systems for software engineering are now capable of fixing bugs and developing features. These systems are commonly evaluated on SWE-bench (Jimenez et al., 2024a), which assesses their ability to solve software issues from GitHub repositories. However, SWE-bench uses only Python repositories, with problem statements presented predominantly as text and lacking visual el-ements such as images. This limited coverage motivates our inquiry into how existing systems might perform on unrepresented software engineering domains (e.g., front-end, game development, DevOps), which use different programming languages and paradigms. Therefore, we propose SWE-bench Multimodal (SWE-bench M), to evaluate systems on their ability to fix bugs in visual, user-facing JavaScript software. SWE-bench M features 617 task instances collected from 17 JavaScript libraries used for web interface design, diagramming, data visual-ization, syntax highlighting, and interactive mapping. Each SWE-bench M task instance contains at least one image in its problem statement or unit tests. Our analysis finds that top-performing SWE-bench systems struggle with SWE-bench M, revealing limitations in visual problem-solving and cross-language generaliza-tion. Lastly, we show that SWE-agent's flexible language-agnostic features enable it to substantially outperform alternatives on SWE-bench M, resolving 12% of task instances compared to 6% for the next best system.", "sections": [{"title": "1 INTRODUCTION", "content": "Language models (LMs) are being increasingly deployed to assist software engineers (Bagalkote, 2024; Yepis & StackOverflow, 2024). As LMs gain in prominence, the research community has been expanding from building LM-based assistants that work at the code line or function level (Chen et al., 2021; Hendrycks et al., 2021) to building autonomous systems that can maintain and improve large codebases with hundreds of files and thousands of lines (Wang et al., 2024b; Xia et al., 2024; Yang et al., 2024; Zhang et al., 2024b). These systems provide LMs with tools and environments that let them engage in multi-step interactions to solve complex software development tasks.\nSWE-bench (Jimenez et al., 2024a) is the most popular benchmark for evaluating the performance of these systems. Drawn from GitHub issues and pull requests, SWE-bench task instances capture a range of software bugs and verify solution behavior by executing unit tests. Since the introduction of SWE-bench in October 2023, state-of-the-art performance on SWE-bench Lite, the most commonly used subset of SWE-bench, has soared from 3% to 43% (Jimenez et al., 2024b).\nHowever, with respect to the broader landscape of software engineering, SWE-bench reflects only a fraction of real-world applications. Its 17 repositories are predominantly written in Python. Task in-stance codebases tend to be structured similarly because every repository is a PyPI package. Though"}, {"title": "2 SWE-BENCH MULTIMODAL", "content": "We first review SWE-bench's task formulation and limitations (Section 2.1) and describe how SWE-bench M builds upon this work. We then discuss SWE-bench M's data collection heuristics (Sec-tion 2.2). Finally, we thoroughly characterize SWE-bench M (Section 2.3), highlighting the novel challenges it poses to LM agent-based software development."}, {"title": "2.1 PRELIMINARIES", "content": "Formulations. SWE-bench (Jimenez et al., 2024a) has emerged as a popular LM agent benchmark. By drawing from real GitHub workflows, task instances reflect diverse, practical software challenges and require systems to ingest meaningful, long-form inputs. Repository-level coding also involves meticulous refactoring of interwoven modules, each with dependency trees spanning multiple source files. SWE-bench's collection strategy yields human-written unit tests that ensure robust evaluation.\nSWE-bench consists of 2,294 task instances derived from pull requests (PRs) collected across 12 open source Python repositories. Each task instance corresponds to a PR and one or more resolved issues; issue(s) describe a bug or feature request, and the PR contains the corresponding solution code along with unit tests validating its correctness. The unit tests fail before the solution code is applied to the codebase but pass afterwards, also referred to as fail-to-pass (F2P) tests. Additional auxiliary pass-to-pass (P2P) tests verify that the codebase's existing functionality is maintained.\nA task worker is shown the codebase and the issue, also called the problem statement. The worker must then modify the codebase to solve the problem. The proposed modification is run against both F2P and P2P tests to check if (1) the issue is fixed and (2) prior working behavior is not broken. If all unit tests pass, the task instance is considered resolved.\nLimitations. Within the SWE-bench task formulation, several facets of software development re-main unexplored. This work primarily investigates two such facets. First, SWE-bench task instances are predominantly text only, and there are no discussions of the implications of the interplay between images and videos with software development. For the 5.6% of SWE-bench task instances with an image, it is unclear what these images portray and whether they are necessary to solving the task. To fill this gap, we focus on task instances with visual elements and demonstrate the significance of multi-modal reasoning in software engineering evaluations. Second, SWE-bench is comprised exclusively of Python repositories. We show that a benchmark containing SWE-bench-style task in-stances from an alternative programming language, i.e., JavaScript, highlights previously unconsid-ered complexities (e.g., web development, asynchronous programming, DOM/state manipulation)."}, {"title": "2.2 COLLECTION", "content": "JavaScript repositories have a high concentration of visual assets due to its popularity for full stack web development and browser manipulation. We summarize our modifications to SWE-bench's task collection pipeline to identify task instances with visual components. See Appendix B for details.\n1. Find user-facing JavaScript repositories. Using GitHub's search feature, we look for JavaScript repositories with 5,000 or more stars and 500 or more pull requests. We then manually pick 17 repositories, filtering for user-facing libraries that have a visual aspect, such as mapping, plotting,"}, {"title": "2.3 FEATURES", "content": "SWE-bench M is a dataset of 619 multimodal, JavaScript task instances collected from 17 open-source GitHub repositories. It contains a diverse set of libraries, including ones for data visual-ization, building diagrams, website UI components, displaying maps, and syntax highlighting. As shown in Figure 2, the test split consists of 517 task instances from 12 repositories. The development split contains 102 task instances from 5 repositories.\nDiversity of images. Across all SWE-bench M task instances, there are 862 \"problem statement\" images, meaning the image hyperlink is in the issue text. These images feature a multitude of visual processing challenges. Our annotation procedure grouped these images into seven distinct"}, {"title": "3 EVALUATING ON SWE-BENCH M", "content": "We now discuss the challenge of generalization for existing open-source solutions for SWE-bench and describe how we have adapted these methods for evaluation on SWE-bench M (Section 3.1). Additionally, we provide details about our the setup of our experiments (Section 3.2)."}, {"title": "3.1 Do EXISTING SYSTEMS GENERALIZE?", "content": "We attempted to run existing open-source systems that perform well on SWE-bench (Jimenez et al., 2024b) on SWE-bench M. However, when adapting these systems to SWE-bench M, we found that several solutions were so heavily tailored to Python and SWE-bench to the point that they were unusable for JavaScript repositories and SWE-bench M evaluation.\nOur observations from exploring a number of existing solutions led us to identify generalizability as a desirable but overlooked property of automated software engineering systems. Specifically, do existing agent systems perform well on bugs that are not similar to the ones found in SWE-bench (i.e., non-Python or issues that include images)?\nThough current LMs perform well in multiple programming languages (Cassano et al., 2022), a rigidly defined system can force LMs to follow a particular problem solving pipeline that restricts its capabilities. Although such workflow-oriented systems may work well for a specific type of repository or even programming language, they can fail when confronted with minor distribution shifts outside their original design parameters. Simply put, the shift from Python to JavaScript or the addition of the image modality exceeds the abilities of many existing approaches."}, {"title": "3.2 EXPERIMENT SETUP", "content": "Models. Resolving SWE-bench M issues with existing systems requires handling very long con-texts, processing images and text simultaneously, and producing sophisticated structured outputs. Given these constraints, we focus all of our evaluations on GPT-40 (gpt-40-2024-08-06) (Ope-nAI, 2024) and Claude 3.5 Sonnet (claude-3-5-sonnet-20240620) (Anthropic, 2024), the two"}, {"title": "4 RESULTS", "content": "We compare the performance of each baseline system in Table 3. While overall performance on SWE-bench M is relatively low, we observe a substantial performance gap between the interactive SWE-agent systems (11.5% resolved on average) and the Agentless and RAG baselines, which achieve 3.9 and 5.5% resolved on average respectively.\nAcross all SWE-agent configurations, we observe similar absolute performance, suggesting that the JavaScript-specific customizations in SWE-agent JS and SWE-agent M had minimal impact on performance. Additionally, we also observe minimal performance differences when swapping the underlying LM (GPT-40 or Claude 3.5 Sonnet) within a system. However, a study of SWE-agent ablations on the development set reveals that the added multimodal tooling can improve agent performance in some cases, though the overall picture remains ambiguous.\nAn analysis of solution rates segmented by the date each task instance was originally solved, is performed in Appendix C.3. It finds no evidence for a test set advantage from solutions leaking into the LM training sets: we show for example, that SWE-agent M using GPT-40 performs better on task instances based on issues that were resolved after its knowledge cutoff date."}, {"title": "4.1 ANALYSIS", "content": "We present insights into how images influence system performance, evaluate the impact of multi-modal actions in SWE-agent M, and explore the generalization of different approaches to automated software engineering. All analyses are conducted on the development set of SWE-bench M.\nResolving SWE-bench M issues requires improved visual understanding. Table 4 compares the performance of the RAG and SWE-agent JS systems, with and without images. The results"}, {"title": "5 RELATED WORK", "content": "Multimodal Code Benchmarks. Code generation, the task of synthesizing code from natural lan-guage, has served as a long-standing measure of LM performance (Austin et al., 2021; Chen et al., 2021; Hendrycks et al., 2021). As performance on these benchmarks has plateaued\u2014Claude Sonnet 3.5 achieves 92% on HumanEval\u2014subsequent works have extended this task along different di-mensions to increase difficulty, such as more robust evaluation (Liu et al., 2023a; Jain et al., 2024), multilinguality (Cassano et al., 2022; Wang et al., 2023a; Zheng et al., 2024), open-domain gener-ation (Wang et al., 2023b; Zhuo et al., 2024), data science (Lai et al., 2022; Yin et al., 2022; Cao et al., 2024), understanding execution traces (Gu et al., 2024; Muennighoff et al., 2024), repository comprehension (Liu et al., 2023b; Zhang et al., 2023b), cybersecurity (Yang et al., 2023b; Zhang et al., 2024a; Shao et al., 2024; Abramovich et al., 2024), and efficiency (Huang et al., 2024; Liu et al., 2024; Waghjale et al., 2024). SWE-bench M represents a merger of two promising directions: software engineering (Jimenez et al., 2024a) and multimodal code generation (Li et al., 2024; Si et al., 2024; Wu et al., 2024; Nishina & Matsui, 2024). By drawing on real-world problems from GitHub, SWE-bench M overcomes the synthetic and short-form limitations inherent in code gener-ation task formulation. In addition, SWE-bench M represents a significant multimodal upgrade to SWE-bench, which has already become a popular benchmark for LM evaluation.\nLM Agents for Web and Code. Prior to the proliferation of LMs, earlier works explored translating between user interfaces and front-end code (HTML/CSS/JavaScript) (Beltramelli, 2018; Robinson, 2019). More recently, LM agents (Yao et al., 2023b; Sumers et al., 2024) have been increasingly employed and evaluated on various tasks (Wang et al., 2024a; Xie et al., 2024). Two popular use cases, web or application navigation (Hong et al., 2023; Soselia et al., 2023; Yan et al., 2023; Yao et al., 2023a; Zhang et al., 2023a; Koh et al., 2024; Putta et al., 2024; Rawles et al., 2024; Press et al., 2024; Yoran et al., 2024) and software engineering (Yang et al., 2023a; 2024; Zhang et al., 2024b; Xia et al., 2024), have typically been studied separately. To the best of our knowledge, SWE-bench M is the first benchmark that meaningfully couples these two tasks. Though prior approaches have provided web browsing tools to programming agents (Wang et al., 2024b), such tools usually represent webpages in text form and were not designed with a clear downstream objective. SWE-bench M demonstrates how agents can iterate meaningfully between code updates and their effects rendered as images or in a browser."}, {"title": "6 CONCLUSION", "content": "This work introduces SWE-bench Multimodal (SWE-bench M), the first benchmark to evaluate coding agents on real-world software engineering tasks involving visual elements. SWE-bench M contains 619 task instances from 17 user-facing JavaScript repositories, including ones for web user interface design, data visualization, art and mapping. Our analysis reveals that SWE-bench M contains diverse visual challenges and increases task complexity compared to SWE-bench. Fur-thermore, existing systems perform poorly on it, with the top resolve rate reaching only 12.2%; SWE-bench M presents repository-level programming tasks that are challenging even for state-of-the-art systems built on top of the strongest LMs. Incorporating multimodality in SWE-bench M not only expands the coverage of exciting, practical challenges in software engineering, but it also encourages practitioners to develop more general-purpose, language-agnostic solutions that do not overfit to SWE-bench or Python repositories."}]}