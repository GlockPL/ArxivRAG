{"title": "Alignment Between the Decision-Making Logic of LLMs and Human Cognition: A Case Study on Legal LLMS", "authors": ["Lu Chen", "Yuxuan Huang", "Yixing Li", "Yaohui Jin", "Shuai Zhao", "Zilong Zheng", "Quanshi Zhang"], "abstract": "This paper presents a method to evaluate the alignment between the decision-\nmaking logic of Large Language Models (LLMs) and human cognition in a case\nstudy on legal LLMs. Unlike traditional evaluations on language generation results,\nwe propose to evaluate the correctness of the detailed decision-making logic of an\nLLM behind its seemingly correct outputs, which represents the core challenge for\nan LLM to earn human trust. To this end, we quantify the interactions encoded by\nthe LLM as primitive decision-making logic, because recent theoretical achieve-\nments [Li and Zhang, 2023, Ren et al., 2024] have proven several mathematical\nguarantees of the faithfulness of the interaction-based explanation. We design a set\nof metrics to evaluate the detailed decision-making logic of LLMs. Experiments\nshow that even when the language generation results appear correct, a significant\nportion of the internal inference logic contains notable issues.", "sections": [{"title": "Introduction", "content": "The trustworthiness and safety of Large Language Models (LLMs) present significant challenges for\ntheir deployment in high-stake tasks [OpenAI, 2023, Wei et al., 2023]. Previous evaluation methods\nmainly evaluated the correctness of language generation results, in terms of value alignment and\nhallucination problems [Bang et al., 2023, Ji et al., 2023b,a, Shen et al., 2023a].\nIn this study, we hope to go beyond the long-tail evaluation of the generation results, and focus on the\ncorrectness of the detailed decision-making logic used by the LLM behind the language generation\nresult. We focus on the legal LLM as a case study, and the legal LLM may use significantly incorrect\ninformation to make judgment, even when the generation result is correct. The alignment of decision-\nmaking logic between the AI model and human cognition is crucial for alleviating the common fear\nof AI models. The alignment of internal logic via communication is the reason why people naturally\ntrust each other. Particularly, in high-stakes tasks such as autonomous driving [Grigorescu et al.,\n2020], the lack of alignment between AI models and human users makes people would rather delegate\nwork to humans and tolerate potential errors, than trust highly accurate AI models.\nTherefore, this paper aims to explore the possibility of aligning the decision-making logic for\nthe confidence score of the LLM's judgment with human cognition. To this end, exploring the"}, {"title": "Alignment between the LLM and human cognition", "content": ""}, {"title": "Prelimiaries: interactions", "content": "Although there is no widely-accepted definition of concepts, which is an interdisciplinary issue across\ncognitive science, neuroscience, artificial intelligence, and mathematics, the theory of interactions\nhas shown promise in explaining the primitive inference patterns encoded by the DNN. A series\nof properties [Li and Zhang, 2023, Ren et al., 2023a, 2024] have been proposed as mathematical\nguarantees for the faithfulness of the interaction-based explanations."}, {"title": "Definition of AND-OR interactions.", "content": "Given an input sample $x = [x_1,x_2,\\ldots,x_n]^\\top$ with $n$ input\nvariables indexed by $N = \\{1,2, ..., n\\}$, where each input variable can represent a token, a word,\nor a phrase/short sentence. Then, let $v(x) \\in \\mathbb{R}$ denote the scalar confidence of generating the\ntarget output. For example, the target output can be set to a sequence of $m$ ground-truth tokens\n$[y_1, y_2,\\ldots, y_m]$ generated by the LLM. In this way, the scalar confidence of language generation\n$v(x)$ can be defined as follows.\n$v(x) \\stackrel{\\text{def}}{=} \\log \\prod_{t=1}^T \\frac{p(y=y_t|x, Y_{\\text{previous}})}{1-p(y=y_t|x, Y_{\\text{previous}})}$\nwhere $Y_{\\text{previous}} \\stackrel{\\text{def}}{=} [y_1, y_2,\\ldots, y_{t-1}]^\\top$ represents the sequence of the previous $(t-1)$ tokens before\ngenerating the $t$-th token. $p(y=y_t|x, Y_{\\text{previous}})$ denotes the probability of generating the $t$-th token,\ngiven the input sentence $x$ and the previous $(t-1)$ tokens. In particular, $Y_{\\text{previous}} = []$.\nTo explain the inference patterns behind the confidence score $v(x)$, Ren et al. [2024], Shen et al.\n[2023b] show that an LLM usually encodes a set of interactions between input variables (tokens\nor phrases) to compute $v(x)$. There are two types of interactions, i.e., the AND interaction and the\nOR interaction. Each AND interaction and each OR interaction w.r.t. $S \\subseteq N$, $S \\neq \\emptyset$ have specific\nnumerical effects $I_{\\text{and}}(S|x)$ and $I_{\\text{or}}(S|x)$ to the network output, respectively, which are computed as\nfollows.\n$I_{\\text{and}}(S|x) \\stackrel{\\text{def}}{=} \\sum_{T \\subseteq S} (-1)^{|S|-|T|}v_{\\text{and}}(x_T), \\quad I_{\\text{or}}(S|x) \\stackrel{\\text{def}}{=} - \\sum_{T \\subseteq S} (-1)^{|S|-|T|}v_{\\text{or}}(x_{N \\setminus T})$\nwhere $x_T$ denotes the masked sample, where all embeddings of input variables in $N \\setminus T$ are\nmasked. $v(x_T) \\in \\mathbb{R}$ denotes the confidence score of generating the $m$ tokens $[y_1, y_2,\\ldots, y_m]$\ngiven the masked sample $x_T$. $v(x_T)$ is decomposed into the component for AND interactions\n$v_{\\text{and}}(x_T) = 0.5v(x_T) + \\gamma_T$ and the component for OR interactions $v_{\\text{or}}(x_T) = 0.5v(x_T) - \\gamma_T$,\nsubject to $v_{\\text{and}}(x_T) + v_{\\text{or}}(x_T) = v(x_T)$."}, {"title": "Extracting AND-OR interactions.", "content": "According to Equation (2), the extraction of interactions is\nimplemented by learning parameters $\\{\\gamma_T\\}$. We follow [Zhou et al., 2024] to learn parameters\n$\\{\\gamma_T|T \\subseteq N\\}$, and extract the sparest (the simplest) AND-OR interaction explanation via the LASSO-\nlike loss, i.e., $\\min_{\\{\\gamma_T\\}} \\sum_{S \\subseteq N, S \\neq \\emptyset} [|I_{\\text{and}}(S|x)| + |I_{\\text{or}}(S|x)|]$. In this way, we exhaustively compute\ninteraction effects $I_{\\text{and}}(S|x)$ and $I_{\\text{or}}(S|x)$ for all $(2^n - 1)$ non-empty combinations $\\emptyset \\neq S \\subseteq N$.\nRen et al. [2024] have proven that most interactions have almost zero effects $I_{\\text{and/or}}(S|x)$, and\nan LLM usually activates only 100-200 AND-OR interactions with salient effects. These salient\ninteractions are taken as the AND-OR logic really encoded by the LLM."}, {"title": "Relevant tokens, irrelevant tokens, and forbidden tokens", "content": "According to above achievements, we can take a small set of salient AND-OR interactions as the\nfaithful explanation for the decision-making logic used by the legal LLM. Thus, in this subsection, we\nannotate the relevant, irrelevant, and forbidden tokens in the input legal case, in order to accurately\nidentify the reliable and unreliable interactions encoded by the LLM (see Figure 1). Specifically, the\nset of all input variables $N$ is partitioned into three mutually disjoint subsets, i.e., the set of relevant\ntokens $R$, the set of irrelevant tokens $I$, and the set of forbidden tokens $F$, subject to $R \\cup I \\cup F = N$,\nwith $R \\cap I = \\emptyset$, $R \\cap F = \\emptyset$, and $I \\cap F = \\emptyset$, according to human cognition.\nRelevant tokens refer to tokens that are closely related to or serve as the direct reason for the\njudgment, according to human cognition. For instance, given an input legal case \"on June 1,\nduring a conflict on the street, Andy stabbed Bob with a knife, causing Bob's death,\" the legal\nLLM provides judgment \u201cmurder\u201d for Andy. In this case, the input variables can be set as $N =$\n\\{\\[on June 1\\], \\[during a conflict\\], \\[on the street\\], \\[Andy stabbed Bob with a knife\\], \\[causing Bob's death\\]\\}."}, {"title": "Reliable and unreliable interaction effects", "content": "The categorization of relevant, irrelevant, and forbidden tokens enables us to disentangle the reliable\nand unreliable decision-making logic used by a legal LLM. As introduced in Section 2.1, we use\ninteractions as the decision-making logic encoded by a legal LLM. Thus, in this subsection, we\ndecompose the overall interaction effects in Equation (2) into reliable and unreliable interaction\neffects. Reliable interaction effects are interaction effects that align with human cognition, which\nusually contain relevant tokens and exclude forbidden tokens. In contrast, unreliable interaction\neffects are interaction effects that do not match human cognition, which are attributed to irrelevant or\nforbidden tokens."}, {"title": "Evaluation metrics", "content": "In this subsection, we design a set of metrics to evaluate the alignment quality between the interactions\nencoded by the LLM and human cognition."}, {"title": "Ratio of reliable interaction effects.", "content": "Definition 1 introduces the ratio of reliable interaction effects\nthat align with human cognition to all salient interaction effects. Here, we focus on the small number\nof salient interactions in $\\Omega_{\\text{and}}$ and $\\Omega_{\\text{or}}$, rather than conduct evaluation on interactions effects of all $2^n$\nsubsets $S \\subseteq N$. This is because salient interactions can be taken as primitive decision-making logic\nof an LLM, while all other interactions have negligible effects and represent noise patterns."}, {"title": "Definition 1 (Ratio of reliable interaction effects).", "content": "Given an LLM, the ratio of reliable interaction\neffects to all salient interaction effects $s_{\\text{reliable}}$ is computed as follows.\n$s_{\\text{reliable}} = \\frac{\\sum_{S \\in \\Omega_{\\text{and}}} |I_{\\text{reliable}}(S|x)| + \\sum_{S \\in \\Omega_{\\text{or}}} |I_{\\text{reliable}}(S|x)|}{\\sum_{S \\in \\Omega_{\\text{and}}} |I_{\\text{and}}(S|x)| + \\sum_{S \\in \\Omega_{\\text{or}}} |I_{\\text{or}}(S|x)|}$"}, {"title": "Interaction distribution over different orders.", "content": "Zhou et al. [2024] have found that the low-order\ninteractions usually exhibit stronger generalization power than high-order interactions. I.e., low-\norder interactions learned from training samples are more likely to be transferred to (appear in) testing\nsamples. Please see Appendix E for the definition and quantification of the generalization power of\ninteractions over different orders. Specifically, the order is defined as the number of input variables\nin $S$, i.e., $\\text{order}(S) = |S|$. In general, high-order interactions (complex interactions) between a large\nnumber of input variables are usually less generalizable than low-order (simple) interactions.\nTherefore, we utilize the distribution of interactions over different orders as another metric, which\nevaluates the generalization power of the decision-making logic used by the LLM. Specifically, we use\n$\\text{Salient}^+(o) = \\sum_{\\text{op} \\in \\{\\text{and,or}\\} } \\sum_{S \\in \\Omega^{\\text{op}}, |S|=o} \\max(0, I_{\\text{op}}(S|x))$ to quantify the overall strength of positive\nsalient interactions, and use $\\text{Salient}^-(o) = \\sum_{\\text{op} \\in \\{\\text{and,or}\\} } \\sum_{S \\in \\Omega^{\\text{op}}, |S|=o} \\min(0, I_{\\text{op}}(S|x))$ to quantify the\noverall strength of negative salient interactions. A well-trained legal LLM tends to model low-order\ninteractions, while an over-fitted LLM (potentially due to insufficient data or inadequate data cleaning)\nusually relies more on high-order interactions."}, {"title": "Ratio of reliable interaction effects of each order.", "content": "We categorize all salient interaction effects by\ntheir orders, so that for all salient interactions of each $o$-th order, we can compute the ratio of reliable\ninteraction effects.\nThe generalization power of an interaction is defined as the transferability of this interaction from training\nsamples to test samples. Specifically, if an interaction pattern $S \\subseteq N$ frequently occurs in the training set, but\nrarely appears in the test set, then the interaction pattern $S$ exhibits low generalization power. Conversely, if an\ninteraction pattern $S$ consistently appears in both the training and test sets, it demonstrates high generalization\npower. Please see Appendix E for details."}, {"title": "Definition 2 (Ratio of reliable interaction effects of each order).", "content": "The ratio of reliable interac-\ntion effects to all positive salient interaction effects of the $o$-th order is measured by $s_{\\text{reliable}, +} =$\n$\\frac{\\text{Reliable}^+(o)}{\\text{Salient}^+(o)+\\epsilon}$.\nSimilarly, the ratio of reliable interaction effects to all negative salient inter-\naction effects of the $o$-th order is measured by $s_{\\text{reliable}, -} = \\frac{|\\text{Reliable}^-(o)|}{|\\text{Salient}^-(o)|+\\epsilon}$.\n$\\text{Reliable}^+(o) = \\sum_{\\text{op} \\in \\{\\text{and,or}\\} } \\sum_{S \\in \\Omega^{\\text{op}}, |S|=o} \\max(0, I_{\\text{reliable}}(S|x))$ represents the overall strength of positive reliable inter-\nactions of the o-th order, and $\\text{Reliable}^-(o) = \\sum_{\\text{op} \\in \\{\\text{and,or}\\} } \\sum_{S \\in \\Omega^{\\text{op}}, |S|=o} \\min(0, I_{\\text{reliable}}(S|x))$ represents\nthe overall strength of negative reliable interactions of the $o$-th order. $\\epsilon$ is a small constant to avoid\ndividing 0.\nAccording to the findings in [Zhou et al., 2024], low-order interactions generally represent stable\npatterns that are frequently used across a large number of legal cases. Thus, if a considerable ratio\nof low-order interactions contain unreliable effects, it suggests that training data may have a clear\nbias, which makes the LLM stably learns unreliable interactions. In comparison, since high-order\ninteractions typically exhibit poor generalization power, unreliable effects in high-order interactions\nare usually attributed to the memorization of hard/outlier samples. Consequently, low-order unreliable\ninteractions are are mainly owing to stable bias in the training data, while high-order unreliable\ninteractions often indicates that the LLM learns outlier features."}, {"title": "Experiment", "content": "In this section, we conducted experiments to evaluate the alignment quality between the decision-\nmaking logic of the legal LLM and human cognition. In this way, we identified potential representation\nflaws behind the seemingly correct language generation results of legal LLMs.\nWe applied two off-the-shelf legal LLMs, SaulLM-7B-Instruct [Colombo et al., 2024] and BAI-Law-\n13B [Institute, 2023], which were trained for legal judgment prediction on English legal corpora\nand Chinese legal corpora, respectively. Appendix F shows the accuracy of these LLMs. Given an\ninput legal case, the LLM predicted the judgment result based on the fact descriptions of the legal\ncase. We explained judgments made on legal cases in the CAIL2018 dataset [Xiao et al., 2018],\nwhich contained 2.6 million Chinese legal cases, for both legal LLMs. Figure 6 shows the universal-\nmatching property of the extracted interactions, i.e., when we randomly masked input variables in the\nlegal case, we could always use the interactions to accurately match the real confidence scores of the\njudgment estimated by the LLM.\nTo simplify the explanation and avoid ambiguity, we only explained the decision-making logic on\nlegal cases, which were correctly judged by the LLM. For each input legal case, we manually selected\nsome informative tokens or phrases as input variables. Some tokens or phrases were annotated as\nrelevant tokens in R, while others were identified as irrelevant tokens in I. It was ensured that the\nremoval of all input variables would substantially change the legal judgment result.\nWe extracted AND-OR interactions that determined the confidence score $v(x)$ of generating judgment\nresults with a sequence of tokens, according to Equation (1). To accurately identify and analyze\npotential representation flaws from these interactions, in this paper, we mainly focused on potential\nrepresentation flaws w.r.t. legal judgments in the following three types, i.e., (1) judgments influenced\nby unreliable sentimental tokens, (2) judgments affected by incorrect entity matching, and (3)\njudgments biased by discrimination in occupation."}, {"title": "Problem 1: making judgments based on unreliable sentimental tokens.", "content": "We observed that although\nlegal LLMs achieved relatively high accuracy in predicting judgment results (see Appendix F), a\nconsiderable number of interactions contributing to the confidence score $v(x)$ were attributed to\nsemantically irrelevant or unreliable sentimental tokens. The legal LLM was supposed to focus more\non real criminal actions, than unreliable sentimental tokens behind the actions, when criminal actions\nhad been given. We believed these indicated potential representation flaws behind the seemingly\ncorrect legal judgments produced by legal LLMs. To evaluate the impact of unreliable sentimental"}, {"title": "Conclusion", "content": "In this paper, we have proposed a method to evaluate the correctness of the detailed decision-making\nlogic of an LLM. The sparsity property and the universal matching property of interactions provide\ndirect mathematical supports for the faithfulness of the interaction-based explanation. Thus, in this\npaper, we have designed two new metrics to quantify reliable and unreliable interaction effects,\naccording to their alignment with human cognition. Experiments showed that the legal LLMs often\nrelied on a considerable number of problematic interactions to make judgments, even when the\njudgement prediction was correct. The evaluation of the alignment between the decision-making\nlogic of LLMs and human cognition also contributes to other real applications. For example, it may\nassist in debugging the hallucination problems, and identifying potential bias behind the language\ngeneration results of LLMs."}, {"title": "Limitations.", "content": "Our analysis does not assess the correctness of the numerical scores for interactions,\nas these scores are often determined by many factors. Positive interactions typically indicate logics\nthat contribute positively to the judgments, while negative interactions may also be intended for\nother possible correct judgments. Besides, the evaluation based on relevant, irrelavant, and forbidden\ntokens is only one of the conditions for reliable interactions, and reliable interactions may not\nalways be correct. Nevertheless, this paper presents a precedent for evaluating the correctness of\ndecision-making logic of LLMs."}, {"title": "A Related work", "content": "Factuality in LLMs refers to whether the language gen-\neralization results of LLMs align with the verificable facts. This includes the ability of LLMs to\navoid producing misleading or incorrect information (i.e., factual hallucination), and to effectively\ngenerate factually accurate results. For instance, several studies have evaluated the correctness of\nLLM-generated answers to specific questions [Lin et al., 2021, OpenAI, 2023, Wang et al., 2024].\nOther works have standardized fact consistency tasks into binary labels, evaluating whether there\nwere factual conflicts within the input text [Honovich et al., 2022]. Min et al. [2023] further de-\ncomposed language generation results into \u201catomic\u201d facts, and calculated the proportion of these\nfacts that aligned with a given knowledge source. Additionally, Manakul et al. [2023] introduced\na sampling-based method to verify whether LLMs generated factually consistent results, based on\nthe assumption that if an LLM had knowledge of a concept, then the sampled generation results\ncontained consistent factual information."}, {"title": "B Proof of Theorem", "content": "Theorem 1 (Universal matching property) Given an input sample x, the network output score\nv(xT) \u2208 R on each masked sample {xT|TC N} can be well matched by a surrogate logical model\nh(x) on each masked sample {xT|T \u2286 N}. The surrogate logical model h(xT) uses the sum of"}, {"title": "C Proof of Lemma", "content": "The surrogate logical model h(x) on each randomly masked sample\nx,TCN mainly uses the sum of a small number of salient AND interactions and salient OR"}, {"title": "D OR interactions can be considered specific AND interactions", "content": "The OR interaction I(S|x) can be considered as a specific AND interaction I(S|x), if we inverse\nthe definition of the masked state and the unmasked state of an input variable."}, {"title": "E Generalization power of interactions over different orders", "content": "In this section, we will give the definition and quantification of the generalization power of interactions\nover different orders. The generalization power of an interaction is defined as the transferability of\nthis interaction from training samples to test samples. Specifically, if an interaction pattern SN\nfrequently occurs in the training set, but rarely appears in the test set, then the interaction pattern S\nexhibits low generalization power. Conversely, if an interaction pattern S consistently appears in both\nthe training and test sets, it demonstrates high generalization power."}, {"title": "F Accuracy of the legal LLM", "content": "Colombo et al. [2024] reported the accuracy of the SaulLM-7B-Instruct model, which achieved state-\nof-the-art results among 7B models, within the legal domain. Specifically, they followed [Guha et al.,\n2023] to use balanced accuracy as the metric. Balanced accuracy shows its strength for handling\nimbalanced classification tasks. They tested the balanced accuracy on two popular benchmarks,\ni.e., the LegalBench-Instruct benchmark [Guha et al., 2023] and the Massive Multitask Language\nUnderstanding (MMLU) benchmark [Hendrycks et al., 2021]. The LegalBench-Instruct benchmark is\na supplemental iteration of LegalBench [Guha et al., 2023], designed to evaluate the legal proficiency\nof LLMs. To further evaluate the performance of LLMs in legal contexts, the authors incorporated\nlegal tasks from the MMLU benchmark, focusing specifically on the international law, professional\nlaw and jurisprudence."}, {"title": "G More experiment results and details", "content": ""}, {"title": "More results of judgments influenced by unreliable sentimental tokens", "content": "We conducted more experiments to show the judgments influenced by unreliable sentimental tokens\nin Figure 7, Figure 8, and Figure 9, respectively. We observed that a considerable number of\ninteractions contributing to the confidence score v(x) were attributed to semantically irrelevant or\nunreliable sentimental tokens. In different legal cases, the ratio of reliable interaction effects to\nall salient interactions was within the range of 32.6% to 87.1%. It means that about 13~68% of\ninteractions used semantically irrelevant tokens or unreliable sentimental tokens for the judgment."}, {"title": "More results of judgments affected by incorrect entity matching", "content": "We conducted more experiments to show the judgments affected by incorrect entity matching\nin Figure 10, Figure 11, and Figure 12, respectively. We observed that a considerable ratio of the\nconfidence score v(x) was mistakenly attributed to interactions on criminal actions made by incorrect\nentities. In different legal cases, the ratio of reliable interaction effects to all salient interactions was\nwithin the range of 31.9% to 67.8%. It means that about 22~68% of interactions used semantically"}]}