{"title": "DYNAMIC LANGUAGE GROUP-BASED MOE: ENHANCING EFFICIENCY AND\nFLEXIBILITY FOR CODE-SWITCHING SPEECH RECOGNITION", "authors": ["Hukai Huang", "Shenghui Lu", "Yahui Shan", "He Qu", "Wenhao Guan", "Qingyang Hong", "Lin Li"], "abstract": "The Mixture of Experts (MoE) approach is ideally suited\nfor tackling multilingual and code-switching (CS) challenges\ndue to its multi-expert architecture. This work introduces the\nDLG-MoE, which is optimized for bilingual and CS scenar-\nios. Our novel Dynamic Language Group-based MoE layer\nfeatures a language router with shared weights for explicit\nlanguage modeling, while independent unsupervised routers\nwithin the language group handle attributes beyond language.\nThis structure not only enhances expert extension capabili-\nties but also supports dynamic top-k training, allowing for\nflexible inference across various top-k values and improv-\ning overall performance. The model requires no pre-training\nand supports streaming recognition, achieving state-of-the-art\n(SOTA) results with unmatched flexibility compared to other\nmethods. The Code will be released.", "sections": [{"title": "1. INTRODUCTION", "content": "Thanks to its ability to scale parameters with minimal impact\non inference cost, the Mixture of Experts (MoE) approach has\nachieved exceptional results in natural language processing\nand computer vision [1, 2, 3]. The inherent multi-expert struc-\nture of MoE provides a natural advantage for multilingual\ntasks. Numerous studies [4, 5, 6] have demonstrated its effec-\ntiveness in multilingual automatic speech recognition (ASR),\nhighlighting MoE's strong performance in this domain. How-\never, these architectures still struggle to effectively handle CS\nscenarios.\nCS scenarios often involve using different languages\nwithin utterances, leading to significant challenges in model\ndevelopment due to language confusion. In response, Bi-\nencoder architectures [7, 8, 9, 10] use separate encoders for\neach language and employ language-specific masking strate-\ngies [8, 9, 11] to enhance individual language processing."}, {"title": "2. PROPOSED METHOD", "content": null}, {"title": "2.1. Overall Model Architecture", "content": "The overall architecture of our model is shown in Fig.1(a),\ncomprising Conformer [15] Blocks and DLG-MoE Blocks.\nThe Conformer Block is stacked from vanilla Conformer lay-\ners, while the DLG-MoE Block is formed by stacking DLG-\nMoE layers. Fig.1(b) shows the structure of the DLG-MoE\nlayer, where the second FFN of the Conformer layer is re-\nplaced by a dynamic language group. The weight-shared lan-\nguage router (SLR) performs the LID task based on the in-\ntermediate representations hinter extracted by the Conformer\nblock and uses the LID results to guide the selection of the\nappropriate language group."}, {"title": "2.2. Language Router", "content": "As illustrated in Fig.1(c), we expand MoE by introducing lan-\nguage expert groups, specifically the Chinese expert group\n(Zh-Group) and the English expert group (En-Group). Un-\nlike other methods [14, 12, 4], our language router explicitly\nlearns frame-level LID and is trained using an inter-loss func-\ntion, as expressed by the following formula Equation1.\n$L_{inter} = (log P_{CTC} (V_{LID} | h_{inter} W_{dLID}^{aL+1})\n+ log P_{CTC} (V_{ASR} | h_{inter} W_{dASR}^{aV})) $\nWe enhance the SLR's language routing capability through\na multi-task training approach that integrates ASR and LID\ntasks. Here, $V_{ASR}$ denotes the ground truth for the ASR task,\nwhile $V_{LID}$ is derived by mapping the tokens in $V_{ASR}$ to their\ncorresponding language IDs. The linear layers $W_{dLID}^{aL+1}$ and\n$W_{dASR}^{aV}$ represent the transformations, where d is the embed-"}, {"title": "2.3. Dynamic Language Group", "content": "As shown in Fig. 1(c), once the representations for the same\nlanguage are obtained, the UnSup-Router within the language\nexpert group focuses on modeling attributes beyond language.\nFor each frame in $h_{pre}^{zh}$, the UnSup-Router selects k experts\nfrom the total n experts within the group to collaborate, as\ndescribed below:\n$G^{zh} = \\sum_{i=1}^{k} G_i^{zh} E_i$\n$G_r = Softmax (TopK (h_{pre}^{zh} W_{dR}^{aR}, k))$\nHere, $G^{zh}$ and Ei denote the representations from the se-\nlected experts and their corresponding weights. In Equation\n3, $W_{dR}^{aR}$ is a linear layer that projects the representation"}, {"title": "3. EXPERIMENTS AND ANALYSIS", "content": null}, {"title": "3.1. Experiment setup", "content": "Our experiments are conducted on the ASRU-2019 Mandarin-\nEnglish code-switching challenge dataset [16], along with a\nsubset of 460 hours from Librispeech [17], consistent with\n[7, 12, 13]. For testing, we use Librispeech-clean (EN),\nASRU-Man-test (ZH), and ASRU-CS-test (CS) datasets.\nAdditionally, we have validated our model on an internal\nmulti-accent, multi-domain dataset, which, combined with\nthe ASRU-2019 Challenge dataset, totals 6,000 hours of\ntraining data.\nWe conduct our experiments on the WeNet [18] toolkit.\nFor acoustic features, 80-dimensional log-mel filter banks\n(FBank) are extracted with a step size of 10ms and a window\nsize of 25ms. All our experimental configurations used a\n6-layer Transformer decoder. We set $d_{model}=256$, $n_{heads}=4$\nand $d_{ffn}=2048$. Causal convolution was used during training\nto support streaming. We built Baseline-12 and Baseline-\n18 with 12 and 18 Conformer layers, respectively. All MoE\nmodels had 12 layers, with the first 6 being vanilla Conformer\nlayers and the last 6 incorporating MoE layers. Dense-MoE-\n4e and Sparse-MoE-4e, which do not include language rout-\ning, each had 4 MoE experts. In contrast, DLG-MoE-4e,\nalso with 4 experts, included language routing, allocating two\nexperts per language group. To explore model capacity and\ndifferent top-k strategies, we created DLG-MoE-8e with 8\nexperts. For large-scale comparisons, we used $d_{model}=512$\nand $n_{heads}=8$ for Baseline-18 and DLG-MoE-8e, while keep-\ning other configurations the same. The hyperparameters $A_{cte}$\nand $A_{inter}$ were set to 0.3 and 0.1, respectively, with the\nAdam optimizer using lr=0.001 and $warmup_{steps}=25000$.\nDetailed information can be found in the link provided"}, {"title": "3.2. Results of different MoE structures", "content": "The results in Table 1 show that MoE structures (S3, S4)\noutperform S2 with the same number of parameters, indi-\ncating that widening the model with MoE might more effec-\ntive than simply increasing he number of layers. This sug-\ngests that even when using only the UnSup-Router, experts\ncan still implicitly learn to model different attributes, such\nas language or accent, leading to better results. S4, which\nactivates only half the experts compared to S3, performs bet-\nter, showing that selective activation of experts is more rea-\nsonable. Our proposed S6 shows a significant performance\nimprovement over S3 and S4, proving the effectiveness of\nlanguage expert groups. The language router's explicit lan-\nguage differentiation ensures that our language groups fo-\ncus on specific language representations without interference.\nIn S7, without the guidance of an unsupervised router, the\nexpert weights within the language group are assigned the\nsame value, contrasting with the situation in experiment S6.\nIn S6, the expert weights are dynamically learned through\nan unsupervised router, highlighting the significant impact of\nthe learned weights in the UnSup-Router. DLG-MoE's lan-\nguage router focuses on explicit language modeling, allow-\ning the unsupervised routers within the language groups to\nmodel attributes beyond language specifically, such as accent\nor different domains, resulting in better performance. Our\nmodel achieves SOTA performance on the CS test set com-\npared to previous MoE works. Specifically, it outperforms\nBiencoder-MoE(9.76%) [7], LSR-MoE(10.30%) [12], and\nFLR-MoE(9.7%) [13], all using the same training data, while\nalso offering higher flexibility and customizability."}, {"title": "3.3. Analysis of expert capacity and top-k", "content": "In this section, we analyze the impact of different top-k train-\ning strategies and increasing the number of experts on the\nperformance of DLG-MoE. Contrary to the conclusions from\nLSR-MoE [12], we find that increasing the number of experts\ncan yield better results. This discrepancy is likely because\nLSR-MoE did not incorporate language routing, making im-\nplicit language modeling more challenging with additional\nexperts. As shown in S9-S11, using top-2 training and in-\nference generally yields better results, and performance drops\nsignificantly when the top-k values for training and inference\nare inconsistent. Dynamic training allows our model to sup-\nport different top-k inference while maintaining good perfor-\nmance. Additionally, we found that with 8 experts, S11 out-\nperforms the fixed top-k trained models S9 and S10, while S5\nperforms worse than S4. This is primarily because dynamic\ntop-k is applied within the language groups, where language\nattributes are already determined by language router. In con-\ntrast, dynamic top-k in S5 exacerbates the difficulty of im-\nplicit language modeling. Our result S11 maintains the same\nnumber of activated parameters and similar FLOPs as S1 in\ntop-1 decoding, meaning that with a total of 8 experts, only"}, {"title": "3.4. Streaming Capability", "content": "In this section, we tested the model's streaming recognition\nability with different chunk sizes. As illustrated in Table 3,\nour model supports inference with varying chunk sizes while\nmaintaining reasonable performance in streaming scenarios.\nThe method also allows specifying a language, sending all\nframes to the designated language expert group to avoid in-\ncorrect routing during streaming. Although we did not ob-\nserve significant performance improvements, this might be"}, {"title": "4. CONCLUSIONS", "content": "This paper presents an efficient and flexible MoE structure.\nThrough the proposed language-group-based routing and dy-\nnamic top-k mechanism, the model achieves SOTA perfor-\nmance, significantly improving over baselines with compa-\nrable computational costs. The study also investigates the\nimpact of expert capacity and different top-k strategies on\nmodel performance, demonstrating that language groups and\nthe proposed dynamic top-k strategy are beneficial for flexi-\nbly expanding expert capacity. In the future, we will explore\nthe potential of the proposed model across more languages."}]}