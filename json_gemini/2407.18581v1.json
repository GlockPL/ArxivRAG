{"title": "DYNAMIC LANGUAGE GROUP-BASED MOE: ENHANCING EFFICIENCY AND\nFLEXIBILITY FOR CODE-SWITCHING SPEECH RECOGNITION", "authors": ["Hukai Huang", "Shenghui Lu", "Yahui Shan", "He Qu", "Wenhao Guan", "Qingyang Hong", "Lin Li"], "abstract": "The Mixture of Experts (MoE) approach is ideally suited\nfor tackling multilingual and code-switching (CS) challenges\ndue to its multi-expert architecture. This work introduces the\nDLG-MoE, which is optimized for bilingual and CS scenar-\nios. Our novel Dynamic Language Group-based MoE layer\nfeatures a language router with shared weights for explicit\nlanguage modeling, while independent unsupervised routers\nwithin the language group handle attributes beyond language.\nThis structure not only enhances expert extension capabili-\nties but also supports dynamic top-k training, allowing for\nflexible inference across various top-k values and improv-\ning overall performance. The model requires no pre-training\nand supports streaming recognition, achieving state-of-the-art\n(SOTA) results with unmatched flexibility compared to other\nmethods. The Code will be released.", "sections": [{"title": "1. INTRODUCTION", "content": "Thanks to its ability to scale parameters with minimal impact\non inference cost, the Mixture of Experts (MoE) approach has\nachieved exceptional results in natural language processing\nand computer vision [1, 2, 3]. The inherent multi-expert struc-\nture of MoE provides a natural advantage for multilingual\ntasks. Numerous studies [4, 5, 6] have demonstrated its effec-\ntiveness in multilingual automatic speech recognition (ASR),\nhighlighting MoE's strong performance in this domain. How-\never, these architectures still struggle to effectively handle CS\nscenarios.\nCS scenarios often involve using different languages\nwithin utterances, leading to significant challenges in model\ndevelopment due to language confusion. In response, Bi-\nencoder architectures [7, 8, 9, 10] use separate encoders for\neach language and employ language-specific masking strate-\ngies [8, 9, 11] to enhance individual language processing.\nHowever, this approach fails to leverage shared representa-\ntions across languages, resulting in the loss of cross-lingual\ncontextual information and increased computational com-\nplexity. LSR-MoE [12] and LR-MoE [13] mitigate these\nissues by incorporating two experts within the MoE layer,\neach dedicated to handling Chinese or English separately,\nand activate only one expert at a time during training and\ninference for computational efficiency. Similarly, Google [4]\nuses multiple experts in the MoE layer, activating two experts\nto manage multilingual ASR. U2++ MoE [14] highlights\nMoE's effectiveness on large-scale Chinese datasets, provid-\ning both streaming and non-streaming capabilities within a\nsingle MoE and outperforming other models in this aspect.\nIn this paper, we present DLG-MoE, an efficient and\nflexible MoE structure. Addressing limitations of previ-\nous approaches, we designed language expert groups and a\nlanguage router with weight sharing at the MoE layer to sup-\nport flexible expansion. The language router explicitly learns\nlanguage identification (LID), focusing on distributing frame-\nlevel representations to matching language groups based on\nLID. Within each language group, an unsupervised router\nspecializes in modeling attributes beyond language, such as\naccents and different domains, distributing inputs to appro-\npriate experts within the group. We also introduce a dynamic\ntop-k routing mechanism during training, allowing for the\nactivation of a variable number of experts during inference.\nOur contributions are summarized as follows:\n\u2022 Our DLG-MoE achieves SOTA performance compared\nto existing MoE works [7, 12, 13] and also offers\nstreaming recognition capabilities and greater scala-\nbility, supporting language-specific and different top-k\ninference.\n\u2022 This paper further investigates the impact of different\nrouting strategies and increasing the number of experts\non model performance and also validates the approach\non larger datasets and with larger parameters."}, {"title": "2. PROPOSED METHOD", "content": "The overall architecture of our model is shown in Fig.1(a),\ncomprising Conformer [15] Blocks and DLG-MoE Blocks.\nThe Conformer Block is stacked from vanilla Conformer lay-\ners, while the DLG-MoE Block is formed by stacking DLG-\nMoE layers. Fig.1(b) shows the structure of the DLG-MoE\nlayer, where the second FFN of the Conformer layer is re-\nplaced by a dynamic language group. The weight-shared lan-\nguage router (SLR) performs the LID task based on the in-\ntermediate representations hinter extracted by the Conformer\nblock and uses the LID results to guide the selection of the\nappropriate language group."}, {"title": "2.2. Language Router", "content": "As illustrated in Fig.1(c), we expand MoE by introducing lan-\nguage expert groups, specifically the Chinese expert group\n(Zh-Group) and the English expert group (En-Group). Un-\nlike other methods [14, 12, 4], our language router explicitly\nlearns frame-level LID and is trained using an inter-loss func-\ntion, as expressed by the following formula Equation1.\n$$L_{inter} = (log P_{CTC} (V_{LID} | h_{inter} W_{a_{LID}}^{d} ) + log P_{CTC} (V_{ASR} | h_{inter} W_{a_{ASR}}^{d}))$$"}, {"content": "We enhance the SLR's language routing capability through\na multi-task training approach that integrates ASR and LID\ntasks. Here, $V_{ASR}$ denotes the ground truth for the ASR task,\nwhile $V_{LID}$ is derived by mapping the tokens in $V_{ASR}$ to their\ncorresponding language IDs. The linear layers $W_{a_{LID}}^{d}$ and\n$W_{a_{ASR}}^{d}$ represent the transformations, where d is the embed-\nding dimension, V is the vocabulary size, and L+1 accounts\nfor the number of languages plus the CTC blank token.\nConsidering the peaky distribution of CTC which classi-\nfies most frames as blank, we use a simple yet empirically\neffective routing strategy. For the logits from the SLR, which\na shape of $R^{T\u00d7(L+1)}$, where T represents the number of\nframes, we first remove the blank class to get $R^{T\u00d7L}$, then\nselect the highest probability among the remaining categories\nas the language ID for every frame, resulting in a routing ta-\nble of shape $R^{T\u00d71}$. This greedy strategy, which does not rely\non other frames, is effective and beneficial to support stream-\ning ASR. Thus, for the input $h^{pre}$ to the dynamic language\ngroup, it is dispatched as $h_{zh}^{pre}$ and $h_{en}^{pre}$ based on the rout-\ning information-specifically, the LID results-provided by\nthe SLR, and then processed by the corresponding language\nexpert groups."}, {"title": "2.3. Dynamic Language Group", "content": "As shown in Fig. 1(c), once the representations for the same\nlanguage are obtained, the UnSup-Router within the language\nexpert group focuses on modeling attributes beyond language.\nFor each frame in $h_{zh}^{pre}$, the UnSup-Router selects k experts\nfrom the total n experts within the group to collaborate, as\ndescribed below:\n$$\\hat{h}_{zh}^{out} = \\sum_{i=1}^{k} G_{i}^{zh} E_{i}$$\n$$G^{zh} = Softmax (TopK (h_{zh}^{pre} W_{R}^{n}, k))$$"}, {"title": "3. EXPERIMENTS AND ANALYSIS", "content": "Our experiments are conducted on the ASRU-2019 Mandarin-\nEnglish code-switching challenge dataset [16], along with a\nsubset of 460 hours from Librispeech [17], consistent with\n[7, 12, 13]. For testing, we use Librispeech-clean (EN),\nASRU-Man-test (ZH), and ASRU-CS-test (CS) datasets.\nAdditionally, we have validated our model on an internal\nmulti-accent, multi-domain dataset, which, combined with\nthe ASRU-2019 Challenge dataset, totals 6,000 hours of\ntraining data.\nWe conduct our experiments on the WeNet [18] toolkit.\nFor acoustic features, 80-dimensional log-mel filter banks\n(FBank) are extracted with a step size of 10ms and a window\nsize of 25ms. All our experimental configurations used a\n6-layer Transformer decoder. We set $d_{model}$=256, $n_{heads}$=4\nand $d_{ffn}$=2048. Causal convolution was used during training\nto support streaming. We built Baseline-12 and Baseline-\n18 with 12 and 18 Conformer layers, respectively. All MoE\nmodels had 12 layers, with the first 6 being vanilla Conformer\nlayers and the last 6 incorporating MoE layers. Dense-MoE-\n4e and Sparse-MoE-4e, which do not include language rout-\ning, each had 4 MoE experts. In contrast, DLG-MoE-4e,\nalso with 4 experts, included language routing, allocating two\nexperts per language group. To explore model capacity and\ndifferent top-k strategies, we created DLG-MoE-8e with 8\nexperts. For large-scale comparisons, we used $d_{model}$=512\nand $n_{heads}$=8 for Baseline-18 and DLG-MoE-8e, while keep-\ning other configurations the same. The hyperparameters $\u03bb_{ctc}$\nand $\u03bb_{inter}$ were set to 0.3 and 0.1, respectively, with the\nAdam optimizer using lr=0.001 and $warmup_{steps}$=25000.\nDetailed information can be found in the link provided 1"}, {"title": "3.2. Results of different MoE structures", "content": "The results in Table 1 show that MoE structures (S3, S4)\noutperform S2 with the same number of parameters, indi-\ncating that widening the model with MoE might more effec-\ntive than simply increasing he number of layers. This sug-\ngests that even when using only the UnSup-Router, experts\ncan still implicitly learn to model different attributes, such\nas language or accent, leading to better results. S4, which\nactivates only half the experts compared to S3, performs bet-\nter, showing that selective activation of experts is more rea-\nsonable. Our proposed S6 shows a significant performance\nimprovement over S3 and S4, proving the effectiveness of\nlanguage expert groups. The language router's explicit lan-\nguage differentiation ensures that our language groups fo-\ncus on specific language representations without interference.\nIn S7, without the guidance of an unsupervised router, the\nexpert weights within the language group are assigned the\nsame value, contrasting with the situation in experiment S6.\nIn S6, the expert weights are dynamically learned through\nan unsupervised router, highlighting the significant impact of\nthe learned weights in the UnSup-Router. DLG-MoE's lan-\nguage router focuses on explicit language modeling, allow-\ning the unsupervised routers within the language groups to\nmodel attributes beyond language specifically, such as accent\nor different domains, resulting in better performance. Our\nmodel achieves SOTA performance on the CS test set com-\npared to previous MoE works. Specifically, it outperforms\nBiencoder-MoE(9.76%) [7], LSR-MoE(10.30%) [12], and\nFLR-MoE(9.7%) [13], all using the same training data, while\nalso offering higher flexibility and customizability."}, {"title": "3.3. Analysis of expert capacity and top-k", "content": "In this section, we analyze the impact of different top-k train-\ning strategies and increasing the number of experts on the\nperformance of DLG-MoE. Contrary to the conclusions from\nLSR-MoE [12], we find that increasing the number of experts\ncan yield better results. This discrepancy is likely because\nLSR-MoE did not incorporate language routing, making im-\nplicit language modeling more challenging with additional\nexperts. As shown in S9-S11, using top-2 training and in-\nference generally yields better results, and performance drops\nsignificantly when the top-k values for training and inference\nare inconsistent. Dynamic training allows our model to sup-\nport different top-k inference while maintaining good perfor-\nmance. Additionally, we found that with 8 experts, S11 out-\nperforms the fixed top-k trained models S9 and S10, while S5\nperforms worse than S4. This is primarily because dynamic\ntop-k is applied within the language groups, where language\nattributes are already determined by language router. In con-\ntrast, dynamic top-k in S5 exacerbates the difficulty of im-\nplicit language modeling. Our result S11 maintains the same\nnumber of activated parameters and similar FLOPs as S1 in\ntop-1 decoding, meaning that with a total of 8 experts, only\none is activated. This achieves up to 22.2%, 26.9%, and 8.9%\nrelative improvement in EN, ZH, and CS, respectively. Our\nmethod continues to show significant advantages on larger\ndatasets and with more parameters, as shown in Table 2."}, {"title": "3.4. Streaming Capability", "content": "In this section, we tested the model's streaming recognition\nability with different chunk sizes. As illustrated in Table 3,\nour model supports inference with varying chunk sizes while\nmaintaining reasonable performance in streaming scenarios.\nThe method also allows specifying a language, sending all\nframes to the designated language expert group to avoid in-\ncorrect routing during streaming. Although we did not ob-\nserve significant performance improvements, this might be\nmore noticeable on out-of-domain test sets. Fig.2 shows the\nvisualization of the routing results of the language router in\nS11. We found that the language router has high routing ac-\ncuracy, especially for monolingual utterances. It also tends\nto dispatch the first few frames of English to the Zh-Group,\nwhich may correspond to frames with no speech. For CS ut-\nterances, a high routing accuracy was also observed."}, {"title": "4. CONCLUSIONS", "content": "This paper presents an efficient and flexible MoE structure.\nThrough the proposed language-group-based routing and dy-\nnamic top-k mechanism, the model achieves SOTA perfor-\nmance, significantly improving over baselines with compa-\nrable computational costs. The study also investigates the\nimpact of expert capacity and different top-k strategies on\nmodel performance, demonstrating that language groups and\nthe proposed dynamic top-k strategy are beneficial for flexi-\nbly expanding expert capacity. In the future, we will explore\nthe potential of the proposed model across more languages."}]}