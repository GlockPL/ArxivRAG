{"title": "LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in Vision-Language Models", "authors": ["Jingyi Wang", "Jianzhong Ju", "Jian Luan", "Zhidong Deng"], "abstract": "Recent advances in large vision-language models (VLMs) typically employ vision encoders based on the Vision Transformer (ViT) architecture. The division of the images into patches by ViT results in a fragmented perception, thereby hindering the visual understanding capabilities of VLMs. In this paper, we propose an innovative enhancement to address this limitation by introducing a Scene Graph Expression (SGE) module in VLMs. This module extracts and structurally expresses the complex semantic information within images, thereby improving the foundational perception and understanding abilities of VLMs. Extensive experiments demonstrate that integrating our SGE module significantly enhances the VLM's performance in vision-language tasks, indicating its effectiveness in preserving intricate semantic details and facilitating better visual understanding. Code and data would be available.", "sections": [{"title": "I. INTRODUCTION", "content": "Large vision-language models (VLMs) integrate data from visual and language modalities, enabling comprehensive multimodal understanding [1]-[3]. With images and query text as input, VLMs can answer queries by incorporating the visual information. However, most VLMs utilize the Vision Transformer (ViT) [4] as their visual backbone, which results in perceiving images as fragmented patches, as shown in Fig. 1(a) with LLaVA as the VLM baseline [5]. This fragmented approach fails to preserve the intrinsic semantic information in images, thus limiting the VLMs' visual comprehension capabilities [5]. To address this limitation, we propose using scene graphs, which express the objects in a scene and the relationships between them to retain and structurally express the complex semantic information in images, as shown in Fig. 1(b). In this way, the visual perception and understanding capabilities of VLMs are enhanced with the proposed Scene Graph Expression (SGE) module.\nImproving the perceptual capabilities of VLMs has been the focus of several research efforts. For instance, Lyrics [6] introduces a visual refiner to incorporate semantic-aware visual objects. IVE [7] integrates multitask encoders and visual tools into existing models. Several studies have also explored the use of scene graphs in VLMs [1], [8]. However, training directly on scene graph data or fine-tuning on scene graph text has not yielded optimal results [1], [8].\nTo achieve the structured expression of visual semantic information without directly training on scene graph data, we designed the Scene Graph Expression module. Specifically, we use pretrained visual models to extract entities from images, preserving semantic information at the entity level rather than the patch level. Next, we construct a scene graph using these entities and perform message passing between the nodes in the scene graph. Building on LLaVA [5], we construct LLaVA-SceneGraph (LLaVA-SG), a model that incorporate SGE module to enhance the foundational perception and understanding capabilities of VLMs. To enhance visual understanding, we train LLaVA-SG to classify visual relationships between the extracted entities, avoiding the catastrophic forgetting that might occur with direct training on scene graph data. Our contributions are threefold:\n\u2022 We design a Scene Graph Expression (SGE) module to extract and structurally express the intrinsic semantic information in images.\n\u2022 We incorporate SGE into VLM, resulting in the LLaVA-SG model to enhance the foundational visual perception and understanding capabilities.\n\u2022 We conduct extensive evaluations of the trained LLaVA-SG model on multiple benchmarks, demonstrating that the integration of SGE significantly improves visual perception and understanding capabilities of VLM."}, {"title": "II. METHOD", "content": "In this section, we first introduce the scene graph construction to preserve and express complex semantic information within images. Next, we detail the integration of SGE module into standard VLM models, forming the LLaVA-SG model. Finally, we present the training strategies devised for LLaVA-SG. An overview of our approach is shown in Fig. 2(b).\n### A. Semantic Information Expression\n1) Visual Entity Extraction: To achieve the structured expression of semantic information in a scene, we first extract the entities within the scene. Specifically, in the Visual Perception Module in Fig. 2(b), we first perform image tagging on the input to identify the categories of the entities it contains. Taking the input image in Fig. 2(b) as an example, the image tagging process will output tags such as \"man\", \"racket\", \"tennis ball\", and \"sports field\". The object detection module then detects the bounding boxes of these entities based on the tagging results. The bounding boxes are rectangular boxes that represents the position and size of each entity. Next, we conduct semantic segmentation within the bounding boxes to obtain the pixels comprising each entity, achieving pixel-level semantic understanding with the help of segmentation masks. We denote the segmentation masks as $M \\in \\mathbb{R}^{N \\times h_m \\times W_m}$, where N is the number of segmented entities. $h_m$ and $W_m$ denote the height and width of masks.\nTo express the semantic information in images, we design the Scene Graph Expression module, as depicted in Fig. 2(a). The segmentation masks M serve as the input of the SGE module representing entities in the input image.\n2) Scene Graph Expression: We introduce the Scene Graph Expression (SGE) module to structurally preserve and express the semantic information in a scene at the entity level. A scene graph is a graphical representation that captures and expresses the entities and their relationships in a scene. Scene graphs aid in visual comprehension of images and videos, and facilitate scene understanding [19]. Entities in a scene are represented as entities in a scene graph. Additionally, we introduce the prompt feature to activate key nodes in the scene graph. Next, we detail the steps involved in the SGE module.\nTo construct the scene graph for the input image, we first extract the visual features of the entities within the image to represent the nodes. Specifically, we use $F_v \\in \\mathbb{R}^{d_e \\times h_V \\times W_v}$ to denote the visual features of the input image obtained from the pretrained vision encoder, where $d_e$ is the dimension of $F_v$. $h_V$ and $W_v$ denote the shape of the feature map. Following [20], we extract the features of points in the masks M from $F_v$ using bilinear interpolation. Then, for each mask in M, we average the features of the points it contains to obtain the feature representation of the mask. The features of masks are collectively denoted as $F_e \\in \\mathbb{R}^{N \\times d_e}$, representing the visual features of the corresponding entities. Second, we construct the scene graph G with these N entities as nodes of the graph and $F_e$ as the node features. Edges among the nodes depict their relationships. We perform message passing among the N nodes to implicitly encode the relationship information between nodes. Third, to make the scene graph expression adaptive, we utilize the prompt feature to activate the key nodes within G. We use $F_t \\in \\mathbb{R}^{N_t \\times d_t}$ to denote the prompt feature where $N_t$ is the number of prompt tokens, and $d_t$ is the dimension of the prompt feature. Specifically, we adopt the attention mechanism to inject the prompt feature $F_t$ into G, highlighting the nodes in G that are relevant to the prompt feature. The updated scene graph is denoted as $G'$, which is the output of the SGE module, representing the structured semantic information contained in the input image.\n### B. Scene Graph Expression in VLM\nTo address the degradation of complex semantic information in VLMs caused by the processing of images as set of patches, we introduce the SGE module into the VLM, as shown in"}, {"title": "C. Training", "content": "Following LLaVA [5], we train our LLaVA-SG on the prediction tokens of the LLM using an auto-regressive training objective. Building on LLaVA's original two-stage training, we insert an additional training stage specifically for our SGE module. The training of LLaVA-SG thus includes the following three stages:\nVisual Feature Alignment. We initialize the image encoder and the LLM of LLaVA-SG with pretrained weights and keep them frozen. We do not use the SGE module at this stage. Only the visual projection layer $W_v$ is trainable. The dataset of 558K LAION-CCSBU image-text pairs [5], [21] is adopted for training in this stage.\nSGE training. In the second stage, we focus training the SGE module and the corresponding projection layer $W_g$. The visual encoder, visual projection layer, and LLM are all frozen during this stage. Considering that a scene graph contains both entities and the relationships between them, we endow the SGE module with the ability to preserve and express visual semantic information from both perspectives. For entity recognition, we use the pretrained visual perception module. For expressing the relationships between entities, we train the SGE module and the projection layer $W_g$ on visual relationship understanding datasets. Specifically, the visual relationship understanding datasets we use are derived from two sources. First, existing fine-grained visual understanding datasets, such as Visual Genome [22] and Open Image V6 [23], are reformatted into the visual question-answering format. However, the relationships in these datasets are limited and not open-vocabulary. Therefore, we also construct an open-vocabulary visual relationship understanding dataset based on the large-scale visual grounding dataset GRIT [24], utilizing GPT-4v to produce the data.\nFine-tuning. In the third stage, only the visual encoder remains frozen, while all other parameters are trained in LLaVA-SG. We use the 665K image-text instruction dataset from LLaVA-1.5 [18], which contains diverse instructions for fine-tuning in this stage."}, {"title": "III. EXPERIMENTS", "content": "### A. Experimental Setup\nIn LLaVA-SG, we adopt CLIP ViT-L/14@336p [4], [25] as the vision encoder and Vicuna [26] as the LLM. The pretrained visual perception module includes RAM [27] for image tagging, Grounding-DINO [28] for detection, and SAM [29] for semantic segmentation. The message passing and attention mechanism in the SGE module are implemented using lightweight transformers. In the first stage, the learning rate for the trainable parameters is set to 2e-3. In the second and third stages, we use a learning rate of 2e \u2013 5.\n### B. Overall Performance Assessments\nThe comparison results of LLaVA-SG and baseline models are summarized in Table I following the evaluation metrics of [30]. The reported results of the compared models are collected from the corresponding papers. Analyzing the experimental results in Table I, it is evident that our LLaVA-SG model shows significant and consistent improvements over"}, {"title": "C. Ablation Study", "content": "We perform ablation experiments to explore the effect of the SGE module and the effect of training strategies. These ablation experiments are based on LLaVA-SG-7B.\nThe Effect of the SGE Module. We conduct ablation studies on the components within the Scene Graph Expression module. The results are shown in Table II. \"SG\" denotes the construction and use of a basic scene graph without message passing and prompt feature adaptation. \"MP\" denotes the message passing among nodes in G. \"Prompt\" denotes the attention mechanism with the prompt feature. \"Rel\" indicates that the test sets from Visual Genome and Open Images are used for evaluating visual relationship classification. As shown in Table II, each component contributes to the improvement of the LLaVA baseline.\nThe Effect of Training Strategies. We performed ablation experiments on the additional visual relationship understanding data used for training the SGE module and the separate SGE training stage. The results are shown in Table III. The first row shows the results without using the SGE module, where the visual relationship understanding data used for the second training stage, i.e., the SGE-D in Table III is simply appended to the fine-tuning data for training the LLaVA model. The second row shows the results without a separate SGE training stage, where SGE-D is incorporated into the fine-tuning stage and SGE is trained together with the LLM. The third row shows the performance of LLaVA-SG under the full training strategy. The comparison between the first and third rows of Table III indicates that the improvement of LLaVA-SG over LLaVA is not primarily due to the additional visual relationship understanding data. Instead, the SGE module plays a crucial role. The comparison between the second and third rows highlights the necessity of the separate SGE training stage. The separate SGE training stage allows the SGE module to focus more specifically on expressing the visual semantic information within the input images."}, {"title": "IV. CONCLUSION", "content": "In this paper, we propose a Scene Graph Expression (SGE) module to extract and express visual semantic information structurally for VLM. With the SGE module, the perception and understanding abilities of VLMs are enhanced. The LLaVA-SG model, constructed based on the SGE module, shows significant and consistent performance improvements over the baseline methods."}, {"title": "Ha = LLM([Wv \u22c5 Fv, <sg>, Wg \u22c5 G', <text>, Ft]),", "content": null}]}