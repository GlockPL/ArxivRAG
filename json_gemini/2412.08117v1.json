{"title": "LatentSpeech: Latent Diffusion for Text-To-Speech Generation", "authors": ["Haowei Lou", "Helen Paik", "Pari Delir Haghighi", "Wen Hu", "Lina Yao"], "abstract": "Diffusion-based Generative AI gains significant attention for its superior performance over other generative techniques like Generative Adversarial Networks and Variational Autoencoders. While it has achieved notable advancements in fields such as computer vision and natural language processing, their application in speech generation remains under-explored. Mainstream Text-to-Speech systems primarily map outputs to Mel-Spectrograms in the spectral space, leading to high computational loads due to the sparsity of MelSpecs. To address these limitations, we propose LatentSpeech, a novel TTS generation approach utilizing latent diffusion models. By using latent embeddings as the intermediate representation, LatentSpeech reduces the target dimension to 5% of what is required for MelSpecs, simplifying the processing for the TTS encoder and vocoder and enabling efficient high-quality speech generation. This study marks the first integration of latent diffusion models in TTS, enhancing the accuracy and naturalness of generated speech. Experimental results on benchmark datasets demonstrate that LatentSpeech achieves a 25% improvement in Word Error Rate and a 24% improvement in Mel Cepstral Distortion compared to existing models, with further improvements rising to 49.5% and 26%, respectively, with additional training data. These findings highlight the potential of LatentSpeech to advance the state-of-the-art in TTS technology", "sections": [{"title": "I. INTRODUCTION", "content": "Generative AI has made significant strides in recent years. It revolutionises various fields with its ability to generate high-quality data. Among numerous GAI techniques, diffusion-based generative models have garnered increased attention for their superior performance compared to other methods such as Generative Adversarial Networks [1] and Variational Autoencoders [2]. Diffusion models demonstrate remarkable advancements in areas like image generation [3], large language models [4], and video generation [5].\nMainstream Text-to-Speech (TTS) systems, which convert linguistic context to speech using deep learning approaches, have explored the application of advanced deep learning techniques in speech generation. For instance, Tacotron [6] employs a sequence-to-sequence framework for speech generation, FastSpeech [7] uses a transformer architecture to enable parallel computation and address issues like word skipping, and StyleSpeech [8] enhances phoneme and style embedding efficiency to improve speech quality."}, {"title": "II. LATENTSPEECH", "content": "In this section, we introduce the architecture of LatentSpeech. We first encode speech A into latent space using an Autoencoder (AE). Then, we set latent embeddings as the intermediate representation Z and train a diffusion-based TTS model to map embeddings. In the end, we generate speech directly from the latent space to the audio space using the trained decoder. An overview of the entire system is provided in Figure 1."}, {"title": "A. Latent Encoder", "content": "To lower the computation demand of training TTS system and sparsity of intermediate representation. We follow a similar training setup to RAVE [12] to train an Autoencoder to encode speech from audio space to latent space. Specifically, given a raw waveform $A \\in \\mathbb{R}^{L_{audio}}$ where $L_{audio}$ is the number of time points in the speech. We first apply a multi-band decomposition to the raw speech using Pseudo Quadrature Mirror Filters (PQMF) [13].\n$$PQMF(A) = \\mathbb{R}^{N \\times L_{sub}}, L_{audio} = N \\times L_{sub}$$\nN is the number of frequency sub-bands and $L_{sub}$ is the number of time points in each sub-band. An encoder is applied E(.) to encode PQMF(A) into latent space $Z \\in \\mathbb{R}^{N \\times L_{latent}}$. Here, N denotes the number of channels $L_{latent}$ represents the latent space temporal resolution. The latent embeddings are passed into a decoder D(.) to reconstruct PQMF(A), yielding D(Z). The resultant multi-band speech is then processed using the inverse PQMF function to produce the reconstructed speech, $A' = PQMF^{-1}(D(Z))$. We use the multiscale spectral distance in the multi-band speech as the loss function [14] to train the encoder and decoder. N and L will be used in the following sections to denote the number of channels and time resolution in the latent space."}, {"title": "B. Text-to-Speech Encoder", "content": "TTS encoder transforms linguistic inputs to TTS embedding, which serves as conditions for the diffusion model to map latent embedding. In this work, we adopt the transformer-based TTS system StyleSpeech [8] as our TTS encoder. It includes the following key components: an acoustic pattern encoder, a duration adapter, and an integration encoder, each consisting of multiple layers of Feed-Forward Transformers (FFT Blocks) [7].\nGiven sequences of phonemes P and styles S linguistic input, the Acoustic Pattern Encoder (APE) transforms input text into sequences of phoneme $H_P = (h_{p_1},...,h_{p_n})$ and style embeddings $H_S = (h_{s_1},...,h_{s_n})$. The phoneme and style embeddings are fused to produce acoustic embedding $H = H_P + H_S$. The Duration Adapter controls the duration of acoustic embeddings to align acoustic embedding with real speech. It has two main components: the duration predictor and the length regulator. The duration predictor estimates the duration of each acoustic feature $L = \\{l_1,..., l_n\\}$, where $m = \\Sigma_{i=1}^{n}l_i$. These durations adjust the length of each acoustic embedding to the adaptive embedding $H_L = \\{h_{l_1},..., h_{l_m}\\}$. The adaptive embeddings $H_L$ are then passed through the embedding generator to generate the TTS embedding $H_{TTS}$. This is followed by a linear layer to broadcast $H_{TTS}$ to the dimensions of the latent embedding Z."}, {"title": "C. Latent Diffusion", "content": "Diffusion model is a probabilistic generative model that learns to produce data that match latent embedding distribution p(Z), by denoising a normally distributed variable through a reverse Markov Chain of length T. We define q(Zo) as the data distribution of the latent embedding $Z \\in \\mathbb{R}^{N \\times L}$. Let $Z_{\u0141} \\in \\mathbb{R}^{N \\times L}$ for t = 0, 1, . . ., T represent the forward diffusion process:\n$$q(Z_{1:T}|Z_0) = \\prod_{t=1}^{T}q(Z_t|Z_{t-1})$$\nwhere Gaussian noise N(\u00b7) is gradually added to the Markov chain from $Z_0$ to $Z_T$ until $q(Z_T) \\sim N(0, I)$.\n$$q(Z_t|Z_{t-1}) = N(Z_t; \\sqrt{\\alpha_t}Z_{t-1}, (1 - \\alpha_t)I)$$\nHere, $\\alpha_t$ refers to the scaling factor that controls the amount of noise added at diffusion step t. Then, we apply a conditional denoiser $P_\u03b8$(.) parameterized by \u03b8 to reverse the diffusion"}, {"title": "III. EXPERIMENTS AND RESULT ANALYSIS", "content": "Dataset: In this study, we evaluate our method using a Chinese speech dataset, which presents unique challenges due to its complex pronunciation and tonal variations compared to other languages, such as English. We use the Baker dataset [18], which contains approximately 12 hours of speech recorded using professional instruments at a frequency of 48kHz. The dataset consists of 10k speech samples from a female Mandarin speaker.\nExperimental setups: The experiment is conducted using an NVIDIA RTX A5000 with a PyTorch implementation. All experimental settings closely follow those proposed in StyleSpeech [8]. Specifically, we use 4k sentences for training and 1k sentences for testing. The batch size is set to 64, and the model is trained for 300 epochs. The number of diffusion steps, T, is set to 50. To further validate our method, we also train our model on a larger dataset consisting of 9k training sentences and 1k testing sentences. An ablation study on the effect of the duration target I was conducted to evaluate the impact of the duration adaptor on the output speech. In this study, phoneme samples adapted with the ground truth duration target are labelled as (w/l), while those adapted using the adaptor-predicted duration are labelled as (w/ol). Our source code will be released upon acceptance.\nMetrics: We employ Word Error Rate (WER), Mel Cepstral Distortion (MCD) [19], and Perceptual Evaluation of Speech Quality (PESQ) [20], to evaluate model's performance. For"}, {"title": "IV. CONCLUSION", "content": "In conclusion, we propose LatentSpeech, a new TTS framework that uses latent embeddings that reduce intermediate representation dimension to 5% of mainstream approaches. By incorporating a latent diffusion model, LatentSpeech refines speech in latent space for more accurate and natural output. Extensive experiments demonstrate that LatentSpeech achieves a 25% improvement in WER and a 24% improvement in MCD compared to existing models, with further improvements to 49.5% and 26% when trained with more data."}]}