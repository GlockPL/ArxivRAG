{"title": "Temporal Causal Discovery in Dynamic Bayesian Networks Using Federated Learning", "authors": ["Jianhong Chen", "Ying Ma", "Xubo Yue"], "abstract": "Traditionally, learning the structure of a Dynamic Bayesian Network has been centralized, with all data pooled in one location. However, in real-world scenarios, data are often dispersed among multiple parties (e.g., companies, devices) that aim to collaboratively learn a Dynamic Bayesian Network while preserving their data privacy and security. In this study, we introduce a federated learning approach for estimating the structure of a Dynamic Bayesian Network from data distributed horizontally across different parties. We propose a distributed structure learning method that leverages continuous optimization so that only model parameters are exchanged during optimization. Experimental results on synthetic and real datasets reveal that our method outperforms other state-of-the-art techniques, particularly when there are many clients with limited individual sample sizes.", "sections": [{"title": "Introduction", "content": "The learning of Dynamic Bayesian Network (DBN) structures is a crucial technique in causal learning and representation learning. More importantly, it has been applied in various real-world applications, such as modeling gene expression [20], measuring network security [7], studying manufacturing process parameters [46], and recognizing interaction activities [53]. Traditionally, the learning of DBNs has been conducted at a centralized location where all data is aggregated. However, with the rapid advancements in technology and the Internet of Things (IoT) [26] devices, data collection has become significantly more accessible. Consequently, in real-world scenarios, data is typically owned and dispersed among multiple entities, including mobile devices, individuals, companies, and healthcare institutions.\nIn many cases, these entities, referred to as clients, may lack an adequate number of samples to independently construct a meaningful DBN. They might seek to collaboratively gain comprehensive insights into the DBN structure without disclosing their individual data due to privacy and security concerns. For instance, consider several hospitals aiming to work together to develop a DBN that uncovers the conditional independence relationships among their variables of interest, such as"}, {"title": "Problem Statement", "content": "We consider a scenario with a total of K clients, indexed by $k \\in \\{1,2,...,K\\}$. Each client k possesses its own local dataset consisting of M realizations of a stationary time series. Specifically, for the k-th client, we have time series $\\{x_{m,t}\\}_{t=0}^{T}$ for $x_{m,t} \\in \\mathbb{R}^d$, where $m\\in \\{1,2,..., M\\}$ indexes each realization, and d is the number of variables. Due to privacy regulations, these time series cannot be shared directly among clients. For clarity and conciseness, the main text omits the subscript m corresponding to multiple time series, with the complete m notation detailed in the supplementary material \u00a77.1, and consider a generic realization $\\{x_{t}\\}_{t=0}^{T}$. In this setting, we assume that the data from all clients follow the same underlying distribution. The extension to non-homogeneous distributions is left for future work (see Sec. 6).\nFor each client k, its dataset (for a single realization) can be written as $D^k = \\{x_t\\}_{t=p}^T$, where $t\\in \\{p, p + 1, ...,T\\}$ and p denotes the autoregressive order. We model our data using Structural Equation Modeling (SEM) and consider a Vector Autoregressive structure (SVAR) of order p:\n$(x_t)^k = (x_t)^kW+ (x_{t-1})^kA_1+\u2026\u2026+(x_{t-p})^kA_p + (u)^k,$\nwhere:\n1. $u \\sim N(0, I)$ is a vector of centered noise terms, independent across time and variables;\n2. W is a weighted adjacency matrix capturing intra-slice dependencies (edges at the same time step);\n3. $A_i$, for $i \\in \\{1,...,p\\}$, are the coefficient matrices capturing inter-slice dependencies (edges across different time steps).\nFor each client k, the effective sample size is $n_k = T+1-p$. Given the collection of datasets $X = \\cup_{k=1}^{K} D^k$ from all clients, the objective is to recover the true W and A in a privacy-preserving"}, {"title": "Federated DBN Inference with ADMM", "content": "To implement Federated learning to DBN, our method builds on the DYNOTEARS approach introduced by Pamfil et al. [35], which frames the structural learning of linear Dynamical Bayesian Networks as a continuous constrained optimization problem. By incorporating $l_1$-norm penalties to encourage sparsity, the optimization problem is formulated as follows:\n$\\min_{W,A} l(W, A) \t ext{ subject to } W \\text{ is acyclic},$ \n$l(W, A) = \\frac{1}{2n}  ||X_t - X_tW - X_{(t-p):(t-1)}A||_F^2 + \\lambda_w ||W||_1 + \\lambda_a||A||_1.$\nFor the acyclicity constraint, following Zheng et al. [54],\n$h(W) = tr (e^{W \\circ W}) - d$\nis equal to zero if and only if W is acyclic. Here, $\\circ$ represents the Hadamard product (element-wise multiplication) of two matrices. By replacing the acyclicity constraint with the equality constraint h(W) = 0, the problem can be reformulated as an equality-constrained optimization task.\nThe ADMM [4] is an optimization algorithm designed to solve convex problems by breaking them into smaller, more manageable subproblems. It is particularly effective for large-scale optimization tasks with complex constraints. By following ADMM framework, we decompose the constrained problem (2) into multiple subproblems and utilize an iterative message-passing approach to converge to the final solution. ADMM proves particularly effective when subproblems have closed-form solutions, which we derive for the first subproblem. To cast problem (2) in an ADMM framework, we reformulate it using local variables $B_1,...,B_K \\in \\mathbb{R}^{d \\times d}$ for the intra-slice matrices and $D_1,...,D_K \\in \\mathbb{R}^{d \\times d}$ for the inter-slice matrices, along with shared global variables $W \\in \\mathbb{R}^{d \\times d}$ and $A \\in \\mathbb{R}^{d \\times d}$, representing an equivalent formulation:\n$\\min_{B_k, A}\t ext{ }\\sum_{k=1}^K l_k(B_k, D_k) + \\lambda_w||W||_1 + \\lambda_a||A||_1$\n$\\t ext{subject to } h(W) = 0,$\n$B_k = W, k = 1, 2, ..., K,$\n$D_k = A, k = 1, 2, . . ., K.$\nThe local variables $B_1, . . ., B_K$ and $D_1, ..., D_K$ represent the model parameters specific to each client. Notably, this problem resembles the global variable consensus ADMM framework. The constraints $B_k = W$ and $D_k = A$ are imposed to enforce consistency, ensuring that the local model parameters across clients remain identical.\nSince ADMM combines elements of dual decomposition and the augmented Lagrangian method, making it efficient for handling separable objective functions and facilitating parallel computation,"}, {"title": "Experiments", "content": "In this section, we first study the performance of 2Dbn on simulated data generated by a linear SVAR structure [14]. We then compare it against three linear baseline methods using three evaluation metrics, demonstrating the effectiveness of our proposed method. Figure 1 provides an illustrative example.\nBenchmark Methods. We compare our ADMM-based approach described in Sec. 3, referred to as 2Dbn, with three other methods. The first is a baseline, denoted as Ave, which computes the average of the weighted adjacency matrices estimated by DYNOTEARS [35] from each client's local dataset, followed by thresholding to determine the edges. The second baseline, referred to as Best, selects the best graph from among those estimated by each client based on the lowest Structural Hamming Distance (SHD) to the ground truth. While this approach is unrealistic in practical scenarios (since it assumes knowledge of the ground truth), it serves as a useful point of reference. For additional context, we also consider DYNOTEARS applied to the combined dataset from all clients, denoted as Alldata. Note that the final graphs produced by Ave may contain cycles, and we do not apply any post-processing steps to remove them, as doing so could reduce performance. Since there is no official source code for DYNOTEARS, we re-implement it using only the numpy and scipy packages in about a hundred lines of code. This simpler, self-contained implementation eases understanding and reusability compared to available versions on GitHub.\nEvaluation Metrics. We use three metrics to evaluate performance: Structural Hamming Dis-tance, True Positive Rate (TPR), and False Discovery Rate (FDR). SHD measures the dissimilarity between the inferred graph and the ground truth, accounting for missing edges, extra edges, and incorrectly oriented edges [48]. Lower SHD indicates closer alignment with the ground truth. TPR (also known as sensitivity or recall) quantifies the proportion of true edges correctly identified, calculated from true positives and false negatives [12]. Higher TPR means the model identifies more true edges. FDR measures the proportion of false positives among all predicted edges, computed as the ratio of false positives to the sum of false positives and true positives [3]. A lower FDR indicates that most detected edges are correct. Together, these metrics provide a comprehensive"}, {"title": "Varying Number of Variables", "content": "In this section, we focus on DBNs with n = 5d samples evenly distributed across K = 10 clients for d = 10, 20, and n = 6d for d\u2208 {5,15}. Note that setting n = 5d or 6d ensures that each client has an integer number of samples. We generate datasets for each of these cases. Typically, each client has very few samples, making this a challenging scenario."}, {"title": "Varying Number of Clients", "content": "We now examine scenarios where a fixed total number of samples is distributed among varying numbers of clients. For d\u2208 {10,20}, we generate n = 512 samples with p = 1. These are evenly allocated across K\u2208 {2,4,8, 16, 32, 64} clients. For \u03bb\u03c9 and \u03bb\u03b1, we follow the recommendations of [35] for DYNOTEARS, Ave, and Best. For 2Dbn, we select the best regularization parameters from [0.05, 0.5] in increments of 0.05 by minimizing SHD."}, {"title": "Applications", "content": "These findings highlight the importance of both method selection and hyperparameter tuning, as well as potential directions for future research in improving the efficiency of information exchange mechanisms for DBN learning."}, {"title": "DREAM4", "content": "Building on the DREAM4 Challenge [28, 44, 45], our paper focuses on the time-series track of the InSilico_Size100 subchallenge. In this problem, the DREAM4 gene expression data are used to infer gene regulatory networks. The InSilico_Size100 subchallenge dataset contains 5 independent datasets, each consisting of 10 time-series for 100 genes, measured over 21 time steps. We assume that each dataset corresponds to data collected from different hospitals or research centers, thereby motivating our federated approach. More information and data about DREAM4 are available at https://gnw.sourceforge.net/resources/DREAM4%20in%20silico%20challenge.pdf.\nLet $X_{t,r}^g$ denote the expression level of gene g at time $t \\in \\{0,1,2,...,20\\}$ in replicate r \u2208 \\{0,1,2,...,R\\}. Note that R depends on the dataset used for instance, if only one dataset is used, then R= 10. Consequently, $X_t \\in \\mathbb{R}^{R \\times 100}$ and $X_t \\in \\mathbb{R}^{R \\times 100}$. In this experiment, we set p = 1, aligning with the VAR method proposed in the DREAM4 Challenge by Lu et al. [25].\nThe federated setting of our 2Dbn approach includes K = 5 clients, each with R = 2 replicates. Thus, each client contains a time-series dataset for 100 genes over 42 time steps. We found that small regularization parameters, \u03bb\u03c9 = \u03bb\u03b1 = 0.0025, work well for all DREAM4 datasets. Lu et al. [25] evaluated various methods for learning these networks, including approaches based on Mutual Information(MI), Granger causality, dynamical systems, Decision Trees, Gaussian Processes (GPs), and Dynamic Bayesian Networks. Notably, we did not compare DYNOTEARS in this study, as its source code is not publicly available, and our implemented version did not achieve optimal results. The comparisons were made using AUPR (Area Under the Precision-Recall Curve) and AUROC (Area Under the Receiver Operating Characteristic Curve) across the 5 datasets. From Table. 1, the GP-based method outperforms all others, achieving the highest mean AUPR (0.208) and AUROC (0.728). In terms of AUPR, our 2Dbn approach outperforms the TSNI (ODE-based) method in its non-federated version. Furthermore, the AUPR of 2Dbn is comparable to those of Ebdnet (DBN-based), GCCA (VAR-based), and ARACNE (MI-based) approaches. Importantly, the mean AUROC of 2Dbn surpasses those of GCCA (VAR-based), ARACNE (MI-based), and TSNI (ODE-based) methods, and is close to those of Ebdnet and VBSSMa (DBN-based).\nOverall, 2Dbn is comparable to other non-federated benchmarks. However, the GP-based method still achieves superior results in both AUPR and AUROC. One possible reason for this superiority is that the GP-based approach may more effectively capture nonlinear relationships and complex temporal dynamics, which can be challenging in a federated setting where data distri-butions and conditions vary across sources. Unsurprisingly, the GP-based method outperforms all approaches because Gaussian Processes naturally handle nonlinear dynamics and are more adaptive to varying, complex relationships. In contrast, the 2Dbn model, although suitable for distributed data, might impose stronger parametric assumptions or face computational constraints that limit its ability to capture subtle nonlinear interactions. Thus, we believe GP-based methods could be ex-tended to a distributed approach for structure learning, making them more applicable to real-world problems. We discuss this further in Section 6."}, {"title": "Functional Magnetic Resonance Imaging (FMRI)", "content": "In this experiment, we apply the proposed learning methods to estimate connections in the human brain using simulated blood oxygenation level-dependent (BOLD) imaging data [42]. The dataset consists of 28 independent datasets with the number of observed variables d\u2208 {5,10,15}. Each dataset contains 50 subjects (i.e., 50 ground-truth networks) with 200 time steps. To conduct the experiments, we use simulated time series measurements corresponding to five different human subjects for each d and compute the Average AUROC using the sklearn package.\nFor the federated setting, we partition the 200 time steps among 5 clients (K = 5). Detailed in-formation and descriptions of the data are available at https://www.fmrib.ox.ac.uk/datasets/netsim/index.html. In our experiments, we evaluate the proposed method for d \u2208 {5, 10, 15}. For d = 5, we use the 3rd, 6th, 9th, 12th, and 15th subjects from Sim-1.mat. For d = 10, we use the 2nd, 4th, 6th, 8th, and 10th subjects from Sim-2.mat. Finally, for d = 15, we use the 1st, 3rd, 5th, 7th, and 9th subjects from Sim-3.mat. Further details of this experiment are provided in the supplementary materials (see \u00a77.5). We compare our method to the economy Statistical Recurrent Units (eSRU) proposed by Khanna and Tan [18] for inferring Granger causality, as well as existing methods based on a Multilayer Perceptron (MLP), a Long Short-Term Memory (LSTM) network [47], and a Convolutional Neural Network (CNN)-based model, the Temporal Causal Discovery Framework (TCDF) [33], on multivariate time series data for d = 15, as examined by Khanna and Tan [18]. As shown in Tab.2, our proposed 2DBNs achieve an AUROC of 0.74, outperforming the LSTM-based approach and approaching the performance of the CNN-based TCDF method. Even though 2DBNs do not surpass the MLP-based and eSRU-based methods, it is notable that the Fed-erated version of our approach outperforms or closely matches several established non-Federated benchmarks. This outcome is reasonable, as the MLP and eSRU methods rely on deep architectures adept at modeling complex structural dependencies. Notably, our 2DBN method provides a new per-spective on this problem by ensuring data security through its Federated approach. This capability is particularly important in scenarios involving sensitive or distributed datasets, as it allows for effective analysis without compromising privacy or data integrity. We have further elaborated on this aspect in Sec.6."}, {"title": "Discussion", "content": "We proposed a federated framework for learning DBNs on horizontally partitioned data. Specifi-cally, we designed a distributed DBN learning method using ADMM, where only model parameters are exchanged during optimization. Our experimental results demonstrate that this approach outperforms other state-of-the-art methods, particularly in scenarios with numerous clients, each possessing a small sample size a common situation in federated learning that motivates client collaboration. Below, we address some limitations of our approach and suggest potential directions for future research.\nAssumptions. We assumed that the structure of the DBN is fixed over time and is identical for all time series in the dataset (i.e., it is the same for all m \u2208 M). Relaxing these assumptions could be useful in various ways, such as allowing the structure to change smoothly over time [43]. Another direction for future work is to investigate the behavior of the algorithm on non-stationary or cointegrated time series [27] or in scenarios with confounding variables [16].\nFederated Learning. The proposed ADMM-based approach to federated learning relies on a stateful setup, requiring each participating client to be involved in every round of communication and optimization. This \u201calways-on\" requirement can be burdensome in real-world scenarios. For instance, in large-scale deployments, clients such as mobile devices or IoT sensors may experience intermittent connectivity, limited power, or varying levels of availability. Ensuring that all such de-vices participate consistently and synchronously in every round is often impractical and can result in significant performance bottlenecks. An important future direction is to explore asynchronous techniques, which would enable stateless clients and facilitate cross-device learning. Furthermore, data may be vertically partitioned across clients, meaning that each client owns different variables but collectively aims to perform Bayesian Network Structure Learning (BNSL). Developing a fed-erated approach tailored to this vertical setting represents another promising research direction. Additionally, the ADMM procedure involves sharing model parameters with a central server, rais-ing concerns about potential privacy risks. Research has shown that these parameters can leak sensitive information in certain scenarios, such as with image data [39]. To address this, exploring differential privacy techniques [9] to enhance the protection of shared model parameters is a critical avenue for future work. Moreover, our current work focuses on non-heterogeneous data. However, heterogeneous settings are more applicable to many real-world problems, as clients often differ in computational capabilities, communication bandwidth, and local data distributions. These vari-ations pose significant challenges for model convergence and performance. Several methods have been proposed to address these issues. For example, FedProx [23] incorporates a proximal term to stabilize and unify updates from clients with varying local training conditions, while FedNova"}, {"title": "Supplementary Materials", "content": "Notation of multivariate time-series\nAfter reintroducing the index m for the M realizations, we can stack the data for client k. For each client k, define $n_k = M(T + 1 \u2212 p)$ as the effective sample size. Then, we can write:\n$X^k = X^kW + X_{t-1}^kA_1 + \u2026 + X_{t-p}^kA_p +Z^k,$\nwhere:\n\u2022 $X^k$ is a $n_k \u00d7 d$ matrix whose rows are $(x_{m,t})^k$ for m = 1, ..., M;\n\u2022 $X_{t-i}^k$ are similarly defined time-lagged matrices for i = 1, . . ., p;\n\u2022 $Z^k$ aggregates the noise terms $(u_{m,t})^k$.\nSimulation Data Generating:\nIntra-slice graph: We use the Erd\u0151s-R\u00e9nyi (ER) model to generate a random, directed acyclic graph (DAG) with a target mean degree pr. In the ER model, edges are generated independently using i.i.d. Bernoulli trials with a probability pr/dr, where dr is the number of nodes. The resulting graph is first represented as an adjacency matrix and then oriented to ensure acyclicity by imposing a lower triangular structure, producing a valid DAG. Finally, the nodes of the DAG are randomly permuted to remove any trivial ordering, resulting in a randomized and realistic structure suitable for downstream applications.\nInter-slice graph: We still use ER model to generate the weighted matrix. The edges are directed from node $i_{t-1}$ at time t-1 to node $j_{t}$ at time t. The binary adjacency matrix $A_{bin}$ is constructed as:\n$A_{it-1,jt} = \\begin{cases} \\t ext{with probability } pr/dr & \\t ext{for edges from node } i_{t\u22121} \\t ext{ to } j_{t}, \\\\ 0 & \\t ext{otherwise.} \\end{cases}$\nAssigning Weights: Once the binary adjacency matrix is generated, we assign edge weights from a uniform distribution over the range [-0.5, -0.3]U[0.3, 0.5] for W and [-0.5\u03b1, \u22120.3a]U[0.3a, 0.50] for A, where:\n$\\alpha = \\frac{1}{\\eta^{p-1}},$\nand n \u2265 1 is a decay parameter controlling how the influence of edges decreases as time steps get further apart.\nHyperparameters analysis\nIn this section, we present the optimal parameter values for each simulation. The following table records the optimal $\u03bb_\u03b1$ and $\u03bb_w$ values for experiments with varying numbers of variables (d). In general, $\u03bb_\u03b1$ = 0.5 and $\u03bb_w$ = 0.5 perform well in all cases. However, when $\u03bb_\u03b1, \u03bb_w$ > 0.5, the algorithm sometimes outputs zero matrices.\nClosed form for Bk and Dk\nMinimize with respect to $B_k$ and $D_k$:"}]}