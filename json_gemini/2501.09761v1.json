{"title": "VERITAS: Verifying the Performance of AI-native Transceiver Actions in Base-Stations", "authors": ["Nasim Soltani", "Michael L\u00f6hning", "Kaushik Chowdhury"], "abstract": "Artificial Intelligence (AI)-native receivers prove significant performance improvement in high noise regimes and can potentially reduce communication overhead compared to the traditional receiver. However, their performance highly depends on the representativeness of the training dataset. A major issue is the uncertainty of whether the training dataset covers all test environments and waveform configurations, and thus, whether the trained model is robust in practical deployment conditions. To this end, we propose a joint measurement-recovery framework for AI-native transceivers post deployment, called VERITAS, that continuously looks for distribution shifts in the received signals and triggers finite re-training spurts. VERITAS monitors the wireless channel using 5G pilots fed to an auxiliary neural network that detects out-of-distribution channel profile, transmitter speed, and delay spread. As soon as such a change is detected, a traditional (reference) receiver is activated, which runs for a period of time in parallel to the AI-native receiver. Finally, VERTIAS compares the bit probabilities of the AI-native and the reference receivers for the same received data inputs, and decides whether or not a retraining process needs to be initiated. Our evaluations reveal that VERITAS can detect changes in the channel profile, transmitter speed, and delay spread with 99%, 97%, and 69% accuracies, respectively, followed by timely initiation of retraining for 86%, 93.3%, and 94.8% of inputs in channel profile, transmitter speed, and delay spread test sets, respectively.", "sections": [{"title": "I. INTRODUCTION", "content": "Artificial Intelligence native Air Interface (AI-AI) offers a fully AI-based wireless interface for next-generation wireless, where Al is integrated in both the data and control paths [1], [2]. AI-AI provides a myriad of flexibilities and opportunities for the physical layer design and implementation, including but not limited to: merging data decoding and application in the physical layer, providing flexibility in the choice of waveform with respect to the radio hardware and environment constraints, obviating costly hardware implementation for each individual processing block by being fully AI-based, reduction in standardization need, and the possibility of physical and MAC layer fusion [1], [2]. Furthermore, as data decoding and interpretation happens through neural networks (NNs) that have learned to map received data to originally transmitted bits, AI-based receivers have previously shown to yield lower bit error rate (BER) under low SNR conditions, compared to receivers with traditional signal processing blocks [3], [4].\nProblem. Despite the several benefits that AI-AI provides for 6G communications, this newly proposed paradigm faces key challenges that need to be addressed before its successful de-ployment in 6G systems. For example, since wireless channel is a major contributor in the NN-based receiver performance, verifying and maintaining the performance of the AI-native receiver becomes a real challenge. NNs might not deliver the expected performance if they are deployed in an environment different from what they were trained in, as evident in several different examples: Recent work [5], [6] show that NN-based RF fingerprinting accuracy drops drastically when training channels are different from deployment channels. Authors in [3] show retraining is necessary to maintain the performance of an NN-based channel estimator in new environments. Authors in [7] show that automatic modulation classification performance drops when the environment changes, and propose transfer learning as an effective light retraining technique. The critical role of the wireless channel in AI/ML-based wireless systems renders the performance verification and maintenance of AI-AI necessary.\nPreliminaries. As shown on the left side of Fig. 1, we assume an NN-based receiver (a.k.a., AI-native receiver) that is responsible for converting received wireless signals (Rx Data & Pilots) to bits. Such models are well-explored in the literature in a number of prior works [8], [9], [10], [11], [12], [13], [3], [14]. Due to practical limitations, the training set of the NN-based receiver cannot contain all possible data variations or signals recorded under all possible channels encountered in the real world. Instead, the NN-based receiver is trained on a number of channel profiles, mobility conditions. While it performs well under the seen configurations, it is an open question whether it may suffer from a performance drop due to changes in the wireless environment (channel) during deployment.\nProposed Solution. On the right side of Fig. 1, we propose VERITAS a framework for verifying the performance of AI-native receiver to ensure its maintained superior performance over the traditional receiver. VERITAS has 3 components:\n\u2022 the Monitor, the traditional receiver (TradRx), and the Performance Comparator. The Monitor runs continuously in parallel to the AI-native receiver to observe the wireless channel and detect potential changes in the channel profile, transmitter speed, or delay spread. As soon as such a change is detected, TradRx is activated that is used as a comparison point against the AI-native receiver. The Performance Comparator compares output bit probabilities of the two receivers and determines which receiver is underperforming. Notably, this step does not require the true bit labels. If the AI-native receiver is identified as the underperforming receiver, a retraining process is initiated to lightly retrain the AI-native receiver. The proposed Performance Comparator in VERITAS is able to operate on encoded as well as raw (i.e., unencoded) bits, which obviates the need for a costly decoding block withing the proposed framework.\nContributions. Our contributions are as follows:\n\u2022 We propose VERITAS as a framework for verifying the performance of an AI-native receiver, to ensure its maintained superior decoding performance compared to traditional receiver (Section IV-A).\n\u2022 To demonstrate VERITAS works for generic AI-native receivers, we choose a widely used NN-based 5G receiver called DeepRx [15] as our AI-native receiver (Section III-B), which is designed to give lower BER compared to the traditional receiver (TradRx). We extensively analyze DeepRx performance for different training and test set configurations, and determine configurations where DeepRx yields higher BER compared to TradRx (Section III-C).\n\u2022 We propose an environment change detector called Monitor to identify any potential changes in one of the wireless channel properties of channel profile, transmitter speed, and delay spread. We design the Monitor as a NN-based out-of-distribution (OOD) detector, that extracts features out of received pilots, and uses a novel OOD detection algorithm based on K-nearest neighbor (KNN) for detecting environment changes (Section IV-B).\n\u2022 We propose an analytical method based on histogram binning to compare the output bit probabilities of the AI-native receiver against those of TradRx as reference. The proposed Performance Comparator compares output bit probabilities at the deployment phase and without having the true bit labels. This comparison determines if the AI-native receiver is underperforming with respect to the reference which initiates a retraining process (Section IV-C).\nWe pledge to publicly release our code for VERITAS including pipelines for the Monitor and the Performance Comparator, upon the acceptance of this paper."}, {"title": "II. RELATED WORK", "content": "In this section, we summarize the closest related work in three different areas of AI-native receiver performance maintenance (Section II-A), wireless channel change detection (Section II-B), and OOD detection (Section II-C)."}, {"title": "A. AI-native Receiver Performance Maintenance", "content": "The issue of performance drop in the NN-based receivers due to channel variations has been studied extensively. Authors in [16] propose a fixed time interval (periodic) retraining technique to adapt NN-based Orthogonal Frequency Division Multiplexing (OFDM) receivers to occasionally changing channel conditions. Naive periodic retraining is a way of maintaining performance of an NN-based receiver, however, it periodically imposes often unnecessary training computational complexity to the system as well as wastage of data frames that are used as the retraining dataset. Authors in [17] propose a de-noising approach during training for learning OFDM channel coefficients. They construct their training set out of estimated channel coefficients of low noise signals, but dynamically add Additive White Gaussian Noise (AWGN) to inputs during training. This method makes the NN-based channel estimator robust to changes in the noise level, however, this does not solve the problem of transitioning between different wireless channels between training and deployment phases."}, {"title": "B. Wireless Channel Change Detection", "content": "Authors in [18] use the channel state information (CSI) of IEEE 802.11p signals for environment identification in V2V communication. They consider 5 different environments of rural line-of-sight (LOS), urban LOS, urban non-line-of-sight (NLOS), highway LOS, and highway NLOS for V2V communication and perform a multi-class classification using a deep convolutional NN, KNN, support vector machine, random forest, and Gaussian Naive Bayes algorithms. They show superior performance of the NN compared to the other algorithms, however, they do not go beyond the fixed training set environments and do not show any method for identifying new unseen environments. Authors in [19] classify speeds of users in a 5G network using the reference signal received power (RSRP) passed through a deep NN. They categorize speeds between 0 and \u221e km/h into 8 non-overlapping classes, with the last class spanning from 90 km/h to \u221e. This categorization encompasses all the possible speeds, however, simple speed classification without out-of-library detection does not satisfy the requirements in our proposed AI-AI maintenance framework."}, {"title": "C. Out-of-Distribution (OOD) Detection", "content": "Detecting OOD samples is a well-investigated problem in machine learning [20], [21]. In wireless communications, autoencoders have been vastly used for OOD detection. In such methods an autoencoder is trained to reconstruct an input, and the reconstruction error for known in-distribution (ID) inputs are averaged and recorded as a reference. At the deployment phase, all unknown inputs are fed to the autoencoder and their reconstruction errors are compared against the reference error. If the reconstruction error of the unknown test input is larger than the reference, the input is identified as an OOD input. Authors in [22] use a variational autoencoder and study the signal reconstruction error for identifying OOD modulation schemes. The disadvantage of autoencoder-based OOD de-tection is that completely different pipelines are needed for OOD detection and ID data classification. On the other hand, classification-based OOD detection methods provide a unified pipeline for both tasks. Authors in [23] detect unseen devices in the well-known RFMLS [24] WiFi and ADS-B datasets using a classification-based OOD detection method. They train their classifier NN with a custom loss function that has 3 components of intra-centroid loss, nearest neighbor loss and a final loss component that pushes the cluster centers away to spread in the space. However, their proposed method requires exposure of the NN to out-of-library devices (classes) that are not categorized into meaningful classes during training. Authors in [25] include a feature-based new device detection in their proposed LoRa RF fingerprinting scheme. They calculate the average of distances of each test feature from all of its K nearest neighbors. They compare this average distance to a predefined A value, and decide if the device is OOD or has been seen during training. This feature-based OOD detection scheme, although light and efficient in real-time, has a few downsides. First, averaging the distances of the test feature from its K neighbors causes a lot of information regarding the individual distances from neighbors to be loss. Second, considering the distance of the test feature from its neighbors for OOD detection relies on the assumption that the ID points are close to each other and the OOD features are far from them. This limits the OOD detection scheme to work well only with ID clusters that are centrally dense and with only far OOD data.\nIn the rest of this paper, we introduce a widely used NN-based receiver as our example AI-native receiver, and explore its performance for different training and test set configurations. Then, we describe and evaluate VERITAS as a framework for verifying the performance of this AI-native receiver to avoid its naive periodical and often unnecessary retraining."}, {"title": "III. PRELIMINARIES", "content": "In this section, first we describe the data generation pipeline and the traditional receiver (TradRx) that are implemented in Python using Sionna libraries (Section III-A). Second, we describe the NN-based receiver that we use in this paper as our example Al-native receiver (Section III-B), and finally, we explore and study how the NN-based receiver performs compared to the TradRx in different training and test con-figurations (Section III-C). Specifically, we attempt to show cases where varying one of the parameters of channel profile, transmitter speed, and delay spread between the training and test sets increases the BER of the NN-based receiver above that of TradRx."}, {"title": "A. Data Generation and TradRx Pipeline: Sionna", "content": "Data Generation Pipeline. To generate 5G radio frames with different configurations for our training and test experiments, we implement a data generation pipeline using Sionna that is an open source Python library. Sionna enables the rapid prototyping of complex communication system architectures and provides native support for integration of NNs that are based on Tensorflow [26]. We implement a 5G transmitter that generates 5G radio frames with standard-compliant structure, grid size, and pilot patterns. To simulate the wireless channel, we use 3GPP 38.901 tap-delay line models of tdl_a, tdl_b, tdl_c, tdl_d, and tdl_e, that are implemented and available within Sionna. We also use the Sionna API AWGN() to add specific levels of noise to the data after wireless channel. Our data generation pipeline has the capabilities of simulating and generating large amounts of 5G data for a vast range of configurations, including but not limited to different pilot patterns, transmitter speeds, noise levels, delay spreads, and different channel models. The generated data is saved in different stages of the processing chain, such as at the transmit bit level, at the received frame level, etc., all in the signal metadata format (SigMF) [27].\nTraditional Receiver (TradRx) Pipeline. Apart from the design of transmitter, wireless channels and noise, we use Sionna APIs also to simulate a traditional 5G receiver that we refer to as TradRx. To implement TradRx, different Sionna classes and functions including OFDMDemodulator, LSChannelEstimator, LMMSEEqualizer, and Demapper are used. The imple-mented TradRx is used as a reference receiver for benchmark-ing the performance of the AI-native receiver for different datasets with different configurations."}, {"title": "B. The 5G AI-native Receiver: DeepRx", "content": "As our Al-native receiver, we choose a widely used fully convolutional 5G receiver with 672k parameters called DeepRx [15]. DeepRx interprets frequency domain I/Q samples of 5G subframes to their corresponding softbits (a.k.a., log likelyhood ratios (LLRs)) as outputs. The softbits represent output of the modulation symbol demapper in a traditional receiver, which are fed into the error correction block. As shown in Fig. 3, the inputs of DeepRx are 3 components: (i) the frequency domain received 5G subframe: that includes received data and pilots in the proper indices, where real and imaginary components are separated in the last dimension. The subframe has dimensions (14, 72, 2) that represent time, frequency, and real/imaginary parts, respectively. (ii) raw es-timated channel coefficients: that occupy pilot indices in a matrix of zeros with size (14, 72, 2) where real and imaginary components come in the last dimension. (iii) transmitter-side pilot symbols: that occupy the pilot indices in a matrix of zeros with size (14, 72, 2) similar to the raw estimated channel matrix. These three matrices are concatenated in the last dimension to create an input with dimensions (14, 72, 6) for DeepRx. The output of DeepRx is a vector of 8 LLR values for each I/Q sample, that add up to 14 \u00d7 72 \u00d7 8 = 8064 LLR values. This output size is designed to accommodate to the highest modulation scheme of 256QAM. However, a portion of the LLRs are invalid for lower modulation schemes and need to be filtered out for BER calculations. Specifically, the LLRs that fall in pilot indices and the most significant bits in LLRs that represent a higher modulation scheme than that of DeepRx input are invalid LLRs and must be filtered out. The DeepRx pipeline that we use in this paper is implemented in python using Tensorflow-Keras library. More details about DeepRx NN architecture can be found in [15]. We note that as the error correction block is not part of the DeepRx NN in [15], we also do not include this block in the implementation of either DeepRx or TradRx. Therefore, without losing generality of our proposed method, all the BER results reported in the rest of this paper are reported for unencoded bits."}, {"title": "C. Exploration of DeepRx Performance in Comparison to TradRx", "content": "One of the advantages of AI-native receivers is providing lower BER compared to the traditional receivers. Authors in [15], [28] show superior performance of DeepRx in terms of BER compared to the traditional non-NN receiver, however, only limited training/test configurations are studied. Various cases where the training and test sets might differ in terms of channel profiles, speeds, etc., are not considered in [15]. In the real world, training the AI-native receiver on all possible wireless channels is not possible due to practical limitations. Therefore, the trained AI-native receiver might be deployed in completely new settings where any or all the wireless channel properties are different. In this case, DeepRx (or any general AI-native wireless receiver) might fail to outperform the traditional receiver.\nIn this subsection, we attempt to identify corner cases where DeepRx BER increases above the traditional receiver BER, and refer to them as \u201cperformance drop cases\". While we limit our studies to DeepRx as a widely used 5G receiver, our core method can be deployed to any general AI-native receiver.\nHere, three different parameters of channel profile, transmit-ter speed, and delay spread are studied and the impact of their variations on the performance of DeepRx is explored. We per-form 6 training experiments and provide detailed description for each experiment that renders them repeatable by interested readers. Key takeaways of these experiments are summarized at the end of Section III-C. For each experiment, the training and test datasets are created with the three parameters set to specific values, described with one or multiple configuration triples of (channel profile, transmitter speed, delay spread). Data modulation scheme is set to 16QAM for all the datasets, and additive white Gaussian noise in range Eb/N0 = 0 to 20 dB with steps of 2 dB is generated and added to each 5G radio frame. Each training and test set contains 5000 and 500 uplink 5G radio frames, respectively, per Eb/N0 level for each configuration triple. In each training run, the DeepRx model is fully trained for ~20 epochs. To measure TradRx BER and DeepRx BER in different Eb/N0 levels, the corresponding test set is passed through the TradRx (described in Section III-A), and the trained DeepRx model (described in Section III-B), respectively, and BER versus Eb/N0 is plotted in Figures 4-9. In all these figures, each marker represents a certain test configuration triple, and DeepRx and TradRx BER plots are shown using solid lines and dashed lines, respectively. It is expected that DeepRx BER is lower than TradRx BER in all Eb/N0 (DeepRx outperforms TradRx), otherwise that specific training/test configuration is flagged as a performance drop cases.\n1) Impact of Change in Channel Profile on DeepRx BER: The wireless channel profile defines channel properties such as the number of reflections of the signal in the environment (a.k.a., number of channel taps), the relative delays between those reflections, and the signal attenuation caused by each reflection. To explore the impact of wireless channel profile types and find any potential performance drop cases we create two different training sets and perform two different experiments: Channel Profile - Exp. 1 and 2.\nChannel Profile - Exp. 1: The goal of the first channel experiment is to investigate DeepRx performance when it is trained on a NLOS channel and tested on different channel profiles including NLOS and LOS. We create the first training set with configuration triple of (channel profile, transmitter speed, delay spread) fixed as (tdl_a, 10 m/s, 400 ns). Here, tdl_a channel profile is selected as an example NLOS channel profile, and 10 m/s and 400 ns are chosen as arbitrary values for speed and delay spread parameters, respectively. After training DeepRx, we create five different test sets. In each test set we keep the speed and delay spread same as in the training set and set different channel profiles among the list of [tdl_a, tdl_b, tdl_c, tdl_d, tdl_e]. For each channel profile DeepRx BER and TradRx BER versus Eb/N0 levels are shown in Fig. 4. As it can be seen for all of the test channel profiles, DeepRx outperforms TradRx by yielding lower BER, which shows the trained model is robust to channel profile changes between the training and deployment phases. This is due to the training set channel (NLOS) being equally or more complicated compared to the test channel profiles (NLOS/LOS), which leads to training a DeepRx model robust to channel profile changes.\nChannel Profile - Exp. 2: To further explore the impact of change in the channel profile, we create a second training set with configuration triple of (channel profile, transmitter speed, delay spread) set as (tdl_d, 10 m/s, 400 ns). Here we choose tdl_d as an example LOS channel and keep speed and delay spread same as in Channel Profile - Exp. 1. After DeepRx model is trained, we test it on the five previously generated test sets with different channel profiles. Fig. 5 shows DeepRx and TradRx BER plots versus Eb/N0 for the five test channel profiles. As it can be seen, DeepRx BER is below TradRx BER in all Eb/N0 levels for tdl_d (LOS) (same channel as the training set) and tdl_e (LOS) (similar channel to the training set). However, BER of DeepRx increases above TradRx BER for some Eb/N0 levels in test sets of NLOS channels (i.e., tdl_a, tdl_b, and tdl_c). This increase is up to 111% for tdl_a in Eb/N0=20 dB. This shows that a performance drop case happens if DeepRx is trained on LOS and tested on NLOS channel profiles.\n2) Impact of Transmitter Speed Change on DeepRx BER: Another parameter that can vary between NN-based receiver training and deployment phases is transmitter speed. An AI-native receiver could be trained on data collected with certain transmitter speeds, however, deployed in other speeds. To study the impact of change in speed on DeepRx performance, we design and run two experiments: Speed - Exp. 1 and 2.\nSpeed - Exp. 1: The goal of the first speed experiment is to see how DeepRx performs if trained on higher and deployed in lower speeds. We compose a training set with configuration triple of (channel profile, transmitter speed, delay spread) set as (tdl_d, 18 m/s, 400 ns), (tdl_d, 19 m/s, 400 ns), and (tdl_d, 20 m/s, 400 ns). In this case, we select transmitter speed to take different values of 18, 19, and 20 m/s, and channel profile and delay spread are arbitrarily selected as tdl_d and 400 ns.\nSpeed - Exp. 2: The purpose of the second speed experiment is to investigate DeepRx performance when trained on lower and tested on higher speeds. To do this, we create a training set with the same channel profile and delay spread as in Speed - Exp. 1 (i.e., tdl_d and 400 ns) however, we chose the speeds to be 0, 1, and 2 m/s. Therefore, the training set comprises configuration triple of (channel profile, transmitter speed, delay spread) set as (tdl_d, 0 m/s, 400 ns), (tdl_d, 1 m/s, 400 ns), and (tdl_d, 2 m/s, 400 ns). After training, we test DeepRx on three different test sets with the same channel profile and delay spread as the training set, but speeds of 3, 4, 20 m/s. In this case, 3 and 4 m/s are chosen to be the closest higher speeds with respect to the training speeds, and 20 m/s is chosen as a higher speed far away from the training speed range. We plot BER versus Eb/N0 results of DeepRx and TradRx in Fig. 7 and observe that for 3 m/s test case that is 1 m/s higher than the training speed range, DeepRx BER is lower than TradRx BER. However, at 4 m/s that is 2 m/s higher than the training speed range, DeepRx BER starts to increase above TradRx BER. For 20 m/s test case which is a higher speed far away from the training speed range, DeepRx underperforms the TradRx with a larger gap of up to 16648% in Eb/N0 = 20 dB. This shows that a performance drop case might happen if DeepRx is tested on speeds that are 2 m/s or more higher than the training speed range.\n3) Impact of Change in Delay Spread on DeepRx BER: The last parameter we study is the delay spread that is another important property of a wireless dataset. In a given environment, delay spread can change as the objects and obstacles in the environment change. Consequently an NN-based receiver can be trained in an environment with certain delay spread values, however, it can be deployed in another environment with other delay spread values. To study the impact of change in the delay spread between the training and test environments we design and implement two experiments: Delay Spread - Exp. 1 and 2.\nDelay Spread - Exp. 1: The purpose of the first delay spread experiment is to investigate how DeepRx performs if it is trained on higher and deployed on lower delay spreads. We compose a training set with configuration triple of (channel profile, transmitter speed, delay spread) set as (tdl_b, 2 m/s, 400 ns), (tdl_b, 2 m/s, 450 ns), and (tdl_b, 2 m/s, 500 ns). In this case, we select the channel profile to be a NLOS channel (i.e., tdl_b) as the delay spread manifests a more significant impact in such channels. We select different delay spreads of 400, 450, and 500 ns as high delay spreads, and 2 m/s as a arbitrary value for speed. After DeepRx model is trained, we generate three test sets with the same channel profile and speed as the training set, however, we choose different delay spreads of 10, 50, and 80 ns as low delay spreads. We show BER results versus Eb/N0 for DeepRx and TradRx in Fig. 8. We observe that for all test delay spreads DeepRx BER is below TradRx BER, which means DeepRx outperforms TradRx if it is trained on higher, and tested on lower delay spreads. In this case no performance drop case occurs.\nDelay Spread - Exp. 2: The purpose of the second delay spread experiment is to investigate DeepRx performance when it is trained on lower and tested on higher delay spreads. To do this, we create another training set with configuration triple of (channel profile, transmitter speed, delay spread) set as (tdl_b, 2 m/s, 10 ns), (tdl_b, 2 m/s, 50 ns), and (tdl_b, 2 m/s, 80 ns). In this case, we keep the channel profile and speed configurations same as Delay Spread - Exp. 1, but select different values of 10, 50, and 80 ns for delay spread. After DeepRx is fully trained, we create three different test sets with the same channel profile and speed as in the training set, but different delay spreads of 100, 200, 400 ns. We plot BER versus Eb/N0 results of DeepRx and TradRx in Fig. 9 and observe that for delay spread of 100 ns, DeepRx BER is below TradRx BER. At 200 ns DeepRx starts to underperform the TradRx, even though only by a negligible margin in Eb/N0=20 dB. In 400 ns DeepRx BER is above the TradRx BER with a larger gap of up to 357% in Eb/N0=20 dB. This shows that a performance drop case might happen if DeepRx is trained on lower and tested on higher delay spreads.\n\u2022 Key take away: We explored the performance of DeepRx with respect to TradRx in different training and test configurations. We showed that DeepRx might provide higher BER compared to TradRx in three different cases: (i) change in the channel profile: if DeepRx is trained on a LOS channel such as tdl_d and deployed in NLOS channel profiles such as tdl_a, tdl_b, and tdl_c. (ii) change in the transmitter speed: if DeepRx is trained on a specific speed range such as speeds 0, 1, and 2 m/s and is tested on speeds that are 2 m/s or more higher than the training speed range. (iii) change in the delay spread: if DeepRx is trained on low delay spreads such as 10, 50, and 80 ns and tested on higher delay spreads such as 400 ns. The explored configurations that led to a performance drop for DeepRx are summarized in Table I."}, {"title": "IV. VERITAS FOR VERIFYING THE PERFORMANCE OF AI-NATIVE RECEIVERS", "content": "In this section, we describe VERITAS as a framework for verifying the performance of AI-native receiver in the AI-\u0391\u0399. We first discuss the overview of the system and provide high level description of interactions between different components in Section IV-A. Next, we go into the details of the Monitor, and the Performance Comparator in Sections IV-B and IV-C, respectively."}, {"title": "A. System Overview", "content": "AI-native receivers interpret received signals into bit se-quences. As shown in Fig. 1 VERITAS that is placed in parallel to the AI-native receiver, consists of 3 different components of the Monitor, the Performance Comparator, and the TradRx (introduced in Section III-A). The Monitor is pretrained on the same training set as the AI-native receiver. Therefore, the specific channel profile, speed, and delay spread covered in the training set are considered ID data for the Monitor NN. The Monitor constantly runs in parallel to the AI-native receiver (i.e., DeepRx) and detects any changes in the wireless channel as OOD samples. Following this detection one could retrain the AI-native receiver to adapt it to the new environment and restore its performance, however, not all the environment changes might cause a performance drop in the AI-native receiver, and retraining might be unnecessary. To avoid unnecessary retraining, as soon as the Monitor detects an environment change, the Performance Comparator is activated and triggers TradRx to run as the reference point for a certain period of time, in parallel to the AI-native receiver. The Performance Comparator compares bit probabilities generated by TradRx and the AI-native receiver, and decides if the AI-native receiver is still outperforming TradRx. Obviously, this performance comparison happens based on only bit probabili-ties, without having the true bit labels. If the AI-native receiver is outperforming TradRx, no retraining is required, and the Monitor should continue observing the wireless channel to detect future potential changes. If the AI-native receiver is not outperforming TradRx, a retraining process is initiated to adapt the AI-native receiver to the new environment. If a retraining process is initiated for the AI-native receiver, the Monitor needs to be retrained as well to update its set of ID classes and be able to continue detecting further changes in the wireless channel."}, {"title": "B. Environment Change Detector: Monitor", "content": "The job of the Monitor is to observe the wireless channel and detect potential changes in the channel profile", "21": [20], "29": "we propose an OOD detection algorithm based on the K-nearest neighbor (KNN) [30", "Structure": "Here", "channel first\\\" configuration in PyTorch, we bring this dimension of 2 to the beginning. Consquently, we form a matrix (tensor) of size (2, 90, 36) that is the input to the Monitor NN. The process of preparing inputs for the Monitor NN using 3 consecutive 5G radio frames each comprising 10 subframes (with pattern (c) in Fig. 2) is shown in Fig. 10.\nOutput. Output of the Monitor is a binary decision (ID or OOD) per input tensor.\n2) Architecture and Training Process": "For the Monitor NN architecture, we cascade a custom designed encoder network that is in charge of feature extraction and a projection network that contributes to more processing to form more distinguish-able and less scattered features.\nOur encoder network is a convolutional NN with residual blocks, consisting of convolutional, maxpooling, dense (or fully connected), and dropout layers. We design the output layer of encoder network to be of size I=512, and normalize its output tensor using the normalization function $f_{norm}(x)$ in (1) before sending it out as y.\n$f_{norm}(x) = \\frac{x}{max(|x|)}$    (1)\nWe note that the output of the encoder network, y, is a vector of I elements, however, we do not denote its vector indexing in this paper for the sake of simplicity.\nOur projection network consists of two dense (i.e., fully connected) layers"}]}