{"title": "Learning the Latent Rules of a Game from Data: A Chess Story", "authors": ["Ben Fauber"], "abstract": "We demonstrate that small pretrained foundational generative language models with millions of parameters can learn the latent rules of a process from data associated with the process. Inspired by Stefan Zweig's novella Schachnovelle, also known as The Royal Game in English, we show that 28M and 125M parameter pretrained foundational small language models (SLMs) can be instruction fine-tuned with 1,000-to-1,000,000 examples to learn the rules of chess, propose legal moves, and accurately solve chess problems. We also explore the impact of successive language model fine-tuning epochs on improved outcomes and demonstrate reductions in model hallucinations by increasing the number of instruction fine-tuning examples.", "sections": [{"title": "Introduction", "content": "The novella Schachnovelle, also known as The Royal Game in English, was written by the renowned Austrian author Stefan Zweig and first published in 1942 (Zweig, 1942). The story follows an anonymous narrator on an ocean liner traveling from New York to Buenos Aires. Among the passengers is world chess champion Mirko Czentovic. The narrator and a businessman named McConnor challenge Czentovic to a chess match, in which Czentovic easily wins the first game. In the next game, a mysterious passenger known only as Dr. B. intervenes, confounding Czentovic's strategy and unexpectedly forcing a draw.\nAs the story progresses, Dr. B. confides in the narrator that he was once a prisoner of the Gestapo. To maintain his sanity during his imprisonment, Dr. B. obsessively studied a book containing 150 championship chess games. The story culminates when Dr. B. agrees to a match against Czentovic. Dr. B. stuns Czentovic by winning the first game but grows increasingly agitated by the slow tempo of the second game. The narrator intervenes before Dr. B.'s frustration with the game's tempo overwhelms him, prompting Dr. B. to resign and withdraw from the second game as the story concludes.\nWe have shown that instruction fine-tuning small pretrained generative language models (SLMs) leads to highly specialized models capable of accurately performing challenging tasks that the base models cannot perform (Fauber, 2024a;b). Our advances in fine-tuning language models prompted us to revisit Stefan Zweig's fictional character, Dr. B., and his process of mastering chess.\nWe found it compelling that Dr. B., who initially had a limited knowledge of chess, mastered the game after intensely studying a book of championship chess games during his imprisonment. Even more remarkable was that his book contained no board diagrams and only used standard algebraic chess notation (SAN) to chronicle each game, which Dr. B. initially did not understand at all. Dr. B. described the book's contents as, \"...what were to me the almost unintelligible symbols... It all seemed a kind of algebra to me, to which I could find no key. Only gradually did I puzzle out that the numbers stood for the ranks and the letters for the files, so that you could establish the position of each piece.\"\nDr. B.'s fictitious yet plausible experience led us to explore the real possibility of learning the latent rules of a game from data. Herein, we explore: 1) can an instruction fine-tuned language model learn the rules of chess from only standard algebraic chess notation (SAN) game data; 2) how much data is required to learn the rules well, and does learning scale proportionally with the data; and 3) how well do instruction fine-tuned language models play the game of chess?"}, {"title": "Background", "content": "Chess is a strategic board game played between two opponents on an 8 \u00d7 8 grid called a chessboard. Each player begins with 16 pieces of a single color, usually white or black: one king, one queen, two rooks, two knights, two bishops, and eight pawns. The player with the white pieces makes the first move to initiate the game.\nThe objective is to checkmate the opponent's king, meaning the king is in a position to be captured (\"in check\") and there is no legal move to escape the threat. Players take turns moving one piece at a time, with each type of piece having its own unique movement pattern. The game can also end in a draw under various conditions, such as a stalemate or insufficient material to achieve checkmate. Strategy and tactics are crucial, as players aim to control the board, capture opponent pieces, and protect their own king (Harkness, 1956).\nThe game of chess is thought to of originated in India circa 600 CE. By 1000, the game had spread throughout Asia, the Middle East, and Europe. The game was widely adopted by nobility in these regions around 1500, and thus became known as the \"Royal Game\" (Lombardy, 1975).\nAlthough the game was widely played circa 1500, the rules varied widely by region. The rules of the game were not unified and codified until 1851 during a European congress of players organized by St. George's Chess Club of London at the Crystal Palace (Lombardy, 1975). The international chess code underwent some minor modifications in 1929 and 1954, but has since been largely unchanged (Harkness, 1956).\nAutomated chess game play dates back to 1769 with the advent of the Mechanical Turk automaton by Wolfgang von Kempelen (Hooper & Whyld, 1992). Serious interest in chess software began in the 1950s and gained momentum with the discovery and application of the alpha-beta pruning search algorithm to optimize move evaluation (Newell et al., 1958). Computers have since transformed chess by expanding opening theory, providing definitive endgame solutions, and bots/engines that are widely used for online play.\nDeep Blue was an expert system that combined a vast database of chess knowledge and heuristics with a powerful tree-search algorithm. Most modern and stronger chess engines, such as Stockfish, follow a similar approach. Notable architectural exceptions include AlphaZero (Silver et al., 2018) and and its open-source counterpart, Leela Chess Zero. Both utilize a combination of deep neural networks (DNNs) trained on data from millions of chess games, and powerful search algorithms paired with reinforcement learning (RL) to select moves with the highest probability of winning.\nA recent paper described the results of training a transformer-based model from scratch on a billions- scale move action-value annotated chess dataset (Ruoss et al., 2024). Unlike previous chess engines, this work did not explicitly use reinforcement learning (RL) or search algorithms. Instead, it heavily relied on the search capabilities of the Stockfish chess engine to accurately annotate billions of boards with move"}, {"title": "Our Contribution", "content": "Dr. B., the fictitious character in Stefan Zweig's Schachnovelle, led us to explore the real possibility of learning the latent rules of chess from game data. Dr. B. achieved Grandmaster-level chess skills by obsessively studying a book containing 150 championship chess games. In his book, each game was described solely in standard algebraic chess notation (SAN), with no accompanying board diagrams.\nChess engines, such as Stockfish, have learned game play from board states paired with powerful search algorithms to select the next best move (Hooper & Whyld, 1992). AlphaZero paired that approach with reinforcement learning (RL) to emphasize and learn the strategies with the highest probabilities of winning. The recent approach of combining billions of Stockfish-annotated move action-values with supervised learning differs slightly from the chess engine and reinforcement learning (RL) methods (Ruoss et al., 2024). However, it still utilized billions-scale static move action-values derived from chess engines with the goal of creating a Grandmaster-level chess bot. Additionally, to our knowledge, all prior approaches to chess engines/bots have been built/trained from scratch with the sole purpose of playing chess.\nUnlike these prior approaches, we used only SAN data from chess games and problems to replicate the situation experienced by Dr. B. in Zweig's novella. Further, we did not train our models from the ground-up, rather, we chose to explore the instruction fine-tuning of pretrained generative small language models (SLMs). We have previously demonstrated that instruction fine-tuning small pretrained generative language models leads to highly specialized models capable of accurately performing challenging tasks that the base models cannot perform (Fauber, 2024a;b). In this work, we sought to address the following questions:\n1. Can an instruction fine-tuned small generative language model learn the rules of chess from only standard algebraic chess notation (SAN) game data?\n2. If a fine-tuned model can learn the rules, how much data is required to learn them well, and does learning scale proportionally with the data?\n3. How well do instruction fine-tuned small language models play the game of chess?"}, {"title": "Methods", "content": "The python library python-chess provides extensive functionality to analyze portable game notation (PGN) format chess game files. The library also contains functions to extract board status of a game before/after a move and to check the legality of proposed moves given a board status."}, {"title": "Standard Algebraic Chess Notation", "content": "Standard algebraic chess notation (SAN) assigns a unique two-character identifier to each of the 64 squares on a chessboard. The first character represents the file (column) of the square, labeled from \"a\" (leftmost or queenside) to \"h\" (rightmost or kingside). The second character represents the rank (row) of the square, numbered from \"1\" (bottom side, white's first rank) to \"8\" (top side, black's first rank). For example, the initial positions of some pieces are: white queen's rook at al, white king at el, black queen's knight pawn at b7, and black king's rook at h8 (Hooper & Whyld, 1992).\nSAN identifies each piece by a single letter. The standard English values include pawn = \"P\", rook = \"R\", knight = \"N\", bishop = \"B\", queen = \"Q\", and king = \"K\". In our studies, we used uppercase letters to represent white pieces and lowercase letters to represent black pieces.\nNotably, the letter code for a pawn is not used in SAN moves within PGN format move text. However, letter codes for pawns and other pieces are sometimes necessary for certain tag pair and annotation constructs. For example, we used the letter codes for pawns in this study to differentiate between white and black pawn pieces when indicating our board status. An example of SAN move sequences is show in Figure 2, illustrating the \"Queen's Gambit Declined\" opening moves (Matanovic, 1978).\nA standard SAN move lists the moving piece's letter (omitted for pawns) followed by the destination square. Capture moves are indicated by a lowercase \"x\" before the destination square. For pawn captures, the file letter of the capturing pawn's starting square is placed immediately before the \"x\"."}, {"title": "Forsyth-Edwards Board Notation", "content": "Forsyth-Edwards notation (FEN) denotes a single board status. It does not record the moves that lead to the board status. For example, the starting positions of the game pieces are denoted by the sequence \"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR\" (Edwards, 1994). Each row of the board is separated by a forward-slash \"/\" character, with uppercase letters representing white pieces and lowercase letters representing black pieces, using the same piece designations as in SAN."}, {"title": "Dataset", "content": "Stefan Zweig's novella Schachnovelle was initially published in 1942 (Zweig, 1942). Therefore, we focused on games from prominent chess masters of that era as these were likely the games Dr. B., Zweig's character, might have studied. In the story, Dr. B. specifically mentions only two players: Alekhine and Bogoljubow, and their 1922 game in Pistyan, Slovakia.\nThe chess Grandmasters and International Chess Federation or FIDE (F\u00e9d\u00e9ration Internationale des \u00c9checs) world champions leading up to the early 1940's included Alexander A. Alekhine (1892\u20131946) (Alekhine, 1927; 1939; Kotov, 1975; Bjelica, 1993), Jos\u00e9 Ra\u00fal Capablanca (1888\u20131942) (Capablanca, 1935; Reinfeld, 1942; Panov, 1973; Chernev, 1982; Bjelica, 1994; Golembek, 1996), Emanuel Lasker (1868- 1941) (Lasker, 1960; 1965; Reinfeld & Fine, 1965; Charushin, 1998), Aron I. Nimzowitsch (1886\u20131935) (Nimzowitsch, 1925; 1929), Akiba K. Rubinstein (1880\u20131961) (Kmoch, 1960; Razuvaev & Murakhevri, 1980), and Savielly Tartakower (1887\u20131956) (Tartakower, 1924). We included many game sources for these notable players. We also included texts for short games (Chernev, 1954), openings (Mednis, 1986), middle games (Leininger, 1997), endings (Chernev, 1969), and esteemed games (Tartakower & du Mont, 1952; Lombardy, 1975; Chernev, 1992; 1995) to cover key tactics of game play.\nAll chess game data was collected from the referenced texts as portable game notation (PGN) format files, a standard format for recording single or multiple games in a text file. The PGN format contains the chess board configuration at the initiation of a game, or sequence, where the board state is encoded as a Forsyth-Edwards notation (FEN) text string (Edwards, 1994). Each white/black move pair are encoded"}, {"title": "Data Sampling", "content": "Following best practices in machine learning, we randomly divided parent datasets into training/fine-tuning data and test data by sampling without replacement. We varied the number of available fine-tuning data instances from 1,000 to 1,000,000 examples of board states and their subsequent white piece single-moves, by random selection without replacement from the parent pool of fine-tuning data instances for each instance cohort.\nThe fine-tuning data instances were used for language model instruction fine-tuning. The language models were never exposed to the test data (i.e., out-of-sample \"hold-out\" data) during the fine-tuning process to avoid train/test data contamination.\nWe acknowledge that early-game board states are limited, and players frequently use popular openings. Consequently, these common board states and moves appear in both the fine-tuning and test datasets. We decided to keep them in both sets to avoid distributional shifts that could skew our evaluation metrics.\nAll data was formatted into an instruction-based format where the \"instruction\" was the input, and the \"output\" was the desired outcome. For example, the instruction was, \"You are a chess Grandmaster and checkmate # is your goal. Predict the next best move on this SAN chess board: h1:K, a2:P, g2:P, h3:P, b4:p, g4:R, f5:r, a6:R, f6:p, b7:r, f7:k,\" and the corresponding output was the white piece single-move, \"Rg3\". The instruction formatting was consistent throughout the fine-tuning and testing datasets unless otherwise noted."}, {"title": "Pretrained Foundational Small Language Models", "content": "We selected the OPT (open pretrained transformer) family of pretrained foundational generative language models as the starting point for our studies (Zhang et al., 2022). We also explored the TinyStories (Eldan & Li, 2023) family of language models. The OPT-125M model contained 125M parameters, whereas the TinyStories-28M model contained 28M parameters. Both models provided up to 2,048 positional embeddings for their inputs, permitting context for long string sequences which can be present in our method.\nIn our work, we defined model fine-tuning as initialization of a pretrained foundational language model followed by updates to the model weights and biases. In our fine-tuning setting, all language model parameters could undergo gradient updates \u2013 there were no frozen layers nor adapters. In our prior work (Fauber, 2024b), we found the full fine-tuning approach was superior to adapter-based methods like LoRA (Low-Rank Adaptation) (Hu et al., 2021). Other research groups have since confirmed our initial findings (Biderman et al., 2024).\nThe prompt for the language models was consistent throughout our evaluation and across all models. The language model prompt was general and agnostic to the dataset instructions. The prompt used for our evaluation was: \"Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: {instruction} ### Response:\"."}, {"title": "Evaluation of Our Method", "content": "We evaluated the performance of our instruction fine-tuned language models on their ability to correctly propose legal white piece single-moves given a board state in the test dataset. We also evaluated the ability of our fine-tuned models to correctly predict the next white piece single-move in a \"check or checkmate in one move\" chess problem. The problems were drawn from a parent dataset of 1,836 chess problems (Polg\u00e1r, 2013). This data is referred to as the \"Check/Mate-in-1\" dataset."}, {"title": "Baseline Performance", "content": "Language models can be generalists, or specialists, and it is valuable to understand what is required to create a specialist language model. It is important to determine how much fine-tuning data, and which fine-tuning paradigm, are reasonable starting points to create a specialist language model.\nWe recognize that fine-tuning language models over multiple epochs may obliterate some portion of information that resides within the pretrained foundational language model. This potential change did not concern us as our objective was to create specialized language models from pretrained foundational language models, with the objective of effectively executing a highly specialized task that the original pretrained foundation models were incapable of performing.\nWe evaluated a series of pretrained generative foundational language models on their ability to provide a legal chess move in SAN given a board state in SAN. Many popular pretrained foundational language models (Radford et al., 2019; Black et al., 2021; Scao et al., 2022; Zhang et al., 2022; Eldan & Li, 2023; Jiang et al., 2023; Dubey et al., 2024) were unable to perform this task with any reasonable level of proficiency."}, {"title": "Increasing Dataset Size Improves Performance", "content": "Our objective was to expose small pretrained foundational language models to increasing orders of magnitude of domain-specific instruction fine-tuning data and assess their performance against test data with well- defined metrics. In our primary assessment, we evaluated the percentage of legal proposed moves as a function of increasing orders of magnitude of instruction fine-tuning data.\nWe observed that increasing the amount of domain-specific instruction fine-tuning data, as well as increasing the pretrained foundational language model parameter count (i.e., model size), improved the fine-tuned language model performance on our task (Figure 3). Thus, learning the latent rules of the game did scale with the data. Further, the OPT-125M model instruction fine-tuned with 1 million examples nearly always proposed a legal move.\nNext, we explored the influence of unique board and move combinations in the instruction fine-tuning dataset. Filtering the WSM-10M dataset to only unique board and move combinations resulted in the Unique-WSM-10M dataset, which contained approximately 80k white piece single-moves. The percentage of legal moves proposed by OPT-125M language models, which were instruction fine-tuned on progressively larger amounts of Unique-WSM-10M data, showed similar performance to the unfiltered WSM-10M fine-tuned models on the same number of example cohorts (Table 2)."}, {"title": "Chess Game Play Improves with More Data", "content": "We evaluated the game play performance of our instruction fine-tuned models using check or checkmate in one white piece single-move chess problems (Polg\u00e1r, 2013). The data for this evaluation process was our Check/Mate-in-1 dataset. Our instruction fine-tuned models were evaluated on: 1) their abilities to both propose legal moves and; 2) their abilities to propose legal moves that resulted in either check or checkmate to solve the chess problems.\nOur Check/Mate-in-1 dataset contained 1,836 examples. We sampled 1,000 test examples from this parent dataset for our evaluation. In our test instance containing 1,000 examples, the black king piece resided on 61 of the possible 64 squares of the board, representing a diversity of locations and strategies required to solve the problems. The test set problems contained between 3 and 32 pieces on the board. On average, each test set problem included a total of seven pawns, two knights, two bishops, three rooks, two queens, and two kings.\nBoth the TinyStories-28M and OPT-125M instruction fine-tuned models demonstrated reasonable abilities to propose legal moves (Figure 4) and solve the chess problems with legal moves (Figure 5). Again, the instruction fine-tuned OPT-125M models outperformed the smaller TinyStories-28M models. As noted in our earlier studies, the ability to accurately perform both tasks improved as the number of instruction"}, {"title": "Hallucinations Decrease with More Instruction Fine-Tuning Examples", "content": "While evaluating the performance of our instruction fine-tuned SLMs and their abilities to propose legal moves that resulted in check or checkmate on the Check/Mate-in-1 dataset, we also observed interesting trends regarding hallucinations. Generative language models are trained to accurately predict the next token in a sequence, thus generating human like text that resembles their training data. Yet, it is well known that generative language models can create text that sometimes contain inaccuracies or nonsensical information known as \"hallucinations\" (Agrawal et al., 2024).\nWe noted that increasing orders of magnitude of instruction fine-tuning data resulted in fine-tuned OPT-125M models which proposed fewer illegal moves and hallucinations of pieces on a board which did not exist (Figure 6). Based on these results, we concluded that the instruction fine-tuned language models with fewer examples exhibited game play somewhat akin to a novice child.\nTypically, when teaching children to play chess, they quickly grasp the objective and may suggest illegal moves to rapidly achieve check or checkmate. We observed a similar outcome with the 1,000 and 10,000 example instruction fine-tuned OPT-125M models, where the models suggested a majority of illegal moves to achieve check or checkmate. Yet, these tendencies diminished as the number of instruction fine-tuning examples increased."}, {"title": "Models Learn the Game Objective from Game Data", "content": "We also explored the role of the instruction fine-tuning text in achieving a legal move which resulted in check or checkmate. For all instruction fine-tuning examples, we included the statement, \"You are a chess Grandmaster and checkmate # is your goal.\" One could posit that this statement explicitly states the game's objective, and the model was learning this objective from the instruction text, along with all moves marked with the hash sign \"#\" to indicate checkmate. To evaluate the role of this statement in the instruction fine-tuning text, we used the exact same 1,000 to 1,000,000 instruction fine-tuning cohorts for WSM-10M without the statement, \"You are a chess Grandmaster and checkmate # is your goal.\" We referred to this revised instruction fine-tuning dataset as NoGoal-WSM- 10M.\nWe found that usage of the WSM-10M cohort or the NoGoal-WSM-10M cohort for instruction fine- tuning the OPT-125M model resulted in essentially the same outcomes for the percentage of legal proposed moves and legal proposed moves which resulted in check or checkmate (Table 3, Figure 4, and Figure 5).\nWe found that usage of the WSM-10M cohort or the NoGoal-WSM-10M cohort for instruction fine- tuning the OPT-125M model resulted in essentially the same outcomes for the percentage of legal proposed moves and legal proposed moves which resulted in check or checkmate (Table 3, Figure 4, and Figure 5)."}, {"title": "Revisiting the Same Data Provides Limited Benefits", "content": "We examined the model's ability to learn from a dataset by repeatedly revisiting the data and then evaluated its performance after multiple revisits. This process was analogous to Stefan Zweig's character Dr. B. obsessively studying his book of championship chess games.\nWe discovered that increasing the number of instruction fine-tuning epochs for the OPT-125M language model initially improved the model's ability to perform our tasks (Figure 7). This outcome was consistent for both the percentage of legal proposed moves and the legal moves that resulted in check or checkmate on the Check/Mate-in-1 dataset.\nThese results also demonstrated the potential to improve an instruction fine-tuned model's performance for a task with a constrained amount of instruction fine-tuning data. Yet, the successive improvements in results from increasing epochs do appear to plateau after 5-10 fine-tuning epochs."}, {"title": "Requiring Legal Moves with Increased Temperature Improves Outcomes", "content": "Throughout our studies, we used the default settings for the transformers.GenerationConfig() text generation function, unless otherwise specified (see Appendix). We also disabled the do_sample hyperpa- rameter within that function to ensure consistent and reproducible text generation, as documented in our previous work (Fauber, 2024a;b).\nConversely, enabling of the do_sample hyperparameter makes use of the temperature hyperparameter to allow for variation in text generation based on the temperature setting (default = 1.0). Thus, we explored the role of increasing generation variability, with the temperature hyperparameter \u2265 1.0, and a requirement that the proposed moves be legal.\nIn this analysis, the text generation process was allowed up to 100 iterations to obtain a legal proposed move. If no legal move was proposed by the final iteration, the attempt was labeled as illegal, and the process moved on to the next test example. For this study, we used an OPT-125M language model that was instruction fine-tuned with 1,000,000 examples drawn from the WSM-10M dataset.\nThe baseline text generation for this study, where the do_sample hyperparameter was disabled (tempera- ture = 1.0 as the default setting) and only a single text generation iteration was allowed, is shown in Figures 4 and 5. Conversely, enabling the do_sample hyperparameter and allowing up to 100 text generation iterations to achieve a legal proposed move, while keeping the temperature at the default setting of 1.0, resulted in a higher percentage of proposed legal moves and legal moves that led to check or checkmate (Figure 8)."}, {"title": "Discussion", "content": "We have provided a framework for rigorous and systematic evaluation of our instruction fine-tuned language model outputs. We demonstrated that small pretrained foundational generative language models with millions of parameters can learn the latent rules of a process from data associated with the process.\nWe were inspired by Stefan Zweig's novella Schachnovelle, also known as The Royal Game in English, in which his character, Dr. B., learns how to play Grandmaster-level chess by compulsively studying a book of 150 championship chess games. We showed that 28M and 125M parameter pretrained foundational language models can be instruction fine-tuned with 1,000-to-1,000,000 instruction examples to learn the rules of chess, propose legal moves, and accurately solve chess problems.\nWe also demonstrated that increasing the number of instruction fine-tuning examples leads to models learning the objective of the game before learning the rules of the game. We also showed that increasing the number of instruction fine-tuning examples minimizes hallucinations associated with illegal proposed moves and imagining pieces on the game board to enable check or checkmate.\nFinally, we showed that increasing the number of instruction fine-tuning epochs can lead to some improvements in legal moves and accurately solve chess problems, but the improvements quickly plateau after 5-10 epochs. At a high level, these results further demonstrate that pretrained generative language models can serve as general learning frameworks for sequence-based tasks.\nOverall, our results suggest that instruction fine-tuning generative language models with chess game play data can replicate the learning experience of Stefan Zweig's character, Dr. B, in Schachnovelle. Specifically, increasing the amount of SAN-formatted game data used for instruction fine-tuning improves our models' understanding of the game rules and game play."}, {"title": "Conclusion", "content": "In conclusion, our results indicate that instruction fine-tuned generative language models can learn the rules of chess from only standard algebraic chess notation (SAN) game data. Further, model performance scales increasingly well with increasing orders of magnitude of instruction fine-tuning data. For example, instruction fine-tuning of the OPT-125M generative language model on 1 million examples resulted in nearly perfect proposals of legal moves when evaluated on 10,000 test examples drawn our WSM-10M dataset. Finally, we demonstrated that our instruction fine-tuned language models can propose winning strategies for chess problems."}, {"title": "Appendix", "sections": [{"title": "Computational Infrastructure and Code", "content": "The results described in this article were carried out using a Dell Technologies PowerEdge C4140 server with 4 x V100 NVIDIA\u00ae SXM GPU cards with 32 GB VRAM each and NVLinkTM connectivity. There were 2 x Intel\u00ae Xeon\u00ae processors on the server with 1.5 TB of CPU RAM.\nThe server was configured with the Ubuntu v22.04 Linux operating system, Anaconda v23.1.0, NVIDIA\u00ae CUDA v12.2, and NVIDIA\u00ae drivers v535.54.03. Additional python dependencies included: python-chess v1.10.0, torch v2.1.1, and transformers v4.45.1.\nThe Stanford ALPACA language model code was git cloned directly from https://github.com/ tatsu-lab/stanford_alpaca (accessed 30Dec2023). The train.py file in the GitHub repo, along with our corresponding instruction fine-tuning dataset, was used to instruction fine-tune the language models in our study. The language model fine-tuning code was executed via the command line interface (CLI).\nAs an example, the following CLI command was used to instruction fine-tune a pretrained foundational language model on 4 GPUs:\ntorchrun --nproc_per_node=4 [TRAINING_PY_FILE] \\\n--model_name_or_path [HUGGINGFACE_MODEL_NAME] \\\n--data_path [DATA_PATH_TO_FORMATTED_JSON_FILE] \\\n--bf16 False \\\n--output_dir [OUTPUT_DIRECTORY] \\\n--overwrite_output_dir True \\\n-num_train_epochs 3 \\\n--per_device_train_batch_size 4 \\\n--per_device_eval_batch_size 4 \\\n-gradient_accumulation_steps 8 \\\n-save_strategy \"steps\" \\\n-save_steps 5000 \\\n--save_total_limit 1\\\n--learning_rate 2e-4 \\\n--weight_decay 0. \\\n-warmup_ratio 0.03 \\\n--lr_scheduler_type \"cosine\" \\\n-seed 41 \\\n--logging_steps 1 \\\n--tf32 False"}, {"title": "Dataset Creation", "content": "The dataset was focused on, but not exclusive to, games from prominent chess masters in the early 1900's to 1940's. The chess Grandmasters and International Chess Federation or FIDE (F\u00e9d\u00e9ration Internationale des \u00c9checs) world champions leading up to the early 1940's included Alexander A. Alekhine (1892\u20131946) (Alekhine, 1927; 1939; Kotov, 1975; Bjelica, 1993), Jos\u00e9 Ra\u00fal Capablanca (1888\u20131942) (Capablanca, 1935; Reinfeld, 1942; Panov, 1973; Chernev, 1982; Bjelica, 1994; Golembek, 1996), Emanuel Lasker (1868\u2013 1941) (Lasker, 1960; 1965; Reinfeld & Fine, 1965; Charushin, 1998), Aron I. Nimzowitsch (1886\u20131935) (Nimzowitsch, 1925; 1929), Akiba K. Rubinstein (1880\u20131961) (Kmoch, 1960; Razuvaev & Murakhevri, 1980), and Savielly Tartakower (1887\u20131956) (Tartakower, 1924). In addition to the above-cited texts, we also included texts for short games (Chernev, 1954), openings (Mednis, 1986), middle games (Leininger, 1997), endings (Chernev, 1969), and esteemed games (Tartakower & du Mont, 1952; Lombardy, 1975; Chernev, 1992; 1995) to cover key tactics of game play.\nAll chess game data was collected from the referenced texts as portable game notation (PGN) format files. All games were parsed into individual moves for both white and black pieces, along with the board status before each move. The FEN game boards were converted into SAN, ensuring that both the move and board status were in SAN. The conversion to SAN was conducted to align with the experience of Stefan Zweig's character, Dr. B., in Schachnovelle."}, {"title": "Language Model Text Generation Configuration", "content": "The same language model fine-tuning and generation configurations were utilized throughout our studies", "with": "n\u2022 num_beams = 2", "identical": "n\"Below is an"}]}]}