{"title": "Pre-training Auto-regressive Robotic Models with 4D Representations", "authors": ["Dantong Niu", "Yuvan Sharma", "Haoru Xue", "Giscard Biamby", "Junyi Zhang", "Ziteng Ji", "Trevor Darrell", "Roei Herzig"], "abstract": "Foundation models pre-trained on massive un-labeled datasets have revolutionized natural language and computer vision, exhibiting remarkable generalization capabilities, thus highlighting the importance of pre-training. Yet, efforts in robotics have struggled to achieve similar success, limited by either the need for costly robotic annotations or the lack of representations that effectively model the physical world. In this paper, we introduce ARM4R, an Auto-regressive Robotic Model that leverages low-level 4D Representations learned from human video data to yield a better pre-trained robotic model. Specifically, we focus on utilizing 3D point tracking representations from videos derived by lifting 2D representations into 3D space via monocular depth estimation across time. These 4D representations maintain a shared geometric structure between the points and robot state representations up to a linear transformation, enabling efficient transfer learning from human video data to low-level robotic control. Our experiments show that ARM4R can transfer efficiently from human video data to robotics and consistently improve performance on tasks across various robot environments and configurations.", "sections": [{"title": "1. Introduction", "content": "Recently, foundation models (FMs) have shown remarkable success, particularly in the domains of language (Brown et al., 2020; Touvron et al., 2023), vision (Kirillov et al., 2023), and multi-modal models (Chen et al., 2022; Alayrac et al., 2022; Liu et al., 2023; Li et al., 2023; OpenAI, 2023) pre-trained on vast amounts of vision and text data. These models exhibit impressive zero-shot and few-shot learning capabilities (Radford et al., 2021; Ouyang et al., 2022; Wei et al., 2021; Chung et al., 2024), highlighting the power of pre-training on generic data. However, numerous attempts in robotics (Xiao et al., 2022; Kim et al., 2024; Zhen et al., 2024b; Niu et al., 2024; Ye et al., 2024) have yet to achieve the same pre-training success seen in other domains. This could potentially be attributed to the scarcity of large-scale, diverse robotic data, unlike the abundance of text and image data available for vision and language FMs.\nThe lack of robotic data poses a significant bottleneck in training foundation models that can effectively generalize across diverse robotic platforms and tasks. To overcome this limitation, several recent approaches (Xiao et al., 2022; Ye et al., 2024) employ representation learning by pre-training on an abundance of human data, enabling transfer to robotic systems. These approaches aim to recognize the inherent similarities between human and robot manipulation tasks and exploit the vast repositories of human video data available on the internet. Yet, these approaches have not been able to demonstrate effective generalization to downstream tasks. In part, this is due to their representations lacking an understanding of the physical world (Zhen et al., 2024a), and therefore being less effective for robotics.\nIn contrast with these methods, Vision-Language-Action (VLAs) models take a slightly different approach, implicitly leveraging human data in robotics by incorporating pre-trained components from Vision-and-Language Models (VLMs). In particular, they use language decoders pre-trained on tasks like visual question answering (e.g., RT-2 (Brohan et al., 2023a)) and image captioning (e.g., OpenVLA (Kim et al., 2024)). Despite such efforts, there is a discrepancy between these models' high-level pre-training objective and the goal of enabling robotic models to handle low-level action prediction. While these initial objectives are valuable for comprehending visual and linguistic content, they don't directly address the nuances of low-level robot control, which involves aspects like precise manipulation and spatial reasoning. To address this, this paper's method employs a lower-level pre-training objective by starting with a model that utilizes next-token prediction to learn 4D representations from human video data. These representations can then be transferred to more specialized scenarios by fine-tuning on robotic scenes and subsequently on proprioceptive data, while maintaining the same training objective.\nIn this paper, we introduce ARM4R (Auto-regressive"}, {"title": "2. Related Work", "content": "Vision-Language-Action Models. VLAs are a type of robotic model that combines visual perception, language understanding, and action generation capabilities. VLAs take as input visual observations along with a language instruction, and output a sequence of robot control actions. Several VLAs, such as LLARVA (Niu et al., 2024), OpenVLA (Kim et al., 2024), LLaRA (Li et al., 2024a), and RoboPoint (Yuan et al., 2024) directly fine-tune a VLM to predict robot actions, often using special tokens to represent the action space. These models differ in the choice of VLM and the specific method used to encode robot actions, but they share the underlying principle of adapting a pretrained VLM for robotic control. A similar model is 3D-VLA (Zhen et al., 2024a), which consists of components for generating future states of an environment based on data that includes 3D information, such as point clouds. These existing VLAs utilize language decoders that have been pre-trained for high-level tasks like image captioning (Kim et al., 2024) and VQA (Brohan et al., 2023b), which may be inadequate for low-level robotic environments. In contrast, we show that leveraging low-level vision representations from human video data can result in a better pre-trained robotic model.\n3D Motion Fields. Motion estimation spans from 2D optical flow (Horn & Schunck, 1981) and object tracking (Wu et al., 2013) to recent dense point tracking (Harley et al., 2022). Moving from 2D to 3D further enriches the geometric understanding. Early work on scene flow (Vedula et al., 1999) estimates short-term 3D motion based on explicit 3D structure (Menze & Geiger, 2015) or depth images (Teed & Deng, 2021). More recently, SpatialTracker (Xiao et al., 2024) tackles long-range 3D point tracking by lifting 2D pixels into 3D with monocular depth estimates and iteratively refining 3D trajectories with as-rigid-as-possible motion priors. This 3D-driven strategy greatly improves occlusion robustness and yields impressive 3D point tracking results.\nIn robot learning, 2D motion fields have been used to enable fine-grained control, guiding manipulation and imitation learning (Goyal et al., 2022; Vecerik et al., 2023; Gu et al., 2023; Yuan et al., 2024; Zheng et al., 2024; Bharadhwaj et al., 2024; Xu et al., 2024). Despite their success, these approaches remain limited by the lack of geometric cues and less spatial awareness. In contrast, 3D motion fields offer more spatially grounded representations, enabling more efficient policy learning. ToolFlowNet (Seita et al., 2022) leverages scene flow to estimate tool trajectories in behavior cloning, though it uses only a relatively coarse 3D signal. We instead adopt dense 3D point tracking on diverse human videos, and use these rich 4D representations (3D points tracked across time) to pre-train a general auto-regressive robotic model with robust and versatile action generation.\nPre-training for Robotic Models. Pre-training has emerged as a crucial technique for improving the performance and generalization capabilities in robotics. Large-scale datasets such as OpenX (Collaboration et al., 2023) contain diverse sensor modalities, tasks and action spaces across various robots. Models trained with these datasets, such as RT-1-X (Brohan et al., 2023c), RT-2-X (Brohan et al., 2023a), Octo (Team et al., 2024), OpenVLA (Kim et al., 2024) and LLARVA (Niu et al., 2024), can be applied in various robot embodiments and tasks. Yet, these robot pre-training datasets are still orders of magnitude smaller than the data that current LLMs and VLMs are trained on.\nTo address the data issue, another prominent pre-training approach is to leverage large-scale datasets of human videos. This harnesses the abundance of freely available human activity data on the internet, offering a scalable alternative to collecting expensive robot demonstrations. For example, Track2Act (Bharadhwaj et al., 2024) trains a 2D point-tracking model on human videos from Epic-Kitchens100 (Damen et al., 2018) and Something-Something-v2 (Goyal et al., 2017), then re-purposes it to guide robotic manipulation. Any-Point Trajectory Modeling (ATM) (Wen et al., 2024) similarly utilizes a small set of human demonstrations to aid cross-embodiment transfer, though in a more task-specific setting and still relying on 2D motion. By contrast, our approach lifts 2D observations into 4D representations (3D plus time), which not only enhances spatial awareness and occlusion handling, but also allows pre-training on human videos at scale, providing broader applicability and more robust policy learning in robotics."}, {"title": "3. Auto-regressive Robotic Models", "content": "To address the challenge of leveraging pre-trained vision representations from human video in robotic models, we present an auto-regressive model that relies on low-level 4D representations. The model is trained in three stages. The first stage the pre-training stage-focuses on learning generalized low-level representations through 3D point tracking from human videos. In the second stage, the model is fine-tuned for the same task, but using a small amount of data for the robot that we intend to use in downstream tasks. Finally, the third stage fine-tunes the model for robotic control. We begin by discussing the preliminaries (Section 3.1), then introduce the architecture (Section 3.2), and the training procedures (Section 3.3). Our method is shown in Figure 2.\n3.1. Preliminaries\n4D Scene Representations. Our 4D representations result from solving the 3D point tracking problem, which involves finding the 3D coordinates of discrete points across time, given a monocular video consisting of T discrete frames. Formally, the objective is to find pt as defined below:\n$P_t = \\{(X_{jt}, Y_{jt}, z_{jt})| 0 \\leq j < n\\}$ (1)\nwhere n is the total number of points being tracked, and 0 < t < T. In solving this tracking problem, the identities of the points are fixed and consistent across all frames: the j-th point in pt refers to the same physical point in 3D space across all time steps t \u2208 [0,T). To initialize these points, we define a square grid of size g\u00d7 g on the first frame (frame t = 0), resulting in n = g\u00b2 points. The task is to track the 3D coordinates of these initial queried n points throughout the video while maintaining their unique identities.\nRobotic Episodes. Robotic control can be formulated as a finite-horizon Markov Decision Process (MDP), characterized by temporal sequences that capture the robot completing a particular task. The task is described by the language instruction l. The temporal sequences typically consist of visual observations $i_{0:T-1}$ and proprioceptive states $s_{0:T-1}$, which can lie in Cartesian space or joint position space. Then, the objective is to learn a policy that predicts one or more future actions, conditioned on a finite number of previous timesteps, to successfully complete a given task.\nInputs. Given any video, we structure our models' input at timestep t, into three parts: the language instruction l, the"}, {"title": "3.2. Architecture", "content": "In the first and second training stages, our objective is to develop an auto-regressive model \u3160 capable of predicting 3D point tracks. The predictions are conditioned on the input (l, it, pt) from a context window of C' timesteps:\n$\\pi(l, i_{t-C+1:t}, p_{t-C+1:t}) \\rightarrow p_{t+1}$ (2)\nDuring control fine-tuning, the objective changes slightly, as the model conditions on, and predicts proprioceptive states:\n$\\pi(l, i_{t-C+1:t}, s_{t-C+1:t}) \\rightarrow s_{t+1}$ (3)\nBefore being fed into the causal transformer for next-token"}, {"title": "3.3. Training", "content": "As previously mentioned, ARM4R is trained in three stages: the first two stages focus on the 3D point tracking task for human and robot videos respectively, and the last stage focuses on robotic control. Next, we describe these stages.\nStage 1: Human Videos Pre-training. In the pre-training stage, we focus on learning 3D point tracking, since this task allows our model to leverage large-scale human video data with a representation that also transfers over to the robotic domain. Specifically, we train our model on 76K videos from the Epic-Kitchens100 dataset (Damen et al., 2018), which contains rich human-object interactions with 97 verbs and 300 noun classes. By training to predict 3D point tracks for such large-scale human data, ARM4R gains a deeper understanding of the spatial dynamics and physical interactions of different bodies and objects, knowledge that is critical for enhancing robotic models.\nTo extract pseudo-annotations for 3D point tracks, we use an off-the-shelf tracker that generates 3D tracks for points arranged on a g\u00d7 g grid. Points on the grid are initialized in the first frame of the video and tracked throughout the sequence. We note that the pseudo-labeled tracks are generated in the camera coordinate frame, inherently capturing both object and camera motion due to the egocentric nature of the human videos. In contrast, our robotic applications typically involve stationary cameras and different object-hand interaction patterns, introducing discrepancies in both camera dynamics and embodiment. To reconcile these differences and ensure smooth transfer to robotic domains, we introduce a fine-tuning stage focusing on 3D point tracking in the downstream robotic setup.\nStage 2: Fine-tuning on 3D Point Tracking for Robotic Settings. After the pre-training stage on human video data, we fine-tune ARM4R for the same 3D point tracking task with videos from the robotic setup we use in the downstream application. We note that this fine-tuning only needs to be performed once for every robot setup for all tasks combined, with a modest amount of data (\u2248 5-10% compared to Stage 1). This step helps transition from the camera dynamics and embodiment gaps between the human video pre-training and the control fine-tuning in the next stage.\nStage 3: Fine-tuning for Robotic Control. Having trained the model on 3D point tracking, we then fine-tune it for robotic control. In this stage, we collect a number of robotic demonstrations depending on the downstream tasks. We note that we use significantly fewer demonstrations for real robotic tasks than other baselines (See Section 4.3). After collecting successful data of the robot performing the target task, we replace the current and predicted point tracks in the training process with current and predicted robot states."}, {"title": "4. Experiments and Results", "content": "We evaluate ARM4R on 12 tasks in RLBench (James et al., 2020) and compare to relevant 2D and 3D baselines. We also test and ablate our model on two real robots: a 7-DoF Kinova Gen3 robot, and a 7-DoF Franka Emika Panda robot.\n4.1. Implementation Details\nARM4R is implemented using PyTorch (Paszke et al., 2019). We use ViT-Base as our vision encoder, which is pretrained as described in Section 3.2. We use SpatialTracker (Xiao et al., 2024) as our off-the-shelf 3D point tracker. We note that the model uses a maximum context window C', which is the number of previous timesteps it considers when predicting the next action. In practice, we use C = 16 for most tasks, increasing it to C = 32 for some long-horizon tasks (details in Appendix D). The model is also trained to predict the next 16 actions, but we only execute the first prediction during evaluation. In both our simulation and real settings, we use end-effector control, with the model predicting the Cartesian position and rotation of the end-effector, and a binary value for the gripper. Finally, we use 4 NVIDIA A6000 GPUs for training and a single NVIDIA A6000 GPU for evaluation. More information, like training and fine-tuning recipes, is in Appendix C.2.\n4.2. Simulation Evaluation\nExperimental Setup. We evaluate ARM4R on 12 RLBench tasks, and follow the settings in PerAct (Shridhar et al., 2023). A task is defined as a collection of demonstrations of the robot interacting in a given scene, with object variations (such as color or size). We train ARM4R for each task using 190 successful demos for every variation of the task (for more details, see Appendix D), and evaluate using 25 episodes per task in the validation set. Every episode is scored either 0 for failure or 100 for success. We use 5 seeds, which are averaged to get the final success rate.\nBaselines. We compare to several baselines for our simulation evaluation. Image-BC (ViT) is a 2D language-conditioned baseline model that uses a ViT vision encoder, reported in PerAct (Shridhar et al., 2023). To compare against two different methods that use 3D representations, we select C2FARM-BC (James et al., 2022) and PerAct, which use voxels as 3D input to calculate robot actions. To compare to a method with 3D temporal tracking similar to ours, we evaluate against ManiGaussian (Lu et al., 2025), which uses a dynamic Gaussian splatting representation to predict robot actions. Lastly, LLARVA (Niu et al., 2024) is a recent state-of-the-art VLA that directly predicts low-level robot actions given an image and proprioceptive information as part of a language prompt.\nResults. We report our simulation results in Table 1. ARM4R achieves the highest average success rate across all the tasks, and the best success rate for 4 out of 12 tasks. In particular, ARM4R surpasses PerAct, which directly uses voxel information from the simulation environment as input. This approach is not scalable since voxel data is expensive to collect in the real world. Instead, ARM4R learns to model the 3D world by pre-training on 3D point tracking, and the impressive performance highlights the model's strong grasp of physical understanding. We also note that ARM4R's superior performance compared to LLARVA a VLA which uses a pre-trained language decoder emphasizes the effectiveness of our representation and pre-training approach.\n4.3. Real Robot Evaluation\nExperimental Setup. For our real experiments, we use a 7-DoF Kinova Gen3 robot mounted with a Robotiq 2F-85 adaptive gripper. We test our model and the baselines on 13 total tasks, grouped into five broad categories based on the dominant action: pick, destack, stack, pick and place, and push. For each task, training is performed using 190 episodes of every variation. Evaluation is conducted over 25 episodes per task, with results averaged across three different seeds to calculate the final success rate.\nBaselines. We evaluate our model against two baselines in real-world settings: ATM (Wen et al., 2024) and OpenVLA (Kim et al., 2024). ATM utilizes a hierarchical framework to predict 2D point trajectories, which are then used to condition a policy. In contrast, ARM4R predicts 3D point trajectories, a more intuitive and natural representation for robotic tasks. OpenVLA, a state-of-the-art 7B-parameter VLA model, is pre-trained on the OpenX dataset, while ARM4R is trained on a significantly smaller dataset, with pre-training consisting exclusively of human video data. More details for these implementations are in Appendix D.1.\nResults. Table 2 shows that ARM4R outperforms both baselines across all tasks, achieving an average success rate of 83.1%, compared to OpenVLA's 37.2% and ATM's 6.4%. ATM in particular does not perform well in our real setting despite training with a significantly larger number of demonstrations than we use in our fine-tuning. We believe that this significant gap in performance is due to how we track points: ARM4R utilizes 3D coordinates, while ATM relies on 2D. The use of 3D coordinates provides a more natural and accurate representation for robotic tasks, which may contribute to our model's improved performance.\nIn contrast to ATM, OpenVLA uses a similar number of fine-tuning episodes to our evaluation setting. However, we believe that our superior performance over OpenVLA can again be attributed to our use of low-level 4D representations, which enable 3D scene understanding.\n4.4. Ablation Studies\nWe conduct ablations to assess the importance of human video pre-training (Stage 1), and the robotic fine-tuning (Stage 2). All model versions in this section include robotic control fine-tuning (Stage 3). For this, we train the following versions: (i) Stages 1, 2, and 3; (ii) Stages 1 and 3; (iii) Stages 2 and 3, and (iv) Stage 3 only.\nHuman Video Pre-Training. Figure 3 shows that the model with all stages performs better on all tasks than the Stages 2+3 model, indicating that pre-training on the human dataset provides a large benefit compared to only training for 3D point tracking on robotics videos. The performance boost observed when adding Stage 1 to Stage 3 is greater than the boost from adding Stage 2 to Stage 3, indicating that 4D pre-training on human videos provides a larger increase in performance than robotic videos. The key resulting insight is that when sufficient robotic pre-training data is unavailable, human video data can be a viable alternative, provided the proper 4D representations are used.\nRobotic Video Fine-Tuning. The ablation results shown in Figure 3 reveal that adding robotic video fine-tuning (Stages 2+3; green) leads to improved performance over"}, {"title": "4.5. Additional Experiments", "content": "We perform additional experiments to evaluate our pre-training effectiveness, and how well the 3D point representations can generalize. More experiments are in Appendix A.\nThe Effectiveness of Pre-training. In order to study the effectiveness of pre-training on the 3D point track prediction task, we take three tasks from our real setting: pick cube, destack cubes, and stack cubes, and compare to other works that use pre-training. MVP focuses on pre-training the vision encoder using human data, while RPT focuses on pre-training with visual and proprioceptive states. Octo, which is a transformer-based policy, is pre-trained on the OpenX dataset, similar to the VLA models LLARVA and OpenVLA. Lastly, ATM pre-trains a 2D point track transformer whose output is used to condition a policy.\nThe results are shown in Table 3. It can be seen that our pre-training improves performance over the baselines. ARM4R outperforms other representation learning based pre-training methods, such as MVP, RPT, Octo and ATM, validating the benefits of using a 4D point-tracking based representation. In addition, while the two VLA baselines (OpenVLA and LLARVA) perform well, our approach still surpasses their results, possibly demonstrating the importance of using low-level representations as opposed to language decoders that were pre-trained on high-level vision-language tasks.\nGeneralization from Kinova to Franka. In order to study how our low-level 4D representations can help a model generalize across different robots, we perform an ablation experiment involving fine-tuning ARM4R on Kinova robot"}, {"title": "5. Conclusion", "content": "In this work, we demonstrate that our pre-training approach from human video data to robot learning is effective in addressing longstanding challenges of robotic learning pre-training. We introduced ARM4R, an Auto-regressive Robotic Model that leverages low-level 4D representations by lifting 2D representations into 3D using monocular depth estimators, and tracking 3D points in videos. Our results in simulation and real-world setups show that our method consistently outperforms existing methods across diverse robotic tasks, showcasing its superior transferability and generalization capabilities. More broadly, our approach shows that training solely on human video data can lead to better performance than methods like OpenVLA that are pre-trained on robotic data alone. This suggests that effective pre-training can be achieved without the need for large-scale robotic datasets by bridging the gap between human-centric visual data and robotic applications, unlocking new possibilities for scalable and data-efficient robotics. As we continue to explore the boundaries of representation learning for robotics, ARM4R lays a foundation for future research into autonomous systems that can learn from the vast repository of human experience available in video data."}, {"title": "6. Limitations and Future Work", "content": "While ARM4R offers substantial benefits for pre-training with human video data for robotic learning, it is important to recognize certain limitations that accompany our approach. Our approach tracks 3D points in camera coordinates, leading to learned representations that combine object and camera motion. This coupling makes it difficult for the model to disentangle the two, leading to potentially inaccurate predictions due to a lack of invariance to camera intrinsics and motion. An improvement addressing this concern could involve pre-training on 3D tracks in world coordinates, leveraging recent dynamic SLAM methods such as MonST3R (Zhang et al., 2024) or MegaSAM (Li et al., 2024b), which we leave for future work. Other improvements could include scaling the pre-training data to help the model generalize better to different camera viewpoints, or using multi-view fusion to reduce the dependency on a single viewpoint to improve robustness to occlusions. Another line of future work could focus on selectively tracking only relevant or moving points instead of a fixed uniform grid across frames. This would allow greater resolution in areas with small objects, and also help the model focus on objects critical to the task, improving its ability to disentangle object motion from background noise and camera movement."}, {"title": "7. Impact Statement", "content": "In this paper, we demonstrate how pre-training on low-level 4D representations from human video data can benefit robotic action prediction. This suggests that effective pre-training can be achieved without the need for large-scale robotic datasets by bridging the gap between human-centric visual data and robotic applications, unlocking new possibilities for scalable and data-efficient robotics. Finally, we note that this paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "Appendix", "content": "Here, we provide qualitative results of our 3D point tracking for both in-domain and out-of-domain videos (Appendix A), statistics of the used datasets (Appendix B), implementation details (Appendix C), evaluation details (Appendix D), and robotic setup details (Appendices E and F).\nA. Additional Experiment Results\nA.1. Qualitative Results of 3D Points Tracking.\nWe conduct additional experiments to evaluate our pre-trained model's ability to track 3D points. Specifically, we run inference on a few randomly chosen episodes from Epic-Kitchens100 (Damen et al., 2018) (in-domain human videos), Ego4D (Grauman et al., 2021) (out-of-domain human videos), Kinova robot videos (in-domain robot videos) and Open X Embodiment (Collaboration et al., 2023) (out-of-domain robot videos).\nIn Figure 4, we present the tracking results on human videos from a version of our model that has undergone human video pre-training (Stage 1). The top two rows show the results for an episode from Epic-Kitchens (in-domain human videos) with the action \"stir potatoes.\" The bottom two rows display monocular human videos and their corresponding 3D point tracking predictions for an episode from Ego-4D (out-of-domain human videos) with the action \"pick up plate\".\nIn Figure 5, we present the tracking results on robot videos, from a version of our model that has undergone human video pre-training on Epic-Kitchens100 as well as robot video fine-tuning on Kinova demonstration videos (Stage 1+2). The top four rows display monocular robot videos and their corresponding 3D point tracking predictions for two episodes from in-domain Kinova robot videos, with the actions \"push red button\" and \"place spiderman into bowl\" respectively. The bottom two rows show the results for an episode from the Autolab subset of the OpenX Embodiment dataset (out-of-domain robot videos) with the action \"pick the tiger and place it into bowl.\"\nThese visualizations verify that our model is not overfit to a certain dataset or robotic setup, but can in fact generalize well to new videos.\nB. Additional Dataset Details\nB.1. Epic-Kitchens100\nEpic-Kitchens100 (Damen et al., 2018) is a large-scale, egocentric video dataset designed for action recognition and understanding in daily kitchen activities. Captured from a first-person perspective using head-mounted cameras, the dataset provides rich, untrimmed video recordings of individuals performing various cooking and kitchen-related tasks. It features a diverse range of object interactions, fine-grained action labels, and naturalistic, unscripted activities, making it particularly valuable for studying human-object interactions and long-term temporal dependencies.\nThe dataset includes diverse hand-object interactions, described by combinations of 97 verbs (for the hand motions) with 300 nouns (for the object categories). In our human video pre-training stage, we use almost all the labeled episodes available in the original dataset. We first subsample videos at 10 fps, an experimentally chosen rate, as the original 50 fps provides unnecessary redundancy for slow movements. We then model the duration distribution of all 75,886 episodes, and filter out \u2248 1% of episodes that are of length > 256 frames. As a result, we get a final set of 75,041 episodes for pre-training. For each episode, we use a simple 'verb + noun' instruction derived from the official annotation files.\nB.2. RLBench Robot Episodes\nRLBench (James et al., 2020) is a large-scale benchmark dataset for robotic learning, designed to facilitate research in vision-based reinforcement learning and imitation learning. It consists of a diverse set of robot manipulation tasks performed in a simulated environment using a Franka Emika Panda arm. The dataset provides high-quality demonstrations with multi-modal observations, including RGB images, depth maps, and proprioceptive data.\nIn our experiments, we use 128 \u00d7 128 resolution images for training. For most tasks, we use the 'front rgb' and 'wrist rgb' views for point track and control fine-tuning. However, in some cases, we find that using other views yields better performance (details on task-specific implementations are provided in Appendix C). For robot control, we use end-effector control: x = (x, y, z, 0x, \u03b8y, 0z), where (x, y, z) is the position and (\u03b8\u03b1, \u03b8y, 0z) the Euler angles for orientation. We also have a one-dimensional binary element to control the gripper. For language instructions, we use variation 0 from the official list of instructions for all tasks. We do not subsample episodes for our RLBench experiments.\nC. Additional Implementation Details\nC.1. Architecture of Auto-regressive Model\nHere, we provide details on processing the visual input of the auto-regressive model to support two views when adapting to robot control fine-tuning. The images from both views are fed separately into the image encoder to obtain the image embeddings zim for each view. Each view is then pooled using attention pooling with the state embeddings to form the image tokens zobs. Next, we project each token to half of its original hidden dimension (768 \u2192 384 in our implementation) and concatenate them to obtain the final"}]}