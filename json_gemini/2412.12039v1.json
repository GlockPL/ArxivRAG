{"title": "Can LLM Prompting Serve as a Proxy for Static Analysis in Vulnerability Detection", "authors": ["Ira Ceka", "Feitong Qiao", "Anik Dey", "Aastha Valechia", "Gail Kaiser", "Baishakhi Ray"], "abstract": "Despite their remarkable success, large language models (LLMs) have shown limited ability on applied tasks such as vulnerability detection. We investigate various prompting strategies for vulnerability detection, and, as part of this exploration, propose a prompting strategy that integrates natural language descriptions of vulnerabilities with a contrastive chain-of-thought reasoning approach, augmented using contrastive samples from a synthetic dataset. Our study highlights the potential of LLMs to detect vulnerabilities by integrating natural language descriptions, contrastive reasoning, and synthetic examples into a comprehensive prompting framework. Our results show this approach can enhance LLM understanding of vulnerabilities. On a high-quality vulnerability detection dataset such as SVEN, our prompting strategies can improve accuracies, F1-scores, and pairwise accuracies by 23%, 11%, and 14% respectively.", "sections": [{"title": "1 Introduction", "content": "The rise of large language models (LLMs) has led to dramatic improvements in natural language processing (NLP), demonstrating remarkable performance across various tasks, such as language understanding, translation, text generation, and text summarization. These advancements have extended into the domain of code, facilitating complex tasks such as code generation, code summarization, and code repair (Touvron et al., 2023; Li et al., 2023). One particularly critical yet challenging task in the code domain is vulnerability detection, which is an ever-growing concern by software security practitioners and users. The task of vulnerability detection can be thought of a binary classification task-given a piece of code, a detector will predict vulnerable/non-vulnerable. Traditionally researchers used a range of program analysis techniques, including static and dynamic analysis (Chakraborty et al., 2021).\nDeep-learning based methods have also employed the use of transformer-based models and graph-based networks (such as Devign (Zhou et al., 2019), ReVeal (Chakraborty et al., 2021), IVDetect (Li et al., 2021) and LineVD (Hin et al., 2022)), which analyze relationships and dependencies in code. More recent works have shown that models fine-tuned on Transformer-based models (such as CodeBERT (Feng et al., 2020), LineVul (Fu and Tantithamthavorn, 2022) and UniXcoder (Guo et al., 2022)) can outperform specialized techniques. There has also been work (Thapa et al., 2022) that evaluates a fine-tuned language model on multi-class classification, where the classes corresponds to groups of similar types of vulnerabilities. Other less common approaches have even taken a causality route to vulnerability detection (Rahman et al., 2024). However, Ding et al. (2024) reported that the datasets used for training these prior models exhibit several issues, including data leakage, inaccurate labels, and data duplication. This has driven recent initiatives to develop datasets that are cleaner, higher-quality, and more reliable (Ding et al., 2024; He and Vechev, 2023). With these improved datasets that contain more real world vulnerabilities, even state-of-the-art LLMs like GPT-4 exhibit limited performance in zero-shot settings (Steenhoek et al., 2024; Ding et al., 2024). This challenge has led to new strategies that focus on prompt construction. These approaches attempt to incorporate domain-specific knowledge into the prompts, particularly concerning the different types of vulnerabilities. While these new prompting strategies show initial potential on synthetic datasets, their performance on high-quality, real-world datasets still remains limited (Ullah et al., 2024; Khare et al., 2023). Even when LLMs are prompted with specific hints about the type of vulnerability\u2014such as, \"Given"}, {"title": "2 Background", "content": "We introduce relevant terms related to vulnerability detection."}, {"title": "2.1 Vulnerability Terms", "content": "Vulnerable: This label indicates a code that has a security flaw or weakness that can be exploited by an attacker. A vulnerable piece of code is at risk of being the entry point or cause of a security breach. These vulnerabilities are often categorized by their types (see CWEs below). Fixed: A term used when a previously identified vulnerability in the code has been \"remedied\" or fixed. The fix adjusts the \"vulnerable\" code to eliminate the security weakness. A fragment of code that has been \"fixed\" for a particular CWE is considered non-vulnerable to that CWE; thus, we may use the terms interchangeably throughout the paper. Non-vulnerable: This label indicates code that has no vulnerabilities detected within the specific checks conducted. However, this does not guarantee the code is entirely secure; it only means no issues were found within the tested scope."}, {"title": "2.2 CWE: Common Enumeration Weaknesses", "content": "In our studies, we direct our efforts on 4 critical categories. These appear in MITRE's CWE Top 25 Most Dangerous Software Weaknesses.\n\u2022 CWE-78 (OS Command Injection): occurs when external user inputs are directly passed into system commands without proper sanitization.\n\u2022 CWE-190 (Integer Overflow): occurs when an arithmetical operation on integers results in a value outside of the variable's allowable"}, {"title": "3 Prompting Strategies", "content": "In this section, we illustrate our prompting techniques. Our methodology uses curated Juliet C/C++ data and LLM prompts to generate detailed vulnerability explanations. We select vulnerable and fixed code pairs (Phase 1), prompt the LLM for CWE-specific detection instructions (Phase 2), and generate chain-of-thought explanations comparing the pairs (Phase 3). Finally, we synthesize a contrastive template highlighting vulnerabilities and fixes (Phase 4). See A.3 for detail."}, {"title": "3.1 Vanilla Prompt", "content": "We carefully design a basic vanilla prompt. As shown in Figure 1, the code sample is included before the instruction.\nTo avoid ambiguous outputs while ensuring that the model provides sufficient and meaningful information, we craft the prompt as follows: we ask, \"Is the example vulnerable or non-vulnerable?\", the inclusion of the instruction \"Do not give any extra information\" within the prompt is informed by our preliminary findings from a pilot study. Our pilot study reveals that, without explicit instructions to constrain the response to either \"vulnerable\" or \"non-vulnerable,\" the model frequently produces superfluous and inconclusive results. By incorporating this specific instruction, we significantly enhance the quality of the LLM's output for vulnerability assessment."}, {"title": "3.2 Natural Language Instructions", "content": "We adapt three different NL-based prompting strategies that describe the vulnerability type in different ways. The three prompting strategies are designed to explore the efficacy of feeding different instructional sources to the LLM.\nNL (S1): In the first setting, we utilize the LLM to generate the instructions for detecting a specific CWE, leveraging its inherent understanding of CWEs to provide insightful and relevant instructions.\nNL (S2): In the second setting, we employ a more tailored approach, where we utilize the LLM to generate instructions from few-shot samples. We ask the LLM to culminate a set of instructions based on 3 pairwise (both vulnerable and fixed) samples extracted from the SVEN validation set for that particular CWE. This method allows us to closely align the generated instructions with a practical, high-quality real-world dataset of vulnerabilities.\nNL (S3): In the third setting, we opt for an approach that uses an exact match from the descriptions provided by MITRE for that particular CWE. This setting ensures that the instructions are directly correlated with authoritative, human-written, and widely recognized descriptions of vulnerabilities. It is important to note that these settings are not developed within a contrastive chain of thought framework, as we will describe in Section 3.3, hence they do not incorporate synthetic examples. This approach allows us to independently evaluate the effectiveness of each setting in enhancing the natural language-based descriptions."}, {"title": "3.3 Natural Language Description +\nContrastive chain-of-thought", "content": "Contrastive examples. In our methodology, we extract contrastive examples from simple synthetic vulnerable-fixed pairs. These are simple code samples that show a vulnerable pattern and its corresponding fix. We utilize the Juliet C/C++ Dynamic Test Suite, a comprehensive and widely recognized benchmark for evaluating static analysis tools and techniques in identifying vulnerabilities. We extract one vulnerable example and its corresponding fixed example from the dataset; the vulnerable example is defined as a piece of code that is susceptible to a specific CWE described in the NL"}, {"title": "3.4 Metrics", "content": "To evaluate our study, we assess the performance of our proposed methodology using established metrics commonly applied to vulnerability detection: Accuracy and F1 scores are standard metrics.\n$Acc = \\frac{tp + tn}{tp + tn + fp + fn}$\n$F1 = \\frac{2tp}{2tp + fp + fn}$\nHowever, Ding et al. (2024) point out a significant limitation of these metrics: in real-world applications, the majority of code is non-vulnerable, leading to a potential bias where models achieve high accuracy simply by predicting code as non-vulnerable. On the other hand, the F1 score, which is a harmonic mean between both precision and recall, is regarded as more suitable for imbalanced datasets. Yet it also fails to reflect the asymmetry in its penalty. Thus, they use an alternate metric pairwise accuracy as the most appropriate metric.\n$PairwiseAccuracy(pAcc) = \\frac{Correct Pairs}{Numberof Pairs}$\nPairwise accuracy is a measure of the model's ability to correctly predict the ground-truth labels for both elements of a vulnerable and fixed pair / total # of pairs. Thus, while we report and demonstrate improvement across all metrics, we particularly emphasize the success of our prompting strategies in enhancing pairwise accuracy. This focus highlights our approach's ability to better address the balance between detecting vulnerabilities accurately (and thus preventing attacks) and minimizing false alarms, enhancing our methodologies' applicability to the real world."}, {"title": "4 Experimental Setup", "content": "For our main experiments, we use GPT-4 to generate our instructions and sample responses. We evaluate the prompts and report our results on the following:"}, {"title": "4.1 Models", "content": "We employ advanced language models to evaluate our methodologies, including: GPT-4o (gpt-4o-2024-05-13) (the latest version publicly available at the time of this study), GPT-4 (gpt-4-turbo-2024-04-09), and GPT-3.5 (gpt-3.5-turbo-0125). For all models, we set the temperature to 0.7 and the maximum token count to 4096. We utilize a top_p sampling parameter set to 1 and employ the Chat Completions API to generate responses. All other settings are maintained at their default values."}, {"title": "4.2 Datasets", "content": "Previous work has shown that high-quality and reliable datasets for vulnerability detection are notably sparse and limited.\nSpecifically, datasets like CVEFixes exhibit issues of data duplication and label inaccuracy. Nevertheless, they are frequently used in benchmark evaluations by the related work. Consequently, our study also incorporates CVEFixes to facilitate comparative analysis of our prompting techniques. We direct our efforts on demonstrating performance on novel high-quality, real-world datasets, specifically SVEN (He and Vechev, 2023).\n\u2022 SVEN (real-world): a manually-labeled, balanced dataset known to have 94% label accuracy. This dataset originally comprises of 803 vulnerable/non-vulnerable pairs (1.6k samples total). We filter for the 4 CWE's, extracting 184 samples total (11.5% of the full dataset). Due to cost constraints, we only test on this randomized subset, allocating 46 samples to each CWE.\n\u2022 CVEFixes (real-world): Bhandari et al. (2021) retrieved samples from open-source repositories using an automated collection tool. The dataset contains 39.8k samples of both the vulnerable and non-vulnerable class. We filter for our 4 CWE's of interest. We experiment on 1,784 samples total. See Table 2."}, {"title": "4.3 Baselines", "content": "For our baseline, we test on a blank template that simply prompts the model to identify whether a SVEN sample is vulnerable (See Fig 1). This does not include any signals from the synthetic data, nor does it include any additional natural language augmentations. This is because it is important to assess how the model performs without instructions or the presence of the few-shot synthetic samples in the augmented prompt. The vanilla prompt performance reflects the minimum expected accuracy for the task and provides a clear reference point to evaluate the improvements achieved by our prompting strategies."}, {"title": "5 Evaluation", "content": "RQ1: Which prompting strategy performs best? We investigate how different settings and models perform on our metrics.\nRQ2: How do our methods compare to other relevant work?\nRQ3: What are the strengths and limitations of LLMs in detecting software vulnerabilities using our prompting techniques? We conduct a comprehensive analysis to uncover the reasons behind the varying effectiveness of different prompting strategies and models across the CWEs."}, {"title": "5.1 RQ1: Prompting Strategies", "content": "We compute the average performance across three trials for each type of experiment, covering all CWEs and models. Our results indicate that each prompting strategy improves upon the performance of the basic vanilla prompting strategies (see Figure 4). These prompting strategies consistently show higher pairwise accuracy compared to the"}, {"title": "5.2 RQ2: Comparison with SOTA Methods", "content": "The CVEFixes dataset spans multiple programming languages. Previous studies, namely Khare et al. (2023), primarily focused on applying GPT-3.5 and GPT-4 to analyze the C/C++ subset of the dataset, covering our 4 CWEs of interest. To align with this study, we similarly restricted our analysis to CVEFixes C/C++ for our 4 CWEs of interest. We tested on 1,201 samples total spanning the 4 CWEs. We sought to replicate the studies conducted by the authors as closely as possible; note, deprecations on the OpenAI platform have resulted in the unavailability of certain GPT models referenced in their work. As such, we adopted the recommended replacement models\u00b2 suggested by OpenAI in these cases in order to ensure continuity in our analysis. Furthermore, the lack of reported F1 scores for CWEs further restricts the depth of comparison. We report the result from the best-performing model and prompting strategy. As shown in Table 4, we show that for each CWE, there exists a strategy that achieves the best results, achieving up to a 12% improvement in accuracy with our prompting. Further experiments reveal that performance can be further improved with state-of-the-art models"}, {"title": "5.3 RQ3: Strengths and Limitations of LLMs\nin Identifying Vulnerabilities", "content": "In our analysis, we manually review 276 output responses from the LLMs. We elect to analyze runs that display either exceptionally high or exceptionally low performance. The instances we select are outlined in the table.\nFor each selected instance, we evaluate the output responses on both the vulnerable and fixed versions, with 46 samples review per instance. Then, we classify the responses into 4 distinct groups:\nLR: correct label, correct reasoning/root cause; LR: correct label, wrong reasoning/root cause; LR: wrong label, correct reasoning/root cause; LR: wrong label,wrong reasoning/root cause.\nAs shown in Table 5, GPT-4o achieved an impressive performance on classifying CWE-78 samples in NL (S1) and NL+CoT (S3), identifying most of the vulnerable samples accurately with correct reasoning. The LLM succeeded in detecting unsafe command construction practices, such as concatenating or interpolating user inputs with command strings without sanitizing, and using shell commands to execute user inputs. For more elaboration, please refer to 'B.1\u2019.\nAmong the few fixed samples that the model incorrectly identified, the failures were due to not having the full context of the samples. Most of the samples in the dataset did not explicitly contain proper sanitization and validation of user inputs.\nFurthermore, GPT-4 was able to accurately identify the vulnerable parts of code from CWE-190 samples in NL+CoT (S3) which contributed to determining required memories sizes without necessary checks. The LLM succeeded in detecting underlying vulnerabilities in buffer size calculations"}, {"title": "6 Related Work", "content": "Ding et al. (2024) highlight the challenges of using LLMs for vulnerability detection. They show that LLMs struggle in zero and few-shot settings. The former also emphasizes the critical need for high-quality datasets devoid of data-leakage and label inaccuracies. Ullah et al. (2024) also evaluate current state-of-the-art LLMs, revealing they are unreliabile in detecting vulnerabilities. Despite their exploration of reasoning analysis and CWE-specific templates, their study is constrained to a small dataset, testing 248 samples with only"}, {"title": "7 Conclusions", "content": "The prompting strategies employed in this work enhanced the identification of vulnerabilities in real-world samples. The results show that these strategies outperform the basic prompts, particularly on the most significant metric: pairwise accuracy. Specifically, pairwise accuracy improved by 15-36%, effectively increasing the detection of true vulnerabilities while reducing false alarms\u2013 a critical challenge in practical applications. Furthermore, these strategies surpassed the performance of existing methods in prompting for vulnerability detection. Finally, our manual analysis of the LLM responses highlights their ability to identify issues such as string interpolation and shell command execution, while also exposing their difficulties in understanding context, memory allocation, and data structures."}, {"title": "8 Limitations", "content": "Cost poses a significant limitation in our experiments, particularly as academic researchers aiming to utilize state-of-the-art models. We opt to use OpenAI's GPT-based models due to their recognized quality. Throughout the course of this work, we made over 120,000 API calls to support our experiments and analyses. As a result of these constraints, we focused our investigation on 4 specific CWEs and limited our analysis to variations within the GPT-model family. We have provided a cost analysis; see Table 6. Consequently, we do not conduct a comprehensive cross-analysis across CWEs.\nAs emphasized in our study, the quality of the real-world datasets at hand is critical (for this reason we targeted SVEN (He and Vechev, 2023)). Additionally, constructing a reasoning framework using synthetic examples requires identifying CWES that are consistently present across both our high-quality/real-world and synthetic datasets, which further restricts our experimental scope."}, {"title": "9 Ethics Statement", "content": "Use of Generative AI. Employing generative models for vulnerability detection carries inherent ethical considerations. This includes the potential misuse of vulnerability detection methods for malicious purposes. In our research, we ensure that the techniques are used responsibly, focusing on enhancing security (discovering true vulnerabilities so they can be patched) and mitigating risks associated with software vulnerabilities (such as alert fatigue).\nPotential for Misuse. While our methods improve the detection of software vulnerabilities, there is a potential risk that these techniques could be utilized for exploitative purposes. We emphasize the importance of ethical use.\nCompute. Our research does not involve training new models; therefore, the computational impact is relatively low."}, {"title": "A Appendix", "content": "respectively (see below for an example)."}, {"title": "A.1 Approach Overview", "content": "A visual overview of the pipeline: first, the LLM generates instructions (see A.3 for how), which are then used to construct a prompt. Finally, the constructed prompt is used to run inference on a vulnerable or non-vulnerable code example from a real-world dataset."}, {"title": "A.2 Juliet C/C++ Synthetic Examples", "content": "Two C/C++ synthetic samples from the Juliet Dataset are given below."}, {"title": "A.3 How We Leverage LLMs for Prompt\nGeneration", "content": "We illustrate the step-by-step process for how the prompts are generated in the most comprehensive case Natural Language + CoT. This provides clarity of insight into how we harness LLMs at each step.\nOur approach involves a multi-phase process where we leverage manually curated data from the Juliet C/C++ dataset along with LLM-generated explanations (using ChatGPT-4). The process is divided into 4 key phases:"}, {"title": "Phase 1: Pairwise Sample Selection", "content": "In the first phase, we manually select pairwise samples for each CWE of interest from the Juliet C/C++ dataset.\nIn the Juliet C/C++ Dynamic Test Suite, the synthetic pairwise samples are organized into files, with each file representing a distinct vulnerability scenario. These vulnerability scenarios are constructed in various ways to demonstrate or expose vulnerabilities (as well as their fixes) by combining different variations of good sources (safe input data), bad sources (insecure input or actions), and bad sinks (unsafe downstream commands or operations). This variability illustrates how different scenarios can lead to vulnerabilities and how they can be mitigated. We randomly select one such scenario for each CWE and locate the pairwise samples in the file (Ex. we have provided an example of such a pair for CWE-78: OS Command Injection in A.2. Note: though a bad sink is mentioned, it may be abstracted and referenced indirectly in the sample.)\nThis selection process ensures that the LLM works with real-world vulnerability cases, enabling more practical and accurate detection prompts."}, {"title": "Phase 2: Instruction Generation with LLM", "content": "In the second phase, we prompt the LLM (ChatGPT-4) to generate a set of instructions specific to each CWE. The process begins with a predefined prompt, which includes the CWE-ID and a consistent format for instruction generation. For example, the prompt for CWE-78: OS Command Injection is structured as follows:"}, {"title": "Phase 3: Generating Chain-of-Thought Reasoning", "content": "In Phase 3, we build upon the outputs generated in Phases 1 and 2 to create additional content that contributes to the construction of the final prompt. Specifically, we take the LLM-generated instructions from Phase 2 and combine them with the manually curated pairwise code samples from Phase 1, which include both the vulnerable and fixed synthetic examples. These combined inputs are then used to prompt the LLM once more, guiding it to analyze the provided examples in depth.\nThe primary goal is to produce a detailed chain-of-thought explanation for each pair, focusing on two key aspects: first, how the vulnerable function from Phase 1 is deemed unsafe based on the instructions generated in Phase 2, and second, how the fixed function from Phase 1 resolves the identified issues and is considered safe according to the same instructions."}, {"title": "Phase 4: Final Contrastive Chain-of-Thought\nTemplate Generation", "content": "In the final phase, we consolidate the results by manually synthesizing a contrastive chain-of-thought template that highlights the differences between the vulnerable and fixed versions of the code. This template combines the generated instructions, the chain-of-thought for each code pair, and the reasoning process used by the LLM from each phase, restructured in a Q-and-A format. Finally, we append the instructions to identify the real-world vulnerability example.\nBy the end, we obtain a generalized template for a particular CWE, developed through the iteration through Phases 1-4. This process is repeated for each CWE and for each setting within the NL+COT"}, {"title": "A.4 CVEFixes C/C++ Improvements", "content": "We can further improve performance with GPT-4o. With this model we report 63.8% accuracy, 67.7% F1 on CWE-190. We report 77.8% accuracy and 50% F1 for CWE-78. This is on the dataset CVEFixes C/C++."}, {"title": "A.5 NL+CoT (S3) Sample Prompt (CWE-78)", "content": "\u00ab\u00ab\u00ab\u00ab\u00ab\u00abPROMPT START\u00bb\u00bb\u00bb\u00bb\u00bb\u00bb You are a security expert. You will analyze new examples for vulnerabilities.\nUse the following instructions:\n\u2022 User Input Handling: Scrutinize how user inputs are received and processed. Pay particular attention to any point where user input could be included in commands to be executed at the OS level. This includes web form inputs, URL parameters, headers, cookies, and any external source that a user can manipulate.\n\u2022 Command Construction: Identify all instances where system commands are constructed and executed. Look for functions like exec(), system(), Runtime.exec() in Java, ProcessBuilder in Java, subprocess.Popen() in Python, or similar functions in other languages. Review how these commands are constructed, especially any concatenation operations that involve user-controlled data.\n\u2022 Validation and Sanitization: Check for the presence and effectiveness of input validation and sanitization routines. Ensure that data used in command execution undergoes strict validation against a whitelist of allowed characters or patterns. Sanitization should remove or neutralize potentially malicious characters (e.g., semicolons, ampersands, and other shell metacharacters).\n\u2022 Use of Safe APIs: Verify whether the code uses safer API alternatives that avoid shell interpretation, such as argument arrays in exec() calls instead of single string commands. These APIs are designed to reduce the risk of injection by handling each argument as separate from the command itself, thereby preventing the incorporation of shell control characters from user input.\n\u2022 Parameterized Commands: For languages and frameworks that support it, check if commands are parameterized or templated in a way that separates code from data. This pattern is similar to using prepared statements in SQL and ensures that user input cannot be interpreted as part of the command.\n\u2022 External Libraries and Dependencies: Review the use of external libraries or dependencies that might introduce command execution functionality. Ensure these libraries are reputable, well-maintained, and do not have known vulnerabilities that could be exploited for command injection.\nQ: Check for any vulnerable sections of code in the new example, then determine whether the new example is vulnerable or non-vulnerable based on the tests."}, {"title": "A.6 NL+CoT (S1) Sample Prompt (CWE-190)", "content": "\u00ab\u00ab\u00ab\u00ab\u00ab\u00abPROMPT START\u00bb\u00bb\u00bb\u00bb\u00bb\u00bb You are a security expert. You will analyze new examples for vulnerabilities.\nUse the following tests:"}, {"title": "A.7 Cost Analysis", "content": "Table 6 provides the average token count for each experiment using the gpt-3.5-turbo tokenizer. An experiment in setting 1 means that it is carried out using 23 samples; an experiment in setting 2 means that it is carried out using all available samples."}, {"title": "B Refined Manual Analysis Appendix", "content": null}, {"title": "B.1 Refined Analysis: Strengths and\nLimitations in CWE-78", "content": "We conducted an analysis to understand why the LLM demonstrated significantly better performance in addressing CWE-78 vulnerabilities.\nCWE-78 mainly refers to the type of vulnerability which occurs when external user inputs are directly passed into system commands without proper sanitization as these inputs can be manipulated by the attacker to append additional arbitrary commands with vulnerable application to execute on the host operating system, which can lead to dangerous consequences like data theft, data deletion, or interruption of services. Vulnerabilities which were predominantly featured in the CWE-78 dataset are:\n1) Unsafe command construction using string interpolations or concatenations with unsanitized and unvalidated user-controlled inputs.\n2) Using shell commands to execute user-controlled inputs.\nMost of the samples accepted user inputs directly and used them in constructing commands by interpolation or concatenation. Attackers can manipulate these inputs by adding special metacharacters like (e.g., semicolons ';', ampersands '&', pipes 'l') which can be interpreted by the shell as additional commands leading to dangerous consequences. If an input variable accepts the value 'input; rm rf /' without sanitization, the ';' will work as a command separator, leading to delete all the files and directories in the system by the appended command if executed with root privileges. The LLM successfully identified all such cases and suggested constructing commands with lists of arguments instead to prevent such cases.\nOn the other hand, some of the sample codes explicitly demonstrated executing commands containing unsanitized user inputs through the system shell by setting 'shell=true' which increases the risk of shell command injection. It is a safer practice to set 'shell=false' to prevent the shell from interpreting command codes and to use alternatives like 'subprocess.Popen' for executing commands.\nThe model identified some fixed samples as vulnerable because of missing the full context of the samples. Samples that contained commands constructed by directly interpolating or concatenating user-controlled inputs (apparently unsanitized and unvalidated to the LLM due to not having the full code as context) into the command string were identified as vulnerable or potentially vulnerable. Moreover, some fixed samples, which resolved the corresponding unfixed issues, did not guarantee the absence of other CWE bugs to the LLM because of missing the complete context. For instance, certain fixed samples addressed vulnerabilities arising from accepting filepaths from user inputs and directly injecting them into commands through string concatenation but both samples used 'os.system' to execute the command in the default shell of the operating system. This can be considered an unsafe practice if the filepath is not sanitized. However, these sanitization procedures were not often shown in the samples fed to the LLM, thus leading it to identify them as vulnerable. In some fixed samples, while there were no risk of OS Command Injection, but contained codes which can potentially lead to"}, {"title": "B.2 Refined Analysis: Limitations in\nCWE-190 and CWE-476", "content": "As shown in Table 5, GPT-3.5 performs poorly on the CWE-190 and CWE-476 datasets in NL+CoT (S1) setting.\nIn the CWE-190 experiment, GPT-3.5 does not change its prediction for each pair of unfixed and fixed samples. In the CWE-476 experiment, GPT-3.5 only changed its prediction for 2 pairs of the unfixed and fixed samples (one from non-vulnerable to vulnerable, and the other from vulnerable to non-vulnerable). This indicates that, in these settings, GPT-3.5 is not carrying out the required reasoning for the assigned task. The model identified some samples as vulnerable for mainly two reasons:\n1) Similar to before, the LLM did not have full-context of the samples. The samples may call external functions that are not included within the sample. These functions may have semantics known to the developers that would prevent the occurrence of the CWE's in interest, but the LLM would not have such context.\n2) The LLM is pedantic about inserting overflow checks (for CWE-190) and NULL checks (for CWE-476) for each declared variable. In some cases, if some intermediate variables do not have a corresponding check, the LLM would flag these as possibly vulnerable even if they are not the root cause.\nOne key observation is that while the LLM is pedantic about inserting checks for declared variables, it often overlooks scenarios where fields within structs may be the culprit, leading to suboptimal results in this test. For example, for many samples of CWE-476, the variables are not NULL, but the fields accessed within the variables are possible NULL-valued pointers. This oversight may stem from the model's insufficient comprehension of the data structures employed in the program samples. This leads the model to predict most samples as non-vulnerable. In the cases where it does predict samples as vulnerable, it is usually due to lacking context or assumptions about intermediate variables."}, {"title": "B.3 Refined Analysis: Limitations in\nCWE-416", "content": "We analysed the GPT-4 results in the NL+COT (S1) setting for CWE-416. It occurs when a program continues to use a pointer or reference to a memory location after it has been freed. There were many cases where vulnerability was not detected accurately with valid explanation, which can be attributed to the following reasons:\n1) This query requires deep understanding of context from code-above-code behavior that might not be evident without an in-depth exploration of the surrounding codebase or without understanding the historical context of the changes, which is not always available in the snippets.\n2) Its efficacy is contingent on the model's foundational understanding of the underlying principles of memory management and allocation, areas where even minor inaccuracies or oversimplifications can lead to significant misjudgments.\nThere were samples in fixed and unfixed samples that the models was able to identify correctly as it addressed the intricate relationship between memory lifecycle management and code execution pathways. The experiment may reveal hidden vulnerabilities that arise from timing differences between memory release and subsequent access by focusing on memory allocation, use, freeing, and possible misuse following memory release. This strategy was essential because a lot of the vulnerabilities that were covered relied on taking advantage of the window of time during which freed memory is still available through dangling pointers but has not yet been reassigned."}]}