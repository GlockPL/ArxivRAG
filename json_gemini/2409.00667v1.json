{"title": "Comprehensive Botnet Detection by Mitigating Adversarial Attacks, Navigating the Subtleties of Perturbation Distances and Fortifying Predictions with Conformal Layers", "authors": ["Rahul Yumlembama", "Biju Issac", "Seibu Mary Jacob", "Longzhi Yang"], "abstract": "Botnets are computer networks controlled by malicious actors that present significant cyber-security challenges. They autonomously infect, propagate, and coordinate to conduct cyber-crimes, necessitating robust detection methods. This research addresses the sophisticated adversarial manipulations posed by attackers, aiming to undermine machine learning-based botnet detection systems. We introduce a flow-based detection approach, leveraging machine learning and deep learning algorithms trained on the ISCX and ISOT datasets. The detection algorithms are optimized using the Genetic Algorithm (GA) and Particle Swarm Optimization (PSO) to obtain a baseline detection method. The Carlini & Wagner (C&W) attack and Generative Adversarial Network (GAN) generate deceptive data with subtle perturbations, targeting each feature used for classification while preserving their semantic and syntactic relationships, which ensures that the adversarial samples retain meaningfulness and realism. An in-depth analysis of the required L2 distance from the original sample for the malware sample to misclassify is performed across various iteration checkpoints, showing different levels of misclassification at different L2 distances of the Pertrub sample from the original sample. Our work delves into the vulnerability of various models, examining the transferability of adversarial examples from a Neural Network surrogate model to Tree-based algorithms. Subsequently, models that initially misclassified the perturbed samples are re-trained, enhancing their resilience and detection capabilities. In the final phase, a conformal prediction layer is integrated, significantly rejecting incorrect predictions 58.20% in the ISCX dataset and 98.94% in the ISOT dataset.", "sections": [{"title": "1. Introduction", "content": "Cybercriminals can infect an organization's computing device or machine with a 'bot binary' executable using traditional attack techniques such as viruses and worms distributed through user downloads and email links. The bot binary runs silently in the background of a user machine, turning it into a 'zombie' waiting for commands from a Command and Control (C&C) server controlled by the botmaster or another bot. A botmaster controls a group of zombies and forms a botnet to perform distributed computing tasks [1]. Cybercriminals have shifted in favour of botnet usage, with a recent report recording over 10,000 C&C servers were added to the blocking lists [4]. For instance, in 2016, a DDoS attack carried out on DNS provider Dyn using the Mirai Internet of Things (IoT) Botnet caused users of the DNS service to have issues resolving domain names, ultimately causing many well-known sites to become inaccessible [5]. Cybercriminals frequently resort to botnets to earn profits, mainly through Cybercrime-as-a-Service (CaaS), where they rent out parts of a botnet to clients [7]. Botmasters must ensure that their bot evades the Intrusion Detection Systems (IDSs) to maintain operational persistence, allowing them to gain buyers' trust. Doing so increases the potential for botnet growth, attracting higher profits from buyers requiring a large amount of cumulative bandwidth and processing power.\nHistorically, the first line of defence against such threats has been Network Intrusion Detection Systems (NIDS). Traditional signature-based NIDS has revealed shortcomings, particularly in identifying novel or modified botnets. Consequently, a recent shift has been towards leveraging machine learning algorithms within NIDS. Machine learning systems promise to detect known threats and unearth previously unknown or zero-day threats. However, as with most advancements, this shift has vulnerabilities. Recent research underscores the susceptibility of machine learning models to adversarial attacks, especially evasion attacks. These manipulations create deceptive instances that could pass undetected, presenting a significant challenge. While there has been extensive research on adversarial attacks in fields like computer vision, the implications for NIDS still need to be explored more in detail. The features used for training the machine learning-based NIDS system are central to its effectiveness. Therefore, understanding and assessing the vulnerabilities that arise when adversaries manipulate these features is paramount. However, the extent to which adversaries can exploit these features and the magnitude of feature-based perturbations needed to compromise NIDS remains unexplored and, therefore, raises pressing questions: How susceptible are these features to adversarial alterations? Can subtle feature manipulations successfully bypass a sophisticated machine-learning detector? In this research, we delve into these questions, aiming to discover the vulnerabilities of each feature used in the training. A common approach to generating adversarial examples involves using a surrogate model. When we choose a neural network as the surrogate model, an essential question arises: Will these adversarial examples maintain their deceptive potency against different architectures? In this research, we aim to find out the extent of transferability of the adversarial samples.\nFirst, we train both our machine learning and deep learning algorithms. To ensure the optimal performance of these models, we employ Genetic Algorithms (GA) and Particle Swarm Optimisation (PSO) to fine-tune the hyperparameters meticulously. This process"}, {"title": "2. Related Ideas and Work", "content": "Botnets often communicate with Command and Control (C&C) servers or other bots through network interactions. In order to identify such communications, Network Intrusion Detection Systems (NIDS) like Snort are deployed to identify them. These systems use signature-based detection to check each network packet against predefined signatures. A signature-based detection system matches predefined signatures to each packet's signature and generates an alert if a match is found, deeming it as a malicious behaviour [8]. However, this requires analysis of every network packet, which is computationally intensive [9] and shown in research to suffer from a large proportion of packet drops when saturated at higher network speeds [10]. Furthermore, experimentation shows that Snort's false positive rate (FPR) can be high, with the default rule sets, rendering it challenging to analyze or trust the alerts [10]. On the other hand, as these detection methods mature, bot developers innovate, too, crafting ways to dodge detection and ensure their bots remain stealthily active. Techniques include encrypting communication payloads and fragmenting packets. In sophisticated techniques like the polymorphic blending attack, a bot first understands the typical traffic profile of a network, it then mimics this profile when communicating, making activities blend seamlessly with regular traffic. In response to these challenges, machine learning is increasingly employed in NIDS. The advantage of machine learning-based approaches is their ability to learn and recognize complex patterns, often surpassing traditional signature-based methods. Such systems can adapt to evolving threats, reducing false positives and increasing detection rates. They are especially effective when previously unknown or zero-day threats emerge, as they can detect anomalies without relying on predefined signatures.\nRecent work such as Chen et al. explored conversational features within the CTU-13 botnet dataset scenarios [12]. They utilized multiple classifiers, such as Decision Tree, BayesNet, and the Random Forest Classifier. Velasco-Mata et al. delved into feature selection by employing the Information Gain and Gini Importance techniques [13]. Their efforts resulted in three pre-selected subsets containing five to seven features. On evaluating these subsets with Decision Tree, Random Forest, and k-Nearest Neighbors models, the Decision Trees with a five-feature set emerged as the top performers, achieving an impressive mean F1 score of 85%. Dollah et al. focused their investigation on the detection of HTTP-based botnets [14]. They curated a labelled dataset by merging botnet traffic with genuine web browsing traffic. Their evaluation metrics spanned Accuracy, Precision, Recall, and FPR while utilizing classifiers like Decision Tree, k-nearest Neighbour, Na\u00efve Bayes, and Random Forest. Remarkably, the k-Nearest Neighbours classifier stood out, registering an average accuracy of 92.93%. Decision Tree's performance was superior to the Random Forest for the HTTP datasets they employed. Haddadi et al. Discriminatorally evaluated the per-"}, {"title": "3. Preliminaries", "content": "This section describes the dataset, the feature extraction technique used, and the process of classification of network flow into Malware and Benign."}, {"title": "3.1. Datasets", "content": "In this research, we employed two distinct datasets: ISOT [36] and ISCX[37]. The ISOT dataset is a merged dataset of malicious and normal traffic datasets [17]. The malicious traffic from the French chapter of the Honeynet Project comprises activity from the Storm, Waledac and Zeus botnets. The normal background traffic datasets are the product of merged datasets from Ericsson Research and the Lawrence Berkeley National Laboratory. This data includes traffic from HTTP web browsing, gaming and P2P clients. The ISCX Botnet dataset includes network traffic from a range of botnets such as Neris, Rbot, Virut, NSIS, SMTP, Zeus and normal activity traffic, which are captured by replaying over a network testbed topology[18]. The ISOT dataset has been divided into two parts, with a 70/30 split for training and testing purposes, respectively. From the training portion, we further reserve 5% as a validation set to optimize the performance of the machine learning models. Similarly, for the ISCX dataset, although dedicated training and testing files were provided in the original publication, we also reserve 10% of the training set as a validation set. We tune all our model's hyperparameters using the validation set. In our experiment, the random seed is set to 42 to ensure the experiments are reproducible."}, {"title": "3.2. Feature Extraction and Feature Selection", "content": "In order to work with the ISOT and ISCX datasets, which are only available as .pcap files, feature extraction into network flow is necessary. Following the method used in [25], we extracted 32 features and their corresponding labels. The extracted features are in Table 1. The complete descriptions of all the features for both the base and extended datasets can be found in the 'ra' man page [26].\nWe employed the Information Gain (IG) metric to determine the most relevant features. Information Gain (IG) is a statistical measure used in machine learning and information"}, {"title": "3.3. Botnet Classification", "content": "To detect botnet network flow and compare the results, we experimented with three classifiers: Decision Tree (DT), Random Forest (RF), and Neural Network (NN). Based on the feature selection described in 3.2, we used the following features to train the classifier: 'SrcWin', 'sHops', 'sTtl', 'dTtl', 'SrcBytes', 'DstBytes', 'Dur', 'TotBytes', and 'Rate'. We initially employed default hyperparameter configurations for DT and RF. We constructed a sequential model for the NN classifier consisting of six layers with nine units each, using the ReLU activation function. Following these were two layers with six units and ReLU activations, culminating in a final layer with a single unit employing the sigmoid activation function. The Adam optimizer and the binary cross-entropy loss function were used to train the neural network. We conducted training for fifty epochs with a batch size of 120. While these results provided valuable insights, we found that hyperparameter tuning was essential to enhance the model's predictive performance. Therefore, we employed three optimization algorithms, Random Search, Genetic Algorithm (GA) and Particle Swarm Optimization (PSO), to further optimize the hyperparameters of the DT, RF, and NN classifiers."}, {"title": "3.3.1. Hyper Parameter Optimization", "content": "To optimize hyperparameters, we start with Random search, which operates by randomly selecting combinations of hyperparameters; we explore random search to establish a baseline"}, {"title": "4. Adversarial Attack on Classification Model", "content": "In order to evaluate the robustness of our machine learning-based Network Intrusion Detection Systems (NIDSs), we crafted adversarial samples. Adversarial samples are instances of data that are intentionally perturbed in such a way as to deceive the model, leading to incorrect predictions. The best model in Section 3 acts as a classifier with no architectural information about the classifier to the attacker. We adopted the C&W [27] and GAN[53] attack, both renowned adversarial attack techniques, to evaluate the robustness of our model against adversarial perturbations on individual features. This section outlines the threat model, constraints associated with crafting these adversarial samples, the background of the C&W and GAN attack and how we adapted it for our problem."}, {"title": "4.1. Threat Model", "content": "In our experimental setup, we operate under a grey-box attack scenario. Here, the attacker has complete knowledge of the dataset and features utilized by the model but lacks access to the model's parameters and architecture. This scenario is plausible, as in practical applications, features and datasets used in ML-based NIDSs are often disclosed through publications or documentation. Our primary objective is to scrutinize the vulnerabilities associated with specific features and investigate the transferability of adversarial samples across different models."}, {"title": "4.2. Problem Definition", "content": "Let $N = {N_1, N_2, ..., N_m}$ be a dataset consisting of network traffic samples, where each sample $N_i$ is characterized by a set of features ${F_1, F_2, ..., F_k}$. The task at hand is to evaluate the robustness of a machine learning-based Network Intrusion Detection System (NIDS) model $M : N \\rightarrow {0,1}$, which classifies traffic as benign ($M(N_i) = 0$) or malicious ($M(N_i) = 1$).\nThe focus is specifically on adversarial attacks targeting malicious network traffic samples with the intention of transforming them into adversarial samples that can evade detection by the NIDS model. For a given malicious sample $N_i$ where $M(N_i) = 1$, the objective is to generate an adversarial counterpart $N'$ that not only deceives the NIDS model into making an incorrect prediction, $M(N') = 0$, but also maintains plausible and coherent feature relationships to resemble legitimate network traffic, adhering to a set of constraints. To accomplish this, we utilize the Carlini & Wagner (C&W)[27] method and GAN[53] method, adapting it to ensure that the perturbations applied to the malicious samples are subtle yet effective in evading detection while preserving the essential characteristics of legitimate network traffic."}, {"title": "4.3. C&W Attack on the Classification model", "content": "The primary goal of the C&W attack is to find a perturbation that minimizes the distance between the original and perturbed input, while ensuring that the perturbed input is misclassified by the model. The objective function used in the C&W attack is defined as:\n$\\text{Objective} = ||x' - x||_2 + c \\times f(x')$ (3)\n$x'$ is the adversarial example, $x$ is the original input, and $c$ is a constant that balances the trade-off between the perturbation magnitude and the misclassification. The function $f(x')$ is designed to capture the misclassification condition and is defined as:\n$f(x') = \\max \\left( \\max_{i \\neq t} (Z(x')_i) - Z(x')_t, -k \\right)$ (4)\nHere, $Z(x')$ represents the logits or scores produced by the model for input $x'$, $t$ is the target class, and $i$ iterates over all classes except the target class. The parameter $k$ plays a pivotal role in determining the confidence with which the adversarial example should be misclassified. The C&W attack makes use of gradient-based optimization to craft the"}, {"title": "4.4. Generating Adversarial Network (GAN) attack:", "content": "The general architecture of GAN consists of a generator and a discriminator. In the context of fooling the NIDS, it also includes a substitute detector. The generator's (G) function creates synthetic data samples that mimic real network traffic. It starts from a random noise vector z and transforms it into a data sample x'. Mathematically, the generator is represented by the following equation:\n$G(z;\\theta_g) = x'$ (8)\nHere, $\\theta_g$ is the parameters of the generator. In our experiment, the generator is a neural network.\nThe Discriminator (D) role is to differentiate between real ( x ) and synthetic (x') data samples. It outputs a probability score indicating the likelihood of a sample being real. The Discriminator is represented by the following equation:\n$D(x; \\theta_d) = p$ (9)\nHere, $\\theta_d$ are the parameters of the Discriminator, and p is the probability score. In our experiment, the Discriminator is a neural network.\nThe Substitute Detector (Classifier, C) is a surrogate model for the actual NIDS model, classifying traffic as benign or malicious. It evaluates the synthetic samples x' and is represented as:\n$C(x'; \\theta_c) = y'$ (10)\nHere, $\\theta_c$ are the classifier's parameters, and $y'$ is the predicted class. The generator aims to produce samples that the Discriminator should classify as real, and the substitute detector"}, {"title": "4.5. Targeting Individual Feature", "content": "Our attack focuses on attacking individual features without altering the entire data instance. A feature mask is employed to achieve these selective perturbations. A feature mask is a binary vector where each entry corresponds to a feature in the data. An entry with a value of 1 indicates that the corresponding feature can be modified, while 0 ensures that the feature remains untouched. This mask guides the adversarial crafting process, ensuring that only the desired features are susceptible to change. For instance, if the goal is to perturb the \"Dur\" (Duration) feature, the feature mask would have a one at the position corresponding to the \"Dur\" feature and 0 for all other positions. When the loss gradient to the input data is computed, this feature mask is applied, effectively zeroing out the gradients for all features except \"Dur\", which ensures that only the \"Dur\" feature is updated during the optimization process while all other features remain constant. The application of the feature mask is straightforward. After computing the gradients, they are element-wise multiplied with the feature mask. This operation ensures that gradients for untargeted features are nullified, preventing any changes during the optimization step."}, {"title": "4.6. Constraint on Generating Adversarial sample", "content": "After the optimization, it is essential to perturb a specific feature and ensure that related features are adjusted accordingly to maintain the coherence and plausibility of the traffic"}, {"title": "4.7. Defense Against Adversarial Sample", "content": "To protect against the Adversarial sample generated, we use Adversarial retraining. Adversarial retraining is a defence strategy adopted to enhance the robustness of machine learning models against adversarial attacks. At its core, adversarial retraining involves incorporating adversarial samples into the training dataset and retraining the model. By doing this, the model learns from genuine data and adversarial perturbations. The hope is that this dual exposure during training will equip the model to better recognize and thwart adversarial attempts during actual deployment."}, {"title": "5. Conformal Prediction in Network Flow Detection", "content": "Reliable and confident prediction is an essential aspect of network intrusion detection systems (NIDS), where the stakes are high and the margin for error is minimal. In this"}, {"title": "6. Evaluation", "content": null}, {"title": "6.1. Experimental Setup and Results for Crafting Adversarial Sample", "content": "To ensure a comprehensive evaluation of the robustness of our models against adversarial attacks, we meticulously crafted our experimental procedure. Our approach utilizes a surrogate model, a neural network we have pre-trained and discussed in the preliminaries section. This surrogate model serves as a substitute for the target model during the adversarial crafting process, providing gradient information that guides the generation of adversarial samples. Initially, we identify instances correctly predicted as malware by our surrogate model for each dataset ISCX and ISOT. By filtering the dataset based on these criteria, we concentrated our adversarial efforts on the samples most confidently identified by our model, thereby offering a test for our model's resilience. The attack was performed using the C&W and GAN adversarial attack method described in Section 4. We employed Algorithm 3 to iteratively generate adversarial examples using C&W attack and Algorithm 4 for GAN attack. This technique was iteratively applied to each of our selected features, namely 'SrcWin', 'sHops', 'sTtl', 'dTtl', 'SrcBytes', 'DstBytes', 'Dur', 'TotBytes', and 'Rate'. This algorithm ensures that only specific features are perturbed, maintaining the semantic integrity of the data. The process involves initializing with relative noise in C&W and generation of sample from random noise in GAN, applying a feature mask, and then optimizing the objective function to craft the adversarial examples. The primary objective was to subtly manipulate these samples so our model would misclassify them benign using the objective function shown in equation 3 and 11."}, {"title": "6.3. Experimental Analysis", "content": null}, {"title": "6.3.1. Evaluating Feature Sensitivity: Average L2 distance and Misclassification Rates", "content": "In order to comprehensively evaluate the subtlety and effectiveness of adversarial perturbations, we focused on two key metrics: the change in average L2 distances between original and adversarial samples and the misclassification rate of the surrogate model. The average L2 distance offers insights into the magnitude of perturbations; lower values signify more imperceptible changes, while higher values may hint at overt alterations. On the other hand, the misclassification rate gauges the success of these perturbations in misleading the model.\nFor the ISCX dataset,the baseline misclassification rate of the surrogate model was identified as 9.45%. During the C&W attack, Starting with the 'SrcWin' feature, a gradual increase in misclassification rates was observed, moving from the baseline of 9.45% and stabilizing at 60%. This uptrend coincided with an increase in the L2 distance, suggesting a direct correlation between the magnitude of perturbation and the likelihood of misclassification. However, after 250 iterations, the generation of adversarial samples ceased, implying a threshold of robustness specific to the 'SrcWin' feature. The 'Shops' feature initially presented a misclassification rate slightly above the baseline at 14%, with an L2 distance starting at 191.0. As iterations progressed, the L2 distance increased marginally to 191.6, yet the misclassification rate intriguingly converged towards the baseline, settling at 9.5%. This pattern indicates the model's capacity to adapt to and mitigate the impact of increasing perturbations on the 'Shops' feature. Conversely, the 'sTtl' and 'dTtl' features exhibited a consistent vulnerability, with misclassification rates persistently at 100%. The L2 distance for these features showed a slight decline, suggesting that even minimal perturbations were sufficient to compromise the model's accuracy, highlighting a significant susceptibility in handling TTL-related attributes. In examining the 'SrcBytes' and 'DstBytes' features, a notable decrease in L2 distance was observed, from 650,000 to 450,000 and from 800,000 to 450,000, respectively. Despite the reduction in perturbation magnitude, the misclassification rates remained around 20%, showing the efficacy of the attack in deceiving the model even with subtler alterations. The 'Duration' feature demonstrated an initial decrease in L2 distance, indicating an attempt to refine the adversarial perturbations. However, the misclassification rate showed minimal variation, suggesting that the model's performance could be influenced by more pronounced perturbations, as evidenced by the subsequent increase in L2 distance. 'TotBytes' and 'Rate' features further elucidated the model's response spectrum to adversarial challenges. 'TotBytes' experienced a substantial reduction in L2 distance, which correlated with an improvement in classification accuracy, as the misclassification rate declined to 17.5%. Meanwhile, the 'Rate' feature underwent a significant drop in L2 distance but only saw a slight decrease in the misclassification rate, suggesting that certain features may require more nuanced adjustments to enhance the model's resilience. In summary, 'SrcWin' and 'Shops' showcase the model's capacity to withstand increasing perturbations; others, notably TTL-related features, highlight intrinsic vulnerabilities that adversarial attacks can exploit.\nDuring the GAN attack of the ISCX dataset, the 'SrcWin' feature experienced an incremental rise in L2 distance, from an ideal baseline of zero to 2.33, which corresponded with an elevation in the misclassification rate from 30% to a stable 60%. This trend indicates"}, {"title": "6.3.2. Experimental Analysis of Adversarial Attack Transferability", "content": "To understand the transferability of the attack in the ISCX dataset, we tested adversarial examples that are successful against the surrogate model on a Decision Tree. The original Decision Tree had a misclassification rate of 3.42%. For features like 'SrcWin' and 'Duration', the Decision Tree held its ground with misclassification rates near this baseline for the C&W attack; however, during the GAN attack, the misclassification increased to 35%. For 'Shops', the rate increased to 7% before settling back to 3.4% during the C&W attack and during the GAN attack, the misclassification increased to around 85% but eventually dropped down to around 15% as the L2 distance became smaller noticeably, the surrogate model, and the Decision Tree displayed same misclassification rate implying the attacks are transferable. The 'sTtl' and 'dTtl' features significantly rose, with rates hovering at around 50% during both attacks, indicating the tree's vulnerability for these features. In contrast, 'TotBytes' and 'Rate' only saw minor increases in both attacks, with rates around 4%. This highlighted that while the surrogate model (NN) was susceptible to the adversarial attack, the Decision Tree largely resisted the attack on most of the features except for sTtl and dTtl in both attacks. Although SrcWin and SHops are hard to attack using C&W attack, with GAN, the two features saw an increase in the misclassification rate.\nFor the ISOT dataset, in the 'SrcWin' feature, the attack pushed the RF model's misclassification rate up to 20% for C&W and 60% for GAN. However, this heightened misclassification dipped back to the 7.7% baseline as we iterated. This swing back to the baseline suggests the C&W attack might lose some of its edge as its perturbed samples draw closer"}, {"title": "6.3.3. Experimental Analysis of Adversarial Retraining and Conformal Prediction", "content": "Upon the completion of the adversarial retraining process, the models displayed remarkable improvement in their capability to correctly identify adversarial samples, as can be seen in Table 11. In the initial state, prior to retraining, the models were unable to correctly classify any of the adversarial samples in both the ISCX and ISOT datasets, misclassifying them all as benign, which changed post-retraining. The Decision Tree model misclassified a mere 121 out of the 1,946,863 adversarial samples in the ISCX dataset, showcasing a significant leap in its ability to detect and correctly classify adversarial inputs. In the case of the ISOT dataset, the Random Forest model demonstrated even more impressive performance, only misclassifying 7 adversarial samples. In addition to this notable improvement in adversarial sample detection, the models also maintained robust performance metrics on the test set, as shown in Table 10.\nConformal prediction has played an indispensable role in enhancing the trustworthiness and reliability of our models, as demonstrated by the comprehensive results laid out in Table 12. By implementing this technique, we have been able to substantially increase our"}, {"title": "7. Limitations", "content": "The enhancement of F1 scores through the implementation of a conformal prediction layer has been demonstrated; however, it comes with a cost of rejecting correct predictions, and future studies can explore the reduction of rejection of correct predictions. The efficacy of both the classifiers and the conformal prediction layer is heavily contingent upon the quality and representativeness of the training data. In scenarios where the training data are not sufficiently diverse or fail to encapsulate the full spectrum of potential attack vectors and normal traffic patterns, the model's performance may not accurately reflect its effectiveness in real-world applications. Despite advancements aimed at enhancing robustness against adversarial attacks, the dynamic and evolving nature of these threats may still pose challenges. Newly developed or previously unseen adversarial strategies could potentially compromise the models. Ongoing adaptation and refinement of the models, informed by the latest adversarial tactics, are imperative to sustain high-security levels. Our investigation into the transferability of adversarial examples highlights significant complexities and dependencies on specific model characteristics. Although our findings provide valuable insights, they represent only a preliminary exploration within a complex, multifaceted research area. Further studies are needed to elaborate on these dynamics, potentially broadening the investigation to include diverse models and adversarial conditions."}, {"title": "8. Time Complexity Analysis", "content": "This section delves into the computational complexities associated with various components of our study, providing insights for understanding the computational demands of the employed methodologies.\nThe primary classifiers under consideration are the Decision Tree (DT) and Random Forest (RF), utilized respectively in the ISCX and ISOT datasets. The Decision Tree exhibits a time complexity of O(N\u00b7 Mf \u00b7log(N)), where N denotes the number of training examples, and Mf represents the number of features. On the other hand, the Random Forest, an ensemble of multiple decision trees, has a time complexity of O(N\u00b7 Mf \u00b7 log(N) \u00b7 TRF), with TRF indicating the number of trees within the forest.\nIn hyperparameter optimization, the Genetic Algorithm (GA) was identified as the optimal choice through comparative analysis with Particle Swarm Optimization (PSO). The GA initializes populations with a complexity of O(P\u00b7 nhp), where P stands for the population size and nhp signifies the number of hyperparameters. Each population's evaluation demands O(P\u00b7 E) time complexity, with E encapsulating the classifier training and fitness function computation. The tournament selection process in GA adds a complexity of O(P.TGA), and the crossover operation necessitates O(P\u00b7nhp) time. Aggregating these complexities, the overall per generation complexity of the GA algorithm sums up to O(G\u00b7 P \u00b7 (3nhp + E + TGA)), with G representing the number of generations. When considering the Decision Tree classifier inclusive of hyperparameter optimization, the resultant time complexity is O(N\u00b7Mf.log(N)) + O(G\u00b7 P \u00b7 (3nhp + EDT + TGA)). Similarly, for the Random Forest classifier, the overall time complexity is expressed as O(N\u00b7Mf.log(N) \u00b7 TRF) + O(G\u00b7 P \u00b7 (3nhp + ERF + TGA)).\nThe C&W attack methodology involves multiple computational steps. The calculation of the L2 norm, with D representing the input space's dimensionality, incurs a time complexity of O(D). Neural network output evaluation, denoted as Z(x'), and subsequent operations result in a complexity of O(MNN + C), where Man encapsulates the neural network evaluation complexity, and C represents the number of classes. The gradient computation through backpropagation, alongside additional operations for the perturbation term, introduces a complexity of O(D + MNN). Factoring in the iterative nature of the Adam optimization algorithm, denoted by I iterations, the cumulative time complexity of the C&W attack approximates to O(I \u00b7 (D + MNN + C')).\nThe GAN attack consists of training two neural network Generator and Discriminator. The time complexity of neural network is $\\Ot(\\t.\\sum_{i=1}^{n-1} x_i x_{i+1})$. Let the generator G have $n_g$ layers with $g_1, g_2, ..., g_{n_g}$ nodes in each respective layer and the discriminator D has $n_d$ layers with $d_1, d_2, ..., d_{n_d}$ nodes in each respective layer. If t is the number of training examples, n is the number of epochs. Assuming the discriminator is updated k times for each update of the generator, the total complexity is $O \\(tn \\(k.\\sum_{i=1}^{n_d-1} d_i d_{i+1} + \\sum_{i=1}^{n_g-1} g_i g_{i+1} g_{i+1}))$.\nIn conformal prediction, the calibration set, consisting of $N_c$ instances, necessitates probability estimates and conformal scores computation, resulting in a time complexity of $O(N_c. Mf)$. The subsequent threshold determination requires sorting the conformal scores, adding a $O(N_c.log(N_c))$ complexity. For the test set, comprising $N_t$ instances, the formulation of prediction sets demands a time complexity of $O(N_t\\cdot D_t)$ for Decision Trees and $O(N_t\\cdot TRF\\cdot D_{RF})$ for Random Forests. This results in a total time complexity of $O(N_c \\cdot Mf + N_c \\cdot log(N_c) + N_t \\cdot D_t)$ for Conformal Prediction with Decision Trees and $O(N_c \\cdot Mf + N_c \\cdot log(N_c) + N_t \\cdot TRF\\cdot D_{RF})$ for Random Forests. The final time complexity of the whole process will be $O(N \\cdot Mf \\cdot log(N) + G \\cdot P \\cdot (n_{hp} + E_{DT} +T_{GA}) + N_c \\cdot Mf + N_c \\cdot log(N_c) + N_t. D_t + I \\cdot (D + M_{NN} + C'))$ for Decision tree and $O(N \\cdot Mf \\cdot log(N) \\cdot T_{RF} + G \\cdot P. (n_{hp} + E_{RF} + T_{GA}) + N_c \\cdot Mf + N_c \\cdot log(N_c) + N_t \\cdot T_{RF} \\cdot D_{RF} + I \\cdot (D + M_{NN} + C'))$ for Random Forrest.\nIn the case of the Decision Tree (DT) and Random Forest (RF) classifiers, the convergence of the algorithm where the model's performance stabilizes and does not significantly improve with further training is highly influenced by the tree depth, number of trees, and"}, {"title": "9. Conclusion", "content": "Our research has made significant strides in advancing network security defences, particularly in the realm of botnet detection and adversarial sample mitigation. By leveraging both machine learning and deep learning algorithms and fine-tuning their hyperparameters with Genetic Algorithms and Particle Swarm Optimization, we established a strong foundation and achieved optimal predictive accuracy. Our in-depth analysis of feature vulnerabilities using GAN and C&W attack method revealed crucial insights, allowing us to maintain meaningful semantic and syntactic relationships even when features were manipulated. This meticulous approach to adversarial example generation and our investigation into their transferability across different model architectures shows the breadth and complexity of the threat landscape. The introduction of conformal prediction to Network Intrusion Detection Systems marked a significant innovation in our research. This robust, statistically grounded method enhanced the reliability of our model's predictions by confidently accepting correct predictions and crucially rejecting incorrect ones. The impressive rejection rates of 58.20% for incorrect predictions in the ISCX dataset and 98.94% in the ISOT dataset speak volumes about the efficacy of this approach. In future, we plan to explore additional adversarial attack methods that could offer a broader understanding of potential vulnerabilities in Network Intrusion Detection Systems (NIDS). By exposing our models to a broader array of attack vectors, we can further strengthen their resilience and improve"}, {"title": "10. Data availability", "content": "The datasets used in this study are essential for validating our proposed methodologies and are publicly available, ensuring transparency and reproducibility of our results. Specifically, we utilized the following datasets: ISOT Botnet Dataset [36] and ISCX 2014 Botnet Dataset [37]. The dataset can be downloaded after requesting the owner of the dataset."}]}