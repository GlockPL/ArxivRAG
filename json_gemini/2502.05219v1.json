{"title": "Enabling External Scrutiny of AI Systems with Privacy-Enhancing Technologies", "authors": ["Kendrea Beers", "Helen Toner"], "abstract": "This article describes how technical infrastructure developed by the nonprofit OpenMined enables external scrutiny of AI systems without compromising sensitive information.\n\nIndependent external scrutiny of AI systems provides crucial transparency into AI development, so it should be an integral component of any approach to AI governance. In practice, external researchers have struggled to gain access to AI systems because of AI companies' legitimate concerns about security, privacy, and intellectual property.\nBut now, privacy-enhancing technologies (PETs) have reached a new level of maturity: end-to-end technical infrastructure developed by OpenMined combines several PETs into various setups that enable privacy-preserving audits of AI systems. We showcase two case studies where this infrastructure has been deployed in real-world governance scenarios: \"Understanding Social Media Recommendation Algorithms with the Christchurch Call\" and \"Evaluating Frontier Models with the UK AI Safety Institute.\" We describe types of scrutiny of AI systems that could be facilitated by current setups and OpenMined's proposed future setups.\nWe conclude that these innovative approaches deserve further exploration and support from the AI governance community. Interested policymakers can focus on empowering researchers on a legal level.", "sections": [{"title": "Introduction", "content": "From the recommender system underlying Facebook's newsfeed to the large language model powering ChatGPT, large-scale AI systems are now operating at global scale and shaping the lives of billions of users. The extraordinary reach of these systems has raised widespread concerns about their risks, from personalized misinformation to biased decision-making algorithms to novel cyberattacks against critical infrastructure.\nAlthough experts disagree on the extent and relative severity of different AI threats, one thing they almost all agree on is the need for independent external scrutiny of consequential AI systems [15, 17, 3, 10]. Given the scale and influence of these systems, there is a clear public interest in better understanding how they work and how they impact society.\nIn practice, however, efforts to facilitate scrutiny by external researchers have faced challenges. The owners of AI systems have been reluctant to grant access to third-party researchers due to legitimate concerns about compromising the data privacy of their users, the security of their systems, or the value of their intellectual property. Although research teams have managed to carry out studies, current approaches are difficult to scale; they typically require lengthy legal agreements, access to a secure physical facility, and/or significant limitations on"}, {"title": "The Technology", "content": "Over the past few decades, researchers in a variety of disciplines have developed techniques for privacy-preserving study of sensitive information. Recently, these fields\u2014ranging from cryptography to distributed systems to machine learning\u2014have embraced the umbrella term \"privacy-enhancing technologies.\u201d Only even more recently have these technologies been incorporated into end-to-end infrastructure that can flexibly adapt to different use cases.\nOpenMined's end-to-end technical infrastructure draws on several well-established privacy-enhancing technologies, combining them into various setups that enable various kinds of privacy-preserving research. These technologies, including secure enclaves [18], secure multi-party computation [9], zero-knowledge proofs [16], federated learning [8], and differential privacy [13], enable users to verify their trust in a system's relevant privacy and security properties.\nThe core component of OpenMined's technical infrastructure is an open-source software library called PySyft [14] that works with the popular programming language Python. The infrastructure supports the following core workflow: a researcher remotely proposes questions to a model owner, the model owner approves the researchers' questions, and then the researcher receives the answers to the questions without learning anything else about the model owner's proprietary systems. OpenMined has proposed a range of specific setups, all based on this core workflow, that can vary depending on budget, how much the model owner and the researcher trust each other, and what types of questions the researcher wants to ask [19]."}, {"title": "Case Study 1: Understanding Social Media Recommendation Algorithms with the Christchurch Call", "content": "In the aftermath of the 2019 Christchurch terrorist shootings in New Zealand, a community of more than 120 governments, online service providers, and civil society organizations came together as the Christchurch Call to tackle the effects of extremist content online. In 2022, the Christchurch Call launched the Initiative on Algorithmic Outcomes (CCIAO) to develop privacy-enhancing software infrastructure that can enable independent researchers to scrutinize"}, {"title": "Case Study 2: Evaluating Frontier Models with the UK AI Safety Institute", "content": "More recently, OpenMined partnered with the United Kingdom AI Safety Institute (UK AISI) and the frontier AI company Anthropic to run a trial of a method for conducting safety evaluations of frontier AI models while keeping sensitive information private [20]. Anthropic provided the AI model, and UK AISI provided evaluation code that made use of a dataset of questions and answers about biology. The setup was designed to enable mutual secrecy, meaning that the contents of the biology dataset remained private to UK AISI while the AI model weights remained private to Anthropic. Anthropic did have access to the evaluation code that UK AISI used to feed inputs from the dataset into the model. For the purposes of this exercise, both the AI model (GPT-2 [4]) and the biological dataset (CAMEL Biology [6]) were public assets, but the exercise served as a trial run for a higher-stakes scenario in which both the model and the evaluation data would need to be private.\nThe exercise successfully demonstrated that a government entity and an AI company can negotiate and enforce shared governance over evaluations of frontier models. Specifically, the focus was on ensuring that each party (1) received all the information it needed, and (2) could approve or deny all of the necessary computations during the evaluation. Scaling up this setup to accommodate the largest frontier models will"}, {"title": "Future Directions for Privacy-Preserving Scrutiny of AI Systems", "content": "OpenMined's existing technical setups have the flexibility to apply to a variety of paradigms of external scrutiny of AI systems. The simple setup for Case Study 1 enables researchers to analyze user logs that reveal trends in any AI system's real behavior when deployed-for example, whether a recommender system has a partisan lean, or whether a chatbot gives toxic responses. Using secure enclave technology to allow researchers to bring their own datasets, as shown in Case Study 2, does not just enable government entities to leverage datasets that are kept secret for national security reasons: it could also make it more feasible for auditors to operate as private businesses by using audit prompts as intellectual property, and prevent AI companies from \"teaching to the test\" on audits by training their models on audit prompts.\nOpenMined's proposals for more sophisticated setups open up ways to scrutinize AI systems with stronger privacy and verification guarantees. OpenMined is currently developing features that will allow researchers to keep their code private in addition to their data; this will empower researchers to conduct more sophisticated research without as much oversight by model owners. Future setups could also include verification mechanisms to prevent model owners from gaming audits by deleting a model's prediction logs or switching out an audited model for an unaudited one.\nIt is worth noting that OpenMined's technical infrastructure can facilitate not just AI research, but also privacy-preserving research on digital systems more broadly. For example, OpenMined recently kicked off a beta program with Reddit that will enable researchers to responsibly access Reddit data in bulk [5]. OpenMined is also supporting the national statistical agencies of the US, Canada (StatCan), and Italy (IStat) in conducting joint privacy-preserving analyses of their data [11]."}, {"title": "Conclusion", "content": "Now that privacy-enhancing technologies have reached a new level of maturity, AI model owners can no longer use privacy, security, and IP as conclusive excuses for refusing access for external researchers. This means that interested policymakers can now focus on empowering researchers on a legal level. One aspect of empowering researchers is promoting legal safe harbor for AI auditors, i.e. protecting them from account suspensions and legal reprisal [7]. A second aspect is providing legal backing for research projects that AI model owners may wish to decline. Although the above case studies demonstrate that some AI model owners are willing to approve external research projects, many model owners may not be so willing; recall that, in deployments so far, OpenMined's workflow has given model owners the right to approve or deny all research questions. Legislation could serve as a backstop by requiring AI model owners to comply with important research nonetheless. For example, the proposed Platform Accountability and Transparency Act (PATA) would require social media companies to make internal information available to researchers whose projects have been vetted by the National Science Foundation for this purpose [12].\nIn summary, external scrutiny of AI systems provides crucial transparency into AI development, so it should be an integral component of any approach to AI governance. Historically, external researchers have struggled to gain access to AI systems because of AI companies' valid concerns about security, privacy, and intellectual property. But now, trustworthy privacy-preserving technical solutions for external scrutiny of AI systems have succeeded in real-world governance scenarios. These innovative approaches deserve further exploration and support from the AI governance community."}]}