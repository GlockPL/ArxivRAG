{"title": "LLM Hallucination Reasoning with Zero-shot Knowledge Test", "authors": ["Seongmin Lee", "Hsiang Hsu", "Chun-Fu (Richard) Chen"], "abstract": "LLM hallucination, where LLMs occasionally generate unfaithful text, poses significant challenges for their practical applications. Most existing detection methods rely on external knowledge, LLM fine-tuning, or hallucination-labeled datasets, and they do not distinguish between different types of hallucinations, which are crucial for improving detection performance. We introduce a new task, Hallucination Reasoning, which classifies LLM-generated text into one of three categories: aligned, misaligned, and fabricated. Our novel zero-shot method assesses whether LLM has enough knowledge about a given prompt and text. Our experiments conducted on new datasets demonstrate the effectiveness of our method in hallucination reasoning and underscore its importance for enhancing detection performance.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have shown remarkable ability in generating text on various topics [35, 32]. However, they often produce hallucinations incorrect or unverifiable content that pose significant risks to their practical applications [2]. Detecting these hallucinations is crucial for ensuring reliability [12] yet challenging due to the plausible appearance of the hallucinated text [37].\nResearch on detecting hallucinations in LLM-generated text has explored several approaches, including comparing the text with external knowledge [19, 25, 31], fine-tuning LLMs [40, 36, 18], and training classifiers to identify hallucinations [1, 4, 30]. However, these methods require external knowledge, LLM fine-tuning, or supervised training with hallucination-labeled data. To address these limitations, there has been growing interest in source-free, zero-shot methods that analyze LLM outputs directly. These methods encompass consistency checks [22], uncertainty estimation [42, 5, 16, 3, 38], and prompting LLMs to assess the correctness of the text [6, 43].\nHowever, existing detection methods fail to distinguish between different types and causes of hallucinations [44, 10], which is crucial for accurately detecting and resolving them. To be specific, LLM-prompting methods may randomly guess the correctness of text when the LLM lacks relevant knowledge, while most uncertainty-based methods cannot identify errors caused by the inherent randomness of the LLM [29, 7]. Differentiating the underlying causes of hallucinations enables more accurate detection and can even suggest potential solutions: if the LLM lacks knowledge, external knowledge can be provided; otherwise, responses can simply be regenerated.\nTo fill this gap, we categorize LLM-generated text into three types: aligned, misaligned, and fabricated (Table 1). Misaligned text arises from sampling randomness or dependencies on previous tokens [29, 7, 41], while fabricated text is generated when the LLM lacks relevant knowledge [10, 44]. Based on this categorization, we propose a new task, hallucination reasoning, which aims to classify LLM-generated text into one of these three types. We contribute:"}, {"title": "2 Related Work", "content": "Hallucination reasoning. Efforts have been made to investigate the causes of hallucinations by inspecting data, training algorithms, and the inference process (Sec 3 of [10]). The primary issues during inference include LLM's insufficient knowledge and overconfidence, its tendency to prioritize user preferences over factual accuracy, inherent randomness in the generation process, and dependency on earlier tokens (Sec 4 of [44]). Based on the literature, we categorize hallucinations into two key types: (1) fabrication, which encompasses lack of knowledge and overconfidence, and (2) misalignment, attributable to randomness or dependency on earlier tokens.\nHallucination detection. Some approaches have verified the factualness of LLM-generated text by comparing it to external knowledge [19, 25, 31]. For example, FactScore [25] checks atomic facts in text against reliable sources. To reduce dependency on external sources, researchers have inspected LLM internals and trained classifiers to differentiate faithful and hallucinated text [4, 1, 30], or fine-tuned LLMs to respond with \"I don't know\" to uncertain questions [40, 36, 18]. However, since these methods require large labeled datasets, others assessed correctness through prompting [6, 17, 43] or by checking generation consistency [22, 38, 39, 3], but these may fail when LLMs fabricate with overconfidence [14]. Efforts to identify the prompts that would lead LLMs to hallucinate using uncertainty [11, 27, 42, 5, 9] often overlook hallucinations from random sampling [29, 7, 41]. We propose a new direction to identify hallucinations more accurately and insightfully by understanding their causes without any external knowledge, model training, or impractical assumptions."}, {"title": "3 Hallucination Reasoning", "content": "Background. LLM's text generation involves iterative next-token prediction. For a given prompt, the LLM predicts a token likely to follow the input and appends it to the end. A tokenizer with a token vocabulary set T splits the input prompt into a token sequence $[t_1,...,t_m]$, where $t_i \\in T$. Each token $t_i$ is then mapped to an embedding vector $e_i$ by the token embedding map. The LLM f takes the embedding vector sequence $e_{1:M} = [e_1, . . ., e_M]$ as input and computes the probability of each token $t \\in T$ appearing after each token position i; i.e., $f(e_{1:M}) = P \\in R^{M\\times|T|}$, where $P = [P_1, ..., P_M], P_i \\in R^{|T|}$. Based on $P_M$, a token is sampled from T and added to the end of the input token sequence, resulting in a new input. This process is repeated until a predefined stopping criteria is met, such as an end-of-sequence token or a specified number of tokens."}, {"title": "3.1 Model Knowledge Test", "content": "We design a novel zero-shot method based on recent findings that perturbing the token embeddings of the subject in a statement significantly hinders an LLM from retrieving relevant knowledge about the subject [24]; Fig. 1 illustrates the overall process. For example, if the LLM has enough knowledge about the animal Pika, perturbing the token embeddings of the word Pika in the text \u201cPika is found in rocky areas...\" would lead the LLM to perceive the perturbed text as referring to a different animal, thus preventing it from associating rocky areas with the subject. However, for text about Snakadsau, a non-existent fabricated word, the LLM would not exploit any knowledge but generate random plausible text. Therefore, perturbing Snakadsau would have little impact on text generation. The MKT consists of three steps: (1) identifying the key subject, (2) perturbing the subject tokens' embeddings, and (3) measuring the perturbation's impact on text generation.\nStep 1. Subject Identification. To determine the subject in a prompt, we identify the noun phrase that receives the most attention during text generation. We input the prompt and the generated text into the LLM and compute the attention that each token in the prompt receives. Using the noun chunk extraction function of SpaCy [8] library, we extract noun phrases and evaluate each phrase's attention by summing the attention values of its tokens. The noun phrase with the highest attention is selected as the subject [34].\nStep 2. Subject Perturbation. After extracting the subject, we perturb it by adding Gaussian noise to the embeddings of the subject tokens. Given a prompt P and generated text G with M and N tokens, respectively, we concatenate them into a token sequence (P, G) = [t1,...,tM+N], which is converted into d-dimensional embedding vectors $e_{1:M+N} = [e_1,..., e_{M+N}]$. For the extracted subject S = $[t_\\tau, ..., t_{\\tau+K-1}]$, let $I_s$ be the set of token positions where S occurs in (P, G):\n$I_s = {i|[t_i,..., t_{i+K\u22121}] = S}$.\nWe perturb the subject's embeddings by adding Gaussian noise \\epsilon with zero mean and standard deviation \u03c3 (i.e., $\\epsilon \\sim N(0,\\sigma^2) \\in R^{K\\times d}$) to all occurrences of the subject in $I_s$, i.e., $\\hat{e}_{i:i+K-1} = e_{i:i+K-1} + \\epsilon$, while leaving other tokens unchanged. Then, we input the perturbed embedding vectors $\\hat{e}_{1:M+N}$ to the LLM f to compute the perturbed probability distribution $\\hat{P} = f(\\hat{e}_{1:M+N}) \\in IR^{(M+N)\\times|T|}$. The unperturbed probability distribution is obtained by $P = f(e_{1:M+N})$.\nAs the perturbation strength can be directly controlled by o, we further adjust the strength based on the LLM's familiary with the subject S.Since LLMs tend to fabricate for unfamiliar subjects [21, 15, 25, 13], we aim to yield a small perturbation effect for such subjects. Familiarity is derived using the"}, {"title": "3.2 Alignment Test", "content": "After ensuring the LLM has enough knowledge about (P, G) through MKT, we check if text G aligns with the LLM's knowledge. For the Alignment Test, we directly use SelfCheckGPT [22], which verifies alignment between text and LLM knowledge more effectively compared to other zero-shot methods; Semantic Entropy [16] evaluates the uncertainty of the prompt without considering the text, while Hallucination Score [42] shows limited performance (Table 3)."}, {"title": "4 Experiments", "content": "We require datasets of prompts, LLM-generated responses, and labels, where the label is one of aligned, misaligned or fabricated (cf. Table 1). Since existing datasets only provide binary labels indicating whether a response is hallucinated or not, we utilize existing datasets from [20, 25] and create two new datasets, the NEC and Biography datasets, which have trinary labels. The NEC dataset contains questions across various topics (e.g., sports, animals), with 359 responses each for the aligned, misaligned and fabricated categories, split into validation (70) and test (289) sets. The Biography dataset contains 67 biographies in the validation set (21 aligned, 21 misaligned, 25 fabricated), and 280 in the test set (88, 88, 104 for each label). Throughout the experiments, we adopt the LLM model with LLaMA2-Chat-GPTQ 13B [33]. For detailed information on the datasets, see Appendix B."}, {"title": "5 Conclusion", "content": "We develop a method to classify LLM-generated text into aligned, misaligned, and fabricated to identify the causes of hallucinations and improve existing detection methods. While MKT effectively detects fabricated text, we use SelfCheckGPT for the Alignment Test, which requires multiple text generations and can be time-consuming and computationally expensive. We aim to develop a more efficient and effective technique for the Alignment Test and to evaluate our method on a broader range of datasets."}, {"title": "A Social Impacts Statement", "content": "We expect our method to help end users assess whether to trust an LLM's output by enhancing hallucination detection and providing more details about the causes. Also, hallucination reasoning can assist developers in identifying and correcting erroneous generations. However, since our approach relies on the LLM's internal knowledge rather than real-world facts, there's a risk of amplifying incorrect beliefs embedded in the LLM, which requires careful consideration for deployment."}, {"title": "B Dataset", "content": "In this section, we elaborate on how we construct the NEC and Biography datasets. Both datasets consist of tuples of (prompt, LLM response, type), where the type is one of aligned, misaligned, and fabricated. We also provide example data points in Table 4 and Table 5."}, {"title": "B.1 NEC dataset", "content": "NEC dataset [20] consists of 2,073 questions about existent and 2,078 questions about non-existent concepts covering various topics (foods, sports, countries, animals, medicines, generic) curated to examine LLMs' behaviors asked about unknown questions. While we can ensure that LLM responses for the questions about non-existent concepts are fabricated, we cannot guarantee that the LLM has knowledge to answer all questions about the existent concepts. To identify the questions about which LLM has enough knowledge, we first leave only 1,369 questions about the existent concepts on Wikipedia. Then, for each question, we generate 10 responses with the studied LLM f (LLaMA2-Chat-GPTQ 13B) and evaluate the correctness of each response by prompting GPT-3.5 Turbo [26] with the question, LLM response, and Wikipedia article related to the concept\u00b3. If more than 80% (8 out of 10) of the LLM responses are supported by the Wikipedia article, we consider the question to be known and include the question in our dataset. Then, for each known question, we sample one of the supported responses and add the tuple of the (question, response, aligned) in our dataset. To generate the tuples with misaligned labels, we prompt the LLM f to induce factual contradiction [17]. As a result, we collect 359 data points for each aligned, misaligned and fabricated categories. We split these data into validation and test sets so that 20% of the data points are in the validation set and the rest of them are in the test set, i.e., 70 validation and 289 test date for each class."}, {"title": "B.2 Biography dataset", "content": "Biography dataset [25] consists of people's names on Wikipedia so that we can prompt LLMs to tell a biography of each person. As an LLM's knowledge about each name greatly varies [25], we identify the names that the LLM knows well by generating a biography, masking out the name from the biography, and asking the LLM to guess what the masked name is; the LLM would be able to correctly recover the name only when the LLM-generated biography contains a lot of information so that it can uniquely indicate the person. However, there is a possibility that the LLM contains a little information about the people that are labeled as unknown. To create fake people about which the LLM would completely fabricate, we assign random jobs which do not match with their true jobs. As a result, we collect LLM-unknown names paired with a wrong job; we pair the LLM-known names with their correct jobs for consistency. From the collected name-job pairs, we create questions of \"Tell me a biography of the [job] [name].\" and generate responses using the LLM. While we label biography of fake people are labeled as fabricated, we take additional care to generate aligned biographies of the LLM-known people as biography is easily hallucinated [25]; we collect correct facts that the LLM knows by generating 10 biographies, atomizing each of them, and verifying the correctness of each fact atom using GPT-3.5 Turbo. Then, we generate correct biography based on the correct facts. To generate misaligned biographies, we change 50% of the facts to be contradictory, and construct biographies based on them. We split these data points into validation and test sets so that 20% of the data points are in the validation set and the rest of them are in the test set. As a result, we have 67 validation data points (21 correct text, 21 mistake, 25 fabrication) and 280 test data points (88, 88, 104 for each label)."}]}