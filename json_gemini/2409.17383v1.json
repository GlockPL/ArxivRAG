{"title": "VectorSearch: Enhancing Document Retrieval with Semantic Embeddings and Optimized Search", "authors": ["Solmaz Seyed Monir", "Irene Lau", "Shubing Yang", "Dongfang Zhao"], "abstract": "Abstract-Traditional retrieval methods have been essential for assessing document similarity but struggle with capturing semantic nuances. Despite advancements in latent semantic analysis (LSA) and deep learning, achieving comprehensive semantic understanding and accurate retrieval remains challenging due to high dimensionality and semantic gaps. The above challenges call for new techniques to effectively reduce the dimensions and close the semantic gaps. To this end, we propose VectorSearch, which leverages advanced algorithms, embeddings, and indexing techniques for refined retrieval. By utilizing innovative multi-vector search operations and encoding searches with advanced language models, our approach significantly improves retrieval accuracy. Experiments on real-world datasets show that VectorSearch outperforms baseline metrics, demonstrating its efficacy for large-scale retrieval tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "With the exponential growth of digital text data, efficient methods for searching and retrieving relevant information have become increasingly important. Traditional keyword-based search techniques often struggle to capture the semantic meaning of text, leading to suboptimal search results [1].\nThe increasing volume of unstructured data, spanning diverse media types like images, videos, textual content, as well as various records such as medical data and real estate information, is largely fueled by its extensive use across multiple domains. This surge can be attributed to the widespread adoption of smartphones, smart devices, and various social networking platforms. According to IDC forecasts, by 2025 [2], unstructured data is poised to dominate the data landscape, constituting a staggering 80% [3], [4] of total data volume. This exponential growth, concurrent with the rapid advancements in machine learning, underscores the critical necessity for robust methodologies aimed at converting this unstructured data into feature vectors. Vector embedding, a prevalent technique harnessed in recommender systems for transforming raw data into structured feature vectors, has gained substantial traction in recent years. However, existing paradigms in vector data management predominantly center on vector similarity search, encountering significant challenges in meeting evolving demands due to their inherent limitations, including constrained support for multi-vector queries and suboptimal performance, particularly in handling large-scale and dynamically evolving vector datasets [4].\nThe connection between efficient information retrieval and vector databases is based on the ability of vector embeddings to capture complex semantic relationships within data. This capability is essential for developing advanced IR systems that can provide more accurate and contextually relevant results. However, existing research on vector data management, as documented in prior studies [5]-[7], predominantly centers around enhancing vector similarity search capabilities. Nonetheless, these approaches confront difficulties in addressing evolving needs due to their limited functionalities, such as inadequate support for handling multi-vector queries, and subpar performance in dealing with large-scale and dynamically changing vector datasets [4]. Current systems, such as Milvus [4], offer multi-vector support and are optimized for large-scale vector data management. However, even with Milvus's distributed architecture, handling dynamically changing datasets especially in environments with high-dimensional data can introduce performance challenges, due to the reindexing overhead and query latency. Moreover, while Retrieval Augmented Generation (RAG) [8] frameworks effectively integrate retrieval mechanisms with generative models, they are not specifically designed for real-time updates and high-dimensional indexing in distributed systems. These frameworks do not fully leverage hybrid indexing techniques for optimal performance in multi-vector search.\nIn focusing solely on algorithms, we uncover several limitations of vector similarity search algorithms [5]-[7], [9]. Many methodologies and libraries depend heavily on main memory storage and lack the capability to distribute data across multiple machines, thereby hindering scalability [4], [10]. Additionally, current algorithms are predominantly designed for static datasets and struggle to accommodate dynamic data. This restriction significantly impacts their adaptability to real-world scenarios where data is constantly changing. Moreover, the absence of advanced query processing capabilities in existing solutions further curtails their practical applicability [4]. Sophisticated query processing is essential for handling complex queries involving multiple vectors, which are common in many information retrieval tasks.\nAddressing these limitations is crucial for developing more effective and scalable vector data management systems, which can better support the complex and evolving needs of modern information retrieval applications. Advancements in this area"}, {"title": "B. Proposed Approach", "content": "Our proposed approach, VectorSearch, represents a novel advancement in the realm of information retrieval. It operates as a hybrid system, combining the strengths of vector embeddings and traditional indexing techniques to overcome various limitations encountered in existing algorithms and systems. By integrating advanced methods such as FAISS for efficient distributed indexing, VectorSearch enables seamless management of large-scale datasets across multiple machines. Additionally, VectorSearch incorporates HNSWlib for further optimization and enhancement of search capabilities, ensuring efficient retrieval of relevant documents even in dynamic and evolving environments. This hybrid format empowers VectorSearch to deliver superior performance, scalability. Our VectorSearch algorithm is uniquely designed to handle dynamic data with mechanisms for multi-vector query handling, we enable advanced query processing, facilitating sophisticated search operations beyond mere similarity searches. Leveraging embeddings and vector databases for multi-vector search, we encode text data into high-dimensional embeddings $E = {e_1, e_2,..., e_n}$ and index them in a vector database [11], enabling efficient retrieval of relevant text pieces based on user queries. Furthermore, the search operation in VectorSearch involves finding the nearest neighbors of a query embedding vector q. Let I(E) represent the index structure mapping the embedding vectors to their corresponding positions. The search operation can be denoted as Search(q, I(E)), where q is the query embedding vector. The result of this operation is a set of embedding vectors representing relevant documents.\nWe propose VectorSearch, a hybrid document retrieval framework that integrates advanced language models, multi-vector indexing techniques, and hyperparameter optimization to improve retrieval precision and query time in high-dimensional spaces. Unlike existing solutions, our approach:\n1) We propose innovative Multi-Vector Search algorithms that encode documents into high-dimensional embeddings, significantly optimizing retrieval efficiency. These algorithms leverage advanced techniques in semantic embeddings to represent documents in a high-dimensional vector space (Section III).\n2) We propose an innovative algorithm that optimizes nearest neighbor search using both single- and multi-vector strategies, significantly improving search efficiency (Section III-B).\n3) We implement an innovative strategy that systematically tunes index dimension, similarity threshold, and model selection to optimize the retrieval system (Section III-D)."}, {"title": "II. RELATED WORK", "content": "Previous research on similarity search can be organized into four main categories [4]: tree-based methods [6], LSH-based techniques [4], [5], [12], [13], quantization-based approaches [14], and graph-based algorithms [9]. While previous research primarily focuses on index-centric approaches, VectorSearch distinguishes itself as a comprehensive vector data management system. Beyond mere indexing, VectorSearch integrates query engines, CPU optimization providing a comprehensive solution for efficient and scalable document retrieval. Additionally, it incorporates cache mechanisms [15]\u2013[17] to further enhance performance and response times.\nEmbedding-driven Retrieval presents formidable hurdles for search engines due to the massive scale of textual data involved. Unlike ranking layers, which typically manage a few hundred items per session, the retrieval layer of a search engine must efficiently process trillions of text documents within its index. This extensive scale poses a dual challenge for search engines, involving both serving and training embeddings tailored for textual content [18].\nPrior research in the field of vector similarity search has primarily focused on developing algorithms and systems for efficient retrieval of similar vectors. Existing works [5]\u2013[7], [9] along with their associated Faiss. However, these efforts often suffer from several limitations. Firstly, lacking comprehensive systems capable of managing large volumes of vector data efficiently, they struggle to handle datasets that exceed main memory. Additionally, most existing solutions assume static data once ingested, making it challenging to accommodate dynamic updates. Our proposed algorithm addresses these shortcomings by providing a solution that integrates embeddings, vector databases, and mechanisms for multi-vector query handling.\nThe emergence of various models aimed at enhancing precision and recall in information retrieval tasks. Notably, approaches such as SimCSE, ESimCSE, VaSCL, Prompt-ROBERTa, and CARDS [19] have made substantial strides in improving performance across a spectrum of tasks, including STS evaluations. Our research seeks to address these challenges by presenting models like MiniLM-L6-v2 and BERT-base-uncased [20], [21], which demonstrate competitive precision and recall rates while optimizing query time. NCI [22] requires a significantly larger model capacity to extend to web scale. To address this, the VectorSearch utilizes advanced hybrid format indexing techniques which allow for the seamless management of large-scale datasets, providing scalability without the need for excessively large models. NCI needs improvement to serve online queries in real time. VectorSearch enhances the speed of searches by proposed indexing for efficient high-dimensional vector searches. This significantly reduces query times, NCI faces challenges in updating the model-based index when new documents are added. In contrast, VectorSearch is designed to handle dynamic data."}, {"title": "III. DESIGN AND IMPLEMNTATION", "content": "VectorSearch benefits from a hybrid approach that leverages the strengths of both indexes. HNSWlib's hierarchical [9] structure enables efficient navigation of high-dimensional semantic embedding spaces. This hierarchical structure organizes embeddings into a navigable graph, enabling fast and accurate similarity search. Let $X = {X_1,X_2,...,X_n}$ represent the input text corpus, where $x_i \\in R^d$ is a high-dimensional vector"}, {"title": "Algorithm 2 Proposed Scalable Multi-Vector Search Algorithm", "content": "Indexing. We built indexes on vector embeddings to utilize the HNSWlib [28] and FAISS [29] indexes in the VectorSearch framework. These indexes enable fast and accurate retrieval of similar documents by organizing the vector embeddings into efficient data structures, such as navigable graphs HNSWlib and inverted files FAISS, which allow for approximate nearest neighbor (ANN) search [9], [30].\nQuery Processing. We handled user queries by encoding them into vector representations $q \\in R^d$ and performing similarity search using the indexed vectors $E = {e_1, e_2, ..., e_n}$, where each $e_i \\in R^d$ is an embedding vector. Algorithm 2 leverages the advanced capabilities of multi-vector search, facilitating the efficient retrieval of similar vectors across diverse datasets. By implementing a robust indexing mechanism, our approach establishes a high-performance structure adept at managing high-dimensional vector data [17].The single-vector search operations within the index are enhanced, extending the methodology to efficiently manage multi-vector queries. Specifically, for a multi-vector query $Q = {q_1, q_2, ..., q_m }$, our algorithm searches for each query vector $q_i$ in Q, retrieving the nearest neighbors $N(q_j)$ from the indexed vectors. The similarity search operation can be represented as:\n$Search(q, E) = arg \\min d(q, e_i)$,\n$e_i \\in E$\nwhere $d(q, e_i)$ is a distance metric. We propose a comprehensive evaluation methodology for assessing the effectiveness of the VectorSearch system ref alg:VectorSearch Framework. This methodology involves conducting comprehensive evaluations across diverse hyperparameter configurations. The performance metrics, including mean precision $P$ and query time $T_q$, were measured for each combination of hyperparameters 0. We utilized ParameterGrid from the scikit-learn library to systematically explore the hyperparameter space $\\Theta$. By iterating over the parameter grid $\\Theta = {\\theta_1, \\theta_2,...,\\theta_k}$, we identified optimal configurations $\\theta^*$ that maximized precision while minimizing query time. The optimization process can be expressed as:\n$\\theta^* = arg \\max \\frac{P(\\Theta)}{\\theta \\in \\Theta T_q(\\Theta)}$"}, {"title": "A. VectorSearch Design", "content": "We proposed the VectorSearch framework, as shown in Figure 1, and Algorithm 1 as a systematic approach to document retrieval leveraging state-of-the-art techniques. This framework provides a structured methodology for optimizing document retrieval systems, offering insights into the effectiveness of different hyperparameter configurations and facilitating the identification of optimal settings. The Parameter Grid is utilized to define a comprehensive parameter grid, encompassing various combinations of hyperparameters such as pretrained model selection (@model), index dimensionality (@dimension) and similarity threshold (@threshold).\nFeature Extraction (Embedding). That utilized a deep learning model, denoted as Embedding(.), to convert the preprocessed document titles into embeddings. Thus, each document di is transformed into an embedding $e_i$. $e_i = Embedding(Preprocess(d_i))$.\nVector Database Creation (V). That constructed a vector database V using the embeddings of the document titles. The database V is indexed using the FAISS, facilitating efficient similarity search operations where $\\Psi(V): V = {e_1, e_2, ..., e_n}$.\nTo effectively implement this process, we propose the efficient multi-vector search algorithm 5. Model Training and Evaluation that defined a function trainEvaluate($\\theta$), where $\\theta$ represents the hyperparameters of the VectorSearch framework. This function trains and evaluates the model, returning performance metrics.\nHyperparameter Tuning ($\\Theta$). Defined a set of hyperparameters $\\Theta = {\\theta_1, \\theta_2, ..., \\theta_M}$, where each $\\theta_i$ represents a combination of hyperparameters.\nOptimization Objective. Goal is to find the optimal hyperparameters $\\theta^*$ and maximizing the precision metric.\n$\\theta^* = arg \\max Precision(\\Theta)$"}, {"title": "B. Scalable Multi-Vector Search", "content": "We peoposed Algorithm 2 scalable multi-vector search approach for retrieving similar vectors efficiently. It utilizes the FAISS index and accepts query vectors along with the desired number of nearest neighbors. The algorithm begins by initializing the FAISS index with a specified dimensionality. Then, it defines two functions: single-vector-search and multi-vector-search. The single-vector-search function conducts a nearest neighbor search for a single query vector, while the multi-vector-search function extends this process to multiple query vectors, aggregating the results. Finally, the algorithm outputs the set of similar vectors retrieved from the multi-vector search. This algorithm complements the VectorSearch"}, {"title": "design 3 by providing a mechanism for efficient multi-vector search operations that document titles are encoded using the SentenceTransformer model, and an index is constructed for efficient similarity search. Systematic evaluation of hyperparameter combinations aids in algorithm fine-tuning. The VectorSearch design Algorithm 3 involves systematically exploring various hyperparameter combinations to tune the system's settings and optimize the performance of the document retrieval system. For data preprocessing we removed HTML tags, handling missing values, tokenization, removing stopwords, and lemmatization. Document title encoding operates at O(n) time complexity (n: number of titles), while vector normalization incurs O(nd) time (d: embedding dimension). Adding the normalized vectors to the index involves inserting each vector into the index, resulting in a time complexity of O(nd). Evaluating the performance of different hyperparameter combinations involves iterating over all combinations and evaluating the performance for each combination. This results in a time complexity of O(mn), where m is the number of hyperparameter combinations and n is the number of documents in the dataset. Overall, the complexity of the VectorSearch Framework algorithm can be summarized as O(nd + mn), where n is the number of documents, d is the dimensionality of the embeddings, and m is the number of hyperparameter combinations.", "content": null}, {"title": "Algorithm 3 Proposed VectorSearch Algorithm", "content": "1) VectorSearch Algorithm and Complexity: We propose a novel approach 3 to document retrieval, leveraging state-of-the-art techniques in natural language processing and information retrieval. Our methodology integrates advanced encoding models such as SentenceTransformer with efficient indexing techniques to enable rapid and accurate retrieval of relevant documents. By systematically exploring various hyperparameter configurations and employing rigorous evaluation methods, we aim to optimize the performance of our document retrieval system. The proposed VectorSearch algorithm outlines the process of conducting a nearest neighbor search within a given index to retrieve relevant documents for a set of queries. Given a set of queries Q, an index I, and a specified number k for the top results to be retrieved, the algorithm iterates over each query qi. For each query, the algorithm encodes it into a vector qi using a SentenceTransformer model and normalizes the vector. It then performs a nearest neighbor search with the normalized query vector qi using the index I. The top k results from the search, each consisting of a document dj and its corresponding similarity score sj, are extracted. The algorithm retrieves the documents corresponding to these top results from the index and assigns the similarity scores sj to them."}, {"title": "C. Efficient Multi-Vector Search Algorithm", "content": "In the proposed Algorithm 5, the complexity of encoding documents(f(D)) into embeddings is contingent upon the specific embedding model employed. Let N represent the number of documents in the dataset, and let d denote the dimensionality of the embeddings. The time complexity for encoding all documents is typically O(Nd). The complexity of creating Index (create_index(E)) depends on the indexing algorithm. Let's denote the number of embeddings as M. For approximate nearest neighbor methods, the complexity is $\\mathcal{O}(M log(M))$ or $\\mathcal{O}(Md)$. Encoding Query (f(Q)): Similar to encoding documents, the complexity of encoding"}, {"title": "D. Optimizing Vector Search Systems", "content": "We introduce Algorithm 4 as a comprehensive method for training and evaluating document retrieval systems. This algorithm focuses on systematically exploring various hyperparameter configurations to optimize the performance of the document retrieval system. Initially, the algorithm loads a dataset containing document titles and associated metadata. Subsequently, it extracts a representative subset, pdf_subset, comprising the initial 1000 documents. Using the encoder function fenc, document titles are transformed into dense vector representations in Rd, resulting in a feature matrix $f_e \\in R^{n \\times d}$. These vectors are integrated into (fi) and (hi) hybrid index to facilitate efficient nearest neighbor search operations. An evaluation function evaluate(p) is defined to quantify system performance metrics and evaluate the system's effectiveness across diverse hyperparameter configurations.\nAlgorithm Complexity: Data Loading and Preprocessing: O(n). Embedding Generation: O(nd). Index Construction: $\\mathcal{O}(nd\\log{n})$ - $\\mathcal{O}(nd)$. Parameter Grid Generation: O(pm). Model Training and Evaluation: O(ks)."}, {"title": "IV. EXPERIMENTAL EVALUATIONS", "content": "The experiments used a labeled dataset of 1000 news articles. We implemented the algorithm in Python, using libraries for data manipulation, computations, and NLP. SentenceTransformer encoded document titles into embeddings. Indexes facilitated retrieval. Hyperparameter optimization evaluated combinations of dimensions, thresholds, and models using grid search. All our experiments were performed using the same hardware consisting of RTX NVIDIA 3050 GPUs and i5-11400H @ 2.70GHz with 16GB of memory. The details of each experiment are the following. We implemented a caching mechanism to store and reuse precomputed embeddings from the Chroma model, enhancing efficiency by eliminating redundant computations. This mechanism efficiently saved embeddings to disk, minimizing the need for recomputation and optimizing resource management."}, {"title": "A. Datasets", "content": "NewsCatcher. [31] Data on news topics was collected by the NewsCatcherteam, which collects and indexes 108k news articles spanning eight topics. All the News. [32] This dataset contains 2,688,878 news articles and essays from 27 American publications, spanning January 1, 2016 to April 2, 2020. We conducted experiments using three models: all-MiniLM-L6-v2 [20], [33], roberta-base [34], and bert-base-uncased [21]. The hyperparameters varied included the index dimension (256, 512, 1024) and the similarity threshold (0.7, 0.8, 0.9). The principal component analysis reduction enabled us to visualize the embeddings in a plot 2b, where each point represents a document title. Latent Dirichlet Allocation (LDA) can be represented as follows:\n$p(w, z, \\Theta, \\phi) = p(\\theta) \\prod_{d=1}^{D} (p(\\theta_d) \\prod_{n=1}^{N_d} p(w_{dn}|\\phi_{z_{dn}}))$,\nwhere w represents a word in the corpus, z represents the topic assignment for each word, $\\theta$ represents the distribution of topics in documents, $\\phi$ represents the distribution of words in topics, p($\\theta$) and p($\\phi$) are Dirichlet priors, and $p(w_{dn})$ is the probability of word $w_{dn}$ given topic $z_d$. After performing LDA, each document is represented as a probability distribution over topics. The topic distribution of a document d can be denoted as:\n$\\theta_d = (\\theta_{d1}, \\theta_{d2},..., \\theta_{dk})$,\nwhere $\\theta_{dk}$ represents the probability of topic k in document d. This distribution provides insights into the thematic composition of the document. The length of a document can be quantified as the number of words it contains. If a document d contains $N_d$ words, its length can be expressed as:\n$Length(d) = N_d$."}, {"title": "B. Comparison between Models", "content": "In table II, we compare the performance of different models. ROBERTa-base consistently achieves higher precision (0.99) and recall (0.77) compared to all-MiniLM-L6-v2 and BERT-base-uncased. However, all-MiniLM-L6-v2 demonstrates significantly lower query times (0.32 seconds). ROBERTa-base with precision of 0.99 and recall of 0.77, all-MiniLM-L6-v2 with precision of 0.97 and recall of 0.93, and BERT-base-uncased with precision of 0.99 and recall of 0.44. In terms of query times, all-MiniLM-L6-v2 achieves the fastest time at 0.32 seconds, followed by BERT-base-uncased at 0.47 seconds and ROBERTa-base at 1.37 seconds. This indicates that while ROBERTa-base achieves the highest precision and recall, all-MiniLM-L6-v2 offers the fastest query times. Increasing the index dimension generally led to improved performance in terms of precision and recall. For instance, with an index dimension of 1024, VectorSearch achieved a recall of 76.62% and a precision of 98.68%, as shown in Table II, different models exhibited varying levels of effectiveness, roberta-base consistently demonstrated high precision and recall across different index dimensions, with a recall of 76.62% and a precision of 98.68%, as shown in Table II."}, {"title": "C. Performance Comparison of VectorSearch Models", "content": "Table IV is a comprehensive comparison of the performance metrics for different combinations of language models and index types utilized in vector search systems. Index dimension refers to vector dimensionality, impacting computational complexity. Similarity threshold determines document relevance based on scores. Precision assesses retrieval accuracy, indicating variations across configurations in query accuracy, with values ranging from approximately 0.68 to 0.98, as shown in Tables I and III. Recall indicates document retrieval effectiveness. Higher values mean more relevant documents retrieved, reducing false negatives. Ranges from about 0.39 to 0.92, showing retrieval effectiveness across configurations, as shown in Table IV. Query Time (s) represents the average processing time per query for retrieving results from the index. Lower query times indicate faster retrieval speeds. Ranging from about 0.11 to 1.37 seconds, it highlights efficiency variations across configurations, as shown in Tables, II and III."}, {"title": "D. Performance Comparison with Baselines", "content": "Our research involved comparing the performance of different setups for VectorSearch based document retrieval systems. We used various models and index types to establish baselines for effectiveness and efficiency in retrieving relevant documents. This included data loading and preprocessing of news articles, vector encoding using three models, and indexing with two techniques. We also conducted sensitivity analysis by adjusting parameters like index dimension and similarity threshold for each model-index combination. This allowed us to evaluate the sensitivity of the retrieval systems to these parameters and identify optimal configurations. Table V shows the performance comparison of vector search models. The evaluation metrics include precision, recall, and query time (in seconds). Notably, models utilizing the bert-base-uncased model consistently achieve high precision and recall across different index types and configurations. We evaluated three models across hybrid indexing. For each combination of hyperparameters, we measured the mean precision, mean recall, and mean query time, as shown in Figure, 5. The mean precision values ranged from approximately 0.68 to 0.98 across different combinations of hyperparameters and pretrained models. The combination that achieved the highest mean precision was an index dimension of 1024 with the FAISS index type, using the bert-base-uncased model, with a precision of 0.98, the mean recall values varied between 0.01 and 0.92 across the evaluated hyperparameter combinations. The combination with the highest mean recall was also an index dimension of 1024 with the FAISS index type, using the bert-base-uncased model, with a recall of 0.92. The mean query time ranged from approximately 0.17 to 1.92 seconds. It's important to note that while the all-MiniLM-L6-v2 model with the HNSWlib index type and an index dimension of 256 demonstrated the highest mean query time of 1.92 seconds, lower query times are generally preferred for efficient retrieval."}, {"title": "E. Impact of Hyperparameters on Query Time", "content": "The sensitivity analysis revealed notable trends in the relationship between hyperparameters and query time. As depicted in 6 the plot illustrates the influence of index dimension and similarity threshold on the query time. It is evident that higher index dimensions generally lead to increased query times, particularly when combined with higher similarity thresholds. Conversely, lower similarity thresholds demonstrate a more nuanced impact on query time, with a slight decrease observed in certain cases. Our experiments reveal significant variations in retrieval performance across different hyperparameter configurations. Notably, higher index dimensions tend to improve precision but may lead to increased query times. This plot 7 illustrates the relationship between query time and index dimension for the Newscatcher dataset. Each point represents the average query time for retrieving documents using Nearest Neighbors indexing with cosine similarity, with varying index dimensions and similarity thresholds. We observe that increasing the index dimension improves precision and recall@10 but leads to longer query times due to the higher computational complexity of nearest neighbors search in higher-dimensional spaces. Similarly, higher similarity thresholds result in faster query times but may compromise precision and recall@10."}, {"title": "F. Performance Analysis", "content": "In table VI, we present models utilizing MiniLM, BERT, and ROBERTa architectures consistently outperformed baseline models across all tested configurations. Specifically, MiniLM-L6-v2 demonstrated a precision of 0.91 and recall of 0.22, BERT-base-uncased achieved a precision of 0.98 and recall of 0.73, while RoBERTa-base attained a precision of 0.68 and recall of 0.64. Further analysis revealed that fine-tuning the models with specific indexing methods improved their performance. For instance, MiniLM models indexed with HNSWlib exhibited higher precision and recall compared to those indexed with FAISS. Similar trends were observed for BERT and ROBERTa models across different indexing methods. Additionally, when comparing our models against baseline performance RoBERTabase SimCSE [19] exhibited a precision of -0.43 compared to the baseline, while RoBERTalarge CARDS [19] showed an improvement of +1.94 precision. These results underscore the effectiveness of our proposed models in enhancing retrieval performance. MiniLM-L6-v2 and BERT-base-uncased, demonstrate remarkable precision levels when juxtaposed with the provided STS task baseline [19]. For instance, BERT-base-uncased achieves an impressive precision score of 0.98, surpassing both RoBERTabase and ROBERTalarge models. Table IV shows the comparative performance of different models across indexing techniques and dimensions. This confirms that BERT embeddings improve retrieval accuracy by 8% for longer texts compared to Sentence Transformers, as evidenced by both precision and recall metrics. While ROBERTa-base exhibits a slightly lower precision rate, our models excel in recall metrics, an essential aspect in information retrieval tasks. MiniLM-L6-v2 and BERT-base-uncased attain recall rates of 0.22 and 0.73, respectively. MiniLM-L6-v2 exhibited a precision of 0.91 and a recall of 0.22, representing a \u2248 36% improvement in precision and a \u2248 30% improvement in recall compared to the IS-"}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "In conclusion, our proposed methodologies offer a novel perspective on document retrieval. We streamline dataset loading and preprocessing, efficiently encode document titles into"}, {"title": "embeddings, optimize nearest neighbor search efficiency, and present a comprehensive framework for training and evaluating vector search systems. Through our evaluation framework encompassing model hyperparameters, index dimensionality, and similarity thresholds, we've demonstrated the efficacy of our approach in achieving high precision and recall rates while maintaining low query times. Future work will focus on integrating techniques such as attention mechanisms into the VectorSearch framework to provide insights into how specific terms and their semantic relationships contribute to the similarity scores between queries and documents.", "content": null}]}