{"title": "AmpliNetECG12: A lightweight SoftMax-based relativistic amplitude amplification architecture for 12 lead ECG classification.", "authors": ["Shreya Srivastava"], "abstract": "The urgent need to promptly detect cardiac disorders from 12-lead Electrocardiograms using limited computations is motivated by the heart's fast and complex electrical activity and restricted computational power of portable devices. Timely and precise diagnoses are crucial since delays might significantly impact patient health outcomes. This research presents a novel deep-learning architecture that aims to diagnose heart abnormalities quickly and accurately. We devised a new activation function called aSoftMax, designed to improve the visibility of ECG deflections. The proposed activation function is used with Convolutional Neural Network architecture to includes kernel weight sharing across the ECG's various leads. This innovative method thoroughly generalizes the global 12-lead ECG features and minimizes the model's complexity by decreasing the trainable parameters. aSoftMax, combined with enhanced CNN architecture yielded AmpliNetECG12, we obtain exceptional accuracy of 84% in diagnosing cardiac disorders. AmpliNetECG12 shows outstanding prediction ability when used with the CPSC2018 dataset for arrhythmia classification. The model attains an F1-score of 80.71% and a ROC-AUC score of 96.00%, with 280,000 trainable parameters which signifies the lightweight yet efficient nature of AmpliNetECG12. The stochastic characteristics of aSoftMax, a fundamental element of AmpliNetECG12, improve prediction accuracy and also increasse the model's interpretability. This feature enhances comprehension of important ECG segments in different forms of arrhythmias, establishing a new standard of explainable architecture for cardiac disorder classification.", "sections": [{"title": "1. Introduction", "content": "Heart diseases and strokes have become much more common around the world, almost doubling from 271 million cases in 1990 to 523 million cases in 2019. In the same way, cardiovascular diseases have caused more deaths, from 12.1 million in 1990 to 20.5 million in 2021. CVDs are now becoming more common and killing more people [1]. This has put a lot of stress on healthcare systems, especially in developing countries where CVDs are more common and rising faster. The fact that healthcare services in these places are hard to afford makes this problem even worse. Because of these past patterns, it is now more important than ever to detect CVDs quickly and correctly. Electrocardiograms are one of the most important diagnostic tools for heart diseases [2]. This technology, which is painless and doesn't cost much, records the heart's electrical activity. It tells us the important things about heart rhythm, conduction patterns, and heart's health in general. ECGs (Figure 1) are being used for many things, like finding arrhythmias, myocardial infarctions, and changes in ischemia [3]. Because of this, it is important in both short-term and long-term medical situations. The ECG's ability to quickly and accurately pick up on changes in the heart's activity is very important for finding and treating CVDs early on. Physiologists use their years of training and experience to figure out what's wrong with the heart by reading complicated heart rhythms by hand. However, algorithms and machine learning are used in automated ECG analysis that uses artificial intelligence to look at these patterns quickly and correctly [4]. AI-based analysis can handle huge amounts of data, speed up diagnosis, and maybe even improve accuracy, especially when it comes to finding subtle or uncommon abnormalities. Manual analysis, on the other hand, requires more detailed knowledge and judgement based on the situation. Heart diseases are becoming more common, and there is a huge amount of ECG data that needs to be analyzed automatically. Al's inclusion into ECG analysis has been studied progressively because it might democratize healthcare, diagnose CVDs early, and help design individualized treatments.\nTraditional methods for arrhythmia classification typically involve hand-extracted features from ECG signals and these features are then used to differentiate various types of arrhythmias by rule-based methods or machine learning methods for instance Z. Qibin et al. used Wavelet Transformation and Support Vector Machines [5], P. Tadejko et al. used Mathematical morphology transformations and Self-Organization Map and Learning Vector Quantization [6], A. Jovic et al. applied Chaos theory to ECG feature extraction [7], and S. Sahoo et al. detect QRS complex features based on the multiresolution wavelet transform [8]. These methods rely heavily on the expertise in signal processing and understanding of the ECG's physiological aspects, making it somewhat subjective and potentially variable in accuracy. In recent years these handcrafted features are further processed using machine learning algorithms most used algorithms are SVM, RF, KNN [9-11].\nDeep learning-based ECG analysis: The transition to deep learning methods in ECG classification marks a significant shift from traditional feature engineering approaches. Deep learning algorithms have the ability to automatically learn and extract features from raw ECG data, eliminating the need for explicit feature engineering. Various neural networks, including CNN [12], RNN [13], LSTM [14], BiLSTM [15] and Transformer [16], have been used to identify arrhythmia. While scientists have also used combination of different Deep learning architecture and multiple data modalities for instance C. Chen et al. used CNN-LSTM [17], and Z. Ahmad et al. used Gramian Angular Field (GAF), Recurrence Plot (RP) and Markov Transition Field (MTF) of ECG signal as input to the CNN [18].\n12 lead ECG analysis: While the above- mentioned approaches primarily tackle the problem of heartbeat (Smaller Section of ECG) classification into cardiac disorder classes. There is also a significant literature for long term ECG classification using publicly available datasets. Recent studies have explored multi-label classification of ECG signals using the public CPSC2018 dataset. Notable contributions include He et al.'s deep neural network combining a residual convolution module and a bidirectional LSTM with a high F1-score [15], Yao et al.'s attention-based time- incremental CNN for variable-length ECG signals [19], and Zhang et al.'s use of the Shapley Additive exPlanations method for model interpretation [20]. These studies highlight a shift from traditional methods, focusing on direct input of 12-lead ECG signals into neural networks and emphasizing leads with higher contributions for enhanced performance.\nThis research article presented a new method for cardiac diagnostics using deep learning. It involves the creation of a lightweight Convolutional based Neural Network architecture. The architecture is improved by including a unique activation function inspired from the Swish activation [21]. The novelty of this work lies in the capacity of proposed architecture to get almost state-of-the-art outcomes in ECG classification with just 280k trainable parameters and using compressed ECG data with a much lower sample frequency of just 50Hz. This signifies a decrease by a factor of 10 from the initial signal frequency often used in ECG analysis. An essential component of this study is the introduction of newly created activation function termed aSoftmax. The efficacy of aSoftmax activation for the proposed architecture is likely responsible for high accuracy even with lower resolution data. This has significant ramifications for practical healthcare uses, particularly in environments with limited resources where the capacity for storing and transmitting data may be restricted.\nProposed architecture uses the strategy of kernel weight sharing across given ECG leads, instead of the typical approach of training separate feature extraction models for each lead. This novel technique optimizes the model by enabling the use of identical kernels (or filters) in the convolutional layers across different leads. As a result, the model's trainable parameters are significantly reduced without any detrimental impact on its performance. Kernel sharing exploits the notion that certain inherent patterns and characteristics in ECG signals are universally present across several leads, hence eliminating the need for distinct feature extractors for each lead. This not only results in a smaller model with fewer parameters, but also has the potential to improve the model's capacity to generalize and be resistant to disturbances, as it acquires a more comprehensive representation of the heart's electrical activity. This strategy effectively tackles issues like as overfitting,"}, {"title": "2. Materials and methods", "content": "The CPSC-2018 [22], provided as part of the 7th International Conference on Biomedical Engineering and Biotechnology, achieved a noteworthy accomplishment in ECG research by making a complete ECG dataset accessible to the public.\nThis dataset is a valuable resource for the worldwide research community. It consists of 6,877 12-lead ECG recordings, with an approx. equal number of individuals from both genders: 3,178 females and 3,699 males further statistical information about data is provided in Table 1. Each recording is obtained at a high sampling rate of 500 Hz and has a length ranging from 6 to 60 seconds. The amplitude of the recordings is measured in millivolts. The dataset is characterized by its wide range and variety, which is emphasized by its multi-label categorization, including 9 diagnostic categories. The cardiac rhythms that are included are Normal Sinus Rhythm (NSR), Atrial Fibrillation (AF), First-Degree Atrioventricular Block (IAVB), Left Bundle Branch Block (LBBB), Right Bundle Branch Block (RBBB), Premature Atrial Contraction (PAC), Premature Ventricular Contraction (PVC), ST-Segment Depression (STD), and ST-Segment Elevation (STE)."}, {"title": "2.2 Data processing and compression", "content": "The data processing technique for CPSC2018 in ECG analysis entails crucial steps aimed at transforming and standardizing the electrocardiogram data. Initially, the sampling frequency of the original data is reduced by a factor of 10, achieving a 50Hz rate, which decreases data size while preserving essential ECG details. Subsequently, for each data point, 1500 sequential potential values were sampled from the full signal, using zero padding if the signal has less than 1500 values and excess values been dropped, to maintain consistency in segment length across various recordings. This is followed by a simple normalization process that standardizes the potential values, ensuring that amplitude variations in the ECG signals are uniform and comparable. Lastly, all leads, were stacked side by side to replicate the conventional paper-printed ECG layout, facilitating visual inspection and analysis."}, {"title": "2.3 Model architecture", "content": "In this study, we introduce a novel neural network architecture specifically designed for advanced ECG feature extraction and classification tasks. The architecture commences with an input layer accommodating the predefined input shape, followed by a series of convolutional layers with varying filter sizes and strides, each accompanied by layer normalization and then the aSoftmax activation. Notably, the model integrates custom global max and min pooling layers after convolutional stages, effectively capturing extremal features. These pooled outputs are subsequently concatenated and flattened, leading to a final classification layer implemented to classify inputs into 9 different types of arrhythmias. Table 2 and Figure 2 captures the summary of the model architecture and flow of proposed methodology. Further in the paper we have discussed about kernel weight sharing and aSoftmax activation, which are cornerstone to the model's performance."}, {"title": "2.4 Kernel weight sharing", "content": "Kernel weight sharing is a concept in Convolutional Neural Networks (CNNs), particularly when dealing with multi- dimensional data like electrocardiogram (ECG) recordings. In the context of ECG data, which typically consists of multiple leads, kernel weight sharing can be applied in a unique way. When dealing with multi-lead ECG data, each lead can be thought of as a separate channel (like RGB channels in images). However, in our study, the leads are not stacked as channels but treated as separate dimensions. And the same convolutional kernel can be applied across different leads, sharing weights across these. This approach can capture the inter- relationships between different leads while maintaining the low trainable weights."}, {"title": "2.5 aSoftmax activation", "content": "The activation function described by Eq. X which I'll refer to as \"Absolute Softmax Activation\" (aSoftmax), introduces a novel approach to modifying input tensors for use in the proposed network. This function operates by first taking the absolute value of each element in the input tensor, ensuring that all inputs are non-negative. This step is crucial, especially in ECG signal processing, where the polarity of signal components varies, and absolute magnitudes are more informative than raw values.\n$A(x) = x \\odot softmax(|x|)$\nSubsequently, the SoftMax function is applied to these absolute values. The SoftMax transformation is widely used in neural networks for its ability to convert a vector of values into a probability distribution, where the relative scale of each element is preserved, but all values are transformed to lie in the range (0,1) and sum to 1. The relativistic amplification/suppression constant then multiplied with original signal to enhance the underlying ECG feature. By combining these two steps, the aSoftmax activation ensures that the relative importance of each input feature is maintained and normalized in a probabilistic framework. This is especially important in ECG classification, where the strength and duration of various signal components (like QRS complexes, P-waves, etc.) are key indicators of cardiac health. Standard activation functions like ReLU might not handle such variations or will require more training to converge effectively, leading to loss of crucial information. Figure represents the plot of aSoftmax activation and first order derivate of it."}, {"title": "2.6 Loss and optimizer", "content": "Due to the noisy nature of ECG signal we have adopted Adamax for optimization of the proposed model. Adamax is a variant of the Adam optimizer [23], which is itself an extension of stochastic gradient descent [24] that has been widely adopted in deep learning. The Adamax optimizer is defined by the following equations:"}, {"title": "2.7 Experiment setup", "content": "The CPSC2018 dataset records were stratified and split into training and testing sets. The labels from the dataset were one-hot encoded to transform the problem into a multi-class classification task. The implementation of the model was carried out using TensorFlow, and a P100 GPU was utilized for training purposes. The training of the proposed system was optimized from scratch using the Adamax optimizer. This optimizer was chosen for its effectiveness in handling sparse gradients on noisy problems. An exponential decaying learning rate was employed, starting with an initial value of 0.007, to fine-tune the model training dynamically. Additionally, categorical Focal cross entropy with label smoothing of 0.3 was used. For the evaluation of the model's performance, standard metrics such as precision, recall, F1-score, and the Area Under the Curve (AUC) were used. These metrics are critical in assessing the effectiveness of the model, especially in a multi-class classification scenario where the balance between sensitivity and specificity is crucial."}, {"title": "3. Results", "content": "The evaluation of the model's performance in classifying cardiac arrhythmias was thoroughly presented in our study, classification efficacy was provided by the Receiver Operating Characteristic (ROC) curves and the Area Under the Curve (AUC) values, as illustrated in Figure 3. The model achieved a micro-average AUC of 0.97, precision 82%, recall 80%, and fl-score 81%, demonstrating its robustness in arrhythmia classification. Notably, performance varied across categories, with Atrial Fibrillation (AF), Left Bundle Branch Block (LBBB) and Right Bundle Branch Block (RBBB) showing high AUCs, while categories like Intra-Atrial Ventricular Block (IAVB), Premature Ventricular"}, {"title": "3.1 Comparative Analysis", "content": "In our study, we benchmarked our model against several leading ECG classification methods, including 1D-ResNet34 [20], 1D- SEResNet34 [26], TI-ResNet18 [27], InceptionTime [28], ECGNet [28], and lightX3ECG [46]. These methods are recognized for their efficiency and smaller nature and follows similar experimental setup as this study. The comparison focused on key performance metrics such as F1 scores, computational complexity, and model compactness, as detailed in Table 4 of our study."}, {"title": "3.2 Explainability", "content": "A comprehensive validation was conducted to demonstrate the interpretability and explainability of our model. This validation process involved an in-depth feature map analysis, which provided crucial insights into how our model processed and interpreted the ECG data. Additionally, the application of Shapley Values played a pivotal role in quantifying the contribution of each feature to the model's predictions. This approach was instrumental in unraveling the complex decision-making process of our deep learning model, ensuring a transparent and understandable AI system. Our findings highlighted the model's capability to classify ECG signals effectively and reliably, paving the way for enhanced diagnostic accuracy in clinical settings. The use of these advanced interpretability techniques underscored the potential of Al in medical diagnostics, providing a promising outlook for future developments in the field."}, {"title": "4. Discussion", "content": "Our system demonstrates competitive performance in diagnosis and interpretation when compared to previous works in ECG signal analysis. The improvements can be attributed to several innovative approaches: (i) We employ a unique strategy of processing three input ECG leads independently using three distinct backbones, each optimized for ECG signal efficiency. (ii) The introduction of the Lead-wise Attention module is a crucial element, significantly enhancing the system's overall performance. Furthermore, this module allows for a lead-wise explanation of the predictions, leveraging Explainable AI (XAI) techniques to identify the most influential ECG lead. However, our system has certain limitations: (i) Relying on reduced-lead ECG data might lead to missed critical information, potentially hindering the detection of specific cardiovascular abnormalities. (ii) The multi- input architecture of poses challenges in training with small-scale datasets and results in higher storage costs, necessitating the use of practical solutions like weight pruning to mitigate these issues."}, {"title": "5. Limitation and future prospects", "content": "Our approach, which utilizes relativistic amplitude amplification, has demonstrated commendable performance in ECG signal analysis, particularly notable for its efficient use of fewer model parameters while still delivering effective predictions. This aspect of our model underscores its potential for applications where computational resources are limited. However, it's important to note that our model exhibits certain limitations in its current form, especially in comprehensively understanding rhythmic anomalies such as RR interval variation. This suggests a gap in the model's capability to fully interpret complex temporal features in ECG signals, which are critical for diagnosing certain types of cardiac irregularities. The model's current architecture, while optimized for specific aspects of ECG analysis, may require further refinement or the integration of additional techniques to enhance its proficiency in capturing and analyzing these subtle yet clinically significant variations in heart rhythm."}, {"title": "6. Conclusion", "content": "In this research article, we introduce an innovative deep learning architecture that leverages a comprehensive analysis of 12-lead, long-term electrocardiograms (ECGs) in their most condensed form to accurately classify different types of Arrhythmias. Our architecture is distinguished by two pioneering approaches: a novel activation function specifically designed to enhance the amplitude of key morphological features in the ECG signal, and a unique kernel weight sharing mechanism that effectively captures global, cross-lead ECG characteristics. This dual strategy enables our model to maintain a minimal number of trainable parameters while delivering performance that surpasses existing models in the lightweight category, as demonstrated in the CPSC2018 challenge. Quantitatively, our proposed architecture has achieved an impressive average F1-Score of 80.74% and an ROC-AUC of 97.0%, positioning it as a leading solution in the domain of Arrhythmia detection. Beyond its numerical achievements, our model marks a significant advance in the interpretability and explainability of deep learning models for ECG analysis. It exhibits a remarkable ability to identify critical ECG regions that contribute to Arrhythmia classification, a feature attributed to the innovative aSoftmax activation function. This architecture not only excels in accuracy and efficiency but also represents a significant step forward in the development of explainable AI (XAI) models. Such advancements are crucial for clinical decision-making tools, where understanding the rationale behind model predictions is essential for gaining the trust of medical professionals and facilitating their adoption in clinical settings."}]}