{"title": "Enhanced Infield Agriculture with Interpretable Machine Learning Approaches for Crop Classification", "authors": ["SUDI MURINDANYI", "JOYCE NAKATUMBA-NABENDE", "RAHMAN SANYA", "ROSE NAKIBUULE", "ANDREW KATUMBA"], "abstract": "The increasing popularity of Artificial Intelligence (AI) in recent years has led to a surge in interest in image classification, especially\nin the agricultural sector. With the help of Computer Vision (CV), Machine Learning (ML), and Deep Learning (DL), the sector has\nundergone a significant transformation, leading to the development of new techniques for crop classification in the field. Despite\nthe extensive research on various image classification techniques, most have limitations such as low accuracy, limited use of data,\nand a lack of reporting model size and prediction. The most significant limitation of all is the need for model explainability. This\nresearch evaluates four different approaches for crop classification, namely traditional ML with handcrafted feature extraction methods\nlike Scale-Invariant Feature Transform (SIFT), Oriented FAST and Rotated BRIEF (ORB), and Color Histogram; Custom Designed\nConvolution Neural Network (CNN) and established DL architecture like AlexNet; transfer learning on five models pre-trained\nusing ImageNet such as EfficientNetV2, ResNet152V2, Xception, Inception-ResNetV2, MobileNetV3; and cutting-edge foundation\nmodels like YOLOv8 (You Only Look Once) and DINOv2, a self-supervised Vision Transformer Model. All models performed well,\nbut Xception outperformed all of them in terms of generalization, achieving 98% accuracy on the test data, with a model size of\n80.03 MB and a prediction time of 0.0633 seconds. A key aspect of this research was the application of Explainable AI (XAI) to\nprovide the explainability of all the models. This journal presents the explainability of Xception model with LIME (Local Interpretable\nModel-agnostic Explanations), SHAP (SHapley Additive explanations), and GradCAM (Gradient-weighted Class Activation Mapping),\nensuring transparency and trustworthiness in the models' predictions. This study highlights the importance of selecting the right\nmodel according to task-specific needs. It also underscores the important role of explainability in deploying AI in agriculture, providing\ninsightful information to help enhance AI-driven crop management strategies, leading to more efficient and sustainable farming\npractices.", "sections": [{"title": "1 INTRODUCTION", "content": null}, {"title": "1.1 Oveview", "content": "Image classification is regarded as one of the major topics in computer vision and artificial intelligence (AI) [48][34].\nIt involves correctly identifying and categorising objects in images. Various techniques, such as handcrafted feature\nextraction methods, are used to extract features from images and then are used for image classification using machine\nlearning classification algorithms [22]. Moreover, deep learning (DL), a powerful subset of machine learning (ML), has\ncurrently achieved impressive results in computer vision applications such as image classification, object detection, and\nimage processing [3]. In the deep learning approach, feature extraction and classification are seamlessly integrated\nsimultaneously to classify images. In addition to the methods mentioned above, transfer learning in deep learning has\nbecome a crucial strategy, particularly when having a small amount of labelled data [4]. It involves leveraging knowledge\nfrom one problem domain to solve a different but related problem. For instance, a model trained on Large-scale datasets\nlike ImageNet can be adapted to classify images in specialized domains, reducing the need for extensive labelling [4].\nFurthermore, foundation models, large-scale pre-trained models trained on diverse datasets, have revolutionized the\nfield [59][27][23]. By fine-tuning these models to specific tasks, significant performance improvements can be achieved\neven with limited labelled data. This method utilizes the already learned complex and profound representations of\nfoundational models, resulting in a more efficient and effective knowledge transfer to novel tasks [59]. The challenge is\nthat most researchers only consider one or two of these approaches to develop a classification model for their task.\nIt should be noted that no conclusion should be made before evaluating the options. Each of them has the potential\nto perform well if used with better approaches and techniques. The best practice would be to evaluate them and\nchoose the most suitable one for the task. Additionally, It is important to ensure that all machine learning models\nare explainable. Despite their reputation as black boxes, these models offer more than just impressive accuracy and\nfavourable performance metrics [63]. To fully utilize the capabilities of these models, it is crucial to explain how they\nmake decisions. By creating visual representations demonstrating how models arrive at their conclusions. With this,\nindividuals from different fields can better understand the results more intuitively. Therefore, Developing and presenting\ninterpretable outcomes is necessary for promoting trust in Al technology.\nThe agricultural sector has experienced a significant transformation in recent years, mainly due to the resurgence\nof AI [56]. Thanks to Computer Vision(CV), ML, and DL advancements, sophisticated algorithms now power intel-\nligent systems that automate various agricultural processes [43]. One specific area that still needs more significant\ntransformation is the classification of plants in the infield. Farmers can gain precise insights into this crucial area with\nmodern AI-driven crop classification. Targeting this area can enhance precision agriculture by providing a detailed\nunderstanding of crop variations and infield needs with the help of modern AI. Additionally, this can facilitate crop\nyield estimation, which is essential in ensuring optimal agricultural output and sustainability [33].\nThis research targeted to develop and assess the four distinct image classification methodologies within the agri-\ncultural domain. The initial approach involved manually extracting features using handcrafted feature extraction\ntechniques from images and using traditional machine learning algorithms for classification. Subsequently, we explored\nthe potential of deep learning architectures: firstly, a custom convolutional neural network (CNN) model was designed\nfrom the ground up to perform classification task, and secondly, a well-established CNN architecture, AlexNet architec-\nture, was used for classification. The third approach involved applying transfer learning to preeminent architectures\ntrained using Imagenet, an effective strategy when dealing with limited labelled data. Finally, extensively utilized\npre-trained foundation models, known to be trained on vast and varied datasets, to develop a crop classifier. Using"}, {"title": "1.2 Background", "content": null}, {"title": "1.2.1 Classification Using Handcrafted Features and Classical Machine Learning Models:", "content": "Image classification\nheavily relies on the extracted features [2][22]. These features can be classified based on colour, shape, or texture [22].\nDifferent handcrafted feature extraction methods can be used to extract these features, which can then be used to classify\ncrops with traditional ML models [49]. One of the main challenges using this approach is the quality and relevance\nof extracted features. The performance of these models is directly linked to how well these features represent the\nunderlying patterns in the images, and sometimes, it takes much work to do it. Various handcrafted feature extraction\ntechniques have been employed for image classification [18] [22], including Scale-Invariant Feature Transform (SIFT),\nOriented FAST and Rotated BRIEF (ORB), Speeded-Up Robust Features (Surt), and colour histograms, to mention\na few. Only three were implemented for this research: SIFT, ORB, and colour histogram. SIFT, a widely recognized\napproach, detects critical points in an image that remain invariant under affine, rotational, and scale transformations.\nThis technique can represent leaves' texture, shape, and appearance, proving robust against variations in lighting and\nviewpoint [17][18]. On the other hand, ORB is a feature extraction algorithm that detects essential areas in images\nusing corner points or regions with significant intensity shifts. It employs a pyramid scheme with a FAST keypoint\ndetector and a BRIEF keypoint descriptor [62]. The colour histogram, however, represents the frequency distribution of\ncolours that appear in an image. It is especially effective in distinguishing images based on their unique colour profiles\nand analyzing the intensity and distribution of colours present in them [26].\nWhile handcrafted feature extraction methods have succeeded in classification tasks, it is essential to acknowledge\ntheir limitations [30]. They rely on domain expertise and manual tuning. In cases where specific leaf textures for crop\nclassification or structural attributes are crucial in distinguishing between crops, they can be effective. However, in\ncases where the visual patterns and attributes that differentiate crops are complex, high-dimensional, or not easily\ncaptured by predefined rules, they may require additional assistance.\nAfter feature extraction, different methods can be used to select the best features, such as Principal Component\nAnalysis (PCA). Then, these features can be used to train any classical ML model, like a Support Vector Machine (SVM),\nto classify images into different classes."}, {"title": "1.2.2 Classification Using Deep Learning:", "content": "Deep learning methods, especially Convolutional Neural Networks\n(CNNs), have demonstrated remarkable potential in automatically learning and extracting distinctive features directly\nfrom image data [55]. CNNs are highly effective in capturing hierarchical representations of images, which allows them\nto learn complex and abstract patterns that handcrafted features may not explicitly define [38] [55]. Using large datasets,\nCNNs can generalize to diverse visual variations and handle complex, high-dimensional feature spaces. This makes\nthem a valuable tool for image recognition and classification tasks."}, {"title": "1.2.3 Crop Classification Using Foundation Models:", "content": "Foundation models represent a distinct class of Deep Learning\nmodels, distinguished by their training on immense amounts of data and adaptability to various downstream tasks [59].\nUnlike transfer learning, their comprehensive training allows them to understand patterns and features broadly. These\nmodels can be fine-tuned for specific tasks, such as crop classification, using smaller labelled datasets, often leading\nto impressive results. While foundation models leverage transfer learning, their approach differs from conventional\nmethods. Their ability to be used in many ways allows for new approaches in agriculture, like predicting crop diseases\nwith very small data and yield estimation. However, they come with challenges, one being that the computational\ndemands for fine-tuning can be significant, and their back box nature often complicates the interpretation of predictions,\na crucial factor for practical agricultural applications [27]."}, {"title": "1.2.4 Explainablility of Crop Classification Models:", "content": "While advanced AI models have improved classification\naccuracy and other metrics, there is still a need for transparency and interpretability because their results are considered\na black box and cannot be understood or trusted by anyone [15] [52]. Understanding predictions is crucial for strategic\nplanning, yield estimation, and sustainable agriculture. Delving deeper into the reasons for each classification provides\ninsights into which features the model considers most significant, guiding the understanding of why predictions are\nmade in a certain way [6]. Insights like these are crucial for making informed decisions and implementing sustainable\nagricultural practices.\nOne solution to the challenge of explainability in Al is using eXplainable AI (XAI) tools, which include LIME (Local\nInterpretable Model-agnostic Explanations) [39], SHAP (SHapley Additive exPlanations) [24], and GradCAM (Gradient-\nweighted Class Activation Mapping) [45], mention but a few. LIME perturbs input data and observes prediction changes\nfor local explanations. SHAP values, rooted in cooperative game theory, allocate contributions of each feature to every\npossible prediction. GradCAM provides visual explanations from convolutional networks via class-discriminative\nregions. These techniques enhance model interpretability, building trust and enabling informed crop classification.\nThe remaining sections of this work are organized as follows: Section two provides a systematic literature review\nof the related work done around some of the methods used for crop classification. Section three presents some of the\ngaps in reviewed related work. Section four presents the journal contributions that tried to solve the gaps in related\nwork. Section five presents the methodologies used, including step-by-step explanations of the tools employed. Section\nsix provides the results of all the methods used and discussions of the outcomes. Finally, section seven presents the\nconclusion of the work, including a discussion of future work."}, {"title": "2 RELATED WORK", "content": "The section discusses recent research on crop classification using AI-driven models. The literature review was conducted\nby searching databases for papers published between 2018 and 2023 using specific keywords. Only papers that met\nstrict criteria were considered, including being in English, employing machine learning techniques, and discussing new\nmethodologies, architectures, or significant accuracy improvements.\nS. Roopashree et al. [40] highlighted the application of machine learning in classifying medicinal plants. Their\nmodel, Herbmodel, demonstrated high accuracy by employing feature extraction techniques and a support vector\nmachine classifier. The study achieved an average accuracy of 96.22% across a dataset of 2515 samples from 40 species.\nJyoti Madake et al. [25] highlighted the significance of utilizing image processing and feature extraction techniques.\nThey discovered that the Random Forest classifier combined with SIFT produced promising results. The trained model\ncould accurately detect and classify healthy and wilted plants, achieving an impressive accuracy rate of 85.41% when\nSIFT was combined with the Random Forest classifier. This performance was comparable to that of existing models.\nM. Waqas and N. Fukushima [58] showed that AKAZE, SIFT and SURF, a feature descriptor, outperformed others\nregarding computational speed and classification accuracy. AKAZE achieved a classification accuracy of 81.56%, slightly\ncomparable to SIFT's 87.25% and SURF's 85.73%. Further, Bansal et al. [5] Combined deep learning techniques with\ntraditional handcrafted feature extraction to achieve impressive classification results. Their approach used the VGG19\nnetwork and various classifiers, with the Random Forest classifier outperforming other classifiers and methods. The\nexperimental evaluation on the Caltech-101 benchmark dataset achieved an accuracy of 93.73%. Selvaraj et al. [16]\nemployed a pixel-based classification approach for classifying crops from aerial images, achieving high accuracy rates.\nThey utilized Support Vector Machines (SVMs) and Random Forest (RF) machine learning models and combined features\nderived from vegetation indices (VIs) and PCA for pixel-based classification. The results showed an overall accuracy of"}, {"title": "3 GAPS IN REVIEWED LITERATURE", "content": "These studies and many more reviewed collectively underscore the promise of employing image feature extraction\nalongside machine learning techniques for crop classification and detection and the flourishing role of deep learning in\nthese areas. However, they also point to the necessity for more in-depth research to create classification models that are\nboth robust and accurate. These enhanced models could significantly contribute to advancing agricultural practices\nand more effective crop management on a larger scale. Notably, current literature reveals a gap in harnessing the\ncapabilities of foundation models in crop classification. Foundation models, known for their vast scale and versatility in\nvarious domains, might offer untapped potential in this field. A notable limitation in existing research is the need for\nmore interpretability in the results produced. Understanding and explaining the decisions made by machine learning\nmodels is crucial, particularly in applications impacting critical sectors like agriculture. These studies also reveal some\ncritical limitations around several models demonstrating relatively lower accuracies, indicating room for improvement.\nFurthermore, the reviewed papers often did not provide essential details such as the sizes of the models or the prediction\ntimes, which are crucial for evaluating the feasibility and efficiency of these approaches in real-world applications.\nMore information is needed to ensure the ability to fully assess the practicality of deploying these models, especially in\ntime-sensitive agricultural settings. Finally, there needs to be more comparative analysis across different methodologies.\nMost studies focus on a single approach rather than testing various methods to ascertain the most effective strategies"}, {"title": "4 JOURNAL CONTRIBUTION", "content": "This research aims to try and overcome the limitations of previous studies by utilizing a comprehensive and multifaceted\napproach to crop classification through advanced machine learning and deep learning techniques while also providing\nexplainability. The journal's contributions are as follows:\n(1) Diverse Data Acquisition: The research uses a comprehensive dataset from multiple sources to acquire diverse\nimages from the infield, such as drone imagery, phone images, and online images from various repositories. The\ndataset consists of various visual perspectives and contexts, crucial in developing a robust model capable of\nhandling real-world variability in crop images.\n(2) Employment of Multiple Modelling Approaches: The journal explored four distinct modelling approaches:\n\u2022 Traditional machine learning models use features extracted using handcrafted feature extraction techniques\nlike ORB, SIFT, and colour histograms combined with KNN and SVM classifiers.\n\u2022 Deep learning models, specifically custom-designed CNNs and AlexNet Architecture, to leverage hierarchical\nfeature extraction.\n\u2022 Transfer learning using state-of-the-art architectures pre-trained on ImageNet, including EfficientNetV2,\nResNet152V2, Xception, InceptionResNetV2, and MobileNetV3.\n\u2022 Foundation models like YOLOv8 and DINOv2 to capitalize on their extensive pre-training and generalization\ncapabilities.\n(3) Multi-Dimensional Evaluation Criteria: The model performance evaluation was comprehensive, focusing\non recall, precision, accuracy, and confusion matrix analysis. This multi-dimensional evaluation thoroughly\nunderstood each model's strengths and weaknesses.\n(4) Focus on Explainability and Transparency: A significant contribution of this study is its emphasis on\nExplainable AI (XAI) techniques, such as LIME, SHAP, and GradCAM. These tools provided insights into the\ndecision-making processes of the models, enhancing their transparency and understandability. This is particularly\nimportant in agriculture, where decisions based on model predictions can have significant real-world impacts.\n(5) Practical Application and Relevance: The journal's methodology is tailored to real-world applications in\nagriculture, emphasizing the need for accurate, reliable, and understandable crop classification models. By\ncombining advanced machine learning techniques with a focus on model interpretability, the study contributes\nsignificantly to precision agriculture."}, {"title": "5 METHODOLOGY", "content": "The methodology encompassed a comprehensive multi-stage process, including robust data acquisition, preparation,\nmodel development, evaluation, and explainability. Data was sourced from various means, including drone-captured\nimagery, phone videos and images, and online image repositories, to ensure a diverse and comprehensive dataset. The\npreparation phase involved video frame extraction, precise data cropping, annotation, and image combination to create a\nstandardized dataset for subsequent modelling. Then, four modelling approaches were employed, beginning with feature\nextraction techniques such as ORB, SIFT and colour histogram analysis and then using the features to train traditional\nmodels for classification like KNN and SVM algorithms. Secondly, deep learning models, specifically custom-designed"}, {"title": "5.1 Data Acquisition", "content": "Seven classes were considered, including five crops (cassava, sugarcane, maize, grass, cashew, and coffee), weeds in the\ninfield, and unknown images. The first phase of the methodology was focused on collecting a comprehensive dataset.\nFor the drone-captured imagery, sample images from high-resolution aerial views and sides of cashew and coffee were\npicked from the large dataset collected in partnership with Makerere AI Lab, Uganda Marconi Lab, National Coffee\nResearch Institute, and National Crops Resources Research Institute [44]. This data was published for the research\ncommunity to study. In addition, phone videos were recorded in the infield garden, which contained three crops (cassava,\nsugarcane, and maize) and the weeds. The images extracted from the videos provided on-the-ground perspectives.\nFinally, the unknown images were extracted from online image repositories, contributing a breadth of pre-captured\nvisual data. The collection process was designed to ensure a wide variety of visual angles, resolutions, and contexts\nto create a dataset."}, {"title": "5.2 Data Preparation", "content": "Various data preparation techniques were employed after acquiring different corresponding data. The aim was to use\nwell-prepared data. More emphasis was placed on creating good images from the collected infield videos. Several steps\nwere taken to convert the provided video into a dataset of images, crop out specific categories of crops, add these images\nto the drone and online images, and address the slight imbalances from different classes."}, {"title": "5.3 Classification Models", "content": "After preparing the data, the next task was to develop a crop classifier using the data. Four different modelling approaches\nwere considered based on a literature review to ensure that the best model is selected. The first method involved\ndeveloping a classifier using important features or information extracted using popular algorithms such as SIFT and"}, {"title": "5.3.1 Traditional Method Using Feature Extraction:", "content": "The objective was to explore different techniques for extract-\ning meaningful information from images using feature extraction. Three standout techniques were SIFT, ORB, and\ncolour histogram, which were then used for image classification. KNN and SVM were chosen as the classical machine\nlearning algorithms for their simplicity and effectiveness in image classification tasks. For each image in the dataset,\nedge detection was performed, and the SIFT and ORB algorithms were applied to extract feature descriptors that capture\ndistinctive characteristics of the images. Feature descriptors were collected and organized into a data frame, each row\nrepresenting an image. Each image was represented as a row in a data frame for the colour histogram. The features in\nthe data frame were the colour histograms with a fixed number of bins. This method enabled a concise representation\nof colour distribution for each image. For KNN, the hyperparameter tuning process was performed to determine the\noptimal hyperparameter k value in the KNN algorithm. The value of k was systematically varied, and the model's\nperformance was evaluated on the validation set. The iterative process was employed to search for the best k value that\nyielded the most accurate classification results.\nTo evaluate the effectiveness of the KNN and SVM algorithms, a dataset represented in Table 1 was used, considering\nonly the training and testing sets. Using it like that allowed the model to learn from a significant portion of the data\nwhile still being able to evaluate its performance on unseen samples. Additionally, feature normalization was performed\nbefore training the KNN and SVM models to ensure that all features had similar scales and distributions. This step was\nnecessary to prevent any bias towards features with larger magnitudes and ultimately improve the accuracy of the\nclassifier. The results obtained from different feature extraction methods were compared to identify the most effective\napproach for classifying images using the KNN and SVM algorithms.\n(1) SIFT Algorithm: Image feature points are valuable for image identification and classification, as they possess\nunique characteristics that can distinguish one image from another. The SIFT algorithm identifies key points\nacross different scale-spaces and calculates their directions. These key points correspond to distinctive features\nsuch as corners and mutations, enabling effective image classification based on their characteristics. This approach\ndemonstrates robustness against various image distortions, including rotation, scaling, and noise [57]. However,\nthe accuracy of image classification relying on feature points is influenced by the quality of the extracted\nfeature points. In this study, the SIFT algorithm is employed for feature extraction due to its ability to generate\nhigh-quality feature points, resulting in improved classification accuracy compared to other algorithms like\nSURF [7]. Examples of these features are shown in image 8 second column.\n(2) ORB Algorithm: In addition to SIFT, the Oriented FAST and Rotated BRIEF (ORB) algorithm is another popular\nalgorithm for extracting key points and descriptors. ORB is known for its efficiency and robustness in various\ncomputer vision applications, including image classification. Like SIFT, ORB identifies local features or keypoints\nin an image invariant to scale and rotation [21]. These keypoints are characterized by quantitative descriptors,"}, {"title": "5.3.2 Classification using a CNN:", "content": "CNNs have significantly advanced image classification tasks, showing remarkable\nefficacy across various fields, including agriculture. Mimicking the human visual cortex, CNNs autonomously learn\nand extract hierarchical image features. Their architecture comprises convolutional layers to detect local patterns,\npooling layers to reduce spatial dimensions, and fully connected layers to represent and classify high-level features. This\nstructure effectively captures complex spatial dependencies in images, which is particularly useful for crop classification\ntasks.\nThis study explicitly developed a custom CNN architecture for crop differentiation. The CNN comprises multiple\nstacked layers, including convolutional layers with adjustable filters for feature extraction, pooling layers for down-\nsampling, and fully connected layers. Hyperparameters such as the number of filters and neurons were adjusted\nexperimentally to optimize performance. The Rectified Linear Unit (ReLU) was used as the activation function to\nintroduce non-linearity into the network. Additionally, the model's performance was evaluated by training it on our\nlarge enough dataset and testing it on validation data. This iterative process aimed to improve the accuracy, precision,"}, {"title": "5.3.3 Transfer Learning:", "content": "Transfer learning is actually a technique within the field of deep learning, not an alternative\nto it. It involves taking a pre-trained deep learning model and adapting it to a new but related problem. These pre-trained\nmodels are often used in deep learning for computer vision and natural language processing tasks. Developing custom\nneural network models for these tasks requires vast computing and time resources. Pre-trained models can improve\nrelated problems significantly and provide huge jumps in skill. This approach has gained significant popularity due to\nits benefits, especially when compared to training a deep learning model from scratch. Machine learning practitioners\nprefer to use pre-trained models for such tasks.\nOne of the most significant advantages of transfer learning is that it does not require the large datasets necessary\nfor training a deep learning model from scratch. The pre-trained model has already learned a lot of useful features\nfrom its initial training, which can be applied to the new task, even with a relatively small dataset. Secondly, Since the\nmodel is already trained on a large dataset, adapting it to a new task requires much less computational time and less\ntime for fine-tuning the model to find the perfect parameters. This is particularly beneficial when resources or time are\nlimited. Transfer learning can perform better tasks where the available data is limited. The pre-trained model brings in\nknowledge from a related task, which can be especially helpful when the new task needs more data to train a model\neffectively from scratch. When working with small datasets, there is a high risk of overfitting if a model is trained\nfrom scratch. Transfer learning also helps mitigate this risk since the model has already learned generalizable features.\nPre-trained models, especially those trained on large and diverse datasets like ImageNet, have learned complex feature\nrepresentations. Additionally, it allows these advanced features to be leveraged in tasks that may need to be developed\nindependently due to data or resource constraints. Transfer learning is instrumental in domain adaptation, where the\ntask is to apply knowledge learned in one domain to a different but related domain."}, {"title": "5.3.4 Foundation Models:", "content": "Foundation models are deep learning models pre-trained on a large amount of data. They\ncan be fine-tuned for specific tasks and are also great starting points for many applications. They can learn general\nrepresentations that can be applied to many downstream tasks. The term \"foundation\" reflects their broad applicability\nand the fact that they are the basis for specialized models. The main difference between foundation models and transfer\nlearning is the scale and capability of the models. Foundation models are usually cutting-edge deep learning models\nthat provide state-of-the-art performance and require significant computational resources to train and fine-tune. On the\nother hand, transfer learning can be achieved with less computational effort and may not necessarily require models as\nadvanced as foundation models.\nTwo foundation models were trained for this task YOLOv8 (You Only Look Once, version 8) and DINOv2 (Knowledge\nDistillation with No Labels, version 2). YOLOv8 is an evolution of the YOLO family, known for its real-time object\ndetection capabilities, which can be crucial for identifying and classifying crops in images quickly and accurately. Its\nability to process images in a single pass enables rapid detection of multiple crop types within a scene, making it"}, {"title": "5.3.5 Evaluation Metrics:", "content": "To assess the performance of the crop classification models, we employed a suite of\nstandard evaluation metrics, each providing insights into different aspects of the model's predictive capabilities. Their\nsummary and equations follow where TP is the true positive, FN is the false negative, and FP is the false positive.\n\u2022 Recall measures the ability of the model to identify all relevant instances of a particular class. In crop classification,\na high recall rate indicates that the model can identify most of the crop instances from the data, which is crucial\nfor applications where missing a particular crop type could have significant consequences, such as pest detection\nor yield estimation.\n$$recall = \\frac{TP}{TP + FN}$$\n(1)\n\u2022 Precision quantifies the model's accuracy in labelling an instance belonging to a particular crop class. A high\nprecision score reflects the model's ability to minimize false positives, ensuring it is classified correctly when a\ncrop type is identified. This is especially important in precision agriculture, where misclassification can lead to\ninappropriate treatment or harvesting strategies.\n$$precision = \\frac{TP}{TP + FP}$$\n(2)\n\u2022 Accuracy is the most intuitive performance measure, and it is simply a ratio of correctly predicted observations\nto the total observations. It gives a general sense of how often the model is correct across all classes, but it may\nnot be as informative in imbalanced datasets where some crop types are much more common than others.\n$$Accuracy = \\frac{Number\\,of\\,Correct\\,Predictions}{Total\\,Number\\,of\\,Predictions}$$\n(3)\n\u2022 Confusion Matrix offers a detailed model performance breakdown, showing the number of correct and incorrect\npredictions made for each class. This matrix is particularly useful for identifying specific classes where the model\nmay be underperforming and need additional training data or re-tuning of the model parameters.\nThese metrics were computed using a validation dataset the model did not see during training. By analyzing these\nmetrics, we can gauge the robustness and reliability of our crop classification models, ensuring they are both accurate\nand practical for deployment in agricultural settings."}, {"title": "5.3.6 Explainable AI (XAI):", "content": "In advanced machine learning and AI, achieving high-performance metrics across\nvarious models, including traditional machine learning, deep learning architectures, transfer learning, and foundation\nmodels, marks a significant accomplishment. However, these models' complexity and often opaque nature necessitate"}, {"title": "6 EXPERIMENTAL RESULTS AND DISCUSSION", "content": "In this section, the results and analysis of the work on the models and their performance are presented."}, {"title": "6.1 First Approach: Feature Extraction with Traditional Algorithms and Classical ML Models", "content": "For the traditional feature extraction method, after performing feature selection and preprocessing, the features were\nprepared for training by partitioning them into a 70% training set and a 30% test set. Both K-nearest neighbours and SVM\nalgorithms were trained using the prepared features. For KNN, a prediction process that considered five neighbours\nwas utilized. The training set comprised 954 images, while the test set contained 410 images.\nThe findings, as summarized in Table 2, indicate notable differences in performance among various feature extractors\nand machine learning models. Firstly, combining Color Histogram with SVM achieved the highest accuracy in training\nat 96% and testing at 93%. It also showed impressive precision, recall, and F1 scores above 90%. Additionally, this"}, {"title": "6.2 Second Approach: DNN", "content": "Deep learning enables direct input of raw data into the model, resulting in end-to-end learning. This contrasts traditional\nmethods requiring separate stages, including feature extraction, selection, and classification. Each stage requires fine-\ntuning and optimization, and the process is often a time-consuming and expertise-intensive process. Deep learning\neliminates all these and automatically allows for faster development and deployment of models.\nThe experiments on deep learning models for crop classification provide a convincing comparison between a custom\nCNN and the well-known deep learning neural network AlexNet architecture. The findings, presented in Table 3,"}, {"title": "6.3 Comparison Between the first approach and the second approach", "content": "The first approach used for this project was to use traditional feature extractors such as SIFT, ORB, and Color Histogram,\ncombined with classical machine learning models like KNN and SVM models to classify various crop types in image\nclassification tasks. This approach emphasizes the significance of feature selection, particularly the quality and relevance\nof manually chosen features, such as colour, to ensure effective classification. The performance of different feature\nextractors and classifier combinations varied significantly, with Color Histogram and SVM showing the highest\neffectiveness. This approach requires less computational power than deep learning models, as indicated by their smaller\nmodel sizes and faster processing times. However, it may need more depth of learning and generalization capabilities\nthat deep learning offers, particularly in cases where crop images have complex and subtle features.\nA custom CNN and the AlexNet architecture were utilized for the deep learning approach, demonstrating the power\nof convolutional neural networks in automatically learning hierarchical feature representations from the images. The\ncustom CNN, developed specifically for this task, outperformed AlexNet, showing higher accuracy, precision, recall, and\nF1 scores. Additionally, DNNs, especially CNNs, excel in handling high-dimensional data and capturing intricate image\npatterns, which is critical for accurate crop classification. Although using deep learning models for crop classification"}, {"title": "6.4 Third Approach: Transfer learning", "content": "Transfer learning as a technique that uses a pre-trained model that has already learned important features from a\nrelated task. It significantly reduces the computational time and effort needed to fine-tune the model for a new task and\nmitigates the risk of overfitting when working with small datasets. Since the pre-trained model has already learned\ngeneralizable features, it produces better results."}, {"title": "6.5 Fourth Approach: State-of-the-Art Foundation Models", "content": "In the fourth approach to crop classification, the foundation models, YOLOv8 and DINOv2, have been implemented,\nrepresenting cutting-edge developments in deep learning and computer vision. These models demonstrate advanced\nneural networks' evolving capabilities and potential in specialized tasks like crop classification."}, {"title": "6.6 General Comparison", "content": "In summarizing the four distinct approaches to crop classification: traditional machine learning with feature extraction,\ncustom and established deep learning architectures, transfer learning models, and foundation models, a comprehensive\nunderstanding emerges, highlighting the strengths and suitability of each method for specific scenarios.\n\u2022 Traditional Machine Learning with Feature Extraction: The combination of Color Histogram and SVM\nemerged as the best in this category, showing high effectiveness with a test accuracy of 93% and precision,\nrecall, and F1-scores above 90%. It is notably efficient regarding computational resources, making it suitable for\nenvironments with limited processing power. However, relying on manually selected features might limit its\ncapability for deployment."}, {"title": "6.7 Explainable Al", "content": "It is impressive to achieve high accuracy in Al models. However, more is needed for practical applications. These\nmodels have a 'black-box' nature that conceals the reasoning behind their predictions. Therefore, it is necessary to use\nXAI to make these models more transparent and understandable. As previously discussed, XAI is crucial for ensuring\ntransparency and ethical integrity in the real-world deployment of AI. XAI allows for understanding how these models\narrive at their decisions, which is essential for"}]}