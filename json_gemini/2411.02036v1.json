{"title": "Explainable cognitive decline detection in free dialogues with a Machine Learning approach based on pre-trained Large Language Models", "authors": ["Francisco de Arriba-P\u00e9rez", "Silvia Garc\u00eda-M\u00e9ndez", "Javier Otero-Mosquera", "Francisco J. Gonz\u00e1lez-Casta\u00f1o"], "abstract": "Cognitive and neurological impairments are very common, but only a small proportion of affected individuals are diagnosed and treated, partly because of the high costs associated with frequent screening. Detecting pre-illness stages and analyzing the progression of neurological disorders through effective and efficient intelligent systems can be beneficial for timely diagnosis and early intervention. We propose using Large Language Models to extract features from free dialogues to detect cognitive decline. These features comprise high-level reasoning content-independent features (such as comprehension, decreased awareness, increased distraction, and memory problems). Our solution comprises (i) preprocessing, (ii) feature engineering via Natural Language Processing techniques and prompt engineering, (iii) feature analysis and selection to optimize performance, and (iv) classification, supported by automatic explainability. We also explore how to improve ChatGPT's direct cognitive impairment prediction capabilities using the best features in our models. Evaluation metrics obtained endorse the effectiveness of a mixed approach combining feature extraction with ChatGPT and a specialized Machine Learning model to detect cognitive decline within free-form conversational dialogues with older adults. Ultimately, our work may facilitate the development of an inexpensive, non-invasive, and rapid means of detecting and explaining cognitive decline.", "sections": [{"title": "1 Introduction", "content": "Progressive neurological disorders (e.g., Alzheimer's disease) affect 40 million people worldwide [1] and are a common cause of death [2, 3]. However, only 25% of affected people receive a diagnosis. There are multiple reasons for this, including stigma and a lack of awareness and resources [4, 5]. The number of older adults with Alzheimer's disease is predicted to rise to 150 million by 2050 [6]. Consequently, detecting pre-illness stages and analyzing the progression of neurological disorders using cost-effective and efficient intelligent systems is crucial to ensure timely diagnosis, risk assessment, and early intervention [3, 7].\nNumerous cognitive tests (CT, e.g., Alzheimer's Disease Assessment Scale-Cognition - ADASCog, Cognitive Dementia Rating - CDR, Mini-Mental State Examination - MMSE, Montreal Cognitive Assessment - MOCA, etc.) are currently used to diagnose and monitor neurological disorders, but they need to be applied manually and are therefore costly [5, 8, 9]. These tests can be automated with the help of Artificial Intelligence (AI), enabling more frequent screening of target populations [10, 11] and the conduct of longitudinal studies. The proposal described in this work consists of an automatic, continuous evaluation of cognitive performance based on engaging dialogues established with end users. Engagement fosters evaluation over time, which is of interest to longitudinal studies. The dialogues are supported by advanced AI techniques.\nAI-based solutions for clinical assessment purposes is a fast-growing research field, and numerous studies have already analyzed applications in the fields of progressive neurological disorders and dementia [12, 13, 14, 15, 16]. Machine Learning (ML) models [1, 17, 18, 19] (e.g., Convolutional Neural Networks - CNN [20]) and Natural Language Processing (NLP) techniques [21, 22, 15, 23] have been applied to textual and voice data. These techniques can effectively predict cognitive impairment and content-dependent and context-independent features can be leveraged to improve the predictive performance of ML models in this area.\nAmong the multiple bio-markers available for the study of cognitive decline [24, 25, 26], language acquisition is an inexpensive, non-invasive, and readily accessible tool [9]. However, language conceals large volumes of information within complex relationships that can be difficult to decipher. Large Language Models (LLMS) are better equipped to navigate and process complex information and have therefore received increasing attention in the medical field [1, 27, 28]. LLMs have been used to analyze images and prescribe medical treatments and have also proven capable of passing medical accreditation exams"}, {"title": "2 Related work", "content": "AI solutions for healthcare seek to enhance diagnosis and treatment while simultaneously optimizing resources [42]. One promising line in this regard is the development of intelligent assistants or chatbots [43, 44]. A representative example is the work by Kurtz et al [45]. They presented a cognitive decline detection solution based on a voice assistant. The authors exploited lexical and semantic features, and embeddings for that purpose.\nAlthough many NLP-based speech analysis solutions exist for the early pre-diction of Alzheimer's disease [46, 47], there is limited, preliminary research on the use of LLMS in the field, apart from specific use cases [9] and medical applications [48].\nLLMS have the ability to offer powerful NLP functionalities that can be uti-lized in various medical tasks [49]. They have demonstrated clinical reasoning abilities [50], passed medical licensing exams [51], provided medical advice [52], and even generated clinical notes [53].\nThe following prior works explore using LLMS for cognitive evaluation.\nYuan et al [54] applied BERT and ERNIE models to model disfluencies and language problems in patients with Alzheimer's disease. The experimental data consisted of transcriptions from cognitive tests in the ADRESS (Alzheimer's Dementia Recognition through Spontaneous Speech) data set. However, after fine-tuning, they relied mainly on word embeddings extracted from the LLMS. The only side (content-independent) features considered were word frequency and speech pauses, thereby limiting the generalizability of the results when non-standard tests were used. Qiao et al [19] extracted disfluency measures and combined them into a stacking classification approach. The absence of content-and context-independent side features makes this approach less robust than ours in settings prone to contextual changes.\nRoshanzamir et al [28] combined transformer-based deep neural network language models (BERT, XLM, XLNet) with ML (Logistic Regression - LR, Long Short-Term Memory - LSTM, CNN) to detect Alzheimer's disease using image description testing. Their approach exclusively used embeddings extracted from transcripts and content-dependent features. Zhu et al [55] screened for dementia using both non-semantic (speech pauses) and semantic features (word embeddings) extracted from cognitive test speech data by BERT. As in the work by Yuan et al [54], the solution was fine-tuned with non-semantic information. However, compared to our solution, the approaches in both [28] and [55] were highly dependent on semantic information.\nLi et al [56] estimated the ability of Llama and ChatGPT LLMS to detect cognitive impairment from electronic health record (EHR) notes. The prediction outcome, combined with manual assessments by experts, was used to fine-tune the BERT model. However, no information is provided about the system's performance without expert backup.\nAgbavor and Liang [9] predicted dementia from standard cognitive tests driven by GPT-3. The authors both classified and predicted the severity of Alzheimer's disease. For the classification task, they employed traditional ML"}, {"title": "2.1 Contributions", "content": "We propose using the ChatGPT LLM to extract high-level reasoning, content-independent side features from loosely driven entertainment dialogues with a chatbot (as explained in Section 4.1). We identified relevant features by ana-lyzing previous research on cognitive function decline (e.g., changes in the content, comprehension, decreased awareness, increased distraction, memory problems, etc.) [1, 3, 8, 28, 57]. In addition to being context-independent and thus adequate for free dialogue, unlike word-embedding or content-dependent features, the side features used in our proposal support much more inter-pretable descriptions of the decisions on cognitive decline. As shown in Table 1, few studies have applied XAI techniques to our target problem. Although Bellantuono et al [58] did not use LLMs, they combined an RF classification model for dementia with SHapley Additive ExPlanations (SHAP) techniques to infer the contributions of the features to the model's predictive performance. A similar approach was applied by Lombardi et al [59], who stated that XAI was still in its infancy in computational neuroscience.\nSumming up, our work is the first to apply ML techniques to high-level reasoning features extracted from free-form dialogues using an LLM to detect cognitive decline. The ML techniques used were Naive Bayes - NB, Decision Tree - DT, and RF, and we further describe the decisions using explainability techniques."}, {"title": "3 Methodology", "content": "Figure 1 illustrates the proposed solution for assessing cognitive impairment using free dialogues with older adults. The solution is composed of (i) a pre-processing module to prepare the content for further analysis, (ii) a feature engineering module that generates an appropriate and comprehensive set of features using NLP techniques to train the cognitive design classification mod-els, (iii) a feature analysis and selection module, and (iv) a classification module, which is evaluated using standard ML metrics (accuracy, precision, recall) and provides explainability results. Finally, the most representative fea-tures of the ML model were selected to evaluate if they can enhance the direct cognitive impairment prediction capabilities of the ChatGPT LLM by means of prompt engineering."}, {"title": "3.1 Preprocessing module", "content": "The conversations used in this research are free dialogues between users and an intelligent conversational assistant [60], organized into daily sessions and automatically transcribed into text. The preprocessing module is essential for ensuring the quality of the input data in the feature engineering process involv-ing both prompt engineering and n-gram generation. For prompt engineering, NLP techniques are applied to remove emoticons and hashtags. For n-gram gen-eration, images, links, and special characters (e.g., &, $, \u20ac) are removed using regular expressions. Then, the textual content is tokenized and lemmatized. Because this use case is based on free dialogues, an ad hoc stopword list was created to exclude the terms 'no', 'yes' (si), 'more' (m\u00e1s), 'but' (pero), 'very' (muy), 'without'(sin), 'much' (mucho), \u2018little', (poco) and 'nothing' (nada). The final step is the removal of numeric values, isolated characters, and accents."}, {"title": "3.2 Feature engineering module", "content": "Once the textual content of the user's utterances is processed, the feature engi-neering module generates a broad set of features, detailed in Table 2. Each entry in the data set corresponds to a user dialogue session. Special atten-tion is paid to user engagement (features 1-8), emotional state (features 9-12),"}, {"title": "3.3 Feature analysis & selection module", "content": "In the feature analysis and selection stage, irrelevant and other features that could degrade the system's performance are removed to ensure optimal training of the ML classifiers. We have implemented two feature analysis and selection techniques: (i) a relevance-based technique using a tree algorithm and (ii) correlation analysis with the Pearson coefficient.\nA meta-transformer wrapper based on a tree-based ML model is applied to select the most significant feature components for training the classification module, regardless of the specific ML model used (thus, we follow a model-agnostic strategy [61]). The wrapper, using the Mean Decrease in Impurity (MDI) technique, leverages importance weights to identify and eliminate fea-tures based on their impurity contributions [62]. This technique calculates the"}, {"title": "3.4 Classification module", "content": "Three different scenarios are considered:\nScenario 1 trains an ML classification based exclusively on content-dependent textual features derived from user utterances (features 27-28 in Table 2). This scenario is intended to evaluate n-grams as a source, as in prior works, [19, 21, 22] and serves as a baseline for our study.\nScenario 2 trains an ML classification considering context-independent side features 1-26 as defined in Table 2, where the base measurements are calculated by applying an LLM to user utterances, the machine-user utterance pairs and the whole dialogue, depending on the case.\nScenario 3 evaluates the performance of the LLM as a classifier of cogni-tive impairment [56, 33] using two approaches: (a) the LLM is directly used to analyze the dialogue of a user session as a whole and guess the target \u201ccogni-tive decline\" with prompt engineering, and (b) the same setting plus prompt enhancements based on a pick of the most relevant features from scenario 2, using the Pearson coefficient as described in the previous section.\nIn summary, scenario 1 is our baseline, and scenario 2 is used to evaluate our approach. Scenario 3 uses an LLM exclusively to directly classify cognitive impairment and compare its performance with our proposal, in which the LLM is employed to generate high-level reasoning features.\nIn scenarios 1 and 2, we use the meta-transformer wrapper to identify the most significant features (see Section 3.3).\nIn scenarios 2 and 3, we apply prompt engineering. The prompts are divided into a context section, a request, and the output format. Specific prompts are used in scenario 2 to generate the measurements for the context-independent side features from the textual input (both for features 1-24 and counter-type features 25-26). In scenario 3, two prompts directly address the cognitive impairment level. The first prompt directly applies to the whole dialogue to assess cognitive decline. The second adds a pick of the most relevant fea-tures from scenario 2, as previously explained, to the context section of the first prompt. Algorithms 2, 3, 4 and 5 respectively describe the step-by-step processes in scenarios 1, 2, 3a and 3b."}, {"title": "3.5 Explainability module", "content": "Natural language explanations about decisions on users' cognitive state are based on the relevant feature components in scenario 2 (the components selected by the meta-transformer wrapper). The relevant features are arranged in descending order of relevance. Note that counter-type features 25-26 are normalized by the number of words. For each decision to be explained, six components of features 1-26 with the highest and lowest values, three of each type, are employed. In the case of a tie components are chosen randomly for the explanation template."}, {"title": "4 Evaluation and discussion", "content": "This section provides an overview of the experimental data set, describes the implementations to facilitate their reproducibility, and presents the results achieved. The experiments were conducted on a computer with the following specifications.\n\u2022 Operating System: Ubuntu 22.10 LTS 64 bits\n\u2022 Processor: INTEL\u00ae Core\u2122 i7-12700K\n\u2022 RAM: 32 GB DDR4\n\u2022 Disk: 1 TB SSD"}, {"title": "4.1 Experimental data set", "content": "The dialogues were collected via the Celia web application\u00b2 (see Figure 2), and transcribed to text using the Google Cloud Speech-to-Text library\u00b3. Celia has been designed to entertain and accompany elderly people. A rich dialogue is achieved by combining questions generated through an LLM with services such as weather, curiosities, and saint days, which vary throughout the year. This allows the free generation of a dynamic conversation on varying topics based on user preferences.\nThe experimental data set\u2074 consists of 8220 utterances in Spanish from 523 sessions held with 42 users registered in the application. Of these, 14 had cognitive impairment. Each user participated in an average of 12.45 sessions, with a standard deviation of \u00b1 6.32 sessions, and each session had on average 15.72 utterances, with a standard deviation of \u00b1 4.89 utterances. Each session had 67.95 words on average, with a standard deviation of \u00b1 70.14 words. Utterances by people with cognitive impairment comprised 49.55 words \u00b1 43.74 on average. This represents a reduction of 38.41 % compared with people without this condition (80.45 words \u00b1 81.15). Table 3 details the percentages"}, {"title": "4.2 Preprocessing module", "content": "Hashtags, images, links, and special characters were identified using the regular expressions in Listing 1 and removed. Emoticons and accents were deleted using the NFKD Unicode normalization form\u2075. The NLTK stop word list was used to delete stop words\u2076. Finally, textual content was lemmatized using the es_core_news_md core model from the spacy Python library\u2077."}, {"title": "4.3 Feature engineering module", "content": "Side features (1-26 in Table 2) were obtained with the gpt-3.5-turbo\u2078 LLM, selected for its performance and cost-effectiveness ($0.0005/1,000 tokens for prompt text input and $0.0015/1,000 tokens for model text output at the time this paper was written).\nListings 2 and 3 respectively illustrate the prompt used for feature engineer-ing and the measurements obtained from the LLM for features 1-24 in scenario 2 for the particular machine-user utterance pair example at the bottom of List-ing 2. The same prompt was used for the whole dialogue (for features 1-24). Listings 4 and 5 respectively show the prompt and the results obtained for features 25-26, for the user utterance at the bottom of Listing 4.\nListings 6 and 7 respectively show the prompt for the baseline in scenario 3 (i) and an example of the corresponding response by the LLM. Finally, Listing 8 shows the prompt designed for the enhanced scenario 3 (ii), including the effect of relevant features selected from scenario 2 as described in Section 3.3. The ultimate goal is to improve the ability of the LLM to detect cognitive decline directly.\nContent-dependent features 27-28 in Table 2 were generated with the CountVectorizer library\u2079 from the scikit-learn Python library. The opti-mal parameters in Listing 9 and 10 were calculated using the GridSearchCV\u00b9\u2070 function from the scikit-learn Python library. A total of 1282 features were obtained."}, {"title": "4.4 Feature analysis & selection module", "content": "In scenarios 1 and 2, features were selected with the SelectFromModel\u00b9\u00b9 transformer wrapper using RF, with default parameter settings. The result of the selection was:\n\u2022 Scenario 1: 435 char-gram and word-gram features.\n\u2022 Scenario 2: 262 feature components comparing statistics from the current session with those of other sessions plus 7 feature components corresponding to statistics from the current session.\nThe Pearson correlation coefficient\u00b9\u00b2 was employed in scenario 3 to further select features from scenario 2 with a stronger direct or indirect relationship with the target. Only features with a correlation over 0.45 were selected based on empirical tests (see Table 4). All the features corresponded to feature com-ponents, comparing statistics from the current session and those from other sessions."}, {"title": "4.5 Classification module", "content": "The implementations of the ML algorithms were:\n\u2022 NB. Gaussian Naive Bayes\u00b9\u00b3 from the scikit-learn Python library.\n\u2022 DT. DecisionTreeClassifier \u00b9\u2074 from the scikit-learn Python library.\n\u2022 RF. RandomForestClassifier \u00b9\u2075 from the scikit-learn Python library.\nFor each implementation, optimal hyperparameters were tuned with the aforementioned GridSearchCV method using 10-fold cross-validation. Listings 11-13 show the ranges of values explored for scenarios 1 and 2. The final parameters selected were the following:\nScenario 1\n\u2022 DT: splitter=random, class_weight=None, max_features=log2, max_depth=100, min_samples_split=0.1, min_samples_leaf=0.001\n\u2022 NB: var_smoothing=1e-9\n\u2022 RF: n_estimators=75, class_weight=None, max_features=log2, max_depth=100, min_samples_split=5, min_samples_leaf=1\nScenario 2\n\u2022 DT: splitter=best, class_weight=None, max_features=None, max_depth=100, min_samples _split=0.001, min_samples_leaf=0.001\n\u2022 NB: var_smoothing=1e-6"}, {"title": "4.6 Explainability module", "content": "To explain the model's predictions, we used the template in Listing 14. This template details the most relevant <features> used to classify a <user> from a given <conversation> (a dialogue session in our case). The features used in this template are the six relevant features selected using the meta-transformer wrapper, comprising several components described in Section 3.2. Only the components with the highest and lowest values were considered for explain-ability after discarding the minimum and maximum statistics. The values of these components were sorted, and the three largest and three most minor were used."}, {"title": "5 Conclusions", "content": "This work is the first to apply ML techniques to high-level reasoning fea-tures extracted using LLMS from free dialogues to detect cognitive decline with the Celia entertainment chatbot. High-level reasoning features are context-independent and are, therefore, more widely applicable for characterizing free dialogues than word-embedding or content-dependent features. They also support significantly more interpretable descriptions of the decisions using explainability techniques. The essential advantage of free dialogues with engag-ing systems is that they may encourage end users to participate in longitudinal studies, which currently need to be more feasible in healthcare systems due to the high cost of manual screening.\nA mixed approach combining a specialized ML with feature extraction using the ChatGPT LLM outperformed direct evaluation of cognitive decline by ChatGPT, even when prompt engineering was used to boost ChatGPT with the best features of the ML model. The performance levels achieved are remarkable.\nIn future work, we plan to train our models using a streaming mode that incorporates the session history. We will also employ new methodologies (e.g., reinforcement learning) to study different motivational topics and analyze how new features influence prediction outcomes. Note that we are only using a LLM to obtain precise answers on the semantic relationships between different pieces of text. Thus, our approach is less affected by certain issues of generative LLMS (e.g., context and memory management, layer pruning, hallucination issues, etc.) than the direct application of a LLM in scenario 3. However, in future work, we plan to analyze the sensitivity of these issues."}, {"title": "6 Declarations", "content": "Funding\nThis work was partially supported by (i) Xunta de Galicia grants ED481B-2022-093, ED481D 2024/014, and ED431C 2022/04, Spain; (ii) Ministerio de Ciencia e Innovaci\u00f3n grant TED2021-130824B-C21, Spain; and (iii) University of Vigo/CISUG for open access charge.\nCompeting interests\nThe authors have no competing interests to declare relevant to this article's content.\nAuthors contribution\nFrancisco de Arriba-P\u00e9rez: Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Resources, Data Curation, Writing - Original Draft, Writing - Review & Editing, Visualization, Supervision, Project administration, Funding acquisition. Silvia Garc\u00eda-M\u00e9ndez: Conceptual-ization, Methodology, Software, Validation, Formal analysis, Investigation,"}]}