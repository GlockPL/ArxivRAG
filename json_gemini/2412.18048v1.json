{"title": "Fair Knowledge Tracing in Second Language Acquisition", "authors": ["Weitao Tang", "Dr. Guanliang Chen", "Shuaishuai Zu", "Jiangyi Luo"], "abstract": "In the domain of second-language acquisition, predictive modeling serves as a pivotal tool for facilitating educators in implementing diversified teaching strategies, thereby garnering extensive research attention. Despite the prevalent focus on model accuracy in most existing studies, the exploration into model fairness remains substantially underexplored. Model fairness pertains to the equitable treatment of different groups by machine learning algorithms. It ensures that the model's predictions do not exhibit unintentional biases against certain groups based on attributes such as gender, ethnicity, age, or other potentially sensitive characteristics. In essence, a fair model should produce outcomes that are impartial and do not perpetuate existing prejudices, ensuring that no group is systematically disadvantaged. In this research, we evaluate the fairness of two predictive models based on second-language learning, utilizing three tracks from the Duolingo dataset: en_es (English learners who speak Spanish), es_en(Spanish learners who speak English), and fr_en(French learners who speak English). We measure\n(i) algorithmic fairness among different clients such as iOS, Android and Web and\n(ii) algorithmic fairness between developed countries and developing countries.\nOur findings indicate:\n1) Deep learning exhibits a marked advantage over machine learning when applied to knowledge tracing based on second language acquisition, owing to its heightened accuracy and fairness.\n2) Both machine learning and deep learning algorithms exhibit a noticeable bias favoring mobile users over their non-mobile counterparts.\n3) Compared to deep learning algorithms, machine learning algorithms showcase a more pronounced bias against developing countries.\n4) To strike a balanced approach in terms of fairness and accuracy, deep learning is identified as being more apt for making predictions in the en_es and es_en tracks, whereas machine learning emerges as a more suitable option for the fr_en.\nThis study underscores the necessity to delve deeper into the realms of fairness in predictive models, ensuring equitable educational strategies across diverse clients and countries.", "sections": [{"title": "1 Introduction", "content": "The historical reliance on traditional metrics like student performance, feedback, and participation to shape educational methodologies was fraught with potential inaccuracies [1]. Such inaccuracies mainly arise because humans cannot process the vast amount of data generated during the educational process like machines can. A deeper delve suggests that these inaccuracies might be influenced by a range of psychological factors [2]. For instance, educators might be influenced by\n1. The limits of short-term memory.\n2. Feelings of fatigue or boredom.\n3. A positivity bias, where there is a predisposition towards favoring positive feedback from students.(This means that an educator might harbor an undue expectation for positive comments, thereby skewing their interpretation of student feedback)\nAs a result, a single poor examination outcome might prompt an educator to simplify the test content, risking a dilution of educational standards. To mitigate these issues, in 1994, Corbett and Anderson introduced the concept of \"knowledge tracing\" [3]. The crux of this approach was to harness computational technology to model various student indicators such as grades, feedback, participation, and other features. By doing so, predictions about students' future performance could be made, enabling the crafting of more appropriate educational strategies.\nEmploying computational techniques effectively reduces the chances of human-induced errors, enhancing the precision of outcomes. In recent times, educational technology increasingly incorporates artificial intelligence to employ data and predictive models [4, 5, 6, 7, 8]. This provides tailored support and insights for students, educators, and administrators alike [9, 10]. This process closely resembles an adaptive system. As students accumulate knowledge, the system makes predictions regarding their understanding and offers varied learning materials tailored to their needs [11].\nAs outlined by [12], the Intelligent Tutoring System (ITS Figure 1) functions as an adaptive feedback mechanism. The ITS poses questions to students and, based on their responses, discerns their knowledge state. This knowledge state offers insight into the student's proficiency in various skills. For instance, if a student consistently demonstrates over 90% accuracy in addition tasks, one could infer mastery over the skill of addition. However, the same student might struggle with problems combining addition and subtraction, indicating a lack of proficiency in the latter. Certain skills might be interdependent; for example, mastering calculus presupposes proficiency in basic arithmetic operations like addition and subtraction. As learning progresses, students might forget certain concepts, with more complex skills generally having a higher forgetting rate."}, {"title": "2 Related Work", "content": "In the scope of this project, a compilation of 16 research articles was undertaken, all of which explore the application of knowledge tracing on the Duolingo dataset. These research contributions can be broadly bifurcated into three primary categories: those employing Traditional Machine Learning-based Algorithms (7 articles), and those utilizing Deep Learning-based Algorithms (9 articles), with the objective of predicting student performance metrics. A notable 13 of these papers presented predictions across all three tracks, while a pair exclusively targeted the English track, and a singular article was dedicated to investigating both the Spanish and French tracks. Subsequent sections will delve into a meticulous analysis of the models adopted in these articles, along with an exploration of their respective performance metrics in a detailed manner."}, {"title": "2.1 Methodological Approaches", "content": "To better compare the models in these 16 papers, two metrics will be introduced. One is the F1 score, which represents the harmonic mean of Precision and Recall. The other is AUC (Area Under the Curve) [19]. AUC is the area under the ROC (Receiver Operating Characteristic) curve, which primarily describes the relationship between the False Positive Rate and the True Positive Rate. Both values are closer to 1, indicating a better model."}, {"title": "2.2 Comparison between all Algorithms", "content": "Table 1 illustrates that models integrating deep learning or combining it with machine learning tend to outperform, whereas standalone machine learning models might fall short in certain tasks. This points towards a necessity for neural networks in Second Language Acquisition (SLA) tasks to delve deeper into feature extraction, considering the Duolingo dataset may present complexities beyond the capacity of traditional machine learning models. Notably, [20] delivered superior outcomes by employing an encoder-decoder architecture and multi-task learning, which facilitated the use of shared representation layers and the extraction of features even in data-scarce scenarios. Conversely, the LM-KT model presented in [21] exhibited subpar performance, potentially due to intrinsic model constraints and a lack of comprehensive model optimization by the authors. Hence, the selection of an appropriate model emerges as pivotal in the tracing of knowledge in SLA. Both high-achieving and"}, {"title": "3 Experiment", "content": null}, {"title": "3.1 Experimental Environment and Set Up", "content": "All experiments were conducted on a high-performance Alienware X17R2 laptop. The specifications of the device are as follows:\nDevice: Alienware X17R2 Laptop\nCPU: Intel i9-12900HK\nMemory: 64GB DDR5\nThe experiments utilized TensorFlow for both training the models on the CPU."}, {"title": "3.2 Accuracy", "content": null}, {"title": "3.2.1 Accuracy on three tracks (en_es, es_en, fr_en)", "content": null}, {"title": "3.2.2 Accuracy on Client (ios, android, web)", "content": null}, {"title": "3.2.3 Accuracy on Country (developed countries and developing countries)", "content": null}, {"title": "Performance Across Countries:", "content": "Developed Countries: The performance of both models is closely matched in developed countries. Interestingly, for the FR_EN track, GBDT has a distinct advantage in AUC.\nDeveloping Countries: In developing countries, Multi-task learning generally leads in F1 score. However, their AUCs are again closely matched. This suggests that while the more complex model may predict certain classes better, both models rank their predictions similarly."}, {"title": "3.3 Fairness", "content": "Understanding the model's performance in terms of fairness is especially crucial for platforms like Duolingo, where feedback directly influences learners' progress and motivation. Any disparity can lead to differing user experiences, potentially leading to decreased trust in the platform or even reduced learning outcomes for a particular user group."}, {"title": "3.3.1 Fairness on Client", "content": "Figures 2 shows the ABROCA between iOS and Android in en_es track for GBDT. Figure 3 shows the ABROCA between iOS and Android in en_es track for Multi-task learning."}, {"title": "3.3.2 Fairness on Country", "content": "Figures 4 shows the ABROCA between developed country and developing country in en_es track for GBDT. Figure 5 shows the ABROCA between developed country and developing country in en_es track for Multi-task learning. To better elucidate the differences in ABROCA with respect to country-side performance between GBDT and Multi-task learning, we present the following table 6.\nWe can observe that Multi-task learning demonstrates greater fairness than GBDT in the en_es and es_en tracks, while it exhibits less fairness in the fr_en track. This indicates that for the fr_en track, GBDT possesses superior accuracy and fairness. Conversely, for the en_es and es_en tracks,"}, {"title": "4 Discussion and Conclusion", "content": "This paper examines the fairness and accuracy of two knowledge tracing algorithms, GBDT and Multi-task learning, in the domain of second language acquisition across three linguistic tracks: en_es, es_en, and fr_en. Our comprehensive analysis reveals key insights into the interplay between algorithm choice and its impact on fairness and accuracy.\nFirstly, our findings underscore that deep learning, exemplified here by Multi-task learning, tends to be more apt for knowledge tracing based on second language acquisition than machine learning algorithms like GBDT, owing predominantly to its higher accuracy and fairness, especially in the en_es and es_en tracks. Secondly, a discernible discriminatory tendency against non-mobile end-users was observed in both machine learning and deep learning algorithms. Furthermore, our third finding points towards a more pronounced bias inherent in machine learning algorithms, particularly towards developing countries, as opposed to deep learning algorithms.\nTo assess the potential algorithmic bias towards developing countries and users of web and android platforms (excluding iOS), we employed ABROCA as our fairness measurement tool. The fairness and accuracy analysis starkly illuminated the superiority of Multi-task learning in terms of algorithmic fairness compared to the GBDT algorithm.\nDespite Multi-task learning overwhelmingly emerging as the preferable option due to its enhanced accuracy and algorithmic fairness, our fourth finding elucidates a nuanced approach to algorithm selection across different linguistic tracks. While Multi-task learning would be the optimal choice for en_es and es_en due to its fairness and predictive proficiency, for fr_en track, the GBDT algorithm is recommended due to its superior accuracy and fairness, despite Multi-task learning's broader applicability and success.\nIn summary, while Multi-task learning broadly demonstrates enviable aptitude in knowledge tracing, especially when balancing fairness and accuracy, the GBDT algorithm's selective applicability, such as in fr_en track scenarios, indicates that a tailored, context-specific approach to algorithm selection offers an optimal strategy in ensuring equitable and accurate knowledge tracing across varied linguistic and platform-specific contexts."}, {"title": "5 Limitations", "content": "We acknowledge several limitations in our study. Firstly, our analysis was confined to a single predictive task: knowledge tracing in second language acquisition, utilizing only the Duolingo dataset. This limitation restricts the generalizability of our findings across different datasets and contexts.\nTo enhance the universality and robustness of our results, future studies should explore a broader range of machine learning and neural network algorithms and apply them to other predictive tasks in varied datasets. Additionally, our examination of demographic differences was limited to two specific student groups: Client and Country. Future research should extend this analysis to include other demographic factors, such as age and gender, to provide a more comprehensive evaluation of algorithmic fairness across diverse student populations."}, {"title": "A Appendix / supplemental material", "content": "All the graphs related to fairness on Client are listed below"}]}