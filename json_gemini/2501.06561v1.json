{"title": "Where to Go Next Day: Multi-scale Spatial-Temporal Decoupled Model for Mid-term Human Mobility Prediction", "authors": ["Zongyuan Huang", "Weipeng Wang", "Shaoyu Huang", "Marta C. Gonz\u00e1lez", "Yaohui Jin", "Yanyan Xu"], "abstract": "Predicting individual mobility patterns is crucial across various applications. While current methods mainly focus on predicting the next location for personalized services like recommendations, they often fall short in supporting broader applications such as traffic management and epidemic control, which require longer period forecasts of human mobility. This study addresses mid-term mobility prediction, aiming to capture daily travel patterns and forecast trajectories for the upcoming day or week. We propose a novel Multi-scale Spatial-Temporal Decoupled Predictor (MSTDP) designed to efficiently extract spatial and temporal information by decoupling daily trajectories into distinct location-duration chains. Our approach employs a hierarchical encoder to model multi-scale temporal patterns, including daily recurrence and weekly periodicity, and utilizes a transformer-based decoder to globally attend to predicted information in the location or duration chain. Additionally, we introduce a spatial heterogeneous graph learner to capture multi-scale spatial relationships, enhancing semantic-rich repre-sentations. Extensive experiments, including statistical physics analysis, are conducted on large-scale mobile phone records in five cities (Boston, Los Angeles, SF Bay Area, Shanghai, and Tokyo), to demonstrate MSTDP's advantages. Applied to epidemic modeling in Boston, MSTDP significantly outperforms the best-performing baseline, achieving a remarkable 62.8% reduction in MAE for cumulative new cases.", "sections": [{"title": "I. INTRODUCTION", "content": "Human mobility modeling has attracted considerable atten-tion in fields such as commercial planning, healthcare, urbanmanagement, and mobile and network applications [1]\u2013[4].With the rapid advancements in technology, diverse mobilitydata are now being collected from multiple sources, drivinginnovation and application across these domains. For example,location-based social network (LBSN) data from check-inservices like Yelp and Foursquare enable targeted, location-aware advertising [5]. Mobility trajectories obtained frommobile devices are instrumental in analyzing and controllingthe spread of epidemics [6]\u2013[8], while GPS data from vehiclesplays a crucial role in optimizing traffic flow and manage-ment [9]\u2013[11]. These developments underscore the growingimportance of accurate mobility models in addressing complexreal-world challenges.\nHuman mobility prediction is generally addressed at twoscales: collective and individual [12]. This study concentrateson the individual scale, with a specific emphasis on predictingpersonal mobility patterns. The core challenge in individualmobility prediction revolves around next-location forecasting,which aims to predict an individual's future destinations basedon their historical movement data. Mobility trajectories arecommonly represented as sequences of locations, and theprimary task is to extract meaningful transition patterns fromthese sequences. By identifying regularities and trends in anindividual's movement, this approach seeks to enhance theprecision of mobility predictions. Traditional models rely onMarkov chains [13], while deep learning models apply ad-vanced techniques such as Recurrent Neural Networks (RNNs)and Transformers [14]\u2013[16]. Owning to the spatial character-istics of locations, researchers have explored specific modulesto capture spatial relationships. In recent years, the adventof graph representation learning techniques has demonstratedpromising potential, motivating the adoption of Graph NeuralNetworks (GNNs) to enhance the predictive performance [17]\u2013[21].\nNext location prediction is widely applied for personalizedservices, such as recommending points of interest (POI), asillustrated in Figure 1(a). However, urban management appli-cations such as travel management and epidemic transmission"}, {"title": "II. RELATED WORK", "content": "Human mobility modeling typically employs sequentialmodels to capture the transition relationships between loca-tions. Early methods mainly rely on Markov Chain to con-struct location transition probability matrices, with techniquesto incorporate temporal and spatial information to captureindividual travel preferences [13], [23], [24]. The evolutionof deep learning has effectively facilitated the modeling ofcomplex interactions. The foundational framework of locationsequence models has progressed from the RNN series [14],[25], [26] to attention-based Transformer architectures [15],[16], consequently enhancing the capacity to capture high-order transition patterns. In addition to the transition regularity,location sequences offer a wealth of spatiotemporal informa-tion. Thus, researchers have introduced innovative mechanismsfrom diverse perspectives such as temporal and spatial se-mantics to enhance sequence models [27]\u2013[34]. Besides, somework also incorporates location category information to furtherconstrain the location selection [35]\u2013[37]. However, thesemethods would encounter several challenges when appliedto the mid-term mobility prediction task, as discussed inIntroduction I. At present, there is limited research dedicated tomid-term behavioral prediction tasks, such as HTAED [38] andRLMob [39]. But both models still face difficulties includingrepeated consecutive locations and excessively long sequences,and they inadequately capture the multi-scale spatiotemporalpatterns inherent in individual daily travel. To mitigate theirlimitations, we propose a spatiotemporal decoupled hierarchi-cal framework, including a heterogenous graph learner, whichenlarges the perception field and efficiently captures the multi-scale spatiotemporal periodic information.\nOver the past few years, graph neural networks have en-joyed rapid development and have been increasingly appliedto learn location representations [18], [40]\u2013[43]. Researcherscustomarily construct a flow graph and an adjacency graph,from which they derive two distinct location representationsand devise mechanisms to effectively combine them [17],[19], [44]. In this work, we integrated these two graphsinto a heterogeneous graph to learn an informative locationrepresentation. Additionally, our graph considers multi-leveladjacency relationships and flow relationships to efficientlycapture complex geographic spatial information."}, {"title": "III. PRELIMINARY", "content": "In this section, we first formulate the next day mobilityprediction task, and then introduce the definition of two basiclocation graphs.\nAnalyzing individual mobility patterns throughout the daynecessitates dense data. To this end, we leverage Call DetailRecord (CDR) data, which records locations using latitude andlongitude coordinates rather than Points of Interest (POI) asin LBSN data. To address privacy concerns and simplify themodeling process, researchers partition the urban area into gridcells of equal size and map individuals' trajectories to cells.Each grid cell is treated as a discrete location visited by indi-viduals [24]. The cell size used in this study is 1 km \u00d7 1 km.\nHere, the location set is denoted as $\\mathcal{L} = \\{l_1,...,l_{|\\mathcal{L}|}\\}$. We thenpartition each day into fixed-size timeslots. Using an hourlytime window as an example, the daily trajectory is expressed as$\\mathcal{D} = \\{l_0, ..., l_{23}\\}$, where $l_t$ denotes the stay location $l_p$ within the hour $[t,t+1)$, $t \\in \\mathbb{N}$. Let $\\mathcal{U} = \\{u_1,...,u_{|\\mathcal{U}|}\\}$ denotes a set of individuals. Then all of trajectories of an individual $u$ can be represented as $TRAJ_u = \\{\\mathcal{D}_1,...,\\mathcal{D}_N\\}$, where $N$ is the number of total trajectories of the user, $N \\leq |TRAJ_u|$.\nGiven an individual $u$ and his or herhistorical trajectories $\\{\\mathcal{D}_1,...,\\mathcal{D}_k\\}$, the goal is to predict the next day trajectory $\\mathcal{D}_{k+1} = \\{l_0^\\prime, ..., l_w^\\prime\\}$, where $k \\leq |TRAJ_u|-1$. $w$ is determined by the size of the time window. In themethodology section, we present the model's details usinghour-long windows as an example. In the experiments, differ-ent datasets require varying time window sizes, selected basedon the level of data sparsity.\nThe flow graph and adjacency graph have been used inrecent studies to capture the spatial relationships of loca-tions [17]\u2013[19]. We define these two graphs as follows.\nFlow Graph is a directed graph $G_f = (V, E_f)$ to de-scribe transition relationships among locations, where the node$v_j, v_j \\in V$, denotes a location, the edge $e_{pq}^f, e_{pq}^f \\in E^f$,represents the number of trips from $v_p$ to $v_q$ produced byall individuals. In this study, we only count $E_f$ in the trainingset.\nAdjacency Graph is an undirected graph $G_a = (V, E_a)$ todepict the proximity between locations. The node set $V$ alignswith the location set in the flow graph $G_f$. The undirected edge$e_{pq}^a$ in the edge set $E^a$ indicates that the geospatial distancebetween $v_p$ and $v_q$, denoted as $dist_{pq}$, is less than a pre-definedthreshold $d$. In our work, $d$ is set to 2 km for a cell with 1 kmwidth. $e_{pq}^a = 1$ if $dist_{pq} \\leq d$ and $e_{pq}^a = 0$ otherwise."}, {"title": "IV. METHODOLOGY", "content": "In this section, we will sequentially introduce the fourprimary components of the Multi-scale Spatial-Temporal De-coupled Predictor (MSTDP), as illustrated in Figure 3.\nModeling the daily travel behavior of individuals at theurban scale presents challenges of location repetition andexcessively long sequences, as mentioned in Introduction I.Location repetition arises from individuals being observedmultiple times at the same location due to their prolongedstays. Therefore, we can extract the stay duration from a dailytrajectory to obtain the location sequence that eliminates con-secutive repetitions. Specifically, the proposed spatiotemporaldecoupler separates the trajectory of the k-th day $\\mathcal{D}^k$ intoa location chain $C^{loc} = \\{l_p,...,l_q\\}$ and a duration chain$C^{dur} = \\{t_1,...,t_n\\}$, where $t_i \\in \\mathbb{N}^+$ and $\\sum_{i=1}^nt_i = 24$.\nThe elements $t_i$ in the duration chain correspond to thestay duration in the i-th location of $C^{loc}$. Both chains areof equal lengths $|C^{loc}| = |C^{dur}|$. Taking a daily trajectory$\\{l_9, l_4, l_2, l_3, l_2, ..., l_{31}, l_{22}, l_{23}\\}$ as an example, the decoupledlocation and duration chains would be $\\{l_9,l_4,l_2,...,l_3,l_1\\}$ and$\\{3,2,...,1,2\\}$ respectively. In this way, we can disentanglelengthy sequences with consecutively repeated locations intotwo shorter chains, effectively reducing the complexity ofpattern learning. Moreover, the decoupled location chain canbe viewed as a location-specific motif [45] that enables themodel to learn the integrity of an individual's daily travelpatterns and the cyclical nature of their everyday behavior.\nLocations inherently possess spatial semantic information,inspiring the utilization of graph neural networks to learnlocation representation from the adjacency graph and flowgraph [17], [19], [44]. However, these methods tend to learnembeddings independently from each graph and then mergethe embeddings as the location representation, disregardingthe interdependencies between the two graphs. Towards this,we introduce a heterogeneous graph, as illustrated in Figure 4,that combines both flow and adjacency relationships betweenlocations. Furthermore, we expect to integrate multi-scaleurban structural information into the graph. Considering theadministrative divisions among regions, we build an admin-level graph with the administrative areas as nodes to enrichthe spatial connections between locations. In this way, theheterogeneous graph considers both the cell-level and admin-level flow and adjacency relationships, along with the inclusionrelationship between cells and administrative areas. The scaleof administrative areas can be chosen as block, tract, town,etc.\nWe formulate the heterogeneous graph as $G_H = (V^{cell}, V^{adm}, E^{cell}, E^{adm}, E_f^{cell}, E_f^{adm}, E^r)$, where $V^{cell}$ and$V^{adm}$ are the node sets of cell and administrative area, $E_*^a$and $E_f^* (* \\in \\{cell, adm\\})$ are the flow and adjacency edgesets, and $E^r$ is the inclusion edge sets. The undirected edge$e_{pq}^a, e_{pq}^a \\in E_*^a$, means that the cell $v^{cell}_p$ is located in theadministrative area $v^{adm}_q$. The description of edges in $E_f$ and$E^a$ remain consistent with Preliminary III. Moreover, the flowedge in our study includes hourly population flow betweenconnected nodes. The edge is defined as $e_{pq} = \\{f_{pq}^0,..., f_{pq}^{23}\\}$,where $f_{pq}^t$ means the number of trips from $v_p$ to $v_q$ during thetime interval from the hour t to (t + 1), t \u2208 [0, 23).\nThe node embeddings incorporate neighbor informationfrom various edges to enhance the semantic richness. Weemploy different message-passing mechanisms based on thecharacteristics of the heterogeneous edges. Due to the simplistics semantics of the adjacent edge and the inclusion edge, weapply GraphSage [46] as the backbone to model the messagepassing of these two types of edges. The graph layer is definedas:\n$x_j^{(n)} = W_1 x_j^{(n-1)} + W_2 \\underset{q \\in \\mathcal{N}(j)}{mean} x_q^{(n-1)}$,\nwhere $x_j^{(n)}$ is node embedding in the n-th layer, $W_1$ and $W_2$are learnable parameters, and \u201cmean\u201d is the mean operatorto aggregate the neighborhood. The initial node features $x_j^{(0)}$comprises word embeddings of region IDs and the time-varying population count within the region. Regions refer tocells or administrative areas.\nConsidering the complexity of semantic information in theflow edges, we employ an edge-considered variant of GraphAttention Network (GAT) [47] for effective modeling:\n$x_p^{(n)} = \\underset{\\mathcal{N}_\\big(p\\big)}{agg} [ W_p^{(n-1)} x_p^{(n-1)} + \\sum_{q \\in \\mathcal{N}(p)} \\alpha_{pq} W_q^{(n-1)} x_q^{(n-1)} ]$,\nwhere $W_p^{(n-1)}$ and $W_q^{(n-1)}$ are learnable parameters. Theattention coefficient $\\alpha_{pq}$ is calculated as:\n$\\alpha_{pq} = \\frac{exp(o(a^T [W_p x_p || W_q x_q || W_{pq} e_{pq}]))}{\\Sigma_{s \\in \\mathcal{N}(p) \\cup \\{p\\}} exp(o(a^T [W_p x_p || W_s x_s || W_{ps} e_{ps}]))}$,\nwhere o is LeakyReLU and these x are the abbreviations of$x^{(n-1)}$. These $W_*, * \\in \\{p,q, pq, s, ps\\}$ are trainble paramters.After n-layers heterogeneous graph propagation, we obtainthe final cell representation as the learned location embedding.These learning processes can be summarized as:\n$H_{loc} = GraphEmbedder(C_{loc})$.\nUrban residents exhibit multi-scale temporal regularities,such as daily and weekly patterns in their travel routines, asmentioned in the Introduction I. To capture these periodicitiesmore effectively, we introduce a hierarchical trajectory encoderwith two layers: the daily encoder and the weekly encoder.The daily encoder captures daily travel transitions, while theweekly encoder focuses on weekly travel variation patterns. Byconsidering information from the corresponding day a weekago and the within-week behavioral changes, our model iscapable of predicting trips on the eighth day with enhancedaccuracy.\nBoth the daily encoder and the weekly encoder utilize apositional encoding module and a Transformer encoder [48],denoted as DailyEnc and WeeklyEnc, to capture sequentialrelationships. The Transformer encoder mainly consists of amulti-head self-attention mechanism and a fully connectedfeed-forward neural network (FNN). The attention mechanismapplies various heads to simultaneously attend to differentaspects, enhancing the model to capture complex dependenciesfrom sequences and acquire information-rich representation.The detailed descriptions of the positional encoding moduleand the attention mechanism refer to [48].\nWe utilize two hierarchical encoders, namely spatial encoderand temporal encoder, to model the decoupled location chainand duration chain. With the daily encoder, we have the k-thdaily chain representation $h_{loc}^k \\in \\mathbb{R}^{d_{hi}}$ and $h_{dur}^k \\in \\mathbb{R}^{d_{ht}}$ asfollows:\n$h_{dur}^k = DailyEnc_{dur} (Hour)$,\n$h_{loc}^k = DailyEnc_{loc} (H_{loc})$.\ndhi and dht denotes the dimension of $h_{loc}^k$ and $h_{dur}^k$. $H_{loc} \\in \\mathbb{R}^{|C| \\times d_{el}}$ is the embedding of location chain $C_{loc}$ learnedwith the geospatial embedder, as introduced in the subsection IV-B. $Hour \\in \\mathbb{R}^{|C_{dur}| \\times d_{et}}$ is the embedding of durationchain $C_{dur}$ learned with a word embedder. The word embedderreceives the vocabulary size and embedding dimension asinput, learning dense vector representations for each word.In contrast to one-hot encoding, it generates more compactand semantically informative embeddings, capturing contex-tual relationships between words. $d_{el}$ and $d_{et}$ denote theembedding dimension of location and duration. Then weobtain the historical information representation $z_{loc} \\in \\mathbb{R}^{dz_i}$and $z_{dur} \\in \\mathbb{R}^{dz_t}$ through the weekly encoder, formulated as:\n$z_{dur} = WeeklyEnc_{dur}(\\{h_{dur}^1,..., h_{dur}^{-6}\\})$,\n$z_{loc} = WeeklyEnc_{loc}(\\{h_{loc}^1,..., h_{loc}^{-6}\\})$.\ndzi and dzt denotes the dimension of zloc and zdur. Here, thesymbol || represents vector concatenation and the concatenated vector, $h_{dur}^k || h_{loc}^k$, has the dimension $d_{loc} + d_{dur}$. Themissing days during a week are masked in the weekly encoderto preserve the temporal structure integrity.\nThe trajectory decoder is comprised of a spatial decoderand a temporal decoder. Considering the dependency betweenduration and location, we expect to predict the location chainfirst and then utilize it to enhance the prediction of theduration chain. Inspired by the natural language process tasks,we add the start and end tokens into the location chain toaddress the challenges of the unknown start location and theuncertain chain length. Besides, we employ the Transformerdecoder [48] as the backbone of the spatial decoder, alongwith a positional encoding module and an FNN. The self-attention mechanism in the Transformer decoder and positionalencodings enable the model to effectively capture long-rangedependencies and maintain the temporal order of the inputsequence, thereby alleviating accumulative errors in multi-stepprediction tasks. Commencing from the start token $\\lt SOS\\gt$,the spatial decoder utilizes the historical information andpredicted results to make iterative predictions for the dailylocation chain. This process is formulated as:\n$\\hat{l}_{i+1} = SpatialDec(\\{\\hat{s}_0, \\hat{s}_1 ..., \\hat{s}_i\\}, z_{loc})$,\nwhere SpatialEnc is the abbreviation of the spatial decoder, $\\hat{s}_i$ is the embedding of i-th predicted location $\\hat{l}_i =GraphEmbed(l_i)$ and $\\hat{s}_0$ is the embedding of the start token.The iterative prediction process halts upon the prediction ofthe end token. Consequently, the predicted location chainis $\\hat{C}_{loc} = \\{\\hat{l}_1...,\\hat{l}_m\\}$, with m representing the number oflocation appeared on the target day.\nThe temporal decoder comprises a positional encodingmodule, Transformer encoder, and FNN. We utilize the Trans-former encoder to enable the model to employ attentionmechanisms for observing daily location access patterns whenpredicting duration. Historical time data and predicted locationrepresentations are combined into the time encoder to derivethe forecasted duration chain, formulated as:\n$\\hat{C}_{dur} = \\{\\hat{t}_1...,\\hat{t}_m\\} = TemporalDec(\\{s'_1 \\bigoplus z_{dur},...,s'_m \\bigoplus z_{dur}\\})$.\nwhere TemporalDec is the abbreviation of the temporaldecoder and \u2295 represents vector concatenation as mentionedbefore. $s'_k = FNN(\\hat{s}_k)$, where a FNN is used to adjust thedimension of the predicted location embeddings. Finally, wecombine the predicted duration chain and location chain toderive the predicted trajectory for the following day."}, {"title": "E. Model Optimization", "content": "We apply the most commonly used cross-entropy loss forthe location chain prediction. The Huber loss is used for theduration chain prediction as it integrates the advantages ofMSE and MAE. The total loss function is defined as:\n$\\mathcal{L} = \\mathcal{L}_{ce}(l_j, \\hat{l}_j) + \\lambda \\cdot \\mathcal{L}_{huber}(t_j, \\hat{t}_j)$,\nwhere $\\mathcal{L}_{huber}(t_j, \\hat{t}_j) =  \\begin{cases}(t_j-\\hat{t}_j)^2, & |t_j-\\hat{t}_j| < 1 \\\\ (t_j - \\hat{t}_j) - 1/2, & otherwise\\end{cases}$.\n$\\mathcal{L}_{ce} (l_j, \\hat{l}_j) = -l_j \\cdot log \\hat{l}_j$, and \u03bb is a hyperparamter to adjustloss weight. $l_j$ and $t_j$ are the targets, while $l_j$ and $t_j$ are thepredicted values."}, {"title": "V. EXPERIMENTS", "content": "Experiments are conducted on fourprivate 1 urban-scale datasets, Boston, Los Angeles(LA), SanFrancisco Bay Area(SFBay), Shanghai, collected from real-world mobile records, and one public dataset Tokyo [49], com-prising synthetic mobility data. The preprocessing steps for theCDR datasets follow [24]. Besides, we partition each city intouniform grids with grid sizes of 1 kilometer. As describedin Preliminary, a day is divided into multiple time windows,and only the record with the longest stay duration is retainedwithin each window. Boston, LA, and SFBay utilize half-hour windows, whereas Shanghai and Tokyo use hour-longwindows. The statistics of the datasets are presented in Table I,where \"# Days\" means the average number of days and\"# Admins\" means the number of the administrative division.Boston, LA, and SFBay employ census tracts as administrativeunits, whereas Shanghai uses towns. In the Tokyo dataset,due to blurred exact coordinates, we partitioned administrativeregions using 7x7 grid squares. The training, validation, andtest sets are obtained by partitioning the original dataset in aratio of 6:1:3 based on the time span.\nSeven models for predicting next loca-tions and two models for successive mobility prediction areutilized for comparative analysis. They are CFPRec [15],GETNext [16], GFlash [50], HGARN [51], SNPM [41],AGRAN [42], MTNet [33], HTAED [38], and RLMob [39].A concise introduction to these baselines is listed as follows:\nWe evaluate these models withfour metrics: Acc, DevDist, TravelDist, and DepartTime. Accindicates the proportion of the accurately predicted locations.DevDist is the average deviation distance, measured in kilome-ters, between all actual and predicted locations. TravelDist andDepartTime quantify the Jensen-Shannon divergence (JSD)between actual and predicted travel distances and departuretimes for individuals.\nAcc and DevDist are assessed point-wise, while TravelDistand DepartTime are evaluated individually. Specifically, Accdescribes the proportion of the accurately predicted loca-tions, calculated with $Acc = \\frac{Count(i==\\hat{i})}{\\mathcal{N}_i}$, where $Count()$measures the number of correctly predicted locations, with$\\mathcal{N}_i$ denoting the total number of locations in the trajectory.DevDist measures the averaged deviation distance betweenactual and predicted locations, formulated with $DevDist =\\underset{i \\in \\mathcal{N}_i}{mean} ((distance(l_i, \\hat{l}_i)))$. distance() calculates the dis-tance between two locations in kilometers, whereas $\\underset{i \\in \\mathcal{N}_i}{mean}()$computes the average over $\\mathcal{N}_i$ locations. TravelDist initiallycomputes the distribution of individual travel distances, evalu-ates the Jensen-Shannon divergence (JSD) between the actualand predicted distributions per individual, and subsequentlyaverages these differences across all users. It is formallydefined by $\\underset{i \\in \\mathcal{N}_i}{mean}(JSD(\\hat{p}_{dist}, p_{dist}))$, where $\\hat{p}_{dist}$ and $p_{dist}$denote the predicted and actual distribution of travel dis-tances. The calculation of DepartTime is similar to TravelDist,focusing on the distribution of departure times. Its formaldefinition is $\\underset{i \\in \\mathcal{N}_i}{mean}(JSD(\\hat{p}_{time}, p_{time}))$, where $\\hat{p}_{time}$ and$p_{time}$ denote the predicted and actual distribution of departuretime.\nThe improved performance is reflected in larger Acc, as wellas smaller DevDist, TravelDist, and DepartTime.\nThese baselines are implemented withtheir open-source codes. Hyperparameter tuning is performedon the validation set to attain optimal performance. The im-plementation of MSTDP is based on the PyTorch framework.The heterogeneous GNNs adopt a two-layer structure, whilethe layer numbers of Transformer encoder and decoder arealso set to 2. The embedding dimensions of location andtime are 512 and 512. The hidden dimensions of location andtime are 1024 and 512. The head numbers of attention in theTransformer encoder and decoder are set to 8. We apply theAdam optimizer to train MSTDP with a learning rate of $10^{-4}$.The hyperparameter \u03bb is set to 1.\nWe evaluate models in two tasks: next dayprediction and next week prediction. For next day prediction,all models iteratively forecast the location of the next Ttimeslots by considering historical information and previouslypredicted results. This means that the prediction of the i-th location is based on the true historical information$\\{\\mathcal{D}_0,..., \\mathcal{D}_k\\}$ and the predicted locations of the precedingi-1 points $\\{\\hat{l}_1, ..., \\hat{l}_{i-1}\\}$, where i \u2208 [1,T]. The one-hourtime window corresponds to T=24, while the half-hour timewindow corresponds to T=48. Similarly, for next week pre-diction, the next $7 * T$ locations in the following week arealso iteratively forecasted. That is, $\\mathcal{D}_{k+i}$ is predicted with$\\{\\mathcal{D}_0,...,\\mathcal{D}_k\\}$ and $\\{\\mathcal{D}_{k+1},..., \\mathcal{D}_{k+i-1}\\}$, where i \u2208 [2, 7].\nWe initially evaluate all models using datasets Boston,Shanghai, and Tokyo, and subsequently select the more recentbaselines to compare against datasets LA and SFBay. Theresults are exhibited in Table II and Table III. From theseresults, we draw the following observations and analysis: (1)MSTDP demonstrates superior performance on both datasets,validating its effectiveness in mid-term prediction tasks. Com-pared to the top-performing baseline, MSTDP exhibits im-provements of 3.6% in Acc, 9.0% in DevDist, 20.3% inTravelDist and 19.4% in DepartTime. A smaller DevDist ofMSTDP indicates closer proximity between the predicted andthe actual locations. Furthermore, lower metrics for TravelDistand DepartTime demonstrate improved individual consistencybetween predicted and actual trajectories regarding travel timeand distance patterns in our model. (2) Among these baselines,HTAED excels due to its dual-layer attention mechanismtailored for successive prediction, effectively weighing intra-day and inter-day behavioral patterns. In contrast, RLMob,which employs a reinforcement learning framework for suc-cessive prediction, falls short by relying solely on RNNsto model transition relationships, resulting in less favorableperformances. These baselines, initially developed for next-location prediction, overlook temporal patterns such as dailyand weekly cycles, which complicates their direct applicationto mid-term forecasting tasks. Moreover, when applied toBoston, LA, or SFBay datasets, these baseline predictionsshow significant performance degradation compared to Shang-hai and Tokyo, notably highlighted by SNPM. This discrep-ancy primarily stems from varying time window sizes acrossdatasets; for instance, Boston employs a half-hour window,leading to increased location redundancy and longer dailytrajectories. These factors present substantial challenges formodel performance, as discussed in Introduction I."}, {"title": "C. Statistical Physics Analysis", "content": "The metric comparison in Table II does not sufficientlyportray the model's performance. Therefore, we conducta comprehensive statistical physics analysis on the Bostondataset from multiple perspectives to thoroughly examinethe performance of these models.\nFigure 5 presents the statisticaldistribution of daily travel distances for individuals. Thesebaselines predict a higher number of zero travel distances. Itindicates that the models predict a greater occurrence of full-day stays at the same location, which we refer to as \"full-daystays\". Unlike these baselines, MSTDP demonstrates closeralignment between predicted daily travel distances and actualdistributions.\nHuman mobility motif. Motif describes people's mobilitypattern during one day with abstract network [45]. Figure 6illustrates the ten most prevalent motifs in actual daily trajec-tories, covering 74.6% of the population. MTNet and AGRANprimarily identify three patterns, with a notable emphasis onpattern ID=6, indicative of their focus on daily full-day stays.Our model adeptly captures these prevalent travel patterns,demonstrating performance superiority over HTAED throughclose alignment with the actual distribution.\nOrigin-Destination (OD) flow pairs. We collect the cell-level OD flow pairs between the actual and predicted flowsand illustrate them in Figure 7. We calculate $R^2$ and CPC [52]between actual and predicted flows for numerical comparison.AGRAN and MTNet predict a limited number of flows,consistent with previous analyses indicating their preferencefor predicting full-day stays. HTAED overestimates flows withan $R^2$ of -0.442 and CPC of 0.461, whereas our MSTDPmodel achieves closer predictions to actual flow quantities,with an improved $R^2$ of 0.728 and CPC of 0.672.\nOrigin-Destination flows in map. Figure 8 illustrates the actual and predicted OD flows in the map, where thetrips between cells are aggregated at the census tract level.The global travel patterns predicted by the three baselinemodels deviate significantly from the actual results. HTAEDachieves $R^2$ and CPC values of 0.534 and 0.629, whereasMSTDP achieves 0.872 and 0.792, indicating performanceimprovements of 63.3% and 25.9% in our model. In thissubsection, we examine OD flows at the cell and tract levels.Additionally, Figure 9 illustrates OD flows at the zip codelevel. MSTDP achieves R-squared ($R^2$) and CPC values of0.969 and 0.877, while HTAED obtains 0.928 and 0.817. Theperformance improvements are 4.4% and 7.3%, respectively.This observation suggests that larger spatial aggregations mayobscure differences between models.\nSeven-day consecutive forecasting. We employ an iterativeforecasting approach to predict future travel continuouslythroughout the upcoming week. The accuracy and deviationdistance fluctuation of the predictions over seven days areillustrated in Figure 10. As the forecasting horizon extends,the accuracy of all models gradually diminishes, alongsideincreased deviation distances between actual and predictedlocations. This trend highlights the cumulative error inherentin iterative prediction approaches. Notably, when forecastingthe subsequent second and third days, MSTDP exhibits slowerperformance degradation than HTAED. This disparity canbe attributed to our model's broader contextual awareness,leveraging 5 and 4 days of historical data for respectivepredictions."}, {"title": "D. Ablation Study", "content": "We compare MSTDP with three variants to examine theeffectiveness of the modules in MSTDP. The ablated variantsof MSTDP are as follows: (1) MSTDP-D: it eliminates thespatial-temporal decoupler and directly encodes the long tra-jectories; (2) MSTDP-H: it abandons the hierarchical struc-ture and solely utilizes the daily encoder; (3) MSTDP-G: itreplaces the graph-based location representation with a wordembedder; (4) MSTDP-LD: the temporal decoder excludes theconcatenation from location decoder and uses a Transformerdecoder structure rather than the Transformer encoder.\nThe results in Table IV demonstrate that MSTDP outper-forms these ablation variants, highlighting the effectiveness ofeach designed module. Among these variants, the MSTDP-H exhibits the weakest performance in both prediction tasks,likely due to reduced capability in capturing long-term de-pendencies and weekly periodicity following the removal ofits hierarchical structure. MSTDP-LD exhibits inferior perfor-mance compared to MSTDP-D and MSTDP-G in next dayprediction tasks, yet surpasses them in next week predictiontasks. This could be attributed to the incorporation of predictedlocations in the temporal decoder, resulting in increased cumu-lative errors in next week predictions. Inaccuracies in locationpredictions adversely impact the model's overall performance."}, {"title": "E. Hyperparameter Analysis", "content": "To examine the MSTDP's sensitivity to hyperparameters, wevary two hyperparameters to observe performance fluctuation.\u039b adjusts the weight of the temporal loss, impacting theoptimization process. We test \u03bb at values of 0.01, 0.1, 1, 10,and 100. Additionally, we also evaluate the head number (HN)of the attention in the Transformer encoder, considering valuesof 2, 4, and 8.\nTable V presents the performance fluctuation with thehyperparameter variations. The results indicate that increasingthe number of attention heads improves model performance.The influence of \u03bb on performance fluctuations is relativelymodest; larger values such as 1 or 100 yield superior resultscompared to smaller values like 0.1 or 0.01. This highlightsthe critical role of duration time learning."}, {"title": "F. Application to Epidemic Transmission", "content": "The predicted mid-term trajectory can be applied to urbanmanagement applications. To demonstrate its practicality", "22": [53], "states": "susceptible (S), exposed (E), infec-tious (I), and removed (R). Susceptible individuals can becomeinfected through close contact with those carrying the infectionand subsequently transition into the exposed state. Thereexists a probability of transitioning from the exposed stateto infection. Lastly, infectious individuals transition intothe removed state with a probability. The removed state assumesthat individuals"}]}