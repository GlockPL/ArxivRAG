{"title": "REPEAT: Improving Uncertainty Estimation in Representation Learning Explainability", "authors": ["Kristoffer K. Wickstr\u00f8m", "Thea Br\u00fcsch", "Michael C. Kampffmeyer", "Robert Jenssen"], "abstract": "Incorporating uncertainty is crucial to provide trustworthy explanations of deep learning models. Recent works have demonstrated how uncertainty modeling can be particularly important in the unsupervised field of representation learning explainable artificial intelligence (R-XAI). Current R-XAI methods provide uncertainty by measuring variability in the importance score. However, they fail to provide meaningful estimates of whether a pixel is certainly important or not. In this work, we propose a new R-XAI method called REPEAT that addresses the key question of whether or not a pixel is certainly important. REPEAT leverages the stochasticity of current R-XAI methods to produce multiple estimates of importance, thus considering each pixel in an image as a Bernoulli random variable that is either important or unimportant. From these Bernoulli random variables we can directly estimate the importance of a pixel and its associated certainty, thus enabling users to determine certainty in pixel importance. Our extensive evaluation shows that REPEAT gives certainty estimates that are more intuitive, better at detecting out-of-distribution data, and more concise. Code is available at https://github.com/Wickstrom/REPEAT.", "sections": [{"title": "Introduction", "content": "Representation learning through self-supervision is the cornerstone of recent improvements in the computer vision domain (Caron et al. 2018; He et al. 2022; Assran et al. 2023; Bardes, Ponce, and LeCun 2022). Transforming images into a new representation has been shown to improve performance in a wide range of unsupervised tasks (Trosten et al. 2023; Wickstr\u00f8m et al. 2023; Sehwag, Chiang, and Mittal 2021). Despite the benefits, unsupervised representation learning also suffers from some significant drawbacks, particularly a lack of explainability. Current methods in explainable artificial intelligence (XAI) are designed with supervised learning in mind, where a scalar model output is explained in relation to its input (Petsiuk, Das, and Saenko 2018; Bach et al. 2015; Sundararajan, Taly, and Yan 2017). Using these methods to explain representations is either not possible or requires major modifications of the underlying algorithms (Crabb\u00e9 and van der Schaar 2022). Tackling this drawback has lead to a new direction within XAI, namely representation learning XAI (R-XAI). Methods within R-XAI solve the problem of explaining representations by either making adaptations of existing XAI methods (Crabb\u00e9 and van der Schaar 2022) or by designing new methods that are particularly designed to tackle the representation learning setting (Wickstr\u00f8m et al. 2023; Lin et al. 2023; Bertolini, Clevert, and Montanari 2023; M\u00f8ller et al. 2024). A key ingredient in recent R-XAI research is uncertainty estimation (Wickstr\u00f8m et al. 2023), where importance is accompanied by a corresponding uncertainty estimate. Providing an indication of certainty is highly desirable, for instance in safety critical areas such as healthcare (Tonekaboni et al. 2019; Kompa, Snoek, and Beam 2021). However, existing frameworks are limited to measuring the variation in the importance scores. This only gives an indication of how the numerical importance scores spread out, not if we are certain of importance. A more critical aspect is how certain are we that a pixel is important. Consider an estimated importance map, where all pixels with importance scores higher than 2 are considered important. Now, take one pixel with importance value 5.6\u00b10.1 and another with importance value 5.6\u00b11.2. Due to the higher variance of the second pixel, current R-XAI methods would assign high uncertainty to this pixel. However, since all values within the 95% confidence interval of the pixel would still be above the importance threshold. As such, we would still be certain that this pixel is important, despite the higher uncertainty of the exact value. With the aim of answering the question of whether we are certain that a pixel is important or not, we present a new R-XAI method called REPEAT. The key idea of REPEAT is to consider each input pixel as a Bernoulli random variable (RV) that indicates if the pixel is important for the representation of the input image. To generate samples of these Bernoulli RVs, we leverage the stochasticity of prior R-XAI methods combined with classic image thresholding techniques to threshold an image into important and non-important pixels. By repeating the thresholding process on"}, {"title": "Related Work", "content": "Existing R-XAI literature: There are two main approaches to extending the field of XAI to handle representations of data; adapt existing methods to handle the representation learning setting or design new methods designed for this particular use case. For adaptation approaches, Crabb\u00e9 and van der Schaar proposed Label-Free Feature Importance, where an auxiliary scalar function allows standard XAI-methods to be used on each component of the representation (Crabb\u00e9 and van der Schaar 2022). For R-XAI specific methods, Wickstr\u00f8m et al. introduced RELAX, where a representation is explained through similarity measurements between masked and unmasked representations of a particular image (Wickstr\u00f8m et al. 2023). Lin et al. extended existing methods in R-XAI to allow for explanation of a reference corpus in relation to a contrastive foil set (Lin et al. 2023). M\u00f8ller et al. proposed a trainable explanations network aimed at increating the latency of the explanation process (M\u00f8ller et al. 2024). DeTomaso and Yosef proposed Hotspot, which is focused on explaining representations in single-cell genomics (DeTomaso and Yosef 2021). Lastly, Bertolini, Clevert, and Montanari introduced an aggregation method that generalizes attribution maps between any two convolutional layers of a neural network (Bertolini, Clevert, and Montanari 2023). R-XAI has also been used in many applications, for instance in healthcare (Chen et al. 2023; Wickstr\u00f8m et al. 2023; Weinberger, Lin, and Lee 2023) and business (Feng, Li, and Zhang 2023). Uncertainty in XAI: Several approaches have been proposed for modeling uncertainty in XAI. A number of works have investigated how to model uncertainty in surrogate-based XAI (Zhang et al. 2019; Slack et al. 2021; Wang, Zhang, and Lim 2021; Schulz, Santos-Rodriguez, and Poyiadzi 2022), but these approaches are not transferable to the unsupervised setting. Other works have used Monte Carlo Dropout (Gal and Ghahramani 2016) to estimate uncertainty in importance (Wickstr\u00f8m, Kampffmeyer, and Jenssen 2020) but this is restrictive because it requires Dropout (Srivastava et al. 2014) in the feature extractor. Another work has used ensembles (Lakshminarayanan, Pritzel, and Blundell 2017) for uncertainty estimation in XAI (Wick-str\u00f8m et al. 2021), but this is not directly applicable in this context since we are interested in explaining a single feature extractor. In the general field of uncertainty modeling, test-time-augmentation has been shown to be a generally-applicable and effective tool for uncertainty estimation (Wang et al. 2019; Kahl et al. 2024), but has not been explored in the context of R-XAI. The R-XAI framework RELAX (Wickstr\u00f8m et al. 2023) provides uncertainty estimates with its importance scores, but the uncertainty estimates measure the variability in the importance scores and not certainty in pixel importance."}, {"title": "REPEAT: a new method for R-X\u0391\u0399 with Improved Uncertainty Estimation", "content": "We present REPEAT, a new method for R-XAI that indicates which pixels in an image are most important for the representation of the image and provides uncertainty estimates that specify if a pixel is certainly important or not."}, {"title": "Interpreting input pixels as Bernoulli RVs", "content": "Let $X_{ij}$ be a RV following a Bernoulli distribution such that $Pr(X_{ij} = 1) = p_{ij}$ indicates the probability of pixel ${i, j}$ being important for the representation h of X by the feature extractor f, and $Pr(X_{ij} = 0) = q_{ij} = (1 - p_{ij})$ indicates the opposite case. We consider the importance of a pixel as:\n$R_{ij} = E [X_{ij}] = p_{ij}$.\nFurthermore, we consider the uncertainty associated with the importance as:\n$U_{ij} = Var [X_{ij}] = p_{ij} (1 \u2013 p_{ij})$.\nThe value of $p_{ij}$ is unknown, but can be estimated from data. To perform this estimation, we require realizations of $X_{ij}$. The following subsection presents how to generate these realizations."}, {"title": "Generating Samples using Stochastic R-XAI and Thresholding", "content": "To estimate $p_{ij}$ we require samples that indicate whether or not a pixel is important to the representation of an image. We propose to leverage a base stochastic R-XAI method to generate samples as follows:\n$I_{ij} (k) = \\begin{cases} 1 & \\text{if } \\hat{R}_{base}^{(k)} \\geq \\tau \\\\ 0 & \\text{else} \\end{cases}$\nHere, for the kth realization, $I_{ij} (k)$ is an indicator function that activates if the importance score is above a certain threshold $\\tau$ (see next subsection for threshold selection), and $R_{base}^{(k)}$ is the estimated importance score from the base stochastic R-XAI method. If the scores are above the threshold, the pixel is considered important for the representation of the image in question. The stochasticity requirement for the base R-XAI method is critical, since this will ensure diversity and allow new realizations of the RV $X_{ij}$ to be generated. By repeating this process we obtain a set of samples that can be used to estimate $p_{ij}$. The intuition is that pixels which are assigned high importance across numerous realizations will have a high probability of being important. Similarly, pixels that are regularly assigned low scores will have a low probability of being important. And importantly, pixels that fluctuate above and below the threshold will be highlighted as having high uncertainty."}, {"title": "Setting the Threshold", "content": "Finding a proper value for $\\tau$ in Eq. 3 is crucial to generate meaningful samples. We propose to approach this from a foreground-background thresholding perspective, where we consider important pixels as foreground pixels and unimportant pixels as background pixels. Thresholding is a classic"}, {"title": "Importance and Uncertainty With REPEAT", "content": "With repeated use of Eq. 3, new data points can be generated to estimate $p_{ij}$. Assuming $K$ importance maps are generated, we propose to use a weighted sample mean that takes into consideration the base importance scores as follows:\n$\\hat{p}_{ij} = \\frac{1}{K} \\sum_{k=1}^K I_{ij} (k)W_{ij} (k) = \\hat{R}_{ij}$,\nwhere\n$W_{ij} (k) = \\frac{\\hat{R}_{base}^{(k)}}{A(k)}$\nand $A(k)$ is the maximum value of $\\hat{R}_{base}^{(k)}$. Weighting the indicator function with the importance scores facilitates ranking among the most important pixels in an image, and scaling by the maximum value ensures that the importance scores are comparable across the repeated explanations. Finally, the uncertainty of pixel importance in REPEAT can be calculated in the standard way for Bernoulli RVs:\n$U = \\hat{p}_{ij} (1 - \\hat{p}_{ij}).$"}, {"title": "Evaluation Protocol and Experimental Setup", "content": "Here, we describe how we evaluate and compare REPEAT to state-of-the-art alternatives and detail our experimental setup. Evaluation Protocol We describe how we evaluate and compare uncertainty estimates. Our evaluation follows well known tasks in both the uncertainty and the XAI literature. Sanity check: The Model Parameter Randomisation Test (MPRT) (Adebayo et al. 2018) is widely used in XAI to investigate if XAI method behaves as expected (Barkan et al. 2023a; Lei et al. 2023; Barkan et al. 2023b). The general idea is to see if explanations deteriorate when the parameters of a model are randomized before the decision of the model is explained. However, the MPRT can be highly computationally demanding (Hedstr\u00f6m et al. 2024), which makes it difficult to provide a comprehensive analysis. Therefore, we instead use the recently proposed efficient MPRT (eMPRT) (Hedstr\u00f6m et al. 2024). The eMPRT compares the relative rise in explanation complexity for an explanation of a trained model compared to a completely randomized model. The intuition is that explanations of random models should be mostly random and therefore have high entropy, while explanations of a trained model should be more focused and therefore have lower entropy. A high positive value is desirable, as it indicates that the explanations of the random model are more complex, while a negative value indicates that the explanations of the trained model are more complex and is not desirable. Originally, both the MPRT and the eMPRT were designed for evaluation of explanations, but here we use eMPRT on the uncertainty estimates. Out-of-distribution detection: A standard task in uncertainty estimation is out-of-distribution (OOD) detection (Lakshminarayanan, Pritzel, and Blundell 2017; Maddox et al. 2019). When presented with unfamiliar data this should be reflected in the uncertainty. We follow the approach of prior works (Lakshminarayanan, Pritzel, and Blundell 2017; Hein, Andriushchenko, and Bitterwolf 2019) and measure to what degree the uncertainty estimates can"}, {"title": "Results", "content": "This section present the main results of our work. First, we present the results of the outlined evaluation protocol. Then, we show that REPEAT is also applicable beyond RELAX. In all experiments, we randomly sample 1000 images from the dataset used for evaluation. We found that this was enough samples to provide reliable estimates of performance while still being computationally tractable. Due to their inherent stochasticity, RELAX and REPEAT experiments were repeated 3 times. This revealed that performance was stable with a standard deviation less than the decimal precision reported here. Therefore, for clarity we do not report the standard deviation here. Also, we evaluate the performance of the explanations produced by REPEAT, which shows good performance."}, {"title": "Conclusion", "content": "Current methods for determining certainty in pixel importance are limited, as they only estimate the variability in the importance values. This reduces the reliability of R-XAI, as users cannot decide if a pixel is certainly important or not. In this work, we proposed a new method called REPEAT that addresses this limitation. REPEAT treats each pixel in an image as a Bernoulli RV that is either important or unimportant to the representation of the image. From these Bernoulli RV we can directly estimate the importance of a pixel and its associated certainty, thus enabling users to ascertain certainty in pixel importance. We conducted an extensive evaluation which showed that REPEAT provides more intuitive uncertainty estimates that are better at identifying OOD data and with lower complexity. Further, we also show that REPEAT works effectively with different types of stochastic R-XAI methods. We believe REPEAT can play an important role in moving the field of R-XAI forward."}]}