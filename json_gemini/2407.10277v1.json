{"title": "Disrupting Diffusion-based Inpainters with Semantic Digression", "authors": ["Geonho Son", "Juhun Lee", "Simon S. Woo"], "abstract": "The fabrication of visual misinformation on the web and social media has increased exponentially with the advent of foundational text-to-image diffusion models. Namely, Stable Diffusion inpainters allow the synthesis of maliciously inpainted images of personal and private figures, and copyrighted contents, also known as deepfakes. To combat such generations, a disruption framework, namely Photoguard, has been proposed, where it adds adversarial noise to the context image to disrupt their inpainting synthesis. While their framework suggested a diffusion-friendly approach, the disruption is not sufficiently strong and it requires a significant amount of GPU and time to immunize the context image. In our work, we re-examine both the minimal and favorable conditions for a successful inpainting disruption, proposing DDD, a \u201cDigression guided Diffusion Disruption\u201d framework. First, we identify the most adversarially vulnerable diffusion timestep range with respect to the hidden space. Within this scope of noised manifold, we pose the problem as a semantic digression optimization. We maximize the distance between the inpainting instance's hidden states and a semantic-aware hidden state centroid, calibrated both by Monte Carlo sampling of hidden states and a discretely projected optimization in the token space. Effectively, our approach achieves stronger disruption and a higher success rate than Photoguard while lowering the GPU memory requirement, and speeding the optimization up to three times faster.", "sections": [{"title": "1 Introduction", "content": "The malicious editing of visual content is a long-standing ethical issue in the online community. Lately, this concern has only been significantly amplified with the integration of deep learning algorithms into the online community. Today, with recent advancements in the deep learning community, high-quality content crafting, known as Deepfake [Le et al., 2024; Le and Woo, 2023; Lee et al., 2022; Cho et al., 2023; Hong et al., 2024; Le et al., 2023], has been refined, and the discernment against real content is harder than ever. In tandem with malicious editing, this technology has led to increasing cases of social chaos and misinformation.\nRecently, much interest has been directed to diffusion-based generative models [Ho et al., 2020; Song et al., 2020b]. One impactful model among them is the text-to-image generator called Stable Diffusion [Rombach et al., 2022], which was trained on LAION, a large-scale captioned image dataset [Schuhmann et al., 2022]. This type of magnitude of training and model size unlocked the generative power of \"foundational\" scale and generalizability to unseen and complex prompts. With further fine-tuning [Zhang et al., 2023; Hu et al., 2021; Wang et al., 2023; Lin et al., 2023; Xu et al., 2023], powerful variants such as inpainting models came to being, which allow the user to input a context image and inpaint the remainder with text guidance, quickly be-coming the commercial inpainting approach. Additionally, a major advantage of inpainting over image editing algorithms [Mokady et al., 2023; Wallace et al., 2023] is that edits are exclusive to the user-defined region.\nHowever, this created a breach for adversaries, as these models are well-suited for malicious and unconsented image edits of personal and private figures, copyrighted content, etc. This poses a serious problem on the internet, since the weights of foundational generative models are public to all individuals [von Platen et al., 2022]. To counter the production of such malicious content including deepfakes, researchers [Salman et al., 2023; Ruiz et al., 2020] have proposed ways to disrupt their generation by injecting adversarial noise into the image to disrupt or fool either the face synthesizer or other subnetworks necessary in the generation. While considerable advancement has been made to disrupt GAN-based deepfake generators, the rise of diffusion-based deepfake pleads for an equivalent countering disruption algorithm. Unfortunately, because of the heterogeneous and complex characteristics underlying diffusion models, in which the generation process is gradual and iterative, previously proposed Deepfake disruption approaches are not compatible with the diffusion class of models.\nPhotoguard [Salman et al., 2023] has been proposed to address disruption for diffusion-based inpainters. It optimizes the context image so that it contains adversarial noise to disrupt the inpainting during inference time. While the effort of addressing the complications introduced by the diffusion process is noteworthy, it undergoes a significant computational overhead, amounting to 25GBs of VRAM and 19 min. of optimization. This computation cost is beyond the average consumer's budget. Furthermore, the disruption efficacy across different images, inpainting strengths, prompts is unstable.\nOur work departs from the motivation that, while it is true that diffusion-based models introduce new technical challenges, we challenge the base assumptions that Photoguard considers to be necessary. In particular, to bypass the multiple feedforwards across the entire diffusion reverse process, we take into account the timestep range that, when perturbed with some adversarial signal, can cause the most amount of visual disparity, and harness the timestep constraint-free hidden states for our loss function.\nNext, we leverage the untargeted attack approach in the generative setting. Specifically, we find a centroid of representations to digress away from, which effectively eliminates the burden of defining a target point and its optimization complications of a targeted attack. First, we search for hidden state samples centered around the representation of the context image and calibrated through a discretely projected optimization in the token space. This optimized text embedding cooperates in defining a representative centroid through Monte Carlo sampling. This semantic-aware centroid faithfully captures the user's input in all modalities. Ultimately, distancing away from this point results in an edit that completely disassociates from the context image. Our \"Semantic Digression-guided Diffusion Disruption\", DDD in short, is 3x faster, requires less GBs of VRAM, and the disruption is more effective than the current SoTA, Photoguard, across various images. We provide extensive results and experiments to support the integrity of our framework. Our main contribution is summarized as follows:"}, {"title": "2 Related Works", "content": "GAN-based deepfake generators [Choi et al., 2018; Pumarola et al., 2018] have been the major consideration in both the research community and industry due to their long legacy, fast sampling, and quality. One distinguished architecture is Star-GAN [Choi et al., 2018], trained to transfer the domain of the input image to cross-domains. And, GANimation [Pumarola et al., 2018], a conditional generator, can generate faces according to the expression annotation.\nTo combat unconsented real image edits, Yeh et al. [Yeh et al., 2020] pioneered the first attack on deep generative models, such as CycleGAN [Zhu et al., 2017] and pix2pix [Isola et al., 2017]. They also introduced the Nullifying Attack to invalidate the model's generation and the Distorting Attack to make the generated images blurry or distorted. Ruiz et al. [Ruiz et al., 2020] synthesize adversarial noise to the input image of these image-to-image GANs, so that their outputs are disrupted. Meanwhile, Wang et al. [Wang et al., 2022] designed a perturbation generator, ensuring that the disrupted image output is correctly classified as a fake image by deepfake detectors. Furthermore, Huang et al. [Huang et al., 2021] introduced an iteratively trained surrogate model to provide feedback to the perturbation generator, enabling the suppression of facial manipulation. While these disrupters are effective, the expressivity of the deepfake images made with GAN-based models is limited by the annotation, quality of the dataset, and model scalability.\nOn the other hand, the recent text-to-image latent diffusion such as Stable Diffusion is trained on LAION [Schuhmann et al., 2022], a large-scale dataset scraped from the web. Naturally, it can generalize to complex and unseen prompts, and synthesize with high coherence. In particular, the Stable Diffusion (SD) Inpainter model [Rombach et al., 2022] is a finetuned Stable diffusion with masked context image conditioning and it can edit via inpainting the masked area. To disrupt"}, {"title": "3 Background", "content": "3.1 Adversarial Attacks\nAn adversarial attack is a method to generate adversarial examples to deceive the ML system [Goodfellow et al., 2014]. In the perspective of discriminators, given an objective function L, a projective function II, and an image x its true label y, the PGD [Madry et al., 2017] attack is performed as follows:\n$$x^{t+1} = I_{X+\\delta}(x^t - \\alpha \\operatorname{sign}(\\nabla_xL(X, Y_{target}))).$$ (1)\nThis iterative update algorithm helps in identifying local maxima that induce misclassification and it is the engine behind most disruption frameworks. Likewise, we adopt PGD in our framework to update and refine our disruptive perturbation.\n3.2 Diffusion Models\nConsider $$x_t \\in \\mathbb{R}^{1\\times 3\\times W\\times H}$$, where $$x_T$$ is an isotropic Gaussian noise, $$x_0$$ is true data observation, and any $$x_t$$ between these is defined as follows:\n$$q(x_t | x_{t-1}) := \\mathcal{N} (x_t; \\sqrt{\\alpha_t}x_{t-1}, (1 - \\alpha_t) I)$$\n$$q(x_{T} | x_{0}) \\approx \\mathcal{N}(x_{T}; 0, I)$$. (2)\nAccordingly, diffusion models [Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020b; Song et al., 2020a] are a class of generative models that gradually denoises pure random noise $$x_T$$ until it becomes true data observation $$x_0$$. The training of such a model consists of predicting $$x_{t-1}$$ given $$X_t$$, where ground truth $$X_{t-1}$$ can be analytically yielded as an interpolation between $$x_0$$ and $$x_t$$ through Bayes rules. With reparametrization of $$x_t$$, it is rather common to predict the $$\\epsilon$$ injected to $$x_0$$ to sample $$x_t$$, formulated as:\n$$\\mathcal{L}_{diffusion} = E_{x_t, t, \\epsilon \\sim \\mathcal{N}(0,I)} [||\\epsilon - \\epsilon_{\\theta}(x, t)||_{2}^{2}]$$. (3)\n3.3 Stable Diffusion Inpainters\nFollowing from the definition of diffusion models, the Stable Diffusion uses the same training paradigm [Rombach et al., 2022] but diffuses in the latent space. Formally, $$x_0$$ is first encoded with a VAE encoder to the latent space as $$z_0 \\in \\mathbb{R}^{1\\times 4\\times 64\\times 64}$$, and the diffusion process occurs in the latent space, where $$\\epsilon$$ is predicted, given $$z_t$$ and a text embedding condition $$\\tau$$. These components change the formulation to:\n$$\\mathcal{L}_{LDM} = E_{t, \\tau, \\epsilon \\sim \\mathcal{N}(0,I)} [||\\epsilon - \\epsilon_{\\theta} (z_t, \\tau, t)||_{2}^{2}]$$, (4)\nwhere $$\\tau$$ is the text embedding. The so-called \u201cInpainters\u201d are fine-tuned models from the Stable Diffusion checkpoints [Rombach et al., 2022; Zhang et al., 2023]. Typically, given some latent code $$z_0$$ of a real image $$X$$ to be inpainted, inpainting models minimally receive as input a context image latent $$C \\in \\mathbb{R}^{1\\times 4\\times 64\\times 64}$$ and a binary mask $$M \\in \\mathbb{R}^{1\\times 4\\times 64\\times 64}$$ as follows:\n$$C = M \\odot X$$, (5)\n$$\\text{where} \\quad M_{i, j} = \\begin{cases} 0, & \\text{if to be inpainted} \\\\ 1, & \\text{if to be context} \\end{cases}$$\nIn the Inpainter's finetuning process, Stable Diffusion model's original input channels are extended so that both $$M$$ and $$C$$ can be fed as additional conditional signal for denoising $$z_t$$. Then, $$M$$ and $$C$$ will be conditioned throughout the iterative denoising of $$z_t$$ up to $$z_0$$.\nIn particular, the generation process spans over n feedforward iterations, where n commonly ranges between 20 to 50 for a reasonable generation. The backpropagation of n feedforward is memory-wise infeasible. The current SoTA framework Photoguard [Salman et al., 2023] optimizes the context image $$C$$ with PGD, the subject of protection from malicious edits. The authors of Photoguard approximate $$z_0$$ by iterating over just 4 denoising steps, which is assumed to be sufficient to synthesize an approximation $$\\hat{z_0}$$. With this sample $$\\hat{z_0}$$, $$z_0$$ is decoded to $$x_0$$ and the L2 distance between $$x_0$$ and an arbitrary target image is minimized. Formally, with some simplification, Photoguard's loss is as follows:\n$$\\delta = \\operatorname{argmin}_{||\\delta||_2} || f(x+\\delta, C) - X_{target} ||_2$$. (6)\nwhere f denotes the entire LDM pipeline over n feedforward steps, $$C$$ is the context image, and $$\\delta$$ is the adversarial perturbation. To the best of our knowledge, Photoguard is the only algorithm that tackles disruption in diffusion-based deepfake generation inpainters."}, {"title": "4 Method", "content": "4.1 Search for the Vulnerable Timestep\nOne of the most self-evident complications presented by SD Inpainter is the diffusion process. To take into perspective, the denoising diffusion reverse process can be thought of as the random latent code being decoded by a VAE [Oord et al., 2017]. Then, it is plausible that we need to feedforward through all synthesis \"stages\" to optimize efficiently.\nInstead of directly complying with this computational overhead, we take advantage of the \u201cprogressive synthesis\u201d property of the diffusion process to strategically target the early timesteps. It is known to the community that early timesteps have sovereignty over the overall spatial structure and global semantics of the image [Meng et al., 2021; Huang et al., 2023; Chen et al., 2023]. To gain a sufficient degree of freedom of disruption, we optimize our adversarial context with respect to the early stage, leading to global damage. It is noted that the model should behave similarly across timesteps adjacent to our target timestep range to sustain the vulnerability of predicted scores $$\\nabla \\log p(x)$$.\nWhile many researchers already rely on the linearization of adjacent timesteps [Huberman-Spiegelglas et al., 2023; Miyake et al., 2023], we reaffirm this linearity through both PCA decomposition of the hidden states across all timesteps, discussed in Appendix E, and empirical results of effective disruption.\n4.2 Timestep Constraint-Free Loss Function\nWhile the narrowing of the sampled timestep disables access to both the diffusion-native and off-the-shelf losses (LPIPS,CLIP, Perceptual loss), we take advantage of the denoiser's hidden states to yield a timestep-agnostic loss space. Specifically, we are concerned with strategically drifting the hidden state out of its learned manifold in the Euclidean space. Then, if we have the text embedding $$\\tau_{text}$$ obtained from the text encoder $$\\&$$ and the feature map $$F_{C, \\tau_{text}}$$, our loss on the space of H is as follows:\n$$\\mathcal{L}_{hidden} = \\sum_{i}^{N} ||H_{target} - H_{source}||_2$$, (7)\nwhere $$H(C, \\tau_{text}, z_T) = Attention(F_{C, \\tau_{text}}, z_T)$$\n$$F_{C, \\tau_{text}}$$ is the feature map conditioned with $$C, \\tau_{text}$$. While all three conditions are indispensable for F, we omit their inscription when they are constant. Because we want to disrupt the information pathway for multi-modal signals, (C and $$\\tau$$), we choose the self-attention layers. Ultimately, this loss space enables the mining of both dense and timestep-agnostic features.\n4.3 Aligning the Objective with Disruption's Goal\nThe success of untargeted attacks is well-known to the adversarial attack community.\nIn our setting where the sole concern is pulling away from what is correct, the untargeted attack approach is highly aligned with our implicit objective, with no other constraints imposed other than disruption itself.\nNow, it is noteworthy that in the discriminative setting, the notion of \u201ccorrectness\u201d is trivially given by the probability vector. Then, cross-entropy gives us a scalar metric to evaluate the correctness. However, for the most part in generative models, this type of reduced evaluative metric is not plainly given, hence $$Y_{true}$$ is ambiguous. As an alternative tentative approach, we discuss the consequences of framing the approach with a targeted attack in Appendix D.\nTo adopt untargeted attacks to diffusion models, we approximate the concept of a correct synthesis. A simple approach to obtain the correct output synthesis is to take the average at the $$x_t$$ space in $$\\mathbb{R}^{D_{pixel}}$$, where $$D_{pixel} = W \\times H$$. Unfortunately, such averaging yields a low-fidelity and ambiguous ground truth target (equivalent to an averaged blurred image). Instead, the hidden space $$\\mathbb{R}^{D_{hidden}}$$, where $$D_{hidden} << D_{pixel}$$, is suitable for yielding a high-fidelity centroid with less variance and higher spatial agnosticism. Accordingly, through Monte Carlo sampling, we approximate the ground truth representation centroid $$\\phi_{hidden}$$, defined as following:\n$$\\phi(C, \\tau_{text}) = E_{z_t \\sim \\mathcal{N}(0, I), t \\sim \\mathcal{N}(720, 5.8)} [H(z_T, C, \\tau_{text}, t)]$$ (8)\nWhile the context image is given apriori, the choice of the text prompt $$\\tau$$ is still ambiguous. In other words, given that the deep activations accommodate for multi-modal signal [Hussain et al., 2023; Elhage and et al., 2021], ignoring the text modality is prone to yield sub-optimal results.\n4.4 Token Projective Textual Optimization\nOne simple approach is to condition over the null text, which yields a reasonable centroid that captures the signal of the context image. However, the null text is also implicitly biased and fitted to the global dataset, which is naturally long-tailed. To this end, we address this by searching in the text embedding space for the representation that embodies the context image's content and use it to condition upon our centroid-constituting instances. Specifically, we optimize text embedding $$\\tau$$ that minimizes the diffusion loss:\n$$\\tau^* = \\operatorname{argmin}_{\\tau} ||\\epsilon^* \\odot M - \\epsilon(z_T, C, \\tau, M_{inv})^* \\odot M||_2$$, (9)\nwhere $$M_{inv}$$ is M inverted and $$\\tau$$ is initialized from a random text embedding. One common issue with most image inversion algorithms [Zhu et al., 2020; Xia et al., 2022] is the propensity of the embedding to overfit to the reference image, which results in anomalous expressivity and poor generalization. While premeditatively underfitting it viable, convergence points vary for every instance.\nPEZ has shown that one can invert a reference image with \u201cdecodable\u201d token embeddings [Wen et al., 2023]. Formally,in between every gradient descent, the updated token embeddings are projected to their nearest tokens, which are ultimately used for text decoding. Instead, we view this intermediate discretization as an approach to optimization regularization. In essence, the token embedding matrix constitutes the keypoints for the topology of the token space manifold. Because this token embedding matrix encompasses the primal semantics, having the continuous embedding projected to this learned manifold ensures that it stays in-distribution. Then, while a constraint-free embedding optimization eventually leads to overfitted solutions, the inherent discreteness of the token projection intercepts any adversarial build-up. Formally, let $$\\pi$$ be token embeddings, Proj be the token projection function, $$\\&$$ be the text encoder, $$\\alpha$$ be the step size, and $$\\\\mathcal{L}$$ be the diffusion loss, we optimize for the token embedding that minimizes the diffusion loss with a projective update as follows:\n$$\\pi_{t+1} = \\pi_{t} - \\alpha \\nabla_{\\pi_t} \\mathcal{L}(\\&(Proj(\\pi_t)), z_t, t, C)$$ (10)\nHaving obtained the optimized $$\\pi^*$$, we have implicitly yielded its bijective counterpart $$\\tau^* = \\&(\\pi^*)$$, the text embedding. Ultimately, this optimization will lead to a $$\\tau^*$$ highly descriptive of the context image, and the centroid $$\\phi(C, \\tau^*)$$ will embody the expected multi-modal representation of the context image. Given that we obtained this surrogate for $$Y_{true}$$, we now have access to the digression formulation in MSE, which, due to what the centroid is encoding, the optimization will lead to a digression orthogonal to the semantics of the context image. Ultimately, our final loss is as follows:\n$$C^* = \\operatorname{argmax}_{C} || \\phi(C, \\tau^*) - H(z_T, C, \\tau^*, M) ||_2$$ (11)\nThe iterative update of $$C$$ follows the PGD update protocol. Ultimately, as summarized in Fig. 2, due to its digressive objective, we name our framework \u201cDigressive Guidance for Disrupting Diffusion-based Inpainters\u201d, or in short, DDD."}, {"title": "5 Experimental Results", "content": "5.1 Technical Details\nFor DDD's disruption synthesis, we have used Nvidia's A100 40GB GPU, taking under 6 minutes of optimization, more than 3 times faster than Photoguard. All of our experiments are conducted with PGD's epsilon budget of 12/255, step size of 3/255, and gradient averaging of 7 for 250 iterations. While the loss curve hints that further optimization is possible with noticeable returns, we verified that 250 is enough to cover all"}, {"title": "6 Discussion", "content": "Our work entails discussions from various ends. Particularly, note that DDD is a conceptual framework. Put differently, the applicability of our framework is compatible with untargeted disruption of upcoming variants of inpainters and synthesizers, even generalizing beyond diffusion models.\nInpainting disruption is an emerging task, especially with the introduction of powerful text-to-image models. The current literature on inpainting disruption is limited and has much room for growth. For example, no disruption-specific metric has been proposed yet. As of now, human evaluation and PickScore are the metrics with the highest fidelity in grading the inpainting synthesis completeness.\nAlso, we found that both DDD and Photoguard need more robustness to data augmentations and agnosticity to the context image's mask positioning. We believe this field will highly benefit from these developments. In addition to these topics, we discuss experimental configurations of our framework, strength interplay, transferability, survey form, metrics, and more in the Appendix."}, {"title": "7 Conclusion", "content": "We proposed to immunize context images and disrupt their malicious inpainting edits through our framework DDD. To strategically manage the diffusion process, we focused on the model's hidden state statistics across different timesteps. This showed us how to select a vulnerable timestep range and reduced the complexity of the attack's forward pass. Next, we formulate the disruption objective as a semantic digression optimization, which not only offers a greater degree of optimization, but also takes full consideration of the context image. This is only possible through searching for the multimodal centroid, calibrated and regulated by token projective embedding optimization, and constructed through Monte Carlo sampling. As a result, we significantly reduce GPU VRAM usage and speed up the optimization time by 3x. Our framework is supported by quantitative and qualitative results, and contributional research resources on this novel task in diffusion inpainters."}, {"title": "Ethical Statements", "content": "Our work involves nudity and sexually explicit content, but as all models are publicly available, our institution's IRB advised that approval was not required. All researchers involved are over 21 and have carefully reviewed relevant ethics guidelines [NeurIPS, 2023] and undergone training to handle and analyze research results properly. Although no practical defense against creating nudity in generative models exists, we emphasize the urgency of developing preventive technologies given our work's focus on explicit and unsafe content."}, {"title": "Acknowledgments", "content": "The authors would thank anonymous reviewers. Geonho Son and Juhun Lee contributed equally as joint first authors. Simon S. Woo is the corresponding author. This work was"}, {"title": "Appendix", "content": "A Immunization Transferability\nTransferability refers to the optimization of the adversarial context image on one network to only be applied on a different network. To the extent of open-sourced foundational inpainter checkpoints, we check transferability between Runwayml's 1.5v model and Stabilityai's 2.0v checkpoint, the main checkpoints in the community. It is noteworthy that while these two share the same network architecture, they are independent checkpoints with different training protocols. In addition to the quantitative results in Table 2, Fig. 6 qualitatively shows that our model transfers disruption bilaterally. On the other hand.\nB Inpainting Strength\nWe also take note of the strength functionality of the SD Inpainter. Given a real image, strength decides how much content of this image we will preserve in the to-be-inpainted area. For example, considering the timestep t to be normalized between 0 and 1, strength = 1 completely noise the inpainted area into x1, whereas strength = 0.8 noises the area up to x0.8. While crafting the adversarial noise by sampling close to z1 (zT) can prove to be the strongest for inpainting with strength=1, immunization considering all test case scenarios and cover harder thresholds such as 0.8. Therefore, weconsider both the strength factor and the similarity between the hyperfeatures [Luo et al., 2023] to define our sampling timesteps range as t ~ N(720, 6). The sampling distribution is directly inferred from the previously discussed factors of considerations and shows robust performance even with an offset of 720 \u00b110.\nC Robustness\nAnother important factor for image protection in the web is robustness to data augmentations and compression. As shown in Fig. 8, we test if the adversarial perturbation survives across Gaussian noise, color jitter, JPEG compression, and rotation with crop. While the disruption performance degrades by a considerable margin, some images still retain the disruptive signal. Note that these are only representative cases where the adversarial signal survives different augmentations, but more research has to be conducted to improve its effect retention. Additionally, the task of disruption defines the protected area apriori. Obstruction of the adversarial noise is also expected to result in less effective attacks. Thus, these areas under robustness are of utmost contribution in this area in future works.\nD Discussion on Targeted Optimization\nThe targeted optimization approach in both frameworks is how the disruption is bounded by when $$\\mathcal{L}_{targeted} = 0$$. At this stage, it reaches the maximum disruption possible under the chosen $$\\\\mathcal{L}$$ and some target image/latent. Yet, there is little guarantee that this maximum disruption will be sufficient and may fall short of our evaluation criteria. The success of the targeted approach relies on the search for an optimal target, which is an approach that asks for non-trivial considerations."}]}