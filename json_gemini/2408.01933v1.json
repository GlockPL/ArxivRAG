{"title": "DiReCT: Diagnostic Reasoning for Clinical Notes\nvia Large Language Models", "authors": ["Bowen Wang", "Jiuyang Chang", "Yiming Qian", "Guoxin Chen", "Junhao Chen", "Zhouqiang Jiang", "Jiahao Zhang", "Yuta Nakashima", "Hajime Nagahara"], "abstract": "Large language models (LLMs) have recently showcased remarkable capabilities,\nspanning a wide range of tasks and applications, including those in the medical\ndomain. Models like GPT-4 excel in medical question answering but may face\nchallenges in the lack of interpretability when handling complex tasks in real\nclinical settings. We thus introduce the diagnostic reasoning dataset for clinical\nnotes (DiReCT), aiming at evaluating the reasoning ability and interpretability of\nLLMs compared to human doctors. It contains 511 clinical notes, each meticu-\nlously annotated by physicians, detailing the diagnostic reasoning process from\nobservations in a clinical note to the final diagnosis. Additionally, a diagnostic\nknowledge graph is provided to offer essential knowledge for reasoning, which\nmay not be covered in the training data of existing LLMs. Evaluations of leading\nLLMs on DiReCT bring out a significant gap between their reasoning ability and\nthat of human doctors, highlighting the critical need for models that can reason\neffectively in real-world clinical scenarios *.", "sections": [{"title": "1 Introduction", "content": "Recent advancements of large language models (LLMs) [Zhao et al., 2023] have ushered in new\npossibilities and challenges for a wide range of natural language processing (NLP) tasks [Min\net al., 2023]. In the medical domain, these models have demonstrated remarkable prowess [Anil\net al., 2023, Han et al., 2023], particularly in medical question answering (QA) [Jin et al., 2021].\nLeading-edge models, such as GPT-4 [OpenAI, 2023a], exhibit profound proficiency in understanding\nand generating text [Bubeck et al., 2023], even achieved high scores on the United States Medical\nLicensing Examination (USMLE) questions [Nori et al., 2023].\nDespite the advancements, interpretability is critical, particularly in medical NLP tasks [Li\u00e9vin et al.,\n2024]. Some studies assess this capability over medical QA [Pal et al., 2022, Li et al., 2023, Chen\net al., 2024] or natural language inference (NLI) [Jullien et al., 2023]. Putting more attention on\ninterpretability, they use relatively simple tasks as testbeds, taking short text as input. However,\ntasks in real clinical settings can be more complex [Gao et al., 2023a]. As shown in Figure 1, a\ntypical diagnosis requires comprehending and combining various information, such as health records,\nphysical examinations, and laboratory tests, for further reasoning of possible diseases in a step-by-step\nmanner following the established guidelines. This observation suggests that both perception, or\nreading, (e.g., finding necessary information in medical record) and reasoning (determining the\ndisease based on the observations) should be counted when evaluating interpretability in LLM-based\nmedical NLP tasks.\nFor a more comprehensive evaluation of LLMs for supporting diagnosis in a more realistic setting,\nwe propose a Diagnostic Reasoning dataset for Clinical noTes (DiReCT). The task basically is\npredicting the diagnosis from a clinical note of a patient, which is a collection of various medical\nrecords, written in natural language. Our dataset contains 511 clinical notes spanning 25 disease\ncategories, sampled from a publicly available database, MIMIC-IV [Johnson et al., 2023]. Each\nclinical note undergoes fine-grained annotation by professional physicians. The annotators (i.e., the\nphysicians) are responsible for identifying the text, or the observation, in the note that leads to a\ncertain diagnosis, as well as the explanation. The dataset also provides a diagnostic knowledge graph\nbased on existing diagnostic guidelines to facilitate more consistent annotations and to supply a\nmodel with essential knowledge for reasoning that might not be encompassed in its training data.\nTo underscore the challenge offered by our dataset, we evaluate a simple AI-agent based baseline [Xi\net al., 2023, Tang et al., 2023] that utilizes the knowledge graph to decompose the diagnosis into a\nsequence of diagnoses from a smaller number of observations. Our experimental findings indicate\nthat current state-of-the-art LLMs still fall short of aligning well with human doctors.\nContribution. DiReCT offers a new challenge in diagnosis from a complex clinical note with explicit\nknowledge of established guidelines. This challenge aligns with a realistic medical scenario that\ndoctors are experiencing. In the application aspect, the dataset facilitates the development of a model\nto support doctors in diagnosis, which is error-prone [Middleton et al., 2013, Liu et al., 2022]. From\nthe technical aspect, the dataset can benchmark models' ability to read long text and find necessary\nobservations for multi-evidence entailment tree reasoning. As shown in Figure 3, this is not trivial\nbecause of the variations in writing; superficial matching does not help, and medical knowledge\nis vital. Meanwhile, reasoning itself is facilitated by the knowledge graph. The model does not\nnecessarily have the knowledge of diagnostic guidelines. With this choice, the knowledge graph\nexplains the reasoning process, which is also beneficial when deploying such a diagnosis assistant\nsystem in practical uses."}, {"title": "2 Related Works", "content": "Natural language explanation. Recent advancements in NLP have led to significant achieve-\nments [Min et al., 2023]. However, existing models often lack explainability, posing potential risks\n[Danilevsky et al., 2020, Gurrapu et al., 2023]. Numerous efforts have been made to address this\nchallenge. One effective approach is to provide a human-understandable plain text explanation\nalongside the model's output [Camburu et al., 2018, Rajani et al., 2019]. Another strategy involves\nidentifying evidence within the input that serves as a rationale for the model's decisions, aligning with\nhuman reasoning [DeYoung et al., 2020]. Expanding on this concept, [Jhamtani and Clark, 2020]\nintroduces chain-structured explanations, given that a diagnosis can demand multi-hop reasoning.\nThis idea is further refined by ProofWriter [Tafjord et al., 2021] through a proof stage for explanations,"}, {"title": "3 A benchmark for Clinical Notes Diagnosis", "content": "This section first detail clinical notes (Section 3.1). We also describes the knowledge graph that\nencodes existing guidelines (Section 3.2). Our task definition, which tasks a clinical note and the\nknowledge graph as input is given in Section 3.4. We then present our annotation process for clinical\nnotes (Section 3.3) and the evaluation metrics (Section 3.5)."}, {"title": "3.1 Clinical Notes", "content": "Clinical notes used in DiReCT are stored in the SOAP format [Weed, 1970]. A clinical note comprises\nfour components: In the subjective section, the physician records the patient's chief complaint, the\nhistory of present illness, and other subjective experiences reported by the patient. The objective\nsection contains structural data obtained through examinations (inspection, auscultation, etc.) and\nother measurable means. The assessment section involves the physician's analysis and evaluation of\nthe patient's condition. This may include a summary of current status, etc. Finally, the plan section\noutlines the physician's proposed treatment and management plan. This may include prescribed\nmedications, recommended therapies, and further investigations. A clinical note also includes a\nprimary discharge diagnosis (PDD) in the assessment section.\nDiReCT's clinical notes are sourced from the MIMIC-IV dataset [Johnson et al., 2023] (PhysioNet\nCredentialed Health Data License 1.5.0), which encompasses over 40,000 patients admitted to the\nintensive care units. Each note contains clinical data for a patient. To construct DiReCT, we curated a\nsubset of 511 notes whose PDDs fell within one of 25 disease categories i in 5 medical domains.\nIn our task, a note R = {r} is an excerpt of 6 clinical data in the subjective and objective sections (i.e.,\nR = 6): chief complaint, history of present illness, past medical history, family history, physical"}, {"title": "3.2 Diagnostic Knowledge Graph", "content": "Existing knowledge graphs for the medical domain, e.g., UMLS KG [Bodenreider, 2004], lack the\nability to provide specific clinical decision support (e.g., diagnostic threshold, context-specific data,\ndosage information, etc.), which are critical for accurate diagnosis.\nOur knowledge graphs K = {Ki}i is a collection of graph Ki for disease category i. Ki is based on\nthe diagnosis criteria in existing guidelines (refer to supplementary material for details). K\u2081's nodes\nare either premise $p \\in P_i$ (medical statement, e.g., Headache is a symptom of) and diagnoses\n$d \\in D_i$ (e.g., Suspected Stroke). Ki consists of two different types of edges. One is premise-\nto-diagnosis edges $S_i = \\{(p,d)\\}$, where $p \\in P_i$ and $d \\in D_i$; an edge is from p to d. This edge\nrepresents the necessary premise p to make a diagnosis d. We refer to them as supporting edges. The\nother is diagnosis-to-diagnosis edges $F_i = \\{(d,d')\\}$, where $d, d' \\in D_i$ and the edge is from d to d',\nwhich represents the diagnostic flow. These edges are referred to as procedural edges.\nA disease category is defined according to an existing guideline, which starts from a certain diagnosis;\ntherefore, a procedural graph $G_i = (D_i, F_i)$ has only one root node and arbitrarily branches toward\nmultiple leaf nodes that represent PDDs (i.e., the clinical notes in DiReCT are chosen to cover all\nleaf nodes of $G_i$). Thus, $G_i$ is a tree. We denote the set of the leaf nodes (or PDDs) as $D_i' \\subset D_i$. The\nknowledge graph is denoted by $K_i = (D_i, P_i, S_i, F_i)$.\nK serves two essential functions: (1) They serve as the gold standard for annotation, guiding doctors\nin the precise and uniform interpretation of clinical notes. (2) Our task also allows a model to use\nthem to ensure the output from an LLM can be closely aligned with the reasoning processes of\nmedical professionals."}, {"title": "3.3 Data Annotation", "content": "Let $d^* \\in D$ denote the PDD of disease category i associated with R. We can find a subgraph $K_i(d^*)$\nof $K_i$ that contains all ancestors of $d^*$, including premises in $P_i$. We also denote the set of supporting\nedges in $K_i(d^*)$ as $S_i(d^*)$. Our annotation process is, for each supporting edge $(p, d) \\in S_i(d^*)$,\nto extract observation $o \\in O$ in R (highlighted text in the clinical note in Figure 3) and provide\nrationalization z of this deduction why o is a support for d or corresponds to p. They form the\nexplanation $E = \\{(o, z,d)\\}$ for (R, d*). This annotation process was carried out by 9 clinical\nphysicians and subsequently verified for accuracy and completeness by three senior medical experts."}, {"title": "3.4 Task Definition", "content": "We propose two tasks with different levels of\nsupplied external knowledge. The first task\nis, given R and G, to predict the associated\nPDD d* and generate an explanation & that\nexplains the model's diagnostic procedure\nfrom R to d*, i.e., letting M denote a model:\n$\\hat{d^*},\\hat{E} = M(R,G)$,\nwhere $\\hat{d^*} \\in \\cup_iD$ and $\\hat{E}$ are predictions for the PDD and explanation, respectively. With this task,\nthe knowledge of specific diagnostic procedures in existing guidelines can be used for prediction,\nfacilitating interpretability. The second task takes K as input instead of G, i.e.,:\n$\\hat{d^*},\\hat{E} = M(R,K)$.\nThis task allows for the use of broader knowledge of premises for prediction. One may also try a task\nwithout any external knowledge."}, {"title": "3.5 Evaluation Metrics", "content": "We designed three metrics to quantify the predictive performance over our benchmark.\n(1) Accuracy of diagnosis Accdiag evaluates if a model can correctly identify the diagnosis. Accdiag = 1\nif d* = d, and Accdiag = 0 otherwise. The average is reported.\n(2) Completeness of observations Obscomp evaluates whether a model extracts all and only necessary\nobservations for the prediction. Let O and \u00d4 denote the sets of observations in E and \u00ca, respectively.\nThe metric is defined as Obscomp = $\\frac{|O \\cap \\hat{O}|}{|O \\cup \\hat{O}|}$, where the numerator is the number of\nobservations that are common in both O and \u00d4. This metric simultaneously evaluates the correctness\nof each observation and the coverage. To supplement it, we also report the precision Obspre and recall\nObsrec, given by Obspre = $\\frac{|O \\cap \\hat{O}|}{|\\hat{O}|}$ and Obsrec $\\frac{|O \\cap \\hat{O}|}{|O|}$.\n(3) Faithfulness of explanations Faith evaluates if the diagnostic flow toward the PDD is fully\nsupported by observations with faithful rationalizations. This involves establishing a one-to-one cor-\nrespondence between deductions in the prediction and the ground truth. We use the correspondences\nestablished for computing Obscomp. Let o \u2208 O and \u00f4 \u2208 \u00d4 denote corresponding observations. This"}, {"title": "4 Baseline", "content": "Figure 4 shows an overview of our baseline with three LLM-based modules narrowing-down,\nperception, and reasoning (refer to the supplementary material for more details). The narrowing-\ndown module U takes R as input to make a prediction i of the disease category, i.e., i = U(R).\nLet dt \u2208 D be the diagnosis that has been reached with t iterations over K\u2081, where t corresponds to\nthe depth of node d\u2081 and so is less than or equal to the depth of K\u2081. do is the root node of K. For do,\nwe apply the perception module to extract all observations in R and explanation E to support do as\n$\\hat{O}, \\hat{E_o} = \\Psi(do, K_i)$.\nK is supplied to facilitate the model to extract all observations for the following reasoning process.\nDiagnosis dt identifies the set {dn}n of its children and so the set $P_i(\\{d_n\\}_n) = \\{p \\in P_i|(p, d_n) \\in$\n$S_i, d_n \\in \\{d_n\\}\\}$ of premises that support dn. Therefore, our reasoning module V iteratively and\ngreedily identifies the next step's diagnosis (i.e., dt+1) from {dn}n, making a rationalization for\neach deduction. That is, V verifies whether there exist o's in \u00d4 that supports one dn. If dn is fully\nsupported, dn is identified as dt+1 for the (t + 1)-th iteration, i.e.,\n$d_{t+1}, \\hat{E}_{t+1} = \\Upsilon(\\hat{O}, \\{d_n\\}, P_i(\\hat{\\{d_n\\}_n}))$.\nOtherwise, the reasoning module fails. V is repeated until d\u2081 in D is found or it fails. In our\nannotation, each observation contributes to deducing only one dt. Therefore, if an observation in\n$\\hat{E}_{t+1}$ is included in the preceding sets of explanations \u0109o to \u00cat, the corresponding explanation in\nthe preceding sets is removed."}, {"title": "5 Experiments", "content": "5.1 Experimental Setup\nWe assess the reasoning capabilities of 7 recent LLMs from diverse families and model sizes,\nincluding 5 instruction-tuned models that are openly accessible: LLama3 8B and 70B [AI@Meta,\n2024], Zephyr 7B [Tunstall et al., 2023], Mistral 7B [Jiang et al., 2023], and Mixtral 8\u00d77B [Jiang\net al., 2023]. We have also obtained access to private versions of the GPT-3.5 turbo [OpenAI, 2023b]"}, {"title": "5.2 Results", "content": "Comparison among LLMs. Table 3 shows the performance of our baseline built on top of various\nLLMs. We first evaluate a variant of our task that takes graph G = {G} consisting of only procedural\nflow as external knowledge instead of K. Comparison between G and K demonstrates the importance\nof supplying premises with the model and LLMs' capability to make use of extensive external\nknowledge that may be superficially different from statements in R. Subsequently, some models are\nevaluated with our task using K. In addition to the metrics in Section 3.5, we also adopt the accuracy\nof disease category Acccat, which gives 1 when \u00ec = i, as our baseline's performance depends on it.\nWith G, we can see that GPT-4 achieves the best performance in most metrics, especially related\nto observations and explanations, surpassing LLama3 70B by a large margin. In terms of accuracy\n(in both category and diagnosis levels), LLama3 70B is comparable to GPT-4. LLama3 70B also\nhas a higher Obsrec but low Obspre and Obscomp, which means that this model tends to extract many\nobservations. Models with high diagnostic accuracy are not necessarily excel in finding essential\ninformation in long text (i.e., observations) and generating reasons (i.e., explanations).\nWhen K is given, all models show better diagnostic accuracy (except GPT-3.5) and explanations,\nwhile observations are slightly degraded. GPT-4 with K enhances Accdiag, Expcom, and Expall scores.\nThis suggests that premises and supporting edges are beneficial for diagnosis and explanation. Lower\nobservational performance may indicate that the models lack the ability to associate premises and\ntext in R, which are often superficially different though semantically consistent.\nLLMs may undergo inherent challenges for evaluation when no external knowledge is supplied. They\nmay have the knowledge to diagnose but cannot make consistent observations and explanations that\nour task expects through K. To explore this, we evaluate two settings: (1) giving D* and (2) no\nknowledge is supplied to a model (shown in Table 4). The prompts used for this setup are detailed\nin the supplementary material. We do not evaluate the accuracy of disease category prediction\nas it is basically the same as Table 3. We can clearly see that with D*, GPT-4's diagnostic and\nobservational scores are comparable to those of the task with K, though explanatory performance\nis much worse. Without any external knowledge, the diagnostic accuracy is also inferior. The\ndeteriorated performance can be attributed to inconsistent wording of diagnosis names, which makes\nevaluation tough. High observational scores imply that observations in R can be identified without\nrelying on external knowledge. There can be some cues to spot them.\nPerformance in individual domains. Figure 5 summarizes the performance of LLama3 70B,\nGPT-3.5, and GPT-4 across different medical domains, evaluated using Acccat, Obscomp, and Expall.\nNeurology gives the best diagnostic accuracy, where GPT-4 achieved an accuracy of 0.806. LLama3\nalso performed well (0.786). In terms of Obscomp and Expall, GPT-4's results were 0.458 and 0.340,\nrespectively, with the smallest difference between the two scores among all domains. This smaller\ngap indicates that in Neurology, the common observations in prediction and ground truth lead to the\ncorrect diagnoses with faithful rationalizations. However, GPT-4 yields a higher diagnostic accuracy\nscore while a lower explanatory score, suggesting that the observations captured by the model or their\nrationalizations differ from human doctors.\nFor Cardiology and Endocrinology, the diagnostic accuracy of the models is relatively low (GPT\n4 achieved 0.458 and 0.468, respectively). Nevertheless, Obscomp and Expall are relatively high.\nEndocrinology results in lower diagnostic accuracy and higher explanatory performance. A smaller\ngap may imply that in these two domains, successful predictions are associated with observations\nsimilar to those of human doctors, and the reasoning process may be analogous. Conversely, in\nGastroenterology, higher Acccat) is accompanied by lower Obscomp and Expall (especially for LLama3),\npotentially indicating a significant divergence in the reasoning process from human doctors. Overall,\nDiReCT demonstrates that the degree of alignment between the model's diagnostic reasoning ability\nand that of human doctors varies across different medical domains.\nReliability of automatic evaluation. We randomly pick\nout 100 samples from DiReCT and their prediction by\nGPT-4 over the task with G to assess the consistency of\nour automated metrics to evaluate the observational and ex-\nplanatory performance in Section 3.3 to human judgments.\nThree physicians joined this experiment. For each predic-\ntion \u00f4 \u2208 \u00d4, they are asked to find a similar observation\nin ground truth O. For explanatory metrics, they verify if\neach prediction \u2248 \u2208 \u00ca for \u00f4 \u2208 \u00d4 align with ground-truth\nz \u2208 E corresponding to o. A prediction and a ground truth are deemed aligned for both assessments\nif at least two specialists agree. We compare LLama3's and GPT-4's judgments to explore if there is a\ngap between these LLMs. As summarized in Table 5, GPT-4 achieves the best results, with LLama3\n8B also displaying a similar performance. From these results, we argue that our automated evaluation"}, {"title": "6 Conclusion and Limitations", "content": "We proposed DiReCT as the first benchmark for evaluating the diagnostic reasoning ability of LLMs\nwith interpretability by supplying external knowledge as a graph. Our evaluations reveal a notable\ndisparity between current leading-edge LLMs and human experts, underscoring the urgent need for\nAI models that can perform reliable and interpretable reasoning in clinical environments. DiReCT\ncan be easily extended to more challenging settings by removing the knowledge graph from the input,\nfacilitating evaluations of future LLMs.\nLimitations. DiReCT encompasses only a subset of disease categories and considers only one PDD,\nomitting the inter-diagnostic relationships due to their complexity\u2014a significant challenge even for\nhuman doctors. Additionally, our baseline may not use optimal prompts, chain-of-thought reasoning,\nor address issues related to hallucinations in task responses. Our dataset is solely intended for model\nevaluation but not for use in clinical environments. The use of the diagnostic knowledge graph is\nalso limited to serving merely as a part of input. Future work will focus on constructing a more\ncomprehensive disease dataset and developing an extensive diagnostic knowledge graph."}, {"title": "A Details of DiReCT", "content": "A.1 Data Statistics\nTable 6 provides a detailed breakdown of the disease categories included in DiReCT. The column\nlabeled # samples indicates the number of data points. The symbols |Di and |D| denote the total\nnumber of diagnoses (diseases) and PDDs, respectively. Existing guidelines for diagnosing diseases\nwere used as References, forming the foundation for constructing the diagnostic knowledge graphs.\nAs some premise may not included in the referred guidelines. During annotation, physicians will\nincorporate their own knowledge to complete the knowledge graph."}, {"title": "A.2 Structure of Knowledge Graph", "content": "The entire knowledge graph, denoted as K, is stored in separate JSON files, each corresponding to a\nspecific disease category i as Ki. Each Ki comprises a procedural graph Gi and the corresponding\npremise p for each disease. As illustrated in Figure 7, the procedural graph G\u2081 is stored under the\nkey \"Diagnostic\" in a dictionary structure. A key with an empty list as its value indicates a leaf\ndiagnostic node as d*. The premise for each disease is saved under the key of \"Knowledge\" with the\ncorresponding disease name as an index. For all the root nodes (e.g., Suspected Heart Failure),\nwe further divide the premise into \"Risk Factors\", \"Symptoms\", and \"Signs\". Note that each premise\nis separated by \";\"."}, {"title": "A.3 Annotation and Tools", "content": "We have developed proprietary software for annotation purposes. As depicted in Figure 8, annotators\nare presented with the original text as observations o and are required to provide rationales (z) to\nexplain why a particular observation o supports a disease d. The left section of the figure, labeled\nInput1 to Input6, corresponds to different parts of the clinical note, specifically the chief complaint,\nhistory of present illness, past medical history, family history, physical exam, and pertinent results,\nrespectively. Annotators will add the raw text into the first layer by left-clicking and dragging to\nselect the original text, then right-clicking to add it. After each observation, a white box will be\nused to record the rationales. Finally, a connection will be made from each rationale to a disease,"}, {"title": "A.4 Access to DiReCT", "content": "Implementation code and annotation tool are available through https://github.com/wbw520/DiReCT.\nData will be released through PhysioNet due to safety issues according to the license of MIMIC-IV\n(PhysioNet Credentialed Health Data License 1.5.0). We will use the same license for DiReCT. The\ndownload link will be accessible via GitHub. We confirm that this GitHub link and data link are\nalways accessible. We confirm that we will bear all responsibility in case of violation of rights."}, {"title": "B Implementation of Baseline Method", "content": "B.1 Prompt Settings\nIn this section, we demonstrate the prompt we used for each module (From Table 7-9 for narrowing-\ndown, perception, and reasoning module, respectively).\nIn Table 7, {disease_option} is the name for all disease categories, and {note} is the content for the\nwhole clinical note. The response for the model is the name of a possible disease \u00ee.\nIn Table 8, {disease} is the disease category name predicted in narrowing-down. The content marked\nblue is the premise, which is only provided during the || setting. In this module, {premise} is offered\nwith all information in the knowledge graph. Different to narrowing-down, {note} is implemented\nfor each clinical data R = {r} and the outputs are combined together for \u00d4 and \u00ca.\nIn Table 9, {disease} is the disease category name and {disease_option} is consisted by the children\nnodes {dn}n. Similarly, the premise on the blue is only available for the || setting. It provides the\npremise that are criteria for the diagnosis of each children node. {observation} is the extracted \u00d4 in\nprevious step. We provide all the prompts and the complete implementation code on GitHub."}, {"title": "B.2 Details of Automatic Evaluation", "content": "The automatic evaluation is realized by LLama3 8B. We demonstrate the prompt for this implement\nin Table 10 (for observation) and Table 11 (for rationalization). Note that we do not use few-shot\nsamples for the evaluation of observation. In Table 10, {gt_observation} and {pred_observation} are\nfrom model prediction and ground-truth. As this is a simple similarity comparison task to discriminate\nwhether the model finds similar observations to humans, LLama3 itself have such ability. We do not\nstrict to exactly match due to the difference in length of extracted raw text (as long as the observation\nexpresses the same description). In Table 11, {gt_reasoning} and {pred_reasoning} are from model\nprediction and ground-truth. We require the rationale to be complete (content of the expression can\nbe understood from the rationale alone) and meaningful; therefore, we provide five samples for this\nevaluation. We also provide all the prompts and the complete implementation code on GitHub."}, {"title": "B.3 Prediction Samples", "content": "Figure 9 and 10 shows two sample generated by GPT-4. The ground-truth PDD of the input\nclinical note is Gastroesophageal Reflux Disease (GERD) and Heart Failure (HF). In\nthese figure, purple, orange, and red indicate explanations only in the ground truth, only in prediction,\nand common in both, respectively; therefore, red is a successful prediction of an explanation, while\npurple and orange are a false negative and false positive."}, {"title": "B.4 Experiments for No Extra Knowledge", "content": "We demonstrate the prompt used for D* and no knowledge settings in Table 12 and Table 13,\nrespectively. {note} is the text of whole clinical note and {note} in Table 12 is the name of all leaf\nnode D*."}, {"title": "B.5 Experimental Settings", "content": "All experiments are implemented with a temperature value of 0. All close sourced models are\nimplemented in a local server with 4 NVIDIA A100 GPU."}, {"title": "C Failed Attempts on DiReCT", "content": "In this section, we discuss some unsuccessful attempts during the experiments.\nExtract observation from the whole clinical note. We try to diagnose the disease and extract\nobservation, and the corresponding rationale using the prompt shown in Table 14. The {note} is"}, {"title": "End-to-End prediction.", "content": "We also try to output the whole reasoning process in one step (without\niteration) when given observations. We show our prompt in Table 15. We find that using such a\nprompt model can not correctly recognize the relation between observation, rationale, and diagnosis."}, {"title": "D Ethical Considerations", "content": "Utilizing real-world EHRs, even in a de-identified form, poses inherent risks to patient privacy.\nTherefore, it is essential to implement rigorous data protection and privacy measures to safeguard\nsensitive information, in accordance with regulations such as HIPAA. We strictly adhere to the Data\nUse Agreement of the MIMIC dataset, ensuring that the data is not shared with any third parties. All\nexperiments are implement on a private server. The access to GPT is also a private version.\nAl models are susceptible to replicating and even intensifying the biases inherent in their training\ndata. These biases, if not addressed, can have profound implications, particularly in sensitive domains\nsuch as healthcare. Unconscious biases in healthcare systems can result in significant disparities\nin the quality of care and health outcomes among different demographic groups. Therefore, it is\nimperative to rigorously examine AI models for potential biases and implement robust mechanisms\nfor ongoing monitoring and evaluation. This involves analyzing the model's performance across\nvarious demographic groups, identifying any disparities, and making necessary adjustments to ensure\nequitable treatment for all. Continual vigilance and proactive measures are essential to mitigate\nthe risk of biased decision-making and to uphold the principles of fairness and justice in AI-driven\nhealthcare solutions."}]}