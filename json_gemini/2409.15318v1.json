{"title": "On the Complexity of Neural Computation in Superposition", "authors": ["Micah Adler", "Nir Shavit"], "abstract": "Recent advances in the understanding of neural networks suggest that superposition, the ability of a single neuron to represent multiple features simultaneously, is a key mechanism underlying the computational efficiency of large-scale networks. This paper explores the theoretical foundations of computing in superposition, focusing on explicit, provably correct algorithms and their efficiency.\nWe present the first lower bounds showing that for a broad class of problems, including permutations and pairwise logical operations, a neural network computing in superposition requires at least $\\Omega(m' \\log m')$ parameters and $\\Omega(\\sqrt{m' \\log m'})$ neurons, where $m'$ is the number of output features being computed. This implies that any \"lottery ticket\" sparse sub-network must have at least $\\Omega(m' \\log m')$ parameters no matter what the initial dense network size.\nConversely, we show a nearly tight upper bound: logical operations like pairwise AND can be computed using $O(\\sqrt{m' \\logm'})$ neurons and $O(m' \\log^2 m')$ parameters. There is thus an exponential gap between computing in superposition, the subject of this work, and representing features in superposition, which can require as little as $O(\\log m')$ neurons based on the Johnson-Lindenstrauss Lemma.\nOur hope is that our results open a path for using complexity theoretic techniques in neural network interpretability research.", "sections": [{"title": "Introduction", "content": "A collection of groundbreaking publications [5, 8, 19, 23] by researchers at Anthropic present compelling evidence that features, functions applied to the input to recognize specific properties, are a fundamental computational unit of neural networks. It is also believed that superposition [3, 7], the computational phenomenon allowing a single neuron to fire as part of a polysemantic representation [23, 25] of many different features, is key to how neural networks compute [8]. This is in contrast to monosemantic representations, where each neuron only represents a single feature. There has recently been exciting work on the problem of taking a trained neural network, and extracting the monosemantic features from the superposed representations of the network [5, 8, 23]. This has been primarily driven by safety considerations, since being able to understand the features being computed by a neural network is crucial in order to understand the logic being used.\nIt has been suggested that because the set of features that are active at any given time is very small relative to the total number of features, superposition allows the network to represent the current state of a computation much more efficiently [8]. Superposition can be thought of as a compressed representation of the current state, and this more succinct representation allows the neural network to represent many more features than it has neurons. Furthermore, by doing superposition the right way, the neural network performs computation in that superposed representation. It is conjectured that computing in superposition is important from an efficiency perspective since the work on extracting features seems to indicate that large neural networks use sets of at least hundreds of millions of features and quite likely orders of magnitude more than that [23], and so inference in the non-compressed form would require tremendous computational resources [8, 19]. The superposed representation of the neural network provides compression, much greater computational efficiency and also lends itself well to all of the GPU optimizations that have made the AI revolution possible.\nIn this work, we address the question of whether it is possible to design explicit and provably correct algorithms for computing in superposition, and how efficient can such algorithms be? Our primary measure of efficiency is the level of superposition achievable: given a set of features to be computed, what is the minimum number neurons required to perform that computation in superposition, and how many parameters do those neurons require? Our work here is inspired by the important work of Vaintrob, Mendel, and H\u00e4nni in [24], that suggests the problem and shows a way of computing a k-AND circuit using a single layer of a neural network in partial superposition. We will elaborate on how we build and expand on this work in the related work section.\nThe superposition question is central to understanding how real trained neural networks compute. It also enables us to prove upper and lower bounds on the efficiency of neural networks. In fact, our lower bounds apply independently of representation. Furthermore, addressing this question allows us to separate the process of determining a feature set and a feature circuit (a description of what is being computed) from the process of mapping these to a superposed representation. We believe that during training, neural networks are implicitly doing both processes simultaneously, and that this disentangling (pun intended) of the computation in superposition from the process of finding the features and feature circuit opens up a much deeper understanding of neural networks, specifically what they are computing and how they work.\nFinally, perhaps a bit further in the future, solving this problem could lead to a powerful way of designing neural networks by breaking the design problem up into two subproblems: (1) determine the feature set and feature circuit for a given problem domain, and then (2) map those using canonical techniques to a neural network in superposition. Subproblem (1) could be done either through feature circuit extraction from existing networks, or training an initial neural network to be monosemantic, and then using that as the feature circuit. The whole process can then be viewed as an algorithmically driven form of distillation [14]."}, {"title": "1.1 Our Results", "content": "In this paper, we present nearly matching upper and lower bounds on computing in superposition. Essentially, we demonstrate that when computing in superposition, we can compress the representation of the features down to roughly the square root of the number of output features, but no further. These bounds and the techniques used to derive them open a number of further interesting questions concerning the design of neural networks, mechanistic interpretability, and further understanding the complexity of neural networks. We discuss these questions in the Conclusion.\nWe define our models below, but for now, let m be the number of input features to a monosemantic neural network description, and $m'$ be the number of output features. Let n be the number of neurons being used in the superposed computation."}, {"title": "Lower Bounds", "content": "Our lower bounds apply to a very general model of computation that includes neural networks, and is independent of many implementation details, such as network structure, activation functions, etc. In this model, we introduce a lower bound technique that applies both to neural networks that must always be correct, as well as when the network, as in the real world, is allowed to make mistakes sometimes. This technique relies on what we call the expressibility of a neural network: how many different functions the neural network implementation (structure, activation functions, etc) is able to compute through different settings of the parameter weights of the model. We provide a Kolmogorov Complexity based proof that shows that if a neural network has high expressibility, the parameters of the final model must have a large description length, even if the model is allowed to make mistakes sometimes.\nWe use this technique to show that the minimum description of the parameters of the neural network must be at least\n$\\Omega(m' \\log m')$ bits,\nfor a broad class of problems that includes computing a permutation of the input features and computing $m'$ pairwise ANDs of the m input features. For neural networks using square matrices and a constant number of bits per parameter, this implies that\n$n = \\Omega(\\sqrt{m' \\log m'})$.\nThis has several interesting implications for neural network compression, a topic of great interest given the memory limitations of today's GPUs [15]. The typical representation of the parameters of a neural network is as 8 or 16 bit values, and so asymptotically such neural networks will require $\\Omega(m' \\log m')$ parameters. Moreover, for quantization [15], the process of cutting the number of bits in parameter representations to improve performance, this bound suggests that reducing the number of bits beyond a certain point will require increasing the number of parameters.\nAnother form of compression of neural network representations is sparsity [10, 15]. It has been conjectured that typical neural networks contain a sparse sub-network with sometimes 90% or more of the original parameters removable while preserving accuracy. If this kind of sparsification is possible in most cases, then the sub-network has similar expressibility as the original network, and so our lower bounds imply that parameter sparsity is bound by the number of features the network is computing, and any \"lottery ticket\" sparse sub-network [10] must use at least $\\Omega(m' \\log m')$ bits to describe its parameters no matter the size of the original network it is derived from.\nA final form of compression is knowledge distillation [14], where a large dense neural network is used to train a much smaller dense network preserving the original \"knowledge\" and hence the accuracy. Again, if this retains the expressibility of the original neural network, then there is a limit to how small a network can be distilled from a larger one without loosing accuracy.\nTo the best of our knowledge, these are the first general lower bounds on the number of parameters or the number of neurons required for neural network computation, i.e. without any structural assumptions [11] on the specific network architecture.\nSeveral authors have pointed out that superposition is an application of the Johnson-Lindenstrauss Lemma [16]. It can also be viewed as a type of Bloom filter [4, 6]. In both cases, the previous work has focused on representation: how efficiently can the current state be represented? Our focus is on computation. In fact, it is not hard to show with either of these previous techniques that when there is only a constant number of active features, they can be represented together using $O(\\log m')$ neurons. Since we demonstrate that $\\Omega(\\sqrt{m' \\log m'})$ neurons are necessary for computation, our results imply an exponential gap between representation and computation."}, {"title": "Upper Bounds", "content": "According to [7, 12, 19] the ability of neural networks to compute in superposition is the result of feature sparsity: it is observed that in any layer only a small subset of input features are activated. Under that assumption, we provide an algorithm that takes a description of any $m'$ pairwise ANDs of $m$ inputs, and produces a neural network that correctly computes those ANDs using\n$n = 0 (\\sqrt{m' \\logm'})$\nneurons.\nThis neural network does not make errors, and multiple layers of this network can be chained together. Our lower bound also applies to problems with feature sparsity, and so the upper bound is within a factor of $\\sqrt{\\log m'}$ to asymptotically optimal in terms of neurons. To the best of our knowledge, this is the first provably correct algorithm for computing a non-trivial function wholly in superposition.\nWe also introduce a concept closely related to feature sparsity, hinted at in [7], which we call feature influence: for a layer of the network, how many"}, {"title": "1.2 Related Work", "content": "Our work here is inspired by the groundbreaking work of Vaintrob, Mendel, and H\u00e4nni in [24], that lays out the problem of computing in superposition and shows a way of computing a k-AND circuit using a single layer of a neural network partly in superposition. Their work paves the way for ours, but falls short in several fundamental ways which we aim to improve and expand here.\nFirst and foremost, we say \"partly in superposition,\" because in their problem setup the neurons are in superposition, but neither the inputs nor the outputs are in superposition, which significantly simplifies the problem and avoids some its main challenges. In our work the computation as a whole is in superposition: the inputs, the neurons, and the outputs.\nSecondly, their technique allows them to compute with a single neural network layer but does not extend to multiple layers due to error blowup. The new approach we propose here allows us to remove the error for an arbitrary depth of layers.\nAnother powerful aspect of our work relative to [24] is that we show you can use (and/or think about) much larger matrices in the process of constructing the layers of the algorithm and then multiply these matrices together for the construction, getting matrices and vectors of size dependent only on the number of neurons. The combined techniques allow us to implement arbitrary width AND expressions and arbitrary depth circuits.\nLastly, we introduce here a general framework for reasoning about superposition in neural networks. We separate the circuit being implemented from the neural network implementing it, with the observation that superposition is about the relation between the two. Unlike [24], this allows us to introduce the study of the complexity of a neural network algorithm implementing a circuit with a given level of superposition, capturing this complexity via not only the number of neurons but also the number of parameters in the network. In particular, unlike [24], we present not only upper bounds, but also for the first time lower bounds on the complexity of computing in superposition.\nAnother paper that studies the impact of superposition on neural network computation is [21]. However, they look at a very different question from us: the problem of allocating the capacity afforded by superposition to each feature in order to minimize a loss function, which becomes a constrained optimization problem. They do not address the algorithmic questions we study here.\nOur work is also inspired by various papers from the reserach team at Anthropic [7, 8, 19]. Apart from their work influencing our general modelling approach, their findings also inspired our definitions of feature sparsity and feature influence. We also recognize that there is a body of work [11, 13] on lower bounds for the depth and computational complexity of specific network constructions such as ones with a single hidden layer, or the number of additional neurons needed if one reduces the number of layers of a ReLU neural network [2]. Our work here aims at a complexity interpretation relating the amount of superposition and the number of parameters in the neural network to the underlying features it detects, a recent development in mechanistic interpretability research [5, 7, 8, 19, 23]."}, {"title": "2 Modeling Neural Computation", "content": "Our goal is the following: for a given set F of computational problems and set of possible inputs X to those problems, design an algorithm for converting a description of any $F \\in F$ into a neural network $N(F)$ that computes $F$ in superposition for any input $x \\in X$, possibly incorrectly for some of the $x \\in X$. In the example of this we study in depth here, F is the set of all possible mappings of m Boolean input variables to $m'$ Boolean output variables, where each output variable is the pairwise AND of two input variables. $F \\in F$ is a specific mapping of pairwise ANDs, and X is the set of all settings of the m inputs where only a small number are simultaneously active. Thus, there are two layers of algorithms here: there is the algorithm that converts any $F \\in F$ into $N(F)$, and then $N(F)$ is itself an algorithm for computing the result of F\non any input $x \\in X$.\nWe here use two different models of computation for the neural network $N(F)$. For our lower bounds, we consider a general model for computation, called parameterized algorithms. This model includes any neural network algorithm, and so our lower bounds apply to neural networks but also more broadly. Our upper bounds utilize a specific type of parameterized algorithm, based on a widely used type of neural network. Both of these are described below. We also provide a framework for describing the computational problem F, called a feature circuit. This is also described below."}, {"title": "2.1 Parameterized algorithms", "content": "We use the term parameterized algorithm to be any algorithm that computes on an input based on a set of parameters and produces an output. The function that the algorithm computes can depend on the parameters. In our main lower bounds, we do not require any further assumptions on how computation proceeds. We can think of the parameterized algorithm as a black box that computes different functions $F \\in F$ based on different parameters. As a result, our lower bounds apply to a neural network with any structure, any activation function, or any other aspect of the computation, or even some other structure that has parameters but would not be considered a neural network. Any such structure can be a specific parameterized algorithm, and it will have a set of different functions F that it can compute based on its parameters. We use this model to prove lower bounds on the description length of the parameters required, based on the expressibility of the parameterized algorithm. Essentially, we show that if F is large then the number of parameters must also be large, both in the case where the parameterized algorithm is always correct, and when it can make mistakes on some of the inputs $x \\in X$.\nA neural network could be described as a parameterized algorithm in two different ways: the parameters can describe the structure of the neural network, as well as the weights associated with that structure, or the parameters can represent just the weights, in which case the structure of the network is considered part of the algorithm itself. Once we have a lower bound on parameters, we use this latter approach. Specifically, we assume a network structure that relies on the type of square $n \\times n$ matrices we use in our upper bound model described below. With that structure, any lower bound of B on the number of parameters directly implies a lower bound of $\\sqrt{B}$ on the number of neurons. Note that in this case, the lower bound does not count the description of the network itself, making the lower bounds even more compelling than if they included the description of the network layout and computation.\nIn the past, the notion of parameterized algorithms [1, 9] was used to refine known algorithms by introducing a small set of input dependent parameters, in addition to inputs and outputs, as a complexity measure. This was useful for tackling NP-hard problems by isolating the complexity to a parameter that is small in practical instances so as to provide efficient solutions for otherwise intractable problems. Here, we re-introduce this notion, but with a twist: the"}, {"title": "2.2 The Feature Circuit View of Neural Computation", "content": "We next provide a framework for describing F, the computational problem our parameterized algorithm N(F) is intended to solve. We follow the intuitive definition in [8, 24] of capturing neural network computation as a set of output features that are the result of applying a circuit to a set of input features.\nMore formally, as depicted in Figure 1 below, a feature set is a set of functions $f_1, f_2,..., f_m$ applied to an original input (an image, text etc). A feature circuit is an abstract computation that receives an input vector of m inputs, the results of the feature set computation on the original input, and computes a vector of $m'$ outputs that are features $f'_1, f'_2,..., f'_{m'}$ defined by the input feature set and the feature circuit applied to the inputs. Though the inputs and outputs of this circuit could be in any range, we choose to simplify it as depicted in Figure 1: the $f_i$ and $f'_i$ are Boolean indicators of whether a given feature appears or not in the input (and output, respectively). The feature circuit is thus a Boolean function from a Boolean input vector, indicating which features from the feature set appeared in the input, to a Boolean output vector indicating which output features appeared in the original input. For example, if the input features include detecting 4 legs, a tail, and a collar, then output features for cat and dog might be valid outputs of the feature circuit computation.\nThe circuit can be described using a neural network [20], a Boolean circuit, or any other algorithmic description of a Turing computable function. We note that one can use a single feature circuit to describe the computation of a neural network as a whole, or use a cascade of feature circuits, as depicted in Figure 2, to describe the details of the computation of the layers of such a network, where each layer's input feature set is a function of the previous ones. The final output can be a readout of the final layers output features, the equivalent of a softmax or some more complex function that might require another layer of computation. We do not touch on this exercise as it has little effect on the complexity measures we study here.\nWe also do not attempt here to address the problem of determining the feature sets we simply assume that they are given to us, and we look at the problem of mapping them to an efficient neural network computation. However,"}, {"title": "2.3 Complexity and Superposition", "content": "Our primary measure of complexity is n, the number of neurons used by the neural network N(P) for computation, but we will also examine the number of bits required to represent parameters. We describe the computation to be performed using a feature circuit. We are agnostic as to how the feature circuit F, specifically the features $f_i$, are described. However, we do point out that if extracting the logic behind the feature circuit is resource intensive, this will cause the algorithm that translates F into N(F) to be resource intensive as well. In general though, given our focus on n, we do not provide an analysis here of the computational requirements of translating F into N(F). However, all algorithms we provide in our upper bounds have a running time that is polynomial in m. We also are confident that they are more efficient than running a traditional training algorithm to determine N(F).\nThe inputs to a feature circuit are said to have feature sparsity if only a small fraction of the m input features entries are activated (are non-zero) for every input. This corresponds to observed behaviors of real neural networks [12]. There is one more property of a feature circuit that we wish to examine: feature influence. The feature influence of an input $x_i$ in a feature circuit is the number of output features $f_j$, for which there exists a settings of the other inputs, such that keeping those other inputs fixed at s while changing $x_i$ results in a change to $f_j$. The maximum (average, minimum, respectively) feature influence of a feature circuit is the maximum (average, minimum, respectively) influence of all its input features. As mentioned above, the algorithms of our upper bounds for 2-AND work for all variations of feature influence, but it does have a significant impact on what techniques are deployed in those algorithms. We also point out in Section 5.4 that if the maximum feature influence of a 2-OR problem is too high, then superposition is not helpful.\nWe will say that a layer of a neural network computes in superposition if $n < m'$, that is, there are fewer neurons than output features in the circuit being computed by this layer. The network as a whole will be said to compute"}, {"title": "2.4 The k-AND problem", "content": "Our upper bound work will focus on a specific algorithm that converts a 2-AND feature circuit into a superposed neural network computation. In a 2-AND feature circuit, the input is m Boolean variables, and the output is $m'$ different conjunctions of any two of these m inputs $(m' \\leq \\binom{m}{2})$ returning a 1 in any output entry corresponding to two non-zero inputs, and zero otherwise. Our interest will be in computing this function using a superposed network that has significantly fewer than $m'$ neurons. Later in the paper, we will explain how to compute k-AND from a collection of 2-AND computations, where a k-AND feature circuit produces $m'$ different conjunctions of k inputs."}, {"title": "3 Lower Bounds", "content": "We here present our lower bounds for parameterized algorithm. We start by assuming that the parameterized algorithm does not make any errors but will add errors to the mix later below. For the error-free case, we are perhaps slightly more formal than might be necessary; we do so in order to set up a framework that makes it much easier to demonstrate the lower bound for when the parameterized algorithm can make mistakes.\nLet U and V be finite sets, and let F be a set of distinct functions that map $U \\rightarrow V$. Let T be a parameterized algorithm that can compute any function $F \\in F$ where the decision which function is computed by T only depends on the setting of the parameters of T. Let P(F) be any description function describing the parameters used by T when T is computing F, and let |P(F)| be the length of the description P(F) measured in bits.\nTheorem 3.1. For almost all $F \\in F$, $|P(F)| \\geq \\log_2 |F|$.\nIt is important to stress that this is not a bound on an instantiation of an algorithm that has been designed to compute a specific function $F \\in F$, for example, a neural network with an architecture for computing a single 2-AND function by keeping only the weights that connect the inputs to neurons fitting"}, {"title": "3.1 Parameterized algorithms with errors", "content": "The lower bound we just showed does not really capture what is going on in neural networks since neural networks are typically allowed to make mistakes on some inputs. There are two different ways that an algorithm can make mistakes: either there are random choices during the execution of the algorithm and a mistake is dependent on those choices, or there is some subset of the inputs where the algorithm will always make a mistake. The former type of mistake is actually relatively easy to incorporate into the above protocol. Bob would simply try every input enough times to have very high confidence in the resulting mapping (assuming that the parameterized algorithm returns the correct answer with probability greater than 0.5). By doing so, he is very likely to find the correct answer, and we again reach a contradiction (although some care needs to be taken to ensure it is in fact a contradiction when the correct message is not transmitted 100% of the time).\nThe harder case for errors is the one that represents what happens with neural networks in practice: the algorithm T parameterized by P(F) correctly computes F(x) for some inputs x but is allowed to make mistakes on other inputs. The lower bound we show does not depend on how the mistaken inputs are chosen they can be randomly or arbitrarily chosen, but we here argue the case where they are arbitrarily chosen. More specifically, given a set of functions F, a set X of allowed inputs to F, and any $\\epsilon < 0.5$, the goal is to define a parameterized algorithm T such that for any $F \\in F$, T parameterized by P(F) must determine F(x) correctly for at least a fraction $(1 \u2013 \\epsilon)$ of the inputs $x \\in X$. We say that any T with this property on a set of functions F and inputs X computes $F \\epsilon$-correctly.\nIn this case, we are not able to demonstrate as general a result as the error-free case since the ability to make errors could allow the algorithm to use a smaller parameter description. Consider, for example, the case where all of the differentiation between the functions in F is on the same input (i.e., all other inputs always map to the same result). In this case, T can have a single error on that one input for all $F \\in F$, which means that there is effectively only one function that T needs to compute, which in turn means that no parameters at all are needed. Instead, we will prove a lower bound based on a subset of functions in F that can always be distinguished from each other. Specifically,"}, {"title": "3.2 Other potential applications", "content": "The last topic we address here with respect to lower bounds is showing lower bounds on the parameterization of the problems we use neural networks for in practice, such as LLMs and image generation. In these cases, it might seem like there is only one (or a small number) of functions being computed, such as a specific LLM, and so this lower bound does not apply. In theory, this is probably true: if there exists an algorithm that computes that LLM, there exists a parameter-free algorithm for it. In practice, however, that is not how LLMs are constructed. Instead, all existing neural networks use a general network structure that could be trained to compute very different functions through their parameterization. They start with a structure that has a high degree of expressibility that is then made more specific through a training process that sets their parameters. Our lower bound applies to the expressability of the neural network prior to training.\nAs a result, our lower bound technique could potentially be used to prove lower bounds on the parameterization of LLMs. While we do not prove any such lower bounds, we demonstrate here that it has the potential to do so. The idea is that if we take a fixed neural network structure, and train it on two very different data sets, this should lead to it computing very different things. (The details of the training process itself can cause differences as well, but we ignore that here). For example, for a neural network structure being used as an LLM, we will get very different results if we train it with a data set consisting entirely of English language text versus only using Mandarin Chinese. We formalize this concept here using our lower bound framework:\nCorollary 3.2.3. Let Y be any neural network that is capable of computing every function $F \\in F' \\epsilon$-correctly with some setting of its parameters for any $\\epsilon < 0.5$ and any set of functions F' that is 2$\\epsilon$-robust. Almost all functions $F \\in F'$ require at least $\\log |F'|$ bits to describe their parameter settings in \u03a5."}, {"title": "4 Upper Bounds", "content": "In this section, we provide an algorithm that converts any description of a 2-AND feature circuit into a neural network that computes that feature circuit in superposition, with both the input and output in superposition as well. Given any 2-AND circuit with m inputs and $m'$ outputs, this neural network requires $n = O(\\sqrt{m'\\log m'})$ neurons. Even though our model allows the network to make errors on some of the inputs, our resulting network does not take advantage of this, and will always be correct. We here assume that the input consists of at most 2 inputs being active in any input; in Section 5 we describe how to extend this to a larger number of inputs, albeit with an exponential dependency on the number of active inputs. The bound on neurons achieved by this algorithm nearly matches the lower bound of the previous section: since $\\Omega(m' \\log m')$ parameters are needed for 2-AND by any algorithm that uses a constant number of bits per parameter, any $n \\times n$ matrix multiplication based algorithm requires $n = \\Omega(\\sqrt{m' \\log m'})$. Our algorithm uses a constant number of $n \\times n$ matrices,"}, {"title": "4.1 Superposed Inputs", "content": "In the following, the computation proceeds in superposition throughout. However, it is helpful to think about the design of the algorithm in terms of going back and forth between representing the input and subsequent computations using the n-vector superposed representation, and the m-vector monosemantic representation (we describe below how to do this in a way such that the final computation is entirely in superposition). As a warm-up, we start by describing how to move back and forth between these representations. Let y be an m-vector representation and x be the n-vector representation of the same state, where $n = O(\\sqrt{m}\\log m)$. To go from y to x we can multiply y on the left by C, an $n \\times m$ binary matrix where all entries are chosen i.i.d. with a probability $p = O(\\log m/n)$ of each entry being a 1. To compress y into x, we use $x = Cy$. We will refer to a matrix like C as a compression matrix but will use different forms of compression matrices below.\nWe see that if y is sparse then Cy is fairly sparse as well and we can recover y from Cy. In order to do so, we use a decompression matrix D where D depends on C as follows: let $C^T$ be the transpose of C. D is $C^T$ with every entry equal to 1 in $C^T$ (which is binary) replaced by 1 divided by the number of 1s in that entry's respective row of $C^T$. Consider the matrix R = DC and consider the entry $R_{ij}$ of R. If i = j then $R_{ij} = 1$ since the non-zero entries will line up exactly and we are normalizing. And if $i \\neq j$ then $R_{ij}$ will be very close to 0 since the non-zero entries in row i of D will (with high probability) have very little overlap with the non-zero entries in column j of C. Thus D is an approximate left inverse of C and can be used to recover the original y through $y \\approx DCy$. The approximation here is due to some of the values intended to be zero not being exactly zero.\nWe can also do the compression/decompression pairing in the reverse order: $x \\approx CDx$. And we can insert other matrices in between C and D. For example, we could insert a permutation matrix P to get $x' \\approx CPDx$, which would allow us to approximately compute the permutation P of the compressed representation of x to the compressed representation $x'$ of the permutation of x. Note that while P is an $m \\times m$ matrix, CPD is an $n \\times n$ matrix, and thus maintains superposition. The challenge here is that Dx contains some noise in it due to the small but non-zero values that should have been zero: in expectation, Dx will have $O(m\\log^2 m/n)$ non-zero values. As a result, $x'$ also has some noise in it, and in fact the multiplication by C causes the noise to add up from the different entries of Dx, and become distributed to many entries of CDx. The expected noise in each entry of CDx is $O(m\\log^2m/n^2)$. If that noise is small (say less than 1/4), then we can remove it through a ReLU operation, as described below. This leads us to a value of $n = O(\\sqrt{m}\\log m)$, which is sufficient to compute the permutation function in superposition.\nWe know from our lower bounds that we cannot compute the permutation with an $n \\times n$ matrix for $n = \\omega(\\sqrt{m} \\log m)$; it is exactly this noise that keeps us"}, {"title": "4.2 Algorithm for maximum feature influence 1", "content": "We start off by providing an algorithm for a special case of the problem. Let the maximum feature influence of any input be denoted by t. We here consider the case where t = 1", "C_0$": "once for each input that appears in the ith AND that is computed.\nAfter multiplying by the compression matrix, we add b to the result where b is a column vector consisting of n entries, all of which are -1. And then we take the pointwise ReLU of the result giving us $x_1 = ReLU[C_0D_0x_0 + b"}]}