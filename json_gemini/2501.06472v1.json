{"title": "YO-CSA-T: A Real-time Badminton Tracking System Utilizing YOLO Based on Contextual and Spatial Attention", "authors": ["Yuan Lai", "Zhiwei Shi", "Chengxi Zhu"], "abstract": "The 3D trajectory of a shuttlecock required for a badminton rally robot for human-robot competition demands real-time performance with high accuracy. However, the fast flight speed of the shuttlecock, along with various visual effects, and its tendency to blend with environmental elements, such as court lines and lighting, present challenges for rapid and accurate 2D detection. In this paper, we first propose the YO-CSA detection network, which optimizes and reconfigures the YOLOv8s model's backbone, neck, and head by incorporating contextual and spatial attention mechanisms to enhance model's ability in extracting and integrating both global and local features. Next, we integrate three major sub-tasks-detection, prediction, and compensation-into a real-time 3D shuttlecock trajectory detection system. Specifically, our system maps the 2D coordinate sequence extracted by YO-CSA into 3D space using stereo vision, then predicts the future 3D coordinates based on historical information, and re-projects them onto the left and right views to update the position constraints for 2D detection. Additionally, our system includes a compensation module to fill in missing intermediate frames, ensuring a more complete trajectory. We conduct extensive experiments on our own dataset to evaluate both YO-CSA's performance and system effectiveness. Experimental results show that YO-CSA achieves a high accuracy of 90.43% mAP@0.75, surpassing both YOLOv8s and YOLO11s. Our system performs excellently, maintaining a speed of over 130 fps across 12 test sequences.", "sections": [{"title": "I. INTRODUCTION", "content": "Over the past decade, deep learning has advanced rapidly and has found widespread applications across numerous fields, sparking multiple high-profile human-machine competitions.\nHowever, in the field of badminton match, existing research still faces many challenges before achieving realistic human-machine competitions, including real-time extraction of shuttlecock's 3D trajectory, prediction of its future landing point, formulation, and timely optimization of robotic arm movements and striking strategies. Among these, real-time extraction of shuttlecock's 3D trajectory, as the first step in human-machine competitions, directly impacts the effectiveness of subsequent strategies due to its speed and accuracy.\nAmong various small ball sports, unlike spherical structures such as tennis and table tennis, shuttlecock has a more complex conical structure composed of a cork base and feathers. During high-speed flight, the shuttlecock's shape in images varies significantly depending on the viewing angle. Additionally, during rallies, its high-speed motion can cause the shuttlecock to blend seamlessly with background elements, such as court lines, or lighting. This makes even the 2D detection of the shuttlecock quite challenging.\nAccurately and in real time capturing shuttlecock's trajectory in 3D space not only provides valuable data for predicting its landing point and formulating a striking strategy for robots, but also enables match data analysis. This can facilitate the creation of 3D-based automated match analysis systems, helping coaches precisely assess players' capabilities and develop more focused and efficient training plans for athletes.\nAgainst this backdrop, we aim to propose a 3D shuttlecock tracking system that integrates real-time performance with high accuracy. Considering the shuttlecock's small size and susceptibility to false detection, we propose a 3D shuttlecock tracking system based on YO-CSA detection network. We build a simplified stereo vision system to map 2D trajectories to 3D space. Additionally, our tracking algorithm integrates three key modules, shuttlecock detection, prediction, and compensation, to achieve robust and reliable tracking.\nTo validate the effectiveness of our system, we compare our detection module against mainstream networks. The results show that YO-CSA detection network significantly outperforms these baseline models, particularly in the mAP@0.75 metric. Additionally, we conduct comparative experiments on our tracking strategy which demonstrate the system's effectiveness."}, {"title": "II. RELATED WORK", "content": ""}, {"title": "A. Object Detection", "content": "Object detection has extensive applications, including the identification of nearby pedestrians and vehicles in autonomous driving, and intelligent target tracking in surveillance systems. Similarly, object detection technology is crucial for badminton rally robots. By providing the coordinates of the shuttlecock, object detection methods enable the robot to predict the future landing location of the shuttlecock, thereby optimizing its movement and return strategies.\nBeginning with R-CNN [1], which achieving a mean average precision (mAP) of 53.3% on VOC12 [2], object detection has rapidly advanced over the past decade. In 2016, the introduction of YOLO [3] marks a significant breakthrough, establishing the single-stage model paradigm. This approach detects objects directly on the entire image, and can simultaneously predict object locations and class probabilities in a single inference. More recently, popular backbone networks like CenterNet [4] and EfficientNet [5], along with Transformer-based models such as DETR [7] and Swin Transformer [8], gain widespread use. The latest YOLO"}, {"title": "B. Self-Attention Mechanism", "content": "The self-attention mechanism, first introduced in [6], differs from traditional convolution operations, which process the entire image uniformly. Self-attention allows the network to dynamically adjust its focus on specific regions, enabling it to prioritize target objects while reducing attention to less important areas, such as the back-ground. It also facilitates the capture of long-range dependencies, overcoming the locality limitations of the convolutional receptive field.\nOriginally developed for natural language processing tasks, the self-attention mechanism has shown promising results in the field of computer vision as well. ViT [9] is the first to apply Transformer architecture to image recognition, achieving an accuracy of 88.55% on ImageNet [10], even surpassing ResNet [11]. [12] [13] demonstrate the practical use of Transformers in medical imaging, where they effectively leverage self-attention to capture global features. Additionally, [14], based on ViT and fully connected conditional random fields, optimize the modeling of point cloud data. In object detection, DETR [7] entirely eliminates the region proposal, region extraction, and non-maximum suppression steps, achieving end-to-end object detection using a pure Transformer architecture. Swin Transformer [8] further addresses the computational inefficiencies of ViT by introducing local windows for self-attention.\nWhile self-attention mechanisms can greatly improve network performance, pure Transformer-based models typically require large datasets for training, and their high complexity can limit inference speed-factors that do not align with our objectives. Consequently, we focus on combining the advantages of both convolution and self-attention mechanisms to enable real-time detection of the shuttlecock."}, {"title": "C. Tracking in Small-Size Ball Sports", "content": "Small ball sports, such as badminton and table tennis, are globally popular and have attracted significant research interest. Whether as essential data for human-machine competitions or as part of a match video analysis system, extracting the ball's trajectory is crucial.\nCenterNet detects targets by learning a center heatmap. Similarly, a series of works adapt the same fundamental approach by using heatmaps for precise localization. TTNet [15] is a multifunctional neural network designed for high-resolution video, integrating table tennis ball detection, event classification, and semantic segmentation. It takes consecutive video frames as input, using a pure convolution network and heatmap learning, performs coarse-to-fine detection and segmentation of the ball. TrackNetV2 [16], inspired by the UNet [18], introduces heatmaps and uses a Gaussian 2D distribution to determine the ball's location. WASB-SBDT [17] builds a neural network to predict the heatmap of ball coordinates, inspired by HRNet [19]. It proposes High-Resolution Modules (HRMs) to address the semantic and spatial resolution loss typically seen in traditional encoder-decoder architectures.\nOn the other hand, some works focus on optimizing existing detection frameworks. For example, [20] enhances model training using gradient estimation theory, improving tennis ball recognition accuracy in videos. [21] combines YOLOv3 [22] with Kalman filter [23] to achieve a 2D golf ball tracking system. [24] utilizes Transformer-based secondary feature processing to build global information, proposing a target detection algorithm for identifying the table tennis ball and determining its position on the table.\nMost current research focuses on 2D video processing, with few addressing trajectory extraction in 3D space. This is likely due to the high cost of building devices capable of supporting 3D trajectory extraction. Furthermore, video analysis tasks, unlike robot game-play, do not require 3D data.The field of robotics is still developing, and current research is relatively limited. Therefore, this paper focuses on the development of a real-time, accurate 3D badminton tracking system, aiming to make a meaningful contribution to the field of robotics."}, {"title": "III. REAL-TIME DETECTION MODULE", "content": "In this section, we introduce a novel detection network integrated contextual and spatial attention mechanisms as well as convolution to balance the trade-off between speed and accuracy in high-speed, small-object detection for badminton."}, {"title": "A. Brief Review of YOLO", "content": "Since the introduction of YOLO [3] in 2016, the YOLO series has undergone annual iterations, consistently leading the field of object detection with its superior performance. As an end-to-end architecture, YOLO stands in contrast to models based on region proposals such as Faster-RCNN [25], as it unifies a single network to perform detection tasks, streamlining the process.\nYOLO architecture consists of the backbone, neck, and head, which are responsible for representative feature extraction, feature enhancement, and task-specific operations such as classification and regression, respectively. YOLOv5 achieves 50.5% mAP on the COCO, and was soon surpassed by YOLOv8, which attains 53.9% mAP on the same dataset."}, {"title": "B. Overview of Detection Network", "content": "Although YOLOv8 has achieved certain results in practical applications, there is still room for improvement in scenarios like badminton matches, where require both high real-time performance and precision.\nWe propose YO-CSA architecture, depicted in Fig.1. Inspired by contextual transformer block (CoT) [26] and spatial group-wise enhance (SGE) [27], we introduce contextual transformer block with 2 convolutions (CoT2f) and spatial attention-integrated neck (SANeck), which strengthen the network's ability to extract and enhance features, particularly in terms of positional distribution, within the backbone and neck processes. Furthermore, we optimize the detection head to facilitate more efficient learning of spatial distributions. Further details on the YO-CSA architecture is provided in the following sections."}, {"title": "C. Contextual Transformer Block with 2 Convolutions", "content": "It is crucial to design a deep neural network with robust representative extraction capability while mitigating computational overhead and minimizing information loss. CoT2f is implemented in the backbone of YOLO aiming to enhancing the property to extract global context as well as alleviate representative information degradation. The foundational launching point for our approach is to fully exploit the contextual self-attention mechanism and Bottleneck [11] recognized as an efficient architectural paradigm to boost network's learning while reducing computational demands.\nAs Fig.2 illustrates, CoT2f comprises two convolution layers of different sizes and a CoT-Bottleneck. Initially, input X with the size of $W \\times H \\times C_1$ is fed into convolution layer 1, resulting in an output with the shape of $W \\times H \\times 2c$, here 2c denotes the number of channels in the convolution layer 1. Subsequently, an inter-mediate product $[Y_1, Y_2]$ is obtained through a chunking operation.\nCoT-Bottleneck is capable of conducting finer-grained feature extraction on $Y_1$ to yield $Y_{CoT-Bottleneck}$, whereas $Y_2$ is directly injected into the concatenation layer. Analogous to a residual structure, $Y_{CoT-Bottleneck}$ and $Y_2$ are concatenated, followed by processing through convolution layer 2 to facilitate the fusion of features. CoT [26], incorporated within CoT-BottleNeck, integrates contextual information mining"}, {"title": "D. Spatial Attention-Integrated Neck", "content": "Within the context of neural networks, the target object is composed of a sequence of sub-features, which implies that accurately identifying these sub-features enables the precise localization of the target object. Therefore, we emphasize the importance of sub-features perception and extraction in the neck of YOLO, specifically within the feature fusion process of the dual pyramid structure. To be more precise, a new dual pyramid structure is reconstructed based on SANeck, with the aim of enhancing network's semantic extraction capability through self-attention mechanism.\nFor the purpose of minimizing computational overhead, SANeck appropriately references the lightweight structure SGE [27], which draws inspiration from CapsNet [28]. Inspired by the C2f structure in YOLOv8, we reconstruct neck by C2f-SGE to explicitly introduce self-attention in the neck. Fig.3 illustrates detail of SANeck. Firstly, the output from the previous layer is fed to convolution layer 1. Then, the convolved output is chunked into two slices, one slice, $Y_1$, is compressed by n SGE-2f modules, while the other slice, $Y_2$, is directly transported to the concatenation operation."}, {"title": "E. Decouple head with SGE", "content": "According to the paradigm of object detection, the backbone extracts representative features, while the downstream layers handle tasks, such as classification and bounding box regression. Thus, greater attention should be given to spatial distribution in these downstream layers.\nContrasting with the traditional detection head, the decoupled head no longer shares the parameters between classification and regression. Instead, it utilities two parallel branches to decouple the two tasks, allowing the network to learn the spatial distribution for each task independently.\nAs mentioned, SGE [27] highlights the similarity between local and global features, helping the network learn spatial distributions more effectively. Therefore, the decoupled head integrated with SGE outperforms the traditional version.\nAs shown in Fig.5, after processed by SGE, the single branch is split into two parallel branches, each containing two 3x3 convolutions, which perform classification and regression, respectively. The output of classification and regression are $[H, W, nc]$ (where $nc$ denotes the number of classes) and $[H, W, 64]$ (with 16 channels for the DFL module and 4 essential parameters for the bounding box), respectively."}, {"title": "IV. YO-CSA-T SYSTEM DESIGN", "content": "Our total system can be decomposed into two main components: hardware infrastructure of stereo vision and software design. The software design integrates 2D object detection, advanced 3D tracking methodologies and the compensation module."}, {"title": "A. Hardware Infrastructure of Stereo Vision", "content": "To obtain 3D coordinates, cameras with 3D reconstruction capabilities are required. However, stereo cameras available on the market, like ZED, often struggle to achieve frame rates above 90 fps, which compromises the reliability of trajectory extraction for high-speed shuttlecocks. This is particularly problematic when capturing the distinct trajectory of a serve, which involves an initial descent followed by a parabolic ascent and then another descent.\nTo address this issue, we employ two monocular cameras (model A7200CU130, manufactured by Huairui Technology) to construct a stereo vision system. This setup enables the creation of a high-precision binocular system capable of covering the entire badminton court while maintaining accuracy. Fig.6 illustrates the placement of the cameras and their supporting structures during implementation. Specifically, the stereo cameras are positioned at the rear of the robot's court to clearly capture the trajectory of incoming shuttlecocks. The baseline between cameras is set at 0.8 m, and the cameras are mounted at a height of approximately 1.8 m to simulate the perspective of an adult male."}, {"title": "B. Detection Module", "content": "The detection network YO-CSA, core of detection module, explicitly incorporates con-textual and spatial attention mechanisms and successfully achieves performance that surpasses YOLOv8 and YOLO11. The specific structure of our detection network is presented in detail in Section III."}, {"title": "C. Comprehensive Tracking Workflow", "content": "Our pipeline not only detects the shuttlecock in the current frame, but also predicts its future trajectory based on historical information, thereby incorporating reasonable constraints to enhances the plausibility of the detection range. Throughout the process, we implement constraints to ensure trajectory consistency, such as region of interest (ROI) and threshold settings $\\epsilon_1$, $\\epsilon_2$, $\\epsilon_3$ evaluating the plausibility of coordinates.\nFig.7 displays our entire tracking pipeline based on 2 image sequences, denoted as $I^L = [I_1^L,...,I_m^L]$, $I^R = [I_1^R,...,I_m^R]$, captured by the left and right monocular cameras respectively, where each image in the sequences corresponds to a specific timestamp, ensuring temporal alignment between the left and right camera frames.\nIn accordance with the rules of the game, the shuttlecock's initial position is typically located at the center of the frame. So we center a 640 \u00d7 640 ROI area on $I_L^{(i)}$ and $I_R^{(i)}$, which not only focuses the detection process on a smaller region but also alleviates the computational burden on the detection network."}, {"title": "D. Compensation Module", "content": "Inspired by TrackNetv3 [29], we add a compensation module as an auxiliary branch. We use the 2D trajectory $P_L, P_R$ obtained from the detection module and a corresponding trajectory mask as input, employing a compensation network for interpolation. This network compensates for missing frames that were either missed during detection or discarded due to violations of spatiotemporal constraints. The compensation network is a U-shaped network [18] based on 1D convolution operations, leveraging an encoder-decoder architecture to extract and integrate both shallow spatial information and deep semantic features from the 2D trajectory.\nWe reintroduce the compensated trajectory $P'_L, P'_R$ into the stereo vision module to obtain the complete 3D trajectory $P'_w$. Based on spline interpolation, we generated another complete trajectory $P_w$ from the trajectory $P_w$. Subsequently, $P'_w$, the compensated trajectory $P$ and mask $M$ are integrated through function $\\Theta$ to obtain fully completed trajectory $P'_w$ as (1).\n$P'_w = \\Theta (P_w, P, M) = \\begin{cases} P_w^{(i)}, & \\text{if } M_i = 1 \\\\ \\alpha P^{(i)} + (1-\\alpha) P_w^{(i)}, & \\text{if } M_i = 0 \\text{ and } ||P^{(i)} - P_w^{(i)}|| \\le \\epsilon_3  \\end{cases}$"}, {"title": "V. EXPERIMENTS AND RESULTS", "content": ""}, {"title": "A. Dataset", "content": "Based on the custom-built 3D vision system, we collect badminton rally data from 10 distinct venues or environments, encompassing various angles and natural lighting conditions, totaling 32,539 images. Random 640 \u00d7 640 pixel regions were sampled from the original images, with the condition that the shuttlecock must be present within the cropped area. Specifically, the shuttlecock is not necessarily centered within the cropping region but is randomly distributed across the 640 \u00d7 640 area. Fig. 8a illustrates a subset of our dataset. During actual training, to enhance the generalization and robustness of the network, we also applies data augmentation"}, {"title": "B. Detection Experiments", "content": "Aside from enhancing the data augmentation techniques, we retain the original parameters of YOLOv8 and train the model for 300 epochs on our custom dataset."}, {"title": "C. Tracking Experiments", "content": "In order to assess the effectiveness of our approach, we collect 12 video clips of serve and hit actions, each captured at 160fps, using our custom-built vision system. Although the YO-CSA design already incorporates a lightweight structure, we aim to further optimize the detection speed to meet the demands of high-level competitions. Therefore, we accelerate the model using ONNX. Table III presents the detection performance of the accelerated YO-CSA. Despite maintaining a detection accuracy of 73.94% mAP, we are able to improve the detection speed to 5.82ms per frame, achieving a speedup of 12.74%.\nIn the whole tracking process, in addition to defining ROI to alleviate the detection module, we further impose constraints in the 3D visual space on the 2D detections.\nWe compare the performance of 4 strategies in Table IV: (a) performing object detection directly on the 2D image sequences, obtaining paired left and right view coordinate sequences, and then conducting 3D vision matching; (b) restricting fixed 640x640 ROI regions on the left and right views; (c) based on (b), utilizing historical trajectory information to predict the next 3D position in 3D space and projecting it onto the left and right views to update the ROI; (d) based on (c), adopting compensation module to optimize the trajectory.\nSince obtaining the ground truth of 3D trajectories is highly costly, we evaluate 4 strategies by computing the smoothness of the trajectory based on velocity and acceleration, namely velocity smoothness ($S_v$) ans acceleration smoothness ($S_a$), as well as the average centroid shift of the trajectory ($C_{avg}$). For the 3D trajectory $P_w (x_w^{(i)}, y_w^{(i)}, z_w^{(i)})$: $\\begin{aligned} & v_i = \\frac{p^{(i+1)} - p^{(i)}}{t_{i+1} - t_i}  \\\\ & a_i = \\frac{v_{i+1} - v_{i}}{t_{i+1} - t_i}  \\\\ & S_v = \\frac{1}{n-1} \\sum_{i=2}^{n-1} \\Delta v_i^2 , \\Delta v_i = ||v_i - v_{i-1}||  \\\\ & S_a = \\frac{1}{n-2} \\sum_{i=2}^{n-1} \\Delta a_i^2 , \\Delta a_i = || \\frac{v_i - v_{i-1}}{(t_{i+1} - t_{i-1})/2} ||  \\\\ & C_{avg} = \\frac{1}{n-1} \\sum_{i=1}^{n-1} \\Delta C_i,  \\\\ & \\Delta C = \\sqrt{(x_w^{(i+1)} - x_w^{(i)})^2 + (y_w^{(i+1)} - y_w^{(i)})^2 + (z_w^{(i+1)} - z_w^{(i)})^2}  \\\\ \\end{aligned}$ We assume that smaller smoothness and centroid shifts indicate trajectories that are closer to reality. As displayed in Table IV, Strategy D performs the best. Fig.10 illustrates the results of four strategies."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "We construct YO-CSA, a network built upon YOLOv8s, leveraging spatial and contextual attention mechanisms to achieve a substantial improvement in detection performance. Additionally, we introduce a multi-dimensional spatiotemporal constraint strategy and design a real-time system for the precise extraction of the shuttlecock's 3D trajectory based on YO-CSA. Experimental results demonstrate that our system can extract the shuttlecock's trajectory with high accuracy and in real-time. Building upon the current foundation, we intend to further investigate the practical implementation of human-machine competitions in future work."}]}