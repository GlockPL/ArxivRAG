{"title": "Bandit Multiclass List Classification", "authors": ["Liad Erez", "Tomer Koren"], "abstract": "We study the problem of multiclass list classification with (semi-)bandit feedback, where input examples are mapped into subsets of size m of a collection of K possible labels, and the feedback consists of the predicted labels which lie in the set of true labels of the given example. Our main result is for the (\u03b5, \u03b4)-PAC variant of the problem for which we design an algorithm that returns an \u025b-optimal hypothesis with high probability using a sample complexity of O((poly(K/m) + sm/\u03b5\u00b2)log(|H|/\u03b4)) where H is the underlying (finite) hypothesis class and s is an upper bound on the number of true labels for a given example. This bound improves upon known bounds for combinatorial semi-bandits whenever s\u226a K. Moreover, in the regime where s = O(1) the leading terms in our bound match the corresponding full-information rates, implying that bandit feedback essentially comes at no cost. Our PAC learning algorithm is also computationally efficient given access to an ERM oracle for H. Additionally, we consider the regret minimization setting where data can be generated adversarially, and establish a regret bound of \u00d5(|H|+\u221asmTlog|H|). Our results generalize and extend those of Erez et al. (2024b,a) who consider the simpler single-label setting corresponding to s = m = 1, and in fact hold for the more general contextual combinatorial semi-bandit problem with s-sparse rewards.", "sections": [{"title": "1 Introduction", "content": "Multiclass list classification is a learning problem where the goal is to map input examples to lists (i.e., subsets) of a collection of K possible labels, and is a natural generalization of the fundamental problem of multiclass classification where traditionally only a single label is predicted for each example. Existing work on multiclass list classification focuses on the full-information single-label setting, where a given example has a single correct label which is revealed to the learner after each prediction. The primary focus on this literature is on characterizing properties of the underlying hypothesis class H under which various notions of learnability can be achieved; e.g., PAC learnability (Charikar and Pabbaraju, 2023), uniform convergence (Hanneke et al., 2024) and online learnability (Moran et al., 2023).\nIn this work, we introduce the problem of bandit multiclass list classification, where each example may have up to s < K correct labels,\u00b9 and upon predicting a list of labels of size m \u2264 K, the learner observes partial, a.k.a. \u201cbandit\u201d\u00b2 feedback consisting only of the predicted labels which belong to the collection of s correct labels for the given example. As a natural application, consider a recommendation system visited sequentially by users, where upon each visit the system is tasked"}, {"title": "1.1 Summary of Contributions", "content": "We summarize our results for bandit multiclass list classification over a finite hypothesis class H:\n\u2022 Our main result involves the PAC setting (see Section 3), where we design an algorithm that with probability at least 1 d outputs a hypothesis hout \u2208 H that is \u025b-optimal with respect to the population reward, using a sample complexity of at most\nO((poly(K/m) + sm/\u03b5\u00b2)log(|H|/\u03b4)).\nMoreover, our algorithm is computationally efficient given an ERM oracle for H. We also present the high-level construction of a sample complexity lower bound of \u03a9(sm/\u03b5\u00b2) which holds even for a single context, showing that the dominant term in our bound is in fact optimal.\n\u2022 We also consider the online regret minimization setting, where we design an algorithm achieving an expected regret bound of\n\u00d5(|H|+ \u221asmTlog |H|),\nwhich holds even when the input and reward sequence is adversarial. Our algorithm is an instantiation of Follow-the-Regularized-Leader (FTRL) and uses a mixture between negative entropy and log-barrier regularizations, with importance-weighted loss estimators specialized for semi-bandit feedback. See Section 4 for more details.\n\u2022 We further show that the above results hold in a broad setting of contextual combinatorial semi-bandits (CCSB) with s-sparse rewards over a finite policy class.\nOur starting point for addressing the PAC setting in CCSB is the recent work of Erez et al. (2024a) who study the single-label (s = m = 1) classification setting. Following the general scheme they proposed, our algorithm operates in two stages by first computing a low-variance exploration distribution over policies in II and then using this distribution to uniformly estimate the policies'\nexpected rewards. Generalizing the single-label classification setting to CCSB, however, comes with some nontrivial technical challenges. First, Erez et al. (2024a) use the single-label assumption in order to collect a labeled dataset by uniformly sampling labels. In the general setting, however, this approach cannot work effectively, since the full reward function cannot be inferred unless the predicted set contains all s correct labels, which is a very unlikely event. We circumvent the issue by introducing additional importance sampling estimation with an appropriate modification of the convex potential in use, at the price of a slightly worse dependence on K in the lower order term due to additional noise. As a result of this added complexity, the stochastic optimization problem solved for obtaining the desired low-variance exploration distribution crucially requires a modification to allow for general (bounded) rewards, unlike the one-hot rewards in the single-label case."}, {"title": "1.2 Additional Related Work", "content": "List multiclass Classification. The theoretical framework of multiclass list classification was originally introduced by Brukhim et al. (2022). Charikar and Pabbaraju (2023) provide a characterization of PAC learnability for multiclass list classification by generalizing the DS-dimension (Daniely and Shalev-Shwartz, 2014), Moran et al. (2023) consider the regret minimization setting"}, {"title": "2 Problem Setup", "content": ""}, {"title": "2.1 Bandit Multiclass List Classification", "content": "We study a learning scenario where a learner has to map objects (examples) from a domain X to subsets (lists) of size m of a collection of K > m possible labels, denoted by y \u2252 [K]. We denote by L the set of all subsets of Y of size m, corresponding to all possible predictions. In the semi-bandit setting, the learner iteratively interacts with an environment according to the following, for t = 1,2,... :\n(i) The environment generates a pair (xt, Yt) where xt \u2208 X and Y \u2286 Y, the learner receives xt;\n(ii) The learner predicts a list Lt \u2208 L;\n(iii) The learner observes semi-bandit feedback consisting of Lt \u2229 Yt, namely the set of predicted labels belonging to the true label set.\nWe assume that there is a known bound s < K on the cardinality of all true label sets Yt. In the settings we consider, the learner's performance is measured with respect to an underlying hypothesis class H \u2286 {X \u2192 L}; focusing in this work on the case where H is finite.\nAgnostic PAC setting. In the (\u03b5, \u03b4)-agnostic PAC setting, the pairs (xt, Yt) are generated in an i.i.d manner by some unknown distribution D. The learner's goal is to compute, using as few samples as possible, a hypothesis h\u2208 H such that with probability at least 1-d (over all randomness during the interaction with the environment),\nrp(h*) \u2013 rp(h) \u2264 \u03b5,\nwhere $r_D(h) = E_{(x,Y)\\sim D}[r(h(x); Y)] = E_{(x,Y)\\sim D} [\\frac{|h(x) \\cap Y|}{m}]$ is the population reward of h, and\nh* argmaxh\u2208HrD(h) is the best hypothesis in the class H w.r.t. D. The learner's performance in this setting is measured in terms of sample complexity, that is, the number of samples (xt, Yt) generated by the environment during the interaction as a function of (\u03b5, \u03b4) after which the learner outputs a hypothesis h satisfying the guarantee above.\nRegret minimization setting. In the online regret setting, the pairs (xt, Yt) are generated by an oblivious adversary. The interaction lasts for T rounds, where in each round t, after predicting Lt \u2208 L, the learner gains a reward rt(Lt; Yt) \u2261 |Lt \u2229 Yt| \u2208 [0, s], and the objective is to ultimately minimize regret with respect to H, defined as\n$R_T \\triangleq \\sup_{h^*\\in H} \\sum_{t=1}^{T}r_t(h^*(x_t); Y_t) - \\sum_{t=1}^{T}r_t(L_t; Y_t)$."}, {"title": "2.2 Contextual Combinatorial Semi-Bandits (CCSB)", "content": "The semi-bandit multiclass list classification problem is in fact a special case of contextual combinatorial semi-bandits (abbreviated as CCSB for conciseness), with the condition that the true label sets are of bounded size corresponding to a bound on the L\u2081-norm of the reward vectors."}, {"title": "3 Main Result: Agnostic PAC Setting", "content": "In this section we design and analyze a PAC learning algorithm for CCSB with s-sparse rewards. Our algorithm is displayed in Algorithm 1, and our main result is detailed in the following theorem:"}, {"title": "3.1 Analysis", "content": "Here we detail the main steps in the analysis of Algorithm 1, and in particular, outline the main challenges compared to the single-label setting where s = m = 1.\nInitial exploration. While in the single label classification setting it is possible to collect a dataset containing poly(K) i.i.d samples from D by simply predicting labels uniformly, this approach does not work in the multilabel classification setting (i.e. s > 1) and neither in CCSB with s-sparse rewards. The reason is that in these settings a uniform prediction of a list of actions will simply not yield a full observation of the reward vector, even after poly(K) such predictions.\nThus, instead of collecting a dataset, we use semi-bandit feedback directly in order to estimate the convex objective of interest using importance sampling, as detailed in Algorithm 1. These random objectives are unbiased with respect to F(\u00b7) defined in Eq. (3), while importance weighting causes their smoothness parameter to increase by a factor of K/m. This ultimately results in a slightly worse dependence on K in the lower order term of our bound.\nOptimizing for low-variance exploration. In order to approximately solve the stochastic convex optimization problem defined in Eq. (1), we employ a stochastic Frank-Wolfe (FW) algorithm with SPIDER gradient estimates (Yurtsever et al., 2019), or more specifically, an L1/L\u221e-specialized variant analyzed by Erez et al. (2024a), formally described in Appendix A.1. For our setup, we make use of Algorithm 3 over samples generated by the distribution D' instead of D, where crucially the gradients of the random objectives defined in Eq. (2) can be calculated exactly via\n$\\frac{\\partial f(p; x, r, a)}{\\partial p_j} = -\\frac{K}{m} (1 - \\gamma) \\sum_{y \\in a} \\frac{1\\{y \\in \\pi_j(x)\\}r(y)}{Q_{x,y}(\\hat{p})} \\quad \\forall j \\in [|\\Pi|].$"}, {"title": "3.2 Lower Bound", "content": "We now present a lower bound by which the dependence on m and s in Theorem 1 is tight even in the non-contextual setting.\nTheorem 2. For any combinatorial semi-bandit algorithm Alg over K actions and combinatorial actions of size m < K/2, there exists an s-sparse instance for which in order for Alg to output \u00e2 \u2208 A which is \u025b-optimal for \u025b \u00ab1/K with constant probability, Alg requires a sample complexity of at least \u03a9(sm/\u03b5\u00b2).\nWe provide a sketch of the proof of Theorem 2 below, and emphasize that it can be made formal using standard information-theoretic arguments. Additionally, this result can be easily extended to the classification setting by introducing correlation between the arms such that any realized reward will have s arms with a reward of 1 and the others with reward zero.\nProof of Theorem 2 (sketch). Consider an instance specified by m Bernoulli arms with expected reward + each while the other K m arms have expected reward - . Denote the set of m arms with highest expected reward by S. Now, note that in order to find an \u00a7-optimal subset, Alg must identify at least half of the arms in S, and thus is required to estimate all of the arms'\nrewards up to an accuracy of . Since each arm's reward distribution has variance of \u2248 *, the number of samples required to make such an estimation of the reward for each arm y is\n$\\frac{Var(r_y)}{(\\varepsilon/m)^2} \\approx \\frac{s}{K} \\cdot \\frac{m^2}{\\varepsilon^2}$.\nSince such an estimation needs to be performed for all K arms and each time step gives Alg samples for m arms via semi-bandit feedback, the total sample complexity is of order at least\n$\\frac{K}{m} \\cdot \\frac{s}{K} \\cdot \\frac{m^2}{\\varepsilon^2} = \\frac{sm}{\\varepsilon^2}$."}, {"title": "4 Online Regret Minimization Setting", "content": "In this section we present a regret minimization algorithm for CCSB with rewards that satisfy ||rt||2 < s which is a slightly relaxed version of the assumption ||rt||1 < s. Instead of rewards in [0, 1], it will be convenient for us to instead consider losses, so we introduce the following negative loss vectors:\nlt(y) = -rt(y) \u2200t \u2208 [T], y \u2208 V.\nIt is obvious that the losses satisfy ||lt||2 < s and that the transformation from rewards to losses does not affect the regret. A natural approach for regret minimization in CCSB would be to use the EXP4 algorithm (Auer et al., 2002) over the policy class I, which amounts to Follow-the-regularized-leader (FTRL) using negative entropy regularization, defined as\nH(p) =pilog pi, \u03c1\u03b5\u0394\u03c0.\nHowever, as been observed by Erez et al. (2024b), since the loss vectors are negative, this approach alone would not suffice to achieve a regret bound that is adaptive to the sparsity level s, and would instead yield a regret bound of the form O(\u221amKT log |\u03a0|). In order to adapt to sparsity, they introduce an additional log-barrier regularization, defined as\n\u03a6(p) =log pi, \u03c1\u03b5\u0394\u03c0.\nWe adopt this approach and generalize it to the combinatorial setup with Algorithm 2.\nWe prove the following result for Algorithm 2:\nTheorem 3. Assume that the loss vectors lt satisfy ||lt||2 < s for all t \u2208 [T]. Then Algorithm 2 with v = and \u03b7 = \u221alog(|I|)/(msT) attains the following expected regret bound w.r.t. a finite policy class \u03a0:\nE[RT] \u2264 0(|II|log(|II|T) + \u221a smT log |II|).\nWe remark that the linear dependence on |\u041f| is essentially tight if we require the leading term to depend on s rather than K, as shown in Erez et al. (2024b) for the single-label setting."}, {"title": "5 Discussion & Open Problems", "content": "We conclude the paper with a brief discussion of several open questions and interesting directions for further research.\nFull-bandit feedback. Our results for PAC learning in the semi-bandit setting raise a natural question regarding the full-bandit feedback model, which is characterized by the fact that the observation upon predicting at \u2208 A consists of a single number rtat. Specifically, it is unclear given our results whether adapting to reward sparsity and obtaining improved sample complexity bounds of the form poly(s,m)/\u03b5\u00b2 is possible in the full-bandit model. While we do not fully know how to extend our techniques to the full-bandit setting, we present some interesting ideas which may provide initial directions towards answering this question. We focus on the question of how we should modify the log-barrier potential given in Eq. (1) to accommodate full-bandit feedback, that is, such that its gradient would correspond to the variance of an appropriate full-bandit reward estimator. Indeed, importance-weighted reward estimators for combinatorial bandits are widely used in the literature (see, e.g., Audibert et al., 2014), and it is straightforward to construct such"}, {"title": "A Proofs for Section 3", "content": "To prove Lemma 1, we will need the following auxiliary result:\nLemma 2. Suppose y \u2264 and let p* = argminp\u2208\u2206\u03c0 F(P). Then it holds that ||\u2207F(p*)||\u221e\u2264 s.\nProof. First note that the gradient of F(\u00b7) is given by\n$( \\nabla F(p))_\\pi = E_z \\left[ \\sum_{y \\in Y} \\frac{(1 - \\gamma)r(y)1\\{y \\in \\pi(x)\\}}{Q_{x,y}(\\hat{p})} \\right]$\nThus, using first-order convex optimality conditions, the following holds that for any p\u2208 \u0394\u03c0,\nVF(p*) \u00b7 (p - p*) \u2265 0,\nwhich with the explicit form of the gradient becomes\n$E_x \\left[ \\sum_{y \\in Y} \\frac{(1 - \\gamma)r(y) \\sum_{\\pi \\in \\Pi}(p(\\pi) - p^*(\\pi))1\\{y \\in \\pi(x)\\} }{Q_{x,y}(p^*)} \\right] \\le 0,$\nor equivalently, after dividing by 1 \u2013 \u03b3,\n$\\frac{E\\sum_{y \\in Y} r(y)(Q_{x,y}(p) - Q_{x,y}(p^*)) }{Q_{x,y}(p^*)} \\le 0,$\nwhich rearranges to\n$\\frac{E\\sum_{y \\in Y} \\frac{r(y)Q_{x,y}(p)}{Q_{x,y}(p^*)} }{E[||r||_1]}  0,\\quad s.t.\\quad |p| \\le B \\}$.\nLemma 3. In Algorithm 3 with the random objectives defined in Eq. (2), each call to LOOn can be implemented via a single call to ERMII."}, {"title": "B Proofs for Section 4", "content": "Theorem 3 primarily stems from the following the following result which is a consequence of a generic analysis of FTRL (see e.g. Hazan et al. (2016), Orabona (2019) (Lemma 7.14)):\nLemma 4. For all p* \u2208 \u2206n, the following regret bound holds for Algorithm 2:\n$\\sum_{t=1}^T \\hat{\\ell}_t (p_t - p^*) \\le R(p^*) - R(p_1) + \\frac{\\nu}{2} \\sum_{t=1}^T ||p_t - p_{t+1}||_{\\Phi, \\rho_t}^2,$\nwhere R(p) \u2261 H\u2081(p) + \u03a6\u2081(p), and pt \u2208 [Pt,Pt+1] is some point which lies on the line segment connecting pt and pt+1.\nIn order to relate pt given in the bound to pt, we use the following result which is where we make use of the properties of the log-barrier regularization (.).\nLemma 5. Assuming that v < , it holds that pt+1(i) \u2264 8Pt(i) for all i \u2208 [|I|].\nErez et al. (2024b) prove this claim for the single-label setting, for completeness we include the proof for the multilabel setting. The proof requires the following preliminary definition:\nLocal and dual norms. Given a strictly convex twice-differentiable function F : W \u2192 R where WC Rd is a convex domain, we define the local norm of a vector g\u2208 W about a point z \u2208 W with respect to F by\n$||g||_{F,z} \\triangleq \\sqrt{g^T \\nabla^2 F(z) g},$\nand its dual norm by\n$||g||^*_{F,z} \\triangleq \\sqrt{g^T \\nabla^2 F(z)^{-1} g}.$"}]}