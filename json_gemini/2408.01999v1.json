{"title": "Reinforcement Learning for an Efficient and Effective Malware Investigation during Cyber Incident Response", "authors": ["Dipo Dunsin", "Mohamed Chahine Ghanem", "Karim Ouazzane", "Vassil Vassilev"], "abstract": "Our research focused on enhancing post-incident malware forensic investigation using reinforcement learning (RL). We reviewed existing literature to identify the limitations of RL, heuristics, and signature-based methods, emphasizing RL's potential to overcome these limitations. We proposed an advanced RL-based post-incident malware forensics investigation model and framework to expedite post-incident forensics. We developed a robust malware workflow diagram and datasets, examining both uninfected and infected memory dumps for anomalies and artefacts. We created a unified Markov decision-process (MDP) model to guide an RL agent through well-defined state and action spaces. We elaborated distinct MDP environments for each of the 13 malware variants. We then implement our RL Post- Incident Malware Investigation Model based on structured MDP within the proposed framework. To identify malware artefacts, the RL agent acquires and examines forensics evidence files, iteratively improving its capabilities using Q-table and temporal difference learning. The Q-learning algorithm significantly improved the agent's ability to identify malware. An epsilon-greedy exploration strategy and Q-learning updates enabled efficient learning and decision-making. Our experimental testing revealed that optimal learning rates depend on the MDP environment's complexity, with simpler environments benefiting from higher rates for quicker convergence and complex ones requiring lower rates for stability. Our model's performance in identifying and classifying malware reduced malware analysis time compared to human experts, demonstrating robustness and adaptability. The study highlighted the significance of hyper-parameter tuning and suggested adaptive strategies for complex environments. Our RL- based approach produced promising results and is validated as an alternative to traditional methods notably by offering continuous learning and adaptation to new and evolving malware threats which ultimately enhance the post-incident forensics investigations.", "sections": [{"title": "I. INTRODUCTION", "content": "Malware, also known as malicious software, is a type of software that infiltrates and compromises data and information, performing harmful and unauthorised functions. Its presence can lead to severe consequences, such as data theft, information destruction, extortion, and the crippling of organisational systems. In today's digital landscape, investigating malware has become an urgent and paramount concern due to its potential for significant damage and loss. Recent studies reveal a startling reality: malicious software is proliferating at an alarming rate, with some strains employing deceptive tactics to evade cyber forensics investigations. According to Quertier et al., [1], AV-TEST estimates the daily discovery of approximately 450,000 new malware instances, 93 per cent of which are Windows-based malicious files, primarily in the form of portable executable (PE) files. This underscores the critical need for swift malware investigations when an attack occurs to prevent widespread damage and mitigate the risk of malware evolving into more sophisticated and destructive forms. Quertier et al., [1] also describe various malware inves- tigation approaches, demonstrate the use of machine learning in malware analysis, and highlight how reinforcement learning improves malware models' performance and accuracy.\nThe heuristic-based malware technique is a widely employed approach that analyses various system files, categorising them as normal, unusual, or potentially harmful. Aslan and Samet [2], emphasise that while signature-based methods struggle with new malware, a combination of heuristic and signature- based approaches offers a reliable and expeditious means of identifying malicious software. In contrast, deep learning- based approaches exhibit remarkable capabilities in identifying both known and previously unseen malicious software, surpassing the performance of behaviour-based and cloud- based techniques. Malware analysis and classification heavily rely on machine learning algorithms trained to distinguish between malware and benign files. Machine learning-based approaches, according to Akhtar [3], face several challenges, including the frequent return of false positives and the ability of new malware with polymorphic traits to alter their file signatures and evade identification. To overcome these challenges, ma- chine learning techniques train algorithms to better identify and classify various forms of malware based on patterns and features found in extensive databases. By equipping algorithms to identify and classify different types of malware, machine-learning approaches enhance the security of computer systems and networks [4].\nWhile many malware instances exhibit distinct features and static structures that differentiate them from benign soft-"}, {"title": "A. Research Aim", "content": "The goal of this research is to improve malware forensics in- investigations by leveraging reinforcement learning techniques. Specifically, the research aims to identify, analyse, and en- hance malware forensics investigation models to enhance their accuracy in post-incident investigations. This, in turn, should expedite malware forensics investigations and mitigate the current \u2018miscarriage of justice' within the UK justice system. In addition, the study concentrates on enhancing heuristic- and signature-based analysis techniques using reinforcement learning in the event of a post-incident forensic investigation."}, {"title": "B. Research Objectives", "content": "Investigate Reinforcement Learning Methodologies: The primary objective is to investigate and develop reinforcement learning-based methodologies for automating a human forensics expert's routine task, specifically identifying malware artefacts following a security incident.\nIntroduce Reinforcement Learning Model for Malware Forensics: Develop a sophisticated reinforcement learning model that can efficiently analyze malware. This model aims to: 1. Precisely identify and classify malware sam- ples. 2. Adapt seamlessly to new malware types and their variants. 3. Enhance performance in post-incident scenarios. 4. Improve the overall effectiveness of post- incident malware forensics investigations.\nDevelop a Multi-Approaches Malware Forensics Frame- work: Enhance current manual post-incident forensics analysis by leveraging AI techniques combined with heuristic and signature-based approaches to introduce a comprehensive post-incident malware forensics investigations framework.\nProduce an Empirical Evaluation of the Proposed Model and Framework: Test the developed MDP model and Framework using real-world data to validate their effectiveness in identifying malware compared to traditional methods."}, {"title": "C. Related Work", "content": "In our research, we use Q-learning in a set action and state space to train RL agents on how to investigate for malware in post-incident malware forensics. We improve the agent's performance by continuously providing it with feedback on its performance in the MDP environment. Similarly, Binxiang\n[11] leveraged deep reinforcement learning to overcome the limitations of traditional signature-based methods, using Q- Q-learning to adapt to evolving malware threats, demonstrating the superiority of RL over static approaches. In a similar approach, Fang et al., [10] extended this by proposing the DQEAF model, which uses deep Q-networks to evade anti- malware engines, emphasising the creation of evasive mal- ware that bypasses traditional malware analysis methods. The Markov Decision Process (MDP) model helped us organise our research even more. It helped us compile a list of the states and actions that make up the proposed reinforcement learning post-incident malware investigation framework. For instance, it assisted us in acquiring live memory images and identifying the operating systems in use. In a related study, Quertier et al., [1], used the DQN and REINFORCE algorithms in an MDP framework to test machine learning-based malware analysis engines and find actions that could turn malware into undetected files. In defining our action and state space, we identified 67 unique states and"}, {"title": "D. Research Contribution", "content": "This research aims to answer the following research question:\nRQ1: How effective are current reinforcement learning models in distinguishing between benign and malicious software, and what improvements are necessary to en- hance their accuracy? This question seeks to evaluate the performance of existing reinforcement learning-based malware analysis models and identify areas where these models may require refinements or completely new ap- proaches to improve their effectiveness.\nRQ2: What role can reinforcement learning play in enhancing malware analysis capabilities against known and novel malware threats during post-incident mal- ware forensics investigations? This question explores the potential of using reinforcement learning to identify malware patterns that traditional malware analysis tools struggle to investigate. This could help find and stop a wider range of threats.\nRQ3: Can post-incident malware forensics investigations effectively integrate reinforcement learning and adapt to evolving malware signatures? The question seeks to explore whether reinforcement learning can constantly improve and update RL model algorithms, thereby im- proving the model's ability to handle new types of malware during post-forensics investigations.\nRQ4: How can hybrid models combining heuristic and reinforcement learning approaches improve the reliability and speed of post-incident malware forensics investiga- tion? This question seeks to determine the feasibility and effectiveness of blending traditional heuristic-based methods with reinforcement learning techniques to create a robust framework for analysing post-incident malware more efficiently and with greater accuracy.\nOur research significantly enhances post-incident malware forensics investigations through reinforcement learning, offer- ing a robust framework for combating complex cyber threats. Our contributions include:\nRC1: We created a comprehensive malware analysis workflow diagram that incorporates a variety of malware analysis techniques. We designed the malware workflow diagram to examine and analyse malware from live mem- ory dumps using methods like static analysis, signature- based analysis, behavioural analysis, and machine learn- ing algorithms. This malware workflow diagram increases adaptability and robustness in malware analysis, improves overall information security, and aids in forensic investi- gations post-incident.\nRC2: We implemented a unified MDP model that com- bines several MDP subsections into one holistic overview. This model includes states, actions, rewards, and tran- sition probabilities, providing a structured approach for the RL agent to identify and isolate suspicious files. The unified MDP model offers a systematic method for malware analysis, enabling the RL agent to perform optimally by understanding the environment through a series of well-defined steps.\nRC3: We successfully developed sophisticated reinforce- ment learning Framework. This Framework outperform any existing automation or human expert in investigating malware infection requiring little time and resources and resulting in higher accuracy.\nRC4: We implemented an new approach using AWK module and Volatility 3 to extract specific data from memory dumps, which we then analyzed to identify active processes and potential malware. This approach improves malware investigations by providing detailed insights into the memory dump and facilitating the iden- tification of anomalies and compromise indicators.\nRC5: We Elaborated an important marlware forensics analysis dataset made out real-world malware scenarios."}, {"title": "E. Paper Outline", "content": "We organise this research paper as follows: The abstract summarises the study's primary focus on leveraging RL to expedite and improve post-incident forensic processes. As mentioned in Section I, the introduction sets the stage by emphasising the critical need for rapid and efficient malware investigations in light of the increasing prevalence of cyber threats. Following this, the literature review in Section II presents a thorough examination of existing malware analysis methods, underscoring the limitations of traditional approaches and highlighting the promise of RL. Furthermore, Section III and Section IV detail the research methodology development and implementation of the RL-based model, describing the design of the Markov decision-process (MDP) environments, the integration of reinforcement learning techniques, and the testing and evaluation of the RL agent in Section V. In addition, the results and discussion Section VI illustrate and describe the experimental findings, demonstrating the RL model's superior performance in terms of speed and accuracy compared to traditional methods and human forensics experts capabilities.\nMoreover, the conclusion Section VII encapsulates the key findings, reiterating the study's significant contributions to cy- bersecurity and malware forensics. It emphasises the potential of RL to revolutionise post-incident investigations, providing faster and more accurate results. Additionally, the research offers a comparative analysis, highlighting the advantages of RL over heuristic and signature-based methods. Notably, the hybrid approach integrating heuristic and RL methods shows promising results. Finally, the paper suggests some artificial intelligence techniques for future research, such as exploring"}, {"title": "II. LITERATURE REVIEW AND BACKGROUND", "content": "Quertier et al., [1] research highlights the challenges of machine learning classifiers in identifying potential malware, especially when there is limited insight into the malware output. The study suggests using reinforcement learning with REINFORCE and DQN algorithms to test the effectiveness of EMBER and MalConv machine learning analysis on commer- cial antivirus solutions. The study found that REINFORCE has a higher evasion rate and better performance than DQN, espe- cially when tested against a commercial antivirus. However, a more comprehensive approach could have included training these models on a broader array of diverse models."}, {"title": "B. Deep RL for Malware Analysis", "content": "In 2019, Binxiang, Gang, and Ruoying [11], introduced a deep reinforcement learning-based technique for malware identification, aiming to address the vulnerabilities of tradi- tional signature-based and machine learning-based approaches. The research demonstrated that deep reinforcement learning outperformed traditional methods based on static signatures and demonstrated the ability to quickly adapt to the ever-changing landscape of malware. However, the study had limitations, including a lack of comprehensive details about the experimental design, datasets used, and evaluation metrics. Expanding the training dataset and incorporating domain- specific knowledge could improve malware analysis. Notably, expanding the training dataset's size and diversity, as suggested by Szegedy et al., [14], could significantly enhance the effec- tiveness of the malware investigation. Researchers like Silver"}, {"title": "C. RL-Based Attacks on Static Malware Detectors", "content": "Ebrahimi et al., [12] research aims to improve the effec- tiveness of static malware detectors in countering black-box cyberattacks. They propose using reinforcement learning (RL) to optimise the decision-making process of static malware detectors in the presence of black-box attacks. They create the Variational Actor-Critic for Discrete Adversarial Malware Generation (AMG-VAC) using discrete operations and an approximate sampling operator. They use RL to optimise the decision-making process, adjusting the neural network's weights based on the reward signal. In terms of accuracy, the RL-based AMG detector outperforms the original detector, particularly in the presence of black-box attacks. In addition, their results show that the RL-based AMG detector is much more accurate than the original detector when it comes to black-box attacks. However, RL in discrete action spaces may not align with all types of malware detectors, and its effective- ness depends on factors like the training dataset quality and the neural network's architecture."}, {"title": "D. Malware Analysis Using Intelligent Feature Selection", "content": "Fang et al., [10] developed a specialised architectural solution called DQFSA to address the shortcomings of traditional malware classification methods. The architecture uses deep Q- learning to identify crucial features, reducing human interven- tion and allowing data selection and analysis across various cases and data volumes. The methodology incorporates multi- view features, focusing on high classification accuracy during the validation phase. The key difference lies in exposing an Al agent to sample features with minimal human intervention. Experiments validated the DQFSA architecture by comparing its performance against various classifiers and related works."}, {"title": "E. Modern Incident Response Enhanced by AI", "content": "Dunsin et al., [18] present a study on the application of artificial intelligence (AI) and machine learning (ML) in digital forensics, focusing on enhancing malware investigation through innovative methodologies. The paper highlights the integration of AI and ML techniques to improve investigative precision and efficacy in digital forensics, leveraging advanced computational models to automate the investigation and analy- sis of cyber threats. Another focus is memory forensics, which focuses on machine learning algorithms to analyse memory dumps and malware, enhancing the reliability of forensic investigations by extracting and analysing multiple artefacts.\nThe study highlights the advantages of AI and ML in digital forensics, such as data mining techniques, reinforcement learn-ing, and Markov decision process (MDP) for automated mal- ware analysis. However, the study acknowledges challenges such as data validity, appropriate tools for memory dump retrieval, and adhering to ethical and legal standards. The study also proposes reinforcement learning, modelled as a Markov decision process (MDP), as a method for investigating malware in digital forensics. The MDP framework allows for systematic evaluation of different states and actions, facili- tating the development of effective RL models for malware investigation, as illustrated in Figure 1."}, {"title": "F. ML and Knowledge Based System for Malware Analysis", "content": "Piplai et al., [7]) propose a framework that uses reinforcement learning and open-source knowledge to enhance malware anal-ysis. The framework consists of two components: reinforce-ment learning for malware analysis and knowledge from open sources detailing past cyberattacks. The research experiments create 99 distinct processes during data collection, enabling the model to identify new malware. In similar research, Gallant [19] conducted a malware investigation experiment in which the researchers trained and employed multiple machine learn-ing algorithms, including Perceptrons, and rigorously tested their performance. However, Piplai et al., [7] leave unspecified aspects, such as determining which prior knowledge is relevant for new malware analysis and whether prior knowledge might introduce biases from previous cases. Despite these concerns, the framework's incorporation of prior knowledge remains valuable, guiding new models with increased efficiency and accuracy."}, {"title": "G. RL for Malware Investigations", "content": "Reinforcement learning in malware forensics investigations involves an agent that seeks to optimise cumulative rewards by effectively managing the trade-off between exploration and exploitation. Our research's primary focus is on using reinforcement learning techniques to automate the process of conducting post-incident malware forensics investigations following a security incident. The agent performs actions within the environment, leading to changes in its state. The"}, {"title": "III. RESEARCH METHODOLOGY", "content": "To implement and validate our proposed reinforcement learn- ing malware investigation framework, we took a systematic approach to creating a comprehensive malware dataset us- ing the London Metropolitan University Digital Forensics Laboratory. First, we established thirteen virtual machines within an isolated network to ensure a secure and controlled environment for our experiments and the eduroam network. This setup was critical to preventing the spread of unintended malware and maintaining the integrity of our data collection process. Next, we uploaded 13 different ISO files, each rep- resenting various versions of the Windows operating system. This diverse selection of operating systems allowed us to test our framework across a broad spectrum of environments.\nNext, we introduced a variety of malware to infect each of these operating systems. We specifically chose each malware type to represent different attack vectors and behaviours, providing a robust challenge for our investigation framework. For each ISO file installed on the virtual machine, we took an initial snapshot of the environment and saved the live memory dump. Following this, we infected the virtual machine with the chosen malware and took another snapshot. This process resulted in pairs of snapshots, one uninfected and one infected, for each operating system. This methodology yielded 13 RAM files from the uninfected environments and another 13 from the infected ones. To analyse these files, we used the Volatility framework, a powerful tool for memory forensics. We man- ually examined both the infected and uninfected RAM files, which, as a result, enabled us to identify significant changes and behaviours indicative of malware presence. To ensure replication and verification of our procedures, we diligently documented each stage of the analysis. This documentation was critical for maintaining the integrity of our research, as well as for future reference. Finally, based on our analysis of the 26 files, we created a detailed malware workflow"}, {"title": "B. Malware Workflow Diagram Creation", "content": "The research methodology extends from our comprehensive experimental setup and dataset generation process to the development of a detailed malware analysis workflow diagram, as depicted in Figure 3. This diagram is integral to our rein-forcement learning malware investigation framework, encom- passing various malware analysis techniques, including data collection, examination, and analysis. Our dataset, comprising live memory dumps from 13 different versions of Windows operating systems-both infected and uninfected provides the foundation for this workflow. We examined these dumps to detect anomalies, indicators of compromise, and potential malware artefacts by using the Volatility framework for mem- ory forensics. The analysis phase incorporates a diverse array of techniques such as static analysis, signature-based analysis, behavioural analysis, and machine learning algorithms. The resulting malware workflow analysis diagram not only maps out the typical processes and behaviours associated with our chosen malware samples, but it also serves as a crucial tool for improving information security and post-incident mal- ware forensics investigations. Our structured approach rigorously trains and validates our reinforcement learning model, strengthening our malware investigation capabilities."}, {"title": "C. Markov Decision Process (MDP) Formulation", "content": "In our problem, the Malware Investigation associated Markov Decision Process (MDP) provides a mathematical framework for modelling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker. Our MDP is defined by the following components:\nStates (S): In this case, |S| = 67 states.\nActions (A): In this case, |A| = 10 actions.\nTransition Function (T): T (s, a, s') represents the prob- ability to transition from state s to state s' under action \u03b1.\nReward Function (R): R(s, a) represents the immediate reward received after performing action a in state s.\nDiscount Factor (\u03b3): A factor \u03b3 \u2208 [0, 1] that discounts future rewards."}, {"title": "D. Leveraging Reinforcement Learning", "content": "In the context of our proposed Reinforcement Learning (RL), the agent learns the optimal policy \u03c0* by interacting with the three proposed MDP environments. A common algorithm used is Q-learning, which updates the Q-values based on the Bellman equation.\n1) Value Function and Policy: The value function for policy \u03c0 is given by:\nV\u03c0(s) = \\sum_{a}\u03c0(\u03b1 | s) \\sum_{s'} T (s, a, s') [R(s, a) + \u03b3V \u03c0 (s')]\n\\bullet V (s): Expected cumulative reward starting from state s and following policy \u03c0.\n\u03c0(\u03b1 | s): Probability of taking action a given state s under policy \u03c0.\n2) Reinforcement Learning (RL) with Q-Learning: subsectionLeveraging Reinforcement Learning In the context of our proposed Reinforcement Learning (RL), the agent learns the optimal policy \u03c0* by interacting with the three proposed MDP environments. A common algorithm used is Q-learning, which updates the Q-values based on the Bellman equation.\nThe Q-learning update rule is given by:\nQ(s, a) - Q(s, a) + ar + y max_{a'} Q(s', a') \u2013 Q(s, a)\nWhere:\na is the learning rate.\nr is the reward received after taking action a in state s.\ns' is the next state resulting from action a.\nmax_{a'} Q(s, a) is the maximum estimated future reward from state s'.\nUsing the specifications as a result of the workflow diagram:\nWe have 67 states and 10 actions.\nThe transition and reward functions would be defined based on the specific malware identification tasks.\n3) Q-Learning Update Rule:\nQ(s, a) \u2013 Q(s, a) + ar + y max Q(s, a) \u2013 Q(s, a)\nStep 1: Initialize Q-Table\nInitialize Q(s, a) for all s \u2208 S and a \u2208 A to some arbitrary values (e.g., 0).\nStep 2: Choose Learning Rate a and Discount Factor \u03b3"}, {"title": "E. Setting the Parameters for MDPs", "content": "The Reinforcement Learning Post-Incident Malware Investiga- tive Model uses the malware workflow diagram to define parameters for action and state spaces. The agent uses live memory dumps to analyse and identify malware artefacts, with 109 distinct actions within a defined environment. The state array aligns with the malware workflow diagram, encompass- ing 67 unique states. To achieve this alignment, we follow steps such as installing WinPmem, obtaining live memory images, understanding the operating system, extracting process information, listing DLLs, tracking open handles, collecting network data, figuring out registry hives, listing keys, dupli- cating processes into executable files, and sending them to Known Files Filters Servers."}, {"title": "F. The Motivation behind Implementing Q-Learning", "content": "The proposed Reinforcement Learning Post-Incident Mal- ware Investigation Framework uses Q-learning, an off-policy, model-free algorithm. We use it because it employs a value- based approach to determine the optimal actions based on the current state. The algorithm learns the relative value of different states and actions through experiential knowledge without relying on explicit transition or reward functions. This approach is suited for the proposed RL model for analysing malware artefacts. In this context, 'Q' signifies quality, representing the action's value in terms of optimising future rewards. On the other hand, model-based algorithms employ transition and reward functions to estimate the optimal policy and construct a model, whereas model-free algorithms acquire knowledge about action outcomes experientially, with- out explicit transition or reward functions. In our proposed implementation, we opt for the value-based approach, which entails training the value function in order for the agent to learn the relative value of different states and take actions accordingly. Conversely, policy-based methods directly train the policy to determine the appropriate action for a given state. On the other hand, in off-policy methods, the algorithm assesses and improves a policy that is different from the action execution policy. In contrast, on-policy algorithms evaluate and refine the same policy employed for action execution."}, {"title": "G. Q-Learning Terminologies", "content": "In the following sections, we will implement the proposed Reinforcement Learning Post-Incident Malware Investigation Model. The following terminologies are defined and explained in brief. An Environment is the space or world in which the agent operates and takes actions. An Agent is the entity that learns and makes decisions by interacting with the environ- ment. States (s) signify the agent's present location within the environment. An Action (a) is the set of all possible moves or decisions the agent can make in the environment. Every action the agent takes results in either a positive reward or a penalty. Episodes mark the end of a stage, indicating that the agent cannot perform further actions. This occurs when the agent either accomplishes its objective or faces failure.\nFor each state-action pair, the agent uses a Q-Table to manage or store Q-values. We use Temporal Differences (TD) to estimate the expected value by comparing the current state and action with the previous state and action. The learning rate is a parameter that determines how much new information overrides old information. A policy is a strategy or mapping from states to actions that defines an agent's behaviours. The Discount Factor is a parameter that determines the importance of future rewards. The Bellman Equation is a fundamental equation in Q-Learning that expresses the relationship between the Q-value of a state-action pair and the Q-values of the subsequent state-action pairs. The Epsilon-Greedy Strategy is a method for balancing exploration and exploitation."}, {"title": "H. Q-Table and Q-Function", "content": "As previously mentioned, the Q-table is one of the key com- ponents that facilitate the agent's decision-making. It guides the agent in selecting the most favourable action based on expected rewards within the provided environments. The Q-learning algorithm updates the values of a Q-table, which essentially functions as a structured repository encapsulating sets of actions and states. However, defining the state and action spaces is a crucial preliminary step in effectively setting up the Q-table, a task that the malware workflow diagram facilitates. Furthermore, the Q-function plays a central role, using the Bellman equation and considering the state(s) and action (a) as its input. This equation significantly streamlines the calculation of both state values and state-action values."}, {"title": "I. Subsections of the Markov Decision Process Model", "content": "The proposed subsection of the Markov Decision Process (MDP) represents a segment of the comprehensive and unified MDP model. Each subsection of the Markov Decision Process (MDP) model contains states, actions, rewards, and a transition probability function. These subsections are crucial components of the unified MDP that provide an agent with the capabilities of identifying and isolating suspicious portable executable files for further investigations. This approach sets a benchmark for processes such as recognising process identities, analysing process DLLs and handles, examining network artefacts, and checking for evidence of code injection."}, {"title": "K. The Proposed RL Post-Incident Malware Investigation Framework", "content": "The Reinforcement Learning Post-Incident Malware Investiga- tion Framework consists of six core fundamental components, as seen in Figure 9: the data collection, the mapping of the workflow diagram, defining actions, and state spaces. We formulated the Markov Decision Process (MDP) Model's subsections as well as the proposed RL Post-Incident Mal- ware Investigation Model. We divided the environment into three sections: creating dependencies and gym environments, importing required libraries, implementing the training data, MDP solver, and continuous learning and adaptation. The workflow diagram outlines a comprehensive approach to pro- cess information gathering, starting with the collection of"}, {"title": "L. The Proposed RL Post-Incident Malware Investigation Model", "content": "In the proposed Reinforcement Learning Post-Incident Mal- ware Investigation Model, the 'Agent' is the decision-maker that interacts with the environment. The 'Environment' is the live memory dump in which the agent interacts. It provides the agent with state and reward data. The 'State' (s) is a represen- tation of the agent's current situation in the environment. The \u2018Action' (a) is the set of all possible moves the agent can take. The environment provides feedback, known as the 'Reward' (r), to evaluate the agent's actions. The agent uses the 'Policy' as a strategy to decide the next action based on the current state. The 'Value Function' (V(s)) is a function that estimates the expected cumulative reward from a given state following a particular policy. The 'Q-Function' (Q(s, a)) is a function"}, {"title": "Algorithm 1 - Implementation of the Q-Learning Algorithm", "content": "Algorithm 1 implements the Q-learning algorithm, a rein-forcement learning technique, to train an agent to make opti-mal decisions in an environment. The code initially initialises a Q-table with zeros, symbolising the agent's understanding of the environment, where rows represent states and columns represent actions.\nThe code establishes key parameters such as the learning rate, discount factor, and exploration probability (initially set to 0.9), as well as decay schedules and structures for storing data (storage, storage new, reward list). The main loop runs for a specified number of episodes, resetting the environment and relevant variables at the start of each episode to obtain the initial state, reset the episodic reward, and initialise a step counter. A 'greedy policy' selects actions within each episode: if the probability is high, it selects a random action (exploration); if not, it selects the action with the highest Q-value for the current state (exploitation). The environment executes the selected action, providing the next state, reward, and a done flag indicating the episode's end. The Bellman equation updates the Q-value, accounting for the immediate reward and the maximum future Q-value from the next state. The Q-table then stores the updated Q-value. Certain condi-"}, {"title": "N. Algorithm -2 Iterating Learning Rates Variation over MDP Environments", "content": "Algorithm 2 is an algorithm that trains and stores models using different learning rates (LRs) across multiple environ-ments. The initialization phase initiates the process, defining various environments (envs) and creating an empty dictionary named 'final dict' to store results.\nNext, we set the training parameters, which include a list of learning rates ranging from '0.001 to 0.9', and store the re-sulting Q-tables, intermediate storage, rewards, and additional storage collections. For each learning rate ('Ir') in the list of learning rates ('Irs'), the algorithm executes the 'new q learning' algorithm. Finally, the algorithm stores the results"}, {"title": "G. Implementation of the BlankEnvironment with Time", "content": "In BlankEnvironment_with_Time, the agent incurs a more severe negative reward of -0.1 per step, compared to the standard penalty of -0.04 in the other two environments. This technique aims to incentivise the agent to efficiently identify malicious files by taking the most direct path, thereby discouraging any superfluous actions. When the agent extends episodes by taking additional steps, significant penalties are in- curred. Notably, this incentive is considered a hyperparameter, as it is subject to continuous refinement. The expression 'done = k[3] assigns the fourth element of the tuple 'k' to 'done', indicating whether the episode is complete. If \u2018done' is 'True', a reward of +4 is assigned; otherwise, a penalty of -0.1 is given to the agent. The tuple 'new k' is then created, keeping the original values of 'k' but updating the reward value. We return this updated tuple for future interactions with the environment."}, {"title": "Iterating MDP Environments over Learning Rates", "content": "We implemented a Python code and iterated the three MDP environments over a range of learning rates (0.001\u20130.9). The name list\n['env_newl', 'env new2\u2032, 'env_new3\u2032] defines a list containing the names of the environments. We initialise an empty dictionary to store the final results and iterate over each environment using a for loop. We use the Q-learning function to convert the current learning rate to a float and store the results in dictionaries. We convert the output into a list and save it in the output dictionary. Finally, we group the collected data into a tuple and store it in the final dictionary, consolidating all results for further insight."}, {"title": "IV. TESTING AND EVALUATION", "content": "We implemented a Python code and defined several vari-ables, including Q-tables for different learning rates (q1,q2, q3), changes in Q-values (store1, store2, store3), cumulative rewards (reward1, reward2, reward3), and Q-values for multiple states (store_new1, store_new2, store_new3) for environments env_newl, env_new2, and env_new3. It initializes these variables by retrieving data from final_dict, ensuring each set of variables cor-responds to a specific environment. This consistent structure allows for efficient tracking and storage of Q-learning out- comes across multiple environments."}, {"title": "B. Comparing the Speed of Convergence", "content": "We implemented a Python code to visualise the speed of convergence across the three MDP environments, (env_newl, env_new2, and env_new3) representing BlankEnvironment, BlankEnvironment_with_Rewards(), and BlankEnvironment_with_Time(), respectively. Each dictionary maps learning rates to the number of episodes required for convergence. The code line x = [float(key) for key in env_new1.keys()) creates a list of floating-point learning rates from env_new1. The command"}, {"title": "C. Using Argmax to iterate over different learning rates and mdp environments", "content": "We implemented a Python code that initialises a list lrs with various learning rates and creates empty dictionaries q1_dct and q1_dict to store results for three different environments, env1, env2, and env3. The code then outputs a message indicating the processing of env1, then iterates over each learning rate in 1rs, initializing lists within the dictionaries and retrieving the corresponding Q-values from q1. Within a nested loop running 67 times for different states, it prints the state index and the action index with the highest Q-value using np.argmax(q_new[i]), appending this information as a string to q1_dct and as an integer to q1_dict."}, {"title": "D. Using Softmax to iterate over different learning rates and mdp environments", "content": "We implemented a Python code that defines a stable_softmax function to calculate the softmax of an input array x in a numerically stable manner. It initialises a list of learning rates (0.001-0.9) and empty dictionaries for three environments: env1, env2, and env3. The code then iterates over the learning rates for each environment, processes Q-values, and converts them into probability distributions using the stable_softmax function. It then samples actions for 67 states, appending the action with the highest Q-value to the respective dictionary, and prints the current environment and learning rate at each step. Upon completion, it prints a done message, ensuring consistent performance across different environments and learning rates for further comparative analysis."}, {"title": "E. Evaluating rewards dynamics using learning rates and MDP environments", "content": "We implemented a Python code that examines reward changes in the three MDP environments (env_newl, env_new2, and env_new3), with learning rates ranging from 0.001 to 0.9. To create interactive plots, it imports the plotly.graph_objects module as go. The code ex- tracts cumulative rewards, episode numbers, steps per"}]}