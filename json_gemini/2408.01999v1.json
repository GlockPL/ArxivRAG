{"title": "Reinforcement Learning for an Efficient and Effective Malware Investigation during Cyber Incident Response", "authors": ["Dipo Dunsin", "Mohamed Chahine Ghanem", "Karim Ouazzane", "Vassil Vassilev"], "abstract": "Our research focused on enhancing post-incident malware forensic investigation using reinforcement learning (RL). We reviewed existing literature to identify the limitations of RL, heuristics, and signature-based methods, emphasizing RL's potential to overcome these limitations. We proposed an advanced RL-based post-incident malware forensics investigation model and framework to expedite post-incident forensics. We developed a robust malware workflow diagram and datasets, examining both uninfected and infected memory dumps for anomalies and artefacts. We created a unified Markov decision-process (MDP) model to guide an RL agent through well-defined state and action spaces. We elaborated distinct MDP environments for each of the 13 malware variants. We then implement our RL Post- Incident Malware Investigation Model based on structured MDP within the proposed framework. To identify malware artefacts, the RL agent acquires and examines forensics evidence files, iteratively improving its capabilities using Q-table and temporal difference learning. The Q-learning algorithm significantly improved the agent's ability to identify malware. An epsilon-greedy exploration strategy and Q-learning updates enabled efficient learning and decision-making. Our experimental testing revealed that optimal learning rates depend on the MDP environment's complexity, with simpler environments benefiting from higher rates for quicker convergence and complex ones requiring lower rates for stability. Our model's performance in identifying and classifying malware reduced malware analysis time compared to human experts, demonstrating robustness and adaptability. The study highlighted the significance of hyper-parameter tuning and suggested adaptive strategies for complex environments. Our RL- based approach produced promising results and is validated as an alternative to traditional methods notably by offering continuous learning and adaptation to new and evolving malware threats which ultimately enhance the post-incident forensics investigations.", "sections": [{"title": "I. INTRODUCTION", "content": "Malware, also known as malicious software, is a type of software that infiltrates and compromises data and information, performing harmful and unauthorised functions. Its presence can lead to severe consequences, such as data theft, information destruction, extortion, and the crippling of organisational systems. In today's digital landscape, investigating malware has become an urgent and paramount concern due to its potential for significant damage and loss. Recent studies reveal a startling reality: malicious software is proliferating at an alarming rate, with some strains employing deceptive tactics to evade cyber forensics investigations. According to Quertier et al., [1], AV-TEST estimates the daily discovery of approximately 450,000 new malware instances, 93 per cent of which are Windows-based malicious files, primarily in the form of portable executable (PE) files. This underscores the critical need for swift malware investigations when an attack occurs to prevent widespread damage and mitigate the risk of malware evolving into more sophisticated and destructive forms. Quertier et al., [1] also describe various malware inves- tigation approaches, demonstrate the use of machine learning in malware analysis, and highlight how reinforcement learning improves malware models' performance and accuracy.\nThe heuristic-based malware technique is a widely employed approach that analyses various system files, categorising them as normal, unusual, or potentially harmful. Aslan and Samet [2], emphasise that while signature-based methods struggle with new malware, a combination of heuristic and signature- based approaches offers a reliable and expeditious means of identifying malicious software. In contrast, deep learning- based approaches exhibit remarkable capabilities in identifying both known and previously unseen malicious software, surpassing the performance of behaviour-based and cloud- based techniques. Malware analysis and classification heavily rely on machine learning algorithms trained to distinguish between malware and benign files. Machine learning-based approaches, according to Akhtar [3], face several challenges, including the frequent return of false positives and the ability of new malware with polymorphic traits to alter their file signatures and evade identification. To overcome these challenges, ma- chine learning techniques train algorithms to better identify and classify various forms of malware based on patterns and features found in extensive databases. By equipping algorithms to identify and classify different types of malware, machine-learning approaches enhance the security of computer systems and networks [4].\nWhile many malware instances exhibit distinct features and static structures that differentiate them from benign soft-"}, {"title": "", "content": "ware, some possess characteristics that make them challenging to identify accurately [5]. Even with advances in machine learning, complex, evolving malware can evade these models and remain hidden, particularly when it is novel or highly adaptable. Reinforcement learning becomes a valuable tool in these scenarios, as it enables the creation of new malware samples capable of evading machine learning identification. These new samples retrain the malware models to identify more unknown threats [6]. Reinforcement learning distin- guishes itself from conventional machine learning models by embracing uncertainty and extensive trial and error as opposed to predefined mappings [7]. This quality makes reinforcement learning particularly effective in situations where specific answers are elusive, such as in the analysis of new and unknown malware threats [8]. During the implementation of reinforcement learning algorithms, exploration plays a pivotal role. Through exploration, the model actively explores various features, expanding the breadth of knowledge about different malware types. Subsequently, exploitation comes into play, enhancing the model's performance by selecting the most beneficial attributes [9].\nIn reinforcement learning, the reward techniques set it apart from other machine learning approaches. Reinforcement learn- ing provides the agent with either negative or positive evaluation feedback, which may not necessarily indicate the correct actions in the environment. Generally, we depict the agent as capable of choosing a specific set of features that, when applied, enhance the model's accuracy. The ever- changing environment, shaped by the agent's actions, facilitates the collection of relevant features for classification. According to Fang et al., [10], 'the accuracy of the classifier serves as a reward,' with the DQFSA architecture being a noteworthy example that employs reinforcement learning for feature selection. Reinforcement learning typically involves an agent that interacts with the malware analysis environment, introducing modifications to files to relate them to expected performance outcomes. Recent research, such as REINFORCE and Deep Q-Network (DQN), has harnessed reinforcement learning to enhance malware investigations by leveraging past knowledge, according to Quertier et al., [1]. This is particularly advan- tageous, as traditional machine learning models often lack the ability to incorporate background knowledge into their malware analysis. Reinforcement algorithms can reduce trial- and-error efforts and rely on past experiences to analyse and classify malware more efficiently using verified knowledge [7]."}, {"title": "A. Research Aim", "content": "The goal of this research is to improve malware forensics in- investigations by leveraging reinforcement learning techniques. Specifically, the research aims to identify, analyse, and en- hance malware forensics investigation models to enhance their accuracy in post-incident investigations. This, in turn, should expedite malware forensics investigations and mitigate the current \u2018miscarriage of justice' within the UK justice system. In addition, the study concentrates on enhancing heuristic- and signature-based analysis techniques using reinforcement learning in the event of a post-incident forensic investigation.\nThe study aims to enhance and optimise malware forensics investigation by implementing a reinforcement learning model and effectively handling new and unknown types of malware. This, in turn, should ultimately improve overall cybersecurity measures in the aftermath of an incident."}, {"title": "B. Research Objectives", "content": "Investigate Reinforcement Learning Methodologies: The primary objective is to investigate and develop reinforcement learning-based methodologies for automating a human forensics expert's routine task, specifically identifying malware artefacts following a security incident.\nIntroduce Reinforcement Learning Model for Malware Forensics: Develop a sophisticated reinforcement learning model that can efficiently analyze malware. This model aims to: 1. Precisely identify and classify malware sam- ples. 2. Adapt seamlessly to new malware types and their variants. 3. Enhance performance in post-incident scenarios. 4. Improve the overall effectiveness of post- incident malware forensics investigations.\nDevelop a Multi-Approaches Malware Forensics Frame- work: Enhance current manual post-incident forensics analysis by leveraging AI techniques combined with heuristic and signature-based approaches to introduce a comprehensive post-incident malware forensics investigations framework.\nProduce an Empirical Evaluation of the Proposed Model and Framework: Test the developed MDP model and Framework using real-world data to validate their effectiveness in identifying malware compared to traditional methods."}, {"title": "C. Related Work", "content": "In our research, we use Q-learning in a set action and state space to train RL agents on how to investigate for malware in post-incident malware forensics. We improve the agent's performance by continuously providing it with feedback on its performance in the MDP environment. Similarly, Binxiang\n[11] leveraged deep reinforcement learning to overcome the limitations of traditional signature-based methods, using Q- Q- learning to adapt to evolving malware threats, demonstrating the superiority of RL over static approaches. In a similar approach, Fang et al., [10] extended this by proposing the DQEAF model, which uses deep Q-networks to evade anti- malware engines, emphasising the creation of evasive mal- ware that bypasses traditional malware analysis methods. The Markov Decision Process (MDP) model helped us organise our research even more. It helped us compile a list of the states and actions that make up the proposed reinforcement learning post-incident malware investigation framework. For instance, it assisted us in acquiring live memory images and identifying the operating systems in use. In a related study, Quertier et al., [1], used the DQN and REINFORCE algorithms in an MDP framework to test machine learning-based malware analysis engines and find actions that could turn malware into undetected files. In defining our action and state space, we identified 67 unique states and"}, {"title": "", "content": "up to 10 actions within our RL model, facilitating a thorough malware forensics investigation. This detailed mapping is similar to the work of Ebrahimi et al., [12], who used action and state spaces in their AMG- VAC model to improve static malware analysis in black-box attack scenarios. Their study showed the pros and cons of using separate action spaces for various malware identification needs.\nAdditionally, we integrated various machine learning tech- niques, including static and behavioural analysis, to enhance our proposed framework's robustness. This integration was adapted from Wu et al. work [6], who emphasised the en- hancement of malware analysis models using reinforcement learning by incorporating past knowledge into RL algorithms to improve malware identification and classification. In par- allel, Piplai et al., [7] also explored the use of knowledge graphs to inform RL algorithms for malware identification, highlighting the benefits of incorporating historical data into machine learning processes. In addition, we evaluated the per- formance of the RL model in our research methodology based on its ability to reduce the time required for post-incident malware forensic investigations and its accuracy in identifying malware. We measure this by conducting extensive experi- mental testing in simulated real-world scenarios. Similarly, in the broader literature, performance metrics often include the accuracy of malware identification and the time efficiency of the forensic process. For example, a study by Raff et al., [13] evaluates their RL-based malware identification system on similar parameters, emphasising the efficiency of the RL agent in real-time scenarios. While the related work provides a solid foundation in malware analysis and MDP modelling, our research methodology builds upon this foundation by offering practical, detailed methodologies and demonstrating their application in real-world scenarios. This progression from theoretical concepts to practical implementation marks a significant contribution to the field of cybersecurity and port-incident malware forensics investigation."}, {"title": "D. Research Contribution", "content": "This research aims to answer the following research question:\nRQ1: How effective are current reinforcement learning models in distinguishing between benign and malicious software, and what improvements are necessary to en- hance their accuracy? This question seeks to evaluate the performance of existing reinforcement learning-based malware analysis models and identify areas where these models may require refinements or completely new ap- proaches to improve their effectiveness.\nRQ2: What role can reinforcement learning play in enhancing malware analysis capabilities against known and novel malware threats during post-incident mal- ware forensics investigations? This question explores the potential of using reinforcement learning to identify malware patterns that traditional malware analysis tools struggle to investigate. This could help find and stop a wider range of threats.\nRQ3: Can post-incident malware forensics investigations effectively integrate reinforcement learning and adapt to evolving malware signatures? The question seeks to explore whether reinforcement learning can constantly improve and update RL model algorithms, thereby im- proving the model's ability to handle new types of malware during post-forensics investigations.\nRQ4: How can hybrid models combining heuristic and reinforcement learning approaches improve the reliability and speed of post-incident malware forensics investiga- tion? This question seeks to determine the feasibility and effectiveness of blending traditional heuristic-based methods with reinforcement learning techniques to create a robust framework for analysing post-incident malware more efficiently and with greater accuracy.\nOur research significantly enhances post-incident malware forensics investigations through reinforcement learning, offer- ing a robust framework for combating complex cyber threats. Our contributions include:\nRC1: We created a comprehensive malware analysis workflow diagram that incorporates a variety of malware analysis techniques. We designed the malware workflow diagram to examine and analyse malware from live mem- ory dumps using methods like static analysis, signature- based analysis, behavioural analysis, and machine learn- ing algorithms. This malware workflow diagram increases adaptability and robustness in malware analysis, improves overall information security, and aids in forensic investi- gations post-incident.\nRC2: We implemented a unified MDP model that com- bines several MDP subsections into one holistic overview. This model includes states, actions, rewards, and tran- sition probabilities, providing a structured approach for the RL agent to identify and isolate suspicious files. The unified MDP model offers a systematic method for malware analysis, enabling the RL agent to perform optimally by understanding the environment through a series of well-defined steps.\nRC3: We successfully developed sophisticated reinforce- ment learning Framework. This Framework outperform any existing automation or human expert in investigating malware infection requiring little time and resources and resulting in higher accuracy.\nRC4: We implemented an new approach using AWK module and Volatility 3 to extract specific data from memory dumps, which we then analyzed to identify active processes and potential malware. This approach improves malware investigations by providing detailed insights into the memory dump and facilitating the iden- tification of anomalies and compromise indicators.\nRC5: We Elaborated an important marlware forensics analysis dataset made out real-world malware scenarios."}, {"title": "E. Paper Outline", "content": "We organise this research paper as follows: The abstract summarises the study's primary focus on leveraging RL to expedite and improve post-incident forensic processes. As mentioned in Section I, the introduction sets the stage by emphasising the critical need for rapid and efficient malware investigations in light of the increasing prevalence of cyber threats. Following this, the literature review in Section II presents a thorough examination of existing malware analysis methods, underscoring the limitations of traditional approaches and highlighting the promise of RL. Furthermore, Section III and Section IV detail the research methodology development and implementation of the RL-based model, describing the design of the Markov decision-process (MDP) environments, the integration of reinforcement learning techniques, and the testing and evaluation of the RL agent in Section V. In addition, the results and discussion Section VI illustrate and describe the experimental findings, demonstrating the RL model's superior performance in terms of speed and accuracy compared to traditional methods and human forensics experts capabilities.\nMoreover, the conclusion Section VII encapsulates the key findings, reiterating the study's significant contributions to cy- bersecurity and malware forensics. It emphasises the potential of RL to revolutionise post-incident investigations, providing faster and more accurate results. Additionally, the research offers a comparative analysis, highlighting the advantages of RL over heuristic and signature-based methods. Notably, the hybrid approach integrating heuristic and RL methods shows promising results. Finally, the paper suggests some artificial intelligence techniques for future research, such as exploring advanced RL techniques and refining hybrid models, while emphasising the need for continuous learning and adaptation in RL models. The comprehensive references section supports the study, citing relevant literature on RL, malware analysis, and cybersecurity, thus providing a solid foundation for the research."}, {"title": "II. LITERATURE REVIEW AND BACKGROUND", "content": ""}, {"title": "A. Reinforcement Learning for Malware Analysis", "content": "Quertier et al., [1] research highlights the challenges of machine learning classifiers in identifying potential malware, especially when there is limited insight into the malware output. The study suggests using reinforcement learning with REINFORCE and DQN algorithms to test the effectiveness of EMBER and MalConv machine learning analysis on commer- cial antivirus solutions. The study found that REINFORCE has a higher evasion rate and better performance than DQN, espe- cially when tested against a commercial antivirus. However, a more comprehensive approach could have included training these models on a broader array of diverse models."}, {"title": "B. Deep RL for Malware Analysis", "content": "In 2019, Binxiang, Gang, and Ruoying [11], introduced a deep reinforcement learning-based technique for malware identification, aiming to address the vulnerabilities of tradi- tional signature-based and machine learning-based approaches. The research demonstrated that deep reinforcement learning outperformed traditional methods based on static signatures and demonstrated the ability to quickly adapt to the ever-changing landscape of malware. However, the study had limitations, including a lack of comprehensive details about the experimental design, datasets used, and evaluation metrics. Expanding the training dataset and incorporating domain- specific knowledge could improve malware analysis. Notably, expanding the training dataset's size and diversity, as suggested by Szegedy et al., [14], could significantly enhance the effec- tiveness of the malware investigation. Researchers like Silver"}, {"title": "C. RL-Based Attacks on Static Malware Detectors", "content": "Ebrahimi et al., [12] research aims to improve the effec- tiveness of static malware detectors in countering black-box cyberattacks. They propose using reinforcement learning (RL) to optimise the decision-making process of static malware detectors in the presence of black-box attacks. They create the Variational Actor-Critic for Discrete Adversarial Malware Generation (AMG-VAC) using discrete operations and an approximate sampling operator. They use RL to optimise the decision-making process, adjusting the neural network's weights based on the reward signal. In terms of accuracy, the RL-based AMG detector outperforms the original detector, particularly in the presence of black-box attacks. In addition, their results show that the RL-based AMG detector is much more accurate than the original detector when it comes to black-box attacks. However, RL in discrete action spaces may not align with all types of malware detectors, and its effective- ness depends on factors like the training dataset quality and the neural network's architecture."}, {"title": "D. Malware Analysis Using Intelligent Feature Selection", "content": "Fang et al., [10] developed a specialised architectural solution called DQFSA to address the shortcomings of traditional malware classification methods. The architecture uses deep Q- learning to identify crucial features, reducing human interven- tion and allowing data selection and analysis across various cases and data volumes. The methodology incorporates multi- view features, focusing on high classification accuracy during the validation phase. The key difference lies in exposing an Al agent to sample features with minimal human intervention. Experiments validated the DQFSA architecture by comparing its performance against various classifiers and related works."}, {"title": "E. Modern Incident Response Enhanced by AI", "content": "Dunsin et al., [18] present a study on the application of artificial intelligence (AI) and machine learning (ML) in digital forensics, focusing on enhancing malware investigation through innovative methodologies. The paper highlights the integration of AI and ML techniques to improve investigative precision and efficacy in digital forensics, leveraging advanced computational models to automate the investigation and analy- sis of cyber threats. Another focus is memory forensics, which focuses on machine learning algorithms to analyse memory dumps and malware, enhancing the reliability of forensic investigations by extracting and analysing multiple artefacts.\nThe study highlights the advantages of AI and ML in digital forensics, such as data mining techniques, reinforcement learn- ing, and Markov decision process (MDP) for automated mal- ware analysis. However, the study acknowledges challenges such as data validity, appropriate tools for memory dump retrieval, and adhering to ethical and legal standards. The study also proposes reinforcement learning, modelled as a Markov decision process (MDP), as a method for investigating malware in digital forensics. The MDP framework allows for systematic evaluation of different states and actions, facili- tating the development of effective RL models for malware investigation"}, {"title": "F. ML and Knowledge Based System for Malware Analysis", "content": "Piplai et al., [7]) propose a framework that uses reinforcement learning and open-source knowledge to enhance malware anal- ysis. The framework consists of two components: reinforce- ment learning for malware analysis and knowledge from open sources detailing past cyberattacks. The research experiments create 99 distinct processes during data collection, enabling the model to identify new malware. In similar research, Gallant [19] conducted a malware investigation experiment in which the researchers trained and employed multiple machine learn- ing algorithms, including Perceptrons, and rigorously tested their performance. However, Piplai et al., [7] leave unspecified aspects, such as determining which prior knowledge is relevant for new malware analysis and whether prior knowledge might introduce biases from previous cases. Despite these concerns, the framework's incorporation of prior knowledge remains valuable, guiding new models with increased efficiency and accuracy."}, {"title": "G. RL for Malware Investigations", "content": "Reinforcement learning in malware forensics investigations involves an agent that seeks to optimise cumulative rewards by effectively managing the trade-off between exploration and exploitation. Our research's primary focus is on using reinforcement learning techniques to automate the process of conducting post-incident malware forensics investigations following a security incident. The agent performs actions within the environment, leading to changes in its state. The"}, {"title": "III. RESEARCH METHODOLOGY", "content": ""}, {"title": "A. Experimental Setup and Dataset Generation", "content": "To implement and validate our proposed reinforcement learn- ing malware investigation framework, we took a systematic approach to creating a comprehensive malware dataset us- ing the London Metropolitan University Digital Forensics Laboratory. First, we established thirteen virtual machines within an isolated network to ensure a secure and controlled environment for our experiments and the eduroam network. This setup was critical to preventing the spread of unintended malware and maintaining the integrity of our data collection process. Next, we uploaded 13 different ISO files, each rep- resenting various versions of the Windows operating system. This diverse selection of operating systems allowed us to test our framework across a broad spectrum of environments.\nNext, we introduced a variety of malware to infect each of these operating systems. We specifically chose each malware type to represent different attack vectors and behaviours, providing a robust challenge for our investigation framework. For each ISO file installed on the virtual machine, we took an initial snapshot of the environment and saved the live memory dump. Following this, we infected the virtual machine with the chosen malware and took another snapshot. This process resulted in pairs of snapshots, one uninfected and one infected, for each operating system. This methodology yielded 13 RAM files from the uninfected environments and another 13 from the infected ones. To analyse these files, we used the Volatility framework, a powerful tool for memory forensics. We man- ually examined both the infected and uninfected RAM files, which, as a result, enabled us to identify significant changes and behaviours indicative of malware presence. To ensure replication and verification of our procedures, we diligently documented each stage of the analysis. This documentation was critical for maintaining the integrity of our research, as well as for future reference. Finally, based on our analysis of the 26 files, we created a detailed malware workflow diagram. This diagram mapped out the typical processes and behaviours associated with the malware samples, providing a visual and analytical aid for understanding how different malware affects system memory. This workflow diagram is a crucial component of our proposed reinforcement learn- ing post-incident malware forensics investigation framework, serving as a foundational element for training and validating our model."}, {"title": "B. Malware Workflow Diagram Creation", "content": "The research methodology extends from our comprehensive experimental setup and dataset generation process to the development of a detailed malware analysis workflow diagram. This diagram is integral to our rein- forcement learning malware investigation framework, encom- passing various malware analysis techniques, including data collection, examination, and analysis. Our dataset, comprising live memory dumps from 13 different versions of Windows operating systems-both infected and uninfected provides the foundation for this workflow. We examined these dumps to detect anomalies, indicators of compromise, and potential malware artefacts by using the Volatility framework for mem- ory forensics. The analysis phase incorporates a diverse array of techniques such as static analysis, signature-based analysis, behavioural analysis, and machine learning algorithms. The resulting malware workflow analysis diagram not only maps out the typical processes and behaviours associated with our chosen malware samples, but it also serves as a crucial tool for improving information security and post-incident mal- ware forensic investigations. Our structured approach rigorously trains and validates our reinforcement learning model, strengthening our malware investigation capabilities."}, {"title": "C. Markov Decision Process (MDP) Formulation", "content": "In our problem, the Malware Investigation associated Markov Decision Process (MDP) provides a mathematical framework for modelling decision-making in situations where outcomes are partly random and partly under the control of a decision- maker. Our MDP is defined by the following components:\nStates (S): In this case, |S| = 67 states.\nActions (A): In this case, |A| = 10 actions.\nTransition Function (T): $T (s, a, s')$ represents the prob- ability to transition from state s to state s' under action \u03b1.\nReward Function (R): R(s, a) represents the immediate reward received after performing action a in state s.\nDiscount Factor (\u03b3): A factor y \u2208 [0, 1] that discounts future rewards.\nStep 1: Define States and Actions\nLet S = {$S_0, S_1, S_2, . . ., S_{66}$} where each s represents a unique state in the malware investigation model process."}, {"title": "D. Leveraging Reinforcement Learning", "content": "In the context of our proposed Reinforcement Learning (RL), the agent learns the optimal policy $\\pi^*$ by interacting with the three proposed MDP environments. A common algorithm used is Q-learning, which updates the Q-values based on the Bellman equation.\n1) Value Function and Policy: The value function for policy \u03c0 is given by:\n$V^{\\pi}(s) = \\sum_{a} \\pi (a | s) \\sum_{s'} T (s, a, s') [R(s, a) + \\gamma V^{\\pi} (s')]$\n\u2022 $V^{\\pi}(s)$: Expected cumulative reward starting from state s and following policy \u03c0.\n$\\pi(\u03b1 | s)$: Probability of taking action a given state s under policy \u03c0.\n2) Reinforcement Learning (RL) with Q-Learning: subsectionLeveraging Reinforcement Learning In the context of our proposed Reinforcement Learning (RL), the agent learns the optimal policy $\\pi^*$ by interacting with the three proposed MDP environments. A common algorithm used is Q-learning, which updates the Q-values based on the Bellman equation.\nThe Q-learning update rule is given by:\n$Q(s, a) \\leftarrow Q(s, a) + \\alpha[r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]$\nWhere:\n\u03b1 is the learning rate.\nr is the reward received after taking action a in state s.\ns' is the next state resulting from action a.\n$\\max_{a'} Q(s, a)$ is the maximum estimated future reward from state s'.\nUsing the specifications as a result of the workflow diagram:\nWe have 67 states and 10 actions.\nThe transition and reward functions would be defined based on the specific malware identification tasks.\n3) Q-Learning Update Rule:\n$Q(s, a) \\leftarrow Q(s, a) + \\alpha[r + \\gamma \\max_{a'} Q(s, a) - Q(s, a)]$\nStep 1: Initialize Q-Table\nInitialize Q(s, a) for all s \u2208 S and a \u2208 A to some arbitrary values (e.g., 0).\nStep 2: Choose Learning Rate \u03b1 and Discount Factor \u03b3"}, {"title": "E. Setting the Parameters for MDPs", "content": "The Reinforcement Learning Post-Incident Malware Investiga- tive Model uses the malware workflow diagram to define parameters for action and state spaces. The agent uses live memory dumps to analyse and identify malware artefacts, with 109 distinct actions within a defined environment. The state array aligns with the malware workflow diagram, encompass- ing 67 unique states. To achieve this alignment, we follow steps such as installing WinPmem, obtaining live memory images, understanding the operating system, extracting process information, listing DLLs, tracking open handles, collecting network data, figuring out registry hives, listing keys, dupli- cating processes into executable files, and sending them to Known Files Filters Servers."}, {"title": "F. The Motivation behind Implementing Q-Learning", "content": "The proposed Reinforcement Learning Post-Incident Mal- ware Investigation Framework uses Q-learning, an off-policy, model-free algorithm. We use it because it employs a value- based approach to determine the optimal actions based on the current state. The algorithm learns the relative value of different states and actions through experiential knowledge without relying on explicit transition or reward functions. This approach is suited for the proposed RL model for analysing malware artefacts. In this context, 'Q' signifies quality, representing the action's value in terms of optimising future rewards. On the other hand, model-based algorithms employ transition and reward functions to estimate the optimal policy and construct a model, whereas model-free algorithms acquire knowledge about action outcomes experientially, with- out explicit transition or reward functions. In our proposed implementation, we opt for the value-based approach, which entails training the value function in order for the agent to learn the relative value of different states and take actions accordingly. Conversely, policy-based methods directly train the policy to determine the appropriate action for a given state. On the other hand, in off-policy methods, the algorithm assesses and improves a policy that is different from the action execution policy. In contrast, on-policy algorithms evaluate and refine the same policy employed for action execution."}, {"title": "G. Q-Learning Terminologies", "content": "In the following sections, we will implement the proposed Reinforcement Learning Post-Incident Malware Investigation Model. The following terminologies are defined and explained in brief. An Environment is the space or world in which the agent operates and takes actions. An Agent is the entity that learns and makes decisions by interacting with the environ- ment. States (s) signify the agent's present location within the environment. An Action (a) is the set of all possible moves or decisions the agent can make in the environment. Every action the agent takes results in either a positive reward or a penalty. Episodes mark the end of a stage, indicating that the agent cannot perform further actions. This occurs when the agent either accomplishes its objective or faces failure.\nFor each state-action pair, the agent uses a Q-Table to manage or store Q-values. We use Temporal Differences (TD) to estimate the expected value by comparing the current state and action with the previous state and action. The learning rate is a parameter that determines how much new information overrides old information. A policy is a strategy or mapping from states to actions that defines an agent's behaviours. The Discount Factor is a parameter that determines the importance of future rewards. The Bellman Equation is a fundamental equation in Q-Learning that expresses the relationship between the Q-value of a state-action pair and the Q-values of the subsequent state-action pairs. The Epsilon-Greedy Strategy is a method for balancing exploration and exploitation."}, {"title": "H. Q-Table and Q-Function", "content": "As previously mentioned, the Q-table is one of the key com- ponents that facilitate the agent's decision-making. It guides the agent in selecting the most favourable action based on expected rewards within the provided environments. The Q- learning algorithm updates the values of a Q-table, which essentially functions as a structured repository encapsulating sets of actions and states. However, defining the state and action spaces is a crucial preliminary step in effectively setting up the Q-table, a task that the malware workflow diagram facilitates. Furthermore, the Q-function plays a central role, using the Bellman equation and considering the state(s) and action (a) as its input. This equation significantly streamlines the calculation of both state values and state-action values."}, {"title": "I. Subsections of the Markov Decision Process Model", "content": "The proposed subsection of the Markov Decision Process (MDP) represents a segment of the comprehensive and unified MDP model. Each subsection of the Markov Decision Process (MDP) model contains states, actions, rewards, and a transition probability function. These subsections are crucial components of the unified MDP that provide an agent with the capabilities of identifying and isolating suspicious portable executable files for further investigations. This approach sets a benchmark for processes such as recognising process identities, analysing process DLLs and handles, examining network artefacts, and checking for evidence of code injection."}, {"title": "J. The Unified Markov Decision Process", "content": "The Unified Markov Decision Process (MDP), as depicted consolidates all the subsections of MDPs into a singular process, providing a comprehensive perspective. This synthesis allows the agent to effectively navigate the environment and make informed decisions regarding malware investigation."}, {"title": "K. The Proposed RL Post-Incident Malware Investigation Framework", "content": "The Reinforcement Learning Post-Incident Malware Investiga- tion Framework consists of six core fundamental components, as seen in Figure 9: the data collection, the mapping of the workflow diagram, defining actions, and state spaces. We formulated the Markov Decision Process (MDP) Model's subsections as well as the proposed RL Post-Incident Mal- ware Investigation Model. We divided the environment into three sections: creating dependencies and gym environments, importing required libraries, implementing the training data, MDP solver, and continuous learning and adaptation. The workflow diagram outlines a comprehensive approach to pro- cess information gathering, starting with the collection of detailed information about various processes. We use the AWK module to extract features from the identified processes. An essential aspect of this workflow is listing DLLs, which helps keep track of loaded DLLs for each process. Additionally, tracking open handles is crucial for monitoring the open handles associated with each process. Another key focus is collecting network data, which ensures that all relevant network-related information is collected. Registry hive anal- ysis involves identifying the registry hives and listing their keys. To determine if processes are malicious or benign, we duplicate them into executable files and send them to VirusTotal via API. Furthermore, we duplicate the addressable memory to perform a grep search using specific keywords.\nMapping the Workflow Diagram comprises three subsections: data collection, data examination, and analysis techniques. Data collection involves creating a live dump of Windows operating systems. The data examination phase analyses the collected data to identify anomalies, compromise indicators, and potential malware artefacts. Finally, the analysis tech- niques phase focuses on identifying malware infections using techniques such as static analysis, signature-based analysis, behavioural analysis, and machine learning algorithms. We designed the state spaces to align with the malware work- flow diagram, encompassing 67 unique states. Based on this workflow, the action and state spaces are defined, with actions ranging from three to ten, exposing the agent to 109 distinct actions within a defined environment.\nWe formulate the subsections of the Markov Decision Process (MDP) model based on the states, actions, rewards, and transition probability function. These subsections are crucial for the MDP model, enabling the agent to identify and isolate suspicious portable executable files for further investigation. The RL agent starts from state zero, takes actions, and receives rewards or penalties based on the outcomes. This iterative learning process continues until the agent optimally identifies suspicious executables by accurately classifying files as ma- licious or benign. The agent maximises cumulative rewards by balancing exploration and exploitation. The agent's actions result in state transitions, and the goal is to improve the agent's performance over time."}, {"title": "L. The Proposed RL Post-Incident Malware Investigation Model", "content": "In the proposed Reinforcement Learning Post-Incident Mal- ware Investigation Model, the 'Agent' is the decision-maker that interacts with the environment. The 'Environment' is the live memory dump in which the agent interacts. It provides the agent with state and reward data. The 'State' (s) is a represen- tation of the agent's current situation in the environment. The \u2018Action' (a) is the set of all possible moves the agent can take. The environment provides feedback, known as the 'Reward' (r), to evaluate the agent's actions. The agent uses the 'Policy' as a strategy to decide the next action based on the current state. The 'Value Function' (V(s)) is a function that estimates the expected cumulative reward from a given state following a particular policy. The 'Q-Function' (Q("}]}