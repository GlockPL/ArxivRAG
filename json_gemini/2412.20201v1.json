{"title": "Injecting Explainability and Lightweight Design into Weakly Supervised Video Anomaly Detection Systems", "authors": ["Wen-Dong Jiang", "Chih-Yung Chang", "Hsiang-Chuan Chang", "Ji-Yuan Chen", "Diptendu Sinha Roy"], "abstract": "Weakly Supervised Monitoring Anomaly Detection (WSMAD) employs weak supervision learning to detect anomalies, crucial for monitoring in smart city. Existing multimodal methods often fail to meet real-time monitoring, and interpretability needs on edge devices due to their complexity. This paper introduces a Two-stage Cross-modal Video Anomaly Detection System named TCVADS. Using knowledge distillation and cross-modal contrastive learning, TCVADS enables efficient, accurate, and explainable anomaly recognition on edge devices. The system operates in two stages: coarse-grained rapid classification and fine-grained detailed analysis. In the first stage, TCVADS extracts features from video frames, inputting them into a time series analysis module that serves as the teacher model. Knowledge distillation then transfers insights to a simplified convolutional network (student model) for binary classification. The second stage, triggered upon anomaly detection, employs a fine-grained multi-class classification model. It uses CLIP for cross-modal contrastive learning with text and images, achieving more refined and interpretable classification through specially designed triplet textual relationships. Experimental results demonstrate TCVADS significantly outperforms existing technologies in model performance, detection efficiency, and interpretability, contributing to monitoring applications in smart cities.", "sections": [{"title": "I. INTRODUCTION", "content": "Weakly supervised Monitoring anomaly detection (WSMAD) has emerged as a research hotspot in the realm of IoT-based computer vision. Its growing importance is driven by its wide-ranging potential applications in intelligent surveillance [1], smart city [2], and traffic monitoring [3].\nWSMAD employs weakly supervised learning methods to detect anomalous behaviors in videos, enabling models to be trained without the need for explicitly labeled anomalies.\nPrevious WSMAD research typically relied on a single visual modality [6] [38] to detect abnormal behaviors. However, visual cues alone may fail to accurately capture potential anomalies. For instance, in a school environment, verbal abuse by a teacher towards a student is difficult to detect through images alone. To improve the accuracy of anomaly detection systems, current WSMAD research frameworks often combine multi-modal information. This includes gathering data from images, audio, and other sources. For example, in smart monitoring systems [32] [27], data from sound sensors, environmental sensors, and even social media information is used to enhance the detection of abnormal behaviors. However, these methods rely heavily on external devices, which can create significant challenges during real-world deployment. These challenges include high equipment procurement costs, difficulties in sensor deployment, and substantial data transmission delays. Additionally, as the number of modalities increases, the issue of interpretability also becomes a complex problem [31] [33] [42] [43].\nIn recent years, some researchers [34] [4] have tried to use cross-modal methods to solve challenges in single-image anomaly detection. This method typically used information from one modality to infer or supplement information from another modality, reducing reliance on external devices. However, using cross-modality for anomaly detection in video content still faced two major challenges. The first challenge was the high computational complexity associated with cross-modal methods. These methods required processing data from different modalities and establishing cross-modal connections. This extensive computational workload reduced the real-time performance of the system, making it challenging to make quick judgments in practical applications. The second challenge was the lack of in-depth cross-modal analysis. Existing cross-modal detection methods often remained at a shallow level of data fusion, making it difficult to fully explore the connections between different modalities. This limitation led to a model with a limited ability to detect abnormal behavior effectively.\nIn terms of immediacy, the current WSMAD system framework [24] [25] [26] typically used pre-trained visual models (such as C3D [5], I3D [6], ViT [7]) to extract video frame features. These features were then fed into temporal models like Transformers and GCNs for local and global analysis. Finally, the model training was conducted based on binary or multi-class classifiers using multi-instance learning (MIL) to detect abnormal event confidence. However, the high"}, {"title": "II. RELATED WORK", "content": "This chapter presents a review of some relevant studies in Video Anomaly Detection. These studies are categorized into two groups: Weakly supervised video anomaly detection and using visual language pre-training model to detect anomalies."}, {"title": "A. Weakly supervised video anomaly detection", "content": "In WSMAD, Sultani et al. [5] and Hasan et al. [6] first proposed a deep multi-instance learning model for anomaly detection. The study treated the frame of the video as a package, in which multiple paragraphs were regarded as instances to enhance the detection performance of the model. Subsequent research was based on self-attention or the Transformer or GCN model to model the temporal and contextual relationships in video content. Zhong et al. [11] proposed a GCN-based method for modeling feature similarity and temporal consistency between video segments. Tian et al. [12] used a self-attention network to capture the global temporal context of videos. Wu et al. [13] proposed a global and local attention module to capture temporal dependencies in videos to obtain more expressive embeddings. Ji and Lee [38] proposed OCSVM for anomaly detection. The above methods, only detect video frames were abnormal, which only provided a solution for a binary problem. While Wu et al. [14] proposed a fine-grained WSMAD method to distinguish different types of abnormal frames. Recently, the CLIP model has also attracted great attention in the VAD field. Based on the visual features of CLIP, Lv et al. [3] proposed a multi-instance learning framework called UMIL to learn unbiased anomaly features that improved WSMAD task performance. Joo et al. [4] used the visual features of CLIP to efficiently extract discriminative representations, long-term, and short-term temporal dependencies through temporal self-attention, and nominated interesting clips.\nAll the above methods were based on the classification paradigm and detected abnormal events by predicting the probability of abnormal frames. However, this classification paradigm did not fully exploit the semantic information of text labels. In addition, multi-modal fusion is still difficult in practical applications and many methods cannot fully exploit the semantic connections between text and visual features. Finally, although Transformer or GCN-based architectures are effective in modeling complex temporal relationships, they make the real-time performance of video anomaly detection challenging."}, {"title": "B. Visual language pre-training", "content": "Significant progress had been made in visual-language pre-training, which aimed to learn the semantic correspondence between vision and language by pre-training on large-scale data. As one of the most representative works, CLIP had demonstrated excellent performance in a series of visual-linguistic downstream tasks. For example, Zhou et al. [15] proposed an improved image classification method based on CLIP; Mokady et al. [16] made a breakthrough in image subtitle generation; Zhou et al. [17] applied the contrastive learning method to target detection; Yu et al. [18] made achievements in the field of scene text detection. Rao and Zhou et al. [19] conducted research on dense prediction.\nRecently, some follow-up work attempted to utilize such pre-trained models for applications in the video field. For example, Luo et al. [20] proposed CLIP4Clip, a method aimed at transferring the knowledge of CLIP models to the field of video-text retrieval; Wu et al. [21] proposed a simple but powerful baseline that efficiently adapted pre-trained image-based visual-language models to leverage their powerful capabilities in general video understanding and applied them to video-level downstream weakly supervised video anomaly detection. Zhou et al. [36] proposed an effective zero-shot anomaly defect detection method using a pre-trained CLIP model. This approach allowed anomaly detection without the need for specific training data by leveraging CLIP's ability to match visual features with textual descriptions. Wu et al. [37] introduced a CLIP-based three-branch architecture to address classification and localization in weakly supervised anomaly detection. This architecture was designed to simultaneously tackle the problems of coarse-grained detection, fine-grained detection, and localization in weakly supervised violence detection. However, most of these CLIP-based methods are designed using only text or image encoders, and the prompt words in the text encoder are mostly designed manually or through simple prompt engineering, making it difficult to fully utilize the advantages of contrastive learning."}, {"title": "III. ASSUMPTIONS AND PROBLEM FORMULATION", "content": "This section introduces the assumptions and problem statements of this study. Given a video V with a duration of t, this paper aims to identify whether or not the anomaly in V. Let Vi\u2208V and Vi \u2208 V denote non-anomaly and anomaly videos, respectively. Let video V = {\u03a61, \u03a62, ..., \u03a6n} be divided into n equal-length segments, each containing non-anomaly, anomaly or a combination of both. Each segment \u03a6i = Vi \u222a Vimight comprise both Vi and Vi behaviors.\nLet C = {c1, c2, ..., cm, \u0109} denote all possible classes, where ci denotes the m anomaly classes for 1 \u2264 i \u2264 m and \u0109 denotes the non-anomaly class. Let L = {l1, l2, ..., lm, \u00ce} be the set of labels, where li is the corresponding label of class ci for 1 \u2264 i \u2264 m and \u00ce denote the label of non-anomaly class \u0109. This study assumes that each Vi falls into a specific anomaly category cj. Consider an anomaly detection mechanism M which aims to detect the occurrence of an anomaly event. Let RM be a segment of video which is detected as the anomaly event by applying mechanism M. That is, RM can be represented as RM = {RR,RM,RM RM,RM}.\nLet \u03b4i be a Boolean variable that indicates whether or not a given video segment Ri actually contains an anomaly event. Let \u03b8 be the prediction threshold.\nThat is\n\u03b4i = { (1, \u0158i \u2208 Cj), (0, \u0158i \u2208 \u0109) }.\nLet \u03b4M(\u03b8) denote whether or not the result of video segment Ri predicted by M contains anomaly event. Let probability Pi denote the prediction score of Ri \u2208 RM. That is \u03b4M(\u03b8) = { (1, if Pi \u2265 \u03b8), (0, otherwise) }.\nLet TPi, TNi, FPiand FNi represent True Positive, True Negative, False Negative and False Positive respectively, of the prediction result of input Ri by applying mechanism M. The values of TPi, FPi ,TNi and FNi can be calculated using TPi = \u03b4i \u00d7 \u03b4M(\u03b8), TNi = (1 \u2212 \u03b4i) \u00d7 (1 \u2212 \u03b4M(\u03b8) ), FPi = (1 \u2212 \u03b4i) \u00d7 (1 \u2013 \u03b4M(\u03b8) )and FNi = \u03b4i \u00d7 (1 \u2212 \u03b4M(\u03b8) ), respectively.\nLet TP, TN, FP and FN represent the cumulative True Positive, True Negative, False Positive, and False Negative, respectively, of the prediction results by applying mechanism M to all segments Ri \u2208 R, for 1 \u2264 i \u2264 n. The value of TP, TN, FP, and FN can be calculated by TP = \u03a3i=1TPi, TN = \u03a3i=1TNi, FP = \u03a3i=1FPi and FN = \u03a3i=1FNi , respectively.\nLet PM, RM denote the Precision and Recall of the predictions by applying mechanism M to predict a given video V. The values of PM, RM can be calculated by PM = TP / (TP + FP) and RM = TP / (TP + FN)."}, {"title": "A. Average Precision (AP)", "content": "Let \u03b8j denote the j-th prediction threshold, let APM denote the Average Precision of mechanism M. The APM can be calculated by Exp. (1) :\nAPM =  \u2211j=1n-1 (RM(\u03b8j+1) \u2013 RM(\u03b8j)) \u00d7 pM(\u03b8j+1)\n= \u2211j=1n-1 ( TP(\u03b8j+1) / (TP(\u03b8j+1) + FN(\u03b8j+1)) - TP(\u03b8j) / (TP(\u03b8j) + FN(\u03b8j)) ) \u00d7  ( TP(\u03b8j+1) / (TP(\u03b8j+1) + FP(\u03b8j+1)) )\n= \u2211j=1n-1 ( \u03b4i \u00d7 \u03b4M(\u03b8j+1) / \u2211i=1n \u03b4i - \u2211i=1n \u03b4i \u00d7 (1 \u2013 \u03b4M(\u03b8j+1)) / \u2211i=1n (1 \u2013 \u03b4i) \u00d7 \u03b4M(\u03b8j+1) ) \u00d7  ( \u2211i=1n \u03b4i \u00d7 \u03b4M(\u03b8j+1) / \u2211i=1n(1 \u2013 \u03b4i) \u00d7 \u03b4M(\u03b8j+1) )   (1)\nSimilar to previous works [14], [21], [38], the first objective of this paper is to develop mechanism Mbest that satisfies Exp. (2) :\nFirst Objective in XD-Violence:\nMbest = arg Max(APM)   (2)"}, {"title": "B. Area Under the Curve (AUC) and AUC-ano", "content": "Let AM denote the AUC of mechanism M. The value of AM can be calculated by Exp. (3)\nAM = \u222bTPR(\u03b8) d(FPR(\u03b8) = \u222b(TP(\u03b8) / (TP(\u03b8) + FN(\u03b8)) d(FP(\u03b8) / (FP(\u03b8) + TN(\u03b8))\n= \u2211i=1n \u03b4i\u00d7 \u03b4M(\u03b8) / \u2211i=1n \u03b4i * \u2211i=1n(1 \u2013 \u03b4i) \u00d7 \u03b4M(\u03b8) / \u2211i=1n (1 \u2013 \u03b4i)\n(3)\nSimilar to previous works [14", "21": [36], "38": "the second objective of this paper is to develop mechanism Mbest that satisfies Exp. (4):\nSecond Objective in UCF-Crime:\nMbest = arg Max(AM)   (4)\nMEM\nLet \u03b4i denote the indicator variable for the j-th instance, where \u03b4j = 1 indicates an anomaly, and \u03b4j = 0 indicates a normal instance. Let \u03a0(\u03b4M > \u03b4M) denote an indicator function that equals 1 if the anomaly score of instances i is greater than that of instance j, and 0 otherwise. Let A"}]}