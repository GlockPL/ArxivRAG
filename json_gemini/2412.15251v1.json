{"title": "AgentPS: Agentic Process Supervision for Multi-modal Content Quality Assurance through Multi-round QA", "authors": ["Gorden Liu", "Yu Sun", "Ruixiao Sun", "Xin Dong", "Hongyu Xiong"], "abstract": "The advanced processing and reasoning capabilities of multimodal large language models (MLLMs) have driven substantial progress in vision-language (VL) understanding tasks. However, while effective for tasks governed by straightforward logic, MLLMs often encounter challenges when reasoning over complex, interdependent logic structures. To address this limitation, we introduce AgentPS, a novel framework that integrates Agentic Process Supervision into MLLMs via multi-round question answering during fine-tuning. AgentPS demonstrates significant performance improvements over baseline MLLMs on proprietary TikTok datasets, due to its integration of process supervision and structured sequential reasoning. Furthermore, we show that replacing human-annotated labels with LLM-generated labels retains much of the performance gain, highlighting the framework's practical scalability in industrial applications. These results position AgentPS as a highly effective and efficient architecture for multimodal classification tasks. Its adaptability and scalability, especially when enhanced by automated annotation generation, make it a powerful tool for handling large-scale, real-world challenges.", "sections": [{"title": "1 Introduction", "content": "The rapid advancement of large language models (LLMs) and multimodal large language models (MLLMs) (Achiam et al., 2023; Team et al., 2023) has demonstrated their impressive capabilities across a variety of applications, such as visual question answering (Li et al., 2024; Wang et al., 2024) and contextual comprehension (Li et al., 2021; Wang et al., 2024). These models excel at identifying fine-grained features in visual content and text, as well as interpreting complex interactions across modalities, enhancing their generalizability for diverse vision-language (VL) tasks.\nDespite their strengths, the performance of MLLMs in highly specialized or sensitive domains remains an open area of exploration. While these models effectively extract detailed features, standard end-to-end supervised fine-tuning (SFT) approaches often struggle with the nuanced demands of domain-specific tasks. For example, assessing whether a video contains unoriginal content or determining its appropriateness for social media users presents unique challenges. Unlike traditional object detection tasks, where MLLMs demonstrate strong performance, these tasks require sophisticated reasoning and the application of intricate, context-dependent policies. The complexity of these policies, coupled with the difficulty of accurately interpreting them, poses significant challenges even for human annotators.\nFor example, determining whether a TikTok post (video or album) is unoriginal involves applying nuanced criteria. A straightforward movie clip would be classified as \"unoriginal,\" whereas the same clip paired with commentary could be considered \"original.\" Crowd-sourced annotators must adhere to a set of predefined questions, such as \"Does the content involve copyrighted material?\" \"Are there user-generated edits?\" and \"Do these edits add meaningful relevance?\" These tasks underscore the need for intricate logic and structured reasoning.\nIn this paper, we present a novel approach to enhance the content understanding capabilities of multimodal large language models (MLLMs) for domain-specific tasks, addressing the limitations of traditional end-to-end supervised fine-tuning (SFT). Our work introduces two key innovations:\nAgentPS Framework. We introduce AgentPS, an architecture that integrates process supervision through an agentic framework. By utilizing intermediate process labels to sequentially guide the MLLM's decision-making, AgentPS better aligns"}, {"title": "2 Methodology", "content": "2.1 Agentic Process Supervision (AgentPS)\nAs illustrated in Figure 1, we introduce Agentic Process Supervision (AgentPS), a novel framework that integrates multi-round question prompts, with corresponding answers serving as process labels, to provide intermediate supervision during MLLM fine-tuning. This approach is designed to significantly enhance the reasoning capabilities of MLLMs and improve their accuracy in generating precise final predictions.\nTo effectively process multimodal inputs, the MLLM comprises three key components: a vision encoder for handling images or video frames, a vision-language (VL) modality alignment projector, and a language model that seamlessly integrates both visual and textual tokens:\nMLLM Architecture. For each video frame or album image, the input \\( X_f \\) is first processed by a vision encoder \\( g(\\cdot; \\theta) \\) to extract its visual features. These features then passed through a projector module \\( p(\\cdot; \\theta) \\), typically implemented as a two-layer MLP, to map the visual features into a shared rep-"}, {"title": null, "content": "resentation space for visual and textual modalities:\n\\[ Z_f = g(X_f; \\theta) \\] (1)\n\\[ H_f = p(Z_f; \\theta) \\] (2)\nHere, \\( Z_f \\) captures the essential visual features extracted from the image or frame, while \\( H_f \\) transforms these visual features into a modality-aligned representation suitable for integration with textual tokens.\nThe language model \\( Lm(\\cdot; \\theta) \\) processes a token sequence of length L, consisting of visual tokens \\( H_f \\) and text tokens \\( X_t \\) (e.g., the title or description of a video or album). Typically, the language model utilizes a decoder-only transformer architecture. This architecture accepts the multimodal token sequence as input and generates a sequence of hidden representations H, where each representation corresponds to an input token, preserving the original sequence length.\n\\[ H = Lm([H_f, X_t]; \\theta) \\] (3)\nAgentPS and Classification Layers. During supervised fine-tuning (SFT), N rounds of intermediate question-answer (QA) pairs are introduced as additional inputs to provide process supervision. These N-round QA pairs are inserted after the standard text inputs, such as the title and description of the video or album, and precede the final question, which targets the ultimate task-specific output. Each intermediate question i concludes with an <ans> token. The hidden state corresponding to this token \\( H_{\\text{ans}^i} \\) is processed by an MLP \\( f_{\\theta_1^i}(\\cdot; \\theta_1) \\). The MLP projects \\( H_{\\text{ans}^i} \\) \u2208 \\( \\mathbb{R}^d \\) into a predicted answer space \\( \\hat{y}^i \\) \u2208 \\( \\mathbb{R}^{|Y^i|} \\):\n\\[ \\hat{y}^i = f_{\\theta_1^i}(H_{\\text{ans}^i}; \\theta_1) \\] (4)\nwhere \\( Y^i \\) denotes the set of possible target classes for the corresponding question i. Finally, the hidden representation of the sequence's last token is passed through another MLP, which produces the prediction for the final answer.\nThe core intuition behind AgentPS is twofold: (1) Task-Specific Optimization: By associating each intermediate question with its corresponding representation, the framework enables the model to capture the unique semantics of each sub-task effectively. (2) Chain-of-Thought Reasoning in Decoder-Only Architectures: The framework leverages the sequential processing nature of decoder-only architectures, allowing the model to learn dependencies between earlier and later questions. This approach moves beyond treating questions as independent or parallel entities, enabling the model to develop a coherent reasoning process that reflects the interdependencies inherent in complex tasks."}, {"title": "2.2 Process Annotation through LLM", "content": "By incorporating intermediate process labels into the AgentPS framework, the approach requires additional human annotation resources beyond final label annotations. This increased demand for labeling manpower poses a significant challenge for the widespread adoption of the technique across diverse tasks. Moreover, the scalability of AgentPS for SFT, especially for MLLM with extremely large parameter sizes, becomes constrained by the intensive resource requirements.\nTo address this limitation, we propose using LLMs, either internally developed or accessible via APIs, as a substitute for human-generated process labels. LLMs can efficiently generate process labels, and since each intermediate question is designed to be clear and straightforward, the likelihood of hallucination is significantly reduced compared to directly annotating final labels. By automating process labeling with LLMs, AgentPS becomes significantly more scalable and applicable to a broader range of tasks. This approach allows AgentPS's seamless integration into the SFT of MLLMs at scale for dynamic and complex tasks, such as evaluating content quality or originality, on large platforms like TikTok, where efficiency and accuracy are essential for real-world applications."}, {"title": "3 Experiments and Results", "content": "3.1 Experimental Setup\n3.1.1 Dataset\nWe evaluate the effectiveness of our proposed framework, AgentPS, on TikTok's in-house unoriginal content classification (UCC) task. UCC involves classifying album posts as either OC (Original Content), where most content is likely created by the publisher, or UC (Unoriginal Content), where most content is likely copied from other sources.\nThe experimental dataset comprises 158K labeled album posts, with 150K samples in the training set and 8K samples in the test set. Each album post includes one or more images and accompanying text, such as titles and hashtags."}, {"title": "3.1.2 Model Details", "content": "For MLLM, we use the LLaVA-OneVision model (Li et al., 2024) as the VL backbone, selecting its 0.5B and 7B versions for their balance between computational efficiency and strong multimodal reasoning capabilities. For SFT, we adopt LORA (Hu et al., 2021) with default parameters (rank = 128, alpha = 256) to optimize training efficiency. We trained the model with 16 A100-SXM-80GB GPU. The typical 5-epoch fine-tuning and evaluation process takes 18 hours for 7b version and 10 hours for the 0.5b version. The fine-tuning achieves optimal performance after three epochs of training.\nTo process multiple images, all image representations are converted to the 3d-Array format and uniformly resized to a shape of 360x636. To align albums with different number of images in it to a constant number of 16, uniform sampling is applied when the number of images exceed 16. When we do uniform sampling, we preserve the original order of images. If the number of images is less than 16, we repeat the images cyclically until there are exactly 16.\nGiven the model's input length constraint of 256 tokens, album textual descriptions are tokenized and truncated to 196 tokens. The left padding is applied to maintain uniform input lengths, ensuring compatibility with the model architecture. Additionally, a fixed 60-token prompt for AgentPS is appended to the input sequence, facilitating structured process supervision.\nTo assess model performance, we primarily evaluate recall at various precision thresholds, alongside the F1 score where applicable. These metrics provide a comprehensive view of the model's effectiveness in classifying UC or OC while maintaining stringent precision requirements."}, {"title": "3.1.3 LLM for Process Label Annotation", "content": "The annotation process relies on VL input, which can be processed through non-open-source APIs (like GPT-4v (Achiam et al., 2023) and GPT-40 (Achiam et al., 2023)) or fine-tuned open-source models (such as LLaVA, and LLaMA). In this experiment, we selected the GPT-40 API for annotation due to its advanced capabilities in handling"}, {"title": "3.2 Key Results and Analyses of AgentPS", "content": "We evaluate the effectiveness of AgentPS on the UCC task, as shown in Table 1. SigLIP (Zhai et al., 2023), the online baseline model, has 370M parameters. All other models in the experiment are built upon the enhanced vision-language backbone, LLaVA-OV, and are tested with two different configurations of the large language model: 0.5B and 7B parameters. Three model variants are assessed: the vanilla model, the multitask model, and the AgentPS model. The vanilla model relies solely on final labels and does not incorporate intermediate process supervision. The multitask model serves as an ablation study to evaluate the impact of the sequential structure. The AgentPS model integrates the full framework of intermediate process supervision.\nAs shown in Table 1, LLaVA-OV AgentPS achieves significant offline recall improvements across various precision thresholds. Notably, LLaVA-OV-0.5B with AgentPS outperforms not only its vanilla counterpart but also the larger 7B vanilla model. These results highlight the efficacy of AgentPS, which introduces intermediate supervision by extracting embeddings for five predefined questions in addition to the final originality decision. By aligning the model's reasoning process with human-defined logic, AgentPS enhances classification accuracy and optimizes parameter utilization, demonstrating its ability to improve the MLLM's performance on UCC tasks.\nTo further validate the role of the sequential structure within the AgentPS framework, we devel-"}, {"title": "3.3 Process Label Generation through LLM", "content": "3.3.1 Evaluation of LLM generated labels\nTo evaluate the quality of annotations generated by GPT-40, we selected a subset of 1,000 samples. GPT-40 was tasked with producing both the final originality label for each album and the five intermediate labels, enabling a comprehensive comparison of its performance across different levels of annotation complexity.\nFor the final album originality annotation, GPT-4o achieved a recall of 72.1%, precision of 27.6%, and an overall accuracy of 57.6%. However, it exhibited a missing rate of 12.23%, where no valid binary response was generated. This relatively low performance is expected, as GPT-40"}, {"title": "3.3.2 Results using LLM generated data", "content": "We utilized GPT-40 to generate process labels for 90k training samples. To investigate the impact of data size on performance, we selected subsets of 12k, 24k, 54k, 90k samples from this dataset. Training on these subsets, we compared AgentPS framework for models trained on LLM-generated data against those trained on human-annotated data.\nAs shown in Table 2, the performance of AgentPS trained on LLM-generated data is comparable to that of models trained with human-labeled data, and it consistently outperforms the vanilla SFT approach in all sizes of training data sets. This finding highlights the robustness of the AgentPS: Although the accuracy of LLM-generated intermediate labels may not reach 100%, it can serve as effective process supervision under the AgentPS framework.\nFurthermore, this capability is particularly beneficial in scenarios where obtaining high-quality human annotations is resource-intensive or impractical. These results underline the potential of integrating LLMs as process supervisors to reduce"}, {"title": "4 Related Work", "content": "Multimodel Large Language Models (MLLMs).\nRecent advancements in cutting-edge Multimodal Large Language Models (MLLMs), such as GPT-4V (Achiam et al., 2023), GPT-40 (Achiam et al., 2023), Gemini (Team et al., 2023), and Claude-3.5, demonstrate remarkable versatility across a wide range of vision tasks, including single-image, multi-image, and video analysis. While many models are tailored for specific task types, LLaVA-OneVision (Li et al., 2024), evaluated in this study, is engineered for high performance across these diverse tasks. It exhibits strong generalization and effective feature transfer capabilities, adapting seamlessly to varied scenarios. Offered in 0.5B and 7B versions, LLaVA-OneVision strikes a balance between scalability and computational efficiency, with an extended 72B version available for addressing more complex and demanding tasks.\nSeveral other versatile models, such as Video-LLaMA (Zhang et al., 2023), which integrates visual and auditory content understanding, and VILA (Lin et al., 2024), which explores pretraining strategies to adapt LLMs for vision tasks, further highlight the growing potential of multi-scenario MLLMs.\nProcess Supervision. Chain-of-Thought (CoT) prompting, introduced by Wei et al. (Wei et al., 2022), has emerged as a powerful method for enabling process supervision in large language models. By leveraging structured prompting, CoT decomposes complex tasks into intermediate reasoning steps, allowing models to solve problems systematically. Building upon this foundation, recent works (Yao et al., 2024; Long, 2023) introduced Tree-of-Thought (ToT) framework, a novel approach to multi-round question answering (QA) using language models.\nThe ToT framework employs a structured workflow comprising a status set, a prompt generator, and an evaluator. Within a pre-defined tree-like search space, language models navigate potential solutions using breadth-first search (BFS) or depth-first search (DFS). This method emphasizes inter-"}, {"title": null, "content": "mediate status evaluation, ensuring that each step in the reasoning process is scrutinized to converge on the optimal answer.\nAgentPS shares conceptual alignment with the ToT framework by embedding intermediate reasoning questions into its prompts. This strategy serves as a form of process supervision, guiding the model's reasoning pathways and enhancing its learning and inference capabilities.\nSimultaneously, task-specific prompt engineering has demonstrated its efficacy in practical applications, as evidenced by the successes of Flamingo (Alayrac et al., 2022) and BLIP (Li et al., 2022). These models highlight the potential of specialized prompt designs to significantly enhance performance on targeted tasks. Extending these principles, AgentPS integrates carefully designed prompts that incorporate intermediate questions, embedding process supervision directly into the model's reasoning architecture. This approach further amplifies the model's ability to decompose and solve complex problems efficiently, marking a significant step forward in prompt engineering methodologies."}, {"title": "5 Conclusion", "content": "In this work, we present AgentPS, a novel framework to address difficult multimodal content understanding tasks that require complex reasoning. By integrating intermediate labels through a sequentially structured process supervision mechanism, AgentPS aligns model reasoning with human-defined logic, achieving notable performance improvements over conventional end-to-end SFT approaches. In addition, we demonstrate the scalability of AgentPS through the use of intermediate labels generated by LLM, which match the performance of human annotations with minimal degradation. This approach significantly reduces the need for manual labeling, making it applicable to a broad spectrum of real-world tasks. Furthermore, integrating LLM-annotated data enables an agile and iterative design of intermediate questions, reducing manual annotation, reducing costs, and speeding up deployment.\nTo conclude, AgentPS represents an effective, scalable, and practical solution to advance the understanding of multimodal content in industrial and research contexts."}, {"title": "A Deploy AgentPS for Content Quality Assurance", "content": "To leverage this newly proposed framework to keep users from low quality content, we deploy MLLMs fine-tuned through AgentPS onto the Video Management Platform (VMP) at TikTok, such that newly published videos or updated album could be interacted and safe-guarded.\nThe functions and key components of VMP are illustrated in Figure 2. When a new video or album is published, model scores related to various quality and policy compliance factors, such as detecting originality issues, are stored in the VMP stage. These scores are subsequently processed and utilized in downstream multi-stage recommendation funnel (like Recall, Pre-rank, Rank, etc.).\nThe VMP workflow consists of four sequential stages-Source, Retriever, Filter, and Sink-that collaboratively ensure that only high-quality, policy-compliant video candidates proceed to the recommendation stage."}, {"title": "B Prompt for LLM Process Annotation", "content": "The prompt consists of two parts: (1) A list of image collections. (2) A sequence of five text-based questions.\nB.1 Image-based Prompt\nThe image collection list includes 1 to 16 images, which are encoded using b64encode and then concatenated after the questions. This combined input is then provided to the LLM.\nB.2 Text-based Prompt\nThe five questions are input as a single session, and the questions are as follows such as:"}, {"title": null, "content": "1. Watermark presence. \"\"Watermark' is like '@username' from social media, not simple timestamp. Each image_url is considered as one image. Count the number of images with watermarks in the album. ONLY return the number.\"\n2. Whether it is UGC (User-Generated Content). \"UGC (User Generated Content) is considered as content is generated by regular users, such as selfies, artistic creations, life recordings, or concatenated images from online sources combined with self-created content. The opposite of UGC is PGC(Professionally Generated Content). PGC refers to content such as pictures of celebrities in entertainment/sports/politics, screenshots, posters, or coverage from TV series, movies, documentaries, and other platforms. Each image_url is considered as one image. Count the number of UGC images in the album. ONLY return the number.\"\n3. Whether the image and the text title are relevant. \"Original text is defined as content with emotional words (e.g., 'good,' 'happy,' 'disgusting') or symbols, subjective comments (e.g., 'I think the Doors are the best rock band'), or narrative storytelling (e.g., 'This movie tells the story of...'). Simple expressions without detail, like song lyrics or standalone sentences, are considered non-original.. Determine if the given text '%s' is original. If it is not original, ONLY return 0. If it is original, count the number of images in the album related to the given text. ONLY return the"}, {"title": null, "content": "number.\"%(text)\n4. Whether the image and the text content are relevant. \"Original text is defined as content with emotional words (e.g., 'good,' 'happy,' 'disgusting') or symbols, subjective comments (e.g., 'I think the Doors are the best rock band'), or narrative storytelling (e.g., 'This movie tells the story of...'). Simple expressions without detail, like song lyrics or standalone sentences, are considered non-original.. Determine if the given text '%s' is original. If it is not original, ONLY return 0. If it is original, count the number of images in the album related to the given text. ONLY return the number.\"%(title)\n5. Whether the image and the overall theme of the image collection are relevant. \"Each image_url is considered as one image. Count the number of images whose content is related to the overall theme of the album. ONLY return the number.\" For every question, each image_url is counted as one image, so each return number should be smaller than %d. The final return format is ONLY five numbers separated by ',' in one line.\"%(len(image_infos))"}]}