{"title": "Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models", "authors": ["Asma Gandeharioun", "Avi Caciularu", "Adam Pearce", "Lucas Dixon", "Mor Geva"], "abstract": "Inspecting the information encoded in hidden representations of large language models (LLMs) can explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of research questions about an LLM's computation. We show that prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation, can be viewed as special instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by a Patchscope. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model, and unlocks new applications such as self-correction in multi-hop reasoning.", "sections": [{"title": "1. Introduction", "content": "The question of what information is captured within the hidden representations of large language models (LLMs) is of key importance in control and understanding of modern generative AI, and has drawn substantial attention recently (Casper et al., 2022; Madsen et al., 2022; Patel & Pavlick, 2021; Nanda et al., 2023). To tackle this question, prior work has introduced a diverse array of interpretability methods, which largely rely on three prominent approaches: training linear classifiers, called probes, on top of hidden representations (Belinkov & Glass, 2019; Belinkov, 2022; Alain & Bengio, 2017), projecting representations to the model's vocabulary space (nostalgebraist, 2020; Din et al., 2023; Belrose et al., 2023), and intervening on the computation to identify if a representation is critical for certain predictions (Meng et al., 2022a; Wallat et al., 2020; Wang et al., 2022; Conmy et al., 2023; Geva et al., 2023).\nDespite the wide success of these methods, they each exhibit practical shortcomings. First, probing relies on supervised training for pre-defined classes, which is hard to scale when the feature of interest has a large number of classes or when all the categories are not known a priori. Second, the accuracy of vocabulary projections substantially decreases in early layers and their outputs are often hard to interpret. Last, all the above methods are not expressive: they provide class probabilities or most likely tokens, as opposed to a high-quality explanation in natural language.\nIn this work, we argue that the advanced capabilities of LLMs in generating human-like text can be leveraged for \"translating\" the information in their representations for humans. We introduce a modular framework, called Patchscopes (see \u00a73), that can easily be configured to query various information from LLM representations. Given a representation, we propose to decode specific information from it by \"patching\" it into a separate inference pass that encourages the extraction of that information, independently of the original context. A configuration of our framework (a Patchscope) can be viewed as an inspection tool geared towards a particular objective, as illustrated in Fig. 1.\nWe show that many existing methods, including those that rely on vocabulary projections and interventions, can be cast as Patchscopes. Moreover, new configurations of our framework introduce more effective tools in addressing the same questions, while mitigating several limitations of prior approaches. Additionally, Patchscopes enables addressing underexplored questions, such as fine-grained analysis of the input contextualization process and the extent to which a more expressive model can be used to inspect hidden representations of a smaller model.\nWe conduct a series of experiments to evaluate the benefits and opportunities introduced by Patchscopes, focusing on auto-regressive LLMs. First, we consider the problem of estimating the model's next-token prediction from its intermediate representations (see \u00a74.1). Across multiple LLMs, we show that using a few-shot token identity prompt leads to substantial gains over vocabulary projection methods. Next, we evaluate how well Patchscopes can decode specific attributes of an entity from its LLM representations, when these are detached from the original context (see \u00a74.2). We observe that, despite using no training data, Patchscopes significantly outperforms probing in six out of twelve commonsense and factual reasoning tasks, and works comparably well to all but one of the remaining six.\nBeyond output estimation and attribute decoding, Patchscopes can address questions that are hard to answer with existing methods. In \u00a74.3, we apply Patchscopes to study how LLMs contextualize input entity names in early layers, where vocabulary projections mostly fail and other methods only provide a binary signal of whether the entity has been resolved, at best"}, {"title": "2. Related Work", "content": "Activation patching is a causal intervention, commonly used as a tool for studying if certain activations play a key role in a model's computation (Geiger et al., 2021; Vig et al., 2020). Patching has been used largely for localizing specific information to specific layers and token positions (Goldowsky-Dill et al., 2023; Meng et al., 2022a;b; Stolfo et al., 2023), and for finding paths explaining how information propagates in the computation (Wang et al., 2022; Geva et al., 2023; Hendel et al., 2023; Hanna et al., 2023; Lieberum et al., 2023). Despite certain limitations (Hase et al., 2023; Zhang & Nanda, 2023), patching remains a key tool for mechanistic interpretability (Conmy et al., 2023).\nGiven promising results from emerging interpretability efforts that employ LLMs to generate human-like text for inspection (e.g., Mousi et al., 2023; Slobodkin et al., 2023; Bills et al., 2023), we argue that using patching only for localization purposes is myopic, and propose to use it for \"translating\" LLM representations into natural language. Very recently, patching has been used to study new prob-"}, {"title": "3. Patchscopes", "content": "In this section, we introduce Patchscopes and show how it extends prior interpretability methods with new capabilities. While not limited to particular LLM architectures, this work focuses on auto-regressive transformer-based LLMs."}, {"title": "3.1. Framework Description", "content": "The key idea in Patchscopes is to leverage the advanced capabilities of LLMs to generate human-like text for \"translating\" the information encoded in their own hidden representations. Concretely, given a hidden representation obtained from an LLM inference pass, we propose to decode specific information from it by \"patching\" it into a different inference pass (of the same or a different LLM) that encourages the translation of that specific information.\nNotably, the rest of the forward computation after patching can augment the representation with additional information, hence, this approach does not guarantee that the patched representation itself stores all that information. However, dispatching the representation from its original context (the source prompt) stops contextualization and guarantees that no further information from the source prompt is incorporated to it in the post-patching computation. Therefore, our framework reveals if specific information can be decoded from the patched representation via the post-patching computation, which is an implicit way to expose the information contextualized within it.\nGiven an input sequence of $n$ tokens $S = (s_1, ..., s_n)$ and a model $M$ with $L$ layers, $h^l_i$ denotes the hidden representation obtained at layer $l \\in [1,...,L]$ and position $i \\in [1,...,n]$, when running $M$ on $S$. To inspect $h^l_i$, we consider a separate inference pass of a model $M^*$ with $L^*$ layers on a target sequence $T = (t_1,...,t_m)$ of $m$ tokens. Specifically, we choose a hidden representation $h^{l^*}_{i^*}$ at layer $l^* \\in [1,..., L^*]$ and position $i^* \\in [1,..., m]$ in the execution of $M^*$ on $T$. Moreover, we define a mapping function $f (h; \\theta) : \\mathbb{R}^d \\rightarrow \\mathbb{R}^{d^*}$ parameterized by $\\theta$ that operates on hidden representations of $M$, where $d$ and $d^*$ denote the hidden dimension of representations in $M$ and $M^*$, respectively. This function can be the identity function, a linear or affine function learned on task-specific pairs of representations, or even more complex functions that incorporate other sources of data. The patching operation refers to dynamically replacing the representation $h^{l^*}_{i^*}$ during the inference of $M^*$ on $T$ with $f(h^l_i)$. Namely, by applying $h^{l^*}_{i^*} \\leftarrow f(h^l_i)$, we intervene on the generation process and modify the computation after layer $l^*$.\nOverall, a Patchscope intervention applied to a representation determined by $(S, i, M, l)$, is defined by a quintuplet $(T, i^*, f, M^*, l^*)$ of a target prompt $T$, a target position $i^*$ in this prompt, a mapping function $f$, a target model $M^*$, and a target layer $l^*$ of this model. Notably, it is possible that $M$ and $M^*$ are the same model, $S$ and $T$ are the same prompt, and $f$ is the identity function $I$ (i.e., $I(h) = h$). In the following sections, we show how this formulation covers prior interpretability methods and further extends them with new capabilities."}, {"title": "3.2. Patchscopes Encompasses Prior Methods", "content": "We show how prominent interpretability methods can be cast as Patchscope instances. See a summary in Tab. 1.\nRecent methods inspect LLM representations by projecting them to the output vocabulary space (Dar et al., 2023; nostalgebraist, 2020; Din et al., 2023; Belrose et al., 2023). Formally, an estimation of the output distribution is obtained from the representation $h^l_i$ at position $i$ and layer $l$ by:\n$p^l_i = softmax(W_u f(h^l_i)) \\in \\mathbb{R}^{|V|}$,\nwhere $W_u \\in \\mathbb{R}^{|V|\\times d}$ is the model's unembedding matrix and $f$ is a simple mapping function, such as the identity function or an affine mapping. We note that the operation applied to $f(h^l_i)$ is the same computation applied by the model to the last-layer representation for obtaining the next-token prediction. Therefore, prior methods that inspect representations in the vocabulary space can be viewed as a class of Patchscopes with identical source and target prompts ($T = S$) that maps representations from any source layer $l$ to the last target layer $L^*$. Differences between these methods lie in the choice of $f$; logit lens (nostalgebraist, 2020; Dar et al., 2023) applies the identity function, linear shortcuts (Din et al., 2023) uses a linear mapping function, and tuned lens (Belrose et al., 2023) trains an affine mapping. Recently, Hernandez et al. (2023b) introduced LRE Attribute Lens that builds $f$ based on a relation linearity assumption, and they showcase its effectiveness in attribute extraction.\nThis class of methods has proven to be effective for different applications, for example, in improving inference efficiency via early exiting (Din et al., 2023). While the majority of methods and applications in this category use a single model ($M^* = M$), Merullo et al. (2022) had demonstrated successful caption generation with a generative image model as $M$ and a language model as $M^*$.\nAnother category of inspection methods intervene on the LLM computation. Contemporary to our work, Pal et al. (2023) have investigated whether it is possible to anticipate multiple generated tokens ahead from a given hidden representation, rather than estimating just the next-token prediction. Their method, called Future Lens, uses a target prompt that is different from the original prompt (i.e., $T\\neq S$) and is designed to decode subsequent tokens from information encoded in a hidden representation $h^l_i$. Example target prompts are \"The multi-tokens present here are \" and \"Hello! Could you please tell me more about \". Notably, Future Lens can be cast as another Patchscope with $M^* = M$ and $l^* = l$.\nMore broadly, Patchscopes also covers recent mechanistic interpretability methods that analyze internal processes in LLMs with inference computation interventions. Specifically, causal tracing (Meng et al., 2022a) uses a source prompt augmented with Gaussian noise as the target prompt. In addition, previous work have intervened on one or more target layers during inference by patching zero vectors to the computation (Wang et al., 2022; Conmy et al., 2023; Geva et al., 2023), namely, setting $f(h) = 0$."}, {"title": "3.3. Patchscopes Enables Novel Inspection Methods", "content": "Prior work has utilized specific patching configurations for interpretability, largely focusing on patching the same model while using the same prompt (i.e., $M^* = M, T = S$). The framing of Patchscopes introduces a wide range of unexplored configurations that could potentially unlock new inspection capabilities.\nSpecifically, we observe that modifying the target prompt enables an expressive decoding of any feature of our choice, detached from the source prompt computation. For instance, we can use the prompt \"The capital of X is\" to check if the capital city of a given country is extractable from the hidden representation of this country at specific layer. Similarly, a prompt like \"Tell me facts about X\" can be leveraged to assess whether the model has resolved the entity name corresponding to a given description in a specific layer (see Fig. 1). Importantly, contrary to probing, this approach is not restricted by the number of classes of the chosen feature."}, {"title": "4. Experiments", "content": "We evaluate how well our framework allows decoding different types of information from LLM representations, including next-token predictions (\u00a74.1) and specific attributes (\u00a74.2). Then, we demonstrate the new possibilities that Patchscopes introduce, focusing on analyzing the contextualization of entity names (\u00a74.3) and leveraging stronger models for inspection via cross-model patching (\u00a74.4). Tab. 1 summarizes the new proposed Patchscopes and their configurations compared to prior work."}, {"title": "4.1. Decoding of Next-Token Predictions", "content": "As introduced in \u00a73.2, let $p^{\\mathcal{U}}_i$ be the output probability distribution for some input, obtained by multiplying the finallayer last-position hidden representation $h^L_i$ by the unembedding matrix $W_u \\in \\mathbb{R}^{|V|\\times d}$. We wish to estimate $p^{\\mathcal{U}}$ from intermediate representations $h^l_i$ s.t. $l < L$. Particularly, we ask how early in the computation the model has concluded its final prediction from the given context. In our experiments, we consider multiple LLMs \u2013 LLaMA2 (13B) (Touvron et al., 2023b), Vicuna (13B) (Chiang et al., 2023), GPT-J (6B) (Wang & Komatsuzaki, 2021), and Pythia (12B) (Biderman et al., 2023) (see more details in \u00a7A.1).\nMethods We compare vocabulary projection methods (\u00a73.2) with a new Patchscope. Each method yields an estimated output probability $p^l_i$ by patching an intermediate representation $h^l_i$ to the model's final layer. Here, we focus on the common setting where $M = M^*$, and discuss extensions to $M \\neq M^*$ in \u00a74.4.\n\\bullet Logit Lens: Following prior work (nostalgebraist, 2020; Geva et al., 2022a), we define $f$ as the identity function, meaning no change is applied to the patched representation. That is, $f(h) := I(h)$.\n\\bullet Tuned Lens: Motivated by Belrose et al. (2023); Din et al. (2023), we employ an affine mapping function between representations at layer $l$ and the final layer $L$. Specifically, we feed the model examples from a training set $T$ and for each example $s \\in T$ obtain a pair $(h^l_i, h^L_i)$ of hidden representations. Then, we fit linear regression to find a matrix $A_l \\in \\mathbb{R}^{d \\times d}$ and a bias vector $b_l \\in \\mathbb{R}^{d}$ that are numerical minimizers for $\\sum_{s \\in T} || A_l h^l_i - h^L_i + b_l ||^2_2$. We define $f$ as:\n$f(h^l) := A_l h^l + b_l$.\n\\bullet Token Identity Patchscope: Unlike the previous methods, here we use a target prompt that is different from the source prompt ($T \\neq S$) and is meant to encourage the model to decode the token identity of the hidden representation. Also, while the above methods skip the computation between layers $l$ and $L$, here we modify it such that all the information from the source prompt computation is discarded, except for the patched representation. We craft a prompt with $k$ demonstrations representing an identity-like function, formatted as \"tok\u2081\u2192tok1; tok\u2082\u2192tok2;...; tokk\". Further details and an experiment showing the robustness of the method to this selection are provided in \u00a7A.3. Note that this Patchscope does not require any training.\nEvaluation We follow Din et al. (2023) and evaluate the estimated prediction on the Pile evaluation set (see \u00a7A.2 for details) using two metrics:\n\\bullet Precision@1 ($\\uparrow$ is better): The portion of examples for which the highest-probability token $t$ in the estimated probability distribution matches the highest-probability token in the original output distribution. That is, if $argmax_t(p^l_i) = argmax_t(p^{\\mathcal{U}}_i)$.\n\\bullet Surprisal ($\\downarrow$ is better): The minus log-probability of the highest-probability token in the predicted distribution $p^l_i$ according to $p^{\\mathcal{U}}$, i.e., $- log p^l_i$, where $t = argmax_t(p^{\\mathcal{U}}_i)$.\nResults Fig. 2 depicts the results. Across all the models, from layer 10 and upwards, the token identity Patchscope consistently outperforms the other baselines,"}, {"title": "4.2. Extraction of Specific Attributes", "content": "Classification probes are arguably the most commonly used method for checking if certain attributes are encoded in hidden representations (Belinkov, 2022; Belinkov & Glass, 2019). However, they need to be trained, and the range of attribute classes needs to be known a priori. Here we show that repurposing Patchscopes for attribute extraction overcomes these limitations. First, it does not require training. Second, it is not limited by a predefined set of labels, but rather can benefit from an open vocabulary. In addition, by taking advantage of the model's nonlinearities, it is more flexible in capturing complex relations compared to linear probes.\nExperimental Setup Consider factual and commonsense knowledge represented as triplets $(\\sigma, \\rho, \\omega)$ of a subject (e.g., \"United States\"), a relation (e.g., \"largest city of\"), and an object (\"New York City\"). We investigate to what extent the object $\\omega$ can be extracted from the last token representation of the subject $\\sigma$ in an arbitrary input context. To this end, we conduct experiments on 8 commonsense and 25 factual knowledge tasks curated by Hernandez et al. (2023b). This dataset includes $(\\sigma,\\rho,\\omega)$ triplets for different relations, along with prompt templates that verbalize them in natural language. We conduct experiments with GPT-J (6B) (Wang & Komatsuzaki, 2021), filtering the data to keep only the examples where $\\omega$ appears in the the model's continuation of the prompt up to 20 tokens. For each example, we sample 5 utterances from the WikiText-103 dataset (Merity et al., 2016) that include $\\sigma$ and use them as $S$. Lastly, we keep tasks with at least 15 samples, which results in 5 commonsense and 7 factual tasks with a total of 1,453 datapoints. For more details, see \u00a7B.\nMethods We devise a Patchscope for feature extraction and compare it with linear probing (K\u00f6hn, 2015; Gupta et al., 2015) as a baseline.\n\\bullet Zero-shot Feature Extraction Patchscope: We craft $T$ as a general verbalization of $\\rho$ followed by a placeholder for $\\omega$, such that $i^* = m$. For example, we use $T \\leftarrow$ \"The largest city in x\" with \"x\" as a placeholder for the subject. To extract the object from the entity representation in $S$, we patch the representation of token \"x\" at layer $l^*$ with the representation of \"States\" from layer $l$, and consider if the generated text includes $\\omega$. The remaining configurations of this Patchscope are $f \\leftarrow I, M^* \\leftarrow M, i \\leftarrow$ the last token of $\\sigma$ in $S$. We consider all combinations of $l \\in [1,..., L] \\times l^* \\in [1, . . ., L^*]$. Later in this section, we discuss the role of $l$ pertaining to attribute extraction.\n\\bullet Logistic Regression Probe: Let $\\Omega$ represent the range of possible objects for a given relation. We use the set of unique values of $\\omega$ in the training set as a proxy for $\\Omega$. We train a logistic regression probe (K\u00f6hn, 2015; Gupta et al., 2015) for each layer that predicts $\\omega \\in \\Omega$ from last token representation of $\\sigma$. Given that 6 out of 12 tasks have fewer than 40 datapoints, we use three-fold cross-validation for training and evaluation of this baseline. Note that we have excluded tasks where the probe fails completely due to insufficient number of training examples (fewer than 15 datapoints).\nEvaluation We measure the average attribute extraction accuracy. For a given sample, the Patchscope is considered correct if $\\exists l^* \\in [1, . . ., L^*]$ where the generated text up to 20 tokens includes $\\omega$. For the probe, a prediction is correct if the highest probability is assigned to $\\omega$.\nResults Tab. 2 summarizes the results, averaged over $l \\in [1,..., L]$. We conduct a T-test with Bonferroni correction to compare the two methods. Despite using no training data and having no restrictions on the output, the Patchscope achieves a significantly higher accuracy than the probe on 6 out of 12 tasks (p<1e-5), compared to only one task where probing is better. For the remaining 5 tasks, the difference between the Patchscope and baseline is not significant."}, {"title": "4.3. Analyzing Entity Resolution in Early Layers", "content": "The previous sections focused on analyzing the information encoded in a single hidden state. Here we turn to consider a more global question of how LLMs resolve entity mentions across multiple layers. Concretely, given a subject entity name, such as \"the summer Olympics of 1996\", how does the model contextualize the input tokens of the entity and at which layer is it fully resolved?\nAnswering these questions is hard with existing methods; vocabulary projections focus on the output prediction and fail to show clear patterns in early layers, and probing is restricted to outputs from a fixed number of classes, which may not be expressive enough to describe this process. Alternative approaches have studied this process indirectly via interventions (Meng et al., 2022a), showing that the model constructs a subject representation at the last token of the entity name. However, it is still unclear how this contextualization is performed.\nWe analyze how LLMs contextualize input entity names by leveraging Patchscopes. Particularly, we craft a target prompt for generating a description of a given subject, and apply it to the hidden representation at the last subject position in the source prompt (where the model forms the subject representation (Geva et al., 2023; Hernandez et al., 2023a)) across the early layers. This will allow us to see how the model describes the subject in each layer.\nAnalysis Setting We use a few-shot target prompt template for decoding an entity description: \u201csubject\u0131:\ndescription of subject1,..., subjectk:\ndescription of subjectk, x\u201d, while patching the last position corresponding to x. We take the 200 most popular and 200 least popular subject entities from the PopQA dataset (Mallen et al., 2023). The popular entities should appear frequently in LLMs' pre-training data, and are thus likely to be captured by the model, while resolving the rare entities is expected to be more challenging (Kandpal et al., 2023; Mallen et al., 2023). Then, for the source prompt we use the entity name, and for the target prompt we sample k = 3 random subject entities. We obtain a short (up to one sentence) description of every subject entity from Wikipedia. Our target prompt and more"}, {"title": "4.4. Expressiveness from Cross-Model Patching", "content": "A possible avenue for improving inspection capabilities is to explain a given model with a model that is more expressive (Bills et al., 2023). In the context of Patchscopes, this means to patch a representation of $M$ into a more expressive model $M^*$. However, it is not clear if such an intervention would yield plausible results, due to possible discrepancies between the two models resulting from different architectures, optimization processes, and so on.\nWe show that when patching across models from the same family, such interventions are possible and can improve expressiveness. Specifically, we consider patching representations across different sizes of Vicuna ($M\\leftarrow7B, M^*\\leftarrow13B$) and Pythia ($M\\leftarrow6.9B, M^*\\leftarrow12B$), and measure how well the larger model estimates the next-token predictions and entity resolution process of the smaller model.\nNext-Token Prediction We repeat the experiment in \u00a74.1, using the token identity Patchscope. To overcome discrepancies between the models, we learn affine mappings between their layers (similarly to Tuned Lens). Fig. 5 depicts the Precision@1 scores for different combinations of source-target layers, showing that patching with a simple affine Patchscope proves to be effective with precision of up to 0.7 and 0.8 for Vicuna and Pythia, respectively. Specifically, patching representations to an early layer of the larger model seems to be the most effective. Furthermore, it appears that there is a subtle matching among some layers of"}, {"title": "5. Application: Self-Correction in Multi-Hop Reasoning", "content": "Multi-hop reasoning is a challenging problem. While a language model may be capable of correctly answering each step independently, it could still fail at processing the connection between different steps, resulting in an incorrect prediction. Recent attempts to improve multi-hop reasoning rely on prompting the model to generate a step-by-step answer autoregressively (e.g., Wei et al., 2022; Yao et al., 2023; Besta et al., 2024), possibly with an iterative process of self-refinement (Madaan et al., 2023). While effective in terms of performance, these methods incur additional inference costs that could be avoidable.\nIn this section, we show that Patchscopes can improve multi-hop reasoning performance without generating the reasoning steps, particularly in cases when the model fails at completing a multi-hop query despite being successful in each reasoning step independently. Via Patchscopes, one can surgically operate on the model representations, reroute model's intermediate answer to one step of the reasoning task, simplify the consequent reasoning step, and ultimately correct the final prediction.\nExperimental Setup Following the notation in \u00a74.2, let $\\tau_1 = (\\sigma_1, \\rho_1, \\omega_1)$ represent the relation $\\rho_1$ between a subject entity $\\sigma_1$ and an object entity $\\omega_1$. Let $\\tau_2 = (\\sigma_2, \\rho_2, \\omega_2)$ represent another tuple such that $\\sigma_2 = \\omega_1$. A multi-hop reasoning query pertaining to $\\tau_1$ and $\\tau_2$ is a prompt composed of two parts: $\\pi_1$ is a verbalization of $\\sigma_1$ and $\\rho_1$ from which $\\omega_1$ can be inferred; $\\pi_2$ is a verbalization of $\\rho_2$, from which $\\omega_2$ can be inferred after its concatenation with $\\pi_1$. For example, Let $\\tau_1 \\leftarrow$ (\u201cVisual Basic\u201d, \u201cproduct of\u201d, \u201cMicrosoft\") and $\\tau_2 \\leftarrow$ (\u201cMicrosoft\u201d, \u201ccompany CEO\u201d, \u201cSatya Nadella\u201d). An example verbalization of these tuples would result in $\\pi_1 \\leftarrow$\u201cthe company that created Visual Basic Script\u201d, and $\\pi_2 \\leftarrow$\u201cThe current CEO of\u201d. This leads to systematic generation of the multi-hop reasoning query $[\\pi_2][\\pi_1] \\leftarrow$\u201cThe current CEO of the company that created Visual Basic Script\u201d. Building on Hernandez et al. (2023b), we systematically generate all valid multi-hop factual and commonsense reasoning queries where $\\omega_1 = \\sigma_2$. We conduct experiments on Vicuna (13B), focusing on samples where M accurately represents both $\\tau_1$ and $\\tau_2$ independently, that is, $\\omega_1$ appears in the next 20 tokens M generates conditioned on the prompt $\\pi_1$ that verbalizes $\\sigma_1$ and $\\rho_1$. This process yields 1,104 multi-hop reasoning samples, out of which 46 satisfy the above criteria and are used for evaluation. For more details, see \u00a7E.\nMethod and Evaluation We introduce a Chain-of-Thought (CoT) Patchscope to fix multi-hop reasoning via intervening on the computation graph and rerouting representation likely to capture $\\omega_1$ in place of $\\sigma_2$. Concretely, $S$ refers to the formed query discussed above, and we use the following configuration: $T\\leftarrow S, M^* \\leftarrow M, i \\leftarrow n, i^* \\leftarrow$ the token preceding $\\pi_1$. As a baseline, we evaluate a vanilla generation of $M$ conditioned on $S$ without any intervention. We evaluate the outputs in terms of accuracy, similarly to \u00a74.2. For a sample $S$, the Patchscope is considered accurate if $\\exists (l, l^*) : l \\in [1, . . ., L], l^* \\in [1, . . ., L^*]$ where the autoregressive generation up to 20 tokens includes $\\omega_2$.\nResults While the baseline accuracy is only 19.57%, the Patchscope achieves 50% accuracy. Fig. 6 shows the interaction between $l$ and $l^*$ and how it affects the success rate. Patching representations from most source layers $l$ into early-to-mid $l^*$ (6-16) is most effective in making the right prediction. Our interpretation is that patching into late\""}, {"title": "6. Conclusion", "content": "We present Patchscopes, a simple and effective framework that leverages the ability of LLMs to generate human-like text for decoding information from intermediate LLM representations. We show that existing interpretability methods can be cast as specific instances of Patchscopes, which cover only a small portion of all the possible configurations of the framework. Moreover, using new underexplored Patchscopes substantially improves our ability to decode various types of information from the model's internal computation, such as the output prediction and knowledge attributes, typically outperforming prominent methods that rely on projection to the vocabulary and probing. In addition, our framework enables new capabilities, such as analyzing the contextualization process of input tokens in the very early layers of the model, and is beneficial for practical applications, such as multi-hop reasoning correction. This paper only scratches the surface of the opportunities this framework creates. Future work could study its application across different domains and modalities, investigate its variants with simultaneous multi-token patching, and present recipes for task-specific and task-agonstic Patchscopes."}]}