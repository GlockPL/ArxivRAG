{"title": "An Adaptive Differential Privacy Method Based on Federated Learning", "authors": ["Zhiqiang Wang", "Xinyue Yu", "Qianli Huang", "Yongguang Gong"], "abstract": "Differential privacy is one of the methods to solve the problem of pri- vacy protection in federated learning. Setting the same privacy budget for each round will result in reduced accuracy in training. The existing methods of the adjustment of privacy budget consider fewer influencing factors and tend to ig- nore the boundaries, resulting in unreasonable privacy budgets. Therefore, we proposed an adaptive differential privacy method based on federated learning. The method sets the adjustment coefficient and scoring function according to ac- curacy, loss, training rounds, and the number of datasets and clients. And the privacy budget is adjusted based on them. Then the local model update is pro- cessed according to the scaling factor and the noise. Finally, the server aggregates the noised local model update and distributes the noised global model. The range of parameters and the privacy of the method are analyzed. Through the experi- mental evaluation, it can reduce the privacy budget by about 16%, while the ac- curacy remains roughly the same.", "sections": [{"title": "Introduction", "content": "In response to industry demands and the challenge of \u201cdata island\", Google introduced the concept of \u201cfederated learning\u201d in 2016 [1]. Federated learning is a distributed ar- chitecture [2] that requires multiple clients to locally analyze and compute their respec- tive stored data. The computed results are shared with a global model, without sharing the participants' raw data. This approach makes the data \"accessible but not visible\", reducing privacy risks and communication complexity associated with traditional ma- chine learning methods, and alleviating the problem of data island. Although federated learning does not involve the direct exchange of raw data, it provides stronger privacy protection compared to traditional machine learning. However, since the computed re- sults themselves contain certain privacy information, there is still a risk of privacy leak- age if comprehensive and sufficient privacy safeguards are not in place. Therefore, en- suring privacy protection in federated learning has become one of the research direc- tions in this field."}, {"title": "Related Work", "content": "Differential privacy is one of the commonly used privacy protection techniques in fed- erated learning. Since 2017, both domestic and international researchers have proposed various solutions in this field. In 2017, Geyer et al. [4] introduced user-level differential privacy protection algorithms specifically for federated learning. They averaged the client models and utilized a randomization mechanism to approximate the average, thereby concealing the contributions of individual clients and protecting their entire datasets. However, the performance of their approach was heavily influenced by the number of clients. It failed to guarantee accuracy or convergence when the number of clients was small. In 2018, McMahan et al. [5] further improved user-level privacy protection by implementing long short-term memory. Nevertheless, their algorithm re- quired pre-designed scaling factors, which meant that the norms of each vector needed to be estimated in advance, posing practical challenges. In 2019, Triastcyn et al. [6] introduced Bayesian differential privacy into federated learning and proposed DP-"}, {"title": "Methodology", "content": "In the adjusting the privacy budget part, we proposed an algorithm to adjust the privacy budget, which involves three steps: calculation of adjustment coefficient, calculation of scoring function, and judgments and adjustments. The detailed algorithm is presented in Algorithm 2."}, {"title": "Methodology overview", "content": "An adaptive differential privacy method based on federated learning can be divided into four stages: initialization, local training, adding noise and uploading, and aggregation and distribution. The initialization and local training stages are consistent with the Fe- dAvg [15]. The adding noise and uploading stage involve three parts: adjusting the privacy budget, processing local model updates, and uploading. In the aggregation and distribution stage, the global model is aggregated and distributed, as well as the accu- racy and loss are recorded, which are used to adjust the privacy budget in the next"}, {"title": "Adjusting the privacy budget", "content": ""}, {"title": "Process of local model updates", "content": "The process of local model updates can be divided into three steps: calculation of scal- ing factor, generated noise, and get noised local model updates. The cosine similarity is used as the criterion to compute the scaling factor. The calculation formula is pre- sented as Formula 7, where A represents the previous round's global model, B repre- sents the local model corresponding to the current client in the current round, G repre- sents the local model update, G' represents the transmitted local model update, $\u03b5'$ de- notes the adjusted privacy budget, $\u0394f$ represents the sensitivity, and Lap(0, $\u0394f/\u03b5'$)\nrepresents the Laplace-distributed noise."}, {"title": "Bound And Privacy Analysis", "content": ""}, {"title": "Bound analysis", "content": ""}, {"title": "Adjustment coefficients", "content": "According to Formula 1, An exploration of its limits is as follows: When the cosine value is less than 0, the adjustment coefficient of 1 is a constant, and a large number of experiments have found that the probability of this situation is very small, so the anal- ysis is not carried out [13]. When cosine is greater than or equal to 0, Let f (A, B) =\ncos(A, B), where A and B are two vectors whose cosine similarity ranges from -1 to 1,\nalso because cos(A, B) \u2265 0, so f (A, B) = cos(A, B) \u2208 [0,1]. Let g(M, N) = $\\frac{M}{N}$, where\nM and N are positive integers and M > N \u2265 1. So 1 \u2265 $\\frac{N}{M}$ \u2265 $\\frac{1}{M}$ and M \u2265 $\\frac{M}{N}$ \u2265 1,\nnamely g(M, N) = \u2208 [1, M]. Let z($D_i$, D) = $\\frac{D_i}{D}$, where $D_i$ and D are positive inte-\ngers and D > $D_i$ \u2265 1. So 1\u2265$\\frac{D_i}{D}$ \u2265 $\\frac{1}{D}$ namely z($D_i$, D) = [$\\frac{1}{D}$,1].\nf(A, B),g(M, N), and z($D_i$, D) are non-negative and independent of each other."}, {"title": "Scoring function", "content": "The score of loss and accuracy is either 0 or 1, so the scoring function of training rounds is mainly analyzed. Formula 5 can be rewritten as Formula 8.\nIt is a piecewise function, where the T and t are positive integers and 1 \u2264 t \u2264 T. So\n$\\frac{1}{2}$<$(\\frac{t}{T})^2$\u2264$\\frac{1}{2}$,$\\frac{1}{2}$>$(\\frac{t}{T})^2$\u2264$\\frac{1}{2}$according to 0<$(\\frac{t}{T})^2$\u2264$\\frac{1}{2}$\u2264$\\frac{1}{2}$.Therefore, when 0 <$\\frac{1}{2}$<\n\u2264 socret \u2264 1; when when>$\\frac{1}{2}$.\nAs a result, socret \u2208 [1]. socreloss,socreacc,\nand socret are non-negative and independent of each other. According to Formula 2:"}, {"title": "Privacy budget", "content": "When socre \u2264 50 or p > 1, privacy budget stays the same. Otherwise, it needs to be reduced by adjusting coefficients. According to the analysis, p\u2208 [0, M - 1] can be ob- tained, and according to Formula 6, p \u2264 1 can be obtained. So p\u2208 [0,1], \u03b5' = \u03c1\u03b5 \u0395\n[0, \u03b5]. To sum up, privacy budget \u03b5' \u2208 [0, \u03b5]."}, {"title": "Privacy analysis", "content": "Theorem 1: Assuming an initial privacy budget of \u025b, the upload local model up- dates satisfied \u025b'-differential privacy.\nProve or analyze: It is known that this algorithm is an improvement on Laplace mechanism, where the adjusted privacy budget \u03b5' \u2208 [0, \u025b] Let $D_i$ and $D'_i$ be adjacent\ndata sets, F be the output function, G($D'_i$) = ($g^t_1$ + $\u2206g^t_1$, $g^t_2$ + $\u2206g^t_2$, \u2026\u2026,$g^t_m$ + $\u2206g^t_m$),\noutput set Y = ($y^t_1$, $y^t_2$, ..., $y^t_n$), $\u0394f_t$ = $max||G(D_i) \u2013 G(D'_i)||_1$ = $max||\u2211^m_{m=1}\u2206g^t_m||$.\nAccording to the Laplace noise probability density formula [17], then each round has:"}, {"title": "Experimental Evaluation", "content": ""}, {"title": "Experimental environment", "content": "The experimental environment is shown in Table 1, simulating the federated learning training process of 1 server and 100 clients. MNIST dataset and Logistics model were selected. The experimental divided into Equal and Random groups according to differ- ent partitioning methods of data sets [18]. The accuracy of the model named best_acc, will be analyzed and compared in the experiment."}, {"title": "Overall Performance", "content": "Based on the different numbers of selected clients and the initial privacy budget, we analyzed the experimental results. The specific experimental parameters are shown in Table 2."}, {"title": "Accuracy.", "content": "Based on the same initial privacy budget and with a different number of selected clients, the training curve results for the Equal group are shown in Figure 2, while the training curve results for the Random group are displayed in Figure 3."}, {"title": "Privacy budget", "content": "The initial privacy budget for each client in each round is set to 100, 120, 150, 170, and 200, respectively. The number of selected clients in each round is 3, 5, and 7. The ex- perimental results for the Equal group are shown in Figure 6, and the results for the Random group are shown in Figure 7."}, {"title": "Comparison experiment", "content": "In this section, we conduct comparative experiments to validate the superiority of co- sAFed over LAPFed, ADPFL [13], and cosFed [11] under the aforementioned"}, {"title": "Conclusion", "content": "With the wide application of federated learning, the problem of privacy protection has also aroused people's attention. As one of the common solutions, differential privacy [19] still needs to consider the balance between security and accuracy, as well as the adaptive addition of noise according to multiple factors. Therefore, we proposed an adaptive differential privacy method based on federated learning. By setting the adjust- ment coefficient and scoring function, the privacy budget can be dynamically adjusted according to multiple factors. Then, the scaling factor is calculated to process the local"}]}