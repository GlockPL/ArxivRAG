{"title": "An Adaptive Differential Privacy Method Based on Federated Learning", "authors": ["Zhiqiang Wang", "Xinyue Yu", "Qianli Huang", "Yongguang Gong"], "abstract": "Differential privacy is one of the methods to solve the problem of privacy protection in federated learning. Setting the same privacy budget for each round will result in reduced accuracy in training. The existing methods of the adjustment of privacy budget consider fewer influencing factors and tend to ignore the boundaries, resulting in unreasonable privacy budgets. Therefore, we proposed an adaptive differential privacy method based on federated learning. The method sets the adjustment coefficient and scoring function according to accuracy, loss, training rounds, and the number of datasets and clients. And the privacy budget is adjusted based on them. Then the local model update is processed according to the scaling factor and the noise. Finally, the server aggregates the noised local model update and distributes the noised global model. The range of parameters and the privacy of the method are analyzed. Through the experimental evaluation, it can reduce the privacy budget by about 16%, while the accuracy remains roughly the same.", "sections": [{"title": "Introduction", "content": "In response to industry demands and the challenge of \u201cdata island\", Google introduced the concept of \u201cfederated learning\u201d in 2016 [1]. Federated learning is a distributed architecture [2] that requires multiple clients to locally analyze and compute their respective stored data. The computed results are shared with a global model, without sharing the participants' raw data. This approach makes the data \"accessible but not visible\", reducing privacy risks and communication complexity associated with traditional machine learning methods, and alleviating the problem of data island. Although federated learning does not involve the direct exchange of raw data, it provides stronger privacy protection compared to traditional machine learning. However, since the computed results themselves contain certain privacy information, there is still a risk of privacy leakage if comprehensive and sufficient privacy safeguards are not in place. Therefore, ensuring privacy protection in federated learning has become one of the research directions in this field."}, {"title": "Related Work", "content": "Differential privacy is one of the commonly used privacy protection techniques in federated learning. Since 2017, both domestic and international researchers have proposed various solutions in this field. In 2017, Geyer et al. [4] introduced user-level differential privacy protection algorithms specifically for federated learning. They averaged the client models and utilized a randomization mechanism to approximate the average, thereby concealing the contributions of individual clients and protecting their entire datasets. However, the performance of their approach was heavily influenced by the number of clients. It failed to guarantee accuracy or convergence when the number of clients was small. In 2018, McMahan et al. [5] further improved user-level privacy protection by implementing long short-term memory. Nevertheless, their algorithm required pre-designed scaling factors, which meant that the norms of each vector needed to be estimated in advance, posing practical challenges. In 2019, Triastcyn et al. [6] introduced Bayesian differential privacy into federated learning and proposed DP-"}, {"title": "Methodology", "content": "An adaptive differential privacy method based on federated learning can be divided into four stages: initialization, local training, adding noise and uploading, and aggregation and distribution. The initialization and local training stages are consistent with the Fe- dAvg [15]. The adding noise and uploading stage involve three parts: adjusting the privacy budget, processing local model updates, and uploading. In the aggregation and distribution stage, the global model is aggregated and distributed, as well as the accuracy and loss are recorded, which are used to adjust the privacy budget in the next"}, {"title": "Adjusting the privacy budget", "content": "In the adjusting the privacy budget part, we proposed an algorithm to adjust the privacy budget, which involves three steps: calculation of adjustment coefficient, calculation of scoring function, and judgments and adjustments. The detailed algorithm is presented in Algorithm 2."}, {"title": "Bound And Privacy Analysis", "content": ""}, {"title": "Adjustment coefficients", "content": "According to Formula 1, An exploration of its limits is as follows: When the cosine value is less than 0, the adjustment coefficient of 1 is a constant, and a large number of experiments have found that the probability of this situation is very small, so the analysis is not carried out [13]. When cosine is greater than or equal to 0, Let $f (A, B) = cos(A, B)$, where $A$ and $B$ are two vectors whose cosine similarity ranges from -1 to 1, also because $cos(A, B) \u2265 0$, so $f (A, B) = cos(A, B) \u2208 [0,1]$. Let $g(M, N) = \\frac{N}{M}$, where $M$ and $N$ are positive integers and $M > N > 1$. So $1 \u2265 \\frac{N}{M} \u2265 \\frac{1}{M}$ and $M \u2265 \\frac{1}{M} \u2265 1$, namely $g(M, N) = \\frac{N}{M} \u2208 [1, M]$. Let $z(D_i, D) = \\frac{D_i}{D}$, where $D_i$ and $D$ are positive integers and $D > D_i \u2265 1$. So $1\u2265 \\frac{D_i}{D}\u2265\\frac{1}{D}$, namely $z(D_i, D) = \\frac{D_i}{D} \u2208 [\\frac{1}{D}, 1]$\n$f(A, B),g(M, N)$, and $z(D_i, D)$ are non-negative and independent of each other.\nCase 1:\nCase 2:"}, {"title": "Scoring function", "content": "The score of loss and accuracy is either 0 or 1, so the scoring function of training rounds is mainly analyzed. Formula 5 can be rewritten as Formula 8.\n\nIt is a piecewise function, where the $T$ and $t$ are positive integers and $1 \u2264 t \u2264 T$. So $\\frac{1}{2} < \\frac{t}{T} \u2264 1$, according to $0 <\\frac{2t}{T} \u2264 1.\\Therefore, when $0 < \\frac{t}{T} < \\frac{1}{2}$,\\$\\leq socret \\leq 1$; when when\nAs a result, $socret E [1]$. $socreloss,socreacc$,\nand $socret$ are non-negative and independent of each other. According to Formula 2:\nCase 1:\n\nCase 2:\n\nTo sum up, $ \\frac{-60}{T}$$\\leq socre \\leq 100$,\\$socre e [\\frac{-60}{T}, 100]$"}, {"title": "Privacy budget", "content": "When socre \u2264 50 or p > 1, privacy budget stays the same. Otherwise, it needs to be reduced by adjusting coefficients. According to the analysis, p\u2208 [0, M - 1] can be obtained, and according to Formula 6, p \u2264 1 can be obtained. So p\u2208 [0,1], \u03b5' = \u03c1\u03b5 \u0395\n[0, \u03b5]. To sum up, privacy budget \u03b5' \u2208 [0, \u03b5]."}, {"title": "Privacy analysis", "content": "Theorem 1: Assuming an initial privacy budget of \u025b, the upload local model updates satisfied \u025b'-differential privacy.\nProve or analyze: It is known that this algorithm is an improvement on Laplace\nmechanism, where the adjusted privacy budget \u03b5' \u2208 [0, \u025b] Let D\u012f and D'\u012f be adjacent\ndata sets, F be the output function, $G(D'i) = (g\u0142 + \u2206g\u00ec, g} + \u2206g, \u2026\u2026,g + \u2206g)$,\noutput set $Y = (y\u00ec, y}, ..., y\u00ee\u00ba)$, $Aft = max||G(D\u2081) \u2013 G(D'i)||_1 = max||\u2211m=1\u2206gm||$.\nAccording to the Laplace noise probability density formula [17], then each round has:"}, {"title": "Experimental Evaluation", "content": ""}, {"title": "Experimental environment", "content": "The experimental environment is shown in Table 1, simulating the federated learning training process of 1 server and 100 clients. MNIST dataset and Logistics model were selected. The experimental divided into Equal and Random groups according to different partitioning methods of data sets [18]. The accuracy of the model named best_acc, will be analyzed and compared in the experiment."}, {"title": "Overall Performance", "content": "Based on the different numbers of selected clients and the initial privacy budget, we analyzed the experimental results. The specific experimental parameters are shown in Table 2."}, {"title": "Accuracy.", "content": "Based on the same initial privacy budget and with a different number of selected clients, the training curve results for the Equal group are shown in Figure 2, while the training curve results for the Random group are displayed in Figure 3."}, {"title": "Privacy budget", "content": "The initial privacy budget for each client in each round is set to 100, 120, 150, 170, and 200, respectively. The number of selected clients in each round is 3, 5, and 7. The experimental results for the Equal group are shown in Figure 6, and the results for the Random group are shown in Figure 7."}, {"title": "Comparison experiment", "content": "In this section, we conduct comparative experiments to validate the superiority of cosAFed over LAPFed, ADPFL [13], and cosFed [11] under the aforementioned"}, {"title": "Conclusion", "content": "With the wide application of federated learning, the problem of privacy protection has also aroused people's attention. As one of the common solutions, differential privacy [19] still needs to consider the balance between security and accuracy, as well as the adaptive addition of noise according to multiple factors. Therefore, we proposed an adaptive differential privacy method based on federated learning. By setting the adjustment coefficient and scoring function, the privacy budget can be dynamically adjusted according to multiple factors. Then, the scaling factor is calculated to process the local"}, {"title": "Conclusion", "content": "With the wide application of federated learning, the problem of privacy protection has also aroused people's attention. As one of the common solutions, differential privacy [19] still needs to consider the balance between security and accuracy, as well as the adaptive addition of noise according to multiple factors. Therefore, we proposed an adaptive differential privacy method based on federated learning. By setting the adjustment coefficient and scoring function, the privacy budget can be dynamically adjusted according to multiple factors. Then, the scaling factor is calculated to process the local model update, and finally adds the noise generated to get the noised local model update. Through method analysis and experimental evaluation, we analyzed the range of parameters and privacy. And we discovered that it reduces the privacy budget by about 16%, while the accuracy remains roughly the same. This method can enhance the data protection ability and ensure the accuracy of the model. In the future, we will further design a more suitable calculation method and use other implementation mechanisms such as Gaussian mechanism to verify the feasibility of this method."}]}