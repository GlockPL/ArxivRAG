{"title": "On Uncertainty In Natural Language Processing", "authors": ["Dennis Ulmer"], "abstract": "The last decade in deep learning has brought on increasingly capable systems that are deployed on a wide variety of applications. In natural language processing, the field has been transformed by a number of breakthroughs including large language models, which are used in increasingly many user-facing applications. In order to reap the benefits of this technology and reduce potential harms, it is important to quantify the reliability of model predictions and the uncertainties that shroud their development.\nThis thesis studies how uncertainty in natural language processing can be characterized from a linguistic, statistical and neural perspective, and how it can be reduced and quantified through the design of the experimental pipeline. We further explore uncertainty quantification in modeling by theoretically and empirically investigating the effect of inductive model biases in text classification tasks. The corresponding experiments include data for three different languages (Danish, English and Finnish) and tasks as well as a large set of different uncertainty quantification approaches. Additionally, we propose a method for calibrated sampling in natural language generation based on non-exchangeable conformal prediction, which provides tighter token sets with better coverage of the actual continuation. Lastly, we develop an approach to quantify confidence in large black-box language models using auxiliary predictors, where the confidence is predicted from the input to and generated output text of the target model alone.", "sections": [{"title": "1 Introduction", "content": "Every person's life is full of decisions. Is this restaurant really as good as the reviews suggest? Should I take a job here or take a more interesting job in a city far away? These decisions can be hard to evaluate, since not all necessary information is known beforehand: Restaurant reviews might be fraudulent or biased, and a promising job opportunity might turn out to be different than advertised. Compare that with the example of making a move in a game of chess: Chess is called a game with perfect information, so all the positions and possible moves of the pieces on the board are known, and one could in theory make the optimal move at every step (assuming good chess-playing abilities). However, in real life we often do not have all the information necessary to make a perfect decision. As such, humans take into account the uncertainty that permeates their decision-making in order to manage risk.\nIn this way, machines are (or should be) no different. The decades-old research in machine learning (ML) and especially the most recent advances in the last decade or so have produced systems that make decisions from the mundane (\u201cis this a picture of a cat or an airplane?\") to the potentially risky (\u201cwhat treatment should be recommended to this patient?\"). This trend has been accelerated by the paradigm of deep learning, which allows us to build evermore complex systems that could solve increasingly complex tasks. The complexity of these systems through comes at the cost of losing a detailed understanding of all the \u201ccogs and gears\" involved due to the sheer size of models (including millions, billions and sometimes even trillions of such \u201cgears\u201d). This fact has spurred numerous lines of research to develop methods to make\""}, {"title": "1.1 Motivation", "content": "Every person's life is full of decisions. Is this restaurant really as good as the reviews suggest? Should I take a job here or take a more interesting job in a city far away? These decisions can be hard to evaluate, since not all necessary information is known beforehand: Restaurant reviews might be fraudulent or biased, and a promising job opportunity might turn out to be different than advertised. Compare that with the example of making a move in a game of chess: Chess is called a game with perfect information, so all the positions and possible moves of the pieces on the board are known, and one could in theory make the optimal move at every step (assuming good chess-playing abilities). However, in real life we often do not have all the information necessary to make a perfect decision. As such, humans take into account the uncertainty that permeates their decision-making in order to manage risk.\nIn this way, machines are (or should be) no different. The decades-old research in machine learning (ML) and especially the most recent advances in the last decade or so have produced systems that make decisions from the mundane (\u201cis this a picture of a cat or an airplane?\") to the potentially risky (\u201cwhat treatment should be recommended to this patient?\"). This trend has been accelerated by the paradigm of deep learning, which allows us to build evermore complex systems that could solve increasingly complex tasks. The complexity of these systems through comes at the cost of losing a detailed understanding of all the \u201ccogs and gears\" involved due to the sheer size of models (including millions, billions and sometimes even trillions of such \u201cgears\u201d). This fact has spurred numerous lines of research to develop methods to make deep learning systems more robust, fair and safe.\nOne such line of research is concerned with uncertainty quantification, i.e. reflecting the degree of trustworthiness of a prediction. In systems with automatic decision-making, such scores can for instance be used to withhold a prediction or request human oversight. One popular example is autonomous driving: Consider an important traffic sign that cannot be accurately evaluated by the onboard computer, or a traffic situation that is hard to analyze. In these cases, a human driver might appreciate the opportunity to intervene with the car, e.g. by reducing its speed in the face of uncertainty, instead of the car sticking to a wrong assessment and endangering the driver's or other traffic users' lives.\nAt this point, the reader might be rightfully wonder whether such high-risk scenarios also exist for language applications. And indeed, such problems can arise in sometimes more, sometimes less obvious places. An intuitive application with these considerations is healthcare: More and more work has recently gone into building artificial intelligence (AI) systems that provide decision-support for medical staff. For instance, models could analyze text written by a user to detect signs of mental illness or triage (i.e., prioritize) pa- tients when resources are limited (Cohan et al., 2016; Rozova et al., 2022; Stewart et al., 2022). In this case, uncertainty can serve as a signal to request an additional human review of a case. Confident but wrong predictions here can lead to a waste of resources, a loss of trust of the medical professionals in the system, and, in the worst case, leaving urgent cases untreated. As another example, natural language systems are also used to assist in legal deliberations (Chalkidis et al., 2019a; Martinez-Gil, 2023; Chalkidis, 2023). While the scenario of a \u201crobo judge\u201d is usually ruled out, there still remain risks where models used for legal discovery or research might overlook relevant or produce misleading or incorrect outputs.\nUncertainty quantification is an active research area for systems that operate for instance on images or tabular data, but it has only recently started to receive attention in the natural language processing (NLP) community. This thesis gives an introduction to uncertainty quantification in machine learning and natural language processing for novices, summarizes the current state of progress in the field, and presents some novel and relevant methods for some of the most pressing problems for automated languages processing: These include for instance determining the most viable methods in text classification and proposing new approaches to calibrated\""}, {"title": "1.2 Applications", "content": "A lot of research on uncertainty quantification makes only superficial statements or tacit assumptions about its usefulness. The following, non-exhaustive list of aspects therefore underline potential practical use-cases.\nSafety. In general, uncertainty estimates can improve safety whenever a system with automated decision capabilities could potentially have real-world effects. Some of these situations are studied in the AI safety literature (see e.g. Amodei et al., 2016): They can include preventing an intelligent agent from exploring unsafe options, or acting in a risky manner as its environment changes from the version it was trained with, which is often referred to as distributional shift (Shimodaira, 2000; Moreno-Torres et al., 2012). In these cases, uncertain options can either be outright rejected or decisions can be delegated to a human user.\nTrust. In order to reap the benefits of automation and the abil- ity to extract intricate patterns from large amounts of data, users have to trust the system's output, or otherwise run the danger of being mislead. In the worst case, they might grow to ignore or even antagonize an automatic system. Since our systems are inanimate and often inscrutable building trust between humans and machines can be a tricky endeavor. Nevertheless, there exists a notion of trust that can be built by consistency (i.e., knowing what to expect from a system) and by using uncertainty to understand the behavior of a model (Jacovi et al., 2021). We dedicate Section 2.4 to discuss this connection in more detail.\nFairness. A long line of works has demonstrated how modern deep learning systems have a tendency to discriminate against subpopulations in the dataset and how to mitigate these effects (see Caton and Haas, 2024; Mehrabi et al., 2021 for an overview). Additional studies have argued that this is the result of human biases in the machine learning pipeline (Waseem et al., 2021) as well as biases and underrepresentation of groups in the training dataset (Meng et al., 2022a). In the latter case, specific uncertainty quantification methods can indicate whenever the correct prediction is uncertain due to a lack of similar training data (see Sections 2.2.2 and 2.2.3). In other instances, unfairness might occur when models favor a prediction corresponding to a majority group in the dataset"}, {"title": "1.3 Challenges in Natural Language Processing", "content": "is limited, and very few works develop solutions for this purpose specifically. This is disconcerting for the following reasons:\nChallenges of Natural Language. In contrast to other ma- chine learning problems, processing language is a rather messy affair. First of all, language is incredibly diverse, displaying vast differences between languages, dialects, demographics, domains or even individual speakers (Bender, 2011; Plank, 2016; Zampieri et al., 2020; van Esch et al., 2022). It is secondly embedded in a social and cultural context that is often necessary to understand its meaning (Hershcovich et al., 2022), and due to its paraphrastic nature, the idea same idea can often be expressed in a multitude of ways (Baan et al., 2023). Thirdly, the sequential nature often breaks the i.i.d. assumption that is a fundamental underlying as- sumption for many algorithms. One might assume that language data could just be treated as a time series and apply corresponding methods for uncertainty quantification (see e.g. Zhu and Laptev, 2017; Wang et al., 2020a; Blasco et al., 2024). Unfortunately though, encodings of language usually behave very erratically, as Figure 1.1b demonstrates. Modeling techniques for time series however subtly assume a certain behavior of the underlying data, e.g. a limit in the allowed rate of change encountered between two time steps. The sometimes abrupt token-level changes encoun- tered during language processing therefore prevent the application of time series modeling techniques.\nData Scarcity. Large amounts of both unstructured and annotated data exist for English, but most of the world's 7000+ languages are not blessed with such resources (Ruder, 2020; Joshi et al., 2020). Figure 1.1c shows the number of articles of a variety of Wikipedias on a log-scale. Due to its openness, Wikipedia remains a popular source of training data in NLP, however high-resource languages like English and German provide exponentially more potential training data compared to languages such as Afrikaans, Amharic or N'Ko. This runs contrary to the strength of modern deep learning architectures: Weak architectural inductive biases such as in transformers enable us to learn complex meaning representations, but only when enough data is supplied (Tay et al., 2023). In the case of low-resource languages for instances, such data is often not available, and thus we can end"}, {"title": "1.4 Objectives", "content": "This thesis analyzes the current state of uncertainty quantification research in deep learning and connects it to the methodological challenges that arise when they are applied to language data. As such, it aims to familiarize the reader with the most popular strategies for uncertainty quantification, as well as giving an intuition about their limitations. In this thesis, we seek to answer the following research questions:\nRQ1: How can uncertainty in NLP be characterized?\nUncertainty can be a somewhat vague concept, and its defini- tion is often passed over in different research works. Therefore, this thesis tries to gain a multi-disciplinary perspective on the matter, investigating different perspectives on the concept and how they are related.\nRQ2: How can choices in experimental design help to reduce and quantify uncertainty?\nAnother overlooked factor in empirical research in NLP is"}, {"title": "1.5 Publications", "content": "The following works were produced during the PhD and are dis- cussed in detail in this thesis (ordered chronologically). In all cases, the author's contributions amount to the main or complete share of the conception, implementation and description of ideas and experiments and the writing of the resulting publications, unless shared authorship is indicated.\n1.  Ulmer, Dennis*, and Giovanni Cin\u00e0*. \u201cKnow Your Lim- its: Uncertainty Estimation with ReLU Classifiers Fails at Reliable OOD Detection.\u201d In: Uncertainty in Artificial Intel- ligence. PMLR (2021) (discussed in Section 4.1).\n2.  Ulmer, Dennis, Christian Hardmeier, and Jes Frellsen. \u201cdeep-significance-Easy and Meaningful Statistical Signifi- cance Testing in the Age of Neural Networks.\" In: The\""}, {"title": "1.6 Structure", "content": "This thesis is structured as to provide a comprehensive overview over the topic of uncertainty from both a statistical and linguistic point of view. Both perspectives are then woven together in a overview over uncertainty quantification in deep learning and natural language processing. This part serves as a foundation for later chapters about the uncertainty in the experimental design in NLP, before concretely tackling specific problem scenarios: Uncertainty in text classification problems, uncertainty in language generation problems and uncertainty in lat- ter problems specifically involving the use of large language models.\nTo be more detailed, Chapter 2 introduces the reader to different concepts in uncertainty quantification and related literature. It begins with a definition of uncertainty from a variety of perspectives, for instance frequentist and Bayesian statistics, linguistics, and several popular approaches in deep learning. In this context, we also discuss Ulmer et al. (2023), which surveys works related to a novel class of uncertainty quantification methods called evidential deep learning. In the end, this includes a discussion of the relationship of uncertainty quantification with the end-user with both a motivation in trust and communication.\nWhile most of the research that makes up this thesis is focused on uncertainty in modeling language, uncertainty also occurs in the experimental design and execution of day-to-day research. Therefore, Chapter 3 presents an interlude on challenges with the notions of reproducibility & replicability in deep learning, their connection to uncertainty, and the use of statistical hypothesis testing, all of which inform the methodology of later chapters. This encompasses the published works of Ulmer et al. (2022a), giving an account of ongoing discussions about experimental methods in deep learning, as well as Ulmer et al. (2022c), introducing a package for better statistical hypothesis testing and its application to a case study with large language models.\nIn the subsequent Chapter 4, we tackle the problem of uncertainty in classification problems. First we demonstrate the pitfalls of uncertainty quanitification for classification using simple ReLU networks, drawing from Ulmer and Cin\u00e0 (2021). Afterwards, we discuss uncertainty quantification in the context of different classification problems specific to NLP, based on Ulmer et al. (2022b). Here we show how well exisiting methods for NLP fare on different languages and tasks, and"}, {"title": "2 Background", "content": "Uncertainty is a common occurrence in everyone's life, and thus most people have an intuitive understanding of the concept. To define it concretely, however, can be challenging. Colloquially, we might define uncertainty as a phenomenon or state that is filled with doubts, lack knowledge or that is simply hard to predict. In research papers, the term uncertainty often only remains vaguely defined, either building on an intuitive definition or presupposing a certain school of thinking.\nThe aim of this chapter is to bring some clarity to the different ways uncertainty is defined, and to give an fairly comprehensive account of its applications. This entails a journey from its origins in statistics (Sections 2.1.1 and 2.1.2) and linguistics (Sections 2.1.3 and 2.1.4) to its implementation with neural networks, specifically in deep learning (Section 2.2) and natural language processing (Section 2.3). In the latter contexts, this comes with a focus on modeling uncertainty, and this is indeed also where many of the research papers in the field end. Therefore, an additional goal of this chapter is to not take uncertainty modeling as the ultimate goal per se, but to see beyond it and grasp the bigger picture. As uncertainty quantification is often motivated by increasing trustworthiness and safety, we take a closer look at the relationship between uncertainty and trust in Section 2.4, as well as how to communicate uncertainty in Section 2.5. Furthermore, the chapter outlines diverse applications of uncertainty in Section 2.6.\nWe start by defining the most central concept in this thesis: Un- certainty. Since this thesis is focused on NLP, we aim to define"}, {"title": "2.1 What Is Uncertainty, anyway?", "content": "the concept from all the perspectives modern NLP touches on. This includes building up some basic concepts from frequentist and Bayesian statistics as well from different parts of linguistics."}, {"title": "2.1.1 The Frequentist Perspective", "content": "Frequentist statistics in an approach to statistics that aims to make inferences and draw conclusions from sampled data, alone. The term is based on the fact that probabilities are seen as equivalent to the observed frequencies of events in the data, assuming (potentially infinitely) many repetitions of an experiment (Willink and White, 2011). Let us reason about the popular example of a coin flip here to illustrate this notion. We are given a coin and would like to estimate the probability of heads, which we define as the parameter of interest to estimate and will denote by $\u03b8$. We do not know whether the coin is fair, so we flip it a number of times and count the heads and tails to estimate this probability. We obtain the following five coin flips:\nBased on this experiment, we then estimate the probability of heads as $0 = \\frac{\\text{#heads}}{\\text{#coin flips}} = \\frac{2}{5} = 0.4$. However, how can we be sure that this reflects the actual probability of heads? We thus repeat the experiment three more times, and obtain:\nAs we gather more and more samples and take their average, we will provably converge to the true value of $\u03b8$ in the limit due to the law of large numbers (Dekking et al., 2005). But in light of a limited number of samples like above, how can we quantify the uncertainty of our estimate?"}, {"title": "2.1.2 The Bayesian Perspective", "content": "Bayesian statistics delineates itself from frequentist statistics by seeing probability itself as more than just the mere relative fre- quency of an event, and instead as the degree of belief in the occurrence of an event. This difference has caused (and is still causing) ideological chasms among statisticians, as illustrated by the quote above. The name of Bayesian statistics is derived from Thomas Bayes, an English presbytarian minister in the 18th cen- tury who first formulated the eponymous Bayes' theorem. It should be noted however that Bayes only formulated his theory in a very specific setting, and that a general version of Bayesian statistics was instead pioneered by Pierre-Simon Laplace (McGrayne, 2011; Leonard, 2014). The theorem can be formulated as follows: Given a set of observations $D$ and a parameter of interest $\u03b8$, we can express the probability of the parameter given the observational data as\n$p(\u03b8 | D) = \\frac{p(D | \u03b8)p(\u03b8)}{p(D)}$\nwhere the different parts of the equation are commonly referred to as the posterior $p(\u03b8 | D)$, the likelihood $p(D | \u03b8)$, the prior $p(\u03b8)$, and the evidence $p(D)$. We already discussed likelihoods in the previous section. The prior $p(\u03b8)$ is a probability distribution over possible values of $\u03b8$, and thus allows us to express our prior belief by attributing higher probability to values of $\u03b8$ we deem more likely. This also implies a philosophical difference with frequentist statistics: While $\u03b8$ was treated as an unknown constant before, it is now seen as another random variable. The evidence $p(D)$ encodes the general probability of the observed data under any value of $\u03b8$. This somewhat hidden interpretation becomes more clear when rewriting the term:\n$p(D) = \\int p(D, \u03b8)d\u03b8 = \\int p(D | \u03b8)p(\u03b8)d\u03b8.$\nWe can therefore interpret the evidence as the likelihood of the data averaged over all possible parameter values of $\u03b8$, weighed by their prior probabilities. Lastly, the posterior $p(\u03b8 | D)$ describes a probability distribution over values of $\u03b8$ given our observations. We can think of the posterior as starting with our prior belief, using the data to update it and arriving at a final distribution that takes both of these into account. This has several advantages: We can now choose to encode our suspicions about the value of the target parameter into the prior. But as we will see, obtaining more and more data points results in outweighing the prior belief, completely relying on the observations in the limit.\nCoin Flipping Redux. We now illustrate these concepts using the coin flipping example from Section 2.1.1, showing how uncer- tainty is modeled from the Bayesian perspective. In order to do so, we first have to make some design choices, i.e. the choice of likelihood and prior function as well as prior parameters. We again use the Bernoulli likelihood from the previous section, and now would like to define a prior over $\u03b8$. A good choice for a prior for the Bernoulli distribution is the Beta distribution:\n$\\text{Beta}(\u03b8; \u03b1_1,\u03b1_2) = \\frac{1}{\\text{B}(\u03b1_1,\u03b1_2)} -\u03b8^{\u03b1_1-1}(1 - \u03b8)^{\u03b1_2-1}$\\\n$\\text{B}(\u03b1_1,\u03b1_2) = \\frac{\u0393(\u03b1_1)\u0393(\u03b1_2)}{\u0393(\u03b1_1 + \u03b1_2)},$"}, {"title": "2.1.3 The Linguistic Perspective: Underspecification, Ambiguity & Vagueness", "content": "Linguistics can be categorized into multiple sub-disciplines that are concerned with different aspects of human language (Akmajian et al., 2017). This thesis focuses on written language, which is why we will not discuss any uncertainty in e.g. phonetics and phonology (the studies of the production of sounds and how they are organized in a language). Instead, we focus on the following three levels: se- mantics, syntax and pragmatics. In linguistics, uncertainty appears through different phenomena, for instance ambiguity or polysemy (Tuggy, 1993; Kennedy, 2011), underspecification (Pustejovsky, 1991, 2017) and vagueness (Tuggy, 1993; Brown, 2005; Kennedy, 2011), which manifests in different ways in different linguistic levels. This creates uncertainty by creating multiple different interpre- tations of a sentence, which are often but not always resolved through additional context, either linguistic, situational or from world knowledge. Describing this interplay between uncertainty and resolve on different linguistic levels is goal of this chapter.\nUncertainty in Semantics. The field of semantics is concerned with the literal meaning of words and the ways in which these are combined (Kearns, 2017). One way in which uncertainty arises in semantics is polysemy, a phenomenon where two or more distinct senses are associated with the same word (Gries, 2015). Gries for instance mentions the examples of \u201cI emptied the glass\" compared to \u201cI drank a glass\", where glass corresponds in the first case to a container, and to its content in the second. A more subtle case of polysemy is exemplified by the examples\n(a)  Jocelyn walked to the school.\n(b)  The concerned mother talked to the school.\nwhere \u201cschool\u201d in the former refers to the physical building, and the latter to the an administrative unit inside the organization that operates within the school building (Frisson, 2009). Resolving these cases can be highly non-trivial, leading in NLP to the field of word sense disambiguation (see e.g. Sch\u00fctze, 1997; Agirre and Edmonds, 2007; Navigli, 2009). Another case is homonymy, where two unrelated meanings map onto the same form (Devos, 2003), as in the case of bank as a financial institute, a place for sitting, or the terrain alongside a river bed. Vagueness can be defined in contrast to these notions as whether \"a piece of semantic information is part of the underlying semantic structure of the item, or the"}, {"title": "2.1.4 The Linguistic Perspective: Expressing Uncertainty", "content": "Besides paraphrastic language, a different type of uncertainty lies in explicit uncertainty expressions by the speaker. This spans the overall tone of a series of utterances to the usage of diverse linguistic expression (see e.g. Rubin, 2006 pp. 21-40; Lorson et al., 2023, Zhou et al., 2023), for instance hedges (Lakoff, 1973; Fraser, 1975; Prince et al., 1982; Holmes, 1982), i.e. words or phrases to express ambiguity or uncertainty. Additionally, uncertainty expressions might also be chosen circumstantially, for instance based on whether the other speaker is cooperative or uncooperative (Lorson et al., 2021), politeness (Sirota and Juanchich, 2015; Holtgraves and Perdew, 2016) or power differences between"}, {"title": "2.1.5 A Pragmatic Answer", "content": "The astute reader might have noticed that while the title of Section 2.1 was \u201cwhat is uncertainty, anyway?\u201d, it might appear that we have thus far been tiptoeing around this question, enumerating and explaining different perspectives to it without giving a satisfying answer.\nIn the end, uncertainty is a multifaceted and perhaps vague concept, whose definition varies based on the phenomenon of interest. At its very core, it describes a lack of knowledge about the true state of the world among competing alternative states. The definitions of these world states can differ tremendously on the context, and can include all the possible interpretations of the sentence \u201cI saw her duck\u201d to plausible values of a data-generating parameter $\u03b8$. For the purpose of this thesis, we reduce its definition to the following aspects: Firstly, there is the uncertainty that is inherent to language described in Section 2.1.3, describing how interpretation and production are not a one-to-one processes of meaning; Secondly, the statistical models we apply to language are themselves faced with multiple possible specifications and can produce different potential predictions. As these models are at best informative but incomplete abstractions of reality that are fit on finite data, we accept their uncertainty as the price for practicality. While the last two points refer to uncertainty as phenomena, however and thirdly, uncertainty is also a tool: It enables us to reason about and express our own knowledge about possible states, and convey our lack thereof. This notion captures"}, {"title": "2.2 Uncertainty in Deep Learning", "content": "In contrast to the interviewer's quote in the epigraph, the very promising approach of using computational models of neurons did not completely die out, but rather remained dormant for decades. First known as cybernetics at the time of the first models of artificial neurons (McCulloch and Pitts, 1943; Rosenblatt, 1958; Rosenblatt et al., 1962), it became known as connectionism in the 1980\u20131990s, before assuming its current name deep learning in 2006 (Goodfellow et al., 2016). Nowadays, deep learning is commonly and vaguely defined as a family of machine learning networks that employ artifi- cial neural networks of increasing depth (Goodfellow et al., 2016).\nUncertainty in deep learning materializes in a wide variety of ap- proaches, as depicted as a hierarchical taxonomy in Figure 2.7: As shown in Section 2.2.1, the frequentist school uses neural networks"}, {"title": "2.2.1 Frequentist Neural Networks", "content": "Before we turn to how frequentist methods allow the quantification of uncertainty in neural networks, we first review the similarities in parameter estimation when applied to neural predictors. In the following, we term the application of frequentist methods to neural network as frequentist neural networks.\nAs introduced in Section 2.1.1, frequentist statistics refers to an interpretation of probability as the relative frequency of an event. In a neural network setting, the estimation of the parameter(s) of interest, in this case the network's parameters \u03b8, is analogous to the maximum likelihood estimation in Section 2.1.1. The main differences are that firstly, instead of parameterizing a distribution with $\u03b8$ directly, we parameterize it with the prediction obtained from a neural net with parameters $\u03b8$. Whereas in the coin flipping example, $\u03b8$ referred to the probability of heads, a neural network in a binary classification setting is equipped with some parameters $\u03b8$ now predicts the probability of the positive class $p$. And secondly, due to the model's non-linear and hierarchical dependencies, the solution to $\u03b8$ is not available in closed form anymore. Instead, we iteratively optimize $\u03b8$ through procedures such as gradient descent, where we compute the gradient of some loss function w.r.t. the parameters and take a step in the direction of the (anti- )gradient. The loss functions vary depending on the intended purpose, but in some cases can be directly related to maximum likelihood estimation. In analogy to the coin flipping in the previous section, a network trained on a binary classification task for instance predicts $p = \u03c3(f_\u03b8(x))$ (with $\u03c3(\u00b7)$ denoting the sigmoid function) and is then optimized using the binary cross-entropy loss (here for a single input using a gold label $y \u2208 \\{0,1\\}$):"}]}