{"title": "SES: BRIDGING THE GAP BETWEEN EXPLAINABILITY AND\nPREDICTION OF GRAPH NEURAL NETWORKS", "authors": ["Zhenhua Huang", "Kunhao Li", "Shaojie Wang", "Zhaohong Jia", "Wentao Zhu", "Sharad Mehrotra"], "abstract": "Despite the Graph Neural Networks' (GNNs) proficiency in analyzing graph data, achieving high-\naccuracy and interpretable predictions remains challenging. Existing GNN interpreters typically\nprovide post-hoc explanations disjointed from GNNs' predictions, resulting in misrepresentations.\nSelf-explainable GNNs offer built-in explanations during the training process. However, they cannot\nexploit the explanatory outcomes to augment prediction performance, and they fail to provide\nhigh-quality explanations of node features and require additional processes to generate explainable\nsubgraphs, which is costly. To address the aforementioned limitations, we propose a self-explained and\nself-supervised graph neural network (SES) to bridge the gap between explainability and prediction.\nSES comprises two processes: explainable training and enhanced predictive learning. During\nexplainable training, SES employs a global mask generator co-trained with a graph encoder and\ndirectly produces crucial structure and feature masks, reducing time consumption and providing\nnode feature and subgraph explanations. In the enhanced predictive learning phase, mask-based\npositive-negative pairs are constructed utilizing the explanations to compute a triplet loss and enhance\nthe node representations by contrastive learning.\nExtensive experiments demonstrate the superiority of SES on multiple datasets and tasks. SES\noutperforms baselines on real-world node classification datasets by notable margins of up to 2.59%\nand achieves state-of-the-art (SOTA) performance in explanation tasks on synthetic datasets with\nimprovements of up to 3.0%. Moreover, SES delivers more coherent explanations on real-world\ndatasets, has a fourfold increase in Fidelity+ score for explanation quality, and demonstrates faster\ntraining and explanation generating times. To our knowledge, SES is a pioneering GNN to achieve\nSOTA performance on both explanation and prediction tasks.", "sections": [{"title": "1 Introduction", "content": "Graph neural networks (GNNs) have become pivotal in handling graph data, proving essential in a variety of applications\nincluding node classification [1, 2], knowledge representation [3, 4], molecular classification [5, 6], traffic prediction [7,\n8], recommendation system [9, 10], sentiment analysis [11, 12], pose estimation [13, 14], and text classification [15,\n16], etc.\nOne group of existing research on graph neural networks focuses on developing novel architectures to enhance predictive\naccuracy. Typical GNNs include graph convolution networks (GCN) [17], graph attention networks (GAT) [18], Graph-"}, {"title": "2 Related Works", "content": "2.1 Prediction of Graph Neural Networks\nGraph neural networks (GNNs) have shown a strong representation ability for dealing with graph format data [36].\nThe prediction of GNNs is to utilize nodes' or graphs' representation to classify their labels for the downstream tasks.\nGCN [17] is a typical GNN based on the message-passing among neighbors to aggregate information. GAT [18] utilizes\nan attention mechanism to enhance the ability of information aggregation based on GCN. GraphSAGE [19] applies a\nnew sampling method to aggregate neighbor nodes to make GNN more scalable and effective. ARMA [21] is a graph\nconvolutional layer by autoregressive and moving average filters to provide a more flexible frequency response and\nbetter global graph structure representation. FusedGAT [23] is an optimized version of GAT that fuses message-passing\ncomputation for accelerated execution and lower memory footprint. ASDGN [24] is a stable and non-dissipative DGN\ndesign framework conceived through ordinary differential equations and preserves remote information between nodes.\nRAHG [37] is a role-aware GNN considering role features to improve node representation. These GNNs focus on\nimproving the prediction performance without providing explanations.\n2.2 Explanation of Graph Neural Networks\nTo offer interpretations of GNNs, a considerable amount of methods or GNN explainers are proposed [38]. Many\npopular explainers are post-hoc models that can be categorized into instance-level and model-level approaches. GNNEx-\nplainer [26] is the pioneering instance-level model that provides edge and feature explanations by maximizing the mutual"}, {"title": "2.3 Self-Supervised Learning of Graph Neural Networks", "content": "Self-supervised learning has emerged as a promising technique for training deep learning models without extensive\nlabeled data [39]. In GNNs, self-supervised learning has gained significant attention due to its ability to learn useful\nrepresentations from graph data without extra labeling works [40]. GraphCL [41] develops comparative learning with\ndata enhancement for GNN pre-training to address the heterogeneity of graph data. GCC [42] is an unsupervised\ngraph representation learning framework that captures common network topology properties from multiple networks.\nMERIT [43] combines the advantages of Siamese knowledge distillation and conventional graph contrastive learning.\nHeCo [44] contrasts heterogeneous graphs using two perspectives (network schema and meta-path) and trains an encoder\nto maximize the mutual information between node embeddings. MolCLR [45] is a self-supervised GNN framework\nfor molecular contrast learning, proposing enhancement methods to ensure consistency within the same molecule and\ninconsistency among different molecules. These methods are effective in the pre-training and general training phases of\nGNNs. However, there are currently no approaches to integrate self-supervised learning into explainable GNNs training\neffectively."}, {"title": "3 Problem Definition", "content": "A graph is denoted by $G = (V, A, X)$, where $V = \\{v_1,\\cdots,v_n\\}$ denotes the node set with $N$ nodes, $A \\in \\mathbb{R}^{N \\times N}$\ndenotes the adjacency matrix of G, $a_{ij} \\in A$ and $a_{ij} = 1$ if $v_i$ and $v_j$ are connected. $X \\in \\mathbb{R}^{N\\times F}$ represents the node\nfeatures with $F$ dimensions.\nFor predicting the node's label task (i.e., node classification task), the label set of nodes is denoted as $Y_L$. The labeled\nand unlabeled node sets from $V$ can be written as $V_L = \\{v_{l1},\\cdots, v_{l n}\\}$, and $V_U = \\{v_{u1},\\cdots, V_{u n}\\}$. The objective of\nthe node classification task is to utilize the graph $G$ and the set of labeled nodes $V_L$ to train the GNN, enabling the\nprediction of labels for the unlabeled nodes in $V_U$.\nFor the instance-level explanation, a model offers explanations for node features or substructures. A feature explanation\nin SES represented as $E_{feat} \\in \\mathbb{R}^{N\\times F}$ is provided to clarify the significance of each feature dimension in predicting\na node's label. Additionally, a substructure explanation denoted as $E_{sub} \\in \\mathbb{R}^{N\\times N}$ in SES is applied to highlight the\nimportance of a node's local neighbors.\nFollowing GNNExplainer [26], the problem of describing a self-explainable and self-supervised GNN can be written as:\nGiven a general graph $G(V, A, X)$, with labeled and unlabeled node set $V_L$ and $V_U$, we learn a self-explainable and\ndiscriminative self-supervised GNN $f: v_{ui} \\rightarrow y$, which provide feature explanation $E_{feat}$ and substructure explanation\n$E_{sub}$ as the instance-level explanation for each $v_{ui} \\in V_U$.\nTo facilitate comprehension, symbols used in this paper and their definitions are summarized in TABLE 2."}, {"title": "4 Proposed Method", "content": "The framework of SES is depicted in Fig. 2. SES consists of two primary phases: explainable training and enhanced\npredictive learning. SES employs a mask generator co-trained with a graph encoder in the explainable training phase.\nThe mask generator generates feature masks for nodes and structure masks for the subgraphs and is optimized during\ntraining. In the enhanced predictive learning phase, SES constructs positive-negative node pairs based on the structure\nmasks. These node pairs are then utilized to optimize the node features, which are subsequently processed by a shared\ngraph encoder. By contrastive learning, SES designs a triplet loss to supervise the representation learning of nodes.\nSupervised learning is also employed to bolster the contrastive learning. The detailed procedures of SES is delineated\nin Algorithm 2."}, {"title": "4.1 Graph Encoder and Mask Generator", "content": "4.1.1 Graph Encoder\nWe utilize a GNN backbone as a graph encoder to generate node embeddings. The GNN backbone is flexible and can be\nGCN [17] and GAT [18], etc. A general process of one convolution layer $Conv$ in GNN based on message passing [46]\nfor a node $v$ is described as follows:\n$h_v^{(l)} = COB^{(l)} \\{AGG^{(l)} (h_u^{(l-1)} : u \\in N(v)), h_v^{(l-1)}\\},$ (1)\nwhere $h_v^{(l)}$ is the feature vector of $v$ in the $l^{(th)}$ layer and $N(v)$ is the neighbors of $v$. $h_v^{(l-1)}$ represents the feature\nvector of $v$ in the $(l \u2013 1)^{(th)}$ layer. $AGG$ is the function to aggregate features of nodes with their neighbors. $COB$ is\nthe combination function to update the representation of nodes.\nThe process of a graph encoder with two convolution layers in Eq. (1) is summarized as follows:\n$Z = Conv_2(\\sigma(Conv_1(A, X)), A),$ (2)\nwhere $\\sigma$ is the activation function, and $Z$ is the output of the graph encoder used for node classification. $H \\in\n\\mathbb{R}^{N\\times F_{hid}} = Conv_1(A, X))$ is the output from the first convolution layer $Conv_1$ employed for feature mask generation.\n$F_{hid}$ denotes the hidden size of $Conv_1$. $X = \\{h_v, v \\in V\\}$, $H = \\{h_v, v \\in V\\}$, $Z = \\{h_v, v \\in V\\}$.\n4.1.2 Mask Generator\nThe masks are weight matrices to provide explanations of features and structures, emphasizing crucial feature dimensions\nand neighboring nodes. The framework of the global mask generator is depicted in Fig. 3."}, {"title": "4.2 Explanation with Masks", "content": "After explainable training, a matrix $M_f \\in \\mathbb{R}^{N\\times F}$ is obtained from a well-trained mask generator and contains important\nweights for node features. By applying $M_f$ to the feature matrix $X$, we derive an explanation matrix $E_{feat} = M_f \\odot X$,\nfor the nonzero features of the node to provide feature explanations, where $\\odot$ is Hadamard product represents the\nmultiplication of the corresponding elements.\nSimilarly, the subgraph explanation $E_{sub}$, computed by $E_{sub} = M_s \\odot A^{(k)}$, provides important weights for the central\nnodes' k-hop neighbors. By covering these weights over the edges, SES provides explanations of the central node's\nneighbors in the subgraph."}, {"title": "4.3 Construction of Positive-Negative Pairs", "content": "To enhance the improvement of GNN's training, we extend supervisory information from the generated masks.\nSpecifically, $M_s$ is used to construct positive and negative sample pairs for subsequent contrastive learning. This\nprocess involves a k-hop weight matrix, denoted as $\\hat{A}^{(k)}$. For each node $v_i$ in $G$, we sort and sample $v_j$'s neighbors\nbased on their weights in $\\hat{A}^{(k)}$, forming the positive node set $S_p(v_i)$. In addition, we sample an equal number of nodes\nfrom $P_n (v_i)$ to construct the negative node set $S_n(v_i)$. The complete process is summarized in Algorithm 1, wherein\nsorted returns sorted neighbors of $v_i$ according to their weights in $\\hat{A}^{(k)}$, len returns the number of neighbors of $v_i$, and\nrandom_sample randomly selects $num\\_sample$ nodes from $P_n (v_i)$ to serve as negative samples."}, {"title": "4.4 Learning Objective", "content": "4.4.1 Explainable Training\nDuring the explainable training phase, we simultaneously train the graph encoder and the mask generator."}, {"title": "4.5 Time Complexity", "content": "During the training process, the time consumption of the explainable training phase in SES contains the following main\ncomponents: the backbone GNN, the computation of the mask generator, and Algorithm 1. In the case of backbone\nGCN [17], the model complexity is $O(|E| \\times F \\times F_{hid})$. The Algorithm 1 has a time complexity of $O(Nlog(N))$.\nFor the feature mask $M_f$ in the mask generator, its time complexity is $O(F \\times F_{hid})$. For the structure mask $M_s$, and\nthe negative mask $M_{sneg}$ in the mask generator both have time complexities of $O(|V| \\times N_k \\times F_{hid})$. Similarly, the\ntime complexity for the enhanced predictive learning phase is $O(|E| \\times F \\times F_{hid}) + O(F_{hid} \\times \\Sigma N_i^2)$. Assuming\nsparsity in the graphs, we can simplify the complexities by considering $|E| = |V|\\times K_1/2$, $N_k = |V| \\times K_2/2$, where\n$K_1$ and $K_2$ is the average degree of nodes in $V$ and $A^{(k)}$ respectively. After discarding lower-order terms, the final\ntime complexity for SES is $O(|V|^2 \\times K_1 \\times K_2 \\times F)$.\nThe time complexity of SEGNN [33] can be expressed as $O(F \\times \\Sigma_{v_i \\in V_t} \\Sigma_{v_j \\in V_t} \\Sigma_{l \\in S_t} |E_t| \\cdot |E_1|)$, where $V_t$ is the set of\ntarget nodes, $S_t$ denotes the sampled positive and negative nodes, and $E_t$ are the edges of the union set of $V_t$ and $S_t$. The\nnumber of nodes in the union set are comparable to $|V|$. The time complexity of SEGNN is $O(|V|^3 \\times K_3 \\times K_4 \\times F)$,\nwhere $K_3$ is the average degree of nodes of the union set, and $K_4$ is the average number of the sample nodes. Neglecting\nthe constants, the time complexity of SEGNN is higher than SES."}, {"title": "5 Experimental Results", "content": "5.1 Datasets Description\n5.1.1 Real-World Datasets\nThree classic datasets are considered: (1) Cora [48] is a citation network with 2,708 nodes and 10,556 edges. Nodes\nrepresent documents, and edges represent citation links. Each paper is represented by a 1433-dimensional word vector\nand labeled into one of seven machine-learning topics. (2) CiteSeer [48] is also a citation network with 3,327 nodes\nand 9,104 edges; it has six classes and the data structure is the same as Cora. (3) PolBlogs [49] is a graph with 1,490\nnodes and 19,025 edges. A node represents political blogs, and edges represent links between blogs. Each node has a\nlabel that indicates its political inclination: liberal or conservative. (4) CS: The Coauthor CS network from [50] with\n18,333 nodes and 163,788 edges. Nodes represent authors that are connected by an edge if they co-authored a paper.\n5.1.2 Synthetic Datasets\nFollowing previous works [26, 27] to construct four synthetic datasets to validate the performance of GNNs on\nexplainability tasks: (1) BAShapes contains a Barabasi-Albert graph with 300 nodes and a set of 80 \u201chouse\u201d-structured\ngraphs connected to it. The nodes are classified into four categories based on their structural roles. (2) BACommunity"}, {"title": "5.2 Baselines", "content": "We consider the following strong baselines and verify whether the SES can improve the performance of backbone\nGNNs and achieve SOTA performance on the node classification tasks. We only report results of SES with GCN and\nGAT as backbone GNNs following ProtGNN [34] and report the best performance of ProtGNN by GCN and GAT as\nbackbone GNNs. PxGNN [35] has neither been published nor has public codes, making it unsuitable as a baseline.\n\u2022 GCN [17] is a typical graph neural network that integrates nodes' information from their neighbors.\n\u2022 GAT [18] fuses the attention mechanism based on GCN and enhances the ability of node representation.\n\u2022 UniMP [22] is a novel unified message-passing model incorporating feature and label propagation at training\nand inference time.\n\u2022 SEGNN [33] can find K-nearest labeled nodes for each unlabeled node to give explainable node classification.\n\u2022 FusedGAT [23] is an optimized version of GAT that fuses message-passing computation for accelerated\nexecution and lower memory footprint.\n\u2022 ProtGNN [34] combines prototype learning with GNNs and provides a new perspective on the explanations of\nGNNs.\n\u2022 ASDGN [24] is a framework for stable and non-dissipative DGN design conceived through the lens of ordinary\ndifferential equations.\nFor the explainability tasks of GNNs, we consider the following strong baselines. Note that the SEGNN and ProtGNN\nare unsuitable for the feature explanation tasks.\n\u2022 GRAD [26] is a gradient-based method that computes the gradient of the GNN's loss function for the adjacency\nmatrix and the associated node features.\n\u2022 ATT [18] is a graph attention network that is used to provide explanations.\n\u2022 GNNExplainer [26] is a GNN explanation method that maximizes the mutual information between a GNN's\nprediction and distribution of possible subgraph structures to provide consistent and concise explanations for\nan entire class of instances.\n\u2022 PGExplainer [27] employs deep neural networks to parameterize the interpretation generation process, which\nmakes it a natural way to explain multiple instances collectively.\n\u2022 PGMExplainer [28] utilizes a generation probability model for graph data. This approach enables the model to\nlearn concise underlying structures from observed graph data.\n\u2022 GraphLIME [29] is a model-agnostic, local, and nonlinear explanation method for GNN for node classification\ntasks motivated from LIME [51], which uses Hilbert-Schmit Independence Criterion (HSIC) Lasso, a nonlinear\ninterpretable model."}, {"title": "5.3 Experimental Settings", "content": "For the prediction task on node classification, we randomly divide the datasets as 60% training set, 20% validation\nset, and 20% test set [52]. For the explanation task, the synthetic datasets are divided into an 80% training set, 10%\nvalidation set, and 10% test set corresponding to the settings of [26]. For SES and baselines, the learning rate with the\nAdam optimizer is set to 0.003. The hidden layer size is set to 128. The sample ratio in Algorithm 1 is set to 0.8 and the\nmargin $m$ in Eq. (12) is set to 1.0."}, {"title": "5.4 Node Classification", "content": "We evaluated the performance of SES and baselines on real-world datasets for node classification. The experimental\nresults are summarized in TABLE 3. The second highest performance is highlighted with underline. The Imp. represents\nimprovements by SES compared to the best baseline."}, {"title": "5.5 Explanation Qualification", "content": "We performed experiments on widely applied synthetic datasets to compare SES with other GNN explanation methods.\nFollowing the experimental settings in GNNExplainer [26], we employed ground-truth explanations available for the\nsynthetic datasets to quantify the accuracy of different explanation methods. The AUC scores for explanation tasks are\nsummarized in TABLE 4.\nSES performs superior on the BAShape and Tree-grid datasets over all other methods, demonstrating significantly\nenhanced interpretability. SES showcases the least relative improvement of 2.5% on the BAShape dataset and 3.0% on\nthe Tree-grid dataset. SES also performs outstandingly on the Tree-Cycle dataset, achieving an AUC score close to\n100%. On the BACommunity dataset, both SES and PGExplainer achieved comparable performance. SEGNN performs\nwell on the BAShapes while performing much less on the other three datasets. SES showcases its effectiveness in\nproviding accurate and reliable explanations of synthetic datasets."}, {"title": "5.6 Time Consumption", "content": "We evaluate and compare the time consumption post-hoc explainers and self-explainable GNNs to generate explanations\non Cora. Traditionally speaking in machine learning, inference time is the duration time from the input to a well-trained\nmodel to its producing outputs, which are milliseconds and is not particularly meaningful for comparing GNNs. we\ndefine inference time for generating explanations as the period required from a well-trained backbone GNN to produce\nexplanations for all nodes following the way of PGExplainer [27]. For GNNExplainer and GraphLIME, despite\nbackbone GNNs have been trained, they necessitate re-training for each individual node [26, 29], so the inference time\ncounts on the re-training duration. For SES and SEGNN, the inference time incorporates the training time since they\ntrain backbone GNN models while giving explanations for the same process. This allows for a fair comparison of\nthe time consumption between post-hoc methods and self-explainable GNNs. However, ProtGNN cannot construct\nexplainable subgraphs for node classification tasks, making it inapplicable for this specific task. The explainable training\nof SES spans 300 epochs with an additional 15 epochs dedicated to enhanced predictive learning. The training epochs\nfor other explainers are aligned with SES. The experiments were on an NVIDIA RTX 3090 GPU, with GCN as the\nbackbone GNN, and are presented in TABLE 6."}, {"title": "5.7 Parameter Sensitivity", "content": "The parameter sensitivity analyses are presented in Fig. 4, the learning rate (lr), k (k-hop), and hyperparameters $\\alpha$ and\n$\\beta$ exhibit varying degrees of influence on SES's performance.\nIn Fig. 4(a), SES (GCN)'s accuracy on CiteSeer improves as the learning rate and k decrease. In the case of a larger\nneighborhood in Cora, SES (GCN) performs well owing to the wider range of papers cited from different fields.\nAdditionally, the effect on the PolBlogs increases with the learning rate. Fig. 4(b) indicates that assigning higher\nweights to $\\alpha$ and $\\beta$ enhances the performance of SES (GCN) on Cora and PolBlogs. It suggests that mask training and\nself-supervised learning contribute to characterizing GNNs. However, a lower $\\alpha$ weight is required for CiteSeer."}, {"title": "5.8 Visualization", "content": "We visualized the learned node representations of GNNs to validate their representation ability on the CiteSeer dataset.\nTwo self-explainable GNN models (SEGNN and ProtGNN) are applied. The output vectors of these GNNs are\noriginally 128-dimensional and transformed into two dimensions by TSNE [56]. Fig. 5 depicts the visualizations\nof node representations. We observe that SES (GCN) and SES (GAT) provide more densely connected clusters\nthan baselines. To qualify the clustering effectiveness of different methods, classical clustering evaluation indicators\nSilhouette score [54] and Calinski Harabasz score [55] are employed, with higher values indicating better outcomes.\nTABLE 9 presents the assessment of cluster effects following the visualizations of Fig. 5. From TABLE 9, SES exhibits\ntighter clusters among nodes belonging to the same classes, resulting in higher scores.\nTo comprehensively evaluate the effects of different explanation methods, we visualize the explanations of GNNEX-\nplainer, PGExplainer, PGMExplainer, and SES on four synthetic datasets [26]. These visualizations highlight the\nimportant subgraph structures of the datasets. In the visualizations, the color of the edges in the corresponding graphs\nrepresents the importance weight, with darker edges indicating higher importance. The visualizations are presented"}, {"title": "5.9 Ablation Studies and Variants", "content": "Ablation studies and variants are conducted to investigate the contributions of components in SES. To verify the\nfunctionality of the feature mask on GNN, we remove the $M_f$ on node features and denote it as -\\{$M_f$\\}. Similarly,\na general adjacency matrix rather than $M_s$ on adjacency put into the graph encoder during the enhanced predictive\nlearning phase is denoted as \u2013\\{$M_s$\\}. To verify the guiding effect of cross-entropy on triplet loss, we remove $L_{xent}$ and\ndenote it as -\\{$L_{xent}$\\} in the enhanced predictive learning phase. To verify the effectiveness of enhanced predictive\nlearning, the triplet loss is removed, denoted as -\\{Triplet\\}. To validate the importance of explainable training, we\nreplace the mask generator with the post-hoc GNNs (GNNExplainer (GEX), PGExplainer (PGE)) and denote them\nas +\\{epl\\}. The performance of ablation studies is reported in TABLE 10. The performance of SES was significantly\nimpacted after removing the contrast learning. It suggests that self-supervised learning is significant in the prediction\nperformance of SES, and the interpretation results play a crucial role in providing feedback and enhancing the predictive\nlearning of models. The cross-entropy also plays an important role in enhancing predictive learning since removing\n$L_{xent}$ in the second phase in SES leads to a sharply decreased accuracy, especially for the CS dataset. The absence\nof $M_f$ and $M_s$ both results in a decline in performance. This indicates that interpreting node features and structure\nimportance plays significant roles in SES. Integrating self-supervised learning and the mask generator that considers\nstructural and feature explanations leads to the best performance in SES. The results of +\\{epl\\} combined with masks\nusing the post-hoc model are worse than SES and most variants of SES. This indicates that the explanations provided\nby SES are more reliable and better suited for downstream constrastive learning."}, {"title": "5.10 Case Studies", "content": "For the real-world datasets, we visualize the subgraphs by potential neighbor scores produced from the $M_s$ of SES and\nedge masks of baselines to demonstrate their rationality in explanations. Especially, SES ranks the important neighbors\nwith the weights obtained by $M_s$, and the other baselines (GNNExplainer, PGExplainer, and PGMExplainer) rank\nimportant neighbors by the weights of edge masks for the central node in the neighborhood. Fig. 8 draws the 2-hop\nsubgraphs of node 78 in Cora, node 50 in CiteSeer, node 539 in PolBlogs, and node 212 in CS and lists the ranked node\nsequence of models."}, {"title": "5.11 Mask Optimization", "content": "We analyze the training and validation loss curves presented in Figure 7, as well as the evolution of the feature and\nstructure masks during the training phase. The feature mask is presented for all dimensions of all nodes at epochs 0,\n150, and 299. Moreover, we draw the structural mask of the 2-hop for nodes ranging from 1700 to 1800. In Figure 7,"}, {"title": "6 Conclusion", "content": "Previous self-explainable GNNs provide built-in explanations while suffering from a subpar performance in prediction.\nThe feedback explanations are ineffectively applied to supervise the training phase in current GNNs. To address\nthe challenges, we introduce a self-explainable and self-supervised graph neural network (SES) that bridges the\nexplainability and prediction of GNNs by two-phase training. A global mask generator in SES is designed to generate\nreliable instance-level explanations until explainable training is finished, resulting in notable time savings. The\nparameters of the graph encoder are shared between two phases of SES. The explanations derived in the explainable\ntraining phase are utilized as supervisory information with a self-supervised objective loss during the enhanced predictive\nlearning phase. Extensive experiments demonstrate that SES achieves SOTA explanation quality and significantly\nimproves the prediction accuracy of current GNNs. Our work illustrates that the tasks of prediction and explainability\ncan be concurrently enhanced during the training of GNNs."}]}