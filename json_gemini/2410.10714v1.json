{"title": "SEEDLM: COMPRESSING LLM WEIGHTS INTO SEEDS OF\nPSEUDO-RANDOM GENERATORS", "authors": ["Rasoul Shafipour", "David Harrison", "Maxwell Horton", "Jeffrey Marker", "Houman Bedayat", "Sachin Mehta", "Mohammad Rastegari", "Mahyar Najibi", "Saman Naderiparizi"], "abstract": "Large Language Models (LLMs) have transformed natural language processing, but face sig-\nnificant challenges in widespread deployment due to their high runtime cost. In this paper,\nwe introduce SeedLM, a novel post-training compression method that uses seeds of pseudo-\nrandom generators to encode and compress model weights. Specifically, for each block of\nweights, we find a seed that is fed into a Linear Feedback Shift Register (LFSR) during in-\nference to efficiently generate a random matrix. This matrix is then linearly combined with\ncompressed coefficients to reconstruct the weight block. SeedLM reduces memory access and\nleverages idle compute cycles during inference, effectively speeding up memory-bound tasks\nby trading compute for fewer memory accesses. Unlike state-of-the-art compression methods\nthat rely on calibration data, our approach is data-free and generalizes well across diverse tasks.\nOur experiments with Llama 3 70B, which is particularly challenging to compress, show that\nSeedLM achieves significantly better zero-shot accuracy retention at 4- and 3-bit than state-of-\nthe-art techniques, while maintaining performance comparable to FP16 baselines. Additionally,\nFPGA-based tests demonstrate that 4-bit SeedLM, as model size increases to 70B, approaches\na 4x speed-up over an FP16 Llama 2/3 baseline.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated impressive performance across numerous benchmarks\n(Achiam et al., 2023; Touvron et al., 2023). However, the practical deployment of these models often encounters\nlimitations due to substantial memory transfer requirements. This issue is especially pronounced during autore-\ngressive generation, which is primarily memory-bound and takes the majority of the inference time (Lee et al.,\n2024). In contrast, operations like 8-bit integer multiplication performed at 45nm 0.9V are demonstrated to be\nover 800x more energy-efficient than reading the same 8 bits from DRAM (Horowitz, 2014). In this paper, we ex-\nplore the following question: Can we trade a reasonable increase in compute for a reduction in memory accesses?\nA positive answer here not only transforms energy-intensive memory access operations into more energy-efficient\ncompute operations but also alleviates the memory bandwidth limitations that pose a significant bottleneck during\nLLM inference.\nPost-training weight compression is an effective method to reduce the size of pretrained LLMs, making them\nsuitable for on-device execution or reducing power consumption through fewer memory reads. Current state-of-\nthe-art techniques for compressing weights typically require calibration data and involve meticulously adjusting\nthe weights to ensure that the learned knowledge is retained.\nWe introduce SeedLM, a simple yet effective compression technique which can compress weights to 3-4 bits with\nminimal accuracy loss. SeedLM is an innovative method for compressing the weights of LLMs by projecting\nweight blocks into pseudo-random projection basis sets. By finding the optimal seeds to generate these pseudo-\nrandom projections per weight block, SeedLM ensures a low compression error and consequently, maintains the\naccuracy of the original model. Our approach only requires storing the seed and few projection coefficients instead\nof all the weight values to reconstruct high dimensional weight blocks.\nAs a result, SeedLM significantly reduces the memory footprint required for operating large-scale models during\ninference. To generate pseudo-random matrix blocks given a seed, we leverage Linear Feedback Shift Register\n(LFSR) hardware blocks that are widely used in applications such as cryptography, communication, and error\ndetection (Gaitonde & Ramabadran, 1988; Zeng et al., 2013; Xiang et al., 2016). LFSRs can be efficiently imple-\nmented in the silicon with minimal energy and area footprint."}, {"title": "2 Related Work", "content": "Significant research has been conducted in model compression for LLMs, a critical approach for reducing both\nthe memory footprint and computational demands of these models. In this section, we highlight some of the most\nrelevant techniques from prior work.\nCompression With Random Basis: Recent works have demonstrated that neural networks can be decomposed\ninto random number generator seeds and weight coefficients. In PRANC (Nooralinejad et al., 2022), full net-\nworks are compressed by orders of magnitude to improve storage and transmission efficiency. LoRA (Hu et al.,\n2021) compresses the weights by injecting trainable rank decomposition matrices into each layer of the network.\nNOLA (Koohpayegani et al., 2023) builds upon LoRA by compressing the low-rank matrices through a linear\ncombination of random basis vectors, further reducing memory and computational overhead.\nOur work (SeedLM) is conceptually similar in that we compress networks using a random basis. However, a\nkey distinction from NOLA is that they do not utilize lightweight pseudo-random generator modules. Thus, they\nhaven't demonstrated the ability to efficiently generate weights on-the-fly by colocating weight generation with\ncomputation. Additionally, unlike SeedLM, they don't find an optimal seed but instead rely on a random seed."}, {"title": "3 Methodology", "content": "In this section, we introduce SeedLM, our method for compressing the weights of LLMs by using seeds from\na pseudo-random generator. Initially, each weight matrix is segmented into blocks of $C$ contiguous elements.\nRepresenting each block as a vector $w \\in \\mathbb{R}^C$, we approximate it as a linear combination of columns from a matrix\n$U\\in \\mathbb{R}^{C\\times P}$. This matrix $U$ is constructed using a pseudo-random generator given a seed specifically selected to\ngenerate a subspace that most effectively reconstructs $w$ linearly.\nFigure 2 illustrates this setup. Our primary goal is to find the optimal seed, $s$, and coefficient vector, $t \\in \\mathbb{R}^P$, that\nminimize the reconstruction error between the original and the approximated weights. For this reconstruction, only\nthe seed and the coefficients are stored. In the following subsection, we first outline a mechanism to efficiently\ngenerate $U$ using a $K$-bit seed in our Linear Feedback Shift Register (LFSR) framework. We will then discuss\nthe methodologies employed to determine $s$ and $t$."}, {"title": "3.1 Linear Feedback Shift Register (LFSR)", "content": "A Linear Feedback Shift Register (LFSR) is a simple yet effective type of shift register, ideal for generating\npseudo-random binary sequences. The primary advantages of LFSRs in hardware include cost-effectiveness and\nminimal resource consumption due to their straightforward implementation with basic flip-flops and XOR gates.\nThis simplicity facilitates rapid and efficient sequence generation, which is integral to our compression technique.\nAn LFSR operation can be characterized by its length $K$ (which determines the number of bits in its shift register)\nand its feedback polynomial. To generate next pseudo-random number in the sequence, each bit in the register is\nfirst shifted to the next position. Then, the new bit entering the register is calculated as a linear combination of\ncertain bits of the current state as specified by the feedback polynomial, typically implemented by XOR operations.\nMathematically, the new bit $X_{n+1}$ generated by the LFSR can be expressed as:\n$X_{n+1} = \\sum_{i=0}^{K-1} A_iX_{n+i-K+1} \\mod 2$,\nwhere $K \\ge 2$ and $\\alpha_0,...,\\alpha_K$ are the binary coefficients that define the feedback polynomial, with each $a_j$\ndetermining whether the bit $x_j$ is selected or not.\nThe state transition in the LFSR can be described as follows: if the current state is represented by the bits\n$X_n, X_{n-1},..., X_{n-K+1}$, then after the shift, the new state will be $X_{n+1},X_n,..., X_{n-K+2}$. This transition re-\nflects the shift of every bit to the right by one position, with the new bit $X_{n+1}$ entering at the leftmost position.\nGiven its finite state nature, an LFSR will eventually enter a repeating cycle, suggesting an asymptotic uniform\ndistribution over this cycle. An LFSR can cycle through at most $2^K - 1$ states, excluding the all-zero state.\nA key goal when designing an LFSR is to guarantee a maximal-length sequence. This ensures that the LFSR will\nproduce the longest possible sequence of non-repeating states before repeating. Intuitively, this means the LFSR\nwill cycle through every possible state (except the all-zero state), maximizing the number of distinct pseudo-\nrandom values generated. To achieve this maximal-length property, the feedback polynomial must be primitive\nover the Galois field GF(2). In simple terms, a primitive polynomial ensures that the LFSR explores all $2^K \u2212 1$\nstates without prematurely entering a repeating cycle.\nFor our experiments, this means a fixed set of coefficients ${a_j : 0 \\le j \\le K \u22121}$ that is hard-wired, ensuring\nmaximal length if and only if it avoids the all-zero state, in which it stays in zero. Refer to Section A.1 for the\nindexed j coefficients used for each K where $a_j$ equals one; all other coefficients are zero. For a comprehensive\nunderstanding of LFSRs and their properties, see (Bhattacharjee & Das, 2022).\nTo optimize the efficiency of generating random matrices through an LFSR, for a fixed length K and a set of\ncoefficients ${a_j}$, we cache all the $2^{K-1}$ states that sequentially follow each other \u2013 each state uniquely determined\nby its preceding state along with K and ${a_j}$. This cache allows us to extract an arbitrarily sized random matrix\nfrom the sequence given a random seed s, where the matrix begins to fill up starting from the first value generated\nby the LFSR after the seed, not the seed itself. We can cycle through these states to generate matrices of any\ndesired size. For an illustration, refer to Figure 3. With K = 16 and a maximal length LFSR, all states will occupy\napproximately $(2^{16}-1) \\times 2 Bytes \\approx 130KB$ of memory, which is negligible. This setup ensures a highly efficient\nand scalable mechanism for generating the necessary random matrices for our compression technique. K is a\nhyper-parameter of our method which we will elaborate on in Section 3.4."}, {"title": "3.2 Weight Compression Using Pseudo-Random Generators", "content": "Building on the foundations laid by the LFSR mechanisms, our methodology seeks to represent a block of data\n$w\\in \\mathbb{R}^C$ using the decomposition $U(s)t$. Here, $U(s) \\in \\mathbb{R}^{C\\times P}$ is a random matrix derived from a K-bit"}, {"title": "3.3 Approximation Approach", "content": "Looking back at the optimization problem in Eq. 2, while the unconstrained case admits a closed-form solution\ngiven by $U(s)^+w$, where $U(s)^\u2020$ denotes the Moore-Penrose pseudo-inverse of $U(s)$, our discrete constraints\nconvert it to an NP-hard optimization problem. Hence, to solve Eq. 2, we employ an approximate heuristic\napproach that involves the following steps:\n1. Generate $N = 2^{K \u22121}$ random matrices ${U(s_1), U(s_2), ..., U(s_N)}$, each of size $C \\times P$, with an LFSR\nof length K based on Eq. 1 and $s_j :=j$.\n2. For each matrix $U(s_j)$, project the vector w onto the subspace spanned by $U(s_j)$:\n$t_j = U(s_j)^+w$.\n3. Quantize $t_j$ to obtain the vector $t_j$ using 4-bit integers and a 4-bit shared exponent $e_j$.\n4. Compute the reconstruction error for each pair $(U(s_j), t_j)$ as follows:\n$E_j = ||w - U(s_j)t_j||_2$.\n5. Select the pair $(s^*, t^*)$ that minimizes the reconstruction error:\n$(s^*, t^*) = arg\\underset{s_j,t_j}{min} e_j$.\nOur heuristic algorithm leverages randomness to explore multiple subspaces and selects the one that best approx-imates w under the given constraints."}, {"title": "3.4 Design Space Exploration", "content": "The minimum reconstruction error obtained from Eq. 3 depends on the block size C, the latent dimension P, and\nthe LFSR length K. Here, we explore how we select the optimal configuration for an M-bit compression. First,\nlet's examine the total number of bits required to store a SeedLM block of C elements, which consists of the\nfollowing:\n\u2022 K bits to index the selected random seed s* to generate matrix U(s*) among the N = 2^k \u22121 candidates.\n\u2022 4 bits to store the shared exponent e.\n\u2022 4P bits to store the quantized vector t* (P elements each requiring 4 bits).\nSo, the effective bit per element is a function of hyper-parameters K, C, and P. In particular, for an M-bit\ncompression, we have the bit budget per element as\n$M = \\frac{K+4+4P}{C}$\nTo determine the optimal configuration for a given bit budget per element, M, we evaluate the reconstruction\naccuracy of our method in the search space. Specifically, we explore how a standard normal Gaussian vector w\ncan be approximated using any combination of valid hyperparameters given the bit budget M. While assuming\na Gaussian distribution may have its limitations, it has proven effective within our design space and aligns well\nwith real-world benchmarks. Our objective is to find appropriate values for block size C, latent dimension P,\nand LFSR length K, such that the reconstruction error is minimized when the optimal seed is selected. More\nspecifically, let $s^*_{C,P,K}$ and $t^*_{C,P,K}$ denote the solutions obtained from Eq. 3. For an M-bit compression, we\nsolve the following optimization problem:\n$\\mathbb{E}[min] := min_{C,P,K} \\mathbb{E}[||w \u2013 U(S^*_{C,P,K})t^*_{C,P,K} ||_2]$,\nsubject to: $MC = K + 4 + 4P$ and $C, K, P \\in \\mathbb{Z}^+$,\nwhere $\\mathbb{Z}^+$ represents the set of all positive integers. Since the optimization problem in (5) is not analytically\ntractable, we numerically solved it by conducting a grid search over C, P, and K constrained to positive integers\nand the given bit budget. Understanding the trade-offs among C, P, and K is important for optimizing the approx-imation within a given bit budget. Each of these parameters influences the overall performance and contributes to\nminimizing the reconstruction error.\nOne critical trade-off is between the LFSR seed length K and the latent dimension P. Increasing K reduces\nthe expected minimum error $\\mathbb{E}[\u20acmin]$ by providing more opportunities to find a better projection. However, this\ncomes at a cost: as K increases, the number of required bits grows, reducing the available bit budget for P. The\nobjective is to find an optimal K that effectively lowers $\\mathbb{E}[min]$ without overly constraining P, as that could lead\nto a significant increase in the overall error $\\mathbb{E}[min]$. Similarly, increasing P helps capture more of the energy\nof the vector w, thus reducing $\\mathbb{E}[min]$. However, this also requires more bits, which may limit the value of K.\nThe key is to strike a balance where enough energy is captured without overly sacrificing the exploration of better\nprojections through K. Finally, increasing C expands the total bit budget, allowing for larger values of both P\nand K. However, this also increases the potential for higher error, as expanding the space may dilute the precision\nof projections."}, {"title": "4 Experiments", "content": "We apply Algorithm 1 across all weight blocks of pretrained LLMs to find the seeds and coefficients that minimize\nreconstruction error (Eq. 3). Using the configurations from Table 1, we evaluate our compression methods in terms\nof accuracy and performance. Our experiments focus on Llama 2 and Llama 3 models (Touvron et al., 2023), and\nunlike other methods, SeedLM does not require fine-tuning or calibration data while still achieving competitive\nresults. We assess model quality using perplexity and accuracy, followed by performance analysis through FPGA-\nbased matrix multiplication with low-level LFSR generation. This highlights the cost and performance benefits of\nSeedLM, especially in hardware-constrained environments."}, {"title": "4.1 Accuracy Results", "content": "To evaluate the quality of SeedLM, we measure perplexity on the WikiText-2 dataset (Merity et al., 2016) and\nassess accuracy across various zero-shot tasks using the LM Evaluation Harness (Gao et al., 2021)\u00b9. We compare\nour method against established compression techniques such as AWQ (Lin et al., 2024), OmniQuant (Shao et al.,\n2023), and QuIP# (Tseng et al., 2024), using the official GitHub repositories for each baseline as of Septem-ber 2024. A key strength of SeedLM is that it can operate entirely data-free, in contrast to other methods that\nrequire calibration data to achieve comparable results. For the baseline methods, we use the default calibration\nsets from their official repositories. Our experiments involve Llama 2 models (7B, 13B, 70B) and Llama 3 models\n(8B, 70B), tested with 3-bit and 4-bit representations. In the case of AWQ and OmniQuant, we use 4-bit integers\nwith channel-wise scaling to avoid significantly increasing the bits per element beyond the allocated 3 or 4 bits\n(since a group size of 128 in these methods adds roughly 0.25 extra bits per parameter). For QuIP# and Om-niQuant, we ensure a fair comparison with SeedLM and AWQ by not performing fine-tuning on the quantized\nmodels. However, unlike SeedLM, all of them still require per-layer calibration, which relies on calibration data\nand activations, whereas SeedLM achieves its results without any data dependency.\nTo evaluate general language\nmodel performance, we measure\nperplexity on the WikiText-2\nlanguage modeling dataset, us-ing 166 windows, each with a\nlength of 2048 tokens, from the\ntest data split. The results, as\nshown in Table 2, illustrate a clear\ntrade-off between compression\nlevel and model quality. In some\ncases, larger models subjected\nto aggressive compression even\nunderperform smaller models with\nmilder compression. SeedLM\nconsistently outperforms state-of-the-art compression techniques,\nparticularly at higher compression\nlevels. Notably, SeedLM achieves\nthese results without the need for\nany calibration data."}, {"title": "4.2 Performance Analysis", "content": "In this section, we shift our focus from accuracy to performance analysis on hardware. Specifically, we explore\nhow SeedLM can be efficiently implemented on an FPGA. FPGAs are ideal for this task because they allow for\nhighly parallelized computations and can be reconfigured to handle specific workloads, making them well-suited"}, {"title": "5 Concluding Remarks", "content": "In this paper, we presented SeedLM, a post-training compression method that uses pseudo-random generators to\nefficiently encode and compress model weights. SeedLM offers a data-free approach, avoiding the need for cali-\nbration data while retaining competitive accuracy, achieving up to around 99% zero-shot accuracy at 3- and 4-bit\nquantization levels. We demonstrated the method's performance on both Llama 2 and Llama 3 models, showing\nthat it performs comparably to existing state-of-the-art techniques. Furthermore, our FPGA implementation high-\nlights SeedLM's potential for improved computational efficiency in hardware-constrained environments. While\nwe believe that additional fine-tuning could further improve results we leave that to future works."}, {"title": "A Appendix", "content": "A.1 Coefficients Used in LFSRS\nThe following table lists the indexed j coefficients used for each K, where aj equals one, with all other coeffi-cients being zero. These specific coefficients correspond to the Linear Feedback Shift Registers (LFSRs) for eachregister length K, with the coefficients indexed starting from 0, representing the tap positions in the shift register.These coefficients are hard-wired into the hardware configuration of the LFSRs used in our experiments. Thesecoefficients define the feedback polynomial for each LFSR, ensuring maximal-length cycles.\nTable 6: Indexed j coefficients used in LFSRs for each register length K, where aj = 1 and all other coefficientsare zero."}]}