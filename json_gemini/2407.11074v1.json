{"title": "ST-RetNet: A Long-term Spatial-Temporal Traffic Flow Prediction Method", "authors": ["Baichao Long", "Wang Zhu", "Jianli Xiao"], "abstract": "Traffic flow forecasting is considered a critical task in the field of intelligent transportation systems. In this paper, to address the issue of low accuracy in long-term forecasting of spatial-temporal big data on traffic flow, we propose an innovative model called Spatial-Temporal Retentive Network (ST-RetNet). We extend the Retentive Network to address the task of traffic flow forecasting. At the spatial scale, we integrate a topological graph structure into Spatial Retentive Network(S-RetNet), utilizing an adaptive adjacency matrix to extract dynamic spatial features of the road network. We also employ Graph Convolutional Networks to extract static spatial features of the road network. These two components are then fused to capture dynamic and static spatial correlations. At the temporal scale, we propose the Temporal Retentive Network(T-RetNet), which has been demonstrated to excel in capturing long-term dependencies in traffic flow patterns compared to other time series models, including Recurrent Neural Networks based and transformer models. We achieve the spatial-temporal traffic flow forecasting task by integrating S-RetNet and T-RetNet to form ST-RetNet. Through experimental comparisons conducted on four real-world datasets, we demonstrate that ST-RetNet outperforms the state-of-the-art approaches in traffic flow forecasting.", "sections": [{"title": "1 Introduction", "content": "Accurate and real-time short-term or long-term forecasting of traffic flow is essential for driving urban development and autonomous driving technologies. The traffic flow of different roads within the road network exhibit complex spatial-temporal dependencies. With the rapid development of deep learning and large language models, the previous methods based on recurrent neural networks (RNNs) and graph neural networks (GNNs) for traffic flow forecasting are insufficient to meet current requirements [9,17,26]. Therefore, traffic flow forecasting remains a challenging task."}, {"title": "2 Related Works", "content": "In this section, we will present the leading spatial and temporal models in recent years."}, {"title": "2.1 Spatial Models", "content": "CNNs and GCNs are common spatial models. CNNs are widely used in computer vision to process regular grid data, while GCNs perform better in handling complex topological structures [2,13]. Thus, GCNs are extensively applied in traffic flow forecasting. In order to capture the topological graph structure of the road network, it is necessary to have sufficient data information, such as the connectivity and correlation between road segments within the network. To address the issue of difficulty in capturing spatial features of the road network when topological information is insufficient, researchers have proposed adaptive learning of spatial topological information by using adaptive adjacency matrices [23]. In summary, graph-based modeling is currently the main approach to address the spatial features in traffic forecasting."}, {"title": "2.2 Temporal Models", "content": "In the context of deep learning, RNN and their variants, such as LSTM and GRU [5,8,12], have been the main methods for handling time series forecasting tasks in recent years. However, they still suffer from issues such as time-consuming training and limitations in modeling long time series. Graph WaveNet [23] introduced dilated convolutions to capture sequential information at different time scales. When it comes to capturing time correlations between long input sequences, the model's parameter count increases linearly, and the effectiveness of capturing long-term temporal correlations may be compromised. With the rise of Transformers, they excel in learning correlations between sequence data in a parallelizable manner and have evident advantages in long-term forecasting. Similar to Transformers, RetNet [19], known as the successor to Transformers, also performs well in tasks involving sequence data. In conclusion, models based on Transformers and RetNet, which can parallelize the processing of sequence data, have become the mainstream in traffic flow forecasting tasks."}, {"title": "2.3 Hybrid models", "content": "In the evolving field of traffic flow prediction, there's a growing emphasis on spatial-temporal hybrid models that concurrently assess temporal and spatial dimensions. This approach has led to the development and application of combined spatial and temporal models. The initial foray into such hybrid models was marked by T-GCN [27], which integrated GCN with GRU, laying the groundwork for future advancements. Following this, more sophisticated models like STGCN [25], A3T-GCN [1], KST-GCN [29], and AF-unit [14] emerged, employing a blend of GCNs, attention mechanisms, and RNN-based techniques to adeptly capture the intricate spatial-temporal dependencies within road networks. These models also incorporate external elements such as weather conditions, adding depth to the analysis. More recent innovations in this domain include Graph WaveNet [23], DSTAGCN [28], MD-GCN [7], and STTN [24], all of which have demonstrated impressive predictive performance. This trend underscores a strategic shift towards more intricate integration of spatial and temporal models, tailored to the specific needs of traffic flow prediction."}, {"title": "3 Proposed Model", "content": "In this section, we will provide a detailed description of ST-RetNet. Firstly, we define the problem of traffic flow forecasting. Then, we describe the overall framework of ST-RetNet, which consists of three main steps. Pre-processed data undergoes dimension expansion, followed by passing through the stacked Spatial-Temporal Block for extracting spatial-temporal features. Finally, the data is fed into the prediction layer to obtain traffic flow forecasting. Next, we provide a thorough explanation of the main innovative components of the model: S-RetNet and T-RetNet."}, {"title": "3.1 Problem Definition", "content": "The topological graph structure of the road network can be represented as\n$G = (V,E, A)$,\nwhere $V = {V_1, V_2,\u2026, V_v }$ represents the set of N sensors collecting traffic flow data within the road network. E represents the set of edges."}, {"title": "3.2 Overall Architecture", "content": "As shown in Fig. 2, ST-RetNet initially expands the dimensionality of the input feature vector X. This step helps extract more useful feature information, thereby improving the performance of subsequent layers in the network. The expanded feature vector then enters the stacked spatial-temporal blocks for spatial-temporal feature extraction. This part utilizes the concepts of RetNet, GCN model, residual connections, and gate mechanisms to fully capture the spatial-temporal features. Finally, the output is passed into the prediction layer to obtain the result of traffic flow forecasting Y.\nDimension Expansion. The input feature vector $X \u2208 R^{N\u00d7p\u00d71}$ undergoes a two-dimensional convolution operation to expand its feature dimensionality,"}, {"title": "3.3 S-RetNet", "content": "As shown in Fig. 2, the Spatial-Temporal Feature Extractor consists of S-RetNet and T-RetNet, which are next described in detail in subsections 3.3 and 3.4.\nGCN Based on the given spatial relationships of sensors within the road network, GCN can effectively capture the static spatial features of the road network. The GCN in this paper can be represented by the following equation:\n$\\begin{cases}\nH_1 = \u03c3 (A_fX^SW_f)\\\\\nH_2 = \u03c3 (A_bX^SW_b)\n\\end{cases}$\nwhere the forward transition matrix $A_f \u2208 R^{N\u00d7N}$ and the backward transition matrix $A_b \u2208 R^{N\u00d7N}$ are constructed based on the direction and known distance relationships between sensors. The correlation of data between sensors decreases as the physical distance between them increases. The construction methods for $A_f$ and $A_b$ are the same as those in Paper [23]. $W_H \u2208 R^{f_a\u00d7 f_d}$ and $W'_H\u2208 R^{f_a\u00d7 f_a}$ are parameter matrices.\nAdaptive Adjacency Matrix. Adaptive adjacency matrix does not require any prior knowledge and can automatically learn the dynamic spatial dependencies between sensor data through the model's learning process. We randomly initialize two learnable column vectors $E_1 \u2208 R^{N\u00d7c}$ and $E_2 \u2208 R^{N\u00d7c}$. Then, the adaptive adjacency matrix $A_{adp} \u2208 R^{N\u00d7N}$ is generated using the following equation:\n$A_{adp} = softmax (ReLU (E_1E_2^T))$\nwhere we use the ReLU function to filter out weakly correlated sensor relationships and apply the softmax function to normalize the matrix. Through the model's learning process, the adjacency matrix automatically adjusts to reflect the dynamic spatial dependencies between the sensors.\nSpatial Retention Mechanism. The definition of spatial retention mechanism is as follows:\n$\\begin{cases}\nQ^S = (X^SW_q) \u2299 \u03b8^S\\\\\nK^S = (X^SW_k) \u2299 \\overline{\u03b8^S}\\\\\nV^S = X^SW_v\n\\end{cases}$\n$SRM = (Q^S (K^S)^\u2217 \u2299 A_{adp}) V^S$\nwhere $\u03b8^S$ denotes the complex conjugate of $\\overline{\u03b8^S}$, which is a method for relative positional embedding similar to what is described in paper [20]. $W_q \u2208 R^{N\u00d7N}$, $W_k \u2208 R^{N\u00d7N}$, and $W_v \u2208 R^{N\u00d7N}$ are parameter matrices.\nSpatial Features Aggregation. To simultaneously capture the static spatial dependencies and dynamic spatial dependencies of the road network, we need to perform information aggregation based on the following equation:\n$head = Conv (Concat (X^S, H_1, H_2, SRM))$\nGated Multi-Scale S-Retention. In order to better capture the complex relationships between input sequences and extract useful information. Multi-scale s-retention is employed in this paper. The input sequence $X^S$ is analyzed at multiple scales $[X_1^S,...,X_h^S \u2208 R^{N\u00d7p\u00d7f_a/h}]$, and the multi-scale computation results $[head_1,..., head_h]$ are finally computed based on Eq. (5) to (8). Then, the multi-scale s-retention results $head$ are calculated according to Eq. (9), then a gate structure Swish [4,15] is used to increase the model's nonlinearity.\n$\\begin{cases}\nheads = GN^S (Concat (head_1,..., head_h))\\\\\nMSR^S = (Swish (X^S\\overline{W^S_q} head_s) \\overline{W^S_v} + X^S\n\\end{cases}$\nwhere $GN^S$ is group normalization [22], $\\overline{W^S_q}$ and $\\overline{W^S_v}$ are parameter matrices.\nFeed-Forward Network. First, a linear transformation is performed on $MSR^S$, followed by a nonlinear transformation using the SiLU activation function. Another linear transformation and sum MSR are then performed to obtain the final output $Y^S$ of S-RetNet.\n$Y^S = SiLU (MSR^S\\overline{W_q^S}) \\overline{W_v^S} + MSR^S$"}, {"title": "3.4 T-RetNet", "content": "In T-RetNet, the main components are temporal retention mechanism, gated multi-scale t-retention, and feed-forward network. The temporal retention mechanism introduces $D_{nm}$, it combines causal masking and exponential decay along relative distance as one matrix. The gated multi-scale t-retention aggregates features from each head to capture temporal dependencies more comprehensively. Finally, the output YT of T-RetNet is obtained through the feed-forward network.\nTemporal Retention Mechanism. As shown in Fig. 2, the temporal retention mechanism is defined as follows:\n$\\begin{cases}\nQ^T = (X^TW_q) \u2299 \u03b8^T\\\\\nK^T = (X^TW_k) \u2299 \\overline{\u03b8^T}\\\\\nV^T = X^TW_v\n\\end{cases}$\n$TRM (X^T) = (Q^T (K^T)^\u2217 \u2299 D) V^T$\n$D_{nm} = \\begin{cases}\ny^{n-m}, n\u2265m\\\\\n0, n <m\n\\end{cases}$\nwhere $X^T = Layer Norm (Y^S + X^S)$. $\u0398^T$ denotes the complex conjugate of $\\overline{\u0398^T}$.\n$W_q \u2208 R^{P\u00d7P}$, $W_k \u2208 R^{P\u00d7P}$, and $W_v \u2208 R^{P\u00d7P}$ are parameter matrices. D \u2208 $R^{P\u00d7P}$ is a relative position encoding matrix that gradually decays with increasing time intervals.\nGated Multi-Scale T-Retention. In order to better capture the complex relationships between input sequences and extract useful information. Multi-scale t-retention is employed in this paper. XT is analyzed at multiple scales $[X_1^T,...,X_h^T \u2208 R^{\u00d1\u00d7p\u00d7f_a/h}]$. The multi-scale computation results $[head_1,...,head_h]$ are finally computed and the multi-scale t-retention results $head$ are calculated according to Eq. (12), then a gate structure is used to increase the model's nonlinearity.\n$\\begin{cases}\ny = 1 \u2013 2^{-5 * arrange(0,h)} \u2208 R^h\\\\\nhead^i = TRM (X^T, Yi)\\\\\nhead^i = GN^F (Concat(head_1,..., head_h))\\\\\nMSR^T = (Swish (X^T\\overline{W_q^T} head_t) \\overline{W_v^T} + X^T\n\\end{cases}$\nFeed-forward Network. The feed-forward network in T-RetNet uses the same approach as S-RetNet, with the input being MSRT, and the output being YT.\n$Y^T = SiLU (MSR^T\\overline{W_q^T}) \\overline{W_v^T} + MSR^T$\nwhere $\\overline{W_q^T}$ and $\\overline{W_v^T}$ are parameter matrices. The output of spatial-temporal feature extractor $Y^{ST} = Layer Norm (Y^T + X^T)$."}, {"title": "4 Experiments", "content": "In this section, we conducted experiments on four real datasets to address several questions: Q1: Is S-RetNet better than GCN-based methods for capturing dynamic spatial features? Q2: Is T-RetNet more effective than other temporal models? Q3: How does ST-RetNet perform compared to other traffic flow forecasting models? We also conducted ablation experiments to demonstrate the importance of each module."}, {"title": "4.1 Experimental Settings", "content": "Dataset. The datasets we use are publicly available: PEMS03, PEMS04, PEMS07, and PEMS08 [3].\nImplementation Details. We divide each dataset into training, testing, and validation sets with a ratio of 3:1:1. We use historical data from the previous hour to predict the traffic conditions for the next hour. All experiments are conducted on the window platform (CPU: Intel(R) Core(TM) i7-9700K CPU @ 3.60GHz; GPU: NVIDIA GeForce RTX 2080). The GCN part adopts the operation mode shown in equation 5, ReLU as the activation function. The batch size is set to 32, fa is 64, and both the spatial block and the temporal block have one layer. Head (h) is 8. The training objective is L1, the optimizer is RMSprop, and the learning rate is 0.001. The baseline result is obtained after running the open-source code. The maximum number of epochs is 200, and the best model is automatically saved. If the model does not improve for 15 consecutive epochs, training will be stopped."}, {"title": "4.2 Experimental Results", "content": "Performance Comparison of S-RetNet with GCN-based method. This subsection focuses on Q1: Is S-RetNet better than GCN-based methods for capturing dynamic spatial features? To answer Q1, we removed the GCN component from S-RetNet. The GCN was used to capture the dynamic spatial dependencies of the road network using an adaptive adjacency matrix. We conducted comparative experiments on four datasets to compare the performance of S-RetNet without GCN and GCN in capturing dynamic spatial dependencies. As shown in Fig. 3, the results are consistent across all four datasets, with the performance of S-RetNet being more stable than GCN from step 1 to step 12. S-RetNet performs better in medium to long-term forecasting. In overall predictions for one hour, S-RetNet has better average prediction accuracy compared to GCN.\nPerformance Comparison of T-RetNet with Temporal Models. This subsection focuses on Q2: Is T-RetNet more effective than other temporal models? To answer Q2, we conducted comparative experiments between S-RetNet and commonly used temporal models, including RNN, LSTM, GRU, and Transformer. Fig. 4 illustrates the comparison of MAE on four real datasets. Previous studies have shown that RNN-based models are prone to issues such as gradient explosion or vanishing, leading to the development of LSTM and GRU variants."}, {"title": "4.3 Ablation Studies", "content": "RetNet and Transfomer are compared in terms of speed, memory consumption, and latency in paper [19]. The results show that RetNet performs better. Therefore, the ablation studies in this paper are only for the combination of the blocks in the proposed model. To validate the necessity of each block, we conducted comparative experiments by removing the S-RetNet, T-RetNet, and GCN separately. As shown in Tab. 3, each component contributes to improving the performance of the model. We also stacked the optimal number of layers for the temporal and spatial blocks. We set the maximum number of stacked layers to 3. In PEMS08, when S-RetNet is 1 layer, the MAE of T-RetNet from layer 1 to 3 are 16.68, 16.18 and 16.07; when S-RetNet is 2 layers, the MAE of T-RetNet from layer 1 to 3 are 17.13, 16.48 and 16.24; when S-RetNet is 3 layers, the MAE of T-RetNet from layer 1 to layer 3 are 18.09, 16.67 and 16.31. Stacking S-RetNet does not significantly improve the model's performance, while stacking T-RetNet significantly improves the model's performance. The optimal combination is 1 layer of S-RetNet and 3 layer of T-RetNet in PEMS08."}, {"title": "5 Conclusion", "content": "In this paper, we propose a new spatial-temporal retentive network for traffic flow forecasting. It has high accuracy and long-term forecasting capability. We introduce the idea of RetNet and apply it to the traffic flow forecasting task. We extend it to both temporal-scale and spatial-scale modeling, achieving better overall performance while ensuring parallel computation. The proposed model is obtained by integrating temporal-scale and spatial-scale RetNet. In experiments, we test the expanded spatial and temporal scale models on four real-world datasets, and the results show that our model has stronger competitiveness compared to existing spatial and temporal models. And ST-RetNet outperforms the state-of-the-art methods, proving the effectiveness of our approach in capturing spatial-temporal dependencies. In the future, we will further explore the performance improvement of traffic flow forecasting models using multimodal data and how to model spatial-temporal aspects of traffic flow based on existing large language models."}]}