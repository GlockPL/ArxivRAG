{"title": "VCBench: A Controllable Benchmark for Symbolic and Abstract Challenges in Video Cognition", "authors": ["Chenglin Li", "Qianglong Chen", "Zhi Li", "Feng Tao", "Yin Zhang"], "abstract": "Recent advancements in Large Video-Language Models (LVLMs) have led to the development of benchmarks aimed at assessing cognitive abilities. However, most existing benchmarks heavily rely on web-collected videos paired with human annotations or model-generated questions, which limit control over the video content and fall short in evaluating advanced cognitive abilities involving symbolic and abstract concepts. To address these limitations, we introduce VCBench, a controllable benchmark to assess LVLMs' cognitive abilities, involving symbolic and abstract elements at varying difficulty levels. By generating video data with the Python-based engine, VCBench allows for precise control over the video content, creating dynamic, task-oriented videos that feature complex scenes and abstract concepts. Each task is paired with customized question templates tailored to specific cognitive challenges, providing a rigorous evaluation test. Our evaluation reveals that even state-of-the-art (SOTA) models, such as Qwen2-VL-72B, struggle with simple video cognition tasks involving abstract concepts, with performance sharply dropping by 19% as video complexity rises. These findings reveal the current limitations of LVLMs in advanced cognitive tasks and highlight the critical role of VCBench in driving research toward more robust and generalized LVLMs for complex video cognition challenges.", "sections": [{"title": "1. Introduction", "content": "With the rapid advancement of artificial intelligence (AI), large video-language models (LVLMs) have emerged as essential tools for video understanding [23, 24, 29, 45, 60, 63]. To unlock their full potential, LVLMs must exhibit cognitive abilities that approximate human-like perception and reasoning [18, 47]. While recent evaluations of large language models (LLMs) and image-based models have increasingly focused on advanced cognition [7, 9, 41], current benchmarks for video-based models [5, 11, 14, 25, 26, 36, 62] continue to overlook advanced cognitive abilities fundamental to both humans and AI. Furthermore, existing benchmarks largely focus on video domain and duration [14, 25, 26, 51], without precise control over video content and difficulty, resulting in limited assessment of cognitive depth required for advanced video understanding. Constructing benchmarks from real-world videos also poses challenges, including intensive prompt engineering, manual annotation, and data filtering [5, 16, 25, 32, 36, 38, 53], along with the risk of data leakage where video content may inadvertently be included in model training [58]. To address these limitations, we introduce VCBench, a controllable Video Cognitive Benchmark designed to evaluate the cognitive capabilities of LVLMs."}, {"title": "2. Related Work", "content": "Recent advancements in large multi-modal models [30, 31, 50, 64] have greatly enhanced perception and reasoning capabilities across various domains, especially in image-based tasks [13, 54, 67]. As multi-modal research continues to evolve, there is a growing shift from static images to dynamic video content to tackle the complexities of video understanding [27]. Early explorations in video understanding for LMMs have shown promising results [23, 24, 27, 29, 40, 57, 60, 63]. Meanwhile, active research has focused on constructing benchmarks to assess LVLM capabilities [5, 11, 14, 25, 26, 36]. For instance, MVBench [25] provides a comprehensive suite of task-specific videos covering 20 varied tasks, marking substantial progress in video comprehension. MMBench-Video [11] uses extended videos from YouTube and applies free-form questioning to better simulate real-world video understanding tasks. However, most existing video-based benchmarks primarily focus on basic video comprehension, often overlooking abstract cognitive tasks and offering limited control over video content. To address these limitations, we introduce VCBench, a controllable video cognition"}, {"title": "2.2. Synthetic Benchmarks", "content": "Synthetic benchmarks offer precise control over data and are largely immune to data leakage, as they avoid reliance on existing training data [39, 68]. For example, [33] employed synthetic data as benchmarks for language models, while others [19, 20, 39] developed controlled experiments with synthesized images to evaluate visual language models. In the video domain, earlier works[15, 61] employed 3D engines [1, 17] to generate simple videos for action and dynamic analysis. The latest study [68] synthesizes test videos for evaluating understanding by embedding images and text into raw videos. [21], while [21] uses Python-generated videos to evaluate the abilities of video generation models (diffusion models). Unlike existing work, our approach leverages the Python engine to generate videos tailored to various task requirements and difficulty levels, providing a flexible, rigorous testing ground that reflects real-world complexity."}, {"title": "3. VCBench", "content": "As large-scale models continue to evolve, recent research [26] has increasingly focused on evaluating their video cognitive capabilities. Common dimensions of evaluation include Object Perception (OP), Action Perception (AP), Temporal Reasoning (TR), and Spatial Reasoning (SR). In VCbench, we expand this scope of video cognition assessment by introducing two essential dimensions: Game-environment Perception (GP) and Full-modal Perception (FP). VCBench assesses these six key dimensions using synthetic videos incorporating symbolic elements and abstract concepts generated through Python rendering. The following section provides specific descriptions of each dimension."}, {"title": "3.1. Benchmark Design", "content": "\u2022 Object Perception (OP): This dimension involves precise recognition of symbolic objects varying in color, shape, and size [49]. It requires models to sustain high recognition accuracy across diverse visual and abstract attributes.\n\u2022 Action Perception (AP): This capability evaluates the model's proficiency in interpreting the types of actions performed by symbolic objects [4], accounting for variations in action speed and direction."}, {"title": "3.2. Automated Video and QA Generation", "content": "Using video cognitive ability definitions as a guide, we created video scenes to test each specific capability. We developed a synthetic video generation pipeline in Python, inspired by video game environments, to render task scenes with symbolic elements and abstract concepts. Temporal and spatial complexity are controlled through code parameters. Scene logging and paired question templates allow for targeted ability evaluation. Below, we provide specific descriptions of scenes created for VCBench.\n1. Chameleon Grid: This scene features an n x m grid where each cell holds a symbolic object with unique attributes: size (small, medium, large), color (red, green, blue), and shape (triangle, circle, square). Objects are periodically updated to simulate dynamic visual stimuli, inspired by games like Bejeweled and Candy Crush. Complexity is controlled by adjusting grid dimensions, and testing models' object recognition skills in response to changing arrangements.\n2. Action Arena: This scene includes multiple objects performing abstract action types, such as"}, {"title": "4. Experiments", "content": "We evaluate 10 popular open-source LVLMs fine-tuned on video question-answer tasks, including MiniCPM-V [59], Video-LLaMA2 [6], Intern-Video2 [52], Video-LLaVA [29], LLaVA-NEXT-"}, {"title": "4.2. Main Results", "content": "As shown in Table 2, most models struggle with OP and GP tasks, which involve more abstract visual concepts, underscoring their challenges in abstract video cognition. In contrast, their strong performance in FP indicates a solid grasp of integrated audio-visual information. Qwen2-VL-72B stands out among the models tested, consistently achieving the highest accuracy across most tasks, including OP (51.78%), AP (59.08%), TR (56.83%), SR (51.33%), GP (44.0%), and FP (76.67%), leading to an impressive overall average accuracy of 53.7%. Comparatively, other models like MiniCPM-V show competitive results in AP (44.42%) and TR (47.83%), while LLaVA-NEXT-Video-34B excelled in SR (40.44%) and FP (58.89%). InternVideo2 showed strong results in TR (48.33%) and FP (52.22%), whereas"}, {"title": "4.3. Performance Across Different Difficulty Levels", "content": "As shown in Table 3 and summarized in Figure 4, all models exhibit a consistent decline in accuracy as task difficulty rises, highlighting the particular challenge of abstract cognitive tasks. Most models show a roughly 10-point accuracy drop from Easy to Medium levels, with an additional 5-point decline at the Difficult level. Similarly, LLaVA-NEXT-Video-34B shows a sharp performance decline at the Difficult level, especially in TR and SR dimensions. Under the most challenging conditions, even Qwen2-VL-72B achieves only 23.3% accuracy in Scene 7 (Sky Battle), approaching the level of random guessing. The evaluation also reveals that, while Video-LLaMA2 and LLaVA-NEXT-Video-34B perform similarly at the Easy level, LLaVA-NEXT-34B begins to outperform Video-LLaMA2 as tasks become more challenging. These results highlight the current limitations of VLMs in processing complex abstract scenes, emphasizing the need for advancements to improve their robustness in handling complex videos. All results are provided in Appendix A.4."}, {"title": "4.4. Impact of Model Size on Performance", "content": "The scale of parameters plays a crucial role in determining the performance of language models [2, 50]. Figure 5 demonstrates a strong positive correlation between model size and accuracy. For the Qwen model, as the model size scales from 2B to 7B and further to 72B, average performance scores rise significantly, from 31.9 to 42.5 and then to 53.7. This trend suggests that larger models not only achieve higher accuracy but also handle ab-"}, {"title": "4.5. Case Study", "content": "In Figure 6, we present a case from the Maze Navigation task: the symbolic green block represents the player, and the red block is the endpoint. The LVLM needs to perceive the player's spatial movements and memorize the path. Only Qwen2-VL-72B accurately selected option D, demonstrating a comprehensive understanding of the maze's game environment and correctly identifying the optimal path. In contrast, Qwen2-VL-7B and LLaVA-NEXT-Video-34B chose option C, indicating a partial grasp of the task. Meanwhile, Qwen2-VL-2B and LLaVA-NEXT-Video-7B selected option A, making an incorrect initial judgment, which reflects a significantly weaker cognitive capacity in this video environment. This example underscores Qwen2-VL-72B's advantage in processing abstract visual information, highlighting its superior ability to interpret and navigate intricate spatial details."}, {"title": "5. Conclusion", "content": "In our work, we introduced VCBench, a novel benchmark specifically designed to evaluate the cognitive abilities of LVLMs across diverse video scenarios. VCBench provides precise control over video content alignment and allows for the adjustment of video complexity, offering a cost-effective and dynamic framework for evaluation. This dual capability facilitates the development of models that more accurately emulate human-like comprehension of dynamic video content. Our findings re-"}, {"title": "A. Supplementary Materials", "content": "In this appendix, we provide additional details and resources to complement the main text, including more examples, detailed parameters, QA templates, and full experimental results."}, {"title": "A.1. Additional Cases", "content": "We present more examples of different scenes in Figures 7, 8, 9, 10, 11, ??, 13, 14, and 16. These figures illustrate the diversity and complexity of the scenes used in our benchmark, providing visual insights into the tasks designed to evaluate various cognitive abilities."}, {"title": "A.2. Detailed Parameters and QA Templates", "content": "Table 14 presents the parameters setting and Table 13 presents a comprehensive set of QA templates for the various scenes. These templates cover a range of reasoning tasks, including temporal, spatial, and action-based queries."}, {"title": "A.3. Models", "content": "We evaluate ten large video language models (LVLMs) covering seven distinct types:\n\u2022 MiniCPM-V [59]: A vision-language MLLM with 8B parameters. MiniCPM-V 2.6 outperforms GPT-4V in single-image, multi-image, and video understanding tasks. It surpasses GPT-40mini, Gemini 1.5 Pro, and Claude 3.5 Sonnet in single-image understanding. It also offers enhanced OCR capabilities, trustworthy behavior, multilingual support, and real-time video understanding on devices like the iPad.\n\u2022 Video-LLaMA2 [6]: A 7B parameter model with a Spatial-Temporal Convolution (STC) connector, designed to capture the spatial and temporal dynamics in video data."}, {"title": "A.4. Full Results", "content": "We present the complete performance results of the models across different scenes and difficulty levels in Table 15. These results provide a comprehensive comparison of model capabilities, highlighting performance variations under various cognitive tasks. This detailed analysis offers deeper insights into the strengths and weaknesses of each model when handling complex video understanding tasks."}]}