{"title": "Expanding on the BRIAR Dataset: A Comprehensive Whole Body\nBiometric Recognition Resource at Extreme Distances and Real-World\nScenarios (Collections 1-4)", "authors": ["Gavin Jager", "David Cornett III", "Gavin Glenn", "Deniz Aykac", "Christi Johnson", "Robert Zhang", "Ryan Shivers", "David Bolme", "Laura Davies", "Scott Dolvin", "Nell Barber", "Joel Brogan", "Nick Burchfield", "Carl Dukes", "Andrew Duncan", "Regina Ferrell", "Austin Garrett", "Jim Goddard", "Jairus Hines", "Bart Murphy", "Sean Pharris", "Brandon Stockwell", "Leanne Thompson", "Matthew Yohe"], "abstract": "The state-of-the-art in biometric recognition al-\ngorithms and operational systems has advanced quickly in\nrecent years providing high accuracy and robustness in more\nchallenging collection environments and consumer applications.\nHowever, the technology still suffers greatly when applied to\nnon-conventional settings such as those seen when performing\nidentification at extreme distances or from elevated cameras\non buildings or mounted to UAVs. This paper summarizes an\nextension to the largest dataset currently focused on addressing\nthese operational challenges, and describes its composition as\nwell as methodologies of collection, curation, and annotation.", "sections": [{"title": "I. INTRODUCTION", "content": "The Biometric Recognition at Altitude and Range\n(BRIAR) Program is a US Government sponsored initiative\nto advance the state of the art of biometric recognition under\nchallenging conditions. The overarching goal is to develop\nend-to-end software systems capable of overcoming severe\natmospheric distortion and difficult imaging conditions, per-\nform person detection and tracking, and fuse multi-modal\ndata for effective biometric recognition. To enable the devel-\nopment, testing, and evaluation of these software systems, the\nBRIAR Testing and Evaluation Team has gone great lengths\nto build and extend a one-of-a-kind dataset of images and\nvideo over the course of multiple data collection events. The\nBRIAR Government Collections 3 (BGC3) and 4 (BGC4)\nexpand the BRIAR dataset [7] to additional locations, more\ncomplex scenarios, and new sensors."}, {"title": "A. Contributions", "content": "The introduction of the BRIAR dataset has been a\nmonumental contribution to the biometrics community, and\nrepresents a major step forward for the computer vision\ncommunity at large. It is the first dataset is of its kind,\nand has been a foundational benchmark in over 100 papers\non biometrics and turbulence mitigation. Incorporating the\nBGC3 and BGC4 collections, the dataset consists of over\n475,000 images and 3,450 hours of video of 1,760 subjects\neach in two sets of clothing, spanning three locations with\nvarying climate and weather, captured using commercial- to\nmilitary-grade and specialized cameras at ranges up to 1,000-\nm, at view angles up to 50\u00b0, and during both constrained and\nunconstrained imaging scenarios. Model development and\ntesting is driven by continued expansion of the dataset and\nefforts to improve its quality by refining collection, curation,\nand annotation methods [4]. The addition of more diverse\ndata, both in terms of the demographics pool of its enrolled\nsubjects and the imaging conditions of the collection, will\nhelp to ensure that recognition models are equitable and\nrobust [5].\nAccurate biometric identification systems have become\na vital resource supporting security and safety initiatives.\nBeneficial applications are wide-ranging, from combating\nhuman trafficking and terrorism to supporting smart-city\ninfrastructure and disaster response efforts. Even so, potential\nmisuses of identification and recognition models as well\nas the data which shapes them are just as wide-ranging.\nOversight and control of dataset access helps to prevent\nthese potential misuses. For the BRIAR dataset, access\nrequires approval from the US government sponsor and a\ndata use agreement and review by the Institutional Review\nBoard responsible for the dataset. Those interested in access\nto the BRIAR data should contact the authors, who will\ncoordinate the request to the appropriate government contact.\nThe BRIAR dataset is already being utilized across a diverse\nrange of research initiatives.\nThe remainder of this paper is organized as follows: In"}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "The widespread availability of datasets used for facial\nrecognition has rapidly grown in recent years. However, the\nmajority of these are focused on compliant biometric captur-\ning and collected either by utilizing controlled acquisition or\nby scraping the internet for high quality images. The BRIAR\ndataset is unique in that it considers whole-body signatures\n(face, body, and gait) at long range and extreme pitch angles\nwhile participants perform a mixture of structured and open-\nended activities under realistic operational conditions.\nBRIAR BGC1-4 is complemented by additional datasets\ncollected under related efforts [18], [16], [8], [9], [10], [22],\nmany of which are openly available. There also exists a small\nbody of work prior to the BRIAR program that considers\nthe impact of long-range imaging to the performance of\nfacial recognition models. Such works include the studies and\ndatasets listed in Table I. Although these works demonstrate\nthe need for further study, they do not provide the required\nexperimental diversity to explore performance impacts re-\nlated to clothing changes, multiple environmental conditions,\nsensor/optical configuration, subject demographics, or other\nfactors that can only be teased out via a large and complex\ndataset."}, {"title": "III. DATA COLLECTION METHODS", "content": "The BGC3 and BGC4 collections build off the work\nand established procedures of previous collections and reuse\nmuch of the infrastructure and equipment of [7]. These\nadditional data collections for the BRIAR project incorporate\nnew sensors, important updates to collection systems and\nsoftware, new locations around the country, and new scenar-\nios in order to produce useful data for the development and\ntesting of robust recognition models.\nThe majority of the BRIAR dataset consists of videos and\nimages collected at an indoor controlled location and an out-\ndoor field setting, and features individual subjects. However,\nthe BGC3 collection saw the introduction of group activities\nin the field, featuring multiple subjects. Additionally, the\nBGC4 collection participants were recorded during scenarios\nheld in a mock-city setting, dubbed Hogan's Alley by the\ncollection team, often featuring multiple subjects.\nContinuous improvements are made to custom systems\nand software over the course of the program to address\nbugs and issues and to add additional tracking or collection\ncapabilities given site- and collection-specific priorities and\nrequirements."}, {"title": "A. Privacy, Security, and Well-being", "content": "Ethical research is an extremely important concern within\nthe biometrics community; it is a concern that this work\ndoes not take lightly. All BRIAR dataset collection efforts\nare performed with a privacy-first and safety-first approach.\nSubject recruitment, informed consent, participation, and\ndata handling procedures are approved by an Institutional\nReview Board (IRB), and the utmost care is taken to ensure\nthat the data is collected and stored ethically and safely.\nFurthermore, the BRIAR dataset itself is de-identified, and\ndoes not associate any media with the actual identity of any\nof the participants. Instead, each subject is assigned a unique\nsubject ID (e.g. G03045, G04237), which is used to label\nthe data from the collection activities they participate in.\nSubjects may withdraw from the collection at any time, and\ntheir data cannot appear in publications without additional\nexplicit consent."}, {"title": "B. Controlled Collection", "content": "The controlled scenarios were kept consistent with previ-\nous collections. The only exception was the separation of the\n\"random walk\" and \"cell phone\" activities [7], which were\npreviously combined in a single recording.\nImage data was collected at two stations, both of which\nhad three Nikon DSLR cameras arranged on a 12-foot\ntall vertical stand. These cameras are triggered remotely to\ncapture sets of images of the subject as they turn to face\nalong several specified directions relative to the cameras,\nwith one station collecting passport-style photos with neutral\nand smiling facial expressions, and the other collecting\nwhole-body photos in a neutral standing position and with\narms and legs extended in an x-shape pose (similar to TSA\nscreening). Figure 1 provides some examples of the neutral\nexpression and pose images. An application was developed\nas an extension to the existing BRIAR Human Subjects\nTesting Application (BHST App) to provide an interface\nto the cameras for capturing, tracking, and downloading\nimages. The new system eliminated the extensive correction\nand manual checking needed due to the unreliability of the\nprevious manual remote triggering, the independent internal\nclock of each DSLR camera, and human error."}, {"title": "C. Field Collection", "content": "The field portion of the collection takes place in a 10x10-\nm square marked-off area, where subjects are instructed by\na proctor to perform activities like standing facing along a\nseries of colored lines or walking around randomly within\nthe square. Subjects are recorded during these activities\nby commercial surveillance cameras, specialized long-range\ncameras, and unmanned aerial platforms focusing on the ac-\ntivity area from up to 500-m away in BGC3 and 720-m away\nin BGC4 (see Figure 2 for example images). Previously,\nsubjects completed outdoor activities in both clothing sets.\nStarting with BGC3, subjects completed the outdoor field\nportion of the collection wearing only one clothing set in\norder to reduce the time required for participation, which\ncould be up to four hours total.\nThe field portion of the BRIAR collections saw more sub-\nstantial changes following BGC2, including the introduction\nof new activities, examples of which can be seen in Figure\n3:\n\u2022\n\u2022\nCell Phone: the subject is instructed to walk around\nrandomly in the field area while pretending to text and\nreceive a phone call.\nBox Stack: the subject is instructed to randomly place\na cardboard box in the field area, stack another box on\ntop of it, then return both to their original locations.\n\u2022 Backpack: the subject is instructed to pick up and put\non a weighted backpack in the center of the field area,\nwalk around the area randomly, then return the backpack\nto the center.\n\u2022 Group Backpack: Up to three subjects walk around\nrandomly in the field area and pass a weighted backpack\nback and forth between them as they walk.\n\u2022 Pointing (BGC4 only): the subject is instructed to look\naround and physically point to cameras and UAVs they\ncan see.\nThe BGC3 collection event was conducted over August\nand September, 2022, in Oak Ridge, Tennessee. The hosting\nlocation for BGC3 was the same as BGC1, and the geometry\nof the field deployment followed a similar template with a\nfew significant differences (see Figure 4 for layout details).\nBGC4 took place over the month of January, 2023, in\na suburb of Chicago, Illinois. The low temperatures and\nsnow not only presented interesting atmospheric imaging\nchallenges, but also had a detrimental effect on the hardware\nand sensors used during the collection. Several sensors and\npieces of equipment were damaged or malfunctioned, some\nhad to be replaced, and some were not always accessible\nto be adjusted or fixed due to weather. View angles and\nlocations were highly varied, with two stations located on\nrooftops, and most cameras shooting over a mix of asphalt\nand grass (see Figure 5 for layout details). The weather ran"}, {"title": "D. BGC4 Mock City: Hogan's Alley", "content": "A subset of the subjects were also recorded in scenar-\nios taking place in an indoor street scene. The area was\nconstructed to resemble a commercial/urban street setting\nwith false storefronts, asphalt paving, sidewalks, streetlights,\na fire hydrant, and other similar infrastructure. Balconies\nand windows on the upper story made it convenient to\nset up elevated camera views, as seen in Figure 6. In one\ncorner, the collection team set up a mock market area with\na table of snacks and smaller tables and chairs to sit at.\nA car was parked in the street for the subjects to interact\nwith. Individual subject activities in the mock city were not\ntimestamped, instead, the proctor recorded only the entry and\nexit times of each subject. The BHST app was extended so\nthat each subject's ID, clothing set number, and mock city\nentry and exit times could be recorded along with the rest\nof the normal collection data.\nParticipants were given fake money to use in the mock\nmarket and were instructed by proctoring staff to enter the\nstreet scene and complete activities posted on numbered\nsigns throughout the area; however, the activities in the mock\ncity were purposefully unstructured, and participants were\nnot required to complete any of them. Several participants\ncould be active at one time. Generally, subjects would enter\nthe mock city after they had completed the field and con-\ntrolled portions of the collection, and once several subjects\nhad completed most of the activities, they would exit as a\ngroup."}, {"title": "IV. DATASET SUMMARY", "content": "The figures below summarize statistics for the BGC3 and\nBGC4 data sets. The BGC3 data set consists of over 45,000\nimages and 38,000 videos. The BGC4 data set consists\nof over 80,000 images and 45,000 videos. This data is\nfurther split into the BRIAR Research Set (BRS) and BRIAR\nTest Set (BTS) intended for model training and testing\nrespectively. The complete curated data of each subject is\nassigned to one of these subsets with the intention of keeping\nthe distribution of subject demographics consistent between\nBRS and BTS datasets.\nFigures 8 and 9 show the distribution of subject sex, race,\nage, height and weight of BGC3 and BGC4 subjects, respec-\ntively. Age, height, and weight have similar, approximately\nnormal distributions across the two data sets. The two data\nsets also have nearly identical makeups of subject sexes, with\nspecified sexes being slightly more female than male. Both\nBGC3 and BGC4 subjects overwhelmingly report \"white\" as\ntheir race.\nFigure 10 shows the distribution of the number of single\nBGC3/4 subject videos among distance, elevation, and yaw\nangle relative to the subject. Figure 11 shows the distribution\nof the number of BGC3/4 group videos among distance,\nelevation, and yaw angle relative to the group."}, {"title": "V. DATA CURATION METHODOLOGY", "content": "The following section details the specific strategies and\ntechnologies used in the annotation and curation process\nof the BRIAR datasets. This section additionally discusses\nupgrades and changes to procedures used to curate the\nprevious BGC1/2 datasets [7]."}, {"title": "A. Data Curation Pipeline", "content": "The collected data was curated such that there is no\nmore than one video or image for each unique subject,\nactivity, clothing set, and sensor. For each curated video a\ncorresponding XML file was generated that describes the\nactivity details, subject demographics, camera specifications,\ncamera measurements, and weather/atmospheric conditions.\nAfter curation, a set of quality assurance steps were taken\nto improve the overall quality of the final dataset. Each\ncategory of sensor warrants a slightly different curation\nprocess and so each has its own curation pipeline. Similarities\nand differences between these pipelines are shown in Table\nII."}, {"title": "1) Data Cleaning and Compilation:", "content": "Before videos and\nimages were extracted, the collection's raw data had to be\norganized and checked for inconsistencies. It was expected\nthat there would be a few issues with the metadata detailing\nwhich subjects performed certain activities: ferrying subjects\nbetween multiple concurrently running collection locations\nand dealing with de-identified subject ID numbers is bound to\nproduce mistakes. Initial validation tests were run to identify\nand fix these mistakes. These scripts ensured that timestamp\ndata did not show a subject appearing in two locations\nor activities at once, and that there were no overlapping\ntime records of different subjects performing an activity at\nthe same location. For the timestamp collisions that were\ndetected, video from the locations and times in question\nwere manually checked and cross referenced with known\nimagery of subjects to correct the records. As a second initial\nvalidation test, a few videos from each day of collection were\nrandomly selected and spot-checked at specific timestamps\nthat could be verified against a known subject activity.\nOnce these to steps were complete, the surveillance,\nspecialized, and still image sensors were associated with\nmanual measurements taken during the collections. These\nmeasurements include the distance to subject, sensor height,\nsensor yaw in relation to the subject, and sensor pitch angle.\nIn addition to these measurements, all sensor specifications\nare recorded in a csv file detailing the manufacturer, model,\nminimum / maximum focal lengths, and capture spectrum.\nThe specialized cameras and UAV platforms required\nadditional data processing due to their non-standard formats\nand configurations. The long-range specialized R&D cameras\ncapture pre-cut raw and compressed video recordings via\ntiming triggers broadcast over UDP by the BHST app. The\nformat and set of metadata tags included in the videos\nrecorded by the UAV platforms were often unique to that\nplatform, so several tools in combination were required to\nextract their timestamp metadata. Additionally, because they\ncould not be connected to the GPS-synchronized Network\nTime Protocol (NTP) servers [3] that set the internal clocks\nof the ground sensors and recording systems, the UAV time\nmetadata was manually aligned by visually matching to video\nfrom a synchronized camera."}, {"title": "2) Video and Image Extraction:", "content": "Referencing the times-\ntamps recorded at the start and end of each subject activity,\neach video was cut into segments using FFMPEG [1]. The\nvideos were cut such that each curated segment is specific\nto a subject, clothing set, and activity. All videos with a\ncompatible codec were converted to an .mp4 container and"}, {"title": "3) XML Metadata Generation:", "content": "In order to provide context\nfor all of the activities that were recorded during the collec-\ntion event, a corresponding metadata file was generated for\neach curated video. All of the information used to populate\nthis metadata file was loaded at this stage in the pipeline. The\nfinal metadata file contains information regarding the subject\ndemographics, the weather conditions, and scintillometer\n(CN2) readings at the time of the activity, as well as the\nsensor details described above. Weather and atmospheric\ndetails in the metadata file include: temperature, wind chill,\nheat index, relative humidity, wind speed, wind direction,\nbarometric pressure, and solar loading. An XSD schema\nis used to ensure XMLs follow a standard format, and\nautomated tests are run against the XML data to ensure\nrealistic values are recorded. The XSD schema is included\nalong with dataset documentation."}, {"title": "4) Quality Assurance and Finalization:", "content": "To verify that the\ndata was curated correctly, manual annotators checked every\nvideo and image from the first and last subject of each day\nduring the collection event and validated their timestamps,\ncontent, and accompanying metadata. Once these checks\nwere completed, the data was partitioned into BRIAR Re-\nsearch Set (BRS) and BRIAR Test Set (BTS) data, meant\nfor training and testing, respectively. The BRS and BTS\nsets are subject-disjoint and balanced with respect to subject\ndemographics."}, {"title": "5) Group Activity Curation:", "content": "Aside from the standard\nsingle subject activities there were also activities which\ninvolved multiple subjects participating in one video. The\ncuration process for these activities was very similar to the\nstandard curation pipeline with a few key differences. In the\nsingle subject activity videos the subject ID was used in the\ndirectory structure of the final curated video path. Instead of\nlisting multiple IDs, the group scenario videos were placed\ninto a separate directory and labeled with a unique group\nscenario identifier. The XML metadata files contain multiple\nsubject demographic sections, one for each subject in the\nrecorded activity. Because of their unstructured nature, some\nof the longer BGC4 mock city scenarios were evenly split\ninto smaller segments such that no curated video exceeds 20\nminutes in duration. Like the field group scenario videos,\neach of the segments was assigned a unique group scenario\nidentifier, with the majority of videos not requiring a split."}, {"title": "B. Annotations", "content": "Automated annotations were generated using a chain of\nopen-source and pre-trained models. Whole-Body (WB) de-\ntection was done with YOLOv5 and a fine-tuned version\nwas used on long-range and aerial videos [11]. 3D human\nmesh reconstruction with Meshtransformer and 2D keypoint\nestimation with DARK was performed on the WB detection\nresults [21], [14]. Re-ID with DG-Net++ was then performed\non the pose results to determine whether or not a WB de-\ntection was the intended subject [23]. The pose information\nhelped narrow the gallery to reference images at a similar\nyaw angle to the detections. BoT-SORT was used for track\ngeneration, which leveraged the Re-ID results for better track\nconsistency [2]. Finally, various post-processing steps were\nperformed, such as estimating the head bounding box from\nthe 3D mesh and 2D keypoints and removing spurious tracks.\nManual annotations were performed on specific video\nframes to either verify or correct the automatic annotations.\nThe main task given to manual annotators was to verify\nthat the correct subject was associated to a given track,\nbecause the main goal of the project is identification. To\nsave on cost given the size of the dataset, only the first\nand last frame of a track were used for verification instead\nof every frame of a track. Other tasks involve manual\nannotation of frame ranges with unexpected missing whole-\nbody detections, which is more common in low image quality\nconditions. The resulting sparse manual annotations were\nthen merged with automatic annotations to produce an XML\nwith metadata and annotations for each video in the dataset.\nAdditionally, any non-subject persons that were visible, such\nas data collection proctors, were censored by insertion of a\nblack rectangle in the video to satisfy IRB protocols."}, {"title": "C. Evaluation Protocol Design", "content": "The evaluation protocol design incorporates all BTS data\ncollected to date, reflecting the growing complexity of the\nBRIAR program. The BTS set is partitioned into probe and\ngallery sets with probe sets further categorized into two\nmajor types: FaceIncluded and FaceRestricted. The FaceIn-\ncluded probe set contains data where faces are visible and\nhave a head height of at least 20 pixels, ensuring that each\nsubject in every probe has at least one detectable face. In the\nFaceRestricted probe set, all faces are either occluded, have\nlow resolution (less than 20 pixels in head height), or are not\npresent. Pose estimation was the key parameter used to group\nthese categories. The probe set in the evaluation protocol\nis composed of data from long-range cameras, close-range\ncameras with elevations up to 50\u00b0 and UAVs. Each probe\nconsists of 5- to 15-second video clips that are extracted from\nthe captured activities in the field. Biometric algorithms are\ntasked with identifying these probes against the people found\nin the gallery. The evaluation protocol design for the BRIAR\ndataset utilizes two types of galleries, simple and blended.\n\u2022\n\u2022 Simple: This collection features various body and face\nimages captured from different perspectives. It includes\nwalking video sequences from various angles to support\ngait recognition. It is intended to represent an ideal\nenrollment for whole body and face recognition algo-\nrithms.\nBlended: While the Simple gallery serves as a base-\nline throughout the phases in the BRIAR Program,\nthe Blended gallery was introduced to simulate more\nrealistic conditions. It is named \"blended\" because 60%\nof the subjects utilize a full gallery, identical to the\nSimple gallery format. However, 40% of the subjects"}, {"title": "VI. CONCLUSIONS AND FUTURE WORKS", "content": "The BRIAR Program is continuing to refine and expand its\ndataset, incorporating additional locations, subject scenarios,\nand complexity. Currently, BGC5 and BGC6 remain to be\nfully curated and released. These releases, BGC3 and BGC4,\nhave placed a greater emphasis on data collection with\nan expanded range, locations, and conditions: particularly\nwinter weather and clothing. Additional group scenarios\nand the mock city data provide new challenges relating\nto occlusion, detection, and tracking. These updates enable\nresearchers to develop more generalizable models that can\nbetter handle a wider range of conditions.\nFuture work will focus on enhancing data quality through\nimproved curation and annotation processes. The goal is to\ndevelop new methods that can measurably improve both the\nBRS and BTS datasets.\nTo date, the dataset comprises 1,173 full subjects and 587\ndistractors, encompassing over 475,000 images and 3,450\nhours of video. This extensive resource is designed to support\nresearch in biometric identification at long-range and from\nelevated positions, ultimately contributing to critical security\nand intelligence requirements."}, {"title": "VII. ACKNOWLEDGMENTS", "content": "This research is based upon work supported by the Office\nof the Director of National Intelligence (ODNI), Intelli-\ngence Advanced Research Projects Activity (IARPA), via\nD20202007300010. The views and conclusions contained\nherein are those of the authors and should not be interpreted\nas necessarily representing the official policies, either ex-\npressed or implied, of ODNI, IARPA, or the U.S. Govern-\nment. The U.S. Government is authorized to reproduce and\ndistribute reprints for governmental purposes notwithstand-\ning any copyright annotation therein.\nThis research used resources from the Knowledge Dis-\ncovery Infrastructure at the Oak Ridge National Labora-\ntory, which is supported by the Office of Science of the\nU.S. Department of Energy under Contract No. DE-AC05-\n00OR22725.\nNotice: This manuscript has been authored by UT-Battelle,\nLLC, under contract DE-AC05-00OR22725 with the US\nDepartment of Energy (DOE). The US government retains\nand the publisher, by accepting the article for publication,\nacknowledges that the US government retains a nonexclu-\nsive, paid-up, irrevocable, worldwide license to publish or\nreproduce the published form of this manuscript, or allow\nothers to do so, for US government purposes. DOE will\nprovide public access to these results of federally sponsored\nresearch in accordance with the DOE Public Access Plan\n(http://energy.gov/downloads/doe-public-access-plan).\nThe authors of this paper also wish to acknowledge the\ncontributions of:\nKnowledge Discovery Infrastructure (KDI) Staff: Dallas\nSacca and Ryan Tipton.\nProctors and drivers: Raymond Borges-Hink, Nancy En-\ngle, Dale Hensley, Nikki Jones, Michael Jones, Marylin\nLangston, Matt Love, Amanda Mottern, Gio Pascascio, Linda\nPaschal, Christina Peshoff, Donna Pierce, Ryan Styles, and\nLauren Torkelson.\nUAS Pilots: Joe Baldwin, Kase Clapp, Dakota Halde-\nman, Andrew Harter, Amanda Killingsworth, Matt Larson,\nGenevieve Martin, Aaron O'Toole, Jason Richards, and Brad\nStinson.\nHealth and Safety Support Staff: Margaret Smith and\nMiranda Liner."}, {"title": "ETHICAL IMPACT STATEMENT", "content": "As discussed in subsection III-A, safe and ethical collec-\ntion and data management practices are the first priority for\nthe operation of the BRIAR team.\nThe BRIAR program is reviewed and approved by the\nCentral Department of Energy Institutional Review Board.\nThe study number is 00000094. The IRB package included\na protocol, consent, scripts, checklists, and surveys. A mod-\nification was submitted for any change to the research,\nprocedures, or team members.\nEach participant signed a consent form before partici-\npating. They had the opportunity to ask questions of the\ndesignated member of team responsible for the consent\nprocess. The participant was notified of how their data would\nbe shared and were offered the opportunity to allow their\nphotos, videos, and likeness to be used publicly by checking\na box on the consent form. This was optional and the\nparticipant was notified it was their decision.\nThe study posed minimal risk to the participants. The\nprimary risk to subjects participating in BRIAR collections is\nthe risk to privacy from disclosure of personally identifiable\ninformation (PII), i.e. a subject's appearance and likeness. To\nmitigate this risk, the BRIAR datasets are stored on secure,\naccess-controlled computers in accordance with federal re-\nquirements for the protection of PII. Any sharing of the data\nrequires approval and a signed data use agreement (DUA).\nRisks to the physical safety of participants in the study\nwere addressed as comprehensively as possible. Subjects\nincluded in outdoor collections were offered insect repellent,\nsunscreen, water, snacks, shade, and bathroom facilities to\nmitigate any risk to being outdoors. Onsite first aid care\nwas provided at no cost to participants. All BRIAR team\nmembers were required to be certified in first aid and CPR.\nParticipants were always accompanied by a BRIAR team\nmember to ensure they would not enter into areas not\nassociated with the study and in which the participant could\nencounter more significant risks.\nUnmanned aerial systems (UAS) were operated in support\nof this project. A mix of fixed-wing, tethered rotary-wing,\nand untethered rotary-wing aircraft were used. All aircraft\nwere operated by certified FAA Part 107 licensed flight crews\nin accordance with applicable regulations and documented\nsafety protocols. Those safety protocols dictate minimum\nand maximum altitudes, maintenance and inspection re-\nquirements, flight procedures, crew requirements, weather\nrestrictions, and other safety requirements in a variety of\noperational conditions.\nParticipants were compensated based on completion of\nindoor and outdoor activities. They received a gift card at\nthe end of their participation. This amount was determined\nbased on the time and effort given by the participant and was\napproved by the IRB to be appropriate to prevent monetary\ncoercion.\nThe study did include participants associated with the\nemployer of the BRIAR team. This is considered a vulnera-\nble population by the IRB. The research team implemented\nadditional measures to protect against coercion, including\nensuring that individuals recruited did not report to any\nBRIAR study team member. The collections have included\nsome university locations; in each case, the university was\nnotified of the study for review.\nAccess to the dataset is managed by the US Government\nsponsor of the BRIAR program, and requires IRB review.\nThese measures help to ensure that data use is ethical and\nfollows US laws and IRB regulations for the protection of\ncivil liberties."}]}