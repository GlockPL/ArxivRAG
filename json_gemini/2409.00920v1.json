{"title": "ToolACE: Winning the Points of LLM Function Calling", "authors": ["Weiwen Liu", "Xu Huang", "Xingshan Zeng", "Xinlong Hao", "Shuai Yu", "Dexun Li", "Shuai Wang", "Weinan Gan", "Zhengying Liu", "Yuanqing Yu", "Zezhong Wang", "Yuxian Wang", "Wu Ning", "Yutai Hou", "Bin Wang", "Chuhan Wu*", "Xinzhi Wang", "Yong Liu", "Yasheng Wang*1", "Duyu Tang", "Dandan Tu", "Lifeng Shang", "Xin Jiang", "Ruiming Tang*1", "Defu Lian*2", "Qun Liu", "Enhong Chen2"], "abstract": "Function calling significantly extends the application boundary of large language models, where high-quality and diverse training data is critical for unlocking this capability. However, real function-calling data is quite challenging to collect and annotate, while synthetic data generated by existing pipelines tends to lack coverage and accuracy. In this paper, we present ToolACE, an automatic agentic pipeline designed to generate accurate, complex, and diverse tool-learning data. ToolACE leverages a novel self-evolution synthesis process to curate a comprehensive API pool of 26,507 diverse APIs. Dialogs are further generated through the interplay among multiple agents, guided by a formalized thinking process. To ensure data accuracy, we implement a dual-layer verification system combining rule-based and model-based checks. We demonstrate that models trained on our synthesized data, even with only 8B parameters, achieve state-of-the-art performance on the Berkeley Function-Calling Leaderboard, rivaling the latest GPT-4 models. Our model and a subset of the data are publicly available at https://huggingface.co/Team-ACE/.", "sections": [{"title": "1 Introduction", "content": "Equipping Large Language Models (LLMs) with external tools has significantly enhanced the capability of AI Agents to solve complex real-world tasks [10, 15, 16]. The integration of function calling enables LLMs to access up-to-date information, perform delicate computations, and utilize third-party services, thereby unlocking a wide range of potential applications across various fields, e.g., workflow automation [26], financial reporting [20], and travel planning [6].\nDriven by practical applications, function calls can be quite complicated due to the diversity, com-plexity, and interactions of real-world APIs [15].\u00b9 For instance, real-world API parameters often extend beyond simple strings or numbers to include complex types like lists, dictionaries, nested parameters, and even combinations of these. The number of these API parameters can range from zero to dozens, and the application domains of these APIs span a wide range of businesses and industries [24]. Moreover, a single API is often insufficient to complete a task; instead, multiple tools need to be used together to perform real-world tasks [10]. The input of one API may even depend on the output of another [15], making function calls even more complicated and challenging.\nConsidering this diversity and complexity, function calls for specific tasks or user queries can generally be grouped into three categories: single function calls, parallel function calls, and dependent function calls. In a single function call, the LLM selects and executes one function to accomplish the user's task. For parallel function calls, the LLM performs multiple independent function calls simultaneously within one turn. Whereas dependent function calls involve the LLM making a series of sequential calls, with each subsequent call relying on the output of the previous ones.\nHowever, current tool-augmented LLMs primarily focus on simple function-calling scenarios with limited diversity and complexity [16]. Although satisfying performance has been achieved on the single function calling that executes one API in one turn, the capabilities of parallel and dependent function calls are largely overlooked. Furthermore, the constrained API domains, simplistic parameter types, and uniform data formats may hinder the applicability of the function calling to more complex, real-world tasks. Executing real-world function calls requires precise API selection and accurate parameter configuration, both of which are closely tied to the accuracy of the underlying data. Yet ensuring data accuracy for tooling remains a challenging problem, particularly when dealing with diverse and complex tasks.\nTherefore, in this report, we present ToolACE, a systematic tool-learning pipeline that encompasses both synthetic data generation and data verification, demonstrating robust performance with enhanced accuracy, diversity, and complexity.\nDiversity. Exposing LLMs to diverse function-calling scenarios facilitates a more well-rounded cognitive skill set of tool usage [25]. In ToolACE, we propose to emphasize three dimensions of diversity for function calling: tool diversity, format diversity, and dialog diversity. Tool diversity is achieved through our tool self-evolution synthesis method (TSS), which synthesizes tools from various domains with diverse data types and constraints. For format diversity, we propose a tool format generalization method to support any mainstream tool description and tool calling format (e.g., Json, YAML, XML, markdown) in ToolACE. Dialog diversity includes simple, parallel, dependent function calls, and non-tool-use dialogs, encompassing a wide range of function-calling use cases.\nComplexity. In ToolACE, we emphasize the complexity of the data, where the instruction-following data should possess sufficient complexity to develop the necessary skills for function calling. We also find that the LLMs learn more effectively when the data complexity is slightly above their current capability level. Data that is either too simple or too complex proves unproductive for LLMs. This aligns with the Zone of Proximal Development (ZPD) theory in educational psychology, which states that learning is most effective when the tasks are just beyond the learner's current level of independent capability but achievable with suitable support [17].\nAccuracy. Data accuracy is fundamental to the effectiveness of tool-augmented LLMs. To achieve high-quality data, we first implement a formalized thinking and self-consistency strategy to enhance"}, {"title": "2 Data Generation Pipeline", "content": "Effective use of synthetic data significantly enhances the capabilities of large language models (LLMs) [13]. Hence, in ToolACE, we propose an automated agentic framework for tool learning to generate high-quality, diverse, and complex data using advanced LLMs, as illustrated in Figure 1. The proposed framework deploys various agents to recursively synthesize diverse APIs, collaboratively construct formalized dialogs, and rigorously reflect on data quality. The following sections present our Tool Self-evolution Synthesis (TSS) module, Multi-Agent Interactive Dialog Generation (MAI) module, and Dual-Layer Validation Process (DLV)."}, {"title": "2.1 Tool Self-evolution Synthesis", "content": "The variety of APIs significantly underpins the diversity and complexity of function-calling data. As shown in Table 1, ToolACE has established a comprehensive API pool that surpasses other representative tool-augmented LLMs in both quantity and domain coverage, incorporating both real and synthesized APIs. Beyond collecting real API data, we developed a Tool Self-Evolution Synthesis (TSS) module that synthesizes API definitions with various data types and constraints. Specifically, we utilize pretraining data to extract an API context tree, where each node represents a potential application domain and functionalities for function calls, e.g., finance, health, and transport. Combining the sampled functionalities and an API example, an agent powered by a frontier LLM synthesizes new APIs. The diversity and complexity of the APIs are gradually increased through recursive self-evolution and updates. This process is achieved through three major steps: 1) Speciation, 2) Adaption, and 3) Evolution. The detailed process is illustrated in Figure 2."}, {"title": "2.2 Multi-Agent Interactive Dialog Generation", "content": "Real-world tasks involving function calls often encompass complex intents and varied requirements. To solve these real-world tasks, tool-augmented LLMs are expected to accurately identify when, what, and how many function calls are needed. To better represent real-world scenarios, we propose a multi-agent interactive (MAI) dialog generation module to synthesize function-calling dialogs. These dialogs cover a wide range of types, including simple function calls, parallel function calls, dependent functions, and non-tool-use dialogs. The MAI module is designed to ensure accuracy, complexity, and diversity throughout the generation process."}, {"title": "2.2.1 Dialog Generation Overview", "content": "The middle part of Figure 1 illustrates our dialog generation process. Initially, we sample one or more API candidates from our curated API pool, ensuring that the selected APIs share the same domain. This helps maintain topic consistency within a single dialog. Additionally, APIs within the same domain are more likely to have similar functions. Dialogs based on similar APIs can enhance the model's ability to make nuanced distinctions between different APIs.\nWe then generate dialogs through the interplay among three different agents (user, assistant, and tool), each simulated by an LLM. The user agent mainly makes requests or provides additional information, powered by multi-mode prompting and similarity-guided complication to enhance diversity and complexity. The assistant agent addresses the user's queries equipped with the given"}, {"title": "2.2.2 Generation Ensuring Accuracy, Diversity and Complexity", "content": "The MAI module generates function-calling dialogs using multiple strategies to ensure accuracy, diversity, and complexity. We introduce these strategies in sequence below.\nMulti-Mode Prompting. MAI generation begins by determining the target dialog mode for each sample. Our multi-mode prompting allows us to create a wide range of dialog types by varying the instructions given to the user agent. We generate both single-turn and multi-turn dialogs, guided by the specified turn length in the instructions.\nTo further diversify the dialog types, we adjust the number of tool calls required and the relationships among these calls during prompting, resulting in single, parallel, and dependent function calls. Additionally, we produce non-tool-use dialogs, categorized into two scenarios: when no suitable tools are available and when there is insufficient information to call the tools.\nThis variety of dialogs is essential for developing a comprehensive skill set in tool usage for LLMs.\nSimilarity-Guided Complication. Given guidance from the instructions, the user agent is able to generate an appropriate query requiring demanded function types. Apart from that, we also consider complexity in terms of linguistic level, which can be identified based on a similarity metric (please refer to Section 4.2 for details). To capture queries of varying complexity, we employ a range of templates to prompt the user agent with different linguistic styles. The query's complexity can be calculated after the dialog concludes, and then used as a criterion for data selection.\nFormalized Thinking. After the user agent generates the query, the assistant agent must determine the appropriate response action. Research has shown that encouraging LLMs to think before acting -such as through chain-of-thought prompting [21] \u2013 enhances their reasoning abilities. We employ"}, {"title": "3 Data Verification", "content": "A critical factor influencing the function-calling capability of LLMs is the accuracy and reliability of the training data. Data that is inconsistent or inaccurate can hinder the model's ability to interpret and execute functions [11]. Unlike general question-answering data, where verifying correctness can be challenging, function-calling data is more straightforward to validate. This is because a successful function call must strictly match the format specified in the API definition. Building on this insight, we propose an automatic dual-layer verification system (DLV) to verify our synthesized data, as shown in the right part of Figure 1, which consists of a rule verification layer, and a model verification layer, where these results are all overseen by human experts.\nData Structure. Each data sample contains three components: system prompt, tool list, and dialogs, all of which are stored in JSON format with the necessary pattern requirements for ToolACE. JSON format is naturally easy to parse and maintains a clear hierarchical structure. Figure 3 shows our data examples of API definition and function call. Each function call is also formatted in JSON under the field tool_usage of the assistant role, with the API name and parameters explicitly listed."}, {"title": "Rule Verification Layer", "content": "The rule verification layer deploys a rule checker to ensure that the data strictly adheres to the predefined syntactic and structural requirements of the API. The quality of the data is evaluated from four key aspects: API definition clarity, function calling executability, dialog correctness, and data sample consistency, guided by a meticulously curated set of rules, as listed in Appendix A.\nFor instance, to verify function calling executability, we implement the following procedures: First, we confirm that the API name matches one from the given tool list. Next, we verify that all required parameters are accurately provided. Finally, we use regular expressions to ensure that the parameter formats and patterns adhere to those specified in the API documentation. These procedures allow us to validate the correctness and executability of function calls without the need for actual execution, which enhances efficiency and reduces deployment overhead. Examples of possible errors detected by the rule verification layer are displayed in Figure 4."}, {"title": "Model Verification Layer", "content": "The model verification layer further incorporates LLMs to filter out erroneous data that cannot be detected by the rule checker, with a primary focus on content quality. However, we find that presenting a data sample directly to the LLM for correctness evaluation is too complex, often resulting in unsatisfactory outcomes. To address this, we decompose the model verification task into several sub-queries that mainly cover three key aspects:\n\u2022 Hallucination Detection: Identifies whether the values of input parameters in function calls are fabricated\u2014not mentioned in either the user query or the system prompt.\n\u2022 Consistency Validation: Verifies that the responses can effectively complete the user's task and ensures the dialogue content adheres to the constraints and instructions in the user query and system prompt.\n\u2022 Tool Response Check: Ensures that the simulated tool responses align with the API definition.\nEach aspect is evaluated by an individual expert agent, powered by an LLM. We also incorporate several other data quality verification queries to eliminate repetitive responses and meaningless tokens within the data. The pass rate of our data for the rule verification and the model verification respectively are presented in Figure 5. Additionally, we apply the self-consistency strategy mentioned in Section 2.2 with a majority voting to enhance decision accuracy."}, {"title": "4 Data Analysis", "content": "This section provides an in-depth analysis of our final data after verification, focusing on the distribu-tion across multiple dimensions with respect to both diversity and complexity."}, {"title": "4.1 Diversity", "content": "In this section, we quantify and present the diversity of our data across three dimensions: tool diversity, format diversity, and dialog diversity. These dimensions are critical for assessing the richness of the dataset and its capacity to support robust tool learning."}, {"title": "4.1.1 Tool Diversity", "content": "Large API Pool. Through our TSS module in Section 2.1, we curate a comprehensive collection of 26,507 distinct APIs, encompassing both real-world and synthesized API definitions. Our APIs span 30 first-level coarse-grained domains, 390 second-level domains and 3,398 third-level domains, significantly enriching the diversity of our tool-learning data. The left figure in Figure 6 illustrates the distribution of the 26,507 APIs across the 30 first-level domains, including science, data, and travel, among others. The right figure details the distribution of the second-level domains within the 'Entertainment' domain. The diversity of APIs lays a robust foundation for the diversity of the overall dataset, enabling the model to generalize effectively to new APIs and tasks.\nParameter Types. We aim to maintain a balanced distribution of parameter types within the APIs. To prevent the overrepresentation of string parameters, we intentionally increase the proportion of non-string types (e.g., integer, boolean, float, array, dictionary) during the API synthesis process. The resulting distribution is depicted in Figure 7.\nTool diversity in our dataset enhances both the breadth of API coverage and the richness of parameter variety. Such diversity is crucial for the development of models that can adapt to a wide range of scenarios and inputs for real-world tasks."}, {"title": "4.1.2 Format Diversity", "content": "Tool Description. As mentioned in Section 2.2.2, we store API definitions in a standard JSON format for subsequent verification. After the verification process, we convert the API definitions to multiple formats, including JSON, YAML, XML, and Markdown, to accommodate different API specification needs. Figure 17 in Appendix B illustrates examples of the diverse formats.\nFunction Calls. We have created hundreds of unique function call formats through various combi-nations. These format requirements are explicitly declared in the system prompt, ensuring that the dialog data adheres to the specified function call formats, thereby effectively enhancing the format compliance capability of function calling. Examples of such formats are provided in Figure 9.\nFormat diversity strengthens the model's ability to understand various requirements across different scenarios, thereby improving its capability to effectively follow any specific format instructions and to avoid the short cutting."}, {"title": "4.1.3 Dialog Diversity", "content": "Dialog Mode. ToolACE supports a range of function call modes, including single, parallel, dependent function calls, and non-tool-use dialogs. It also covers different interaction types, such"}, {"title": "4.2 Complexity", "content": "An appropriate level of complexity is essential for improving the adaptability of models to more challenging function-calling scenarios. We investigate the factors influencing complexity from two perspectives: query-level complexity and dataset-level complexity."}, {"title": "4.2.1 Query-level Complexity", "content": "We propose to measure the complexity of individual queries by evaluating the relevance between the user query and the description of the used tools. Generally, a higher affinity between these two elements increases the likelihood that the model will select the correct tool. Conversely, a larger discrepancy suggests that more advanced reasoning is needed to identify the appropriate function, thereby indicating a more challenging and complex query.\nTo quantify this relevance, we employ BGE [22] to extract embeddings and utilize cosine similarity to assess the degree of similarity. Let $e_q$ and $e_t$ be the embeddings of the query and the API description, respectively. The complexity is then defined as follows:\n$complexity = -cosine\\_similarity(e_q, e_t)$.\n(1)\nThe distribution of the resulting complexity scores is depicted in Figure 10, with an average score of 0.439."}, {"title": "4.2.2 Dataset-level Complexity", "content": "We found that basic tool capabilities, such as single function calls, require minimal data for effective model learning. In contrast, more advanced and complex capabilities, such as parallel function calls and multi-turn dialogs, usually require more training data. Therefore, we increase the proportion of complex data (i.e., parallel function calls and multi-turn dialogs) in our dataset. The distribution of these dialog modes is illustrated in Figure 8.\nThe distribution of dialog modes is crucial in determining the dataset's complexity. By carefully balancing the composition of dialog modes, we can tailor the dataset's complexity to the needs of our models, ensuring that they are sufficiently challenged to learn effectively."}, {"title": "5 Experiment", "content": ""}, {"title": "5.1 Experiment Setup", "content": "To validate the effectiveness of our approach, we have conducted extensive experiments by training LLMs with the generated data. We train the open-source LLM, LLaMA3.1-8B-Instruct [2], in the supervised fine-tuning (SFT) manner. We refer to the model trained with our data as ToolACE-8B. Due to the limited resources, we adopt the parameter-efficient training strategy LoRA [9] to fine-tune the model. As for the hyper-parameters setting, we adopt one of the most common settings, which sets the rank as 16 and alpha as 32 for all modules in the model. More detailed training settings are shown in Table 2. We compare the overall performance with the state-of-the-art open-source and API-based models, including GPT-4 series \u00b2, Gemini-series \u00b3 and Claude-3 series \u2074, as well as fine-tuned function calling models including Gorilla-OpenFunctions-v2 [14], xLAM-series [11], and Functionary series [12]. We then conduct in-depth ablation study to reveal the effectiveness of accuracy, diversity, and complexity."}, {"title": "5.2 Overall Performance Analysis", "content": "To assess the effectiveness of the proposed ToolACE-series models regarding their functional calling capabilities, we conducted evaluations using the BFCL benchmark [23], including BFCL-v1 and BFCL-v2 \u2075. This benchmark is a comprehensive and executable function call evaluation specifically designed to assess the ability of LLMs to invoke functions. The results for our ToolACE-8B model"}, {"title": "5.3 Ablation Study", "content": "To further validate the effect of various mechanisms introduced in data generation and validation, we conduct a series of in-depth ablation studies from the perspective of accuracy, complexity, and diversity, respectively."}, {"title": "5.3.1 Ablation on Accuracy", "content": "Effects of Formalized Thinking. This part explores the impact of formalized thinking on the dialogue generation process. We randomly selected 1,000 user queries generated by our user agent and continued the generation process under two different conditions: one with formalized thinking and the other without. We then evaluated the generated outputs using our DLV data verification module. Results presented the pass rates for the two generated datasets. The results indicate that data generated with formalized thinking consistently achieves higher pass rates across both verification layers, with"}, {"title": "Effects of the verification system", "content": "As detailed in previous sections, our verification system comprises two layers: a rule checker and a model checker. To evaluate the efficacy of each layer, we train LLaMA3.1-8B-Instruct with LoRA using three distinct datasets: (1) data without any verification (denoted as w.o. dual), (2) data without model checking (denoted as w.o. model), and (3) data subjected to dual-layer verification (denoted as Final). The resulting fine-tuned models are assessed using the BFCL benchmark. Comparative analysis reveals that the model trained on data without model checking surpasses that trained on unverified data in terms of both executable and overall accuracy, thereby validating the rule checker's effectiveness. Moreover, the model trained on dually verified data significantly outperforms both ablation models in terms of AST and overall accuracy, underscoring the indispensable role of the model checker."}, {"title": "5.3.2 Ablation on Complexity", "content": "Data Sampling for Various Complexity. To effectively assess the impact of dataset complexity on the model's performance, we have conducted a sampling of the entire dataset based on the aforementioned complexity assessment metrics. This process has yielded three distinct subsets of varying complexity"}, {"title": "Effects of Complexity", "content": "We conduct experiments on those three subsets with varying complexity and evaluate the fine-tuned models on the BFCL benchmark. The results are illustrated in Figure 12. The model trained on ToolACEmedium shows slight superiority compared with another two subsets, for both overall and tool-use accuracy. This finding aligns with our hypothesis that optimal data complexity is essential for LLM training; excessively simple or complex data can hinder performance."}, {"title": "5.3.3 Ablation on Diversity", "content": "Data Sampling for Various Diversity. To assess the impacts of the diversity, we employed a sampling strategy to generate three subsets with varying degrees of diversity, namely ToolACElow, ToolACEmedium, and ToolACEhigh. Initially, all APIs are clustered into 30 groups using K-means based on their names and descriptions. Subsequently, API sets are constructed by selecting APIs from 6, 14, and 30 clusters, respectively. Instances are then categorized into three subsets according to their associated APIs. Approximately 30,000 instances are randomly selected from each subset, resulting in three training sets with distinct levels of diversity.\nEffects of Diversity. Experiments are conducted to train LLaMA-3.1-8B-Instruct on three subsets described above. The results on BFCL are reported in Figure 13. A clear correlation between training data diversity and overall model accuracy is observed, emphasizing the critical role of API diversity in model performance. Notably, improvements in relevance detection are particularly pronounced, suggesting that exposure to a wider range of APIs enhances the model's ability to discriminate between subtle API differences, thereby enhancing the ability of irrelevance detection."}, {"title": "5.4 Scaling Performance of Model Size", "content": "Scaling laws posit a correlation between model size and performance. To investigate the scalability of functional calling performance, we conduct experiments using the Qwen-1.5-xB-Chat series, which"}, {"title": "5.5 Study on Various Backbone LLMs", "content": "To investigate the influence of the LLM backbone, we experiment with several (approximately) 8B-scale models: Qwen1.5-7B-Chat [3], LLaMA-3-8B-Instruct, and LLaMA-3.1-8B-Instruct. Fine-tuned models are evaluated on the BFCL benchmark. Across all models, fine-tuning yields substantial performance gains, highlighting the effectiveness of our ToolACE. Due to differences in pre-training corpora, such as Qwen is trained with more Chinese conversational samples, raw models exhibit varying functional calling capabilities, with LLaMA-3.1-8B-Instruct demonstrating superior performance. While this hierarchy persisted after fine-tuning, the performance gaps narrowed, suggesting that our dataset can potentially enhance the functional-calling abilities of those LLMs tailored for other skills, such as conversational skills."}, {"title": "5.6 Study on General Capabilities", "content": "To assess the impact of ToolACE training on broader capabilities of LLMs, we conduct experiments across multiple benchmarks evaluating general ability, coding, mathematics, reasoning, and functional calling. Raw LLaMA-3-8B-Instruct, LLaMA-3.1-8B-Instruct, and the functionally specialized xLAM-7B-fc-r serve as baselines."}, {"title": "6 Conclusion", "content": "This paper presents ToolACE, an automated data generation pipeline designed to enhance the function-calling capabilities of large language models. ToolACE utilizes a novel self-evolution synthesis process and a multi-agent interactive system to curate accurate, complex, and diverse APIs and dialogs. Our results demonstrate that even smaller models trained with ToolACE can achieve state-of-the-art performance, thereby advancing the field and setting new benchmarks for tool-augmented AI agents."}]}