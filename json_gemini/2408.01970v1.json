{"title": "SR-CIS: Self-Reflective Incremental System with Decoupled Memory and Reasoning", "authors": ["Biqing Qi", "Junqi Gao", "Xinquan Chen", "Dong Li", "Weinan Zhang", "Bowen Zhou"], "abstract": "The ability of humans to rapidly learn new knowledge while retaining old memories poses a significant challenge for current deep learning models. To handle this challenge, we draw inspiration from human memory and learning mechanisms and propose the Self-Reflective Complementary Incremental System (SR-CIS). Comprising the deconstructed Complementary Inference Module (CIM) and Complementary Memory Module (CMM), SR-CIS features a small model for fast inference and a large model for slow deliberation in CIM, enabled by the Confidence-Aware Online Anomaly Detection (CA-OAD) mechanism for efficient collaboration. CMM consists of task-specific Short-Term Memory (STM) region and a universal Long-Term Memory (LTM) region. By setting task-specific Low-Rank Adaptive (LoRA) and corresponding prototype weights and biases, it instantiates external storage for parameter and representation memory, thus deconstructing the memory module from the inference module. By storing textual descriptions of images during training and combining them with the Scenario Replay Module (SRM) post-training for memory combination, along with periodic short-to-long-term memory restructuring, SR-CIS achieves stable incremental memory with limited storage requirements. Balancing model plasticity and memory stability under constraints of limited storage and low data resources, SR-CIS surpasses existing competitive baselines on multiple standard and few-shot incremental learning benchmarks.", "sections": [{"title": "1. Introduction", "content": "Deep learning models have demonstrated superior performance in a wide range of downstream tasks [12, 14, 22]."}, {"title": "Related Works and Considerations", "content": "To bridge this gap, Continual Learning (CL) has been proposed to develop deep learning models that can continuously learn from new data without forgetting previous knowledge, thus addressing catastrophic forgetting, which is crucial for adapting to changing task scenarios in real-world applications [33, 40]. Mainstream CL strategies can be categorized into three types: 1) maintaining a memory buffer to store data from current tasks and replaying them in subsequent tasks [4, 34], or directly training incremental generative models to generate data for pseudo-replay [7, 11]; 2) imposing regularization constraints on weight changes during updates [1, 18]; and 3) dedicating isolated parameters for different tasks to mitigate the impact on parameters from previous tasks during updates, which can be achieved by dynamically expanding additional parameters for new tasks [3, 35] or by allocating static fixed parameters to different tasks [2, 24]. However, static parameter allocation and the addition of regularization constraints often constrain the learning of new tasks implicitly or explicitly [28]. Additionally, storing data for each class in a memory buffer or continuously expanding parameters for new tasks can result in significant storage overheads when dealing with continual task streams [41]. Although methods employing generative models for pseudo-replay are available, they come with certain limitations. These methods either require fine-tuning conditional generative models for each class in each task, which demands a large amount of training data to be effective [37], or they need well-trained pre-trained classifiers on previous"}, {"title": "System Design", "content": "To construct such a system, we first formalize the system framework to clarify the specific functionalities required for each component. This step facilitates concrete component instantiation and method design. Based on this framework, we propose the Self-Reflective Complementary Incremental System (SR-CIS), which consists of a deconstructed Complementary Inference Module (CIM) and Complementary Memory Module (CMM). The CIM includes a Large Multimodal Model (MLLM) for executing slow reasoning, instantiated by LLaVA [22], and a small model for fast inference, instantiated by pre-trained Vision Transformer (ViT) [8]. To execute accurate mode switches in reasoning, we introduce the Confidence-Aware Online Anomaly Detection (CA-OAD) mechanism. This mechanism merges each sample's confidence and its standard deviation online through Exponential Moving Average (EMA) and performs hard sample assessment. During training, the temperature scaling factor is tuned through feedback to penalize irrational behaviors, such as screening out high-confidence negative samples and low-confidence positive"}, {"title": "2. Problem Formulation", "content": "A standard class-incremental learning problem consists of a series of disjoint tasks $T_1, T_2,..., T_T$. Each task $T_t$ comprises input sample pairs $z^{(t)} = (x^{(t)}, y^{(t)})$ which are i.i.d. samples drawn from distribution $D_t$, forming the corresponding training set $S^{(t)}_{tr} = \\{(x^{(t)}_i, y^{(t)}_i)\\}_{i=1}^{|S^{(t)}_{tr}|}$ and test set $S^{(t)}_{te} = \\{(x^{(t)}_i, y^{(t)}_i)\\}_{i=1}^{|S^{(t)}_{te}|}$. Here, $y_i^{(t)} \\in \\mathcal{Y}_t$ denotes the label set corresponding to $T_t$, with $\\mathcal{Y}_{t_1} \\cap \\mathcal{Y}_{t_2} = \\emptyset, \\forall t_1 \\neq t_2$. During the incremental training phase, the incremental training set $S_t$ is sequentially inputted for model training. Upon completion of training on $S_t$, all the seen training sets $S_1, S_2,..., S_t$ are no longer visible. The incremental model is evaluated on the union of test sets from all previously encountered tasks, denoted as $\\mathcal{E}_t = \\cup_{i=1}^{t}S^{(i)}_{te}$."}, {"title": "3. SR-CIS: A CLS with Memory and Reasoning Deconstruction", "content": "In this section, we first formalize the system components based on their expected functionalities and introduce the three main actions of the system. Next, we detail the execution of these actions while presenting the instantiation of each component."}, {"title": "3.1. System Formulation", "content": "As mentioned earlier, an ideal CLS should consist of two deconstructed parts, namely the CMM $\\mathcal{M}$ and CIM $\\mathcal{I}$. The parameter memory $\\mathbb{M}_p$ and representation memory $\\mathbb{M}_r$ learned in tasks form the LTM region $\\mathbb{M}_{LTM} = \\{\\mathbb{M}^{LTM}_{p}, \\mathbb{M}^{LTM}_{r}\\}$ and STM region $\\mathbb{M}_{STM} = \\{\\mathbb{M}^{STM}_{p}, \\mathbb{M}^{STM}_{r}\\}$. Together with the scenario memory $\\mathbb{M}_{Sce}$ used for recall and memory restructuring, they constitute the CMM $\\mathbb{M} = \\{\\mathbb{M}_{LTM}, \\mathbb{M}_{STM}, \\mathbb{M}_{Sce}\\}$. The CIM consists of a fast inference component $\\mathcal{I}_f$, a slow inference component $\\mathcal{I}_s$, and an inference mode switching mechanism $\\mathcal{I}_{Swi}$, represented as $\\mathcal{I} = \\{\\mathcal{I}_F, \\mathcal{I}_S, \\mathcal{I}_{Swi}\\}$. With these components in place, the system can perform the following actions:"}, {"title": "Action $A_L$: Learning.", "content": "$\\mathcal{I}_F$ receives the training set $S^{(t)}_{tr}$ and performs training. During the training process, a part of samples $z^{(t)}_i$ are memorized as scenarios $s^{(t)}$ through the scenario recording operation $R_M$, and added to the scenario memory: $\\mathbb{M}_{Sce} = \\mathbb{M}_{Sce} \\cup s^{(t)}$, where $s^{(t)} = R_M(x^{(t)}, y^{(t)})$. Additionally, each training sample to the accumulation of online classification experience, denoted as $\\mathbb{W}$, which $\\mathcal{I}_{Swi}$ references during testing. Upon completion of training, the resulting short-term parameter memory $\\mathbb{M}^{STM, t}_p$ and representation memory $\\mathbb{M}^{STM, t}_r$, corresponding to the task are stored in STM: $\\mathbb{M}^{STM} = \\mathbb{M}^{STM} \\cup \\mathbb{M}^{STM, t}_p$, $\\mathbb{M}^{STM} = \\mathbb{M}^{STM} \\cup \\mathbb{M}^{STM, t}_r$. $\\mathbb{M}^{STM}_p, \\mathbb{M}^{STM}_r$ and $\\mathbb{M}^{STM}_r$ are initialized as $\\emptyset$."}, {"title": "Action $A_R$: Memory Restructuring.", "content": "Using the scenarios in $\\mathbb{M}_{Sce}$ for scenario replay, we obtain a set of scenarios $\\mathbb{\\check{S}} = \\{(\\check{x}_i^{(t)}, y_i^{(t)})\\}_{i=1}^{N}$, where the replayed scenario $\\check{x}_i^{(t)}$ is obtained through the scenario reproduction operation $\\mathcal{R}_M$: $\\check{x}_i^{(t)} = \\mathcal{R}_M(s_i^{(t)})$. Subsequently, the restructuring mechanism $\\mathcal{U}$ conducts memory restructuring: $\\mathbb{M}^{STM} = \\mathcal{U}(\\mathbb{M}^{STM}, \\mathbb{M}^{STM}, \\mathbb{\\check{S}})$, and then merged as LTM: $\\mathbb{M}^{LTM} = \\mathbb{M}^{STM}$, $\\mathbb{M}^{LTM} = \\mathbb{M}^{STM}$. When the number of elements in $\\mathbb{M}^{STM}$, denoted as $|\\mathbb{M}^{STM}|$, equals the restructuring period $\\epsilon$, reset is performed: $\\mathbb{M}^{STM} = \\mathbb{M}^{LTM}$"}, {"title": "Action $A_I$: Fast and Slow Inference.", "content": "During the inference phase, for each test input $x^{(t)}$ with an unknown label, the fast inference is first executed as $o^{(t)} = \\mathcal{I}_F (x^{(t)})$. Then, the inference switching mechanism $\\mathcal{I}_{Swi}$, performs anomaly detection combined with previous classification experience $\\mathbb{W}$: $\\hat{o}_{Swi} = \\mathcal{I}_{Swi}(o^{(t)}, \\mathbb{W})$. If $\\hat{o}_{Swi} = 0$, the prediction result from $\\mathcal{I}_F$ is directly returned as $\\hat{y}$. Otherwise, the slow inference is conducted: $\\hat{y} = \\mathcal{I}_S(o^{(t)})$, and the result is returned."}, {"title": "3.2. STM Learning with Scenario Memorization", "content": "Instantiation of Fast and Slow Inference Components In common understanding, fast thinking is more intuitive, while slow thinking is more rational. Based on this characteristic, we instantiate the fast inference component $\\mathcal{I}_f$ using a classification model parameterized by $\\theta$ for classification tasks. For the slow inference component $\\mathcal{I}_s$, we use an MLLM to perform more complex \"rational\" judgments. Specifically, deploying pre-trained models and fine-tuning them for downstream tasks is highly effective in current practices [6, 32]. The use of a pre-trained ViT architecture for incremental learning is also becoming mainstream [21, 38]. Thus, we choose a pre-trained ViT as $\\mathcal{I}_f$. For $\\mathcal{I}_S$, we select LLaVA, the most mainstream MLLM architecture [22].\nInstantiation of Parameter and Representation Memory After instantiating the inference components, we aim to 1) achieve memory deconstruction within the system and 2) ensure the model's plasticity for new tasks. LoRA enables efficient fine-tuning of pre-trained models, and its parameters can be explicitly stored and accessed. Therefore, we instantiate the task-specific parameter memory using LoRA, represented as $W^{(t)} = B^{(t)} A^{(t)}$, to be merged into $\\mathcal{I}_F$ during training. For each newly encountered class $y \\in \\mathcal{Y}_{uns}$, where $\\mathcal{Y}_{uns}$ denotes the set of unseen classes, we allocate a set of prototype weights $p_w$ and prototype biases $p_b$ instantiated by learnable embeddings as representation memory.\nScenario Recording of SRM To efficiently record scenarios corresponding to a portion of input samples for subsequent memory restructuring, we draw inspiration from multisensory learning [26]. For humans, directly recalling a specific scenario may be challenging, but it is easier to recall based on a description of that scenario. Motivated by this, we use $\\mathcal{I}_S$ to generate textual descriptions $s = R_M(x, y)$ for m samples of each class $(x, y)$. These descriptions are then added to the scenario description pool: $\\mathbb{M}_{Sce} = \\mathbb{M}_{Sce} \\cup s$. This approach avoids the direct storage of raw images and instead stores scenarios in a more efficient textual form, significantly reducing storage consumption.\nExperience Accumulation for Anomaly Detection On task $T_t$, let the Softmax confidence denoted as $o^{(t)}_i$ and $c^{(t)}_i = \\max_{1 \\le i \\le |\\mathcal{Y}_{seen}|} o^{(t)}_i$, $g^{(t)} = \\mathcal{I}_{F+W^{(t)}}(x^{(t)})$ is the output class token of $\\mathcal{I}_F$ and $\\mathcal{I}_{F+W^{(t)}}$ represents $\\mathcal{I}_F$ augmented with the LoRA block $W^{(t)}$ corresponding to $T_t$ and $\\mathcal{Y}_{seen}$ denotes the set of seen classes. To provide prior experiential references for Anomaly Detection for more accurate anomaly detection, we need to accumulate classification experience $\\mathbb{W}$ online within the data stream. To this end, we utilize EMA to accumulate the confidence and the standard deviation of confidence online for input samples, avoiding the impact of noise fluctuations on the estimation error of the mean and standard deviation within a single batch:\n$\\mathbb{W}^c = \\beta_c * \\frac{1}{|B|} \\sum_{i=1}^{B} c^{(t)}_i + (1 - \\beta_c) * \\mathbb{W}^c$, (1)\n$\\mathbb{W}^{std} = \\beta_{std} * \\sqrt{\\frac{1}{|B|} \\sum_{i=1}^{B} (c^{(t)}_i - \\frac{1}{|B|} \\sum_{i=1}^{B} c^{(t)}_i )^2} + (1 - \\beta_{std}) * \\mathbb{W}^{std}$, (2)\n$\\mathbb{W} = \\{\\mathbb{W}^c, \\mathbb{W}^{std}\\}$ (3)\nwhere $B = \\{(x^{(t)}, y^{(t)})\\}_{i=1}^{|B|}$ denotes the input training batch and $\\beta_c, \\beta_{std}$ are hyperparameters. Thus the online experience can then be represented as $\\mathbb{W} = \\{\\mathbb{W}^c, \\mathbb{W}^{std}\\}$. To ensure the accuracy of detection during the inference phase, we initialize a learnable temperature scaling parameter $\\tau = 1$ for each $y \\in \\mathcal{Y}_{seen}$. Subsequently, $\\tau^{y}$ is dynamically adjusted based on the feedback from the online classification experience. Specifiacally, for each training batch,"}, {"title": "3.3. Memory Restructuring Based on Scenario Replay", "content": "After learning STM, it is necessary to restructuring STM according to the previous scenario memory, integrating fragmented task memories into a task-general LTM. Since the scenarios in the episodic description pool returned by $A_L$ are stored in text form, to replay them as scenario images, we introduce a pre-trained SDXL [30] to perform the scenario reproduction operation $\\mathcal{R}_M$. We first randomly sample N descriptions $\\{s_i\\}_{i=1}^{N}$ from the description pool $\\mathbb{M}_{Sce}$, then execute the reproduction, and in conjunction with the corresponding class labels of these descriptions to construct a set of scenarios $\\mathbb{\\check{S}} = \\{\\check{x}_i, y_i\\}_{i=1}^{N} = \\{\\mathcal{R}_M(s_i)\\}_{i=1}^{N}$. Subsequently, based on the scenario set, we perform memory restructuring utilizing the task-specific LoRAs. Drawing inspiration from the LoRA-hub's strategy of combining multiple task LoRAs for compositional generalization [16], we conduct composite optimization on $\\alpha = (\\alpha_1, \\alpha_2,..., \\alpha_t)$ over the scenario set targeting the objective $\\mathcal{L}_{TPA}$.\n$\\alpha_s = \\mathop{\\text{min}} \\frac{1}{|\\mathbb{\\check{S}}|} \\sum_{i=1}^{|\\mathbb{\\check{S}}|} - log\\frac{exp\\((\\mathcal{I}_{F+W_S}(\\check{x}^{(t)}_i), p_{y^{(t)}_i}) + p_{b_{y^{(t)}_i}})/\\tau^{y^{(t)}_i}\\)}{\\sum_{y' \\in \\mathcal{Y}^b} exp((\\mathcal{I}_{F+W_S}(\\check{x}^{(t)}_i), p_{y'}) + p_{b_{y'}})/\\tau^{y})}$, (10)\nwhere $W_S = (\\sum_{i=1}^{t} B^{(i)}) (\\sum_{i=1}^{t} \\alpha_i A^{(i)})$. Upon obtaining $\\alpha_s$, we proceed to merge memories to derive the reorganized parameter memory $\\mathbb{W}' = (\\sum_{i=1}^{t} B^{(i)}) (\\sum_{i=1}^{t} \\alpha_i A^{(i)})$, where $\\alpha_i$ is the i-th element of $\\alpha_s$. Subsequently, we consolidate the restructured parameter memory and representation memory into $\\mathbb{M}_{LTM}$: $\\mathbb{M}^{LTM}_p = \\mathbb{W}'$, $\\mathbb{M}^{LTM}_r = \\mathbb{M}^{LTM} \\cup \\bigcup_{y \\in \\mathcal{Y}^{(t)}} \\{p_w, p_b\\}$. When the size of $|\\mathbb{M}^{STM}|$ reaches the restructuring period $\\epsilon$, we perform parameter memory initialization: $\\mathbb{M}^{STM} = W_{\\sum}$ to control the maximum number of short-term parameter memories to be at most $\\epsilon$."}, {"title": "3.4. Fast and Slow Inference Based On Classification Experience", "content": "During the inference stage, for each test sample $X$, the long-term parameter memory is first integrated into $\\mathcal{I}_F$ and fast inference is executed: $\\xi = \\mathcal{I}_{F+W}(x)$, and the predictive confidence is calculated as $\\varphi = \\mathop{\\text{max}}_{1 \\le i \\le |\\mathcal{Y}_{seen}|} exp((\\xi, p_y) + p_b)/\\sum_{y' \\in \\mathcal{Y}_{seen}} exp((\\xi, p_{y'}) + p_b)$. Building upon this, we propose CA-OAD to perform anomaly detection in conjunction with accumulated classification experience. If $\\frac{(\\varphi - \\mathbb{W}^c)}{\\mathbb{W}^{std}} < \\gamma_n$, then proceed with the slow inference $\\mathcal{I}_S$; otherwise, retain the prediction of $\\mathcal{F}^S$, denote as $\\hat{y}$. Specifically, based on the TopK labels $\\hat{y}_{TopK}$ output by the fast inference, we have designed a multiple-choice question as a query $Q(\\hat{y}_{TopK})$:\nHere, 'Textualize' performs the textual representation of the labels. Subsequently, we have the MLLM executing slow inference to answer the constructed query, yielding $\\hat{y} = \\mathcal{I}_S(Q(\\hat{y}_{TopK}))$, and we conduct label correspondence operations on the returned results. If the returned result contains only one of the given classes, it is considered a valid response, otherwise, we retain the result of the fast inference, i.e. $\\hat{y} = \\hat{y}^F$. After the inference is concluded, the action $A_I(\\mathcal{I}_F, \\mathcal{I}_S, \\mathcal{I}_{Swi}, \\mathbb{W})$ returns the final prediction result $\\hat{y} = \\hat{y}$."}, {"title": "3.5. Workflow of SR-CIS", "content": "Integrating the aforementioned components, we have constructed the SR-CIS, which achieves stable memory preservation and restructuring through the decoupling of memory along with inference. It stores scenario memory through textual descriptions and periodically initializing parameter memory, ensuring limited storage requirements. The incorporation of memory restructuring ensures the classification backbone's flexibility to adapt to tasks, thus providing plasticity. Meanwhile, there is no need for extensive data resources for online generative model training. The workflow of SR-CIS is depicted in Fig. 2, for each task $T_t$, we first execute $A_L(S^{(tr)}, \\mathcal{I}_F, \\mathcal{I}_S)$ to learn and store $\\mathbb{M}^{STM}$, while accumulating online experience $\\mathbb{W}$ and $\\mathbb{M}_{Sce}$."}, {"title": "4. Experiments", "content": "4.1. Experimental Settings\nAligned with current baselines based on fine-tuning pre-trained ViTs [21, 36], we employ CIFAR100 [20], ImageNet-R [13], and DomainNet [29] for model training and evaluation. CIFAR100 comprises 100 classes, each with 500 training images and 100 testing images, serving as a common dataset in continual learning scenarios. ImageNet-R is an extension of the ImageNet dataset, encompasses 200 classes totaling 30,000 images, with 20% allocated for testing. It has emerged as a benchmark dataset for continual learning tasks [10, 38]. DomainNet, a dataset of common objects across 6 diverse domains, includes 345 classes with 600,000 images. We constitute our training and testing sets by taking the first 200 images from each class in the training set and the first 100 images from each class in the test set.\nWe follow the setting of [21, 36] and divide CIFAR100 into 10 tasks, each containing 10 classes. ImageNet-R is divided into 5, 10, and 20 tasks, each containing 40, 20, and 10 classes, respectively. DomainNet is divided into 5 tasks, each containing 69 classes.\nWe use existing continuous learning methods [21, 39] to evaluate model performance using two popular metrics: the final average accuracy $(ACC_T)$ and the final current task accuracy $(a_{T, T})$. Here, $ACC_T$ is defined as $ACC_T = \\frac{1}{T} \\sum_{j=1}^{T} a_{i, j}$, where $a_{i, j}$ represents the accuracy of the j-th task once the model has learned the i-th task.\nBaselines We compare our method with state-of-the-art continuous learning methods, including ICL [31], L2P [39], DualPrompt [38], CODA-P [36], LAE [10], and InFLORA [21]. Aligning with [21, 38], we include two naive methods: joint and sequential. The joint method, which involves learning all tasks simultaneously, is generally regarded as the upper bound of continuous learning performance. The sequential method, which uses a backbone network to learn tasks continuously, is considered the lower bound of performance.\nArchitecture and Training Details We follow the existing work settings [21] for our experiment. Specifically, we use the ViT-B/16 model pre-trained on ImageNet 1K. All methods adhere to the original paper's settings. We adopt an online continuous learning setting, where each task trains for only one epoch. The batch size is set to 10 for all exper-"}, {"title": "4.2. Experimental Results", "content": "Comparison on Standard Sequential Tasks. Table 2 presents the results of comparative experiments with SRCIS configured as follows: restructuring period $\\epsilon = 3$, temperature scaling hyperparameter $\\kappa = 1.2$, CA-OAD threshold $\\gamma = 1.28$, and EMA parameters $\\beta_c$ and $\\beta_{std}$ both set to 0.01. Outcomes are documented for SR-CIS with (w/) and without (w/o) memory restructuring (incremental addition of LoRA for new tasks). SR-CIS shows remarkable performance, surpassing other baselines. On ImageNet-R, it exceeds the second-best baseline, LAE, by 11.82%. Additionally, it achieves a single-task accuracy on the final task that is 21.33% higher than the second best method, ICL. Furthermore, SR-CIS with memory restructuring significantly outperforms its counterpart without memory restructuring in both final average accuracy and final current task accuracy, highlighting the efficacy of memory restructuring in enhancing the model's memory stability and plasticity for new tasks.\nComparison on Few-Shot Sequential Tasks. To validate our method's performance in low-data resources scenarios, we extend to a Few-Shot setting and conduct comparative experiments. Specifically, we limit the training data to the first 20 images per class from each of the three datasets, adopting a 20-shot scenario while keeping all other settings unchanged. The experimental results in Table 3 demonstrate that SR-CIS maintains a clear advantage in both average precision and current-task precision. This indicates that SR-CIS retains good plasticity and memory stability even when data resources are scarce.\nValidation across varying task lengths. To examine the memory stability of SR-CIS under different task lengths, following [21, 36], we configure three distinct task splits on ImageNet-R consisting of 5, 10, and 20 tasks respectively, and conduct comparative experiments. We record the average precision at the conclusion of each task epoch and plot forgetting curves, which are depicted in Figure 3. SR-CIS significantly exhibits the highest final average accuracy across all three task splits, demonstrating a lower forgetting rate. Even under the 20 Splits setup, SR-CIS still achieves"}, {"title": "the efficacy of adaptive temperature adjustment in enhancing anomaly detection capabilities.", "content": "The Impact of Memory Restructuring Period on CIL Performance of SR-CIS. The memory restructuring epoch $\\epsilon$ dictates the quantity of parameter memories retained. We employ varied restructuring periods to explore the trade-off between SR-CIS's CIL performance and the restructuring period on extended task sequences, using a 20 Splits task configuration on ImageNet-R. The results, shown in Figure 4, reveal a marked rise in both the average accuracy and final current task accuracy of SR-CIS when the period $\\epsilon$ increases from 2 to 3. However, beyond a restructuring cycle of $\\epsilon = 3$, SR-CIS's incremental performance exhibits minimal further improvement, suggesting that $\\epsilon = 3$ suffices for effective memory restructuring. This ensures that, under the constraint of limited parameter memory, SR-CIS can continue to adapt effectively to prolonged sequences of tasks."}, {"title": "5. Conclusion", "content": "In this paper, inspired by CLS theory, we propose the SR-CIS framework with decoupled memory and reasoning to enable self-reflective memory evolution and continual learning. SR-CIS deconstructs and collaborates short-term and long-term memory, as well as fast and slow inference, by constructing decomposed CIM and CMM. Our designed CA-OAD mechanism ensures accurate detection of hard samples and efficient switching between fast and slow inference. Meanwhile, CMM deconstructs parameter memory and representation memory through LoRA and prototype weights and biases, executing parameter memory combination and periodic LTM restructuring via a scenario description pool. Balancing model plasticity and memory stability under limited storage and data resources, SR-CIS surpasses current competitive baselines in multiple standard and low-sample incremental learning experiments, providing a systematic perspective for future research on better understanding and modeling human learning mechanisms."}, {"title": "6. Case of the scenario description and replay", "content": "Figure 5 provides an example to understand how our scenario description and scenario replay work: by asking LLAVA to describe the input image to generate a scenario description, and then in the subsequent scenario replay, SDXL comprehends the description and generates a replay image for memory restructuring."}, {"title": "7. More Experimental Details", "content": "For all method, we resize all images to 256 \u00d7 256, then use CenterCrop for all images to 224 \u00d7 224. We take 1.28, the 90% quantile of the normal distribution as the threshold, which means we retrieve difficult to classify samples with a 90% confidence level. As done in DualPrompt [38], we utilized 20% of the training data as validation data to define the hyperparameters \u03b2 = {\u03b2e, \u03b2std} and the lora rank r, ultimately determining \u03b2e = \u03b2std = 1 \u00d7 10\u22123 and r = 32. We searched for \u03b2 in value of {1e-3, 1e-2,1e-1,1}.For other methods, we have provided the hyperparameters and their settings mentioned in the original paper. We set the learning rate to le-3, and the detailed parameters are shown in the Table 4. All models are implemented using PyTorch on a single NVIDIA A800 GPU."}]}