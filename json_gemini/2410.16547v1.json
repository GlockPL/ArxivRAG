{"title": "PromptHive: Bringing Subject Matter Experts Back to the Forefront with Collaborative Prompt Engineering for Educational Content Creation", "authors": ["MOHI REZA", "IOANNIS ANASTASOPOULOS", "SHREYA BHANDARI", "ZACHARY A PARDOS"], "abstract": "Involving subject matter experts in prompt engineering can guide LLM outputs toward more helpful, accurate, and tailored content that meets the diverse needs of different domains. However, iterating towards effective prompts can be challenging without adequate interface support for systematic experimentation within specific task contexts. In this work, we introduce PromptHive, a collaborative interface for prompt authoring, designed to better connect domain knowledge with prompt engineering through features that encourage rapid iteration on prompt variations. We conducted an evaluation study with ten subject matter experts in math and validated our design through two collaborative prompt-writing sessions and a learning gain study with 358 learners. Our results elucidate the prompt iteration process and validate the tool's usability, enabling non-AI experts to craft prompts that generate content comparable to human-authored materials while reducing perceived cognitive load by half and shortening the authoring process from several months to just a few hours.", "sections": [{"title": "1 Introduction", "content": "As frontier Large Language Models (LLMs) push the boundaries of what computers can help humans create, the question of how to design authoring interfaces that effectively connect domain experts with the prompt engineering process becomes increasingly salient. With the right design [46], such interfaces could enable experts to steer the output of LLMs toward content that better aligns with the nuances and needs of their domains, and transform the role of the subject matter expert from a producer to a curator\u2014a competent and critical judge who instructs the AI agent on what is needed, evaluates the output, and iterates on the instructions until the results are satisfactory. Instead of replacing human experts, these interfaces could help bridge human intelligence with machine intelligence to dramatically reduce the time and effort required to create content that adheres to expert tastes and standards.\nTo realize the producer-to-curator shift and integrate domain expertise more closely into prompt engineering, we need authoring interfaces that: (i) deeply embed LLMs within existing expert workflows, augmenting content creation with carefully scaffolded interface support for prompt engineering; (ii) encourage experimentation on many prompt variations to systematically test the impact of changes in instructional wording on model output; (iii) offer mechanisms for curating prompt formulations that work well at various levels of abstraction; (iv) integrate generation into the publishing workflow. However, designing authoring interfaces that support experts across all four fronts is difficult as LLMs pose unique usability challenges tied to high metacognitive demands during prompt construction [45], and users can struggle to get the models to integrate well with their existing workflow as even small perturbations such as adding a space at the end of a prompt can cause the LLM to change its output [37]. For domain experts who aren't AI specialists, recent literature on prompt engineering has also highlighted how designing effective prompts can be surprisingly difficult for non-AI experts [8, 51].\nIn this work, we explore ways to enable domain expert-driven prompt engineering and answer empirical questions on how they permute and evolve prompts through the design of PromptHive (Figure 1), an open-source collaborative prompt authoring interface that we apply to the content authoring workflow of an open-source adaptive tutoring system, OATutor [34], to support domain experts with writing prompts for generating hints to homework problems. PromptHive encourages rapid experimentation and systematic testing of alternative prompt variations via a set of random-ization features for sampling problems from a spreadsheet, as well as buttons that pair prompt variations in the scratchpad with corresponding model output for easier comparison. This approach has been shown to be effective in a recent Human-AI authoring interface in a different context [36]. To support prompt curation, PromptHive features a shared library where users can save, clone, and upvote useful prompt formulations at two different levels of abstraction-the textbook-level and lesson-level. PromptHive also features a back-end logging engine that collects rich user interaction data on how prompts evolve as users create, modify and share them.\nTo validate our design, we conducted two studies: (i) a three-stage user study with ten subject matter experts who had prior experience in manually authoring hints using the expert workflow augmented by PromptHive; (ii) a learning gain study with 358 learners, comparing the efficacy of hints generated using PromptHive with hints previously authored manually by subject matter experts. Our findings provide rich empirical data on how users collaboratively iterate on prompts to generate hints for an entire college-level algebra textbook, and show that users can successfully use PromptHive to create hints that are on par with those authored manually by subject matter experts who did not make use of generative AI, while cutting mental workload by more than half"}, {"title": "2 Related Work", "content": "We review the literature in HCI and Generative AI to explicate the need for integrating subject matter expertise into prompt engineering from both ethical and practical standpoints. We also highlight the challenges associated with designing human-centered interface support for prompt engineering and examine existing systems for collaborative prompt engineering to situate and distinguish PromptHive from other tools."}, {"title": "2.1 Integrating Subject Matter Expertise into Prompt Engineering", "content": "Placing subject matter experts in the driver's seat of prompt engineering is crucial as they possess the necessary judgement to evaluate the output of LLMs in their domain [19, 38]. It is also the case that software engineering teams may not be representative of the users and their demographics [1], and therefore may be less able to produce prompts that address a variety of needs and perspectives. Domain expert-driven prompt engineering is also important because of the wide-ranging and lasting impact LLMs are having across many, many domains including art [17, 53], medicine [24, 52], law [11, 12], and education [18, 33]. Unlocking the potential of pretrained generative models hinges on aligning them with human intentions [47]. Researchers, organizations, policy makers, and society at large will need to grapple with the question of how to best involve human experts in AI-intensive workflows. Preliminary studies have shown the potential benefits of expert involvement in the application of LLMs. For example, Kumar et al. found that an instructor-tuned LLM significantly boosted student interactions with a chatbot compared to plain ChatGPT as a baseline [18], and Wang et al. extended the Mixture-of-Experts Paradigm to prompt optimization, demonstrating how breaking up a problem space into sub-regions controlled by specialized human experts can help with prompt optimization [47].\nIn this work, we contribute an open-source system that exemplifies how to effectively integrate subject matter expertise in the context of educational content creation and contribute to the ongoing discourse within the HCI and Human-Centered AI communities on retaining human control while increasing automation [42]. In contrast to Sheridan and Verplank's characterization of automation and human control as a unidimensional spectrum [40, 41], we adopt Shneiderman's two-dimensional HCAI framework [41] and explore ways to increase automation without encroaching upon subject matter experts' sense of control and trust over the content."}, {"title": "2.2 Usability Challenges of Designing Prompt Engineering Interfaces", "content": "At the same time, as noted by Tankelevitch et al., the same unique properties that make LLMs powerful, such as their flexibility across multiple input/output spaces and generality across tasks,"}, {"title": "2.3 Designing Effective Content Authoring Tools for Intelligent Tutoring Systems", "content": "Intelligent Tutoring Systems (ITS) have historically required highly time consuming authoring processes. Early iterations of authoring tools required 200-300 hours of use to produce a single hour of instructional content and required experience with programmatic interfaces [2]. Development of the Cognitive Tutor Authoring Tools (CTAT) quickly reduced this time to 50-100 hours to produce one hour of instructional content [50]. CTAT provided content authors with a GUI which allowed for content authoring with minimal knowledge of programming and coding skills. The GUI allowed for content creation to take place with the respective interface, which greatly supported independent content creation processes [2].\nFollowing the example of CTAT, many future ITS and ITS-like systems supported their content creation ecosystems with sets of builder tools that were focused on providing content authors an easy-to-use interface, usually in the form of a GUI. One example of such tools is that of the ASSISTments builder. While not fully an ITS, ASSISTments is an adaptive tutor that places the instructor in an important role within its system [14]. Supporting such a design philosophy, the ASSISTments Builder was created to simplify the content authoring process for teachers and instructors [35]. To accomplish this, the builder provided easy integration of problem-help features such as hints and scaffolds, while also allowing skill mapping of knowledge components. Furthermore, it managed to match the lower end of CTAT's 50 hour estimate of development for one hour's worth of instructional content despite using a GUI instead of a programmatic interface [35]. The ASSISTments Builder also supported problem variabilization directly through its GUI, allowing for easier variation of problem content [35].\nWhile GUIs have provided many benefits for content authoring, there are still many challenges that accompany them. With the large number of tutoring systems available, it can be difficult for teachers to have to learn a new content authoring environment every time they wish to utilize a new tutor [50]. Furthermore, creating problems with more complicated structures may not just be difficult, but in some GUIs may not even be possible [35]. This has resulted in difficulties for new systems to balance between efficient and easy content creation while also supporting more complex content creation. When compared to the GUI of the ASSISTments builder, the programmatic spreadsheet-based authoring interface used in OATutor did not result in any significant differences"}, {"title": "2.4 Generative Al in Tutoring Systems", "content": "Recent years have seen widespread implementation of generative AI tools into digital technologies. LLM improvements have had a significant influence on tutoring systems and educational contexts. Evaluations of ChatGPT's decimal skills indicated that it can respond accurately to conceptual questions but struggles more with respect to number lines and decimal point problems [27]. Furthermore, when examining student answers, ChatGPT accurately assessed the correctness of seventy-five percent of them, while generating feedback that was similar to that of instructors. ChatGPT's ability to produce informative worked solutions proved effective for learning in Algebra and Statistics subjects [33]. In relation to higher education, across 53 studies, with 114 question sets totaling to over 49,000 multiple choice questions (MCQs), ChatGPT 4.0 answered 75.5% of all MCQs on said problem-sets, receiving a passing score on the majority of them [26]. It has also been shown to effectively evaluate the quality of learnersourced MCQ distractors [25].\nLLMs have seen usage with respect to tutor question quality evaluation. Generative Students is a prompt architecture that utilizes LLMs to simulate student profiles for the purpose of simulating believable MCQ answers [23]. Generative Students demonstrated a high correlation between how real students responded to the sample question and how the simulated students did. Furthermore, the system demonstrated that an instructor could improve their question quality using these simulated students. A similar approach was also used to learn question difficulty parameters using LLM-Respondents, thereby allowing for appropriate quality tutoring questions to be selected [22]. Beyond simulated students, LLM tutoring use also has taken the form of chatbots. An ASSISTments integrated chatbot utilizing GPT 4.0, while not providing statistically significant learning gains, increased the positivity of students' attitudes, even though they displayed a lower confidence of solving a similar problem after the chatbot help intervention [10].\nLLM-generated questions themselves have also been evaluated. When compared against questions from a published Creative Commons textbook, there were no statistically significant differences between the difficulty of algebra textbook questions and similar ones generated by ChatGPT [6]. These results indicate that ChatGPT is capable of producing algebra problems of similar quality to those from textbooks."}, {"title": "3 Designing PromptHive", "content": "In this section, we describe the expert workflow that PromptHive is designed to support, the core design requirements, the development process, and key interface elements."}, {"title": "3.1 Understanding the Expert Workflow", "content": "To understand how generative AI could support educational content creation, we explore the details of the existing expert workflow within OATutor. Open Adaptive Tutor (OATutor) was selected for its Creative Commons content library, rapid experimentation capabilities, structured json-based content format, and documentation of the expert-authoring of its content [3]. Tutoring systems based on ITS principles have proven effective for learning [30], as has OATutor in the subjects of Algebra and Statistics [33]. The lead author conducted a series of in-depth conversations with the head of OATutor's content team, who oversees the training of subject matter experts (SMEs) in"}, {"title": "3.2 Eliciting Design Requirements", "content": "To effectively integrate generative AI into this expert workflow, we surveyed frameworks for Human-AI collaboration and prompt-authoring interfaces for LLMs to derive an overarching design requirement (R0) and five supporting design requirements (R1-5) for PromptHive:\n\u2022 Control (R0): Ensure SMEs retain control while leveraging automation. Drawing on the two-dimensional Human-Centered AI (HCAI) framework by Shneiderman [41], we aimed to automate the content generation process as much as possible without diminishing SMEs\u2019 sense of control. To achieve this, we provided interface tools that allowed SMEs to easily guide content generation and oversee quality, while automation reduced the effort required to produce educational materials.\n\u2022 Integration (R1): Seamlessly integrate AI support within the existing expert workflow. The system needs to read and write content in the same format used by SMEs and preserve human oversight over the AI-generated content. Integration with current processes is crucial to ensure smooth adoption without disrupting established workflows.\n\u2022 Simplicity (R2): Given the usability challenges associated with human-centered gener-ative AI, particularly the high metacognitive demands of prompt engineering [45], a key requirement is to minimize cognitive load by scaffolding the prompt-writing process. The system should enable SMEs to focus on content quality \u2013 such as instructional clarity and hint structure \u2013 rather than dealing with technical complexities like API management or formatting rules.\n\u2022 Trust (R3): Building trust in AI is a central challenge identified in the explainable AI (XAI) literature [4]. Therefore, a core requirement is to foster trust in the system by providing SMEs with tools to evaluate and validate AI-generated content thoroughly, ensuring alignment with their educational goals and quality standards.\n\u2022 Iteration (R4): Given the inherent unpredictability of LLM outputs, experimentation has been recognized as a critical requirement for effective prompt-authoring [16, 36]. PromptHive must support rapid iteration on multiple prompt variations, enabling SMEs to compare outputs and refine prompts efficiently to achieve optimal results.\n\u2022 Collaboration (R5): Finally, given that the existing expert workflow involved multiple content team members, in alignment with the social paradigm of prompt design [49], we seek to support collaborative prompt engineering to write and refine effective formulations together."}, {"title": "3.3 Brainstorming & Design Review Sessions", "content": "To transform these design requirements into concrete interface support for subject matter experts in PromptHive, the lead author held weekly brainstorming and design review sessions with the content team lead over a span of six months and engaged in rapid prototyping, transitioning from low-fidelity sketches to the fully-functional open-source system that this paper contributes. The design process involved three key stages: (i) Paper-Prototyping: Low-fidelity sketches explored ways for SMEs to integrate human-authored content with generative AI and rapidly iterate on hints; (ii) Cognitive Walkthroughs: Informal sessions were held to refine the paper prototypes and ensure the designs were intuitive; (iii) Web-Based Prototyping: High-fidelity prototypes were built based on the refined designs. There was significant overlap and back-and-forth iteration throughout these stages, with feedback from walkthroughs informing subsequent changes."}, {"title": "3.4 Developing the PromptHive Interface", "content": "This iterative process led to the development of an interface that supports a 4-stage prompt-authoring workflow at two levels of abstraction, as described in Figure 3:\n(1) Load: To fulfill R1 and enable seamless integration between human-authored and AI-generated content, we implemented a loading mechanism in PromptHive that allows SMEs to import content from structured data sources, such as an OATutor spreadsheet, by pasting a link.\n(2) Author: Users can author hint prompts and view its output on a sampling of problems. To address R2 and R3, and ensure SMEs can thoroughly test prompts against a variety of problems and lessons even when the content pool is large, we introduced randomization buttons. These allow users to systematically sample problems for more comprehensive testing.\n(3) Share: To fulfill R5 and facilitate collaborative iteration among teams of SMEs, we developed the Shared Prompt Library. This feature allows SMEs to curate effective prompt formulations, enabling others to clone, test, and upvote prompts that perform well during evaluation.\n(4) Iterate: To fulfill R4 and support rapid iteration on prompt variations, the \"Prompt Scratch Pad\" interface enables SMEs to edit and experiment with multiple prompts in parallel. Users can clone prompts to test variations, compare generated outputs side by side, and quickly evaluate how changes impact the content by clicking on buttons that pair each prompt with its corresponding output.\nThese processes are more cyclical than linear, meaning that sampling, evaluation, and sharing can occur at any stage of the prompt crafting process, allowing for frequent back-and-forth iterations between content team members. Prompt authoring happens at two levels of abstraction:\n(1) Textbook Level: Users create prompts that generate hints applicable across an entire textbook, ensuring broad coverage.\n(2) Lesson Level: Prompts are then refined to address specific lessons, ensuring greater speci-ficity and alignment with particular learning objectives."}, {"title": "4 Study 1: User Evaluation with Subject Matter Experts", "content": "To evaluate how subject matter experts use PromptHive to collaboratively author prompts, we designed a three-part user study involving pre-interviews, collaborative prompt-writing sessions, and post-interviews with experts in math."}, {"title": "4.1 Research Questions", "content": "We sought to answer the following research questions through this study:\n\u2022 RQ1: What do SMEs perceive as the most time-consuming tasks in manual content authoring, and where can AI offer the greatest assistance through PromptHive?\n\u2022 RQ2: How do subject-matter experts with prior experience in manual content authoring perceive the trustworthiness and usability of PromptHive?\n\u2022 RQ3: Do SMEs feel they can retain control over the hint generation process when using PromptHive?\n\u2022 RQ4: How does the subjective cognitive workload of SMEs using PromptHive compare to the manual content generation workflow?\n\u2022 RQ5: How do prompts evolve as subject-matter experts refine them through individual iterations and collaboration with other experts?"}, {"title": "4.2 Participants", "content": "We recruited 10 subject-matter experts (6 women, 4 men) in mathematics tutoring, aged 18-24, who were screened for having prior experience in manually authoring content for OATutor. These participants were either 3rd- or 4th-year undergraduate students or recent graduates, with back-grounds in math-intensive fields such as Data Science, Computer Science, Applied Mathematics, Economics, and Electrical Engineering. All participants except P9 reported having experience tutoring math and other subjects beyond their content authoring work for OATutor. However, P9 had extensive experience authoring content for OATutor, contributing around 500 problems to the platform. Each subject-matter expert was compensated with $150 USD for participating in the study. Table 1 summarizes their backgrounds and expertise. Participants had some prior exposure to LLMs, primarily through ChatGPT, but were not advanced users with knowledge of APIs or other technical aspects of prompt engineering."}, {"title": "4.3 Procedure & Tasks", "content": "In addition to completing a short demographics survey, participants took part in three study sessions, all done only via Zoom video-conferencing:\n\u2022 30-minute Pre-Interviews: These sessions were completed individually and began with participants sharing their prior experience with manually authoring content for OATutor. They were then asked to reflect on this experience and complete the NASA-TLX instrument for measuring perceived workload. Following this, participants were given access to an early version of the PromptHive interface and asked to complete a series of steps following the 4-stage workflow outlined in Section 3.4. During this think-aloud session, the researcher noted any usability issues that emerged, which were then addressed before the collaborative prompt-writing sessions described next.\n\u2022 1.5-hour Collaborative Prompt Writing: These were conducted in groups, with par-ticipants choosing between two available time slots. This two-session approach allowed us to simulate both synchronous and asynchronous collaborative scenarios, as prompts written during the first session were made asynchronously available to participants in the second session. The sessions began with participants receiving an overview of how the PromptHive system works, along with a document outlining the necessary instructions and basic prompt-writing strategies adapted from OpenAI's guide to prompt engineering [28]. We excluded technical details and focused on two core strategies: testing changes systematically and writing clear instructions. Participants first worked individually to create their best textbook-level prompts. Next, they were randomly and evenly assigned contiguous\n\u2022 30-minute Post-Interviews: After participants completed the collaborative prompt writing sessions, they scheduled individual post-interviews to share their experience with using PromptHive. They reflected on their experience for generating hints using PromptHive and completed the NASA-TLX [13] instrument for the automated workflow. They also completed the System-Usability Scale (SUS) [20] and the XAI Trust scale adapted from Hoffman et al. [15] questionnaire to share how usable the system felt and whether they trusted using it for the purpose of generating educational content. See Appendix A for the wording of the 8 items in the trust scale. Finally, they participated in a brief semi-structured user interview to unpack their experience with PromptHive."}, {"title": "4.4 Materials", "content": "We used publicly available Creative Commons Algebra content from Open Adaptive Tutor (OATutor) [31], an open-source adaptive tutoring system based on ITS-principles [34]. This content pool contains materials from the OpenStax College Algebra 2e textbook [29]; we contacted the OATutor"}, {"title": "4.5 Analysis", "content": "Our data consisted of interview transcripts and NASA-TLX ratings from the pre- and post-interviews, SUS and XAI Trust ratings from the post-interviews, the textbook-level and lesson-level problems that participants wrote during the collaborative sessions, and the detailed prompt iteration JSON logs captured by the logging engine. We analyzed the qualitative interview data using refexive thematic analysis [9] through an inductive-deductive lens, using the rich theory on Human-AI collaboration and recent literature on prompt-engineering interface design as a pre-existing code that guided our interpretations. For the NASA-TLX scores and SUS ratings, we used the standard procedures for calculating scores."}, {"title": "5 Study 1 Results", "content": "The overall response to PromptHive was largely positive, with participants rating the system as highly usable, achieving an SUS score of 89/100. There was a significant reduction in NASA-TLX ratings for perceived cognitive workload, dropping from 55.17 to 26.73 out of 100 compared to the manual hint authoring workflow (see Figure 4). Regarding trust in the AI system, most participants strongly or somewhat agreed that the system felt trustworthy, without causing any wariness, based on the validated measures from the XAI trust scale. See Figure 5 for the distribution of responses and Appendix A for the wording of the scale items adapted from [15]."}, {"title": "5.1 Unpacking the Al-assisted Hint Authoring Experience in PromptHive", "content": "\u201cHonestly, I'm just amazed by how [PromptHive] works. I've spent the last six months making hints, so seeing this in action and knowing that this is what other people will use is pretty incredible. I'm excited to see where it goes.\u201d \u2013 P10\nDuring the pre- and post-interviews, participants shared their experience with the PromptHive system and commented on how it compares with the manual workflow and their prior experiences with AI. We grouped the findings from their experiences into six themes (F1-6) described below.\nF1: Hint Authoring is Most Time-Consuming and Can Benefit from AI Assistance\nTo answer RQ1, we asked participants during the pre-interviews which parts of the manual hint authoring workflow they found most time-consuming. We posed this question before introducing participants to the PromptHive system to avoid biasing their responses. Multiple participants (P1, P3, P5, P7, P9) pointed to authoring hints (especially those with scaffolded answers), as the most time consuming, in alignment with our design decision to target that aspect of the content authoring process using PromptHive. For example, P5 shared that, \u201cthe most time-consuming part is how to scaffold each step to make it clear and short without including too many words,\u201d while P3 mentioned, \u201ceach problem has only one statement, but for the hints, there could be 10 or more.\u201d Regarding why scaffolded hint generation was time-consuming, P9 discussed the decision-making process involved in determining the appropriate level of detail for hints, such as whether to explain high-level concepts or break down smaller calculations. They questioned, \u201cIf there's a problem that uses the mass-times-density equation, do I need to just say, \u2018Use a density equation and do the calculation with these inputs, or go further down to something like, \u2018What's three times five?\u201d\u201d P10 shared additional considerations when authoring hints, noting that it takes time to \u201c...chain the hints and ensure they flow together.\u201d\nIn contrast, post-interviews revealed a clear consensus among participants that PromptHive-significantly reduced the time spent creating hints compared to the manual workflow. Participants noted that they could generate multiple hints and scaffolds in the time it previously took to create just one manually. This efficiency was particularly important when handling large volumes of problems. For instance, P8 remarked, \u201cI was able to create a prompt to generate hints and scaffolds for 15 to 20 questions, whereas before, it would take me at least five, sometimes ten minutes to do just one.", "super fast, super intuitive,": "nd noted, \u201cAnyone who's done it manually and then tried the AI system would probably agree that it's much more fun and intuitive... much more enjoyable than just typing it all out.", "nF2": "PromptHive Integrates Al into Expert Workflow More Closely Compared to Chat-GPT\nComparing PromptHive to using ChatGPT, P1 mentioned \u201cI also tried, like, ChatGPT, but it always outputs the wrong answer. At least when I was working with this system, the answers were all reliable. I'm curious \u2013 how could you generate the result with such a high percentage of correct answers compared to GPT?\u201d. Commenting on the tighter integration with the existing expert workflow, P4 noted that PromptHive is \u201ca lot faster because you don't have to type the question out into ChatGPT, and then copy, paste everything onto the spreadsheet. And the formatting for the spreadsheet was also like, difficult to do, because in it you had to write six or seven rows for each question. So I think the system is a lot easier to use. I would recommend over manually putting things into ChatGPT.\u201d\nF3: Participants felt that the AI-assisted workflow in PromptHive led to more consistent hints\nSeveral participants (P1, P7, P8 and P9) felt that the AI hints were more consistent than the human-only hints because of the variability in style or structure of the hints across different"}, {"title": "F4: Participants felt that they mostly retained control over Hint Generation", "content": "Many participants (P2, 6, 8, 9, 10) reported that they strongly felt they could steer the output of the LLM and apply their prior subject matter expertise in authoring prompts and evaluating the model output. For example, P6 was able to make hints simpler or complex by altering the student age: \u201cI tried asking the AI to explain to a 7-year-old, and the output was simple. When I changed it to a 10-year-old, it became more complex.\u201d P6 removed redundancy by instructing the LLM to not repeat hints and limit the number of scaffolded hints, like P9. P8 mentioned that they could eventually get the AI to follow instructions after iterating on the prompt a few times: \u201ceven if there are times where they are acting a little bit, you know, a little bit weird, like with capitalization issues or the tone is not right, I can just instantly update it. And then, usually, after two or three tries, I can get like, a version that I think would be even better than something that I would write myself.\"\nOther participants felt less strongly about their ability to control the AI output, e.g., P4 mentioned that while the AI model felt \u201csteerable to an extent\u201d, it could sometimes be \u201cstubborn\" and difficult to steer for general textbook level prompts but that it felt easier to steer prompts at the lesson level. P5 felt that sometimes they struggled to get the AI model to adhere to multiple opposing instructions, especially when the prompts were complex, e.g., in trying to simplify the hints for a younger audience, the model would inadvertently give responses that felt too long.\nF5: Subject Matter Experts did not feel replaced by the AI workflow\nReflecting on the ongoing discussions surrounding how generative AI will affect education, P8 said \u201cI think when I hear AI and LLMs and education right now, I think of a talk that I just attended that was just very extremely pro-LLM. And I think we do need to be a little careful about how we inject LLMs into education. But I think having this idea where the LLM isn't creating the questions, the questions are already there. That is good to me. But then also, beyond that, like your job is to generate prompts...at the end of the day, having a prompt engineer who still goes through the process of needing to understand how to guide a student, I think that that still can be very beneficial,\""}, {"title": "F6: Collaborative authoring can help Subject Matter Experts be more creative with prompts", "content": "The collaborative aspect of the AI assisted workflow in PromptHive was well-received by many participants, who felt that sharing ideas led to a richer variety of more creative prompts. P10 said \u201cseeing other people's prompts gave me ideas that I didn't even think to ask for\u201d and P8 felt \u201cthe collaborative aspect was wonderful. Seeing how others structured their prompts helped me brainstorm and improve my own.\u201d Comments from P5 indicated that exposure to prompts from others helped them think beyond their own experience as a middle school teacher \u2013 \u201cI remember seeing prompts asked for hints to have a positive tone, stuff like that. I think it inspired me to think about that because currently I'm a middle school teacher, so I only think about making things short so people will look at it. When I saw another prompt which said you are a tutor in college, you are a professor, that made me think about the main population of students this model is targeting.\""}, {"title": "5.2 Exploring How Subject Matter Experts Iterate on Prompts", "content": "The PromptHive system logging engine captured data on user interactions whenever subject matter experts executed a prompt or saved it to the shared library. \nFrom this log, we found that participants did between 1 (P2) and 17 (P3 and P8) iterations based on executions to arrive at their final textbook-level prompt, with most taking between 3 to 6 iterations. Below is an example of the textbook-level chain from P5, with the removals and additions across each iteration highlighted in red and blue, respectively:\n\u2022 Iteration 1: You have 20 years of experience in teaching high school math and middle school math and specialized in helping special education students understand the math contents. Currently, you are working with a group of ELD students who have ADHD and severe learning disability to understand math. Make sure the hints that you give are concise which means less than 10 words for each step. They are also easy to understand, encouraging, and interesting for students to follow. Those students are also historically known to be marginalized.\n\u2022 Iteration 2: You have 20 years of experience in teaching high school math and middle school math and specialized in helping special education students understand the math contents. Currently, you are working with a group of ELD students who have ADHD and severe learning disability to understand math. Make sure the hints are enthusiastic, easy to understand, encouraging, and interesting for students to follow. Make sure the hints\nthat you give are concise which means less than 10 words for each step. They are also historically known to be marginalized.\n\u2022 Iteration 3: You have 20 years of experience in teaching high school math and middle school math and specialized in helping special education students understand the math contents. Currently, you are working with a group of ELD students who have ADHD and severe learning disability to understand math. Make sure the hints are enthusiastic, easy to understand, encouraging, and interesting for students to follow. No change from Iteration 2.\n\u2022 Iteration 4: You have 20 years of experience in teaching high school math and middle school math and specialized in helping special education students understand the math contents. Currently, you are working with a group of ELD students who have ADHD and severe learning disability to understand math. Make sure the hints are enthusiastic, easy to understand, encouraging, and interesting for students to follow. No change from Iteration 3.\n\u2022 Iteration 5: You have 20 years of experience in teaching high school math and middle school math and specialized in helping special education students understand the math contents. Currently, you are working with a group of special education students. Make sure the hints are enthusiastic, easy to understand, encouraging, and interesting for students to follow. ELD students who have ADHD and severe learning disability to understand math.\n\u2022 Final Prompt: You have 20 years of experience in teaching high school math and middle school math and specialized in helping special education students understand the math contents. Currently, you are working with a group of special education students. You need to add some emojis to each hint to make it interesting for students to follow. Make sure the hints are enthusiastic, easy to understand, encouraging, and interesting for students to follow.\nHere, we see how P5 starts off by experimenting with writing hints for students with learning disabilities, even though the focus of the textbook was for college-level math, likely drawing from their experience in teaching diverse students as a 7th grade math teacher. In iteration two, they remove the last two sentences on making the hints concise and being mindful of the marginalization of such students. Given how LLMs can generate different outputs for executions of the same prompt, we see there was no change to the content from Iteration 3 to 4. In 5, they mention special education twice, likely to emphasize who they want these hints to target. Finally, they add a note about including emojis to make the hints more engaging. The JSON log captures similar chains for both textbook-level and lesson-level prompts across all participants, which we cannot detail here due to space considerations but will attach as supplementary material as it offers empirical insights on"}, {"title": "6 Study 2: Learning Gain Study with College-Level Math Learners", "content": "In this study, we explore the impact of PromptHive hints on learning gains compared to a human-only authored hints control condition."}, {"title": "6.1 Research Questions", "content": "\u2022 RQ5: Do PromptHive-generated hints lead to learning gains, and how do those gains compare to those from human-only hints?"}, {"title": "6.2 Participants", "content": "Through Prolific, we recruited a total of 358 current undergraduate college students, with 266 unique participant submissions for the human-only authored hints control condition and 263 for the PromptHive experiment condition. Since participants completed a sequence of 3 lessons, we excluded any lesson submissions where participants did not fully complete all parts of the lesson sequence (i.e. 3-question pre-test, 5-question hint condition, 3-question post-test). After this exclusion, we resulted in 225 unique participants, with a total of 549 completed lesson submissions (268 for human-only and 28"}]}