{"title": "Towards Robust Knowledge Tracing Models via k-Sparse Attention", "authors": ["Shuyan Huang", "Zitao Liu", "Xiangyu Zhao", "Weiqi Luo", "Jian Weng"], "abstract": "Knowledge tracing (KT) is the problem of predicting students' future performance based on their historical interaction sequences. With the advanced capability of capturing contextual long-term dependency, attention mechanism becomes one of the essential components in many deep learning based KT (DLKT) models. In spite of the impressive performance achieved by these attentional DLKT models, many of them are often vulnerable to run the risk of overfitting, especially on small-scale educational datasets. Therefore, in this paper, we propose SPARSEKT, a simple yet effective framework to improve the robustness and generalization of the attention based DLKT approaches. Specifically, we incorporate a k-selection module to only pick items with the highest attention scores. We propose two sparsification heuristics : (1) soft-thresholding sparse attention and (2) top-K sparse attention. We show that our SPARSEKT is able to help attentional KT models get rid of irrelevant student interactions and have comparable predictive performance when compared to 11 state-of-the-art KT models on three publicly available real-world educational datasets. To encourage reproducible research, we make our data and code publicly available at https://github.com/pykt-team/pykt-toolkit\u00b9.", "sections": [{"title": "1 INTRODUCTION", "content": "The process of knowledge tracing involves utilizing a student's past learning interactions to construct a model to estimate his/her knowledge mastery to predict his/her future performance over a period of time (as shown in Figure 1). Such predictive abilities have the potential to improve students' learning outcomes and accelerate their progress when combined with high-quality learning materials and instructions. Recently, with the remarkable capability of attention mechanisms in natural language processing (NLP) or computer vision (CV) tasks, many DLKT models achieve accurate students' knowledge state estimations by utilizing attention networks to capture the intrinsic relevance among past interactions [6, 10, 15].\nAlthough the attentional DLKT approaches have achieved impressive results, they may run the risk of overfitting in real-world educational scenarios. Educational data is usually limited compared to large-scale language or image data. In the KT datasets, the question bank is usually bigger than the set of knowledge components (KCs) and a student has a very small number of question responses. Furthermore, since questions may be associated with limited relevant KCs, not all the past question responses contribute equally to the KT prediction task [1]. For instance, during predicting the student's performance for q6 in Figure 1, the KT models need easily look back to important historical information of the student's response in q\u2081 due to both of them are associated by c3. Besides the question q1, other questions likely have limited correlations to 96. However, due to the smooth characteristic of"}, {"title": "2 PRELIMINARY", "content": "2.1 Self Attentive Knowledge Tracing\nThe self-attentive knowledge tracing (SAKT) model is the first method to use attention mechanisms in the context of KT [15]. The standard encoder in the Transformer model is employed in the basic setup of SAKT to extract context-aware interaction information through historical query and key-value pairs for the KT scenario [19]. The definitions of the query and key-value pairs are as follows:\n$h_{t+1} = SelfAttention(Q, K, V)$\n$Q = Z_{t+1}; K = {y_1,\u2026, y_t}; V = {y_1,\u2026, y_t}$\n$y_t = Z_t \\oplus r_t;  Z_t = W^\\pounds  e_q;  r_t = W_q  e^q$\n(1)\nwhere ht+1 is the learned context-aware latent representation that summarizes all available information for prediction at the (t + 1)th timestamp. z\u0142 and yt denote the latent representations of the related KCs and interaction at timestamp t. rt denotes the representation of student response. e is the n-dimensional one-hot vector indicating the corresponding KC and eq is the 2-dimensional one-hot vector indicating whether the question is answered correctly. z\u0142 and rt are d-dimensional learnable real-valued vectors. Wc \u2208 Rdxn and Wq \u2208 Rd\u00d72 are learnable linear transformation operations. \u2295 is the element-wise addition operator and is the standard matrix/vector multiplication. n is the total number of KCs.\n2.2 Related Work\n2.2.1 Attention based Knowledge Tracing. Attention based KT models utilize attention mechanisms to capture the intrinsic dependencies among students' chronologically ordered interactions. SAKT is the first research work that adopted a self-attention network to predict students' future performance [15]. Since then, many KT methods use attention networks to capture the potential relations between questions and responses [5, 6, 16]. Choi et al. applied deep self-attentive layers in a pure Transformer architecture to extract the question and response representations [5]. Ghosh et al. proposed a monotonic attention mechanism that computes attention weights with exponential time-related decay [6].\n2.2.2 Sparse Attention. Sparse attention improves the ordinary attention mechanism by only computing a limited selection of similarity scores from a sequence rather than all possible pairs [4, 13, 17, 24], which has shown promising performance in NLP and CV domains [13, 24]. For example, Martins and Astudillo proposed a new activation function called sparsemax that is able to output sparse probabilities rather than traditional softmax [13]. Child et al. introduced several sparse factorizations of the attention matrix without sacrificing performance [4]. Zhao et al. designed an explicit sparse Transformer by selecting k most relevant components [24]."}, {"title": "3 THE SPARSEKT APPROACH", "content": "We propose that to improve the performance of attention-based knowledge tracing models, it is necessary to further enhance the generalization of the model. One way to accomplish this is by incorporating recent advancements in attention sparsification techniques. Therefore, in this paper, we propose SPARSEKT, a simple yet effective framework to facilitate the robustness of the attention based KT approach. Briefly, our SPARSEKT approach incorporates an additional k-sparse selection module after the standard self-attention function to only select the top K interactions with highest attention scores. Only the selected K interactions are used to make future predictions. The idea of SPARSEKT is illustrated in Figure 2.\n3.1 Embedding\nInspired by the classic and simple Rasch model in psychometrics that explicitly uses a scalar to characterize the latent factor"}, {"title": "3.2 k-Sparse Attention", "content": "of question discrimination, we improve the SAKT's interaction representation (shown in eq.(1)) by utilizing a question-specific discrimination factor to capture the individual differences among questions on the same KC. Specifically, let xt be the enhanced representations that contain question-centric information, i.e.,\n$x_t = m^q  v^q  Z_t$\nwhere mq denotes the question-specific discrimination factor of question qt and vq represents the variation of qt covering this KC set c. Both qt and vq are d-dimensional learnable real-valued vectors. is the element-wise product operator.\n3.2 k-Sparse Attention\nIn this section, we leverage sparsification techniques to enhance the generalization of KT models for better performance. In our work, historical interactions that have limited correlations to the current question will not be assigned to the attention scores. More specifically, we enhance the SAKT's scaled dot-product attention mechanism by using sparse attention to allow the model to focus on only a few pieces of historical information through explicit selection. Let I be the attention distribution computed from our question enhanced representations xt+1, i.e.,\n$I = softmax(\\frac{Q \\cdot K^T}{\\sqrt{d}}); Q = x_{t+1}; K = {x_1,\u2026, x_t}$\nLet M(\u00b7) be the mask operation that selects the k most informative historical interactions, I\u00a1 denotes the attention score of xi. There are two implementations of M() including soft-thresholding and top-K sparse attention, described as follows:\n\u2022 soft-thresholding sparse attention: we order all the attention scores Ii of attention distribution I from largest to smallest. And gradually pick up I\u00a1 into a weighting set N = NU {1;} until the cumulative sum of Iis in N is larger than the predefined soft-threshold k. Hence, we treat all the historical interactions with the attention scores Ii as the most contributive ones to predict a student's future performance. Other interactions are likely to be irrelevant to the prediction which will not be assigned attention scores by:\n$M_{soft}(I_i) = \\begin{cases} I_i & \\text{if } I_i \\in N \\\\ -\\infty & \\text{otherwise} \\end{cases}$\n\u2022 top-K sparse attention: let s be the k-th largest value in I. We select the top k largest scores of in I as the most influential components. Practically, if I\u00a1 is larger than s, we will select I\u00a1 and vice versa, i.e.,\n$M_{topK}(I_i) = \\begin{cases} I_i & \\text{if } I_i \\geq s \\\\ -\\infty & \\text{otherwise} \\end{cases}$\nWe then re-normalized the attention score distribution I, and the normalized scores of I\u012f in negative infinity are approximately 0. Therefore, we get a sparse attention distribution that explicitly chooses the highest attention scores that may influence the KT model decision. Finally, we obtain the knowledge state representation of qt+1 as:\n$h_{t+1} = softmax(M(I))V; V = {y_1,\u2026, y_t}$"}, {"title": "3.3 Prediction Layer", "content": "We use a two-layer fully connected network to refine the knowledge state and the overall optimization function is as follows:\n$\\widehat{r}_{t+1} = \\sigma(w^T \\cdot ReLU(W_2 \\cdot ReLU(W_1 \\cdot [h_{t+1}; x_{t+1}] + b_1) + b_2)+b)$\n$L= -\\sum(r_{t+1} log \\widehat{r}_{t+1}+(1-r_{t+1}) log(1- \\widehat{r}_{t+1}))$\nwhere W1, W2, w, b1, b2 and b are trainable parameters and W\u2081 \u20ac Rd\u00d72d, W2 \u2208 Rdxd, w, b1, b2 \u2208 Rd\u00d71, b is scalar. \u03c3(\u00b7) is the sigmoid function."}, {"title": "4 EXPERIMENTS", "content": "We use three publicly educational datasets to evaluate the effectiveness of our model. We remove student sequences with fewer than 3 attempts and set the maximum length to 200 following the data preprocessing by [11]. The datasets are described as follows:\n\u2022 ASSISTments2015 (AS2015): the dataset comprises mathematical exercises from the ASSISTments platform during the 2015 academic year. It ends up with 682,789 interactions, 19,292 sequences and 100 KCs after pre-processing.\n\u2022 NeurIPS2020 Education Challenge (NIPS34): the dataset is provided by NeurlPS 2020 Education Challenge. We use the dataset of Task 3 & Task 4 to evaluate our models [21]. There are 1,399,470 interactions, 9,401 sequences, 948 questions, 57 KCs.\n\u2022 Peking Online Judge (POJ): the dataset contains programming exercises on the Peking coding platform and is scraped by [16]. It has 987,593 interactions, 20,114 sequences and 2,748 questions.\nWe compare the two instances of our k-sparse self-attention framework, i.e., SPARSEKT-soft and SPARSEKT-topK to the following 11 KT models to evaluate the effectiveness of our approach:\n\u2022 DKT [18]: uses an LSTM layer to encode the students' knowledge state for predicting their response performances.\n\u2022 DKT+ [22]: a variation of DKT that tackles the problems of reconstruction and non-consistent prediction.\n\u2022 KQN [8]: calculates the relevance of student knowledge state encoder and skill encoder via the dot product.\n\u2022 DKVMN [23]: exploits two memory networks to extract the relations between different KCs and students' knowledge states.\n\u2022 ATKT [7]: exploits adversarial perturbations to the interaction embeddings to enhance the models' generalization capability.\n\u2022 GKT [14]: utilizes graph neural networks to model the relation between KCs to predict the student's future performance on KCs.\n\u2022 SAKT [15]: leverages a self-attention mechanism to capture relevance between exercises and responses.\n\u2022 SAINT [5]: uses Transformer architecture to represent students' exercise and response sequences via encoder and decoder.\n\u2022 AKT [6]: introduces monotonic attention to enhance self-attention by considering the students' forgetting behavior.\n\u2022 HawkesKT [20]: utilizes the Hawkes process to model temporal cross-effects in student historical interactions."}, {"title": "4.1 Results", "content": "4.1.1 Overall Performance. Table 1 shows the overall performance of all models. From Table 1, we have the following observations: (1) compared to other baseline methods, our two SPARSEKT models almost always ranks top 3 in terms of AUC scores and accuracy on NIPS34 and POJ datasets. Although our SPARSEKT approaches are worse than several baselines such as DKT, DKT+, AKT on AS2015, the performance gaps are quite minimal and are mostly within a 0.7% range. This indicates the strength of SPARSEKT as a baseline of KT; (2) in spite of our two SPARSEKT approaches are extensions of SAKT only by using sparse attention to replace scaled dot-product attention, they all have remarkable improvements of AUC scores by 0.97%, 4.79% and 1.38% on three datasets on average compared to SAKT. This indicates our sparse attention mechanism allows KT models to pay attention to limited influential historical information that improves the predictive performance; and (3) comparing SPARSEKT-topK and SPARSEKT-soft, we can see, SPARSEKT-soft performs slightly better than SPARSEKT-topK. We believe that the student performance on questions still depends on several numbers of past interactions and the top-K version of sparse attention may bring too limited information to hard to estimate a student's future performance compared to SPARSEKT-soft.\n4.1.2 Impact of the Sparsity Level. We further explore the impacts of the sparse selection k on the model performance. We conduct experiments on the AS2015 to evaluate our two sparse attention approaches with different values of k on the validation set. The results are illustrated in Figure 3. We limit the range of k to [0.1, 1] and [1, 10] in soft-threshold and top-K sparse attention respectively. We observe that neither soft-threshold nor top-K sparse attention, the AUC scores are quite low when the values of k are small, e.g., k=0.1/1 in soft-threshold and top-K sparse attention respectively. We suppose that too limited historical interactions are selected and that the KT model can not obtain enough past learning information to predict students' future performance and hence get low AUC scores. With the increasing values of k, SPARSEKT-topK obtains better performance gradually and performs best AUC scores when k = 7. After that, larger values of k may contain more noise that decreases the model's robustness and limits its generalization yield to get a decreasing AUC score.\n4.1.3 Visualization of KC Relations via k-Sparse Attention. Figure 4 shows the KC relation visualization via our proposed k-sparse attention. We compute the cumulative sum of attention weights among all the KCs during the training of our SPARSEKT-soft. To better observe the relations, we compute the min-max normalization of the cumulative sum results. Due to the space limitation, we visualize the results between the top-6 KCs with the highest frequency on NIPS34. We can see that, since pre-post sequence relations among KCs, the attention weights are different in the same KC pairs. For example, for a KC pair <C2,C3>, C2 has a high influence (0.95 weight) on c3 when c2 is the pre-interaction of c3. On the contrary, c3 has a relatively small impact (0.4 weight) on c2 when c3 is the pre-interaction of c2. Furthermore, there is a limited correlation to <C2, Cc8>, so the attention weights between them are less than 0.2 regardless of which KC is the pre-interaction."}, {"title": "5 CONCLUSION", "content": "In this paper, we propose SPARSEKT which enhances the classical scaled dot-product attention by extracting influential historical interactions to estimate students' mastery of knowledge. Extensive experimental results on three publicly educational datasets show the effectiveness and comparable prediction outcomes and robustness of SPARSEKT. In the future, we would like to explore more sparse attention approaches like dynamic k selection or self-adaptive select k attention weights without the hyperparameter tuning."}, {"title": "ACKNOWLEDGMENTS", "content": "This work was supported in part by National Key R&D Program of China, under Grant No. 2020AAA0104500; in part by Beijing Nova Program (Z201100006820068) from Beijing Municipal Science & Technology Commission; in part by NFSC under Grant No. 61877029; in part by Key Laboratory of Smart Education of Guangdong Higher Education Institutes, Jinan University (2022LSY-S003) and in part by National Joint Engineering Research Center of Network Security Detection and Protection Technology."}]}