{"title": "Segment, Lift and Fit: Automatic 3D Shape Labeling from 2D Prompts", "authors": ["Jianhao Li", "Tianyu Sun", "Zhongdao Wang", "Enze Xie", "Bailan Feng", "Hongbo Zhang", "Ze Yuan", "Ke Xu", "Jiaheng Liu", "Ping Luo"], "abstract": "This paper proposes an algorithm for automatically labeling 3D objects from 2D point or box prompts, especially focusing on applications in autonomous driving. Unlike previous arts, our auto-labeler predicts 3D shapes instead of bounding boxes and does not require training on a specific dataset. We propose a Segment, Lift, and Fit (SLF) paradigm to achieve this goal. Firstly, we segment high-quality instance masks from the prompts using the Segment Anything Model (SAM) and transform the remaining problem into predicting 3D shapes from given 2D masks. Due to the ill-posed nature of this problem, it presents a significant challenge as multiple 3D shapes can project into an identical mask. To tackle this issue, we then lift 2D masks to 3D forms and employ gradient descent to adjust their poses and shapes until the projections fit the masks and the surfaces conform to surrounding LiDAR points. Notably, since we do not train on a specific dataset, the SLF auto-labeler does not overfit to biased annotation patterns in the training set as other methods do. Thus, the generalization ability across different datasets improves. Experimental results on the KITTI dataset demonstrate that the SLF auto-labeler produces high-quality bounding box annotations, achieving an AP@0.5 IoU of nearly 90%. Detectors trained with the generated pseudo-labels perform nearly as well as those trained with actual ground-truth annotations. Furthermore, the SLF auto-labeler shows promising results in detailed shape predictions, providing a potential alternative for the occupancy annotation of dynamic objects.", "sections": [{"title": "1 Introduction", "content": "In modern robotics and autonomous driving, a significant amount of labeled data is required for supervised learning to understand the 3D scene [8,16,17,19,20], emphasizing dynamic objects such as vehicles and pedestrians. Therefore, there is a pressing need to improve the degree of automation for 3D labeling. The preferred labeling format is typically the 3D bounding box, which can be represented compactly with as few as 7-9 degrees of freedom. However, manually labeling a large number of 3D boxes remains a laborious and costly task, posing challenges for scaling up 3D object detectors as shown in Fig. 1(a).\nMeanwhile, as 3D perception models evolve, there is an increasing demand for more precise labeling granularity. While bounding boxes offer a compact representation, they are not optimal for 3D objects and are being replaced by finer representations. For instance, voxel occupancy [10,38,41] has emerged as a dominant format for 3D scenes in autonomous driving, as it unifies the representation of static road elements and dynamic objects. However, finer-grained labels further complicate the annotation pipeline, exacerbating the annotation efficiency problem and presenting a significant obstacle to scaling up 3D models.\nIn this paper, we propose a novel approach for 3D auto-labeling. The proposed auto-labeler follows a Segment, Lift, and Fit (SLF) paradigm as shown in Fig. 1(b). Specifically, SLF utilizes the 2D points and bounding boxes as prompts and outputs the estimated 3D shapes and poses of target objects, thus supporting semi- or fully automatic labeling depending on whether the prompts come from humans or machines. To accomplish this, we leverage the Segment Anything Model (SAM) [12], a foundational vision model, to generate high-quality instance masks from the input prompts. Subsequently, we lift 2D instance masks into 3D forms with Signed Distance Functions (SDFs) [26], and then iteratively optimize the shapes and poses until their projections fit the mask and its surface conforms to the surrounding LiDAR points. Notably, the proposed SLF auto-labeler does not require supervised training on a specific dataset, and is therefore not susceptible to overfitting biased annotation patterns in the training dataset.\nExperimental results show that SLF can generate high-quality 3D labels. As our method outputs detailed shapes of objects, while existing benchmarks [1,7] only provide bounding box annotations, for a fair comparison with existing auto-labelers, we convert shapes to boxes and assess the label quality regarding box accuracy. When compared among unsupervised auto-labelers [23,39,44] that do"}, {"title": "2 Related Works", "content": "3D Auto-labelers. Compared to supervised 3D object detectors [14,18,42,43], auto-labelers require fewer or no training labels. Supervised auto-labelers learn to annotate from limited samples with ground-truth 3D labels, often in the hundreds. For instance, WS3D [24] generates cylindrical object proposals on bird's eye view (BEV) maps using center clicks in BEV and fine-tunes a 3D detector pre-trained on a few human-annotated data. MTrans [15] employs a multi-task design of segmentation, point generation, and box regression to train a 3D detector that takes ground truth 2D boxes and LiDAR points as input. Unsupervised auto-labelers do not require 3D labels for training. CC [35] uses PointNet [30] to segment and regress 3D boxes from frustum points. SDFLabel [44] trains a DeepSDF [27] network supervised with image patch and CAD models of cars on a synthetic dataset, then uses RANSAC [6] to fit the predicted shape to frustum sub-points. VS3D [31] employs an unsupervised UPM module to generate 3D proposals and filter them using a pre-trained image model. Our SLF, is also an unsupervised approach that does not require training with 3D labels. SLF achieves finer annotations with weaker prompts compared with a closely related unsupervised and non-learning approach, FGR [39]. FGR requires amodal boxes as inputs, in which oracle occlusion information has already been considered. LPCG [28] finds that concerning label accuracy, the 3D location part in the label is preferred compared to other parts of labels in monocular 3D object detection and proposed training-free and high-cost label pipeline, which is similar to FGR and MTrans respectively. In contrast, SLF takes normal 2D"}, {"title": "3 Approach", "content": "As shown in Fig. 2(a), we aim to recover the 3D shape and pose of an interested object by asking a labeler to click one or more points or draw a box on the image as the prompt for the target object. A labeler could be a human in semi-automatic labeling and a machine algorithm such as a 2D object detector in case of full-automatic labeling. Fig. 2(a) shows a brief illustration. We propose a Segment, Lift, and Fit (SLF) paradigm to address this problem. We first segment a 2D mask of the target object using the input prompts, then lift it into a 3D model and iteratively optimize its shape and pose until it fits the 2D mask and surrounding LiDAR points. An overall illustration is shown in Fig. 2(b).\n3.1 Segment: From 2D Prompts to Instance Masks\nThe problem of auto-labeling 3D shapes from 2D prompts is a significant issue in autonomous driving because it would be highly beneficial to effectively utilize the already established robust 2D instance segmentation models for this purpose. This work employs the strong Segment Anything Model (SAM) [12] to transfer"}, {"title": "3.2 Optimization Objective", "content": "We employ a pose vector p = (x, y, z, \\theta) \\in \\mathbb{R}^4 and a latent shape vector s \\in \\mathbb{R}^d to represent a 3D object, where (x, y, z) denote the position of the object center in the global coordinate system,  \\theta denotes the heading angle around the z axis, and the latent dimension d is empirically set to 5. We assume all instances are on the ground (the x-y plane) and omit rotations around the x and y axes. Our goal is to minimize an energy function that corresponds to the posterior probability of the 3D reconstruction given p and s as follows:\np^*, s^* = \\arg \\min_{p,s} E(Y, X_f, p, s), \\qquad (1)\nwhere Y \\in [0,1]^{h\\times w} is the 2D instance mask predicted by SAM, and X_f \\in \\mathbb{R}^{N\\times 3} is a subset of the whole scene point cloud X. X_f is comprised of N three-dimensional coordinates of points inside the camera frustum corresponding to the 2D mask. To solve the optimization problem in Eq. (1), we build a differentiable renderer to optimize the energy function E via gradient descent."}, {"title": "3.3 Lift: Differentiable Rendering", "content": "Representation of 3D objects. We use the Signed Distance Function (SDF) [22,37] to represent 3D objects. Formally, an SDF is a three-dimensional scalar field \\phi(x) in which the value of a particular spatial coordinate x is given by its distance to the closest object surface, with positive and negative values meaning exterior and interior area. In practice, we use the discrete approximation of continuous SDF. Consider a cubic divided into a 3D mesh grid with the size of l \u00d7 w \u00d7 h. We can represent the inscribed object with the SDF value in each mesh grid, i.e., m = [f(x_1), ..., f(x_i)..., f(x_n)]^T, where n = l \u00d7 w \u00d7 h.\nShape prior. We aim to condense the nuanced differences present in class-specific 3D shapes into a low-dimensional latent space, where traversing along the manifold of the latent vector s corresponds to the exploration of feasible shapes of the object. We focus on the \"car\" class in this paper, where every instance in the class possesses an overall shape prior and exhibits minor variations in characteristics, such as the shape of the front face or the shape of the cabin.\nTo obtain the shape prior, we first assemble a collection of 3D vehicle models from the Apollo-Car3D dataset [33], which consists of 79 models with exhibit significant variations in shapes and sizes (see the left of Fig. 3). However, using the Signed Distance Function (SDF) representation necessitates that objects be water-tight to avoid ambiguity between their interior and exterior. Some models contain intricate internal structures and are not initially water-tight. To obtain the water-tight surface of 3D models, we employ the SoftRas [21] algorithm to remove redundant vertices and faces inside the model. Finally, we convert the original triangular mesh representation of each 3D model into an SDF representation to obtain a set of models \\mathcal{M} = \\{m_i\\}_{i=1}^{79}.\nGiven the collection of representative 3D models, we perform the PCA method on \\mathcal{M}, and keep the top-d (d = 5) principle components to obtain a projection matrix V \\in \\mathbb{R}^{d\\times n}. Accordingly, we can use the projection matrix V to render a 3D shape m from a given shape vector s. Besides, we can encode a 3D model m into a shape vector s by s = V^T(m - \\bar{m}) and m = Vs + \\bar{m}, where  \\bar{m} indicates the mean vector of the collection \\mathcal{M}. In this work, we select a compact representation d = 5. The right of Fig. 3 illustrates that interpolation along the latent space of s leads to smooth variation in shapes.\nDifferentiable rendering. After recovering the 3D shape in a local coordinate system, we can put the model in the global coordinate system by adding its"}, {"title": "3.4 Fit: Optimization over Pose and Shape", "content": "Given the 2D instance mask and the point cloud scene, we randomly initialize pose p and shape s and optimize them using Eq. (1). Specifically, the optimization objective consists of three terms: the mask alignment term, the point cloud alignment term, and the ground alignment term.\nMask alignment term. The mask alignment term measures the dissimilarity between the target instance segmentation mask Y and the mask  Y_{proj} obtained by projecting the 3D model onto the image plane. To make the projection process differentiable, we follow [22,37] to define the projection as \\pi(p_j) = 1- \\prod_{x \\in R_{p_j}} e^{-\\phi(x)+1}, where  p_j is the j-th pixel in the image plane and \\pi(p_j) \\in (0,1) is the mask value. A ray is cast through  p_j from the ego camera, and a set of points R_{p_j} is collected by sampling along the ray. If the ray does not hit the surface of the 3D model, the SDF values of all sampled points x \\in R_{p_j} will be negative, resulting in a mask value close to zero. Conversely, if the ray intersects with the 3D model, there must exist sampled points with  \\phi(x) > 0, leading to a mask value close to 1. A hyperparameter \\zeta controls the \"hardness\" of the mask values, where a larger \\zeta leads to harder values (close to either 0 or 1). Finally, we define the mask alignment term using the dice loss [34] as follows:\nE_{mask} = 1 - \\frac{2|(Y_{proj} *O) \\cap Y|}{|Y_{proj} * O|+|Y|}, \\qquad (2)\nwhere O is a binary map indicating if the pixel is occluded by a nearer object. As shown in Fig. 4, to obtain the occlusion map, 3D LiDAR points are associated with each 2D mask based on projections, and the depth of each mask is computed as the median depth value of the associated LiDAR points. Masks are sorted by their depth values and then used to compute the occlusion maps.\nPoint cloud alignment term. Solely optimizing the mask alignment term leads to inaccurate estimation due to the missing depth information. To avoid undesired local optimum, we introduce an additional point cloud alignment term. This term measures how well the 3D object's surface aligns with the point cloud inside the corresponding frustum."}, {"title": "4 Experiments", "content": "Previous auto-labelers can be divided into supervised ones and unsupervised ones. The proposed SLF is an unsupervised method. Therefore, we primarily compare it with existing unsupervised methods. Nonetheless, we also compare it with supervised methods and demonstrate that SLF exhibits superior transfer ability across datasets. In addition to bounding box accuracy, we also evaluate the quality of the predicted 3D shape.\n4.1 Comparison with Unsupervised Auto-labelers\nDataset and metric. We conduct experiments on the commonly used KITTI [7] dataset. We follow the official training and validation split to get 3712 training"}, {"title": "4.4 Ablation Study", "content": "In this section, we conduct comprehensive ablation studies on the KITTI validation set and report the AP3D (IOU=0.5) | R40 results. Except in Tab. 6, we input 2D points to SLF, other results are obtained by using 2D bounding boxes as prompt. Effect of energy terms. In Tab. 4, we observe that introducing the mask alignment term significantly improves pseudo-label quality rather than solely applying the point cloud alignment term, and adding the ground alignment term further improves performance. However, as we mentioned above, using mask alignment alone can easily be trapped in sub-optimal solutions, thus it can not obtain valid results (Easy: 6.9, Mod.: 5.9, Hard: 5.6).\nEffect of the number of CAD models. In Tab. 5, we randomly sample 5 and 30 CAD models out of the total 79 samples to investigate the influence of the number of CAD models used for PCA. From the results, we can conclude that better result comes with more models.\nEffect of the PCA dimensions. In Tab. 7, we show how different PCA dimensions affect results. Performance improves when we increase the dimension from 3 to 5. More dimensions like 10 do not further improve. This is possibly attributed to that a higher dimension of shape representation may complicate the optimization process, which means it may need more steps to converge.\nEffect of the number of points. In Tab. 6, we analyze the effect of the number of points input to SAM [12]. When we increase the points from 3 to 8, the improvement of pseudo-label quality is minor. However, we observe a significant decrease in the easy cases of KITTI when using one point as the prompt, which is reasonable as easy cases often have large masks and a single point can not provide sufficient context for SAM to generate high-quality results.\n4.5 Further Analysis\nRobustness of LiDAR Beam. Point clouds in KITTI [7] are collected by 64 beams LiDAR, we downsample point cloud to 32, 16 and 8 beams following [40]. AP3D results on KITTI validation set in Tab. 8 show SLF can still get decent results on 32 and 16 beams LiDAR, and the performances downgrade rapidly on Mod. and Hard cases, since these cases usually far away from ego vehicle which exacerbating the sparsity of point clouds."}, {"title": "5 Limitation", "content": "The proposed SLF uses a strong shape prior learned by PCA from given 3D CAD models. The shape prior is category-specific, which means we need to prepare a collection of 3D models when labeling a specific semantic class."}, {"title": "6 Conclusion", "content": "This paper proposes a Segment, Lift, and Fit paradigm for automatic 3D shape annotation of objects from the 2D point or box prompts. We first segment high-quality instance masks from the prompts using the SAM, and then lift 2D masks to 3D forms, where gradient descent is used to adjust their poses and shapes until the projections fits the masks and the surfaces conform to surrounding LiDAR points. Extensive experimental results on multiple datasets for 3D detection demonstrate the effectiveness of our SLF method. Moreover, the SLF auto-labeler achieves promising results in detailed shape predictions, which can be used for occupancy annotation. In the future, we will investigate the generalization abilities by applying our SLF to more categories."}]}