{"title": "Segment, Lift and Fit: Automatic 3D Shape Labeling from 2D Prompts", "authors": ["Jianhao Li", "Tianyu Sun", "Zhongdao Wang", "Enze Xie", "Bailan Feng", "Hongbo Zhang", "Ze Yuan", "Ke Xu", "Jiaheng Liu", "Ping Luo"], "abstract": "This paper proposes an algorithm for automatically labeling 3D objects from 2D point or box prompts, especially focusing on applications in autonomous driving. Unlike previous arts, our auto-labeler predicts 3D shapes instead of bounding boxes and does not require training on a specific dataset. We propose a Segment, Lift, and Fit (SLF) paradigm to achieve this goal. Firstly, we segment high-quality instance masks from the prompts using the Segment Anything Model (SAM) and transform the remaining problem into predicting 3D shapes from given 2D masks. Due to the ill-posed nature of this problem, it presents a significant challenge as multiple 3D shapes can project into an identical mask. To tackle this issue, we then lift 2D masks to 3D forms and employ gradient descent to adjust their poses and shapes until the projections fit the masks and the surfaces conform to surrounding LiDAR points. Notably, since we do not train on a specific dataset, the SLF auto-labeler does not overfit to biased annotation patterns in the training set as other methods do. Thus, the generalization ability across different datasets improves. Experimental results on the KITTI dataset demonstrate that the SLF auto-labeler produces high-quality bounding box annotations, achieving an AP@0.5 IoU of nearly 90%. Detectors trained with the generated pseudo-labels perform nearly as well as those trained with actual ground-truth annotations. Furthermore, the SLF auto-labeler shows promising results in detailed shape predictions, providing a potential alternative for the occupancy annotation of dynamic objects.", "sections": [{"title": "1 Introduction", "content": "In modern robotics and autonomous driving, a significant amount of labeled data is required for supervised learning to understand the 3D scene [8,16,17,19,20], emphasizing dynamic objects such as vehicles and pedestrians. Therefore, there is a pressing need to improve the degree of automation for 3D labeling. The preferred labeling format is typically the 3D bounding box, which can be represented compactly with as few as 7-9 degrees of freedom. However, manually labeling a large number of 3D boxes remains a laborious and costly task, posing challenges for scaling up 3D object detectors as shown in Fig. 1(a).\nMeanwhile, as 3D perception models evolve, there is an increasing demand for more precise labeling granularity. While bounding boxes offer a compact representation, they are not optimal for 3D objects and are being replaced by finer representations. For instance, voxel occupancy [10,38,41] has emerged as a dominant format for 3D scenes in autonomous driving, as it unifies the representation of static road elements and dynamic objects. However, finer-grained labels further complicate the annotation pipeline, exacerbating the annotation efficiency problem and presenting a significant obstacle to scaling up 3D models.\nIn this paper, we propose a novel approach for 3D auto-labeling. The proposed auto-labeler follows a Segment, Lift, and Fit (SLF) paradigm as shown in Fig. 1(b). Specifically, SLF utilizes the 2D points and bounding boxes as prompts and outputs the estimated 3D shapes and poses of target objects, thus supporting semi- or fully automatic labeling depending on whether the prompts come from humans or machines. To accomplish this, we leverage the Segment Anything Model (SAM) [12], a foundational vision model, to generate high-quality instance masks from the input prompts. Subsequently, we lift 2D instance masks into 3D forms with Signed Distance Functions (SDFs) [26], and then iteratively optimize the shapes and poses until their projections fit the mask and its surface conforms to the surrounding LiDAR points. Notably, the proposed SLF auto-labeler does not require supervised training on a specific dataset, and is therefore not susceptible to overfitting biased annotation patterns in the training dataset.\nExperimental results show that SLF can generate high-quality 3D labels. As our method outputs detailed shapes of objects, while existing benchmarks [1,7] only provide bounding box annotations, for a fair comparison with existing auto-labelers, we convert shapes to boxes and assess the label quality regarding box accuracy. When compared among unsupervised auto-labelers [23,39,44] that do"}, {"title": "2 Related Works", "content": "3D Auto-labelers. Compared to supervised 3D object detectors [14,18,42,43], auto-labelers require fewer or no training labels. Supervised auto-labelers learn to annotate from limited samples with ground-truth 3D labels, often in the hundreds. For instance, WS3D [24] generates cylindrical object proposals on bird's eye view (BEV) maps using center clicks in BEV and fine-tunes a 3D detector pre-trained on a few human-annotated data. MTrans [15] employs a multi-task design of segmentation, point generation, and box regression to train a 3D detector that takes ground truth 2D boxes and LiDAR points as input. Unsupervised auto-labelers do not require 3D labels for training. CC [35] uses PointNet [30] to segment and regress 3D boxes from frustum points. SDFLabel [44] trains a DeepSDF [27] network supervised with image patch and CAD models of cars on a synthetic dataset, then uses RANSAC [6] to fit the predicted shape to frustum sub-points. VS3D [31] employs an unsupervised UPM module to generate 3D proposals and filter them using a pre-trained image model. Our SLF, is also an unsupervised approach that does not require training with 3D labels. SLF achieves finer annotations with weaker prompts compared with a closely related unsupervised and non-learning approach, FGR [39]. FGR requires amodal boxes as inputs, in which oracle occlusion information has already been considered. LPCG [28] finds that concerning label accuracy, the 3D location part in the label is preferred compared to other parts of labels in monocular 3D object detection and proposed training-free and high-cost label pipeline, which is similar to FGR and MTrans respectively. In contrast, SLF takes normal 2D"}, {"title": "3 Approach", "content": "As shown in Fig. 2(a), we aim to recover the 3D shape and pose of an interested object by asking a labeler to click one or more points or draw a box on the image as the prompt for the target object. A labeler could be a human in semi-automatic labeling and a machine algorithm such as a 2D object detector in case of full-automatic labeling. Fig. 2(a) shows a brief illustration. We propose a Segment, Lift, and Fit (SLF) paradigm to address this problem. We first segment a 2D mask of the target object using the input prompts, then lift it into a 3D model and iteratively optimize its shape and pose until it fits the 2D mask and surrounding LiDAR points. An overall illustration is shown in Fig. 2(b).\n3.1 Segment: From 2D Prompts to Instance Masks\nThe problem of auto-labeling 3D shapes from 2D prompts is a significant issue in autonomous driving because it would be highly beneficial to effectively utilize the already established robust 2D instance segmentation models for this purpose. This work employs the strong Segment Anything Model (SAM) [12] to transfer"}, {"title": "3.2 Optimization Objective", "content": "We employ a pose vector \\(p = (x, y, z, \\theta) \\in \\mathbb{R}^4\\) and a latent shape vector \\(s \\in \\mathbb{R}^d\\) to represent a 3D object, where \\((x, y, z)\\) denote the position of the object center in the global coordinate system, \\(\\theta\\) denotes the heading angle around the z axis, and the latent dimension d is empirically set to 5. We assume all instances are on the ground (the x-y plane) and omit rotations around the x and y axes. Our goal is to minimize an energy function that corresponds to the posterior probability of the 3D reconstruction given p and s as follows:\n\\[p^*, s^* = \\arg \\min_{p,s} E(Y, X_f, p, s),\\]\nwhere \\(Y \\in [0,1]^{h \\times w}\\) is the 2D instance mask predicted by SAM, and \\(X_f \\in \\mathbb{R}^{N \\times 3}\\) is a subset of the whole scene point cloud X. \\(X_f\\) is comprised of N three-dimensional coordinates of points inside the camera frustum corresponding to the 2D mask. To solve the optimization problem in Eq. (1), we build a differentiable renderer to optimize the energy function E via gradient descent."}, {"title": "3.3 Lift: Differentiable Rendering", "content": "Representation of 3D objects. We use the Signed Distance Function (SDF) [22,37] to represent 3D objects. Formally, an SDF is a three-dimensional scalar field"}, {"title": "3.4 Fit: Optimization over Pose and Shape", "content": "Given the 2D instance mask and the point cloud scene, we randomly initialize pose p and shape s and optimize them using Eq. (1). Specifically, the optimization objective consists of three terms: the mask alignment term, the point cloud alignment term, and the ground alignment term.\nMask alignment term. The mask alignment term measures the dissimilarity between the target instance segmentation mask Y and the mask \\(Y_{proj}\\) obtained by projecting the 3D model onto the image plane. To make the projection process differentiable, we follow [22,37] to define the projection as \\(\\pi(p_j) = 1-\\prod_{x\\in R_{p_j}} \\frac{e^{-\\phi(x)}}{e^{-\\phi(x)}+1}\\), where \\(p_j\\) is the j-th pixel in the image plane and \\(\\pi(p_j) \\in (0,1)\\) is the mask value. A ray is cast through \\(p_j\\) from the ego camera, and a set of points \\(R_{p_j}\\) is collected by sampling along the ray. If the ray does not hit the surface of the 3D model, the SDF values of all sampled points \\(x \\in R_{p_j}\\) will be negative, resulting in a mask value close to zero. Conversely, if the ray intersects with the 3D model, there must exist sampled points with \\(\\phi(x) > 0\\), leading to a mask value close to 1. A hyperparameter \\(\\zeta\\) controls the \"hardness\" of the mask values, where a larger \\(\\zeta\\) leads to harder values (close to either 0 or 1). Finally, we define the mask alignment term using the dice loss [34] as follows:\n\\[E_{mask} = 1-\\frac{2|(Y_{proj} * O) \\cap Y|}{|Y_{proj} * O|+|Y|},\\]\nwhere O is a binary map indicating if the pixel is occluded by a nearer object. As shown in Fig. 4, to obtain the occlusion map, 3D LiDAR points are associated with each 2D mask based on projections, and the depth of each mask is computed as the median depth value of the associated LiDAR points. Masks are sorted by their depth values and then used to compute the occlusion maps.\nPoint cloud alignment term. Solely optimizing the mask alignment term leads to inaccurate estimation due to the missing depth information. To avoid undesired local optimum, we introduce an additional point cloud alignment term. This term measures how well the 3D object's surface aligns with the point cloud inside the corresponding frustum."}, {"title": "4 Experiments", "content": "Previous auto-labelers can be divided into supervised ones and unsupervised ones. The proposed SLF is an unsupervised method. Therefore, we primarily compare it with existing unsupervised methods. Nonetheless, we also compare it with supervised methods and demonstrate that SLF exhibits superior transfer ability across datasets. In addition to bounding box accuracy, we also evaluate the quality of the predicted 3D shape.\n4.1 Comparison with Unsupervised Auto-labelers\nDataset and metric. We conduct experiments on the commonly used KITTI [7] dataset. We follow the official training and validation split to get 3712 training"}, {"title": "4.2 Cross Dataset Generalization", "content": "Apart from KITTI, we conduct experiments on more challenging nuScenes [1]. We transform the validation set of nuScenes to KITTI format, resulting in 6019 frames with corresponding front view images, point cloud and 3D bounding box annotations. On KITTI, we compare the pseudo label with ground truths and report \\(AP^{3D}\\), and on nuScenes, we report the official metrics including MAP, NDS, MATE, MASE and mAOE. We compare with the best-performing unsupervised auto-labeler FGR, and the supervised auto-labeler MTrans [15]. Both FGR and Mtrans require 2D amodal boxes as prompts, so we project 3d ground truth boxes into 2D as their input. Our SLF supports a more practical setting that only requires regular 2D modal boxes as prompts. Thus we can generate them with an off-the-shelf detector HTC [2] pre-trained on nuImages [1]."}, {"title": "4.3 Voxel Occupancy Annotation", "content": "Since benchmarks [1,7] (i.e., KITTI and nuScences)do not support quantitative evaluation on 3D shapes, we show qualitative results in Fig. 5 to better show the effect of SLF on shape labeling. Specifically, we replace the estimated shape with a mean shape and show comparisons. A clear improvement can be observed by comparing the initial shape and the final optimized shape, both in terms of mask alignment and LiDAR point alignment. Besides, we observe that SLF can serve as a good auto-labeler for occupancy annotation of dynamic objects such as vehicles. We compare two versions of occupancy labels for nuScenes, OpenOccupancy [38] and OFN [25]. Considering that existing occupancy labels are still of low quality, quantitative evaluation may be misleading. Therefore we show qualitative visualizations in Fig. 6, and we observe that OpenOccupancy and OFN labels fail to depict the inner occupancy of the cars. This is mainly attributed to that they label the occupancy by voxelizing the LiDAR points, and therefore self-occlusion can not be avoided. In contrast, SLF can predict dense occupancy more accurately, with more detailed shapes."}, {"title": "4.4 Ablation Study", "content": "In this section, we conduct comprehensive ablation studies on the KITTI validation set and report the AP3D (IOU=0.5) | R40 results. Except in Tab. 6, we input 2D points to SLF, other results are obtained by using 2D bounding boxes as prompt. Effect of energy terms. In Tab. 4, we observe that introducing the mask alignment term significantly improves pseudo-label quality rather than solely applying the point cloud alignment term, and adding the ground alignment term further improves performance. However, as we mentioned above, using mask alignment alone can easily be trapped in sub-optimal solutions, thus it can not obtain valid results (Easy: 6.9, Mod.: 5.9, Hard: 5.6)."}, {"title": "4.5 Further Analysis", "content": "Robustness of LiDAR Beam. Point clouds in KITTI [7] are collected by 64 beams LiDAR, we downsample point cloud to 32, 16 and 8 beams following [40]. AP3D results on KITTI validation set in Tab. 8 show SLF can still get decent results on 32 and 16 beams LiDAR, and the performances downgrade rapidly on Mod. and Hard cases, since these cases usually far away from ego vehicle which exacerbating the sparsity of point clouds."}, {"title": "5 Limitation", "content": "The proposed SLF uses a strong shape prior learned by PCA from given 3D CAD models. The shape prior is category-specific, which means we need to prepare a collection of 3D models when labeling a specific semantic class."}, {"title": "6 Conclusion", "content": "This paper proposes a Segment, Lift, and Fit paradigm for automatic 3D shape annotation of objects from the 2D point or box prompts. We first segment high-quality instance masks from the prompts using the SAM, and then lift 2D masks to 3D forms, where gradient descent is used to adjust their poses and shapes until the projections fits the masks and the surfaces conform to surrounding LiDAR points. Extensive experimental results on multiple datasets for 3D detection demonstrate the effectiveness of our SLF method. Moreover, the SLF auto-labeler achieves promising results in detailed shape predictions, which can be used for occupancy annotation. In the future, we will investigate the generalization abilities by applying our SLF to more categories."}]}