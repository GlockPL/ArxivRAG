{"title": "Adaptive Class Learning to Screen Diabetic Disorders in Fundus Images of Eye*", "authors": ["Shramana Dey", "Pallabi Dutta", "Riddhasree Bhattacharyya", "Surochita Pall", "Sushmita Mitra", "Rajiv Raman"], "abstract": "The prevalence of ocular illnesses is growing globally, pre- senting a substantial public health challenge. Early detection and timely intervention are crucial for averting visual impairment and enhancing pa- tient prognosis. This research introduces a new framework called Class Extension with Limited Data (CELD) to train a classifier to categorize retinal fundus images. The classifier is initially trained to identify rele- vant features concerning Healthy and Diabetic Retinopathy (DR) classes and later fine-tuned to adapt to the task of classifying the input images into three classes, viz. Healthy, DR and Glaucoma. This strategy allows the model to gradually enhance its classification capabilities, which is beneficial in situations where there are only a limited number of la- beled datasets available. Perturbation methods are also used to identify the input image characteristics responsible for influencing the model's decision-making process. We achieve an overall accuracy of 91% on pub- licly available datasets.", "sections": [{"title": "1 Introduction", "content": "With the rapid growth in the global population, the number of individuals di- agnosed with diabetes is increasing at an alarming rate. Diabetes, a metabolic disorder characterized by high blood sugar levels, often leads to various visual impairments. Diabetic individuals must undergo regular eye screening due to the strong correlation between diabetes and eye abnormalities. A significant challenge lies in the shortage of trained eye care professionals, which hampers effective screening and treatment [1]. Diabetes is a precursor for several vision-threatening diseases, notably diabetic retinopathy (DR) and Glaucoma [3]. Ap- proximately one-third of diabetics are likely to develop DR, and those with"}, {"title": "2 Methodology", "content": "A significant challenge in the task of retinal image classification is the limited availability of annotated data, which constrains the generalizability of the model. Specifically, there is a disproportionate ratio of healthy and DR data, with Glau- coma data being even scarcer. This imbalance complicates the task of improving classification accuracy. While data augmentation might intuitively address this issue, it risks overfitting, resulting in an inefficient model [6]. To effectively man- age this data imbalance, the proposed CELD framework helps to classify fundus images as healthy, DR-affected, or Glaucoma-affected. Additionally, we evalu- ate the model's decision-making process using explainability methods based on perturbation techniques. A schematic workflow is provided in Fig. 1.\nThe objective of this study is to develop a detection system for retinal color fundus images of three classes: healthy, DR, and Glaucoma. The subsequent parts of this section provide a detailed explanation of the classifier architecture, the proposed CELD framework, and the perturbation-based explainable methods used to gain insights regarding the decision-making process of the model."}, {"title": "2.1 Class Extension with Limited Data (CELD)", "content": "A deep learning model tends to experience \"catastrophic forgetting\", losing pre- viously learned patterns when trained on new data distributions [19]. In contrast,\nhumans have an inherent ability to learn new skill over the time without for- getting prior knowledge. In this work, the proposed CELD framework exploits this notion of natural learning ability by retaining the knowledge acquired from previously learned classes to enable the network to adapt to new class. This re- duces the requirement for extensive datasets for each incoming new class. This makes it highly suitable for real-world scenarios characterized by limited data availability, such as the classification of retinal fundus images, where obtaining extensive labeled datasets is often a significant challenge. Employing models pre- trained on the ImageNet dataset and subsequently fine-tuning them for adapting to new tasks in the medical domain may lead to suboptimal performance due to the inherent differences in data distribution between natural images and medi- cal images [8]. CELD framework mitigates the issue of performance degradation caused by domain shift since the datasets for source and target tasks belong to the same domain. In this work the source task is to train the classifier for cate- gorizing healthy and DR images and the target task is defined by adapting the classifier from the source task to categorize the input fundus images into DR, Glaucoma and healthy category.\nFormally, $T_S$ and $T_T$ represent the datasets for source and target task re- spectively, and $t_s \\subset T_T \\subset \\tau$, where $\\tau$ represents the universal domain of retinal fundus images. A classifier $C_s$, parameterized by a set of parameters $w_s$, is ini- tially trained on source data $(x_i, y_i) \\in T_S$. Here $x_i \\in X_S$ represent the input images and the corresponding labels $y_i \\in Y_S$ where $Y_S = \\{Healthy, DR\\}$.\n$C_s: X_S \\to Y_S$ (1)\n$\\underset{w_S^*}{\\text{arg min}}\\ \\sum_{i=1}^{M}\\mathcal{L}(\\hat{y_i}= C_s(x_i;w_s), y_i)$ (2)\nHere $\\mathcal{L}$ is the loss function to be minimized and $M = |T_S|$ during training the classifier $C_s$. The optimized weights from the trained classifier $C_s$, $w_s^*$, are then used to initialize a new classifier $C_T$ which classifies $(x_k, y_k) \\in T_T$ where $X_k \\in X_T$ represent the input images and the corresponding labels $y_k \\in Y_T$ where $Y_T = \\{Healthy, DR, Glaucoma\\}$ with $N = |T_T|$. Subsequently, $C_T$ is fine-tuned on the extended dataset.\n$C_T: X_T \\to Y_T$ (3)\n$w_T^* = \\underset{w_T}{\\text{arg min}}\\ \\sum_{i=1}^{N}\\mathcal{L}(\\hat{y_k}= C_T(x_k;w_T), y_k)$ (4)\nHere, $w_T^*$ is the updated weight of $C_T$ after training on $T_T$. The loss function during the incremental learning phase can be defined as:\n$\\mathcal{L}(T_T; w_T) = \\mathbb{E}_{(x, y) \\sim T_T} [-\\sum_{c=1}^{C} y_c \\log \\hat{y}_c]$ (5)\nwhere $(x, y) \\sim T_T$ indicates that the input data $x$ and the corresponding label $y$ is drawn from the expanded dataset $T_T$. $y_c$ is the true label for class $c$ and $\\hat{y}_c$ is the predicted label. $C = |Y_T|$ represents the total number of classes in $T_T$. The expectation $\\mathbb{E}$ denotes averaging over all samples in the dataset. This approach allows $C_T(.)$ to retain patterns learned from $t_s$ while learning features relevant to the new class, thus improving the retention of previously learned knowledge and avoiding overfitting."}, {"title": "2.2 Classifier", "content": "This paper adapts DenseNet121 [10] as the backbone classifier based on the experimental results shown in Sec. 3. DenseNet121 is characterized by a dense connectivity pattern, where each convolutional layer receives inputs from all pre- ceding layers within a dense block, thereby promoting efficient feature reuse and robust gradient flow. This ensures strong gradient signals even for the earliest layers during backpropagation [10]. The architecture consists of 121 convolu- tional layers, organized into four dense blocks and separated by three transition layers. The transition layers apply normalization, followed by a convolution and pooling operations to downsample the feature maps. Finally, the intermediate feature map obtained undergoes global average pooling and is fed to fully con- nected layers for classification. The dense connections reduce redundant parame- ters which lower model complexity and enable faster training of deeper networks [10]. This improved information flow helps to reduce overfitting, which is crucial for handling imbalanced data."}, {"title": "2.3 Perturbation Methods for Explainability", "content": "The black-box nature of deep neural networks hinders the understandability of how predictions are made by the model. This limits the usability of the AI algorithms in critical scenarios like healthcare where the rationale behind the\ndecision-making process of the model must align with the characteristics taken into account by the healthcare professionals. In the realm of deep neural net- works, explainability is not just a desirable feature it is a necessity. Without a clear understanding of how and why these complex models make decisions, par- ticularly in medicine, we risk compromising trust and safety. To make the model more trustworthy and transparent, our framework uses perturbation techniques to identify the relevant characteristics of input data that influence the decision- making process of the model. An efficient model should learn from salient features rather than spurious information or noise that is present in the training data. Perturbation methods, being model-agnostic, allow dynamic analysis without re- quiring access to the model's internal details. Techniques like applying occlusion masks or adding noise to image patches or pixels help in querying the model and developing test hypotheses on the fly. The main challenge is selecting appropriate perturbation techniques to analyze the model's performance effectively.\nThe detection of DR requires identifying pathologies spread across various quadrants of the eye fundus image. In contrast, diagnosing Glaucoma necessi- tates a precise analysis of the optic disc, focusing on the cup-to-disc ratio. The formation of red and bright lesions are the two most common symptoms of DR. Research shows that the identification of red and bright lesions is most effec- tively done using the green channel of color fundus images [20]. Additionally, as the condition worsens, neovascularization, which involves the formation of new blood vessels, takes place. Neovascularization in advanced stages of DR can also impact the optic disc area, resulting in the formation of new blood vessels in the optic disc.\nBased on the above insights gained regarding the relevant clinical features for DR and Glaucoma, we have designed perturbation techniques to further in- vestigate the model's decision-making process. Multiple controlled perturbations are applied to the test dataset and the performance of classifier $C_T$ on this per- turbed data was compared with the one obtained from the unperturbed test dataset. Two techniques were used to assess the influence of the green channel in the decision-making process: Reduce green (RG) which reduces the overall green channel weightage in comparison to the red and blue channels of color fundus images and Random green removal (RGR) which randomly removes segments of the green channel. Additional techniques like reducing image contrast (RC), adding Gaussian noise (GN) and applying edge sharpening (ES) were also used to study the impact of image quality on the model's inference. A strategy namely Optic disk occlusion (ODC) was used to evaluate the relevance of the optic disc in the classification of Glaucoma and DR images. Fig. 2 shows the different kind of perturbated images."}, {"title": "3 Experiments and Results", "content": "This section provides a comprehensive overview of the datasets used, the met- ric used to assess the classifier's performance, the experimental setup, and the resulting experimental outcomes."}, {"title": "3.1 Dataset", "content": "A total of 3,111 retinal color fundus images were obtained from three publicly available datasets: Messidor2 3, Chaksu [11], and LES-AV [17]. The Messidor2 dataset has 1,744 macula-centered RGB images. There are 1017 images belong- ing to the healthy class and 727 images belonging to the DR category. The Chaksu dataset comprises 1,345 images, with 188 images classified as Glau- coma and 1,157 images classified as healthy. These images were captured using three devices, including two non-mydriatic fundus cameras: the Remido non- mydriatic Fundus-on-Phone (FoP) and the Forus 3Nethra Classic non-mydriatic fundus camera and the Bosch handheld fundus camera. These images are Optic Disc-centered for Optic Disc assessment and Glaucoma detection. The LES-AV dataset has 22 images with 11 images categorized into Glaucoma and the re- maining 11 images categorized into healthy category. The data details are given in Table 1."}, {"title": "3.2 Experimental Setup", "content": "CELD is developed using Pytorch 4 and Monai 5 on python 3.9 as the platform. All experiments were performed on a 12 GB NVIDIA Titan XP GPU. The initial learning rate was set to 10-5. Early stopping is used to avoid over-fitting along with the AdamW optimizer[13]. A batch size of 8 is used in training."}, {"title": "3.3 Metric", "content": "The performance of the proposed framework was evaluated using accuracy, pre- cision, recall, and Fl-score. The mathematical definition of the listed metrics in terms of True Positive (TP), False Positive (FP), False Negative (FN) and True Negative (TN) is defined below.\nAccuracy = $\\frac{TP+TN}{TP + FP + FN+TN}$\nPrecision = $\\frac{\u03a4\u03a1}{TP + FP}$\nRecall = $\\frac{TP}{TP+FN}$\nF1 score = $2 \\times \\frac{Precision \\times Recall}{Precsion + Recall}$"}, {"title": "3.4 Result", "content": "The state-of-the-art (SOTA) models such as SeResNet101 [9], DenseNet121, and ViT were employed for classifying retinal images into Healthy, DR and Glau- coma, with their performance summarized in Fig. 3. DenseNet121 achieved the\nhighest accuracy at 0.7910. However, all models exhibited poor performance in classifying Glaucoma and DR due to significant class imbalance.\nFurther, the same models were tested, with results summarized in Table 2 where the classifiers are trained to categorize images into Healthy and DR only. Significant performance improvements were observed, particularly in the DR class, as indicated by balanced precision-recall scores. DenseNet121 outper- formed the other models, achieving an overall accuracy of 0.8729 and F1-scores of 0.8987 for healthy and 0.8296 for DR, leading to its selection as the backbone architecture for the CELD framework.\nFor this three-class classification problem, the proposed CELD framework outperformed the state-of-the-art (SOTA) models, achieving an overall accuracy of 0.9100. The performance of the CELD framework has been listed in Fig. 3. It demonstrated significant improvement in the Fl-scores for all classes, particu- larly for DR and Glaucoma. While the ViT model achieved a maximum F1-score of 0.5797 for DR, the CELD framework substantially improved this to 0.8971, with a high yet balanced precision-recall. Similarly, SeResNet's highest F1-score of 0.6286 for Glaucoma was improved to 0.6667 by the CELD framework. In med- ical image analysis, it is crucial for models to accurately detect positive cases, even if it occasionally results in false alarms. The CELD framework showed significant improvement in recall, albeit with a slight drop in precision for Glau- coma. Overall, the CELD framework significantly outperformed other models across various parameters.\nExplaining CELD framework with Data Perturbation: The perfor- mance of the proposed framework was evaluated using perturbed data and com- pared to unperturbed data, as summarized in Fig. 4. The corresponding confusion matrix is represented in Fig. 5. Reducing the weight of the green channel signif- icantly decreased performance for DR and Glaucoma classifications, while the classification of healthy samples remained mostly unaffected. The confusion ma- trix shows increased mis-classification of DR and Glaucoma images as healthy when the green channel's weight is negatively altered or partially removed. No- tably, reducing the green channel's weight across the entire image leads to higher mis-classification rates than randomly removing segments of the green channel. When random patches of green channel are removed, the classifier's decision for the DR class is influenced by the remaining unperturbed data. The performance drop for the Glaucoma class is less significant, as the optic disc region often remains unperturbed in many images.\nReducing contrast does not significantly impact the classification of DR or healthy categories, as shown in the confusion matrix, but it does impair Glau- coma classification. Visually, this perturbation blurs the optic disc region, thus, obscuring key features. Edge sharpening, which enhances pixels having high- intensity changes w.r.t it's neighborhood, leads to a high mis-classification rate of Glaucoma as healthy, as reflected by the Fl-score. Adding random Gaussian noise causes the model to falsely classify only one DR image as Glaucoma and most other DR images as healthy, resulting in high precision but a low F1-score for the DR class. It is important to note that in this study this the first per- turbation strategy that generates ambiguous decisions between the two disease classes. The model's decision for Glaucoma is less affected by noise but becomes prone to classifying healthy images as Glaucoma, leading to high recall and low precision for the Glaucoma class. In summary, DR identification is challenging in poor-quality images, while Glaucoma can be diagnosed in noisy, low-quality images but not in those with poor contrast or excessive sharpening.\nObserving the optic disc occlusion strategy reveals the model's high depen- dency on the optic disc for classifying Glaucoma and healthy eyes. These two classes exhibit high mis-classification rates, which is understandable since eye\nexperts often diagnose Glaucoma by examining the optic disc region. For Glau- coma, the absence of relevant features leads to mis-classification, as shown in the confusion matrix and Fig. 4. Most mis-classified images are labeled as healthy, indicating the model relies on this feature for Glaucoma identification, thereby increasing the precision score for the normal class. The optic disc's features are crucial for determining eye health, reflected in the lower F1-score compared to unperturbed data for healthy eyes. For DR, performance is less affected since neovascularization in the optic disc is not always present in DR-affected im- ages. However, there is an increase in false positives for the DR class due to the model's insufficient features for reliable decisions, resulting in high recall and low precision for the DR class. In conclusion, optic disc occlusion significantly impacts overall model accuracy, highlighting its importance as an input feature.\nTo conclude, the perturbation techniques revealed that the model heavily relies on the green channel and image quality for accurate classification, espe- cially for DR and Glaucoma. Occlusion of the optic disc significantly impacts Glaucoma detection, emphasizing its critical role in the model's decision-making process."}, {"title": "4 Conclusion and Discussion", "content": "This research demonstrates the potential of deep neural networks to improve medical image classification, particularly for identifying conditions like diabetic retinopathy (DR) and Glaucoma from fundus images. Initially, we trained the network to differentiate between healthy and DR-affected images. Using the Class Extension with Limited Data (CELD) framework, we fine-tuned the model also to classify Glaucoma, transforming it into a three-class classifier. The CELD framework enables the model to maintain its performance on previously learned tasks while adapting to new classes while efficiently tackling data imbalance with minimal computational overhead and data requirements. Consequently, the model retains its proficiency in identifying DR while learning to classify Glau- coma, ensuring efficiency and resource-friendliness.\nOur extensive empirical analysis compared the performance of two-class and three-class classifiers. The results highlighted that the Densenet121 architecture significantly improves classification accuracy, proving its suitability for this ap- plication. We conducted various experiments to assess accuracy and robustness, confirming the model's effectiveness. Additionally, we explored feature relevance through explainability using perturbed data. These studies provided insights into how changes in input data affect model performance, identifying the most criti- cal features for accurate classification. The perturbation analysis summarized the robustness of the CELD framework. This approach represents a significant ad- vancement in medical imaging and deep learning, providing an efficient method to expand model capabilities with limited data and computational resources. The CELD framework has the potential to be applied to diagnose a variety of other ocular diseases common in diabetic eyes."}]}