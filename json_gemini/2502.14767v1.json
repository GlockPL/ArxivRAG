{"title": "Tree-of-Debate: Multi-Persona Debate Trees Elicit Critical Thinking for Scientific Comparative Analysis", "authors": ["Priyanka Kargupta", "Ishika Agarwal", "Tal August", "Jiawei Han"], "abstract": "With the exponential growth of research facilitated by modern technology and improved accessibility, scientific discoveries have become increasingly fragmented within and across fields. This makes it challenging to assess the significance, novelty, incremental findings, and equivalent ideas between related works, particularly those from different research communities. Large language models (LLMs) have recently demonstrated strong quantitative and qualitative reasoning abilities, and multi-agent LLM debates have shown promise in handling complex reasoning tasks by exploring diverse perspectives and reasoning paths. Inspired by this, we introduce Tree-of-Debate (TOD), a framework which converts scientific papers into LLM personas that debate their respective novelties. To emphasize structured, critical reasoning rather than focusing solely on outcomes, ToD dynamically constructs a debate tree, enabling fine-grained analysis of independent novelty arguments within scholarly articles. Through experiments on scientific literature across various domains, evaluated by expert researchers, we demonstrate that ToD generates informative arguments, effectively contrasts papers, and supports researchers in their literature review.", "sections": [{"title": "1 Introduction", "content": "Navigating and identifying new and relevant research findings has become non-trivial with the popularity of open-access repositories. For example, arXiv received over 24,000 submissions in October 2024 (arXiv, 2024), inundating researchers with an overwhelming volume of information. This astronomical surge in scholarly articles makes it difficult to identify novel findings and discern the distinctions between related papers, especially those presenting similar ideas from different angles (e.g., papers from different research communities).\nAutomatically generating comparative summaries of research papers has proven valuable for addressing these challenges (Hayashi et al., 2023). Existing comparative summarization works (Str\u00f6hle et al., 2023) typically follow a two-step pipeline: (1) construct extractive summaries for each document to (2) identify their similarities and differences (Lerman and McDonald, 2009; Gunel et al., 2024). However, despite using large language models (LLMs), these methods often focus on surface-level semantic differences, which may not capture the most relevant distinctions. For example, when comparing pre-trained models like \u201cBERT\u201d (Devlin et al., 2019) and \u201cRoBERTa\u201d (Liu et al., 2019), it is crucial to note that ROBERTa omits next-sentence prediction, trains on ten times more data, and achieves superior performance. These insights require complex, comparative reasoning beyond basic semantics, as they rely on understanding BERT's contributions in the context of ROBERTa's. Thus, we propose the following principles:\nMulti-persona debates elicit complex, comparative reasoning. We explore the use of multi-agent debates for inducing fine-grained, comparative reasoning. These debates simulate group discussions where agents suggest diverse answers, critique one another, and refine responses to produce better outputs (Chan et al., 2023; Liang et al., 2024). Recent work has also introduced defining LLM agents as personas with distinct characteristics or values, enabling them to generate outputs that reflect the diverse perspectives needed to solve multi-faceted problems (Tseng et al., 2024; Wang et al., 2024c). Inspired by this, we propose converting scientific papers into personas that debate each other to foster critical analysis. For instance, while the papers debate their respective contributions to a topic, they critically evaluate each other's novelty and significance relative to their own claims.\nTree-structured debates allow for independent assessments of different contributions at varying depths. A scientific paper often makes contributions (e.g., methodology, dataset, evalua-"}, {"title": "2 Related Work", "content": "2.1 Persona Creation & Debate\nSimilar to how a person's background shapes their abilities, recent work has explored assigning personas to LLMs to capture diverse perspectives and extract unique capabilities (Fu et al., 2023). For instance, Portenoy et al. (2022) creates author personas for author recommendation by applying named entity recognition to papers and matches authors based on shared terminology. As we aim to highlight specific novelties and incremental contributions between two papers, we instead define a paper persona. While Portenoy et al. (2022)'s personas represent the author's multiple works and\n2.2 Comparative Summarization\nGenerating comparative summaries is challenging due to the diverse ways that differences between two entities can be represented. Traditional graph-based methods (Chen et al., 2022; Str\u00f6hle et al., 2023) classify sentences as a claim, similarity, or difference and score them to produce extractive summaries. While we use extractive summarization questions for self-deliberation, ultimately we aim to generate an abstractive summary that synthesizes the debate results. More recent works (Luu et al., 2021) fine-tune models to generate explanation sentences by first extracting in-text citation sentences that compare a principal document with a cited one, then maximizing the probability of generating the explanation given the two documents; however, this approach typically yields only a single sentence, which may not fully capture the nuanced differences between papers.\n2.3 Generation of Related Works Sections\nMulti-document summarization consolidates information from various sources, a task that grows in importance as scientific literature expands (Chen et al., 2022). Certain works within the HCI space (Palani et al., 2023; Lee et al., 2024), which have designed off-the-shelf, interactive systems. However, from a methodological standpoint, one approach (Shi et al., 2023) expands a paper's abstract into semantically similar sentences to form search queries for retrieving relevant papers, and then uses in-context examples to generate related work sections. On the other hand, DIR (Wang et al., 2024a) employs a structured fine-tuning process by prompting a language model to extract commonalities and differences from candidate summaries compared to a gold standard. However, these methods face limita-"}, {"title": "3 Methodology", "content": "TREE-OF-DEBATE aims to determine and compare the fine-grained scientific claims of two papers through a methodology inspired by formal debate. Our overall framework is presented in Figure 2.\n3.1 Preliminaries\n3.1.1 Problem Formulation\nWe assume two papers, p\u2081 and p2, and a topic n\u2080 (e.g., \"inference-time LLM reasoning methods\") are provided as input by the user. Our goal is to determine the specific novelties, incremental additions, and equivalent contributions relevant to n\u2080 between p\u2081 and p2, producing a debate tree T with a corresponding comparative summary S. In T, each node n\u1d62 represents a topic (with n\u2080 as the root), where topic n\u1d62 guides the specific debate occurring at that node. Topics may pertain to both papers or only one (e.g., in Figure 1, only p\u1d62 = B includes \u201cBacktracking\"). An edge from n\u1d62 to n\u2c7c indicates that n\u2c7c is a subtopic of n\u1d62 that merits further exploration.\n3.1.2 Segment-level Retrieval\nAn effective debate is contingent on an individual's preparation before the debate and their ability to retrieve knowledge dynamically during the debate. We employ a retrieval embedding model (Xiao et al., 2023) and cosine-similarity to compute and rank segment-level embeddings. We chunk each paper into roughly three-sentence segments such that it is easily comprehensible during the debate.\n3.2 TREE-OF-DEBATE Setup\nWe conduct a multi-persona debate between two paper personas, p\u2081 and p2, based on the high-level claim, p\u2081 is better than the p2 for topic n\u1d62. Our goal is not to determine a final debate \"winner\""}, {"title": "3.2.1 Constructing the Personas", "content": "We leverage an LLM agent to embody each debate persona, allowing for retrieved information from the papers and the debate history to be easily integrated into its context:\n\u2022 Papers: Each paper persona is given the title, abstract, and retrieved segments relevant to the starting topic n\u2080 (updated at each self-deliberation stage (Section 3.3)). Each paper persona's p\u1d62 role is to argue that their contributions towards the topic n\u1d62 are better than persona p\u2c7c's.\n\u2022 Moderator: Using the same underlying model as the paper personas, the moderator (i) identifies key debate subtopics for determining the papers' similarities and differences, (ii) judges the progression of the debate based on the authors' arguments, and (iii) synthesizes the debate tree into a comparative summary."}, {"title": "3.2.2 Tree Node Format", "content": "Each tree node with topic n\u1d62 undergoes a three-stage debate (pre-, during, and post-debate). The format is as follows:\n1. Self-Deliberation (Section 3.3): Each persona pa retrieves segments Sa relevant to n\u1d62, generates k claims Ca for their novel contributions, cites corresponding evidence E\u1d62 \u2208 Sa, and collects counter-evidence from their own paper Ea. The moderator then selects k new subtopics for the next level of k children, n \u2208 Ni.\n2. Debate (Prompts 13, 14, and 15): For each child debate node n\u2c7c, each persona pa (i) presents an argument that pa is better than pb on n\u2c7c, (ii) responds to the opposing argument, and (iii) revises their argument accordingly.\n3. Determine Expansion (Section 3.4): Based on the debate at n\u2c7c, the moderator determines whether the arguments progressed or introduced any unresolved questions meriting another round. If so, the moderator triggers self-deliberation for ni \u2192 n."}, {"title": "3.3 Self-Deliberation", "content": "Self-deliberation is an argumentative strategy (Tindale, 2020) that enables one to \"argue with oneself\" by considering alternative views, aiming to arrive at the best, most well-justified conclusion. We integrate self-deliberation into our multi-persona debate for a given topic node n\u1d62 and paper Pa\u2208{1,2}:\n1. Retrieve relevant segments S from pa that are closely related to n\u1d62.\n2. Generate k claims cj \u2208 Ca on the novel contributions of pa toward n\u1d62. Each claim includes a title, description, and a set of mapped evidence Ei,j) \u2286 S (see Prompt 11)."}, {"title": "3.4 Debate Tree Expansion & Synthesis", "content": "While we motivate the personas to examine and debate whether their work proposes a better idea than their opposition, this mechanism is intended to (1) emphasize the reasoning behind the idea and (2) provoke debate on the novelty behind the ideas, relative to each other. In other words, we hypothesize that two very similar ideas (e.g., \"Reasoning Evaluation\" in Figure 1) will typically lead to a longer debate subtree on which approach is better. Conversely, a uniquely novel approach or"}, {"title": "3.4.1 Determining Round Depth Expansion", "content": "For debate node n\u1d62, the moderator assesses the following (Prompt 9):\n1. Argument Progression: Is there sufficient evolution in the arguments or new, deeper concepts being introduced to justify further debate?\n2. Meaningful Questions: Have clarifying questions been raised that remain unanswered and merit further discussion? If no questions are raised, the moderator returns False.\n3. Clear Winner: Is it clear that one paper has won the debate, as their contributions are truly better and do not warrant deconstruction (to determine which subcomponents are truly better)?"}, {"title": "3.4.2 Debate Synthesis", "content": "Once ToD has converged (i.e., all debate paths have been adequately expanded), the moderator synthesizes the entire debate tree into a paragraph-long comparative summary. The debate tree is provided in-context, with each node n\u1d62 \u2208 T containing the following information: node topic title, node topic description, persona p\u2081's revised argument (at the end of the debate round), and persona p2's revised argument. The synthesis should first explain the papers' novelty similarities and then detail their differences, with greater emphasis on the latter. We provide the prompt in Prompt 10 and an example subtree in Appendix H."}, {"title": "4 Experimental Design", "content": "We choose Llama-3.1-Nemotron-70B-Instruct-HF, an open-source model, as the base model for all experiments. We sample from the top 1% of the tokens and use the same temperature settings across all samples (details on setting and hardware provided in Appendix A)."}, {"title": "4.1 Dataset", "content": "No datasets currently exist for comparing non-citing pairs of scientific papers- an overlooked setting, especially given the growing scale of literature where not all relevant work can be cited. Consequently, we aimed to construct a dataset with papers that both cite and do not cite each other, in order to test the robustness of Tree-of-Debate. However, novelty comparison between papers is a highly specialized and expensive task, requiring rich domain expertise to verify- especially if such papers do not explicitly cite one another. Thus, we gathered five domain expert researchers (detailed provided in Appendix D) to construct a dataset of 100 paper pairs across natural language processing, data mining, electrical engineering, and aerospace engineering (further details provided in Appendix E). Each researcher identified at least five papers they were highly familiar with, such that they could perform a detailed and informed human evaluation. They were instructed to annotate each pair with a root topic and whether: (1) the papers roughly focus on the same task but differ in methodology, or (2) they work on different tasks that are applied to similar motivations. Furthermore, they noted if the papers explicitly cited each other or not."}, {"title": "4.2 Baselines", "content": "Given that our primary goal is to demonstrate the difference in inference-time comparative reasoning capabilities between ToD and current LLMs, we design the following prompting-based baselines: (1) Single Stage uses the title, abstract and introduction sections of both papers to directly generate a comparative summary (Martin-Boyle et al., 2024); (2) Two Stage first individually summarizes each paper based on the title, abstract and introductions, and then uses the generated summaries to generate a comparative summary (Zhang et al., 2024).\nTo contextualize improvements from each component in ToD, we construct the following ablative methods: (1) ToD (No Tree) removes the tree structure by merging child arguments into one and considering the combined subtopic as the debate node"}, {"title": "4.3 Evaluation Metrics", "content": "The same domain-experts from Section 4.1 manually evaluate each of their chosen pairs in-depth, assessing various qualities of the 100 summaries. We normalize each of the scores below and scale them by 100 for the final results in Table 2 (full metric guidelines provided in Appendix C):\n\u2022 Factuality: How factual is the summary? Each sentence is given a 1/0 binary score for factuality, and the scores are averaged across the summary.\n\u2022 Breadth: Is the summary comprehensive and complete? Each summary is rated from 0-4 (\u201cnot at all\u201d to \u201cvery\").\n\u2022 Contextualization: Does the summary explain and/or justify the posed similarities/differences between the papers, as opposed to just mentioning them? Each summary is rated from 0-4 (\u201cnot at all\" to \"very\")."}, {"title": "5 Experimental Results", "content": "Overall Performance & Analysis. Table 2 shows the performance of Tree-of-Debate (ToD) compared with the baselines on factuality, breadth of comparison (completeness), and contextualization. We observe that the domain-experts found ToD summaries 6.85% more complete and 25.98% more contextualized compared to the most competitive baseline. This observation indicates that multi-persona debate trees help analyze pairs of papers to uncover more fine-grained contributions, as well as identifying connections between the papers. Given that all samples were carefully annotated and evaluated by domain-experts, we are able to draw several interesting insights, which we list below:\nStructured debate is necessary for eliciting contextualized comparative summaries. Our results show that TREE-OF-DEBATE significantly improves contextualization, achieving an average score of 95.21% across all settings, compared to 75.57% for the strongest baseline (Two-Stage). Our domain-expert evaluators frequently observed that the LLM-generated summaries (Single and Two Stage) at face value mention a breadth of"}, {"title": "5.1 Qualitative Case Study", "content": "Evolution via Critical Reasoning. Our approach enables paper personas to refine their comparisons by addressing counterarguments elicited through debate. Table 3 illustrates this through a debate round between Tree of Thoughts (ToT) and Chain-of-Thought (CoT) on their generalizability and flexibility. ToT initially highlights its flexibility through deliberate reasoning, exploring multiple paths with lookahead/backtracking. However, CoT counters that ToT's complexity hinders broad applicability, whereas CoT can easily generalize to a multitude of tasks with minimal setup while still achieving state-of-the-art results. In the revision stage, both refine their arguments: ToT emphasizes adaptive explo-"}, {"title": "6 Conclusion", "content": "Automatic summarization is essential for managing the growing volume of research. We introduce TREE-OF-DEBATE, a structured approach that models papers as personas engaging in a debate to extract their key similarities and differences. Our method organizes debates into a hierarchical structure to produce abstractive and contextualization summaries while preserving their factuality. With thorough domain-expert empirical evaluation and qualitative case studies, we demonstrate that Tree-of-Debate significantly outperforms baselines."}, {"title": "7 Limitations & Future Work", "content": "We explore some limitations of our work. Within each debate round (each persona presenting their arguments, responding to one another, and revising their arguments), we note that a crucial element to a productive debate round is each persona providing meaningful feedback (e.g., doubts, clarifying questions) within the \u201crespond\u201d stage. However, the quality of this critical response may vary based on the difficulty of the task (e.g., a more fine-grained topic that has no presence within the model's existing pre-training dataset) and/or the size of the model.\nWe also observe a slight trade-off between contextualization and factuality. Especially in more challenging comparisons where the overlap between the papers may not be explicit (e.g., task-based comparisons), ToD must take more of a leap with respect to its reasoning to identify similarities and differences between the papers, which may lead to further hallucinations. Furthermore, we notice that as a debate path progresses to deeper levels, if certain evidence is not present to support a paper's fine-grained claims, the personas begin to suggest potential future studies or even new methods (e.g., combining certain strengths of the two methods). While this does introduce some noise (as we can see through our competitive factuality scores, this is minimal) to our output summary, these types of \u201challucinations\u201d present exciting new paths for research to explore.\nTree-of-Debate can be extended to other general, complex reasoning tasks which can exploit our tree-based decomposition and debate-based critical feedback. For instance, complex quantitative reasoning problems can often be decomposed into several sub-problems, which can be represented within our tree structure. Each persona can instead represent a different approach to solve that specific sub-problem. We can also consider extending this to a negotiation setting, where there are various aspects to consider when determining the optimal compromise. For example, two parties can negotiate on a price of a car with respect to its make, model, mileage, etc. Each of these can be explored within their own respective subtrees."}, {"title": "A Experimental Settings", "content": "We choose Llama-3.1-Nemotron-70B-Instruct-HF, an open-source model, as the base model for all experiments. We sample from the top 1% of the tokens and set the temperature between (0,0.5) to trade-off between deterministic and creative generation based on the nature of the given task (same setting across all samples):\n\u2022 Paper generates arguments: 0.3\n\u2022 Paper whether evidence is relevant: 0\n\u2022 Paper presents its argument: 0.1\n\u2022 Paper responds to the opposition's argument: 0.4\n\u2022 Paper revises its argument: 0.4\n\u2022 Moderator generating subtopics: 0.3\n\u2022 Moderator determines whether to expand the debate note: 0.1\n\u2022 Moderator summarizes a debate path: 0.4\nWe set the number of retrieved segments \u03b4 = 5 so that we can gather a sufficient amount of evidence while not overwhelming the debate with long-context. We set the number of generated subtopics k = 3, for covering a reasonable breadth of topics while minimizing redundancy. Finally, we set the maximum debate tree depth l = 3 for adequate exploration. We use vLLM (Kwon et al., 2023) for distributed and constrained generation on four NVIDIA A100 GPUs."}, {"title": "B Baselines", "content": "We compare Tree-of-Debate (ToD) with the following prompting-based baseline methods. We use the same base language model for all comparisons.\n\u2022 Single-stage: We prompt an LLM with the title, abstract and introduction sections of both focus and opposition papers. We prompt the model to directly generate a contrastive summary of the two papers (Martin-Boyle et al., 2024).\n\u2022 Two-stage: We first instruct an LLM to individually summarize each paper based on the title, abstract and introductions. We then use the generated summaries to prompt the model to generate a contrastive summary (Zhang et al., 2024).\nTo contextualize improvements from each component in Tree-of-Debate we construct the following ablative methods:\n\u2022 ToD (No Tree): We remove the tree structure from Tree-of-Debate by merging child arguments into one. We do so by concatenating the topics and descriptions of the child subtopics and tag them to distinguish the topics. In each debate round, the model is prompted with the combined subtopic and its corresponding description.\n\u2022 TOD (No SD): We remove self-deliberation (SD) to test the impact of iterative retrieval based on debate progression. We do so by prompting the model with title, abstract, and introduction of each paper instead of retrieving based on the subtopic."}, {"title": "C Evaluation Metrics", "content": "The same domain-experts from Section 4.1 manually evaluate each sample in-depth, assessing various qualities of the summaries:\n\u2022 Factuality: How factual is the summary? Each sentence is given a 1/0 binary score for factual or not, and the scores are averaged across the summary.\n\u2022 Completeness: Is the summary comprehensive and complete? This is evaluated using the following Likert scale:\n0. Not at all, the summary misses (MULTIPLE) major points.\n1. No, the summary misses a (SINGULAR) major point.\n2. Somewhat, the summary misses minor points.\n3. Yes, the summary covers the major points, but still is not what I would expect.\n4. Very comprehensive, the summary covers the major points.\n\u2022 Contextualization: Does the summary explain and/or justify the posed similarities/differences between the papers, as opposed to just mentioning them?\n0. Not at all, the summary is simply extractive- it just seems to take different subtopics from each paper and doesn't synthesize them- no justification behind similarities and differences.\n1. No, the summary attempts at some level of synthesis, but it is not meaningful.\n2. Somewhat, the summary attempts at synthesizing at most one point.\n3. Yes, the summary contains meaningful synthesis but only for a minority of points.\n4. Strongly contextualized, the summary contains meaningful synthesis across all major points."}, {"title": "D Domain-Expert Profiles", "content": "Given that novelty comparison between papers is a highly specialized and expensive task, we gather five domain experts to both collect and annotate our dataset, as well as evaluate Tree-of-Debate's generated summaries based on their respective samples. Each domain expert is a graduate student with 3+ years of research experience in a specialized area:\n1. Domain Expert #1: A third-year PhD in computer science with ten publications; research expertise is text mining and data mining.\n2. Domain Expert #2: A third-year PhD in aerospace engineering with two publications; research expertise is in electric propulsion.\n3. Domain Expert #3: A second-year PhD (with two years of a research-track Masters) in electrical engineering with four publications; research expertise is in in-memory computing and wireless communications.\n4. Domain Expert #4: A first-year PhD (with two years of a research-track Masters) in computer science with six publications; research expertise is data-efficient natural language processing.\n5. Domain Expert #5: A first-year PhD (with two years of a research-track Masters) in computer science with twenty-five publications; research expertise is large language model training and efficiency."}, {"title": "E Dataset Specifications", "content": "As mentioned earlier, TREE-OF-DEBATE'S dataset contains 100 samples, and Table 1 specifies the breakdown. Each sample contains the following:\n(a) Topic: a short, vague description of the theme of the two papers\n(b) Paper #1 arXiv Link\n(c) Paper #1 Title\n(d) Paper #1 Abstract\n(e) Paper #1 Introduction\n(f) Paper #2 arXiv Link\n(g) Paper #2 Title\n(h) Paper #2 Abstract\n(i) Paper #2 Introduction\n(j) Method/Task: 0 if the papers differ in methodology (but have the same task) and 1 if the papers differ in the task (but the methodology is generally the same)\n(k) Cite/No: 0 if the papers do not cite each other, and 1 if the papers cite each other.\nWe provide the dataset (as a tab-separated file) in our code repository: https://anonymous.4open.science/r/tree-of-debate-5961/dataset/tree_of_debate_dataset.tsv. 5 contains a few rows from the dataset."}, {"title": "F Qualitative Case Study: TreeInstruct vs. BRIDGE", "content": "Tables 6 and 7 contain comparative summaries from the baseline, ablations, and our final method on the papers, TreeInstruct (Kargupta et al., 2024) and BRIDGE (Wang et al., 2024b)- Papers 0 and 1, respectively. Below, we qualitatively compare each summary, pointing out the weaknesses and strengths, and show how our method is able to address all the issues brought up in the baseline summaries.\nThe top left contains the Two-Stage baseline. The Two-Stage baseline tends to contain near-copy phrases from the paper, resulting in an overly specific, extractive and unnatural summary (an example is the first line highlighted in red: \u201cMethodological Approach: Paper 0 employs...\"). As a result, the differences that are extracted are not explained very\""}, {"title": "G Prompts", "content": "In this appendix, we provide each of the prompts."}, {"title": "H Tree Example", "content": "Below, we provide an example of a path that we use in our prompts. Given input topic: Helping Students Fix their Mistakes.\n1. Level 1 Child Argument: Personalized Error Resolution Strategies: Debate the effectiveness of adaptive, student-centered approaches in resolving errors, focusing on the role of dynamic question trees and expert decision-making models. This subtopic encourages discussion on the importance of tailoring guidance to individual students' needs and knowledge gaps.\n\u2022 Author 0's Argument: Adaptive Guidance Enhances Personalization via Nuanced Student Modeling. By integrating TreeInstruct's dynamic question tree with a nuanced student modeling framework, our approach provides more effective personalized error resolution strategies than Bridge. This integration enables TreeInstruct to capture the complexities of student thought processes, addressing both independent and dependent mistakes concurrently, while also scaling to accommodate large, diverse student populations. In contrast, Bridge's reliance on predetermined error types and remediation strategies may limit its versatility in complex, multi-bug scenarios.\n\u2022 Author 1's Argument: Expert-Guided Decision-Making for Adaptive, Context-Sensitive Error Resolution. Our approach leverages expert decision-making to inform adaptive, context-sensitive error resolution strategies, demonstrated through the extension of our Bridge model to accommodate complex, multi-bug scenarios. By incorporating nuanced expert thought processes, our method provides more effective personalized guidance than TreeInstruct's adaptive question tree approach. This integration of expert guidance and adaptability addresses the opposition's concerns regarding versatility and scalability in handling diverse student populations.\n(a) Level 2 Child Argument: Adaptive Guidance in Error Resolution: Debate the effectiveness of adaptive guidance in error resolution strategies, focusing on how each approach tailors feedback to individual students' knowledge states. Discuss the benefits and limitations of each method.\n\u2022 Author 0's Argument: Adaptive Guidance with Latent Expertise for Personalized Error Resolution. By integrating the strengths of both approaches, our revised argument proposes a hybrid model that combines the adaptive guidance of TreeInstruct with the latent thought processes of expert decision-making. This fusion enables a more nuanced understanding of individual student needs, providing targeted support while maintaining scalability and adaptability. Ultimately, this hybrid approach offers a more effective and personalized error resolution strategy, surpassing the limitations of both adaptive guidance and expert decision-making alone.\n\u2022 Author 1's Argument: Expert-Infused Adaptive Guidance for Error Resolution. Our revised approach integrates expert decision-making processes into adaptive guidance frameworks, enabling a more nuanced understanding of individual student needs and providing targeted support while maintaining scalability and adaptability. This hybrid model combines the strengths of both approaches, offering a more effective and personalized error resolution strategy. By incorporating expert-informed decision-making into adaptive guidance, we bridge the novice-expert knowledge gap more effectively than either approach alone."}]}