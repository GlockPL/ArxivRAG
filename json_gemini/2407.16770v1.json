{"title": "Infinite Ends from Finite Samples: Open-Ended Goal Inference as Top-Down Bayesian Filtering of Bottom-Up Proposals", "authors": ["Tan Zhi-Xuan", "Gloria Kang", "Vikash Mansinghka", "Joshua B. Tenenbaum"], "abstract": "The space of human goals is tremendously vast; and yet, from just a few moments of watching a scene or reading a story, we seem to spontaneously infer a range of plausible motivations for the people and characters involved. What explains this remarkable capacity for intuiting other agents' goals, despite the infinitude of ends they might pursue? And how does this cohere with our understanding of other people as approximately rational agents? In this paper, we introduce a sequential Monte Carlo model of open-ended goal inference, which combines top-down Bayesian inverse planning with bottom-up sampling based on the statistics of co-occurring subgoals. By proposing goal hypotheses related to the subgoals achieved by an agent, our model rapidly generates plausible goals without exhaustive search, then filters out goals that would be irrational given the actions taken so far. We validate this model in a goal inference task called Block Words, where participants try to guess the word that someone is stacking out of lettered blocks. In comparison to both heuristic bottom-up guessing and exact Bayesian inference over hundreds of goals, our model better predicts the mean, variance, efficiency, and resource rationality of human goal inferences, achieving similar accuracy to the exact model at a fraction of the cognitive cost, while also explaining garden-path effects that arise from misleading bottom-up cues. Our experiments thus highlight the importance of uniting top-down and bottom-up models for explaining the speed, accuracy, and generality of human theory-of-mind.", "sections": [{"title": "Introduction", "content": "Whether one is watching a play, reading a novel, or spending time with a friend at their house, inferences about others' goals and motivations often arise spontaneously and unbidden (Moskowitz & Olcaysoy Okten, 2016): Is the person crouching behind a tree trying to hide from, or spy on someone? Does the strange warrior who has just entered the fray of battle intend to kill the protagonist, or save them? When your friend gets up from the couch and walks to the kitchen, are they getting a snack, or making some tea? Despite the seemingly infinite space of possible goals, we have little trouble in coming up with plausible hypotheses, and then \u2014 as the story unfolds \u2014 filtering out those that fail to explain our observations. If the warrior defends our protagonist from a stray arrow, they are likely an ally. If your friend opens the fridge, they are probably having a snack. What computational mechanisms underlie this ability to both hypothesize and evaluate the goals that explain others' behavior, even when the set of possibilities is vast and open-ended?\nWhile psychologists have long studied how people both generate (Heider & Simmel, 1944; Graesser et al., 1994; Hassin et al., 2005; Van Overwalle et al., 2012) and evaluate hypotheses about the goals that other agents have (Gergely & Csibra, 2003; Jara-Ettinger et al., 2015; Liu et al., 2017), computational models of human goal inference have focused on the latter, assuming a small and fixed set of possible goals, then modeling how people infer their relative likelihoods (Baker et al., 2009; Ullman et al., 2009; Kleiman-Weiner et al., 2016; Vered et al., 2016; Jara-Ettinger et al., 2019). This leaves open how people come up with plausible goals in the first place, especially in large hypothesis spaces where enumeration over all possibilities makes inference intractable (Kwisthout & Van Rooij, 2013; Blokpoel et al., 2013). How then are people solving this seemingly intractable problem (if they do so at all)? Even though recent advances in Bayesian inverse planning have shown how modeling the plans of other agents (Zhi-Xuan et al., 2020; Alanqary et al., 2021) and inferring goals from static scenes (Chandra et al., 2023) can be made orders of magnitude more efficient, they do not address the key challenge posed by open-ended settings: Efficiently generating plausible goal hypotheses.\nIn this paper, we develop an algorithmic account of open-ended goal inference, which combines top-down Bayesian inverse planning and bottom-up sampling in a sequential Monte Carlo (SMC) algorithm (Del Moral et al., 2006). Instead of exhaustively enumerating the space of goals, our model assumes that humans are familiar with the statistics of their environments (Griffiths & Tenenbaum, 2006), and can rapidly generate relevant hypotheses based on contextual, data-driven cues (Schulz, 2012; Phillips et al., 2019). In particular, we assume familiarity with the statistics of co-occurring subgoals, such that complete goals can rapidly be generated once some subgoals have been achieved. Our model then filters these goals according to the principle of rational action (Gergely & Csibra, 2003; Baker et al., 2009), keeping those that best explain the agent's actions. We evaluate this model in Block Words, a game where observers have to guess the word that someone is stacking out of lettered blocks (Ram\u00edrez & Geffner, 2010; Alanqary et al., 2021). Subgoals correspond to partial words, so observers can generate plausible goals by \u201cauto-completion\u201d. However, this bottom-up strategy is insufficient in general \u2014 some goals may be irrational given the actions observed so far, necessitating inverse planning."}, {"title": "Computational Model", "content": "Building upon prior accounts of human goal inference (Baker et al., 2009; Zhi-Xuan et al., 2020; Alanqary et al., 2021), we assume that observers perform approximately Bayesian inference over a generative model of how other agents plan and act to achieve their goals:\nGoal Prior: $g \\sim P(g)$ (1)\nOnline Planning: $\\pi_t \\sim P(\\pi_t | s_{t-1},\\pi_{t-1},g)$ (2)\nAction Selection: $a_t \\sim P(a_t | s_{t-1}, \\pi_t)$ (3)\nState Transition: $s_t \\sim P(s_t | s_{t-1}, a_t)$ (4)\nHere $g$ is the agent's goal, and at each step $t$, $\\pi_t$ is the agent's current plan or policy, $a_t$ is the agent's action, and $s_t$ is the state of the environment. Given a sequence of states $s_{0:T}$ and actions $a_{1:T}$, the observer's task is to infer the goal $g$ by approximating the posterior $P(g|s_{0:T}, a_{1:T})$. Approximating this posterior presents numerous computational challenges. Among these, our focus is on the challenge posed by open-ended settings, where the set of possible goals $g \\in G$ is large or potentially infinite. In this section, we first review recent advances that render goal inference over fixed spaces algorithmically tractable, before explaining how we can extend these ideas to open-ended spaces.\nSince computing the posterior requires simulating the plans $\\pi_t$ that an agent might follow to each goal $g$, this process is also known as Bayesian inverse planning. In general this is a difficult problem, because planning itself is a complicated and often intractable task. However, as Zhi-Xuan et al. (2020) show, this difficulty can be alleviated by treating agents as boundedly rational planners, who spend only limited computation at each step $t$ on planning. We adopt a more recent version of this architecture (Zhi-Xuan et al., 2024; Ying et al., 2023), modeling agents that update a policy $\\pi_t$ (i.e. a conditional plan) that defines a Boltzmann distribution over actions $a_t$ that can be taken at state $s_{t-1}$:\n$P(a_t | s_{t-1}, \\pi_t) = \\frac{\\exp(-\\beta \\pi_t (s_{t-1}, a_t))}{\\sum_{a'} \\exp(-\\beta \\pi_t (s_{t-1}, a'))}$ (5)"}, {"title": "Bottom-Up Sampling of Plausible Goals", "content": "While the analysis above suggests that Bayesian inverse planning is not only tractable, but linear in computational complexity, it neglects the fact that the number of goals $G$ can grow very large. As suggested by Blokpoel et al. (2013), this might be because $G$ itself grows exponentially with some other natural parameter \u2014 in Block Words, for example, just 9-11 lettered blocks can be used to spell anywhere from 150 to 800 English words. But even without this exponential dependence, a large value of $G$ can quickly render (exact) goal inference too costly to be algorithmically plausible."}, {"title": "Experiments", "content": "We evaluated open-ended SIPS as a model of human goal inference on a set of 16 scenarios in Block Words, a variant of the classic Blocksworld domain where the goal is to infer the word that an agent is spelling by stacking a tower of lettered blocks. In contrast to previous Block Words tasks (Ram\u00edrez & Geffner, 2009, 2010; Alanqary et al., 2021; Chandra et al., 2023), we did not specify a fixed set of 5 to 20 goal words. Instead, we told participants that the goal might be any English word between 3 to 8 letters long, with the implied restriction that the word had to be spelled out of the available blocks.\nParticipants were first shown the initial layout of the blocks. They could then advance the scenario, watching several actions play out as an animated video. The video would then pause at a judgment point, giving participants time to guess the word being spelled via text box entry. Participants could add as many guesses as they liked, and also remove any previous guesses that they no longer considered likely. They could then advance to the next judgment point, continuing in this way until the end of the scenario. Each participant was presented 8 out of the 16 scenarios, after first completing a tutorial and a comprehension quiz. To incentivize high quality responses, we paid participants a reward based on the accuracy of their guesses ($0.1/n for every correct answer out of n guesses), and presented the bonus point breakdown after they completed each scenario."}, {"title": "Results", "content": "We analyzed human responses and model outputs by comparing them in terms of distribution similarity (Fig. 2a), average accuracy (Fig. 2b), step-by-step inferences (Fig. 3), response variance (Fig. 4a), sample efficiency (Fig. 4b), and resource rationality (Fig. 4c). Additional results (e.g. accuracy vs. runtime) and sensitivity analyses are in the Appendix.\nOpen-ended SIPS is most similar to human inferences across all conditions. As we predicted, human inferences showed the highest similarity with open-ended SIPS (IoU = 0.33\u20130.36 for all $N$) compared to exact inference (IoU = 0.31) or bottom-up guessing (IoU = 0.30\u20130.32), with $N = 2$ samples being the most similar. Notably, open-ended SIPS was more similar to humans in the Irrational Alternatives condition, with both achieving considerably higher accuracy than the bottom-up only heuristic, indicating that humans indeed engage in inverse planning. Our model was also more similar to humans than exact inference, especially in the Bottom-Up Friendly and Garden Path conditions, consistent with the hypothesis that humans engage in bottom-up sampling.\nStep-by-step human inferences are best matched by open-ended SIPS. The step-by-step comparisons in Figure 3 help to elucidate these aggregate findings. On one hand, open-ended SIPS and the proposal-only model make initial guesses that are biased towards words that complete the first few stacked letters, whereas the exact posterior is much more uncertain. On the other hand, humans account for the rationality of the observed actions when drawing inferences (e.g. Figure 3(a), t = 10), just like our exact and approximate Bayesian inverse planning algorithms.\nOur model's algorithmic properties best explain human variance and guess counts. In Figure 4, we compare the algorithmic properties of the models. Human variance was best matched by open-ended SIPS with $N = 2$. Bottom-up proposals had lower variance, and did not prune samples as effectively as sample-matched counterparts. Exact inference is zero-variance, but at the cost of tracking drastically more hypotheses. As such, it was dominated by open-ended SIPS in terms of net reward when accounting for cognitive costs (Fig. 4c). The comparison with pure bottom-up proposals was more nuanced. If reweighting a sample via inverse planning is costly enough, pure bottom-up guessing can be more resource-rational (Lieder & Griffiths, 2020). However, there is a large range of cost-ratios where it pays to do inverse planning. Since humans attained more reward than all proposal-only baselines in the Irrational Alternatives condition, this suggests that they indeed find inverse planning worthwhile."}, {"title": "Discussion", "content": "In comparison to alternative models, our sampling-based account of open-ended goal inference is best supported on both empirical and theoretical grounds, providing an algorithmically plausible explanation for the speed and flexibility of human goal inference. Still, our experiments find that humans remain more similar to themselves (IoU = 0.44) than our best-fitting model (IoU = 0.35). Part of this might be explained by the discrepancy between the statistics of how humans guess in word games versus the text corpus frequencies that inform our model. This could be addressed by deriving a prior and proposal from human guesses. Humans also appear to exhibit stickier inferences in garden path cases, whereas open-ended SIPS tends to avoid them when run with larger values of $N$ by proposing new goals at every step. This suggests that humans may be adaptive in deciding when to rejuvenate their hypotheses (Del Moral et al., 2012; Elvira et al., 2016). Finally, unlike our model, humans might forget older observations, becoming more inaccurate, but also more efficient at inference. SMC algorithms that selectively forget past observations could mimic this (Beronov et al., 2021).\nAnother open question is how bottom-up sampling can be made more general. In future work, we plan to explore how the statistics of co-occurring subgoals can be distilled from web-scale language models (West et al., 2022) into domain-specific models for rapid hypothesis generation. These statistics might be augmented by static analysis of environment models, automatically determining which subgoals are instrumental for other goals (Blum & Furst, 1997). Such mechanisms for flexible domain adaptation could provide an even richer picture of how we contend with the infinitude of ends that others pursue, even in the face of our very finite means."}, {"title": "Appendix", "content": "Experiment Interface\nThe web interface used by participants is shown in Figure A1. At each judgment point, participants typed their guesses into the text box, which validated whether the guess was between 3 and 8 characters and used only the letters that were available. Participants could also remove their guesses by clicking the symbol next to each guess. The list of guesses was converted into a probability distribution by assigning a probability of 1/n to each word among the n guesses. Participants could rewatch the most recent segment of the animation by pressing the Replay button, or re-watch the whole animation up to the judgment point by pressing the Replay All button. This interface is accessible at https://block-words.web.app/?local=true.\nModel Fitting and Sensitivity Analysis\nOur model of open-ended goal inference is characterized by two sets of parameters: The parameters of the generative model $P(g, \\pi_{1:t}, s_{0:t}, a_{1:t})$, and the parameters of the inference algorithm which approximates $P(g|s_{0:t}, a_{1:t})$. We fit the parameters of the generative model across the following ranges:\n\u2022 Goal prior word temperature $T_w \\in \\{1,2,4,8,16\\}$\n\u2022 Inverse temperature $\\beta \\in \\{1,1,1,2,4\\}$\n\u2022 Planning budget $B \\in \\{5, 10, 20, 50, 100, 200, 500\\}$\n\u2022 Replanning cadence $\\Delta t \\in \\{1,2\\}$\n\u2022 RTHS search strategy $\\sigma = A^* or BFS$\n$T_w$ controls tempering of the wordfreq-derived word frequencies used for the goal prior $P(g)$, and $\\beta$ controls the optimality of action selection. $B$ is the planning budget for real-time heuristic search (RTHS) algorithm, $\\Delta t$ is the number of timesteps between each call to RTHS that updates the policy $\\pi_t$, and $\\sigma$ controls how nodes are expanded by RTHS, which is done either via $A^*$ search around each neighbor of the current state $s_t$ (guided by the FF heuristic as the default $\\hat{Q}_g$ value) as in LSS-LRTA* (Koenig & Sun, 2009), or via breadth-first search (BFS) around the current state $s_t$, as in LRTA*-LS (Hern\u00e1ndez & Meseguer, 2007).\nFor the inference algorithm, we fit these parameters:\n\u2022 n-gram word temperature $T_w \\in \\{1,2,4,8,16\\}$\n\u2022 n-gram termination bias $\\epsilon \\in \\{0,0.05,0.1,0.15,0.2,0.25\\}$\n\u2022 Bottom-up proposal strategy $Q \\in \\{LAST-AND-NEXT,...\\}$\n\u2022 Number of samples $N \\in \\{2,5, 10, 20, 50\\}$\n$T_w$ tempers the word frequencies used to fit the n-gram model for the bottom-up proposal $Q$, and is matched to be the same value used for the goal prior $P(g)$. To capture the difficulty of guessing longer words, we modified the n-gram to have an additional $\\epsilon$ probability of terminating after each character. For simplicity, we fixed the context length of the n-gram model to $n = 5$. Various ways of implementing the bottom-up proposal $Q$ are discussed in the next section. We also vary the number of particles $N$ used by open-ended SIPS.\nFitting procedure. Model fitting proceeded in two stages. We first fit the generative model parameters to improve similarity with humans, using exact inference to factor out stochasticity or performance issues in the inference algorithm from the quality of the generative model itself. Instead of Pearson's correlation coefficient (commonly used in other BTOM studies), we used the intersection-over-union between human and model distributions (i.e. the Jaccard index) as our similarity metric, since it does not consider two probability vectors similar just because they both contain many zeros. Having determined values of $B = 100$, $\\Delta t = 2$ and $\\sigma = BFS$ that led to the most similarity with humans under the constraint of a reasonable runtime, we then fit the parameters of the open-ended SIPS algorithm. The best fitting inference parameters were $T_w = 4$ (which was matched with the goal prior's $T_w$), $\\epsilon = 0.05$, $Q = LAST-AND-NEXT$, and $N = 2$.\nGenerative model sensitivity analysis. Figure A2 shows how similarity with humans varies across generative model parameters when using exact Bayesian inference. A higher planning budget $B$ leads to a stronger fit, showing the importance of computing a good estimate of the agent's policy via"}]}