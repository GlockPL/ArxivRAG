{"title": "Data-driven Modeling of Combined Sewer Systems for Urban Sustainability: An Empirical Evaluation", "authors": ["Vipin Singh", "Tianheng Ling", "Teodor Chiaburu", "Felix Biessmann"], "abstract": "Climate change poses complex challenges, with extreme weather events becoming increasingly frequent and difficult to model. Examples include the dynamics of Combined Sewer Systems (CSS). Overburdened CSS during heavy rainfall will overflow untreated wastewater into surface water bodies. Classical approaches to modeling the impact of extreme rainfall events rely on physical simulations, which are particularly challenging to create for large urban infrastructures. Deep Learning (DL) models offer a cost-effective alternative for modeling the complex dynamics of sewer systems. In this study, we present a comprehensive empirical evaluation of several state-of-the-art DL time series models for predicting sewer system dynamics in a large urban infrastructure, utilizing three years of measurement data. We especially investigate the potential of DL models to maintain predictive precision during network outages by comparing global models, which have access to all variables within the sewer system, and local models, which are limited to data from a restricted set of local sensors. Our findings demonstrate that DL models can accurately predict the dynamics of sewer system load, even under network outage conditions. These results suggest that DL models can effectively aid in balancing the load redistribution in CSS, thereby enhancing the sustainability and resilience of urban infrastructures.", "sections": [{"title": "1. Introduction and Related Work", "content": "Climate change has increased the frequency and intensity of extreme weather events [1], which pose significant challenges to urban infrastructure and environmental management [2]. Managing Combined Sewer Systems (CSS) becomes particularly difficult [3]. Heavy rainfall can overwhelm the capacity of these systems, leading to overflows that release untreated sewage into rivers and lakes [4]. This contamination compromises water quality and poses direct risks to human health [5].\nMany urban areas that utilize CSS have implemented overflow basins to mitigate this risk [4]. However, there remains a significant gap in understanding the dynamics of water levels in these overflow basins. Traditional methods for modeling the dynamics of sewer systems rely on physical simulations [6]. These systems are challenging to apply to large urban infrastructures as they require domain expertise and detailed data on the system components, which is often unavailable or imposes significant financial costs. Improving the forecasting of these water levels can significantly enhance real-time flow control and inform maintenance and extension planning for sewage overflows.\nData-driven approaches, such as Deep Learning (DL) models, particularly time-series models, offer a promising alternative for modeling sewer system dynamics. Any target variable can be modeled with a combination of variables of the sewer system and exogenous variables, such as rainfall, without an explicit model of the sewer system, as it would be required for classical hydrological systems. These models enable accurate and flexible modeling of sewage treatment facilities to proactively manage and redistribute the load, thus preventing overflows and mitigating their impacts [7]. Such predictive capabilities are crucial for timely interventions and informed decision-making in urban water management. Moreover, by supporting proactive management of critical urban infrastructure, this approach aligns with the principles of public interest AI, by addressing societal needs [8].\nIt is worth mentioning that there are approaches aiming at combining classical physics-based simulations with the flexibility of DL methods [9]. The challenge with these combinations is that the model architectures require the same detailed knowledge about the sewer system as traditional methods. In addition, modeling the mixed viscosity of wastewater imposes significant complexity on the physical models. Here we focus on a data-driven approach that learns the system dynamics from measurements and is thus readily applicable without building cost-intense digital twins reflecting the physical properties of the sewer system.\nIn this work, we evaluate the performance of various advanced time series models for forecasting the water levels in CSS overflow basins. We explore and compare two approaches to time series forecasting: (1) a global model approach, which incorporates a number of exogenous variables, such as rainfall data, and (2) a local model approach, which relies solely on historical"}, {"title": "2. Data and Preprocessing", "content": "In our study, we utilized data\u00b9 provided by Wirtschaftsbetriebe Duisburg\u00b2. The dataset comprises time series sensory data, including water levels in rain basins and water tanks, energy consumption of pumps, and rainfall amounts. The sensor data were collected from six locations in Duisburg's Vierlinden district, covering three years, from January 1, 2021, at 00:00 AM until January 1, 2024, at 00:00 AM. The recording intervals are irregular as the sensors were read out event-based, with sensor update intervals ranging from 1 second to 1 hour.\nTo standardize the data, we resampled it by calculating the mean values closest to each full-hour mark. This resampling procedure resulted in a total of 26,280 data points, with each data point comprising 35 features derived from the sensory data across the different locations. For missing values in the rainfall measurements, we utilized data from the nearest weather station of the Deutsche Wetterdienst\u00b3, specifically the station in Duisburg-Baerl, located 4.5 km from the sewage treatment plant that recorded the rainfall. For the other features, linear interpolation was employed to fill the missing values. Additionally, an indicator column was added for each feature with missing values to denote whether the corresponding value was interpolated."}, {"title": "3. Methodology", "content": "This section details the time series models employed in this study, including the selection, implementation, and comparisons of global and local model approaches."}, {"title": "3.1. Neural Network Architectures", "content": "For our empirical evaluation, we selected six state-of-the-art neural time series models. While classical regression models, such as tree-based methods, can be effective for time series data, the state of the art in water modeling increasingly relies on neural network models [13]. We thus focus on these models based on their effectiveness and versatility in forecasting tasks. The selected models are:\n\u2022 LSTM [11]: LSTM networks are well-suited for capturing long-term dependencies in sequential data, making them ideal for time series forecasting.\n\u2022 DeepAR [14]: This probabilistic forecasting model leverages autoregressive recurrent networks, providing robust predictions with uncertainty estimates.\n\u2022 Neural Hierarchical Interpolation for Time Series Forecasting (N-HiTS) [15]: As a neural hierarchical time series model, N-HiTS excels in capturing complex temporal patterns.\n\u2022 Transformer [16]: Originally designed for nature language processing [17], Transformers use attention mechanism to effectively capture relationships across different time steps [16].\n\u2022 Temporal Convolutional Network (TCN) [18]: TCNs can model long-range dependencies in time series data while being computationally efficient.\n\u2022 TFT [12]: TFT combines the strengths of LSTM and attention mechanisms to provide interpretable and accurate forecasts.\nThese models were implemented using the Darts\u2074 Python library, which offers a user-friendly interface for time-series forecasting. PyTorch was used as the supporting framework."}, {"title": "3.2. Global vs Local Model Approach", "content": "Our study considers two approaches to time series forecasting, each motivated by distinct real-world scenarios.\n1. Global Model Approach: This approach corresponds to the scenario of normal CSS operation, where all sensors are fully operational, and all data can be transmitted reliably over the network. In this case, all available data, including exogenous variables such as rainfall data, are integrated into a single model for forecasting the relevant target variable. This approach, referred to as the global model, allows the models to leverage additional contextual information to improve forecasting precision.\n2. Local Model Approach: This approach is designed for scenarios where not all data is available. Such circumstances can arise when sensors are damaged or network connections are unstable due to extreme weather events, or security incidents. To mimic these cases,"}, {"title": "4. Experiments", "content": "This section introduces the experimental settings, including data splits, model development, and error metrics."}, {"title": "4.1. Datasets", "content": "The dataset was divided into training, validation, and testing sets. The first two years of data were used for training and validation, while the last year was reserved for testing. Within the initial two years, 80% of the data was allocated for training and 20% for validation. Given the sequential nature of time series data, the data were not shuffled, and the split was performed in chronological order to maintain temporal dependencies. To ensure consistency across features, standard scaling was applied using default parameters ($\\mu = 0, \\sigma^2 = 1$) prior to model training.\nAfter fine-tuning (see Appendix A), we determined that a 72-hour input sequence was optimal for forecasting a 12-hour prediction sequence. This prediction sequence was determined together with domain experts to meet operational requirements."}, {"title": "4.2. Model Development", "content": "To prevent overfitting, early stopping with patience of 10 epochs was adopted. All models were optimized using the Adam optimizer [20] with 32-bit floating point precision. Training sessions were conducted on an NVIDIA A100 with 40GB VRAM, utilizing CUDA 12.2 and Python 3.10. Hyperparameter optimization was performed using the Tree-structured Parzen Estimator algorithm provided by the Optuna library. Each model had a training budget of 600 trials, with each trial consisting of 100 epochs. Further details on the hyperparameter optimization process are provided in Appendix A. The best hyperparameter configuration for each model was then evaluated using 100 different random weight initializations to ensure robust comparisons."}, {"title": "4.3. Error Metrics", "content": "We compared various error metrics well established in the field of time series forecasting [21], including MSE and Mean Absolute Percentage Error (MAPE). For model training, MSE was selected as the loss function, except for the probabilistic model DeepAR, which utilized the"}, {"title": "5. Results", "content": "Figure 2 shows the distribution of test MSE and MAPE across the 100 random weight initializations for each model type and approach. It is evident in both figures that the LSTM and DeepAR models have a larger spread in the results compared to the other models."}, {"title": "6. Conclusion and Future Work", "content": "Our results demonstrate that DL models can accurately predict the complex dynamics of wastewater levels in real-world scenarios. Global models, with full access to all sensor readings under normal operation without network outage, exhibit high forecast precision for wastewater levels in the overflow basin. This enhanced precision can significantly aid sewage treatment facilities in effectively redistributing the load of the CSS.\nIn contrast, local models perform worse in forecasting precision than global models. The reason could be the heavy concentration of target values around the mean. Due to sudden changes after longer periods of stability, the local models struggle with longer forecasting periods. However, local models can serve as a fallback in the event of a network interruption where exogenous variables become unavailable. Our results indicate that even when all network connections are lost, and only historical sensor readings of an individual sensor are available, adequate forecasts can still be made. Additionally, due to their lower computational costs, it is worthwhile to explore the potential of deploying the local models on edge devices in the future."}, {"title": "A. Hyperparameter Optimization", "content": "Hyperparameter optimization was performed using the Tree-structured Parzen Estimator algorithm provided in the Optuna library. The optimization process was conducted in two iterations with a total of 600 trials:\n1. Broad Search with 500 trials: An extensive hyperparameter search space will be explored to identify potential optimal values.\n2. Refined Search with 100 trials: Based on the results of the broad search, a more focused and fine-grained search will be conducted around the best-performing hyperparameters."}]}