{"title": "Advancing Comprehensive Aesthetic Insight with Multi-Scale Text-Guided Self-Supervised Learning", "authors": ["Yuti Liu", "Shice Liu", "Junyuan Gao", "Pengtao Jiang", "Hao Zhang", "Jinwei Chen", "Bo Li"], "abstract": "Image Aesthetic Assessment (IAA) is a vital and intricate task that entails analyzing and assessing an image's aesthetic values, and identifying its highlights and areas for improvement. Traditional methods of IAA often concentrate on a single aesthetic task and suffer from inadequate labeled datasets, thus impairing in-depth aesthetic comprehension. Despite efforts to overcome this challenge through the application of Multi-modal Large Language Models (MLLMs), such models remain underdeveloped for IAA purposes. To address this, we propose a comprehensive aesthetic MLLM capable of nuanced aesthetic insight. Central to our approach is an innovative multi-scale text-guided self-supervised learning technique. This technique features a multi-scale feature alignment module and capitalizes on a wealth of unlabeled data in a self-supervised manner to structurally and functionally enhance aesthetic ability. The empirical evidence indicates that accompanied with extensive instruct-tuning, our model sets new state-of-the-art benchmarks across multiple tasks, including aesthetic scoring, aesthetic commenting, and personalized image aesthetic assessment. Remarkably, it also demonstrates zero-shot learning capabilities in the emerging task of aesthetic suggesting. Furthermore, for personalized image aesthetic assessment, we harness the potential of in-context learning and showcase its inherent advantages.", "sections": [{"title": "Introduction", "content": "As artificial intelligence evolves, there's a growing demand for agents to mimic human perception and exhibit emotional responses to their surroundings. IAA emerges as a key area within this scope, and gauges images' aesthetic appeal akin to human judgment. Its complexity lies in its subjectivity, governed by factors like photographic subjects and personal experiences, which makes IAA a challenging endeavor.\nIn the last decade, IAA has been concretized into a variety of tasks. EAT (He et al. 2023b) predicts aesthetics based on a single human-assigned score per image\u2014a task known as Aesthetic Scoring (AS). Meanwhile, CWS (Ghosal et al. 2019) assesses an image's aesthetic appeal directly through language, which is referred to Aesthetic Commenting (AC).\nRecently, Personalized Image Aesthetic Assessment (PIAA) has emerged as a burgeoning field, which aims to predict an individual's aesthetic preferences based on his historical image scoring. While effective in certain scenarios, approaches focusing on a single task often fail to address linkages between different tasks, suffering from overfitting to specific tasks. This realization has inspired us to prioritize holistic aesthetic analysis and comprehension in our research efforts.\nRecently, MLLMs have demonstrated strong comprehension and reasoning abilities across various domains. Models such as VILA (Ke et al. 2023), Q-Align (Wu et al. 2023), and UNIAA (Zhou et al. 2024) have also attempted to utilize MLLMs for IAA to compensate for perceptual and reasoning processes. However, two major obstacles limit their effectiveness. First, these models rely solely on semantic features, neglecting a wealth of valuable aesthetic information. Second, despite efforts by Q-Align and UNIAA to construct aesthetic question-answer pairs for enhancement, the scarcity of labeled data and the presence of potentially mislabeled data continue to restrict their performance. Consequently, integrating comprehensive aesthetic information into MLLMs and developing a refined learning strategy to accurately leverage massive image data are essential.\nIn this paper, we propose Comprehensive Aesthetic Large language Model (CALM) which excels in various IAA tasks and demonstrates deep aesthetic comprehension and analytical skills in dialogues. Fig. 1 illustrates the functional differences between CALM and other IAA models.\nInspired by popular MLLMS, CALM incorporates a visual encoder, a Multi-scale Feature Alignment Module (MFAM) and a Large Language Model (LLM). Recognizing that mainstream visual encoders and LLMs excel at feature extraction and language expression, we have focused our efforts on the MFAM to ensure that the subsequent LLM can fully leverage a broader spectrum of aesthetic information provided by the visual encoder. To achieve this, we introduce a multi-scale text-guided self-supervised learning technique.\nSpecifically, the MFAM is designed to structurally access aesthetic features at multiple levels, while text-guided self-supervised learning enables the MFAM to benefit from unlabeled data. Unlike previous aesthetic self-supervised approaches that rely on score pseudo-labels, our method uses attribute-related textual pseudo-labels. This change ensures accurate learning and simplifies the integration of pseudo-labels when superimposing multiple augmentations on a single image. Additionally, we utilize a wider range of image augmentations, from low-level to high-level, to guarantee that more aesthetic elements are captured and learned.\nTo enhance holistic aesthetic insight, we developed various instruct-tuning techniques to adapt CALM to common aesthetic tasks, ultimately outperforming other approaches in AS, AC, and PIAA tasks. Moreover, CALM achieves comparable PIAA results through in-context learning at runtime, establishing a new paradigm for PIAA. Additionally, we are the first to define the aesthetic suggesting task, and CALM's zero-shot success in this task demonstrates its ability to grasp and comprehend aesthetic principles effectively.\nThe contributions of our work are concluded as follows:\n\u25c7 We propose CALM, a cutting-edge multi-modal large language model specialized in comprehending image aesthetics. Our extensive experiments demonstrate that CALM sets a new benchmark for AS, AC, and PIAA tasks.\n\u25c7 We have pioneered a multi-scale text-guided self-supervised learning technique that not only ensures multi-scale perception for MLLMs, but also effectively and efficiently leverages abundant unlabeled images for enhancement.\n\u25c7 The remarkable zero-shot capabilities of CALM are explored, particularly in in-context PIAA and providing aesthetic suggestions. These capabilities demonstrate CALM's comprehensive aesthetic insight and analytical prowess."}, {"title": "Related Work", "content": "Image Aesthetic Assessment involves algorithms that measure the visual appeal of images. Initially, convolutional neural networks (CNN) and transformers have been leveraged to refine aesthetic score predictions, such as TANet (He et al. 2022), ResNext (Hou et al. 2022b), DAT (Xia et al. 2022) and MaxViT (Tu et al. 2022). In order to regulate aesthetic features to refine scoring, Comm (Niu et al. 2022) and AesCLIP (Sheng et al. 2023) harness textual data and CLIP (Radford et al. 2021), respectively. Besides, language generation models for AC task have also emerged, such as Yeo (Yeo et al. 2021). Moreover, realizing the importance of personal tastes, models and the FLICKR-AES dataset (Ren et al. 2017) for PIAA are gaining traction. However, previous methods usually concentrate on a single aesthetic task so that they can barely really understand aesthetics.\nMulti-modal Large Language Models achieve image content analysis by integrating visual features in LLMs. LLaVA-1.5 (Liu et al. 2023) and mPLUG-Owl2 (Ye et al. 2024) have showcased impressive image reasoning skills. In the realm of IAA, VILA employs CoCa (Yu et al. 2022) to explore zero-shot aesthetic judgement, while Q-ALIGN directly utilizes the original mPLUG-Owl2. UNIAA leverages ChatGPT to generate comments to fine-tune LLaVA-1.5. However, these methods do not modify the pre-existing MLLMs and rely on a limited number of constructed data, which may prevent a comprehensive aesthetic understanding. Consequently, it is vital to improve the structural design and functional learning for deeper aesthetic comprehension.\nMulti-scale Aesthetic Perception is a key approach for promoting IAA. (Chen et al. 2020) combined multi-level spatial features and employed adaptive dilated CNNs, while Comm designed a module to process multi-scale features. EAT and ICAA (He et al. 2023a) incorporated interest points and delegate transformers, aligning better on specific scales. Drawing on these observations, we develop a technique for MLLMs that harnesses multi-scale features effectively.\nSelf-supervised Learning seeks to leverage large quantities of unlabeled data and artificially assigned pseudo-labels to enhance models' generalization. In IAA, where expert annotation is often costly, self-supervised methods are particularly prevalent. (Sheng et al. 2020; Pfister et al. 2021) intuitively assigned lower aesthetic scores to augmented images for contrastive learning, generating score pseudo-labels. However, due to the still ambiguous factors influencing aesthetics and cases where depth-of-field blur can enhance aesthetic appeal, these methods risk producing inaccurate pseudo-scores. Moreover, these methods primarily focus on low-level data augmentations and require separate classifiers to regress scores, limiting their effectiveness."}, {"title": "Methodology", "content": ""}, {"title": "The Architecture of CALM", "content": "As represented in Fig. 2, CALM is composed of three principal elements: a visual encoder $g(\u00b7)$ transforming an image $X$ into a sequence of visual tokens $Z_v = g(X)$; an MFAM $W(\u00b7)$ converting visual tokens $Z_v$ into vision-language tokens $H_v = W(Z_v)$; an LLM $f(\u00b7)$ that receives the vision-language tokens $H_v$ and user instructions $X_q$ to produce the relevant language responses $X_a = f(H_v, X_q)$.\nReferring to most MLLMs, we employ the open-sourced ViT-L/14 as $g(\u00b7)$ and Vicuna-7B (Chiang et al. 2023) as $f(\u00b7)$ without any modification. For our proposed MFAM, we detail its structural design in Sec. 3.2 and its functional promotion via text-guided self-supervised learning in Sec. 3.3. Subsequently, we show how CALM simultaneously addresses various IAA tasks through two-stage instruct tuning in Sec. 3.4. What's more, only regression loss is employed to reduce the gap between $X_a$ and the ground truth $X_{gt}$."}, {"title": "Multi-scale Feature Alignment Module", "content": "(Jin et al. 2019) have revealed that image clarity and color schemes are encoded in lower-level features, while composition and impression requires higher-level features for interpretation. Although multi-scale features have been broadly explored in IAA, MLLMs, which typically process tokens from the last several layers of the visual encoder, lacks a structural basis for handling multi-scale features. Hence, we design the MFAM to emphasize multi-scale information.\nWe define four levels based on their positions in $g(\u00b7)$, from shallow to deep sequentially named as low-, middle-, high-, and thematic-level. To preserve the original reasoning ability, we utilize fully connection to yield thematic-level features $H_{thematic} \\in \\mathbb{R}^{N \\times d_l}$, where $N$ and $d_l$ are the number of vision tokens and the dimension of language tokens, respectively. And then, three two-layer Qformers (Li et al. 2023) are introduced, which use cross attentions to make learnable queries pinpoint aesthetic features at the targeted levels. With $g(\u00b7)$ offering 24 hidden state layers, we strategically tap into the 4th, 12th, and 24th layers to compute low-level features $H_{low} \\in \\mathbb{R}^{N_{low} \\times d_l}$, middle-level features $H_{middle} \\in \\mathbb{R}^{N_{middle} \\times d_l}$, and high-level features $H_{high} \\in \\mathbb{R}^{N_{high} \\times d_l}$, where $N_{low}$, $N_{middle}$ and $N_{high}$ denote the number of learnable queries at each level. The design of MFAM makes it effective and efficient to capture key aesthetic features, considering that the number of queries is much smaller than that of visual tokens."}, {"title": "Text-guided Self-supervised Learning", "content": "For the purpose of effectively unlocking the potential of abundant unlabeled image data to accurately enhance aesthetic perception, we propose text-guided self-supervised learning, which offers the following three advantages.\nFirstly, we use accurate attribute pseudo-labels to replace flawed score pseudo-labels for self-supervision. Concretely, we introduce various image augmentation algorithms targeting attributes mentioned in (Jin et al. 2019), such as color and subject. During training, unlabeled images are randomly augmented in certain attributes and assigned the corresponding attribute pseudo-labels. For instance, if an image is blurred, its attribute pseudo-label is \"the blurred image\".\nSecondly, we leverage a broader spectrum of data augmentations compared to previous aesthetic self-supervised methods. These include low-level augmentations such as blurring and brightness adjustments, as well as high-level augmentations like cropping and masking significant objects. Details of all augmentations and their corresponding pseudo-labels can be found in the Appendix A. Subsequent experiments confirm that these image augmentations significantly enhance aesthetic insight.\nThirdly, we employ GPT-3.5 to generate various textual contrastive pseudo-labels, which eliminates the need for specialized classifiers in (Jin et al. 2019). Two examples are provided in the self-supervised pre-training part in Fig. 3. Additionally, multiple augmentations can be applied simultaneously with their textual pseudo-labels conveniently spliced into a cohesive target, increasing both the data volume and the variety of contrastive learning. For instance, if an image is blurred and added noise, the pseudo-label would be, \"The first image is blurrier and noisier than the second\"."}, {"title": "Comprehensive Aesthetic Assessment", "content": "To achieve comprehensive aesthetic insight, we employ two-stage instruct tuning to adapt CALM to various aesthetic tasks, such as AS, AC, and PIAA. Specific instruction examples are shown in Fig. 3. The complete training cycle, illustrated in Fig. 4, encompasses pre-training and fine-tuning.\nThe pre-training stage consists of two parts that can be launched simultaneously. Self-Supervised Pre-Training encourages the three Qformers in the MFAM to learn aesthetic attributes in a self-supervised manner, utilizing unlabeled images from diverse sources, including AVA (Murray et al. 2012), AADB (Kong et al. 2016), EVA (Kang et al. 2020), ICAA, PCCD (Chang et al. 2017), pexels (Pfister et al. 2021), SPAQ (Fang et al. 2020) and TAD66K (He et al. 2022). To refine the learning process, augmentations on quality and color are designed to optimize $H_{middle}$ and $H_{low}$, while those on topics and composition benefit $H_{middle}$ and $H_{high}$. Generic Pre-Training focuses on training the fully connection to align $H_{thematic}$, considering the value of generic knowledge for IAA (Ke et al. 2023). The training data comprises a 558K subset of LAION-CC-SBU (Schuhmann et al. 2022; Changpinyo et al. 2021; Saleh and Elgammal 2015) and ShareGPT4V (Chen et al. 2023).\nThe fine-tuning stage consists of two task-specific processes that fine-tune MFAM and LLM concurrently. Aesthetic Commenting Fine-Tuning uses the AVA-Captions dataset (Ghosal et al. 2019) to address AC task. Aesthetic Scoring and PIAA Fine-Tuning follows the aesthetic commenting fine-tuning, based on the insight from VILA that mastering AC can bolster effectiveness in AS. We use the AVA dataset for AS and the FLICKR-AES dataset for PIAA.\nHaving progressed through the two training stages, we are thrilled to find that CALM exhibits a strong aesthetic insight, primarily in the ability to accomplish some zero-shot activities such as giving aesthetic suggestions and conducting in-context PIAA. We highlight an examples of this in Fig. 3 and share more detailed experimental results later."}, {"title": "Experiments", "content": ""}, {"title": "Experimental Settings", "content": "Datasets. The AVA dataset comprises over 250,000 images with scores rated by users on the DPChallenge website. We used the official split, designating 19,928 images as the test set and the remainder for training. The AVA-Captions dataset contains approximately 230,000 images, each with an average of 5 user comments. To prevent data leakage, images from the AVA test set are excluded from AVA-Captions training, resulting in 210,000 images for training and 9,361 for testing. FLICKR-AES includes 35,263 images rated by 173 annotators in the training set and 4,737 images evaluated by 37 annotators in the test set, along with user identifications. Additionally, during the pre-training stage, around 460,000 unlabeled images and approximately 660,000 generic image-text pairs were utilized. Notably, we can further expand the unlabeled images as needed.\nImplementation Details. The input resolution for $g(\u00b7)$ is 224, and N = 256 visual tokens are processed, each with a dimension of $d_l$ = 1024. In subsequent experiments, we set $N_{low}$, $N_{middle}$ and $N_{high}$ to 32. The dimension of language tokens is $d_l$ = 4096. Training was conducted on eight 80GB A100 GPUs, utilizing the Adam optimizer. The peak learning rate was set to 1e-3 for the pre-training stage, and 2.5e-5 and 7e-5 for the two processes in the fine-tuning stage, respectively. Both stages commenced with a linear warm-up, followed by a cosine annealing schedule, with durations of 5 hours and 16.5 hours, respectively.\nEvaluation Metrics. For AS, we use Spearman Rank-order Correlation Coefficient (SRCC) and Pearson Linear Correlation Coefficient (PLCC) as metrics. SRCC and PLCC measure the ranking accuracy and linear correlation between the predictions and the ground truth, respectively. For AC, we employ BLEU, ROUGE, CIDEr, and METEOR. BLEU and ROUGE focus on the precision of generated words. CIDEr underscores semantic alignment. METEOR accounts for both semantic and structural similarity. For PIAA, SRCC is adopted again as the primary metric.\nAesthetic Data Extension. CALM strictly followed the procedure outlined in Sec. 3.4 for a fair comparison. Besides, we enhanced CALM by using a number of generic"}, {"title": "Ablation Study", "content": "Does the MFAM help? To investigate this, we conducted trials by maintaining different Qformers within the MFAM and compared their effects on the AS task. Tab. 5 presents the comparative results. Our baseline maintained only the thematic-level projection, closely resembling LLaVA-1.5. However, with the addition of each layer of Qformers, a notable improvement was observed\u2014a boost by 0.061 in PLCC and 0.056 in SRCC. This clearly demonstrates the necessity of aligning features across all three levels.\nHow many aesthetic queries in Qformers are optimal? Intuitively, the more the aesthetic queries, the higher the accuracy and computational cost will be. To explore the tradeoff between effectiveness and performance, we conducted an ablation study on the AC task. As shown in Fig. 6, with #queries increasing, the effect improves rapidly at first and then stabilizes when the number reaches 32. Besides, due to the attention operations in Qformers and the LLM, the overall computational cost increases quadratically, becoming noticeable if #queries is large. Therefore, we opted for 32 queries per Qformer. Of course, having more queries per Qformer may be better if computing resources are sufficient.\nIs the text-guided self-supervised learning useful? To answer this, we removed the self-supervised pre-training from the standard training process for AS task. Evaluation results are shown in the first column of Tab. 7. Compared with the original CALM, CALM without self-supervised learning underperforms by 0.047 in PLCC and by 0.041 in SRCC. Although MFAM structurally facilitates multi-scale feature extraction, the absence of text-guided self-"}, {"title": "Conclusion", "content": "Our study presents CALM, an advanced comprehensive aesthetic large language model. To ensure the extraction of multi-scale aesthetic features both structurally and functionally, we propose the multi-scale text-guided self-supervised learning. Additionally, the instruct-tuning technique is developed to enable CALM to perform multiple aesthetic tasks. Extensive testing reveals that CALM outperforms the current leading approaches across all IAA tasks, solidifying its dominance in the field of IAA. Furthermore, its remarkable zero-shot capabilities in in-context learning PIAA and offering aesthetic suggestions are fully exploited."}, {"title": "Appendix", "content": ""}, {"title": "Image Augmentations in Text-Guided Self-Supervised Learning", "content": "In Table 1, we document the image augmentations, their parameters, and the corresponding guided textual pseudo-labels used during the self-supervised pre-training. To conserve space, we omit the pseudo-labels for more intense augmentations applied to the second image. These augmentations are categorized into three types: degradation of image quality (e.g., Gaussian blur, impulse noise, JPEG compression, pixelate, motion blur, defocus blur), alteration of image color (e.g., brightness, saturation, contrast adjustments), and modification of image content (e.g., subject or nonsubject object masking, foreground or background blurring). Additionally, we provide guided textual pseudo-labels for cases where the same augmentations are applied to both images."}, {"title": "Aesthetic QA Data Construction", "content": "In this section, illustrated in Listings1, we demonstrate how GPT-3.5 can assist in generating aesthetic QA dialogues, using the PCCD dataset as a case study. We start by creating a prompt template that includes variable fields enclosed in angled brackets. Next, we design an example QA dialogue for GPT-3.5 to emulate in style and content. Finally, we replace the placeholders with image-specific information and input this data into GPT-3.5, resulting in a five-round QA dialogue."}, {"title": "Visualization of Aesthetic Comments", "content": "In this section, we evaluate the performance of our CALM and CALM-E models against leading MLLMs in the domain of aesthetic commenting, including GPT-4v, qwen-vl, spark-multi-3, cogvlm, and glm-4v. We use a standard question prompt across all models: \"Please comment on the current image aesthetically\". For illustrative clarity, Fig. 1 through Fig. 8 show each model's output, with comments relevant to aesthetic analysis highlighted in red for easy comparison.\nThe figures reveal that while CALM is capable of generating aesthetically relevant keywords, its responses tend to be concise. In contrast, CALM-E provides comprehensive evaluations, detailing both strengths and weaknesses in each image's aesthetics and offering suggestions for improvement. GPT-4v, qwen-vl, spark-multi-3, and cogvlm predominantly focus on content description, significantly overlooking aesthetic aspects. Although their comments are generally accurate content-wise, the lack of aesthetic insight is a notable shortcoming. glm-4v occasionally produces relevant aesthetic commentary but often fails to address all aesthetic attributes; for example, Fig. 4 and Fig. 5 show it addressing aesthetics primarily from a content-based perspective."}, {"title": "Visualization of Aesthetic Suggestions", "content": "In this section, we evaluate our CALM and CALM-E models against prominent mainstream MLLMs in terms of the aesthetic suggesting task. The comparative panel of MLLMs includes GPT-4v, qwen-vl, spark-multi-3, cogvlm, and glm-4v. Each model was given the identical query: \"Please suggest some aesthetic improvements to this image.\" We present the results in Fig. 9 through Fig. 16. For images subjected to artificial degradation, we indicate the type of degradation in the top right corner. Additionally, we color-code the suggestions: red for those aligning with the standard answer, blue for those deemed highly reasonable upon manual review, and green for those considered highly unreasonable. For unaltered original images, blue highlights very reasonable suggestions, and green notes particularly unreasonable ones upon manual examination.\nThese visualizations reveal that CALM provides succinct but targeted improvement suggestions, effectively identifying key areas for aesthetic improvement. Moreover, CALM-E delivers comprehensive and detailed advice for each image, thoroughly addressing various aesthetic attributes. Models such as GPT-4v, qwen-vl, spark-multi-3, cogvlm, and glm-4v occasionally align with standard recommendations and offer additional plausible suggestions. However, they also frequently propose numerous unreasonable or irrelevant suggestions that do not pertain to aesthetic comments."}]}