{"title": "FoRA: Low-Rank Adaptation Model beyond Multimodal Siamese Network", "authors": ["Wenying Xie", "Yusi Zhang", "Tianlin Hui", "Jiaqing Zhang", "Jie Lei", "Yunsong Li"], "abstract": "Multimodal object detection offers a promising prospect to facilitate robust detection in various visual conditions. However, existing two-stream backbone networks are challenged by complex fusion and substantial parameter increments. This is primarily due to large data distribution biases of multimodal homogeneous information. In this paper, we propose a novel multimodal object detector, named Low-rank Modal Adaptors (LMA) with a shared backbone. The shared parameters enhance the consistency of homogeneous information, while lightweight modal adaptors focus on modality unique features. Furthermore, we design an adaptive rank allocation strategy to adapt to the varying heterogeneity at different feature levels. When applied to two multimodal object detection datasets, experiments validate the effectiveness of our method. Notably, on DroneVehicle, LMA attains a 10.4% accuracy improvement over the state-of-the-art method with a 149M-parameters reduction. The code is available at FORA.", "sections": [{"title": "1 INTRODUCTION", "content": "Object detection is a crucial task in computer vision as it involves locating and identifying interested objects [8, 9]. During the detection, only considering one modality has limitations, e.g. visible images may not provide sufficient high chromatic contrast and visual fidelity information in poor visual and other increasingly complex conditions [18, 24, 31, 46]. In contrast, infrared images [17, 19] reflect the surface temperature of objects with temperatures above absolute zero. And they exhibit highlighted contrast information and sharp edge contours, which are less susceptible to changes in visual conditions [27].\nIn this case, the assistance of combining visible and infrared images can gain deep insights into multimodal object detection (MOD) to maximize their respective strengths though modality cooperation [38, 39, 51, 52]. A direct way to perform MOD with deep neural networks (DNNs) is to build a symmetric two-stream backbone where features are extracted independently from each modality. And then these independent multimodal features are fused to complete and enhance the detection performance [5, 38], as shown in Figure 1 (b).\nSuch an independent two-stream backbone is inadequate due to distribution biases of multimodalities, thus resulting in suboptimal solutions. On the one hand, the two modalities (visible and infrared images) have not been learned together making biases of multimodal features larger. Therefore, well-designed fusion and interaction strategies are need to retain the consistency of homogeneous information, such as Cross-modal Conflict rectification module (CCR) in [15], Differential Modality Aware Fusion module (DFMA) in [58], and Inter-modality Cross-Attention module (ICA) in [50]. On the other hand, significant and substantial increases in the parameters of additional backbone and fusion module are observed when integrating the unimodal model. For instance, YOLOv81 [23] (44.48M) requires at least one backbone for each additional modality, which results in a parameter increase of 19.78M (44.47%).\nIn this paper, we propose a novel multimodal object detector named Low-rank Modal Adaptor (LMA) to address distribution biases of multimodalities while emphasizing lightweight. Different from the previous methods which extract multimodal respective information with completely independent network parameters, LMA introduces identical network parameters to extract homogeneous information and distinct parameters to focus on heterogeneous information. Concretely, we employ a shared backbone to extract the modality-shared features while lightweight modal adaptors for characteristics unique to each modality. Due to the shared parameters, the identification of objects is more uniform because the data distribution bias of homo-information is greatly reduced as illustrated in Figure 1, thus avoiding complex design of fusion and interaction modules. Considering that varying heterogeneity in multimodal features at different network levels seriously affects the choice of parameter counts in adaptors, we employ singular value decomposition (SVD)-like low-rank matrices as our adaptors and design an importance-aware training strategy to dynamically allocate the matrix ranks. Note that adaptors with low-rank matrices own few parameters, which means the network solely introduces little computational overhead for each additional modality. Our contributions can be summarized as follows:\n\u2022 We propose a novel detector named LMA to tackle the issue of large data distribution bias between two modalities. To our best knowledge, our structure is the first work that combines a shared backbone and lightweight modal adaptors for multimodalities, enabling more effective extraction of both homogeneous and heterogeneous information.\n\u2022 To meet the demand for varying parameter counts of adaptors across different multimodal feature levels, we design an adaptive matrix rank allocation strategy that incorporates joint gradients and singular values as importance metric.\n\u2022 We conduct experiments on two multimodal object detection benchmarks: DroneVehicle [46] and LLVIP [20]. We prove the large data distribution bias of multimodal features and"}, {"title": "2 RELATED WORKS", "content": "2.1 Visible Object Detection\nObject Detection on visible images is a pivotal and extensively utilized task in computer vision. With the publication of numerous benchmark datasets such as Pascal VOC [11], ADE20K [57] and COCO-Stuff [2], object detectors based on deep neural network have been extensively researched. The detection models can be roughly divided into single-stage detectors and two-stage detectors. Two-stage detectors, such as Fast R-CNN [13], Faster R-CNN [43], can achieve high precision through coarse localization and fine feature extraction of region of interest (RoI), while the model sizes are generally large. In contrast, the structure of the single-stage detectors, such as RetinaNet [30], SSD [33] and YOLO-series detectors [1, 6, 7, 12, 22, 40-42], are more lightweight but less accurate. In these methods, Convolutional Neural Networks (CNN) are typically used to construct the backbones. Among various object detection algorithms, single-stage detector YOLOv8 [23] strikes an excellent balance between accuracy and the count of model parameters. Therefore, we have chosen it as our unimodal model.\nOriented Object Detection refers to using rotated bounding boxes to represent detection results, which can provide more accurate location of objects. Detectors based on horizontal bounding box are plagued by multiple objects of interest in one anchor. Therefore, many researches about oriented object detection [10, 21, 34, 45, 60] have been presented to alleviate this problem. Oriented R-CNN [48] adjusts receptive fields of neurons in accordance with object shapes and orientations. S2Anet [14] adopts alignments between horizontal receptive fields and rotated anchors to get a better feature representation. In this paper, we replace the detection head of YOLOv8 with an oriented bounding box (OBB) detection head to construct an oriented object detector.\n2.2 Multimodal Object Detection\nMultimodal object detection based on visible-infrared image pairs has become a promising research field to address the issue that visible images are insufficient for capturing crucial information in poor visual conditions [35, 36, 59]. Current multimodal object detection algorithms concentrate on two primary concerns: (1) how to extract valid information of each modality? and (2) how to effectively fuse the multimodal features to generate fusion features that contain distinct object features and complementary information from both modalities? Zhao et al. [56] coarsely remove interfering information within each modality and design a dynamic feature selection module to finely select desired features for feature fusion. He et al. [15] exam contextual information of analogous pixels to alleviate multimodal information with semantic conflicts, and then fuse multimodal features by assessing intra-modal importance to select semantically rich features and mining inter-modal complementary information. Chen et al. [5] emphasize object region"}, {"title": "3 METHOD", "content": "In this section, we firstly introduce a general formulation of multimodal object detection and point out the limitations of symmetric two-stream backbone networks. Secondly, we detail our detector combining a shared backbone with modal adaptors. We demonstrate the validity of this structure through formulas. Lastly, as our method involves the choices of adaptor parameter counts, we introduce an importance-aware adaptive rank allocation strategy to control the parameter budget.\n3.1 MOD Formulation\n3.1.1 Formulation of MOD. As shown in Figure 1 (a), assume the distribution of each valid information in multimodal images is Gaussian Distribution with a mean \u00b5 and a variance \u03c3\u00b2, denoted as X ~ N(\u00b5, \u03c3\u00b2). The data distribution of every visible-infrared image pair can be represented as follows:\n$$D_{mod} ~ C_{mod}N(\\mu_{mod}, \\sigma_{mod}^2) + \\sum_{i=1}^{n_{mod}} C_iN(\\mu_i, \\sigma_i^2),$$\nmod = {v, t},\n(1)\nwhere the subscripts v and t refer to the visible and infrared modality respectively. $C_{mod}$ denotes the intensities of homogeneous information in each modality, while $\\mu_{mod}$ and $\\sigma_{mod}^2$ are their corresponding mean and variance. Similarly, $C_i$ and $\\mu_i, \\sigma_i^2$ for i = 1, ..., $n_{mod}$ represent the intensities of unique information in each modality and their data distribution properties.\nIn the multimodal object detection, it is desired to generate features that maintain consistency of homogeneous information while minimizing loss of modality-unique information as possible. Therefore, fusion feature maps with ideal data distribution can be represented as follows:\n$$D_f ~ (C_o + C_i)N(\\mu_c, \\sigma_c^2) + \\sum_{i=1}^{(n_o+n_t)} C_iN(\\mu_i, \\sigma_i^2),$$\ns.t. $$\\frac{1}{n_o + n_t} \\sum_{i=1}^{(n_o+n_t)}\\mu_i \u2248 \\mu_c,$$\n(2)\nwhere the merged distribution sums the original intensities of the homogeneous information and generates a new distribution $N(\\mu_c, \\sigma_c^2)$. Meanwhile, all heterogeneous information intensities are preserved to leverage multimodal complementary information. Ideally, the geometric center of their means is approximated as the mean of the homogeneous information, as shown in Formula (2).\n3.1.2 Two-stream Backbone Networks. Deep neural networks (DNN) can be viewed as functions with learnable parameters that maps input data to a high-dimensional feature space. Therefore, the process of feature extraction in two-stream backbone networks can be represented as follows:\n$$E_{mod} = B(D_{mod}; @_{mod})$$\n$$\\sim C_{mod}N (\\mu_{mod}, \\sigma_{mod}^2) + \\sum_{i=1}^{n_{mod}}C_iN (\\mu_i, \\sigma_i^2),$$\n$$\\frac{1}{n_{mod}} \\sum_{i=1}^{n_{mod}} \\mu_i \\approx \\mu,$$\n(3)\nHere, $E$ is the data distribution of modality features and $B$ denotes the mapping function of the backbone with parameters $@_{mod}$. The distribution of input data significantly impacts the training and formation of parameters. In turn, a backbone with specific parameters maps the inputs to corresponding data space. As illustrated in Formula (3) and Figure 1 (b), multimodal features extracted by two-stream backbone will form respective new geometric center $\\mu_{mod}$. This will significantly large the biases of homogeneous information and we prove it in subsequent experiments (as illustrated in Figure 5):\n$$|\\mu_o^* - \\mu| > |\\mu_o - \\mu_c|.$$\n(4)\nTo obtain the ideal fusion features as shown Formula (2) from Formula (3), it is necessary to design efficient fusion functions, which are generally complex and sometimes lack generalization to various datasets. Moreover, two-stream backbones employs completely independent parameters $\\omega_v$ and $\\omega_t$ to extract homogeneous information. Such structures are empirically parameter redundant.\n3.2 Proposed Detector LMA\n3.2.1 Overview of the Framework. The overall architecture of the proposed method is shown in Figure 2. The LMA extends a unimodal model and attaches two lightweight modal adaptors to all feature extraction layers, such as Convolutional, Transformer [47], and Fully Connected layers [28], of the backbone. Adaptor parameters can be merged with corresponding backbone layer to form a new feature extraction layer with the same shape. Details of modal adapters are introduced in Section 3.2.2.\nThe parameters of the shared backbone, visible adaptors and infrared adaptors are denoted as $@_{shared}, @_{vada}$ and $@_{tada}$ respectively. When a modality data is received, a new backbone layer is created by merging shared layer and corresponding modal adaptor to extract modality features, which can be represented as follows:\n$$E_v = B(D_v; @_{vada} + @_{shared}), E_t = B(D_t; @_{tada} + @_{shared}),$$\nwhere shared backbone parameters $@_{shared}$ are influenced by both modalities. When acting in turn on multimodal data, they thus tend to map homogeneous information in different modality to the same feature space. The modal adaptors then focus more on the"}, {"title": "3.2.2 Modal Adaptor", "content": "remaining heterogeneous information:\n$$E_{mod} ~ C' _{mod}N(\\mu'_{mod} o'^2) + \\sum_{i=1}^{n_{mod}} C'_iN (\\mu'_i, \\sigma'_i^2),$$\n$$|\\mu'_o - \\mu' | \\ll |\\mu_o^* - \\mu_c|.$$\nFormulas (6)-(7) demonstrate that the shared parameter structure effectively reduces distribution biases in homogeneous information during feature extraction, as shown in Figure 1 (c). We also experimentally prove this in subsequent experiments that illustrated in Figure 5. Based on Formulas (6)-(7), we can obtain the ideal fusion features data distribution approximating that in Formula (2) through simple addition operation. The method thus gets rid of designs of complex feature fusion mapping functions and enhances robustness of the model.\nIn terms of model scale, the lightweight adaptors introduce only a small increase in parameters to the unimodal model:\n$$S(\\omega_v) = S(\\omega_t) = S(@_{shared}),$$\nand $$S(@_{vada}) = S(@_{tada}) < S(@_{shared}),$$\nso $$S(LMA) = S(@_{vada}) + S(@_{tada}) + S(@_{shared})$$\n<$$S(\\omega_v) + S(\\omega_t) = S(two-stream backbone),$$\nwhere S(.) denotes the count of parameters. Formula (10) means that our method greatly reduces parameter redundancy in multimodal models. In this case, if a new modality m needs to be added, only $S(@_{mada})$ will be introduced to the original model. This is a little addition compared to the computational overhead of the entire model.\n3.2.2 Modal Adaptor. As mentioned in Section 3.2.1, modal adaptors should be lightweight compared to the entire modal and its parameters could be merged with the feature extraction layers of the backbone. Inspired by [16] and [54], we apply SVD-like low-rank matrices as modal adaptors. Concretely, taking a convolutional layer of the backbone as an example, the weight parameter, denoted by $K_{shared} \\in R^{C_1\\times C_2\\times K\\times K}$, is a 4-dimensional tensor, where K"}, {"title": "3.3 Adaptive Rank Allocation Strategy", "content": "R^{r\\times r} contains the singular values {i}1<i<r with inner rank:\nr < min(C1. K, C2 K).\n(12)\nWe further denote Gi = {Pi, di, Qi} as triplet containing the i-th singular value and vectors (as shown in Figure 3, the dark part of P, A, Q represents a triplet). In practice, since A is diagonal, we only need to save it as a vector in Rr.\nTo merge parameters of shared backbone layer and adaptor, we reshape the adaptor matrix to an adaptor convolution kernel $K_{adaptor} \\in R^{C_1\\times C_2\\times K\\times K}$ with the same shape as $K_{shared}$:\n$$K_{modalilty} = K_{shared} + K_{adaptor},$$\n(13)\nsubsequently, the convolutional kernel $K_{modalilty} \\in R^{C_1\\times C_2\\times K\\times K}$ for modal feature extraction is obtained by adding the weights of shared kernel and the adaptor kernel. Following basic theory of convolution [26], modality kernel is equal to the parallel architectural branches illustrated in Figure 4, which enables the layer to focus on different types of information in the input data simultaneously. In addition to convolution layers, adaptor parameter matrices can be reshaped to the shape of various common feature extraction structures. Our method thus could be generalized to any models with common layers.\nThe calculations of parameter counts for the adaptor and the convolutional layer are as follows:\nS(adaptor) = S(P) + S(A) + S(Q)\n= C2. Kr+r.C1K + r\n= r(K(C1 + C2) + 1),\nS(Kshared) = C1 C2\u00b7K\u00b7K,\nS(adaptor) \u2264 S(Kshared),\n(14)\n(15)\n(16)\nwhen $$r < \\frac{C_1 C_2 K.K}{K(C_1+C_2) + 1} \\frac{C_1 + C_2}{C_1 C_2 K}$$\n(17)\nBased on Formula (12) and Formulas (14)-(17), Formula (9) could be easily satisfied.\n3.3 Adaptive Rank Allocation Strategy\nAs mentioned in Section 3.2.1, adaptors are designed for heterogeneous information. In DNNs, shallow features primarily contain detailed information such as texture and contours of images, while deep features mainly include the semantic information of the objects. Therefore, the amount of heterogeneous information in features maps between modalities varies with feature levels. We prove this in subsequent experiments as shown in Figure 7. The importance of adaptor matrices thus varies significantly across blocks and layers. Adding more trainable parameters to the critical adaptor matrices can lead to better model performance. In contrast, adding more parameters to those less important matrices yields marginal gains or even hurts model performance. The parameters of the adaptor depend on the inner rank, which is the number of triplets Gi in the SVD matrix. Setting ranks evenly to all adaptor matrices/layers thus often leads to suboptimal performance.\nTo address this problem, we propose a training strategy to adaptively allocate rank of adaptor based on importance of SVD matrix triplets under limited parameter budget. We define parameter budget b(it) as the total inner rank of all adaptors, where it = 0, . . ., T represents the index of training step. We set the initial rank of each adaptor as r = b(0) /n, where n is the number of adaptors and gradually reduce b(it) to target budget b(T) during training. We iteratively prune singular values by setting the singular value in triplet of low importance to zero, which will discard the entire triplet, to achieve corresponding budget.\nFor clear reference, we use k to index the adaptor, i.e., Mk = PkAkQk for k = 1,...,n and denote the i-th triplet of Mk as Gk,i = {Pk,*i, Ak,i, Qk,i*} and its importance score as ISk,i. Previous methods [3, 25] apply the magnitude of singular values to quantify the importance of every triplet or use the gradient values of the training loss to quantify the importance of every parameter [29, 37, 44, 55]. We combine the both metrics to determine the basic importance I() of each entry in the adaptor triplets:\n$$I(P_{k,ij}) = I(Q_{k,ij}) = I(@_{k,ij}) = |\\nabla w_{k,ij} L(P, A, Q)|,$$\n(18)\n$$I(A_{k,i}) = |A_{k,i} \u00b7 \\nabla A_{k,i} L(P, A, Q)|,$$\n(19)\nwhere @k,ij is any trainable parameter in lift and right singular vectors. L() is the loss of the task, which contains categorical loss and regression loss in object detection, and \u2207 denotes the gradient operation. Here, importance of singular vectors is determined only by gradients of loss, while that of singular vales is quantified by the magnitude of gradient-weight product. However, such a score in Equations (18)-(19) is estimated on the sampled mini batch. The stochastic sampling and complicated training dynamics incur high variability and large uncertainty for estimating the importance. Therefore, we utilize the importance smoothing and uncertainty quantification in [55] to resolve this issue:\n$$\\bar{I}^{(it)} = \\beta_1 \\bar{I}^{(it-1)} + (1 - \\beta_1)I^{(it)},$$\n(20)\n$$\\tilde{U}^{(it)} = \\beta_2 \\bar{U}^{(it-1)} + (1 - \\beta_2)|I^{(it)} - \\bar{I}^{(it)}|,$$\n(21)\nwhere 0 < \u03b21, \u03b22 < 1. $\\bar{I}^{(it)}$ is the smoothed importance by exponential moving average and $\\tilde{U}^{(it)}$ is the uncertainty term quantified by the local variation between $I^{(it)}$ and $\\bar{I}^{(it)}$. Then the importance score of each entry is defined as the product between $\\hat{I}^{(it)}$ and $\\hat{U}^{(it)}$ and denoted as s():\n$$s^{(it)} = \\hat{I}^{(it)}. \\hat{U}^{(it)}.$$\n(22)\nThe importance score of i-th triplet of Mk can be represented as:\n$$IS_{k,i} = s(A_{k,i}) + \\frac{1}{C_2\u00b7K} \\sum_{j=1}^{C_2-K} s(P_{k,ji}) + \\frac{1}{C_1\u00b7K} \\sum_{j=1}^{C_1-K} s(Q_{k,ij}),$$\n(23)"}, {"title": "4 EXPERIMENTS", "content": "where we employ the mean importance of $P_{k,*i}$ and $Q_{k,i*}$ as its importance score such that ISki does not scale with the number of parameters in triplet.\nAll trainable parameters of adaptors can be denoted as P = {$P_k$}$x=1$, A = {Ak}k=1 and Q = {Qk}=1\u00b7 At the i\u2212th step, we first update the trainable parameter by gradient descent:\n$$P_k^{(it+1)} = P_k^{(it)} - \\eta\\nabla_{P_k}L(P, A, Q);$$\n(24)\n$$A_k^{(it+1)} = A_k^{(it)} - \\eta\\nabla_{A_k}L(P, A, Q),$$\n$$Q_k^{(it+1)} = Q_k^{(it)} \u2013 \\eta\\nabla_{Q_k}L(P, A, Q),$$\n(25)\nwhere \u03b7 > 0 is learning rate. Then, given importance score of triplet $IS_k^{(it)}$ that contains all adaptor martrices, the singular values are pruned following:\n$$A_{k,i\u2019}^{(it+1)} = \\\nA_{k,i\u2019}^{(it)}, if IS_k^{(it)} is in the top \u2013 b^{(it)} of IS^{(it)}\n\\\n0, otherwise$$\n(26)\nIn this way, we adaptively allocate higher rank to more important adaptors under a limited parameter budget.\n4 EXPERIMENTS\n4.1 Dataset and Evaluation Metrics\nDroneVehicle Dataset [46] is a large-scale drone-based visible-infrared dataset that contains 953,087 vehicle instances in 56,878 images with oriented bounding boxes for five categories (car, bus, truck, van and freight car). The dataset is collected using drone platform under different scenes and lighting conditions. The whole dataset is split into a training set, a validation set and a test set.\nLLVIP Dataset [20] is a very recently released visible-infrared paired pedestrians dataset which is collected in low-light environments. And most of the images were captured in very dark scenes. The dataset contains 16,836 image pairs with horizontal boxes for one category (person).\nEvaluation Metrics. We employ the COCO-style metric of the mean average precision (mAP) to assess the accuracy of MOD. For mAP, an Intersection over Union (IoU) threshold such as 0.5 (denoted as mAP@0.5), 0.5-0.95 (denoted as mAP) is used to calculate True Positives (TP) and False Positives (FP). Meanwhile, we utilize the parameter counts (Megabyte, denoted as M) to characterize the size of models, along with its computational and storage overheads.\n4.2 Implementation Details\nWe select YOLOv81 as the unimodal model. We use oriented detection head for DroneVehicle dataset while horizontal detection head for LLVIP. All detectors are trained for 50 epochs with a batch size of 8 on an NVIDIA A100-SXM-80GB GPU. We optimize the training process using Adam algorithm. The input image size is set to 640 \u00d7 640. We set the initial average rank to 9 and the target average rank to 6. We warm up the training for 8 epochs, where the rank budget remains constant, and then follow a cubic schedule to decrease the budget until it reaches target budget in the 25-th epoch. After that, we fix the rank allocation and train the model for last 25 epochs. All experiments are conducted based on the modified code library Ultralytics [23] and all other hyperparameters is the same as its default settings."}, {"title": "4.3 Comparison with State-of-the-Art Methods", "content": "4.3 Comparison with State-of-the-Art Methods\nComparison of Accuracy on DroneVehicle. We compare unimodal models and our LMA with the recent state-of-the-art methods on the val set of the DroneVehicle (shown in Table 1). Among the single-modal methods, the YOLOv81 outperforms all single-modal detectors in both modalities. Detection with infrared modality generally performs better than detection with visible modality. The YOLOv8l exhibits lower accuracy in detecting 'Truck' in infrared images compared to visible images, while the opposite is true for 'Freight Car'. Among the multi-modal methods, our method utilizes the complementary information from both modalities and achieve the highest accuracy in three categories \u2013 'Car', 'Truck' and 'Freight Car'. Multimodal detector LMA achieves a 4.4% mAP@0.5 higher than the unimodal model. In addition, LMA outperforms the current state-of-the-art method by 10.4% in mAP@0.5!\nComparison of Accuracy on LLVIP. As shown in Table 2, our proposed method is compared with several state-of-the-art detectors namely Cascade R-CNN [4], CFT [38] and CALNet [15]. LMA achieves promising performance on this dataset, outperforming the highest accuracy among all methods by 1.8% mAP. The superior performance on both multimodal object detection datasets demonstrates the effectiveness and robustness of LMA.\nComparison of Network Parameters. As shown in Table 3, previous methods would introduce over 50% of the parameter counts to adapt the inputs of two modalities based on unimodal models. Otherwise, the unimodal models are already of large size. Moreover, if there are complex fusion or interaction modules in multimodal detectors, the parameter increments would reach more than three times of the original models. Among the five methods, LMA is the smallest detector with only a 0.34% parameter increment. We reduce the parameter increment by approximately 1000\u00d7 compared to the state-of-the-art method while still achieving the best performance. This demonstrates the effectiveness of our method in reducing parameter redundancy.\n4.4 Ablation Studies\nAblation experiments are conducted on the DroneVehicle val set. We introduce the Pearson product-moment correlation coefficient (|p|) between two modality feature maps to represent the biases of the multimodal information data distribution.\nAblation on the Structure of LMA. The baseline is a two-stream backbone detector based on YOLOv8l and fused by addition. We calculate the Pearson product-moment correlation coefficient between two modality feature maps of multimodal detectors in the stages that correspond to the P3, P4, P5 in YOLOv81. For LMA, the modal kernels are split into shared kernels and adaptor kernels as shown in Figure 4 to extract the homogenous information and heterogeneous information separately. We thus calculate the distribution biases of features generated by both shared backbone and modal adaptors. The statistics are made on the correlation coefficients |p| between two modalities and illustrate the proportion of different correlation coefficients |p| (0.0 ~ 1.0) as a line chart in Figure 5. There is a significant bias between the data distributions of all the multimodal information extracted by two-steam backbone networks. In contrast, LMA utilizes a shared backbone to enhance the consistency of homogeneous information while preserving the"}, {"title": "4.4 Ablation Studies - Ablation on the Adaptive Rank Allocation Strategy", "content": "4.  ,   and \nnoise. LMA enhances the representation of homogeneous object features in each modality, while suppressing the extraction of heterogeneous noise to some extent. This makes the object information in the fusion features more significant.\nAblation on the Adaptive Rank Allocation Strategy. The baseline is a LMA detector with all adaptor matrices have evenly been set fixed rank to 6. Empirically, if there is a high degree of heterogeneity between two modalities, the data distribution of their feature maps will be biased. We thus utilize the multimodal data distribution biases to represent the amount of heterogeneous information. We extracted the features that would be fused in the three stages, P3, P4 and P5, of the two-stream baseline detector. And the data distribution biases between the multimodal feature maps of each stage are calculated separately. As shown in Figure 7, the data distribution bias is significantly lower in the P5 stage compared to the P3 stage. The experimental results demonstrate that shallow features contain more heterogeneous information than deep features in multimodal networks.\nFigure 8 displays the average adaptor rank for each block in the backbone of the LMA following adaptive rank allocation strategy. As analyzed, LMA tends to allocate higher ranks to blocks of shallow levels such as 'bo', 'b1', 'b2' and 'b3', while allocating lower ranks to deep levels such as 'b6', 'b7', 'b8' and 'b9'. The lower-level blocks all have an average rank higher than the target rank, or even the same as the initial rank of the setup. In contrast, the higher-level blocks have smaller averages, some ('b7' and 'b8') even 0, indicating that the visible and infrared modalities share network parameters in these blocks. The experimental results in Table 4 demonstrate that the adaptive rank allocation strategy mitigates parameter redundancy, while improving the accuracy performance of the detector."}, {"title": "5 CONCLUSION", "content": "In this paper, we theoretically elaborate on the feature fusion process in multimodal two-stream backbone networks from the perspective of data distribution and point out their limitations. To solve this, we propose a novel multimodal detector integrated a shared backbone and lightweight modal adaptors, named LMA, to mitigate the distribution biases between homogeneous information. In addition, we design an adaptive rank allocation strategy to account for the varying heterogeneity in multimodal features across different levels. Extensive experiments are conducted on two multimodal detection datasets. We experimentally prove the large distribution biases and varying heterogeneity in multimodal features. The experimental results demonstrate that LMA achieves higher accuracy compared to two-stream backbone networks with minimal computation overhead. Specifically, on DroneVehicle val set, LMA achieves 10.4% accuracy improvement compared to the state-of-the-art method while achieving a 149M-parameters reduction. In the future, we plan to work on how to fuse heterogeneous information effectively with still little computation overhead, which we have not take into account in this paper."}]}