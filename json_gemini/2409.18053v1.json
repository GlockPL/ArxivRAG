{"title": "DualAD: Dual-Layer Planning for Reasoning in Autonomous Driving", "authors": ["Dingrui Wang", "Marc Kaufeld", "Johannes Betz"], "abstract": "We present a novel autonomous driving framework, DualAD, designed to imitate human reasoning during driving. DualAD comprises two layers: a rule-based motion planner at the bottom layer that handles routine driving tasks requiring minimal reasoning, and an upper layer featuring a rule-based text encoder that converts driving scenarios from absolute states into text description. This text is then processed by a large language model (LLM) to make driving decisions. The upper layer intervenes in the bottom layer's decisions when potential danger is detected, mimicking human reasoning in critical situations. Closed-loop experiments demonstrate that DualAD, using a zero-shot pre-trained model, significantly outperforms rule-based motion planners that lack reasoning abilities. Our experiments also highlight the effectiveness of the text encoder, which considerably enhances the model's scenario understanding. Additionally, the integrated DualAD model improves with stronger LLMs, indicating the framework's potential for further enhancement. We make code and benchmarks publicly available.", "sections": [{"title": "I. INTRODUCTION", "content": "At the current stage of Autonomous Driving (AD), critical and rare scenarios, also called corner cases, are becoming one of the biggest challenges [1]. These cases often require a high level of reasoning ability. In response, researchers have been working to integrate reasoning capabilities into AD systems [2]-[5]. Recently, the emergence of large language models (LLMs) like GPT has provided researchers with another potential tool: leveraging the intelligence exhibited by these models to enhance the reasoning abilities required in autonomous driving. Some pioneering approaches that apply LLMs in autonomous driving have been developed [6]-[9]. However, while these approaches have shown LLMs' reasoning potential for autonomous driving, they haven't included close loop simulation, which is essential to validate a planner's performance. Also, recent approaches primarily focus on replacing current AD systems with LLMs instead of maximizing their full potential of the overall system. To fully maximize the potential of LLMs in autonomous driving systems, we need to explore strategies beyond merely replacing existing components. One promising approach is to draw inspiration from human driving styles.\nUnlike power-intensive computers, the human brain has evolved to be computationally efficient, using roughly 15 watts [10] to perform most daily tasks, including driving cars. Research shows that, for humans, complex driving scenarios are more cognitively demanding than simple ones [11]. Furthermore, human drivers can adapt their attention to meet the demands of different traffic conditions [12], [13]. These studies suggest that human drivers do not necessarily pay full attention to all driving situations; rather, they rely on simple causal relationships to handle regular scenarios. High-level reasoning abilities are used to supervise the environment in a non-intensive manner, intervening primarily when drivers encounter more cognitively demanding situations such as critical, risky, or challenging scenarios.\nBuilding on these insights, we propose an AD framework that mimics human cognitive processes. Our core insight is that we can use LLMs as the reasoning module to check potential dangers and improve the current planners' performance. In this way, we can reduce the inference cost. Concretely, our contributions are twofold:\n\u2022 We develop a rule-based text encoder to convert driving scenario into a format of text description. The experiments shown that with this text encoder, the LLMs tend to have a better understanding of the driving scenario and the integrated model can plan better.\n\u2022 We introduce DualAD, a Dual-layer Autonomous Driving framework designed to replicate the human approach to driving by combining simple rule-based motion planning with an LLM for reasoning about desired velocity. Closed-loop experiments show that even with a weak, zero-shot LLM, our approach significantly improves performance of the rule-based planners."}, {"title": "II. RELATED WORK", "content": "Reasoning in Autonomous Driving. The task of integrating reasoning abilities into Autonomous Driving (AD) systems has been explored in various studies. Scene segmentation has been used to enhance models' semantic understanding [5], [14], [15] of the surrounding environment. However, these methods are perception-oriented and do not significantly enhance planning-oriented reasoning. Esterle et al. [16] treat the task as a combinatorial problem, combining trajectory planning and maneuver reasoning, but complex scenarios pose a significant challenge for this method. Multiple approaches have applied spatial and temporal modules to encode mul- timodal data to extend AD systems' reasoning abilities [2], [3], [17], [18]. However, since these methods solely rely on data-driven approaches, they suffer from data-related issues such as covariate shift and domain adaptation [1]. Kothawade et al. [19] developed a rule-based model to encode the driving scenario into a sequence of text and use a predefined answer set to map the scenario description into a driving decision. However, the reasoning ability of this model is highly limited by the variety of the predefined answer set. In contrast, our approach leverages the flexibility of large language models to reason about complex driving scenarios.\nLarge Language Models in Autonomous Driving. A signif- icant portion of the literature has focused on the application of Vision-Language Models (VLMs) and Large Language Models (LLMs) for driving tasks. Several approaches [6], [7], [20] utilize a visual encoder to parse driving images and build scene descriptions. While Chen et al. [21] trained a scenario encoder to encode the agents' states. Some approaches [8], [22] apply Vision-Language Models for planning to enhance scene understanding. [23] created a dataset of instruction- response pairs to improve scene understanding in multimodal LLMs. GPT-Driver [24] utilizes text descriptions of scenarios as input, allowing the model to directly output planning trajectories. However, all the approaches mentioned above haven't conducted closed-loop simulation to validate the method. And recent work has emphasized the need to reconsider the metrics and evaluation methods for open-loop simulations [25]\u2013[27], and closed-loop evaluation is regarded more correlated with driving quality. Some works incorporate closed-loop simulations [28], [29], albeit in limited and simple scenarios, which did not fully utilize the reasoning capabilities of LLMs. Tian et al. [30] used VLMs to extract information such as weather and traffic signs to suggest driving speed limits, but dynamic agents were not considered during decision-making. Shao et al. [31] trained a scenario encoder and a action decoder alongside a frozen LLM. However, their results are unstable and vary significantly across different scenarios. Our proposed DualAD framework not only uses closed-loop evaluation to examine the method, but also tests the model in complex and critical scenarios.\nHuman-Like Reasoning Style in Driving. The idea of mimicking human cognitive processes in artificial intelligence has been a long-standing goal in the field [32], [33]. Recent approaches have aimed to create more holistic models that replicate broader aspects of human reasoning and adaptability [34]. For example, Kircher et al. [12] and Liu et al. [13] explored how human drivers adjust their cognitive resources depending on the complexity of the driving task. Inspired by these insights, our work introduces a dual-layer framework that mirrors the human tendency to apply higher cognitive effort during complex or dangerous driving scenarios, thus enhancing the system's overall efficiency and safety."}, {"title": "III. METHODOLOGY", "content": "The ultimate goal of our model is to produce a safe trajectory over n seconds in each planning cycle. During each planning cycle, the planner receives a variety of inputs, which include tracking data of nearby objects, the current and historical kinematic states of the ego vehicle, traffic light information, high-definition (HD) maps, speed limits, and the specified route. We first present a rule-based encoder that converts the environment into a text description. Then we introduce the reference path planner and rule-based motion planners, followed by an explanation of how a large language model influences driving decisions."}, {"title": "A. Upper Layer: Convert Driving Scenario into Text", "content": "As illustrated in Fig. 2, [35] proposed that neural networks, when trained with different objectives on diverse data and modalities, converge towards a shared statistical model of reality. This suggests that reasoning results from different modalities (e.g., images, text, etc.) should be equivalent if the model is intensively trained on the corresponding forms. However, considering that the current LLM's capabilities are primarily trained on textual descriptions [36], the reasoning results derived from text are generally regarded as superior. Hence, instead of relying solely on tuples of exact numbers, such as position and velocity states, we aim to design this encoder to be able to give a text description that enable the large language model to achieve a deeper understanding of the semantics of driving scenarios. As shown in Fig. 3, we convert driving scenarios into text description. Each scenario S comprises different types of agent, such as vehicles, pedestrians, traffic objects etc. S is converted into a text description D through the text encoder $f_{text}$ as shown below,\nD = f_{text}(S)\nAll agents are abstracted into states, which are tuples of agent's ID, position, orientation, speed and size. Using the reference path $P_{ref}$ planned by the path planner, we convert the pose $Pos_{cart} = (x, y, \\Theta_{cart})$ of each agent from Cartesian coordinates (in the local frame) to Frenet coordinates $Pos_{fren} = (s, d, \\Theta_{fren})$.\n$Pos_{fren} = Frenet(P_{ref}, Pos_{cart})$\nGiven an agent's pose in Frenet coordinates $(s, d, \\Theta_{fren})$, the following methodology is used to describe the agent's relative position and orientation:\n1) Longitudinal Position: The longitudinal position $D_{lon}$ relative to the ego vehicle is determined as follows:\n$D_{lon} = \\begin{cases} s \\text{ meters ahead} & \\text{if } s > 1, \\\\ s \\text{ meters behind} & \\text{if } s < -1, \\\\ \\text{parallel with the ego} & \\text{if } -1 < s < 1. \\end{cases}$\n2) Lateral Position: The lateral position $D_{lat}$ relative to the ego vehicle is determined as follows:\n$D_{lat} = \\begin{cases} d \\text{ meters left} & \\text{if } d > 1, \\\\ |d| \\text{ meters right} & \\text{if } d < -1, \\\\ \\text{directly in line with the ego} & \\text{if } -1 \\leq d \\leq 1. \\end{cases}$\n3) Orientation Description: First, the agent's orientation $\\Theta_{fren}$ is normalized to fall within the range $[-\\pi, \\pi]$ with:\n$\\Theta_{norm} = (\\Theta_{fren} + \\pi) \\mod (2\\pi) - \\pi$.\nThe orientation of an agent is described as:\n$\\begin{aligned} D_{or} = \\begin{cases} L_{or}[0] & \\text{if } -\\alpha \\leq \\Theta_{norm} \\leq \\alpha, \\\\ L_{or}[1] & \\text{if } \\Theta_{norm} \\leq -\\beta \\text{ or } \\Theta_{norm} \\geq \\beta, \\\\ L_{or}[2] & \\text{if } \\gamma \\geq \\gamma \\text{ and } \\beta < \\Theta_{norm} \\leq -\\alpha, \\\\ L_{or}[2] & \\text{if } \\gamma \\leq -\\gamma \\text{ and } \\beta > \\Theta_{norm} \\geq \\alpha, \\\\ \\text{otherwise.} \\end{cases} \\end{aligned}$\nwith $\\alpha$ is set to 0.06 rad, $\\beta$ is set to 3.08 rad, $\\gamma$ is equal to 1 and $L_{or}$ is the orientation description list as shown below,\n'W in the same direction as the ego vehicle',\n'W in the opposite direction of the ego vehicle',\n'W towards the ego vehicle's planned trajectory',\n'W away from the ego vehicle's planned trajectory'\nFor agents with speed $v \\geq 0.01$ m/s (moving), the [W] variable in $L_{or}$ is set as \"moving\". For agents with speed $v < 0.01$ m/s (stationary), [W] is set to \"facing\".\n4) Final Description: The final description combines the relative position and orientation description is represented as:\n$D = f_m(D_{lon} + D_{lat} + D_{or})$\nThe format of the final description $f_m$ is shown below,\n$f_m=\\begin{cases}\nID: agent_id\\Position: (x, y) \\text{ meters} (D_{vp} + D_{hp})\\Size: Width: w \\text{m}, Length: l \\text{m}\\Speed: v \\text{m/s}\\Orientation: \\theta \\text{ rad} (D_{or})\n\\end{cases}$\nand a concrete example of the description is shown in Fig. 4."}, {"title": "B. Bottom Layer: Rule-based planners", "content": "In this section we briefly introduce the different rule-based planners used in the experiments with our DaulAD framework.\nIntelligent Driver Model. The Intelligent Driver Model (IDM) [37] is a basic planning method that is used to provide a reference for the ego-vehicle's planning. IDM calculates the path based on the road centerline and adapts the velocity along that path during simulation. Based on the speed v, and the longitudinal distance s to the vehicle ahead on the centerline, IDM repeatedly applies the following rule to determine the acceleration along the path:\n$\\frac{d \\dot v}{dt} = a(1-(\\frac{v}{v_0})^\\delta - (\\frac{s^*}{s})^2)$\nwhere a is the acceleration limit, vo is the target speed, s* is the safety distance, and $\\delta$ is an exponent. These values are chosen manually. The behavior of the model is as follows. It accelerates the ego vehicle to the target speed vo or decelerates if it's too close to the vehicle ahead (at a distance s*).\nLattice Planner. A lattice planner used in [38] discretizes the continuous search space into a regular grid or lattice, where each point on the grid represents a potential state or position that the agent can occupy. The primary objective of a lattice planner is to find a feasible and optimal path from a start state to a goal state while satisfying the system's motion constraints and avoiding obstacles.\nThe essence of a lattice planner can be mathematically de- scribed by considering the state space S, which is discretized into a lattice grid L. The planner's goal is to find a sequence of states {$s_0, s_1,..., s_n$} C L where $s_0 = s_{start}, s_n = s_{goal}$ that minimizes a cost function J, subject to the dynamic constraints of the system. The optimal path can be represented by solving the following optimization problem:\n$\\min_{s_0, s_1,..., s_n} \\sum_{i=0}^{n-1} c(s_i, s_{i+1}) s.t. \\phi (s_i, s_{i+1}) \\leq 0$\nwhere $s_i \\in L$, and $\\phi (s_i, s_{i+1}) \\leq 0$, $c(s_i, s_{i+1})$ is the cost of moving from state si to state $s_{i+1}$, $\\Phi$ represents the motion constraints of the system and $s_0$ and $s_n$ are the initial and goal states, respectively.\nFrenetix Motion Planner. Frenetix Motion Planner [39] uses a sampling approach to improve comfort, safety, and accuracy for trajectory planning in complex environments. In each iteration, a number of kinematically feasible polynomial trajectories T is spanned from the current ego vehicle's position. The trajetories vary in their final lateral displacement, velocity as well as their planning horizon. For each trajectory $\\xi \\in T$, the costs are calculated based on a cost function which is related to different factors $J_i(\\xi)$ with weighting $w_i$.\n$J_{sum}(\\xi|f) = \\sum_{i=1}^{n} w_i \\cdot J_i(\\xi)$\nThe cost terms encompass accelerations for a comfortable trajectory selection, the deviation from the desired velocity as well as the distance to the desired global route and the risk of colliding with observed agents. This combination of factors ensures an efficient and comfortable progress while ensuring safety. The trajectory with the lowest cost is then chosen as the optimal trajectory $\\xi_{optimal}$."}, {"title": "C. Planning Intervention by the LLM", "content": "After the text-based scenario description is fed into the LLM, we update the desired velocity based on the LLM output. The reasoning result $I_R$ is shown below,\n$I_R = LLM(f_{text}(S))$\nThe reasoning result includes the suggested driving speed limit. The allowed speed suggestions range from 0 to 15 m/s. If the rule-based planner plan a speed $V_{rule}$ that is higher than the LLM's suggestion, then the decision of LLMs will be used to overwrite the driving decision of the rule-based planner as shown below,\n$v_d = \\begin{cases} I_R[\\text{\"'speed\"'}] & \\text{if } I_R[\\text{\"'speed\"'}] \\leq V_{rule}\\\\ V_{rule} & \\text{otherwise.} \\end{cases}$"}, {"title": "IV. EXPERIMENTS", "content": "Dataset and Simulation. NuPlan [43] serves as the closed- loop ML-based benchmark for autonomous vehicle planning. The dataset includes 1300 hours of driving data recorded across four different cities. We conducted all evaluations on the public nuPlan mini set, which contains over 2,000 diverse scenarios. We also explored two different strategies for selecting scenarios: Hard-55: In this strategy, scenarios are filtered out by the score of the R_CLS metric for the IDM. The 55 scenarios with the worst scores are chosen. Since the IDM achieves an intermediate performance in closed-loop scenarios, the filtered result scenarios are regarded relatively hard. Super-Hard-24: This strategy is designed to test the planner's ability to handle even more challenging scenarios. We ran 2000 scenarios in total using the PDM-Closed [40] planner, which is current state-of-the-art as it ranks first in the planning leaderboard. The 24 scenarios with the worst scores are chosen and are regarded more challenging.\nOur simulation environment is nuPlan's closed-loop sim- ulator. Each simulation consists of a 15-second rollout at a frequency of 10 Hz. The simulator uses an LQR controller for tracking the planned trajectory. Background traffic behavior is influenced by the simulation mode, which can be either non-reactive (log-replay) or reactive.\nMetrics. The evaluation metrics used in this study are the official ones provided by nuPlan [43], including the open- loop score (OLS), non-reactive closed-loop score (NR-CLS), and reactive closed-loop score (R-CLS). While Codevilla et al. [25] argued that open-loop evaluation is not necessarily correlated with driving quality, we use only closed-loop evaluation. The NR-CLS and R-CLS are calculated using the same methodology, with the key difference being that R- CLS includes background traffic control using the IDM [37] during the simulations. The closed-loop score is a composite score derived from a weighted combination of several factors, including similarity to human driving, vehicle dynamics, and goal achievement etc. The score ranges from 0 to 100."}, {"title": "Zero-shot Large Language Models", "content": "We selected two large language models to build the reasoning module, which uses LLMs for inference. The first is the freely available GLM- 4-Flash, a member of the General Language Model (GLM) family [44]. The second model is GPT-40 [45], which is considered one of the most advanced LLM to date. It is important to note that, unless otherwise stated, we use GLM- 4-Flash as the default reasoning module. GPT-40 is used only in experiments involving comparisons of different levels of LLM influence and in ablation studies.\nWe compare the performance of different rule based planners with and without the Reasoning of LLMs and compare them against the current state-of-the-art."}, {"title": "A. Rule-based Model with and without LLM Reasoning", "content": "IDM. We evaluated the performance of IDM with and without the integration of an LLM. Our findings indicate that incorporating the LLM significantly enhances IDM's effectiveness. As shown in Table. I, without the reasoning module, IDM's performance is relatively low across both benchmarks for reactive (NR-CLS) and nonreactive (R-CLS) simulations. However, when the LLM is integrated, the R- CLS score in the Hard-55 benchmark increases by 16%, and in the SuperHard-24 benchmark, the R-CLS score saw an improvement of nearly 20%. Notably, the R-CLS score in the Hard-55 benchmark surpasses most of rule-based and learning-based methods, including PDM-Closed [40], which is regarded as the state-of-the-art. In contrast, the NR-CLS score did not show a significant increase in either benchmark.\nLattice-IDM. Next, we evaluated the Lattice-IDM approach, both with and without LLM support. The Lattice-IDM planner combines the traditional IDM model with a lattice-based path planning approach. As indicated in Table. I, without LLM support, although the Lattice-IDM achieves higher R- CLS scores on the Hard-55 benchmark than IDM, PDM- Closed and UrbanDriver [41], the number is still relatively low. Additionally, the Lattice-IDM's overall performance across both benchmarks is also very low. While with the assistance of the LLM, however, the scores in R-CLS not only surpass most of rule-based and all the learning-based methods on the Hard-55 benchmark, but also exceed those of DualAD (IDM + LLM). And the scores on other metrics are also improved significantly. Notably, when compared to not using LLMs, the scores of DualAD with the Lattice-IDM planner increased on the Hard-55 R-CLS benchmark by 44%, and on the SuperHard-24 benchmark by 36%. As shown in Fig. 5, this example demonstrates that DualAD with the help of LLM, not only avoids collisions by reducing speed in advance but also presents a smoother velocity profile.\nFrenetix. Then we tested the Frenetix planner with and without the assistance of the LLM. Without LLM, Frenetix outperforms all rule-based and learning-based methods. And with the assitance of LLM, scores in R-CLS on both benchmarks is still enhanced, the improvements are not very evident. And DualAD with Frenetix scores similarly in NR- CLS on both benchmarks compare to without using LLM."}, {"title": "B. Different Levels of LLMs", "content": "We randomly choose ten scenarios from the SuperHard- 24 benchmark to explore how different reasoning levels of LLMs can influence the integrated DualAD's performance. As shown in Table II, the performance of the integrated model is directly influenced by the level of the LLMs used. We used two different levels of LLMs: GPT-40 (considered the most advanced LLM currently available) and GLM-4-flash (a relatively less powerful model). The results demonstrate that using a stronger LLM like GPT-40 in Lattice-IDM led to a nearly 20% improvement in NR-CLS and over a 100% increase in R-CLS for the integrated DualAD model. Additionally, GPT-40 not only enhanced performance but also provided more stable outputs. In our experiments, 20% of simulations using GLM-4-flash failed because the model's output did not conform to the required format. In contrast, all simulations using GPT-40 were successful."}, {"title": "C. Ablation Study", "content": "We used the same benchmark setting as Subsection IV-B to evaluate the impact of text encoder on the overall system performance, we conducted an ablation study using GPT-40. Table III illustrates that omitting the text encoder adversely impacts the performance of the Large Language Model. In experiments where Lattice-IDM is integrated with the LLM, the exclusion of the text encoder leads to a reduction of over 11% in R-CLS scores and more than 7% in NR-CLS scores."}, {"title": "V. DISCUSSION", "content": "We compared DualAD's performance with several state- of-the-art planners. It is notable that PDM-Closed [40], a rule-based method that integrates IDM with different hyperparameters, achieves the highest score on the official leaderboard. PlanTF [42] achieves the best performance among all learning-based planners, and UrbanDriver [41] is a vectorized Transformer-based planner. However, DualAD with the Lattice-IDM planner outperforms PDM-Closed and UrbanDriver on all benchmarks. Additionally, DualAD with the Lattice-IDM planner achieves similar performance and even outperforms PlanTF in R-CLS on the Hard-55 benchmark. Furthermore, with a large language model's help, DualAD with different rule-based models, including IDM and Lattice-IDM, significantly outperforms their original models. Most importantly, we achieved these improvements using only a relatively weak LLM, GLM-4-Flash, by simply adjusting the desired speed of the vehicle. All of these improvements demonstrate that the DualAD framework, which combines rule-based methods with LLMs, can contribute to more reliable autonomous driving, particularly in critical scenarios.\nWe should also notice that Frenetix [39] is a well-designed planner, which is a complex system including modules for sampling, kinematic checks, prediction, and planning. Although the improvements for Frenetix are not as evident, we can observe significant improvements in weaker models. Since the reasoning ability of weak planners is minimal compared to LLMs, integrating LLMs leads to notable enhancements. However, because Frenetix already incorporates reasoning capabilities through its complex system of modules, its reasoning ability is relatively comparable to that of an LLM, and thus the LLM does not further improve its performance. One possible reason is that the current benchmark is not critical enough to challenge Frenetix's reasoning ability."}, {"title": "VI. CONCLUSION", "content": "This paper introduced DualAD, an autonomous driving framework that integrates a rule-based motion planner with a large language model to enhance decision-making in complex driving scenarios. DualAD effectively mimics human reasoning by employing the LLM for high-level reasoning in critical situations, while relying on the rule-based planner for regular tasks. The closed-loop experiments validated the framework's stable capability, showing that DualAD improves performance in challenging scenarios compared to other planners. The incorporation of a text encoder, which converts driving scenarios into a text format, is important for the framework's success.\nLimitation and Future Work. DualAD's text descriptions currently focus solely on surrounding agents, omitting crucial map details like lanes and sidewalks. Integrating this map information could enhance the LLM's understanding of the driving environment. Additionally, the reasoning module's driving decisions lack steering direction, which could be explored to maximize the LLM's reasoning abilities. Further- more, the LLM only processes one frame at a time, which results in lacking historical data. Another future work can be feeding it multiple frames could provide temporal context."}]}