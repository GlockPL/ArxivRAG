{"title": "Sequence-level Large Language Model Training with Contrastive Preference Optimization", "authors": ["Zhili Feng", "Dhananjay Ram", "Cole Hawkins", "Aditya Rawal", "Jinman Zhao", "Sheng Zha"], "abstract": "The next token prediction loss is the dominant self-supervised training objective for large language models and has achieved promising results in a variety of downstream tasks. However, upon closer investigation of this objective, we find that it lacks an understanding of sequence-level signals, leading to a mismatch between training and inference processes. To bridge this gap, we introduce a contrastive preference optimization (CPO) procedure that can inject sequence-level information into the language model at any training stage without expensive human labeled data. Our experiments show that the proposed objective surpasses the next token prediction in terms of win rate in the instruction-following and text generation tasks.", "sections": [{"title": "1 Introduction", "content": "Next token prediction is now the predominant way for pre-training and supervised fine-tuning (SFT) of large language models (LLM). This loss function can be easily scaled up to train models with trillions of parameters and tokens, and it has demonstrated the ability to generate coherent and contextually relevant text. Let P be the unknown target language distribution and let Q be the distribution of our model at hand. The goal of next token prediction is to minimize the forward-KL divergence between Pand Q. This training process only supervises the prediction of one token at a time, given the full context of the ground truth. On the other hand, during inference, the model needs to generate a whole sequence (for a given prompt) relying on its own prior predictions. This mismatch between the training and inference stage is known as exposure-bias in the literature of RNN and sequence-to-sequence model (Bengio et al., 2015; Ranzato et al., 2015). In other words, next token prediction injects only token-level information into the model, but missing sequence-level signal. The latter requires a generation of a longer horizon, which often relies on reinforcement learning algorithms; for example, reinforcement learning with human feedback (RLHF) (Ouyang et al., 2022); and is computationally expensive. In this work, we ask the following question: Can we introduce sequence-level information in LLM pre-training / SFT with a small computational cost?\nWe answer the question affirmatively with our proposed CONTRASTIVE PREFERENCE OPTIMIZATION (CPO) method. The goal of CPO is to improve generation quality. Unlike RLHF, the proposed CPO method does not require human preference information as the training signal. While we demonstrate CPO in the SFT case, the loss can be seemlessly applied to the late stage of pretraining as well."}, {"title": "2 Related work", "content": "LLMs trained with next token prediction loss (Radford et al., 2019; Chung et al., 2022; Sanh et al., 2021; Zhou et al., 2023) have demonstrated many fascinating capabilities, including the ability to perform zero-shot or few-shot tasks (Radford et al., 2019; Brown et al., 2020) and the ability to reason (Wei et al., 2022).\nSeveral works have investigated the shortcomings of MLE and exposure bias. Arora et al. (2022) measured the accumulation of errors in language generation due to exposure bias. Schmidt (2019) connected exposure bias to generalization. Wang and Sennrich (2020) studied how exposure bias leads to hallucination in neural machine translation. To mitigate exposure bias, there exists a long line of work that has explored sequence-level training methods. Bengio et al. (2015); Ranzato et al. (2015) proposed to train RNN with RL or RL-related algorithms rather than teacher-forcing. BRIO Liu et al. (2022) targeted the summarization task with the ROUGE signal. Pang and He (2020) trained the language models with an offline RL algorithm. There also exists a line of works that generate samples during training and mix the samples with ground truth data (Shen et al., 2015; Zhang et al., 2019; Duckworth et al., 2019).\nRecently, RLHF (Stiennon et al., 2020; Ouyang et al., 2022) and its supervised version DPO (Rafailov et al., 2023) were developed for alignment. They are effectively sequence-level training techniques. These algorithms require a pair of preferred and rejected samples, which are usually gathered by human labeling. The RL approach to language modeling is also closely related to energy-based models (EBM) (Korbak et al., 2022; Deng et al., 2020). This EBM form has also been studied in controlled text generation Kumar et al. (2022). Pace et al. (2024) also consider synthetic data generation, but their purpose is to improve reward modeling in RLHF rather than sequence-level training."}, {"title": "3 Proposed approach", "content": "Consider a sentence of T tokens X = {x_1,...,x_T} \\in X, we use \\pi(x) to represent the distribution of x under some language policy \\pi. In particular, we write \\pi_\\theta for a distribution that is parameterized by \\theta, where \\theta is usually the set of trainable parameters of the LLM; we write \\pi_{ref} for a reference distribution that should be clear given the context. Inspired by DPO, we introduce our CPO objective:\n\nL_{CPO} (\\pi_{\\theta}, \\pi_{ref}) = E_{(x,y_1)~D,y_2,..., y_K \\sim A} log \\frac{\\pi_{\\theta}(y_1|x)}{exp(\\beta log \\frac{\\pi_{\\theta}(y_1|x)}{\\pi_{ref}(y_1|x)}) + \\sum_{j=2}^K exp(\\beta log \\frac{\\pi_{\\theta}(y_j|x)}{\\pi_{ref}(y_j|x)})}\n\nHere (x, y_1) is the ground truth prefix-continuation pair from the natural language distribution D, and y_2,..., y_K are K \u2013 1 negative continuations sampled from a to-be-discussed distribution A. The derivation is deferred to the appendix. If some ranking of the data quality is presented, i.\u0435. \u03c4 : [K] \u2192 [K] where \u03c4(i) < \u03c4(j) means y_i is preferred over y_j, we also have the following CPO objective with ranking:\n\nL_{CPO} (\\pi_{\\theta}, \\pi_{ref}) = E_{(x,y_1)~D,y_2,..., y_K \\sim A} log \\prod_{k=1}^K \\frac{\\pi_{\\theta}(y_{\\tau(k)})|x)}{exp(\\beta log \\frac{\\pi_{\\theta}(y_{\\tau(k)})|x)}{\\pi_{ref}(y_{\\tau(k)}|x)})} \\sum_{j=k}^K exp(\\beta log \\frac{\\pi_{\\theta}(y_{\\tau(j)})|x)}{\\pi_{ref}(y_{\\tau(j)}|x)})\n\nUnlike RLHF or DPO, which require human preference data y_1 \u2265 y_2 \u2265 . . . \u2265 y_K, CPO requires only ground truth data (x, y_1) ~ D, and K-1 synthetic negative samples y_2, \u2026\u2026\u2026, y_K ~ A. Possibly, we can also get a ranking among the K-1 synthetic samples in a fully automatic way. On a high level, CPO implicitly rewards the ground truth more than the synthetic negative samples.\nWe consider four ways to generate synthetic data. (1) autoregressive negatives (AN): We use the language model to autoregressively generate the negative samples given a prefix. We fixed the synthetic data generation strategy to be top-k sampling with k = 50. (2) batch negatives (BN): given a batch of prefixes and continuations {x_i, y_i}_{i=1}^n, the negative samples to the prefix x_i are composed of {y_j}_{j\u2260i}. (3) meanfield negatives (MN): given a sequence y = {y_1,..., y_T}, we randomly select c percent of the positions {t_1,...,t_j} \u2286 [T], and substitute each y_{t_i} independently based on \\pi_{\\theta}(y_{t_i}|y_1, ..., y_{t_i}), i.e. we independently resample c% of the tokens according to their original autoregressive distribution. (4) truncation negatives (TN): for each ground truth continuation, we truncate them at a random position and append an extra EOS token at the end.\nIn our experiments, we observe that CPO can often benefit from a ranking among K samples, where the ranking is based on their cosine similarity to the ground truth. Let e_1, . . ., e_K be the embeddings of given sequences y_1,..., y_K and without loss of generality assume that e_1 is the ground truth, we define \u03c4(i) < \u03c4(j) if  \\frac{(e_i,e_1)}{\\lVert e_i \\rVert \\lVert e_1 \\rVert} < \\frac{(e_j,e_1)}{\\lVert e_j \\rVert \\lVert e_1 \\rVert}, with the lower ranking index indicating the better sample. Using the objective eq. (2), this process gives us denser signals during training and can lead to better downstream performance."}, {"title": "4 Experimental Setup", "content": "Throughout this section, BN, AN, MN, TN represents batch negatives, autoregressive negatives, meanfield negatives, and truncation negatives respectively. MixN represents a mixed negative sampling strategy for which the details can be found in its context. We use ANR for models trained with autoregressive negatives and ranking signals, similarly we can denote MixNR, etc. We always randomly swap 15% tokens using MN. Although this choice is mainly heuristic, such a ratio appears quite frequently since BERT (Wettig et al., 2022).\nTask and model. We consider two tasks in this paper. The first is an instruction-following task, trained and evaluated on the Dolly dataset (Conover et al., 2023). This dataset is composed of 15011 total instruction and response pairs. We train with 7505 sequences and test with the rest 7506. We use pre-trained GPT2-XL (Radford et al., 2019) and OpenLlama-3B(Touvron et al., 2023; Geng and Liu, 2023) as the base model. The second is an open-ended text generation task on Wikidump data (Foundation). We train the OpenLlama-3B model to predict 85% tokens per sample given the leading 15% tokens.\nBaselines. We consider three main baselines: MLE, DPO, and parallel scheduled sampling (PSS, (Duckworth et al., 2019)). Importantly, the difference between DPO and CPO lies in their negative samples. For DPO, we query GPT-3.5 to generate unhelpful response to the Dolly instructions. PSS is trained to sample 3 sequences for each training data, and each token is replaced with p = 0.5 (see Duckworth et al. (2019) for more details).\nTraining details. Throughout the experiment, we fix the learning rate to be 1e-5, we use the AdamW optimizer with weight decay of 0.05. We keep the batch size to be 64. Unless otherwise specified, for the baseline model, we train GPT2-XL and OpenLlama-3B with the next token prediction loss for 2000 steps. Using these models as the reference model \\pi_{ref}, we continue to train with the CPO objective either with or without ranking signals, with \u03b2 = 5, for 1000 steps. For both models, each training data in a batch contains 11 negative samples in total. For MixN and MixNR, we also use a negative sample size of 11, consisting of 3 BN, 5 MN, and 3 TN. The MLE models used for evaluation are continually trained for the same number of steps from the reference model, like the CPO models. All experiments are conducted on two AWS machines, each with 8 A100 GPUs.\nEvaluation. As discussed in Goyal et al. (2022), almost all automated evaluation metrics have been shown to not align with human evaluations in the modern era of LLMs, so we decide to use GPT (Brown et al., 2020) as the evaluator. See the query template in the appendix. For efficiency, we generate and evaluate 1000 samples chosen from the 7506 test set. A similar template is used for Wiki text generation, see the detail in the appendix. During inference, we consider greedy decoding, top-p and top-k sampling, as well as beam search.\nWeight-space ensemble. Previous works (Liu et al., 2022) have also suggested to combine the auxilliary loss function with the MLE training objective \u03b1\\theta_{LMLE} + \\theta_{LCPO}, the downside of combining loss functions in this way is that for a different choice of \u03b1 one will have to retrain the model. To investigate the importance of loss combination, we instead perform a weight-space ensemble (Wortsman et al., 2022). In particular, denote \\theta_{CPO} and \\theta_{MLE} the model parameters trained solely with CPO or MLE respectively, we generate with the interpolated weights \u03b8 = \u03b1\u03b8_{MLE} + (1 \u2212 \u03b1)\u03b8_{CPO}."}, {"title": "5 Experimental Analysis", "content": "5.1 Instruction-Following Task\nOur proposed CPO method with various negative sampling strategies consistently outperforms the MLE baseline models on the Dolly instruction-following task. Using greedy sampling with GPT2-XL, the CPO model has a clear margin over the MLE model, and CPO+ANR has a 3.5% higher win rate, see table 1. Note that CPO incurs very little computation overhead during the actual training: the overhead only comes a larger batch size, and even if we generate the negative samples autoregressively, it is a one-time offline cost.\nThe improvement in OpenLlama-3B is more significant: CPO+ANR has a 13.8% higher win rate than the MLE baseline, and CPO+MixNR has a 9.8% higher win rate in table 2. We also observe that weight-space ensemble has a positive impact on the model. Heuristically, for OpenLlama-3B, a smaller \u03b1 is preferred (more emphasis on the CPO weights) (table 2), but the reverse holds for GPT2-XL (table 1). We hypothesize that the choice of \u03b1 should depend on the model: if the model is more capable, then it can benefit more from CPO. Here, we show the existence of a good \u03b1, and we leave further exploration to future research.\nComparison with DPO and PSS. The proposed CPO method performs better than other two baseline methods: DPO and PSS (see Table 2). We believe that DPO performs poorly because unhelpful/irrelevant continuations (even generated by ChatGPT) do not provide a very strong signal as human generated samples. Unlike in alignment, where the toxic/harmful samples provide a clear indication of what not to generate, here it is not clear what DPO can gain from merely a single irrelevant sample. On the other hand, CPO can benefit from larger negative sample size.\nSampling Strategy. In addition to greedy decoding, we also experiment with different choice of sampling strategies. In all settings, CPO has consistently demonstrated superior performance over MLE, see fig. 1.\nEffect of different negative samples. We perform a study on the effects of different negative sampling strategies; the results are presented in fig. 1. We first train the OpenLlama-3B model with MLE loss for 1000 steps, then continue to train with CPO for 200 steps. For all ground truth sequences, we use 4 negative sequences. In this setting, we always use the ranking information to train CPO. We observe that the effects of BNR and TNR on the reward model preference are similar and that they perform slightly better than MNR."}, {"title": "5.2 Open-ended Text Generation Task", "content": "We further test OpenLlama-3B's ability on an open-ended text generation task with CPO. Using Wikidump data (Foundation), for each test sample, we take its first 15% tokens as the prefix and train the model with CPO on the rest 85%. For negative sampling, we use four BNR examples. The results in table 3 show that CPO can improve the model's win rate against the MLE baseline by 3%. We observe that increasing \u03b1 improves the score, the opposite of the instruction-following task. It is likely because the negative samples here are too noisy, since only 15% prefixes are provided.\nAdditionally, we test the MAUVE score (Pillutla et al., 2021) of MLE and CPO compared to the ground truth. See the results in fig. 2 and table 4."}, {"title": "6 Conclusions and Limitations", "content": "In this paper, we propose an auxiliary CPO loss function for LLM training, which can be used with or without ranking signals, depending on the quality of the negative samples. We investigated several ways to generate negative samples. One limitation of this work is that the synthetic data are very noisy unless generated autoregressively; it is interesting to explore other ways to efficiently generate high-quality negative data beyond the autoregressive fashion."}, {"title": "A Appendix", "content": "A.1 A brief introduction to DPO, RLHF, and EBM\nThe equivalence of RLHF and EBM For the completeness of this paper, we include the result of the equivalence between RLHF and EBM. For the full proofs, we refer the reader to (Rafailov et al., 2023; Korbak et al., 2022).\nThe RLHF objective is the following:\n\nmax_{\\pi_{\\theta}} E_{x~D,y~\\pi_{\\theta} (y|x)} [r(x, y)] - \\beta D_{KL} (\\pi_{\\theta}(y|x)||\\pi_{ref}(y|x)),\n\nwhere x ~ D is a given prefix, y ~ \\pi_{\\theta}(y|x) is a sampled continuation from the trainable model \\pi_{\\theta}, and r(x, y) \u2208 R is the reward. Meanwhile, we want to control the divergence between \\pi_{\\theta} and \\pi_{ref}, where \\pi_{ref} is usually an already pretrained or finetuned LLM. The RLHF optimum is achieved at the following EBM:\n\n\\pi^*(y|x) = \\frac{1}{Z(x)} \\pi_{ref} (y|x) exp (\\frac{1}{\\beta} r(x,y)),\n\nwhere  Z(x) = \\sum_{y} \\pi_{ref} (y|x) exp (\\frac{1}{\\beta} r(x, y)) is the partition function.\nRafailov et al. (2023) assume that the preference over two sequences yw and y\u012b given \u00e6 is parameterized by the Bradley-Terry model:\n\nP(y_w > y_l|x) = \\frac{e^{r(x,y_w)}}{e^{r(x,y_l)} + e^{r(x,y_w)}}.\n\nUnder the Bradley-Terry model, DPO establishes the equivalence between the original RLHF objective eq. (3) and the following supervised objective:\n\nL_{DPO}(\\pi_{\\theta}; \\pi_{ref}) = E_{(x,y_w,y_l)~D} [log \u03c3 (\\beta log \\frac{\\pi_{\\theta}(y_w | x)}{\\pi_{ref}(y_l | x)} - \\beta log \\frac{\\pi_{\\theta}(y_l | x)}{\\pi_{ref}(y_l | x)})],\n\nwhere \u03c3(\u00b7) is the Sigmoid function.\nThey also generalize the formulation to the Plackett-Luce model, where we have a linear ordering \u03c4(\u00b7) among K sequences:\n\nL_{DPO} (\\pi_{\\theta}, \\pi_{ref}) = E_{T,x~D \\atop y_1,..., y_K} log \\prod_{k=1}^K \\frac{exp (\\beta log \\frac{\\pi_{\\theta} (y_{\\tau(k)}|x)}{\\pi_{ref} (y_{\\tau(k)}|x)})}{\\sum_{j=k}^K exp (\\beta log \\frac{\\pi_{\\theta} (y_{\\tau(j)}|x)}{\\pi_{ref} (y_{\\tau(j)}|x)})},\n\nHere, \u03c4(1), . . ., \u03c4(K) induce a ranking among K sequences."}, {"title": "A.2 Derivation of the CPO objective function", "content": "Here we give a full derivation of the CPO objective function in eq. (1).\nLet y_1,..., y_K be K continuations of a given prefix x. Without loss of generality, let y\u2081 be the best candidate. We are interested in the MLE of the event P(y1 is the best among K candidates|x).\nWe start from the sequence-level (RLHF) objective, notice that here r(\u00b7) is a reward over language quality, not human preference.\n\nmax_{\\pi_{\\theta}} E_{x~D,y~\\pi_{\\theta} (y|x)} [r(x, y)] - \\beta D_{KL} (\\pi_{\\theta}(y|x)||\\pi_{ref}(y|x)),\n\nIts optimum is achieved at the following EBM:\n\n\\pi^*(y|x) = \\frac{1}{Z(x)} \\pi_{ref} (y|x) exp (\\frac{1}{\\beta} r(x, y)),\n\nwhere Z(x) = \\sum_{y} \\pi_{ref} (y/x) exp (\\frac{1}{\\beta} r(x, y)) is the partition function. See the proof in (Rafailov et al., 2023; Korbak et al., 2022).\nNow we consider the natural extension of the Bradley-Terry model to K candidates:\n\nP(y_1 is the best among K candidates|x) = \\frac{exp (r^*(x, y_1))}{\\sum_{k \\in [K]} exp(r^*(x, y_k))}.\n\nNow assuming we have the optimal policy \u03c0*, we can reparameterize r by rearranging eq. (8):\n\nr^*(x, y) = \\beta log \\frac{\\pi^*(y|x)}{\\pi_{ref} (y|x)} + \\beta log Z(x).\n\nPlugging eq. (10) into eq. (9), we get eq. (1)."}, {"title": "A.3 Query template of Dolly and Wiki text generation", "content": "The query template for the Dolly instruction-following is the following: \"For the following query to a chatbot, which response is more helpful?\\n Query: {}\\n Response A: {}\\n Response B: {}\\n State only \"A\" or \"B\" to indicate which response is more helpful.\\n More helpful:\"\nThe template for Wiki is the following: \u201cFor the following prefix, which continuation is better?\\n Prefix: {}\\n Continuation A: {}\\n Continuation B: {}\\n State only \"A\" or \"B\" to indicate which continuation is more helpful.\\n Better:\""}, {"title": "A.4 DPO generation template and example", "content": "When generating unhelpful responses for DPO, we query GPT with the following template:\nGiven the ground truth instruction and response, can you generate a not helpful response?\\n Instruction: {}\\nResponse: {}\\nNot helpful response:.\nOne example of the generated response is the following: instruction: When did Virgin Australia start operating? chosen response: Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. rejected response: Virgin Australia definitely exists and has airplanes that fly to different places."}, {"title": "A.5 Connection to noise contrastive estimation", "content": "Noise contrastrive estimation (NCE) (Gutmann and Hyv\u00e4rinen, 2010) is a novel estimation technique introduced to tackle the computational infeasibility of traditional likelihood-based methods in large-scale machine learning models, particularly those involving high-dimensional data. NCE diverges from typical maximum likelihood estimation by transforming the problem into a classification task, which is deeply connected to both DPO and CPO. In NCE, the model is trained to distinguish between real data and noise/synthetic data. Beyond binary classification, RankingNCE\u00b9 also trains the model to rank the real data higher than all noise samples (Ma and Collins, 2018).\nThere are two important distinctions between CPO and NCE. First, instead of training the model to distinguish between real data and noise (at which any reasonable language model should already be good), we train the model to distinguish better than a reference model does, hence making the model better at recognizing natural text. Second, we also introduce a denser ranking signal by incorporating the similarity among embeddings of different samples. The experiments in this paper demonstrate that such a dense training signal consistently improves text generation quality."}]}