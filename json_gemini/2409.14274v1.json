{"title": "Proof Automation with Large Language Models", "authors": ["Minghai Lu", "Benjamin Delaware", "Tianyi Zhang"], "abstract": "Interactive theorem provers such as Coq are powerful tools to formally guarantee the correctness of software. However, using these tools requires significant manual effort and expertise. While Large Language Models (LLMs) have shown promise in automatically generating informal proofs in natural language, they are less effective at generating formal proofs in interactive theorem provers. In this paper, we conduct a formative study to identify common mistakes made by LLMs when asked to generate formal proofs. By analyzing 520 proof generation errors made by GPT-3.5, we found that GPT-3.5 often identified the correct high-level structure of a proof, but struggled to get the lower-level details correct. Based on this insight, we propose PALM, a novel generate-then-repair approach that first prompts an LLM to generate an initial proof and then leverages targeted symbolic methods to iteratively repair low-level problems. We evaluate PALM on a large dataset that includes more than 10K theorems. Our results show that PALM significantly outperforms other state-of-the-art approaches, successfully proving 76.6% to 180.4% more theorems. Moreover, PALM proves 1270 theorems beyond the reach of existing approaches. We also demonstrate the generalizability of PALM across different LLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Correctness is crucial to software systems. Interactive theorem provers (ITPs) such as Coq [43], Isabelle [34] and Lean [17], are powerful tools for providing semantically rich guarantees about software. In an ITP, users can state and prove formal theorems about a program; these proofs are then mechanically checked by the ITP, providing a strong, foundational guarantee about its correctness. This strategy has been successfully applied to several application domains, including compilers [31], distributed systems [46], and OS kernels [29]. While powerful, this approach comes at a cost, as users must supply a proof script that helps the ITP construct the proof of the desired theorem. Constructing these proof scripts can require considerable effort. For example, it took 6 person-years to write 100,000 lines of Coq proof scripts to verify the CompCert C compiler [31].\nMany proof automation techniques have been proposed to reduce the effort required by ITPs. These techniques mainly fall into two categories: symbolic methods [15, 28, 36, 44] and machine learning methods [20, 21, 39, 48]. Symbolic methods use a combination of previously established theorems and external automated theorem provers (ATPs), such as Z3 [16] and CVC5 [14], to automate the proof of a theorem. While effective, these approaches are constrained by their inability to perform higher-order and inductive reasoning, limiting their ability to prove complex theorems. Machine learning methods utilize models to predict the next proof step in a heuristic-guided search process. These methods do not have the same limitations as symbolic approaches but require a significant amount of training data [20, 21, 48].\nRecently, pretrained Large Language Models (LLMs) have shown promise in generating informal natural language proofs [47], suggesting a potential to further improve existing proof automation approaches. Unfortunately, even state-of-the-art LLMs are ineffective at generating formal proofs in one shot: GPT-3.5 proves 3.7% of theorems in our evaluation, and Llama-3-70b-Instruct proves 3.6%. In order to understand why, we have conducted a formative study to analyze mistakes that GPT-3.5 made when generating formal proofs. In this study, we analyzed 579 theorems of varied complexity and identified seven categories of errors. Overall, we found that while GPT-3.5 often produced proofs with the right high-level structure, it struggled getting lower-level details of these proofs correct. Promisingly, we also observed that many of these errors can be potentially fixed using symbolic methods, including heuristic-based search and proof repair.\nGuided by this formative study, we propose PALM, a novel generate-then-repair approach that combines LLMs and symbolic methods. Our key insight is to use LLMs to generate an initial proof that is likely to have the correct high-level structure, and then use targeted symbolic methods to iteratively repair low-level problems related to individual proof steps. PALM relies on four repair mechanisms that target the common types of errors identified in our formative study. If our repair mechanisms fail, PALM uses a backtracking procedure to regenerate previous proof steps in an attempt to fix errors in the high-level proof structure. Although PALM targets Coq, its underlying principles can be applied to other ITPs, such as Isabelle [34] and Lean [17].\nTo evaluate the effectiveness of our approach, we have conducted an extensive evaluation using the CoqGym dataset [48] with 10842 theorems. Our results suggest that PALM can successfully prove"}, {"title": "2 PRELIMINARIES", "content": "2.1 Interactive Theorem Proving in Coq\nThe Coq proof assistant [43] is a popular tool for developing machine-checked proofs of mathematical theorems and verifying complex software systems. Coq helps users interactively construct these proofs using a set of proof tactics. This section first introduces the basic concepts of interactive proof development in Coq, and then illustrates the process via an example theorem shown in Figure 1.\nTheorems: In Coq, the definition of a theorem typically starts with the keyword Theorem or Lemma, followed by its name and the theorem statement. Figure 1 shows the theorem add_comm, which states that natural number addition is commutative. This is then followed by a proof script, a sequence of tactics that explain how to build a proof of the desired statement. Proof scripts are typically developed in an interactive proof mode. Processing the first line of Figure 1 causes Coq to enter proof mode. During the proof process, users can freely reuse previously proven theorems.\nProof States: In proof mode, Coq's interface displays the current proof state, i.e., a list of unproven goals. Each of these goals is a pair of a local context lc and an outstanding proof obligation st. A local context includes hypotheses and assumptions that can be used to prove st; these are distinct from the set of previously proven theorems, which are part of the global context. Figure 2 shows the intermediate proof states that appear during the proof of add_comm: each listing shows the proof states shown to the user\nafter processing each tactic in Figure 1. Following the conventions of Coq's user interface, the local context is shown above the double line, and the current proof obligation is shown below.\nTactics: Tactics specify strategies for decomposing the current proof obligation into a set of simpler subgoals, in order to eventually produce a complete proof. Conceptually, a tactic t is a state-transition function: $t \\in S \\times \\Sigma \\rightarrow S'$, where S is a goal, $\\Sigma$ is a set of arguments if any, and S' is the set of resulting goals. As an example, the tactic induction n on Line 4 of Figure 1 tells Coq to do induction on the natural number n in the local context. Processing this tactic transforms the proof state in Figure 2b to the proof state in Figure 2c, which has two subgoals: (1) a base case in which n is 0, and (2) an inductive case in which n is an arbitrary natural number. Note that Coq only displays the local context of the first goal when there are multiple goals. Importantly, a tactic can fail if, for example, it is applied to a proof state of the wrong form or it is supplied with wrong arguments. Coq reports the failure back to the user when this occurs.\nProofs: A proof of a theorem consists of a sequence of tactics that transform the initial goal, i.e., the theorem statement, into subgoals until none remain. The beginning and end of a proof are delimited"}, {"title": "2.2 Hammers", "content": "To facilitate proof construction, Coq is equipped with many established proof automation tactics (e.g., auto). These tactics either completely solve the current goal, or leave it unchanged if they fail. Among them, hammers [15, 28, 36] are powerful tactics that dispatch goals using external automated theorem provers (ATPs), such as Vampire [37], CVC5 [14], E [3] and Z3 [16]. Many popular ITPs have hammers, including CoqHammer [15] for Coq, SledgeHammer [36] for Isabelle, and HOLyHammer [28] for HOL Light. At a high level, hammers work by first encoding the current goal into a form solvable by an ATP, typically a formula in first-order logic. This is necessary because ITPs support much richer logic, e.g., higher-order logic, than most ATPs. In order to enable the underlying ATP to use previously proven theorems, a subset of the theorems in the global context are encoded alongside the current goal; the task of selecting a relevant set of these theorems is sometimes called premise selection [13]. Early hammers relied"}, {"title": "3 FORMATIVE STUDY", "content": "While LLMs have previously been used to generate proofs in Coq, they have not proven particularly effective at the task [22, 42, 50]. To understand the root causes of this, we have conducted a formative study to identify the common errors made by LLMs when asked to generate proof scripts. In this study, we evaluated the ability of GPT-3.5 [4] to prove 579 theorems from Verdi, a distributed system verification project [46]. Verdi has also been used in other studies [20, 21, 48]. We carefully designed our prompt based on the widely used retrieval augmented generation (RAG) method [32]. This prompt is also used by PALM, and is described in more detail in Sections 4.2 and 4.3. We prompted GPT-3.5-turbo-1106 API, the latest version available at the time of this study with the default decoding temperature. For each theorem, we sampled only one proof script. We ran the generated proof in Coq, and recorded the error message of the first encountered error in the proof.\nWe collected a total of 520 errors and conducted an in-depth manual analysis, following the grounded theory [23] and the open coding method [24]. The first author first labeled 100 errors and came up with an initial categorization. He then discussed and refined the labels and the categorization with the last author in two meetings. The first author then labeled and categorized the remaining errors based on the refined labels and categorization. Finally, all authors met to discuss and finalize the categorization, where the second author, an expert in theorem proving, offered insights that further enhanced the categorization. The whole process took approximately 52 person-hours. We categorized the 520 errors into seven types, as shown in Table 1."}, {"title": "4 APPROACH", "content": "Guided by our formative study, we propose PALM, a proof automation approach that combines LLMs and symbolic methods. Figure 4 provides an overview of PALM. PALM includes three components: (1) a retrieval-augmented proof generation method, (2) a set of repair mechanisms, and (3) a backtracking procedure."}, {"title": "4.1 The Overall Algorithm", "content": "Algorithm 1 describes the overall generate-then-repair procedure used by PALM. The inputs are a theorem statement t, an environment env, and a language model LM. First, using the retrieval augmented generation (RAG) method described in Section 4.2, PALM retrieves relevant premises from env based on t (Line 3). Next, PALM creates a prompt using t and the selected premises (Line 4), and prompts LM to obtain an initial proof script (Line 5). PALM then executes these tactics in Coq (Lines 6-15). If an error occurs, PALM employs a set of repair mechanisms to fix the problem based on the error message, the tactic that throws the error, and the current proof state (Line 9). If PALM cannot fix an error, it invokes the backtracking procedure (Line 11) described in Algorithm 2, which attempts to fix the previous proof using CoqHammer. The proof is successful if no goals remain unsolved after all tactics have been executed (Line 16)."}, {"title": "4.2 Premise Retrieval", "content": "High-quality context is essential for LLMs to produce accurate responses. For theorem proving, we consider the previously proven theorems and definitions available in the environment as the context of constructing a proof script.\nGiven there are many available theorems and definitions, it is difficult to encode all of them in the proof generation prompt. Thus, we develop an information retrieval method to identify the ones relevant to the theorem to be proven. Specifically, PALM predicts relevant premises using Term Frequency-Inverse Document Frequency (TF-IDF) [40] and k nearest neighbors (KNN) [18]. While more advanced methods such as deep learning [13] can be more accurate, they also require significant amounts of training data and can take a longer time to make predictions [19, 30]. The premises predicted by the KNN algorithm are initially ranked by their TF-IDF scores. PALM then employs the BM25 algorithm [11] to rerank these premises based on their text similarity to the statement of the theorem, since we observed that BM25 tends to rank premises used in human-written proofs higher than TF-IDF."}, {"title": "4.3 Prompt Design", "content": "To optimize the quality of the initial proof generated by the LLMs, we carefully designed the prompt used by PALM following strategies for few-shot in-context learning. Our strategy for designing this prompt was inspired by recent findings that LLMs can produce instructions that are superior or equivalent to those crafted by humans [53]. We first asked GPT-4 to infer the five most effective instructions for two theorems accompanied by human-written proof scripts. Next, we constructed candidate prompts by combining these five sets of instructions with the two examples, premises and a new theorem to be proven. The inclusion of the two example theorems in this query was meant to demonstrate the correct Coq syntax and our desired proof style. The first author then manually examined 20 proofs produced by the LLM in response to these prompts and chose the prompt that yielded the highest quality proofs. Proof quality was assessed based on correctness and adherence to the instructions, e.g., using bullets for structure, etc. When multiple proofs met these criteria, the simplest (i.e. shortest) correct proof was preferred. Figure 9 illustrates the final prompt template with an example and the response of GPT-3.5."}, {"title": "4.4 Repair Mechanisms", "content": "The proofs produced by LLMs typically feature a good high-level structure that decomposes the proof into reasonable subgoals. However, most of these proofs are rejected by Coq due to errors, as"}, {"title": "4.5 Backtracking", "content": "PALM leverages CoqHammer to solve goals that the initial script fails to prove due to errors that cannot be repaired. Although other proof automation techniques could be employed, we found hammers to be effective in practice. Applying a wrong tactic can result in a new goal that is more difficult, or even impossible to prove. Thus, when CoqHammer fails to solve a goal, it is clear that PALM needs to backtrack to an earlier point in the proof to see if it can be solved instead. Particularly, if a tactic produces multiple subgoals, all these subgoals must be proven. If PALM fails to prove any of them, it needs to revert to the goal before that tactic. For example, when reasoning by induction, if the base case is proven but the inductive case fails, the entire induction attempt has failed, and we need to try a different proof strategy instead of induction.\nAlgorithm 2 presents our backtracking procedure, which aims to prove unsolved goals using CoqHammer. The input to the procedure is an unsolved goal g. If g is successfully solved by CoqHammer, it returns the proof found by CoqHammer immediately (Lines 4-5). Otherwise, PALM reverts to the goal before the last applied tactic, and tries CoqHammer again. If the last command is a bullet (Line 6), it means that our algorithm will not be able to prove this subgoal using CoqHammer. When this happens, the algorithm identifies the tactic that produced this subgoal as the root (Line 7), and then discards root and all its associated subgoals (Line 8). Having the"}, {"title": "5 EVALUATION", "content": "Our experimental evaluation of our approach addresses four key research questions:\n\u2022 RQ1: Is PALM more effective at proving theorems than other state-of-the-art proof automation approaches?\n\u2022 RQ2: Can PALM generalize to other LLMs with different parameter sizes?\n\u2022 RQ3: How much does each component of PALM contribute to its effectiveness?\n\u2022 RQ4: Is PALM time-efficient?\nWe conducted experiments on a workstation with an AMD EPYC 7313 CPU, an NVIDIA A5500 GPU, and 512GB memory. The operating system was 64-bit Ubuntu 22.04 LTS."}, {"title": "5.1 Comparison baselines", "content": "We compare PALM against three state-of-the-art proof automation approaches: Passport [39], Proverbot9001 [38] and Draft, Sketch, and Prove (DSP) [27]. Both Passport and Proverbot9001 are machine learning methods. Passport employs a Tree-LSTM [41] to model proof states, incomplete proof scripts, and identifiers in proofs. Proverbot9001 adopts an RNN to model manually engineered features of the proof states. DSP prompts LLMs to translate natural language proofs into formal proof sketches that outline high-level proof steps without low-level details. The informal proofs can be either written by humans or generated by LLMs. It then uses off-the-shelf proof automation tools such as hammers to fill in the gaps. Unlike DSP, PALM does not require informal proofs, and employs repair mechanisms and a backtracking procedure to fix proof errors. As human-written proofs were unavailable for the benchmarks used in our test set, in order to reproduce DSP, we used GPT-3.5 to generate informal proofs and sketches, and use CoqHammer as the underlying proof automation tool."}, {"title": "5.2 Benchmark construction", "content": "Following prior work [38, 39, 48], we use the test set of CoqGym [48] as the evaluation dataset, which consists of 13,137 theorems from 27 open-source Coq projects. Since the theorems from the Verdi project used in our formative study are also included in CoqGym, we exclude them to avoid biases. As we ran the baselines on Coq- Gym, we found that Passport is compatible exclusively with Coq 8.9, and relies on CoqGym's original dataset. Proverbot9001, which does not use CoqGym, supports only newer versions of Coq, namely Coq 8.10, 8.11, and 8.12. To ensure fairness, our evaluation is conducted on a subset of CoqGym, including 10842 theorems that are compatible with all relevant versions of Coq. We implement PALM for Coq 8.10, 8.11, and 8.12, since many language features and standard libraries of Coq 8.9 are outdated [2]."}, {"title": "5.3 Results", "content": "In RQ1, we compare the performance of PALM using GPT-3.5 as the underlying LLM against the baselines. In RQ2, we evaluate the performance of PALM when using different LLMs."}, {"title": "5.3.1 RQ1: Effectiveness of PALM", "content": "Table 2 shows the number and percentage of theorems each approach can successfully prove. Compared with existing approaches, Passport, Proverbot9001, and DSP,"}, {"title": "5.3.2 RQ2: Generalizability of PALM", "content": "To demonstrate the generalizability of PALM across LLMs with different parameter sizes, we further evaluate PALM with GPT-40 [6], Llama-3-70B-Instruct [5] and Llama-8B-Instruct [5] as the underlying LLMs.\nTable 2 presents the theorems proven by each LLM individually, and by PALM when using them as underlying LLMs. We observe that all evaluated LLMs perform poorly when used alone, proving only 0.1%-6.4% of theorems. Augmenting these LLMs with PALM significantly improves the performance. With the most powerful GPT-40 model, PALM proves 4614 theorems, achieving a 5.5% absolute improvement compared with using the second most powerful LLM, GPT-3.5. This highlights the potential enhancements PALM can achieve with the latest LLMs. When using Llama-3-70B-Instruct, PALM proves 4155 theorems, which is comparable with the result obtained using GPT-3.5. When using the smaller Llama-8B-Instruct, PALM proves 3433 theorems, 21.6% fewer than when using GPT-3.5. Despite this, PALM still outperforms DSP by 38.5%, suggesting it can be effective even when using less powerful LLMs. Using all four LLMS, PALM successfully proves a total of 5210 theorems."}, {"title": "5.3.3 RQ3: Effectiveness of each component", "content": "We have conducted an ablation study to evaluate the effectiveness of each component within PALM.\nEffectiveness of the repair mechanisms. To study the effectiveness of each repair mechanism, we constructed four variants of PALM: PALM_ref, PALM_rename, PALM_bullet, and PALM_aug. These variants disable the reference replacement, renaming, bullet"}, {"title": "5.3.4 RQ4: Efficiency of PALM", "content": "On average, PALM takes 32.89 seconds to successfully prove a theorem, while Passport, Proverbot9001 and DSP require 3.1, 4.7 and 8.2 seconds, respectively. The main source of time overhead for PALM is its use of CoqHammer. On average, CoqHammer is invoked 1.96 times per proof, with each invocation having a timeout of 10 seconds. This additional time is justified by PALM's ability to prove more complex theorems than other approaches. We further examined the time each approach takes on all theorems, regardless of whether they were successfully proven or not. On average, PALM takes 105.6 seconds, while Passport, Proverbot9001 and DSP take 67.2, 31.8 and 20.6 seconds"}, {"title": "5.4 Case Studies", "content": "Despite its effectiveness, PALM still fails to prove 59.6% theorems in our dataset. To understand the underlying reasons for these failures, we randomly sampled 100 theorems that PALM fails to prove and conducted a manual analysis. Table 4 outlines the 3 primary reasons for these failures. To illustrate these reasons further, we now describe a typical case of failure for each."}, {"title": "5.4.1 Missing premises", "content": "A key reason for PALM's failures (58%) is the omission of necessary premises in the retrieval process. Figure 13 presents a theorem that PALM fails to prove because a critical premise, reduceplus_cb1, was not retrieved. Hence this premise cannot be used by the LLM, hindering the proof process."}, {"title": "5.4.2 Premises retrieved but not used", "content": "In 14% of the failures, even when a premise is successfully retrieved and included in the prompt, it may not be used by the LLM. Figure 14 shows a case where the lemmas map_insert and map_map_exchange are included in the prompt, but they are not used by the LLM, causing PALM's failure to prove the theorem. Although providing CoqHammer with unused retrieved premises during the backtracking process could solve such issues, we choose not to do so, as providing too many unrelated premises slows down CoqHammer and can lead it to timeout."}, {"title": "5.4.3 Tactics not used", "content": "Some theorems require specific tactics to be proven, and PALM will fail if these tactics are not included in the proof script generated by the LLM. This accounts for 39% of the failure cases. Figure 15 shows an example where the proof of a theorem requires the use of the induction tactic. Since the initial proof script did not include this tactic, and both CoqHammer and our repair mechanisms do not perform induction, PALM cannot prove this theorem."}, {"title": "6 DISCUSSION", "content": "6.1 Threats to Validity\nInternal validity. One threat to internal validity comes from the inherent randomness of LLMs. This randomness is due to the use of temperature sampling [12, 54] as the decoding strategy, where LLMs randomly select the next token based on a probability distribution. To reduce this threat, we conduct large-scale experiments using three state-of-the-art and widely used LLMs: GPT-3.5, GPT-40, Llama-3-70B-Instruct, and Llama-3-8B-Instruct, as the underlying LLMs for PALM. We evaluate their performance across a benchmark consisting of 10842 theorems from diverse domains. The consistent results observed in our experiments help reduce this threat. Another threat is that due to the limitation of computational resources and evaluation time, we only run each of our experiments once; this may introduce statistical biases into our results.\nExternal validity. The threat to external validity is alone the generalizability of our experimental results. We implement and evaluate only on Coq, while other widely used ITPs, such as Isabelle, HOL Light, and Mizar, are not included. Nonetheless, we believe the approach and algorithm in PALM can be easily applied to other ITPs that use tactics for proof construction and support automation tools like CoqHammer. However, the specifics will need to be adapted"}, {"title": "6.2 Limitations and Future Work", "content": "PALM fundamentally depends on the initial proof script generated by LLMs. If the LLM generates a completely wrong initial proof, PALM struggles to fix it. Future improvements to PALM could involve leveraging LLMs to repair incorrect proofs [22] or sampling multiple initial proofs.\nThe initial proof script sometimes fails to use relevant tactics, such as a user-defined tactic with an ambiguous or uninformative name. As a result, PALM cannot effectively prove theorems that depend on custom user-defined tactics. This can be improved by adopting more powerful retrievers [49] that learn from the usage patterns of these user-defined tactics.\nFinally, we did not spend significant effort optimizing the prompt used by PALM, since our focus was not on prompt engineering. Different combinations of instructions or using a more advanced prompting design, such as Chain-of-Thought [45] and Least-to- Most [52] prompting, may improve the performance of PALM. These approaches are worth exploring in future work."}, {"title": "7 RELATED WORK", "content": "Machine Learning for Formal Verification. There have been various machine learning-based techniques that aim to automatically generate formal proofs for different ITPs. ASTactic [48] is the first deep learning-based proof generation technique for ITPs. It leverages Tree-LSTM [41] to model proof states with all Coq terms parsed into abstract syntax trees (ASTs), and searches for a complete proof via depth-first search (DFS). Many other techniques have been proposed to enhance the performance of ASTactic. TacTok [21], for example, models not only the proof states, but also the incomplete proof scripts to provide more context information. To enlarge the search space, Diva [20] combines multiple models that are trained with different hyperparameters, such as learning rate and embedding size, and different orderings of training data. Passport [39] further extends ASTactic and TacTok by adding new encoding mechanisms for identifiers in proof scripts. These techniques are all evaluated on the CoqGym [48] dataset. Proverbot9001 [38] learns to predict the tactics and arguments using an RNN model and a set of manually engineered features. It also leverages advanced search algorithms such as A-star, and several pruning techniques.\nUnlike existing machine learning methods that require significant training, PALM leverages LLMs and does not require any training or fine-tuning. Instead of using search strategies, PALM employs repair mechanisms and a backtracking procedure to address errors and solve the goals that LLMs fail to prove.\nLanguage Models for Formal Verification. Recently, there has been considerable interest in applying LLMs to formal verification. The"}, {"title": "8 CONCLUSION", "content": "Large Language Models (LLMs) have shown promise in automatically generating informal proofs in natural language, but these systems have proven to be less effective at generating formal proofs in interactive theorem provers (ITPs). This paper described a formative study that identified common errors made by GPT-3.5 when generating machine-checked proofs. Guided by these findings, we proposed PALM, which combines LLMs and symbolic methods to automatically prove theorems in an ITP. PALM adopts a premise retriever to select relevant premises such as lemmas and definitions, in order to enhance the quality of proofs generated by an LLMs. It additionally uses a set of repair mechanisms and a backtracking algorithm to correct errors in proof scripts generated by an LLM. We evaluated PALM on a dataset of 10842 theorems. In the evaluation, PALM significantly outperforms existing approaches, and demonstrates its generalizability across different LLMs. Furthermore, our ablation study suggests that all components of PALM are effective."}]}