{"title": "Tangram: A Challenging Benchmark for Geometric Element Recognizing", "authors": ["Jiamin Tang", "Chao Zhang", "Xudong Zhu", "Mengchi Liu"], "abstract": "Significant advancements in Large Multimodal Models (LMMs) have enabled them to tackle complex problems involving visual-mathematical reasoning. However, their ability to identify geometric elements remains understudied. To bridge this gap, we introduce Tangram, a novel benchmark designed to evaluate the performance of LMMs on geometric element recognition. Tangram includes 1,080 diverse geometric diagrams sourced from primary and secondary school exams, competitions, and textbooks, covering from simple basic geometric shapes to complex combinations. Each diagram is associated with four questions, resulting in a total of 4,320 visual-question-answer pairs. Unlike existing benchmarks that seek higher-level cognition and reasoning, Tangram focuses on the understanding of geometric elements, requiring models to perform a \"simple but interesting\" counting task. Systematic evaluation of 10 prominent LMMs, such as GPT-40 and Claude 3.5 Sonnet, shows that even in the seemingly simple task, these models still face significant challenges. Notably, the overall accuracy of the top performer across all tested models is only 56.8%, marking a significant gap when compared to human performance. These findings highlight the limitations of current multimodal artificial intelligence systems in handling basic perception tasks, and will inspire the development of the next generation of expert-level multimodal foundational models. The Tangram and evaluation code will be available soon.", "sections": [{"title": "Introduction", "content": "Recent years have witnessed a technological revolution driven by large language models (LLMs) such as GPT-40 (OpenAI 2024) and Claude 3.5 Sonnet (Anthropic 2024), as well as large multimodal models (LMMs). Their exceptional performance in tasks like natural language processing (NLP) and computer vision (CV) has sparked widespread academic interest in their potential for higher-level cognitive tasks. Particularly in the pursuit of artificial general intelligence(AGI), evaluating the mathematical reasoning capabilities of LLMs and LMMs has become a crucial metric of their potential.\nEarly benchmarks like SVAMP (Patel, Bhattamishra, and Goyal 2021), MATH (Hendrycks et al. 2021), and GSM8K (Cobbe et al. 2021) were widely used to assess the mathematical reasoning capabilities of LLMs. With the rapid development of LMMs, researchers have developed multimodal benchmarks such as MathVista (Lu et al. 2024b), MATH-Vision (Wang et al. 2024), and Math-Verse (Zhang et al. 2024c) to comprehensively evaluate the performance of LMMs on visual context-based mathematical reasoning tasks. The difficulty of these benchmarks has gradually increased, with OlympiadBench (He et al. 2024) raising the bar to the level of Olympic mathematics competitions. However, existing experimental results indicate that LMMs exhibit a significant gap in performance compared to humans in multimodal mathematical reasoning. For example, the authors of MathVerse found that many existing LMMs achieve even higher accuracy without visual input, highlighting their difficulties in understanding mathematical diagrams.\nSolving geometric problems involves multiple aspects such as multimodal information fusion, spatial reasoning, and common-sense knowledge, making it a longstanding challenge in the field of artificial intelligence (Chen et al. 2021; Lu et al. 2021). The GeoEval (Zhang et al. 2024a) benchmark, specifically designed to evaluate the performance of LMMs in solving geometric problems, has shown that while current multimodal foundation models possess certain general capabilities, their performance in geometric reasoning is still significantly inferior to humans. Experiments have also shown that models achieve significantly higher accuracy when dealing with pure text problems. A series of experimental results indicate that existing LMMs have difficulties in understanding mathematical diagrams. To verify this, as shown in Figure 1, we conducted a simple test on GPT-40, and the answer reveals a clear deficiency in GPT-40's understanding of geometric diagrams. However, there is still a lack of specialized benchmarks to verify whether these LMMs truly understand geometric diagrams. Therefore, to advance geometric reasoning tasks, it is crucial to develop a benchmark specifically designed to evaluate the geometric diagram comprehension capabilities of LMMs.\nTo promote progress in geometric reasoning for LMMs, inspired by the traditional Chinese puzzle Tangram, we propose the Tangram benchmark. This benchmark consists of 1,080 geometric diagrams sourced from real-world primary and secondary school exams, competitions and textbooks, covering from simple basic geometric shapes to complex combinations. As illustrated in Figure 7, each diagram is associated with 4 questions and detailed explanations, resulting a total of 4,320 visual-question-answer pairs. Additionally, to more clearly compare the performance of LMMs, we divide the Tangram into three subsets: Tangram-Easy, Tangram-Medium, and Tangram-Hard. Furthermore, due to the complexity of geometric diagrams, differences in the training corpus of LMMs, and variations in the generation styles of different models, using traditional Image Caption methods cannot fairly evaluate the understanding capabilities of LMMs for geometric diagrams. Therefore, we adopt a simple counting task to evaluate the performance of existing LMMs on Tangram. We only require the model to count the letters, triangles, circles, and line segments in the diagram.\nBased on Tangram, we conducted extensive experiments on several well-known closed-source multimodal models such as GPT-40, Gemini 1.5 Pro, Claude 3.5 Sonnet, Qwen-VL-Plus (Bai et al. 2023), and Qwen-VL-Max (Bai et al. 2023), as well as open-source large multimodal models such as InternVL2-40B (Team 2024), DeepSeek-VL-7B (Lu et al. 2024a), InternLM-XComposer2.5-7B (Zhang et al. 2024b), Llama3-LLaVA-NeXT-8B (Liu et al. 2024b), and Yi-VL-34B (AI, Young, and et al. 2024). Experimental results show that the accuracy of the best-performing model is only 50%, significantly lower than the 93.6% accuracy of middle school students and the 99.5% accuracy of human experts. Even with the zero-shot-CoT (Kojima et al. 2022a) method, there was no significant improvement in model performance. These experimental results highlight the limitations of current LMMs in recognizing geometric elements, which may be one of the reasons for their poor performance in multimodal geometric reasoning tasks.\nWe believe that Tangram will contribute to advancing research in geometric diagram understanding, thereby promoting the development of mathematical reasoning and providing new directions for developing more powerful LMMs. In summary, our contributions are as follows:\n\u2022 We propose Tangram, a benchmark consisting of 1,080 geometric diagrams and 4,320 questions. All diagrams are sourced from primary and secondary school exams, competitions and textbooks. Tangram is categorized into three difficulty levels, aiming to evaluate the capability of LMMs to recognize geometric elements.\n\u2022 We design a novel evaluation method that can fairly compare the performance of various models on Tangram.\n\u2022 We conduct extensive experiments on multiple existing LMMs, and the results show that most existing LMMs struggle to accurately recognize geometric elements. Even in our designed simple counting tasks, the performance gap between existing LMMs and humans remains significant, indicating substantial potential for improvement.\n\u2022 Benefited from the fine-grained annotations we provide, we believe that Tangram will provide new insights for the future development of LMMs in geometric mathematical reasoning and multimodal reasoning."}, {"title": "Related Work", "content": "Large Multimodal Models (LMMs) have made significant advancements building upon the success of large language models (LLMs) such as GPT (Radford et al. 2019; Brown, Mann, and et al. 2020; Ouyang et al. 2022), LLaMA (Touvron et al. 2023; Touvron and et al. 2023; Dubey and et al. 2024), and Vicuna (Chiang et al. 2023). These advancements have enabled effective integration of multiple modalities, bridging the gap between visual and textual representations. Typical closed-source models, like GPT-40 (OpenAI 2024), Gemini 1.5 Pro (Team and et al. 2024), and Claude 3.5 Sonnet (Anthropic 2024), have demonstrated state-of-the-art performance across various benchmarks. Concurrently, the open-source community has also contributed numerous LMMs. For instance, LLaVA (Liu et al. 2023, 2024a,b) and MiniGPT-4 (Zhu et al. 2024) adopt simple but effective projection schemes to map image features into the language space. InternLM-XComposer-VL (Zhang et al. 2023; Dong et al. 2024; Zhang et al. 2024b) combine the EVA-CLIP (Sun et al. 2023) vision encoder, perceive sampler, and the InternLM-Chat-7B (Cai and et al. 2024) large language model. Qwen-VL (Bai et al. 2023) enhance multimodal collaboration by introducing a position-aware vision-language adapter. InternVL (Chen et al. 2023, 2024; Team 2024) introduce a progressive image-text alignment strategy and propose QLLaMA as a bridge connecting the vision encoder with an off-the-shelf LLM decoder. These LMMs have exhibited impressive performance on benchmarks such as MMMU (Yue et al. 2024), MathVista (Lu et al. 2024b)"}, {"title": "The Tangram Benchmark", "content": "We introduce Tangram, a novel benchmark comprising 1,080 geometric diagrams. As is shown in Figure 7, each diagram is accompanied by high-quality annotations detailing the counts of geometric elements within the figure, as well as 4 accompanying questions, resulting in a total of 4,320 visual-question-answer pairs. Tangram encompasses both plane and solid geometric diagrams, with each diagram composed of several geometric elements. Our benchmark requires models to count points, triangles, circles, and line segments in the diagrams, aiming to evaluate the recognition capability of large multimodal models for geometric elements."}, {"title": "Motivation", "content": "Large language models (LLMs) and large multimodal models (LMMs) perform well on math word problems. However, their performance is less satisfactory on multimodal math problems, especially those involving complex reasoning in geometry. Early experiments indicate that state-of-the-art models achieve accuracy rates of only 54.4%, 51.0%, and 55.67% on MathVerse (Zhang et al. 2024c), MathVista (Lu et al. 2024b) and GeoEval (Zhang et al. 2024a), respectively. The performance on a series of mathematical reasoning benchmarks demonstrates that existing large multimodal models still have a long way to go in solving geometry problems. As the old Chinese proverb goes, A journey of a thousand miles starts with a single step. We argue that accurately understanding the elements within a geometric diagram is a fundamental prerequisite for conducting effective reasoning, especially prior to engaging in complex reasoning tasks."}, {"title": "Data Collection and Annotation", "content": "In the aforementioned context, we develop Tangram. First, we collect 3,197 geometry problems with corresponding diagrams from online educational websites and textbooks (Gu 2021). Subsequently, we filter these diagrams based on the criterion that each must clearly allow for the counting of geometric elements such as circles and triangles, to avoid issues of discrimination and bias, ensuring the quality and fairness of the data. After a rigorous selection process, 1,080 diagrams are incorporated into Tangram. Finally, the selected geometric diagrams undergo a meticulous annotation phase.\nWe categorize geometric elements into four types (point, circle, triangle, and line segment) for statistical annotation. We recruit 10 rigorously selected master's students majoring in mathematics as annotators to meticulously label each diagram in the Tangram. To ensure annotation accuracy and consistency, we implement a double-checking mechanism where each diagram is evaluated by three independent annotators. If the results of the three annotators are inconsistent, the diagram will be sent to a specialized review team. This team consisted of experienced senior annotators who will re-annotate the disputed diagrams based on detailed annotation guidelines to eliminate uncertainties and ensure the precision of the final annotations. To facilitate and fairly evaluate the performance of different large multimodal models in geometric element recognition, we design corresponding questions for each statistical category. Figure 7 shows an example from Tangram."}, {"title": "Difficulty Classification", "content": "As shown in Figure 3, we categorize the diagrams in Tangram into three levels: Easy, Medium and Hard, based on the total number of elements in each diagram. This classification determines the difficulty of the four corresponding questions. For instance, once a chart is labeled as Hard, its four corresponding geometric element counting questions are also labeled as Hard. Table 1 presents the statistical details of the Tangram benchmark."}, {"title": "Features of Tangram", "content": "To the best of our knowledge, Tangram is the first benchmark specifically designed to assess a model's ability to recognize geometric elements. Unlike existing benchmarks, Tangram does not seek advanced cognition and reasoning, but rather requires the model to perform counting tasks on geometric elements in diagrams. Below we will delve into more features of the Tangram.\nDiverse Elements: The Tangram benchmark includes 1080 plane and solid geometry diagrams, which are rich in common geometric elements such as points, lines, circles, and triangles.\nUncontaminated Data: Tangram is a novel benchmark where all questions are newly constructed visual question-answer pairs, effectively preventing data leakage and ensuring the fairness of all tests.\nDiverse Challenges: Each diagram in Tangram is annotated with a complexity level corresponding to its question, which serves as a guideline for evaluating the proficiency of LMMs in recognizing diagram elements."}, {"title": "Experiments", "content": "In this section, we conduct systematic experiments on Tangram with existing well-known open-source and closed-source LMMs. Our findings reveal that even for simple counting tasks that humans can easily perform, the accuracy of the most advanced LMMs remains low. Furthermore, we compare the performance of open-source and closed-source models, demonstrating that closed-source models generally achieve superior overall performance. Additionally, we compare the models' recognition performance on different geometric elements and conduct both detailed analysis."}, {"title": "Experimental Setup", "content": "LMMs: We evaluate two types of foundational models, open-source and closed-source models, on the Tangram benchmark. Closed-source models are evaluated using their official APIs, while open-source models are evaluated by running inferences on NVIDIA H800 GPUs. For the closed-source models, we select the most representative ones, including GPT-40 (OpenAI 2024), Gemini 1.5 pro (Team and et al. 2024), Claude 3.5 Sonnet (Anthropic 2024), Qwen-VL-Plus (Bai et al. 2023) and Qwen-VL-Max (Bai et al."}, {"title": "Challenging nature of Tangram", "content": "Table 2 clearly demonstrates the challenges posed by the Tangram benchmark for current Large Language Models (LLMs). While GPT-40 achieves the best performance with an accuracy of 56.8%, there is still a significant gap compared to human students (93.6%) and a much larger gap compared to human experts (99.5%)."}, {"title": "Closed-source LMMs are better-performed", "content": "According to the experimental results, the closed-source models generally outperform open-source models. Notably, InternVL-40B achieves an accuracy of 44.9%, approaching the 50.2% accuracy of Claude 3.5 Sonnet. Moreover, InternVL-40B significantly outperforms the closed-source models Qwen-VL-Plus and Qwen-VL-Max. These findings indicate that while open-source models have made substantial progress, there is still significant room for improvement. Furthermore, as depicted in Figure 6, closed-source models exhibit an average accuracy of 42.7%, which is 13.2% higher than the 29.5% average of open-source models."}, {"title": "Comparison among different geometric elements", "content": "Each geometric diagram in Tangram contains a rich variety of geometric elements, and LMMs exhibit significant variations in their ability to recognize different elements. As shown in Figure 5, all models demonstrate the highest recognition rates for circles and letters. We speculate that this is due to the frequent overlap of triangles and lines within a single diagram. This finding indirectly suggests that current LMMs have limitations in recognizing overlapping elements. In addition, according to Table 3, the accuracy of the model on solid geometric diagrams is significantly lower than that on plane diagrams, which is because usually solid geometry are more complex and abstract, resulting in a model that is more difficult to understand."}, {"title": "Conclusion", "content": "In this paper, we present Tangram, a benchmark designed to evaluate the capability of large multimodal models (LMMs) in recognizing geometric elements. This benchmark comprises 1,080 geometric diagrams and 4,320 questions, with each diagram classified into one of three difficulty levels. Experiments with various open-source and closed-source models reveal that as the complexity of the diagrams increases, the accuracy of nearly all models in recognizing geometric elements declines. Furthermore, a notable gap still exists between LMMs and human performance on Tangram, underscoring the need for further advancements in LMMs' visual comprehension. Tangram highlights that most existing LMMs have difficulty accurately recognizing geometric diagrams, especially when dealing with overlapping geometric elements. We anticipate that this work will offer new insights into multimodal mathematical reasoning, advancing the field by enhancing the visual comprehension capabilities of LMMs."}]}