{"title": "Tangram: A Challenging Benchmark for Geometric Element Recognizing", "authors": ["Jiamin Tang", "Chao Zhang", "Xudong Zhu", "Mengchi Liu"], "abstract": "Significant advancements in Large Multimodal Models (LMMs) have enabled them to tackle complex problems involving visual-mathematical reasoning. However, their ability to identify geometric elements remains understudied. To bridge this gap, we introduce Tangram, a novel benchmark designed to evaluate the performance of LMMs on geometric element recognition. Tangram includes 1,080 diverse geometric diagrams sourced from primary and secondary school exams, competitions, and textbooks, covering from simple basic geometric shapes to complex combinations. Each diagram is associated with four questions, resulting in a total of 4,320 visual-question-answer pairs. Unlike existing benchmarks that seek higher-level cognition and reasoning, Tangram focuses on the understanding of geometric elements, requiring models to perform a \"simple but interesting\" counting task. Systematic evaluation of 10 prominent LMMs, such as GPT-40 and Claude 3.5 Sonnet, shows that even in the seemingly simple task, these models still face significant challenges. Notably, the overall accuracy of the top performer across all tested models is only 56.8%, marking a significant gap when compared to human performance. These findings highlight the limitations of current multimodal artificial intelligence systems in handling basic perception tasks, and will inspire the development of the next generation of expert-level multimodal foundational models. The Tangram and evaluation code will be available soon.", "sections": [{"title": "Introduction", "content": "Recent years have witnessed a technological revolution driven by large language models (LLMs) such as GPT-40 (OpenAI 2024) and Claude 3.5 Sonnet (Anthropic 2024), as well as large multimodal models (LMMs). Their exceptional performance in tasks like natural language processing (NLP) and computer vision (CV) has sparked widespread academic interest in their potential for higher-level cognitive tasks. Particularly in the pursuit of artificial general intelligence(AGI), evaluating the mathematical reasoning capabilities of LLMs and LMMs has become a crucial metric of their potential.\nEarly benchmarks like SVAMP (Patel, Bhattamishra, and Goyal 2021), MATH (Hendrycks et al. 2021), and GSM8K (Cobbe et al. 2021) were widely used to assess"}, {"title": "Related Work", "content": "Large Multimodal Models (LMMs) have made significant advancements building upon the success of large language models (LLMs) such as GPT (Radford et al. 2019; Brown, Mann, and et al. 2020; Ouyang et al. 2022), LLaMA (Touvron et al. 2023; Touvron and et al. 2023; Dubey and et al. 2024), and Vicuna (Chiang et al. 2023). These advancements have enabled effective integration of multiple modalities, bridging the gap between visual and textual representations. Typical closed-source models, like GPT-40 (OpenAI 2024), Gemini 1.5 Pro (Team and et al. 2024), and Claude 3.5 Sonnet (Anthropic 2024), have demonstrated state-of-the-art performance across various benchmarks. Concurrently, the open-source community has also contributed numerous LMMs. For instance, LLaVA (Liu et al. 2023, 2024a,b) and MiniGPT-4 (Zhu et al. 2024) adopt simple but effective projection schemes to map image features into the language space. InternLM-XComposer-VL (Zhang et al. 2023; Dong et al. 2024; Zhang et al. 2024b) combine the EVA-CLIP (Sun et al. 2023) vision encoder, perceive sampler, and the InternLM-Chat-7B (Cai and et al. 2024) large language model. Qwen-VL (Bai et al. 2023) enhance multimodal collaboration by introducing a position-aware vision-language adapter. InternVL (Chen et al. 2023, 2024; Team 2024) introduce a progressive image-text alignment strategy and propose QLLaMA as a bridge connecting the vision encoder with an off-the-shelf LLM decoder. These LMMs have exhibited impressive performance on benchmarks such as MMMU (Yue et al. 2024), MathVista (Lu et al. 2024b)"}, {"title": "Multimodal Reasoning Benchmarks", "content": "As large multimodal models (LMMs) advance, the academic community has shown increasing interest in evaluating the capabilities of them. In the early stages, datasets such as COCO (Lin et al. 2014), NoCaps (Agrawal et al. 2019), and Flickr30K (Plummer et al. 2015) are primarily used for image captioning tasks, while VQAv2 (Goyal et al. 2017), TextVQA (Singh et al. 2019), and GQA (Hudson and Manning 2019) are applied to visual question answering tasks. Researchers have achieved significant results on these datasets as LMMs have evolved. However, these datasets often focus on single tasks and cannot comprehensively reflect the overall performance of LMMs. Therefore, recent studies have evaluated them from various perspectives. For instance, LVLM-eHub (Xu et al. 2023) collects 47 existing benchmarks to assess six capabilities of LMMs, but it does not create any new benchmarks. While MME (Fu et al. 2024) can comprehensively measure a model's perceptual and cognitive abilities, its question types are relatively simple, requiring only yes or no answers. Additionally, MMBench (Liu et al. 2024c) and SEED-Bench (Li et al. 2023) contain a large number of multiple-choice questions, covering a wide range of ability dimensions, but these datasets primarily consist of common-sense questions that do not require extensive domain knowledge or complex reasoning. To strengthen the evaluation of specific domain knowledge, ScienceQA (Lu et al. 2022) was introduced, covering a wide range of scientific topics from elementary to high school. MathVista (Lu et al. 2024b), on the other hand, proposes a series of visually challenging problems, but its scope is limited to the field of mathematics. In comparison, MMMU (Yue et al. 2024) includes more difficult expert-level problems that cover 30 different subjects and require nuanced perception, recalling domain-specific knowledge to derive the solution. MathVerse (Zhang et al. 2024c) focuses on mathematical problems and expands the dataset size to 20K questions. The problems in MATH-Vision (Wang et al. 2024) are sourced from competitions, but their excessive difficulty hinders the ability to effectively differentiate the performance of various LMMs."}, {"title": "The Tangram Benchmark", "content": "We introduce Tangram, a novel benchmark comprising 1,080 geometric diagrams.  Each diagram is accompanied by high-quality annotations detailing the counts of geometric elements within the figure, as well as 4 accompanying questions, resulting in a total of 4,320 visual-question-answer pairs. Tangram encompasses both plane and solid geometric diagrams, with each diagram composed of several geometric elements. Our benchmark requires models to count points, triangles, circles, and line segments in the diagrams, aiming to evaluate the recognition capability of large multimodal models for geometric elements."}, {"title": "Motivation", "content": "Large language models (LLMs) and large multimodal models (LMMs) perform well on math word problems. However, their performance is less satisfactory on multimodal math problems, especially those involving complex reasoning in geometry. Early experiments indicate that state-of-the-art models achieve accuracy rates of only 54.4%, 51.0%, and 55.67% on MathVerse (Zhang et al. 2024c), MathVista (Lu et al. 2024b) and GeoEval (Zhang et al. 2024a), respectively. The performance on a series of mathematical reasoning benchmarks demonstrates that existing large multimodal models still have a long way to go in solving geometry problems. As the old Chinese proverb goes, A journey of a thousand miles starts with a single step. We argue that accurately understanding the elements within a geometric diagram is a fundamental prerequisite for conducting effective reasoning, especially prior to engaging in complex reasoning tasks."}, {"title": "Features of Tangram", "content": "To the best of our knowledge, Tangram is the first benchmark specifically designed to assess a model's ability to recognize geometric elements. Unlike existing benchmarks, Tangram does not seek advanced cognition and reasoning, but rather requires the model to perform counting tasks on geometric elements in diagrams. Below we will delve into more features of the Tangram.\nDiverse Elements The Tangram benchmark includes 1080 plane and solid geometry diagrams, which are rich in common geometric elements such as points, lines, circles, and triangles.\nUncontaminated Data Tangram is a novel benchmark where all questions are newly constructed visual question-answer pairs, effectively preventing data leakage and ensuring the fairness of all tests.\nDiverse Challenges Each diagram in Tangram is annotated with a complexity level corresponding to its question, which serves as a guideline for evaluating the proficiency of LMMs in recognizing diagram elements."}, {"title": "Experiments", "content": "In this section, we conduct systematic experiments on Tangram with existing well-known open-source and closed-source LMMs. Our findings reveal that even for simple counting tasks that humans can easily perform, the accuracy of the most advanced LMMs remains low. Furthermore, we compare the performance of open-source and closed-source models, demonstrating that closed-source models generally achieve superior overall performance. Additionally, we compare the models' recognition performance on different geometric elements and conduct both detailed analysis."}, {"title": "Experimental Setup", "content": "LMMs We evaluate two types of foundational models, open-source and closed-source models, on the Tangram benchmark. Closed-source models are evaluated using their official APIs, while open-source models are evaluated by running inferences on NVIDIA H800 GPUs. For the closed-source models, we select the most representative ones, including GPT-40 (OpenAI 2024), Gemini 1.5 pro (Team and et al. 2024), Claude 3.5 Sonnet (Anthropic 2024), Qwen-VL-Plus (Bai et al. 2023) and Qwen-VL-Max (Bai et al."}, {"title": "Challenging nature of Tangram", "content": "Table 2 clearly demonstrates the challenges posed by the Tangram benchmark for current Large Language Models (LLMs). While GPT-40 achieves the best performance with an accuracy of 56.8%, there is still a significant gap compared to human students (93.6%) and a much larger gap compared to human experts (99.5%)."}, {"title": "Closed-source LMMs are better-performed", "content": "According to the experimental results, the closed-source models generally outperform open-source models. Notably, InternVL-40B achieves an accuracy of 44.9%, approaching the 50.2% accuracy of Claude 3.5 Sonnet. Moreover, InternVL-40B significantly outperforms the closed-source models Qwen-VL-Plus and Qwen-VL-Max. These findings indicate that while open-source models have made substantial progress, there is still significant room for improvement. Furthermore, as depicted in Figure 6, closed-source models exhibit an average accuracy of 42.7%, which is 13.2% higher than the 29.5% average of open-source models."}, {"title": "Comparison among different geometric elements", "content": "Each geometric diagram in Tangram contains a rich variety of geometric elements, and LMMs exhibit significant variations in their ability to recognize different elements. As shown in Figure 5, all models demonstrate the highest recognition rates for circles and letters. We speculate that this is due to the frequent overlap of triangles and lines within a single diagram. This finding indirectly suggests that current LMMs have limitations in recognizing overlapping elements. In addition, according to Table 3, the accuracy of the model on solid geometric diagrams is significantly lower than that on plane diagrams, which is because usually solid geometry are more complex and abstract, resulting in a model that is more difficult to understand."}, {"title": "Conclusion", "content": "In this paper, we present Tangram, a benchmark designed to evaluate the capability of large multimodal models (LMMs) in recognizing geometric elements. This benchmark comprises 1,080 geometric diagrams and 4,320 questions, with each diagram classified into one of three difficulty levels. Experiments with various open-source and closed-source models reveal that as the complexity of the diagrams increases, the accuracy of nearly all models in recognizing geometric elements declines. Furthermore, a notable gap still exists between LMMs and human performance on Tangram, underscoring the need for further advancements in LMMs' visual comprehension. Tangram highlights that most existing LMMs have difficulty accurately recognizing geometric diagrams, especially when dealing with overlapping geometric elements. We anticipate that this work will offer new insights into multimodal mathematical reasoning, advancing the field by enhancing the visual comprehension capabilities of LMMs."}, {"title": "Distributions of The Tangram", "content": "Figure 6 presents a distributions of the Tangram benchmark. This benchmark consists of a total of 1,080 geometric diagrams, categorized into three difficulty levels. These diagrams are sourced from real-world primary and secondary school exams, competitions, and textbooks."}, {"title": "B.2 Instruction Prompt Used for Evaluating Models", "content": "As shown in Table 5, we combined the question with the diagram and fed them into LMMs to to generate responses. Furthermore, we conducted evaluations using both Zero-shot"}]}