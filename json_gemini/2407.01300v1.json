{"title": "Collaborative Performance Prediction for Large Language Models", "authors": ["Qiyuan Zhang", "Fuyuan Lyu", "Xue Liu", "Chen Ma"], "abstract": "Comprehensively understanding and accurately predicting the performance of large language models across diverse downstream tasks has emerged as a pivotal challenge in NLP research. The pioneering scaling law on downstream works (Hu et al., 2024; Isik et al., 2024) demonstrated intrinsic similarities within model families and utilized such similarities for performance prediction. However, they tend to overlook the similarities between model families and only consider design factors listed in the original scaling law. To overcome these limitations, we introduce a novel framework, Collaborative Performance Prediction (CPP), which significantly enhances prediction accuracy by leveraging the historical performance of various models on downstream tasks and other design factors for both model and task. We also collect a collaborative data sourced from online platforms containing both historical performance and additional design factors. With the support of the collaborative data, CPP not only surpasses traditional scaling laws in predicting the performance of scaled LLMs but also facilitates a detailed analysis of factor importance, an area previously overlooked.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) (Brown et al., 2020; Ouyang et al., 2022) have emerged as one of the most important AI research powered by large-scale parameters, high computational resources, and massive training data. With the substantial increase in model sizes, the evaluation cost of LLMs' performance becomes even more significant. For example, testing a single LLM on certain benchmarks often requires $10K+ and 4K+ GPU hours (Liang et al., 2023). Therefore, understanding the behaviors and predicting the capabilities of LLMs across scales under various tasks becomes a vital question (Ganguli et al., 2022a; Owen, 2024; Finnveden, 2020; Hu et al., 2024) for both researchers and engineers.\nScaling laws (Kaplan et al., 2020; Hoffmann et al., 2022; Hernandez et al., 2022; Gordon et al., 2021; Bahri et al., 2024; Muennighoff et al., 2023) have been powerful tools for predicting the capabilities of LLMs. It indicates a power-law correlation between the model performance and design factors such as computational measure (FLOPs) utilized during training. Although the scaling law was originally proposed as a strong intuitive guide for designing LLM, researchers (Hu et al., 2024; Ruan et al., 2024; Isik et al., 2024) have extended its utility into predicting model performances on various metrics, such as BLEU in Machine Translation, and different tasks. These works can accurately predict model performances by utilizing the similarity within each model family, e.g., models within each family are usually trained on the same dataset. However, there are several issues rooted in their methods: the performance prediction 1) requires transparent design factors that consume substantial training resources to fit the curve, 2) is only tailored to a certain model family and a specific task metric, and 3) neglects the connections among different models and tasks.\nThe aforementioned limitations motivate us to design more effective methods for predicting the performance of LLMs on downstream tasks. Two observations sparked our attention. Firstly, A strong similarity exists between model families, e.g.LLama-family and GPT family. Models from different families behave similarly in prediction distribution (Shrivastava et al., 2023) and emergent phenomenon (Wei et al., 2022). Secondly, with the emerging LLM models and the increasingly diverse tasks, the cost of enumerating and benchmarking models with tasks increases exponentially. Therefore, we aim to utilize the similarities across model families in order to collaboratively predict the model performance over diverse tasks in an accurate yet efficient way.\nTo incorporate the aforementioned intuitions, we propose a new scheme, Collaborative Performance Prediction (CPP), to efficiently predict the performance of LLMs on evaluation tasks. This scheme learns the latent representations of LLMs and tasks, which captures the intrinsic similarity among different models and tasks. The interaction (e.g., inner product) between the latent representations of LLMs and tasks can be utilized to predict the performance of LLMs on certain tasks. To fulfil the proposed scheme, we collect the LLM performance data from academic papers, technical reports, and open leaderboards covering 72 models and 29 tasks. To summarize, our scheme has several advantages:\n\u2022 Low Training Cost: Compared with methods (Hu et al., 2024) that extend scaling law to various downstream tasks, no pre-training or fine-tuning of LLM is required in our scheme.\n\u2022 Prediction over proprietary model: Unlike previous methods (Ruan et al., 2024), our scheme supports prediction over proprietary models without knowing key design factors, such as computational measures.\n\u2022 Prediction from small to large: By utilizing cross-family information, our scheme can accurately estimate model performance, e.g., emergent ability, of large models on downstream tasks given the information from small models.\n\u2022 Beyond Scaling Laws: Our scheme is more general and can incorporate diverse factors, such as task description factors.\n\u2022 Factor-level Interpretability: Our scheme can provide interpretability by analyzing the factors importance of LLMs.\nUnder our scheme, multiple customized prediction methods (e.g., COLLABORATIVE FILTERING (Koren et al., 2022)) can be incorporated to predict the performance of LLMs, further validating the feasibility and generality. Our method enables more diverse factors as input, ranging from traditional LLM design factors to task design factors, e.g., targeted ability and few-shot setting.\nUpon extensive experimentation within the open-released core leaderboard of HELM (Liang et al., 2023) and our collected historical matrix, our predictive performance demonstrated exceptionally well. Specifically, even without any input of model factors or task factors: in HELM, we use 50% of the scores to predict the other 50%, the predictive ranking (derived from predicted scores) achieves $Accuracy=10%$, and $MAE@2=39%$; in our collected matrix (characterized by a 44% sparsity level) achieves an $Accuracy=45%$, and the $MAE@2=84%$. Notably, the accuracy of our prediction from small to large LMs significantly exceeded that predicted by scaling laws. Using an analysis method similar to SHAPLEY-VALUES (Lundberg and Lee, 2017; Shapley, 1952), we elucidate the importance of different factors, which surprisingly does not fully align with scaling law (Kaplan et al., 2020). Therefore, our method is undoubtedly more versatile."}, {"title": "2 Related Work", "content": "2.1 Downstream Scaling Law and\nPerformance Predictability of LLM\nScaling laws (Kaplan et al., 2020; Hoffmann et al., 2022; Hernandez et al., 2022; Bahri et al., 2024; Muennighoff et al., 2023) for LLMs have increasingly become a focal point in understanding and guiding critical design decisions, such as model size and the characteristics and volume of pre-training data. Traditionally, most research in this area has concentrated on how measures like cross-entropy loss or perplexity scale. Subsequent studies have extended these efforts to the scaling behavior on translation (Isik et al., 2024; Ghorbani et al., 2021; Zhuocheng et al., 2023) and other downstream tasks modeling (Caballero et al., 2023; Henighan et al., 2020). The high predictability in LLMs capability has directly spurred extensive research work (see Survey Anwar et al. (2024)) exploring whether LLMs can demonstrate predictability on downstream tasks, which are considered highly unpredictable in traditional ML knowledge (Ganguli et al., 2022a). Particularly, the \"emergence\" phenomenon (Suzgun et al., 2022; Wei et al., 2022) has challenged predictability, where models suddenly exhibit striking capabilities at specific training reources. Recent studies (Schaeffer et al., 2023) have made remarkable achievements in breaking the discontinuities in performance brought about by emergence, and Ganguli et al. (2022a); Owen (2024); Finnveden (2020) demonstrated the predictability on downstream tasks, for instance, Hu et al. (2024) directly fits a curve of training resources and downstream task performance by repeatedly pretraining a specific model. Furthermore, Arora and Goyal (2023) predicted the performance through decomposing the complex capabilities of LMs to some base skills.\nGiven that predictability has now been established, we reassess the underlying premises that enable this predictability: the prevailing similarities across multiple models and various downstream tasks (Liu et al., 2023; Perlitz et al., 2024; Polo et al., 2024; Torregrossa et al., 2020; Ili\u0107, 2023). Based on this, we step beyond the limitations defined by scaling laws and propose a new methodology to predict the performance of LLMs on various downstream tasks.\n2.2 Collaborative Filtering\nCollaborative filtering (CF) (Koren et al., 2022) is a widely used technique in recommendation systems that predicts users' preferences by collecting the historical preferences of many other users. The underlying assumption of CF is that similar users will share similar preferences on similar items. A seminal method in CF is matrix factorization (Koren et al., 2009) (MF). It reduces the dimensionality of the user-item matrix by learning the latent factors associated with users and items, respectively. This approach helps handle sparse data and improves scalability. The factorization of the user-item matrix $R$ can be represented as\n$R \\approx P^T \\cdot Q,$\nwhere each column vector in $P$ and $Q$ represents a specific user or item, respectively, with hidden dimension $d$. The latent representations of users and items capture the user preferences and item properties in the latent space, and the inner product can be utilized to predict the interaction between users and items. To optimize the latent feature vectors, the following loss function is employed:\n$ \\min_{P,Q} \\sum_{(u,i) \\in \\Omega} (r_{ui} - p_u^T q_i)^2 ,$\nwhich measures the squared differences between the observed ratings $r_{ui}$ and the ratings predicted by the model $p_u^T q_i$ for each user-item pair $(u, i)$ in the set $\\Omega$ of observed interactions.\nHere, Yang et al. (2019) transferred the collaborative filtering for ML model selection by predicting the cross-valided errors, which demonstrates CF's adaptability and efficiency in diverse application areas."}, {"title": "3 Background and Pilot Demonstration", "content": "3.1 Scaling Law on Downstream Tasks\nFor classic scaling laws, researchers propose a hypothesized power-law relationship between a model's computational measures $C_m$ (e.g., training FLOPs) and their performance loss $L_m$ (e.g., perplexity). Specifically, for a model $m$ within a family $f$ (e.g., Llama-2 7B, 13B, and 70B), the relationship is hypothesized as\n$log(L_m) \\approx w_f log(C_m) + b_f,$\nwhere $w_f$ and $b_f$ are scaling coefficients customized for each model family. Researchers fit this formula through repeated scaling experiments, then use it to accurately predict performance when larger-scale ($C' > C$). Some studies (Finnveden, 2020; Owen, 2024) have adapted scaling laws to specific downstream task metrics, proposing that sigmoidal functions are more suitable for predictions, as follows:\n$\\sigma^{-1}(S_m) \\approx w_f log(C_m) + b_f,$\nwhere $S_m$ refers to the normalized downstream scores of models within the range [0,1]. However, applying scaling laws across different model families on various specific tasks presents a trade-off: fitting unique coefficients for each evaluation scenario (e.g., Llama 2 on MMLU) is a resource-intensive endeavor (Hu et al., 2024); alternatively, estimating these coefficients using a limited number (3-5) of models within the same family may compromise the accuracy of the predictions. Moreover, the recent work (Ruan et al., 2024) extends scaling law by incorporating latent variables to capture the patterns across model families and tasks.\n3.2 Pilot Demonstration on HELM\nScaling laws reveal that models from any family exhibit a similar performance trend as computational measures increase. This insight suggests there are commonalities and connections between different models. These motivate us to employ the MF method to explore more similarities beyond computational measures, e.g., the relationship among the different model families and tasks.\nWe perform the aforementioned MF on the benchmark matrix to observe the error gap between predicted and truth (normalized) scores. Specifically, we select the core leaderboard provided by HELM for our exploratory experiments with only model name, task name, and performance scores. This leaderboard, 68 models and 16 tasks, presented in a score matrix with a density of 82.5%, which includes both open-source and proprietary models, e.g., GPT-4 and Jurassic-2. Our method treats all models and tasks as independent entities without introducing any prior similarity factors. We hope to observe whether MF can predict the remaining scores, giving a small part of the matrix, where we evaluate two training/validation sets split strategies: 10%/90%, 50%/50%. As illustrated in Figure 2, MF can accurately predict most of the missing scores within a low error range, which proves that it can encode the similarity across the model and the task without regression depending on explicit computational measures variable."}, {"title": "4 Collaborative Performance Prediction", "content": "4.1 Definition\nMotivated by pilot experiments, we introduce the concept of \"Collaborative Performance Prediction\" (CPP) to facilitate the performance prediction of LLMs.\nDefinition 1. Let $M = \\{M_1, M_2,..., M_n\\}$ be a set of $n$ LLMs, and $T = \\{T_1, T_2, ..., T_m\\}$ be a suite of $m$ tasks. Define the Score Matrix $S$, which is an $n \\times m$ matrix where each element $s_{ij}$ represents the performance score of model $M_i$ on task $T_j$. $s_{ij}$ is defined as\n$ s_{ij} = \\begin{cases}\n        score & \\text{if tested,}\\\\\n        unknown & \\text{otherwise.}\n        \\end{cases}$\nFunction: Employ an prediction method $F$ to estimate the unknown elements of $S$, denoted by $\\hat{s_{ij}}$, based on the known values.\nExtention: Accommodate model design factors $V_m = \\{V^m_1, V^m_2,...., V^m_{M}\\}$ , such as common computational meatures, and task design factors $V_t = \\{V^t_1, V^t_2,...,V^t_{T}\\}$, such as targeted capabilities and few-shot settings.\nBased on this definition, our framework consists of two components: 1) collaborative performance data, 2) collaborative prediction methods. We anticipate that an accurate score can be predicted based on the historical performance of various models on downstream tasks and other design factors for both model and task. Moreover, we can incorporate or solely rely on the factors describing the LLM and the associated downstream tasks.\n4.2 Collaborative Data\nUnlike the scaling law approach, which requires training resource factors to obtain the correlation between metric scores and factors at a high training cost, our proposed method makes use of evaluation results and other design factors reported from existing studies, referred to as collaborative data. Open-source leaderboards, such as Open-LLM, HELM, and OpenCompass, have made tremendous efforts on this issue in fairly evaluating different LLMs. Our efforts extend beyond merely (Ruan et al., 2024) utilizing data from open-source leaderboards with matrix sparsity of 0%. We also extract test results from different models' papers, technical reports, and model cards. Ultimately, we have collected a score matrix of n = 72, m = 29 with a density of only 56%. Furthermore, we collected 12 and 4 detailed design factors for models and tasks. These details are listed in Appendix B.1. Our data analysis is shown in Figure 3 and Figure 8.\nData Analysis. Based on the collective data, we can make the following observations: a) Uneven distribution of testing resources. We observe significant variability in the deployment of testing efforts, as shown in Figure 3. For instance, models from the LLAMA series have undergone extensive testing across various tasks, in contrast to models like GOPHER, where testing has largely stagnated. A similar disparity is also evident among tasks, with MMLU and HELLASWAG receiving considerable evaluation, whereas RACE has been relatively underexplored. This trend suggests that as LLMs proliferate and tasks evolve, scores across the matrix will increasingly skew. This leads to a pronounced long-tail effect in testing coverage for many tasks, barring a few that consistently receive comprehensive evaluations. b) Widespread variations in the scores. It is noteworthy that identical models yield varying scores on the same tasks across different studies (Shrivastava et al., 2023; AI@Meta, 2024), a variation often attributed to differences in prompt settings, model versions, and the volume of test samples employed. Typically, these score variations range within 0.1, with scores normalized between [0, 1]. This phenomenon underscores the importance of public leaderboards and highlights researchers' need to articulate their testing frameworks when performing customized evaluations. When conflicted, we prefer the results from the Open-LLM leaderboard in the collective data. c) Missing description/model card. We advocate for consistently providing complete model cards for open-source and proprietary models. Such a phenomenon is shown in Figure 8 and, unsurprisingly, a long-tail distribution is witnessed. While it is understandable that proprietary models might withhold specific details about parameters, they can still divulge information about parameter scale and the extent of pre-training. Furthermore, we recommend a more thorough description of testing tasks, including suggested few-shot settings and detailed descriptions of targeted capabilities.\n4.3 Prediction Methods\nIn Section 2.2, classical collaborative filtering methods are inspired to conduct the performance prediction. In principle, most collaborative filtering methods can be applied. Here, in addition to the abovementioned MF, we also leverage neural collaborative filtering (He et al., 2017) (NCF) meth-ods, which uses a multi-layer perceptron to learn the model-task interaction function to predict the score $s_{ij}$ for a model $i$ on a task $j$, providing a way to learn non-linearities in the data:\n$\\hat{s_{ij}} = f(i, j|M, T, [V_i, V_j], \\theta)\n= MLP(p_i, q_j, [ev_i, ev_j]),$\nwhere $M$ and $T$ denote the sets of collaborative models and tasks, and their descriptive factors $V_i$, $V_j$ optionally enrich the input data. Here, $p_j$ and $q_j$ are the latent vectors for model $i$ and task $j$ that capture the intrinsic properties of models and tasks, as well as embeddings $[ev_i, ev_j]$ derived from their descriptive factors, and $\\theta$ represents the parameters of NCF.\nMoreover, we further simplify the model to verify whether it is feasible to predict a score when only inputting the descriptive factors $V_i$, $V_j$ into the prediction model:\n$\\hat{s_{ij}} = f(i, j |V_i, V_j, \\theta)\n= MLP(ev_i, ev_j),$\nFor both settings, where the goal is to predict the scores accurately, the loss function can be defined as follows:\n$L(\\theta) = \\frac{1}{N} \\sum_{(i,j) \\in D} (s_{ij} - \\hat{s_{ij}})^2,$\nwhere N is the total number of scores set D for training, and $s_{ij}$ is the true score for model i and task j."}, {"title": "5 Experiments", "content": "In this section, we evaluate the feasibility of CPP from an overall benchmark perspective and a model perspective in Section 5.1 and 5.2, respectively; we then analyze the importance of factors for both models and tasks in Section 5.3. Additionally, a substantial amount of ablation and analysis is placed in the appendix D, such as sparsity, the correlations in tasks and models, and which models and tasks are more critical for prediction.\nExperimental Setting. Our validation framework utilizes the aforementioned collaborative dataset as the score matrix S. We partition scores ${s_{ij}}$ into train and validation set, detailed in Appendix C.2.\nEvaluation Metric. To accurately evaluate CPP, we adopt two types of metrics: 1) SCORE-LOSS metrics including MSE Loss and L1 Loss between predicted scores and true scores (normalized) on downstream tasks and 2) RANK-ACCURACY metrics including ACCURACY and MAE@2 between the rank of predicted scores and true scores. We elaborate on these metrics in Appendix C.1.\n5.1 Evaluation from Benchmark Perspective\nIn this study, we select the abovementioned methods, MF and NCF, to verify whether $s_{ij}$ can be accurately predicted based on the input of model i and task j. To examine whether enhancements are helpful, we modify NCF to support the input of design factors, detailed in Appendix C.2. Based on Figure 4 and Table 1, we can make the following observations:\nFirst, all methods accurately predicted model performance, demonstrating that collaborative filtering mechanisms can predict model outcomes based on collaborative data across different models and tasks. This prediction is achieved without explicit scaling factors or fitting a log-power curve. Second, from MF to NCF, the transformation in interaction mechanisms further enhances accuracy, suggesting that model improvements can further augment the efficacy of our methodology. Additionally, we further increased accuracy by incorporating factors, such as model scaling variables and task descriptions, into the NCF framework alongside ID information. This confirms that incorporating explicit factors can enhance model and task similarities. Finally, among all metrics, we particularly noted that the accuracy of the predictive ranking was acceptable. In other words, researchers can use our method to accurately predict the ranking range of their developed models on test tasks, thereby enhancing model performance on specific tasks.\nPredictability with Only Description Factors. We validate whether high predictive accuracy can still be achieved by only inputting the models' and tasks' design factors. As demonstrated in Table 1, the accuracy of predicted rankings (derived from predicted scores) remains high, affirming that our method supports predictions based solely on factors. However, the accuracy is lower than other models, suggesting that finer-grained latent similarities remain encoded as potential factors within the identity information across different models and tasks.\n5.2 Evaluation from Model Perspective\nTo mimic the utilization of CPP in the real world, this section takes a model perspective to investigate the predictive accuracy of CPP upon each model. Specifically, we propose two scenarios: (i) prediction with no prior testing information and (ii) prediction with prior testing information on 2 tasks. These two scenarios correspond to real-world cases when the model has not been developed or is tested on a few tasks and expects an accurate prediction of its ability on other tasks. In both scenarios, we focus on larger LLMs, e.g., LLama2-70b, as they are more computationally expensive to develop and test, requiring an accurate LLM prediction.\nWe report the results of CPP and SL on both scenarios in Figure 5 and can draw the following conclusions. Under the CPP-0 scenario, CPP demonstrated greater adaptability across different tasks compared to SL, with points closely aligned along the y = x line (\"perfect prediction\") in Figure 5 (a). This suggests that CPP has effectively captured task-specific characteristics, such as value ranges, whereas SL, despite achieving a lower MSE-LOSS, tends to concentrate its predictions around 0.5. Under the CPP-2 scenario, the distribution of points of CPP is noticeably closer to y = x, as shown in Figure 5 (b), and its MSE-LOSS is also lower than that of SL. This indicates that leveraging performance data from other tasks considerably enhances the model's cross-task prediction capabilities, underscoring a degree of consistency across tasks for the same model. This approach demonstrates that predictions for scaling LLMs on downstream tasks can be dynamically improved by evaluating performance on less computationally intensive tasks and using those outcomes to predict scores on subsequent tasks more accurately.\n5.3 Factor Importance Analysis via SHAPLEY-VALUE\nIn this section, we aim to analyze each design factor's importance over CPP. The Shapley value, a concept derived from cooperative game theory (Shapley, 1952), offers a systematic approach to measuring individual factors' contribution in predictive models (Lundberg and Lee, 2017; Covert et al., 2021). Visualization for Shapley values of each design factor is shown in Figure 6.\nBased on Figure 6 (a), we can make the following observations regarding model factors. First, we have discovered that in addition to traditionally important factors such as training data size and parameter size mentioned in scaling law (Kaplan et al., 2020), other design factors significantly influence predictive outcomes. These include the model family, context window size, and batch size. Second, the importance of the model family cannot be overlooked, as it may relate to differences in data quality across models, including proprietary data or specific architectural details. For instance, using a particular model family might mean adopting architectures or optimization techniques better suited to specific tasks. Moreover, the size of the context window also significantly affects model performance. A larger context window allows the model to better understand the context in long texts, which is particularly crucial for long-context LLMs (Xiong et al., 2023). Experience (Google, 2024) has shown that such models perform better across various tasks. Batch size, as another crucial factor, affects the stability and speed of model training. An appropriate batch size ensures a balance between the accuracy of gradient estimation and computational efficiency during training.\nAs for the importance of task factors, results in Figure 6 (b) show that the target ability among all factors is more important. This also implies that similarities between the domains of different tasks can help predict outcomes. This conclusion is consistent with previous observations (Ruan et al., 2024; Perlitz et al., 2024; Polo et al., 2024)\nIn summary, these findings indicate that LLMs performance prediction should not rely solely on traditional design factors limited by scaling law but also on other key factors that might impact overall model performance."}, {"title": "6 Conclusion and Discussion", "content": "Advancing beyond traditional scaling laws on downstream tasks, we propose a collaborative performance prediction framework for large language models. It offers significant advantages, including easy deployment, low training costs, and superior predictive accuracy. Uniquely, it enables incorporating additional design factors and supports an in-depth analysis of their impact, including factor importance and correlations in models and tasks. For prediction, we collect collaborative data containing many historical performances and factors.\nOur method's predictive accuracy is expected to improve as it benefits from an expanding pool of collaborative data. Moreover, this approach highlights the potential to identify neglected but vital factors beyond traditional scaling laws, such as task design factors, thereby enriching our comprehension of LLM performance predictability on downstream tasks."}, {"title": "Limitations", "content": "\"Single-source-of-truth\". When collecting the collaborative data, we hypothesize that each model's performance on each task is identical. However, in the real world, the detailed testing setting, for instance, the testing prompt writing, can influence LLM's performance variance. Although we observed this, we only saved one score from different sources. How to incorporate the setting of testing as an additional dimension remains to be solved in future works.\nSusceptibility to data quality. The prediction accuracy of CPP highly depends on the quality of collaborative data. The current version passively collects collaborative data from online resources. Should information from either of these data sources be incorrect, the prediction capability of CPP would decrease correspondingly. To overcome such a limitation, jointly considering passive information collected from data sources and active information, such as performances of models tested on some tasks by the user, might be a solution. Utilizing techniques such as efficient benchmarking (Perlitz et al., 2024; Polo et al., 2024) could alleviate the cost of obtaining active information."}, {"title": "Ethics Statement", "content": "The data we use are collected from public papers, technical reports, open leaderboards, and model cards on GitHub."}, {"title": "A Pilot Demonstrations using Neural\nCollaborative Filtering", "content": "In this section, we supplemented the error distribution in Figure 7, which is generated using neural collaborative filtering on the HELM lite leaderboard. Compared to Figure 2, it is evident that neural collaborative filtering consistently outperforms MF across each setting."}, {"title": "B Collaborative Data", "content": "B.1 Data Description\nList of Models and Tasks. The table 2 contains all the models and tasks we have collected.\nDescription Factors for Models and Tasks We have collected the characteristics of models and tasks in relevant aspects through model cards, technical reports, and academic papers. We have organized and introduced these characteristics, as well as the corresponding embedding methods, as listed in Table 3.\nNote that during data collection, not all factors are available. For these missing factors, such as CO2 and GPU hours, we replace them as zero when entering data.\nB.2 Data Analysis\nWe conducted a statistical analysis of the data we collected, specifically examining the number of models tested for each task, the number of tasks tested for each model, and the number of models described by each factor. Since each task is consistently associated with four factors, we did not create a distribution chart for this aspect."}, {"title": "C Experimental Setup", "content": "C.1 Evaluation Metrics\nApart from visualization, we also evaluate the method based on two types of metrics: 1) SCORE-Loss Metric: we calculate MSE Loss and L1 Loss between predicted scores and true scores (normalized) on downstream tasks; 2) RANK-ACCURACY Metric: researchers are sometimes not concerned with detailed scores but rather the rankings the model is in, so we calculate the accuracy of rank derived from the predicted scores, ACCURACY and MAE@2. ACCURACY refers to the percentage of instances where the predicted rank equals the true rank, and MAE@2 refers to the per-centage of instances where the absolute difference between the predicted rank and the true rank is in 2, the formulation as below:\n$Accuracy = (\\frac{\\sum_{i=1}^{N} 1(r_i = \\hat{r_i})}{N}) \\times 100\\%,$\n$MAE@2 = (\\frac{\\sum_{i=1}^{N} 1(|r_i - \\hat{r_i}| \\leq 2)}{N}) \\times 100\\%,$\nwhere N is the total number of validation instances, $r_i$ is the true rank, $\\hat{r_i}$ is the predicted rank derived by the predicted score; 1(\u00b7) is the indicator function that evaluates to 1 if the argument is true and 0 otherwise; | | denotes the absolute value.\nC.2 Detailed Setting of Validation Prediction\nAccuracy Experiments\nIn this section, we detail the setup of each experiment in 5.\nDifferent Prediction Methods. Due to the 44% sparsity of the collected collaboration matrix, we used 5% of the known data as the validation set, with the remaining data serving as the observed training set. We trained each model five times through random splitting, deriving an average performance and variance. We configured our models with latent factors = 10, learning rate = 0.01, and iteration = 250,000. The Figure 4 is the results when random_seed = 1.\nPredicting from Small to Large LMs. The focus here is on deriving the scaling law applicable to specific task metrics. Undeniably, traditional methods do not provide a directly usable scaling law across all downstream tasks for comparative analysis. However, we observed in the literature (Ruan et al., 2024) that a sigmoidal curve with a single coefficient and a single bias value represents the scaling law for downstream tasks. Moreover, this curve's coefficients and bias values have a general range across all tasks, w = [0.5, 2], b = [-10, -3]. Consequently, we set this range of coefficients and bias for this curve. Then we used the normalized scores of smaller models within the same model family and their corresponding parameter sizes to fit the scaling law curve for each task. This approach generally follows a \u201cpretrain-finetune\u201d methodology. Additionally, CPP-2 refers to randomly selecting two scores from the observed performances of the model to be included in the training data. In this experiment, we use factor-enhanced NCF (setting is same as above).\nC.3 Detailed Setting of Analysis Experiments\nShapley-Value for Factor Importance Analysis. Given a predictive model $f$ and a set of factors N, the Shapley value of a factor i is computed as follows:\n$\\Phi_i(v) = \\sum_{S \\subseteq N\\{i\\}} \\frac{|S|! (|N|-|S|-1)!}{|N|!} \\cdot [v(S \\cup \\{i\\}) - v(S)],$\nwhere:"}, {"title": "D Ablation Study", "content": "D.1 Ablation on Sparsity Threshold\nTo ascertain whether matrices composed of collaborative performance data can accurately predict the performance of LLMs", "variable": "the matrix sparsity. We assessed the impact of sparsity on prediction accuracy by manipulating the sparsity of the training matrix via masking. This method allowed us to obtain a reliable measure of average accuracy, as illustrated in Figure. 9. It is noteworthy that our method of controlling sparsity only reduces the number of training samples. We ensured fairness in each comparative experiment by maintaining a consistent validation set throughout. During the experiment, we maintained the same settings for the learning rate and number of iterations as in the main experiment. To ensure the robustness of our experimental results, each reported outcome represents the average score after conducting five random splits.\nD.2 Ablation on Predicting Performance on\nComplex Reasoning and CoT Tasks\nFrom the model perspective, it is crucial for validating the feasibility of predictive methodologies to assess the predictive accuracy on special tasks potentially exhibiting \u201cemergent\u201d phenomena (Suzgun et al., 2022; Wei et al., 2022), including complex reasoning and Chain of Thought (CoT) tasks (Wei et al., 2023). \"Emergent' phenomena refers to the challenges associated with predicting performance from smaller models when the scale of a model ex-pands significantly, resulting in discontinuous leaps in model capabilities. The existence of this phenomenon is subject to ongoing debate. Nonetheless, recent efforts (Ganguli et al., 2022b; Hu et al., 2024; Owen, 2024; Ruan et al., 2024; Schaeffer et al., 2023) continue to focus on how scaling laws can be modified to mitigate the \u201cgap\u201d between smaller and larger models. This may involve modifying metrics or incorporating additional data points to linearize the growth curve or alternatively opting for a sigmoidal curve.\nTheoretically, these challenges are not too difficult for our prediction method, as the underlying mechanism of \u201cemergent\" abilities reflects a type of similarity. This commonality manifests when models exceed a certain scale. By analyzing cross-model similarities-how other larger models demonstrate emergent capabilities compared to their smaller counterparts\u2014we can enhance our predictive accuracy for the current model. Overall, these tasks are pivotal for comprehensive validation processes, e.g., GSM8K (Cobbe et al., 2021), BBH (Suzgun et al., 2022), HUMANEVAL (Chen et al., 2021) and MBPP (Austin et al., 2021).\nIn detail, if we want to evaluate the performance of predicting a model on these special tasks, the training data is the performance information from other model families, the smaller model of the same family, and the randomly selected two non-special tasks prior to the performance of this model. In our experiment, we tested the"}]}