{"title": "Collaborative Performance Prediction for Large Language Models", "authors": ["Qiyuan Zhang", "Fuyuan Lyu", "Xue Liu", "Chen Ma"], "abstract": "Comprehensively understanding and accurately predicting the performance of large language models across diverse downstream tasks has emerged as a pivotal challenge in NLP research. The pioneering scaling law on downstream works (Hu et al., 2024; Isik et al., 2024) demonstrated intrinsic similarities within model families and utilized such similarities for performance prediction. However, they tend to overlook the similarities between model families and only consider design factors listed in the original scaling law. To overcome these limitations, we introduce a novel framework, Collaborative Performance Prediction (CPP), which significantly enhances prediction accuracy by leveraging the historical performance of various models on downstream tasks and other design factors for both model and task. We also collect a collaborative data sourced from online platforms containing both historical performance and additional design factors. With the support of the collaborative data, CPP not only surpasses traditional scaling laws in predicting the performance of scaled LLMs but also facilitates a detailed analysis of factor importance, an area previously overlooked.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) (Brown et al., 2020; Ouyang et al., 2022) have emerged as one of the most important AI research powered by large-scale parameters, high computational resources, and massive training data. With the substantial increase in model sizes, the evaluation cost of LLMs' performance becomes even more significant. For example, testing a single LLM on certain benchmarks often requires $10K+ and 4K+ GPU hours (Liang et al., 2023). Therefore, understanding the behaviors and predicting the capabilities of LLMs across scales under various tasks becomes a vital question (Ganguli et al., 2022a; Owen, 2024; Finnveden, 2020; Hu et al., 2024) for both researchers and engineers.\nScaling laws (Kaplan et al., 2020; Hoffmann et al., 2022; Hernandez et al., 2022; Gordon et al., 2021; Bahri et al., 2024; Muennighoff et al., 2023) have been powerful tools for predicting the capabilities of LLMs. It indicates a power-law correlation between the model performance and design factors such as computational measure (FLOPs) utilized during training. Although the scaling law was originally proposed as a strong intuitive guide for designing LLM, researchers (Hu et al., 2024; Ruan et al., 2024; Isik et al., 2024) have extended its utility into predicting model performances on various metrics, such as BLEU in Machine Translation, and different tasks. These works can accurately predict model performances by utilizing the similarity within each model family, e.g., models within each family are usually trained on the same dataset. However, there are several issues rooted in their methods: the performance prediction 1) requires transparent design factors that consume substantial training resources to fit the curve, 2) is only tailored to a certain model family and a specific task metric, and 3) neglects the connections among different models and tasks.\nThe aforementioned limitations motivate us to design more effective methods for predicting the performance of LLMs on downstream tasks. Two observations sparked our attention. Firstly, A strong similarity exists between model families, e.g.LLama-family and GPT family. Models from different families behave similarly in prediction distribution (Shrivastava et al., 2023) and emergent phenomenon (Wei et al., 2022). Secondly, with the emerging LLM models and the increasingly diverse tasks, the cost of enumerating and benchmarking models with tasks increases exponentially. Therefore, we aim to utilize the similarities across model families in order to collaboratively predict the model performance over diverse tasks in an accurate yet efficient way.\nTo incorporate the aforementioned intuitions, we propose a new scheme, Collaborative Performance Prediction (CPP), to efficiently predict the performance of LLMs on evaluation tasks. This scheme learns the latent representations of LLMs and tasks, which captures the intrinsic similarity among different models and tasks. The interaction (e.g., inner product) between the latent representations of LLMs and tasks can be utilized to predict the performance of LLMs on certain tasks. To fulfil the proposed scheme, we collect the LLM performance data from academic papers, technical reports, and open leaderboards covering 72 models and 29 tasks. To summarize, our scheme has several advantages:\n\u2022 Low Training Cost: Compared with methods (Hu et al., 2024) that extend scaling law to various downstream tasks, no pre-training or fine-tuning of LLM is required in our scheme.\n\u2022 Prediction over proprietary model: Unlike previous methods (Ruan et al., 2024), our scheme supports prediction over proprietary models without knowing key design factors, such as computational measures.\n\u2022 Prediction from small to large: By utilizing cross-family information, our scheme can accurately estimate model performance, e.g., emergent ability, of large models on downstream tasks given the information from small models.\n\u2022 Beyond Scaling Laws: Our scheme is more general and can incorporate diverse factors, such as task description factors.\n\u2022 Factor-level Interpretability: Our scheme can provide interpretability by analyzing the factors importance of LLMs.\nUnder our scheme, multiple customized prediction methods (e.g., COLLABORATIVE FILTERING (Koren et al., 2022)) can be incorporated to predict the performance of LLMs, further validating the feasibility and generality. Our method enables more diverse factors as input, ranging from traditional LLM design factors to task design factors, e.g., targeted ability and few-shot setting.\nUpon extensive experimentation within the open-released core leaderboard of HELM (Liang et al., 2023) and our collected historical matrix, our predictive performance demonstrated exceptionally well. Specifically, even without any input of model factors or task factors: in HELM, we use 50% of the scores to predict the other 50%, the predictive ranking (derived from predicted scores) achieves $Accuracy =10%$, and $MAE@2=39%$; in our collected matrix (characterized by a 44% sparsity level) achieves an $Accuracy =45%$, and the $MAE@2 =84%$. Notably, the accuracy of our prediction from small to large LMs significantly exceeded that predicted by scaling laws. Using an analysis method similar to SHAPLEY-VALUES (Lundberg and Lee, 2017; Shapley, 1952), we elucidate the importance of different factors, which surprisingly does not fully align with scaling law (Kaplan et al., 2020). Therefore, our method is undoubtedly more versatile."}, {"title": "Related Work", "content": "Scaling laws (Kaplan et al., 2020; Hoffmann et al., 2022; Hernandez et al., 2022; Bahri et al., 2024; Muennighoff et al., 2023) for LLMs have increasingly become a focal point in understanding and guiding critical design decisions, such as model size and the characteristics and volume of training data. Traditionally, most research in this area has concentrated on how measures like cross-entropy loss or perplexity scale. Subsequent studies have extended these efforts to the scaling behavior on translation (Isik et al., 2024; Ghorbani et al., 2021; Zhuocheng et al., 2023) and other downstream tasks modeling (Caballero et al., 2023; Henighan et al., 2020). The high predictability in LLMs capability has directly spurred extensive research work (see Survey Anwar et al. (2024)) exploring whether LLMs can demonstrate predictability on downstream tasks, which are considered highly unpredictable in traditional ML knowledge (Ganguli et al., 2022a). Particularly, the \"emergence\" phenomenon (Suzgun et al., 2022; Wei et al., 2022) has challenged predictability, where models suddenly exhibit striking capabilities at specific training reources. Recent studies (Schaeffer et al., 2023) have made remarkable achievements in breaking the discontinuities in performance brought about by emergence, and Ganguli et al. (2022a); Owen (2024); Finnveden (2020) demonstrated the predictability on downstream tasks, for instance, Hu et al. (2024) directly fits a curve of training resources and downstream task performance by repeatedly pretraining a specific model. Furthermore, Arora and Goyal (2023) predicted the performance through decomposing the complex capabilities of LMs to some base skills.\nGiven that predictability has now been established, we reassess the underlying premises that enable this predictability: the prevailing similarities across multiple models and various downstream tasks (Liu et al., 2023; Perlitz et al., 2024; Polo et al., 2024; Torregrossa et al., 2020; Ili\u0107, 2023). Based on this, we step beyond the limitations defined by scaling laws and propose a new methodology to predict the performance of LLMs on various downstream tasks."}, {"title": "Collaborative Filtering", "content": "Collaborative filtering (CF) (Koren et al., 2022) is a widely used technique in recommendation systems that predicts users' preferences by collecting the historical preferences of many other users. The underlying assumption of CF is that similar users will share similar preferences on similar items. A seminal method in CF is matrix factorization (Koren et al., 2009) (MF). It reduces the dimensionality of the user-item matrix by learning the latent factors associated with users and items, respectively. This approach helps handle sparse data and improves scalability. The factorization of the user-item matrix R can be represented as\n$R \\approx P^T \\cdot Q$,                                                            (1)\nwhere each column vector in P and Q represents a specific user or item, respectively, with hidden dimension d. The latent representations of users and items capture the user preferences and item properties in the latent space, and the inner product can be utilized to predict the interaction between users and items. To optimize the latent feature vectors, the following loss function is employed:\n$\\min\\limits_{P,Q} \\sum\\limits_{(u,i) \\in \\Omega} (r_{ui} - p_u^Tq_i)^2$,                                        (2)\nwhich measures the squared differences between the observed ratings $r_{ui}$ and the ratings predicted by the model $p_u^Tq_i$ for each user-item pair $(u, i)$ in the set $\\Omega$ of observed interactions.\nHere, Yang et al. (2019) transferred the collaborative filtering for ML model selection by predicting the cross-valided errors, which demonstrates CF's adaptability and efficiency in diverse application areas."}, {"title": "Background and Pilot Demonstration", "content": "For classic scaling laws, researchers propose a hypothesized power-law relationship between a model's computational measures $C_m$ (e.g., training FLOPs) and their performance loss $L_m$ (e.g., perplexity). Specifically, for a model m within a family f (e.g., Llama-2 7B, 13B, and 70B), the relationship is hypothesized as\n$log(L_m) \\approx w_f log(C_m) + b_f$,(3)\nwhere $w_f$ and $b_f$ are scaling coefficients customized for each model family. Researchers fit this formula through repeated scaling experiments, then use it to accurately predict performance when larger-scale ($C' > C$). Some studies (Finnveden, 2020; Owen, 2024) have adapted scaling laws to specific downstream task metrics, proposing that sigmoidal functions are more suitable for predictions, as follows:\n$\\sigma^{-1}(S_m) \\approx w_f log(C_m) + b_f$,(4)\nwhere $S_m$ refers to the normalized downstream scores of models within the range [0,1]. However, applying scaling laws across different model families on various specific tasks presents a trade-off: fitting unique coefficients for each evaluation scenario (e.g., Llama 2 on MMLU) is a resource-intensive endeavor (Hu et al., 2024); alternatively, estimating these coefficients using a limited number (3-5) of models within the same family may compromise the accuracy of the predictions. Moreover, the recent work (Ruan et al., 2024) extends scaling law by incorporating latent variables to capture the patterns across model families and tasks."}, {"title": "Pilot Demonstration on HELM", "content": "Scaling laws reveal that models from any family exhibit a similar performance trend as computational measures increase. This insight suggests there are commonalities and connections between different models. These motivate us to employ the MF method to explore more similarities beyond computational measures, e.g., the relationship among the different model families and tasks.\nWe perform the aforementioned MF on the benchmark matrix to observe the error gap between predicted and truth (normalized) scores. Specifically, we select the core leaderboard provided by HELM for our exploratory experiments with only model name, task name, and performance scores. This leaderboard, 68 models and 16 tasks, presented in a score matrix with a density of 82.5%, which includes both open-source and proprietary models, e.g., GPT-4 and Jurassic-2. Our method treats all models and tasks as independent entities without introducing any prior similarity factors. We hope to observe whether MF can predict the remaining scores, giving a small part of the matrix, where we evaluate two training/validation sets split strategies: 10%/90%, 50%/50%. As illustrated in Figure 2, MF can accurately predict most of the missing scores within a low error range, which proves that it can encode the similarity across the model and the task without regression depending on explicit computational measures variable."}, {"title": "Collaborative Performance Prediction", "content": "Motivated by pilot experiments, we introduce the concept of \"Collaborative Performance Prediction\" (CPP) to facilitate the performance prediction of LLMs.\nDefinition 1. Let $M = \\{M_1, M_2,..., M_n\\}$ be a set of n LLMs, and $T = \\{T_1, T_2, ..., T_m\\}$ be a suite of m tasks. Define the Score Matrix $S$, which is an $n \\times m$ matrix where each element $s_{ij}$ represents the performance score of model $M_i$ on task $T_j$. $s_{ij}$ is defined as\n$s_{ij} = \\begin{cases}\\text{score} & \\text{if tested,} \\\\\\text{unknown} & \\text{otherwise.}\\end{cases}$\nFunction: Employ an prediction method F to estimate the unknown elements of S, denoted by $S_{ij}$, based on the known values.\nExtention: Accommodate model design factors $V_m = \\{V_m^1, V_m^2,..., V_m^{M'}\\}$, such as common computational meatures, and task design factors $V_t = \\{V_t^1, V_t^2,...,V_t^{T'}\\}$, such as targeted capabilities and few-shot settings.\nBased on this definition, our framework consists of two components: 1) collaborative performance data, 2) collaborative prediction methods. We anticipate that an accurate score can be predicted based on the historical performance of various models on downstream tasks and other design factors for both model and task. Moreover, we can incorporate or solely rely on the factors describing the LLM and the associated downstream tasks."}, {"title": "Collaborative Data", "content": "Unlike the scaling law approach, which requires training resource factors to obtain the correlation between metric scores and factors at a high training cost, our proposed method makes use of evaluation results and other design factors reported from existing studies, referred to as collaborative data. Open-source leaderboards, such as Open-LLM\u00b2, HELM, and OpenCompass\u00b3, have made tremendous efforts on this issue in fairly evaluating different LLMs. Our efforts extend beyond merely (Ruan et al., 2024) utilizing data from open-source leaderboards with matrix sparsity of 0%. We also extract test results from different models' papers, technical reports, and model cards. Ultimately, we have collected a score matrix of n = 72, m = 29 with a density of only 56%. Furthermore, we collected 12 and 4 detailed design factors for models and tasks. These details are listed in Appendix B.1. Our data analysis is shown in Figure 3 and Figure 8.\nData Analysis. Based on the collective data, we can make the following observations: a) Uneven distribution of testing resources. We observe significant variability in the deployment of testing efforts, as shown in Figure 3. For instance, models from the LLAMA series have undergone extensive testing across various tasks, in contrast to models like GOPHER, where testing has largely stagnated. A similar disparity is also evident among tasks, with MMLU and HELLASWAG receiving considerable evaluation, whereas RACE has been relatively underexplored. This trend suggests that as LLMs proliferate and tasks evolve, scores across the matrix will increasingly skew. This leads to a pronounced long-tail effect in testing coverage for many tasks, barring a few that consistently receive comprehensive evaluations. b) Widespread variations in the scores. It is noteworthy that identical models yield varying scores on the same tasks across different studies (Shrivastava et al., 2023; AI@Meta, 2024), a variation often attributed to differences in prompt settings, model versions, and the volume of test samples employed. Typically, these score variations range within 0.1, with scores normalized between [0, 1]. This phenomenon underscores the importance of public leaderboards and highlights researchers' need to articulate their testing frameworks when performing customized evaluations. When conflicted, we prefer the results from the Open-LLM leaderboard in the collective data. c) Missing description/model card. We advocate for consistently providing complete model cards for open-source and proprietary models. Such a phenomenon is shown in Figure 8 and, unsurprisingly, a long-tail distribution is witnessed. While it is understandable that proprietary models might withhold specific details about parameters, they can still divulge information about parameter scale and the extent of pre-training. Furthermore, we recommend a more thorough description of testing tasks, including suggested few-shot settings and detailed descriptions of targeted capabilities."}, {"title": "Prediction Methods", "content": "In Section 2.2, classical collaborative filtering methods are inspired to conduct the performance prediction. In principle, most collaborative filtering methods can be applied. Here, in addition to the abovementioned MF, we also leverage neural collaborative filtering (He et al., 2017) (NCF) methods, which uses a multi-layer perceptron to learn the model-task interaction function to predict the score $S_{ij}$ for a model i on a task j, providing a way to learn non-linearities in the data:\n$\\hat{s}_{ij} = f(i, j|M, T, [V_i, V_j], \\theta)$                                                (5)\n$= MLP(p_i, q_j, [e_{v_i}, e_{v_j}]),$ \nwhere M and T denote the sets of collaborative models and tasks, and their descriptive factors $V_i$, $V_j$ optionally enrich the input data. Here, $p_i$ and $q_j$ are the latent vectors for model i and task j that capture the intrinsic properties of models and tasks, as well as embeddings $[e_{v_i}, e_{v_j}]$ derived from their descriptive factors, and $\\theta$ represents the parameters of NCF.\nMoreover, we further simplify the model to verify whether it is feasible to predict a score when only inputting the descriptive factors $V_i$, $V_j$ into the prediction model:\n$\\hat{s}_{ij} = f(i, j |V_i, V_j, \\theta)$                                                     (6)\n$= MLP(e_{v_i}, e_{v_j}),$\nFor both settings, where the goal is to predict the scores accurately, the loss function can be defined as follows:\n$L(\\theta) = \\frac{1}{N} \\sum\\limits_{(i,j)\\in D} (s_{ij} - \\hat{s}_{ij})^2,$(7)\nwhere N is the total number of scores set D for training, and $s_{ij}$ is the true score for model i and task j."}, {"title": "Experiments", "content": "In this section, we evaluate the feasibility of CPP from an overall benchmark perspective and a model perspective in Section 5.1 and 5.2, respectively; we then analyze the importance of factors for both models and tasks in Section 5.3. Additionally, a substantial amount of ablation and analysis is placed in the appendix D, such as sparsity, the correlations in tasks and models, and which models and tasks are more critical for prediction.\nOur validation framework utilizes the aforementioned collaborative dataset as the score matrix S. We partition scores {sij} into train and validation set, detailed in Appendix C.2."}, {"title": "Evaluation from Benchmark Perspective", "content": "In this study, we select the abovementioned methods, MF and NCF, to verify whether $s_{ij}$ can be accurately predicted based on the input of model i and task j. To examine whether enhancements are helpful, we modify NCF to support the input of design factors, detailed in Appendix C.2. Based on Figure 4 and Table 1, we can make the following observations:\nFirst, all methods accurately predicted model performance, demonstrating that collaborative filtering mechanisms can predict model outcomes based on collaborative data across different models and tasks. This prediction is achieved without explicit scaling factors or fitting a log-power curve. Second, from MF to NCF, the transformation in interaction mechanisms further enhances accuracy, suggesting that model improvements can further augment the efficacy of our methodology. Additionally, we further increased accuracy by incorporating factors, such as model scaling variables and task descriptions, into the NCF framework alongside ID information. This confirms that incorporating explicit factors can enhance model and task similarities. Finally, among all metrics, we particularly noted that the accuracy of the predictive ranking was acceptable. In other words, researchers can use our method to accurately predict the ranking range of their developed models on test tasks, thereby enhancing model performance on specific tasks."}, {"title": "Predictability with Only Description Factors", "content": "We validate whether high predictive accuracy can still be achieved by only inputting the models' and tasks' design factors. As demonstrated in Table 1, the accuracy of predicted rankings (derived from predicted scores) remains high, affirming that our method supports predictions based solely on factors. However, the accuracy is lower than other models, suggesting that finer-grained latent similarities remain encoded as potential factors within the identity information across different models and tasks."}, {"title": "Evaluation from Model Perspective", "content": "To mimic the utilization of CPP in the real world, this section takes a model perspective to investigate the predictive accuracy of CPP upon each model. Specifically, we propose two scenarios: (i) prediction with no prior testing information and (ii) prediction with prior testing information on 2 tasks. These two scenarios correspond to real-world cases when the model has not been developed or is tested on a few tasks and expects an accurate prediction of its ability on other tasks. In both scenarios, we focus on larger LLMs, e.g., LLama2-70b, as they are more computationally expensive to develop and test, requiring an accurate LLM prediction.\nWe report the results of CPP and SL on both scenarios in Figure 5 and can draw the following conclusions. Under the CPP-0 scenario, CPP demonstrated greater adaptability across different tasks compared to SL, with points closely aligned along the y = x line (\"perfect prediction\") in Figure 5 (a). This suggests that CPP has effectively captured task-specific characteristics, such as value ranges, whereas SL, despite achieving a lower MSE-LOSS, tends to concentrate its predictions around 0.5. Under the CPP-2 scenario, the distribution of points of CPP is noticeably closer to y = x, as shown in Figure 5 (b), and its MSE-LOSS is also lower than that of SL. This indicates that leveraging performance data from other tasks considerably enhances the model's cross-task prediction capabilities, underscoring a degree of consistency across tasks for the same model. This approach demonstrates that predictions for scaling LLMs on downstream tasks can be dynamically improved by evaluating performance on less computationally intensive tasks and using those outcomes to predict scores on subsequent tasks more accurately."}, {"title": "Factor Importance Analysis via SHAPLEY-VALUE", "content": "In this section, we aim to analyze each design factor's importance over CPP. The Shapley value, a concept derived from cooperative game theory (Shapley, 1952), offers a systematic approach to measuring individual factors' contribution in predictive models (Lundberg and Lee, 2017; Covert et al., 2021). Appendix C.3 shows a detailed formulation of the Shapley value. Visualization for Shapley values of each design factor is shown in Figure 6.\nBased on Figure 6 (a), we can make the following observations regarding model factors. First, we have discovered that in addition to traditionally important factors such as training data size and parameter size mentioned in scaling law (Kaplan et al., 2020), other design factors significantly influence predictive outcomes. These include the model family, context window size, and batch size. Second, the importance of the model family cannot be overlooked, as it may relate to differences in data quality across models, including proprietary data or specific architectural details. For instance, using a particular model family might mean adopting architectures or optimization techniques better suited to specific tasks. Moreover, the size of the context window also significantly affects model performance. A larger context window allows the model to better understand the context in long texts, which is particularly crucial for long-context LLMs (Xiong et al., 2023). Experience (Google, 2024) has shown that such models perform better across various tasks. Batch size, as another crucial factor, affects the stability and speed of model training. An appropriate batch size ensures a balance between the accuracy of gradient estimation and computational efficiency during training.\nAs for the importance of task factors, results in Figure 6 (b) show that the target ability among all factors is more important. This also implies that similarities between the domains of different tasks can help predict outcomes. This conclusion is consistent with previous observations (Ruan et al., 2024; Perlitz et al., 2024; Polo et al., 2024)\nIn summary, these findings indicate that LLMs performance prediction should not rely solely on traditional design factors limited by scaling law but also on other key factors that might impact overall model performance."}, {"title": "Conclusion and Discussion", "content": "Advancing beyond traditional scaling laws on downstream tasks, we propose a collaborative performance prediction framework for large language models. It offers significant advantages, including easy deployment, low training costs, and superior predictive accuracy. Uniquely, it enables incorporating additional design factors and supports an in-depth analysis of their impact, including factor importance and correlations in models and tasks. For prediction, we collect collaborative data containing many historical performances and factors.\nOur method's predictive accuracy is expected to improve as it benefits from an expanding pool of collaborative data. Moreover, this approach highlights the potential to identify neglected but vital factors beyond traditional scaling laws, such as task design factors, thereby enriching our comprehension of LLM performance predictability on downstream tasks."}, {"title": "Limitations", "content": "\u201cSingle-source-of-truth\u201d. When collecting the collaborative data, we hypothesize that each model's performance on each task is identical. However, in the real world, the detailed testing setting, for instance, the testing prompt writing, can influence LLM's performance variance. Although we observed this, we only saved one score from different sources. How to incorporate the setting of testing as an additional dimension remains to be solved in future works.\nSusceptibility to data quality. The prediction accuracy of CPP highly depends on the quality of collaborative data. The current version passively collects collaborative data from online resources. Should information from either of these data sources be incorrect, the prediction capability of CPP would decrease correspondingly. To overcome such a limitation, jointly considering passive information collected from data sources and active information, such as performances of models tested on some tasks by the user, might be a solution. Utilizing techniques such as efficient benchmarking (Perlitz et al., 2024; Polo et al., 2024) could alleviate the cost of obtaining active information."}, {"title": "Ethics Statement", "content": "The data we use are collected from public papers, technical reports, open leaderboards, and model cards on GitHub."}, {"title": "Pilot Demonstrations using Neural Collaborative Filtering", "content": "In this section, we supplemented the error distribution in Figure 7, which is generated using neural collaborative filtering on the HELM lite leaderboard. Compared to Figure 2, it is evident that neural collaborative filtering consistently outperforms MF across each setting."}, {"title": "Collaborative Data", "content": ""}, {"title": "Data Description", "content": ""}, {"title": "Experimental Setup", "content": ""}, {"title": "Evaluation Metrics", "content": "Apart from visualization, we also evaluate the method based on two types of metrics: 1) SCORE-Loss Metric: we calculate MSE Loss and L1 Loss between predicted scores and true scores (normalized) on downstream tasks; 2) RANK-ACCURACY Metric: researchers are sometimes not concerned with detailed scores but rather the rankings the model is in, so we calculate the accuracy of rank derived from the predicted scores, ACCURACY and MAE@2. ACCURACY refers to the percentage of instances where the predicted rank equals the true rank, and MAE@2 refers to the percentage of instances where the absolute difference between the predicted rank and the true rank is in 2, the formulation as below:"}, {"title": "Detailed Setting of Validation Prediction Accuracy Experiments", "content": "In this section, we detail the setup of each experiment in 5.\nDue to the 44% sparsity of the collected collaboration matrix, we used 5% of the known data as the validation set, with the remaining data serving as the observed training set. We trained each model five times through random splitting, deriving an average performance and variance. We configured our models with latent factors = 10, learning rate = 0.01, and iteration = 250,000. The Figure 4 is the results when random_seed = 1."}, {"title": "Detailed Setting of Analysis Experiments", "content": "the Shapley value of a factor i is computed as follows:\n$\\Phi\\epsilon(v) = \\sum\\limits_{S \\subseteq N\\{i\\}} \\frac{\\vert S\\vert! \\vert N\\vert - \\vert S\\vert - 1)!}{\\vert N\\vert!} \\cdot [v(S\\cup \\{i\\}) - v(S)],$(10)\nwhere:"}, {"title": "Ablation Study", "content": ""}, {"title": "Ablation on Sparsity Threshold", "content": "To ascertain whether matrices composed of collaborative performance data can accurately predict the performance of LLMs, it is essential to consider the critical variable: the matrix sparsity. We assessed the impact of sparsity on prediction accuracy by manipulating the sparsity of the training matrix via masking. This method allowed us to obtain a reliable measure of average accuracy, as illustrated in Figure. 9. It is noteworthy that our method of controlling sparsity only reduces the number of training samples. We ensured fairness in each comparative experiment by maintaining a consistent validation set throughout. During the experiment, we maintained the same settings for the learning rate and number of iterations as in the main experiment. To ensure the robustness of our experimental results, each reported outcome represents the average score after conducting five random splits."}, {"title": "Ablation on Predicting Performance on Complex Reasoning and CoT Tasks", "content": "From the model perspective, it is crucial for validating the feasibility of predictive methodologies to assess the predictive accuracy on special tasks potentially exhibiting \u201cemergent\u201d phenomena (Suzgun et al., 2022; Wei et al., 2022), including complex reasoning and Chain of Thought (CoT) tasks (Wei et al., 2023). \"Emergent' phenomena refers to the challenges associated with predicting performance from smaller models when the scale of a model expands significantly, resulting in discontinuous leaps in model capabilities. The existence of this phenomenon is subject to ongoing debate. Nonetheless, recent efforts (Ganguli et al., 2022b; Hu et al., 2024; Owen, 2024; Ruan et al., 2024; Schaeffer et al., 2023) continue to focus on how scaling laws can be modified to mitigate the \"gap\u201d between smaller and larger models. This may involve modifying metrics or incorporating additional data points to linearize the growth curve or alternatively opting for a sigmoidal curve.\nTheoretically, these challenges are not too difficult for our prediction method, as the underlying mechanism of \u201cemergent\" abilities reflects a type of similarity. This commonality manifests when models exceed a certain scale. By analyzing cross-model similarities\u2014how other larger models demonstrate emergent capabilities compared to their smaller counterparts\u2014we can enhance our predictive accuracy for the current model. Overall, these tasks are pivotal for comprehensive validation processes, e.g., GSM8K (Cobbe et al., 2021), BBH (Suzgun et al., 2022), HUMANEVAL (Chen et al., 2021) and MBPP (Austin et al., 2021).\nIn detail, if we want to evaluate the performance of predicting a model on these special tasks, the training data is the performance information from other model families, the smaller model of the same family, and the randomly selected two non-special tasks prior to the performance of this model. In our experiment, we tested the 4 models on these tasks, and then we plotted the test results on Figure 10. As illustrated in Figure 10, our predictive scores are more adaptive to each task, where the points are close along the \"perfect prediction\u201d line, which means our prediction method captures the similarity in the specific task across models. Our proposed method's MSE Loss is comparable to the scaling law, which shows the feasibility of CPP (in CPP-2)."}, {"title": "Correlation between Models", "content": "Experiment. We conducted a \"leave-one-out\" experiment to test the impact of Model A on the predictive performance of Model B. This involved masking Model A and using the performance of other models to train predictive methods, which were then validated on Model B to observe changes in loss. This approach generated a matrix with the masked model names on the X-axis and the validation model names on the Y-axis, with the values representing the change in loss.\nThe \u201cLeave-one-out\u201d experiment is a robust method commonly used in statistical analysis. To assess the impact of different models on the predictive performance of a specific model, we implemented a strategy where we systematically masked each selected model in the training set. The procedure involved masking each model individually and then training and testing the loss on a validation model. This process was repeated across all models, culminating in creating a matrix where axis=0 represents the masked model ID, and axis=1 represents the validation model ID. The values in the matrix correspond to the loss observed. This experiment was conducted under three different random seeds to ensure the stability and reliability of the results.\nSubsequently, each model was used as a validation set, with the remaining data serving as the training set to calculate the loss for each model. This also resulted in a matrix where axis=1 indicates the validation model ID, and the columns[:, valid model id] represent the corresponding loss for that validation model. We derived a delta loss matrix by calculating the difference between these two matrices.\nGiven that each validation model has its own range of loss variations, we normalized the delta loss matrix. We then performed a row-based correlation analysis on this normalized matrix to assess each model's impact on predictive performance."}, {"title": "Analysis", "content": "Based on this correlation matrix, we further conducted a hierarchical clustering analysis (Nielsen, 2016). The results indicate that a set of models exists that are similar in their impact on the predictive performance of the model. Other models are far away from them. (Details in Table 5)\nThis analysis not only helps us understand each model's specific contributions to predictive performance but also reveals the similarities and differences in functionality among the models, providing a crucial basis for model optimization and selection.\nWe performed a row-wise correlation analysis 13 on this matrix and discovered that models from the same family tend to have similar impacts on predictions, as do models of the same size. After conducting a hierarchical distance analysis, we concluded that a group of models exists that, when more performance data is available, can significantly enhance the accuracy of the predictive models. There are also what might be termed \u201cnoise model performances\u201d in our analysis D.3."}, {"title": "Correlation between Tasks", "content": "We also conducted \u201cleave-one-out\" experiments on these tasks and created a heatmap figure. 14 of the correlations. Tasks with similar targeted ability testing capabilities demonstrated similar influences, such as GSM8K, MATH (Hendrycks et al., 2021), ARC (Chollet, 2019), and HUMANEVAL, which all require complex reasoning abilities."}, {"title": "Others", "content": ""}, {"title": "Visualization", "content": "The figure 15 is the visualization for the prediction performance of scaled language models on downstream tasks."}]}