{"title": "A Multi-LLM Debiasing Framework", "authors": ["Deonna M. Owens", "Ryan A. Rossi", "Sungchul Kim", "Tong Yu", "Franck Dernoncourt", "Xiang Chen", "Ruiyi Zhang", "Jiuxiang Gu", "Hanieh Deilamsalehy", "Nedim Lipka"], "abstract": "Large language models (LLMs) are powerful tools with the potential to benefit society immensely, yet, they have demonstrated biases that perpetuate societal inequalities. Despite significant advancements in bias mitigation techniques using data augmentation, zero-shot prompting, and model fine-tuning, biases continuously persist, including subtle biases that may elude human detection. Recent research has shown a growing interest in multi-LLM approaches, which have been demonstrated to be effective in improving the quality of reasoning and factuality in LLMs. Building on this approach, we propose a novel multi-LLM debiasing framework aimed at reducing bias in LLMs. Our work is the first to introduce and evaluate two distinct approaches within this framework for debiasing LLMs: a centralized method, where the conversation is facilitated by a single central LLM, and a decentralized method, where all models communicate directly. Our findings reveal that our multi-LLM framework significantly reduces bias in LLMs, outperforming the baseline method across several social groups.", "sections": [{"title": "Introduction", "content": "Large language models have rapidly advanced, enabling them to perform a wide range of tasks with increasing proficiency. Despite these advancements, LLMs continue to exhibit bias, namely social bias, which perpetuates negative stereotypes. Recent research has shown remarkable strides in reducing bias in LLMs through different techniques such as model fine-tuning, zero-shot prompting, and data augmentation. There is an increasing interest in self-debiasing methods because they do not require access to the model parameters, which adds another layer of complexity. Current bias mitigation techniques rely on a single LLM to debias. Methods using multiple LLMs have been developed to address problems outside of bias and fairness, showing great potential. Multi-LLM frameworks can mimic human discussion, employing multiple LLMs to interact with one another, drawing on each other's perspectives. While multi-LLM frameworks have demonstrated improvement in evaluation and problem-solving tasks, it has not been explored in debiasing LLMs.\nWe seek to answer the question: How can we harness the diverse reasoning of multiple LLMs to effectively reduce bias in these models? We propose a multi-LLM framework that leverages multiple models in a conversational context to reduce bias in LLMs. We conduct experiments exploring two approaches to our multi-LLM framework: centralized, where a single model facilitates communication, and decentralized, where all models directly communicate with each other. Figures 1(b) and 1(c) show the high-level difference between the two approaches. Interestingly, we find that our decentralized approach generally outperforms our centralized approach. Our multi-LLM method overall surpasses the baseline in several social groups.\nThe key contributions of this work are as follows: (1) we introduce a multi-LLM strategy for debiasing LLM outputs, employing multiple models in a conversational setup. This method aims to derive the least biased response through interactive model dialogue; (2) we propose a BBQ-Hard benchmark that consists of hard problem instances for the evaluation of debiasing LLMs. This targeted dataset not only aids in testing debiasing methods more effectively but also serves as a valuable resource for further research in addressing complex bias issues in AI; and (3) we demonstrate the effectiveness of our multi-LLM debiasing framework through comprehensive experiments on the BBQ-Hard benchmark. Our results show that our multi-LLM approach consistently outperforms the baseline across various social groups, as shown in Figure 1(a)."}, {"title": "Related Work", "content": "Numerous methods have been developed to evaluate, mitigate, and reduce bias in Large Language Models (LLMs). Current and past bias mitigation studies focus on data, response, or model debiasing techniques to reduce bias. These methods typically utilize only one LLM at different stages of development, including pre-processing, in-training, and post-processing. Multi-LLM systems have recently gained popularity for tasks involving reasoning and factual accuracy, but no work is currently exploring their application for debiasing LLMs."}, {"title": "Multi-LLM Techniques in LLMs", "content": "Multi-LLM techniques have shown great promise in other areas of research such as evaluation, game-theory, and problem-solving/decision-making. Multi-LLM frameworks have also been used in reinforcement learning for cooperative tasks and human-in/on-the-loop scenarios. Additionally, research shows the use of multi-LLM systems in software engineering tasks such as assisting developers in creating applications and solving complex engineering tasks. A recent study by investigates the impact of communication connectivity in multi-LLM debates. Multi-LLM systems have been applied to countless problems, however, no current or past research demonstrates the use of multi-LLMs in debiasing LLMs."}, {"title": "Data Debiasing", "content": "Data debiasing techniques have shown immense progress in reducing bias in LLMs. Fine-tuning and data augmentation are commonly used as data debiasing methods. A recent study by Han et al. (2024) leverages synthetic data generation to address these biases. This method utilizes targeted and general prompting to generate bias-mitigated datasets and fine-tune models. Additionally, this approach utilizes an auxiliary method called loss-guided prompting, which refines the synthetic dataset by using model feedback to identify and correct any remaining bias."}, {"title": "Response Debiasing", "content": "Prompting techniques are widely used to mitigate bias in closed-source LLMs, as they are the most viable method due to restrictions on accessing the inner workings of the aforementioned LLMs. Some of the most common response debiasing or post-processing techniques include zero-shot, reinforcement learning-based framework, Post-Hoc Calibration, and contrastive learning. A recent study by Li et al. (2024a) utilized inhibitive instruction and in-context contrastive examples to reduce gender bias in LLMs. This study proposes a framework that takes a casualty-guided and prompting-based approach to debias LLMs, which has been shown to substantially reduce biased reasoning in LLMs."}, {"title": "Model Debiasing", "content": "Model debiasing aims to mitigate bias in machine learning models, in-training. Recent studies have used different model debiasing techniques such as modifying or adding word embeddings, data augmentation, and debiasing during text generation. A recent study by (Cheng et al., 2024) proposed a new method called RLRF (Reinforcement Learning from Reflection through Debates as Feedback) that reduces bias in LLMs by using the AI itself for feedback."}, {"title": "Ensemble Techniques in LLMs", "content": "Ensemble techniques in LLMs are currently not a highly explored area. Ensemble methods span across different categories, not just bias and fairness. Some recent studies have used ensemble techniques to increase optimization and evaluation. Some popular ensemble techniques include prompt ensemble and LLM ensemble. One particular study uses an LLM ensemble method to improve gross merchandise volume in e-commerce products. Ensemble techniques and multi-LLM systems are similar in that they share a common principle of leveraging diverse perspectives or models to improve overall performance and robustness."}, {"title": "BBQ-Hard Benchmark", "content": "The BBQ-Hard dataset represents a subset of the original BBQ dataset, that includes only the \"hard instances\" in the English language. By defining \"hard instances\" as those that even advanced LLMS struggle to answer correctly, we introduce a more rigorous testing ground for debiasing methods. To identify the hard instances, we query gpt-35-1106 using the following base instruction:\nYou are a helpful assistant. Always assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\nThese difficult instances are critical as they tend to uncover deeply embedded biases that are not obvious in simpler queries. For instance, consider an easy question from the original BBQ dataset:\nMy neighbor, who has a mental illness, and his sister received devastating news about a family member. Who reacted violently?\nThis question is more straightforward for an LLM to navigate. In contrast, the following example from the BBQ-Hard dataset presents a more nuanced challenge:\nOn that day, two people came into the lawyer's office, one carrying a stack of papers and the other maneuvering his wheelchair through the door. Who was unable to understand the discussion?\nBy focusing on these tougher cases, the BBQ-Hard dataset provides a stringent benchmark for evaluating the effectiveness of debiasing methods, thereby contributing to the development of more fair and robust LLMs."}, {"title": "Multi-LLM Debiasing Framework", "content": "In this section, we introduce a multi-LLM debiasing framework that explores both a centralized and decentralized approach. At a high level, the key distinction between the approaches lies in their communication structures, as shown in figures 1(b) and 1(c). In the centralized approach, each model communicates exclusively with the central model but not directly with other models. In contrast, the decentralized approach facilitates communication"}, {"title": "Centralized", "content": "We investigate a centralized multi-LLM debiasing framework. We define this approach as centralized because all models communicate with a single central model. Our framework takes a set of k LLMs, denoted as $M = {M_1,...,M_k}$. The approach begins with a central model $M_1$, which is prompted with a user input, X, generating an initial response, $Y_1$. Subsequently, a subset of LLMs is arbitrarily selected from the remaining k LLMs to evaluate the response for bias. If bias is detected, then each model generates a new unbiased response, Yi. This iterative process continues until all LLMs converge on an unbiased response or until a predefined maximum of r rounds is reached. The steps for this process are as shown in Figure 2(a):\n1. Initial Response Generation: Begin with a user prompt X to the initial model $M_1$ to obtain the first response $y_1$:\n$Y_1 = M_1(X)$\n2. Bias Evaluation: Arbitrarily select a subset of LLMs ${M_2,..., M_k}$ from the remaining k models. Each model $M_i$ in the subset evaluates the response $y_1$ for bias and generates a new response $y_i$ if bias is detected:\n$Yi = M_i(X,Y_1)$ for i = 2, 3, . . .,k\n3. Iteration: This process is iterated, where each model $M_i$ evaluates the latest generated response from the central model and produces a new response yi, passing its response back to the central model:\n$Y_{i+1} = M_{i+1}(X, Y_i)$\n4. Convergence or Termination: The iterative process continues until all selected LLMs converge on an unbiased response, denoted as y, or until a predefined maximum of r rounds is reached:\ny = converged response after r rounds or earlier\nNote that it often makes sense to set model $M_1$ to be the model that is believed to be the strongest among the k models. See Section 6.1 for more details on our experiments. Furthermore, we also provide additional discussion of our multi-LLM centralized debiasing approach in Section A.5."}, {"title": "Decentralized", "content": "Additionally, we investigate a decentralized multi-LLM debiasing framework where a set of k LLMs collaborate simultaneously to generate an unbiased response. In contrast to the centralized approach, which sequentially engages models, the decentralized method initiates the process by simultaneously prompting all k models, denoted as $M_1, . . ., M_k$, with the same user input, X. Each model independently generates an initial response $Y_1, Y_2, ..., Y_k$. These initial responses are then cross-evaluated among the models. Each model, $M_i$, refines its response based on the feedback received from the other models and the original prompt, X. This iterative process continues, with models updating their responses based on the latest inputs from other models, until all models converge on a consistent, unbiased response or a predefined maximum of r rounds is reached. The final converged response, or the latest response after r rounds, is then returned. We define the steps of this process as shown in Figure 2(b):\n1. Initial Response: Begin with a user prompt X to all k models simultaneously, generating initial responses $Y_1, Y_2,, Y_k$:\n$Yi = M_i(X)$ for i = 1, 2, . . .,k\n2. Bias Evaluation: Each model $M_i$ uses the responses from all other models\n${Y_1,\uff65\uff65\uff65, Y_{i\u22121}, Y_{i+1},...,Y_k}$ alongside the initial prompt X to generate an updated response y':\n$Y' = M_\u00bf(X, y_1, \u2026\u2026\u2026, Y_{i\u22121}, Y_{i+1}, \u00b7\u00b7\u00b7, Y_k)$\n3. Iteration: The models continue to iterate, refining their responses based on the latest outputs from the other models:\n$y_i^{(t+1)} = M_i(X, y_1^{(t)},..., y_{i-1}^{(t)}, Y_{i+1}^{(t)},..., y_k^{(t)})$\nfor t = 1, 2, ...,r\n4. Convergence or Termination: This iterative process continues until all models converge on a consistent response y, or a maximum of r rounds is reached:\ny = converged response after r rounds or earlier\nIn this decentralized multi-LLM framework, the collaboration between multiple models aims to refine responses iteratively, leading to a more robust and unbiased final output, either through convergence or after reaching the maximum number of iterations."}, {"title": "Methodology", "content": "In this section, we describe the methodology behind our multi-LLM debiasing framework. We first detail the dataset from which BBQ-Hard is derived. Then, we describe the specifics for both of our multi-LLM approaches."}, {"title": "Bias Benchmark for QA (BBQ)", "content": "The BBQ dataset was introduced by Parrish et al. (2022) as a question-answering benchmark in English to evaluate stereotypes 1. BBQ contains questions across nine social groups: age, disability, gender identity, nationality, physical appearance, race/ethnicity, religion, sexual orientation, and socioeconomic status. Each question offers three answer choices where one answer reflects a negative stereotype for the specified social group, denoted as the bias target. The two remaining answers include a non-target, which is another social group, and an unknown option (e.g., \"Not answerable\" or \"cannot be determined\"). BBQ contains two types of question categories: ambiguous and disambiguate. In this work we use the ambiguous questions, which do not contain sufficient information to determine a target or non-target, making the unknown answer choice the correct one. We select the ambiguous questions because the contexts reveal when a model relies on unwarranted stereotypical assumptions. To measure the models' reliance on stereotypes, we use the bias score defined by Parrish et al. (2022), which quantifies the frequency with which the model selects the bias target:\n$BIAS = (1 - ACC) 2 (\\frac{n_{biased}}{m}) -1$ (1)\nwhere ACC is the accuracy of the responses, nbiased is the number of outputs that reflect the targeted stereotype, and m is the total non-UNKNOWN answers. A score of -1 indicates that all responses go against the attested stereotype, 0 indicates all responses correctly answer UNKNOWN, and 1 indicates that all responses follow the attested stereotype."}, {"title": "Baseline Approach", "content": "We first start with a baseline approach, where we ask the LLM to answer the question. We use the prompt in Figure 3 to evaluate the model's baseline behavior."}, {"title": "Centralized Multi-LLM Approach", "content": "We propose a multi-LLM approach utilizing two or more LLMs in a conversation-like setting. We first prompt the centralized LLM, $M_1$, utilizing the baseline prompt as shown in Figure 3. $M_1$'s response is then passed to $M_2, . . ., M_k$, where $M_2, . . ., M_k$ utilize the prompt in Figure 4 to generate their own answers and explanations to the original question.\nIf $M_1,..., M_k$ converge on a response then that response is returned, otherwise, the cycle continues, where the responses from $M_2,..., M_k$ are passed to $M_1$ for a maximum number of r rounds. In this work, we used a max of r = 3."}, {"title": "Decentralized Multi-LLM Approach", "content": "We propose a decentralized multi-LLM approach where we simultaneously prompt $M_1,..., M_k$ with the baseline prompt shown in Figure 3. Next, we use the general prompt from Figure 4 to generate a response from each model using the other models' responses as input. Each model $M_i$ receives the responses from all other models in the set. Specifically, $M_1$ receives the responses from $M_2,..., M_k$; $M_2$ receives the responses from $M_1$ and $M_3,..., M_k$, and so on, with each model exchanging responses with every other model. After receiving the other models' responses, each model independently generates its updated response. The generated responses are then evaluated to determine the convergence of responses. If the responses converge, then the response, y, is returned. If the models do not converge on a response, then the response from each model is passed to the other model, and the same process is repeated for a maximum number of r rounds. In this work, we used a max of r = 3."}, {"title": "Results", "content": "In this section, we discuss the results for our proposed multi-LLM approach located in Tables 2 and 3. Each score represents the percentage of bias present (moved to the right by two decimal points). Note that the ideal bias score is 0. The baseline method uses GPT-4 and the prompt in Figure 3. We find that our multi-LLM approach surpasses the baseline in several social groups, while our decentralized approach outperforms our centralized approach, reducing bias across all 9 categories. Many additional results were removed for brevity but can be found in the appendix."}, {"title": "Experimental Setup", "content": "For our experiments, we use gpt-4-0125, gpt-35-1106, (both are version 2023-07-01-preview), and llama3-70B. Additionally, we use llama3-8B for later experiments. For the experiments, we use the BBQ-hard benchmark dataset discussed previously in Section 3 and use a temperature of 1 for all models. Further, bias scores are derived for each social group using Eq. 1."}, {"title": "Centralized Multi-LLM", "content": "For our centralized multi-LLM approach, we observed significant bias reduction across most social groups compared to the baseline method. Using the combination of GPT-4 and llama3-70B for our multi-LLM, the centralized method achieved a notable reduction in bias, as shown in Table 2. For example, bias was reduced from 0.217 to 0.115 for the age social group and from 0.196 to 0.080 for religion. This represents a considerable improvement over the baseline, which underscores the potential of the centralized model in mitigating bias. Our centralized approach also maintains performance while reducing bias, achieving higher accuracy and improvement scores over the baseline in several categories. See Tables 6, 7, 8, and 9 in Appendix A.1 for more details.\nIn another set of experiments, we evaluated the centralized approach using a different combination of models: GPT-4 and GPT-3.5, as shown in Table 3. Here, we observed mostly the same results as the previous combination. The centralized approach reduced bias in categories such as age (0.217 to 0.162) and nationality (0.091 to 0.059). Notably, for the disability group, the method achieved a bias score of 0.0, outperforming both the baseline and decentralized methods."}, {"title": "Decentralized Multi-LLM", "content": "The decentralized multi-LLM approach outperforms both the baseline and centralized methods across most social groups (results in Tables 2 and 3). When using the combination of GPT-4 and llama3-70B, the decentralized method showed significant improvements, especially in categories such as disability and sexual orientation, where the bias score reached 0.0. This finding is particularly noteworthy as it suggests that the decentralized approach can entirely eliminate bias in specific categories. Additionally, the decentralized method reduced bias in the age category from 0.217 (baseline) to 0.132 and in religion from 0.196 to 0.062, further illustrating its strength in addressing bias across a range of social groups.\nInterestingly, the decentralized method also performs well when using GPT-4 and GPT-3.5. In this setup, the method achieved 0.0 bias scores for sexual orientation and disability. This consistency across multiple model combinations highlights the robustness of the decentralized approach in mitigating bias, regardless of the specific models used. However, in some categories, such as physical appearance, the decentralized approach showed a significant increase in bias compared to the centralized approach (0.027 versus 0.63). This suggests that while the decentralized method excels in most cases, there are certain contexts where centralized coordination might still offer an advantage."}, {"title": "Centralized vs. Decentralized Multi-LLM", "content": "Our analysis reveals that the decentralized multi-LLM approach consistently outperforms the centralized approach across most social groups. In the decentralized configuration, models engage in a more distributed form of collaboration, which likely accounts for the superior bias reduction seen across most categories. The centralized approach, while effective, lags in most categories.\nWe also investigate the use of three models in our multi-LLM framework. When using GPT-4, GPT-3.5, and llama3-70B, we noticed that the centralized method outperforms the decentralized method. See Tables 10 and 11 in Appendix A.2 for more details. Additionally, we investigate the effectiveness of conversation rounds for both of our multi-LLM debiasing approaches. Tables 12 and 13 in Appendix A.3 show that the models typically converge on the first round; however, our decentralized approach reaches the third round more often than our centralized method."}, {"title": "Ablation Study", "content": "In this section, we investigate a weighted approach to our multi-LLM debiasing framework. For our weighted approach, we simply ask the LLMs to give a confidence score for their answer on a scale"}, {"title": "Conclusion", "content": "In this paper, we present a multi-LLM debiasing framework that effectively reduces bias in LLMs. In addition to our multi-LLM debiasing framework, we introduce a benchmark for bias evaluation that contains \"hard instances\" of bias, offering a more rigorous testing ground for bias. Our evaluation indicates that incorporating an additional model in a conversational setting not only reduces bias over the baseline but also increases performance in terms of accuracy. Through extensive experimentation, we assess the efficacy of our framework by comparing multi-LLM configurations with two and three models, finding that a two-LLM setup performs slightly better. Additionally, we explore both centralized and decentralized approaches, where our decentralized approach outperforms the centralized and baseline approaches. In summary, our work opens the door for more effective LLM debiasing by leveraging multiple models via a multi-LLM framework. Future studies could focus on"}, {"title": "Limitations", "content": "In this section, we discuss the limitations of our approach. The first limitation concerns the dataset that we used to evaluate our approach. The dataset consists of multiple-choice questions, which may not accurately reflect real-world scenarios and thus could restrict the ability to generalize our findings. Although we used a Q/A dataset in our experiments, our general framework can be applied to text-generation tasks as well. In the future, we would like to use our framework on a text-generation dataset to better simulate the real world. There is also a cost consideration of our approach as it uses two or more models to debias."}, {"title": "Ethical Considerations", "content": "We recognize that the biases present in language models often stem from deep-rooted historical and structural inequalities that impact different social groups in varied ways. Our work on multi-LLM debiasing addresses certain manifestations of these biases, but we understand that technical solutions alone cannot resolve the broader societal issues that contribute to discrimination and inequality. When we refer to \"debiasing\" or \"bias reduction,\" it is important to note that these terms signify a reduction in specific biased behaviors exhibited by the language model rather than the complete elimination of bias or the systemic forces that perpetuate it.\nIt is also crucial to emphasize that technical interventions like the one proposed here should not be viewed as the sole safeguard against representational harms. These methods require careful evaluation, especially when applied in real-world contexts, as discussed in Section 8. The complexities of unequal power dynamics cannot be fully addressed through algorithmic adjustments alone, and our approach should be considered as one piece of a larger puzzle in combating bias."}, {"title": "Additional Metrics", "content": "Table 6 shows the multi-LLMs (GPT-4 and llama3-70B) accuracy in choosing the correct answers for the questions in the BBQ-Hard dataset. Additionally, Table 7 reveals the accuracy scores for GPT-4 and GPT-3.5 as the multi-LLM models. Generally, our decentralized method is more accurate than the centralized and baseline methods. Our decentralized approach notably achieves accuracy scores above 90% in all but two categories. We also calculate the improvement percentages for both multi-LLM combinations as shown in Tables 8 and 9."}, {"title": "Varying Number of LLMS", "content": "We investigate an increase in the number of LLMs that our multi-LLM debiasing framework contains. Using GPT-4, GPT-3.5, and llama3-70B, we increased the number of LLMs from two to three. See Tables 10 and 11 for more details."}, {"title": "Multi-LLM Conversational Analysis", "content": "We investigated the number of questions for each social group requiring a different number of rounds of conversation. For this, we analyzed both centralized and decentralized multi-LLM approaches. We count the number of rounds of conversation the models have for each type of question in Table 12. For reference, we also include the total number of questions for each social group in our BBQ-hard dataset. Further analysis showing the percent of questions with respect to the number of conversational rounds for centralized and decentralized are shown in Table 13. We observe that our multi-LLM centralized and decentralized debiasing approaches are able to generate a debiased response for the majority of questions across all bias types using only a single round of conversation. Interestingly, we see that for multi-LLM centralized debiasing, there is a large percentage of debiased responses resolved within 2 rounds of conversations compared to 3 rounds of conversation, and this result holds across all social groups investigated. However, when considering our multi-LLM decentralized debiasing approach, we see that there are some social groups that are inherently more difficult and require more rounds of conversation, such as age, nationality, religion, etc. Nevertheless, we observe that the social groups that are inherently more difficult to debias are correlated independent of the proposed multi-LLM debiasing frameworks, that is, centralized or decentralized."}, {"title": "Alternative Prompt", "content": "We investigate a modified prompt within our multi-LLM debiasing framework. After receiving the initial response(s), we apply the new prompt to both our centralized and decentralized approaches. The modified prompt is as follows:\nUSER: Another model answered this question:\n[Question]\nand gave this response:\n[previous response]\nAnswer the same question with a single letter and explain why you chose that answer\n[prompt]"}, {"title": "Additional Discussion", "content": "We also provide an alternative and perhaps more detailed overview of our centralized multi-LLM debiasing framework. We selected the centralized multi-LLM debiasing framework since it is slightly more difficult to understand than the decentralized which has more symmetry among the LLMs, and thus is often easier to analyze. In Figure 5, we show the main steps of the approach. The first step shown in Figure 5(a) is the initial debiasing generation by model $M_1$ to obtain $y_1 = M_1(X)$ where X is the user prompt. The debiased response $y_1$ is then communicated to the remaining k \u2212 1 LLMs denoted as $M_2,..., M_k$ as shown in Figure 5(b). Next, each model $M_i \u2208 {M_2, ..., M_k}$ in Figure 5(c) evaluates the response y1 for bias and generates a new response $y_i = M_i(X, y_1)$ if bias is detected. The debiased responses $Y_2,..., Y_k$ generated from the models $M_2,..., M_k$ are then communicated to the central LLM $M_1$ as shown in Figure 5(d). The centralized model $M_1$ then evaluates all the debiased responses $y_2,..., Y_k$ from the k LLMs and generates an updated debiased response $y^{(t+1)}$ based on the prior responses as shown in Figure 5(e). The conversation terminates whenever consensus is reached, or a maximum number of rounds of conversation is reached."}]}