{"title": "SceneDiffuser: Efficient and Controllable Driving Simulation Initialization and Rollout", "authors": ["Chiyu Max Jiang", "Yijing Bai*", "Andre Cornman*", "Christopher Davis*", "Xiukun Huang*", "Hong Jeon*", "Sakshum Kulshrestha*", "John Lambert*", "Shuangyu Li*", "Xuanyu Zhou*", "Carlos Fuertes", "Chang Yuan", "Mingxing Tan", "Yin Zhou", "Dragomir Anguelov"], "abstract": "Realistic and interactive scene simulation is a key prerequisite for autonomous vehicle (AV) development. In this work, we present SceneDiffuser, a scene-level diffusion prior designed for traffic simulation. It offers a unified framework that addresses two key stages of simulation: scene initialization, which involves generating initial traffic layouts, and scene rollout, which encompasses the closed-loop simulation of agent behaviors. While diffusion models have been proven effective in learning realistic and multimodal agent distributions, several challenges remain, including controllability, maintaining realism in closed-loop simulations, and ensuring inference efficiency. To address these issues, we introduce amortized diffusion for simulation. This novel diffusion denoising paradigm amortizes the computational cost of denoising over future simulation steps, significantly reducing the cost per rollout step (16x less inference steps) while also mitigating closed-loop errors. We further enhance controllability through the introduction of generalized hard constraints, a simple yet effective inference-time constraint mechanism, as well as language-based constrained scene generation via few-shot prompting of a large language model (LLM). Our investigations into model scaling reveal that increased computational resources significantly improve overall simulation realism. We demonstrate the effectiveness of our approach on the Waymo Open Sim Agents Challenge, achieving top open-loop performance and the best closed-loop performance among diffusion models.", "sections": [{"title": "Introduction", "content": "Simulation environments allow efficient and safe evaluation of autonomous driving systems [1, 8, 15, 22, 31, 32, 46, 50\u201352, 54]. Simulation involves initialization (determining starting conditions for agents) and rollout (simulating agent behavior over time), typically treated as separate problems [44]. Inspired by diffusion models' success in generative media, such as video generation [2, 10] and video editing (inpainting [21, 24, 28], extension, uncropping etc.), we propose SceneDiffuser, a unified spatiotemporal diffusion model that addresses both initialization and rollout for autonomous driving, trained end-to-end on logged driving scenes. To our knowledge, SceneDiffuser is the first model to jointly enable scene generation, controllable editing, and efficient learned closed-loop rollout (Fig. 1).\nOne challenge in simulation is evaluating long-tail safety-critical scenarios [1, 8, 22, 32, 46]. While data mining can help, such scenarios are often rare. We address this by learning a generative scene realism prior that allows editing logged scenes or generating diverse scenarios. Our model supports scene perturbation (modifying a scene while retaining similarity) and agent injection (adding agents to create challenging scenarios). We also enable synthetic scene generation on roadgraphs with realistic layouts. We design a protocol for specifying scenario constraints, enabling scalable generation, and demonstrate how a few-shot prompted LLM can generate constraints from natural language."}, {"title": "Related Work", "content": "A variety of generative models have been explored for scene initialization and simulation, including autoregressive models [8, 22, 46], cVAEs [45], cGANs [1], and Gaussian Mixture Models (GMMs) [8, 47]. For closed-loop rollouts, these models have been extended with GMMs [51], GANs [15], AR models over discrete motion vocabularies [31], cVAE [54], and deterministic policies [50, 52]. Open-loop rollouts have also been explored using cVAE [35]."}, {"title": "Diffusion Models for Agent Simulation", "content": "Open-loop Sim Open-loop simulation generates behavior for agents that all lie within one's control, i.e. does not receive any external inputs between steps. Open-loop simulation thus cannot respond to an external planner stack (AV), the evaluation of which is the purpose of simulation. Diffusion models have recently gained traction in multi-agent simulation, particularly in open-loop scenarios (multi-agent trajectory forecasting) [31, 39], using either single-shot or autoregressive (AR) generation. Single-shot approaches employ spatiotemporal transformers in ego-centric [6, 18] or scene-centric frames with motion/velocity deltas [9, 53]. Soft guidance techniques enhance controllability [17, 56]. DJINN [27] uses 2d condition masks for flexible generation.\nClosed-loop Sim Closed-loop simulation with diffusion remains challenging due to compounding errors and efficiency concerns. Chang et al. [3] explore route and collision avoidance guidance in closed-loop diffusion, while VBD [14] combines denoising and behavior prediction losses with a query-centric Transformer encoder [42]. VBD found it computationally infeasible to replan at a 1Hz frequency in a receding horizon fashion over the full WOSAC test split due to the high diffusion inference cost, therefore testing in open-loop except over 500 selected scenarios."}, {"title": "Diffusion for Temporal World Modeling and Planning", "content": "Outside of the autonomous driving domain, diffusion models have proven effective for world simulation through video and for planning. Various diffusion models for 4d data have been proposed, often involving spatiotemporal convolutions and attention mechanisms [11, 12, 43]. In robotics, diffusion-based temporal models leverage Model Predictive Control (MPC) for closed-loop control [4] and have shown state-of-the-art performance for imitation learning [29].\nSimilar to our Amortized Diffusion approach, TEDi [55] proposes to entangle the physical timestep and diffusion steps for human animation, thereby reducing $O(T \\cdot T)$ complexity for $T$ physical timesteps and $T$ denoising steps to $O(T)$. However, we are the first work to demonstrate the effectiveness of this approach for reducing closed-loop simulation errors, and the first to extend it to a multi-agent simulation setting."}, {"title": "Method", "content": "We denote the scene tensor as $x \\in \\mathbb{R}^{A \\times T \\times D}$, where $A$ is the number of agents jointly modeled in the scene, $T$ is the total number of modeled physical timesteps, and $D$ is the dimensionality of all the features that are jointly modeled. We learn to predict the following attributes for each agent: positional coordinates $x, y, z$, heading $\\psi$, bounding box dimensions $l, h, w$, and object type $k \\sim \\{AV, car, pedestrian, cyclist\\}$. We model all tasks considered in SceneDiffuser as multi-task inpainting on this scene tensor. Given an inpainting mask $m \\in \\mathbb{B}^{A \\times T \\times D}$, the corresponding inpainting context values $\\tilde{x} := m \\odot x$, a set of global context $c$ (such as roadgraph and traffic signals), and a validity mask for a given agent at a given timestep $v \\in \\mathbb{B}^{A,T}$ (to account for there being $< A$ agents in the scene or for occlusion), we train a diffusion model to learn the conditional probability $p(x|C)$, where $C := \\{m, \\tilde{x}, c, v\\}$. See Fig. 2 for an illustration of the scene tensor.\nFeature Normalization To simplify the diffusion model's learning task, we normalize all feature channels before concatenating them along $D$ to form the scene tensor. We first encode the entire scene in a scene-centric coordinate system, namely the AV's coordinate frame just before the simulation commences. We then scale $x, y, z$ by fixed constants, $l, h, w$ by their standard deviation, and one-hot encode $k$. See Appendix A.6 for more details. This simple yet generalizable process allows us to jointly predict float, boolean, and even categorical attributes by converting into a normalized space of floats. After generating a scene tensor $x$, we apply a reverse process to obtain the generated features.\nDiffusion Preliminaries We adopt the notation and setup for diffusion models from [13]. The forward diffusion process gradually adds Gaussian noise to $x$. The noisy scene tensor at diffusion step $t$ can be expressed as $q(z_t|x) = \\mathcal{N}(z_t|\\sqrt{\\bar{\\alpha}_t}x, (1 - \\bar{\\alpha}_t)I)$, where $\\sqrt{\\bar{\\alpha}_t}$ and $\\sqrt{1-\\bar{\\alpha}_t}$ are parameters which control the magnitude and variances of the noise schedule under a variance-preserving model. Therefore $z_t = \\sqrt{\\bar{\\alpha}_t}x + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon_t$, where $\\epsilon_t \\sim \\mathcal{N}(0, I)$. One major departure from the classic diffusion setup in our amortized diffusion regime is that we do not assume a uniform noise level $t \\in \\mathbb{R}$ for the entire scene tensor $x$. Instead, we have $t \\in \\mathbb{R}^{A \\times T}$ where $t$ can be relaxed to have a different value per physical timestep in the scene tensor as described in Sec. 3.2. We utilize the commonly used $\\alpha$-cosine schedule where $\\bar{\\alpha}_t = cos(\\pi t/2)$ and $\\sqrt{1-\\bar{\\alpha}_t} = sin(\\pi t/2)$. At the highest noise level of $t = 1$, the forward diffusion process completely destroys the initial scene tensor $x$ resulting in $z_t = \\epsilon_t \\sim \\mathcal{N}(0, I)$. Assuming a Markovian transition process, we have the transition distributions $q(z_t|z_s) = \\mathcal{N}(z_t|\\sqrt{\\bar{\\alpha}_{t|s}}z_s, (1 - \\bar{\\alpha}_{t|s})I)$, where $\\sqrt{\\bar{\\alpha}_{t|s}} = \\sqrt{\\bar{\\alpha}_t}/\\sqrt{\\bar{\\alpha}_s}$ and $1 - \\bar{\\alpha}_{t|s} = \\sqrt{1-\\bar{\\alpha}_t}^2 - \\sqrt{\\bar{\\alpha}_{t|s}}^2$ and $t > s$. In the denoising process, conditioned on a single datapoint $x$, the denoising process can be written as\n\\begin{equation}\nq(z_s|z_t, x) = \\mathcal{N}(z_t|\\mu_{t\\rightarrow s}, \\sigma_{t\\rightarrow s}^2 I),\n\\end{equation}\nwhere $\\mu_{t\\rightarrow s} = \\frac{\\sqrt{\\bar{\\alpha}_s}(1-\\bar{\\alpha}_t)}{\\sqrt{\\bar{\\alpha}_t}}z_t + \\frac{\\sqrt{1-\\bar{\\alpha}_s^2} \\sqrt{\\bar{\\alpha}_t}}x$ and $\\sigma_{t\\rightarrow s}^2 = \\frac{1-\\bar{\\alpha}_s^2}{1-\\bar{\\alpha}_t}$. In the denoising process, $x$ is approximated using a learned denoiser $\\epsilon$. Following [13] and [37], we adopt the commonly used v prediction, defined as $v_t (\\epsilon_t, x) = \\sqrt{\\bar{\\alpha}_t} x + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon_t$. We trained a model parameterized by $\\theta$ to predict $v_t$ given $z_t$, $t$ and context $C$: $\\hat{v}_t := \\hat{v}_\\theta(z_t, t, C)$. The predicted $\\epsilon_t$ can be recovered via $\\hat{\\epsilon}_t = \\frac{\\sqrt{\\bar{\\alpha}_t}}{\\sqrt{1-\\bar{\\alpha}_t}}z_t - \\frac{\\sqrt{\\bar{\\alpha}_t}}{\\sqrt{1-\\bar{\\alpha}_t}}\\hat{v}_t$. The model is end-to-end trained with a single loss:\n\\begin{equation}\n\\mathbb{E}_{(x,C)\\sim\\mathcal{D},t\\sim U(0,1),\\epsilon_t\\sim\\mathcal{N}(0,I)}[||\\hat{v}_\\theta (z_t, t, C) - v_t (\\epsilon_t, x)||^2],\n\\end{equation}"}, {"title": "Scene Rollout", "content": "Future prediction with no replanning ('One-Shot') is not used in simulation due to its non-reactivity, and forward scene inference, under the standard diffusion paradigm ('Full AR'), is computationally intensive due to the double for-loop over both physical rollout steps and denoising diffusion steps [55]. Moreover, executing only the first step while discarding the remainder leads to inconsistent plans that result in compounding errors. We adopt an amortized autoregressive (\u2018Amortized AR') rollout, aligning the diffusion steps with physical timesteps to amortize diffusion steps over physical time, requiring a single diffusion step at each simulation step while reusing previous plans.\nWe illustrate the three algorithms in Algorithm 1-3 using the same model trained with a noise mixture $t \\sim \\{U(0,1); t\\}$ (Eqn. 2). We also illustrate Algorithm 3 in Fig. 4. We denote the total number of timesteps $T = H + F$, where $H, F$ denote the number of past and future steps. We denote $x := x[-H:F]$ to be the temporal slicing operator where $x[0]$ is the final history step."}, {"title": "Controllable Scene Generation", "content": "To simulate long-tail scenarios such as rare behavior of other agents, it is important to effectively insert controls into the scene generation process. To do so, we input an inpainting context scene tensor, where some pixels are pre-filled. Through pre-filled feature values in $x$, we can specify a particular agent of a specified type to be appear at a specific position at a specific timestamp.\nData Augmentation via Log Perturbation The diffusion framework makes it straightforward to produce additional perturbed examples of existing ground truth (log) scenes. Instead of starting from pure noise $z_t \\sim \\mathcal{N}(0, I)$ and diffusing backwards from $t \\rightarrow 0$, we take our original log scene $x'$ and add noise to it such that our initial $z_t = \\sqrt{\\bar{\\alpha}_t}x' + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon_t$ where $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma_t^2I)$. Starting the diffusion process at $t = 0$ yields the original data, while $t = 1$ produces purely synthetic data. For $t \\in (0, 1)$, higher values increase diversity and decrease resemblance to the log. See Figs. 1 and 12 (Appendix).\nLanguage-based Few-shot Scene Generation The diffusion model inpaint constraints can be defined through structured data such as a Protocol Buffer (\u2018proto'). Protos can be converted into inpainting values, and we leverage the off-the-shelf generalization capabilities of a publicly accessible"}, {"title": "Generalized Hard Constraints", "content": "Users of simulation often require agents to have specific behaviors while maintaining realistic trajectories. However, diffusion soft constraints [27, 56, 57] require a differentiable cost for the constraint and do not guarantee constraint satisfaction. Diffusion hard constraints [21] are modeled as inpainting values and are limited in their expressivity.\nInspired by dynamic thresholding [36] in the image generation domain, where intermediate images are dynamically clipped to a range at every denoising step, we introduce generalized hard constraints (GHC), where a generalized clipping function is iteratively applied at each denoising step. We modify Eqn. 1 such that at each denoising step $\\mu_{t\\rightarrow s} = \\frac{\\sqrt{\\bar{\\alpha}_s}(1-\\bar{\\alpha}_t)}{\\sqrt{\\bar{\\alpha}_t}}z_t + \\frac{\\sqrt{1-\\bar{\\alpha}_s^2} \\sqrt{\\bar{\\alpha}_t}}{\\sqrt{\\bar{\\alpha}_t}}clip(x)$, where $clip(\\cdot)$ denotes the GHC-specific clipping operator. See more details on constraints in Appendix A.9.\nWe qualitatively demonstrate the effect of hard constraints for unconditional scene generation in Fig. 8. Applying hard constraints post-diffusion removes overlapping agents but results in unrealistic layouts, while applying the hard constraints after each diffusion step both removes the overlapping agents and takes advantage of the prior to improve the realism of the trajectories. We find that the basis on which the hard constraints operate is important: a good constraint will modify a significant fraction of the scene tensor (for example, shifting an agent's entire trajectory rather than just the overlapping waypoints), or else the model \"rejects\" the constraint on the next denoising step."}, {"title": "Experimental Results", "content": "Dataset We use the Waymo Open Motion Dataset (WOMD)[7] for both our scene generation and agent simulation experiments. WOMD includes tracks of all agents and corresponding vectorized maps in each scenario, and offers a large quantity of high-fidelity object behaviors and shapes produced by a state-of-the-art offboard perception system."}, {"title": "Simulation Rollout", "content": "Benchmark We evaluate our closed-loop simulation models on the Waymo Open Sim Agent Challenge (WOSAC) [23] metrics (see Appendix A.1), a popular sim agent benchmark used in many recent works [9, 14, 31, 51, 53]. Challenge submissions consist of x/y/z/$\\psi$ trajectories representing centroid coordinates and heading of the objects' boxes that must be generated in closed-loop and with factorized AV vs. agent models. WOSAC uses the test data from the Waymo Open Motion"}, {"title": "Scene Generation", "content": "Unconstrained Scene Generation We use the unconditional scene generation task as a means to quantitatively measure the distributional realism of our model. We condition the scene using the same logged road graph and traffic signals, as well as the logged agent validity to control for the same number of agents generated per scene. All agent attributes are generated by the model.\nDue to a lack of public benchmarks for this task, we adopt a slightly modified version of the WOSAC [23] metrics, where different metrics buckets are aggregated per-scene instead of per-agent, due to the lack of one-to-one correspondence between agents in the generated scene versus the logged scene (see Appendix A.2 for more details). Metrics are aggregated over all agents that are ever valid in the 9 second trajectory.\nWe show our model's realism metrics in Tab. 1. Even compared to the oracle performance (comparing logged versus logged distributions), our model achieves comparable realism scores in every realism bucket. Introducing hard constraints on collisions can significantly improve the composite metric by preventing collisions, while scaling the model without hard constraints improves most realism"}, {"title": "Model Design Analysis and Ablation Studies", "content": "Scaling Analysis Given two options of scaling model compute, either by increasing transformer temporal resolution by decreasing temporal patch sizes, or increasing the number of model parameters, we investigate the performance of multiple transformer backbones: {Model Size} \u00d7 {Temporal Patch Size} = {L, M, S} \u00d7 {8, 4, 2, 1}. We vary model size by jointly scaling the number of transformer layers, hidden dimensions, and attention heads (see Sec. A.6 of Appendix for details). We show quantitative results from this model scaling in Fig. 6 and qualitative comparisons in Fig. 11. Increasing both temporal resolution and number of model parameters improves realism of the simulation.\nMulti-task Compatibility We find that multitask co-training across BP, SceneGen and with random control masks improves performance compared to a single-task, BP only model on the sim agent rollout task, notably reducing collision and offroad rates. We find that jointly learning multiple agent features (x, y, z, $\\psi$, size, type) achieves on-par performance with a pose-only (x, y, z, $\\psi$) model."}, {"title": "Conclusion", "content": "We have introduced SceneDiffuser, a scene-level diffusion prior designed for traffic simulation. SceneDiffuser combines scene initialization with scene rollout to provide a diffusion-based approach to closed-loop agent simulation that is efficient (through amortized autoregression) and controllable (through generalized hard constraints). We performed scaling and ablation studies and demonstrated model improvements with computational resources. On WOSAC, we demonstrate competitive results with the leaderboard and state-of-the-art performance among diffusion methods.\nLimitations While our amortized diffusion approach is, to our knowledge, the only and best performing closed-loop diffusion-based agent model with competitive performance, we do not exceed current SOTA performance for other autoregressive models. We do not explicitly model validity masks and resort to logged validity in this work. Future work looks to also model the validity mask.\nBroader Impact This paper aims to improve AV technologies. With our work we aim to make AVs safer by providing more realistic and controllable simulations. The generative scene modeling techniques developed in this work could have broader social implications regarding generative media and content generation, which poses known social benefits as well as risks of misinformation."}, {"title": "Appendix / supplemental material", "content": "Suppose there are $N \\approx 500k$ scenarios, each of length $T = 80$ steps, each containing $A < 128$ agents (objects). For each scenario, we generate $K = 32$ samples (conditioned on the true initial state), which is a set of trajectories for each object for each time step, where each point in the trajectory is a $D = 4$-dim vector recording location ($x, y, z$) and orientation $\\theta$. Let all this generated data be denoted by $x(1 : N, 1 : A, 1 : K, 1 : T, 1 : D)$. Let the ground truth data be denoted $x^*(1 : N, 1: A', 1 : T, 1 : D)$. Below we discuss how to evaluate the likelihood of the true (test) dataset $x^*$ under the distribution induced by the simulated dataset $x$.\n(Note that we may have $A' > A$, since the ground truth (GT) can contain cars that enter the scene after the initial prefix used by the simulator; this is handled by defining a validity mask, $\\upsilon(1 : N, 1 : T, 1 : A')$, which is set to 0 if we want to exclude a GT car from the evaluation, and is set to 1 otherwise.)\nRather than evaluating the realism of the full trajectories in the raw ($x, y, z, \\theta$) state space, WOSAC defines $M = 9$ statistics (scalar quantities of interest) from each trajectory. Let $F_j(x(i, a, :))$ represent the set of statistics/features (of type j) derived from $x(i, a, 1 : K, 1 : T)$ by pooling over $T, K$. This is used to compute a histogram $p_{ija}(.)$ for the empirical distribution of $F_j$ for scenario i. Let $F_j(x^* (i, a, t))$ be the value of this statistic from the true trajectory i for vehicle a at time t . Then we define the negative log likelihood to be\n\\begin{equation}\nNLL(i,a,t, j) = - log p_{ija}(F_j(x^* (i, a, t))\n\\end{equation}\nThe j'th metric for scenario i is defined as\n\\begin{equation}\nm(a,i,j) = exp\\{-\\frac{1}{N(i, a)} [\\sum_{t} \\upsilon(i, a, t)NLL(i, a, t, j))]\n\\end{equation}\n\\begin{equation}\nN(i, a) = \\sum_{t} \\upsilon(i,a,t)\n\\end{equation}\n$N(i, a)$ is the number of valid points.\nFinally an aggregated metric, used to rank entries, is computed as\n\\begin{equation}\nscore = \\frac{1}{N' M} \\sum_{i=1}^{N'} \\sum_{j=1}^{M} w_jm(i, j)\n\\end{equation}\nwhere $0 \\leq w_j \\leq 1$.\nThe 9 component metrics are defined as linear speed, linear acceleration magnitude, angular speed, angular acceleration magnitude, distance to nearest object, collisions, time-to-collision (TTC), distance to road edge, and road departures."}, {"title": "Additional Evaluation Details", "content": "Simulation step validity Due to the requirement of validity masks during inference, which is applied as an attention padding mask within the transformer, the model does not generate valid values for invalid steps. As the WOSAC challenge evaluates simulation agents for all steps, regardless of the step's validity, we use linear interpolation / extrapolation to impute values for all invalid steps in our simulations for the final evaluation."}, {"title": "Additional Dataset Information", "content": "WOSAC uses the v1.2.0 release of WOMD, and we treat WOMD as a set D of scenarios where each scenario is a history-future pair. This dataset offers a large quantity of high-fidelity object behaviors and shapes produced by a state-of-the-art offboard perception system. We use WOMD's 9 second 10 Hz sequences (comprising H = 11 observations from 1.1 seconds of history and 80 observations from 8 seconds of future data), which contain object tracks at 10 Hz and map data for the area covered by the sequence. Across the dataset splits, there exists 486,995 scenarios in train, 44,097 in validation, and 44,920 in test."}, {"title": "Additional Amortized Diffusion Algorithm Details", "content": "Warm up: At inference time, the rollout process is preceded by a warm up step. The warm up step is necessary for initializing a buffer of future timesteps before any diffusion iterations take place. The warm up entails a single iteration of a one-shot prediction process described in Algorithm 1. This process samples pure noise for some future steps and conditions the denoising process on the set of past steps.\nAmortized autoregressive rollout: In Fig. 4, we provide a visual illustration of our amortized autoregressive rollout procedure. We operate the rollout procedure using a buffer to track future steps in the trajectory. After the warm up, the future buffer contains T predicted steps with an increasing noise level. Note that step $\\tau = 1$ has very little noise applied . The future buffer in this state is denoised for a single iteration using past steps to condition the process. After a single iteration, the clean step at $\\tau = 1$ is popped off of the buffer, and it is added to the past steps. Before the next iteration, a step $\\tau' = T + 1$ is sampled from a pure noise distribution and is appended to the end of the future buffer. The described rollout process can be repeated to generate trajectories of arbitrary length as clean steps are popped off the buffer."}, {"title": "Additional Implementation Details", "content": "For our base model, our scene encoder architecture uses 256 latent queries. Each scene token is 256-dimensional, with 4 transformer layers and 4 transformer heads, with a transformer model dimension of 256. We train and run inference with all 128 agents."}, {"title": "Prompts used in Language-based few-shot Scene Generation", "content": "Prompt 1: Prompts used in Language-based few-shot Scene Generation.\nYou are writing proto to generate and control an agent's behavior for an autonomous vehicle (AV) simulation. I will give the follow 2 examples of input and the generated proto MultiAgent Multi Constraint, which will constrain the agent's position in either past, current, or future timestep. 5 timesteps equals 1 second, so for example 15 steps would equal 3 seconds. Given a natural language description of the agent's desired behavior, please generate the corresponding MultiAgent MultiConstraint.\nVery Important limitations:\n1. Only time_step_idx values in [0,8] are valid for PAST time step.\n2. Only time_step_idx values in [0] are valid for CURRENT time step.\n3. Only time_step_idx values in [0,49] are valid for FUTURE time step.\n4. You may only use types POT_CAR, POT_MOTORCYCLIST, POT_PEDESTRIAN to generate these examples\n5. No two agents should overlap each other at the same time step in the same time frame."}, {"title": "Generalized Hard Constraint Definitions", "content": "Non-collision Constraints ensure the boxes of generated agents do not overlap. We define the potential field of agent a to be a rounded square potential $s_a(x, y) = \\frac{1}{(x-x_a)^4+(y-y_a)^4+e}$ if $||(x - X_a, Y - Y_a)||_2 < 1.5$ else 0. We define clip_{collision}(x) = arg min $(\\sum_{a\\epsilon A} \\sum_{i = +0.5,j = +0.5} \\sum_{a'\\epsilon A, a'\\neq a} s_{a'}(x_a + iw_a, y_a + jl_a))$ that minimizes the potential of each agent's corners against all other agents. (x, y) is defined in the normalized space.\nRange Constraints limit a certain feature $x_a$ within the range of $d_{min}$ and $d_{max}$. In the context of Scene Generation for example, this can be used to limit the length of a vehicle to an arbitrary range, e.g. between 7-9 meters. We have $clip_{range}(x_d) = min(max(x_a, d_{min}), d_{max})$."}, {"title": "Onroad Constraints", "content": "ensure that the bounding boxes of specified generated agents stay on road. We define the offroad potential of road graph polyline i to be $o_i(x, y) = (x - x_i)^2 + (y - y_i)^2$ if $W_i(x, y) = 0$ else 0, where $(x_i, Y_i)$ is the closest point on the road graph with respect to (x, y) and $W_i(x, y)$ is the winding number of position (x, y) to polyline i, such that we only penalize a trajectory for going offroad. We only consider the closest road graph segment and only consider trajectories that are more than > 20% onroad. We define $clip_{onroad}(x) = arg min (min_{i\\epsilon RG} o_i(x, y))$."}]}