{"title": "Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization", "authors": ["Ermo Hua", "Che Jiang", "Xingtai Lv", "Kaiyan Zhang", "Ning Ding", "Youbang Sun", "Biqing Qi", "Yuchen Fan", "Xuekai Zhu", "Bowen Zhou"], "abstract": "Extending the context length of Language Models (LMs) by improving Rotary Position Embedding (ROPE) has become a trend. While existing works mainly address RoPE's limitations within attention mechanism, this paper provides an analysis across nearly all parts of LMs, uncovering their adverse effects on length generalization for RoPE-based attention. Using Discrete Signal Processing theory, we show that RoPE enables periodic attention by implicitly achieving Non-Uniform Discrete Fourier Transform. However, this periodicity is undermined by the spectral damage caused by: 1) linear layers and activation functions outside of attention; 2) insufficiently trained frequency components brought by time-domain truncation. Building on our observations, we propose Fourier Position Embedding (FoPE), which enhances attention's frequency-domain properties to improve both its periodic extension and length generalization. FoPE constructs Fourier Series and zero-outs the destructive frequency components, increasing model robustness against the spectrum damage. Experiments across various model scales show that, within varying context windows, FoPE can maintain a more stable perplexity and a more consistent accuracy in a needle-in-haystack task compared to ROPE and ALiBi. Several analyses and ablations bring further support to our method and theoretical modeling.", "sections": [{"title": "1 Introduction", "content": "Generation based on the information from long contexts is crucial for Language Models (LMs). However, LMs are typically trained on a fixed context window (Vaswani, 2017; Touvron et al., 2023; Groeneveld et al., 2024) and tends to overfit to the specific context length.\nMany studies consider the absolute position embedding (Vaswani, 2017) to be the source of overfitting in length generalization. As mitigation, several relative position embedding methods have been proposed (Press et al., 2021; Su et al., 2024; Peng et al., 2023; Jin et al., 2024) to improve LMs' long-distance dependency. Among these, AL\u0130B\u0130 (Press et al., 2021) introduced a position-biased attention mask, which linearly declines the attention weights based on distance. ALiBi delivers stable perplexity in pre-training, but it loses the information from long-distance tokens, resulting in poor performance on long-context downstream tasks. Another method, RoPE (Su et al., 2024), uses the phase of complex numbers to store the position information. Combined with continual pre-training and other interpolation-based methods (Peng et al., 2023; Xiong et al., 2024; Chen et al., 2024; Jin et al., 2024), RoPE provides better access to long-distance information, making it one of the most widely used position embedding. However, RoPE-based LMs still struggle with length generalization without supplementary methods."}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.1 Non-Uniform Discrete Fourier Transform", "content": "Given a finite sequence of equally-spaced samples {Xn} := X0, X1, ..., XN\u22121 of a continuous function x, Discrete Fourier Transform (DFT) converts them into equally-spaced frequency-domain components {Xm} := X0, X1, ..., XM-1, the original samples can be recovered by Inverse DFT (IDFT):\n$$X_m = \\sum_{n=0}^{N-1}x_n e^{-i \\frac{2 \\pi}{N} mn}, x_n = \\frac{1}{M} \\sum_{m=0}^{M-1} X_m e^{i \\frac{2 \\pi}{M} mn}$$\nAs $e^{iwn} = cos wn + i sin wn$ is periodic in the original domain, DFT implicitly transforms the original function into a linear combination of periodic waves with frequency $Wm = \\frac{2\\pi}{N}$. Thus, DFT is an estimation of the original function, which is lossless only if the original function exactly composed of these specific periodic components.\nTo achieve a more precise approximation, the sampled frequencies can follow any arbitrary distribution {$w_m$} := $w_0, w_1, \u2026, w_{M\u22121}$, where the only constraint is wm \u2208 [0, 2\u03c0). The generalization of DFT is called Non-Uniform DFT (NUDFT)."}, {"title": "2.2 RoPE implicitly achieves Periodic Attention based on NUDFT", "content": "Given a M dimension Query Q and Key K of token a and b, RoPE rotates them to different phase based on the dimension m:\n$$q_m(n_a) = Q_m e^{iw_m n_a}, k_m(n_b) = K_m e^{i w_m n_b}$$\nwhere $w_m = 1/\u03b8(2m/M)$ and @ is the pre-defined parameters in RoPE. Then, the attention weight hm (n) in each dimension will be calculated as:\n$$h_m(n) = q_m(n_a) k_m^*(n_b) = H_m e^{i w_m n}$$\nwhere n = na-n\u044c and Hm = QmKm. Finally, the overall attention weight between different tokens can be formalized as:\n$$h(n) = \\sum_{m=0}^{M-1} h_m(n) = \\sum_{m=0}^{M-1} H_m e^{i w_m n}$$\nComparing it with Eq (1), it can be observed that ROPE implicitly achieves a token-level Inverse NUDFT with frequency components {$Wm$}.\nBased on NUDFT, ROPE models the interactions between different tokens as functions composed of several periodic components, which brings periodic extension in each dimension m:\n$$h_m(n + N_{w_m}) = h_m(n)$$\nwhere $N_{w_m} = \\frac{2 \\pi}{w_m}$ is this component's period. This property can generalize LMs to longer context."}, {"title": "3 Spectrum Damage Confine the Length Generalization", "content": "Ideally, RoPE-based Attention achieves periodic extension in any length scenario. However, this extension is confined as a key ideal property that is not guaranteed in LMs."}, {"title": "3.1 Negative Influence of Spectrum Damage", "content": "The ideal coefficients and frequencies of NUDFT have one-to-one correspondence. The coefficient of each frequency represents the influence of each token on others propagated at a specific wavelength. However, the periodic extension is hindered, if the coefficient also contains the information from another frequency component w\uff61 with coefficient $H_{\u03c9_0} = \u03c3H_\u03c9$, called the Spectrum Damage.\nIf we define the damaged function as $h'_m = H_{w_m} [(1 \u2013 \u03c3)e^{iw_m n} + \u03c3e^{iw_0 n}]$, we find:\n$$h'_m(n + N_{w_m}) \u2260 h_m(n)$$\nas $N_{w_m}$ is not the period of $h_{w_0}$. In other words, the information from each component is transmitted through waves with mismatched wavelengths, leading to inaccurate estimation of the influence propagated within each wavelength. As a result, the periodic extension and length generalization of attention are adversely affected."}, {"title": "3.2 Spectrum Damage Outside Attention", "content": "The LMs' linear layers and activation functions outside attention bring two types of spectrum damage, destroying the one-to-one correspondence between coefficients and frequencies.\nLinear Layer uses weights $W \u2208 R^{M\u00d7M}$ to map a M dimension hidden state $X \u2208 R^M$ to another hidden state Y\u2208 RM. Thus, each dimension of Y will be a linear combination of different components of X:\n$$Y_m = \\sum_{k=0}^{M-1} W_{km}X_k$$\nThis results in Spectrum Leakage, as different frequency components exhibit interplay."}, {"title": "Activation Function", "content": "Activation Function has non-linearity in the time domain, generating harmonic frequencies as described by the following Lemma:\nLemma 1. Given a double-frequency sinusoid function x(n) = cos w\u2081n + cos w2n and any time-independent non-linear function g. The effect of g on x(n) will produce waves whose frequencies are the linear combinations of w\u2081 and w2:\n$$g(x(n)) = \\sum_{j \\in N} \\sum_{k \\in N} a_{j,k} cos(jw_1 + kw_2)n$$\nwhich can be generalized to any multi-frequency function x(n) = \u2211(aw sinwn + bw cos wn).\nAs the hidden states have been transformed into multi-frequency functions by Linear Layer, passing them across Activation Functions introduces additional harmonic components, leading to serious Spectrum Distortion.\nThese two types of Spectrum Damage undermine the periodic extension of attention (as shown in Eq.(5)(6)), hindering the model's length generalization property."}, {"title": "3.3 Spectrum Damage Inside Attention", "content": "Besides the spectrum damage outside attention, the undertrained components of attention within extremely low frequencies ($w_m < \\frac{2\\pi}{N}$) also bring spectrum damage.\nConsider a single-frequency function $x_m(n) = e^{iw_m n}rect(n)$ truncated by a square wave:\n$$rect(n) = \\begin{cases} 1, n \\le N \\\\ 0, n > N \\end{cases}$$\nBased on the results of DFT, the spectrum estimation of x(n) is\u00b9:\n$$X(\u03c9) = \u03b1 \u03b4(\u03c9_m) + \\frac{N \\cdot sin[(N \u2013 \u03b1 N_m)(\u03c9 \u2013 \u03c9_m)]}{\u03c9 \u2013 \u03c9_m}$$\nwhere \u03b1 = [\\frac{N}{N_m}] and $N_m = \\frac{2\\pi}{w_m}$.\nIn the frequency domain, time-domain truncation introduces noisy components via the latter sub-function. When the period of the primary frequency component exceeds the truncation length, its amplitude is significantly weakened. Consequently, noisy components dominate these dimensions, impairing the periodic extension (as defined by Eq.(5)(6)). In contrast, high-frequency components are minimally affected because their coefficients a dominate over the noisy components."}, {"title": "4 Fourier Position Embedding", "content": "To mitigate the negative affect of the non-ideal frequency-domain properties in LMs, we propose Fourier Position Embedding (FoPE) to modify frequency-domain properties of attention:\nTreating Each Dimension as Multi-Frequency. Although Linear Layers and Activation Functions bring serious Spectrum Leakage and Spectrum Distortion, they are crucial for enhancing expressive capacity. Therefore, we keep these modules unchanged but focus on modifying how attention processes information within each dimension.\nTo achieve this, we replace the single frequency in each dimension with Fourier Series:\n$$h_m(n) = H_m(n)(e^{iw_m n} + \\sum_w \u03b1_\u03c9 e^{iw n})$$\nwhere aw < 1 because Wm is the dominant frequency. This allows attention modules to capture multi-frequency information in each dimension.\nWe initialize vector {wm} as same as RoPE, and initialize vector {w} and matrix {aw} based on the analysis in Sec 3.2: For {w} \u2208 RD, we make sure M < D so that {wm} \u2286 {w}, and the other frequencies can be sampled within [0, \u03c0] in any distribution. For {aw} \u2208 RD\u00d7M, we initialize it with N(0, \u03c3) based on the hypothesis that the Spectrum Damage obeys the similar distribution as the Linear Layers. The coefficients for the real and imaginary part of the frequency are sampled separately in our implementation, which can also use the same coefficient. The D and o are kept as hyper-parameters to be adjusted.\nZero-out Undertrained Frequencies. As analyzed in Sec 3.3, the inadequate training of extremely-low frequencies $w_m < \\frac{2\\pi}{N}$ impairs the frequency-domain properties of attention. Thus, we define the floor frequency as $w_l = \\frac{2\\pi}{N}$, and clip the frequencies under the floor frequency to zero.\nWe choose zero as the substitute because the zero-frequency component can represent any period, making it easier to train and ensuring stable periodic extensions. Also, since the zero-frequency component has the longest wavelength and typically carries the most information, this substitution does not compromise the length generalization or hinder model fitting.\nOverall function of FoPE can be formalized as:\n$$h_m(n) = H_m(n)f(w_m)$$\n$$f(w_m) = \\begin{cases} 1 \\\\ e^{iw_m n} + \\sum_w \u03b1_\u03c9 e^{iw n} \\end{cases}, \\begin{aligned} w_m < w_l \\\\ w_m \\ge w_l \\end{aligned}$$\nwhich treats each dimension either as a Fourier Series or as a zero-frequency component.\nImplementation of FoPE can be easily achieved with a weight matrix WF \u2208 RD\u00d7(M-Mo), where Mo is the number of zero-frequency components in each head (details in B). This matrix maps the coefficients of all frequencies to a Fourier Series for each dimension. Since the zero-frequency sinusoidal function does not affect the original hidden states, the output dimension is less than the dimension of each head. To introduce more diversity and better simulate the randomness of the Spectrum Damage, we assign separate weights for different heads, as well as for the cosine and sine functions. In our implementation, gradients are not required for these matrices, so FoPE adds negligible memory and computation overhead compared to ROPE."}, {"title": "5 Experiments", "content": "To demonstrate the effectiveness of FoPE as both a position embedding and an extrapolation method, we conduct experiments during pre-training (Sec. 5.2) and fine-tuning (Sec. 5.3). Additionally, we perform ablation studies to analyze the impact of hyperparameters on FoPE (Sec. 5.4) and analysis to demonstrate the necessity to zero-out undertrained components (Sec. 5.5)."}, {"title": "5.1 Basic Settings", "content": "We mainly consider two metrics: perplexity for pre-training and accuracy on Passkey Retrieval.\nPerplexity quantifies how well a language model predicts a sequence of words or tokens. A lower perplexity indicates the model is more confident and accurate in its predictions.\nAccuracy on Passkey Retrieval (Mohtashami and Jaggi, 2023) measures the models' ability in retrieving a short passkey (i.e., a five-digit number) from a large context full of meaningless text. We conduct this evaluation based on the implementation from (Peng et al., 2023). During evaluation, the passkey is randomly positioned at uniformly distributed locations within the context. For each context length, we test for 1000 trials to ensure the positions sampled are sufficiently dispersed.\nWe conduct experiments with the OLMO (Groeneveld et al., 2024) framework and consider different scale models having 60M, 180M, 1.2B parameters."}, {"title": "5.2 Length Generalization after Pre-Training", "content": "We consider two settings to evaluate both the intra-domain and out-of-domain generalization:\nSetting 1: We train models with a 10B-tokens subset of C4 (Raffel et al., 2020) and evaluate them in a validation set from C4.\nSetting 2: We train models with ~5B tokens from Gutenberg Books (Hart, 2007) and evaluate them in the same validation set as Setting 1. In this setting, the language distribution is different between the validation set and the training set, which can further evaluate the generalization ability of different methods.\nResults of Perplexity. (See Fig 1.b & 4) In both settings, FoPE shows a significant advantage over ROPE. But FoPE is slightly worse than ALiBi, as there is an issue when ALiBi meets this training corpus, which is also mentioned in other papers (Peng et al., 2023; Chen et al., 2024). On the one hand, the corpus in C4 and Books mainly have short-distance dependency, thus the information from a short context window is enough for the prediction of almost all tokens. On the other hand, AliBi uses linear declined attention to eliminate long-distance information, and only pays attention to short-distance dependency. Based on these two reasons, ALiBi does not have any decline in perplexity as the context length increases.\nResults of Passkey. (See Fig 1.a) In this task, FoPE demonstrates a significant advantage over both ROPE and ALiBi. RoPE's accuracy drops sharply to zero at twice the training length and remains at zero for longer sequences. ALiBi shows a linear decline in accuracy, further illustrating that its linearly declining attention is unable to capture information from long distances. In contrast, FoPE maintains stable retrieval accuracy for passkeys at any position, demonstrating a strong ability to extract subtle information from long sequences."}, {"title": "5.3 Length Generalization after Fine-Tuning", "content": "Beyond the use of positional embeddings during the pre-training phase, several post-pre-training extrapolation methods (Peng et al., 2023; Chen et al., 2024) have been proven critical for enhancing length generalization. Thus, we investigate two key aspects of FoPE: 1) whether existing extrapolation methods are also effective for FoPE; 2) whether FoPE can enable extrapolation on RoPE-based models, thereby allowing seamless integration with existing open-source models. In this sub-experiment, we select a representative extrapolation method, YARN (Peng et al., 2023), as our baseline. We fine-tune the last checkpoint from pre-training for ~ 1B tokens in this setting.\nResults (See Fig 3). Compared to RoPE+YARN, FoPE+YARN achieves significantly better length generalization performance, as demonstrated by lower perplexity on the C4 dataset and higher accuracy in the Passkey Retrieval task. Moreover, FoPE outperforms YARN in length extrapolation for both RoPE-based and FoPE-based models. These findings underscore the effectiveness and practical utility of FoPE, which holds the potential to enhance all RoPE-based open-source models."}, {"title": "5.4 Ablation Studies", "content": "We also conduct ablation studies on various hyperparameters to observe their effects on our algorithm. Considering the consistent performance of FoPE across different parameter scales, we only evaluate the 60M models in ablation studies.\nBoth sub-methods of FoPE are useful (See Fig 5a). FoPE is constitutive of two parts, called Fourier Series (FS) and Clip Floor to Zero (CF). Although these two sub-methods are both useful for length generalization, combining them together brings a more significant improvement. On one hand, FS contributes more to length generalization, which demonstrates that the Spectrum Damage have a significant influence on length generalization. On the other hand, CF contributes more to fitting the current dataset and sequence length, which implies the zero-frequency component is the most informative and indispensable component.\nIncreasing the dimension of attention heads is more beneficial than increasing the number of attention heads or layers (See Fig 5a). More dimensions introduce more frequency components, making attention more robust to Spectral Damage. In contrast, adding more attention heads and layers aggravates Spectrum Damage, which diminishes the benefits of expanding the parameter scale.\nVariance \u03c3 of {aw} (See Fig 5b). We keep D = 16 to only evaluate \u03c3's influence. By grid searching \u03c3 from 0 to 0.5, we find that setting \u03c3 = 0.3 for 60M model can obtain the best perplexity, especially for long context length. The best o implies the estimated strength of Spectrum Damage in 60M models, and the estimation may become larger as the parameter scale increases.\nNumber D of {w}. We keep \u03c3 = 0.3 to only evaluate D's influence. By grid searching \u03c3 from 16 to 128, we find that D does not significantly influence the perplexity, but it is important for Passkey Retrieval. Setting D = 64 can obtain the best accuracy for Passkey Retrieval. The best D is the estimated number of strong enough noisy components of each model, and this number may become larger as the parameter scale increases. The harmonic frequencies tend to be weaker than the base frequencies, and this phenomenon is more significant to the higher-order harmonics. Thus, there are limited noisy frequency components that have enough intensity to disturb the passing of the base wave, paying attention to not important components hinders the effectiveness of the model."}, {"title": "5.5 The Necessity to Zero-Out Undertrained Components", "content": "To further explain the negative influence of the extremely low-frequency components and demonstrate the necessity to zero-out them, we also investigate their properties in the time domain.\nBy visualizing the numerical expectations of the q and k vectors in each dimension (details in Appendix A.1), we observe that the absolute values of the dimensions corresponding to undertrained frequencies are noticeably greater than zero, whereas those of adequately trained dimensions are close to zero. On the other side, the components that do not complete a full cycle during pre-training introduce weights with non-zero means when applied to q,k vectors. Thus, these components introduce positional bias (shown in Fig 7) and may adversely affect robustness to out-of-domain rotation matrix values during length generalization.\nTo verify this hypothesis, we normalized the q,k vectors (enforcing a mean of 0 and variance of 1) before applying the rotation matrix to eliminate the positional bias. Based on the results in Table 1, normalization on naive RoPE showed a positive impact on length generalization. However, if all frequency components are pre-trained to complete full"}, {"title": "6 Related Work", "content": "Frequency-Domain Embedding. Discrete Fourier Transform (DFT) (Oppenheim et al., 1982) has been widely used in various areas having periodic signals (Edfors et al., 2000; Sanchez, 2010). In machine learning, (Uteuliyeva et al., 2020; Lin et al., 2024b; Tancik et al., 2020; Tamkin et al., 2020; Lee-Thorp et al., 2022; Gillman et al., 2024) employed Fourier features into neural networks to enhance performance on NLP or CV tasks. S4 (Gu et al., 2021) also leveraged FFT and IFFT to shift its core computation into the frequency-domain, delivering more efficient computation. (Wang et al., 2019; Su et al., 2024) improved the attention mechanism by defining position embedding with complex number, while \"phase\" used in these methods is a typical concept in frequency-domain.\nLength Generalization. Due to resource constraints, LMs are trained on limited-length corpus chunks and struggle with longer contexts (Voita et al., 2023; Dong et al., 2024; Hong et al., 2024). While absolute position embeddings (Vaswani, 2017) restrict the general use of positional information, methods as (Shaw et al., 2018; Yang, 2019) directly adjust the attention mechanism, another intuitive method is to redesign the position embedding (Press et al., 2021; Chi et al., 2022; Kazemnejad et al., 2024; Su et al., 2024; Wang et al., 2024; Choromanski et al., 2024). Among these, ROPE (Su et al., 2024) encodes positional information using the phase of complex numbers, leveraging their periodicity to enhance access to long-distance dependencies. Several training-free or fine-tuning-based methods can also improve the LM's length generalization by refining RoPE (Peng et al., 2023; Chen et al., 2024; Jin et al., 2024; Lin et al., 2024a). However, these works mainly address the drawbacks of ROPE in attention mechanism, neglecting the influence of other components in LMs."}, {"title": "7 Conclusion", "content": "In this paper, we analyze RoPE-based attention by modeling it in the frequency domain using Discrete Signal Processing (DSP) theory. Our analysis reveals that RoPE achieves periodic attention by implicitly performing Non-Uniform Discrete Fourier Transform (NUDFT), corrupted by the non-ideal spectrum properties brought by other parts in LMs. We propose Fourier Position Embedding (FoPE) to enhances attention's periodic extension and length generalization. FoPE models each dimension as Fourier Series and zero-out inadequately-trained frequency components. Experiments demonstrate that FoPE significantly improves length generalization compared to baselines across diverse tasks and datasets. Our ablation studies and visualizations provide further support for our method and theoretical modeling."}, {"title": "8 Limitations", "content": "Our DSP-based modeling in the frequency domain provides a novel perspective for LMs to enhance length generalization and explore broader applications. These include aligning frequency-domain representations for better model collaboration, optimizing kv-cache compression via spectral analysis, and improving semantic communication with learnable frequency-domain embeddings.\nHowever, as our work focuses on modeling length generalization into the frequency domain, we only pay attention to the undesirable properties hindering this objective. Extending the applicability of this modeling to areas such as kv-cache compression, model collaboration, and semantic communication may require additional effort. A more generalized definition and analysis is left for future work."}, {"title": "A More Experimental Results", "content": ""}, {"title": "A.1 Visualization of q,k vectors before applying RoPE", "content": "We conduct visualization experiments using the Llama2-7B model, which features an attention module with 32 heads, each comprising 128 dimensions, and a pretraining sequence length of 4096 tokens. The number of cycles sampled by each sinusoidal function during pretraining is calculated as $r_i = \\frac{0}{2\u03c0} L_{train}$, where i denotes the dimension index. Based on this, we determined that the dimensions corresponding to incomplete cycles fall within the ranges [45, 64] U [109, 128].\nWe randomly sample 1000 tokens and compute the average activation values across heads for each dimension. The average activation values for every dimension of each layer are then plotted in Fig 6. The absolute activation values are significantly higher for dimensions corresponding to undertrained frequencies compared to others. This indicates that the RoPE rotation matrix pattern during pretraining has a notable impact on the distribution of q,k vector activations."}, {"title": "A.2 Influence of undertrained components in time-domain", "content": "We conduct visualization and ablations to further investigate the influence of undertrained components in RoPE.\nIn Fig 7, we visualize the time-domain pattern of RoPE. To get the \"Ideal ROPE's Contribution\", we suppose the Query and Key vectors are equal to 1 constantly (as in the original paper of ROPE (Su et al., 2024). To get the \"Actual ROPE's Contribution\", we suppose the Query and Key vectors obey the Gaussian distribution and sample 1000 times to get the mathematical expectation of the \"Actual ROPE's Contribution\". It can be seen that RoPE brings decay in attention score in its naive setting, which is brought by the undertrained components in RoPE. We hypothesis that this decay brings positional bias that may adversely affect robustness to out-of-domain rotation matrix values during length generalization.\nAs the positional bias can be eased if the mean of Query and Key vectors is set to zero, we conduct further ablation studies to normalize the Query and Key vectors before attention. Based on the results in 1, only if the position embedding contains components that cannot complete full cycle, the normalization brings better length generalization. Thus, these components are partially proved to deliver negative affect by positional bias. This experiment also demonstrate that the positional decay does not have beneficial influence on length generalization."}, {"title": "B Implementation Details", "content": "Training Settings. Our main experiments are conducted with 4 cards NVIDIA A6000 (maximum GPU memory=48GB), the pre-training of 60M/180M/1.2B in 20B tokens lasts for 10/20/100 hours, respectively. The time-consuming has a linear relation with the number of tokens in other settings. For all model scales and experimental settings, we select 6e-4 as the learning rate and warmup for 10000 steps with cosine scheduler. While the mini-batchsize on each device is different for each model, we accumulate gradients until the global batchsize reaches 1024 in all experiments.\nEvaluation Settings. For pre-training, evaluations are conducted on the checkpoint from the last step. For fine-tuning, we save checkpoints every 100 steps and report the best result for each method. This is partly due to YARN (Peng et al., 2023) being prone to overfitting with excessive fine-tuning steps, a limitation not observed in FoPE.\nPseudo-code of FoPE is shown in the final page."}, {"title": "C Theoretical Details", "content": ""}, {"title": "C.1 Derivation of Lemma 1", "content": "Given a non-linear function g(x), it can be rewritten as a power series by Taylor expansion:\n$$g(x) = \\sum_{p \\in N} a_p x^p$$"}, {"title": "C.2 Derivation of Equal (10)", "content": "Before we begin the derivation, let's familiarize with two functions in time domain: Rectangular Pulse Function and Single-Frequency Function.\nGiven a Rectangular Pulse function:\n$$r(t) = \\begin{cases} 1, |t| \\le T \\\\ 0, |t| > T \\end{cases}$$\nwhose Fourier Transform is a Sa Function:\n$$R(\u03c9) = \\int_{-T}^{T} e^{-jwt} dt = 2 \\frac{sin(\u03c9T)}{\u03c9}$$\nGiven a Single-Frequency function:\n$$f(t) = e^{iw_m t}$$\nwhose Fourier Transform is:\n$$F(\u03c9) = \\int_{-\u221e}^{+\u221e} e^{jw_m t} e^{jwt} dt$$\nwhich is equal to an Impulse Function in frequency domain:\n$$\u03b4(\u03c9_m) = \\begin{cases} 1, \u03c9 = \u03c9_m \\\\ 0, \u03c9 \u2260 \u03c9_m \\end{cases}$$\nThen, Eq (10) can be easily derivated, as the context x truncated with length T can be seen as:\n$$x(t) = f(t) \\cdot r(t)$$\nIts Fourier Transform in discrete case is Eq. (10)."}]}