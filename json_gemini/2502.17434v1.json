{"title": "V-HOP: Visuo-Haptic 6D Object Pose Tracking", "authors": ["Hongyu Li", "Mingxi Jia", "Tuluhan Akbulut", "Yu Xiang", "George Konidaris", "Srinath Sridhar"], "abstract": "Humans naturally integrate vision and haptics for robust object perception during manipulation. The loss of either modality significantly degrades performance. Inspired by this multisensory integration, prior object pose estimation research has attempted to combine visual and haptic/tactile feedback. Although these works demonstrate improvements in controlled environments or synthetic datasets, they often underperform vision-only approaches in real-world settings due to poor generalization across diverse grippers, sensor layouts, or sim-to-real environments. Furthermore, they typically estimate the object pose for each frame independently, resulting in less coherent tracking over sequences in real-world deployments. To address these limitations, we introduce a novel unified haptic representation that effectively handles multiple gripper embodiments. Building on this representation, we introduce a new visuo-haptic transformer-based object pose tracker that seamlessly integrates visual and haptic input. We validate our framework in our dataset and the Feelsight dataset, demonstrating significant performance improvement on challenging sequences. Notably, our method achieves superior generalization and robustness across novel embodiments, objects, and sensor types (both taxel-based and vision-based tactile sensors). In real-world experiments, we demonstrate that our approach outperforms state-of-the-art visual trackers by a large margin. We further show that we can achieve precise manipulation tasks by incorporating our real-time object tracking result into motion plans, underscoring the advantages of visuo-haptic perception. Our model and dataset will be made open source upon acceptance of the paper. Project website https://lhy.xyz/projects/v-hop/", "sections": [{"title": "1. Introduction", "content": "Accurately tracking object poses is a core capability for robotic manipulation, and would enable contact-rich and dexterous manipulations with efficient imitation or reinforcement learning (Wen et al., 2022; Li et al., 2024a; Hsu et al., 2024). Recent state-of-the-art object pose estimation methods, such as FoundationPose (Wen et al., 2024), have significantly advanced visual tracking by leveraging large-scale datasets. However, relying solely on visual information to perceive objects can be challenging, particularly in contact-rich or in-hand manipulation scenarios involving high occlusion and rapid dynamics.\nThe cognitive science findings show that humans naturally integrate visual and haptic information for robust object perception during manipulation (Navarro-Guerrero et al., 2023; Ernst and Banks, 2002; Lacey and Sathian, 2020). For instance, Gordon et al. (1991) demonstrated that humans use vision to hypothesize object properties and haptics to refine precision grasps. The human \u201csense of touch\u201d consists of two distinct senses (Loomis and Lederman, 1986; Dahiya et al., 2010): the cutaneous sense, which detects stimulation on the skin surface, and kinesthesis, which provides information on static and dynamic body posture. This integration, known as haptic perception, allows humans to effectively perceive and manipulate objects (Lacey and Sathian, 2020). In robotics, analogous capabilities are achieved through tactile sensors (cutaneous sense) and joint sensors (kinesthesis) (Navarro-Guerrero et al., 2023)."}, {"title": "2. Background", "content": "In this section, we first define the problem formally and then review existing haptic representations and our proposed unified representation.\n2.1. Problem Definition\nWe tackle the model-based visuo-haptic 6D object pose tracking problem, assuming access to:\n\u2022 Visual observations: An RGB-D sensor observes the object in the environment.\n\u2022 Haptic feedback: The object is manipulated by a rigid gripper equipped with tactile sensors.\nOur approach takes the following as input:\n1. The CAD model \\(M_o\\) of the object.\n2. A sequence of RGB-D images \\(O = {o_i}_{i=1}^n\\), where each observation \\(O_i = [I_i, D_i]\\) includes an RGB image \\(I_i\\) and a depth map \\(D_i\\).\n3. An initial 6D pose \\(T_0 = (R_0, t_0) \\in SE(3)\\), where \\(R_0 \\in SO(3)\\) is 3D rotation and \\(t_0 \\in \\mathbb{R}^3\\) is 3D translation.\nIn practice, the ground-truth initial pose \\(T_0\\) is hard to obtain and can only be estimated through pose estimation (Xiang et al., 2018; Wang et al., 2019a;"}, {"title": "2.2. Haptic Representation", "content": "The effectiveness of haptic learning hinges on its representation. Prior approaches using raw value (Lin et al., 2024b), image (Guzey et al., 2023), or graph-based (Yang et al., 2023; Li et al., 2024b; Rezazadeh et al., 2023) representations often struggle to generalize across diverse embodiments. For instance, Wu et al. (2022) and Guzey et al. (2023) project tactile signals from Xela sensors into a 2D image format. While this allows efficient processing with existing visual models, extending the method to different grippers or sensor layouts proves challenging. Similarly, Li et al. (2024b) and Rezazadeh et al. (2023) employ graph-based mappings, where taxels are represented as vertices. However, variations in sensor layouts result in different graph distributions, creating significant generalization gaps.\nIn contrast, we adopt a point cloud representation, which naturally encode 3D positions and can flexibly accommodate multi-embodiments. We broadly classify tactile sensors into taxel-based and vision-based. A more comprehensive review on tactile sensors can be found at (Yamaguchi and Atkeson, 2019). Below, we outline how they are converted into point clouds in prior works (Dikhale et al., 2022; Suresh et al., 2024; Watkins-Valls et al., 2019; Falco et al., 2017), paving the way for our unified framework.\nTaxel-based Sensors. The tactile data is defined as \\(S = {s_i}_{i=1}^{n_1}\\), which encapsulate \\(n_1\\) taxels. \\(s_i\\) represents individual taxels. The tactile data consists of \\(S = (S_p, S_r)\\):\n\u2022 Positions (\\(S_p\\)): Defined in the gripper frame and transformed into the camera frame using forward kinematics.\n\u2022 Readings (\\(S_r\\)): Capturing contact values. Readings are commonly binarized into contact or no-contact states (Yin et al., 2023; Xue et al., 2023; Li et al., 2023; Dikhale et al., 2022) based on a threshold \\(\\tau\\).\nThe set of taxels in contact is:\n\\[S_c = {s_i \\in S | S_r(s_i) > \\tau},\\] (1)\nand the corresponding tactile point cloud \\(S_{p,c}\\) is defined as\n\\[S_{p,c} = {S_p(s_i) | s_i \\in S_c}.\\] (2)\nVision-based sensors. For vision-based tactile sensors (Lambeta et al., 2020; Yuan et al., 2017; Donlon et al., 2018; Taylor et al., 2022), the tactile data includes \\(S = (S_p, S_I)\\):\n\u2022\n\u2022 Positions (\\(S_p\\)): Sensor positions in the camera frame, similar to taxel-based.\nImages (\\(S_I\\)): Capturing contact states using regular RGB image representation. Using the tactile depth estimation model (Bauza et al., 2019; Suresh et al., 2024; Kuppuswamy et al., 2020; Suresh et al., 2023; 2022; Ambrus et al., 2021), we can convert \\(S_I\\) into tactile point cloud \\(S_{p,c}\\).\nYet we are not the first to employ point cloud representations for tactile learning, prior works (Dikhale et al., 2022; Suresh et al., 2024; Watkins-Valls et al., 2019; Falco et al., 2017) focus on a single type of sensor and overlook the hand posture. Our key contribution is a unified representation spanning both taxel-based and vision-based sensors on multi-embodiments, empowered by our multi-embodied dataset. We demonstrate generalizability on the Barrett hand (taxel-based) during our real-world experiments and on the Allegro hand (vision-based DIGIT sensor) using the Feelsight dataset (Suresh et al., 2024). Our novel haptic representation seamlessly integrates the tactile signals with the hand posture, enabling more effective hand-object interaction reasoning. In subsequent sections, we describe our approach and provide empirical evidence demonstrating"}, {"title": "3. Methodology", "content": "We propose V-HOP, a data-driven approach that fuses visual and haptic modalities to achieve accurate 6D object pose tracking. Our goal is to build a generalizable visuo-haptic pose tracker that accommodates diverse embodiments and objects. We first outline the core representations used in our haptic modality: hand and object representations. Our choice for the representations follows the spirit of the render-and-compare paradigm (Li et al., 2018). Later, we introduce our visuo-haptic model and how it is trained. We use the terms \u201chand\" and \"gripper\" interchangeably.\n3.1. Hand Representation\nTactile signals only represent the cutaneous stimulation, while haptic sensing combines tactile and kinesthetic feedback to provide a more comprehensive spatial understanding of contact and manipulation. We propose a novel haptic representation that integrates tactile signals and hand posture in a unified point cloud representation. This hand-centric representation enables efficient reasoning about spatial contact and hand-object interaction.\nUsing the URDF definition and joint positions \\(j\\), we generate the hand mesh \\(M_h\\) through forward kinematics and calculate the surface normals. The mesh is then downsampled to produce a 9-D hand point cloud \\(P_h = {p_i}_{i=1}^n\\):\n\\[p_i = (x_i, y_i, z_i, n_{ix}, n_{iy}, n_{iz}, c) \\in \\mathbb{R}^9,\\] (3)\nwhere \\(x_i, y_i, z_i\\) represent the 3-D coordinate of the point. \\(n_{ix}, n_{iy}, n_{iz}\\) represent the 3-D normal vectors, and \\(c \\in \\mathbb{R}^3\\) is a one-hot encoded point label:\n\u2022 [1,0,0]: Hand point in contact.\n\u2022 [0, 1, 0]: Hand point not in contact.\n\u2022 [0,0,1]: Object point (for later integration with the object point cloud).\nTo obtain the contact state of each point, we map the tactile point cloud \\(S_{p,c}\\), representing the contact points detected by the tactile sensors (Sec. 2.2), onto the downsampled hand point cloud \\(P_h\\). Specifically, for each point in \\(S_{p,c}\\), we find its neighboring points in \\(P_h\\) within a radius \\(r\\). These neighboring points are labeled as \"in contact\", while all others are labeled as \"not in contact\". The choice of the radius \\(r\\) is randomized during training and determined by the measured effective radius of each taxel during robot deployment. The resulting haptic point cloud, \\(P_h\\), serves as a unified representation for both tactile and kinesthetic data (Fig. 2)."}, {"title": "3.2. Object Representation", "content": "We denote the object model point cloud as \\(P_o = {q_i}_{i=1}^n\\). Similar to the hand point cloud, \\(q_i\\) follows the same 9-D definitions (Equation 3),\n\\[q_i = (x_i, y_i, z_i, n_{ix}, n_{iy}, n_{iz}, c) \\in \\mathbb{R}^9,\\]\nwith \\(c = [0,0,1]\\) for all object points. At each timestep \\(i > 0\\), we transform the model point cloud into a hypothesized point cloud \\(P'_o = {q'_i}_{i=1}^n\\) according to the pose from the previous timestep \\(T_{i-1}\\). For each point \\(q\\) in the hypothesized point cloud \\(P'_o\n\\[q'_i = (x'_i, y'_i, z'_i, n'_{ix}, n'_{iy}, n'_{iz}, c),\\] (4)\nwhere:\n\\[\\begin{bmatrix} x'_i \\\\ y'_i \\\\ z'_i \\end{bmatrix} = R_{i-1} \\begin{bmatrix} x_i \\\\ y_i \\\\ z_i \\end{bmatrix} + t_{i-1}, \\; \\begin{bmatrix} n'_{ix} \\\\ n'_{iy} \\\\ n'_{iz} \\end{bmatrix} = R_{i-1} \\begin{bmatrix} n_{ix} \\\\ n_{iy} \\\\ n_{iz} \\end{bmatrix}\\] (5)\nTo enable reasoning about hand-object interactions, we fuse the hand point cloud \\(P_h\\) and the hypothesized object point cloud \\(P'_o\\) to create a hand-object point cloud \\(P\\),\n\\[P = P_h \\cup P'_o .\\] (6)\nThis novel unified representation adopts the principles of the render-and-compare paradigm from visual approaches (Li et al., 2018; Wen et al., 2020a; Labb\u00e9 et al., 2022; Wen et al., 2024; Tremblay et al., 2023), in which the rendered image (based on pose hypothesis) is compared against the visual observation. The hypothesized object point cloud \\(P'_o\\) serves as the \"rendered\" pose hypothesis (Fig. 2). The hand point cloud \\(P_h\\) represents the real observation using haptic feedback, which we used to compare with. By leveraging this representation, the model captures the contact-rich interactions between the hand and the object by learning feasible object poses informed by haptic feedback."}, {"title": "3.3. Network Design", "content": "Visual modality. Unlike prior works, which train the whole visuo-haptic network from scratch, our approach can effectively leverage the pretrained visual"}, {"title": "3.4. Training Paradigm", "content": "We train our model by adding noise \\((R_e, t_e)\\) to the ground-truth pose \\(T = (R, t)\\) to create the hypothesis pose \\(\\tilde{T} = (\\tilde{R}, \\tilde{t})\\):\n\\[\\tilde{R} = R_e R, \\; \\tilde{t} = -R_e t_e + t.\\] (7)"}, {"title": "4. Experiments", "content": "4.1. Multi-embodied Dataset\nExisting visuo-haptic datasets were not publicly available (Dikhale et al., 2022; Li et al., 2024b; Wan et al., 2024) at the time of completing this work and focused on a single gripper (Suresh et al., 2024), leaving the question of generalization to novel embodiments unanswered. Consequently, we develop a multi-embodied dataset (Fig. 3) using NVIDIA Isaac Sim to enable cross-embodiment learning and thorough evaluation. Our dataset comprises approximately 1,550,000 images collected across eight grippers and thirteen objects. We utilize 85% of the data for training and the rest for validation. The camera trajectories are sampled on the semi-sphere around the gripper, which has a random radius between 0.5 and 2.5 meters. We selected graspable YCB object (Calli et al., 2015) and grippers used in prior works (Ding"}, {"title": "4.2. Pose Tracking Comparison", "content": "In the following experiments, we evaluate performance using the metrics:\n\u2022 Area under the curve (AUC) of ADD and ADD-S (Hinterstoisser et al., 2013; Xiang et al., 2018), and\n\u2022 ADD(-S)-0.1d (He et al., 2022): ADD/ADD-S that is less than 10% of the object diameter.\nWe compare V-HOP against the current state-of-the-art approaches in visual pose tracking (FoundationPose (Wen et al., 2024), or FP in short) and visuo-tactile pose estimation (ViTa (Dikhale et al., 2022)). To ensure a fair comparison, we finetune FoundationPose and train ViTa on our multi-embodied dataset. To verify the generalizability of the novel object and novel gripper, we exclude one object (pudding_box) and one gripper (D'Claw) during training.\nDue to the absence of a visuo-haptic pose tracking approach, we compare V-HOP with ViTa, an instance-"}, {"title": "4.3. Ablation on Modalities", "content": "We conduct an ablation study on the input modalities to evaluate the effectiveness of the haptic representation. Specifically, we train two ablated versions of V-HOP: one without tactile feedback and another without visual input, as shown in Tab. 3. To exclude tactile input, we remove all \u201cin contact\" point labels (Equation 3). Our results indicate that visual input significantly contributes to performance, likely due to the richness of visual information, including texture and spatial details. This finding aligns with previous studies on human perception systems, which suggest that vision plays a dominant role in visuo-haptic integration (Kassuba et al., 2013). Similarly, tactile feedback is crucial; without it, performance degrades notably because reasoning about hand-object contact during interactions becomes more difficult."}, {"title": "4.4. Occlusion's Effect on the Performance", "content": "We evaluate the performance of V-HOP and FoundationPose across varying occlusion ratios (Fig. 4). The occlusion ratio is defined as the proportion of pixels in the segmentation mask relative to the total pixels in the rendered object image, generated using the ground-truth pose. Our results show that V-HOP consistently outperforms FoundationPose in both ADD and ADD-S metrics under different levels of occlusion. These results underscore the importance of integrating visual and haptic information to improve performance in challenging occlusion scenarios."}, {"title": "4.5. Pose Tracking on FeelSight", "content": "To evaluate the generalizability of V-HOP, we benchmark it against NeuralFeels (Suresh et al., 2024), a recently introduced optimization-based visuo-tactile pose tracking approach, using their proposed Feelsight dataset. Specifically, we focus on the occlusion subset of the dataset, FeelSight-Occlusion, which presents significant challenges due to severe occlusions. This subset requires robust generalization capabilities as it includes a novel embodiment (the Allegro hand equipped with DIGIT fingertips), a novel sensor type (a vision-based tactile sensor), and a novel object (a Rubik's cube). For a fair comparison, we compare against their model-based tracking approach, which uses almost the same inputs as V-HOP but with the ground-truth segmentation mask (GT Seg).\nThe results are presented in Tab. 4. V-HOP achieves a 32% lower ADD-S error compared to NeuralFeels and has a similar ADD-S-0.1d score. It is important to note that NeuralFeels leverages the ground-truth segmentation mask, which helps in more accurate object localization, whereas V-HOP does not have such an input, further underscoring its robustness and adaptability.\nIn terms of computational efficiency, V-HOP is approximately 10 times faster than NeuralFeels, achieving 32 FPS compared to NeuralFeels' 3 FPS on an NVIDIA RTX 4070 GPU. This substantial improvement in speed highlights the practicality of V-HOP for real-world manipulation applications, as we will demonstrate in the later sections."}, {"title": "5. Sim-to-Real Transfer Experiments", "content": "To validate the real-world effectiveness of our approach, we perform sim-to-real experiments using our robot platform (Fig. 1). Our bimanual platform comprises dual Franka Research 3 robotic arms (Haddadin et al., 2022) and Barrett Hands BH8-282. Our Barrett Hand has 4 degrees of freedom (DoF) and 96 taxels: 24 taxels on each fingertip and 24 taxels on the"}, {"title": "5.1. Pose Tracking Experiments", "content": "In this experiment (Fig. 5), the gripper stably grasps the object while a human operator guides the robot arm along a random trajectory. This introduces heavy occlusion and high dynamic motion to emulate challenging real-world manipulation scenarios. Under these conditions, FoundationPose often loses tracking due to reliance on visual input alone. In contrast, V-HOP maintains stable object tracking throughout the trajectory, demonstrating the robustness of its visuo-haptic sensing."}, {"title": "5.2. Bimanual Handover Experiment", "content": "In this experiment (Fig. 6), an object is placed on a table within reach of the robot's right arm. The task requires the robot to perform the following sequence of actions:\n1. Use the right arm to grasp the object and transport it to the center.\n2. Use the left arm to grasp the object from the right hand and place it into a designated bin.\nThe robot employs model-based grasping, which depends on real-time object pose estimation. This task presents two key challenges:\n1. If the grasp attempt fails, the robot must detect the failure based on the real-time object pose and reattempt the grasp.\n2. During transport to the center, the robot must maintain precise tracking of the object's pose to ensure that the left arm can accurately grasp it. Inaccurate tracking results could lead to collision during the handover."}, {"title": "5.3. Can-in-Mug Experiment", "content": "The Can-in-Mug task (Fig. 8) involves grasping a tomato can and inserting it into a mug. The bimanual version requires the robot to also grasp the mug and insert the can in the center. Successful execution hinges on precise pose estimation for both objects, as any noise in their poses can lead to failure. Our results (Tab. 6) demonstrate that V-HOP, by integrating visual and haptic inputs, delivers more stable tracking and a higher overall success rate."}, {"title": "5.4. Contribution of each modality", "content": "In this study, we examine the contribution of visual and haptic inputs to the final prediction. We adapt Grad-CAM (Selvaraju et al., 2020), utilizing the final normalization layer of the Transformer encoder as the target layer. Figure 9 illustrates the weight distribution across the visual and haptic modalities."}, {"title": "6. Related Works", "content": "In this work, we consider the problem of 6D object pose tracking problem, which has been widely studied as a visual problem (Wen et al., 2020a; Li et al., 2018; Wen et al., 2024; Deng et al., 2021). In particular, we focus on model-based tracking approaches, which assume access to the object's CAD model. While model-free approaches (Wen and Bekris, 2021; Wen et al., 2023; Suresh et al., 2024) exist, they fall outside the scope of this work. Visual pose tracking has achieved significant progress on established benchmarks, such as BOP (Hodan et al., 2024). Despite these successes, deploying such systems in real-world robotic applications remains challenging, especially under scenarios with high occlusion and dynamic interactions, such as in-hand manipulation tasks.\nTo address these challenges, prior research has explored combining visual and tactile information to improve pose tracking robustness (Li et al., 2023; Suresh et al., 2024; Dikhale et al., 2022; Wan et al., 2024; Rezazadeh et al., 2023; Tu et al., 2023; Gao et al., 2023; Li et al., 2024b). These approaches leverage learning-based techniques to estimate object poses by fusing visuo-tactile inputs. However, these methods estimate poses on a per-frame basis, which lacks temporal coherence. Additionally, cross-embodiment and domain generalization remain significant hurdles, limiting their scalability and practicality for broad deployment.\nMore recent works aim to overcome some of these limitations. For example, Liu et al. (2024) proposes an optimization-based approach that integrates tactile data with visual pose tracking using an ad-hoc slippage detector and velocity predictor. Suresh et al. (2024) extend the model-free tracking frameworks BundleTrack (Wen and Bekris, 2021) and BundleSDF (Wen et al., 2023) by combining visual and tactile point clouds within a pose graph optimization framework. However, these approaches are only validated on a single embodiment and suffer from computational inefficiencies (Suresh et al., 2024), which present challenges for real-time deployment in dynamic manipulation tasks."}, {"title": "7. Limitation", "content": "We follow the model-based object pose tracking setting, which assumes that a CAD model is available for the object. One potential direction to overcome this limitation is to simultaneously reconstruct the object and perform pose tracking, as demonstrated in methods like BundleSDF (Wen et al., 2023)."}, {"title": "8. Conclusion", "content": "We introduced V-HOP, a visuo-haptic 6D object pose tracker that integrates a unified haptic representation and a visuo-haptic transformer. Our experiments demonstrate that V-HOP generalizes effectively to novel sensor types, embodiments, and objects, outperforming state-of-the-art visual and visuo-tactile approaches. Ablation studies highlight the critical role of both visual and haptic modalities in the framework. In the sim-to-real transfer experiments, V-HOP proved robust, delivering stable tracking under high occlusion and dynamic conditions. Furthermore, integrating V-HOP's real-time pose tracking into motion planning enabled accurate manipulation tasks, such as bimanual handover and insertion, showcasing its practical effectiveness."}]}