{"title": "Online Test Synthesis From Requirements: Enhancing Reinforcement Learning with Game Theory", "authors": ["Ocan Sankur", "Thierry J\u00e9ron", "Nicolas Markey", "David Mentr\u00e9", "Reiya Noguchi"], "abstract": "We consider the automatic online synthesis of black-box test cases from functional requirements specified as automata for reactive implementations. The goal of the tester is to reach some given state, so as to satisfy a coverage criterion, while monitoring the violation of the requirements. We develop an approach based on Monte Carlo Tree Search, which is a classical technique in reinforcement learning for efficiently selecting promising inputs. Seeing the automata requirements as a game between the implementation and the tester, we develop a heuristic by biasing the search towards inputs that are promising in this game. We experimentally show that our heuristic accelerates the convergence of the Monte Carlo Tree Search algorithm, thus improving the performance of testing.", "sections": [{"title": "Introduction", "content": "Requirement engineering and testing are two important and related phases in the development process. Indeed, test cases are usually derived from functional specifications and documented with the requirements they are supposed to check. Several existing tools allow one to automatically generate test cases from formal functional specifications and/or formal requirements (see e.g., the tools T-VEC [BB96], TorXakis [TvdL19] or Stimulus [JG16], and surveys [UPL12] and [LLGS18]).This helps the development process since the developer can focus on writing formal specifications or requirements, and test cases are generated automatically. There are two main approaches for test generation: black-box testing focuses on generating tests without having access to the system's internals such as its source code (see e.g., [Bei95]), while white-box testing generates tests by analyzing its source code (see e.g., [MBTS04, AO17]).\nBlack-Box Testing. In this paper, we are interested in automatically synthesizing online test cases from a set of requirements and an implementation under test in a black-box setting. By black-box, we mean that the implementation internal is unknown to the tester (or at least not used), only its interaction with her is used. By online, we mean that the synthesis is essentially performed during execution, while interacting with the implementation. We consider implementations that are reactive: these are programs that alternate between reading an input valuation and writing an output valuation. This setting is interesting for modeling synchronous systems, e.g., controllers for manufacturing systems [Bol15]. The considered requirements are given as automata recognizing sequences of valuations of input and output variables. The conformance of an implementation to a set of requirements is formalized as the absence of input-output valuation sequences generated by the implementation that are rejected by the requirements automaton.\nThe goal of the test cases is to drive the implementation to some particular state or show some particular behaviour where non-conformance is suspected to occur. These are described by test objectives. They are typically derived from coverage criteria, e.g. state or transition coverage, or written from requirements [?]. The selection of the test objectives is out of the scope of this paper; we thus assume that these are given.\nThere are several black-box testing algorithms and tools in the literature. The closest to our approach is TorXakis [TvdL19, Tor] which is a tool based on the ioco testing theory [Tre96] and the previous TorX tool [TB03]. It allows the user to specify the automata-based requirements reading input-output valuations as well as test objectives (called test purposes) in a language based on process algebra, and is able to generate online tests interacting with a given implementation. These tests are performed by picking random input valuations, and observing the outputs from the implementation, while checking for non-conformance. Because tests are performed using random walks in this approach, deep traces satisfying the test objective or violating the requirements are hard to find in practice.\nReinforcement Learning for Testing. This issue has been addressed in many works by interpreting the test synthesis in a reinforcement learning(RL) setting [SB18]. Reinforcement learning is a set of techniques for computing strategies that optimize a given reward function based on interactions of an agent with its environment. It has been applied to learn strategies for playing board games such as Chess and Go [SHM+16]. Here the test synthesis is seen as a game between the tester and the implementation: the former player selects inputs, and the latter player selects outputs. Because the implementation is black-box, the tester is playing an unknown game, but can discover the game through interactions. Using a game approach for test synthesis has long been advocated [Yan04]; and online testing for interface automata specifications were considered in [VRC06] using RL.\nBecause reaching a test objective is a 0/1 problem (an execution either reaches the objective and has a reward 1, or does not reach it and has reward 0), RL algorithms are usually very slow in finding deep traces. The application of RL to black-box testing thus requires the use of reward shaping [NHR99] which consists in assigning intermediate rewards to steps before the objective is reached; these are used to guide the search to more promising input sequences and can empirically accelerate convergence. Reward shaping has been used for testing, e.g., in [KS21] where an RL algorithm (Q-learning) was used for testing GUI applications with respect to linear temporal logic (LTL) specifications; rewards were then computed based on transformations on the target LTL formula.\nContributions. Although reinforcement learning helps one to guide the search towards the test objective, these methods can still be slow in finding traces satisfying the test objective, especially when the number of input bits is high, and when the traces to be found are long.\nIn this work, we target improving the performance of black-box online test algorithms based on reinforcement learning. More precisely, we develop heuristics for a Monte Carlo Tree Search (MCTS) algorithm applied in this setting, based on a game-theoretic analysis of the requirement automaton, combined with an appropriate reward shaping scheme.\nMonte-Carlo Tree Search [Cou06] (see also e.g., the survey [BPW+12]) is a RL technique to search for good moves in games. It consists in exploring the available moves randomly, while estimating the average reward of each newly-explored move and updating the reward estimates of previously selected moves. More precisely, MCTS builds a weighted tree of possible plays of the game, while the decision of which branch to explore at each step is based on a random selection appropriately biased to select unexplored moves but also moves with high reward estimates. The tree policy is the policy used for exploring the branches of the constructed tree, while the roll-out policy is used to run a long execution to estimate the overall reward.\nOur main contribution is a heuristic for biasing both the tree policy and the roll-out policy in MCTS in order to reach test objectives faster, while maintaining convergence guarantees. The heuristic is based on a greedy test strategy computed as follows. We adopt the game-theoretic view and see the testing process as a game played on the requirement automaton state space. At any step, when the tester provides an input, we assume that the implementation can also answer with any output. This defines a zero-sum game: the tester has the objective of reaching the test objective, and the implementation has the objective of avoiding it. We first consider winning strategies in this game: if there is a strategy for the tester which prescribes inputs such that the test objective is reached no matter what the implementation outputs, then this strategy is guaranteed to reach the test objective. However, in general, there are no winning strategies from all states. In this case, we compute winning strategies for the tester to reach so-called cooperative states, from where some output that the implementation can provide reduces the distance to the test objective in the requirement automaton. This is an optimistic strategy: if the implementation \"cooperates\", that is, provides the right outputs at cooperative states, this guarantees the reachability of the test objective; but otherwise, no guarantee is given. The greedy strategy consists in selecting uniformly at random inputs that are part of a winning strategy if any, or allow the implementation to cooperate.\nOur variant of the MCTS algorithm uses the standard UCT algorithm [KS06] as a tree policy, but restricted to those inputs that are part of the greedy strategy at the first M visits at each node of the tree; after the M-th visit to a node, the UCT policy is applied to the set of all inputs. Moreover, the roll-out policies use \u03f5-greedy strategies, which consists in selecting inputs uniformly at random with probability \u03f5, and using the greedy strategy with probability 1-\u03f5, at each step. This corresponds to restricting the input space of the tree policy to only those that appear in the greedy strategy for a bounded number of steps: this helps the algorithm focus on most promising inputs rather than starting to explore randomly all input combinations. In practice, this helps to guide the search quickly towards the test objective, or to states that are nearby. The algorithm does eventually explore all inputs (after M steps at a node) but at each newly created node, it again starts trying the greedy inputs. We also use reward shaping based on the distance remaining to the test objective inspired by [CIK+19]: we give a state a high reward if its shortest path distance to the objective state in the requirement automaton is small.\nWe implemented the computation of greedy strategies, and its combination with MCTS. We present a small case study for which the combination of MCTS with greedy strategies allows one to reach the test objective, while plain MCTS or greedy strategies alone fail to find a solution.\nRelated Works. The notion of cooperative states have been used before in the setting of testing. These were used in [HJM18] in the context of offline test synthesis from timed automata, already inspired by a previous approach of test generation using games for transition systems in an untimed context [Ram98]. In [DLLN08a], in the context of timed systems, the authors rely on cooperative strategies to synthesize test cases when the cooperation of the system is required for winning. However, these yield incomplete testing methods (a reachable test objective is not guaranteed to be found), or completeness is obtained by making strong assumptions on the implementations; the novelty of our approach is to obtain a complete method by using these notions in reinforcement learning. A discussion on model-based testing techniques can be found in [VCG+08].\nSeveral test generation tools based on the ioco conformance relation for input-output labelled transition systems (IOLTS) have been developed. Roughly, an implementation ioco-conforms to its specification if after any of its observed behaviour that is also a specification behaviour, the implementation only produces outputs or quiescences that are also possible in the specification. The tools TGV [JJ05] and TESTOR [MMS18] generate off-line test cases from formal specifications in various languages with IOLTS semantics, driven by test purposes described by automata. The tools JTorx [Bel10] and TorXakis [Tor] are improvement of TorX [TB03] and allow to generate and execute online test cases derived from various specification languages. In the context of timed models, Uppaal-TRON [HLM+08] is an online test generation tool for timed automata based on the real-time extension rtioco of the ioco conformance relation.\n[MPRS11] uses Q-learning to produce tests for GUI applications but without a model for the specification: the objective is to reach a large number of visually different states. RL-based testing for GUI has attracted significant attention. [AKKB18] uses Q-learning with the aim of covering as many states as possible; see also [LPZ+23]. In [RLPS20], reinforcement learning is used to compute valid inputs for testing programs: these consist in producing inputs that satisfy the precondition of a program to be tested so that assertions can be checked. [THMT21] uses RL to learn short synchronizing sequences, where rewards correspond to the size of the powerset of states. In [PZAdS20], reinforcement learning was used to test shared memory programs. MCTS has been used for testing in various settings. In [ABCS20], it is used for testing video games using rewards to cover different areas in the game, but without automata specifications. Deep reinforcement learning has also been used for Android testing; see e.g. [RMCT21]. [FCP23] combines blackbox testing and model learning in order to improve coverage.\nReward shaping for automata-based specifications has been considered for Monte Carlo Tree Search. In [CIK+19], the approach is based on the distance to accepting states of B\u00fcchi automata; and in [VBB+21], the authors collect statistics on the success for all transitions on the specification automaton. The latter approach is not adapted to our case, where the goal is to find a single successful execution, and not to actually learn the optimal values at all states."}, {"title": "Preliminaries", "content": "We first introduce traces that represent observable behaviours of reactive systems, then the automata models that recognize such traces and are used to formally specify requirements of such systems, together with related automata based notions.\nTraces. We fix a set of atomic propositions AP, partitioned into APin and Apout, that represent Boolean input- and output variables of the system. A valuation of AP is an element v of 2AP determining the set of atomic propositions which are true (or equivalently, it is a mapping v: AP \u2192 {\u22a4, \u22a5}). We denote by vin (respectively vout) the projections of v on Apin (resp. Apout) such that v = vin \u2295 vout. We write B(AP) for the set of Boolean combinations of atomic propositions in AP. That a valuation v satisfies a formula \u03c6 \u2208 B(AP), denoted by \u03bd \u22a7 \u03c6, is defined in the usual way.\nWe consider reactive systems that work as a succession of atomic steps: at each step, the environment first sets an input valuation vin, then the system immediately sets an output valuation vout. The valuation observed at this step is thus v = vin \u2295 vout. A trace of the system is a sequence \u03c3 = v1 \u00b7 v2 \u00b7\u00b7\u00b7 vn of input and output valuations. Internal variables may be used by the system to compute outputs from the inputs and the internal state, but these are not observable to the outside.\nAutomata. We use automata to express requirements, and as models for the implementations under test. When considered as requirements, they monitor the system through the observation of the values of the Boolean variables. Transitions of automata are guarded with Boolean constraints on AP that need to be satisfied for the automaton to take that transition. For convenience, we handle input- and output valuations separately. Formally,\nDefinition 2.0.1. An automaton is a tuple A = \u27e8S = Sin \u2295 Sout, Sinit, AP, T, F\u27e9 where S is a finite set of states, Sinit \u2208 Sin is the initial state, T \u2286 (Sin \u00d7 B(APin) \u00d7 Sout) \u222a (Sout \u00d7 B(APout) \u00d7 Sin) is a finite set of transitions, and F \u2286 S is the set of accepting states.\nFor two states sin and s'in and a valuation v, we write sin \u2192v s'in when there exist a state sout and transitions (sin, \u03c6in, sout) and (sout, \u03c6out, s'in) in T such that vin \u22a7 \u03c6in and vout \u22a7 \u03c6out.\nFor a trace \u03c3 = v1 \u00b7 v2 \u00b7\u00b7\u00b7 vn in (2AP)\u2217, we write sin \u2192\u03c3 s'in if there are states s1, s2 ... sn such that s0 = sin, sn = s'in, and for all i \u2208 [1, n], si\u22121 \u2192vi si. A trace \u03c3 \u2208 (2AP)\u2217 is accepted by A if Sinit \u2192\u03c3 s for some s \u2208 F. We denote by Tr(A) the set of accepted traces.\nAn automaton is input-complete if from any (reachable) state sin and any valuation vin \u2208 2APin, there is a transition (sin, \u03c6in, s'out) in T such that vin \u22a7 \u03c6in. It is output-complete if a similar requirement holds for states in Sout and valuations in vout, and it is complete if it is both input- and output-complete. An automaton is deterministic when, for any two transitions (s, g1, s1) and (s, g2, s2) issued from a same source s, if g1 \u2227 g2 is satisfiable, then s1 = s2.\nIn the rest of the paper, we will be mainly interested in states of Sin, since states in Sout are intermediary states that help us distinguish input and output valuations. For a state sin and a valuation v, we let PostA(sin, v) denote the set of states s'in such that sin \u2192v s'in, and PreA(sin, v) denote the set of states s'in such that s'in \u2192v sin. Notice that for deterministic complete automata, PostA(sin, v) is a singleton.\nThe set of immediate predecessors PreA(B) of a set B \u2286 Sin, and the set of its immediate successors PostA(B) are defined respectively as\nPreA(B) = \u22c3sin\u2208B,v\u22082AP PreA(sin, v),\nPostA(B) = \u22c3sin\u2208B,v\u22082AP PostA(sin, v).\nFrom these sets, one can define the set of states from which B is reachable (i.e., that are coreachable from B), as Pre\u2217(B) = lfp(\u03bbX.(B \u222a PreA(X))), and the set of states that are reachable from B, Post\u2217(B) = lfp(\u03bbX.(B \u222a PostA(X))), where lfp denotes the least-fixpoint operator. The fixpoint defining Pre\u2217(B) is equivalent to\nB \u222a PreA(B) \u222a PreA(B \u222a PreA(B))\n\u222aPreA(B \u222a PreA(B) \u222a PreA(B \u222a PreA(B)))\n\u222a...\nThis correspond to an iterative computation of a sequence (Vi)i\u2208\u2115, starting from V0 = \u2205 (which is why we get the least fixpoint) and such that Vi+1 = B \u222a PreA(Vi). Observe, by induction on i, that from any state s in Vi, there is a path to B within i steps. The sequence (Vi)i\u2208\u2115 is non-decreasing, and its limit is the set of all states from which B can be reached: from each state s \u2208 Pre\u2217(B), there is a finite trace \u03c3 such that by reading \u03c3 from s, one ends in B. Similarly, for each state s' \u2208 Post\u2217(B), there exists a state s \u2208 B and a trace \u03c3 such that by reading \u03c3 from s, one ends at s'.\nSafety automata form a subclass of automata having a distinguished set Error \u2286 Sin of error states that are non-accepting and absorbing (i.e., no transitions leave Error), and complement the set F of accepting states in S (i.e., F = S \\ Error). Those automata describe safety properties: nothing bad happened as long as Error is not reached."}, {"title": "The product of automata is defined as follows:", "content": "Definition 2.0.2. Given automata A1 = \u27e8Sin1 \u2295 Sout1, Sinit,1, AP1, T1, F1\u27e9 and A2 = \u27e8Sin2 \u2295 Sout2, Sinit,2, AP2, T2, F2\u27e9, their product A1 \u2297 A2 is an automaton A = \u27e8S, Sinit, AP, T, F\u27e9 where S = (Sin1 \u00d7 Sin2) \u222a (Sout1 \u00d7 Sout2), Sinit = (Sinit,1, Sinit,2), AP = AP1 \u222a AP2, F = F1 \u00d7 F2 and the set of transitions is defined as follows: there is a transition ((s1, s2), \u03c6, (s'1, s'2)) in T if there are transitions (s1, \u03c61, s'1) in T1 and (s2, \u03c62, s'2) in T2 with \u03c6 = \u03c61 \u2227 \u03c62.\nNotice that this definition indeed yields an automaton in the sense of Def. 2.0.1; and that completeness and determinism are preserved by the product. Moreover, the product of two safety automata is a safety automaton: the set of accepting states is F = F1 \u00d7 F2, so the set Error in the product automaton is (Error1 \u00d7 Sin) \u222a (Sin \u00d7 Error2), and thus inherits absorbance. The product of automata is commutative and associative, and can thus be generalized to an arbitrary number of automata.\nWe now consider an example of an automaton that will be used to illustrate other notions we define later in the paper.\nExample 2.0.1. Figure 1 displays an example of an automaton. For the sake of readability, we use input- and output letters instead of atomic propositions. Here, {a,b} are input letters, and {0,1} are output letters. This automaton is deterministic; moreover, letting t be an Error state makes it a safety automaton. It could be made complete by adding looping input-output transitions (similar to the transitions to the bottom left of s0) on t and o.\nThe next example shows how an automaton can be obtained from requirements written for a simple factory automation system.\nExample 2.0.2 (Factory Carriage Example). We consider a controller program in a factory automation system depicted in Fig. 2. In this system, when the carriage is on the right end (bwdlimit) and receives a cargo on top of it, the controller program must move the carriage forward (movefwd) until it reaches the forward limit (fwdlimit). The controller must then push the arm for 3 seconds, and it can only back the carriage up (movebwd) after this duration.\nFor the sake of this example, we only model the requirements concerning the first phase, that is, until the carriage reaches the forward limit. Three of these requirements on the controller program are given below.\nR1 When the carriage is on its backward limit, and a cargo is present, then it immediately moves forwards until reaching the forward limit.\nR2 If the carriage is not already moving forward, and if no cargo is present or the carriage is not in the backward limit, then it is an error to move forward. The carriage must never be moved forward and backwards at the same time.\nR3 When some cargo is present, and the carriage is at its forward limit, it should stop moving forward immediately.\nAll these three requirements are modelled in the automaton of Fig. 3. The initial state is s0, and we distinguish the state err which makes this a safety automaton. Intuitively, state s2 is reached when the carriage receives a cargo and brings it successfully to the forward limit."}, {"title": "Testing from requirements", "content": "We want to use testing to check whether a system implementation satisfies its requirements. We thus formalize a notion of conformance to a set of requirements.\nIn the sequel, we use automata to describe requirements, and as models for implementations, with different assumptions. We identify a requirement with its complete deterministic safety automaton, and write R for both.\nDefinition 3.0.1. For any requirement R defined by a complete deterministic automaton, and any finite trace \u03c3, we write \u03c3 fails R if running \u03c3 in R from its initial state enters its error set ErrorR.\nFor a trace \u03c3 and a set of requirements R we write \u03c3 fails R to mean that \u03c3 fails \u211bR. Note the following simple facts, consequence of the definition of Error states in the product: if \u03c3 fails R then \u03c3 fails R for at least one R in R; given R' \u2286 R, for any trace \u03c3, if \u03c3 fails R', then \u03c3 fails R.\nWe want to test a system against a set of requirements R. We consider a deterministic system implementation I (the implementation under test), producing Boolean traces in (2AP)\u2217. We assume that this system is a black box that proceeds as follows: at each step, an input valuation in 2APin is provided to the system by the tester, and the system answers by producing an output valuation in 2APout (on which the tester has no control). We make the following assumptions on the implementation: I behaves as an unknown finite automaton over AP; it is input-complete, meaning that any valuation in 2APin can be set by the tester 1, and it is output-deterministic, meaning that any state in Sout has exactly one transition2. Last, we assume that from any input state of I, it is possible to reset I to the unique initial state at any time. These properties ensure that if one feeds the implementation with an input sequence from the initial state, then the system produces a unique output sequence. We denote by \u2110 the class of all such implementations producing traces in (2AP)\u2217.\nThe behaviour of I is characterized by the set of all traces produced by the interaction between the tester and the system. Denote by Tr(I) the set of traces that can be produced by I.\nWe now define what it means for an implementation to conform to a set of requirements:\nDefinition 3.0.2. A system implementation I \u2208 \u2110 conforms to a set of requirements R on AP if for all \u03c3 \u2208 Tr(I), it is not the case that \u03c3 fails R.\nTest Objectives. In testing practice, each test case is related to a particular goal, e.g., derived from a coverage criterion. We formalise this now.\nDefinition 3.0.3. Consider a set R of requirements, and let SR denote the state space of \u211bR. A test objective is a set of states O \u2286 SR. A trace \u03c3 covers a test objective O, if the unique execution of \u211bR on \u03c3 ends in O.\nNotice that the more general case where a test objective is an automaton A with a set of accepting locations Acc can be reduced to this one 3. Indeed, it suffices to consider the product automaton of the test objective A and the requirement automaton \u211bR, and consider as objective O the set of states of the product in which A is in Acc.\nThe problem that we address in the rest of the paper is the following:\nDefinition 3.0.4. The Test Problem"}, {"title": "Baseline Test Algorithms", "content": "Consider a set R of requirements, specified by a deterministic complete automaton \u211bR = (SR, SinitR, AP, TR, FR), and an implementation whose behaviour could be modelled as an input-complete, output-deterministic finite automaton I = (SI, SinitI, AP, TI, FI). Recall that in our setting, the set R of requirements, thus also its automaton \u211bR, is known, while the implementation I is considered to be a black box to the tester.\nConsider a particular test objective O \u2286 SR. Let Coreach(\u211bR, O) = Pre\u2217\u211bR(O) denote the set of input states of \u211bR from which O is reachable.\nOur aim is to design online testing algorithms that compute inputs to be given to the implementation I in order to generate a trace that either covers O, or detects non-conformance by reaching an Error state (or both if O contains Error states); notice that since I is output-deterministic, each such input sequence defines a unique trace of I. The testing process runs as follows: from a state s\u211bR of \u211bR and a state sI of I, the test algorithm returns an input valuation vin; this input valuation is fed to the implementation, which returns an output valuation vout and moves to a new state tI; the resulting valuation vin \u222a vout moves the automaton \u211bR from s\u211bR to a new state t\u211bR. The process then continues from t\u211bR and tI, unless we detect that a test objective or an Error state is reached.\nWe write \u211bR \u2297 I for the synchronized product of \u211bR and I: this is a deterministic automaton, of which we observe only the first component (i.e., the part corresponding to \u211bR), while we have no information and no observation concerning the second component except from the produced outputs. Our aim is to build a tester to cover some objectives in this partially-observable deterministic automaton.\nSince we do not know I, each input valuation vin should be selected only based on the trace generated so far, and possibly based on information collected on previous attempts.\nBefore explaining how we define test algorithms, we introduce some vocabulary to describe the configuration where the testing process ends. Assume that we have generated a trace \u03c3 by interacting with I from its initial state. Let s0 denote the state of \u211bR reached after reading \u03c3 from the initial state. Four cases may occur:\n\u2022 if s0 \u2208 O, then \u03c3 is a covering trace for O;\n\u2022 if s0 \u2208 Error \u211bR, then \u03c3 is an error trace;\n\u2022 if s0 \u2209 Coreach(\u211bR, O), then \u03c3 is inconclusive;\n\u2022 otherwise, s0 \u2208 Coreach(\u211bR, O) \\ O and \u03c3 is active.\nIntuitively, in the first case, we have found the desired covering trace, and we can stop. In the second case, we have found a trace failing one of the requirements of \u211bR, and we can also stop: the implementation does not conform to \u211bR. Notice that these two cases are not exclusive since we can have O \u2229 Error\u211bR \u2260 \u2205. In the third case, no matter how we extend \u03c3, we will never cover O; so the tester should stop and start again to look for another trace by resetting I. In the last case, \u03c3 is active in the sense that it might still be possible to try to extend \u03c3 to reach the objective. It should be clear that \u03c3 being active (last case) does not mean that O is reachable from the corresponding state (s0, sI) of the product \u211bR \u2297 I, as this depends on the (unknown) implementation I being considered: states in Coreach(\u211bR, O) are those for which some implementation in \u2110 can reach O. We illustrate this in the following example.\nExample 4.0.1. We consider the requirement expressed by the automaton of Fig. 1, the objective defined by the singleton O = {o}, and the implementation represented to the left of Fig. 4. Their product is represented to the right of Fig. 4. There is a covering trace in this case since the input sequence ab generates the trace (a0b1) in I1, and this reaches the state o in \u211bR.\nAssume now that I1 is modified so that it outputs 1 on input a from s1, then the product would go to a state of the form (t,s1). If t is an Error state, then the implementation does not conform to the requirement; if not, then the test is inconclusive since o is no longer reachable.\nOn the other hand, consider an implementation outputs 0 on any input. Then any trace is an active trace although the implementation does not have a covering trace (in fact, the product cannot reach a state involving o).\nWe start by formalizing the naive uniform test approach, and then cast the problem as a reinforcement-learning problem."}, {"title": "Naive Uniform Testing", "content": "We present a simple test algorithm implemented in tools such as TorXakis [Tor]. Let Coreach\u2032(\u211bR, O) denote the set of pairs (s, vin) where s is a state of \u211bR, and vin is an input valuation for which there exists some output valuation vout such that Post\u211bR(s, vin \u222a vout) \u2208 Coreach(\u211bR, O), where v = vin \u222a vout. In fact, after observing trace \u03c3 ending in states of \u211bR, the tester has no reason to give an input vin such that (s, vin) \u2209 Coreach\u2032(\u211bR, O): such an input would lead to a trace that is inconclusive, and the objective would not be reachable regardless of I.\nA very simple test algorithm is the following: starting from the initial state of the implementation, we store in s the initial state of \u211bR. As long as the trace being produced is active, we select uniformly at random an input valuation among {vin | (s, vin) \u2208 Coreach\u2032(\u211bR, O)}. We observe the output vout given by I, and update s as Post\u211bR(s, vin \u222a vout). There are three cases when this process stops:\n\u2022 if s \u2208 O, then we stop and report the generated trace as a covering trace for O;\n\u2022 if s \u2208 Error \u211bR, we also stop and report a failure;\n\u2022 if the current trace is inconclusive, then we reset I, set s to the initial state of \u211bR, and start again.\nNote that it is possible to generate inconclusive traces since \u211bR is not assumed to be output-deterministic. It is then possible to have (s, vin) \u2208 Coreach\u2032(\u211bR, O) and for some vout and v'out, Post\u211bR(s, vin \u222a vout) \u2208 Coreach\u2032(\u211bR, O) and Post\u211bR(s, vin \u222a v'out) \u2209 Coreach\u2032(\u211bR, O) (see e.g., Fig. 1). Some conformant implementation \u2110 can indeed return v'out, producing an inconclusive trace.\nLet this test algorithm be called uniform. For any bound K, let uniformK be the uniform testing algorithm in which we stop each run after K steps, so that the generated traces have length at most K. This algorithm is almost-surely complete:\nLemma 4.0.1. For each requirement set R, implementation I, and test objective O, there exists K > 0 such that if O is reachable in \u211bR \u2297 I, then uniformK finds a covering trace with probability 1.\nProof. Assume there exists a covering trace \u03c3 in \u211bR \u2297 I, and let K be the length of \u03c3. When playing uniformK ad infinitum, the testing process restarts an infinite number of times. At each step, the algorithm picks each valuation of the input variables with probability 2\u2212|APin|. So at each restart, the probability of choosing exactly \u03c3 is 2\u2212|APin|\u2217K. Therefore, \u03c3 is picked eventually with probability 1.\nNote that there is no need to fix K. Any algorithm that ensures that K is increased towards infinity finds a covering trace with probability 1. Furthermore, the uniform distribution can also be relaxed: any algorithm that picks each input of {vin | (s, vin) \u2208 Coreach\u2032(\u211bR, O)} with probability at least a fixed value \u03f5 > 0 also has this property."}, {"title": "Testing Based on Reinforcement Learning", "content": "The online-testing problem can be seen as a reinforcement-learning (RL) problem as follows.\nThe considered system is the implementation I", "trace": "a covering trace has reward 0", "SB18": ".", "KS06": ".", "follows": "nSelection: from the root of the tree", "explored;\nExpansion": "add a new child corresponding to that move;\nSimulation: simulate a random play", "child;\nPropagation": "assign the reward of that play to the new child, and update the rewards of its ancestors accordingly.\nDifferent tree policies can be used to pick the successive moves during the selection phase, based on statistics obtained from previous iterations. We use UCT (Upper Confidence bounds applied to Trees) [KS06"}]}