{"title": "Online Test Synthesis From Requirements: Enhancing Reinforcement Learning with Game Theory", "authors": ["Ocan Sankur", "Thierry J\u00e9ron", "Nicolas Markey", "David Mentr\u00e9", "Reiya Noguchi"], "abstract": "We consider the automatic online synthesis of black-box test cases from functional requirements specified as automata for reactive implementations. The goal of the tester is to reach some given state, so as to satisfy a coverage criterion, while monitoring the violation of the requirements. We develop an approach based on Monte Carlo Tree Search, which is a classical technique in reinforcement learning for efficiently selecting promising inputs. Seeing the automata requirements as a game between the implementation and the tester, we develop a heuristic by biasing the search towards inputs that are promising in this game. We experimentally show that our heuristic accelerates the convergence of the Monte Carlo Tree Search algorithm, thus improving the performance of testing.", "sections": [{"title": "1 Introduction", "content": "Requirement engineering and testing are two important and related phases in the development process. Indeed, test cases are usually derived from functional specifications and documented with the requirements they are supposed to check. Several existing tools allow one to automatically generate test cases from formal functional specifications and/or formal requirements (see e.g., the tools T-VEC [BB96], TorXakis [TvdL19] or Stimulus [JG16], and surveys [UPL12] and [LLGS18]).This helps the development process since the developer can focus on writing formal specifications or requirements, and test cases are generated automatically. There are two main approaches for test generation: black-box testing focuses on generating tests without having access to the system's internals such as its source code (see e.g., [Bei95]), while white-box testing generates tests by analyzing its source code (see e.g., [MBTS04, AO17]).\nBlack-Box Testing. In this paper, we are interested in automatically synthesizing online test cases from a set of requirements and an implementation under test in a black-box setting. By black-box, we mean that the implementation internal is unknown to the tester (or at least not used), only its interaction with her is used. By online, we mean that the synthesis is essentially performed during execution, while interacting with the implementation. We consider implementations that are reactive: these are programs that alternate between reading an input valuation and writing an output valuation. This setting is interesting for modeling synchronous systems, e.g., controllers for manufacturing systems [Bol15]. The considered requirements are given as automata recognizing sequences of valuations of input and output variables. The conformance of an implementation to a set of requirements is formalized as the absence of input-output valuation sequences generated by the implementation that are rejected by the requirements automaton.\nThe goal of the test cases is to drive the implementation to some particular state or show some particular behaviour where non-conformance is suspected to occur. These are described by test objectives. They are typically derived from coverage criteria, e.g. state or transition coverage, or written from requirements [?]. The selection of the test objectives is out of the scope of this paper; we thus assume that these are given.\nThere are several black-box testing algorithms and tools in the literature. The closest to our approach is TorXakis [TvdL19, Tor] which is a tool based on the ioco testing theory [Tre96] and the previous TorX tool [TB03]. It allows the user to specify the automata-based requirements reading input-output valuations as well as test objectives (called test purposes) in a language based on process algebra, and is able to generate online tests interacting with a given implementation. These tests are performed by picking random input valuations, and observing the outputs from the implementation, while checking for non-conformance. Because tests are performed using random walks in this approach, deep traces satisfying the test objective or violating the requirements are hard to find in practice.\nReinforcement Learning for Testing. This issue has been addressed in many works by interpreting the test synthesis in a reinforcement learning(RL) setting [SB18]. Reinforcement learning is a set of techniques for computing strategies that optimize a given reward function based on interactions of an agent with its environment. It has been applied to learn strategies for playing board games such as Chess and Go [SHM+16]. Here the test synthesis is seen as a game between the tester and the implementation: the former player selects inputs, and the latter player selects outputs. Because the implementation is black-box, the tester is playing an unknown game, but can discover the game through interactions. Using a game approach for test synthesis has long been advocated [Yan04]; and online testing for interface automata specifications were considered in [VRC06] using RL.\nBecause reaching a test objective is a 0/1 problem (an execution either reaches the objective and has a reward 1, or does not reach it and has reward 0), RL algorithms are usually very slow in finding deep traces. The application of RL to black-box testing thus requires the use of reward shaping [NHR99] which consists in assigning intermediate rewards to steps before the objective is reached; these are used to guide the search to more promising input sequences and can empirically accelerate convergence. Reward shaping has been used for testing, e.g., in [KS21] where an RL algorithm (Q-learning) was used for testing GUI applications with respect to linear temporal logic (LTL) specifications; rewards were then computed based on transformations on the target LTL formula.\nContributions. Although reinforcement learning helps one to guide the search towards the test objective, these methods can still be slow in finding traces satisfying the test objective, especially when the number of input bits is high, and when the traces to be found are long.\nIn this work, we target improving the performance of black-box online test algorithms based on reinforcement learning. More precisely, we develop heuristics for a Monte Carlo Tree Search (MCTS) algorithm applied in this setting, based on a game-theoretic analysis of the requirement automaton, combined with an appropriate reward shaping scheme.\nMonte-Carlo Tree Search [Cou06] (see also e.g., the survey [BPW+12]) is a RL technique to search"}, {"title": "2 Preliminaries", "content": "We first introduce traces that represent observable behaviours of reactive systems, then the automata models that recognize such traces and are used to formally specify requirements of such systems, together with related automata based notions.\nTraces. We fix a set of atomic propositions AP, partitioned into APin \u2295 Apout, that represent Boolean input- and output variables of the system. A valuation of AP is an element v of 2AP determining the set of atomic propositions which are true (or equivalently, it is a mapping v: AP \u2192 {T, \u22a5}). We denote by vin (respectively vout) the projections of v on Apin (resp. Apout) such that v = vin \u2295 vout. We write B(AP) for the set of Boolean combinations of atomic propositions in AP. That a valuation v satisfies a formula \u03c6 \u2208 B(AP), denoted by v |= \u03c6, is defined in the usual way.\nWe consider reactive systems that work as a succession of atomic steps: at each step, the environment first sets an input valuation vin, then the system immediately sets an output valuation vout. The valuation observed at this step is thus v = vin \u2295 vout. A trace of the system is a sequence \u03c3 = v1 \u00b7 v2 \u00b7\u00b7\u00b7 vn of input and output valuations.\nInternal variables may be used by the system to compute outputs from the inputs and the internal state, but these are not observable to the outside.\nAutomata. We use automata to express requirements, and as models for the implementations under test. When considered as requirements, they monitor the system through the observation of the values of the Boolean variables. Transitions of automata are guarded with Boolean constraints on AP that need to be satisfied for the automaton to take that transition. For convenience, we handle input- and output valuations separately. Formally,\nDefinition 2.0.1. An automaton is a tuple A = \u3008S = Sin \u2295 Sout, Sinit, AP, T, F\u3009 where S is a finite set of states, Sinit \u2208 Sin is the initial state, T \u2286 (Sin \u00d7 B(APin) \u00d7 Sout) \u222a (Sout \u00d7 B(APout) \u00d7 Sin) is a finite set of transitions, and F \u2286 S is the set of accepting states.\nFor two states sin and s'in and a valuation v, we write sin \u27f6v s'in when there exist a state sout and transitions (sin, gin, sout) and (sout, gout, s'in) in T such that vin |= gin and vout |= gout.\nFor a trace \u03c3 = v1 \u00b7 v2 \u00b7\u00b7\u00b7 vn in (2AP)\u2217, we write sin \u27f6\u03c3 s'in if there are states s0, s1 ... sn such that s0 = sin, sn = s'in, and for all i \u2208 [1, n], si\u22121 \u27f6vi si. A trace \u03c3 \u2208 (2AP)\u2217 is accepted by A if Sinit \u27f6\u03c3 s for some s \u2208 F. We denote by Tr(A) the set of accepted traces.\nAn automaton is input-complete if from any (reachable) state sin and any valuation vin \u2208 2APin, there is a transition (sin, gin, s'out) in T such that vin |= gin. It is output-complete if a similar requirement holds for states in Sout and valuations in vout, and it is complete if it is both input- and output-complete. An automaton is deterministic when, for any two transitions (s, g1, s1) and (s, g2, s2) issued from a same source s, if g1 \u2227 g2 is satisfiable, then s1 = s2.\nIn the rest of the paper, we will be mainly interested in states of Sin, since states in Sout are intermediary states that help us distinguish input and output valuations. For a state sin and a valuation v, we let PostA(sin, v) denote the set of states s'in such that sin \u27f6v s'in, and PreA(sin, v) denote the set of states s'in such that s'in \u27f6v sin. Notice that for deterministic complete automata, PostA(sin, v) is a singleton.\nThe set of immediate predecessors PreA(B) of a set B \u2286 Sin, and the set of its immediate successors PostA(B) are defined respectively as\nPreA(B) = \u222asin\u2208B,v\u22082AP PreA(sin, v), PostA(B) = \u222asin\u2208B,v\u22082AP PostA(sin, v).\nFrom these sets, one can define the set of states from which B is reachable (i.e., that are coreachable from B), as Pre\u2217(B) = lfp(\u03bbX.(B\u222a PreA(X))), and the set of states that are reachable from B, Post\u2217(B) = lfp(\u03bbX.(B\u222a PostA(X))), where lfp denotes the least-fixpoint operator. The fixpoint defining Pre\u2217(B) is equivalent to\nB\u222a PreA(B) \u222a PreA(B\u222a PreA(B)) \u222a PreA(B\u222a PreA(B) \u222a PreA(B\u222a PreA(B))) \u222a ...\nThis correspond to an iterative computation of a sequence (Vi)i\u2208N, starting from V0 = \u2205 (which is why we get the least fixpoint) and such that Vi+1 = B \u222a PreA(Vi). Observe, by induction on i, that from any state s in Vi, there is a path to B within i steps. The sequence (Vi)i\u2208N is non-decreasing, and its limit is the set of all states from which B can be reached: from each state s \u2208 Pre\u2217(B), there is a finite trace \u03c3 such that by reading \u03c3 from s, one ends in B. Similarly, for each state s' \u2208 Post\u2217(B), there exists a state s \u2208 B and a trace \u03c3 such that by reading \u03c3 from s, one ends at s'.\nSafety automata form a subclass of automata having a distinguished set Error \u2286 Sin of error states that are non-accepting and absorbing (i.e., no transitions leave Error), and complement the set F of accepting states in S (i.e., F = S \\ Error). Those automata describe safety properties: nothing bad happened as long as Error is not reached."}, {"title": "3 Testing from requirements", "content": "We want to use testing to check whether a system implementation satisfies its requirements. We thus formalize a notion of conformance to a set of requirements.\nIn the sequel, we use automata to describe requirements, and as models for implementations, with different assumptions. We identify a requirement with its complete deterministic safety automaton, and write R for both.\nDefinition 3.0.1. For any requirement R defined by a complete deterministic automaton, and any finite trace \u03c3, we write \u03c3 fails R if running \u03c3 in R from its initial state enters its error set ErrorR.\nFor a trace \u03c3 and a set of requirements R we write \u03c3 fails R to mean that \u03c3 fails \u2227R. Note the following simple facts, consequence of the definition of Error states in the product: if \u03c3 fails R then \u03c3 fails R for at least one R in R; given R' \u2286 R, for any trace \u03c3, if \u03c3 fails R', then \u03c3 fails R.\nWe want to test a system against a set of requirements R. We consider a deterministic system implementation I (the implementation under test), producing Boolean traces in (2AP)\u2217. We assume that this system is a black box that proceeds as follows: at each step, an input valuation in 2APin is provided to the system by the tester1, and the system answers by producing an output valuation in 2Apout (on which the tester has no control). We make the following assumptions on the implementation: I behaves as an unknown finite automaton over AP; it is input-complete, meaning that any valuation in 2APin can be set by the tester2, and it is output-deterministic, meaning that any state in Sout has exactly one transition. Last, we assume that from any input state of I, it is possible to reset I to the unique initial state at any time. These properties ensure that if one feeds the implementation with an input sequence from the initial state, then the system produces a unique output sequence. We denote by I the class of all such implementations producing traces in (2AP)\u2217.\nThe behaviour of I is characterized by the set of all traces produced by the interaction between the tester and the system. Denote by Tr(I) the set of traces that can be produced by I.\nWe now define what it means for an implementation to conform to a set of requirements:\nDefinition 3.0.2. A system implementation I \u2208 I conforms to a set of requirements R on AP if for all \u03c3 \u2208 Tr(I), it is not the case that \u03c3 fails R.\nTest Objectives. In testing practice, each test case is related to a particular goal, e.g., derived from a coverage criterion. We formalise this now.\nDefinition 3.0.3. Consider a set R of requirements, and let SR denote the state space of \u2227R. A test objective is a set of states O \u2286 SR. A trace \u03c3 covers a test objective O, if the unique execution of \u2227R on \u03c3 ends in O.\nNotice that the more general case where a test objective is an automaton A with a set of accepting locations Acc can be reduced to this one3. Indeed, it suffices to consider the product automaton of the test objective A and the requirement automaton \u2227R, and consider as objective O the set of states of the product in which A is in Acc.\nThe problem that we address in the rest of the paper is the following:\nDefinition 3.0.4. The Test Problem"}, {"title": "4 Baseline Test Algorithms", "content": "Consider a set R of requirements, specified by a deterministic complete automaton \u2227R = (SR, SinitR, AP, TR, FR), and an implementation whose behaviour could be modelled as an input-complete, output-deterministic finite automaton I = (SI, SinitI, AP, TI, FI). Recall that in our setting, the set R of requirements, thus also its automaton \u2227R, is known, while the implementation I is considered to be a black box to the tester.\nConsider a particular test objective O \u2286 SR. Let CoReach(\u2227R, O) = Pre\u2217\u2227R(O) denote the set of input states of \u2227R from which O is reachable.\nOur aim is to design online testing algorithms that compute inputs to be given to the implementation I in order to generate a trace that either covers O, or detects non-conformance by reaching an Error state (or both if O contains Error states); notice that since I is output-deterministic, each such input sequence defines a unique trace of I. The testing process runs as follows: from a state s\u2227R of \u2227R and a state sI of I, the test algorithm returns an input valuation vin; this input valuation is fed to the implementation, which returns an output valuation vout and moves to a new state tI; the resulting valuation vin \u222a vout moves the automaton \u2227R from s\u2227R to a new state t\u2227R. The process then continues from t\u2227R and tI, unless we detect that a test objective or an Error state is reached.\nWe write AR \u2297 I for the synchronized product of AR and I: this is a deterministic automaton, of which we observe only the first component (i.e., the part corresponding to AR), while we have no information and no observation concerning the second component except from the produced outputs. Our aim is to build a tester to cover some objectives in this partially-observable deterministic automaton.\nSince we do not know I, each input valuation vin should be selected only based on the trace generated so far, and possibly based on information collected on previous attempts.\nBefore explaining how we define test algorithms, we introduce some vocabulary to describe the configuration where the testing process ends. Assume that we have generated a trace \u03c3 by interacting with I from its initial state. Let s0 denote the state of AR reached after reading \u03c3 from the initial state. Four cases may occur:\n\u2022 if s0 \u2208 O, then \u03c3 is a covering trace for O;\n\u2022 if s0 \u2208 Error\u2227R, then \u03c3 is an error trace;\n\u2022 if s0 \u2209 CoReach(\u2227R, O), then \u03c3 is inconclusive;\n\u2022 otherwise, s0 \u2208 CoReach(\u2227R, O) \\ O and \u03c3 is active.\nIntuitively, in the first case, we have found the desired covering trace, and we can stop. In the second case, we have found a trace failing one of the requirements of AR, and we can also stop: the implementation does not conform to AR. Notice that these two cases are not exclusive since we can have O \u2229 ErrorAR \u2260 \u2205. In the third case, no matter how we extend \u03c3, we will never cover O; so the tester should stop and start again to look for another trace by resetting I. In the last case, \u03c3 is active in the sense that it might still be possible to try to extend \u03c3 to reach the objective. It should be clear that \u03c3 being active (last case) does not mean that O is reachable from the corresponding state (s0, sI) of the product AR \u2297 I, as this depends on the (unknown) implementation I being considered: states in CoReach (AR, O) are those for which some implementation in I can reach O. We illustrate this in the following example.\n4.  1 Naive Uniform Testing\nWe present a simple test algorithm implemented in tools such as TorXakis [Tor]. Let CoReachin(AR, O) denote the set of pairs (s, vin) where s is a state of AR, and vin is an input valuation for which there exists some output valuation vout such that PostAR(s, v) \u2208 CoReach(AR, O), where v = vin \u222a vout. In fact, after observing trace \u03c3 ending in states of AR, the tester has no reason to give an input vin such that (s, vin) \u2209 CoReachin(AR, O): such an input would lead to a trace that is inconclusive, and the objective would not be reachable regardless of I.\nA very simple test algorithm is the following: starting from the initial state of the implementation, we store in s the initial state of AR. As long as the trace being produced is active, we select uniformly at random an input valuation among {vin | (s, vin) \u2208 CoReachin(AR, O)}. We observe the output vout given by I, and update s as PostAR(s, vin \u222a vout). There are three cases when this process stops:\n\u2022 if s \u2208 O, then we stop and report the generated trace as a covering trace for O;\n\u2022 if s \u2208 ErrorAR, we also stop and report a failure;\n\u2022 if the current trace is inconclusive, then we reset I, set s to the initial state of AR, and start again.\nNote that it is possible to generate inconclusive traces since AR is not assumed to be output-deterministic. It is then possible to have (s, vin) \u2208 CoReachin(AR, O) and for some vout and v'out, PostAR(s, vin \u222a vout) \u2208 CoReachin(AR, O) and PostAR(s, vin \u222a v'out) \u2209 CoReachin(AR, O) (see e.g., Fig. 1). Some conformant implementation I can indeed return v'out, producing an inconclusive trace.\nLet this test algorithm be called uniform. For any bound K, let uniformK be the uniform testing algorithm in which we stop each run after K steps, so that the generated traces have length at most K. This algorithm is almost-surely complete:\nLemma 4.0.1. For each requirement set R, implementation I, and test objective O, there exists K > 0 such that if O is reachable in AR \u2297 I, then uniformK finds a covering trace with probability 1.\nProof. Assume there exists a covering trace \u03c3 in AR \u2297 I, and let K be the length of \u03c3. When playing uniformK ad infinitum, the testing process restarts an infinite number of times. At each step, the algorithm picks each valuation of the input variables with probability 2\u2212|APin|. So at each restart, the probability of choosing exactly \u03c3 is 2\u2212K|APin|. Therefore, \u03c3 is picked eventually with probability 1.\nNote that there is no need to fix K. Any algorithm that ensures that K is increased towards infinity finds a covering trace with probability 1. Furthermore, the uniform distribution can also be relaxed: any algorithm that picks each input of {vin | (s, vin) \u2208 CoReachin(AR, O)} with probability at least a fixed value \u03f5 > 0 also has this property."}, {"title": "4.  2 Testing Based on Reinforcement Learning", "content": "The online-testing problem can be seen as a reinforcement-learning (RL) problem as follows.\nThe considered system is the implementation I, seen as a one-player deterministic game. The goal is to find a sequence of inputs that guides the system to a given objective. We assign a reward to each trace: a covering trace has reward 0, other traces have reward 1. Note that we will consider minimizing the reward for reasons that will be clear later.\nNotice that we do not assign a particular reward to error traces. In fact, we assume that the goal of the tester is to produce a covering trace, while monitoring all traces seen on the way for R. If an error trace is seen, then we simply report it. Furthermore, it is possible to choose an objective in ErrorAR in which case the test strategy will try to reach an error state.\nReinforcement learning is a set of techniques that can be used to learn strategies that maximize the reward in games [SB18]. In this paper, we use Monte Carlo Tree Search [Cou06, KS06].\n5.  2.  1 Monte-Carlo Tree Search.\nMonte-Carlo Tree Search (MCTS) is a RL technique to search for good moves in games. It consists in exploring the available moves randomly, while estimating the potential of each newly-explored move and updating the potential of previously selected moves.\nMore precisely, MCTS builds a weighted tree of possible plays of the game iteratively as follows:\nSelection: from the root of the tree, select moves, using a tree policy, until reaching a node where some move has not been explored;\nExpansion: add a new child corresponding to that move;\nSimulation: simulate a random play, using a roll-out policy, from that new child;\nPropagation: assign the reward of that play to the new child, and update the rewards of its ancestors accordingly.\nDifferent tree policies can be used to pick the successive moves during the selection phase, based on statistics obtained from previous iterations. We use UCT (Upper Confidence bounds applied to Trees) [KS06] which is standard in many applications. At a given node of the tree, if n denotes the number of total visits to this node, and ni the number of times the i-th child node is visited (corresponding to the i-th move from the parent node), and ri the current average reward of the i-th child, we define the score of the i-th child as ri + c\u221aln(n)/ni for some constant c. The UCT policy consists in choosing the child with the best score. Intuitively, this score is the average reward ri biased in order to make sure that each child node is visited frequently enough. In fact, the second term of the score is only relevant when ni is small. If the goal is to maximize the average score, then c > 0, and the UCT policy picks the child node with the maximal score; if, as in this work, we want to minimize the reward, then one chooses c < 0 and the policy picks the child node with minimal score.\nOnce an unvisited action has been selected and the tree has been expanded with a new node, a roll-out policy is applied to evaluate the potential of that new action, usually by randomly selecting inputs that form a path from the resulting configuration. This evaluation gives a first reward to the newly created node, which is back-propagated to all its predecessors in the tree; each node of the tree stores statistics from previous rounds, including the number of visits and its average reward.\nIn the limit, the procedure is guaranteed to provide the optimal reward values for each state and move. In practice, the procedure can be interrupted at any time (depending on the available resources devoted to the search), and the current best moves from all states of the tree provide a strategy.\nIn our case, each simulation is bounded by K steps. Such a bound is necessary since some simulations might never reach the objective, an error, or an inconclusive state and thus never terminate. Last, we consider uniformK (from Section 4.1) as the roll-out policy. Note that choosing the inputs uniformly in the simulation phase is standard. Here, we simply improve this by sampling over inputs that remain in the coreachable set.\n4.  2.  2 Reward Shaping: Accelerating Convergence.\nOne technique that is used to help reinforcement-learning algorithms converge faster is reward shaping [NHR99], which consists in assigning real-valued rewards to traces, to give more information than just the binary 0/1. For instance, if the trace induced a run in AR that became very close to the objective, then it might be given a better (lower) reward than another trace whose run was very far. The computation of such rewards based on automata objectives were formalized in [CIK+19]. We now describe how we apply this to our setting.\nHere AR is used solely to compute rewards of traces, while the actual testing will be done by the MCTS algorithm. Let C0 = O, and for i \u2265 1, define\nCi = PreAR(Ci\u22121) \\ (C0 \u222a ... \u222a Ci\u22121).\nWe have \u222ai>0 Ci = CoReach(AR, O). In fact, each Ci is the set of states that are at distance i from some state in O (in the sense that AR contains a run of length i to O).\nWe consider two ways of assigning rewards to simulation traces. Let m be maximal such that Cm \u2260 \u2205. Let us define Cm+1 = SR \\ CoReach(AR, O), that is, all states that are not coreachable. We assign each trace whose run in AR ends in sR the reward lastreward(\u03c3) = k if, and only if, sR \u2208 Ck. Notice that this is well defined because the sets Ci are pairwise-disjoint and they cover all states. Hence, the closer the trace to objective O, the smaller its reward. A reward of O means that the state satisfies the objective.\nThe second reward assignment considers not only the last reward, but all rewards seen during the simulation, as follows. Let r\u03c3,r1,r2,\u00b7\u00b7\u00b7,rK\u22121 denote the sequence of rewards encountered during simulation (these are the rewards of the prefixes of the trace \u03c3). If the length of the simulation was less than K, we simply repeat the last reward to extend this sequence to size K. Then the reward of the simulation is given by\ndisc \u2212 reward\u03b3(\u03c3) = rK\u22121 + K\u22121 \u2211 i=0 \u03b3 i ri,\nwhere \u03b3 \u2208 (0, 1) is a discount factor. Notice that this value is 0, and minimal, if and only if rK\u22121 = 0. Furthermore, while rK\u22121 is the most important factor, the second factor means that we favor simulations whose first reward values are smaller. This can in fact be seen as a weighted version of lastreward, where the weight is smaller if the simulation has small rewards in the first steps.\n4.  2.  3 Basic MCTS Testing Algorithm.\nThis yields the second testing algorithm we consider which we call basic MCTS. The algorithm is complete in the following sense since MCTS with UCT ensures that each node and action in the tree will be picked infinitely often in the limit.\nLemma 4.0.2. The basic MCTS algorithm is complete: For each requirement set R, implementation I, and test objective O, there exists K > 0 such that, if O is reachable in AR \u2297 I, then the basic MCTS with simulation bound K finds a covering trace.\nNote that this algorithm is not just almost-surely complete, but also complete. This is because the UCT policy deterministically guarantees that all nodes of the tree are visited infinitely often. Thus, when the depth of the tree becomes large enough, any covering trace will be part of it, thus will have been executed. However, we do rely on estimated rewards to guide the search to ensure faster termination in practice.\nThe above lemma holds for both reward assignments lastreward and disc-reward\u03b3. Moreover, as for uniform, it is possible not to fix K, but increase it slowly towards infinity.\nNote that the basic MCTS algorithm is also a baseline since it can be obtained by combining known results from the literature; similar algorithms have been considered e.g. [VRC06, KS21]."}, {"title": "5 Greedy Strategies and Improved Test Algorithms", "content": "In this section", "players": "the tester", "f": "Tr(AR) \u2192 22APin that associates with each trace a subset of input valuations", "CPre\u2217": "nLemma 5.0.1. For all B \u2286 Sin", "DLLN08b": ".", "B": "by definition from these states, there exists a pair of valuations (vin, vout) for which the successor state is in B. From these states the tester cannot always guarantee reaching B in one step; however, it can choose an input valuation vin for which there exists vout which moves the system into B. In fact, when attempting to reach B, if the current state is not in CPre\u2217AR(B), then choosing such a vin is a good choice.\nLet us call a strategy f B-cooperative if for all s \u2208 PreAR(B), and all vin \u2208 f(s), there exists vout such that PostAR(s, vin \u222a vout) \u2208 B.\nExample 5.0.2. In"}]}