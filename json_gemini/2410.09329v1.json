{"title": "Zero-shot Commonsense Reasoning over Machine Imagination", "authors": ["Hyuntae Park", "Yeachan Kim", "Jun-Hyung Park", "SangKeun Lee"], "abstract": "Recent approaches to zero-shot commonsense reasoning have enabled Pre-trained Language Models (PLMs) to learn a broad range of commonsense knowledge without being tailored to specific situations. However, they often suffer from human reporting bias inherent in textual commonsense knowledge, leading to discrepancies in understanding between PLMs and humans. In this work, we aim to bridge this gap by introducing an additional information channel to PLMs. We propose IMAGINE (Machine Imagination-based Reasoning), a novel zero-shot commonsense reasoning framework designed to complement textual inputs with visual signals derived from machine-generated images. To achieve this, we enhance PLMs with imagination capabilities by incorporating an image generator into the reasoning process. To guide PLMs in effectively leveraging machine imagination, we create a synthetic pre-training dataset that simulates visual question-answering. Our extensive experiments on diverse reasoning benchmarks and analysis show that IMAGINE outperforms existing methods by a large margin, highlighting the strength of machine imagination in mitigating reporting bias and enhancing generalization capabilities.", "sections": [{"title": "Introduction", "content": "Commonsense reasoning has been considered a crucial milestone in the pursuit of artificial general intelligence (Gunning, 2018). While Pre-trained Language Models (PLMs; Devlin et al., 2019; Brown et al., 2020) often exhibit near-human reasoning capabilities after being fine-tuned on specific commonsense datasets, they face challenges in zero-shot scenarios where examples differ significantly from their training data distribution (Mitra et al., 2019; Kim et al., 2022). Overcoming this limitation is crucial for achieving human-level proficiency in natural language understanding.\nOne promising approach to this limitation is injecting commonsense knowledge from external Knowledge Bases (KBs; Sap et al., 2019a; He et al., 2022b) into PLMs. Specifically, this involves transforming knowledge entities into a question-answering (QA) format, resulting in a synthetic QA dataset. This constructed dataset is then used to train PLMs similarly to the pre-training phase. Since the knowledge bases can cover a wide spectrum of commonsense knowledge, this approach leads to substantial improvements in reasoning ability across diverse situations without specializing in specific knowledge (Wang et al., 2023, 2024).\nHowever, they often suffer from human reporting bias (Gordon and Durme, 2013), as textual commonsense knowledge only captures the most frequently occurring scenarios, thereby neglecting less common but equally critical knowledge necessary for comprehensive reasoning. Figure 1 illustrates a case where a recent model (Wang et al., 2023) fails to accurately reason about the question \u201cHow do you butter toast?\u201d. Since the existing models rely solely on textual inputs, they often neglect contextual details, such as the fact that butter is typically too solid to be dipped. In contrast, humans can easily answer such questions by visually imagining the shape, solidity, and interactions of butter with other objects. This observation motivates us to explore additional modalities to complement textual commonsense knowledge.\nIn this paper, we introduce IMAGINE (Machine Imagination-based Reasoning), a novel zero-shot commonsense reasoning framework designed to circumvent the reporting bias inherent in textual inputs. Inspired by the cognitive studies highlighting the beneficial effects of visual imagery on language understanding (Gambrell and Bales, 1986; Dessalegn and Landau, 2013), IMAGINE is designed to leverage visual signals to complement textual inputs. To achieve this, we integrate PLMs with a conditional image generator, enabling machine imagination capabilities. To guide the model in learning to utilize visual and textual inputs jointly, we create a Synthetic VQA dataset, which is then used to optimize PLMs. By acquiring a broad spectrum of commonsense knowledge along with visual signals, IMAGINE enhances reasoning capabilities while circumventing human reporting bias.\nTo verify the effectiveness of IMAGINE, we perform extensive experiments, encompassing diverse reasoning benchmarks, architectures, and scales. The experimental results convincingly demonstrate that IMAGINE surpasses existing methods, including large language models, in reasoning capabilities. Moreover, our in-depth analysis reveals that IMAGINE effectively enables PLMs to adaptively leverage machine imagination capabilities in a beneficial manner. The contributions of this paper include the following:\n\u2022 We introduce IMAGINE, a novel zero-shot commonsense reasoning framework, aimed at mitigating reporting bias and enhancing the generalizability of PLMs.\n\u2022 We construct a Synthetic VQA dataset to enable PLMs to jointly utilize textual and visual signals while achieving commonsense reasoning ability.\n\u2022 We demonstrate that IMAGINE surpasses state-of-the-art zero-shot reasoning models across diverse reasoning tasks, highlighting the significance of machine imagination."}, {"title": "Related Work", "content": "There are two major approaches to zero-shot commonsense reasoning. The first approach involves utilizing the inherent capabilities of the off-the-shelf PLMs without updating their parameters. For example, Trinh and Le (2018) utilized the perplexity of vanilla language modeling, and Li et al. (2022) leveraged PLMs with specifically-designed prompting. Shwartz et al. (2020) solicited the commonsense knowledge from the language models through an iterative self-talk. Similarly, Dou and Peng (2022) obtained additional knowledge for reasoning based on the cloze-style translation. The second approach involves leveraging external commonsense knowledge bases (e.g., ATOMIC (Sap et al., 2019a), ConceptNet (Speer et al., 2017)) to provide language models with additional knowledge. Specifically, recent studies have transformed the knowledge entities (e.g., triplets of (head, relation, tail)) into synthetic QA pairs and trained the models with them (Banerjee and Baral, 2020; Ma et al., 2021). Recently, Wang et al. (2023) further improved the synthetic signals through a conceptualization process (Song et al., 2011) which abstracts a commonsense knowledge triplet to many higher-level instances. Subsequently, Wang et al. (2024) injected the instantiation phase into the process of synthetic dataset generation with the help of the generation capabilities of LLMs."}, {"title": "Visual Information for Natural Language Understanding", "content": "A few previous works have leveraged machine imagination to address Natural Language Understanding (NLU) problems. For example, Tan and Bansal (2020) proposed VOKEN, which introduces visual supervision into language model pre-training by incorporating external knowledge from images retrieved for the tokens. Instead of retrieving visual information, Lu et al. (2022) proposed generating synthetic images (i.e., imagination) based on a generative model to tackle downstream NLU tasks. In the context of commonsense reasoning, Liu et al. (2022) utilized visual information to comprehend spatial commonsense knowledge (e.g., how big is a lion?). Similar to the proposed method, Yang et al. (2022) introduced Z-LaVI, which integrated visual information with PLMs through both retrieval and synthesis to achieve zero-shot reasoning abilities. Unlike previous approaches that employ visual signals directly, we introduce a distinct pre-training phase which allows the model to effectively utilize visual imagination for zero-shot reasoning."}, {"title": "Machine Imagination-based Reasoning", "content": "In this section, we elaborate on the proposed method, namely IMAGINE (Machine Imagination-based Reasoning), for zero-shot commonsense reasoning. The core strategy is to complement textual commonsense knowledge with visual signals derived from machine-generated images. To achieve this, we first couple the PLMs with a text-to-image generator (\u00a73.1), enabling machine imagination in text-based PLMs. We then construct a large-scale Synthetic VQA dataset to learn the joint use of textual and visual signals in the reasoning process (\u00a73.2). By optimizing the model with additional signals that encapsulate commonsense knowledge, IMAGINE can effectively perform commonsense reasoning while avoiding human reporting bias inherent in textual inputs (\u00a73.3, \u00a73.4). The overall procedure is depicted in Figure 2."}, {"title": "Machine Imagination in PLMs", "content": "We start by introducing the machine imagination in text-based PLMs. We denote PLMs as \\(M_T\\), which serve as the backbone for zero-shot commonsense reasoning. For machine imagination, we incorporate two additional models to process visual signals. Specifically, we introduce: (i) a text-to-image generator, \\(M_{T2I}\\), which creates relevant images by conditioning the textual inputs, and (ii) a visual encoder, \\(M_I\\), which acts as a feature extractor for the given images.\nThe overall mechanism of machine imagination operates as follows: Given a textual input, the text-to-image model \\(M_{T2I}\\) initially generates an image that captures the essence of the text. With these generated images linked to textual inputs, both PLMs, \\(M_T\\), and the visual encoder, \\(M_I\\), jointly encode the textual input and the generated image. The resultant features are then utilized to derive the comprehensive predictions."}, {"title": "Synthetic VQA Construction", "content": "Following the previous works (Ma et al., 2021; Wang et al., 2023), we achieve zero-shot commonsense reasoning ability by constructing the synthetic QA dataset from the knowledge base. On top of this dataset, we build a synthetic visual question-answering (Synthetic VQA) dataset with the help of machine imagination. Additionally, we incorporate a visual commonsense dataset that contains real images (Zellers et al., 2019). The dataset is designed to: (i) instill commonsense reasoning abilities in PLMs and (ii) teach them to harmoniously utilize both textual and visual inputs. Examples of the Synthetic VQA dataset can be found in Figure 3.\nThe objective of this process is to construct VQA pairs (Q, A, I), where each pair includes a natural language question Q, a set of n answer choices \\(A = A_1, A_2, ..., A_n\\), including one ground-truth answer and n \u2013 1 distractors, along with an image I that corresponds to the question.\nSynthetic QA We first construct textual QA pairs from the KBs by following the recent work (Wang et al., 2023). Specifically, we transform the knowledge entities into the QA pairs through the conceptualized augmentation of the entities (Wang et al., 2023) with the pre-defined natural language templates (e.g., the relation of xWant is transformed to As a result, PersonX wanted to). This process results in textual synthetic QA pairs (Q, A).\nSynthetic VQA On the textual synthetic QA pairs, we input the textual question Q to the text-to-image model \\(M_{T2I}\\) to generate the visual counterpart I that depicts the scenarios described in each question. These generated images provide an additional layer of information, offering a visual context that enhances the reasoning ability based not only on textual descriptions but also on visual evidence. This augmentation leverages the strengths of visual imagery on language understanding (Gambrell and Bales, 1986; Dessalegn and Landau, 2013), potentially improving the robustness and accuracy of the model predictions.\nHowever, relying solely on the synthetic relationships between QA pairs and generated images can introduce challenges related to the alignment of visual content since machines often fail to generate well-aligned images with textual inputs (Feng et al., 2023). Therefore, we augment the Synthetic VQA pairs with the widely used Visual Commonsense Reasoning (VCR) dataset (Zellers et al., 2019). Each pair from this dataset consists of (Q, A, R, I), where R is a rationale for the correct answer; however, we omit R since our focus is on the QA pairs associated with relevant images. Additionally, to enrich the input and enhance visual comprehension for PLMs, we generate textual context information for each image using an image captioning model, which we prepend as a prefix to each Q."}, {"title": "Pre-training IMAGINE on Synthetic VQA", "content": "Based on the Synthetic VQA dataset, we integrate commonsense knowledge into the models. Since IMAGINE involves two distinct modalities (i.e., text and image), we introduce two separate objectives to select the best answer choice: Language Modeling (LM) and Image-Text Matching (ITM). To obtain the LM scores, we calculate the masked language modeling loss for the Transformer encoder-based model, formulated as:\n\\(S_{LM}(T) = \\frac{1}{m} \\sum_{t=1}^{m} log P(w_t | ... w_{t-1}, w_{t+1} ...).\\) \nFor the decoder-based model, we compute the auto-regressive language modeling loss, defined as:\n\\(S'_{LM}(T) = \\frac{1}{m} \\sum_{t=1}^{m} log P(w_t | w_1...w_{t-1}),\\)"}, {"title": "Inference from IMAGINE", "content": "For the zero-shot evaluation, we use the same strategy to compute the LM and ITM scores after synthesizing the image based on the question. Then we assemble two scores to derive the model's prediction after obtaining the probability distribution through softmax.\n\\(P(S) = softmax(S^{(1)}, S^{(2)}, ..., S^{(n)}),\\)\n\\(P(A|Q) = (1 - \\lambda) \\cdot P(S_{LM}) + \\lambda \\cdot P(S_I),\\)\nwhere \\(\\lambda\\) is an ensemble coefficient that controls the contributions between textual and visual features."}, {"title": "Experiments", "content": "In this section, we demonstrate the effectiveness of IMAGINE. Specifically, we conduct extensive experiments and analysis to answer the following research questions:\nQ1 (Generalizability) Does IMAGINE offer better zero-shot performance across a broad range of reasoning benchmarks? (\u00a74.2)\nQ2 (Multimodality) Does IMAGINE effectively integrate visual signals (imagination) with textual knowledge? (\u00a74.3, \u00a74.4)\nQ3 (Effectiveness) How effective are the components of IMAGINE in zero-shot commonsense reasoning? (\u00a74.5)"}, {"title": "Experimental Setup", "content": "Following the previous works on zero-shot reasoning (Ma et al., 2021; Yang et al., 2022), we evaluate our framework on commonsense reasoning tasks and science QA tasks to assess its generalizability 4. Specifically, we evaluate each baseline on the five reasoning benchmarks, including Abductive NLI (@NLI; Bhagavatula et al., 2020), CommonsenseQA (CSQA; Talmor et al., 2019), PhysicalIQA (PIQA; Bisk et al., 2020), SocialIQA (SIQA; Sap et al., 2019b), and Winogrande (WG; Sakaguchi et al., 2020). These datasets vary significantly in format (e.g., natural language inference, QA, pronoun resolution) and required knowledge (e.g., social and physical knowledge for SIQA and PIQA, respectively), enabling a comprehensive evaluation of a wide spectrum of reasoning capabilities. For science QA tasks, we assess each baseline on the four benchmarks, including QA via."}, {"title": "Main Results", "content": "Tables 1, 2, and 3 show the results for the commonsense reasoning tasks and the science question-answering tasks. Models based on IMAGINE reveal either superior or competitive performance on overall reasoning tasks. This demonstrates the effectiveness of IMAGINE and highlights the benefit of leveraging machine imagination for reasoning.\nIn particular, compared to zero-shot commonsense reasoning frameworks in commonsense reasoning tasks (Table 1), IMAGINE-DeBERTa-v3-L model surpasses the previous state-of-the-art by 0.9%p on average, and specifically by 4.1%p on the CSQA. This suggests that Synthetic VQA significantly enhances generalization performance in zero-shot commonsense reasoning. Comparison results with LLMs (Table 2) also shows that IMAGINE outperforms recent LLMs, including ChatGPT and GPT-4 (OpenAI, 2023). This result suggests the superior efficiency and effectiveness of IMAGINE's multimodal approach.\nIMAGINE also proves effective for science QA tasks (Table 3). Compared to the models with KBs and larger models, IMAGINE presents better or competitive reasoning performance. These results confirm the effectiveness of the machine imagination capabilities on science-related contexts. We also highlight the comparison results with Z-LaVI (Yang et al., 2022) that leverages imagination similar to ours. IMAGINE outperforms this method by a significant margin (18.5%p on average), underscoring the importance of the pre-training phase in effectively utilizing machine imagination."}, {"title": "Impact of Imagination on Model Inference", "content": "We analyze the inference results from the text-based model, CAR (Wang et al., 2023), and IMAGINE to confirm the impact of machine imagination on the model inference. The results are shown in Figure 4. We draw three major findings regarding the impact of imagination: (i) When the text contains limited commonsense knowledge, imagination indeed helps the model to correctly infer the answer (First row in the Figure), i.e., positive impact on predictions (ii) When the generated images only partially capture the context of the text query, imagination does not affect the inference results (Second row in the Figure). (iii) When images deviate from the real world, imagination can lead to incorrect inferences (Third row in the Figure). Specifically, we empirically observe that longer text queries often result in such cases.\nTo further assess how often images negatively impact model inference, we calculate the ratio of helpful imagination (i.e., imagination leading to correct reasoning) to harmful imagination (i.e., imagination leading to incorrect reasoning) across different commonsense reasoning benchmarks (Table 4 and 5). Our analysis shows that helpful imagination contributes more than harmful imagination, suggesting that imagination generally has a positive impact. However, we also observe that in certain cases, misaligned imagination can lead to reasoning errors.\nThese results suggest that incorporating a text-to-image model with better alignment capabilities could potentially mitigate the negative impacts of imagination. We provide more examples with the visualization of model attention in Appendix G."}, {"title": "Contributions of Synthetic VQA", "content": "To confirm the effectiveness of each component in Synthetic VQA, we evaluate the contribution of AbsAT and VCR. Table 6 presents the results on commonsense reasoning tasks. The model trained only with AbsAT (i.e., w/o VCR) shows superior performance on datasets that contain longer sequences and require complex knowledge (e.g., PIQA, SIQA). In contrast, the model trained only with VCR (i.e., w/o AbsAT) shows its strength on the dataset that contain simpler questions (aNLI, CSQA) which allows the better use of visual information. When combining these two components, the Synthetic VQA results in well-generalized reasoners across diverse reasoning tasks, demonstrating the complementary effect of each component."}, {"title": "Component Analysis on IMAGINE", "content": "IMAGINE employs two objectives (i.e., LM, ITM) to learn commonsense knowledge from different modalities. We perform ablations on these objectives to verify their contributions in enhancing zero-shot reasoning capabilities. Table 7 shows the ablation results. Notably, omitting the LM objective leads to a significant drop in performance, underscoring the crucial role of language understanding in commonsense reasoning. Furthermore, while ITM alone does not significantly impact reasoning effectiveness, combining ITM with LM results in improved reasoning performance. These findings suggest that integrating visual information in model optimization leads to better reasoning in commonsense situations.\nIMAGINE performs reasoning by ensembling LM and ITM scores. To investigate the contributions in scores obtained from these two different modalities, we evaluate each score independently. The results are presented in Table 8. We observe the lowest performance when evaluating only the ITM scores. However, ensembling LM scores with the ITM results in significant performance improvement across all tasks, even though the scores derived from images are much lower than those from text. This indicates that integrating machine-generated images can complement and enhance language-based reasoning abilities. More analysis on ensemble methods are in Appendix C.\nIMAGINE utilizes parallel adapters (He et al., 2022a) to alleviate the conflicts between the two objectives (i.e., LM, ITM) during the pre-training. In this study, we examine whether separating parameters through adapters for distinct modality objectives is truly effective. Table 9 presents the ablation results on adapters. We observe a significant decline in reasoning performance when adapters are removed. This suggests that direct training of PLMs with images adversely affects the acquisition of textual knowledge. One plausible explanation for this phenomenon is possibly related to catastrophic forgetting (Kirkpatrick et al., 2017), where the model loses previously acquired knowledge (i.e., textual knowledge inherent in PLMs). This highlights the effectiveness of adapters in maintaining the model's linguistic understanding when it learns from new modalities."}, {"title": "Comparison with VL models", "content": "We include the state-of-the-art language models as baselines (e.g., GPT-4, LLaMA2), as our focus is on enhancing language-based reasoning ability using visual signals. Nevertheless, we also provide results from recent powerful vision-language (VL) models (LLaVA-1.5 (Liu et al., 2023a), Instruct-BLIP with Vicuna-7B (Dai et al., 2023)) by feeding the generated images from our framework. The results in Table 10 indicate that these VL models struggle to reason accurately about commonsense questions. We suspect that this issue arises from VL models' tendency to focus on the image scene more than on textual inputs, as they are primarily trained to answer questions based on the entire image scene. The datasets we experiment with prioritize linguistic ability over vision-language grounding and require reasoning rooted in commonsense knowledge. As a result, VL models that are more focused on visual understanding may underperform in zero-shot commonsense reasoning tasks, where strong linguistic reasoning is crucial."}, {"title": "Conclusion", "content": "In this paper, we have proposed IMAGINE, a novel zero-shot commonsense reasoning framework that leverages visual signals to mitigate reporting bias in textual inputs. To steer IMAGINE in effectively utilizing visual information, we have created a large-scale Synthetic VQA dataset and optimized the model to use both textual and visual information. Our extensive experiments have shown that IMAGINE establishes new state-of-the-art results on zero-shot commonsense reasoning tasks compared to strong baselines (including large language models), demonstrating the efficacy of machine imagination. Moreover, the in-depth analysis clearly supports the strength of the proposed method by showing that the model tends to utilize visual information beneficially."}, {"title": "Limitations", "content": "We have demonstrated the efficacy of the machine imagination to improve zero-shot commonsense reasoning ability. However, we still have the following limitations:\nWhile machine imagination leads to performance improvement in PLMs, it necessitates additional computations for generating and processing visual signals. This limitation can be addressed by retrieving relevant images instead of synthesizing new ones, as demonstrated in previous work (Yang et al., 2022). We consider this approach a promising avenue for future research.\nIn this work, we apply IMAGINE to only intermediate-size models (300M to 790M), as one of our objectives is to see if the smaller models with machine imagination outperform LLMs on a broad range of commonsense reasoning tasks. This objective motivates us to apply our method to language models with less than 1B parameters. Additionally, from a practical perspective, the proposed method involves a pre-training phase to teach the joint use of multi-modal data. This process requires substantial computational costs to train larger models. However, we believe that IMAGINE can be effectively combined with LLMs, given that the reporting bias is an inherent issue in the pre-training corpus and not the models themselves. We plan to explore the scaling of machine imagination in our future research."}, {"title": "Appendix", "content": "We construct a Synthetic VQA dataset using AbstractATOMIC and VCR. First, we generate images using the questions from AbstractATOMIC. Since AbstractATOMIC consists only of text, we need to create images based on these questions. In this process, we standardize all the person names in the questions to \"Person\" and remove duplicate questions, resulting in approximately 20K images. To include more realistic images and commonsense questions corresponding to those images, we extract question-answer pairs from VCR images. However, most of these questions are directly related to the images, making it difficult to answer without them, which poses a challenge for LM-based training. To address this, we replace the person indices in the questions with gender-neutral names and generate captions for the images to use as prefixes for the questions. In addition, each QA pair from VCR has four answer candidates, while each pair from AbstractATOMIC has three candidates. To combine them, we match the number of answer choices by randomly discarding one distractor from VCR. The statistic of our dataset is provided in Table 11.\nTo construct the VQA pairs, we primarily use DALL-E 3-XL (Betker et al., 2023), a powerful image synthesis model. For generating images in the Synthetic VQA dataset, we first remove overly specific information, such as personal names, from the questions. Then, we generate images with a resolution of 384 \u00d7 384 using 50 inference steps. During the evaluation, we generate 512 \u00d7 512 images for each task based on the questions, maintaining the same number of inference steps. We use the CLIP-Large (Radford et al., 2021) model to extract image features. Following prior work, we use two power-ful PLMs as the backbone. We add Parallel Adapter (He et al., 2022a) with a reduction factor of 16 to each model and freeze all parameters except for the adapters. We follow the training settings of Ma et al. (2021) and Wang et al. (2023) to train Transformer decoder-based and encoder-based model for the in-depth comparison. We report our results derived from the ensemble score using the optimal ensemble weight for each task. All experiments are conducted using four NVIDIA A5000 GPUs. More details are presented in Table 12."}, {"title": "Ensemble Methods", "content": "To verify the effectiveness of our framework's multimodality approach, we train two unimodal models using different seeds on the Synthetic VQA dataset, utilizing only the text. We then ensemble the scores obtained from these two models. The results are presented in Table 13. While ensembling scores from single modalities (LM+LM) provides performance benefits, ensembling scores from two different modalities (LM+ITM), as done in IMAGINE, proves to be the most effective. This demonstrates that the multimodality approach plays a crucial role in enhancing zero-shot reasoning performance."}, {"title": "Impact of Image Quality", "content": "We aim to observe the changes in inference performance based on image quality by generating images of various qualities using three different methods. First, similar to our main experiment, we utilize the questions from the evaluation dataset to generate images with a resolution of 512 \u00d7 512 using both DALL-E 3-XL and the Latent Diffusion Model (LDM; Rombach et al., 2022), which has relatively lower image synthesis capabilities. Additionally, we generate images with a resolution of 384 \u00d7 384 using DALL-E 3-XL, following the same method used for creating the Synthetic VQA dataset."}, {"title": "IMAGINE with Decoder-based Model", "content": "We conducted experiments using GPT-2, a widely-used decoder-based generative language model, to verify the applicability to recent language models. We follow the settings of (Ma et al., 2021) to train to model on synthetic datasets."}, {"title": "Validation of Synthetic Dataset Quality", "content": "We evaluate the quality of the dataset by measuring the relevance between the question text and the machine-generated images. For this purpose, inspired by the CLIP scores (Hessel et al., 2021), we measure the relevance score between images and text using the CLIP model. A higher relevance score between the two modalities indicates that the image effectively captures the content of the text. As shown in Figure 4, images that are highly relevant to the questions can help to reason about the question.\nFirst, we measure the relevance of datasets containing two sets of real images (A-OKVQA, VCR) to establish a baseline. Then we compare these scores with those of the Synthetic VQA and the synthetic pairs of all evaluation datasets to determine the quality of the synthetic dataset. The results in Table 16 show that most datasets exhibit similar or even higher relevance scores compared to the datasets containing real images (A-OKVQA, VCR). In particular, for Synthetic VQA, we evaluate only the dataset extracted from AbstractATOMIC, which contains only machine-generated images, and found that it has relevance scores closest to those of the real-image datasets A-OKVQA and VCR. This demonstrates that our synthetic dataset has a quality comparable to that of the real VL dataset."}, {"title": "Visualization of Image Attention", "content": "We aim to visualize how the model utilizes specific parts of an image. The formula to compute contextualized visual features used for computing the ITM score calculation process is similar to the attention algorithm, allowing us to derive attention scores for each image patch. Based on these scores, we erase 100 image patches with the lowest scores to understand which parts the model focuses on. As shown in Figure 7, 8, and 9, each model tends to assign relatively high attention scores to objects related to the question in most cases, rather than using the image patches randomly. This is notable because the model can effectively capture the relationship between text and images using adapters, despite training with much less data compared to existing visual-language modeling studies (Li et al., 2023; Zhu et al., 2023). In addition, we observe that the DeBERTa-v3-Large model tends to focus more frequently on the correct parts than the RoBERTa-Large model. Figure 7 shows these cases clearly. This aligns with the result that the IMAGINE is more effective with DeBERTa-v3-Large, suggesting that a model with high generalization performance is also useful for learning new modalities."}, {"title": "Comparison of Inference Time", "content": "Since IMAGINE utilizes both images and text after generating images, inference may take longer. Nevertheless, to demonstrate the effectiveness of our methodology, we analyze the model in terms of inference time. The inference time and the number of parameters required to produce an answer vary depending on the setting, particularly the image quality. As shown in Appendix D, when generating images using the LDM model and then inferring the answer with the IMAGINE-ROBERTa-L framework, the average time taken is 4.5 seconds (image generation: 4 seconds, image processing: 0.2 seconds, text processing: 0.3 seconds). The total number of parameters used is 1.7 billion (LDM: 1 billion, CLIP: 428 million, RoBERTa: 362 million). This model achieves higher performance with significantly fewer parameters compared to the 7 billion parameter large language models shown in Table 2. Although the 7 billion parameter models have an average inference speed of 2.1 seconds, we believe this is justified by the superior performance of our model.\nAdditionally, our largest setting (IMAGINE-DeBERTa-v3-Large framework containing DALL-E 3-XL) takes a total of 21.5 seconds to infer an answer and has 4.6 billion parameters. This model can achieve higher performance than large language models with over 30 billion parameters. This suggests that our framework is a more effective alternative to simply increasing model size."}, {"title": "Versatility of IMAGINE", "content": "To confirm the versatility of IMAGINE, we measure the performance of IMAGINE not only on zero-shot commonsense reasoning but also on several tasks from the GLUE dataset (SST-2, RTE) (Wang et al., 2019).\nAs shown in the Table 17, the results indicate that the IMAGINE-DEBERTa-v3-L model achieves the highest performance across general tasks on average, suggesting that IMAGINE can indeed be a general approach to engage PLMs. Specifically, IMAGINE shows greater performance improvements in datasets where image information can be highly utilized, such as sentiment analysis (SST-2) compared to tasks involving natural language inference (RTE). This suggests that our visual imagination-based approach can actually enhance the general language understanding capabilities by providing additional information."}]}