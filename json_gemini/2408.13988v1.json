{"title": "Automatic Medical Report Generation: Methods and Applications", "authors": ["Li Guo", "Anas M. Tahir", "Dong Zhang", "Z. Jane Wang", "Rabab K. Ward"], "abstract": "The increasing demand for medical imaging has surpassed\nthe capacity of available radiologists, leading to diagnostic\ndelays and potential misdiagnoses. Artificial intelligence (AI)\ntechniques, particularly in automatic medical report genera-\ntion (AMRG), offer a promising solution to this dilemma.\nThis review comprehensively examines AMRG methods from\n2021 to 2024. It (i) presents solutions to primary challenges\nin this field, (ii) explores AMRG applications across vari-\nous imaging modalities, (iii) introduces publicly available\ndatasets, (iv) outlines evaluation metrics, (v) identifies tech-\nniques that significantly enhance model performance, and\n(vi) discusses unresolved issues and potential future research\ndirections. This paper aims to provide a comprehensive\nunderstanding of the existing literature and inspire valuable\nfuture research.", "sections": [{"title": "Introduction", "content": "Automatic medical report generation (AMRG) is an emerging research\narea in artificial intelligence (AI) within the medical field [1, 2]. It\nutilizes computer vision (CV) and natural language processing (NLP)\nto interpret medical images and generate descriptive, human-like reports.\nAMRG has been applied to various imaging modalities, including X-\nrays, computed tomography (CT) scans, magnetic resonance imaging\n(MRI), and ultrasound [3, 4, 5]. This technology has the potential to\nstreamline the diagnostic process, alleviate the workload on radiologists,\nand enhance diagnostic accuracy.\nTraditionally, the interpretation of medical images relies on trained\nradiologists, a labor-intensive and error-prone process [6, 7, 8, 9]. In\nthe US and UK, the number of radiologists is insufficient to meet the\ngrowing demand for imaging and diagnostics [10, 11]. In resource-poor\nregions, the scarcity of radiology services is even more severe [12, 13].\nThis shortage of radiologists leads to delays and backlogs in interpreting\nmedical images. In 2015, approximately 330,000 patients in the UK\nwaited more than 30 days for radiology reports [14]. Due to delayed re-\nports, some urgent images have to be reviewed by emergency physicians.\nHowever, the discernible interpretation differences between emergency\nphysicians and trained radiologists can lead to missed diagnoses and\nmisdiagnoses [15]. Additionally, reports written by professional radi-\nologists exhibit a 3-5% error rate and approximately 35% uncertainty\nrate [16, 17, 18]. As workloads increase, the probability of errors by\nradiologists also rises [19, 20]. For instance, an American doctor was\nsued after failing to detect a case of breast cancer due to reading too\nmany X-rays in one day [21]. AMRG addresses these issues by providing\na systematic approach to image interpretation, potentially improving\ndiagnostic efficiency and accuracy.\nIn recent years, deep learning has made significant progress in image\nanalysis, with convolutional neural networks (CNNs) and Transformers\nexcelling in high-precision lesion detection and classification of medical\nconditions [22, 23, 24]. NLP techniques translate visual information\nfrom medical images into natural language reports, covering imaging\nfindings, diagnostic conclusions, and recommendations, thereby achiev-\ning seamless image-to-text conversion [25, 26, 27]. Researchers have\ndeveloped various AMRG methods by combining CNNs, Transformers,\nand NLP in an encoder-decoder architecture [1, 28, 29].\nDespite these advancements, this field still faces numerous challenges.\nFirstly, bridging the modal gap between image input and text output is\na fundamental challenge for AMRG. Medical images contain complex\ninformation that must be accurately interpreted and translated into"}, {"title": "Problem Statement", "content": "The objective of AMRG is to train a model that can extract meaningful\nfeatures from medical images and generate descriptive text sequences\nthat accurately describe the medical conditions depicted in the images.\nThe primary objective function is the word-level cross-entropy loss,\nwhich measures the discrepancy between the predicted word probabilities\nand the actual words in the ground truth (GT) reports.\nGiven a medical image, the visual encoder extracts a sequence of"}, {"title": "Methods", "content": "In this section, we introduce various methods designed to address the\naforementioned challenges. First, we discuss techniques for bridging\nthe gap between image-text modalities (Section 3.1). Next, we present\nlesion-focused image encoding methods that enhance the model's ability\nto detect and emphasize clinically significant regions (Section 3.2). We\nthen detail approaches for enhancing the text decoder with additional"}, {"title": "Global Alignment", "content": "Global alignment is a method that aligns the entire image with the entire\nreport based on InfoNCE loss [42] and triplet loss [43]. InfoNCE loss is\nwell-suited for large datasets because it processes all negative pairs in\na batch. In contrast, triplet loss specializes in identifying fine-grained\ndifferences by focusing on individual negative samples at a time.\nThe infoNCE loss function creates a joint embedding space by\nmaximizing the cosine similarity between positive image-text pairs and\nminimizing it for negative pairs. This process closely aligns images\nand their corresponding reports. The CLIP framework [44], which\npioneers the use of InfoNCE loss for visual representation learning\nunder natural language supervision, is particularly beneficial for report\ngeneration. This approach ensures that each medical image is effectively\nsupervised by its paired report. Several studies have employed CLIP\nloss (InfoNCE loss) to successfully mitigate the modality discrepancies\nbetween radiographic images and clinical reports [45, 41, 46, 47, 48].\nSpecifically, given a batch of N pairs of image embeddings {I} and text\nembeddings {T}, the CLIP loss can be formulated as follows:\n$L_{IN}^{T}(I,T) = - \\frac{1}{N} \\sum_{i=1}^{N} \\log \\frac{\\exp(S(I_i, T_i)/\\tau_1)}{\\sum_{j=1}^{N} \\exp(S(I_i, T_j)/\\tau_1)}$\n$L_{IN}^{TI}(I,T) = - \\frac{1}{N} \\sum_{i=1}^{N} \\log \\frac{\\exp(S(T_i, I_i)/\\tau_1)}{\\sum_{j=1}^{N} \\exp(S(T_i, I_j)/\\tau_1)}$\n$L_{CL}(I,T) = \\frac{1}{2}(L_{IN}^{TI}(I,T) + L_{IN}^{TI}(I,T)),$\nwhere $\\tau_1$ is a temperature parameter that scales the logits and S(.,.)"}, {"title": "Local Alignment", "content": "Although global alignment is an effective and widely adopted method,\ncontrasting the entire image with the entire report can result in overlook-\ning fine-grained interactions between different modalities. To address\nthis limitation, researchers have introduced two local alignment strate-\ngies: sentence-region alignment and word-region alignment.\nSentence-region alignment matches specific image regions to cor-\nresponding sentences within a report. PhenotypeCLIP [58] employs\ncross-attention to generate sentence-based local textual and visual rep-\nresentations, replacing the global representations used in the InfoNCE\nloss (Equation 3) to enhance contrastive learning. Further refining\nthis approach, PRIOR [59] replaces the softmax function in the cross-\nattention mechanism with a sigmoid function, which generates a sparser\nmatrix and enhances computational efficiency. Moreover, PRIOR sub-\nstitutes the InfoNCE loss, traditionally used in report-to-image local\nalignment, with a loss function based on cosine similarity and asymmet-\nrical projection. This modification mitigates the risk of feature collapse,\nwhich results from the misclassification of positive image regions as\nnegative. Specifically, given a batch of N pairs of image embeddings\nI = {I1, I2, ..., IN } and report embeddings T = {T1, T2, ..., TN}, each im-\nage embedding I\u00a1 is composed of patches I\u2081 = {I, I2, ..., IV }, and each\nreport embedding Ti is composed of sentences T\u2081 = {T},T?,...,TU}.\nHere, V and U represent the number of image patches and sentences\nwithin a report, respectively. For each sentence u, the attention-based\nvisual representation is formulated as:\nC_v^I = \\sum_{v=1}^V \\sigma(\\frac{Q_l^I \\cdot K_v^I}{\\sqrt{D}})V_v^I\nSimilarly, for each image region v, the attention-based textual represen-\ntation is formulated as:\nC_v^R = \\sum_{u=1}^U \\sigma(\\frac{Q_l^R \\cdot K_v^R}{\\sqrt{D}})V_v^R,\nwhere QI, KI, VI, QR, KR, VR are learnable matrices, \u03c3(\u00b7) is the\nsigmoid function, and D is the dimension of embeddings. The new"}, {"title": "Intermediate Matrix", "content": "In addition to contrastive learning, another method to bridge the modal\ngap is the use of a learnable shared matrix to capture the alignment\nbetween images and texts. Chen et al. [64] propose an approach that\nmaps visual features and textual features into a unified intermediate\nspace. Specifically, given an embedding I = {I1, I2, ..., Iv } extracted\nfrom the image and an embedding W = {W1, W2, ..., Wt-1} extracted\nfrom the generated report, these embeddings are mapped to visual\nmemory responses R\u00b9 = {R1, R2, ..., Rv } and textual memory responses\nRW = {R1, R2, ..., Rt_1}. Both R\u00b9 and RW are derived from a shared\nmemory matrix M. Subsequently, R\u00b9 and RW are fed into the text\ndecoder to generate the next word at the time step t. The effectiveness\nof this method has been verified by studies from Qin et al. [65] and\nYou et al. [66].\nWang et al. [67] further improve this mapping method. They\nconcatenate the embeddings of image-report pairs with the same disease\nlabel and apply K-means clustering to initialize the memory matrix.\nMoreover, they integrate both the image and text embeddings I and\nW, along with visual and textual responses R\u00b9 and RW, and feed them\ninto the decoder to enrich the generated content. Additionally, they\nincorporate triplet contrastive loss (Equation 5) into the optimization\nprocess to enhance the alignment of the visual and textual memory\nresponses via explicit supervision signals. Similarly, Li et al. [52] also\nemploy triplet loss to align visual and textual features post-mapping\nand utilize a dual-gate mechanism to more intricately fuse visual and\ntextual features both before and after mapping."}, {"title": "Lesion-Focused Image Encoding", "content": "This section outlines three methods for enhancing the image encoder\nto focus on lesion areas and generate discriminative image representa-\ntions: (i) using a classification task for joint learning (Section 3.2.1), (ii)\nemploying pre-trained detection and segmentation networks as auxil-\niaries (Section 3.2.2), and (iii) modifying the internal structure of the\nimage encoder (Section 3.2.3). The simplified flowcharts of these three\nmethods are shown in Figure 4. The following subsections detail these\napproaches, elucidating how each method refines the encoder's capacity\nto identify and concentrate on clinically significant regions."}, {"title": "Disease Classification", "content": "Adopting the features extracted by the image encoder for multi-label\ndisease classification is an effective joint learning strategy to adapt the\nencoder for the report generation task [68, 69, 70, 47, 2, 71, 72, 57, 73,\n74]. This strategy enables the image encoder to focus on regions where\ndiseases are likely to occur and refine its ability to extract discriminative\nfeatures, which helps decode accurate text. As a result, it enhances the\nencoder's sensitivity to medically relevant areas and clinically significant\ndetails indicative of various diseases.\nNevertheless, due to the distinct operational mechanisms of CNNs\nand Transformers, their implementation approaches exhibit slight vari-\nations. For CNN-based image encoders, all features from the last\nconvolutional layer are used for classification [68, 69, 2], often coupled\nwith average pooling to achieve a global representation [70, 47]. This\napproach may result in image features that contain only high-level\ninformation for classification, while losing the low-level information nec-\nessary for generating descriptive text. Conversely, Transformer-based\nimage encoders utilize an independent [CLS] token to extract global\nfeatures by interacting with other image patch tokens [71, 72, 57]. Only\nthe [CLS] token is used for classification, which prevents the image\ntokens from being encoded too abstractly. Additionally, feeding the\nclassification results into the text decoder improves the quality of the\ngenerated reports. [68, 69, 47, 71, 72, 75, 74].\nA single [CLS] token may not accurately cover all diseases, similar\nto how a general practitioner's diagnosis may not be as precise as that\nof a specialist. METransformer [76] addresses this by concatenating\nmultiple expert tokens in the image encoder and using orthogonal loss\nto minimize overlap among these tokens, thereby encouraging them\nto capture complementary information. The model generates a report\nbased on each expert token and selects the best report through a voting\nstrategy."}, {"title": "Detection and Segmentation", "content": "In addition to using classification tasks to guide the image encoder\ntowards clinically relevant areas, researchers have proposed using pre-\ntrained segmentation or detection networks to explicitly assist the"}, {"title": "Internal Structure of Image Encoder", "content": "In addition to adopting a joint learning strategy and using pre-trained\nauxiliary networks, modifying the internal structure of the image encoder\ncan also enhance its focus on lesion areas. Two effective and widely\nused methods are cross-attention and high-order attention.\nThe cross-attention mechanism [81] assigns weights to image regions\nbased on their relevance to disease tags, thereby emphasizing the features\nof regions containing lesions. This method does not require disease\nannotations for each image, but rather a set of all disease tags [82, 68,\n83]. Specifically, the disease tag set is used as the query, and the image\nis used as the key and value. The dot product in the cross-attention\nmechanism can select disease tags related to the image content and\nenhance the features of regions containing these diseases.\nRecently, several studies have attempted to replace traditional first-\norder attention with X-linear attention [84] in Transformer-based image\nencoders [71, 76, 85, 86]. X-linear attention captures complex high-\norder interactions within medical images, leading to a more nuanced\nand comprehensive understanding of the images and more accurate\nlocalization of abnormalities. In detail, given a query Q \u2208 RDq, a set\nof keys K = {ki}1 and a set of values V = {vi}1, where ki \u2208 RDk"}, {"title": "Enhancing Text Decoder With Supplementary Information", "content": "This section presents three approaches for augmenting the text decoder\nwith supplementary information: (i) retrieving similarity reports (Sec-\ntion 3.3.1), (ii) leveraging memory (Section 3.3.2), and (iii) integrating\nknowledge graphs (Section 3.3.3). Each approach addresses specific\nchallenges, such as ensuring clinical consistency, alleviating privacy con-\ncerns, and building medical knowledge for better model comprehension.\nFigure 5 shows the flowcharts of the three methods."}, {"title": "Retrieve Similarity Reports", "content": "Given the limited diversity of diagnoses in medical reports, a large\nretrieval corpus can adequately cover the potential diagnoses of input\nimages. Some researchers have proposed using a retrieval-based ap-\nproach to generate new reports, with the primary advantage being the\nclinical consistency of the generated reports with manually written\nones [46, 56]. To elaborate, image and text encoders are trained using\nthe CLIP method, which produces higher similarity scores for paired\nimage-text examples and lower scores for unpaired ones. A corpus is\nthen constructed using the reports in the training set. During infer-\nence, the model retrieves the top K reports from the corpus with the\nhighest similarity scores to the input image and combines them into\nthe predicted report. However, since candidate selection is based on\nmaximizing similarity scores, the predicted report is prone to repeating\ninformation.\nPPKED [83] improves the basic retrieval-based approach by modify-\nfing the retrieval process and using a text decoder to generate reports\ninstead of merely combining retrieved candidates. The corpus is con-\nstructed using image-text pairs from the training set. During inference,\nthe system retrieves the top K images in the corpus that are most\nsimilar to the input image and uses their corresponding reports to\nenhance the image features. Finally, the text decoder generates the\nfinal report based on the enhanced image features, ensuring coherence\nand the absence of redundant content."}, {"title": "Memory", "content": "However, retrieving training data during inference raises concerns re-\ngarding the privacy of medical data. Some researchers have proposed\na solution by employing learnable memory to replace the corpus [70,\n59]. The memory stores features derived from the training data, rather\nthan the training data itself, thereby mitigating the risk of data leak-\nage. Yang et al. [70] use cross-attention to update the memory during\ntraining and to enhance image features during inference. Cheng et al.\n[59] adopt a more explicit approach to update the sentence-prototype\nmemory. During training, the sentence prototype most similar to the\ninput sentence is selected using cosine similarity, and the memory is\nupdated based on the L1 loss between the prototype and the input\nsentence.\nFurthermore, integrating memory into the text decoder is another\nmethod to enhance the quality of the generated reports [88, 71, 89,\n58]. This memory records fine-grained medical knowledge and historical\ninformation from previous generation processes, which is valuable for\ngenerating lengthy texts. One approach involves using the memory\nmatrices to augment the keys and values of the Transformer-based\ndecoder [88, 71]. Specifically, given a key K and value V, the memory-\naugment key and value are defined as K = [K, Mk] and V = [V, M\u039c\u03c5],\nwhere Mk and M\u2082 are learnable matrices, and [\u00b7, \u00b7] denotes concatenation.\nAnother approach utilizes a gate mechanism to update the memory\nand map it to the scale and offset parameters in layer normalization,\nthereby injecting the memory into the decoder [89, 58]."}, {"title": "Knowledge Graph", "content": "A more structured memory, in the form of a medical knowledge graph,\ncan group diseases according to organs or body parts. This is because\nabnormalities in the same body part often exhibit strong correlations\nand common features. Initially, a medical graph is designed based on\nprior knowledge from chest findings to cover common abnormalities\nand their relationships [73]. In this graph, disease keywords serve as\nnodes (V) and their relationships as edges (E), denoted as G = {V, E}.\nGraph convolution [90] is used to propagate information within the\ngraph, thereby enhancing the model's capacity to comprehend medical"}, {"title": "Refining Generated Reports", "content": "This section outlines methods designed to refine the accuracy and se-\nmantic coherence of generated medical reports. In particular, these\ntechniques are designed to guarantee that the generated content ac-\ncurately reflects essential medical insights that are critical for clinical\nreliability. Two pivotal approaches will be discussed: (i) the traceback\nmechanism (Section 3.4.1), which evaluates semantic fidelity, and (ii)\nreinforcement learning (Section 3.4.2), which correlates training goals"}, {"title": "Traceback Mechanism", "content": "Most medical report generation methods construct loss functions that\nevaluate the discrepancy between generated and GT reports at the word\nlevel (for further details, please refer to Equation 2). Consequently,\nmodels tend to predict frequently observed words in order to achieve a\nhigh overlap rate [95], which may result in the generation of clinically\nflawed reports. A high-quality generated report should also be seman-\ntically similar to the GT. To achieve this, some researchers propose\na traceback mechanism to control the semantic validity of generated\ncontent through self-assessment [51, 96, 97, 63]. This approach involves\ninputting the generated report T into a text encoder to extract semantic\nfeatures xt, and optimizing the model to ensure that xt is similar to the\nsemantic features xt* of the GT report T*. The process of the traceback\nmechanism is shown in Figure 6.\nA variety of techniques exist for measuring semantic loss, including\ncalculating the L2-norm distance [51] and the cosine similarity [96]\nbetween xt and xt*, as well as using a classifier to ensure that the disease"}, {"title": "Reinforcement Learning", "content": "In contrast to indirect semantic loss, some researchers use reinforcement\nlearning with NLP metrics as rewards to align training goals with final\nevaluation criteria [98, 51, 71, 99, 65, 60]. Specifically, the text decoder is\ntreated as \"agent\" that interacts with an external \u201cenvironment\" (visual\nand textual features). The network parameters, 0, define a \"policy\"\npe, that results in an \"action\" (the prediction of the next word). The\nCIDEr score is used as a reward r, which is calculated by comparing the\ngenerated sequence to the corresponding GT sequence. The objective\nof training is to minimize the negative expected reward:\nL(\u03b8) = \u2212Ews\u223cp\u03b8[r(w\u00b3)],\""}, {"title": "Dataset Limitations", "content": "Medical image-text paired datasets are constrained by two fundamental\nlimitations: noise and limited size. Noise arises from temporal informa-\ntion and false negatives, which can distort the training data and lead to\ninaccurate model predictions. Temporal information noise occurs when\nreports reference earlier images, which introduces inconsistencies that\nmodels struggle to interpret correctly. False negatives, on the other\nhand, arise in contrastive learning when similar reports are incorrectly"}, {"title": "Eliminating Noise", "content": "Although medical image-report datasets authorized by professionals are\ngenerally more accurate than general image-text datasets in terms of\nannotation, they still exhibit inherent noise, including temporal informa-\ntion and false negatives. Temporal information noise can be attributed\nto reports that reference earlier images. For instance, in a report,\n\u201cComparison made to prior study, there is again seen moderate\ncongestive heart failure with increased vascular cephalization, stable.\nThere are large bilateral pleural effusions but decreased since previ-\nous"}, {"title": "Large Language Model", "content": "To address the issue of the limited size of medical image-report datasets,\na feasible approach is to utilize a pre-trained large language model\n(LLM) as a text decoder. This method leverages the LLM's robust\nlanguage generation and zero-shot transfer capabilities, thereby reducing\nthe number of parameters that need to be trained from scratch. Li et al.\n[103] propose a trainable mapping module, Q-Former, to bridge the gap\nbetween a frozen image encoder and a frozen LLM. Specifically, they\nemploy a pre-trained Vision Transformer (ViT) as an image encoder to\nextract image features, which are then mapped to the text feature space\nby Q-Former. Subsequently, the frozen LLM serves as a text decoder\nto generate reports. MSMedCap [77] demonstrates that Q-Former is\neffective for handling limited medical image-report data.\nNevertheless, utilising a frozen LLM as a text decoder concurrently\nwith a frozen image encoder is not the optimal approach. Research\nindicates that fine-tuning both the image encoder and the mapping\nmodule when the LLM decoder is frozen can enhance the quality of\nthe generated reports [104]. Additionally, freezing the LLM may not\nbe the most effective strategy, as an LLM pre-trained on general data\nmay not be suitable for the medical domain. It is therefore generally\nrecommended to fine-tune the LLM on task-specific data. However, it\nshould be noted that fine-tuning an LLM requires a substantial amount\nof data, so directly fine-tuning an LLM with limited medical data may\nresult in suboptimal performance.\nLiu et al. [105] propose a coarse-to-fine decoding strategy to fine-\ntune an LLM on limited medical datasets in a bootstrapping manner.\nThey initially employ MiniGPT-4 [106] to generate a coarse report\nand then use the coarse report as a prompt, along with the image\nfeatures, to input into the decoder of MiniGPT-4 again to generate the\nfinal refined report. Additionally, pseudo self-attention [107] is another\nmethod for fine-tuning an LLM on medical data [79, 9]. This approach\nintroduces new parameters solely within the self-attention block, while\nother parameters of the Transformer are initialized with pre-trained\nvalues. Given an image feature X and a hidden state Y, the pseudo\nself-attention is formulated as follows:\nPSA(X,Y) = softmax((YW)YT\\frac{XU_k}{\\sqrt{\\frac{XU_v}{YW_k}}}{\\sqrt{YW_v}})\nwhere Uk, Uv are new parameters, and Wq, Wk, Wu are parameters\nfrom pre-trained model. Fine-tuning an LLM with pseudo self-attention\nresults in minimal changes to the pre-trained LLM's parameters, thereby\nmaintaining its text generation capabilities [107]."}, {"title": "Semi-Supervised and Unsupervised Learning", "content": "To address the limited size of medical image-text paired datasets, some\nresearchers have explored semi-supervised and unsupervised learning\nmethods to expand the training dataset. The RAMT model [91] employs\na student-teacher network to train in a semi-supervised manner using\n25% paired data and 75% unpaired image data. The student and teacher\nnetworks share the same structure but have distinct parameters. During\nthe training phase, the teacher network parameters are updated by the\nexponential moving average (EMA) of the student network parameters.\nDifferent noises are added to the input images, which are then fed\ninto the student and teacher networks, respectively. The output of the\nteacher network serves as supervision for the student network.\nHowever, the semi-supervised method still requires some images\nwith corresponding reports. KGAE [88] addresses this limitation by\nutilizing unsupervised learning. The model employs a pre-constructed\nknowledge graph G as a shared latent space to bridge the gap between\nimage and text representations. Given an input image I and an input\nreport R, the graph G maps them into the same latent space, G\u2081 and\nGR. For unsupervised learning, the image encoder and text decoder\nare trained separately. To train the image encoder, G\u2081 and GR jointly\nimplement disease classification, ensuring they form a common latent\nspace. To train the text decoder, the report R is reconstructed from\nGR, following the process R \u2192 GR \u2192 R. Additionally, KGAE can be\napplied in semi-supervised and supervised settings. In these settings,\nas well as for inference, the pipeline follows I \u2192 G\u2081 \u2192 R.\nIn a more recent study, Hirsch et al. [108] refine the unsuper-\nvised method, achieving higher accuracy than KGAE. The method\nemploys cycle consistency to ensure that cross-modal mappings re-\ntain information. Cross-modal mappings include image-to-report (I2R)\nand report-to-image (R2I). The objective of cycle consistency is to\nminimise the differences between zi and R2I(I2R(zi)), as well as zr\nand I2R(R2I(zr)), where zi and zr represent image and text repre-"}, {"title": "Applications", "content": "In this section, we introduce the applications of AMRG across various\nmedical imaging modalities, including chest radiography, computed\ntomography (CT), magnetic resonance imaging (MRI), ultrasound,\nophthalmic imaging, endoscopy, surgical scene, and pathological imaging.\nThese modalities are crucial for diagnosing a broad spectrum of medical\nconditions.\nChest Radiography: In recent years, a significant amount of\nresearch focused on chest radiography report generation (refer to Table\n1). This is largely due to the availability of large and publicly accessible\ndatasets such as MIMIC-CXR [36] and IU X-ray [37], which contain\nextensive collections of annotated images and paired reports. The\navailability of these datasets enables the models to effectively learn the\nintricate relationships between visual features and textual descriptions.\nThe application of AMRG in radiography offers several significant\nbenefits. First, it can significantly reduce the radiologist's workload by\nautomating the initial draft of the report, allowing them to focus on\nmore complex and nuanced cases [9, 109, 28, 110, 111]. Second, these\nmodels can improve diagnostic consistency and reduce inter-observer\nvariability by applying standardized criteria and guidelines in the report\ngeneration process [112]. Third, in regions with limited access to\nexperienced radiologists, AMRG models can provide essential diagnostic\nsupport, ensuring timely and accurate medical care for patients [113].\nFinally, these models can support large-scale screening programs by\nrapidly processing and generating reports for large volumes of X-ray\nimages, thereby facilitating the early detection of diseases.\n3D Imaging: CT and MRI provide detailed, three-dimensional"}, {"title": "Public Dataset", "content": "In this section, we introduce several image-report datasets used for\nAMRG. All discussed datasets are publicly available and privacy-safe,\nmeaning they are free of any patient-identifying information. The two\nbenchmark datasets (Section 5.1) are widely used and serve as standards\nfor performance comparison in most AMRG studies. Additionally, other\ndatasets (Section 5.2) have been created to address specific needs, such\nas particular languages and imaging modalities."}, {"title": "Benchmark Datasets", "content": "IU-Xray: The Indiana University Chest X-ray dataset (IU-Xray)\n[37], also known as the OpenI dataset, was released in 2016. This\ndataset was sourced from two large hospital systems within the Indiana\nNetwork for Patient Care database. It comprises 7,470 chest radiographs\n(including both frontal and lateral views) and 3,955 corresponding\nnarrative reports from 3,955 patients. Each report includes two primary\nsections: findings, which provide a detailed natural language description\nof the significant aspects in the image, and impression, which offer\na concise summary of the most immediately relevant findings. The\n3,955 studies are divided into 2,470 abnormal cases and 1,485 normal\ncases. Disease labels were extracted from the reports either manually or\nautomatically using Medical Subject Headings (MeSH) [122], Radiology\nLexicon (RadLex) [123], and the Medical Text Indexer (MTI) [124]. The\nten most frequent disease tags are cardiomegaly, pulmonary atelectasis,\ncalcified granuloma, tortuous aorta, hypoinflated lung, lung base opacity,\npleural effusion, lung hyperinflation, lung cicatrix, and lung calcinosis.\nSince the dataset does not have an official split, the common practice is\nto randomly divide it into training, validation, and test sets in a 7:1:2\nratio.\nMIMIC-CXR: The Medical Information Mart for Intensive Care\nChest X-ray (MIMIC-CXR) [36] is the largest public medical image-\nreport dataset. It includes imaging studies from 65,379 patients from the\nBeth Israel Deaconess Medical Center Emergency Department, collected\nbetween 2011 and 2016. The dataset includes 377,110 chest radiographs\n(including frontal and lateral views) and 227,835 corresponding reports,\nmost of which include findings and impression sections. Each report\nis associated with to one or more images. On average, 3.5 reports\nfrom different time periods are collected for each patient, providing\nlongitudinal data that allows researchers to reference previous images.\nThe dataset is officially split into training, validation, and test sets,\nwhich improves reproducibility. Specifically, the training set contains\n368,960 images and 222,758 reports, the validation set contains 2,991\nimages and 1,808 reports, and the test set contains 5,159 images and\n3,269 reports.\nThe images were originally stored in DICOM format, but a JPEG\nversion (MIMIC-CXR-JPG) [125] was also created to reduce storage\nsize. In addition, 14 structured disease labels were extracted from the\nreports using NegBio [126] and Chexpert [127], including atelectasis, car-\ndiomegaly, consolidation, edema, enlarged cardiomediastinum, fracture,\nlung lesion, lung opacity, pleural effusion, pneumonia, pneumothorax,"}, {"title": "Other Datasets", "content": "In addition to the benchmark dataset of chest radiographs with English\nreports", "communities.\nPadchest": "The Pathology Detection in Chest Radiographs (Pad-\nchest) [129", "views": "npostero-anterior (PA), lateral, AP-horizontal, AP-vertical, costal, and\npediatric. All"}]}