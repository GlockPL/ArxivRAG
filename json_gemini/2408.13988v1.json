{"title": "Automatic Medical Report Generation: Methods and Applications", "authors": ["Li Guo", "Anas M. Tahir", "Dong Zhang", "Z. Jane Wang", "Rabab K. Ward"], "abstract": "The increasing demand for medical imaging has surpassed\nthe capacity of available radiologists, leading to diagnostic\ndelays and potential misdiagnoses. Artificial intelligence (AI)\ntechniques, particularly in automatic medical report genera-\ntion (AMRG), offer a promising solution to this dilemma.\nThis review comprehensively examines AMRG methods from\n2021 to 2024. It (i) presents solutions to primary challenges\nin this field, (ii) explores AMRG applications across vari-\nous imaging modalities, (iii) introduces publicly available\ndatasets, (iv) outlines evaluation metrics, (v) identifies tech-\nniques that significantly enhance model performance, and\n(vi) discusses unresolved issues and potential future research\ndirections. This paper aims to provide a comprehensive\nunderstanding of the existing literature and inspire valuable\nfuture research.", "sections": [{"title": "Introduction", "content": "Automatic medical report generation (AMRG) is an emerging research\narea in artificial intelligence (AI) within the medical field [1, 2]. It\nutilizes computer vision (CV) and natural language processing (NLP)"}, {"title": "Problem Statement", "content": "The objective of AMRG is to train a model that can extract meaningful\nfeatures from medical images and generate descriptive text sequences\nthat accurately describe the medical conditions depicted in the images.\nThe primary objective function is the word-level cross-entropy loss,\nwhich measures the discrepancy between the predicted word probabilities\nand the actual words in the ground truth (GT) reports.\nGiven a medical image, the visual encoder extracts a sequence of"}, {"title": "Methods", "content": "In this section, we introduce various methods designed to address the\naforementioned challenges. First, we discuss techniques for bridging\nthe gap between image-text modalities (Section 3.1). Next, we present\nlesion-focused image encoding methods that enhance the model's ability\nto detect and emphasize clinically significant regions (Section 3.2). We\nthen detail approaches for enhancing the text decoder with additional"}, {"title": "Bridging the Gap Between Modalities", "content": "Bridging the gap between image and text modalities is crucial for\nmedical report generation. This section introduces three key methods\nto address this challenge: (i) global alignment (Section 3.1.1), (ii)\nlocal alignment (Section 3.1.2), and (iii) intermediate matrix alignment"}, {"title": "Global Alignment", "content": "Global alignment is a method that aligns the entire image with the entire\nreport based on InfoNCE loss [42] and triplet loss [43]. InfoNCE loss is\nwell-suited for large datasets because it processes all negative pairs in\na batch. In contrast, triplet loss specializes in identifying fine-grained\ndifferences by focusing on individual negative samples at a time.\nThe infoNCE loss function creates a joint embedding space by\nmaximizing the cosine similarity between positive image-text pairs and\nminimizing it for negative pairs. This process closely aligns images\nand their corresponding reports. The CLIP framework [44], which\npioneers the use of InfoNCE loss for visual representation learning\nunder natural language supervision, is particularly beneficial for report\ngeneration. This approach ensures that each medical image is effectively\nsupervised by its paired report. Several studies have employed CLIP\nloss (InfoNCE loss) to successfully mitigate the modality discrepancies\nbetween radiographic images and clinical reports [45, 41, 46, 47, 48].\nSpecifically, given a batch of N pairs of image embeddings {I} and text\nembeddings {T}, the CLIP loss can be formulated as follows:\n$L_{CL}^{I\\rightarrow T}(I,T) = -\\frac{1}{N} \\sum_{i=1}^{N} log \\frac{exp(S(I_i, T_i)/\\tau_1)}{\\sum_{j=1}^{N} exp(S(I_i, T_j)/\\tau_1)}$\n$L_{CL}^{T\\rightarrow I}(I,T) = -\\frac{1}{N} \\sum_{i=1}^{N} log \\frac{exp(S(T_i, I_i)/\\tau_1)}{\\sum_{j=1}^{N} exp(S(T_i, I_j)/\\tau_1)}$\n$L_{CL}(I, T) = \\frac{1}{2}(L_{CL}^{I\\rightarrow T}(I, T) + L_{CL}^{T\\rightarrow I}(I,T))$, (3)\nwhere t\u2081 is a temperature parameter that scales the logits and S(.,.)"}, {"title": "Local Alignment", "content": "Although global alignment is an effective and widely adopted method,\ncontrasting the entire image with the entire report can result in overlook-\ning fine-grained interactions between different modalities. To address\nthis limitation, researchers have introduced two local alignment strate-\ngies: sentence-region alignment and word-region alignment.\nSentence-region alignment matches specific image regions to cor-\nresponding sentences within a report. PhenotypeCLIP [58] employs\ncross-attention to generate sentence-based local textual and visual rep-\nresentations, replacing the global representations used in the InfoNCE\nloss (Equation 3) to enhance contrastive learning. Further refining\nthis approach, PRIOR [59] replaces the softmax function in the cross-\nattention mechanism with a sigmoid function, which generates a sparser\nmatrix and enhances computational efficiency. Moreover, PRIOR sub-\nstitutes the InfoNCE loss, traditionally used in report-to-image local\nalignment, with a loss function based on cosine similarity and asymmet-\nrical projection. This modification mitigates the risk of feature collapse,\nwhich results from the misclassification of positive image regions as\nnegative. Specifically, given a batch of N pairs of image embeddings\nI = {I1, I2, ..., IN } and report embeddings T = {T1, T2, ..., TN}, each im-\nage embedding I\u00a1 is composed of patches I\u2081 = {I, I2, ..., IV }, and each\nreport embedding Ti is composed of sentences T\u2081 = {T},T?,...,TU}.\nHere, V and U represent the number of image patches and sentences\nwithin a report, respectively. For each sentence u, the attention-based\nvisual representation is formulated as:\n$C_u^V = \\sum_{v=1}^{V} \\sigma(\\frac{Q_i^I T_v K_i^I}{\\sqrt{D}})V_i^I$, (6)\nSimilarly, for each image region v, the attention-based textual represen-\ntation is formulated as:\n$V_v^R = \\sum_{u=1}^{U} \\sigma(\\frac{Q_i^R T_u K_i^R}{\\sqrt{D}})V_u^R$, (7)\nwhere QI, KI, VI, QR, KR, VR are learnable matrices, \u03c3(\u00b7) is the\nsigmoid function, and D is the dimension of embeddings. The new"}, {"title": "Intermediate Matrix", "content": "In addition to contrastive learning, another method to bridge the modal\ngap is the use of a learnable shared matrix to capture the alignment\nbetween images and texts. Chen et al. [64] propose an approach that\nmaps visual features and textual features into a unified intermediate\nspace. Specifically, given an embedding I = {I1, I2, ..., Iv } extracted\nfrom the image and an embedding W = {W1, W2, ..., Wt-1} extracted\nfrom the generated report, these embeddings are mapped to visual\nmemory responses R\u00b9 = {R1, R2, ..., Rv } and textual memory responses\nRW = {R1, R2, ..., Rt_1}. Both R\u00b9 and RW are derived from a shared\nmemory matrix M. Subsequently, R\u00b9 and RW are fed into the text\ndecoder to generate the next word at the time step t. The effectiveness\nof this method has been verified by studies from Qin et al. [65] and\nYou et al. [66]."}, {"title": "Lesion-Focused Image Encoding", "content": "This section outlines three methods for enhancing the image encoder\nto focus on lesion areas and generate discriminative image representa-\ntions: (i) using a classification task for joint learning (Section 3.2.1), (ii)\nemploying pre-trained detection and segmentation networks as auxil-\niaries (Section 3.2.2), and (iii) modifying the internal structure of the\nimage encoder (Section 3.2.3). The simplified flowcharts of these three\nmethods are shown in Figure 4. The following subsections detail these\napproaches, elucidating how each method refines the encoder's capacity\nto identify and concentrate on clinically significant regions."}, {"title": "Disease Classification", "content": "Adopting the features extracted by the image encoder for multi-label\ndisease classification is an effective joint learning strategy to adapt the\nencoder for the report generation task [68, 69, 70, 47, 2, 71, 72, 57, 73,\n74]. This strategy enables the image encoder to focus on regions where\ndiseases are likely to occur and refine its ability to extract discriminative\nfeatures, which helps decode accurate text. As a result, it enhances the\nencoder's sensitivity to medically relevant areas and clinically significant\ndetails indicative of various diseases.\nNevertheless, due to the distinct operational mechanisms of CNNs\nand Transformers, their implementation approaches exhibit slight vari-\nations. For CNN-based image encoders, all features from the last\nconvolutional layer are used for classification [68, 69, 2], often coupled\nwith average pooling to achieve a global representation [70, 47]. This\napproach may result in image features that contain only high-level\ninformation for classification, while losing the low-level information nec-\nessary for generating descriptive text. Conversely, Transformer-based\nimage encoders utilize an independent [CLS] token to extract global\nfeatures by interacting with other image patch tokens [71, 72, 57]. Only\nthe [CLS] token is used for classification, which prevents the image\ntokens from being encoded too abstractly. Additionally, feeding the\nclassification results into the text decoder improves the quality of the\ngenerated reports. [68, 69, 47, 71, 72, 75, 74].\nA single [CLS] token may not accurately cover all diseases, similar\nto how a general practitioner's diagnosis may not be as precise as that\nof a specialist. METransformer [76] addresses this by concatenating\nmultiple expert tokens in the image encoder and using orthogonal loss\nto minimize overlap among these tokens, thereby encouraging them\nto capture complementary information. The model generates a report\nbased on each expert token and selects the best report through a voting\nstrategy."}, {"title": "Detection and Segmentation", "content": "In addition to using classification tasks to guide the image encoder\ntowards clinically relevant areas, researchers have proposed using pre-\ntrained segmentation or detection networks to explicitly assist the"}, {"title": "Internal Structure of Image Encoder", "content": "In addition to adopting a joint learning strategy and using pre-trained\nauxiliary networks, modifying the internal structure of the image encoder\ncan also enhance its focus on lesion areas. Two effective and widely\nused methods are cross-attention and high-order attention.\nThe cross-attention mechanism [81] assigns weights to image regions\nbased on their relevance to disease tags, thereby emphasizing the features\nof regions containing lesions. This method does not require disease\nannotations for each image, but rather a set of all disease tags [82, 68,\n83]. Specifically, the disease tag set is used as the query, and the image\nis used as the key and value. The dot product in the cross-attention\nmechanism can select disease tags related to the image content and\nenhance the features of regions containing these diseases.\nRecently, several studies have attempted to replace traditional first-\norder attention with X-linear attention [84] in Transformer-based image\nencoders [71, 76, 85, 86]. X-linear attention captures complex high-\norder interactions within medical images, leading to a more nuanced\nand comprehensive understanding of the images and more accurate\nlocalization of abnormalities. In detail, given a query Q \u2208 RDq, a set\nof keys K = {ki}1 and a set of values V = {vi}1, where ki \u2208 RDk"}, {"title": "Enhancing Text Decoder With Supplementary Information", "content": "This section presents three approaches for augmenting the text decoder\nwith supplementary information: (i) retrieving similarity reports (Sec-\ntion 3.3.1), (ii) leveraging memory (Section 3.3.2), and (iii) integrating\nknowledge graphs (Section 3.3.3). Each approach addresses specific\nchallenges, such as ensuring clinical consistency, alleviating privacy con-\ncerns, and building medical knowledge for better model comprehension.\nFigure 5 shows the flowcharts of the three methods."}, {"title": "Retrieve Similarity Reports", "content": "Given the limited diversity of diagnoses in medical reports, a large\nretrieval corpus can adequately cover the potential diagnoses of input\nimages. Some researchers have proposed using a retrieval-based ap-\nproach to generate new reports, with the primary advantage being the\nclinical consistency of the generated reports with manually written\nones [46, 56]. To elaborate, image and text encoders are trained using\nthe CLIP method, which produces higher similarity scores for paired\nimage-text examples and lower scores for unpaired ones. A corpus is\nthen constructed using the reports in the training set. During infer-\nence, the model retrieves the top K reports from the corpus with the\nhighest similarity scores to the input image and combines them into\nthe predicted report. However, since candidate selection is based on\nmaximizing similarity scores, the predicted report is prone to repeating\ninformation.\nPPKED [83] improves the basic retrieval-based approach by modify-\nfying the retrieval process and using a text decoder to generate reports\ninstead of merely combining retrieved candidates. The corpus is con-\nstructed using image-text pairs from the training set. During inference,\nthe system retrieves the top K images in the corpus that are most\nsimilar to the input image and uses their corresponding reports to\nenhance the image features. Finally, the text decoder generates the\nfinal report based on the enhanced image features, ensuring coherence\nand the absence of redundant content."}, {"title": "Memory", "content": "However, retrieving training data during inference raises concerns re-\ngarding the privacy of medical data. Some researchers have proposed\na solution by employing learnable memory to replace the corpus [70,\n59]. The memory stores features derived from the training data, rather\nthan the training data itself, thereby mitigating the risk of data leak-\nage. Yang et al. [70] use cross-attention to update the memory during\ntraining and to enhance image features during inference. Cheng et al.\n[59] adopt a more explicit approach to update the sentence-prototype\nmemory. During training, the sentence prototype most similar to the\ninput sentence is selected using cosine similarity, and the memory is\nupdated based on the L1 loss between the prototype and the input\nsentence.\nFurthermore, integrating memory into the text decoder is another\nmethod to enhance the quality of the generated reports [88, 71, 89,\n58]. This memory records fine-grained medical knowledge and historical\ninformation from previous generation processes, which is valuable for\ngenerating lengthy texts. One approach involves using the memory\nmatrices to augment the keys and values of the Transformer-based\ndecoder [88, 71]. Specifically, given a key K and value V, the memory-\naugment key and value are defined as K = [K, Mk] and V = [V, M\u039c\u03c5],\nwhere Mk and M\u2082 are learnable matrices, and [\u00b7, \u00b7] denotes concatenation.\nAnother approach utilizes a gate mechanism to update the memory\nand map it to the scale and offset parameters in layer normalization,\nthereby injecting the memory into the decoder [89, 58]."}, {"title": "Knowledge Graph", "content": "A more structured memory, in the form of a medical knowledge graph,\ncan group diseases according to organs or body parts. This is because\nabnormalities in the same body part often exhibit strong correlations\nand common features. Initially, a medical graph is designed based on\nprior knowledge from chest findings to cover common abnormalities\nand their relationships [73]. In this graph, disease keywords serve as\nnodes (V) and their relationships as edges (E), denoted as G = {V, E}.\nGraph convolution [90] is used to propagate information within the\ngraph, thereby enhancing the model's capacity to comprehend medical"}, {"title": "Refining Generated Reports", "content": "This section outlines methods designed to refine the accuracy and se-\nmantic coherence of generated medical reports. In particular, these\ntechniques are designed to guarantee that the generated content ac-\ncurately reflects essential medical insights that are critical for clinical\nreliability. Two pivotal approaches will be discussed: (i) the traceback\nmechanism (Section 3.4.1), which evaluates semantic fidelity, and (ii)\nreinforcement learning (Section 3.4.2), which correlates training goals"}, {"title": "Traceback Mechanism", "content": "Most medical report generation methods construct loss functions that\nevaluate the discrepancy between generated and GT reports at the word\nlevel (for further details, please refer to Equation 2). Consequently,\nmodels tend to predict frequently observed words in order to achieve a\nhigh overlap rate [95], which may result in the generation of clinically\nflawed reports. A high-quality generated report should also be seman-\ntically similar to the GT. To achieve this, some researchers propose\na traceback mechanism to control the semantic validity of generated\ncontent through self-assessment [51, 96, 97, 63]. This approach involves\ninputting the generated report T into a text encoder to extract semantic\nfeatures xt, and optimizing the model to ensure that xt is similar to the\nsemantic features xt* of the GT report T*. The process of the traceback\nmechanism is shown in Figure 6.\nA variety of techniques exist for measuring semantic loss, including\ncalculating the L2-norm distance [51] and the cosine similarity [96]\nbetween xt and xt*, as well as using a classifier to ensure that the disease"}, {"title": "Reinforcement Learning", "content": "In contrast to indirect semantic loss, some researchers use reinforcement\nlearning with NLP metrics as rewards to align training goals with final\nevaluation criteria [98, 51, 71, 99, 65, 60]. Specifically, the text decoder is\ntreated as \"agent\" that interacts with an external \u201cenvironment\" (visual\nand textual features). The network parameters, 0, define a \"policy\"\npe, that results in an \"action\" (the prediction of the next word). The\nCIDEr score is used as a reward r, which is calculated by comparing the\ngenerated sequence to the corresponding GT sequence. The objective\nof training is to minimize the negative expected reward:\n$L(0) = -E_{ws~pe} [r(w\u00b3)]$, (15)"}, {"title": "Dataset Limitations", "content": "Medical image-text paired datasets are constrained by two fundamental\nlimitations: noise and limited size. Noise arises from temporal informa-\ntion and false negatives, which can distort the training data and lead to\ninaccurate model predictions. Temporal information noise occurs when\nreports reference earlier images, which introduces inconsistencies that\nmodels struggle to interpret correctly. False negatives, on the other\nhand, arise in contrastive learning when similar reports are incorrectly"}, {"title": "Eliminating Noise", "content": "Although medical image-report datasets authorized by professionals are\ngenerally more accurate than general image-text datasets in terms of\nannotation, they still exhibit inherent noise, including temporal informa-\ntion and false negatives. Temporal information noise can be attributed\nto reports that reference earlier images. For instance, in a report,\n\u201cComparison made to prior study, there is again seen moderate\ncongestive heart failure with increased vascular cephalization, stable.\nThere are large bilateral pleural effusions but decreased since previ-\nous\", the bolded words relate to earlier images, yet the current image"}, {"title": "Large Language Model", "content": "To address the issue of the limited size of medical image-report datasets,\na feasible approach is to utilize a pre-trained large language model\n(LLM) as a text decoder. This method leverages the LLM's robust\nlanguage generation and zero-shot transfer capabilities, thereby reducing\nthe number of parameters that need to be trained from scratch. Li et al.\n[103] propose a trainable mapping module, Q-Former, to bridge the gap\nbetween a frozen image encoder and a frozen LLM. Specifically, they\nemploy a pre-trained Vision Transformer (ViT) as an image encoder to\nextract image features, which are then mapped to the text feature space\nby Q-Former. Subsequently, the frozen LLM serves as a text decoder\nto generate reports. MSMedCap [77] demonstrates that Q-Former is\neffective for handling limited medical image-report data.\nNevertheless, utilising a frozen LLM as a text decoder concurrently\nwith a frozen image encoder is not the optimal approach. Research\nindicates that fine-tuning both the image encoder and the mapping\nmodule when the LLM decoder is frozen can enhance the quality of\nthe generated reports [104]. Additionally, freezing the LLM may not\nbe the most effective strategy, as an LLM pre-trained on general data\nmay not be suitable for the medical domain. It is therefore generally\nrecommended to fine-tune the LLM on task-specific data. However, it\nshould be noted that fine-tuning an LLM requires a substantial amount\nof data, so directly fine-tuning an LLM with limited medical data may\nresult in suboptimal performance.\nLiu et al. [105] propose a coarse-to-fine decoding strategy to fine-\ntune an LLM on limited medical datasets in a bootstrapping manner.\nThey initially employ MiniGPT-4 [106] to generate a coarse report\nand then use the coarse report as a prompt, along with the image\nfeatures, to input into the decoder of MiniGPT-4 again to generate the\nfinal refined report. Additionally, pseudo self-attention [107] is another\nmethod for fine-tuning an LLM on medical data [79, 9]. This approach\nintroduces new parameters solely within the self-attention block, while\nother parameters of the Transformer are initialized with pre-trained\nvalues. Given an image feature X and a hidden state Y, the pseudo\nself-attention is formulated as follows:\n$PSA(X,Y) = softmax((\\frac{XU_q}{YW_q})^T \\frac{XU_k}{YW_k})$ (18)"}, {"title": "Semi-Supervised and Unsupervised Learning", "content": "To address the limited size of medical image-text paired datasets, some\nresearchers have explored semi-supervised and unsupervised learning\nmethods to expand the training dataset. The RAMT model [91] employs\na student-teacher network to train in a semi-supervised manner using\n25% paired data and 75% unpaired image data. The student and teacher\nnetworks share the same structure but have distinct parameters. During\nthe training phase, the teacher network parameters are updated by the\nexponential moving average (EMA) of the student network parameters.\nDifferent noises are added to the input images, which are then fed\ninto the student and teacher networks, respectively. The output of the\nteacher network serves as supervision for the student network.\nHowever, the semi-supervised method still requires some images\nwith corresponding reports. KGAE [88] addresses this limitation by\nutilizing unsupervised learning. The model employs a pre-constructed\nknowledge graph G as a shared latent space to bridge the gap between\nimage and text representations. Given an input image I and an input\nreport R, the graph G maps them into the same latent space, G\u2081 and\nGR. For unsupervised learning, the image encoder and text decoder\nare trained separately. To train the image encoder, G\u2081 and GR jointly\nimplement disease classification, ensuring they form a common latent\nspace. To train the text decoder, the report R is reconstructed from\nGR, following the process R \u2192 GR \u2192 R. Additionally, KGAE can be\napplied in semi-supervised and supervised settings. In these settings,\nas well as for inference, the pipeline follows I \u2192 G\u2081 \u2192 R.\nIn a more recent study, Hirsch et al. [108] refine the unsuper-\nvised method, achieving higher accuracy than KGAE. The method\nemploys cycle consistency to ensure that cross-modal mappings re-\ntain information. Cross-modal mappings include image-to-report (I2R)\nand report-to-image (R2I). The objective of cycle consistency is to\nminimise the differences between zi and R2I(I2R(zi)), as well as zr\nand I2R(R2I(zr)), where zi and zr represent image and text repre-"}, {"title": "Applications", "content": "In this section, we introduce the applications of AMRG across various\nmedical imaging modalities, including chest radiography, computed\ntomography (CT), magnetic resonance imaging (MRI), ultrasound,\nophthalmic imaging, endoscopy, surgical scene, and pathological imaging.\nThese modalities are crucial for diagnosing a broad spectrum of medical\nconditions.\nChest Radiography: In recent years, a significant amount of\nresearch focused on chest radiography report generation (refer to Table\n1). This is largely due to the availability of large and publicly accessible\ndatasets such as MIMIC-CXR [36] and IU X-ray [37], which contain\nextensive collections of annotated images and paired reports. The\navailability of these datasets enables the models to effectively learn the\nintricate relationships between visual features and textual descriptions.\nThe application of AMRG in radiography offers several significant\nbenefits. First, it can significantly reduce the radiologist's workload by\nautomating the initial draft of the report, allowing them to focus on\nmore complex and nuanced cases [9, 109, 28, 110, 111]. Second, these\nmodels can improve diagnostic consistency and reduce inter-observer\nvariability by applying standardized criteria and guidelines in the report\ngeneration process [112]. Third, in regions with limited access to\nexperienced radiologists, AMRG models can provide essential diagnostic\nsupport, ensuring timely and accurate medical care for patients [113].\nFinally, these models can support large-scale screening programs by\nrapidly processing and generating reports for large volumes of X-ray\nimages, thereby facilitating the early detection of diseases.\n3D Imaging: CT and MRI provide detailed, three-dimensional"}, {"title": "Public Dataset", "content": "In this section, we introduce several image-report datasets used for\nAMRG. All discussed datasets are publicly available and privacy-safe,\nmeaning they are free of any patient-identifying information. The two\nbenchmark datasets (Section 5.1) are widely used and serve as standards\nfor performance comparison in most AMRG studies. Additionally, other\ndatasets (Section 5.2) have been created to address specific needs, such\nas particular languages and imaging modalities."}, {"title": "Benchmark Datasets", "content": "IU-Xray: The Indiana University Chest X-ray dataset (IU-Xray)\n[37], also known as the OpenI dataset, was released in 2016. This"}, {"title": "Clinical Efficacy", "content": "NLP metrics primarily measure the word overlap between the generated\nreport and the reference report. However, in the medical field, semantic\nsimilarity and factual consistency between the generated report and\nthe reference report are more important. For example, the NLP scores\nfor the sentences \"The heart is within normal size and contour\" and\n\"No cardiomegaly observed\u201d are zero. However, in medical reports,\nthese two sentences convey the same information. Therefore, many\nradiographic report generation studies supplement NLP metrics with\nclinical efficacy (CE) metrics [92, 88, 105, 104, 70, 47, 91, 46, 89, 141,\n100, 64, 108, 65, 69, 60, 142]. Specifically, they use CheXpert [127] to\nextract labels from the generated and reference reports, focusing on 12\npossible chest diseases. The label-based precision, recall, and F1 score\nare then calculated as CE metrics. This approach allows CE metrics to\nassess whether the generated report and the reference report contain\nthe same diseases.\nHowever, CE metrics are currently limited to the evaluation of En-\nglish chest radiography reports. There are no tools similar to CheXpert\nfor extracting disease labels from text for other body parts, modalities,\nand languages, which presents an opportunity for future research."}, {"title": "Human Evaluation", "content": "However, both NLP metrics and CE metrics sometimes are unreliable\nfor evaluating medical reports, and CE metrics are limited to pre-\ntrained disease categories. Some researchers suggest introducing human\nevaluation to comprehensively assess the quality of generated reports\n[100, 65, 132, 119, 143, 144]. Specifically, reports generated by multiple\ncandidate models are mixed together, and multiple board-certified\nradiologists compare the candidate reports with the reference reports\nto avoid personal bias. The radiologists select the generated reports\nthat are most similar to the reference reports based on fluency, factual\nconsistency, and overall quality. While human evaluation is the most\nreliable evaluation method, it is also the most expensive and impractical\nfor large-scale evaluations."}, {"title": "Performance Comparisons", "content": "Table 1 shows the results of SOTA radiographic reporting methods pub-\nlished between 2021 and 2024 on benchmark radiography datasets. By\ncomparing their performance and techniques, we identified six techniques\nthat effectively improve NLP and CE metrics: (i) human-computer inter-\naction, (ii) reinforcement learning, (iii) detection and segmentation, (iv)\ndisease classification, (v) traceback mechanism, and (vi) local alignment.\nIn the following sections, BLEU-4, METEOR, ROUGE-L, and CIDEr\nin NLP metrics, as well as precision, recall, and F1 score in CE metrics,\nare simplified to BL-4, MTR, RG-L, CD, P, R, and F, respectively.\nHuman-computer interaction technology achieves the highest scores\nin both NLP and CE. Specifically, the inclusion of doctors' notes sig-\nnificantly enhances the quality of the generated reports. Nguyen et\nal. [145] achieved the best NLP scores on both MIMIC-CXR (BL-4:\n0.224, MTR: 0.222, and RG-L: 0.390) and IU-Xray (BL-4: 0.235, MTR:\n0.219, and RG-L: 0.436) by incorporating clinical documents. These\nclinical documents, which may include patients' clinical histories or\ndoctors' notes, guide the model to focus on specific areas of the image\nand relevant diseases. For instance, if the doctor's note mentions \u201cshows\ncough and shortness of breath symptoms\", the model will focus on the\nlung area and consider pneumonia. In addition to human-computer\ninteraction, the model involves disease classification, memory retrieval\nand traceback mechanism, as illustrated in Figure 8. Similarly, Liu et\nal. [47] introduced disease labels provided by radiologists, achieving the\nhighest CE scores (P: 0.855, R: 0.730, and F: 0.773). Their model offers\ntwo options: automatic disease classification based on the input image\nor radiologist-provided potential disease labels. The latter results in\nmarkedly higher clinical efficacy in generated reports. Thus, incorporat-\ning human guidance into the model effectively improves the quality of\nthe generated reports.\nFor methods without human interaction, reinforcement learning\n(Section 3.4.2) is most effective. Xu et al. [85] used BL-4, MTR, and\nCD as rewards and achieved the second-highest NLP scores (BL-4:\n0.192, MTR: 0.207, and RG-L: 0.380) on the MIMIC-CXR dataset.\nLikewise, the factual completeness and consistency reward designed by\nMiura et al. [100] resulted in the second-highest CE scores (P: 0.503,\nR: 0.651, and F: 0.567) on MIMIC-CXR dataset and highest CD scores\""}, {"title": "Future Directions", "content": "Finally, we highlight unresolved issues in the current methods that\npresent opportunities for future research in the AMRG field.\nMultimodal Learning: The primary challenge in the AMRG\nfield is bridging the modality gap between images and text. The CLIP\nmodel [44], which utilizes natural language as supervision for contrastive\nlearning, has significantly advanced this area. However, from the current"}, {"title": "Conclusion", "content": "In conclusion", "challenges": "modality gap, visual deviations, text complexity,\nand dataset limitations. We also present AMRG applications across\nvarious imaging modalities, including radiography, CT scans, MRI,\nultrasound, FFA, endoscopic imaging, surgical scenes, and WSI. In\naddition, our review underscores the importance of publicly available\ndatasets and robust evaluation metrics in advancing AMRG research.\nBased on their performance on benchmark datasets, we identify six\nsolutions that can significantly improve evaluation metrics.\nDespite these advancements, the field continues to face ongoing\nchallenges. Future research should focus on developing more effective\nmultimodal learning algorithms, enhancing human-computer interaction,\nexpanding available datasets"}]}