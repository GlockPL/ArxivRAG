{"title": "Efficient Neural Network Encoding for 3D Color Lookup Tables", "authors": ["Vahid Zehtab", "David B. Lindell", "Marcus A. Brubaker", "Michael S. Brown"], "abstract": "3D color lookup tables (LUTs) enable precise color manipulation by mapping input RGB values to specific output RGB values. 3D LUTs are instrumental in various applications, including video editing, in-camera processing, photographic filters, computer graphics, and color processing for displays. While an individual LUT does not incur a high memory overhead, software and devices may need to store dozens to hundreds of LUTs that can take over 100 MB. This work aims to develop a neural network architecture that can encode hundreds of LUTs in a single compact representation. To this end, we propose a model with a memory footprint of less than 0.25 MB that can reconstruct 512 LUTs with only minor color distortion (\u0394\u0395\u039c < 2.0) over the entire color gamut. We also show that our network can weight colors to provide further quality gains on natural image colors (\u0414\u0415\u043c \u2264 1.0). Finally, we show that minor modifications to the network architecture enable a bijective encoding that produces LUTs that are invertible, allowing for reverse color processing. Our code is available at https://github.com/vahidzee/ennelut.", "sections": [{"title": "1 Introduction", "content": "Color manipulation is a fundamental operation in computer vision and image processing, where input RGB values map to output RGB values. A widely used method for encoding such mappings is through a 3D color lookup table (LUT). LUTs are employed in a diverse range of applications, such as video editing, in-camera processing, photographic filters, computer graphics, and color processing for displays. Particularly, LUTs play a pivotal role in ensuring color accuracy and consistency across various display hardware.\nAn individual 3D LUT imposes a manageable memory overhead. For example, a standard 33\u00d733\u00d733 LUT at 16-bit precision requires approximately 70 KB. However, professional LUTs for color grading or color management often rely on a 65\u00d765\u00d765 resolution that requires approximately 0.5 MB at 16-bit precision. Storing a library of hundreds of such LUTs can quickly become a limitation, especially for applications running on resource-constrained devices, such as smartphones and camera hardware. Black-box compression (e.g., zip) provides limited reduction, for instance, a library of 512 zipped LUTs requires approximately 124 MB."}, {"title": "2 Related Work", "content": null}, {"title": "2.1 LUTs for Color Manipulation", "content": "As non-parametric function approximators, 3D Color LUTs (CLUTs or simply LUTs) are suitable for modeling complex transformations by sampling a target transformation on a 3D lattice. LUTs are instrumental for color grading in video editing (Postma and Chorley 2016) and used for correction in display colors (Shi and Luo 2021). LUTs are also essential tools in various stages of ISP pipelines (Kasson et al. 1995; Karaimer and Brown 2018), where they help ensure colorimetric accuracy or are used to render different picture styles (Karaimer and Brown 2016; Delbracio et al. 2021; Zhang et al. 2022).\nLUTs have also been used as building blocks in frameworks that learn image enhancements (Zeng et al. 2020; Wang et al. 2021). For instance, work in (Yang et al. 2022a) learned LUTs using an adaptive lattice for color manipulation, while (Wang et al. 2021; Liu et al. 2023) learn contextualized LUTs for spatially varying and image-dependant color transformations. Although such works typically deal with less than a handful of LUTs, the memory footprint of LUTs has enticed attempts at less memory-intensive formulations such as through a combination of 1D and 3D LUTs (Yang et al. 2022b), or the decomposition of 3D LUTs into learned sub-tables and lower-rank matrices (Zhao, Abdelhamed, and Brown 2022). In this work, we assume the LUTs are provided and seek a neural architecture to encode them efficiently."}, {"title": "2.2 LUT Compression", "content": "LUTs are stored as a 3D input-output lattice, where the input lattice is usually set to a uniform grid at fixed intervals along the R, G, and B color axes. As a result, it is necessary only to store the output colors. LUTs are commonly stored as standard ASCII or Unicode (i.e., . cube files) or as a binary array of floating-point values. Such encodings can be further compressed using off-the-shelf compression algorithms (e.g., zip). Alternatively, a LUT can be stored as an RGB image called a Hald image. The Hald image resolution can be adjusted to mimic different LUT resolutions. Hald images are stored in a lossless format such as png. Hald images also serve to visualize how an individual LUT manipulates colors over the entire color space.\nVarious lossless compression techniques have been proposed (Balaji et al. 2007, 2008; Shaw et al. 2012), achieving average compression rates of \u224830% (similar to the compression rates of png). In (Tang et al. 2016), lossy compressed LUTs were stored as ICC device-link profiles. In (Tschumperl\u00e9, Porquet, and Mahboubi 2019, 2020), sparse color key points were estimated that enabled the reconstruction of the original 3D LUT, providing an average compression rate of \u226595%. Existing LUT compression methods target individual LUTs. Our network implicitly compresses multiple LUTs, capitalizing on their inherent similarities, and achieves compression rates of \u226599%."}, {"title": "2.3 Neural LUTS", "content": "To our knowledge, (Conde et al. 2024) is the only work to target LUT embedding and reconstruction using a neural network. Specifically, (Conde et al. 2024) showed that 3 to 5 LUTs could be encoded using a model requiring approximately 750 KB storage. We show that our network architecture is better suited for LUT embedding. In addition, we describe how a straightforward modification to our network imposes bijectivity on the LUTs encoding. This allows an image processed with our LUT to be restored to its initial RGB values."}, {"title": "3 Proposed Approach", "content": "Formally, a 3D color LUT $F : \\mathbb{R}^3 \\rightarrow \\mathbb{R}^3$ is a function that maps input RGB colors to output RGB colors represented by a set of input-output pairs on a sparse lattice covering the input color space. $F$ computes the output color of an arbitrary input using traditional interpolation techniques (Kasson et al. 1995).\nGiven a set of LUTs $\\{F_1, F_2,\\dots, F_N\\}$, we aim to find an implicit neural representation $f_\\theta(\\cdot, o) : \\mathbb{R}^3 \\times \\mathcal{L} \\rightarrow \\mathbb{R}^3$, where $\\mathcal{L} \\subset \\mathbb{R}^N$ is the set of LUTs and $f$ is a function modeled with a deep neural network parameterized by $\\theta$. The function $f_\\theta$ takes on an RGB input in addition to a desired LUT $F_i$, represented with a one-hot encoded vector $o_i \\in \\mathcal{L}$. We learn the parameters $\\theta$ such that $f_\\theta(\\cdot, o_i) \\approx F_i(\\cdot)$. We formalize this in terms of the optimization problem\n$\\theta^* = \\underset{\\theta}{\\text{argmin}} \\mathbb{E}_{x\\sim \\mathcal{P}} \\sum_{i=1}^N ||f_\\theta(x, o_i) - F_i(x)||_2^2,   \\qquad(1)$\nwhere $\\mathcal{P}$ signifies a probability distribution over the input colors. Different choices for $\\mathcal{P}$ result in different $\\theta^*$s trading off better reconstruction of certain colors over the others. The choice of $\\mathcal{P}$ depends on the intended use case of $f_{\\theta^*}$. As we show in our experimental results, the closer $\\mathcal{P}$ is to the evaluation distribution, the better $f_{\\theta^*}$ trades off the quality of color reconstructions over out-of-distribution colors."}, {"title": "3.1 Network Architecture", "content": "To design an efficient architecture, we desire to capture the inherent characteristics of LUTs. We aim to devise a network structure that embodies the following traits:\n1. LUTs frequently resemble an identity function in many regions of the input space,\n2. the majority of LUTs exhibit local bijectivity, with exceptions being intentional manipulations that compress the color space into lower-dimensional manifolds (e.g., RGB to grayscale LUTs).\nResidual networks (ResNet) (He et al. 2016) are a natural fit for the first property, given their inductive bias toward an identity function. To capture the second property, we propose to utilize architectures from the normalizing flows literature (Kobyzev, Prince, and Brubaker 2020).\nNormalizing flows (Rezende and Mohamed 2015) are a class of bijective neural networks often used for generative modeling and density estimation. We take inspiration from (Jacobsen, Smeulders, and Oyallon 2018; Chen et al. 2019; Behrmann et al. 2019) that advocate modifying a stock residual network with an inductive bias towards bijective maps. As demonstrated in (Jacobsen, Smeulders, and Oyallon 2018), if all the residual functions in a ResNet have a Lipschitz constant strictly smaller than 1 the entire network is invertible by the Banach fixed-point theorem. For such networks, termed residual flows, each residual function can be explicitly restricted through spectral normalization (Gouk et al. 2021; Miyato et al. 2018) and activation functions that have bounded derivatives. However, given that not all LUTs are bijective (e.g., RGB to greyscale), we do not want to rigidly restrict the network to bijectivity. Instead, we initialize it close to such a bijective transformation as a form of inductive bias to regularize learning.\nTo this end, we design our neural architecture with $D$ residual components $T_1, T_2,\\dots,T_D$, as shown in Figure 2. Each residual component $T_i$, is an MLP with LipSwish non-linearities (Chen et al. 2019). $T_i$s are conditioned on the desired LUT $o$ by a learned matrix $E_i \\in \\mathbb{R}^{N\\times h}$, where $N$ is the number of embedded LUTs, and $h$ is the width of the first hidden layer in $T_i$. The one-hot encoded vector $o$, selects a row from $E_i$ ($o^\\intercal E_i$), which is used as the biases for the first hidden layer in $T_i$.\nWe use activation normalization (Kingma and Dhariwal 2018) to mitigate numerical instabilities caused by stacking multiple residual functions together. To ensure that our model is as close to the space of identity and bijective functions, we initialize all the weights and biases of the network with small values and use LipSwish non-linearities with bounded derivatives.\nMoreover, to deal with the bounded normalized input-output space, we convert the inputs using tanh\u207b\u00b9 and then return to the bounded color space by applying tanh on the network outputs. This way, the learnable part of the model always operates in an unbounded space with similar input-output measures. This makes for a better computational model and helps our model deal with colors with high saturation, as the network weights no longer need to grow to produce saturated colors. In addition, compared to clipping the values as in (Conde et al. 2024), our method does not suffer from clipped gradient values, which can hinder training."}, {"title": "3.2 Training", "content": "Our training accommodates the simultaneous embedding of many LUTs on a custom color distribution $\\mathcal{P}$. At each optimization iteration, we perform the following:\n1. Randomly pick a batch of input colors from the 256\u00b3 input color space based on $\\mathcal{P}$ and normalize the source color values.\n2. Compute the target colors by applying all (or optionally a random subset of LUTs) on the batch and normalize the target color values.\n3. Compute the L2 reconstruction error and update the weights. The network can also be trained using a \u0394\u0395 loss which is described in our alternative training approaches experiments section.\nNormalization. We normalize each color channel to fall within the range [-0.83,0.83] ensuring that the values passed through tanh and tanh\u207b\u00b9 remain in a region with sufficiently large gradients, reducing the risk of vanishing gradients. Refer to supplemental materials for details.\nImplementation. During training, input colors are processed with the LUT to produce target color values. We target all LUTs at each training step. We implemented a GPU-based trilinear interpolation using PyTorch (Paszke et al. 2019). An Nvidia RTX4090 GPU was used for training. With LUT interpolation implemented on GPU, we could process batch sizes of 2048 input colors, all transformed with up to 512 different LUTs.\nWe optimize our network using Adam (Kingma and Ba 2014) with default settings and use a stepped learning rate schedule, decreasing the learning rate at fixed intervals. See supplemental materials for more details."}, {"title": "3.3 Evaluation", "content": "To evaluate the quality of the LUT approximation, we use the CIE76 \u0394\u0395 metric that has a direct interpretation in terms of human perception. Specifically, two colors with a \u0394\u0395 \u2264 2 are generally considered indistinguishable for an average observer (Sharma and Bala 2017). As \u0394\u0395 is computed per-color pair, to get an estimate of the overall qualities of color reconstructions, we track the general statistics of AEs over the set of evaluation colors. We use \u2206Eq% and \u0394EM to denote the q-th quantile and the empirical mean of AEs over a particular evaluation set. Since our model embeds multiple LUTs at a time, we average such statistics over all the embedded LUTs in the model to get \u2206Es. For instance, a model with \u2206E90% < 2 can reconstruct 90% of the evaluated colors with a \u0394\u0395 < 2 on average over its embedded LUTs. Note that we also report PSNR values to be consistent with previous work."}, {"title": "4 Experiments", "content": "We begin by describing our training and testing data. We evaluate our network in two scenarios: (1) uniform sampling of the color space and (2) sampling based on natural images. Qualitative results are also shown on images processed by our reconstructed LUTs. Finally, we demonstrate the network's ability to invert a LUT."}, {"title": "4.1 Experimental Setup", "content": "LUTs. We obtained 543 open-source LUTs available under Creative Common licensing to train and test our method. The LUTs range in size between 16\u00b3 to 35\u00b3 and are encoded as.cube files. See supplemental material for details.\nEvaluation. We report \u0394\u0395 on 256\u00b3 Hald images that capture all possible 8-bit color outputs produced by a given LUT. To estimate the reconstruction quality of colors in natural images, we report the metrics averaged over 100 randomly selected images from the Adobe-MIT 5K dataset (Bychkovsky et al. 2011). Our experiments showed minimal differences when using all of the images from the Adobe 5K dataset (Bychkovsky et al. 2011) for evaluation versous using a subset of 100 random selected images from the dataset. Therefore, for raster experimentation, we report the results of our evaluations on natural images using only 100 random images. See supplemental materials for details.\nThe selected images were converted to 8-bit sRGB format and rescaled such that their maximum dimension is 1024 pixels. We ensure the reproducibility of our experiments by retraining the same model with different sets of randomly selected LUTs to provide the average and 95% confidence interval for each metric. We provide the results for different model sizes and number of embedded LUTs.\nRuntime. Our model reconstructs LUTs at any resolution. Execution time to recover LUTs with our medium-sized model at resolutions 65\u00b3, 33\u00b3, 11\u00b3 and 7\u00b3 are 4.11 ms, 1.64 ms, 1.27 ms, 1.21 ms respectively, measured on an Nvidia RTX4090 GPU. See supplemental materials for details and additional runtime results."}, {"title": "4.2 Quantitative Results", "content": "We start with training our models by sampling the LUTs over the entire color space. We refer to this as uniform sampling as every color in the color space is weighted equally. As previously mentioned, we use a GPU-based interpolation of the LUTs to generate these samples over the LUTs.\nThe number of trainable parameters for each model consists of: (1) the core parameters of each Ti, and (2) the embedding weights Eis, which grow linearly with the number of LUTs. With this in mind, we experimented with different numbers of Tis (D) of varying depths and widths to pick the right set of hyper-parameters for our network. We found a hidden structure of [32, 64, 32] neurons for Tis to work best with minimal memory requirements, as the core structure only has 4.3K trainable parameters, and embedding each LUT only requires learning an additional 32 parameters.\nWe consider four variants of our model (tiny, small, medium, large) with increasing modeling capacity by stacking 1 to 4 Tis together. Figure 3 shows the quality of the color reconstructions for each model variant when trained to embed a varying number LUTs. Smaller models can be used depending on the number of LUTs that need to be embedded. Figure 3 also shows that even the smallest model can embed up to 32 LUTs with a low enough \u0394\u0395."}, {"title": "4.3 Alternative Training Approaches", "content": "Training using natural images. As previously mentioned, the choice of the training color distribution $\\mathcal{P}$ plays a role in the overall quality of color reconstructions. To test this, we randomly select 200 images from the Adobe-MIT5K dataset (Bychkovsky et al. 2011), splitting them into a training and testing split of 100 images each. Figure 10 shows a visualization of the distribution of the colors in the natural training images as a heat map corresponding to a Hald image. The distribution heat map reveals that large regions of the color space have a low probability of occurring in the natural images, with approximately 85% of all colors rarely if ever, being observed.\nThis sparsity suggests that a model trained to focus on those regions of the color space which are more likely to occur in natural images should perform better when evaluated on images. We repeat our uniform distribution experiments but instead train by using the distribution of colors in the Adobe-MIT5K training split as our $\\mathcal{P}$. We then evaluate the model on the images in the test split."}, {"title": "4.4 Qualitative Results", "content": "Figure 7 provides qualitative results on images processed by reconstructed LUTs. Our results are computed using our medium-sized model with 32 LUT embedding and trained with uniform sampling. We compared this with the best-performing architecture reported by Conde et al. (Conde et al. 2024). Our model can achieve higher fidelity color reconstructions with a significantly smaller model size. See supplemental material for further details on these experiments and architectural ablation studies.\nFigure 8 shows qualitative results on our model trained using uniform sampling and natural image sampling. As the quantitative experiments also indicate, training our models on natural images improves performance when the reconstructed LUTs are applied to natural images. Nevertheless, uniform training might be preferred when consistency of predictions is required, as the model trained on a sparse distribution might fail to faithfully reconstruct colors that appear infrequently in the training distribution. For example, in Figure 8 (last row), the purple-colored flowers have higher \u0394\u0395 than those processed by the uniformly trained model."}, {"title": "4.5 LUT Inversion", "content": "We designed our model to be initialized near the space of bijective transformations. Here we examine the effect of keeping the network strictly bijective. We use spectral normalization (Miyato et al. 2018; Gouk et al. 2021) to normalize the weights of each residual transformation T\u1d62 by its largest singular value, enforcing a Lipschitz constant of 0.97. This restriction, along with our choice of activation function, forces the model to remain bijective during training, which allows the network to be inverted (Behrmann et al. 2019) with a fixed-point iteration algorithm. As a result, we can compute an inverse color LUT by reversing the order of computations in the network and inverting each transformation. See supplemental materials for details.\nRestricting the architecture to be bijective may slightly decrease its modeling capacity, and we find more depth is needed to approximate the LUTs comparably. For more details, see the supplemental material.\nWe visualize the results of our LUT inversion mechanism on a single image and LUT in Figure 6. For a point of comparison, we fit our small-sized architecture on the LUT applied (referred to as LUT\u2081) as follows. First, we fit the LUT as normal (g\u1d67). Next, we also fit the LUT but swap inputs and outputs to estimate its inverse (h\u03c9). As seen in Figure 6, the invertible architecture works well at inverting the LUT, effectively up to the numerical precision of the fixed-point iteration. In contrast, attempting to directly estimate the LUT and its inverse separately results in significant errors."}, {"title": "5 Concluding Remarks", "content": "This paper has introduced a network architecture designed to efficiently encode 3D color lookup tables (LUTs) into a single compact representation. Our proposed model achieves this with a minimal storage footprint, consuming less than 0.25 MB. The reconstruction capability of the model extends to 512 LUTs, introducing only minor color distortion (\u0414\u0415\u041c \u2264 2.0) across the entire color space. Furthermore, we demonstrate that the network has the ability to weight LUT colors, yielding additional quality improvements, particularly evident in natural image colors with \u2206\u0395\u039c < 1.5. Our network architecture is also able to accommodate bijective encoding, enabling the production of invertible LUTs and facilitating reverse color processing."}]}