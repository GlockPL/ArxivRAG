{"title": "FetalCLIP: A Visual-Language Foundation Model for Fetal Ultrasound Image Analysis", "authors": ["Fadillah Maani", "Numan Saeed", "Tausifa Saleem", "Zaid Farooq", "Hussain Alasmawi", "Werner Diehl", "Ameera Mohammad", "Gareth Waring", "Saudabi Valappi", "Leanne Bricker", "Mohammad Yaqub"], "abstract": "Foundation models are becoming increasingly effective in the medical domain, offering pre-trained models on large datasets that can be readily adapted for downstream tasks. Despite progress, fetal ultrasound images remain a challenging domain for foundation models due to their inherent complexity, often requiring substantial additional training and facing limitations due to the scarcity of paired multimodal data. To overcome these challenges, here we introduce FetalCLIP, a vision-language foundation model capable of generating universal representation of fetal ultrasound images. FetalCLIP was pre-trained using a multimodal learning approach on a diverse dataset of 210,035 fetal ultrasound images paired with text. This represents the largest paired dataset of its kind used for foundation model development to date. This unique training approach allows FetalCLIP to effectively learn the intricate anatomical features present in fetal ultrasound images, resulting in robust representations that can be used for a variety of downstream applications. In extensive benchmarking across a range of key fetal ultrasound applications, including classification, gestational age estimation, congenital heart defect (CHD) detection, and fetal structure segmentation, FetalCLIP outperformed all baselines while demonstrating remarkable generalizability and strong performance even with limited labeled data. We plan to release the FetalCLIP model publicly for the benefit of the broader scientific community", "sections": [{"title": "1 Introduction", "content": "Prenatal care is historically known to be transformed by the integration of ultrasound technology. Widely recognized for its accessibility, safety, and cost-effectiveness, ultrasound now plays a critical role in monitoring fetal development in real time and facilitating the early detection of congenital abnormalities [1-4]. However, despite its transformative role, fetal ultrasound image interpretation remains inherently subjective and heavily operator-dependent, often challenged by subtle visual cues, complex fetal anatomy, and significant inter-observer variability [5, 6]. These challenges can lead to inconsistencies in clinical assessments, particularly in resource-limited settings where access to highly trained sonographers is restricted [7]. Studies have shown significant inter-observer variability in fetal ultrasound measurements, with differences of approximately \u00b14.9% for head circumference (HC), \u00b18.8% for abdominal circumference (AC), and \u00b111.1% for femur length (FL) [8]. These variabilities highlight the subjectivity and potential inconsistencies in fetal ultrasound assessments, emphasizing the need for AI-powered tools to enhance diagnostic objectivity and accuracy. These variabilities underscore the subjectivity and potential inconsistencies in fetal ultrasound assessments, highlighting the urgent need for robust AI-powered tools that enhance diagnostic objectivity, accuracy, and accessibility in fetal ultrasound imaging.\nRecent advances in artificial intelligence (AI), particularly in foundation models, have demonstrated remarkable capabilities in improving medical imaging analysis [9, 10]. These models, pretrained on large-scale datasets, enable powerful feature extraction and knowledge transfer to downstream tasks, enhancing diagnostic precision, optimizing clinical workflows, and broadening access to expert-level interpretation. However, existing foundation models [11-26] exhibit fundamental limitations when applied to fetal ultrasound. For instance, CLIP [11], primarily trained on natural images, lacks the domain-specific anatomical knowledge necessary for medical imaging. BiomedCLIP [12], while tailored for the biomedical domain, is primarily optimized for text-based biomedical knowledge retrieval rather than complex image-text reasoning required for fetal ultrasound interpretation, limiting its ability to effectively capture fine-grained anatomical details. UniMed-CLIP [13], though leveraging a large-scale open-source medical multimodal dataset, demonstrates variable performance across different imaging modalities and remains largely unexplored in the context of fetal ultrasound. Complementary to the aforementioned approaches, numerous efforts have been made to develop foundation models for specific medical imaging modalities, including chest X-ray [14-16], ophthalmic imaging [17, 18], pathology [19-21], echocardiography [22-24], chest CT [25], and head CT [26]. These models struggle to generalize to the unique challenges posed by fetal imaging, failing to capture subtle morphological variations that are crucial for assessing fetal development. Moreover, most AI solutions in fetal ultrasound rely on limited datasets and do not achieve the level of generalizability required for robust clinical deployment, particularly in detecting rare fetal conditions. Encoding the entirety of fetal anatomy while preserving diagnostically critical features remains a significant challenge for existing AI methods. To overcome these barriers, a dedicated fetal ultrasound-specific foundation model is required to harness the full potential of AI for advancing prenatal care.\nHere, we introduce FetalCLIP, a novel visual-language foundation model explicitly engineered for fetal ultrasound analysis, trained on the most comprehensive dataset of its kind. FetalCLIP is pretrained at an unprecedented scale (Fig. 1a), leveraging 207,943 fetal ultrasound images with corresponding GPT-40-generated captions and 2,092 expert-annotated image-caption pairs from a fetal ultrasound textbook. This extensive dataset, which was curated by authors in collaboration with medical experts, covers a broadspectrum of fetal anatomical structures and developmental stages, ensuring both diversity and robustness. FetalCLIP incorporates an innovative multimodal contrastive learning strategy that integrates visual and textual representations of fetal ultrasound data, allowing the model to effectively align anatomical"}, {"title": "2 Results", "content": "structures with diagnostic descriptions and enhance interpretability. This advanced pretraining paradigm empowers FetalCLIP to learn rich, generalizable representations of fetal ultrasound scans and effectively transfer this knowledge to a diverse range of downstream tasks, including zero-shot classification of standard fetal views, congenital heart disease (CHD) detection from ultrasound videos, segmentation of fetal anatomical structures, and feature extraction for downstream fetal ultrasound tasks, ensuring adaptability across clinical applications. Unlike existing models, FetalCLIP's dual-modality learning approach allows it to discern subtle, clinically actionable patterns in fetal ultrasound images, surpassing the capabilities of vision-only models and yielding substantial improvements in diagnostic accuracy and clinical interpretability.\nBy addressing the critical gaps in fetal ultrasound analysis, FetalCLIP represents a significant step toward more reliable, accessible, and AI-driven prenatal diagnostics. FetalCLIP undergoes rigorous evaluation across multiple downstream tasks to assess its generalizability and clinical applicability. It achieves an 87.1% F1 score in zero-shot classification of standard fetal views, outperforming existing foundation models and a state-of-the-art open-source fetal view classifier trained with supervised learning [27]. In the critical task of congenital heart disease (CHD) detection from ultrasound videos, FetalCLIP demonstrates 6.92% improvement in AUC over previous models, showcasing its ability to detect subtle morphological variations crucial for early diagnosis. Furthermore, for fetal anatomical segmentation, it attains an average Dice Similarity Coefficient (DSC) of 84.22% across three different fetal anatomical planes, highlighting its proficiency in delineating fetal structures with high precision. These findings firmly establish FetalCLIP as a pivotal advancement, bridging the gap between human-level expertise and AI-driven prenatal diagnostics and setting a new benchmark for the field of fetal ultrasound analysis."}, {"title": "2.1 Overview of FetalCLIP", "content": "FetalCLIP is a vision-language foundation model for fetal ultrasound analysis, developed by leveraging a large-scale dataset of paired fetal ultrasound images and captions (Fig. 1a). The dataset comprises 207,943 images from routine clinical scans encompassing 64 diverse clinician-labeled keywords across 6,493 patients with a mean gestational age of 148 \u00b1 16 days, supplemented by 2,092 image-caption pairs derived from a fetal ultrasound textbook [28] primarily focused on the fetal heart. As expert-provided image-level text descriptions were absent for the clinical dataset, we used GPT-40 to generate clinically sound captions based on gestational age, clinical labels, and pixel spacing, with details provided in the Methods. This approach ensured each image was paired with a unique, contextually relevant textual description. We adopted the contrastive language-image pretraining (CLIP) [11] framework to pretrain FetalCLIP by aligning images with their corresponding textual descriptions in a shared embedding space while minimizing similarity to unrelated pairs (Fig. 1b). The FetalCLIP architecture includes a ViT-L [29] image encoder, a Byte-Pair Encoding tokenizer [30], and a text encoder [31] capable of processing up to 117 tokens to accommodate the maximum token length in the rich clinical text descriptions. Our extensive evaluations demonstrated the superior performance of FetalCLIP over existing vision-language foundation models across various tasks in fetal ultrasound analysis (Fig. 1c), attributed to its pretraining on a large-scale dataset comprising predominantly clinical data with diverse fetal ultrasound keywords (Fig. 1d)."}, {"title": "2.2 Zero-shot classification of standard fetal views", "content": "We conducted a study to evaluate FetalCLIP's capability in classifying standard fetal ultrasound views using unseen data from different hospitals without any adaptation (Fig. 2a). FetalCLIP was compared against notable models in the field: (1) SonoNet [27], a specialized model specifically trained for classifying standard views of fetal ultrasound; (2) CLIP [11], a visual-language model (VLM) for natural images; and two visual-language foundation models tailored for the general medical domain, (3) BiomedCLIP [12] and (4) UniMed-CLIP [13]. We conducted the evaluation using the Planes DB [32] dataset, which consists of fetal ultrasound images acquired from two hospitals. The models were employed to classify five anatomical planes abdomen, brain, cervix, femur, and thorax-as well as three subplanes within the brain-cerebellum, thalamic, and ventricular. Our experiment (Fig. 2b) demonstrated that FetalCLIP achieved an average F1 score of 87.1%, outperforming SonoNet by a notable margin of 17.2%, UniMed-CLIP by 37.6%, BiomedCLIP by 40.5%, and CLIP by 60.1%. The findings also highlight that CLIP lacks the contextual information required to differentiate between different fetal ultrasound anatomical views, and both BiomedCLIP and UniMed-CLIP struggle in distinguishing between brain subplanes. Our confusion matrix analysis (Extended Data Fig. 1) further added that SonoNet struggled with identifying the cervix view, while UniMed-CLIP could not distinguish the spine view from other fetal planes. In contrast, by harnessing large-scale fetal ultrasound data and incorporating semantic knowledge from textual descriptions during pretraining, FetalCLIP achieved remarkable accuracy in zero-shot classification of standard anatomical views."}, {"title": "2.3 Zero-shot gestational age estimation", "content": "The FetalCLIP pretraining stage requires the model to incorporate gestational age in order to precisely align fetal ultrasound images with their corresponding text descriptions. This enables FetalCLIP to estimate gestational age (GA) directly from the images (Fig. 2c) to a certain degree of precision, without additional fine-tuning, as detailed in the Methods section. To evaluate its zero-shot performance for this specific task, we leveraged the HC18 [33] dataset, which includes fetal brain images along with head circumference (HC) annotations and pixel spacing. As the true GA was not available, we defined a prediction to be valid if the true HC falls within the 2.5th to 97.5th percentile range of HC for the predicted GA, which was calculated using the quantile regression method established by the WHO [34]. FetalCLIP achieved a prediction validity rate of 83.5%, while other models produced much lower validity rates (CLIP: 11%, BiomedCLIP: 24%, and UniMed-CLIP: 9%). This evaluation (Fig. 2d) also highlights that while existing visual-language foundation models failed to reliably infer GA from images, FetalCLIP effectively produced a high proportion of valid GA predictions. Our further investigation demonstrated that FetalCLIP achieved a higher validity rate of 89% within the range of true HC values associated with the 25th to 75th percentile of the pretraining GA distribution (20 weeks 0 days to 21 weeks 6 days). This suggests that FetalCLIP's pretraining data influenced its zero-shot performance in estimating GA."}, {"title": "2.4 FetalCLIP as strong feature extractor for fetal US", "content": "Motivated by the growing need for efficient tuning to adapt large pre-trained models to diverse applications an essential consideration in building large and complex AI systems [35-38]-we assessed the capability of the FetalCLIP image encoder to extract generalizable features for downstream fetal ultrasound tasks. In this setup, the image encoder was entirely frozen, while a lightweight network was trained to utilize the extracted features for a specific ultimate task e.g. a linear layer for classification."}, {"title": "2.4.1 Probing FetalCLIP for fetal views classification", "content": "In some clinical practices, adapting a foundation model is essential to achieve optimal accuracy in identifying specific fetal views from datasets obtained from different hospitals. To this end, we investigated the FetalCLIP ability to provide robust features for accurately distinguishing six fetal ultrasound views and three brain subplanes using the Planes DB dataset. We attached a single linear layer to the frozen FetalCLIP image encoder, harnessing its image representations and transforming them into predictions for the set of output views (Fig. 3a). Across all views, FetalCLIP achieved significantly higher F1 scores compared to CLIP, BiomedCLIP, and UniMed-CLIP (Fig. 3b-c). In addition, we also evaluated their performance in a data-efficient setting, using data from only a few patients for training. Similarly, FetalCLIP consistently outperformed both the natural and medical vision foundation models (Extended Data Fig. 3a-b). Using data from only 32 patients, FetalCLIP demonstrated comparable or even superior accuracy to UniMed-CLIP trained on the full dataset of 717 patients."}, {"title": "2.4.2 Probing FetalCLIP for video-based CHD detection", "content": "Clinicians are often required to analyze ultrasound videos to assess fetal conditions, such as detecting abnormalities in the fetal heart. However, developing AI solutions for such tasks is challenging given the limited availability of annotated ultrasound video data. Thus, leveraging pretrained models is critical for enhancing model generalizability. Motivated by this, we adapted FetalCLIP to analyze fetal ultrasound videos focusing on the 4-chamber view, with the aim of distinguishing between normal fetal hearts and those affected by congenital heart disease (CHD) (Fig. 3d). We leveraged FetalCLIP to extract image features from each frame of the ultrasound videos. These frame-level features were then combined (see Methods), and a linear layer was applied to classify the fetal heart as either normal or having CHD. Our experiment demonstrated that FetalCLIP outperformed the other foundation models by a substantial margin (Fig. 3e-f), achieving a mean AUROC of 78.72%, whereas CLIP, BiomedCLIP, and UniMed-CLIP achieved 67.88% ($P < 10^{-5}$), 64.32% ($P < 10^{-6}$), and 71.8% ($P < 10^{-3}$) respectively. This showcases the excellent adaptability of FetalCLIP in analyzing fetal ultrasound videos."}, {"title": "2.4.3 Probing FetalCLIP for segmenting fetal structures", "content": "Accurate pixel-level classification is critical for precise growing fetal biometry calculations [33, 39]. We investigated the foundation models' ability to provide fine-grained intermediate image features essential for localizing fetal anatomical structures. We apply a lightweight decoder with few parameters (~1.3 million parameters for ViT-B and ~1.6 million for ViT-L encoders) to utilize the intermediate image features for accurate segmentation of fetal anatomical structures (Fig. 4a). We conducted segmentation experiments on three fetal views to segment various structures: 1) brain view (head), 2) abdomen view (abdomen, stomach, and spine), and 3) 4-chamber view (nine structures, see Extended Data Table 1). We reported the average Dice Similarity Coefficient (DSC) for each view and weighted each structure equally. FetalCLIP achieved DSC of 97.92%, 81.82%, and 72.91% for brain, abdomen, and 4-chamber views,"}, {"title": "2.5 FetalCLIP interpretability", "content": "To analyze the FetalCLIP reliability from the clinical perspective, we conducted interpretation studies to investigate how the model derives its prediction and assess whether it aligns with clinical practice. We employed class activation mapping (CAM) via ScoreCAM [40] to visualize the importance of image regions to FetalCLIP's decisions, as depicted in Fig. 5a-b. The CAMs in Fig. 5a suggested that FetalCLIP can effectively highlight key fetal landmarks when identifying anatomical views, such as stomach in the abdomen view, femur bone, heart circumference, cerebellar hemispheres, and cavum septum pellucidi. This showcased FetalCLIP's ability to localize specific structures within fetal ultrasound images. Furthermore, as visualized in Fig. 5b, FetalCLIP highlighted regions surrounding the head circumference and some brain structures, such as the choroid plexus, to estimate gestational age.\nTo gain a deeper understanding of FetalCLIP's image representations, we utilized the Uniform Manifold Approximation and Projection (UMAP) [41] to visualize Fetal-CLIP image embeddings in a two-dimensional space. We found that FetalCLIP could effectively cluster five standard fetal planes (Fig. 5c) and differentiate between brain subviews (Fig. 5d). Our further investigation (Fig. 5e) revealed that FetalCLIP could cluster other fetal views such as profile and spine, as well as non-anatomical elements like tables, and could map images containing related anatomical structures into close proximity, such as the fetal extremities, which includes the leg, feet, arm, and hand. These observations were then verified by clinicians to confirm their validity. Thus, this finding showcases FetalCLIP's ability to enable automatic plane classification and clustering, potentially improving workflow efficiency in clinical practice."}, {"title": "3 Discussion", "content": "The advent of foundation models has shifted the field of image analysis toward an exciting paradigm. However, unlike in the natural domain, developing foundation models for the medical domain is more challenging due to the high heterogeneity of medical modalities. While recent studies have shown that modality-specific medical foundation models often outperform general medical foundation models [13, 16-20], both approaches have largely ignored fetal ultrasound. This slow advancement in fetal ultrasound can be attributed to the data scarcity in this domain that is both sufficient in quantity and contextual information. This study introduces FetalCLIP as the first visual-language foundation model designed explicitly for fetal ultrasound.\nIn this study, our findings suggest that despite the absence of large image-clinical text description pairs, a strong visual-language foundation model can be developed by leveraging routine pregnancy scans with limited contextual information, supplemented by image-caption pairs from a textbook. Unlike other foundation models that place limited emphasis on fetal ultrasound, FetalCLIP is highly adaptable across various fetal ultrasound tasks. Despite not being explicitly trained for specific tasks, Fetal-CLIP achieved excellent zero-shot classification of different fetal anatomical planes, surpassing a specialist model for view classification (SonoNet [27]), even when tested on unseen data from multiple hospitals. Pretrained with over 64 clinician-labeled keywords, FetalCLIP could reduce the labor-intensive process of manually identifying fetal ultrasound images, thus improving the accuracy and efficiency of prenatal assessment, especially in rural or low-resource environments. Furthermore, FetalCLIP is equipped with zero-shot capability to estimate gestational age from fetal images with remarkable accuracy. This highlights the presence of the FetalCLIP knowledge to extract meaningful information from fetal structures for assessing fetal growth. However, FetalCLIP struggles to accurately estimate gestational age for fetuses in early and late gestation. We hypothesize that this limitation arises from the distribution of its pretraining data, with the majority of the images acquired during the second trimester. Expanding pretraining data to cover a broader gestational age range could further improve performance.\nThis study also demonstrates that FetalCLIP can serve as a robust feature extractor. Our extensive downstream experiments across diverse fetal ultrasound tasks revealed substantial performance gains of FetalCLIP over other foundation models. The experiments included standard fetal plane classification, brain subviews classification, CHD detection from four-chamber videos, and segmentation of different fetal structures in the head, abdomen, and four-chamber views. These results establish our visual-language foundation model as the most preferred pretrained model for developing AI models to solve challenging problems in fetal ultrasound analysis, especially in data-efficient settings. In addition, these findings align with recent studies reporting the superiority of modality-specific foundation models over their general medical counterparts when applied to their respective targeted modalities [13, 16-20].\nIn addition to FetalCLIP's superior performance compared to other visual-language foundation models, our interpretability analysis underscores FetalCLIP's reliability and alignment with clinical practice. Using ScoreCAM, we demonstrated that Fetal-CLIP effectively localizes relevant anatomical structures and regions of interest when identifying anatomical views (Fig. 5a), such as the stomach in the abdomen view and cerebellar hemispheres in the transcerebellar plane. This CAM analysis also demonstrates that, when estimating gestational age, the model focuses on regions such as the head circumference and brain structures, including the choroid plexus (Fig. 5b). Moreover, UMAP visualizations (Fig. 5c-e) revealed that FetalCLIP's embeddings can effectively cluster standard fetal planes and differentiate between brain subviews, while mapping related anatomical structures into close proximity, e.g. fetal extremities.\nFetalCLIP's significant gains in zero-shot, transfer learning, and interpretability contribute to advancing fetal ultrasound image analysis, with future improvements possible through enriched pretraining data. While our zero-shot results demonstrate substantial performance gains of FetalCLIP over existing visual-language foundation models, we anticipate that the FetalCLIP's zero-shot capability for detecting abnormalities in fetal ultrasound scans may still be limited. This limitation likely arises from the fact that the majority of FetalCLIP pretraining data was collected from routine pregnancy scans in the second trimester which lacked information on fetal health conditions. Nevertheless, FetalCLIP exhibits superior performance and higher adaptability across various tasks compared to existing foundation models, which were typically trained on millions of image-caption pairs. This leaves immense potential for future studies to enhance FetalCLIP's representations by incorporating more diverse fetal ultrasound images and richer image-level descriptions. Additionally, due to computational constraints and to facilitate a fair benchmark with compared foundation models, we restricted our experiments to 224 \u00d7 224 image size. Notably, leveraging higher resolutions could elevate FetalCLIP's capabilities by providing clear fine-grained details such as valves in fetal hearts and brain structures. This study also underscores potential future exploration that includes expanding the pretraining data to cover wider data distribution, more diverse clinical scenarios, and a broader range of image types, as well as extending FetalCLIP to a video encoder for better capturing spatio-temporal features, thus broadening its applicability and impact in fetal ultrasound analysis. By releasing FetalCLIP to the public, we aim to foster the advancement of fetal ultrasound analysis by enabling researchers and clinicians to develop innovative applications on top of this strong foundation model."}, {"title": "4 Methods", "content": null}, {"title": "4.1 FetalCLIP pretraining data curation", "content": "FetalCLIP was pretrained using two data sources, curated to develop a robust and generalizable visual-language foundation model for fetal ultrasound image analysis. The first data source consists of 207,943 fetal ultrasound images from Corniche Hospital Abu Dhabi, a facility specializing in Women and Newborn care. This data was supplemented by 2,092 image-caption pairs from a fetal ultrasound textbook [28] emphasizing fetal heart conditions, providing critically important clinical information for fetal health assessment in clinical practice."}, {"title": "4.1.1 Private hospital dataset", "content": "This data source constitutes the largest portion of the FetalCLIP pretraining dataset. The images were obtained from routine pregnancy scans at the hospital to monitor fetal development, often performed during the second trimester. During this stage, the fetus begins to resemble a child, and sonographers examine various anatomical planes to detect any potential health abnormalities. The average gestational age in this data source is 148 \u00b116 days, with 50% of the images lying between 20 weeks 0 days and 21 weeks 6 days. Some images were accompanied by clinicians' text annotations embedded within the images, from which we identified 64 distinct keywords predominantly related to fetal anatomical structures (Fig. 1d). However, no associated medical report is available for each image. For this study, we utilized B-mode images and leveraged text annotations written by clinicians to serve as the basis for image labeling and text description generation for each image.\nA considerable effort was made to preprocess, clean, and standardize this data source. We began by detecting clinicians' text annotations using the EasyOCR [42] library, followed by manual text processing to clean and standardize the detected annotations, resulting in the 64 keywords. In addition, to prevent potential model shortcuts, such as reliance on text annotations within the images, we designed a robust image preprocessing pipeline. The ultrasound fan region was extracted by first isolating the foreground, retaining all non-zero pixel values. We then applied the findContours function from the OpenCV [43] library and identified the largest contour as the ultrasound fan region. To address text annotations, colored regions within the images were detected and inpainted using the Fast Marching Method [44]. These preprocessing steps removed confounding information from fetal ultrasound images, facilitating unbiased model training.\nBased on the clinician annotations, we categorized this data source into three subgroups: (1) common standard anatomical views, (2) images with diverse clinical keywords, and (3) images without text annotations.\nSubgroup 1. This subgroup comprised 88,045 images labeled across 12 standard anatomical views [45], including the abdomen, brain, cord, diaphragm, feet, femur, heart, kidney, lips & nose, orbit, profile, and spine, with some examples shown in Extended Data Fig. 2a. To ensure high-quality labeling, a confident learning algorithm [46] was employed to detect potentially mislabeled samples. This process involved training a 5-fold cross-validation (CV) model to classify the 12 views, using a patient-wise dataset split to avoid data leakage across folds. The algorithm identified mislabeled samples by comparing the model's confidence in its predictions with the provided labels. A total of 984 samples were identified as potentially mislabeled (examples shown in Extended Data Fig. 2b) and subsequently excluded, resulting in a final dataset of 87,061 labeled images.\nFor the heart images, clinicians provided subview annotations for LVOT, RVOT, 4-CH, and 3VV/3VT, providing fine-grained information. In contrast, 468 brain images (out of 5,297) were labeled by an expert into three brain subviews, i.e. thalamic, cerebellum, and ventricular. These expert-labeled brain images were then used to train"}, {"title": "4.1.2 Fetal ultrasound textbook dataset", "content": "We extracted 849 image-caption pairs from a textbook [28] focused on fetal heart conditions. Since most figures in the textbook contained multiple subfigures, we manually separated these subfigures into 2,092 independent images. The corresponding captions were then divided into subcaptions and refined using GPT-40 to ensure they were self-contained and to eliminate references to visual markers (e.g. arrows), as illustrated in Extended Data Fig. 4c. Given that this data source was approximately 100 times smaller than the hospital data source, we upsampled the image-caption pairs from this source by a factor of 10 to increase its significance during FetalCLIP pretraining. In addition, we employed data sharding [47] where each shard contained unique image-caption pairs without duplicates. This strategy ensured that an image-caption pair from this data source did not appear multiple times in a single batch, which could otherwise negatively affect the contrastive learning process."}, {"title": "4.2 FetalCLIP architecture and pretraining", "content": "We developed the FetalCLIP model by adapting the architecture and training methodology employed in CLIP [11]. Additionally, our FetalCLIP pretraining code was built on top of the OpenCLIP [47, 48] repository with some modifications to suit our objectives. The FetalCLIP image encoder utilizes a ViT-L [29] architecture, featuring an image input size of 224x224, a patch size of 14\u00d714, and 24 transformer layers. For the text encoder, we implemented a text transformer [31] with 12 transformer layers and a maximum input token of 117-40 tokens more than the original CLIP model to accommodate clinical text descriptions, which are typically more detailed than natural captions. Both the image and text encoders project their respective inputs into a shared 768-dimensional space. We pretrained the FetalCLIP model to maximize the similarity between embeddings of paired fetal ultrasound image-caption data while minimizing the similarity of unpaired images and captions. This pretraining strategy enables FetalCLIP to derive semantically rich feature embeddings from fetal ultrasound images and their associated captions."}, {"title": "4.3 Zero-shot view classification", "content": "For zero-shot standard view classification with vision-language foundation models (VLMs), we first defined target classes and provided five text prompts per class to enable prompt ensembling for improved robustness, as VLMs are sensitive to text prompts. We used GPT-40 to generate the text prompts for inferencing as detailed in Extended Data Fig. 5. Our investigation revealed that specifically for FetalCLIP, incorporating caption templates of our routine prenatal scan data to generate text prompts for inference improved the average F1 score by 2.74% across five standard fetal planes: abdomen, brain, femur, heart, and cervix. During testing, each text prompt was converted into a 768-dimensional text embedding using the VLM tokenizer and encoder, and embeddings were averaged across prompts for every class. Input images were similarly encoded into 768-dimensional embeddings, and classification was performed by selecting the class with the highest cosine similarity between the image embedding and the text embeddings. The average F1 score across all target classes was reported as the main metric for performance.\nThis zero-shot performance was evaluated using the Planes DB [32] dataset. The dataset contains six classes: abdomen, femur, brain, thorax, cervix, and a class labeled \"other,\" representing diverse additional views. The brain category is further divided into three fine-grained subviews: cerebellum, thalamic, and ventricular. Each image was preprocessed by symmetrically padding with zeros to form a square and subsequently resizing to a uniform dimension of 224\u00d7224. We first evaluated the performance on distinguishing five standard views, excluding the \"other\" class, resulting in 8187 test images. To ensure a fair comparison with the specialist model (SonoNet), nine standard views were selected as target classes, i.e. abdomen, brain, femur, heart, kidney, lips & nose, profile, spine, and cervix. Secondly, we evaluated the model's ability to classify the three brain subviews, resulting in 2949 test images. As SonoNet lacks dedicated classes for both cervix and thalamic, we mapped the \"other\" class in SonoNet to cervix and thalamic for the first and second evaluations, respectively.\nIn addition, the performance of the SonoNet models varied across sizes, with SN16, SN32, and SN64 achieving F1 scores of 67.5%, 69.9%, and 67.2%, respectively, where the mid-sized model (SN32) demonstrated the best performance."}, {"title": "4.4 Zero-shot gestational age estimation", "content": "We utilized the HC18 dataset [33] to evaluate VLM's ability to estimate gestational age (GA). The dataset includes fetal brain images alongside head circumference (HC) measurements and pixel spacing information. Since the true GA values are not provided, our evaluation was based on the ground-truth HC. The relationship between HC and GA can be explained using the following quantile regression formula established by the WHO [34]:\n$HC = b_0 + b_1t + b_2t^2 + b_3t^3 + b_4t^4$ (1)\nHere, $t$ represents GA in days, and $b_0, b_1, b_2, b_3, b_4 \\in R$ are coefficients that depend on the quantile of interest. Equation 1 applies to GA ranging from 14 to 40 weeks. Consequently, we restricted the analysis to images with HC values between 100 mm and 342 mm, corresponding to the 50th percentile HC at 14 and 40 weeks of GA, respectively, resulting in a test set of 814 brain images. To assess the plausibility of the predicted GA, we formulated a proxy validation task: predictions were deemed valid if the true HC fell within the 2.5th to 97.5th percentile range (95% of the population distribution) associated with the predicted GA.\nInspired by [22], we estimated GA by first extracting image embeddings and subsequently constructing text prompts that describe the brain view, pixel spacing, and GA values ranging from 14 weeks 0 days to 40 weeks. Similar to our zero-shot view classifier, we provided five text prompts for each GA to enable ensembling. The cosine similarity between the image and the text prompts was computed for each GA, and ensembling was performed by averaging the cosine similarities across the five prompts for a given GA. As a postprocessing step, the final GA prediction was selected as the median of the GAs corresponding to the top 15 text prompts with the highest cosine similarity to the image embeddings. This postprocessing step enhanced the accuracy of GA estimation, raising the validity rate by +3.08% compared to selecting the GA based solely on the highest cosine similarity."}, {"title": "4.5 Probing FetalCLIP for downstream tasks", "content": "The probing experiments were conducted on downstream fetal ultrasound tasks to evaluate the quality of image embeddings from FetalCLIP compared to other foundation models. In these experiments, the image encoder was frozen, and a trainable prediction head was attached. We compared FetalCLIP with both natural and medical vision-language foundation models across various fetal ultrasound benchmarks, with P values computed using the Wilcoxon signed-rank test. Unless otherwise specified, for each evaluation, the corresponding dataset was split by patients into training (80%) and testing (20%) sets to prevent information leakage in the test data due to patient attributes, with stratified splitting employed for classification tasks. The prediction head was tuned and validated using the training set, leaving the testing set remain untouched during model development. In addition, we applied the same preprocessing steps as in the zero-shot experiments, resulting in standardized images with dimensions of 224x224. We then augmented the training images offline multiple times and stored them locally, enabling pre-computation of image embeddings to accelerate the training process. Details of the dataset used for downstream tasks along with the training configurations are presented in Extended Data Table 1.\nWe considered two training scenarios: full-data training and data-efficient training with few patients. For full-data training experiments, we performed stratified 5-fold cross-validation on the training set. The prediction head was trained, and the model with the lowest validation loss was evaluated on the test set. To achieve reliable statistical evaluation, each fold was run five times with different random seeds, resulting in a total of 25 runs. For data-efficient training, we randomly selected five support sets, each consisting of N patients for training and N for validation, while evaluation was performed on the testing set. Each support set was run with five different random seeds, resulting in a total of 25 outcomes per experiment."}, {"title": "4.5.2 View classification", "content": "The probing for view classification experiments was carried out using the Planes DB [32] dataset, with the average F1 score computed as the performance metric. The"}, {"title": "4.5.3 CHD detection", "content": "We evaluated the transferability of image embeddings from foundation models to fetal ultrasound video analysis using an internal dataset. We collected 418 four-chamber fetal heart ultrasound videos, comprising 161 normal and 257 abnormal scans, with temporal lengths ranging from 16 to 128 frames, labeled by an expert in fetal cardiology to ensure diagnostic accuracy. To simplify the task, instead of processing the entire sequences, we extracted 16-frame clips for diagnosis. For videos with up to 64 frames, we sampled clips with approximately uniform spacing, while for longer videos"}]}