{"title": "Enhancing Early Diabetic Retinopathy Detection through Synthetic DR1 Image Generation: A StyleGAN3 Approach", "authors": ["Sagarnil Das", "Pradeep Walia"], "abstract": "Diabetic Retinopathy (DR) is a leading cause of preventable blindness, with early detection at the DR1 stage being critical but hindered by a scarcity of high-quality fundus images. This study leverages StyleGAN3 to generate synthetic DR1 images characterized by microaneurysms with high fidelity and diversity to address this data scarcity and enhance the performance of supervised classifiers. A dataset of 2,602 DR1 images was used to train the model, followed by a comprehensive evaluation employing quantitative metrics, including Fr\u00e9chet Inception Distance (FID), Kernel Inception Distance (KID), and Equivariance with respect to translation (EQ-T) and rotation (EQ-R). Qualitative assessments involved Human Turing tests, where trained ophthalmologists evaluated the realism of synthetic images. Spectral analysis further validated the images' quality. The model achieved a final FID score of 17.29, significantly outperforming the mean FID of 21.18 (95% CI: 20.83\u201321.56) derived from bootstrap resampling. Human Turing tests demonstrated the model's capacity to produce highly realistic images, though some artifacts near the borders were noted. These findings suggest that StyleGAN3-generated synthetic DR1 images hold significant promise for augmenting training datasets, enabling more accurate early detection of Diabetic Retinopathy. This methodology highlights the potential of synthetic data in advancing medical imaging and AI-driven diagnostics.", "sections": [{"title": "1. Introduction", "content": "Diabetic Retinopathy (DR) is one of the leading causes of preventable blindness in the world. The first stage, DR1, is characterized by the presence of microaneurysms [1], which are the earliest visible signs of vascular damage in the retina. Detecting these microaneurysms is critical for initiating timely interventions to prevent disease progression. However, their small size, low contrast, and similarity to other retinal structures make them challenging to identify, even for current AI models, which rely heavily on large annotated datasets for training. Consequently, the availability of high-quality fundus photography with microaneurysms is sparse, leading to poor Al model performance [2] and perpetuating a vicious loop of missed early detections and insufficient data.\nRecent advancements in Generative Adversarial Networks (GANs), particularly StyleGAN3 [3], offer a promising solution to this problem through synthetic data augmentation [4]. StyleGAN3 employs rotationally invariant convolutional layers in its architecture [5], which preserve the geometric and textural fidelity of the generated images. Using these synthetic images, we can closely mimic the spectral and visual characteristics of real fundus images at the DR1 stage.\nThis study focuses on leveraging StyleGAN3 to generate synthetic fundus images in the DR1 set. Our approach aims to explore the effectiveness of synthetic data in improving supervised classifiers to predict DR1 with high accuracy. Furthermore, the scalability of"}, {"title": "2. Materials and Methods", "content": "A large dataset of DR1 stage images was pivotal to our study's success. Unfortunately, we were able to find only 2,602 DR1 images. These images were used in training our StyleGAN3 model to generate synthetic fundus images. The sources of our DR1 images included: Messidor Dataset (Versions 1 & 2), Kaggle Diabetic Retinopathy Detection Dataset, and a proprietary dataset collected using Crystalvue NFC-600 Fundus Camera images.\nTo ensure the quality and relevance of the data, the following steps were undertaken:\n\u2022 Annotation and Verification: Each image in the dataset was carefully annotated by trained optometrists, who identified the DR1 stage based on the presence of microaneurysms.\n\u2022 Quality Control: A total of 345 images that were blurry, improperly illuminated, or otherwise unsuitable for training purposes were excluded.\n\u2022 Standardization: All images were resized to a uniform dimension of 512 \u00d7 512 pixels and pre-processed to ensure consistency during training.\nTo enhance the diversity of the training data and improve model robustness, data augmentation techniques such as flipping, rotation, scaling, and color adjustments were applied. These augmentations allowed the model to learn invariant representations of microaneurysms under various transformations, significantly improving metric outcomes. For example, the application of these techniques contributed to lower Fr\u00e9chet Inception Distance (FID) and Kernel Inception Distance (KID) scores by ensuring that the synthetic images better mirrored the diversity and characteristics of the real dataset.\nStyleGAN3, the latest advancement by NVIDIA, represents a significant leap in Generative Adversarial Networks (GANs) [6], particularly for generating images with improved translation equivariance and reduced artifacts compared to its predecessors [7,8]. StyleGAN3 introduces architectural innovations that produce visually appealing images while adhering closely to the spectral characteristics of real images.\nThe original StyleGAN3 was trained on datasets such as FFHQ-U, FFHQ, METFACES-U, METFACES, AFHQV2, and BEACHES. However, we trained the network from scratch using our DR1 dataset. This approach enabled the creation of a model tailored for DR1 fundus images and allowed us to use it as a backbone for transfer learning on other DR stages.\nThe main features of StyleGAN3 are:\nAlias-Free Design: StyleGAN3's alias-free architecture reduces artifacts caused by digital operations like sampling and transformations, ensuring the generated images are of high fidelity even after transformations. This is particularly crucial for accurately representing fine features like microaneurysms.\nTranslation Equivariance: The model focuses on maintaining translation equivariance [9], which ensures that image details remain correctly positioned and oriented. This property is essential in medical imaging to preserve the spatial and structural integrity of retinal features. To evaluate the impact of translation equivariance, the Peak Signal-to-"}, {"title": "Training Methodology", "content": "A robust methodology was designed to generate high-fidelity, diverse DR1 fundus images using the StyleGAN3 architecture. The StyleGAN3 model was configured with specific hyperparameters to optimize its performance in generating high-fidelity and diverse DR1 fundus images. The generator utilized zdim and Wdim values of 512, a mapping network with two layers, and a channel base of 32,768, capped at 512 channels. An exponential moving average (EMA) with a \u03b2 value of 0.998 stabilized training.\nThe discriminator architecture mirrored the generator's channel configuration and included micro-batch standard deviation grouping in its final layer to enhance generalization. Optimization employed the Adam optimizer for both the generator and discriminator, with learning rates of 0.0025 and 0.002, respectively, and betas set to [0, 0.99]. R1 regularization (\u03b3 = 8.0) was applied explicitly to real images, ensuring gradient stability.\nThe learning rates for the generator (0.0025) and discriminator (0.002) were chosen to facilitate gradual convergence without overshooting optimal parameter values. The exponential moving average (EMA) with a \u03b2 value of 0.998 was used to stabilize training over iterations.\nData preprocessing and augmentation included resizing images to 512 \u00d7 512 pixels, with techniques like flipping, rotation, scaling, and color adjustments. Training was conducted on an NVIDIA RTX-4090 GPU (24 GB) with a batch size of 32, subdivided into sub-batches of 16 samples per GPU. A batch size of 32 was selected to balance memory usage and computational efficiency on the NVIDIA RTX-4090 GPU, which has 24 GB of VRAM. Sub-batching into 16 samples per GPU enabled stable and efficient training while maintaining a high sample throughput. EMA updates were performed every 10,000 images, targeting an ADA value of 0.6. The training process spanned approximately 12 days."}, {"title": "Loss Function", "content": "The StyleGAN3 model incorporates an R1 regularization term into the discriminator's loss function to stabilize training [11]. This regularization controls gradients, preventing destabilization during learning [12]. The R1 regularization for real images is defined as:\n$L_{R1} = \\frac{\\gamma}{2} \\mathbb{E}_{x \\sim P_{data}}[\\lVert \\nabla D(x) \\rVert^2]$"}, {"title": "Quantitative evaluation", "content": "To validate the quality and diversity of the generated images, the following metrics were used:\n\u2022 Fr\u00e9chet Inception Distance (FID): Used to measure the quality and diversity of generated images relative to real ones [13].\n\u2022 Kernel Inception Distance (KID): Provides a complementary measure of distribution similarity between real and generated images [14].\n\u2022 Equivariance Metrics (EQ-T and EQ-R): Measures the model's ability to maintain image fidelity under translations and rotations [15]."}, {"title": "Qualitative evaluation", "content": "We conducted a Human Turing Test [16] with six trained ophthalmologists to assess the realism of synthetic images."}, {"title": "Statistical Validation", "content": "Bootstrap Resampling: Applied to the latter 30% of training epochs to estimate the population mean and construct 95% confidence intervals for FID scores [17]."}, {"title": "3. Results", "content": "This section presents a detailed analysis of the results obtained through quantitative and qualitative evaluations of the synthetic DR1 images generated using the StyleGAN3 model. The experimental results are structured under subheadings for clarity and interpretation."}, {"title": "3.1. Quantitative Evaluation", "content": "The quantitative evaluation employed several metrics to assess the quality and fidelity of the synthetic images compared to the real DR1 dataset. These metrics include the Fr\u00e9chet Inception Distance (FID), Kernel Inception Distance (KID), and Equivariance metrics (EQ-T and EQ-R)."}, {"title": "3.1.1. Fr\u00e9chet Inception Distance (FID)", "content": "The FID score, which evaluates the similarity between real and synthetic datasets, achieved a value of 17.29 . This score indicates a strong resemblance between the generated and real images, demonstrating the StyleGAN3 model's ability to produce high-quality synthetic images. Bootstrap resampling of the final 30% of training epochs confirmed that the mean FID score was 21.18 (95% CI: 20.83\u201321.56), making the achieved FID of 17.29 statistically significant. Compared to similar studies in medical image synthesis, where FID scores typically range from 20\u201330, this result places our synthetic data among the top-performing models for medical imaging tasks. The low FID score ensures that the synthetic dataset can effectively augment real data in classifier training without compromising quality."}, {"title": "3.1.2. Kernel Inception Distance (KID)", "content": "The KID score, which measures the consistency between distributions of real and synthetic images, was recorded as 0.018. This low value signifies a substantial alignment between the two datasets, further affirming the effectiveness of the StyleGAN3 model in replicating the real data's distribution."}, {"title": "3.1.2. Equivariance Metrics (EQ-T and EQ-R)", "content": "The equivariance metrics quantify the fidelity of synthetic images under translations and rotations. The EQ-T and EQ-R scores were 65.65 and 64.64, respectively. These scores indicate that the synthetic images preserve structural integrity and diagnostic features even when subjected to geometric transformations. In clinical settings, this robustness ensures that classifiers trained on these synthetic images can generalize better across varied conditions, such as slight shifts or rotations in fundus photography during patient examinations."}, {"title": "3.2. Qualitative Evaluation", "content": "A Human Turing Test was conducted with a panel of six experienced ophthalmologists to qualitatively assess the realism of the synthetic images. Each participant evaluated 200 images (100 real and 100 synthetic), classifying them as \u201creal\u201d or \u201cfake.\u201d"}, {"title": "4. Discussion", "content": "This study demonstrates a novel approach to tackling the challenge of early Diabetic Retinopathy (DR) detection by leveraging StyleGAN3 to generate synthetic DR1 images. Here, we discuss the implications, limitations, and future directions of this work."}, {"title": "4.1. Synthetic Image Generation and Its Implications", "content": "Detecting microaneurysms in DR1 images is a challenging task, even for experts, due to their small and subtle nature . The quantitative metrics, particularly the low FID (17.29) and KID (0.018) scores, indicate that the synthetic images closely resemble real ones, both visually and in distribution. These results validate the utility of synthetic data as a reliable substitute for real data in training supervised classifiers, especially given the scarcity of early-stage DR images. This approach holds potential to significantly enhance model performance for early DR detection.\nIn underserved regions with limited access to annotated medical imaging data, the use of synthetic data provides an invaluable resource for building AI models. The ability to generate high-quality images resembling real DR1 cases allows clinicians and researchers to overcome data scarcity, train robust classifiers, and implement AI-based screening solutions in remote or resource-constrained settings. By improving early DR detection, synthetic data can contribute to reducing preventable blindness in areas with insufficient diagnostic infrastructure."}, {"title": "4.2. Realism of Synthetic Images", "content": "The Human Turing Test revealed that trained ophthalmologists could distinguish real images from synthetic ones, primarily due to minor artifacts near the image boundaries. While these artifacts highlight areas for improvement, they did not diminish the overall diagnostic value of the images for classifier training. Importantly, these limitations stem from the relatively small size of the training dataset and do not undermine the broader utility of synthetic images in addressing data scarcity."}, {"title": "4.3. Spectral Fidelity as an Attribute for Medical Imaging", "content": "Spectral analysis validated that the synthetic images preserved critical diagnostic features in the frequency domain. The synthetic images closely matched the spectral properties of real images in the central regions, which are most relevant for medical diagnosis. Discrepancies near the edges, consistent with findings from the Turing test, emphasize the need for enhanced preprocessing and potentially larger training datasets. Preserving such spectral fidelity is crucial for generating high-quality synthetic images for medical applications."}, {"title": "4.4. Limitations and Future Directions", "content": "Although the results are encouraging, several limitations must be addressed. The small size of the training dataset (2,602 images) constrained the model's performance, particularly in eliminating edge artifacts. Expanding the dataset through further annotation and image collection is an immediate priority. Our team has initiated efforts to annotate additional images to mitigate this limitation and improve the robustness of future models.\nLooking ahead, we plan to use this StyleGAN3 model as a backbone for developing generative models for higher DR levels (e.g., DR2 and DR3). These stages present additional challenges, such as more complex pathological features, requiring further fine-tuning of the generative process. Beyond DR, this methodology can be extended to other medical imaging modalities, particularly those where data scarcity hinders the development of robust AI models."}, {"title": "5. Conclusions", "content": "This study demonstrates that StyleGAN3 can generate high-fidelity, diverse DR1 images to augment supervised training datasets. By addressing data scarcity, this approach has the potential to significantly improve early detection and treatment of Diabetic Retinopathy, ultimately reducing preventable blindness. The broader adoption of synthetic data in medical imaging could pave the way for transformative advancements in AI-driven diagnostics."}]}