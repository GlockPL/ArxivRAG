{"title": "AI Benchmarks and Datasets for LLM Evaluation", "authors": ["Todor Ivanov", "Valeri Penchev"], "abstract": "This paper provides a comprehensive overview of AI benchmarks and datasets relevant for evaluating Large Language Models (LLMs), with a particular focus on their alignment with the EU AI Act and the COMPL-AI framework. The paper emphasizes the importance of quantitative evaluation to identify systemic vulnerabilities in AI systems and bolster system trustworthiness. A detailed list of benchmarks and datasets with summaries is also provided.", "sections": [{"title": "Introduction", "content": "In the age of Artificial Intelligence, large language models (LLMs) have become the preferred tool for many everyday tasks. As multi-functional and multi-modal LLMs, they can process diverse data formats, including images, audio, and video. Typical tasks encompass text generation, logical reasoning, machine translation, summarization, and multimodal support [25]. As a class of deep learning models, LLMs contain billions to trillions of parameters and are trained on vast datasets using complex, multi-layered neural network architectures, surpassing other neural network approaches. Nevertheless, their development and supervision remain challenging.\nLLMs demand significant computational resources for both pre-training and fine-tuning, requiring distributed computing capabilities due to their large model sizes [24]. Their complex architecture poses challenges throughout the entire AI lifecycle, from data collection to deployment and monitoring [20]. Addressing critical AI system challenges, such as explainability, corrigibility, interpretability, and hallucination, necessitates a systematic methodology and rigorous benchmarking [8]. To effectively improve AI systems, we must precisely identify systemic vulnerabilities through quantitative evaluation, bolstering system trustworthiness.\nThe Z-Inspection [45] represents a pioneering, comprehensive approach to these challenges. This methodology assesses AI system trustworthiness through a holistic, participatory framework that integrates ethical principles from the EU guidelines [7] and other sources. By employing socio-technical scenarios, it systematically identifies potential risks and ethical tensions inherent in AI systems.\nThe enactment of the EU AI Act [6] by the European Parliament on March 13, 2024, establishing the first comprehensive EU-wide requirements for the development, deployment, and use of AI systems, further underscores the importance of tools and methodologies such as Z-Inspection. It highlights the need to enrich this methodology with practical benchmarks to effectively address the technical challenges posed by AI systems. To this end, we have launched a"}, {"title": "Related Work", "content": "project that is part of the AI Safety Bulgaria initiatives [1], aimed at collecting and categorizing AI benchmarks. This will enable practitioners to identify and utilize these benchmarks throughout the AI system lifecycle.\nThe recently introduced COMPL-AI framework [3, 8] underscores the importance of our project. This open-source, compliance-centered evaluation framework for generative AI models provides the first comprehensive technical interpretation of the EU AI Act [6] in the context of LLMs. By proposing the first LLM benchmarking suite based on the EU requirements for Trustworthy AI [7], COMPL-AI highlights the significant gap in LLM compliance. As observed by the framework's authors, no popular LLM currently adheres to the non-technical requirements of the EU AI Act, and several regulatory requirements, such as explainability and corrigibility, lack adequate technical evaluation tools.\nThe remainder of the paper elaborates on Z-Inspection, the EU AI Act, and the COMPL-AI framework, followed by a comprehensive list of AI benchmarks and dataset summaries."}, {"title": "Z-Inspection", "content": "The Z-InspectionR [45, 53, 35] is a novel process grounded in applied ethics to evaluate the trustworthiness of AI systems. It uses a holistic, participatory approach, incorporating ethical principles from the EU framework and other sources, and employing socio-technical scenarios to identify potential risks and ethical tensions. The process involves assembling an interdisciplinary team, analyzing claims and evidence, and mapping issues to ethical principles. Finally, Z-Inspection offers recommendations for mitigating risks and promoting responsible AI development and deployment, with several case studies demonstrating its application. Z-inspection can be applied to a variety of domains such as business, healthcare, public sector, among many others. It leverages the definition of trustworthy AI provided by the European Union's High-Level Expert Group on Artificial Intelligence. The 7 Key EU requirements for Trustworth AI [7] defined as follows:\n\u2022 Human Agency and Oversight [HAO]: AI systems should empower human beings, allowing them to make informed decisions and fostering their fundamental rights. At the same time, proper oversight mechanisms need to be ensured, which can be achieved through human-in-the-loop, human-on-the-loop, and human-in-command approaches.\n\u2022 Technical Robustness and Safety [TRS]: AI systems need to be resilient and secure. They need to be safe, ensuring a fall back plan in case something goes wrong, as well as being accurate, reliable and reproducible. That is the only way to ensure that also unintentional harm can be minimized and prevented."}, {"title": "EU AI Act", "content": "The EU AI Act [6], passed by the European Parliament on March 13, 2024, represents the first comprehensive regulatory framework for artificial intelligence, establishing EU-wide requirements for the development, deployment, and use of AI systems. The regulation seeks to ensure that the benefits of these technologies outweigh potential risks by mandating safe, reliable, transparent, and sustainable practices. The Act categorizes AI systems into four risk levels:\n\u2022 Prohibited AI systems\n\u2022 High-risk AI systems\n\u2022 Limited-risk AI systems\n\u2022 Minimal-risk AI systems"}, {"title": "COMPL-AI Framework", "content": "The majority of regulatory obligations are imposed on providers (developers) of high-risk AI systems. In this context, users are defined as natural or legal persons who deploy an AI system in a professional capacity, distinct from end-users who are ultimately affected by the system. Regarding General Purpose AI (GPAI) model providers, the Act mandates:\n\u2022 For all GPAI model providers:\nProvide comprehensive technical documentation\nSupply detailed instructions for use\nComply with the Copyright Directive\nPublish a summary of training content\n\u2022 For free and open-source GPAI model providers:\nComply with copyright requirements\nPublish a summary of training data\nExempt from additional requirements unless presenting a systemic risk\n\u2022 For GPAI models presenting a systemic risk (whether open or closed):\nConduct thorough model evaluations\nPerform adversarial testing\nTrack and report serious incidents\nImplement robust cybersecurity protections\nThe COMPL-AI framework [3, 8] is an open-source, compliance-centered evaluation framework for generative AI models. It aims to bridge an existing gap by providing the first comprehensive technical interpretation of the EU AI Act [6] in the context of LLMs, and by proposing the first regulation-oriented LLM benchmarking suite. The COMPL-AI approach first extracts the legal requirements that the EU AI Act imposes across the seven EU requirements for Trustworthy AI [7] listed above, and translates them into a comprehensive set of technical requirements. The interpretation relies on the terminology and focus of state-of-the-art technical AI research to guide its analysis. The result is a hierarchical benchmarking suite that closely follows the structure of the EU AI Act, enabling practitioners to easily interpret their results within the Act's context. The benchmarking suite, comprising approximately 27 benchmarks, was executed across 12 state-of-the-art LLMs in relation to the criteria imposed by the EU AI Act. Notably, none of the examined models are fully compliant with the Act's requirements. Moreover, certain technical requirements remain"}, {"title": "Benchmarks and Datasets", "content": "challenging to assess due to current limitations in tools and benchmarks - either because of an incomplete understanding of relevant model aspects (such as explainability) or due to inadequacies in existing benchmarking methodologies (particularly in areas like privacy assessment)."}, {"title": "The Adversarial Natural Language Inference (ANLI)", "content": "The Adversarial Natural Language Inference (ANLI) [19] is an iterative, adversarial human-and-model-in-the-loop enabled training (HAMLET) solution for natural language understanding (NLU) dataset collection that addresses both benchmark longevity and robustness issues. The dataset used here comprises approximately 100 000 samples for the training set, 1200 for the development set, and 1200 for the test set, with each sample containing a context, a hypothesis, and a label. The goal is to determine the logical relationship between the context and the hypothesis by using the label, which is the assigned category indicating that relationship. Finally, ANLI makes available a reason (provided by the HAMLET), explaining why a sample was misclassified."}, {"title": "HellaSwag", "content": "HellaSwag [47] is a benchmark for commonsense natural language inference (NLI), comprising 70,000 question instances. For each question, a model is given a context from a video caption and four ending choices for what might happen next with only one correct choice representing the actual next caption of the video. The dataset, covering diverse domains of world knowledge and logical reasoning for successful interpretation, employs Adversarial Filtering to incorporate machine-generated incorrect responses, which humans can easily solve (95.6% accuracy), yet challenging for machines (<50% accuracy)."}, {"title": "CommonsenseQA", "content": "CommonsenseQA [32] is a multiple-choice question-answering dataset that contains 12,247 questions and aims to test commonsense knowledge by predicting the correct answers (1 correct and 4 distractor answers). The questions are crowdsourced and based on knowledge encoded in CONCEPTNET [26], covering a wide range of topics from real-life situations, elementary science, and social skills."}, {"title": "CNN/Daily Mail", "content": "The CNN/Daily Mail [18] is a widely used dataset based on human generated abstractive summary bullets from new-stories in CNN and Daily Mail websites as questions (with one of the entities hidden), and stories as the corresponding passages from which the system is expected to answer the fill-in-the-blank question. In total, the corpus contains 286,817 training, 13,368 validation, and 11,487 test pairs."}, {"title": "Massive Multitask Language Understanding", "content": "The Massive Multitask Language Understanding (MMLU) [9] benchmark is a comprehensive evaluation framework designed to test the knowledge and problem-solving capabilities of large language models across a wide range of academic and professional domains. Created to provide a more rigorous and diverse assessment than previous benchmarks, MMLU covers 57 different subjects including elementary mathematics, US history, computer science, law, and many scientific disciplines.\nThe MMLU benchmark consists of multiple-choice questions that require both broad knowledge and the ability to apply that knowledge to solve complex problems. It tests models not just on factual recall, but on their ability to reason, analyze, and draw conclusions across different fields of study."}, {"title": "Massive Multitask Language Understanding - Pro", "content": "Massive Multitask Language Understanding-Pro (MMLU-Pro) [40] is a comprehensive benchmark enhancing and extending the mostly knowledge-driven MMLU [9] benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from 4 to 10 options. It spans 14 diverse domains including mathematics, physics, chemistry, law, engineering, psychology, and health, encompassing over 12,000 questions and thus meeting the breadth requirement for evaluation of multi-task language understanding capabilities in LLMs."}, {"title": "Google-Proof Q&A Benchmark", "content": "Google-Proof Q&A Benchmark (GPQA) [22] is a challenging dataset 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. The questions are high-quality and extremely difficult, with 74% estimated objectivity on the basis of expert assessment, and 34% accuracy by highly skilled, resourced, and motivated non-experts."}, {"title": "Multistep Soft Reasoning", "content": "Multistep Soft Reasoning (MuSR) [27] is a reasoning dataset for evaluating LLMs on multistep soft reasoning tasks specified in a natural language (from either murder mysteries, object placement questions, or team allocation domains) using a novel neurosymbolic synthetic-to-natural generation algorithm, which can be scaled in complexity as more powerful models emerge."}, {"title": "Mathematics Aptitude Test of Heuristics", "content": "Mathematics Aptitude Test of Heuristics (MATH) [10] is a benchmark/dataset consisting of 12 500 problems from high school math competitions that measures the problem-solving ability of LLMs. Each problem has a step-by-step solution, final boxed answer, a difficulty tag from 1 to 5, and is one of the 7 subjects like Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus."}, {"title": "Instruction Following Evaluation", "content": "Instruction Following Evaluation (IFEval) [51] is a benchmark for evaluating the proficiency of large language models in instruction following. It consists of a list of 25 verifiable instructions and a set of 541 prompts, with each prompt containing one or multiple verifiable instructions. Also 4 strict accuracy scores are defined to evaluate each model."}, {"title": "Big Bench Hard", "content": "Big Bench Hard (BBH) [31] is a diverse evaluation suite of 23 challenging tasks (27 sub-tasks and in total 6511 evaluation examples) in different categories such as traditional NLP, mathematics, commonsense reasoning, and question-answering with the goal to test the capabilities of large language models using objective metrics."}, {"title": "Cord-19", "content": "CORD-19 [39] is a large collection of publications and pre-prints on COVID-19 and related historical coronaviruses such as SARS and MERS. It now consists of over 140K papers with over 72K full texts, with papers in Medicine (55%), Biology (31%), and Chemistry (3%), which together constitute almost 90% of the corpus. CORD-19 was designed to facilitate the development of text mining and information retrieval systems over its rich collection of metadata and structured full text papers."}, {"title": "LAMBADA", "content": "LAMBADA dataset [21] is a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. It consists of 10,022 passages, divided into 4,869 development and 5,153 test passages with an average passage consisting of 4.6 sentences in the context plus 1 target sentence, for a total length of 75.4 tokens (dev)/ 75 tokens (test). Preliminary experiments suggest that even some cutting-edge neural network approaches that are in principle able to track long-distance effects are far from passing the LAMBADA challenge."}, {"title": "TriviaQA", "content": "TriviaQA [11] is a reading comprehension dataset consisting of high percentage of challenging questions with substantial syntactic and lexical variability and often requiring multi-sentence reasoning. TriviaQA contains over 650K question-answer-evidence triples, that are derived by combining 95K Trivia enthusiast authored question-answer pairs with on average six supporting evidence documents per question coming from two domains - Web search results and Wikipedia pages. TriviaQA also provides a benchmark for a variety of other tasks such as IR-style question answering, QA over structured KBs and joint modeling of KBs and text, with much more data than previously available."}, {"title": "WinoGrande", "content": "WinoGrande [23] is a large-scale dataset of 44k problems, inspired by the original Winograd Schema Challenge (WSC) design (a set of 273 expert-crafted pronoun resolution problems originally designed to be unsolvable for statistical models that rely on selectional preferences or word associations), but adjusted to improve both the scale and the hardness of the dataset. The key steps of the dataset construction consist of (1) a carefully designed crowdsourcing procedure, followed by (2) systematic bias reduction using a novel AFLITE algorithm that generalizes human-detectable word associations to machine-detectable embedding associations. WINOGRANDE as a resource, demonstrates effective transfer learning and achieve state-of-the-art results on several related benchmarks."}, {"title": "CausalBench", "content": "CausalBench [41] is a comprehensive benchmark for evaluating the causal reasoning capabilities of LLMs. It comprises four perspectives of causal reasoning for each scenario: cause-to-effect, effect-to-cause, cause-to-effect with intervention, and effect-to-cause with intervention. CausalBench includes a diverse set of problem types spanning textual, mathematical, and coding domains, enabling a comprehensive assessment of causal reasoning abilities across different modalities. The benchmark consists of more than 60,000 problems and employs six evaluation metrics to measure LLMs' causal reasoning performance."}, {"title": "CausalBench (2)", "content": "CasulaBench(2) [52] is a comprehensive benchmark that encompasses three causal learning-related tasks: to identify correlation, causal skeleton, and causality. It incorporates 15 commonly used real-world causal learning datasets of diverse sizes, enabling comprehensive comparisons of LLMs' performance with the classic causal learning algorithms. CausalBench uses four distinct prompt formats, which include one or more elements of variable names, background knowledge, and structured data and covers causal discovery datasets of various scales, ranging from 5 to 109 nodes, far exceeding what current evaluation works have explored."}, {"title": "BackdoorLLM", "content": "BackdoorLLM [13] is a comprehensive benchmark for studying backdoor attacks on LLMs with a repository designed to facilitate research on backdoor attacks in LLMs. It includes a standardized pipeline for training backdoored LLMs with diverse strategies, such as data poisoning, weight poisoning, hidden state steering, and chain-of-thought attacks. The study (with over 200 experiments on 8 attacks across 7 scenarios and 6 model architectures) provides new insights into the nature of backdoor vulnerabilities in LLMs, which will aid in developing future defense methods against LLM backdoor attacks."}, {"title": "ConflictBank", "content": "ConflictBank [28] is a novel and comprehensive developed to systematically evaluate knowledge conflicts from three aspects: (i) conflicts encountered in retrieved knowledge, (ii) conflicts within the models' encoded knowledge, and (iii) the interplay between these conflict forms. It contains a large diverse dataset of 553K QA pairs and 7M knowledge conflict evidence in high quality. The study also presents in-depth pilot experiments on twelve LLMs across four model series and provide comprehensive analyses about model scales, conflict causes, and conflict types."}, {"title": "Reefknot", "content": "Reefknot [49] is a comprehensive benchmark called Reefknot to evaluate and mitigate relation hallucinations in multimodal large language models (MLLMs). Its dataset is constructed from over 20k data through a scene graph-based construction pipeline, covering two discriminative tasks (Y/N and MCQ) and one generative task (VQA). The paper also proposes a Detect-then-Calibrate method to mitigate the relation hallucination via entropy threshold, with an average reduction of 9.75% in the hallucination rate across Reefknot and two other representative relation hallucination datasets."}, {"title": "DQA", "content": "DQA [50] is the first comprehensive database Q&A benchmark (DQA) consisting of a large-scale dataset with 240,000 Q&A pairs. It also proposes a plug-and-play testbed encapsulating all components potentially involved in the database Q&A, such as Question-Categorization Routing (QCR), Prompt-Template Engineering (PTE), Retriever-Augmented Generation (RAG) and Tool-Invocation Generation (TIG). Using DQA, the study also conducted a comprehensive evaluation to showcase DB Q&A ability of seven general-purpose LLMs and two variants based on pre-training and fine-tuning evaluates."}, {"title": "MultiTrust", "content": "MultiTrust [48] is a comprehensive benchmark designed to evaluate the trustworthiness of Multimodal Large Language Models (MLLMs). The benchmark covers five key aspects of trustworthiness: truthfulness, safety, robustness, fairness, and privacy. It employs a rigorous evaluation strategy that addresses both multimodal risks (new risks introduced by the visual modality) and cross-modal impacts (how the visual modality affects the performance on original text tasks). The authors conducted extensive experiments on 21 MLLMs, revealing that open-source models, despite their progress in general capabilities, still lag behind proprietary models in trustworthiness. The benchmark also highlights the detrimental effects of multimodality on MLLMs' trustworthiness, emphasizing the need for further research to enhance their reliability. To facilitate future research, the authors release a scalable toolbox for standardized trustworthiness evaluation."}, {"title": "LTLBench", "content": "LTLBench [33] is a benchmark designed to evaluate the temporal reasoning capabilities of Large Language Models (LLMs). Temporal reasoning, crucial for AI understanding of event sequences and relationships, is assessed in LLMs using various datasets. The authors propose a novel pipeline for constructing such datasets, leveraging random directed graphs, Linear Temporal Logic (LTL) formulas, and the NuSMV model checker. This approach allows for the generation of diverse and scalable temporal reasoning problems. The authors used this pipeline to create LTLBench, a dataset of 2,000 temporal reasoning challenges, and evaluated six LLMs on it. Results show LLMs exhibit promise in handling temporal reasoning but still struggle with complex scenarios. The work contributes a valuable tool for evaluating and improving the temporal reasoning abilities of LLMs and other AI systems."}, {"title": "Large Langugage Model in the Clinic", "content": "ClinicBench [14] is a comprehensive benchmark designed to evaluate large language models (LLMs) in clinical settings. The benchmark includes a diverse set of tasks, including both traditional machine learning tasks and novel, clinically relevant tasks such as referral question answering, treatment recommendation, and patient education generation. The authors evaluate a range of LLMs on these tasks, finding that while models like GPT-4 show promising performance on certain tasks, there remains a significant gap between current LLM capabilities and the requirements for real-world clinical application. The paper also explores the impact of different types of fine-tuning data on model performance, highlighting the potential benefits of incorporating clinical-standard knowledge bases into the training process."}, {"title": "High-Quality Hallucination Benchmark", "content": "The High-Quality Hallucination (HQH) [44] Benchmark addresses the problem of hallucination in Large Vision-Language Models (LVLMs), where models generate text that's inconsistent with the visual input. The authors point out that existing benchmarks for evaluating this issue suffer from varying quality, impacting the reliability of model assessments. To tackle this, they propose a framework called HQM to measure the quality of these benchmarks based on reliability and validity. Their analysis reveals that current benchmarks have limitations in both aspects.\nIn response, they develop a new benchmark, HQH, which uses free-form Visual Question Answering and a simplified evaluation metric to enhance its quality. Their evaluation on various LVLMs demonstrates that hallucination remains a challenge, particularly with certain types like existence, OCR, and comparison. The paper concludes by emphasizing the need for further research to mitigate hallucination in LVLMs and advocating for the use of their HQM framework to improve the quality of future benchmarks."}, {"title": "AI Benchmarking for Science", "content": "The MLCommons Science Working Group [34] is creating science-specific AI benchmarks to advance AI's application in scientific research. They've developed four benchmarks so far, each with datasets and reference implementations, covering areas like atmospheric science, material science, healthcare, and earth science.\n\u2022 Cloud Masking (cloud-mask): Classifies pixels in satellite images as containing cloud or clear sky, crucial for estimating sea surface temperature.\n\u2022 Space Group Classification of Solid State Materials (stemdl): Classifies the space group of solid-state materials from electron diffraction patterns, aiding in understanding material properties.\n\u2022 Time Evolution Operator (tevelop): Predicts the evolution of time series data, exemplified by earthquake forecasting."}, {"title": "\u03a4\u03a1CX-AI", "content": "TPCx-AI [2] is an industry-standard benchmark designed to evaluate the end-to-end performance of artificial intelligence (AI) and machine learning (ML) systems. It emphasizes that TPCx-AI is unique in its focus on evaluating the entire ML pipeline, from data ingestion to model serving and post-processing, unlike other benchmarks that primarily focus on model training. TPCx-AI is built on a retail scenario and includes ten diverse use cases, covering both traditional ML and deep learning models. It provides a comprehensive toolkit with implementations in Python and Apache Spark, making it readily usable for evaluating ML/AI systems on standard hardware. The benchmark also includes a scalable dataset and a well-defined performance metric, ensuring fair and meaningful comparisons between different systems."}, {"title": "Robust Bench", "content": "RobustBench [5] is a standardized benchmark for evaluating the adversarial robustness of machine learning models. It emphasizes the need for accurate and reliable robustness evaluations, addressing the challenge of robustness overestimation that often hinders progress in the field. RobustBench focuses on image classification tasks and establishes restrictions on the allowed models to ensure meaningful comparisons. It utilizes AutoAttack, an ensemble of white- and black-box attacks, as the standard evaluation method. The benchmark also encourages external evaluations using adaptive attacks to further enhance its reliability. RobustBench provides a leaderboard showcasing the performance of various models on well-defined tasks in different threat models and on common corruptions. Additionally, it offers a Model Zoo, a collection of robust models readily available for downstream applications. Overall, RobustBench aims to accelerate progress in adversarial robustness research by providing a standardized and reliable benchmark, facilitating the identification of the most promising ideas in training robust models."}, {"title": "MetaBox", "content": "MetaBox [16] is the first benchmark platform specifically designed for Meta-Black-Box Optimization with Reinforcement Learning (MetaBBO-RL). MetaBBO-RL aims to automate the fine-tuning of black-box optimizers, thereby enhancing their performance across various problem instances. The platform offers a flexible algorithmic template, a diverse collection of over 300 problem instances, and an extensive library of 19 baseline methods. It also introduces three standardized performance metrics for a more comprehensive evaluation of MetaBBO-RL methods. MetaBox is open-source and accessible on GitHub, contributing to the advancement of research in this field."}, {"title": "SUC", "content": "Structural Understanding Capabilities (SUC) [30] is a comprehensive benchmark consisting of seven tasks, each with its own unique challenges, e.g., cell lookup, row retrieval, and size detection. The authors try to answer the following questions: 1) What input designs and choices are most effective in enabling LLMs to understand tables?; 2) To what extent do LLMs already possess structural understanding capabilities for structured data? The comparison reveals that LLMs have the basic capabilities towards understanding structural information of tables and to further improve it propose self-augmentation for effective structural prompting, such as critical value / range identification using internal knowledge of LLMs."}, {"title": "LogicVista", "content": "LogicVista [42] is a benchmark designed to assess the logical reasoning skills of Multimodal Large Language Models (MLLMs). It focuses on evaluating how well these models can reason and solve problems based on visual information. The benchmark uses a variety of visual inputs, including diagrams, text, patterns, graphs, tables, 3D shapes, puzzles, and sequences. LogicVista tests five types of logical reasoning: deductive, inductive, numerical, spatial, and mechanical. It provides a tool for evaluating how well these models can apply logical reasoning skills across different types of visual data. The project is open source and available on GitHub."}, {"title": "RelevAI-Reviewer", "content": "RelevAI-Reviewer [4] is a new AI tool for automatically reviewing scientific papers. The tool is designed to overcome the limitations of traditional peer review processes, which can be slow, biased, and inconsistent. RelevAI-Reviewer specifically focuses on evaluating the relevance of survey papers based on a given prompt, similar to a \"call for papers.\"\nThe authors created a dataset of over 25,000 survey papers from 22 different fields, categorized into four relevance levels. They tested various machine learning models, including Support Vector Machines (SVM) and BERT, to determine the most effective approach. The results showed that BERT models, particularly with a specific encoding method called \"thermometer encoding,\" outperformed other methods in accurately ranking papers by relevance."}, {"title": "AI Safety Benchmark v0.5", "content": "The AI Safety Benchmark v0.5 [36] is a proof-of-concept for evaluating the safety risks of AI systems, specifically chat-tuned language models. It presents a taxonomy of 13 hazard categories and includes tests for seven of them, using over 43,000 test items (prompts) to assess model responses. The benchmark aims to provide a standardized and interpretable evaluation of AI safety, addressing a critical need as AI systems become increasingly integrated into various domains. The authors acknowledge limitations in the current version and welcome feedback for the development of the full v1.0 benchmark planned for the end of 2024."}, {"title": "Dataset OpenAssistant Conversations", "content": "OpenAssistant Conversations [12] is a large collection of human-generated and human-annotated assistant-style conversations. This dataset was created by over 13,500 volunteers with the goal of democratizing research on large-scale language model alignment. The dataset consists of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 complete conversation trees. The authors used this dataset to fine-tune several language models and found that they show consistent improvements on standard benchmarks over their respective base models. The dataset is released under a fully permissive license and is available on the Hugging Face Hub."}, {"title": "ZebraLogic", "content": "ZebraLogic Benchmark [46] is based on Logic Grid Puzzles, to test how well LLMs can reason. The puzzles involve houses with distinct features that need to be deduced from clues. The article also explores different metrics for evaluating the LLMs' performance, including puzzle-level accuracy and cell-wise accuracy. It was found that LLMs struggle with complex logical reasoning tasks, particularly those that require counterfactual thinking, reflective reasoning, and structured memorization. The article also provides an example of a 2x3 puzzle and explains the evaluation method. Some of the important points are that LLMs are still weak in logical reasoning tasks."}, {"title": "GLoRE Benchmark", "content": "GLORE [15] is a benchmark for evaluating the logical reasoning abilities of LLMs, comprising 12 datasets over three types of tasks, including Multi-choice Reading Comprehension, Natural Language Inference (NLI), and True-or-False (TF) questions. Using GLORE, the authors evaluate the logical reasoning abilities of several LLMs, including ChatGPT, GPT-4, and open-source LLMs based on LLAMA and Falcon. The results indicate that both ChatGPT and GPT-4 outperform open-source LLMs and traditional supervised fine-tuned models in most tasks. However, the performance of these models is not consistent across datasets, indicating their sensitivity to data distribution. The paper also proposes a self-consistency probing method to enhance the accuracy of ChatGPT and a fine-tuned method to boost the performance of an open LLM."}, {"title": "General Language Understanding Evaluation", "content": "The General Language Understanding Evaluation (GLUE) [38] benchmark provides a comprehensive platform for evaluating and analyzing natural language understanding (NLU) systems. Designed to foster the development of generalizable and robust NLU models, GLUE offers a suite of nine tasks, including single-sentence classification, similarity and paraphrase detection, and natural language inference. The tasks were selected to cover a range of genres, dataset sizes, and challenges. GLUE also features a diagnostic dataset for analyzing specific linguistic capabilities of models, including lexical semantics, logical reasoning, and predicate-argument structure.\nThe GLUE benchmark is model-agnostic, allowing the use of any architecture capable of processing single sentences or sentence pairs. Its primary evaluation relies on performance across all tasks, encouraging the development of models that effectively transfer knowledge and perform well even with limited training data. To ensure fairness and prevent overfitting, test labels for some tasks are privately held and results must be submitted to an online platform for evaluation."}, {"title": "A Stickier Benchmark for General-Purpose Language Understanding Systems", "content": "SuperGLUE (A Stickier Benchmark for General-Purpose Language Understanding Systems) [37] is a benchmark designed to address the limitations of its predecessor, GLUE [38], by introducing more challenging natural language understanding (NLU) tasks. The motivation for SuperGLUE stems from significant advances in NLU models, such as BERT and OpenAI GPT, which have largely surpassed human performance on GLUE tasks. This progress highlighted the need for a more robust and challenging evaluation framework to continue driving innovation.\nKey Features of SuperGLUE:\n\u2022 Task Design: SuperGLUE consists of eight tasks that emphasize reasoning, understanding, and contextual comprehension. These tasks include:\nBoolQ: Yes/no question answering from text.\nCB: Commitment Bank for entailment in embedded clauses.\nCOPA: Choice of plausible alternatives requiring causal reasoning.\nMultiRC: Multi-sentence reading comprehension with multiple correct answers.\nReCORD: Cloze-style questions demanding commonsense reasoning.\nRTE: Recognizing textual entailment.\nWiC: Word-in-context sense disambiguation.\nWSC: The Winograd Schema Challenge for coreference resolution.\n\u2022 Diversity in Task Formats: Unlike GLUE, which primarily involved sentence and sentence-pair classification, SuperGLUE incorporates formats like coreference resolution and multi-answer question answering to challenge existing models.\n\u2022 Human Baselines: SuperGLUE provides comprehensive human performance baselines for all tasks, ensuring clear headroom for machine models to improve.\n\u2022 Diagnostic Tools: Includes a diagnostic dataset to assess linguistic, commonsense, and world knowledge."}, {"title": "AI2 Reasoning Challenge", "content": "The AI2 Reasoning Challenge (ARC) [43] dataset is a benchmark designed to assess and advance the capabilities of question-answering (QA) systems in handling scientific reasoning. It includes multiple-choice questions sourced from U.S. grade-school science exams, split into two partitions:\n\u2022 Easy Set: Contains questions that can be answered by simple retrieval or basic reasoning.\n\u2022 Challenge Set: Features more complex questions that require reasoning across multiple sentences or external knowledge sources.\nKey Features:\n\u2022 Complexity: The Challenge Set requires multi-hop reasoning, commonsense knowledge, and understanding of scientific principles, posing a significant difficulty for existing QA models.\n\u2022 Scale: The ARC dataset includes thousands of questions, with four answer choices per question, covering grades 3 to 9 science topics.\n\u2022 Knowledge Base: Accompanying the dataset is a corpus of 14.3 million unstructured text passages, intended as a resource for models to retrieve evidence for answering questions.\n\u2022 Evaluation Metrics: Standard accuracy metrics are used to evaluate QA performance."}, {"title": "Natural Language for Visual Reasoning for Real", "content": "The Natural Language for Visual Reasoning for Real (NLVR2) [29"}]}