{"title": "FUTUREFILL: FAST GENERATION FROM ConVOLUTIONAL SE- QUENCE MODELS", "authors": ["Naman Agarwal", "Xinyi Chen", "Evan Dogariu", "Vlad Feinberg", "Daniel Suo", "Peter Bartlett", "Elad Hazan"], "abstract": "We address the challenge of efficient auto-regressive generation in sequence prediction models by introducing FutureFill\u2014a method for fast generation that applies to any sequence prediction algorithm based on convolutional operators. Our approach reduces the generation time requirement from linear to square root relative to the context length. Additionally, FutureFill requires a prefill cache sized only by the number of tokens generated, which is smaller than the cache requirements for standard convolutional and attention-based models. We validate our theoretical findings with experimental evidence demonstrating correctness and efficiency gains in a synthetic generation task.", "sections": [{"title": "INTRODUCTION", "content": "Large Transformer models Vaswani et al. (2017) have become the method of choice for sequence prediction tasks such as language modeling and machine translation. Despite their success, they face a key computational limitation: the attention mechanism, their core innovation, incurs a quadratic computational cost during training and inference. This inefficiency has spurred interest in alternative architectures that can handle long sequences more efficiently.\nConvolution-based sequence prediction models Li et al. (2022); Poli et al. (2023); Agarwal et al. (2023); Fu et al. (2024) have emerged as strong contenders, primarily due to their ability to leverage the Fast Fourier Transform (FFT) for near-linear scaling with sequence length during training. These models build upon the advancements in State Space Models (SSMs), which have shown promise in modeling long sequences across diverse modalities Gu et al. (2021a); Dao et al. (2022); Gupta et al. (2022); Orvieto et al. (2023); Poli et al. (2023); Gu & Dao (2023). Convolutional models offer a more general framework than SSMs because they can represent any linear dynamical system (LDS) without being constrained by the dimensionality of hidden states Agarwal et al. (2023). This flexibility has led to recent developments that theoretically and empirically handle longer contexts more effectively. Notable among these are Spectral State Space Models or Spectral Transform Units (STUs) Agarwal et al. (2023), which use spectral filtering algorithms Hazan et al. (2017; 2018) to transform inputs into better-conditioned bases for long-term memory. Another approach is Hyena Poli et al. (2023), which learns implicitly parameterized Markov operators. Both methods exploit the duality between time-domain convolution and frequency-domain multiplication to accelerate prediction via the FFT.\nWhile SSMs and recurrent models benefit from fast inference times independent of sequence length, making them attractive for large-scale language modeling, convolutional models have been hindered by slower token generation during inference. The best-known result for generating tokens with convolutional models is quadratic in sequence length-comparable to attention-based models (see Massaroli et al. (2024) Lemma 2.1). This limitation has prompted research into distilling state-space models from convolutional models Massaroli et al. (2024), but such approximations lack comprehensive understanding regarding their approximation gaps due to the broader representational capacity of convolutional models.\nIn this paper, we address the problem of exact auto-regressive generation from given convolutional models, significantly improving both the generation time and cache size requirements. We present our main results in two settings:"}, {"title": "SETTING", "content": "In sequence prediction, the input is a sequence of tokens denoted $u_1, ..., u_T, ...$, where $u_t \\in \\mathbb{R}^{d_{in}}$. The predictor's task is to generates a sequence $\\hat{y}_1, \\dots, \\hat{y}_t, ...$, where $\\hat{y}_t \\in \\mathbb{R}^{d_{out}}$ is generated after observing $u_1,..., u_{t-1}$. The output $y_t$ is observed after the predictor generates $\\hat{y}_t$. The quality of the prediction is measured by the distance between the predicted and observed outputs according to a loss function $l_t(\\hat{y}_t, y_t)$, for example the mean square error $||\\hat{y}_t - y_t ||^2$.\nFor an input sequence ${u_t}$ we denote by $u_{1:t}$ the sequence of inputs $u_1,..., u_t$. For any $i < j$ let $u_{i:j}$ denote the sub-sequence $u_i, u_{i+1},... u_j$. When $i > j$, $u_{i:j}$ denotes the subsequence $u_{j:i}$ in reverse order. Thus $u_{t:1}$ represents the sequence in reverse order. Given any sequence $u_{i:j}$, for any k let $0_k u_{i:j} = 0...0 u_i . . . u_j$ and $u_{i:j} 0_k = u_i . . . u_j 0... 0$ denote the pre and post concatenation with k zeros.\nThe convolution operator between two vectors $v \\in \\mathbb{R}^{t_1}, w \\in \\mathbb{R}^{t_2}$ outputs a sequence of length $t_1 + t_2$ whose element at any position $s \\in [t_1 + t_2]$ is\n\n$[v * w] (s) = \\sum_{i=1}^{s} v_i w_{s+1-i} = (v_{1:s}, w_{s:1}),$\n\nwhere for brevity of notation we assume $v_j = 0, w_j = 0$ for any j greater than $t_1, t_2$ respectively.\nWe henceforth apply the FFT algorithm for computing convolutions. For two vectors $v \\in \\mathbb{R}^{t_1}, w \\in \\mathbb{R}^{t_2}$, their convolution can be computed in time O(t logt) where $t = t_1 + t_2$, yielding, among others, t inner products of the form\n\n$(v_{1:t}, w_{t:1}), (v_{1:t-1}, w_{t-1:1}), ... (v_{1:1}, w_{1:1})$.\n\nGiven a multi-dimensional sequence $u_1 \\dots u_t$ where each $u_i \\in \\mathbb{R}^d$ and given a vector $v \\in \\mathbb{R}^t$, for brevity of notation we overload the definition of inner products by defining $y = (v, u_{1:t})$ with $y \\in \\mathbb{R}^d$ as $y_j = \\sum_{i=1}^t v_i \\cdot [u_i]_j$. That is the inner-product along the time dimension is applied on each input dimension separately. Further when we have a sequence of matrices $v_{1:t} = {v_1...v_t}$ with each $v_i \\in \\mathbb{R}^{d \\times d}$, we define $(v_{1:t}, u_{1:t}) = \\sum_{i=1}^t v_i u_i$. With these definitions the notion of convolution between such sequences can be defined via the natural extension."}, {"title": "ABSTRACTING CONVOLUTIONAL SEQUENCE PREDICTION", "content": "We define a convolutional sequence prediction model to be given by a filter, which is a vector denoted by $\\phi \\in \\mathbb{R}^L$ where L is considered the context length of the model. It takes as an input a sequence u, and outputs a prediction sequence. The above definition can be extended to multiple filter channels and nonlinearities can be added, as we elaborate below with different examples. Formally, a single output in the predicted sequence using a convolutional sequence model is given by\n\n$\\hat{y}_t = (\\phi, u_{t:t-L}).$\n\nThis paradigm captures several prominent convolutional sequence models considered in the literature. We highlight some of them below. The cached convolution technique proposed by us can be used with all the models below in straightforward manner leading to generation time improvement from O(L) to O($\\sqrt{L}$)."}, {"title": "State Space Models", "content": "Discrete state space models such as those considered in Gu et al. (2021a) have shown considerable success/adoption for long range sequence modelling. A typical state space model can be defined via the following definition of a Linear Dynamical System (LDS)\n\n$x_t = Ax_{t-1} + Bu_t$\n$y_t = Cx_t + Du_t$\n\nwhere u, y are the input and output sequences and A, B, C, D are the learned parameters. Various papers deal with specifications of this model including prescriptions for initialization Gu et al. (2020), diagonal versions Gupta et al. (2022), gating Mehta et al. (2023) and other effective simplifications Smith et al. (2023). All these models can be captured via a convolutional model by noticing that the output sequence y in (3) can be written as\n\n$y = \\phi * u + Du$\n\nwhere the filter $\\phi$ takes the form $\\phi_1 = CA^{i-1}B$. Thus a convolutional sequential model with learnable filters $\\phi$ generalizes these state space models. However, SSM are more efficient for generation and require only constant time for generating a token, where the constant depends on the size of the SSM representation."}, {"title": "LongConv/SGConv.", "content": "The LongConv Fu et al. (2023) and SGConv Li et al. (2022) architectures, exploit the above connection and propose direct regularizations of the kernel to bias them towards kernels representing a state space model."}, {"title": "Spectral Transform Units.", "content": "The STU architecture was proposed in Agarwal et al. (2023) based on the technique of spectral filtering for linear dynamical systems Hazan et al. (2017; 2018). These are basically convolutional sequence models based on carefully constructed filters that are not data dependent. Rather, let $\\phi_1, ..., \\phi_k$ be the first k eigenvectors of the Hankel matrix $H_L$ given by\n\n$H_L = \\int_{\\Omega^1} \\mu_t \\mu^T_t dt \\in \\mathbb{R}^{L \\times L}, \\mu_t = (t - 1)[1, t, t^2, .., t^{L-1}].$\n\nThen the STU outputs a prediction according to the following rule 1\n\n$\\hat{y}_t = \\sum_{i=1}^{k} M_i(\\phi_i, u_{t:t-L}),$\n\nwhere $\\phi_i$ are the eigenvectors as above and $M_{1:k}$ are learned projection matrices. The STU architecture is particularly appealing for learning in dynamical systems with long context, as it has theoretical guarantees for this setting, as spelled out in Agarwal et al. (2023)."}, {"title": "Hyena.", "content": "The Hyena architecture proposed in Poli et al. (2023), sequentially applies convolutions and element-wise products alternately. Formally, the given an input $u_{1:T}$, N + 1 linear projections $v, x_1,... x_N$ of the input are constructed (similar to the q, k, v sequence in self-attention). The hyena operator as a sequence of convolution with learnable filters $h_1 . . . h_N$ is then given by\n\n$y = x_N \\cdot (h_N * (x_{N-1} \\cdot (h_{N-1} * (...)))).$"}, {"title": "SEQUENCE GENERATION FROM A PROMPT", "content": "We consider two modes of generation using sequence models. The first is that of prefill generation, where the sequence model has to generate a specified number of tokens given a certain context. The second is online generation, where the length of the generative output is not predetermined, and we are not given a context.\nPrefill generation. The prefill generation setting consists of two stages, the prefill stage and the decode stage. During the prefill stage, the model ingests the context vector and generates a cache that stores information required in the decode stage. We denote the context length during the prefill stage as L.\nIn the decode stage, the model takes the cache and the most recent token as input and generates the next output token. Then the cache updated with the most recent input token. We denote the generation length at the decode stage as K."}, {"title": "ONLINE SEQUENCE PREDICTION", "content": "In the online sequence prediction setting, an online learner iteratively sees an input $u_t$ and has to predict output $\\hat{y}_t$, after which the true output $y_t$ is revealed. The goal is to minimize error according to a given Lipschitz loss function $l_t(y_t, \\hat{y}_t)$. In online learning it is uncommon to assume that the true sequence was generated by the same family of models as those learned by the learner. For this reason the metric of performance is taken to be regret. Given a class of possible predictors, the goal is to minimize regret w.r.t. these predictors. For example, a linear predictor predicts according to the rule\n\n$M_{1:k,N_{1:1}}(u_{1:t}, y_{1:t-1}) = \\sum_{i=1}^{k} M_i u_{t-i} + \\sum_{j=1}^{k} N_j y_{t-j}$\n\nThe performance of a prediction algorithm A is measured by regret, or difference in total loss, vs. a class of predictors \u03a0, such as that of linear predictors, e.g.\n\n$Regret_{\\Pi}(A) = \\sum_{t=1}^{T} l_t (y_t, \\hat{y}^A_t) - min_{\\pi \\in \\Pi} \\sum_{t=1}^{T} l_t (y_t, \\hat{y}^\\pi_t)$.\n\nThis formulation is valid for online sequence prediction of any signal. We are particularly interested in signal that are generated by dynamical systems. A time-invariant linear dynamical system is given by the dynamics equations\n\n$x_{t+1} = Ax_t + Bu_t + w_t$,\n$y_t = Cx_t + Du_t + \\zeta_t,$\n\nwhere $x_t$ is the (hidden) state, $u_t$ is the input or control to the system, and $y_t$ is the observation. The terms $w_t, \\zeta_t$ are noise terms, and the matrices A, B, C, D are called the system matrices. A linear dynamical predictor with parameters A, B, C, D predicts according to\n\n$\\Pi_{ABCD}(u_{1:t}, y_{1:t-1}) = \\sum_{i=1}^{t-1} CA^{i-1} B u_{t-i} + Du_t$.\n\nThe best such predictor for a given sequence is also called the optimal open loop predictor, and it is accurate if the signal is generated by a LDS without noise."}, {"title": "FAST AUTO-REGRESSIVE GENERATION USING A PREFILL CACHE", "content": "A simple variant of our method for batch prediction is described in Algorithm 1. The algorithm uses the FFT in order to compute convolutions between the filters of a convolutional model and the inputs. These convolutions, unlike previous methods, pre-compute a large part of the computation that shall be required during the generation of a particular token during generation phase. Thus the algorithm generates and stores a total of K cache elements if the target is to generate K tokens. For any generated token this cache element captures the entire dependence of that token on the prefill context. We call this technique FutureFill. During the generation phase, the algorithm then complements this cached information for each prediction with the necessary additional information from the inputs that are newly observed (or generated in case of auto-regressive prediction). We note that such a summarization of the prefill context is not possible for attention models, as the exact dependence of an output token during generation on the prefill context cannot be determined till we are generating that token. In the algorithm below for the prefill generation setting, we take $u_t = \\hat{y}_{t-1}$ for $t \\in [L + 1, L + K]$."}, {"title": "Running time analysis.", "content": "Our main claim regarding the performance of Algorithm 1 is as follows.\nLemma 1. Algorithm 1 when supplied with a prompt of sequence length L, generates K tokens in total time $O(Llog L + K^2)$ using a total prefill-cache of size O(K). Hence, the algorithm runs in amortized time $O(\\frac{L log L}{K} + K)$ per token.\nProof. The total runtime of the algorithm is composed of two components as follows\n1. Computation of the Cache (Line 2): The computation of the cache in line 2 can be computed via a convolution between the prompt and the filter. Let $\\bar{u} = u_{1:L} 0_K$, note that $C_j = [\\bar{u} * \\phi]_{L+j}$. We can thus compute all K values $C_1 . . . C_K$ via one application of FFT which in total takes time $O((L + K) log(L + K))$.\n2. The computation in Line 4 splits into two parts, where one term we have already computed and stored as $C_j$ earlier and the other term is a sum of $\\tau$ products, and thus a total compute of $\\tau$. The overall complexity of this phase is obtained by summing over K consecutive iterations which gives\n\n$\\sum_{\\tau=1}^{K} \\tau = O(K^2).$"}, {"title": "FAST ONLINE CONVOLUTIONAL PREDICTION", "content": "The technique of the previous section can naturally be applied to online prediction using a convolutional model in full generality: we describe a setting of online generation without any given prompts. We give the algorithm in Algorithm 2, then describe how it can be used for a particular convolutional model of interest, the Spectral Transform Unit from Agarwal et al. (2023).\nLemma 2. Algorithm 2 with sequence length L runs in amortized time $O (\\sqrt{L} log L)$ per iteration.\nProof. The running time consists of two components as follows:\n1. Every iteration, line 7 is executed. One term, $C_\\tau$, has already been computed and saved in line 4. We can retrieve it in time O(1). The other term is a sum of $\\tau$ products, which can be computed in time $\\tau$.\n2. Every K iterations, we execute line 4 and update the terms in the cache. Let $\\bar{u} = u_{1:t} 0_{\\sqrt{L}}$, note that $C_j = [\\phi * \\bar{u}]_{t+j}$, and ${C_1,...,C_K}$ can be computed using one convolution. Using the FFT, this takes at most $O(L log L)$ time.\nThe overall amortized running time is computed by summing over the L iterations and taking an average. In each block of K iterations, we apply FFT exactly once, and hence the total computational complexity is\n\n$\\frac{L}{K} (L log L + \\sum_{\\tau=1}^{K} \\tau) = O (\\frac{L log L}{K} + K L) = O (L \\sqrt{L} log L),$\n\nwhere the cache size K is chosen to minimize the sum. Taking an average, we conclude that the algorithm runs in $O (\\sqrt{L} log L)$ per iteration."}, {"title": "CASE STUDY: FAST ONLINE SPECTRAL FILTERING", "content": "We illustrate in more detail how the method works for the STU model in Algorithm 3. It improves the amortized running time per iteration from O(L) of the original spectral filtering algorithm from Hazan et al. (2017) to $O(\\sqrt{L})$, albeit maintaining the same regret bound.\nCorollary 3. Algorithm 3 with sequence length L runs in amortized time $O(\\sqrt{L} log L)$ per iteration. In addition it guarantees the same regret bound vs. spectral filtering with context length L."}, {"title": "EXPERIMENTS", "content": "A naive decoding method would explicitly compute Equation 2 for each token generated, resulting in O(T) time spent on each step on average when generating T tokens. By contrast, Lemma 2 has shown that Algorithm 2 runs in a faster amortized $O (\\sqrt{T} log T)$ per iteration. To experimentally verify this speedup, we use a convolutional model to generate tokens in an online fashion (i.e. with full context length at every step) and compare the naive decoding strategy to Algorithm 2.\nFor increasing values of T, we measure the time S(T) it takes for a single layer to generate T tokens. We see the behavior that is expected: the naive decoder runs in amortized O(T) per step, while our method achieves sublinear decoding complexity."}]}