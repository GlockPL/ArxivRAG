{"title": "CXPMRG-Bench: Pre-training and Benchmarking for X-ray Medical Report Generation on CheXpert Plus Dataset", "authors": ["Xiao Wang", "Fuling Wang", "Yuehang Li", "Qingchuan Ma", "Shiao Wang", "Bo Jiang", "Chuanfu Li", "Jin Tang"], "abstract": "X-ray image-based medical report generation (MRG) is\na pivotal area in artificial intelligence which can signifi-\ncantly reduce diagnostic burdens and patient wait times.\nDespite significant progress, we believe that the task has\nreached a bottleneck due to the limited benchmark datasets\nand the existing large models' insufficient capability en-\nhancements in this specialized domain. Specifically, the\nrecently released CheXpert Plus dataset lacks compara-\ntive evaluation algorithms and their results, providing only\nthe dataset itself. This situation makes the training, eval-\nuation, and comparison of subsequent algorithms chal-\nlenging. Thus, we conduct a comprehensive benchmark-\ning of existing mainstream X-ray report generation models\nand large language models (LLMs), on the CheXpert Plus\ndataset. We believe that the proposed benchmark can pro-\nvide a solid comparative basis for subsequent algorithms\nand serve as a guide for researchers to quickly grasp the\nstate-of-the-art models in this field. More importantly, we\npropose a large model for the X-ray image report gener-\nation using a multi-stage pre-training strategy, including\nself-supervised autoregressive generation and Xray-report\ncontrastive learning, and supervised fine-tuning. Extensive\nexperimental results indicate that the autoregressive pre-\ntraining based on Mamba effectively encodes X-ray images,\nand the image-text contrastive pre-training further aligns\nthe feature spaces, achieving better experimental results.", "sections": [{"title": "1. Introduction", "content": "X-ray image based Medical Report Generation (MRG)\nis a critical research problem in artificial intelligence, which\ntargets describing the findings or impressions from the given\nX-ray data using natural language. The successful imple-\nmentation of this task can significantly reduce the diag-\nnostic burden on physicians, decrease patient wait times,\nand foster the positive application of artificial intelligence.\nHowever, the path to progress in this direction is not smooth\nsailing, there remain formidable challenges that need to be\novercome. The challenging issues include image interpreta-\ntion, data annotation, heterogeneity issues, consistency and\nstandardization of reports, diversity and variability of dis-\neases, interpretability of algorithms, etc. How to address\nthese challenges further and improve the quality of medical\nreport generation remains an urgent research problem.\nAfter revisiting the mainstream algorithms of X-ray im-\nage medical report generation, we find that datasets like\nIU X-ray and MIMIC-CXR are widely used for the train-\ning and evaluation of report generation models. However,\nthe IU X-ray only contains 7,470 images and 3,955 radiol-\nogy reports samples, which is rather limited, especially in\nthe large model era. The recently released CheXpert Plus\ndataset [6] is a large-scale dataset for the X-ray report gen-\neration, however, they did not release comparative methods,\nmaking it difficult for subsequent algorithms to conduct ex-\nperiments and comparisons on this dataset. Therefore, we\nconduct a comprehensive benchmarking of existing open-\nsourced mainstream X-ray report generation models, Large\nLanguage Models (LLMs), and Vision-Language Models\n(VLMs), termed CXPMRG-Bench, on the newly released\nCheXpert Plus dataset, as shown in Fig. 1. The completion\nof this work can also help researchers identify which large\nmodels and algorithms are currently leading in the field of\nX-ray report generation.\nOn the other hand, most mainstream algorithms fol-"}, {"title": "2. Related Work", "content": "In this section, we will review the related works on X-\nray Medical Report Generation, Pre-trained Large Models,\nand State Space Models. More works can be found in the\nfollowing surveys [20, 55, 59]."}, {"title": "2.1. X-ray Medical Report Generation", "content": "In recent years, X-ray medical report generation has gar-\nnered increasing attention. To enhance model performance,\nresearchers have pursued various improvements in different\ndirections. Specifically, DCL [30] introduces a Dynamic\nGraph at the visual features of medical images, leveraging\nknowledge to strengthen the feature representation of these\nimages. RGRG [48] takes a novel approach by using object\ndetection methods to extract lesion regions and then gener-\nating text based on these extracted regions, ultimately com-\nbining all the text to form the final report. HERGen [52]\ndiscovers the historical information between medical re-\nports, treating all reports of a patient as a temporally ordered\nwhole. This approach effectively integrates the temporal\nand causal information of the reports. R2GenGPT [63]\nreplaces the decoder part of the traditional medical report\ngeneration framework with a more powerful large language\nmodel, achieving improved performance. R2GenCSR [57]\nis a recently proposed LLM-based framework for X-ray\nMRG which employs the Mamba as the visual backbone\nand retrieves contextual samples from the training set to en-\nhance feature representation and discriminative learning.\nIt is evident that the vision encoders used in these\nmodels are all conventional networks pre-trained on Im-\nageNet [46]: DCL [30] employs ViT [15], RGRG [48]\nuses ResNet50 [21], HERGen [52] utilizes CVT [39],\nR2GenGPT [63] incorporates SwinTransformer [36], and\nR2GenCSR [57] leverages VMamba [35]. These encoders,\npre-trained on non-medical X-ray images, exhibit certain\nlimitations when extracting features from medical X-ray\nimages. In contrast, our proposed MambaXray-VL is pre-\ntrained on millions of datasets and has a natural advantage\nin the extraction of features from medical images, especially\nin the task of medical report generation."}, {"title": "2.2. Pre-trained Large Models", "content": "The pre-trained language models, vision models, and\nvision-language models are widely exploited in nowadays.\nCurrently, the widely used MAE [22] (Masked Autoen-\ncoders) is a self-supervised learning method for computer\nvision, known for its scalability and simplicity. Recently,\nApple's team proposed AIM [17], a series of vision models\nusing autoregressive objectives for pretraining, inspired by\nlarge language models, demonstrating similar scaling prop-\nerties. ARM [45] is a new self-supervised visual representa-\ntion learning method based on AIM [17] and Mamba [19].\nThrough the autoregressive generation based pre-training,\nthe visual capabilities of the Mamba model can be signifi-\ncantly enhanced, outperforming other training strategies in\nterms of both efficiency and performance. CLIP [44] (Con-\ntrastive Language-Image Pre-Training) jointly trains image\nand text encoders using contrastive learning. The key idea is\nto enable the model to understand and process multi-modal\ndata (images and text) through joint training. Inspired by\nthese works, our newly proposed MambaXray-VL utilizes\nautoregressive generation based pre-training, and CLIP pre-\ntraining can achieve better results on medical report gener-\nation."}, {"title": "2.3. State Space Model", "content": "Since its introduction in 2017, Transformer [50] has\nquickly become the preferred model framework for re-\nsearchers due to its strong performance. However, as the\nmodel scales and sequences become longer, its limitations\nhave surfaced. One major drawback is the quadratic growth\nin computational complexity of the self-attention mech-\nanism with increased context length. Mamba [19] ad-\ndresses these issues by using Selective State Space Mod-\nels (SSMs) to improve traditional state space models and\nincorporating a hardware-aware parallel algorithm for re-\ncurrent operations. Vim [75] (Vision Mamba) is the first\nSSM model adapted for vision tasks. It uses positional em-\nbeddings and bidirectional state space models to achieve\nhigh performance, particularly on high-resolution images.\nVMamba [35] extends Mamba by providing a global re-"}, {"title": "3. MambaXray-VL Large Model", "content": "In this section, we will first give an overview of our pro-\nposed MambaXray-VL large model, then, we will dive into\nthe details of the proposed multi-stage training strategy. Fi-\nnally, we highlight some implementation details worth not-\ning in the pre-training phase."}, {"title": "3.1. Overview", "content": "As shown in Fig. 2, we propose a new multi-stage pre-\ntraining strategy for the X-ray image medical report gen-\neration, including self-supervised autoregressive genera-\ntion, Xray-report contrastive learning, and supervised fine-\ntuning. The key insight of multi-stage pre-training instead\nof joint training is that the aligned Xray-report data are lim-\nited, but there are more publicly available X-ray images.\nThus, we first pre-training a large-scale vision backbone\nnetwork on the X-ray images using the Mamba layers, due\nto a better balance between the computational cost and ac-\ncuracy. More importantly, we adopt the autoregressive gen-\neration to achieve self-supervised learning on the X-ray im-\nage. It performs similar or better than the widely used MAE\n(Masked Auto-Encoder) pre-training strategy for this task.\nThen, we transfer the Mamba vision backbone to the second\nstage, i.e., Xray-report contrastive learning. Specifically,\nwe feed the paired data into the pre-trained Mamba vision\nbackbone and language encoder for the vision-language fea-\nture extraction. This stage will project the vision and lan-\nguage representations into a shared feature space to bridge\nthe vision-semantic gaps. Finally, we conduct supervised\nfine-tuning on the training subset of downstream datasets\nfor the X-ray medical report generation."}, {"title": "3.2. Multi-Stage Pre-training", "content": "As illustrated in Fig. 2, our proposed MambaXray-VL\nlarge model contains three training stages which will be in-\ntroduced in the following paragraphs respectively.\n\u2022 Stage #1: Auto-regressive Generation for Mamba Vi-\nsion Encoder Pre-training. To make full use of existing\nX-ray images, we conduct self-supervised learning to ob-\ntain a strong vision backbone network. Different from the\nwidely used MAE (Masked Auto-Encoder)-based frame-\nwork, in this work, we find that the autoregressive gen-\neration based framework works similar or even better for\nthe X-ray images, inspired by the success of autoregressive\ngeneration in ChatGPT [40], GPT-4 [1], and ARM [45].\nLet's denote the X-ray image as $I \\in R^{192 \\times 192 \\times 3}$, we\nfirst partition it into non-overlapping image patches $P_i \\in$\n$R^{16 \\times 16 \\times 3}, i = \\{1,2, ..., N\\}$ and project them into visual\ntokens $T_i \\in R^{1024}, i = \\{1, 2, ..., N\\}$ using a convolutional\nlayer (kernel size 16 \u00d7 16). Here, N is 144 when the res-\nolution of the input X-ray image is set as 3 \u00d7 192 \u00d7 192.\nThen, we feed the visual tokens into the Vim [75] backbone\nnetwork for feature extraction whose complicity O(N) is\nmuch lower than the widely used Transformer O(N2). The\nkey operation of Vim is the Mamba block (a specific vari-\nation of State Space Model [59]), as shown in Fig. 2. The\nvisual tokens will first be normalized and fed into the SSM\nand scan branches. The outputs will be multiplied and\nadded with residual connections. The SwiGLU [47] is\nadopted to further process output features before being fed\ninto subsequent Mamba blocks. Finally, an MLP layer is\nadopted for token reconstruction using the auto-regressive\ngeneration loss function.\nThe objective of autoregressive pre-training is to predict\nthe probability of the next token one by one based on the\ngiven corpus $T = \\{T_1, T_2, ..., T_n\\}$, which can be written\nas:\n$p(T) = \\prod_{i=1}^{n} p(T_i|T_1, ..., T_{i-1}, \\Theta)$.        (1)\nWe can find that the likelihood of each token $T_i$ is com-\nputed based on the context of all the proceeding tokens\n$\\{T_1, ..., T_{i-1}\\}$. Thus, the loss function used for stage 1 can\nbe formulated as follows:\n$L_{AR} = \\sum_{i=1}^{n-1} Vim([T_1, ..., T_i]) - T_{i+1}]^2$.        (2)\n\u2022 Stage #2: Xray-Report Contrastive Learning. We\nadopt the Mamba vision backbone network from the first\nstage and conduct contrastive learning on the paired Xray-\nreport samples. This will further align the dual modalities as\nvalidated in the CLIP [44]. In our implementation, we ran-\ndomly sample a mini-batch and feed the X-ray images and\nmedical reports into the Vim backbone and the language\nmodel (Bio_ClinicalBERT [2], Llama2 [49]) and compute\nthe cosine similarity between the paired and unpaired sam-\nples:\n$L_{CTL} = Similarity(Vim(I_i), LM(R_j))$,   (3)\nwhere i and j are the index of the X-ray image and report\nannotation."}, {"title": "3.3. Implementation Details", "content": "\u2022 Pre-training Stage. Both MambaXray-VL-Base and\nMambaXray-VL-Large were pre-trained for 100 epochs,\nwith batch sizes set at 256 and 128, respectively. The base\nlearning rate, based on a batch size of 256, was set to 1.5e-4.\nWe adopted a cosine decay schedule with a warm-up for 5\nepochs and used the AdamW [37] optimizer with a weight\ndecay of 0.05. The resolution of input images is resized to\n192 \u00d7 192 in the pre-training phase.\nIn the second stage, we utilized a vision-text contrastive\nlearning pre-training method to train MambaXray-VL, en-\nabling alignment to the text feature space. Specifically, we\nused a dataset of 480,000 image-text pairs, composed of\npublicly available datasets from MIMIC-CXR [29], CheX-\npert Plus [6], and IU-Xray [14]. Inspired by ARM [45], we\nused a unidirectional scanning approach in the first stage\nthat fits the autoregressive generation to achieve more effi-\ncient pre-training. In the second stage, we extend the scan-\nning block to four copies in order to improve the perfor-\nmance of the model. During this stage, we chose to pre-\ntraining for 50 epochs, with a batch size set to 192. The\nvisual encoder was Vim [75], loaded with weights from\nthe first stage of pre-training, while the text encoder was"}, {"title": "4. CXPMRG-Bench", "content": "In this paper, we benchmark the newly released CheX-\npert Plus dataset for the X-ray image based medical report\ngeneration. The mainstream MRG algorithms and large lan-\nguage models are listed in the following subsections. For\nthe experimental results, please refer to Table 1, Table 2,\nand Fig. 1."}, {"title": "4.1. Mainstream MRG Algorithms", "content": "For the mainstream X-ray image MRG algorithms, as\nshown in Table 1, we train and test 21 open-sourced algo-\nrithms from the year 2020 to the year 2024. These models\nadopt the CNN (ORGan [24], M2KT [68], ASGMD [65],\nToken-Mixer [69], PromptMRG [28]), Transformer\n(R2GenRL [42], XProNet [53], MSAT [61], TIMER [64],\nCvT2DistilGPT2 [39], R2Gen [8], R2GenCMN [9], Zhu\net al. [76], CAMANet [54], R2GenGPT [63], WCL [66],\nVLCI [7], Wang et al. [58]), and Mamba (R2GenCSR [57],\nMambaXray-VL-B, MambaXray-VL-L) as their vision\nbackbone network, and utilize the LSTM, Transformer\nbased model as the decoder network. Note that, the\nMambaXray-VL-B and MambaXray-VL-L are two models\nproposed in this paper which will be introduced in the next\nsection.\nWhen reproducing these X-ray based MRG models, we\nfound that some algorithms use truncated ground truth for\ncomparison, which we believe may not accurately reflect\nthe true evaluation results. Therefore, we abandoned the\ntruncation mechanism and used the complete ground truth\nfor result evaluation, making the obtained results more ac-\ncurate and reliable."}, {"title": "4.2. LLMs for MRG", "content": "We evaluate a total of 16 open-source LLMs, as\nshown in Table 2, including Vicuna-7B [74], QWen1.5-\n7B [18], QWen2-7B-Instruct [18], InternLM-7B [5],\nLlama2-7B [49], Llama2-13B [49], Llama3-8B [16],\nLlama3.1-8B [16], GPT2-Medium [43], Orca 2-7B [38],\nOrca 2-13B [38], DeepSeek-LLM-7B-Chat [4], Yi-1.5-\n6B-Chat [73], Yi-1.5-9B-Chat [73]. Note that part of\nthe LLMs is selected from open-llm-leaderboard \u00b9 and\nintegrated with R2GenGPT [63] model by replacing the\nLlama2 language decoder with corresponding LLMs. In\nour implementation, we keep the visual encoder SwinTrans-\nformer unchanged for a fair comparison. In addition, we\nalso test two pre-trained vision-language large models, i.e.,\nInternVL-2 [11] and MiniCPM-V2.5 [70], to check whether\na better performance can be obtained, as shown in Table 2."}, {"title": "4.3. Evaluation Results", "content": "[Mainstream MRG Models] As shown in Table 1,\nthere are five MRG models which achieve a higher B4\nmetric, i.e., the XProNet [53] (0.100), R2GenGPT [63]\n(0.101), R2GenCSR [57] (0.100), and our newly proposed\nMambaXray-VL-B and MambaXray-VL-L which achieves\n0.105, and 0.112, respectively. It is intuitive to find that\nthe large language model Llama2 works well for the MRG\ntask. For F1 in the clinical metric, the top-5 models are\nour newly proposed MambaXray-VL-L (0.335), Token-\nMixer [69] (0.288), PromptMRG [28] (0.281), ORGan [24]\n(0.277) and our proposed MambaXray-VL-B (0.273). From\nthese results, we can find that our proposed multi-stage pre-\ntraining strategy is rather effective in the disease-aware per-\nception of the MRG.\n[LLM/VLM based MRG Models] As shown in Table 2,\nwe also report the performance of existing widely used\nLLMs by replacing the Llama2 based on the R2Gen-GPT\nframework (SwinTransformer is adopted as the vision back-\nbone network). It is easy to find that the Vicuna-V1.5 [74]\nreleased in the year 2023 achieves the best B4 metric and\nthe InternLM [5] performs the best on the F1 clinical met-\nric. For the two vision-language models we evaluated, i.e.,\nthe InternVL-2 and MiniCPM-V2.5, we can find that their\nresults are not as good as other LLM-based models, al-\nthough they have similar parameters. These results demon-.\nstrate that the vision-language models pre-trained on natu-\nral image-pairs may have large gaps with the X-ray medi-\ncal images. Compared with the mainstream MRG models\nreported in Table 1, the LLM-based MRG achieves better\nresults than regular language decoders which demonstrates\nthe effectiveness of pre-trained LLMs.\n[Efficiency & Parameters] From the perspective of run-"}, {"title": "5. Experiments", "content": "5.1. Dataset\nIn the first stage of autoregressive pre-training, we used\nabout 1.27 million medical chest X-ray images proposed in\nthe work [58]. In the second stage of image-text contrastive\nlearning pre-training, we used a combination of training\ndata from the MIMIC-CXR [29], CheXpert Plus [6], and\nIU X-ray [14] datasets, totaling 480k image-report pairs.\nNote that the CheXpert Plus dataset used here consists of\nimages and impressions, not the image and findings com-\nbination used in the third stage. We strictly excluded any\ntesting samples used in the third stage, resulting in a to-\ntal of 210k image-impression pairs. In the third stage, We\nevaluate the performance of our model on three datasets,\nincluding IU X-Ray [14], MIMIC-CXR [29], and CheX-\npert Plus [6] dataset. A brief introduction to these datasets\nis given below.\n\u2022 IU X-ray Dataset [14] 2 published in 2016 is one of\nthe most frequently used publicly available medical image\ndatasets for medical report generation. It contains 7,470\nimages and 3,955 radiology reports, with each report asso-\nciated with either frontal or both frontal and lateral view\nimages. Each report is divided into four sections: Indi-\ncation, Comparison, Findings, and Impression. For a fair\ncomparison, we used the same dataset split protocol as\nR2GenGPT [63], dividing the dataset into training, testing,\nand validation sets with a ratio of 7:1:2.\n\u2022 MIMIC-CXR Dataset [29] 3 is one of the largest pub-\nlicly available chest X-ray datasets, containing free-text ra-\ndiology reports. These records from 2011-2016 include\n377,110 radiographic images and 227,835 radiology re-\nports collected from 65,379 patients at the Beth Israel Dea-\nconess Medical Center Emergency Department in Boston,\nMassachusetts. For fair comparison, we used the same\ndataset split protocol as R2GenGPT, with 270,790 samples\nfor training the model, and 2,130 and 3,858 samples for val-"}, {"title": "5.2. Evaluation Metric", "content": "For the X-ray medical report generation, we evalu-\nate the model using widely used natural language gener-\nation (NLG) metrics, including CIDEr [51], BLEU [41],\nROUGE-L [31], and METEOR [3]. More in detail,\nCIDEr [51] evaluates text through TF-IDF weighted n-gram\nmatching, placing greater emphasis on the importance of\nwords; BLEU [41] evaluates text quality through n-gram\nmatching; ROUGE-L [31] evaluates text using the longest\ncommon subsequence; METEOR [3] improves upon BLEU\nby considering synonyms and word order.\nTo measure the accuracy of descriptions for clinical ab-\nnormalities, we also report Clinical Efficacy (CE) metrics.\nCE metrics require the use of the CheXPert [26] toolkit to\nfirst extract labels from predictive reports and ground truth,\nand then to compare the presence status of important clini-"}, {"title": "5.3. Comparison with SOTA Algorithms", "content": "\u2022 Results on IU X-ray Dataset. As shown in Table 3,\nit can be seen that both our MambaXray-VL-Base and\nMambaXray-VL-Large exhibit excellent performance on\nthe IU X-ray dataset. Among them, the MambaXray-VL-\nLarge model is at the SOTA level on BLEU-2 (B2), BLEU-3\n(B3), and BLEU-4 (B4) metrics with scores of 0.330, 0.241,\nand 0.185, respectively. This result indicates the superiority\nof our method over other report generation methods. How-\never, on some other metrics such as BLEU-1 (B1), ROUGE-\nL (R), METEOR (M), and CIDEr (C), our method does not\nachieve optimal performance. This reflects the need to im-\nprove the generalization of our method on other datasets.\n\u2022 Results on MIMIC-CXR Dataset. As shown in Table\n3, our method also demonstrates outstanding performance\non the MIMIC-CXR dataset, surpasses all other advanced\nreport generation methods, and achieves the most advanced\nlevel in several common indicators (e.g., BLEU-1, BLEU-2,\nBLEU-3, and BLEU-4). Specifically, our method improves\nthe BLEU-4 metric by 6% compared to R2GenGPT. En-\ncouragingly, we achieved favorable results for two of the\nthree remaining metrics, ROUGE-L and METEOR, with\nscores of 0.289 for ROUGE-L and 0.167 for METEOR,\nwhich again demonstrates the superior performance of our\nmodel. In the CIDEr metric, our model achieved a score\nof 0.241, indicating that MambaXray-VL still has room for\nimprovement.\n\u2022 Results on CheXpert Plus Dataset. As shown in Ta-"}, {"title": "5.4. Ablation Study", "content": "\u2022 Effectiveness of Autoregressive Generation for Pre-\ntraining on X-ray Image? As shown in Table 4, we first\ncompare the autoregressive generation (ARG) pre-training\nwith the Masked Auto-Encoder (MAE) pre-training. From\nthe #02 and #03 rows, it can be seen that the results\nachieve 0.130/0.089 on the BLEU-4 metric of the MIMIC-\nCXR and CheXpert Plus datasets, respectively. Note that\nthe ARG pre-training method outperforms the MAE on\nall metrics, with a +45% (i.e., (0.224-0.154)/0.154) im-\nprovement on CIDEr compared to MAE. The ARG-based\npre-training achieves similar performance compared with\nMAE-based pre-training on the CheXpert Plus dataset.\n\u2022 Effectiveness of Xray-Report Contrastive Learning.\nIn addition, we further explored the impact of contrastive\nlearning (CTL) on the final performance. The experimental\nresults in the #05 and #06 rows of Table 4 demonstrate its\neffectiveness. After introducing the CTL loss, we find that\nthe results on the MIMIC-CXR and CheXpert Plus datasets\nhave all received improvement. More in detail, it improves\nthe ROUGE-L metric by over +5% on the CheXpert Plus\ndataset. These experiments demonstrate the positive effect\nof the CTL loss we used in the pre-training stage.\n\u2022 Comparison between ViT and Mamba using Autore-\ngressive Generation. As shown in the #01 and #09 rows\nof Table 4, the #01 row uses a visual coder based on the\nTransformer architecture, while the last row uses a visual\ncoder with auto-regressive pre-training of the Mamba ar-\nchitecture. It can be clearly observed that the encoder\nbased on the Mamba architecture achieves better perfor-\nmance in the vast majority of metrics, both on the MIMIC-\nCXR and CheXpert Plus datasets, especially on BLEU-4 for\nthe MIMIC-CXR data, where the Mamba architecture im-\nproves by +6% compared to the Transformer architecture.\nHowever, on the MIMIC-CXR dataset, the metric CIDEr"}, {"title": "5.5. Visualization", "content": "As shown in Fig. 3, we give some examples to illustrate\nthe effectiveness of our proposed MambaXray-VL model\nfor the X-ray image based report generation. For specific X-\nray images, we compared ground truth with the report gen-\nerated by the MambaXray-VL model and the report gener-\nated by the R2GenGPT model. The X-ray images we chose\ncontain both front and side views, normal images, and im-\nages containing lesion areas, enabling a more comprehen-\nsive and rational visualization. For a more intuitive visual-"}, {"title": "5.6. Limitation Analysis", "content": "This paper provides a comprehensive benchmark for the\nX-ray image based medical report generation, which covers\nthe mainstream MRG models and LLMs. The LLMs eval-\nuated in this work focus on 7B and 13B which is hardware\nfriendly, and the LLMs with more parameters are not dis-\ncussed due to the limited computational resources. On the\nother hand, there are still many Vision-Language Models\n(VLMs) developed for natural images that are not bench-\nmarked, due to the limited performance of the X-ray image-based medical report generation."}, {"title": "6. Conclusion and Future Works", "content": "In this work, we propose to benchmark the CheXpert\nPlus dataset by re-training the mainstream X-ray report gen-\neration models and large language models. This benchmark\nwill help identify which large models and algorithms are\nleading in this domain, significantly promoting academic\nprogress and technological development. In addition, we\nalso propose a new Mamba-based vision-language large\nmodel for the X-ray image based medical report generation.\nIt involves three pre-training stages which make full use\nof auto-regressive generation loss, Xray-report contrastive\nlearning, and supervised fine-tuning. We validate the effec-\ntiveness of our proposed pre-trained large model on IU X-\nray, MIMIC-CXR, and CheXpert Plus datasets. From the\nnewly built benchmark, we can find that the current large\nlanguage models still perform poorly on the report genera-\ntion task.\nIn our future works, we will consider introducing struc-\ntured knowledge graphs into the large language model to\nguide the report generation. In addition, fine-grained X-ray\nimage patch mining guided by the medical report may be\nanother idea worthy of study. We leave them as the future\nworks."}]}