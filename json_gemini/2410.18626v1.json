{"title": "SAMG: State-Action-Aware Offline-to-Online Reinforcement Learning with Offline Model Guidance", "authors": ["Liyu Zhang", "Haochi Wu", "Xu Wan", "Quan Kong", "Ruilong Deng", "Mingyang Sun"], "abstract": "The offline-to-online (O2O) paradigm in reinforcement learning (RL) utilizes pre-trained models on offline datasets for subsequent online fine-tuning. However, conventional O2O RL algorithms typically require maintaining and retraining the large offline datasets to mitigate the effects of out-of-distribution (OOD) data, which limits their efficiency in exploiting online samples. To address this challenge, we introduce a new paradigm called SAMG: State-Action-Conditional Offline-to-Online Reinforcement Learning with Offline Model Guidance. In particular, rather than directly training on offline data, SAMG freezes the pre-trained offline critic to provide offline values for each state-action pair to deliver compact offline information. This framework eliminates the need for retraining with offline data by freezing and leveraging these values of the offline model. These are then incorporated with the online target critic using a Bellman equation weighted by a policy state-action-aware coefficient. This coefficient, derived from a conditional variational auto-encoder (C-VAE), aims to capture the reliability of the offline data on a state-action level. SAMG could be easily integrated with existing Q-function based O2O RL algorithms. Theoretical analysis shows good optimality and lower estimation error of SAMG. Empirical evaluations demonstrate that SAMG outperforms four state-of-the-art O2O RL algorithms in the D4RL benchmark.", "sections": [{"title": "Introduction", "content": "Offline reinforcement learning (RL)[Lowrey et al. 2019, Fujimoto, Meger, and Precup 2019, Mao et al. 2022, Rafailov et al. 2023] has gained significant popularity due to its isolation from online environments. It relies exclusively on the offline datasets, which can be generated by one or several policies, constructed from historical data or even generated randomly. This paradigm eliminates the risks and costs associated with online interactions, thus offering a safe and efficient pathway to pre-train well-behaved RL algorithms. However, existing offline RL algorithms exhibit an inherent limitation that the offline dataset only covers a partial distribution of the state-action space[Prudencio, Maximo, and Colombini 2023]. Therefore, standard online RL algorithms fail to resist the cumulative overestimation on state-action pairs out of the offline distribution[Nakamoto et al. 2023]. To this end, most offline RL algorithms limit the decision-making scope of the estimated policy within the offline dataset distribution[Kumar et al. 2019, Yu et al. 2021, Janner et al. 2019]. Accordingly, offline RL algorithms are confined in performance by the offline dataset distribution.\nTo overcome the performance limitations of offline RL algorithms and further improve their performance, it is inspiring to perform an online fine-tuning process with the offline pre-trained model. Similar to the successful paradigm of fine-tuning in deep learning[Weiss, Khoshgoftaar, and Wang 2016, Iman, Arabnia, and Rasheed 2023], this paradigm, categorized as offline-to-online (O2O) RL algorithms, is anticipated to enable substantially faster convergence compared to online RL. However, online fine-tuning process inevitably encounters out-of-distribution (OOD) samples which are laid aside in offline pre-training process. This leads to another dilemma that the overestimated pre-trained model may be misguided toward structural damage and performance deterioration when coming across OOD samples[Nair et al. 2020, Kostrikov, Nair, and Levine 2022]. As a result, the O2O RL algorithms tend to remain unchanged or even sharply decline in the initial stage of the fine-tuning process. Existing O2O RL algorithms conquer this by maintaining access to the offline dataset and training with the offline data to restore offline information and restrict model deterioration.\nSpecifically, some fine-tuning algorithms directly inherit the offline dataset as the replay buffer and only get access to online data by incrementally replacing offline data with online ones through iterations [Lyu et al. 2022, Lee et al. 2022, Wang et al. 2024]. This paradigm is tedious given that the sample size of the offline datasets tends to exceed the order of millions [Fu et al. 2020]. Hence, these algorithms exhibit low inefficiency in leveraging online data. Other algorithms adopt hybrid configurations in which a buffer is evenly composed of both online and offline samples [Song et al. 2023, Nakamoto et al. 2023]. Though this setting mitigates the inefficiency, it still visits a considerable amount of offline data and has not departed from the burden of offline data. In summary, existing O2O RL algorithms significantly compromise the efficiency of utilizing online data to mitigate the negative impact of OOD samples.\nThis compromise could result in several undesirable outcomes. Firstly, the quality of offline data is often heterogeneous, with portions of the data potentially being non-conducive to algorithmic improvement. Consequently, the"}, {"title": "Related works", "content": "Online RL with assistance of dataset. Expert level demonstrations are considered and combined with online data to accelerate online training process [Hester et al. 2018, Li et al. 2023, Nair et al. 2018, Rajeswaran et al. 2018, Vecerik et al. 2017], while other offline data is utilized to insert into the online replay buffer to supplement online learning process, without the requirement of being expert level, whose setting is called hybrid RL [Hester et al. 2018, Schaal 1996, Song et al. 2023]. Auxiliary behavioral cloning losses with policy gradients are utilized to guide the updates and accelerate convergence [Kang, Jie, and Feng 2018, Zhu et al. 2019, 2018].\nO2O RL. Large task-agnostic datasets are utilized to pre-train a model offline and feed for subsequent RL tasks online [Aytar et al. 2018, Baker et al. 2022, Fujimoto, Meger, and Precup 2019, Lifshitz et al. 2024, Yuan et al. 2024]. Recent developments in model-based algorithms address high-dimensional problems and facilitate smooth, efficient adaptation to online samples [Lowrey et al. 2019, Mao et al. 2022, Rafailov et al. 2023].\nA different category of work is to train the offline models with provided datasets and then fine-tune the pre-trained models online based on the offline datasets [Beeson and Montana 2022, Kostrikov, Nair, and Levine 2022, Lee et al. 2022, Lyu et al. 2022, Mark et al. 2022, Nair et al. 2020, Nakamoto et al. 2023, Wang et al. 2024, Wu et al. 2022]. This line of work is based on offline algorithms [Kumar et al. 2020, Levine et al. 2020, Nair et al. 2020] and falls into the trap of buffer dependency. Most of these works inherit the large offline buffer while Cal-QL [Nakamoto et al. 2023] adopts the hybrid RL setting by evenly mixing online and offline samples [Song et al. 2023]. Nevertheless, we demonstrate that they both reduce sample efficiency and struggle to handle changed environment dilemmas.\nThe pipeline of our work is most closely aligned with the approach proposed by Wang et al.[Wang et al. 2024], in which each paradigm is applicable to RL algorithms utilizing a Q-function. However, our specific implementation diverges significantly. Our method mainly differs from previous ones [Beeson and Montana 2022, Kostrikov, Nair, and Levine 2022, Lee et al. 2022, Lyu et al. 2022, Mark et al. 2022, Nair et al. 2020, Wu et al. 2022] that does not retain offline buffer and directly utilize the offline model for assistance instead."}, {"title": "Preliminaries", "content": "RL problem is defined as a sequential decision-making process, where a RL agent interacts with an environment in the form of Markov Decision Process (MDP), which is formulated as a tuple M = (S, A, P, r, \u03b3, \u03c4). S represents the state space and A represents the action space. P(s'|s, a) denotes the unknown function of dynamics model and r(s, a) denotes the unknown function of reward model limited by |r(s,a)| \u2264 Rmax\u00b7 \u03b3 \u2208 (0,1) denotes the discount factor for future reward and denotes the initial state distribution. The goal of RL is to acquire a policy \u03c0(a|s) to maximize the cumulative discounted value function or state-value function, which are defined as V\u03c0(s) = Eak~\u03c0(sk)[\u03a3\u03ba\u03b3kr(sk, ak)|so = s] and Q\u03c0(s, a) = Eak~\u03c0(sk)[\u2211kykr(sk, ak)|so = s, ao = a] respectively.\nFor actor-critic based RL algorithms, the training process alternates between policy evaluation and policy improvement. Policy evaluation phase maintains an estimated Q-function Qe(s, a) parameterized by @ and updates by applying the Bellman operator, B\u03c0Q = r + P\u03c0Q (referred to as standard Bellman operator), where P\u03c0 couples the dynamics function with the policy: P\u03c0$Q(s,a) = Es'~P(s'|s,a),\u03b1'~\u03c0\u03c6(a'|s')[Q(s', a')]. While policy improvement phase sustains a policy \u03c0\u03c1(\u03b1|s) parameterized by \u03c6 and updates the policy towards higher estimated values by the Qe(s, a)."}, {"title": "SAMG: methodology", "content": "Our approach, Offline-Model-Guided and Distribution-Aware (SAMG) O2O RL, seeks to leverage the pre-trained offline model, rather than relying on the tedious offline data, to guide the online fine-tuning process. Therefore two critical questions arise:\n1 How can we accurately extract the information contained within the offline model?\n2 How can we adaptively incorporate the information into the online training process?\nWe introduce a model-guided technique alongside a state-action-aware weighting mechanism to resolve these issues.\nOffline-model-guided technique\nOffline-model-guided technique is designed to solve the problem 1. For an algorithm containing a state-action value function Q(s, a), which is approximated by some network Qe(s, a) parameterized by 6. This function represents the quality given a specific state-action pair in the view of the offline dataset. So we can freeze and preserve this well-trained offline Q-network and query the offline opinion of the online state-action pairs. To combine the offline information with the online information, we integrate the frozen offline opinion with online Q-values with some weight. Formally, the following policy evaluation equation can be obtained:\n$Q(s, a) =r(s, a) + \u03b3 [(1 \u2212 a(s, a))Q(s', a') + a(s,a)Q^{off}(s', a')] .$\nwhere the Q-network and a \u2208 (0, 1) denote a function class that gives a state-action-adaptive weight and can be implemented with any reasonable form. Novel parts that differ from the standard Bellman equation is marked in blue."}, {"title": "Distribution-aware weight", "content": "The function class a(s, a) provides the distribution-aware weights to solve the problem 2. Intuitively, we tend to assign high weights to samples within the offline distribution because we have prior comprehension of these samples and the judgment of the offline model is reliable. As for samples distant from the offline distribution, which can be treated as OOD samples, we have limited knowledge about them and tend to assign low weights to them. Any structure that satisfies this requirement could be treated as an instantiation of SAMG.\nIn this paper, the conditional variational auto-encoder (C-VAE) [Kingma et al. 2014] is mainly utilized to infer the offline confidence of online samples, which is broadly used in offline algorithms to represent the behavior policy [Fujimoto, Meger, and Precup 2019, Kumar et al. 2019, Xu, Zhan, and Zhu 2022], and an embedding network [Badia et al. 2019] is briefly introduced as well. Previous works leverage the C-VAE structure to construct the divergence error, which is distant from our setting. While Xu [Xu, Zhan, and Zhu 2022] adopts a similar setting to our work, their C-VAE structure is state-conditional. We find such structure might suffer from posterior collapse [Lucas et al. 2019, Wang, Blei, and Cunningham 2021] in certain environments, as illustrated in Appendix B.1.1.\nPosterior collapse implies that the encoder completely fails and the KL-divergence term vanishes to zero for any input. Thus, the decoder structure actually takes noise as input and reconstructs a sample all by itself. This phenomenon is extremely detrimental in RL setting because the output of the encoder is needed for the distribution-aware weight but it now fails to operate.\nTo overcome this challenge, the C-VAE structure is updated that the encoder takes the state-action pairs as input to meet the state-action-adaptive requirement and the decoder reconstructs from the latent layer conditioned on state-action pairs (similar to the role of RL environments). We also utilize the KL-annealing technique [Bowman et al. 2015] in some environments, detailed in Appendix B.1.2. Through this approach, our state-action-aware C-VAE structure could better resist the posterior collapse. Formally, SAMG adopts the following evidence lower bound (ELBO) on the offline dataset to get the state-action-aware C-VAE:\n$max_{\\Psi_1,\\Psi_2} E_{z\\sim Enc_{\\Psi_1}} [log Dnc_{\\Psi_2} (s'|z, s, a)] - \\beta D_{KL} [Enc_{\\Psi_1} (z|s, a)||Dnc_{\\Psi_2}(z)] $\nwhere $Enc_{\\Psi_1} (z|s, a)$ and $Dnc_{\\Psi_2} (s'|z, s, a)$ represent the encoder structure and decoder structure respectively; $Dnc_{\\Psi_2} (z)$ denotes the prior distribution of the encoder; and $D_{KL} [P||q]$ denotes the Kullback-Leibler divergence. The former error term denotes the reconstruction loss of the encoder and the latter one denotes the Kullback-Leibler divergence between encoder distribution and the prior distribution of z.\nAfter fitting the C-VAE network, a well-converged estimator can be obtained which encodes the state-action pair to a nearly standard normal distribution, i.e., the output mean"}, {"title": "Analysis of SAMG", "content": "Intuitive analysis of SAMG\nIntrinsic reward form of SAMG. Equation 4 can be derived as below:\n$Q(s, a) = [r(s, a) + \\gamma p_{0}(s,a)(Q^{off}(s', a') \u2013 Q(s', a'))] +\\gamma Q(s', a').$\nEquation 5 indicates that the induced offline information could be treated in the form of intrinsic reward. The intrinsic reward is composed of the difference between offline and online state-action value factored by the distribution-aware weight. The intrinsic reward term can be analyzed in two possible situations. If the state-action pair is within the offline distribution where the offline Q-value is reasonably trained and the distribution-aware wight a is notable, then this term suggests that the higher the offline Q-value, the higher the reward. Hence, this intrinsic reward term encourages state-action pairs with higher performance. However, if the state-action pair falls outside the offline distribution where the offline Q-value may be mis-estimated, the wight po is paltry or even set to zero according to Equation 3 and this term is insignificant. Therefore, SAMG could properly utilize the knowledge of the offline model, which is exactly what we expect.\nBellman form of SAMG. Furthermore, Equation 1 can be derived to the following equation by applying the bellman operator:\n$Q(s, a) = (1 \u2212 p_{0}(s,a))B^{\\pi} Q + p_{0}(s,a)B^{\\pi}Q^{off}.$\nwhere $B^{\\pi}Q$ is the standard bellman equation and $B^{\\pi}Q^{off} = r(s,a)+yQ^{off} (s', a')$ is the offline bellman equation of the fixed offline state action function.\nIt can be obtained that our method actually incorporates the standard Bellman equation with the offline Bellman equation by weight po(s, a) and this paradigm just combines the offline opinion with the online opinion weighted by the sample property. More importantly, SAMG reverts to the vanilla algorithm when dealing with OOD samples. This allows SAMG to take full advantage of the conservative settings of the vanilla algorithm for OOD samples. Therefore, SAMG does not induce over-estimation on OOD samples even though SAMG gets rid of the offline dataset. Consequently, SAMG effectively utilizes the vanilla wisdom for handling OOD samples."}, {"title": "Theoretical analysis of SAMG", "content": "In this section, the temporal difference algorithm [Sutton 1988, Haarnoja et al. 2018] is adopted and Equation 4 is proven to still converge to the same optimalily, even with an extra term induced in the tabular setting. For the theoretical tools, SAMG gets rid of the offline dataset and therefore diverges from the hybrid realm of Song et al. [Song et al. 2023] and offline RL scope limited by the dataset, but aligns completely with traditional online RL algorithms [T. Jaakkola and Singh 1994, Thomas 2014, Haarnoja et al. 2018].\nContraction property. Since the standard Bellman operator is broken and a term that could be considered as offline Bellman operator is introduced, we first proof that SAMG still satisfies the contraction property [Keeler and Meir 1969]. The contraction property forms the basis for the necessity of convergence of the iterative RL algorithms. The related theorem and detailed proof can be found in Appendix A.1.\nConvergence optimality and estimation error. Formally, the estimated state-value function at time-step k for any given pair (s, a) is denote by Qk(s, a) and the iterative"}, {"title": "Experimental Results", "content": "Our experimental evaluation focuses on the performance of SAMG after fine-tuning based on four state-of-the-art algorithms on a wide range of offline benchmark tasks containing D4RL [Fu et al. 2020]. All experiments are conducted with compute source detailed in Appendix D.\nBaselines. (i) SAMG algorithms, the SAMG paradigm is constructed on a variety of state-of-the-art O2O RL algorithms, including CQL [Kumar et al. 2020], AWAC [Nair et al. 2020], IQL [Kostrikov, Nair, and Levine 2022]and Cal_QL [Nakamoto et al. 2023]. All algorithms are implemented based on library CORL [Tarasov et al. 2024] with implementation details in Appendix C.1 and C.2. (ii) O2O RL algorithms, we implement the aforementioned O2O RL algorithms (CQL, AWAC, IQL and Cal_QL). We also implement TD3+BC [Fujimoto and Gu 2021] and SPOT [Wu et al. 2022]. (iii) Behavior Cloning (BC), we implement Behavior cloning algorithms based on [Chen et al. 2020].\nBenchmark tasks. We evaluate our algorithm and the baselines across multiple benchmark tasks: (1) The D4RL locomotion tasks [Fu et al. 2020], including three different kinds of environments (Halfcheetah, Hopper, Walker2d) where different robots are manipulated to complete different tasks on three different levels of datasets. (2) The AntMaze tasks that an \"Ant\" robot is controlled to explore and navigate to random goal locations in six levels of environments, as detailed in Appendix C.3.\nEmpirical results\nWe first represent the normalized scores of the vanilla algorithms with and without SAMG integrated. The normalized scores through training on Walker2d and Halfcheetah environments are shown in Figure 2. Constrained by the space, we present the final normalized scores of the other environments (Hopper and Antmaze) in Table 1.\nAs shown in the Figure 2 and the Table 1, SAMG consistently outperforms the vanilla algorithms in the majority of environments, illustrating the superiority of our paradigm. Moreover, it is interesting to note that SAMG exhibits the best performance on the simple algorithm AWAC. SAMG still yields significant improvements on the other algorithms with more complex settings, but not as significant as AWAC. The reason for this counter-intuitive phenomenon is discussed in Appendix C.4., which just illustrates the effectiveness of SAMG. We present the cumulative regret on environment Antmaze in appendix C.5., further demonstrating the outstanding online sample efficicy of SAMG."}, {"title": "Conclusion, limitation and future work", "content": "In this paper, a novel paradigm named SAMG is proposed to eliminate the tedious usage of offline data and leverage the pre-trained offline model instead, thereby ensuring 100% online sample utilization and better fine-tuning performance. SAMG freezes the offline critic and combines online and offline critics with a state-action-aware coefficient. The state-action-aware coefficient models the complex distribution of the offline dataset and provides the probability of a given state-action sample deviating from the offline distribution. Theoretical analysis proves the convergence optimality and lower estimation error. Experimental results demonstrate the superiority of SAMG over vanilla baselines. However, it can be noticed that performance improvement of SAMG is limited if the offline dataset distribution is extremely narrow. This limitation could potentially be mitigated by designing specific update strategies for OOD samples, which serves as an interesting direction for future work."}, {"title": "Appendices", "content": "A. Theoretical analysis\nA.1. Contraction property\nOur algorithm actually breaks the typical Bellman Equation of the RL algorithm denoted as $Q = max_{\u03c0\u2208\u03a0}(r + \u03b3PQ)$. Instead we promote Equation 6. In order to prove the convergence of the updating equation, we introduce the contraction mapping theorem which is widely used to prove the convergence optimality of RL algorithm.\nTheorem 3 (Contraction mapping theorem). For an equation that has the form of $x = f(x)$ where x and f(x) are real vectors, if f is a contraction mapping which means that $||f(x1) - f(x2)|| \u2264 \u03b3 ||X1 - X2||(0 < \u03b3 < 1)$, then the following properties hold.\nExistence: There exists a fixed point $x^*$ that satisfies $f(x^*) = x^*$.\nUniqueness: The fixed point $x^*$ is unique.\nAlgorithm: Given any initial state xo, consider the iterative process: $Xk+1 = f(xk)$, where k = 0,1,2, .... Then Xk convergences to $x^*$ as $k \u2192 \u221e$ at an exponential convergence rate.\nWe just need to prove that this equation satisfies the contraction property of theorem 3 and naturally we can ensure the convergence of the algorithm.\nTake the right hand of equation (6) as function $f(Q)$ and consider any two vectors $Q1, Q2 \u2208 R^S$, and suppose that $\u03c0_1 = arg max(r+\u03b3P_\u03c0Q_1)$, $\u03c0_\u00bd = argmax(r_\u03c0+\u03b3P_\u03c0Q_2)$. Then,\n$f(Q1) = max_{\u03c0\u2208\u03a0} [(1 \u2013 p(s, a))B^{\u03c0} Q_1 + p(s, a)B^{\u03c0} Q^{off})]$\n$=(1 \u2212 p(s, a))B^{\u03c0_1} Q_1 + p(s, a)B^{\u03c0_1} Q^{off}$\n$\u2265(1 \u2212 p(s, a))B^{\u03c0_2}Q_1+p(s,a)B^{\u03c0_\u00bd}Q^{off},$\nand similarly:\n$f(Q2) \u2265 (1 \u2212 p(s, a))B^{\u03c0_1} Q_2 + p(s, a)B^{\u03c0_1} Q^{off}.$\nTo simplify the derivation process, we use p\u03c0 to represent p(s, \u03c0(a|s)) considering that values of p function class are determined by the policy of any given state. As a result,\n$f(Q1) - f(Q2)$\n$=(1 \u2212 p^{\u03c0_\u00b9})B^{\u03c0_\u00b9} Q_1 + p^{\u03c0_\u00b9} B^{\u03c0_\u00b9} Q^{off}$\n$- [(1-p^{\u03c0_2}) B^{\u03c0_2} Q_2 + p^{\u03c0_2} B^{\u03c0_2} Q^{off}]$\n$\u2264(1-p^{\u03c0_1})B^{\u03c0_1}Q_1 + p^{\u03c0_1} B^{\u03c0_1} Q^{off} - [(1 \u2212 p^{\u03c0_i})B^{\u03c0_i} Q_2 + p^{\u03c0_i} B^{\u03c0_i} Q^{off}]$\n$=(1-p^{\u03c0_1})(B^{\u03c0_1}Q_1 - B^{\u03c0_2}Q_2)$\n$=\u03b3(1 \u2212 p^{\u03c0_1})P^{\u03c0_1} (Q_1 - Q_2)$\n$\u2264\u03b3P^{\u03c0} (Q_1 - Q_2).$\nWe can see that the result reduces to that of the normal Bellman equation and therefore, the following derivation is omitted. As a result, we get,\n$||f(Q1) \u2212 f(Q2) ||\u221e \u2264 \u03b3 ||Q1 - Q2||\u221e,$\nwhich concludes the proof of the contraction property of f(Q).\nA.2. Convergence optimality\nWe consider a tabular setting for simplicity. We first write down the iterative form of Equation 4 as below:\nif s = sk, a = ak,\n$Q_{k+1}(s, a) = Q_k(s, a) - \u03b1k(s, a) [Q_k(s, a) \u2013 (rk+1 + \u03b3Q_k(s_{k+1},a_{k+1}))] - \u03b1_k(s,a)p(s, a) (Q^{off} (s_{k+1}, a_{k+1}) - Q_k(s_{k+1}, a_{k+1})) .$\nelse,\n$Q_{k+1}(s, a) = Q_k(s,a).$\nThe error of estimation is defined as:\n$\u2206k(s, a) = Qk(s, a) \u2013 Q\u03c0(s, a).$\nwhere Q\u03c0(s, a) is the state action value s under policy \u03c0. Deducting Q\u03c0(s, a) from both sides of 7 gets:\n$Ak+1(s, a) = (1 \u2212 ak(s, a)) \u2206k(\u03c2, \u03b1) + \u03b1\u03ba(\u03c2, \u03b1)\u03b7\u03ba(s, a), S = Sk, a \u03b1\u03ba.$\nwhere\n$\u03b7k(s, \u03b1)$\n$= [rk+1 + \u03b3Qk(Sk+1, Ak+1) - Q\u03c0(s, \u03b1)] + yp(s, a) (Qoff (Sk+1, Ak+1) \u2013 Qk(Sk+1,Ak+1))$\n$= [rk+1+\u03b3Qk(Sk+1, Ak+1) - Q\u03c0(s, a)] + yp(s, a) {[Qoff (Sk+1, Ak+1) - Q\u03c0(Sk+1,Ak+1)] - [Q\u03c0(Sk+1, Ak+1) - Qk(Sk+1,Ak+1)]}$\n$= [rk+1 + \u03b3Qk(Sk+1, Ak+1) - Q\u03c0(s, a)]  - \u03b3p(s, a) (Qk(Sk+1, Ak+1) - Q\u03c0(Sk+1,Ak+1)) + \u03b3p(s, a) (Qoff (Sk+1, Ak+1) \u2013 Q\u03c0(Sk+1, Ak+1)) .$\nSimilarly, deducting Q(s, a) from both side of Equation 14 gets:\n$Ak+1(s, a) = (1 \u2212 ak(s, a)) \u2206k(s,a) + \u03b1\u03ba(s,a)\u03b7\u03ba(s, a), s \u2260 skor a \u2260 \u03b1\u03ba.$\nthis expression is the same as 16 except that ak(s, a) and \u03b7\u03ba(s, a) is zero. Therefore we observe the following unified expression:\n$Ak+1(8, \u03b1) = (1 \u2212 \u03b1\u03ba(s, a)) \u2206k(s, a) + \u03b1\u03ba(s, \u03b1)\u03b7\u03ba(s, a).$\nTo further analyze the convergence property, we introduce Dvoretzky's theorem [T. Jaakkola and Singh 1994]:"}]}