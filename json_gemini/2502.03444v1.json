{"title": "Masked Autoencoders Are Effective Tokenizers for Diffusion Models", "authors": ["Hao Chen", "Yujin Han", "Fangyi Chen", "Xiang Li", "Yidong Wang", "Jindong Wang", "Ze Wang", "Zicheng Liu", "Difan Zou", "Bhiksha Raj"], "abstract": "Recent advances in latent diffusion models\nhave demonstrated their effectiveness for high-\nresolution image synthesis. However, the prop-\nerties of the latent space from tokenizer for bet-\nter learning and generation of diffusion models\nremain under-explored. Theoretically and empiri-\ncally, we find that improved generation quality is\nclosely tied to the latent distributions with better\nstructure, such as the ones with fewer Gaussian\nMixture modes and more discriminative features.\nMotivated by these insights, we propose MAE-\nTok, an autoencoder (AE) leveraging mask mod-\neling to learn semantically rich latent space while\nmaintaining reconstruction fidelity. Extensive ex-\nperiments validate our analysis, demonstrating\nthat the variational form of autoencoders is not\nnecessary, and a discriminative latent space from\nAE alone enables state-of-the-art performance\non ImageNet generation using only 128 tokens.\nMAETok achieves significant practical improve-\nments, enabling a gFID of 1.69 with 76\u00d7 faster\ntraining and 31\u00d7 higher inference throughput for\n512x512 generation. Our findings show that the\nstructure of the latent space, rather than varia-\ntional constraints, is crucial for effective diffusion\nmodels. Code and trained models are released\u00b9.", "sections": [{"title": "1. Introduction", "content": "Diffusion models (Sohl-Dickstein et al., 2015a; Ho et al.,\n2020; Rombach et al., 2022a; Peebles & Xie, 2023) have\nrecently emerged as a powerful class of generative models,\nachieving state-of-the-art (SOTA) performance on various\nimage synthesis tasks (Deng et al., 2009; Ghosh et al., 2024).\nAlthough originally formulated in pixel space (Ho et al.,\n2020; Dhariwal & Nichol, 2021), subsequent research has\nshown that operating in a latent space a compressed repre-\nsentation typically learned by a tokenizer can substantially\nimprove the efficiency and scalability of diffusion models\n(Rombach et al., 2022a). By avoiding the high-dimensional\npixel domain during iterative diffusion and denoising steps,\nlatent diffusion models dramatically reduce computational\noverhead and have quickly become the de facto paradigm\nfor high-resolution generation (Esser et al., 2024).\nHowever, a key question remains: What constitutes a \"good\"\nlatent space for diffusion? Early work primarily employed\nVariational Autoencoders (VAE) (Kingma, 2013) as tok-\nenizers, which ensure that the learned latent codes follow\na relatively smooth distribution (Higgins et al., 2017) via\na Kullback-Leibler (KL) constraint. While VAEs can em-\npower strong generative results (Ma et al., 2024; Li et al.,\n2024b; Deng et al., 2024), they often struggle to achieve\nhigh pixel-level fidelity in reconstructions due to the im-\nposed regularization (Tschannen et al., 2025). In contrast,\nrecent explorations with plain Autoencoders (AE) (Hinton &\nSalakhutdinov, 2006; Vincent et al., 2008) produce higher-\nfidelity reconstructions but may yield latent spaces that are\ninsufficiently organized or too entangled for downstream\ngenerative tasks (Chen et al., 2024b). Indeed, more re-\ncent studies emphasize that high fidelity to pixels does not\nnecessarily translate into robust or semantically disentan-\ngled latent representations (Esser et al., 2021; Yao & Wang,\n2025); leveraging latent alignment with pre-trained models\ncan often improve generation performance further (Li et al.,\n2024c; Chen et al., 2024a; Qu et al., 2024; Zha et al., 2024).\nIn this work, we attempt to answer this question by investi-\ngating the interaction between the latent distribution learned\nby tokenizers, and the training and sampling behavior of\ndiffusion models operating in that latent space. Specifically,\nwe study AE, VAE and the recently emerging representation\naligned VAE (Li et al., 2024c; Chen et al., 2024a; Zha et al.,\n2024; Yao & Wang, 2025), by fitting a Gaussian mixture\nmodel (GMM) into their latent space. Empirically, we show\nthat a latent space with more discriminative features, whose\nGMM modes are fewer, tends to produce a lower diffusion\nloss. Theoretically, we prove that a latent distribution with\nfewer GMM modes indeed leads to a lower loss of diffusion\nmodels and thus to better sampling during inference.\nMotivated by these insights, we demonstrate that diffusion\nmodels trained on AEs with discriminative latent space are\nenough to achieve SOTA performance. We propose to train\nAEs as as Masked Autoencoders (MAE) (He et al., 2022; Xie\net al., 2022; Wei et al., 2022), a self-supervised paradigm\nthat can discover more generalized and discriminative rep-\nresentations by reconstructing proxy features (Zhang et al.,\n2022). More specifically, we adopt the transformer architec-\nture of tokenizers (Yu et al., 2021; 2024c; Li et al., 2024c;\nChen et al., 2024a) and randomly mask the image tokens\nat the encoder, whose features need to be reconstructed at\nthe decoder (Assran et al., 2023). To maintain a pixel de-\ncoder with high reconstruction fidelity, we adopt auxiliary\nshallow decoders that predict the features of unseen tokens\nfrom seen ones to learn the representations, along with the\npixel decoder which is normally trained as previous tok-\nenizers. The auxiliary shallow decoders introduce trivial\ncomputation overhead during training. This design allows\nus to extend the MAE objective that reconstructs masked\nimage patches, to simultaneously predict multiple targets\nsuch as HOG (Dalal & Triggs, 2005) features (Wei et al.,\n2022), DINOv2 features (Oquab et al., 2023), CLIP embed-\ndings (Radford et al., 2021; Zhai et al., 2023), and Byte-Pair\nEncoding (BPE) indices with text (Huang et al., 2024).\nFurthermore, we reveal an interesting decoupling effect: the\ncapacity to learn a discriminative and semantically rich la-\ntent space at the encoder can be separated from the capacity\nto achieve high reconstruction fidelity at the decoder. In par-\nticular, a higher mask ratio (40\u201360%) in MAE training often\ndegrades immediate pixel-level quality. However, by freez-\ning the AE's encoder, thus preserving its well-organized la-\ntent space, and fine-tuning only the decoder, we can recover\nstrong pixel-level reconstruction fidelity without sacrificing"}, {"title": "2. On the Latent Space and Diffusion Models", "content": "To study the relationship of latent space for diffusion models,\nwe start with popular tokenizers, including AE (Hinton &\nSalakhutdinov, 2006), VAE (Kingma, 2013), representation\naligned VAE, i.e., VAVAE (Yao & Wang, 2025). We train\ndiffusion models on them and establish connections between\nlatent space properties and the quality of the final image\ngeneration through empirical and theoretical analysis.\nEmpirical Analysis. Inspired by existing theoretical work\n(Chen et al., 2022; 2023; Benton et al., 2024), our investiga-\ntion of the connection between latent space and generation\nquality starts with a high-level intuition. With optimal dif-\nfusion model parameters, such as sufficient total time steps\nand adequately small discretization steps, the generation\nquality of diffusion models is dominated by the denoising\nnetwork's training loss (Chen et al., 2022; 2023; Benton\net al., 2024), while the effectiveness of training diffusion\nmodel via DDPM (Ho et al., 2020) heavily depends on the\nhardness of learning the latent space distribution (Shah et al.,\n2023; Diakonikolas et al., 2023; Gatmiry et al., 2024). Spe-\ncially, when the training data distribution is too complex\nand multi-modal, i.e., not discriminative enough, the denois-\ning network may struggle to capture such entangled global\nstructure of latent space, resulting in degraded generation.\nBuilding upon this intuition, we use the Gaussian Mixture\nModels (GMM) to evaluate the number of modes in alter-\native latent space representations, where a higher number\nof modes indicates a more complex structure. Fig. 2a ana-\nlyzes the GMM fitting by varying the number of Gaussian\ncomponents and comparing their negative log-likelihood\nlosses (NLL) across different latent spaces, where a lower\nNLL indicates better fitting quality. We observe that, to\nachieve comparable fitting quality, i.e., similar GMM losses,\nVAVAE requires fewer modes compared to VAE and AE.\nFewer modes are sufficient to adequately represent the la-\ntent space distributions of VAVAE compared to those of AE\nand VAE, highlighting simpler global structures in its latent\nspace. Correspondingly, Fig. 2b reports the training losses\nof diffusion models with AE, VAE, and VAVAE, which (al-"}, {"title": "Theoretical Analysis", "content": "After observing experimental phe-\nnomena that align with our high-level intuition, we further\npresent a concise theoretical analysis here to justify the ra-\ntionale behind it, with more details provided in Appendix A.\nFollowing the empirical analysis setup, we first consider a\nlatent data distribution in d dimensions modeled as a GMM\nwith K equally weighted Gaussians:\n$P_o = \\frac{1}{K} \\sum_{i=1}^K \\mathcal{N}(\\mu_i, I),$\n(1)\nConsidering the classic diffusion model DDPM (Ho et al.,\n2020) and following the training objective as Shah et al.\n(2023), the score matching loss of DDPM at timestep t is\n$\\min_{W} \\mathbb{E}[||s_w(x, t) - \\nabla_x \\log p_t(x)||^2],$\n(2)\nwhere $s_w(x,t)$ represents the denoising network and\n$\\nabla_x \\log p_t(x)$ denotes the oracle score function.\nThen, we establish the following theorem to show that more\nmodes typically require larger training sample sizes for\ndiffusion models to achieve comparable generation quality.\nTheorem 2.1. (Informal, see Theorem A.7) Let the data\ndistribution be a mixture of K Gaussians as defined in\nEq. (1). Then assume the norm of each mode is bounded\nby some constants, let d be the data dimension, T be the\ntotal time steps, and $\\epsilon$ be a proper target error parameter. In\norder to achieve a $O(T\\epsilon^2)$ error in KL divergence between\ndata distribution and generation distirbution, the DDPM\nalgorithm may require using $n \\geq n'$ number of samples:\n$n' = \\Theta (\\frac{K^4d^5B^6}{\\epsilon^2}),$\n(3)\nwhere the upper bound of the mean norm satisfies\n$\\max_i ||\\mu_i || \\leq B$.\nTheorem 2.1 combines Theorem 16 from (Shah et al., 2023)\nand Theorem 2.2 from (Chen et al., 2023), showing that\nto achieve a comparable generation quality $O(T\\epsilon^2)$, latent\nspaces with more modes (K) require a larger training sam-\nple size, scaling as $O(K^4)$.This theoretically help explain\nwhy, under a finite number of training samples, latent spaces\nwith more modes (e.g., AE and VAE) produce worse genera-\ntions with higher gFID. We provide additional experimental\nresults in Appendix A, demonstrating that these latent distri-\nbutions share comparable upper bounds B, thus justifying\nour focus primarily on the impact of mode number K."}, {"title": "3. Method", "content": "Motivated by our analysis, we show that the variational form\nof VAEs may not be necessary for diffusion models, and\nsimple AEs are enough to achieve SOTA generation perfor-\nmance with 128 tokens, as long as they have discriminative\nlatent spaces, i.e., with fewer GMM modes. We term our\nmethod as MAETok, with more details as follows.\nWe build MAETok upon the recent 1D tokenizer design with\nlearnable latent tokens (Yu et al., 2024c; Li et al., 2024c;\nChen et al., 2024a). Both the encoder E and decoder D\nadopt the Vision Transformer (ViT) architecture (Dosovit-\nskiy et al., 2021; Yu et al., 2021), but are adapted to handle\nboth image tokens and latent tokens, as shown in Fig. 3.\nEncoder. The encoder first divides the input image $I \\in$\n$\\mathbb{R}^{H\\times W\\times 3}$ into N patches according to a predefined patch\nsize P, each mapped to an embedding vector of dimension\nD, resulting in image tokens $x \\in \\mathbb{R}^{N\\times D}$. In addition, we\ndefine a set of L learnable latent tokens $z \\in \\mathbb{R}^{L\\times D}$. The\nencoder transformer takes the concatenation of image patch\nembeddings and latent tokens $[x; z] \\in \\mathbb{R}^{(N+L)\\times D}$ as its\ninput, and outputs the latent representations $h \\in \\mathbb{R}^{L\\times H}$\nwith a dimension of H from only the latent tokens:\n$h = E ([x; z]).$\n(4)\nDecoder. To reconstruct the image, we use a set of N\nlearnable image tokens $e \\in \\mathbb{R}^{N\\times H}$. We concatenate these\nmask tokens with h as the input to the decoder, and takes\nonly the outputs from mask tokens for reconstruction:\n$x = D([e; h]]).$\n(5)\nWe then use a linear layer on top of $x \\in \\mathbb{R}^{N\\times D}$ to regress\nthe pixel values and obtain the reconstructed image $\\hat{I}$.\nPosition Encoding. To encode spatial information, we ap-\nply 2D Rotary Position Embedding (RoPE) to the image\npatch tokens x at the encoder and the image tokens e at the\ndecoder. In contrast, the latent tokens z (and their encoded\ncounterparts h) use standard 1D absolute position embed-\ndings, since they do not map to specific spatial locations.\nThis design ensures that patch-based tokens retain the notion\nof 2D layout, while the learned latent tokens are treated as a\nset of abstract features within the transformer architecture.\nTraining objectives. We train MAETok using the standard\ntokenizer losses as in previous work (Esser et al., 2021):\n$L = L_{recon} + \\lambda_1 L_{percep} + \\lambda_2L_{adv},$\n(6)\nwith $L_{recon}, L_{percep}$, and $L_{adv}$ denoting as pixel-wise mean-\nsquare-error (MSE) loss, perceptual loss (Larsen et al., 2016;\nJohnson et al., 2016; Dosovitskiy & Brox, 2016; Zhang\net al., 2018), and adversarial loss (Goodfellow et al., 2020;\nIsola et al., 2018), respectively, and $\\lambda_1$ and $\\lambda_2$ being hyper-\nparameters. Note that MAETok is a plain AE architecture,\ntherefore, it does not require any variational loss between\nthe posterior and prior as in VAEs, which simplifies training."}, {"title": "3.2. Mask Modeling", "content": "Token Masking at Encoder. A key property of MAETok is\nthat we introduce mask modeling during training, following\nthe principles of MAE (He et al., 2022; Xie et al., 2022), to\nlearn a more discriminative latent space in a self-supervised\nway. Specifically, we randomly select a certain ratio, e.g.,\n40%-60%, of the image patch tokens according to a binary\nmasking indicator $M \\in \\mathbb{R}^N$, and replace them with the\nlearnable mask tokens $m \\in \\mathbb{R}^D$ before feeding them into the\nencoder. All the latent tokens are maintained to more heavily\naggregate information on the unmasked image tokens and\nused to reconstruct the masked tokens at the decoder output.\nAuxiliary Shallow Decoders. In MAE, a shallow decoder\n(He et al., 2022) or a linear layer (Xie et al., 2022; Wei et al.,\n2022) is required to predict the target features, e.g., raw\npixel values, HOG features, and features from pre-trained\nmodels, of the masked image tokens from the remaining\nones. However, since our goal is to train MAE as tokenizers,\nthe pixel decoder D needs to be able to reconstruct images\nin high fidelity. Thus, we keep D as a similar capacity to\nE, and incorporate auxiliary shallow decoders to predict\nadditional feature targets, which share the same design as\nthe main pixel decoder but with fewer layers. Formally, each\nauxiliary decoder $D_{aux}$ takes the latent representations h\nand concatenate with their own $e^j$ as inputs, and output $y^j$\nas the reconstruction of their feature target $y \\in \\mathbb{R}^{N\\times D^j}$.\n$y^j = D_{aux} ([e^j; h]; \\theta),$\n(7)"}, {"title": "3.3. Pixel Decoder Fine-Tuning", "content": "While mask modeling encourages the encoder to learn a\nbetter latent space, high mask ratios can degrade immediate\nreconstruction. To address this, after training AEs with mask\nmodeling, we freeze the encoder, thus preserving the latent\nrepresentations, and fine-tune only the pixel decoder for a\nsmall number of additional epochs. This process allows\nthe decoder to adapt more closely to frozen latent codes\nof clean images, recovering the details lost during masked\ntraining. We use the same loss as in Eq. (6) for pixel decoder\nfine-tuning and discard all auxiliary decoders in this stage."}, {"title": "4. Experiments", "content": "We conduct comprehensive experiments to validate the de-\nsign choices of MAETok, analyze its latent space, and bench-\nmark the generation performance to show its superiority."}, {"title": "4.1. Experiments Setup", "content": "Implementation Details of Tokenizer. We use XQ-GAN\ncodebase (Li et al., 2024d) to train MAETok. We use ViT-\nBase (Dosovitskiy et al., 2021), initialized from scratch, for\nboth the encoder and the pixel decoder, which in total have\n176M parameters. We set L = 128 and H = 32 for latent\nspace. Three MAETok variants are trained on 256\u00d7256\nImageNet (Deng et al., 2009), and 512\u00d7512 ImageNet, and\na subset of 512\u00d7512 LAION-COCO (Schuhmann et al.,\n2022) for 500K iterations, respectively. In the first stage\ntraining with mask modeling on ImageNet, we adopt a mask\nratio of 40-60%, set by ablation, and 3 auxiliary shallow de-\ncoders for multiple targets of HOG (Dalal & Triggs, 2005),\nDINO-v2-Large (Oquab et al., 2023), and SigCLIP-Large\n(Zhai et al., 2023) features. We adopt an additional auxil-\niary decoder for tokenizer trained on LAION-COCO, which\npredicts the discrete indices of text captions for the image\nusing a BPE tokenizer (Cherti et al., 2023; Huang et al.,\n2024). Each auxiliary decoder has 3 layers also set by abla-\ntion. We set $\\lambda_1 = 1.0$ and $\\lambda_2 = 0.4$. For the pixel decoder\nfine-tuning, we linearly decrease the mask ratio from 60%\nto 0% over 50K iterations, with the same training loss. More\ntraining details of tokenizers are shown in Appendix B.1.\nImplementation Details of Diffusion Models. We use SiT\n(Li et al., 2024a) and LightningDiT (Yao & Wang, 2025)\nfor diffusion-based image generation tasks after training"}, {"title": "4.2. Design Choices of MAETok", "content": "We first present an extensive ablation study to understand\nhow mask modeling and different designs affect the recon-\nstruction of tokenizer and, more importantly, the generation\nof diffusion models. We start with an AE and add different\ncomponents to study both rFID of AE and gFID of SiT-L.\nMask Modeling. In Table 1a, we compare AE and VAE\nwith mask modeling and also study the proposed fine-tuning\nof the pixel decoder. For AE, mask modeling significantly\nimproves gFID and slightly deteriorates rFID, which can\nbe recovered through the decoder fine-tuning stage without\nsacrificing generation performance. In contrast, mask mod-\neling only marginally improves the gFID of VAE, since the\nimposed KL constraint may hinder latent space learning.\nReconstruction Target. In Table 1b, we study how different\nreconstruction targets affect latent space learning in mask\nmodeling. We show that using the low-level reconstruction\nfeatures, such as the raw pixel (with only a pixel decoder)\nand HOG features, can already learn a better latent space, re-\nsulting in a lower gFID. Adopting semantic teachers such as\nDINO-v2 and CLIP instead can significantly improve gFID.\nCombining different reconstruction targets can achieve a\nbalance in reconstruction fidelity and generation quality.\nMask Ratio. In Table 1c, we show the importance of proper\nmask ratio for learning the latent space using HOG target,\nas highlighted in previous works (He et al., 2022; Wei et al.,\n2022; Xie et al., 2022). A low mask ratio prevents the AE\nfrom learning more discriminative latent space. A high mask\nratio imposes a trade-off between reconstruction fidelity and\nthe latent space quality, and thus generation performance.\nAuxiliary Decoder Depth. We study the depth of auxiliary"}, {"title": "4.3. Latent Space Analysis", "content": "We further analyze the relationship between the latent space\nof the AE variants and the generation performance of SiT-L.\nLatent Space Visualization. We provide a UMAP vi-\nsualization (McInnes et al., 2018) in Fig. 4 to intuitively\ncompare the latent space learned by different variants of\nAE. Notably, both the AE and VAE exhibit more entan-\ngled latent embeddings, where samples corresponding to\ndifferent classes tend to overlap substantially. In contrast,\nMAETok shows distinctly separated clusters with relatively\nclear boundaries between classes, suggesting that MAETok\nlearns more discriminative latent representations. In line\nwith our analysis in Section 2 and Fig. 2, a more discrimina-\ntive and separated latent representation of MAETok results\nin much fewer GMM modes and improve the generation\nperformance. More visualization is shown in Appendix C.3."}, {"title": "Latent Distribution and Generation Performance", "content": "We\nassess the latent space's quality by studying the relationship\nbetween the linear probing (LP) accuracy on the latent space,\nas a proxy of how well semantic information is preserved in\nthe latent codes, and the gFID for generation performance.\nIn Fig. 5a, we observe tokenizers with more discriminative"}, {"title": "4.4. Main Results", "content": "Generation. We compare SiT-XL and LightningDiT based\non variants of MAETok in Tables 2 and 3 for the 256x256\nand 512\u00d7512 ImageNet benchmarks, respectively, against\nother SOTA generative models. Notably, the naive SiT-XL\ntrained on MAETok with only 128 tokens and plain AE\narchitecture achieves consistently better gFID and IS with-\nout using CFG: it outperforms REPA (Yu et al., 2024d) by\n3.59 gFID on 256 resolution and establishes a SOTA com-\nparable gFID of 2.79 at 512 resolution. When using CFG,\nSiT-XL achieves a comparable performance with compet-\ning autoregressive and diffusion-based baselines trained on\nVAEs at 256 resolution. It beats the 2B USiT (Chen et al.,\n2024b) with 256 tokens and also achieves a new SOTA of\n1.69 gFID and 304.2 IS at 512 resolution. Better results\nhave been observed with LightningDiT, trained with more\nadvanced tricks (Yao & Wang, 2025), where it outperforms\nMAR-H of 1B parameters and USiT of 2B parameters with-"}, {"title": "4.5. Discussion", "content": "Efficient Training and Generation. A prominent benefit\nof the 1D tokenizer design is that it enables arbitrary number\nof latent tokens. The 256\u00d7256 and 512\u00d7512 images are\nusually encoded to 256 and 1024 tokens, while MAETok\nuses 128 tokens for both. It allows for much more efficient\ntraining and inference of diffusion models. For example,\nwhen using 1024 tokens of 512\u00d7512 images, the Gflops\nand the inference throughput of SiT-XL are 373.3 and 0.1\nimages/second on a single A100, respectively. MAETok\nreduces the Glops to 48.5 and increases throughput to 3.12\nimages/second. With improved convergence, MAETok en-\nables a 76x faster training to perform similarly to REPA.\nUnconditional Generation. An interesting observation\nfrom our results is that diffusion models trained on MAETok\nusually present significantly better generation performance\nwithout CFG, compared to previous methods, yet smaller\nperformance gap with CFG. We hypothesize that the reason\nis that the unconditional class also learns the semantics in\nthe latent space, as shown by the unconditional generation"}, {"title": "5. Related Work", "content": "Image Tokenization. Imgae tokenization aims at trans-\nforming the high-dimension images into more compact and\nstructured latent representations. Early explorations mainly\nused autoencoders (Hinton & Salakhutdinov, 2006; Vin-\ncent et al., 2008), which learn latent codes reduced dimen-\nsionality. These foundations soon inspired methods with\nvariational posteriors, such as VAEs (Van Den Oord et al.,\n2017; Razavi et al., 2019a) and VQ-GAN (Esser et al., 2021;\nRazavi et al., 2019b). Recent work has further improved\ncompression fidelity and scalability (Lee et al., 2022; Yu\net al., 2024a; Mentzer et al., 2023; Zhu et al., 2024), showing\nthe importance of latent structure. More recent efforts have\nshown methods that bridge high-fidelity reconstruction and\nsemantic understanding within a single tokenizer (Yu et al.,\n2024c; Li et al., 2024c; Chen et al., 2024a; Wu et al., 2024;\nGu et al., 2023). Complementary to them, we further high-\nlight the importance of discriminative latent space, which\nallows us to use a simple AE yet achieve better generation.\nImage Generation. The paradigms of image generation\nmainly categorize to autoregressive and diffusion models.\nAutoregressive models initially relied on CNN architectures\n(Van den Oord et al., 2016) and were later augmented with\nTransformer-based models (Vaswani et al., 2023; Yu et al.,\n2024b; Lee et al., 2022; Liu et al., 2024; Sun et al., 2024) for\nimproved scalability (Chang et al., 2022; Tian et al., 2024).\nDiffusion models show strong performance since their debut\nSohl-Dickstein et al. (2015b). Key developments (Nichol\n& Dhariwal, 2021; Dhariwal & Nichol, 2021; Song et al.,\n2022) refined the denoising process for sharper samples.\nA pivotal step in performance and efficiency came with\nlatent diffusion (Vahdat et al., 2021; Rombach et al., 2022b),\nwhich uses tokenizers to reduce dimension and conduct\ndenoising in a compact latent space (Van Den Oord et al.,\n2017; Esser et al., 2021; Peebles & Xie, 2023). Recent\nadvances include designing better tokenizers (Chen et al.,\n2024a; Zha et al., 2024; Yao & Wang, 2025) and combining\ndiffusion with autoregressive models (Li et al., 2024b)."}, {"title": "6. Conclusion", "content": "We presented a theoretical and empirical analysis of latent\nspace properties for diffusion models, demonstrating that\nfewer modes in latent distributions enable more effective\nlearning and better generation quality. Based on these in-\nsights, we developed MAETok, which achieves state-of-the-art performance through mask modeling without requiring\nvariational constraints. Using only 128 tokens, our approach"}, {"title": "Impact Statement", "content": "This work advances the fundamental understanding and tech-\nnical capabilities of machine learning systems, specifically\nin the domain of image generation through diffusion models.\nWhile our contributions are primarily technical, improving\nefficiency and effectiveness of generative models, we ac-\nknowledge that advances in image synthesis technology can\nhave broader societal implications. These may include both\nbeneficial applications in creative tools and design, as well\nas potential concerns regarding synthetic media. We have\nfocused on developing more efficient and robust methods\nfor image generation, and we encourage ongoing discussion\nabout the responsible deployment of such technologies."}, {"title": "A. Theoretical Analysis", "content": "We begin the theoretical analysis by introducing the preliminaries of the problem and the necessary notation.\nFollowing the empirical analysis setting, we first consider the latent data distribution is the GMM with K equally weighted\nGaussians:\n$P_o = \\frac{1}{K} \\sum_{i=1}^K \\mathcal{N}(\\mu_i, I),$\n(9)\nFollowing the the training objective (Shah et al., 2023), we consider the score matching loss of DDPM at timestep t is\n$\\min_{W} \\mathbb{E}[||s_w(x, t) - \\nabla_x \\log p_t(x)||^2]$\n(10)\nwhere $s_w(x, t)$ is the denoising network and $\\log p_t(x)$ is the oracle score. Under the GMM assumption, the explicit solution\nof score function $\\nabla_x \\log p_t(x)$ can be written as\n$\\nabla_x \\ln p_t(x) = \\sum_{i=1}^K w_{i,t}(x) (\\mu_{i,t} - x),$\n(11)\nwhere the weighting parameter is\n$w_{i,t}(x) := \\frac{\\exp(-\\|x - \\mu_{i,t}\\|^2 / 2)}{\\sum_{j=1}^K \\exp(-\\|x - \\mu_{j,t}\\|^2 / 2)}, \\mu_{i,t} := \\mu e^{-t}.$\n(12)\nTherefore, we can consider the denosing neural network with the following format, that is\n$s_{e_t}(x) = \\sum_{i=1}^K W_{i,t}(x) (\\mu_{i,t} - x),$\n(13)\nwhere\n$W_{i,t}(x) := \\frac{\\exp(-\\|x - \\mu_{i,t}\\|^2 / 2)}{\\sum_{j=1}^K \\exp(-\\|x - \\mu_{j,t}\\|^2 / 2)}, \\mu_{i,t} := \\mu e^{-t}.$\n(14)"}, {"title": "Assumptions", "content": "To ensure the denoising network approximates the score function with sufficient accuracy, we consider\nthe following three common assumptions, which constrain the training process from the perspectives of data quality\n(separability), good initialization (warm start), and regularity (bounded mean of target distribution) (Chen et al., 2022; 2023;\nBenton et al., 2024).\nAssumption A.1. (Separation"}]}