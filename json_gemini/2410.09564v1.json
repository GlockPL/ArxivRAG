{"title": "Extended Japanese Commonsense Morality Dataset with Masked Token and Label Enhancement", "authors": ["Takumi Ohashi", "Tsubasa Nakagawa", "Hitoshi Iyatomi"], "abstract": "Rapid advancements in artificial intelligence (AI) have made it crucial to integrate moral reasoning into AI systems. However, existing models and datasets often overlook regional and cultural differences. To address this shortcoming, we have expanded the JCommonsense-Morality (JCM) dataset, the only publicly available dataset focused on Japanese morality. The Extended JCM (eJCM) has grown from the original 13,975 sentences to 31,184 sentences using our proposed sentence expansion method called Masked Token and Label Enhancement (MTLE). MTLE selectively masks important parts of sentences related to moral judgment and replaces them with alternative expressions generated by a large language model (LLM), while re-assigning appropriate labels. The model trained using our eJCM achieved an F1 score of 0.857, higher than the scores for the original JCM (0.837), ChatGPT one-shot classification (0.841), and data augmented using AugGPT, a state-of-the-art augmentation method (0.850). Specifically, in complex moral reasoning tasks unique to Japanese culture, the model trained with eJCM showed a significant improvement in performance (increasing from 0.681 to 0.756) and achieved a performance close to that of GPT-4 Turbo (0.787). These results demonstrate the validity of the eJCM dataset and the importance of developing models and datasets that consider the cultural context.", "sections": [{"title": "1 INTRODUCTION", "content": "With the rapid development and widespread artificial intelligence (AI), the debate over the ethics of AI has intensified. To make better AI, it must have values similar to those of humans, and there is an ongoing debate on how to impart ethics to AI [1, 7, 8]. Various services designed to identify inappropriate content, such as OpenAI's moderation\u00b9 and Microsoft's Azure Cognitive Services\u00b2, have been implemented; however, concerns have been raised regarding bias toward specific languages and cultures exhibited by large language models (LLM) [2, 12, 15]. Since interpretations of ethics vary depending on the region and culture, it is important to develop a model that accounts for this diversity and to construct learning data specific to each language.\nSeveral datasets have been established in English and other major languages to incorporate morality into AI systems [5, 7, 11]. Hendrycks et al. [7] constructed the ETHICS dataset based on five basic concepts of morality-justice, virtue, deontology, utilitarian-ism, and commonsense-and evaluated the learned models on moral judgments. In the commonsense category, the task was to predict whether an action should or should not have been performed according to a commonsense moral judgment. The data were a combination of short (10K sentences) and long (11K sentences) scenarios, and the RoBERTa model showed a correct response rate of approximately 90%. In this context, Takeshita et al. [18] introduced JCommonsenseMorality (JCM), the sole commonsense morality dataset available in Japanese. However, the sentences contained in the JCM dataset lack both quantity and diversity.\nData augmentation techniques are also used in natural language processing (NLP) to address data variability [6, 17, 20]. While conventional text augmentation methods have limited in generating high-quality and diverse data, interactive LLMs equipped with re-inforcement learning from human feedback (RLHF) [13] enable the creation of more varied data [3, 19, 21]. Dai et al. [3] proposed AugGPT, a method that leverages ChatGPT to generate sentences similar to the existing sentences, thus serving as a data augmentation technique. AugGPT has shown superior performance to that of 19 data extension methods in several tasks, including Amazon review classification and NLP tasks in the medical domain. However, as AugGPT mainly paraphrases existing sentences, it cannot provide novel cases or topics. Thus, it does not take full advantage of the extensive knowledge of LLMs.\nIn this paper, we propose a new data enhancement method, Masked Token and Label Enhancement (MTLE), to extend existing datasets and increase case variability. MTLE achieves more diverse sentence expansion by replacing important parts of sentences and"}, {"title": "2 GENERATION OF EXTENDED JCM DATASET", "content": "In this study, we extended the JCM [18]\u2074 dataset, the only publicly available Japanese commonsense morality dataset, using the proposed MTLE, and generating eJCM."}, {"title": "2.1 JCM Dataset", "content": "Table 1 provides examples from the JCM dataset, which consists of sentence pairs with slight differences affecting moral judgment."}, {"title": "2.2 Masked Token and Label Enhancement", "content": "We designed MTLE to augment datasets consisting of sentences whose labels change according to changes in situations or actions, such as JCM. MTLE consists of three steps, as shown in Figure 1: mask creation, sentence generation, and relabeling."}, {"title": "2.2.1 Mask Creation Step", "content": "In this step, the matching parts are extracted from the sentence pairs in the dataset to increase the variation of pairs whose moral evaluation changes with the change of the sentence clause. Using the Japanese NLP library GiNZA\u2075, the sentences are divided into words, and the initial and final identical parts are extracted, with <> inserted between them, forming a new sentence, which is called the mask sentence. If the mask sentence is less than six characters, including <>, it is likely to generate irrelevant sentences, so this mask sentence is not used."}, {"title": "2.2.2 Sentence Generation Step", "content": "Next, using an LLM, three morally acceptable and three morally unacceptable sentences are generated from the mask sentence by replacing <> with a new word or phrase."}, {"title": "2.2.3 Relabeling Step", "content": "The LLM judges each of the six sentences as morally \u201cacceptable,\u201d \u201cunacceptable,\u201d or \u201cindistinguishable\u201d and annotates them with 0, 1, and 2, respectively. We added the label \"indistinguishable\" to increase the accuracy of the label by removing strange or morally ambiguous sentences. We also removed sentences that overlapped with the original or other generated sen-tences. Finally, the augmented dataset included sets of up to three morally acceptable and three morally unacceptable sentences to avoid label bias."}, {"title": "3 EXPERIMENTS", "content": ""}, {"title": "3.1 Evaluation of MTLE", "content": "To verify the effectiveness of the proposed MTLE, we compared the following models to estimate moral applicability for the JCM test dataset: (1) pretrained NLP models fine-tuned with the original JCM dataset, (2) models fine-tuned with the JCM dataset extended using AugGPT, a state-of-the-art data extension methods employing LLM, and (3) models fine-tuned with eJCM dataset created using MTLE.\nUsing AugGPT, we generated three sentences from each sentence in JCM to match the number of sentences generated by MTLE. However, we did not expand sentences if the prompts did not work as intended. The ChatGPT model used for sentence generation and annotation using both AugGPT and MTLE was GPT-3.5 Turbo\u2076 (model as of November 6, 2023).\nWe also conducted an experiment to evaluate the models' com-prehension of sentences that require an understanding of Japan-specific culture and morality. For this experiment, we manually extracted from JCM only those sentences that require an under-standing of Japan-specific words and phrases.\nIn each experiment, we also evaluated and compared the moral judgment performance of ChatGPT (GPT-3.5 Turbo) and GPT-4 Turbo (both models as of November 6, 2023) using a one-shot prompt for each."}, {"title": "3.2 Implementation Details", "content": "We used BERT [4]\u2078 and RoBERTa [9]\u2079 pretrained on the Japanese version of Wikipedia and CC-100. We used cross-entropy as the loss function and performed optimization using AdamW [10]. We applied early stopping with a maximum of 20 epochs. For BERT, the learning rates tested were {1, 2, 3, 4, 5} \u00d7 10\u207b\u2075. For RoBERTa,"}, {"title": "4 RESULTS", "content": ""}, {"title": "4.1 eJCM Dataset", "content": "Table 2 shows examples of sentences generated using MTLE. In the first set of examples, <> has been replaced with words indicating what a 19-year-old was given, reflecting Japanese law, which pro-hibits drinking and smoking for individuals under 20 years of age. In the second set, concerning behavior at a supermarket, <> has been replaced with phrases rather than words.\nTable 3 shows the statistics for the eJCM dataset. This is an extension of the JCM dataset using. The eJCM is available from https://github.com/IyatomiLab/extended-jcm. Using MTLE, we gen-erated approximately four times as many sentences as those con-tained in the original JCM dataset, but after removing strange, overlapping, and unclassifiable sentences, eJCM contained about 2.2 times more sentences than JCM."}, {"title": "4.2 Performance of MTLE", "content": "Table 4 shows the performance of each model in estimating moral acceptability. For both BERT and RoBERTa, fine-tuning with the eJCM dataset resulted in better moral judgment performance than fine-tuning with the original JCM dataset or with data obtained by AugGPT. Moreover, RoBERTa showed better performance than ChatGPT one-shot evaluation. In Japan-specific sentences, the effect of data expansion was significant, with RoBERTa trained on eJCM achieving an F1 score 7.5 points higher than RoBERTa trained on JCM. Its F1 score approached that of GPT-4 Turbo, which has been trained on a much larger dataset."}, {"title": "5 DISCUSSION", "content": ""}, {"title": "5.1 Effects of eJCM", "content": "There are two possible reasons for the fact that the results ob-tained by MTLE were comparable to or better than those obtained by AugGPT, a state-of-the-art sentence extension method using ChatGPT. Firstly, MTLE did not simply paraphrase using an LLM but instead inserted various words and phrases into important parts of sentences to be morally judged. Secondly, MTLE uses the \"indistinguishable\" label to filter out ambiguous sentences, focus-ing on those where ChatGPT shows high confidence, leading to more accurate labeling. MTLE particularly improves performance with Japan-specific sentences as shown in Table 2 by incorporat-ing Japanese norms, demonstrating effective data augmentation through novel instances."}, {"title": "5.2 LLM Bias Due to Language and Cultural Differences", "content": "ROBERTa trained on only 31K sentences showed performance com-parable to that of GPT-4 Turbo in Japan-specific sentences. The difference in F1 scores between the two models was 7.7 points for the entire test dataset and only 3.1 points for the Japan-specific dataset. ROBERTa's F1 score was 8.9 points higher than that of ChatGPT.\nTable 5 shows examples of sentences that were incorrectly esti-mated by GPT-4 Turbo and correctly estimated by RoBERTa trained on eJCM. The first example sentence reflects the unique Japanese custom of taking back the sand in front of the bench when los-ing a Japanese high school baseball game. To correctly judge this sentence, an understanding of Japan's unique culture is necessary.\nGPT-4 Turbo was trained on a large dataset, primarily in Eng-lish, using RLHF [13]. This may introduce a bias from the data and the values of human annotators, making it challenging to han-dle culture-specific topics [14, 16]. In contrast, the eJCM dataset expands JCM with sentences unique to Japanese culture through MTLE. Therefore, models trained on eJCM can judge cases requir-ing deeper moral understanding specific to Japan, a task difficult for GPT-4 Turbo. From the above, to reduce LLM bias, it is important to construct datasets specific to various countries and languages and to conduct additional training, especially for tasks specific to a certain culture or language."}, {"title": "6 CONCLUSION", "content": "In this study, we constructed and published eJCM, a dataset that extends and complements the original JCM, using the MTLE data augmentation method. The models (BERT and RoBERTa) trained on eJCM achieved better performance than the models trained on the original JCM and its AugGPT-obtained extension. Moreover, RoBERTa trained on eJCM outperformed ChatGPT. Furthermore, in sentences requiring an understanding of Japanese culture, the performance of eJCM-trained RoBERTa came close to that of GPT-4 Turbo, suggesting that it is important to construct datasets spe-cific to each culture and language, especially for tasks that require different interpretations depending on the culture and language."}]}