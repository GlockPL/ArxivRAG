{"title": "BIS: NL2SQL Service Evaluation Benchmark for Business Intelligence Scenarios", "authors": ["Bora Caglayan", "Mingxue Wang", "John D. Kelleher", "Shen Fei", "Gui Tong", "Jiandong Ding", "Puchao Zhang"], "abstract": "NL2SQL (Natural Language to Structured Query Language) transformation has seen wide adoption in Business Intelligence (BI) applications in recent years. However, existing NL2SQL benchmarks are not suitable for production BI scenarios, as they are not designed for common business intelligence questions. To address this gap, we have developed a new benchmark focused on typical NL questions in industrial BI scenarios. We discuss the challenges of constructing a BI -focused benchmark and the shortcomings of existing benchmarks. Additionally, we introduce question categories in our benchmark that reflect common BI inquiries. Lastly, we propose two novel semantic similarity evaluation metrics for assessing NL2SQL capabilities in BI applications and services.", "sections": [{"title": "Introduction", "content": "NL2SQL enables users to ask questions using natural language with no or little knowledge of SQL query composition or database schema details [1], [5], [4]. It provides analysis flexibility for non technical business analysts and has started to be adapted as a feature for BI applications. Over the years, multiple benchmarks have been proposed to evaluate NL2SQL models [12], [13]. However, the question types, data schemas and sample database contents of these benchmarks are not designed for common BI scenarios. For example, the WikiSql [13] benchmark mostly contains factual questions over Wikipedia and each query is executed on a single table. In addition, widely used performance measures in NL2SQL evaluate only the match rate of queries as either a perfect match or a no match. These performance measures overly penalize partial matches that may still contain valuable information for the BI analysts.\nNL2SQL can be considered as a use case for no-code software development. No-code is an approach in software development aimed at removing the need for manual coding"}, {"title": "NL2SQL Benchmarks and Their Challenges for BI Applications", "content": "Over the years many NL2SQL benchmarks have been proposed. A non-comprehensive list of open NL2SQL benchmarks is provided in Table 1. We refer readers to a recent survey by Qin et al. for a more comprehensive overview of the NL2SQL literature [9]. Earlier benchmarks such as WikiSql contains queries on a single table [13]. Recent benchmarks such as Spider contains complex queries that operate on databases with relational structures [8] [12]. In general, current benchmarks enable the evaluation of NL2SQL models in terms of exact and execution accuracy. However, some common NL2SQL challenges regarding BI scenarios are not addressed or focused by the current benchmarks. We discuss these challenges in four groups below:\nChallenges regarding database schemas\nBI databases may contain various irregularities in their schema definitions. One irregularity is caused by identical data having different column names on different tables. Such inconsistencies may either be caused by a lack of common naming guidelines across the organization or different naming conventions by different teams. For example, task and task_id may contain the same information in two different tables. This is a challenge for NL2SQL tasks, especially tasks that require joining"}, {"title": "BIS Benchmark Question design", "content": "We analyzed common business intelligence questions in our organization by checking the historical patterns and categorized the questions based on their intent and com-plexity of their relations with the databases. The nine most common query categories identified are defined as follows:\nFilter Queries\nThe user can choose to constrain the set of data in the analysis, usually based on a specific value they have chosen. Such filters might also optionally contain temporal constraints. These type of queries have a basic filter structure and the key challenge for these queries is inferring the filter conditions accurately. The queries might also have implicit temporal constraints depending on the intent of the user. The other points of complexity is the data type of the filters and usage of multiple filters combined with boolean operators. Some data types might be hard to infer. For example in some countries, postal code is an integer while in some others postal codes are strings.\nExample 1: What are the sales in Dublin?\nExample 2: What are the sales in Dublin, London and Paris?\nTemplate: SELECT <columns> FROM <table> WHERE <conditions> AND <time_constraints>\nAggregation and Group by Queries\nThe user can get a calculated summary of the data they have asked for, such as a sum, average, and count, which can be grouped by another value. These type of queries power the key indicator panels in BI dashboards. The main complexity of these types of queries is inference of aggregation function and group by columns. Aggregation functions may be custom functions in different BI scenarios. Therefore mapping to these custom aggregation functions might be challenging. In addition, if the group by columns are inferred incorrectly, the query might not execute correctly. Lastly, aggregations may also be nested in certain cases and the ordering of the aggregation functions in the nested representation may change the result.\nExample 1: Show me the average age of the ad user per city\nExample 2: Get the average age of the ad user per country and city\nTemplate: SELECT aggregator( <column> ) FROM <table> WHERE <conditions> AND <time_constraints> GROUP BY <group_cols>\nTop/Bottom Selection Queries\nThe user can see the top or bottom X number of values for a metric, or rank a value in a metric. These type of queries can be associated with basic reports in BI dashboards. The associated query of these questions can be complex and inference of multiple variables may create accuracy issues. Users might also want to see rankings change based on different filter selections. The system needs to handle this interactivity.\nExample 1: What were the top 10 selling brands last year?\nExample 2: What is the lowest rated products?\nTemplate: SELECT aggregator( <column>), <column> FROM <table> WHERE <conditions> AND <time_constraints> ORDER BY aggregator (<column>) LIMIT <X>\nTime period Queries\nThe user can ask for metrics from the most recent time period. These queries can generate either a single aggregation of the metric or a metric grouped based on intervals. There are many formal or informal ways of specifying time constraints in natural language ranging from formal (isodates, Unix epoch) to very informal (recently, soon). Moreover, the time constraint may indicate a range of time thresholds or a single upper/lower threshold. Temporal constraints may also be explicitly or implicitly stated using natural language by the user. For example when a user asks about the sales, the intent might include sales whole data, last month or last year. NL2SQL should infer such hidden intents of the user to provide useful query output. For these reasons, time constraint inference is quite challenging for NL2SQL models and these types of queries are not provided by current benchmarks to the best of our knowledge.\nExample 1: Sales in last 2 weeks.\nExample 2: (formal temporal constraint) What are the sales in Dublin last month?\nExample 3: (informal temporal constraint) What are the sales in Dublin on 2024-07-01?\nTemplate: SELECT aggregator( <column>), <column> FROM <table> WHERE <conditions> AND <time_constraints>\nComparison Queries\nThese queries are used to compare a metric between two groups. Usually the com-parison is done between two entities and the resulting query is a combination of two queries. A with SQL expression can be used to generate two inner queries and an outer query can join these two findings. A time constraint can also be added. Some models may do the operation in two stages initially extracting the data to compared in the first stage and merging the two queries in the second stage.\nExample: Compare sales of chocolate versus ice cream in 2022.\nTemplate: WITH (query 1) as t1, WITH (query 2) as t2 SELECT <columns> FROM t1, t2 WHERE <joins>\nTrend Queries\nThese queries check the trend of a KPI or metric for a specified time period in set periods such as hours, days and weeks. Note that in real queries these limits or periods might be incomplete in certain cases and the model might have to infer these values intuitively. In addition converting timestamps to time intervals can be different for different databases. Trend is usually generated to build some histogram chart representation. In such visual representations, time chunk aggregations and smoothing might also be challenging for the NL2SQL models in business applications since the parameters used for this functionality might change the trend function.\nExample: Show me weekly revenue for Dublin in the last 3 months.\nTemplate: SELECT time_aggregator( <column>), aggregator ( <column> ) FROM <table> WHERE <time_constraints> ORDER BY time_aggregator (<column>)\nTrend Comparison Queries\nThis category of questions tests the model's ability to generate queries with trend comparison across two time periods. Similar to comparison queries, the easiest way to query this category is usually by using the WITH clause. Trend comparisons are essential in various contexts, such as business analytics, where understanding changes over time can inform strategic decisions. The WITH clause helps in structuring these queries by allowing the creation of temporary result sets, which can then be joined or compared in the final query.\nExample: Compare weekly revenue between this month and last month.\nTemplate: WITH (query 1) as t1, WITH (query 2) as t2 SELECT <columns> FROM t1, t2 WHERE <joins>\nMultiple Tables Queries\nThis category of questions can only be answered after joining multiple tables. In SQL, there are different types of join operations. A model can infer the type of join operation based on domain knowledge or fallback to a default join type such as \"outer join\", to avoid missing data. Join operations can get especially tricky if multiple tables contain similar data such as different tables for predicted and actual metric records. In addition, join operation is risky since an incorrect join might pull cross product of rows from multiple tables.\nExample 1: The revenue of the city with highest population in Germany.\nSample template: SELECT <columns> FROM <tables> WHERE <joins>\nPercentage Queries\nThis category of questions are related to the percentage of some key metric in business. These query outputs are frequently used to generate charts to generate summaries for the key business metrics. Models may either do the percentage calculation of a metric with a single SQL or evaluate the percentage of the metric in two stages.\nExample: Sale shares per category of products.\nSample template: SELECT percentage(<column>) FROM <table> GROUP BY <columns>"}, {"title": "BIS Benchmark Setup", "content": "We designed our benchmark to overcome the challenges encountered by currently available NL2SQL benchmarks and support common BI question types through analyzing common questions of our organization's BI application users. We used a database schema frequently used for business intelligence scenarios to provide a more realistic benchmark with time based observations, value mapping and redundant definition of data (such as predicted and actual metric log values). As discussed earlier, lack of temporal test data was a major limitation of earlier benchmarks making them insufficient to test many common BI questions. The sample database also contains some technical terms and abbreviations with associated mapping table. Questions in a non-Latin language highlight the challenges related to mixed use of the language within the context. Finally, we developed two evaluation metrics to overcome evaluation issues of the currently available metrics (described in detail in Section 5).\nThe proposed benchmark contains two databases. The first database contains real and predicted metrics of advertisement campaigns in 5 tables. The second database contains system operational data in 3 tables. We define the questions in 9 categories as described in Table 2. We provide sample SQLite databases for the tables with sample business data. This set of tables are accessed multiple ways by the business analysts and provide a test bed for a set of business operations challenges in NL2SQL as shown in Section 2. The benchmark contains a mock database to help with evaluation of results for a sample NL2SQL model as well as true SQL results as curated by the analysts. In total, there are 239 questions to test an"}, {"title": "Evaluation Metric Design", "content": "Following a review of the issues of existing performance measures of NL2SQL benchmarks as discussed in Section 2, we defined two evaluation metrics namely SQL statement semantic similarity and SQL result partial similarity to assess the partial and structural similarity between predicted and ground truth SQL queries more effectively.\n5.1 SQL Statement Semantic Similarity\nSQL statement semantic similarity is computed by comparing the AST (abstract syntax tree) of two queries after transpilation to the same target database using"}, {"title": "SQL Result Partial Similarity", "content": "SQL result similarity is based on the predicted and ground truth query result on a sample database. For example if the query is the \"What is top 3 revenue streams?\", comparison of two results having ranks 1, 2 and 3 in one column explicitly or not would not change the information content of the result completely for the user. Two outputs for such query can be seen in Figure 3. In addition, one SQL result may re-label the columns and the other SQL result may keep the table column names. Columns of predicted and actual query results may have different labels and as long as the two column values exactly match we estimate there is a column match. No partial column match is possible since partial comparison of the value tuples of two result tuples would be problematic so there is either a perfect match between two columns or no match. If there are multiple columns in both predicted and actual query results, all the combinations are checked exhaustively to generate the highest possible similarity score. If one query produces M columns and the other produces N columns, we perform MXN comparisons by examining all possible pairs. This approach would cause issues if large results are compared with many columns and rows (>1K rows >100 columns) but for our sample database both column count and row is low for the BI queries. On a Intel 11370H laptop, all the result similarity scores can be computed under 1.5 minutes for evaluation of a model on the whole benchmark (< 1 second per comparison and not taking into account the model inference time). Finally, column matches are aggregated to generate precision, recall and F1 score per instance. The measures for SQL result similarity can be defined as follows per instance: Let A be the set of columns of the result for the predicted query, and B the set of columns of the result for ground truth query. Precision is given by;\n$P=Precision_{result}=|A \\cap B|/|A|$\nRecall for an instance is given by,\n$R=Recall_{result}=|(A \\cup B)\\A|/|B|$\nThe harmonic mean of precision and recall gives the F1 score:\n$F1_{result}=2/(1/P+1/R)$\n$=2/(1/(|A \\cap B|/|A|)+1/(|(A \\cup B)\\A|/|B|))$\nFinally, these measures are aggregated by taking the arithmetic mean across all the instances to generate a summary score. Aggregation can also be done per query category to debug or compare models per different BI question categories. These measures find partial similarity between predicted and true SQL query results with partially overlapping columns in the results instead of an exact match check strategy used by existing benchmarks."}, {"title": "Ethics and Data Protection", "content": "The data sources were carefully reviewed and structured to exclude any form of sensitive information, such as personal data. Furthermore, for security purposes, all company business-related terminology and values in the provided sample SQLite database were obfuscated."}, {"title": "Limitations", "content": "SQL result similarity measures have potential problems for complex cases:\nThe first potential weakness is related to the test database. The calculation of result similarity requires executing the predicted and actual query on a test database, and any problems with data in the test database could create issues. For example, if there were no sales yesterday, aggregations such as SELECT count(price) FROM sales and SELECT min(price) FROM sales would have the same results. The test database should be verified to avoid such issues.\nThe second potential problem is related to column matching. Since we are checking exact match per column, if one column matches partially, the match for that column would be 0. This might penalize models with incorrect filter clauses too much, since just changing one WHERE clause would reduce the result similarity to 0.\nThe third potential weakness is related to computational complexity. If the test database is too large, calculating the result accuracy might require significant resources. The test database should be as small as possible to avoid this weakness.\nTo test the multilingual performance, we provide natural language questions in both Chinese and English as these are the most common languages in our organization. Most of the questions have both English and Chinese versions to highlight language specific issues in models. The benchmark currently does not cover other languages. One of the critical aspects of BI queries is the time constraints. We incorporated the time constraints for most of the questions but mapping the natural language repre-sentations of time constraints exhaustively in natural language with SQL datetime was not done within the scope of the work. We hope to overcome these weaknesses in the next version of our benchmark."}, {"title": "Conclusion", "content": "In this paper, we presented a novel BI-focused benchmark and two evaluation metrics for the estimation of NL2SQL performance in realistic scenarios. We went through the challenges of the existing benchmarks for BI domain and attempted to address these challenges by building a BI-specific benchmark and two novel evaluation metrics. We believe business specific benchmarks provide a good complement to the large scale generic open benchmarks currently available to evaluate models in a given domain. As a future work, we plan to extend the number of questions in each category and the number of business domains in the next version of BIS to increase the coverage of the benchmark."}]}