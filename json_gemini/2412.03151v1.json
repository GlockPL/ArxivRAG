{"title": "Large Language Models show both individual and collective creativity comparable to humans", "authors": ["Luning Sun", "Yuzhuo Yuan", "Yuan Yao", "Yanyan Li", "Hao Zhang", "Xing Xie", "Xiting Wang", "Fang Luo", "David Stillwell"], "abstract": "Artificial intelligence has, so far, largely automated routine tasks, but what does it mean for the future of work if Large Language Models (LLMs) show creativity comparable to humans? To measure the creativity of LLMs holistically, the current study uses 13 creative tasks spanning three domains. We benchmark the LLMs against individual humans, and also take a novel approach by comparing them to the collective creativity of groups of humans. We find that the best LLMs (Claude and GPT-4) rank in the 52nd percentile against humans, and overall LLMs excel in divergent thinking and problem solving but lag in creative writing. When questioned 10 times, an LLM's collective creativity is equivalent to 8-10 humans. When more responses are requested, two additional LLM responses equal one extra human. Ultimately, LLMs, when optimally applied, may compete with a small group of humans in the future of work.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs), such as Generative Pre-trained Transformers (GPTs), are increasingly being piloted in the workplace. It is estimated that 80% of the U.S. workforce belong to an occupation where at least 10% of its tasks could be affected by the introduction of LLMs\u00b9. According to a World Economic Forum white paper\u00b2, routine and repetitive tasks have the highest potential for automation by LLMs. However, remarkable capabilities of LLMs are being reported across a range of domains and tasks\u00b3, and so the belief that only routine tasks can be performed by LLMs is being overturned\u00b9. It is speculated that the most affected occupations are in the content creation industry, which requires a high level of creativity, a skill that used to be considered unique to humans. Therefore, to anticipate and plan for the future of work we need to answer the question of how creative LLMs are.\nCreativity is a multi-facet construct and generally defined as the generation of products or ideas that are both novel and useful7,8. Guilford and Vaughan identified four creativity dimensions: fluency, flexibility, originality, and elaboration, which were subsequently measured in the Torrance Tests of Creative Thinking, one of the most used creativity tests10. It is also widely accepted that creativity performance may vary depending on the specific domain11,12. Therefore, the PISA 2022 Creative Thinking assessment (https://www.oecd.org/pisa/innovation/creative-thinking/) includes four different domains: written expression, visual expression, social problem solving, and scientific problem solving.\nThere have been several attempts at comparing LLMs with humans on creative tasks. As summarised in Table 113-33, the findings are not always consistent. While LLMs achieve comparable or stronger performance on average than humans on divergent thinking tasks, creative writing tasks seem to be more challenging for LLMs.\nOne may argue that average creativity performance is not important because usually only one idea or creative output (or a few) will be put into practice. In terms of the best performance, mixed results are reported as well."}, {"title": "Results", "content": "These existing studies suffer from one major limitation, as they are mostly based on one task from a single domain, which is not sufficient to shed light on the overall picture of LLMs' creativity. This study takes a multi-facet approach to the measurement of LLMs' creativity. Specifically, we designed multiple tasks tapping into different dimensions of creativity (e.g., novelty, usefulness, originality, flexibility, and diversity) in three distinctive domains, namely divergent thinking, problem solving, and creative writing. To our knowledge there is no existing dataset about any of these tasks available online that could have been included in the LLMs' training set, reducing concerns about data contamination34-36. We compared the responses collected from several LLMs with those from a large, diverse group of human participants, who completed the tasks as part of a high-stakes Master's degree admission assessment. All responses were rated by humans following the Consensual Assessment Technique37.\nAnother limitation of existing studies is that all of them focus on individual responses of LLMs. Considering that LLM users frequently request multiple responses for one problem, in the second part of this study we turn to the concept of collective creativity, which refers to the novelty and usefulness of ideas developed by a group of individuals 38,39. To operationalise the measurement of collective creativity, we requested 10 responses from each LLM and examined the contribution of top ideas when these LLM responses were combined with those by a group of humans. By varying the number of humans in the group, we were able to quantitatively assess LLMs' collective creativity and understand how many humans the LLMs equalled. Based on results of both individual and collective creativity, we discuss the potential of LLMs as capable assistants for creative tasks in the future of work.\nWe used 13 creative tasks spanning over three distinctive domains, including divergent thinking, problem solving, and creative writing. A total of 467 human participants from China (330 female, 137 male; age ranging from 21 to 53 years, with a mean of 27.56 and SD of 6.03; all reported to have at least a Bachelor's degree) completed the tasks as part of an admission assessment for a Master's degree. The same tasks were administered to GPT-3.5 and GPT-4 (https://openai.com) in April and May, 2023, respectively, via OpenAI's API, where responses were collected under five different temperature settings. To probe potential differences in the LLMs, we further recorded responses from Claude (https://claude.ai) as well as two LLMs developed in China, namely Qwen (http://tongyi.aliyun.com) and SparkDesk (https://xinghuo.xfyun.cn) using the relevant web portal in June, 2023. We chose two LLMs developed in China to ensure that our evaluation of LLMs' performance of creative tasks was not biased by the language, since the tasks were administered in Chinese.\nResponses of both humans and LLMs were rated by five trained judges independently, who were not aware of which responses were written by whom. Inter- rater reliability was acceptable-to-good (with above .7 Fleiss' Kappa or Intraclass Correlations). Detailed information about the data collection and rating procedure can be found in the Materials and Methods section and Table S1. Fig. 1 and Figs. S1-S3 show the frequencies of the creativity ratings for each task."}, {"title": "Part 1: Individual Creativity", "content": "To understand how creative the LLMs are in comparison to humans, we rank all the human participants on their performance in each task, and then benchmark the LLMs against humans in terms of their percentiles in the humans' distributions. As shown in Fig. 2, across all indicators of the 13 tasks, the five LLMs ranked on average in the 46th percentile against the human participants. Claude and GPT-4 ranked in the 52nd percentile, SparkDesk in the 44th percentile, Qwen in the 42nd percentile, and GPT- 3.5 in the 37th percentile. Domain-wise results suggest that the LLMs performed well in the divergent thinking and problem solving tasks, as on average they ranked in the 55th and 59th percentile. None of the LLMs reached 50th percentile in the domain of creative writing (in the 25th percentile on average), which is consistent with findings reported in previous studies20."}, {"title": "Divergent thinking", "content": "Unlike existing studies that mainly make use of Alternate Uses Tasks, we designed two scenario-based divergent thinking tasks, where participants were asked to generate as many ideas as possible within a limited period of time. On average each human participant generated 3.68 valid ideas. In contrast, LLMs generated a mean of 8.85 valid ideas in each response, ranging from 5.43 in SparkDesk to 12.47 in GPT-4.\nAll ideas were rated on two dimensions: novelty and usefulness7,8. In both tasks, all LLMs showed comparable or higher novelty and usefulness than humans (except SparkDesk on Novelty in Task 1 and Qwen on Usefulness in Task 2; see details in Table S2). In both LLMs and humans, ideas appearing later in each response tend to be rated as more novel but less useful (see Figs. S4-S8). This is also confirmed by the paired sample t-tests, which suggest that ideas in the second half of each response show significantly higher ratings of novelty but lower ratings of usefulness (see details in Table S3). Notably, a significant, negative correlation was observed (see Fig. 3) in the"}, {"title": "Problem solving", "content": "In this domain, three scientific problem solving tasks and three social problem solving tasks were included, following examples of the PISA 2022 Creative Thinking assessment. Participants were asked to come up with either one or three ideas to solve a real-life problem within limited time. One of the tasks measured the creativity dimension of flexibility, whereas the other five focused on the dimension of originality. Notably, Qwen refused to answer Scientific Problem Solving Task 2, likely due to the mention of \u201ctheft\u201d in the question.\nIndependent sample t-tests between LLMs and humans show mixed results (see Table S4). In scientific problem solving, GPT-3.5 underperformed humans on Tasks 1 and 3, whereas GPT-4 exhibited better performance than humans on Task 1 and worse performance on Task 3. Claude, Qwen, and SparkDesk showed comparable or better performance than humans across all three tasks. In social problem solving, GPT-4 outperformed humans on all three tasks. GPT-3.5 showed better performance on Task 2 but worse performance on Task 3. SparkDesk showed exactly the opposite pattern. Both Claude and Qwen failed to outperform humans on all tasks. In a nutshell, no clear pattern is observed: depending on the specific task and the model, LLMs may outperform, underperform, or show comparable performance to humans."}, {"title": "Creative writing", "content": "We introduced five creative writing tasks using different forms of writing prompts. Specifically, participants were asked to create an advert, write two different stories according to a set of keywords and a set of emojis, and write a full story and a continuation of an existing story based on several emojis provided, taking the example of the PISA 2022 Creative Thinking assessment. The judges provided ratings on their creativity and additionally on diversity in the two tasks where two different stories were required.\nThe comparison between LLMs and humans (see Table S4) suggests that humans outperformed all LLMs in the Creative Advert Writing Task. Humans' slogans were more emotionally touching and memorable whereas those by LLMs tended to be generic statements. In the Keyword-prompted Writing Task, compared to humans, GPT-3.5 showed lower creativity, GPT-4 higher creativity, whereas all other models had comparable creativity. In the Emoji-prompted Writing Tasks, all LLMs failed to outperform humans. Notably, Qwen and SparkDesk were not able to complete the story continuation task.\nIn terms of the pairwise diversity in both tasks requiring two stories, judges rated significantly higher diversity for stories generated by humans than those generated by the LLMs except for those generated by Claude in the Keyword-prompted Writing Task."}, {"title": "Diversity within and between responses", "content": "To replicate findings of recent studies42, we further investigated if the responses generated the LLMs are less diverse than those by humans. In the six tasks where more than one idea (or story in the creative writing tasks) is required, we calculated the text"}, {"title": "Effect of temperature", "content": "Temperature is a parameter that controls how random or diverse the output of an LLM is, which might be expected to be related to creativity. As shown in Fig. 4 (and see Tables S7 and S8), the effect of temperature on creativity performance was not consistent. The optimal temperature varied among the tasks even within the same domain. On average, there seems to be an upward trend for GPT-3.5, with temperature 1 generating more creative responses than temperature 0, whereas for GPT-4 no clear pattern could be identified. Therefore, while it might be possible to improve the performance of LLMs in creative tasks by changing the temperature, there is no simple rule for what the optimal temperature should be across creative tasks. Unless specified, all results for GPT-3.5 and GPT-4 presented in this paper are aggregated across responses at different temperatures.\nNonetheless, temperature appears to have an impact on the diversity of the responses by the LLMs. In all of the six tasks that required more than one idea (or"}, {"title": "Part 2: Collective Creativity", "content": "In Part 1 we looked at the creativity of each LLM response on average. In reality there is no reason why one cannot repeatedly ask for multiple responses from an LLM to the same task. We extend our analysis by looking at the collective creativity of a set of LLM responses. Our approach to LLMs' collective creativity is to examine how many of the top ideas are contributed by LLMs relative to humans when their responses are pooled together. This is particularly valuable in real-world applications, because no matter how many reasonably good ideas are presented for a particular task, only the most creative ones will be implemented. When an equal number of top ideas comes from the LLM responses and humans, we take the number of humans as the indicator of the collective creativity of the LLMs. In other words, these LLMs can replace this number of humans in collectively generating creative ideas.\nSpecifically, we analyse the 10 most highly rated responses for each task and identify whether humans or LLMs generated these responses. If there is a tie, we include all responses with the same rating, hence the number of top responses can be higher than 10. Social Problem Solving Task 1 and Emoji-prompted Creative Writing Task 1 are not included here as they measure flexibility and diversity, respectively, which focus on the disparity within each response rather than creativity in the ideas."}, {"title": "Top 10 responses among all responses", "content": "We first pool together all responses by the LLMs and human participants, and examine the top 10 responses for each task. As shown in Fig. 5 (and Table S11), the top responses for the divergent thinking tasks mostly came from human participants. In the domain of problem solving, the five LLMs jointly contributed around 33% - 64% of the top responses in the social problem solving tasks and around 40% - 57% of the top responses in the scientific problem solving tasks. While 79% of the most creative keyword-prompted stories were written by LLMs (primarily GPT-4), stories generated by humans dominated the other creative writing tasks. Averaged across all tasks, the LLMs contributed approximately a third (33%) of the top 10 responses."}, {"title": "When one LLM is asked 10 times", "content": "The above results indicate that humans are still collectively more likely to come up with the best responses on creative tasks. However, our sample included 467 humans whereas most creative brainstorming sessions in the real world include fewer than 1045. We therefore analyse our results further to see what size human group would be necessary to equal one LLM when it is asked 10 times. First, we split the responses of each LLM into five groups of 10 responses. Then we randomly select N human participants (N is an integer starting from 1) and combine their responses with one group of 10 LLM responses. To ensure robustness we adopt a bootstrapping approach, where we repeat the random selection 1000 times and take the average result. If the LLM and the humans contribute the same number of responses in the top 10 responses (i.e., 50% each), we would take the current N value and average them across the five groups as an indicator of the LLM's collective creativity.\nThe results for each LLM averaged across all creative tasks are presented in Fig. 6 (see detailed results for each task in Table S12). GPT-4 and Claude appear to be the most creative LLMs for top responses, as their collective creativity is equivalent to a group of 10 humans. GPT-3.5 shows the poorest collective creativity but is still equivalent to 8 humans. Qwen and SparkDesk, both equivalent to 9 humans, lie in the middle."}, {"title": "When more responses are requested from LLMs", "content": "We continue to probe LLMs' collective creativity when more responses are requested. Specifically, we mix the responses of all LLMs and randomly select a certain number of responses, ranging from 10 responses to 50 responses with an interval of five. Following the method described above, we find out what size human group this number of LLM responses is equivalent to. We repeat this procedure 100 times and take the average of human group sizes as the collective creativity associated with this number of LLM responses (see Table S13 for details). As shown in Fig. 7, a linear relationship is observed. In terms of the incremental improvement, roughly two additional LLM responses (1/0.52) equal one extra human in the group."}, {"title": "Discussion", "content": "The Organisation for Economic Co-operation and Development suggests that creativity is one of the key 21st Century skills that young people must possess to become effective workers in the knowledge society46. With LLMs available to the public, everyone can harness the power of LLMs to generate creative ideas. To understand LLMs' creativity, this study took a multi-facet approach and measured LLMs\u2019 individual and collective creativity in multiple dimensions across different domains. Depending on the task and the model, differential performances were reported. On average, the LLMs ranked in the 46th percentile against humans, exhibiting relatively strong performance on divergent thinking and problem solving tasks but suffering from weaknesses in creative writing. Claude and GPT-4 ranked the highest in the 52nd percentile and GPT-3.5 ranked the lowest in the 38th percentile. In most tasks, at least one LLM ranked above the 50th percentile against humans, suggesting that LLMs can be as creative as humans if not more so. Consistent with previous literature, our results also suggested that the responses generated by LLMs lack diversity in comparison to humans.\nConsidering that it is common to request multiple responses from LLMs while only the best ideas are put into practice, we further examined the collective creativity of the LLMs. When one LLM was asked 10 times, our findings suggest that one LLM could be considered equivalent on average to a group of 8 to 10 humans. When more responses were requested, in terms of incremental improvement, roughly two additional LLM responses equalled one extra human. Taken together, it is reasonable to expect an LLM to generate ideas as creative as a small group of humans when LLM responses are repeatedly collected. This suggests that LLMs, when optimally applied, may greatly contribute to the collective creativity of many organisations, where a typical brainstorming session would not involve more than 10 employees. This is especially beneficial, considering the cost effectiveness of LLMs. However, in a creative endeavour pooling the efforts of thousands of humans, such as the pursuit of scientific knowledge, it is still unlikely that the ideas generated by LLMs would be considered the best47. Indeed, we found that when pooling all LLM responses (up to 50 responses from each of the five LLMs) and all human responses (ranging from 200 to 467 responses per task), around two thirds of the top responses were contributed by humans.\nImportantly, LLMs are very efficient at generating ideas and can serve as effective assistants, but currently it is still down to humans to evaluate the novelty and usefulness of the ideas generated and select the most creative ones. While initial attempts have reported evidence on the productivity effects of humans working with LLMs42,48,49, further research is warranted to explore how much incremental creativity the LLMs would bring to AI-human teaming and more importantly what the best strategy for AI-human collaboration is.\nThis study did not involve extensive prompt engineering, which could potentially improve the performance of LLMs22,50,51. We intentionally kept the prompts identical"}, {"title": "", "content": "to the instructions human participants received, as we were simulating a real-world scenario where a non-AI expert interacts with an LLM, expecting to receive high- quality responses without spending a long time fine-tuning the prompt. Moreover, it is anticipated that future LLMs will become better at understanding users' intentions, reducing the need for prompt engineering52,53.\nAnother limitation of our study is the operationalisation of the measurement of LLMs' collective creativity. In our procedure of human data collection, participants completed the tasks independently, which represents a nominal group setting. For the sake of comparability, we adopted a similar nominal group setting for the LLMs and did not simulate an interactive group setting. While there is abundant evidence that nominal groups outperform interactive groups in both the quantity and the quality of the ideas generated in brainstorming sessions54-56, some literature on organisational creativity suggests that collective creativity is nurtured by interactions between the group members39. Future research should explore the collective creativity of LLMs within an interactive group setting (e.g., a multi-agent system) in comparison to human collaborative efforts.\nAlthough our human sample is relatively large and diverse in comparison to those reported in previous studies that compare LLMs with humans (mostly students or samples collected from Prolific), it is not representative of the human population. In order to find out how generalisable our results are, we tested the demographic differences in humans' performance. The results (see details in Supplementary Text) suggest that in general there is very little demographic difference in humans' creativity performance. This concurs with the fact that most creativity tests, such as the Torrance Tests of Creative Thinking, do not provide adult norms separated according to demographics. Therefore, we are cautiously confident that our findings based on a non- representative but large and diverse human sample can be generalised to a wider population.\nLLMs are being regularly updated and, as revealed here, GPT-4 outperformed GPT-3.5 on most tasks. So we expect that future LLMs (including those published after our data collection) would exhibit higher creativity performance and our study will not be the last word on the creative performance of LLMs. However, this does not diminish the contribution of our study to the ongoing discussion about the impact of AI on the future of work, as it achieves an important milestone towards a better understanding of current LLMs\u2019creativity across a range of domains and tasks, offering practical guidance on the application of current LLMs in the workplace.\nWith public accessibility of LLMs, we foresee radical changes to the future of work. Our findings of LLMs' performance on creative tasks provide evidence for their high-level cognitive capabilities, challenging the notion that only occupations intense in routine tasks would be exposed to AI automation. If LLMs and other Al systems empowered by continued advancement in AI technology deliver on their promised capabilities, they could potentially automate at least parts of the jobs of hundreds of millions of human workers57. It becomes ultimately important for us to understand what the role is for humans in the future of work."}, {"title": "Materials and Methods", "content": "This study aims to holistically measure LLMs' individual and collective creativity. We use 13 creative tasks spanning over three distinctive domains, including divergent thinking, problem solving, and creative writing. All the tasks and the corresponding instructions are available from https://doi.org/10.6084/m9.figshare.24878421.\n\u2022\tDivergent Thinking Task 1/2 (time limit: 5 minutes)\nParticipants are presented with a hypothetical scenario, where they are trapped on a desert island alone after encountering a storm on a sailing trip. To keep them warm (Task 1) or pick fruit from a tall tree (Task 2), they are asked to give as many different ideas as possible. Additional instruction is given to encourage original, unique ideas.\nEach response is split into ideas, which are rated separately on two dimensions: novelty and usefulness. Novelty is defined as the extent to which the idea is uncommon and unique, whereas usefulness reflects how practical and effective the idea is at addressing the current problem.\n\u2022\tSocial Problem Solving Tasks 1, 2, and 3 (time limit: 7 minutes each)\nIn Task 1, participants are asked to describe three different ideas of what people can do to save water. In Task 2, participants are asked to think of an original idea to promote a smartphone application that is designed to reward users for actions they take to save water. In Task 3, participants are asked to think of an improvement to the application that keeps people using it for longer time. Additional instruction is given to encourage original, unique ideas.\nTask 1 measures the dimension of flexibility, which examines if the ideas are sufficiently different from each other. It is operationalised as the number of categories the three ideas in each response fall into. Tasks 2 and 3 measure the dimension of originality, which is defined as the extent to which the response is uncommon and unique.\n\u2022\tScientific Problem Solving Tasks 1, 2, and 3 (time limit: 7 minutes each)\nIn Task 1, participants are asked to imagine a \u2018bicycle of the future' and think of three original improvements to an ordinary bicycle. In Task 2, participants are presented with an idea, which proposes that a camera with facial recognition software could be installed to prevent bicycle theft, and asked to suggest an improvement to make it more effective. In Task 3, participants are asked to suggest an original way to reuse or repurpose a bicycle pedal. Additional instruction is given to encourage original, unique ideas.\nThe responses in all three tasks are rated on the dimension of originality, which is defined as the extent to which the response is uncommon and unique. In Task 1, the three ideas in each response are rated separately and the average rating is used to indicate the originality of the response."}, {"title": "", "content": "\u2022\tCreative Advert Writing Task (time limit: 5 minutes)\nParticipants are asked to come up with an advert (no more than 30 words) for a hypothetical product: a robot that can provide companionship like a nanny to the elderly.\nResponses are rated on their overall creativity that takes into account both novelty (i.e., uniqueness and originality) and usefulness (as an informative advert). Additional ratings are provided on three sub-scales, i.e., conception, expression, and emotion. The average rating across the three sub-scales shows a high correlation with the overall creativity rating (r = 0.886, P < 0.001), hence the overall creativity rating is used in the analysis.\n\u2022\tKeyword-prompted Creative Writing Task (time limit: 10 minutes + 10 minutes)\nParticipants are asked to create an imaginative and creative story (no less than 100 characters) using three keywords (in any order) with no restriction on the genre, theme or style. It is highlighted that the story should be unique, original, distinctive and unexpected. Then participants are asked to create another story (no less than 100 characters) that is different from the previous one using the same set of keywords.\nThe stories are rated on their overall creativity as well as five sub-scales, including originality, imagination, surprisingness, the interpretation of the keywords and the connection of the keywords. A unidimensional factorial model is constructed with the five sub-scales, which shows good model fit (CFI = 0.937) and generates a factor that is highly correlated with the overall creativity rating (r = 0.948, P < 0.001). Hence, the overall rating is used in the analysis. Additionally, the two stories created by each participant are rated on how different they are from each other.\n\u2022\tEmoji-prompted Creative Writing Tasks 1, 2, and 3 (time limit: 7 minutes each)\nIn Task 1, participants are asked to create two different stories (no more than 120 characters each) that connect two emojis. In Task 2, participants are asked to write one creative story (no more than 120 characters) that connects six emojis in the order they appear. It is highlighted that the story should be original, demonstrates a rich imagination and is well structured. In Task 3, participants are presented with a story that is based on six emojis and asked to write a continuation (no more than 120 characters) based on three more emojis.\nSimilar to the Keyword-prompted Creative Writing Task, the two stories created by each participant in Task 1 are rated on how different they are from each other, whereas the stories in Tasks 2 and 3 are rated on their overall creativity as well as five sub-scales, including originality, imagination, surprisingness, the interpretation of the emojis and the connection of the emojis. Unidimensional factorial models are constructed with the five sub-scales for Tasks 2 and 3, respectively, both of which show good model fit (CFIs = 0.948 and 0.974) and generate factors that are highly correlated with the overall creativity ratings (rs = 0.949 and 0.929, Ps <0.001). Hence, the overall ratings are used in the analysis."}, {"title": "Human data collection", "content": "The tasks above were grouped into two test forms. Form A consisted of Divergent Thinking Task 2, Scientific Problem Solving Tasks 1, 2, and 3, Emoji-prompted Creative Writing Tasks 1, 2, and 3, and Creative Advert Writing Task. Form B consisted of Divergent Thinking Task 1, Social Problem Solving Tasks 1, 2, and 3, Keyword-prompted Creative Writing, and Creative Advert Writing Task.\nThe test, embedded in the admission assessment for a Master's programme, was administered as a computer-based exam to 467 human participants from China (330 female, 137 male) simultaneously at a designated testing centre. The programme was in a social science subject which has no formal element of creativity training. The average age was 27.56 years (ranging from 21 to 53, SD = 6.03). When applying for the programme, 103 were current students and 140 were current employees with certain years of work experience. All participants had a Bachelor's degree, and 29 of them had a Master's degree. For their undergraduate studies, 69 studied in arts and humanities subjects, 132 in STEM subjects, 103 in the programme subject, and 163 in other social science subjects. Detailed breakdown can be found in Fig. 8."}, {"title": "LLM data collection", "content": "All 13 creative tasks were administered to five different LLMs, namely GPT-3.5 (GPT-3.5-turbo-0301), GPT-4 (GPT-4-32K-0315-preview), Claude, Qwen and SparkDesk (V1.5).\nResponses were collected from GPT-3.5 using the public OpenAI API in April, 2023 and again in May, 2023. After rating the responses of humans and GPT-3.5 in the first administration, we realised that the comparison might not be fair, as the humans were fully aware of the purpose of the assessment while in certain tasks the LLMs were not explicitly instructed to be creative. Therefore, we applied additional instructions to encourage the LLMs to generate original and unique responses in the second administration, where these additional instructions were introduced as a system prompt in all divergent thinking and problem solving tasks and attached to the end of the Creative Advert Writing Task. We compared the responses of GPT-3.5 with and without these additional instructions, and the results are presented in Table S16, which shows no significant differences in all tasks except the Divergent Thinking Task 1 (Mean). All results presented in Results are based on the responses with the additional instructions.\nWe chose five temperature settings, i.e., 0, 0.25, 0.5, 0.75 and 1. When the responses were collected from GPT3.5 and GPT-4 in this study, the temperature ranged from 0 to 1. At each temperature collected 50 responses in separate sessions.\nResponses from GPT-4 were collected in May, 2023 using the internal OpenAI API that was made available to Microsoft Research Asia. Additional instructions that encouraged original and unique responses were included as a system prompt in the divergent thinking and problem solving tasks and attached to the end of the Creative Advert Writing Task. Similar to GPT-3.5, five temperatures (0, 0.25, 0.5, 0.75 and 1) were tested and each task was administered 30 times in separate sessions at each temperature.\nResponses were collected from Claude, Qwen and SparkDesk in June, 2023 using their web portals, respectively, under the default setting. Each task was administered 50 times in separate conversations. The additional instructions that were applied to GPT-3.5 and GPT-4 were attached to the beginning of each conversation before a creative task was presented."}, {"title": "Rating procedure", "content": "To ensure comparable number of responses across the LLMs and between the LLMs and the humans, we randomly picked 10 responses at each temperature in GPT- 3.5 and in GPT-4 for divergent thinking tasks and creative writing tasks. For the Emoji- prompted Writing Tasks, due to the low rate of valid stories, we examined all responses collected from GPT-3.5 (50 rounds per temperature) and GPT-4 (30 rounds per temperature). If the number of valid stories exceeded 10 for a given temperature, we randomly chose 10 for the rating and subsequent analysis. If not, we kept all stories for the rating and subsequent analysis. It is also noted that in the Creative Advert Writing Task all responses at temperature 0 were identical. For problem solving tasks, we randomly picked 1 response at temperature 0 (as we noticed that responses at"}, {"title": "Statistical Analysis", "content": "The analyses were mostly performed in Python. Specifically, we used the statsmodels package (https://www.statsmodels.org/stable/index.html) to calculate Fleiss' Kappa and the Pingouin package (https://pingouin-stats.org/) to calculate intraclass correlation coefficients and perform one-way analysis of variance, which compared the performance of GPT-3.5 and GPT-4 under different temperatures. We used the SciPy package (https://www.scipy.org/) to run independent sample t-tests (two-sided), which examined the differences between the humans and the LLMs in their performance of the creative tasks and the differences between the two versions of prompts for GPT-3.5, and paired sample t-tests (two-sided), which examined the differences in the novelty and usefulness between the two halves of each response in the divergent thinking tasks. Independent sample t-tests (two-sided) and one-way analysis of variance were carried out to test the demographic differences in the humans' performance. Independent sample t-tests (two-sided) and chi-square tests, also using the SciPy package, were performed to examine potential differences between the human participants assigned to Test A and Test B. Correlations between the novelty and usefulness ratings were calculated with the SciPy package as well. We used the"}, {"title": "", "content": "gaussian_kde function from the SciPy package to estimate the kernel density which was depicted in Fig. 1.\nTo understand the relationship between the sub-scale ratings and the overall creativity ratings in the Keyword-prompted Writing Task and the Emoji-prompted Writing Tasks", "https": "platform.openai.com/docs/guides/embeddings/) to transform the text into vector representations, based on which the cosine similarity was calculated with the Scikit-learn package. Additionally, we used the same package to calculate the Levenshtein distance, which is a string metric for measuring the difference between two sequences of text. A high distance indicates small similarity.\nWe ranked the human participants and benchmarked the LLMs against humans by locating the average performance of the LLMs in the humans' distributions. Focusing on the top creative performance, we examined"}]}