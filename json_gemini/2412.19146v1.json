{"title": "ASKCHART: UNIVERSAL CHART UNDERSTANDING THROUGH TEXTUAL ENHANCEMENT", "authors": ["Xudong Yang", "Yifan Wu", "Yizhang Zhu", "Nan Tang", "Yuyu Luo"], "abstract": "Chart understanding tasks such as ChartQA and Chart-to-Text involve automatically extracting and interpreting key information from charts, enabling users to query or convert visual data into structured formats. State-of-the-art approaches primarily focus on visual cues from chart images, failing to explicitly incorporate rich textual information (e.g., data labels and axis labels) embedded within the charts. This textual information is vital for intuitive human comprehension and interpretation of charts. Moreover, existing models are often large and computationally intensive, limiting their practical applicability. In this paper, we introduce AskChart, a universal model that explicitly integrates both textual and visual cues from charts using a Mixture of Experts (MoE) architecture. AskChart facilitates the learning of enhanced visual-textual representations of charts for effectively handling multiple chart understanding tasks, while maintaining a smaller model size. To capture the synergy between visual and textual modalities, we curate a large-scale dataset named ChartBank with about 7.5M data samples, which helps align textual and visual information and facilitates the extraction of visual entities and text. To effectively train AskChart, we design a three-stage training strategy to align visual and textual modalities for learning robust visual-textual representations and optimizing the learning of the MoE layer. Extensive experiments across five datasets demonstrate the significant performance gains of AskChart in four chart understanding tasks. Remarkably, AskChart with 4.6B parameters outperforms state-of-the-art models with 13B parameters by 68.3% in Open-ended ChartQA and 49.2% in Chart-to-Text tasks, while achieving comparable performance in ChartQA and Chart-to-Table tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Charts are essential tools for data visualization, playing a crucial role in conveying complex data patterns in everyday applications (Luo et al., 2022a; 2023). Chart understanding tasks, including chart question answering (ChartQA) (Hoque et al., 2022), Chart-to-Text (Kantharaj et al., 2022b), and Chart-to-Table translation (Liu et al., 2023), aim to automate the interpretation and extraction of key information from charts, allowing users to query or convert visual data into structured formats.\nWith the advancement of multimodal large language models (MLLMs), recent studies aim to automatically perform various chart understanding tasks (e.g., ChartQA and Chart-to-Text) by pretraining MLLMs on large-scale chart-related corpus (Masry et al., 2023; Han et al., 2023; Meng et al., 2024). For example, ChartAst (Meng et al., 2024) is trained on a large-scale instruction-following chart-related corpus based on Donut (Kim et al., 2022) and SPHINX (Lin et al., 2023) models, and demonstrates strong performance in ChartQA, Chart-to-Text and Chart-to-Table tasks.\nDespite significant advancements, existing specialized MLLMs for chart understanding tasks predominantly rely on image-based representations, failing to explicitly leverage the rich textual information embedded in charts (Masry et al., 2023; Han et al., 2023; Meng et al., 2024). This limitation reduces their effectiveness, particularly in tasks requiring precise interpretation of textual content."}, {"title": "Challenges", "content": "Directly employing OCR tools to extract text from charts often results in errors such as misrecognition, incomplete extraction, or misalignment, particularly when dealing with complex chart structures. This presents the first challenge: (C1: Alignment Challenge) How to accurately align noisy OCR text with the corresponding visual components of the chart, enabling the model to learn meaningful joint representations and avoid misinterpretation? (C2: Architectural Challenge) How can we design a flexible and efficient architecture that can dynamically adapt to different chart types and tasks, effectively integrating visual and textual cues to optimize performance? (C3: Dataset Challenge) Existing datasets lack comprehensive training data that integrates both structural visual elements and textual information for chart understanding tasks."}, {"title": "Our Methodology", "content": "In response to these challenges, we introduce AskChart, a universal model that explicitly integrates both textual and visual cues from charts using a sparse Mixture of Experts (MOE) architecture to tackle multiple chart understanding tasks effectively. Specifically, AskChart utilizes a plug-in text extractor to extract textual information from charts, which is processed alongside user instructions via text encoders. In parallel, the visual encoder captures structural and visual chart information. The attention mechanism in LLMs integrates these components, while visual-textual alignment learning ensures the noisy extracted text is accurately aligned with its corresponding visual elements (addressing CI). To effectively handle diverse chart types and tasks without compromising on performance and efficiency, AskChart employs MoE layers, which allows for sparse computation, activating only the relevant experts and reducing unnecessary overhead by dynamically distributing tasks among specialized experts (addressing C2).\nTo address the third challenge (C3), we construct ChartBank, a large-scale dataset consisting of approximately 7.5 million samples that integrates both visual and textual elements from various chart-related tasks. ChartBank consists of three specialized datasets: (a) the OCR-aware Data"}, {"title": "Contribution", "content": "Our contributions can be summarized as follows:\n(1) New Methodology. We propose AskChart, a lightweight model that explicitly integrates both textual and visual cues through MoE layers. We employ a three-stage training strategy with tailored pretraining objectives to enhance its performance across diverse chart understanding tasks.\n(2) New Dataset. We introduce ChartBank, a large-scale dataset with approximately 7.5 million samples, comprising three specialized sub-datasets: the Visual Prompt Dataset, the OCR-aware Instruction-Following Dataset, and the Chart-to-Table Instruction-Following Dataset.\n(3) Extensive Experiments. Our approach achieves new state-of-the-art performance across multiple benchmarks. AskChart outperforms larger models, such as those with 13B parameters, by 68.3% in Open-ended ChartQA and 49.2% in Chart-to-Text tasks, while delivering comparable results in ChartQA and Chart-to-Table tasks. We make both code and datasets publicly available at https://github.com/Sootung/AskChart."}, {"title": "2 RELATED WORK", "content": "Chart Understanding. In chart understanding, key tasks have emerged, each focusing on interpreting and reasoning over chart data Luo et al. (2022b; 2021); Qin et al. (2020). ChartQA (Hoque et al., 2022; Xu et al., 2023) involves answering questions related to both the content and structure of charts, requiring models to extract insights from graphical elements. The Chart-to-Table (Liu et al., 2023) task converts visual chart data into structured tables for easier analysis, while Chart-to-Text (Kantharaj et al., 2022b) generates descriptive text from chart information. Complex tasks like Open-ended ChartQA (Open CQA) (Kantharaj et al., 2022a) demand higher-level reasoning beyond fact retrieval. Our AskChart is designed to handle these four core chart understanding tasks.\nMLLMs for Chart Understanding. MLLMs like LLaVA (Liu et al., 2024b) and BLIP2 (Li et al., 2023a) have excelled in chart understanding tasks by leveraging abundant natural image datasets (Changpinyo et al., 2021; Lin et al., 2014; Liu et al., 2024b). However, high-quality pre-training datasets for charts are still underexplored. Existing methods like UniChart (Masry et al., 2023) expand task types but struggle with complex reasoning. Models like ChartLLaMA (Han et al., 2023), ChartAssistant (Meng et al., 2024), ChartGemma (Masry et al., 2024c), and ChartInstruct (Masry et al., 2024b) aim to address chart reasoning and editing tasks, while ChartMoE (Xu et al., 2024) improves multimodal input handling. However, open-ended tasks like OpenCQA (Kantharaj et al., 2022a) remain challenging. We propose AskChart with a visual-textual alignment pre-training approach that achieves state-of-the-art results in OpenCQA by better aligning visual chart structure with textual information of charts.\nVisual-Textual Alignment Learning. Recent MLLMs (Zhang et al., 2023; Lin et al., 2024; Han et al., 2023) like LLaVA (Liu et al., 2024b) use single-turn conversations between humans and an assistant to briefly describe natural images. However, for charts, descriptions often include content that visual entities alone cannot capture (e.g., the semantic context of the chart) (Kantharaj et al., 2022b), which results in relatively noisy data for alignment tasks. Models like PresSTU (Kil et al., 2022), PaLI (Chen et al., 2022), and LLaVAR (Zhang et al., 2023) utilize noisy OCR-generated text as ground-truth prediction answers to enhance the model's text comprehension capabilities. Nevertheless, this noisy data remains insufficient for achieving robust alignment (Xu et al., 2020; Ren et al., 2016). LayoutLM (Xu et al., 2020) relies on object detection networks (Ren et al., 2016), which tend to underperform in charts that are rich in structural visual units, as they struggle to compute the patch-OCR loss to align vision and text. Similarly, ChartBERT (Xu et al., 2023), though using OCR-generated text, lacks the ability to effectively represent image and text information jointly. Limited approaches incorporate visual text as input for visual instruction fine-tuning. Our funda-"}, {"title": "3 ASKCHART MODEL", "content": "We will first present the architecture of AskChart (Section 3.1). We will then introduce the training objectives (Section 3.2) and finally elaborate on the training strategy (Section 3.3)."}, {"title": "3.1 ASKCHART ARCHITECTURE", "content": "Overall Architecture. As shown in Figure 2, the architecture of AskChart is designed to efficiently integrate both textual and visual information from charts. AskChart incorporates a text extraction module ($ \\epsilon $), which retrieves textual data from charts, alongside user instructions processed through a word embedding layer $g_t(\\cdot)$. Simultaneously, a vision encoder $g_v(\\cdot)$, captures the structural and visual elements. The extracted multimodal features are then aligned using a projection layer $proj(\\cdot)$, and passed to an LLM, $f_\\theta(\\cdot)$. The LLM is enhanced with the MoE architecture, which dynamically allocates specialized experts to specific tokens. This design not only ensures efficiency and scalability but also enables the model to effectively manage the complex interactions between visual and textual modalities, all while maintaining a lightweight computational footprint."}, {"title": "3.2 TRAINING OBJECTIVES", "content": "We perform instruction-tuning of AskChart. Specifically, we train the LLM with MoE and the Vision Encoder in AskChart on the prediction tokens, using both the original (Lin et al., 2024) auto-regressive loss $L_{reg}$ and an auxiliary loss $L_{aux}$ (Fedus et al., 2021) which encourages the router to efficiently balance the load across multiple experts. The combined objective can be expressed as:\n$L = L_{reg} + \\lambda L_{aux},$\nwhere $ \\lambda $ is a balancing factor that controls the contribution of the auxiliary loss $L_{aux}$.\nGiven a sequence of length L, the auto-regressive loss of the target answers $Y_\\theta$ is defined as,\n$L_{reg} = - \\frac{1}{L} \\sum_{i=1}^{L}  log p_\\theta (y_i | X_v, X_o, X_t, <y_i),$\nwhere $ \\theta $ is the trainable parameters, $y_i$ is the current prediction token.\nFor N experts, the auxiliary loss $L_{aux}$ is computed as,\n$L_{aux} = \\frac{N}{L} \\sum_{i=1}^{N} F_i P_i,$\nwhere F is the fraction of tokens processed by expert i, and P represents the portion of the router probability assigned to expert i, which can be defined as:\n$F_i = \\frac{1}{L} \\sum_{i=1}^{L} {argmax\\:p(x) = i}; P_i = \\frac{1}{L} \\sum_{i=1}^{L} p_i(x).$"}, {"title": "3.3 TRAINING STRATEGY", "content": "To effectively train AskChart, we adopt a three-stage training strategy designed to align visual and textual modalities in charts, ensuring the model learns robust visual-textual representations. This strategy also fine-tunes the MoE layers to handle diverse chart understanding tasks efficiently. Throughout these stages, we employ multi-task tuning based on the ChartBank dataset (will be introduced in Section 4). Unlike existing MLLMs (Liu et al., 2024b; Lin et al., 2024; Meng et al., 2024), which typically freeze the vision encoder during training, we find that unfreezing the vision encoder across all stages significantly improves performance in chart understanding tasks."}, {"title": "Stage I: Visual-Textual Alignment", "content": "Effective chart understanding requires the model to establish a clear relationship between the chart's visual representation and its corresponding textual information. The goal of this stage is to accurately align noisy OCR-extracted text with the visual elements of the chart. To achieve this, we use Chart-to-Table translation as a pretraining task, similar to approaches used in ChartAst (Meng et al., 2024) and Matcha (Liu et al., 2022). The vision encoder and projection layer are trained to map image tokens into pseudo-text tokens. During this phase, we utilize relatively noisy chart-table pairs, where some of the underlying data tables are estimated based on the graphical marks (e.g., bars) as a percentage of the chart's plot area (Masry et al., 2023). Although this introduces some noise, we mitigate it with high-quality datasets during fine-tuning, effectively aiding the model in aligning charts with their corresponding tables."}, {"title": "Stage II: Multi-task Instruction Tuning", "content": "This stage aims to enable the model to generalize across various chart understanding tasks and diverse user instructions. As shown in Table 8, a key task is chart summarization, where the model generates summaries of chart content based on different user instructions, enhancing its ability to produce varying levels of detail. Specifically, Numerical and visual reasoning tasks go beyond the template-based reasoning seen in UniChart (Masry et al., 2023), by incorporating multi-turn conversations, covering sub-tasks like chart structural understanding, data retrieval, and mathematical reasoning. The open-ended ChartQA (Kantharaj et al., 2022a) task involves high-level questions requiring reasoning and explanatory answers. To address these, the model must comprehend visual text, demanding both perceptual and cognitive understanding. In contrast, low-level ChartQA tasks focus on specific goals such as reasoning, searching, and data retrieval. Each chart is marked with visual prompts to guide the model toward specific, highlighted areas of the image, improving task focus and accuracy."}, {"title": "Stage III: Fine-tuning with Mixture of Experts", "content": "To mitigate the learning difficulty associated with the sparse model architecture, we initialize the weights in the third stage using those from the second stage. When tokens are fed into the MoE layers, the router activates the top-k experts to handle the tokens, and their outputs are combined using a weighted sum based on the router's weights. This mechanism helps distribute the computational load across multiple experts, improving the model's efficiency. In this stage, we fine-tune the model on tasks that are highly relevant to downstream tasks. Recognizing the challenges of translating charts to tables, we introduce a Chain-of-Thought (CoT)-based (Wei et al., 2022) translation task. This task requires the model to generate a step-by-step reasoning process (CoT) rather than producing a direct answer. By generating CoT answers, the model is encouraged to explicitly demonstrate its reasoning pathway, which leads to more accurate and interpretable results, particularly for complex Chart-to-Table translation tasks."}, {"title": "4 CHARTBANK DATASET", "content": "To enhance AskChart's chart understanding capabilities, we curate ChartBank, comprising three specialized datasets alongside existing work: (1) the Visual Prompt Dataset, (2) the OCR-aware Data Prompt Dataset, and (3) the Chart-to-Table Instruction-Following Dataset.\nChartBank Overview. Figure 2 illustrates examples from our ChartBank, and Appendix A provides a summary of the ChartBank statistics. Specifically, the Visual Prompt Dataset and OCR-aware Data Prompt Dataset cover 6 representative chart types: pie, common bar, stacked bar, grouped bar, common line, and grouped line charts. Among these types of charts, the common bar and common line both have only one category of data, while the stacked bar, grouped bar, and grouped line all have multiple categories of data. The Chart-to-Table Instruction Following Dataset additionally involves scatter plots. We transform all datasets, including datasets introduced by us and training sets of"}, {"title": "4.1 VISUAL PROMPT DATASET", "content": "Region understanding capabilities are crucial in chart understanding, as questions often target only particular elements, like individual bars in a bar chart. We also aim to strengthen the MLLM's numerical visual reasoning to understand relationships among numerical values. Therefore, we develop and incorporate the Visual Prompt Dataset for second-stage pretraining, as shown in Figure 2(a).\nConstruction. Charts in ChatQA (Masry et al., 2022) are utilized as the foundation to construct the Visual Prompt Dataset. Firstly, we carefully design question templates (Appendix Table 13) to be used in question generation for four tasks: (1) reasoning, (2) extremum, (3) determining range, and (4) data retrieval. Subsequently, for each chart, we randomly select elements to generate questions and record their bounding box indices, thereby overlapping the visual prompt using ViP-LLaVA (Cai et al., 2024a). Charts unable to be visually prompted accurately by ViP-LLaVA, like involving correlation and distribution tasks, will be deemed unsuitable and consequently excluded. For diversity, we randomly select three types of visual prompts from a set of four (namely arrow, ellipsis, bounding box, and triangle) for each question, yielding 417,780 (Chart, Question, Answer) pairs ultimately. Figure 2-a2, a3 illustrates an example with the rectangle visual prompt."}, {"title": "4.2 OCR-AWARE DATA PROMPT DATASET", "content": "As mentioned, the weakness in text capture and utilization is a bottleneck limiting MLLMs' chart understanding capabilities. We aim to enhance MLLMs' such capabilities by providing richer and denser textual information aligned with the features in charts. Also, multi-turn question-answering examples are included to enable the model to better fit real-world scenarios. Therefore, we introduce the OCR-aware Data Prompt Dataset in the second-stage pretraining, as shown in Figure 2(b).\nConstruction. The OCR-aware Data Prompt Dataset includes two parts: single-turn and multi-turn instruction-following data, with each example comprising four essential elements: questions (Figure 2-b1), charts (Figure 2-b2), OCR results (Figure 2-b3), and answers (Figure 2-b4). For both single-turn and multi-turn examples, we employ PaddleOCR to extract textual information from the input charts to obtain OCR results. The single-turn instruction-following data is directly derived from UniChart (Masry et al., 2023) through format transformation, containing 6,791,230 examples. For multi-turn data, we utilize charts in UniChart accompanied by original tables, serving as the foundation for generation. First, we prompt ChatGPT (Ouyang et al., 2022) to identify and summarize the common question types in PlotQA (Methani et al., 2020) templates, which encompass three question-answering task categories: structural understanding, data retrieval, and mathematical reasoning. To enhance the effectiveness and accuracy of question and answer generation, we provide ChatGPT with sequenced original tables instead of charts. Then ChatGPT is prompted to synthetically generate two to three rounds of questions and answers, guided by identified question types (prompts in Appendix Table 14). Finally, we obtain 189,747 multi-turn examples."}, {"title": "4.3 CHART-TO-TABLE INSTRUCTION FOLLOWING DATASET", "content": "To improve AskChart's ability to comprehensively extract and understand information from charts, we propose CoT based the Chart-to-Table Instruction Following Dataset for the third-stage fine-tuning, as shown by the example in Figure 2(c).\nConstruction. We construct a large amount of high-quality (chart, COT annotated table) pairs by converting tables into charts with CoT ground-truth answers (see Appendix F). To this end, we first utilize widely used Text-to-SQL datasets, Spider (Yu et al., 2018) and BIRD (Li et al., 2024), which contain 1,020 and 1,460 tables on 138 and 37 domains, respectively, as the base table. we first employ the automatic visualization system, DeepEye (Luo et al., 2018), to recommend good charts"}, {"title": "5 EXPERIMENTS", "content": "5.1 EXPERIMENTAL SETUP\nDatasets and Tasks. We evaluate AskChart against state-of-the-art (SOTA) methods on four chart understanding tasks using various widely-used benchmarks. For ChartQA, we use the ChartQA benchmark (Masry et al., 2022), which focuses on visual and logical reasoning, where each question typically has a single word or numerical answer. This benchmark also includes the Chart-to-Table translation task, for which we follow the evaluation methodology from prior work. Additionally, we assess the model's performance in the chart summarization task using the Chart-to-Text benchmark (Kantharaj et al., 2022b). For Open-ended ChartQA, we evaluate using the OpenCQA benchmark (Kantharaj et al., 2022a), where questions require more explanatory and detailed answers.\nEvaluation Metrics. We adopt evaluation metrics from prior studies (Masry et al., 2022). For ChartQA, we use relaxed accuracy (RA), allowing a 5% margin of error for numerical answers and exact matches for textual answers. For Chart-to-Table, we report RMS-F1 scores based on the DePlot framework (Liu et al., 2023). Both the Chart-to-Text task and OpenCQA are evaluated using BLEU scores (Post, 2018), consistent with previous works (Masry et al., 2023; Liu et al., 2022).\nBaselines. We compare AskChart with three types of baselines. (i) General-Purpose MLLMs. These models are designed for diverse multimodal tasks and have demonstrated strong image understanding capabilities. We included Blip2 (Li et al., 2023a), SPHINX (Lin et al., 2023), and Qwen-VL (Bai et al., 2023) as representative models to assess their applicability to chart-specific tasks. (ii) Specialist Chart Models. This category focuses on models tailored for document and chart comprehension. In addition to DePlot (Liu et al., 2023) and OneChart (?), We considered UniChart (Masry et al., 2023) and MatCha (Liu et al., 2022), which build on the architectures of Donut (Kim et al., 2022) and Pix2Struct (Lee et al., 2023) respectively, known for their strengths in document understanding. Additionally, we evaluated Chart-T5 (Zhou et al., 2023), an enhanced version of the text-centric T5 model (Raffel et al., 2020), which has been adapted for solving chart-related language tasks. (iii) Chart MLLMs. These models are explicitly designed for chart-related tasks and leverage popular vision-language model architectures to achieve state-of-the-art performance. We selected ChartInstruct (Masry et al., 2024a), ChartLLaMa (Han et al., 2023)Tiny-Chart (?) and ChartAst (Meng et al., 2024), which demonstrate advanced capabilities in various chart comprehension challenges.\nImplementation Details. AskChart integrating SigLIP (Zhai et al., 2023) as the vision encoder and Phi-2 (Li et al., 2023b) as the language model. We trained all models using 8 A100 GPUs. Table 8 shows all datasets used for training. For Stage I, we trained the model for 1 epoch with a learning rate of le-3 and a batch size of 32 per GPU. For Stage II and Stage III, we fine-tuned the model for 1 and 6 epochs, respectively, with a learning rate of 2e-5 and a batch size of 16 per GPU. Please refer to Appendix D for more details."}, {"title": "5.2 MAIN RESULTS", "content": "Figure 3 shows a comparison of AskChart with SOTA models across four chart understanding benchmarks. Remarkably, AskChart outperforms the current state-of-the-art methods by 68.3% and 49.2% (on the Pew sub-dataset), and 6.7% (on the Statista sub-dataset) in the open-ended ChartQA and chart-to-text tasks, respectively. This demonstrates that the lightweight AskChart (4.6B parameters) achieves competitive results on ChartQA and Chart-to-Table tasks, comparable to the performance of ChartAst-S (13B parameters) (Han et al., 2023). We observe that existing models struggle to effectively handle long text generation tasks, such as open-ended ChartQA, which requires generating explanatory answers by reasoning with chart content, and Chart-to-Text, which demands an integrated understanding of visual and textual information in charts. Due to its explicit text enhancement and multitask training, AskChart performs joint visual and explicit text representation, and its MoE architecture enables a single token to be processed by different experts, with weighted outputs providing a more robust representation. This capability allows it to effectively address such complex tasks. Moreover, AskChart demonstrates significant advantages in tasks that demand both"}, {"title": "5.3 FURTHER STUDY", "content": "The ChartInsights benchmark (Wu et al., 2024) evaluates multimodal models' capabilities in low-level chart analysis tasks, challenging them to not only recognize visual elements but also understand their underlying statistical and analytical significance. As shown in Table 1, AskChart demonstrates exceptional performance across various analytical tasks. Notably, it excels in the distribution and correlation tasks, achieving scores of 50% and 58.7%, the highest among all evaluated models. Furthermore, AskChart outperforms competitors in the range task with a leading score of 59.5%. Its performance in retrieval is also remarkable, achieving a score of 71%, significantly surpassing other models. Overall, AskChart attains an impressive total score of 42.7%, ranking first among all models. These results highlight the effectiveness of the OCR-aware data prompt strategy employed during pretraining, which has enabled AskChart to align textual and visual semantics effectively, particularly excelling in tasks requiring nuanced integration of both modalities."}, {"title": "5.4 ABLATION STUDY", "content": "Table 2: Ablation study on different prompts.\nChartQA\nOpen-ended ChartQA\nChart-to-Table\nChart-to-Text\nVisual Prompt Ocr-aware data prompt\naug.\nhuman\navg.\nOpenCQA\nChartQA\nPew\nStatista\nX\nX\nX\n75.5\n44.9\n60.2\n63.1\n63.9\n55.2\n55.1\n83.8\n50.1\n67.0\n79.3\n81.3\n57.2\n58.0\nX\n76.6\n46.1\n61.4\n63.4\n62.6\n50.9\n55.1\n84.6\n50.9\n67.8\n79.3\n81.5\n60.6\n62.8\nThe Impact of Different Prompts. To evaluate the influence of visual prompts and OCR-aware data prompts on model performance, we randomly sampled approximately 1M samples from the sub-datasets of each stage due to limited computational resources. We trained the model from scratch, and the results are shown in Table 2. The results indicate that visual prompts significantly enhance the model's performance on question-answering tasks (notably, we trained with only about 35% of the visual prompt dataset). This suggests that visual cues in charts help the model focus on the relevant areas associated with the questions.\nThe Impact of Training Strategy. To assess which alignment strategy more effectively aligns visual and textual information, we pre-trained the model in Stage I using two different tasks: Chart-to-Text and Chart-to-Table. As shown in Table 3, the model trained with the Chart-to-Table alignment strategy consistently outperforms across multiple tasks. We attribute this to the fact that Chart-to-Table translation helps the model understand the underlying chart content rather than generating potentially irrelevant textual descriptions.\nThe Impact of Number of Experts. To evaluate the effect of the number of experts in the MoE layers on model performance, we conducted the following experiments. First, we varied the total number of experts while keeping the number of activated experts constant. As shown in Table 5, increasing the number of experts leads to improved performance across various tasks."}, {"title": "5.5 ZERO-SHOT STUDY", "content": "To evaluate the generalization capability of our model, we collected data from datasets that the model had never seen before for zero-shot experiments. Specifically, we conducted tests on several datasets, including RealCQA (Ahmed et al., 2023), StructChart (Xia et al., 2023), and ChartX (Xia"}, {"title": "6 CONCLUSION", "content": "In this paper, we introduced AskChart, a lightweight chart understanding model that integrates both textual and visual cues using a Mixture of Experts architecture. By employing a three-stage training strategy with tailored pretraining objectives, AskChart demonstrates enhanced performance across diverse chart understanding tasks. We also presented ChartBank, a large-scale dataset with approximately 7.5M samples, featuring three specialized sub-datasets designed to improve the model's ability to comprehend and interpret chart data. Extensive experiments show that AskChart achieves state-of-the-art results, outperforming larger models in tasks such as Open-ended ChartQA and Chart-to-Text by 68.3% and 49.2%, respectively."}, {"title": "3.1 ASKCHART ARCHITECTURE", "content": "Overall Architecture. As shown in Figure 2, the architecture of AskChart is designed to efficiently integrate both textual and visual information from charts. AskChart incorporates a text extraction module ($\\epsilon$), which retrieves textual data from charts, alongside user instructions processed through a word embedding layer $g_t(\\cdot)$. Simultaneously, a vision encoder $g_v(\\cdot)$, captures the structural and visual elements. The extracted multimodal features are then aligned using a projection layer $proj(\\cdot)$, and passed to an LLM, $f_\\theta(\\cdot)$. The LLM is enhanced with the MoE architecture, which dynamically allocates specialized experts to specific tokens. This design not only ensures efficiency and scalability but also enables the model to effectively manage the complex interactions between visual and textual modalities, all while maintaining a lightweight computational footprint."}, {"title": "3.2 TRAINING OBJECTIVES", "content": "We perform instruction-tuning of AskChart. Specifically, we train the LLM with MoE and the Vision Encoder in AskChart on the prediction tokens, using both the original (Lin et al., 2024) auto-regressive loss $L_{reg}$ and an auxiliary loss $L_{aux}$ (Fedus et al., 2021) which encourages the router to efficiently balance the load across multiple experts. The combined objective can be expressed as:\n$L = L_{reg} + \\lambda L_{aux},$\nwhere $ \\lambda $ is a balancing factor that controls the contribution of the auxiliary loss $L_{aux}$.\nGiven a sequence of length L, the auto-regressive loss of the target answers $Y_\\theta$ is defined as,\n$L_{reg} = - \\frac{1}{L} \\sum_{i=1}^{L}  log p_\\theta (y_i | X_v, X_o, X_t, <y_i),$\nwhere $ \\theta $ is the trainable parameters, $y_i$ is the current prediction token.\nFor N experts, the auxiliary loss $L_{aux}$ is computed as,\n$L_{aux} = \\frac{N}{L} \\sum_{i=1}^{N} F_i P_i,$\nwhere F is the fraction of tokens processed by expert i, and P represents the portion of the router probability assigned to expert i, which can be defined as:\n$F_i = \\frac{1}{L} \\sum_{i=1}^{L} {argmax\\:p(x) = i}; P_i = \\frac{1}{L} \\sum_{i=1}^{L} p_i(x).$"}, {"title": "C LIMITATIONS", "content": "Although AskChart demonstrates competitive performance, hallucinations remain a challenge, particularly when reasoning about fine-grained visual elements within the chart. Future research could focus on enhancing the vision encoder's capabilities, potentially through strategies such as integrating multiple encoders or employing visual token merging techniques. Moreover, the inherent limitations of large language models in managing extended context lengths pose additional constraints. Input tokens exceeding a predefined length are truncated, potentially affecting training outcomes. Investigating methods to effectively support longer context lengths could be a promising direction for improving joint representations of visual and explicit textual information.\nRegarding the experimental setup, it is important to note that most of the reported results are from a single run. Pretraining is computationally intensive and costly, particularly when multiple ablation setups are considered. We believe that the results would benefit from training over a greater number of steps."}, {"title": "D TRAINING DETAILS", "content": "Table 10: Training hyperparameters.\nConfigurations\nStage I\nStage II\nStage III\nExperts\n4\nTop-k\n2\nDeepspeed\nZero2\nZero2\nZero2\nImage resolution\n384x384\nImage encoder\nSigLip/384\nFeature select layer\n-2\nImage projector\n2 Linear layers with GeLU\nEpoch\n1\n1\n6\nLearning rate\n1e-3\n2e-5\n2e-5\nLearning rate schdule\nCosine"}]}