{"title": "Privacy-Preserving Data Deduplication for Enhancing Federated Learning of Language Models", "authors": ["Aydin Abadi", "Vishnu Asutosh Dasu", "Sumanta Sarkar"], "abstract": "Deduplication is a vital preprocessing step that enhances machine learning model performance and saves training time and energy. However, enhancing federated learning through deduplication poses challenges, especially regarding scalability and potential privacy violations if deduplication involves sharing all clients' data. In this paper, we address the problem of deduplication in a federated setup by introducing a pioneering protocol, Efficient Privacy-Preserving Multi-Party Deduplication (EP-MPD). It efficiently removes duplicates from multiple clients' datasets without compromising data privacy. EP-MPD is constructed in a modular fashion, utilizing two novel variants of the Private Set Intersection protocol. Our extensive experiments demonstrate the significant benefits of deduplication in federated learning of large language models. For instance, we observe up to 19.61% improvement in perplexity and up to 27.95% reduction in running time. EP-MPD effectively balances privacy and performance in federated learning, making it a valuable solution for large-scale applications.", "sections": [{"title": "I. INTRODUCTION", "content": "The application of machine learning (ML) has experienced rapid growth, becoming widely embraced by businesses due to its various benefits. As ML relies heavily on data, there is an inherent risk of privacy breaches associated with this process. Federated learning (FL) is one of the main steps towards privacy-preserving ML that allows training across multiple decentralized devices without exchanging data. In FL, devices compute local models based on their data and then share the local model updates with a central server. This server aggregates the updates to derive a global model that encapsulates the features of all the local data held by the individual devices [38].\nThe quality of the training data significantly influences the accuracy of an ML model. Data generated from real-world applications often lacks organization, leading to issues such as missing values, typos, format mismatches, outliers, or duplicated entries within the raw dataset. To ensure meaningful learning, the collected data must undergo a thorough data cleaning process [23]. Duplicated sequences are prevalent in text datasets. They can adversely affect the training process of Language Models (LMs). Lee et al. [31] investigated the Colossal Clean Crawled Corpus (C4) [44] dataset. They discovered a 61-word sequence within C4 that was repeated 61,036 times verbatim in the training dataset and 61 times in the validation set. As a result, the trained model generalized poorly on the dataset, and over 1% of the unprompted model outputs were memorized and copied verbatim from the training dataset. Upon deduplicating the dataset, they reduced memo-rization by up to 10x and, in certain cases, even improved perplexity by up to 10%.\nAdditionally, memorization negatively affects the privacy and fairness of LMs [12]. Memorization makes language models vulnerable to membership inference attacks [48], [37], data extraction attacks [39], [13], and can lead to copyright violations as LMs can regurgitate training data sequences from copyrighted sources [28]. Carlini et al. [12] conclude that bigger LMs memorize more and more duplicates increase memorization. Kandpal et al. [27] show that the success of most privacy attacks on LMs is largely due to the duplication in the training datasets. Furthermore, in the FL setting, malicious clients can exploit the memorization of LMs to extract sensitive information from honest clients' datasets [46]. Therefore, we must adopt data-cleaning practices to improve model utility and reduce the risks of privacy attacks.\nWhile deduplication can improve model performance and reduce memorization, it also enhances training efficiency in various aspects [31]. Removing duplicates reduces GPU time and minimizes training costs in terms of time, money, and carbon footprint. This streamlined process optimizes resource utilization and contributes to more sustainable ML practices [7], [47], [42]. Sustainable development is a global priority, and aligning ML practices with this theme is crucial.\nBased on the discussions thus far, it is evident that the deduplication of training data offers numerous benefits. Several real-world applications involve training with FL such as health-care [3], smart city [25], and edge computing [51]. Some of these applications pose a risk of duplicates in the local training data across multiple devices. For example, Google Keyboard suggestions from a user's text query rely on FL [22]. Local models are trained in situ on Android phones with user data. In this setting, many text queries typed by the users across multiple phones are the same.\nTherefore, for a FL process to be efficient and effective, participating devices must perform deduplication. When a data owner solely intends to remove duplicates from their dataset, the privacy risk is minimal since the data is entirely under the owner's control. In the context of FL, the scenario shifts significantly when deduplication is introduced. Multiple devices participating in FL may possess overlapping data, even after deduplicating their datasets. If they aim to deduplicate the combined dataset, one approach could involve sharing their raw data with each other and checking for intersections. However, this approach compromises privacy."}, {"title": "A. Overview of Privacy-Preserving Deduplication in FL", "content": "Consider nodes $D_1,..., D_n$ are involved in FL, where each $D_i$ has a dataset $S_i$. Effectively, the training in FL is conducted on the union of these datasets, i.e., $\\cup_{i=1}^n S_i$. If intersections exist among the datasets, FL will incorporate duplicates, leading to the drawbacks discussed earlier.\nConsider nodes $D_1$ and $D_2$, each with datasets $S_1$ and $S_2$, respectively implying that FL training occurs on $S_1 \\cup S_2$. If the intersection $S_1 \\cap S_2$ is nonempty, training individually on $S_1$ and $S_2$ would mean training twice on the duplicate $S_1 \\cap S_2$. Thus, one of $D_1$ and $D_2$ should remove the duplicate $S_1 \\cap S_2$. This should be done without harming each other's data privacy. Our solution applies private set intersection (PSI) which securely finds the intersection of $S_1$ and $S_2$ without revealing any other elements. Once $D_1$ and $D_2$ learn $S_1 \\cap S_2$ through PSI, $D_1$ will train on $S'_1 = S_1 \\setminus S_1 \\cap S_2$ and $D_2$ will train on $S_2$. According to set theory, it holds that $S'_1 \\cup S_2 = S_1 \\cup S_2$. Hence, the resulting FL model remains as intended, while the training process is devoid of duplicates, avoiding the associated drawbacks. The scenario with two nodes seems straightforward. However, when more than two nodes participate, it becomes complicated. We now consider n nodes $D_1,..., D_n$, where each node $D_i$ has a set $S_i, \\forall i, 1 \\leq i \\leq n$, and $n > 2$.\nFollowing the approach used in the two-node case, one might be tempted to (i) apply multi-party PSI to n nodes, (ii) find their intersection $I_n$, and (iii) let node $D_1$ train on $S'_1 = S_1 \\setminus I_n$, and rest of the nodes train on their own dataset S. This method removes the duplicates that exist across all the nodes. However, this does not detect and remove the duplicates that exist among a subset of nodes. For instance, if there is a subset of k nodes ($k < n$) with a large intersection $I_k$, then $I_k$ will remain in the full training dataset $\\cup_{i=2}^k (\\cup_{j=2}^{k} S_j)$ as duplicates. A generic multi-party PSI does not help in this case. Therefore, we need to consider each pair of nodes and remove the duplicates accordingly."}, {"title": "B. Our Contributions", "content": "In this paper, our end goal is to develop a scheme such that allows multiple clients to benefit from deduplication while training on their data in a federated learning setup. The first requirement is an efficient deduplication of sets belonging to multiple clients. To address this, we introduce the notion of Privacy-Preserving Multi-Party Deduplication (P-MPD) in Section IV. This essentially describes a functionality that takes input datasets $S_1,..., S_m$ (potentially containing duplicates) from m clients and outputs the datasets $S'_1,..., S'_m$ such that $\\cup_{i=1}^m S'_i = \\cup_{i=1}^m S_i$, where $S'_i \\cap S'_j = \\emptyset, i \\neq j$. We realize that an efficient construction of P-MPD requires a substantially improved PSI protocol that supports scalability. This leads us to introduce a new notion for PSI which we call Group PSI (G-PSI) in Section III. The functionality of G-PSI takes sets from a group of clients and returns each client the intersection of their sets with all the other clients' sets. We provide constructions of Efficient Group PSI (EG-PSI) that efficiently realizes G-PSI, namely EG-PSI(\u00b9) and EG-PSI(\u00b9\u00b9) in Figures 1 and 2, respectively. EG-PSI(\u00b9) is based on symmetric key primitives such as pseudorandom permutation, while EG-PSI(\u00b9\u00b9) is developed using an oblivious pseudorandom function, which is a well-known public key primitive. With the building block EG-PSI, we build Efficient Privacy-Preserving Multi-Party Deduplication (EP-MPD) that realizes P-MPD as shown in Figure 4. We prove the security of EP-MPD, EG-PSI(\u00b9), and EG-PSI(\u00b9\u00b9) within the simulation-based paradigm. Our con-struction of EP-MPD allows for efficient duplicate removal without compromising clients' data privacy, resulting in an improved model after running federated learning on the dedu-plicated datasets, as outlined in Figure 5.\nWe perform an extensive experimental evaluation to bench-mark EP-MPD and the effect of the resulting deduplication on FL. The overall running time for EP-MPD(\u00b9), which employs EG-PSI(\u00b9) (symmetric key primitives based), is much less than EP-MPD(\u00b9\u00b9), which utilizes EG-PSI(\u00b9\u00b9) (public key primitives based). Overall, clients enjoy relatively less computation time in EP-MPD(\u00b9). Our protocols can scale to large datasets and client counts. For example, when 50 clients have $2^{19}$ data points in their datasets comprising of 30% duplicates, then EP-MPD takes 1160 seconds and EP-MPD(\u00b9\u00b9) takes 7653 seconds; client running time is 641 and 111 seconds in EP-MPD(\u00b9) and EP-MPD(\u00b9\u00b9) respectively. We experiment with 7 datasets, 2 language models, and 10 clients in FL. We achieve an improvement of up to 19.61% in perplexity and up to 27.95% improvement in GPU training time."}, {"title": "II. PRELIMINARIES", "content": ""}, {"title": "A. Notations and Assumptions", "content": "We define a wrapper function Update(S, \u015c) \u2192 S which takes two sets, S and \u015c. It updates S by removing from it the elements in set \u015c and returns the updated set S. In this paper, by the sum of sets (e.g., $\\sum_{i=1}^n S_i$) we mean the concatenation of the sets which may result in a multi-set. We denote an empty set by \u00d8. We denote a size of vector \u0e1b\u0e35 with . We assume that the server and all the users have access to secure channels among them. By the notation, $\\mathcal{X} \\stackrel{c}{=} \\mathcal{Y}$, we mean that the two distributions $\\mathcal{X}$ and $\\mathcal{Y}$ are computationally indistinguishable."}, {"title": "B. Federated Learning (FL)", "content": "The concept of FL was proposed by McMahan et al. [38] as a framework for training an ML model where the training data are distributed across multiple devices. In FL, a server orchestrates the training of a global model by aggregating models locally computed by the clients on their local devices. Suppose there are n clients, and each client $D_i$ has a dataset $S_i$. The server has the initial model \u03b8. It sends \u03b8 to the clients, and each client performs gradient descent computation on their local dataset $S_i$ as $J_i(S_i, \\theta) = \\sum_{(x,y) \\in S_i} C'(\\theta, (x, y))$, where C is the cost function and $d_i = |S_i|$. The client $D_i$ computes the local models as $\\theta_i \\leftarrow \\theta - \\eta \\nabla J_i(S_i, \\theta)$, where \u03b7 is the learning rate. After receiving the local models from the clients, the server performs an aggregated averaging on the local models to derive the global model as:\n$\\theta = \\frac{1}{\\sum_{i=1}^n d_i} (d_1 \\theta_1 + ... + d_m \\theta_m)$ (1)\nwhere $d = \\sum_{i=1}^n d_i$ is the total size of the dataset $S = \\sum_{i=1}^n S_i$, and \u03b8 is the locally trained model on the dataset S. This computation is repeated until the model converges.\nWhile this framework achieves a basic level of privacy in which each user's data is not directly sent to the server, it is susceptible to advanced privacy attacks such as membership inference attacks [48], [37] and data extraction attacks [13], [46], [18], [4] as the client models $\\theta_i$ are aggregated in a non-private fashion on the server side. Some privacy-preserving FL protocols have attempted to mitigate these attacks by securely aggregating the client models. These protocols rely on cryptographic techniques like homomorphic encryption [14], [15], functional encryption [52], or secure aggregation [5], [9]. Additionally, differentially private training techniques [2] can be used to ensure differential privacy guarantees on the global model \u0398. We emphasize that in this paper, our proposed schemes are agnostic to the type of FL mechanism used."}, {"title": "C. Causal Language Modeling (CLM)", "content": "Causal Language Modeling (CLM) is a natural language processing task where the goal of the language model is to predict the next word or token given a sequence of tokens. The language model autoregressively generates the next token until a pre-determined sequence length is reached or a special STOP token is generated. Given a sequence of n tokens Y = {$Y_1, Y_2..., Y_{n-1}, Y_n$}, the language model is trained to learn the following probability distribution:\n$P(Y) = \\prod_{i=1}^n P(Y_i | Y_1, ..., Y_{i-1})$ (2)\nThe CLM training objective is to minimize the negative log-likelihood loss given by:\n$L(\\Theta, Y) = -\\sum_{i=1}^n log(\\Theta(y_i | Y_1,..., Y_{i-1}))$ (3)\nAfter training is complete, the text is autoregressively sam-pled from the language model i.e., $\\hat{y}_{t<n} \\sim \\Theta(Y_t | Y_1, ..., Y_{t-1})$.\nThe perplexity metric is commonly used to evaluate the performance of the language model to determine how well it has learned the probability distribution in Equation 2. The perplexity PP of a sequence y is defined as:\n$PP(Y) = exp(\\frac{1}{n} \\sum_{i=1}^n log(\\Theta(y_i | Y_1, ..., Y_{i-1}))$) (4)\nA lower perplexity score implies that model has been trained well to estimate the real-world probability distribution. Informally, a sequence with a low perplexity score implies that the model is less \"surprised\" by a sequence of tokens."}, {"title": "D. Security Model", "content": "In this paper, we use the simulation-based paradigm of secure multi-party computation [21] to define and prove the proposed protocol. Since we focus on the static passive (semi-honest) adversarial model, we will restate the security defini-tion within this context.\n1) Two-party Computation: A two-party protocol \u0393is captured by specifying a random process that maps a pair of input to a pair of outputs (one output for each party). Such process is referred to as a functionality denoted by $f : {0,1}^* \\times {0,1}^* \\rightarrow {0, 1}^* \\times {0,1}^*$, where f := ($f_1, f_2$). For every input pair (x, y), the output pair is a random variable ($f_1(x, y), f_2(x, y)$), such that the party with input x wishes to obtain $f_1(x, y)$ while the party with input y wishes to receive $f_2(x, y)$. The above functionality can be easily extended to more than two parties.\n2) Security in the Presence of Passive Adversaries: In the passive adversarial model, the party corrupted by such an ad-versary correctly follows the protocol specification. Nonethe-less, the adversary obtains the internal state of the corrupted party, including the transcript of all the messages received, and tries to use this to learn information that should remain private. Loosely speaking, a protocol is secure if whatever can be computed by a corrupt party in the protocol can be computed using its input and output only.\nIn the simulation-based model, it is required that a party's view in a protocol's execution can be simulated given only its input and output. This implies that the parties learn nothing from the protocol's execution. Formally, in two-party case, party i's view (during the execution of \u0393) on input pair (x, y) is denoted by $View_i^\\Pi (x, y)$ and equals $(w, r_i, m_1, ..., m_j)$, where $w \\in {x,y}$ is the input of ith party, $r_i$ is the outcome of this party's internal random coin tosses, and $m_j$ represents the jth message this party receives. The output of the ith party during the execution of \u0393on (x, y) is denoted by $Output_i(x, y)$ and can be generated from its own view of the execution.\nDefinition 1. Let f be the deterministic functionality defined above. Protocol \u0393 securely computes f in the presence of a static passive probabilistic polynomial-time (PPT) adversary A, if for every A in the real model, there exist PPT algorithms (Sim\u2081, Sim\u2082) such that:\n${Sim_1(x, f_1(x,y))}_{x,y} \\stackrel{c}{=} {View_1^\\Gamma(x, y)}_{x,y}$\n${Sim_2(y, f_2(x, y))}_{x,y} \\stackrel{c}{=} {View_2^\\Gamma(x, y)}_{x,y}$\nDefinition 1 can be easily extended to m > 2 parties."}, {"title": "E. Pseudorandom Function and Permutation", "content": "Informally, a pseudorandom function PRF(.) is a determin-istic function that takes a key of length \u03bb and an input of length u; and outputs a value of length v indistinguishable from an output of a truly random function. More formally, a pseudorandom function can be defined as PRF : {$0,1$}^\u03bb \u00d7 {$0,1$}^u \u2192 {$0,1$}^v, where \u03bb is the security parameter.\nThe definition of a pseudorandom permutation, PRP : {$0,1$}^\u03bb \u00d7 {$0,1$}^v \u2192 {$0,1$}^v, is very similar to that of a pseudorandom function, with a difference; namely, it is required the keyed function PRP(k,\u00b7) to be indistinguishable from a uniform permutation, instead of a uniform function. In cryptographic schemes that involve PRP, sometimes honest parties may be required to compute the inverse of pseudo-random permutation, i.e., PRP\u207b\u00b9(k,\u00b7), as well. In this case, it would require that PRP(k,\u00b7) be indistinguishable from a uniform permutation even if the distinguisher is additionally given oracle access to the inverse of the permutation."}, {"title": "F. Oblivious Pseudorandom Function", "content": "An Oblivious Pseudorandom Function (OPRF) is a proto-col that involves a client and a server. OPRF enables the client with input x \u2208 {$0,1$}^u, and the server with key k \u2208 {$0,1$}^\u03bb to execute an instance of PRF. Informally, the security of an OPRF asserts that, by the completion of OPRF, the client only learns the output of PRF is evaluated on inputs k and x, i.e., PRF(k,x) while the server gains no information, e.g., about the input of the client and the output of PRF."}, {"title": "G. Trusted Execution Environments", "content": "Trusted Execution Environment (TEE), also known as a secure enclave, constitutes a secure processing environment comprising processing, memory, and storage hardware units [41], [54]. Within this environment, the code and data residing in them remain isolated from other layers in the software stack, including the operating system. An ideal TEE guarantees the preservation of data integrity and confidentiality. Moreover, TEE can provide remote attestation capabilities, allowing a party to remotely verify the execution of an enclave on a genuine TEE hardware platform. Given the assumption that the physical CPU remains uncompromised, enclaves are shielded from attackers with physical access to the machine, including the memory and system bus.\nSide-channel attacks on different deployments of TEEs have been demonstrated in the literature [49]. These attacks pose a threat as they could enable attackers to extract se-crets from TEEs. Nevertheless, TEEs technologies have been evolving to address and mitigate side-channel attacks.\nOur security, trust, and system assumptions regarding TEEs are conservative. Specifically, as detailed in Sections IV-B1 and III-B, our solution (i) avoids disclosing any plaintext messages or private keys to TEE and (ii) does not expect TEE to maintain an extensive storage and memory space or possess strong processing resources. Instead, we establish a formal model and construction under the assumption that TEE guarantees execution integrity (and authenticity), and ensures minimal confidentiality. Specifically, we formally demonstrate that TEE at the most only learns the size of the encrypted computation result (i.e., the intersections' cardinality). In our work, TEE can be seamlessly substituted with any semi-honest server that does not collude with other entities."}, {"title": "III. GROUP PSI (G-PSI)", "content": ""}, {"title": "A. Formal Definition of G-PSI", "content": "In this section, we present the concept of Group PSI (G-PSI). In G-PSI, there are two groups of clients, $G_0$ and $G_1$, where each group contains m clients, $G_j : {C_{j,1},..., C_{j,m}}, 0 \\leq j \\leq 1$. Each client $C_{j,i}$ has a set $S_{j,i}$, such that no pair of clients' sets in the same group share a common element.\nInformally, G-PSI allows every client in one group to (efficiently) find the intersection that their set has with the set of every client of the other group, without allowing them to learn anything beyond that about other clients' set elements. To achieve a high level of computational efficiency in G-PSI, we will involve a third-party TP that assists the clients with computing the intersection. The functionality $f_{G-PSI}$ that G-PSI computes takes a set $S_{j,i}$ from every client $C_{j,i}$ and no input from TP. It returns (i) to every client $C_{j,i}$ a vector \u2081, which contains the intersection that $C_{j,i}$'s set has with every other client's set in the other group and (ii) to TP an empty set \u00d8. Hence, functionality $f_{G-PSI}$ can be formally defined as follows.\n$f_{GPSI} ((S_{0,1},..., S_{0,m}), (S_{1,1},..., S_{1,m}), \\emptyset) \\rightarrow ((\\vec{v}_{0,1},..., \\vec{v}_{0,m}), (\\vec{v}_{1,1},..., \\vec{v}_{1,m}), \\emptyset),$ (5)\nwhere, $\\vec{v}_{j,i} = [[S_{j,i} \\cap S_{1-j,1}],..., [S_{j,i} \\cap S_{1-j,m}]]$, $0 \\leq j \\leq 1$, and $1 \\leq i \\leq m$.\nIn the case where clients of only one of the groups, e.g., $G_0$, receives the result, then the above functionality simply returns \u00d8 to the clients in the other group, e.g., $\\vec{v}_{1,1} = ... = \\vec{v}_{1,m} = \\emptyset$.\nSince TP performs computation on all clients' encrypted sets, there is a possibility of leakage to TP. Depending on the protocol that realizes $f_{G-PSI}$ this leakage could contain different types of information; for instance, it could contain (i) the size of the intersection of any two clients' sets, or (ii) the size of the intersection of all clients' sets, or (iii) nothing at all. Often such leakage is defined by a leakage function L that takes all parties (encoded) inputs and returns the amount of leakage.\nWe assert that a protocol securely realizes $f_{G-PSI}$ if (1) it reveals nothing beyond a predefined leakage to TP and (2) whatever can be computed by a client in the protocol can be obtained from its input and output. This is formalized under the simulation paradigm. We require a client's view during an execution of G-PSI to be simulatable given its input and output. We require that the view of TP can be simulated given the leakage.\nDefinition 2 (Security of G-PSI). Let $G_0$ and $G_1$ be two groups of clients, where each group contains m clients, $G_j : {C_{j,1},..., C_{j,m}}, 0 \\leq j \\leq 1$. Let L denote a leakage function, $f_{G-PSI}$ be the functionality defined above (in Relation 5 on page 4), $S = {S_{0,1},..., S_{0,m}, S_{1,1},..., S_{1,m}}$, and $S_{j,i}$ represent a set belonging to client $C_{i,i}$. Then, a protocol \u0393 securely realizes $f_{G-PSI}$ in the presence of a static semi-honest PPT adversary A, if for every A in the real model, there exists a PPT adversary (simulator) Sim in the ideal model, such that for every $C_{j,i} \\in {C_{0,1},..., C_{0, m}, C_{1,1},..., C_{1,m}}$ and TP, Relations 6 and 7 hold respectively.\n${Sim_{C_{j,i}} (S_{j,i}, \\vec{v}_{j,s})}_S \\stackrel{c}{=} {View_{C_{j,i}}^\\Gamma(S, \\emptyset)}_S$ (6)\n${Sim_{TP} (L(S), \\emptyset)}_S \\stackrel{c}{=} {View_{TP}^\\Gamma(S, \\emptyset)}_S$ (7)"}, {"title": "B. Efficient Construction of G-PSI", "content": "In this section, we introduce two efficient protocols that realize G-PSI. The first one, called EG-PSI(\u00b9), is highly efficient and based on symmetric key cryptography. The second one, called EG-PSI(\u00b9\u00b9), is based on OPRF (in turn depends on public key cryptography) and discloses less information to TEE than the former does. Thus, these two protocols trade-off between performance and leakage amount.\n1) EG-PSI: At a high level, EG-PSI(\u00b9) operates as fol-lows. Initially, each client in group $G_0$ agrees with every client in group $G_1$ on a secret key. Every client encrypts its set elements using every key it has agreed on with other clients (in a different group).\nEach client, for every encrypted set element, temporally stores a triple that includes (i) the encrypted element, (ii) the key used to encrypt that element, and (iii) the index of the client with whom the key was shared. These triples will enable the client to efficiently (a) retrieve the correct key and (b) identify the client with whom it has the element in common when presented with an encrypted element. Once all elements are encrypted using the corresponding keys, each client transmits only its encrypted elements to TEE.\nGiven the sets of encrypted elements from all clients, TEE aggregates these sets and identifies the encrypted elements that appear more than once. Subsequently, TEE forwards to a client those encrypted elements that (1) appear more than once and (2) are among the messages that the client initially sent to TEE. Upon receiving each encrypted element from TEE, a client searches its local list of triples to locate the corresponding key and the index l representing a specific client. Utilizing the key, the client decrypts the element and regards the resultant element as one of the elements within the intersection it shares with the l-th client. For a comprehensive description of EG-PSI(\u00b9), refer to Figure 1.\nThe only information that TEE learns in EG-PSI is the size of the intersection of any two clients' sets, called pair-wise intersection cardinality. Below, we formally define it.\nDefinition 3 (pair-wise intersection cardinality). Let $(S_{0,1},..., S_{0,m}), (S_{1,1},..., S_{1,m})$ be two groups $G_0$ and $G_1$ of (encrypted) sets. Then, vector s represents the pair-wise intersection cardinality: $s = [s_{0,1},..., s_{0,m}, s_{1,1},..., s_{1,m}]$, where $s_i = [[S_{j,i} \\cap S_{1-j,1}]|, ..., |[S_{j,i} \\cap S_{1-j,m}]|]$, $0 \\leq j \\leq 1$, and $1 \\leq i \\leq m$.\nDefinition 4. Let s be a pair-wise intersection cardi-nality of two groups $G_0$ and $G_1$ of encrypted sets: $(S_{0,1},..., S_{0,m}), (S_{1,1},..., S_{1,m})$, with respect to Defini-tion 3. Then, leakage function L is defined as follows: L(($S_{0,1},..., S_{0,m}), ($S_{1,1},..., S_{1,m}$)) \u2192 $s$.\nTheorem 1. Let $f_{G-PSI}$ be the functionality defined in Relation 5. Also, let L be the leakage function presented in Definition 4. If PRP is a secure pseudorandom permutation, then EG-PSI(\u00b9) (presented in Figure 1) securely realizes $f_{G-PSI}$, with respect to Definition 2.\nWe refer to Appendix A for the proof of Theorem 1.\n2) EG-PSI): In this section, we present EG-PSI(\u00b9\u00b9) which is the second variant of EG-PSI. Note that TEE in EG-PSI(\u00b9\u00b9) is able to learn the cardinality of the intersection of each pair of clients' sets. We aim to reduce this leakage in EG-PSI(\u00b9\u00b9). However, EG-PSI(\u00b9) is no longer based on symmetric key cryptography as it depends on OPRF. To the best of our knowledge, all the constructions of OPRFs are based on public key cryptography whose security depends on some hard problem assumptions. The same holds for EG-PSI(\u00b9\u00b9).\nA brief overview of the construction of EG-PSI(\u00b9\u00b9) is as follows. We have two groups of clients $G_0$ and $G_1$ having their own set that they want to find the pairwise intersection. There is a TEE that has a key k for a PRF. During the setup, each client interacts with TEE to encrypt their set using PRF(k,.) through an OPRF call. Then, each client of group $G_1$ will send their encrypted set to every client of $G_0$. Upon receiving an encrypted set from a client of $G_1$, each client of $G_0$ determines the intersection with their encrypted set. Once the client finds the intersection, it marks the index of the elements in the intersection and consider the elements with the same index in the unencrypted set as elements in the intersection. Similarly, each client of $G_0$ shares their encrypted set with each client of $G_1$ and follows the same steps. Ultimately, every client of $G_0$ knows the intersection between their set and all the sets belonging to the clients of $G_1$, and the other way around. We give a detailed description of EG-PSI(\u00b9\u00b9) in Figure 2.\nIn EG-PSI(\u00b9\u00b9), TEE does not participate in the protocol except the OPRF evaluation and that is also a one-time activity. As a result, TEE does not learn anything about the set intersection, it only learns the cardinality of each client's set.\nTheorem 2. Let $f_{G-PSI}$ be the functionality defined in Relation 5 (on page 4). Also, let L be the leakage function presented in Definition 4 with the empty output. If OPRF is secure, then EG-PSI(\u00b9\u00b9) securely realizes $f_{G-PSI}$, w.r.t. Definition 2."}, {"title": "IV. PRIVACY-PRESERVING MULTI-PARTY DEDUPLICATION (P-MPD)", "content": ""}, {"title": "A. Formal Definition of P-MPD", "content": "P-MPD considers the setting where there are n clients C = {$C_1", "follows": "n$f_{P-MPD} (S_1,..., S_m) \\rightarrow (S'_1,..., S'_m),$ (8)\nwhere $S' = \\cup_{i=1}^m S'_i = S_u$ and $S_u$ denotes the union of all initial sets and contains only unique elements.\nTo maintain generality, we define a leakage function, denoted as W. The output of this function relies on the protocol implementing $f_{P-MPD}$. We assert that a protocol securely realizes $f_{P-MPD}$ if whatever can be computed by a party in the protocol can be derived solely from its individual input and output, given the output of W. Below, we formally state it.\nDefinition 5 (Security of P-MPD). Let $S = {S_1,..., S_m}$, and S represents a set belonging to client $C_i$. Let W denote a leakage function and $f_{P-MPD}$ be the functionality defined in Relation 8. Then, a protocol \u03a8 securely realizes $f_{P-MPD}$ in"}]}