{"title": "Whom do Explanations Serve? A Systematic Literature Survey of User Characteristics in Explainable Recommender Systems Evaluation", "authors": ["Kathrin Wardatzky", "Oana Inel", "Luca Rossetto", "Abraham Bernstein"], "abstract": "Adding explanations to recommender systems is said to have multiple benefits, such as increasing user trust or system transparency. Previous work from other application areas suggests that specific user characteristics impact the users' perception of the explanation. However, we rarely find this type of evaluation for recommender systems explanations. This paper addresses this gap by surveying 124 papers in which recommender systems explanations were evaluated in user studies. We analyzed their participant descriptions and study results where the impact of user characteristics on the explanation effects was measured. Our findings suggest that the results from the surveyed studies predominantly cover specific users who do not necessarily represent the users of recommender systems in the evaluation domain. This may seriously hamper the generalizability of any insights we may gain from current studies on explanations in recommender systems. We further find inconsistencies in the data reporting, which impacts the reproducibility of the reported results. Hence, we recommend actions to move toward a more inclusive and reproducible evaluation.", "sections": [{"title": "1 INTRODUCTION", "content": "Recommender systems have become part of many people's everyday online interactions. Whether it is suggestions for what movie to watch, what items to buy, which news to read, or which restaurant to visit, these systems generally reach and serve a broad audience in their decision-making process. Adding explanations to recommender systems is said to serve a multitude of benefits. First, by providing the reasoning behind specific recommendations, explanations can mitigate issues dealing with users' autonomy that can occur when recommendations nudge or persuade users in a particular direction that is not beneficial for them [90]. Carefully designed explanations can help the user to identify when the system is \"wrong\" and provide options to correct it (see the scrutability goal of Tintarev and Masthoff [133]). Second, they can help users understand why they receive a specific recommendation, increase their trust, or allow them to make better decisions [133, 145]. The quality of recommender systems explanations is often assessed based on whether it has such effects on a user. Measuring the explanation quality with offline metrics is still an open research challenge [155]. Text-based explanations have been evaluated with metrics from the natural language processing domain. Ariza-Casabona et al. [10], for example, use BLEU-n, ROUGE-n, and BERT-S along with content repetition, explanation length, and sentence uniqueness measures to measure and compare the text quality of the generated explanations. These metrics are, however, limited to text-based explanations. Other metrics include ranking- and retrieval-based metrics, such as Recall and NDCG used in Yu et al. [153]. Both metric types have the downside that they require a ground truth to compare the generated explanations, which is often approximated through online reviews. The results from using online reviews as a ground truth need to be carefully discussed and interpreted as they can be biased [64] and might exclude certain sociodemographics and personality types [87]. Liao et al. [81] further point out that the usefulness of explanations for users is dependent on their motivation for using the system and their downstream actions, which cannot be captured by current offline evaluation metrics. Previous work found that user characteristics, such as personality, expertise, or cognitive abilities, lead to differences in the effect of explanations on a user. Several works have shown that the Big Five personality trait can impact the persuasiveness of the recommender systems explanations [8, 50, 122]. The characteristics can also impact how visual interfaces are perceived and interacted with [37, 135], or what type of transparency mechanisms for algorithmic decision-making are preferred [123]. Users' cognitive abilities, such as perceptual speed, disembodiment, visual and verbal working memory, and personal characteristics, such as personality traits, impact, for example, the effectiveness of visualizations [38, 119]. Furthermore, interfaces tailored to users' cultural backgrounds improve their satisfaction and efficiency with navigating it [116]. The recommender systems community also has a long-standing history of investigating user characteristics to improve the quality of recommendations [63] and the overall user experience (UX) [137]. However, only a few studies have been conducted in the explainable recommender systems community to investigate how different users perceive explanations [99], which is why this issue is still widely unknown. This subject matter is, nevertheless, of particular importance, as recent research [77] suggests that the content, design, and goal of the explanations for typical decision-support systems should depend on all stakeholders' needs and interests. One popular form of recruiting participants for human-subject evaluation is via crowdsourcing platforms such as Amazon Mechanical Turk (MTurk)\u00b9 or Prolific,\u00b2 who offer access to a global workforce. Research, however, has shown that their participant pool is not as diverse as advertised [41], crowd workers on MTurk being, on average, younger and with a higher income than the average population of their country. Further issues with crowdsourcing platforms were found regarding data quality and internal validity, including when measuring participants' personality [31, 46]. While several user characteristics have been identified as relevant in typical user studies, such as age, gender, geographic location, ethnicity, language proficiency, education level, expertise, political, religious, and sexual orientation, physical and mental health, and disability [14, 44, 68], only a few are actually considered when selecting study participants, namely geographic location and language proficiency [68]. Furthermore, research has shown that participant samples in human-subject experiments often come from Western, Educated, Industrialized, Rich, and Democratic (WEIRD) societies [58, 82, 127], which means that the results of these experiments only represent a subset of the overall population."}, {"title": "2 RELATED WORK", "content": "In the literature, several surveys mention or review aspects of the participants in human-subject evaluation of recom- mender systems explanations or the effects these explanations have on the user. The literature survey by Naiseh et al. [99] is probably closest to this work. They surveyed 48 publications on user needs and implementations of personalized explanations in recommender systems. The authors categorize the findings into users' motivation to interact with the system, their goals to examine the explanations, cognitive load, decision cost, and regulation compliance. They conclude that more user-based research is needed to learn more about how their perception and preferences relate to aspects such as personality, domain knowledge, and user goals. In terms of general overviews about explainable recommender systems, Zhang et al. [155] point out the need for user behavior analysis and user perspectives as recommender systems are \"inherently human-computer interaction systems\". Nunes and Jannach [108] looked at the number of participants for different study designs in the evaluation of explainable decision support and recommender systems. They also investigated the dependent variables evaluated in their corpus but did not connect these findings with the participants. Several surveys focus on the design and evaluation of recommender systems explanations. For example, Tintarev and Masthoff [134] focus their investigations on the measured effects or goals that explanations are evaluated on but do not provide insights with regard to the participants conducting the evaluations. Mohseni et al. [94] follow a similar route and analyze design goals for AI explanations. They relate these goals to the targeted user type, which they classify based on their experience into AI novices, data experts, and Al experts. Outside of the recommender systems community, Chromik and Schuessler [33] propose a general taxonomy for human-subjects evaluation for explainable AI. The participant dimension of their taxonomy includes aspects concerning the study type and design, such as the number of participants, their incentivization to participate, how they were recruited, and the participants' foresight (i.e., is the study assuming that all participants have the same knowledge about the context or can they draw from external facts, such as prior experiences). The taxonomy also includes the participants' AI and domain expertise but no other user characteristics. In contrast, Nauta et al. [102] do not focus their survey on the user-based evaluation. Still, they point out that in their corpus spanning the years 2014-2020, only one in five papers evaluate the explanations with users. Their analysis focuses on 12 properties for good explanations and how these can be evaluated quantitatively but do not include user-based evaluations. Overall, the participants in human-subject evaluation of AI explanations and the effects that explanations can have on the users have been on the radar of some surveys, but these dimensions have not been connected yet. Aside from survey papers, the impact of the Big Five personality traits on the persuasiveness of recommendations using explanations designed according to Cialdini's six persuasive principles (reciprocity, scarcity, authority, social proof, liking, and commitment) [34] has been evaluated by Alslaity and Tran [8], Sofia et al. [122], and Fatahi et al. [50]. Alslaity and Tran [8] found differences between the two evaluated domains-e-commerce and movie recommendations-within the same personality trait group for the majority of the persuasion profiles. They also found statistically significant interactions between personality traits and domain for three persuasive principles (reciprocity, liking, and scarcity). Sofia et al. [122] designed justifications following Cialdini's persuasive principles for a music recommender system to promote new artists' songs. They not only found differences in the reception of the justifications between participants with different personalities but also that the participants were not very good at assessing which justification type would be the most persuasive for them. Fatahi et al. [50] aimed to use the explanations to persuade users of movies that they were initially unmotivated to watch. They show that explanations containing influence strategies that are tailored to the respective user personality can successfully persuade them to interact with items that were previously not of interest and plan to extend the analysis to other personality traits, such as the need for cognition. These findings underline the motivation for this paper to systematically analyze which users were recruited and investigate what is known about the potential impacts of user characteristics on the measured explanation effects."}, {"title": "3 METHODOLOGY", "content": "This section describes the paper's methodology by explaining the data collection, selection, and annotation process."}, {"title": "3.1 Data Collection and Selection", "content": "We conducted a systematic literature survey of peer-reviewed papers published between 2017 and 2022 to answer the research questions outlined in Section 1. We opted to start the collection with the year 2017 as this year seems to have been the starting point of a steadily increasing number of publications in the explainability field [16, 145]. To collect the papers, we queried six library databases (see Table 1) with the following search term: (explain* OR explanation* OR interpretab* OR intelligib* OR justification OR transparen*) AND (recommender OR recommendation OR personalization OR personalized) We constructed the search term to contain words frequently used interchangeably to refer to both explainable AI and recommender systems or recommendations. The first part of the search term includes terms related to the explanation part. Given that there are currently no commonly agreed-upon definitions of AI explanations [55, 89], we included additional terms aiming to capture potentially relevant articles using a different terminology along with variants of explainability and interpretability. Some articles distinguish between justifications and explanations while others use explanation for justifications. Therefore, we opted to include justifications in the search. The variations of transparency aim to include those articles that opt for a different wording. The six library databases were selected to cover major computer science venues and interdisciplinary outlets. This initial search resulted in a total of 129 954 returned articles. In the first round, we filtered out all papers that, based on title and abstract, were clearly not related to explainable AI or recommender systems. Second, we further filtered using the exclusion and inclusion criteria (EC and IC) summarized in Table 2. Hereby, we excluded all publications that were not written in English and where we could not access the full text through the licenses of our institution (EC-1 and EC-2). Furthermore, we excluded papers with results published in another paper matching our inclusion and exclusion criteria (EC-3). We kept the extended paper in our considered set of papers. Duplicated papers that appeared in multiple databases were only considered once (EC-4). The remaining papers had to fulfill all following inclusion criteria to be selected. We included papers that propose an explainable or interpretable recommender system or an explanation generation method (IC-1), and the explanations are evaluated in a recommender systems context (IC-2). To ensure that the explanations are a focus of the papers, we only included publications providing examples or detailed descriptions of the explanations (IC-3). The final two inclusion criteria refer to the evaluation method. Our corpus includes papers in which the explanations were evaluated with a user study (IC-4), where the results were tested for statistical significance (IC-5)."}, {"title": "3.2 Data Annotation", "content": "We extracted general information on the application domain, the evaluation methodology, and the explanation that was evaluated. To analyze the first research question, we focused on the participant descriptions from the 158 user studies in the papers of our corpus. We extracted all user characteristics mentioned and, if provided, the participants' distribution. The characteristics were then sorted into demographic, personality, and experience categories as suggested by Egan [48]. Table 3 provides an overview of the extracted characteristics. To answer the second research question, we annotated the findings in the results sections of each paper. More precisely, we extracted the dependent and independent variables of each finding in which a user characteristic was part of a variable along with the outcome of the evaluation (i.e., if an effect was found or not). The dependent variables, or measured explanation effects, were then categorized in the next step. We noticed that the effects did not consistently follow the same definitions and were frequently named differently. Therefore, we referred to the evaluation task or question used for the evaluation, wherever possible (i.e., when they were made available by the authors of the papers). We categorized the explanation effects by the seven explanation goals defined by Tintarev and Masthoff [133]: effectiveness, efficiency, transparency, persuasiveness, trust, satisfaction, and scrutability. By looking at the user task design, we noticed that the papers stating to evaluate the scrutability of the system were doing so by assessing the perceived system control. We, therefore, combined the goals of scrutability and satisfaction, satisfaction being defined as \"increase the ease of usability or enjoyment\" [133], into a usability/UX category. Aside from these goals, we also observed frequent evaluations of the users' perceived explanation quality. Given that user characteristics might impact the user's perception, we added it as an additional factor to the effects we investigated. As this analysis is a first step to gaining insights into how user characteristics interact with explanation effects, we omitted results for effects specific to the recommendation problem (e.g., group recommendations) or the application domain (e.g., change in driving behavior in autonomous driving)."}, {"title": "4 RESULTS", "content": "In this section, we present the results of the data analysis to answer our two research questions. We analyze the results and participants' information from the 158 user studies conducted in the 124 papers. First, we look at the information we could extract regarding study participants. Then, we analyze the user study findings in which the effects of explanations were measured by disaggregating different user characteristics."}, {"title": "4.1 Participants", "content": "The participants in the user studies presented in our corpus were primarily recruited on crowdsourcing platforms such as MTurk or Prolific and at universities. Table 3 provides an overview of the extracted characteristics, the number of papers reporting this information about their participants, and the number of papers evaluating whether the given user characteristic impacts the explanation effect."}, {"title": "4.1.1 Demographics", "content": "As can be seen in Table 3, it is common practice to provide demographic information about the study participants. Only 18 of the 159 studies in our corpus did not disclose any demographic information about their participants. User demographics reported for most of the studies are the age (114 studies) and gender (105) of the participants. Other demographics that are less frequently mentioned are the country of residence (74), educational background (51), and ethnicity (3). Age. Participants' age is reported in multiple ways in the reviewed studies. It was either stated as average (39 studies), distribution of age brackets (16), the entire age range of all participants (13), a combination of average age and the entire range (27), the majority age range (5), a combination of the entire age range and the majority age range (9), the lower age threshold (4), or as the percentage of participants younger than an age threshold (1). Figure 1 summarizes the reported age values and depicts the diversity in reporting. Depending on the information reported in a paper, the figure uses a different visualization for each presented age distribution. The least detailed information is only a mean-average age value, visualized using a diamond symbol. This symbol can be combined with other information, such as a lower- and upper-bound, shown as a black interval. When two intervals are reported in a paper, referring to a combination of the entire range and a majority range, the former is shown as a grey interval. For papers that report a mean age and a standard deviation, Figure 1 shows a normal distribution centered around the mean, extending to +2 standard deviations, using a blue color. When, in addition, explicit lower and upper bounds are reported, the distribution is capped at these values and shown in yellow. The line segments shown in green represent explicit age brackets. Whenever no explicit lower- or upper bound is given for a range or a collection of age brackets, the figure caps the lower-most at 0 and the upper-most at 100.Since these values are not based on reported information, they are labeled 'open end' in the figure. The visualizations are sorted by (actual or inferred) lower bounds of the stated ranges. Due to these differences, it is impossible to determine the average or standard age of the participants, but we can see certain tendencies. Most user studies reported an average age between 20 and 40 years. Underage participants are rarely covered, while studies including participants above 60 are more frequent; exact participant numbers are seldom explicitly specified for the upper age range. Instead, we often find all participants above the age of a certain threshold grouped together, and it is unclear where the overall age range ends. Thill et al. [132] reported 13 of 123 participants above the age of 50, and Wilkinson et al. [149] had 13 of 310 participants above the age of 55. In both cases, we cannot say if all the participants in this age bracket were 51 or 56, respectively, or if they were significantly older. These open brackets of the oldest participants vary in their starting age as well. While most start in the early 50s, Coba et al. [35], for example, start at the age of 41, and the oldest recruited participants in Shmaryahu et al. [120] are \"above 30\". Gender. We observed that most studies (93) reported the participants' gender in a binary way. We either see a statement of the number or percentage of participants identifying as one gender (45) or the number or percentage of both male and female participants (48). Out of these, seven studies reported participants who did not disclose their gender identity. Eleven studies reported options for non-binary, diverse, self-described, non-listed, or other gender identities. These participants are in the minority, with most studies reporting under 3% of the overall participants not identifying as male or female. Regarding the balance between male and female participants, we found that 55 of the 104 studies that reported the gender had a ratio within 10% difference in participant numbers. We found 49 studies that reported a difference between male and female participants higher than 10% of the number of participants identifying with a binary gender; 30 studies had a male majority, while 19 had a female majority. One study [20] reported their gender distribution as \"comparable distribution across countries in terms of gender\" from which we were not able to determine the options they provided in their questionnaire. Location. Seventy-four studies reported the location of the participants. These 8446 participants came from 28 different countries, covering 40% of the total number of participants in our corpus."}, {"title": "4.1.2 Personality", "content": "We found 36 studies in which information about the participants' personalities was reported across 27 publications. Table 3 provides an overview of the recorded personality traits reported in our corpus. These traits, outlined below, are often evaluated using standardized questionnaires. The most frequent personality traits evaluated were the need for cognition, the so-called Big Five or OCEAN characteristics (openness, conscientiousness, extraversion, agreeableness, and neuroticism), and the participant's propensity to trust others. Need for Cognition. Need for Cognition (NFC) is the \"Tendency for an individual to engage in and enjoy effortful cognitive activities\" [25]. Eleven studies (8 papers) stated that they measured the NFC of their participants. We found that NFC is frequently evaluated by splitting the participants' scores by the median to form two groups with high and low NFC, respectively. Hence, the reported distribution of participants is approximately equal. Millecamp et al. [92] was the only paper in which the average NFC score was reported instead of the distribution. Big Five. Nine of the eleven studies that reported Big Five or OCEAN personality traits evaluated all five traits. The exceptions were Martijn et al. [88], who focused on openness, and Sun and Sundar [129], who recorded the anxiety level of their participants, which can be classified as part of the neuroticism trait. We found different approaches to report the participant distribution. Two studies stated the mean and standard deviation of the respective scores [92, 106], both studies in Abdulrahman et al. [2] provided the distribution of participants in low, medium, and high score-regions of each trait, and Martijn et al. [88] performed a median split and grouped all participants in low and high openness groups. The remaining six studies did not provide details on the distribution. Trust Propensity. Trust propensity is defined as \"Level of intensity of an individual's natural inclination to trust other parties in general\" [73]. Eight studies (8 papers) looked into this personality trait. However, we could only extract the distribution across participants for one paper. Andjelkovic et al. [9] reported that the participants were approximately evenly distributed across high, medium, and low propensity scores. Other Personality Traits. We identified six other personality traits for which no participant distributions were reported. Seven studies looked at the decision-making style of the participants. We found two different dimensions concerning participants' decision-making strategy in our corpus. Four studies from three papers classify their participants as rational or intuitive decision-makers. The remaining three studies split their participants into people who make decisions to maximize the outcome (maximizers) and satisficers who settle for the first choice that is good enough. We did not find further information on how the participants were distributed across the Social Awareness (4 studies), Personal Innovativeness (2 studies), Trust in Technology (1 study), and Valence (1 study) traits."}, {"title": "4.1.3 Experience", "content": "We identified 53 studies in which the participants' prior experience was recorded. The notions of experience that we observed can be grouped into three categories: domain experience, technical expertise, and visualization literacy. Domain Experience. Domain experience is most commonly reported in our corpus. Forty-five studies included domain experience as an aspect of describing their participants. We found that the researchers had different focus areas and notions when measuring and reporting the domain experience of their participants. The knowledge about the domain can be assessed in certain domains with standardized questionnaires. Liang and Willemsen [78] and Martijn et al. [88], for example, both assess the musical sophistication of their participants measured by the Musical Sophistication Index [98]. Others assess the domain knowledge of their participants by having them report their years of working in the field [39, 65] or self-assess their experience on a scale [42, 85]. The level of the participants' experience is also measured by asking the participants how frequently they interact with the domain. This is often the case in everyday-life recommendation scenarios such as movie or music streaming [40, 76, 120, 136] or e-commerce [28, 154]. Overall, the papers reported that their participants are familiar with the evaluated domains and interact with them regularly. This does not necessarily apply to the familiarity with a specific application. Tsai and Brusilovsky [141] reported no participant had prior experience with the application. Technical Expertise. The participants' technical expertise was assessed in 13 studies. The majority of the studies reported self-assessed technical expertise on varying scales. We additionally found that different aspects of the technical expertise were evaluated. Mishra et al. [93] and Jacobs et al. [65] asked their participants about their experience with AI or Machine Learning and reported a general familiarity with AI. Damak et al. [40] and Coba et al. [36] specifically asked about familiarity with recommender systems, while Shmaryahu et al. [120] combined both aspects and recorded if their participants have taken courses in deep learning, information retrieval, or recommender systems. These papers also reported a general familiarity with the aspects in question. Berkovsky et al. [19, 20] asked about the computer literacy of their participants. They reported that 90% of their participants have a high to very high computer literacy. Five studies did not provide information on the distribution of their participants regarding technical expertise. Visualization Literacy. The visualization literacy of the participants was recorded in eight studies. However, only Mil-lecamp et al. [91] reported the distribution of their participants between low (21 participants) and high (51 participants) visualization literacy."}, {"title": "4.2 The Impact of User Characteristics", "content": "We analyzed the findings reported in our corpus to investigate which user characteristics introduced in Section 4.1 might impact the effect of explanations. Overall, we extracted 165 findings from 31 papers in which the effect of explanations was evaluated on different user characteristics. As seen in Table 3, only a fraction of the papers that record a characteristic of their participants also report measurements on how the effect of explanations differs when disaggregating the participant data."}, {"title": "5 DISCUSSION", "content": "Overall, we observe that the data about the participants varies between the publications in terms of completeness. Table 3 illustrates that none of our analyzed characteristics were recorded by all papers in the corpus. Reporting participants' age, gender, location, education, and prior domain experience seems standard practice. However, disaggregating the data by these features to see if the evaluated effect differs between sub-groups is rare."}, {"title": "5.1 Who was recruited to evaluate recommender systems explanations?", "content": "In this subsection, we discuss our results to answer RQ 1. Our analyses demonstrate that it is challenging to aggregate the data in our corpus due to non-uniform ways of measuring and reporting it. Therefore, we cannot conclusively determine an average participant for each application domain. To the best of our knowledge, there is no related work that investigated how representative study participants are in recommender systems evaluation, which leaves us without a reference point to determine whether the reported participant data in our corpus is sampled accordingly. Instead, we focus our discussion on the characteristics reported by at least one-quarter of the papers in our corpus. We specifically discuss the participants' age, gender, location, education, and prior domain experience. Age. Regarding the participants' age, we made two main observations. First, almost all participants are above the age of 18. One possible explanation for this might be age restrictions for crowd workers. Our second observation is that the average age of the participants does not seem to reflect the average age of the overall population. In our corpus, the age averages are predominantly between the ages of 20 and 40, while the average age of the overall population, especially in the countries where the participants were recruited, is shifting towards an older average. A possible reason for this could be the recruiting method. A large proportion of the papers in our corpus recruited their participants on crowdsourcing platforms. The findings of Difallah et al. [41] support our observation, as they found that MTurk workers tend to be younger than the overall population. In their study, conducted in 2018, 60% of the workers were born after 1980, which also aligns with our findings. Studies that did not rely on crowdsourcing platforms frequently recruited participants at universities. In such cases, students often comprised most of the sample. This can be another explanation for the comparatively low age average. Gender. The difference between male and female participants was balanced in about half of the papers that reported the participant distribution for gender. For the other half of the papers, a male majority was more common than a female majority. We found very few papers report non-binary options in their gender distribution. This does not necessarily imply that these options were not available in the study, but it could also mean that all participants identified as male or female, and the possibilities with no answer were omitted from the paper. Especially with the demographic data, we rarely had access to the questions and response options given in the study. As the exact questions are not provided, no conclusion can be drawn about the options provided. Location. The results for the participants' location show a clear dominance of participants living in the USA. Again, the recruiting mechanism might impact the location as Amazon Mechanical Turk is a frequently used crowdsourcing platform in our corpus with a predominantly US population [41]. Education. The results about the education level of the participants show that the majority of the participants on which explanations are evaluated are highly educated. Participants with at least a bachelor's degree were the dominant group in our sample. The average participant has a higher education level than the average population."}, {"title": "5.2 What user groups were not represented?", "content": "After discussing what we know about the participants on which explanations are evaluated, we now focus on possible blind spots where user characteristics are underrepresented or missing. Overall, for each of our analyzed characteristics, we found many papers that did not report recording it. Therefore, we only refer to the papers in which the characteristic was reported when pointing out a gap. We cannot completely rule out that these user groups were part of the participant sample of the papers that did not disclose the information. Regarding participant diversity, the papers in our corpus did not include people from the Global South and Eastern Europe, a significant share of the population. This issue has been pointed out in multiple publications such as Linxen et al. [82], Sturm et al. [127] and is generally known in the HCI community. Given the culturally-dependent differences in user interaction [116], this may have a profound implication on the generalizability of the results to global settings. Children, teenagers, and older people were rarely part of the participant sample. People in these age groups might have different needs than younger adults regarding explanation content and display, so it would be interesting to see an evaluation of the same explanation that includes these user groups. Ekstrand et al. [49], for example, looked at the recommendation effectiveness and found that the recommendation accuracy differs when disaggregating the data by age and gender. While one might argue that children only start independently interacting with most recommender systems applications from a certain age, this might change. Furthermore, investigating explainable recommender systems in use cases where parents use them together with their children opens up a new research direction. The comparisons regarding the participants' education level mainly investigate differences between participants with an above-average education level and participants with an even higher education level (e.g., undergraduate compared to graduate students as seen in Ma et al. [84]). Participants without an academic degree are in the minority in most studies. Instead of comparing participant groups where the overall education level is high already, an evaluation of the effect of explanations on participants with an average or below-average education level would be interesting to see. Regarding gender, we found only a few studies reporting non-binary participants. As mentioned in Section 5.1, most papers did not report any gender other than male/female. Only a small fraction of our corpus reported the personality traits of their participants. However, we cannot identify user groups that might have been left out due to how these traits are reported. In most cases, we do not know anything about the participant distribution. We noticed, however, that the participants are frequently split into high and low-scoring, often based on a median split. In terms of prior experience, we mainly found tech-savvy participants with domain experience in the studies of our corpus. The technical expertise might be related to recruiting participants online, who are often experienced crowd workers, which requires a certain level of expertise. However, in real life, many recommender systems applications also target less experienced users or might not know that an algorithm selects the items they see. Although recruiting these participants might be more challenging, depending on the application domain, it could be relevant to find out how findings generalize to the general populations with all kinds of domain experience. We do not have enough information about the participant distributions of the remaining characteristics to infer user groups that were not represented. As of today, we do not know if these characteristics might impact the explanation effects, so we encourage reporting participant distributions and exploring possible impacts on explanation effects in future work."}, {"title": "5.3 Which user characteristics impact the effect of explanations in recommender systems?", "content": "In this subsection, we discuss the results of Section 4.2 and answer RQ 2. Generally, the number of findings we could extract in which the impact of user characteristics on the explanation effect was evaluated is very low. In our corpus, we have at most five studies that evaluated the same characteristic and the same effect (impact of decision-making strategy on perceived explanation quality). It is far more frequent that only one or two studies examined the same characteristic-effect combination. Therefore, it is not possible to derive significant results from our data. Results, where multiple articles either all found or did not find an impact of a user characteristic on an effect, might indicate a tendency that can be formulated into hypotheses for further experimentation. Table 4 shows the gaps the community should close and allows us to spot these indications easily. Looking at social awareness, for example, all three articles that investigated the impact on transparency and trust and both articles that looked into effectiveness found an effect. This can be taken as motivation for further research that systematically evaluates the impact of social awareness on these explanation effects. In the instances where an impact was observed in one study and not found in another, other factors than the user characteristics might have impacted the results. If we look, for example, at the impact of extraversion on the persuasiveness of the explanations, Nelekar et al. [106", "2": "did not. The work of Nelekar et al. [106"}, {"2": "focusing on participants from India. While their participants had a similar average age and education level, they were from different locations. Additionally, the participant sample in Nelekar et al. [106"}, {"2": "were predominantly female (21 male, 48 female). Both of these aspects could explain the different results. Most cases in which the explanation effects findings were inconclusive were independent studies evaluating different aspects. Looking at the impact of domain knowledge on transparency, Loepp et al. [83", "54": "did not. We do not know a lot about the participants in Loepp et al. [83"}, {"54": "who recruited researchers with at least one scientific publication. Furthermore, both papers differ in the application domain, evaluated explanations, and research goals. Loepp et al. [83"}, {"54": "investigates different detail levels of explanations in the document recommendation domain. Both studies evaluated different explanations with different modalities, complexities, and interaction features. All these aspects could have led to the differences in results, but the differences could also stem from factors that have not been considered yet. We also found, for example, ambiguous results within the same study. This can occur when there are two different studies in one paper. Coba et al. [35"}]}