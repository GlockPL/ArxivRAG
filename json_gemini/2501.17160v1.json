{"title": "A Hybrid Deep Learning CNN Model for Enhanced\nCOVID-19 Detection from Computed Tomography\n(CT) Scan Images", "authors": ["Suresh Babu Nettur", "Shanthi Karpurapu", "Unnati Nettur", "Likhit Sagar Gajja", "Sravanthy\nMyneni", "Akhil Dusi", "Lalithya Posham"], "abstract": "Early detection of COVID-19 is crucial for effective treatment and controlling its spread. This\nstudy proposes a novel hybrid deep learning model for detecting COVID-19 from CT scan images, designed\nto assist overburdened medical professionals. Our proposed model leverages the strengths of VGG16,\nDenseNet121, and MobileNetV2 to extract features, followed by Principal Component Analysis (PCA) for\ndimensionality reduction, after which the features are stacked and classified using a Support Vector Classifier\n(SVC). We conducted comparative analysis between the proposed hybrid model and individual pre-trained\nCNN models, using a dataset of 2,108 training images and 373 test images comprising both COVID-positive\nand non-COVID images. Our proposed hybrid model achieved an accuracy of 98.93%, outperforming the\nindividual models in terms of precision, recall, F1 scores, and ROC curve performance.", "sections": [{"title": "I. INTRODUCTION", "content": "In March 2020, the World Health Organization (WHO)\ndeclared COVID-19, caused by the SARS-CoV-2 virus, a\nglobal pandemic. COVID-19 is highly contagious and can\ndevelop into severe acute respiratory distress syndrome\n(ARDS), which can be life-threatening. Early detection and\ndiagnosis are critical for controlling the spread of the virus.\nThe reverse-transcription polymerase chain reaction (RT-\nPCR) test is the most commonly used method for COVID-19\nscreening. However, this technique is time-consuming, and\nstudies have noted that its sensitivity is low in the early stages\nof infection [1]. Chest imaging techniques, including X-rays\nand computed tomography (CT) scans, have also been used to\ndetect lung abnormalities associated with COVID-19.\nHowever, the accuracy of COVID-19 diagnosis using chest\nimaging relies significantly on the expertise of radiologists [2].\nRecently, several studies have investigated deep learning\ntechniques as a tool to aid in and automate the diagnostic\nprocess [3] [4] [5] [6] [7] [8][9] [10] [11] [12] [13] [14] [15]\n[16] [17] [18] [19] [20] [21].\nA CT scan generates detailed images of organs, bones, soft\ntissues, and blood vessels, providing physicians with critical\ninsights into internal structures, including their shape, size,\ndensity, and texture. Unlike conventional X-rays, which\noverlay different structures in a single image, CT scans"}, {"title": "II. RELATED WORKS", "content": "Since the COVID-19 outbreak, researchers have increasingly\nfocused on creating deep learning methods to screen for the\ndisease using medical imaging techniques such as CT scans\nand chest X-rays. We specifically explored previous research\nfocused on deep learning methods using CT scans for\nCOVID-19 detection, as our approach also utilizes CT scan\nimages. Given our focus on CT-based COVID-19 detection,\nwe analyzed prior research utilizing deep-learning\napproaches related to CT imaging to enhance our\nmethodology."}, {"title": "A. CNN AND TRANSFER LEARNING APPROACHES", "content": "Several studies have focused on leveraging CNNs,\nparticularly transfer learning, for COVID-19 detection using\nCT scans. Xu et al. (2019) [32] developed a novel deep\nlearning method using a location-attention mechanism and\nResNet architecture to automatically screen COVID-19 CT\nimages in this multi-center case study. The model achieved\nan 86.7% accuracy in classifying COVID-19, IAVP, and\nhealthy cases, showing promise as a supplementary\ndiagnostic tool for frontline clinicians [32]. He et al. (2020)\n[22] developed the COVID-19 CT dataset with 349 CT scans\nand proposed the Self-Trans approach, combining self-\nsupervised learning with transfer learning. Their method\nachieved an F1 score of 0.85 and AUC of 0.94,\ndemonstrating high accuracy in diagnosing COVID-19 with\nlimited data [22]. Wang et al. (2021) [26] proposed an\nartificial intelligence-based method using a modified\nInception transfer-learning model to diagnose COVID-19\nfrom CT images, achieving 89.5% accuracy in internal\nvalidation and 79.3% in external validation [26]. Amyar et\nal. (2020) [25] proposed a multi-task deep learning model for"}, {"title": "B. EXPLAINABLE AI APPROACHES", "content": "Some recent works have focused on explainability and\nimproving clinical acceptance through transparent AI.\nSoares et al. (2020) [27] introduced a dataset with 2482 \u0421\u0422\nscans (1252 COVID-19 positive, 1230 negative) collected\nfrom S\u00e3o Paulo, Brazil. They applied the xDNN classifier,\nachieving an F1 score of 97.31%. The xDNN model provides\nexplainable results with IF...THEN rules for early diagnosis\n[27]. Rajpoot et al. (2024) [36] proposed an ensemble\napproach combining CNN models with explainable AI\ntechniques (LIME, SHAP, Grad-CAM, Grad-CAM++),\nachieving high. Their work emphasizes model transparency\nand interpretability, bridging the gap between precision and\nclinical applicability [36]. This approach ensures that deep\nlearning models for COVID-19 detection are not only\naccurate but also interpretable by clinicians, increasing their\ntrust and adoption in clinical settings."}, {"title": "C. HYBRID AND ENSEMBLE APPROACHES", "content": "Other research has focused on hybrid and ensemble models\nthat combine multiple techniques for improved\nperformance. Mobiny et al. (2020) [23] introduced Detail-\nOriented Capsule Networks (DECAPS) for automatic\nCOVID-19 diagnosis from CT scans. DECAPS integrates\nCapsule Networks with enhancements like Inverted\nDynamic Routing, Peekaboo training, and data augmentation\nusing generative adversarial networks. The model achieves\n84.3% precision, 91.5% recall, and 96.1% AUC,\noutperforming state-of-the-art methods and experienced\nradiologists, suggesting its potential to assist in CT scan-\nbased COVID-19 diagnosis [23]. Islam et al. (2022) [37]\nproposed an ensemble model for COVID-19 CT image\nclassification, addressing the limitations of RT-PCR by\nusing CT scans for detection. They applied contrast-limited\nhistogram equalization (CLAHE) for image enhancement\nand developed a Convolutional Neural Network (CNN).\nThe extracted features were used with various machine\nlearning algorithms-Gaussian Naive Bayes (GNB), Support\nVector Machine (SVM), Logistic Regression (LR), Decision\nTree (DT), and Random Forest (RF). The ensemble model\noutperformed state-of-the-art models [37]. Kundu et al.\n(2022) [38] developed an ensemble-based framework called\nET-NET for automated COVID-19 detection using chest\nCT-scan images. Their approach employs a bootstrap\naggregating (bagging) technique, integrating three transfer\nlearning models-Inception v3, ResNet34, and DenseNet201-\nto enhance classification performance, achieving an\nimpressive accuracy of 97.73% [38]. Aversano et al. (2021)\n[39] introduced an ensemble-based approach for COVID-19\ndetection using CT scan images. By combining pre-trained\nnetworks (VGG, Xception, ResNet) optimized via a genetic\nalgorithm, the method classifies clustered lung lobe images\nusing a majority voting strategy. The ensemble outperformed\nsingle classifiers, achieving F1-scores of 0.94-0.95 on an\nintegrated dataset, demonstrating improved generalization\nand stability across diagnostic contexts [39]. Shaik et al.\n(2022) [40] proposed an ensemble-based approach for\ndetecting COVID-19 infection from chest CT scan images,\naggregating predictions from multiple fine-tuned pre-trained\nmodels such as VGG16, InceptionV3, ResNet50, Xception,\nand MobileNet. Their method leverages a composite\nensemble classifier that combines candidate model\npredictions, achieving superior results [40]. Maftouni et al.\n(2021) [41] developed an ensemble model for COVID-19\ndiagnosis using chest CT scans, combining Residual\nAttention-92 and DenseNet-121 to leverage complementary\nfeatures. A meta-learner integrates the outputs of these\nnetworks, achieving superior performance with an accuracy\nof 95.07% and ROC AUC of 96.72% [41]. De Jesus Silva et\nal. (2023) [42] proposed four ensemble CNN models using\ntransfer learning for COVID-19 detection from CT scans and\ncompared them with state-of-the-art CNN architectures.\nAfter testing 11 models, they selected DenseNet169,\nVGG16, and Xception. The ensemble of these three models,\ncalled EnsembleDVX, achieved the best results with an\naccuracy of 97.7%, precision of 97.7%, recall of 97.8%, and\nan F1 score of 97.7% [38].\nOur approach uniquely combines transfer learning using\nDenseNet121, VGG16, and MobileNetV2, followed by\nfeature extraction, dimensionality reduction with PCA, and\nclassification using SVC. Our method significantly improves\naccuracy and AUC scores for distinguishing between\nCOVID-19 and non-COVID-19 cases, which sets our\napproach apart from the studies reviewed. Additionally,\ndimensionality reduction techniques, such as PCA, are often\nnot adequately integrated into these systems, leading to\ninefficient feature representation and model performance\ndegradation. Furthermore, our approach uniquely tackles the\nchallenge of dimensionality reduction through PCA,\neffectively minimizing computational costs and reducing the\nrisk of overfitting, issues that are prevalent in many existing\nmodels, particularly in the context of COVID-19\nclassification."}, {"title": "III. METHODOLOGY", "content": "The proposed hybrid deep learning model is presented in\nFigure 1. We first normalized the images to meet pre-trained\nCNN model input requirements and applied image\naugmentation to enhance dataset diversity and reduce\noverfitting. Next, to leverage the strengths of deep learning,\nwe adopted a transfer learning approach using three pre-\ntrained CNNs: MobileNetV2, DenseNet121, and VGG16.\nThese networks were employed to extract deep features from\nthe processed CT scan images. This step is vital for effectively\ncapturing and representing both high-level and low-level\nfeatures from CT scan images, ensuring that critical image\ndetails are represented for accurate analysis and classification.\nFollowing this, to handle the high dimensionality of the\nextracted features, we applied PCA to transform the features\ninto a lower-dimensional space, retaining the most significant\nvariations in the data. This process reduced redundancy and\nnoise, improved computational efficiency, and ensured that\nthe most discriminative information was preserved for\ndownstream classification. The reduced features from all three\npre-trained networks were then stacked (concatenated)\ntogether to form a final unified feature set, combining the\ndiverse and complementary information captured by each\nmodel. Finally, the stacked features were fed into SVC to train\nthe model and perform the final classification, enabling the\neffective detection of COVID-19. In this section, we will\noutline the key components of the methodology."}, {"title": "A. DATASET", "content": "We used the SARS-CoV-2 CT scan dataset available on\nKaggle [43] (PlamenEduardo, 2020), originally collected by\nAngelov and Almeida Soares [27] (2020) from hospitals in\nS\u00e3o Paulo, Brazil. The SARS-CoV-2 CT-scan dataset consists\nof 2481 CT scans from 120 patients, with 1252 CT scans of\n60 patients infected by SARS-CoV-2 from males (32) and\nfemales (28), and 1229 CT scan images of 60 non-infected\npatients by SARS-CoV-2 from males (30) and females (30),\nbut presenting other pulmonary diseases. Data was collected\nfrom hospitals in S\u00e3o Paulo, Brazil. The dataset includes CT\nimages with varying sizes, ranging from 182\u00d7129 pixels for\nthe smallest images to 484\u00d7416 pixels for the largest. Some\nexamples of these images are shown in Figure 2. The dataset\nwas split into two sections: 85% of the images were used for\ntraining, and 15% were reserved for testing to facilitate model\ntraining and evaluation. We chose this dataset as it is from\nreal-time patients collected from multiple hospitals in S\u00e3o\nPaulo, Brazil [27]. The dataset's diversity of patient cases and\nimage sizes, along with its prior testing with various methods\n[27], makes it a well-established resource for evaluating\nCOVID-19 detection models. As a next step, image pre-\nprocessing and augmentation techniques were applied to\nenhance the quality and variability of the dataset, ensuring its\nsuitability for efficient model training and evaluation."}, {"title": "B. IMAGE PRE-PROCESSING AND AUGMENTATION", "content": "In computer vision tasks, image pre-processing is a crucial\nstep in preparing the data for model training. The pre-\nprocessing technique helps improve model performance by\naddressing various factors such as noise reduction, image\nnormalization, and resizing. These steps are particularly\nimportant for CNNs, which rely on consistent input\ndimensions and well-scaled pixel values. In our work, pixel\nintensity normalization is applied to scale the pixel values to\nthe range of [0, 1]. This normalization step is essential for\nstabilizing the training process and ensuring efficient model\nconvergence, enabling the model to learn patterns more\neffectively without being influenced by variations in image\nbrightness or contrast. Additionally, we resized the images to\nensure compatibility with the network's input dimensions.\nThis resizing process is crucial for maintaining consistency\nacross the dataset, especially when using pre-trained models\nthat require fixed input sizes. The choice of 224x224 pixels\naligns with common practices in the field, where such\ndimensions are frequently used in models like MobileNetV2,\nDenseNet121, and VGG16, ensuring efficient training and\ninference.\nWe also performed image augmentation to enhance dataset\ndiversity by simulating real-world variances, which helps in\nthe development of a more resilient model and reduces\noverfitting. Our approach involved configuring a range of\ntransformations, including rotation, width and height shifts,\nshear, zoom, and brightness adjustments. For instance, slight\nrotations and shifts were applied to allow the model to\nrecognize features from different perspectives, while\nbrightness and zoom adjustments helped the model generalize\nacross varied lighting conditions and scales. These\naugmentations ensure that the model can learn more robust\nfeatures, improving its ability to handle a variety of real-world\nscenarios and data variations. Table 1 provides a detailed\noverview of the augmentation techniques, specifying the\nparameters used for each image transformation. As the next\nstep, the prepared and augmented dataset is used to apply\ntransfer learning for fine-tuning pre-trained CNN models to\ndetect COVID."}, {"title": "C. TRANSFER LEARNING", "content": "We adopted a transfer learning approach to fine-tune pre-\ntrained CNN models, such as MobileNetV2, VGG16, and\nDenseNet121, for COVID-19 detection. These models were\ninitially trained on ImageNet, a large-scale dataset containing\n1.28 million natural images divided into 1,000 categories. By\nleveraging feature representations learned from large datasets\nlike ImageNet, this strategy significantly reduced the need for\nextensive labeled data, which is often scarce during\npandemics. It also enabled rapid adaptation to the specific task\nwhile enhancing diagnostic accuracy. Additionally, the\napproach minimized computational demands, making it\nparticularly effective in resource-constrained settings.\nTransfer learning thus provides a scalable and cost-effective\nsolution to improve COVID-19 detection, addressing\nchallenges of data scarcity and limited resources. The CNN\nmodels chosen for this study, MobileNetV2, DenseNet121,\nand VGG16, were selected based on state-of-the-art review\nand are discussed in detail in this section."}, {"title": "D. MOBILENETV2", "content": "MobileNetV2, introduced by Sandler et al. (2018) [44], is a\nCNN architecture optimized for mobile and embedded vision\napplications. It uses an inverted residual structure with\nshortcut connections between compact bottleneck layers,\nwhich helps reduce the number of parameters and improve\ncomputational efficiency. The design begins with a 32-filter\nconvolutional layer, followed by 19 bottleneck layers, which\nallow for deeper networks while minimizing the size of\nintermediate layers. MobileNetV2 is well-suited for tasks\nrequiring efficient performance, such as object detection,\nimage segmentation, and real-time inference on mobile and\nedge devices [45] [46] [47] [48] [49] [50]."}, {"title": "E. DENSENET121", "content": "DenseNet121, introduced by Huang et al. (2017) [51], consists\nof a dense connectivity design where each layer is directly\nconnected to all its preceding layers. This architecture features\na dense connection pattern that mitigates the vanishing"}, {"title": "F. VGG16", "content": "VGG16, introduced by Simonyan and Zisserman in 2014 [58],\nis a well-known convolutional neural network (CNN)\narchitecture recognized for its simplicity and effectiveness.\nThe model increases depth by stacking small 3x3 convolution\nfilters, allowing it to capture intricate patterns in images\nwithout requiring complex operations. VGG16 consists of 16\nweight layers, including 13 convolutional layers and three\nfully connected layers. The architecture starts with two\nconvolutional layers (64 filters), followed by max pooling.\nThis pattern is repeated, with the number of filters increasing\nat each layer (e.g., 128 filters in Conv_2, 256 filters in Conv_3,\nand 512 filters in Conv_4 and Conv_5), each followed by max\npooling. The network concludes with three fully connected\nlayers and a Softmax activation function. VGG16 excels at\nextracting low-level features such as edges and textures,\nmaking it highly effective for capturing these fundamental\npatterns in images. Trained on the ImageNet dataset, VGG16\nhas become a foundational model in deep learning and\ncomputer vision. It provides a strong baseline for extracting\nessential image features, making it a valuable complement to\nthe strengths of other architectures in a hybrid approach.\nNumerous follow-up research studies have demonstrated the\nmodel's utility and flexibility, leading it to be a foundational\nmodel in deep learning and computer vision research [59] [60]\n[61] [62] [63] [64] [65] [66]."}, {"title": "G. FINE-TUNING PRE-TRAINED MODELS", "content": "After selecting the three pre-trained CNN architectures, we\nimplemented a fine-tuning strategy tailored to COVID-19\ndetection for the models. By leveraging pre-trained CNN\nmodels, we retained several of their initial layers, which were\nfrozen to preserve the general features learned from ImageNet."}, {"title": "H. ENSEMBLE LEARNING", "content": "Ensemble learning, which merges the predictions of several\nclassifiers, has emerged as a robust strategy for image\nclassification, often yielding higher performance than\nindividual classifiers. In the context of image classification,\nensemble techniques typically involve combining the outputs\nof various base classifiers, such as support vector machines,\ndecision trees, and neural networks. These models leverage\nthe unique characteristics of each classifier, enhancing\naccuracy and robustness by capturing different features of the\ndata while compensating for their respective strengths and\nweaknesses. Common methods used to create ensemble\nclassifiers include bagging, boosting, and stacking. Bagging,\nor Bootstrap Aggregating, generates diverse models by\ntraining each classifier on a bootstrapped subset of the training\ndataset. On the other hand, boosting improves weak learners"}, {"title": "I. PROPOSED HYBRID MODEL", "content": "In our research, we employed a stacking approach to build the\nhybrid model. This method is particularly effective, as each\nCNN can extract distinct sets of image features, reducing the\npotential loss of important information and improving the\noverall representation. In CNNs, the initial layers capture\nsimple shapes, while the deeper layers identify complex, high-\nlevel features, making the fusion of features from different\nCNN models highly beneficial for enhanced performance.\nBefore employing the stacking technique, we performed\nfeature extraction from the three models: DenseNet121,\nVGG16, and MobileNetV2. Specifically, the final dense layer\n(the layer just before the output layer) of each model was\nutilized as a feature extractor. This approach captured intricate\npatterns and characteristics from the images. By leveraging\nthis dense layer, we effectively transferred learned features to\nenhance the COVID-19 classification task.\nThe stacking of features from multiple models leads to an\nincrease in the dimensionality of the feature space, which can\nintroduce redundancy and computational challenges. To\naddress these issues, after feature extraction, we applied PCA\nto reduce the dimensionality of the extracted features and\noptimize them for classification tasks. PCA is a statistical\ntechnique that transforms the original feature set into a smaller\nset of uncorrelated components, known as principal\ncomponents, which capture the most variance in the data. Prior\nto applying PCA, the features were standardized to ensure\nuniform scaling. This step was crucial, as it ensured that each\nfeature contributed equally to the PCA analysis, preventing\nfeatures with larger numerical ranges from dominating the\nresults. Standardization was performed by subtracting the\nmean and scaling the features to have unit variance. After this\nstep, we applied PCA to reduce the dimensionality of the data\nwhile preserving as much variance as possible, ensuring a\nmore meaningful and efficient representation of the features.\nIts linear nature also makes it computationally efficient. To\ndecide how many principal components to retain, we\nperformed an explained variance analysis with the goal of\ncapturing at least 95% of the total variance. This approach\nstrikes a balance between reducing dimensionality effectively\nand preserving crucial information, ultimately improving the\nperformance of the COVID-19 classification model."}, {"title": "J. EVALUATION", "content": "We evaluated the results of a direct transfer learning approach,\nwhich involves using pre-trained models directly without\nadditional processing, alongside our proposed hybrid deep\nlearning model. To rigorously assess all the models'\neffectiveness in binary classification for COVID-19 diagnosis,\nwe employed several key evaluation metrics: accuracy,\nprecision, recall, F1 score, the area under the ROC curve\n(AUC-ROC), and the confusion matrix. These metrics\ncomprehensively analyzed each model's ability to distinguish\nbetween COVID-19-positive and normal cases, offering\ninsights into class-specific performance and overall\nclassification strength. This thorough evaluation enabled us to\nclearly compare the direct transfer learning models and our\nproposed hybrid deep learning model, underscoring the\nadvantages of our technique in leveraging diverse feature\nrepresentations for robust COVID-19 detection."}, {"title": "K. ACCURACY", "content": "Accuracy is the simplest evaluation metric, calculated as the\nratio of correct predictions to the total predictions, providing\nan overall measure of each model's performance (Equation 2).\nIt is represented as TP representing True Positives, TN\nrepresenting True Negatives, FP representing False Positives,\nand FN representing False Negatives.\n$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$ (2)"}, {"title": "L. CONFUSION MATRIX", "content": "The confusion matrix offers a deeper view of the model's\nperformance across each class by providing counts of True\nPositives, True Negatives, False Positives, and False\nNegatives. This metric helps evaluate class-level performance\nand diagnose potential class imbalances or misclassifications."}, {"title": "M. PRECISION, RECALL, AND F1 SCORE", "content": "Precision, Recall, and F1 Score offer class-specific insights,\nparticularly for evaluating model performance on imbalanced\ndatasets. These metrics are derived from the confusion matrix\nvalues. Precision (Equation 3) is defined as the ratio of\ncorrectly predicted positive cases (True Positives) to all"}, {"title": "N. WEIGHTED AVERAGE", "content": "In classification performance metrics, weighted averages are\nused to summarize Precision, Recall, and F1 scores across\nmultiple classes, particularly in imbalanced datasets. These\naverages provide insights into model performance by\nconsidering both class imbalance and individual class\nperformance, enhancing the interpretation of results beyond\nper-class metrics. The weighted average (Equation 6) is a\nweighted mean of the metrics (Precision, Recall, F1 Score) for\neach class, where the weight is the support, or the number of\ninstances, for each class. This average takes into account the\nclass distribution, providing a more realistic view of model\nperformance in imbalanced datasets. Larger classes influence\nthe weighted average more, making it ideal for understanding\noverall model performance on the dataset as a whole. The\nweighted average is calculated by summing the metric values\nMi for each class, where Mi corresponds to the metric for the\ni-th class, and multiplying each by the support, ni, the number\nof instances in the i-th class. The sum of these weighted values\nis then divided by the total number of instances N in the\ndataset.\n$Weighted Average = \\frac{1}{N}  \\sum_{i=1}^{n} M_{i} \\times n_{i}$ (6)"}, {"title": "O. AREA UNDER THE CURVE (AUC) AND RECEIVER\nOPERATING CHARACTERISTIC CURVE (ROC)", "content": "The AUC score evaluates the model's ability to distinguish\nbetween COVID and Normal cases. It is derived from the\nROC curve, which plots the True Positive Rate (TPR) against\nthe False Positive Rate (FPR) at various classification\nthresholds. A model achieving an AUC score of 1.0 represents\nperfect discrimination, while 0.5 represents random guessing.\nThe AUC is calculated as shown in Equation 7. The ROC\ncurve demonstrates a binary classifier's diagnostic ability by\nplotting the TPR against the FPR as the discrimination\nthreshold varies. TPR and FPR are calculated as shown in\nEquations 8 and 9.\n$AUC = \\int TPR(t) d(FPR(t))$ (7)\n$TPR = \\frac{TP}{TP+FN}$ (8)\n$FPR = \\frac{FP}{FP+TN}$ (9)"}, {"title": "IV. RESULTS", "content": "We conducted the experiments using Google Colab, with CPU\nresources, 51 GB of RAM, and 225.8 GB of disk space.\nPython 3 and relevant libraries, including Scikit-Learn, Keras,\nand TensorFlow, were employed to implement the proposed\nhybrid deep-learning model. We loaded the pre-trained\nmodels, namely, VGG16, DenseNet121, and MobileNetV2\narchitectures from Keras, each initialized with ImageNet\nweights. We trained the three pretrained learning models and\nproposed a hybrid deep learning model using the 2108 COVID\nand non-COVID patient scan images. For model compilation,\nwe employed an Adam optimizer with a learning rate of 1e-4,\npaired with a binary cross-entropy loss function, which is ideal\nfor binary classification tasks. To enhance the training process,\nwe incorporated several callbacks. Early stopping was utilized\nto prevent overfitting by monitoring the validation loss and\nrestoring the best weights after a patience period of 5 epochs\nwithout improvement. Additionally, we implemented a\nlearning rate reduction strategy that dynamically adjusts the\nlearning rate by a factor of 0.5 when a plateau in validation\nloss is detected, with a minimum learning rate of le-6. Model\ncheckpointing was also integrated to save the best-performing\nmodel based on validation loss, ensuring we retain the most\neffective model after training. The training process was\nexecuted on the augmented data, with 20 epochs and a batch\nsize of 8, using the specified callbacks to optimize\nperformance and training efficiency.\nWe compared the performance of the pre-trained CNN model\nto that of our proposed model. We evaluated all these models\nusing 373 CT scan images, where 186 images are COVID-\ninfected and 187 are noninfected images, based on various\nevaluation metrics defined in the methodology section. We\ngenerated a confusion report, as shown in Table 4, for each\nmodel to evaluate its robustness by determining its accuracy,\nprecision, recall, and fl score (Table 3). Class level metrics,\ni.e., COVID and non-COVID confusion reports, are shown in\nTable 4. The confusion matrix for models is shown in Figure\n5. The ROC curve for all models is shown in Figure 6."}, {"title": "V. Discussions and Limitations", "content": "The main reason for the performance differences between\nVGG16, DenseNet121, and MobileNetV2 can be attributed to\nthe unique strengths and design principles of each architecture.\nDenseNet121 performed better than VGG16 because it has\ndensely connected layers, which allowed for more efficient\ninformation flow and enabled the extraction of intricate\npatterns from the data. On the other hand, we observed that\nMobileNetV2 achieved strong performance compared to\nVGG16 and DenseNet121 due to its efficient use of depth-\nwise separable convolutions, which reduced computational\ncomplexity while maintaining high accuracy, making it ideal\nfor resource-limited environments. Additionally, the use of\nlinear bottleneck layers in MobileNetV2 ensured the\npreservation of important image features, which is critical for\ntasks like COVID-19 detection.\nThe proposed model demonstrates substantial improvements\nin COVID-19 detection accuracy, precision, recall, and F1\nscore, as reflected in the results, where the model\noutperformed individual pre-trained CNNs. These\nimprovements can be attributed to several key factors. First is\nour image augmentation approach, which, by introducing\ntransformations such as rotations, shifts, and brightness\nadjustments, simulates real-world variances, helping the\nmodel generalize better. This exposure to diverse data\nvariations not only reduces overfitting but also allows the\nmodel to learn more discriminative features, contributing to\nthe overall effectiveness of the hybrid model. Another key\nfactor contributing to the improved performance of the\nproposed model is the combination of features extracted from\nMobileNetV2, DenseNet121, and VGG16, allowing the\nmodel to capture a diverse range of data characteristics. The\nintegration of transfer learning further strengthens the model\nby leveraging the complementary strengths of each\narchitecture: VGG16 excels in low-level feature extraction,\nDenseNet121 captures complex high-level patterns, and\nMobileNetV2 offers efficiency and scalability. This\ncombination reduces overfitting, improves classification\naccuracy, and proves particularly effective with smaller\ndatasets, as each model contributes its unique strengths to\nenhance overall performance. Additionally, the use of PCA\nresulted in improved computational efficiency by refining the\nfeature set, retaining significant variations while reducing\nredundancy.\nThe proposed hybrid model demonstrated high performance\nin COVID-19 detection. However, there are limitations that\nrequire further exploration. Our hybrid model has shown that\nusing a transfer learning approach significantly reduces the\nnumber of trainable parameters, improving computational\nefficiency compared to existing ensemble approaches for\nCOVID-19 detection. However, it also required more\ncomputational resources compared to using an individual\ntransfer learning model, such as DenseNet121 or\nMobileNetV2. In future work, we plan to focus on optimizing\nefficiency without reducing the model's performance, making\nit more suitable for deployment in resource-constrained\nenvironments. While our study successfully achieves its\nprimary goal of enhancing COVID detection, future research\nshould evaluate the performance of the proposed ensemble\napproach on larger and more diverse datasets to assess its\nscalability and adaptability. Additionally, exploring\nalternative data augmentation methods, advanced\noptimization strategies, and hyperparameter tuning could"}, {"title": "VI. CONCLUSION", "content": "The results obtained from our study demonstrate that the\nproposed hybrid deep learning model significantly improves\nCOVID-19 detection from CT scan images, achieving an\naccuracy of 98.93%. By combining transfer learning from\nthree pre-trained CNNs, namely VGG16, DenseNet121, and\nMobileNetV2, with Principal Component Analysis (PCA)\nand Support Vector Classification (SVC), the proposed\nmodel outperforms individual CNN models in terms of\naccuracy, precision, recall, F1 scores, and ROC curve\nperformance. The comparative analysis confirms that the\nproposed model exhibits superior performance with minimal\nmisclassifications. Our proposed approach can be extended\nto detect other diseases using CT scan images, leveraging the\npower of deep learning and transfer learning techniques. We\nplan to focus our future research on exploring alternative\nfeature extraction methods, fusion techniques, and advanced\nimage processing or augmentation strategies, which could\nfurther enhance the performance of the proposed model.\nFurthermore, we plan to optimize the computational\nefficiency of the proposed model in the future to enable its\ndeployment in real-world"}]}