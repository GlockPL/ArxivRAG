{"title": "Can RLHF be More Efficient with Imperfect Reward Models? A Policy Coverage Perspective", "authors": ["Jiawei Huang", "Bingcong Li", "Christoph Dann", "Niao He"], "abstract": "Sample efficiency is critical for online Reinforcement Learning from Human Feedback (RLHF). While existing works investigate sample-efficient online exploration strategies, the potential of utilizing misspecified yet relevant reward models to accelerate learning remains underexplored. This paper studies how to transfer knowledge from those imperfect reward models in online RLHF. We start by identifying a novel property of the KL-regularized RLHF objective: a policy's ability to cover the optimal policy is captured by its suboptimality. Building on this insight, we propose a theoretical transfer learning algorithm with provable benefits compared to standard online learning. Our approach achieves low regret in the early stage by quickly adapting to the best available source reward models without prior knowledge of their quality, and over time, it attains an O(VT) regret bound independent of structural complexity measures. Inspired by our theoretical findings, we develop an empirical algorithm with improved computational efficiency, and demonstrate its effectiveness empirically in summarization tasks.", "sections": [{"title": "1. Introduction", "content": "Reinforcement Learning from Human Feedback (RLHF) has achieved remarkable success in fine-tuning Large-Language Models (LLMs) to align with human preferences (Bai et al., 2022; Christiano et al., 2017; Ouyang et al., 2022). Using datasets annotated with human preferences reflecting human intrinsic reward model, RLHF optimizes LLM policies with reinforcement learning (RL) techniques. Due to the high cost of collecting large amounts of human preference labels, there has been significant attention in reducing the sample complexity-the amount of data required for training-of online RLHF through efficient exploration strategies (Cen et al., 2024; Wang et al., 2023; Xie et al., 2024; Zhang et al., 2024). However, a largely overlooked opportunity is to additionally leverage existing reward models for annotation, which have already aligned partially with the human preferences. The growing number of available open-source and high-quality reward models trained on diverse tasks provide a rich pool of candidates for transfer learning. Harnessing guidance embedded in such reward models holds great potential for improving sample efficiency.\nThere are a variety of practical scenarios where such source reward models can be effectively utilized. Firstly, reward models trained on relevant tasks often prove to be valuable in similar tasks. A notable example is cross-lingual reward transfer (Hong et al., 2024; Wu et al., 2024), where reward models in one language can provide effective guidance for tasks in another. Secondly, informative evaluation can also be obtained from well-trained LLMs, such as GPT, LLaMA and Gemini (Achiam et al., 2023; Dubey et al., 2024; Team et al., 2024). For certain tasks, such models can provide evaluation closely aligned with human preferences; see e.g., Ji et al. (2023); Lee et al. (2023). Lastly, there are scenarios where rule-based or heuristic reward functions-built upon experts knowledge and accumulated experience-are inexpensive to obtain and instructive in evaluating the LLM. Taking summarization tasks as an example, expert summaries are available on datasets such as XSum (Narayan et al., 2018) and TL-DR (V\u00f6lske et al., 2017). Similarity with those expert solutions can be measured through metrics such as ROUGE (Lin, 2004) and BERTScore (Zhang et al., 2019), and be employed for scoring the LLM generations.\nMotivated by these considerations, this paper studies how imperfect reward models can be used to learn a near-optimal policy with fewer human annotations. We consider the case where several source reward models are available, yet their quality, i.e., the similarity to human rewards, is unknown a priori. Our contributions are summarized as follows.\n\u2022 In Sec. 3, we identify a distinctive property of RLHF arising from its KL regularization: for any prospective policy candidates, its coverability for the optimal policy improves as its policy value increases. Enlightened by this, we propose two new principles for transfer learning in the context of RLHF (illustrated in Fig. 1).\nFirstly, policy value serves as a crucial criterion to select the transfer policy, since exploiting policies with high values does not conflict with exploration. Secondly, combining insights from offline RL theory, we prove that the offline policy, learned with online data generated by any no-regret online algorithm, converges to the optimal one at a rate of \u00d5(T-2) after a finite time. Such a bound improves existing sample complexity results and regret bounds in online RLHF by eliminating the dependencies on the size of the state and action space, or the complexity of the policy class. This suggests the offline policy computed with the data from the online learning process is a promising candidate to transfer from, which we term as \"self-transfer learning\".\n\u2022 In Sec. 4, following above principles, we design a transfer learning algorithm named Transfer Policy Optimization (TPO; Alg. 1) with provable benefits. At the core of TPO is the self-transfer learning and an adaptive policy selection strategy that picks transfer policies based on estimated value gaps. In the early stage, the cumulative online regret of TPO can be significantly reduced, as long as one of the source rewards is of high quality. After finite time, the selftransfer mechanism ensures the regret of TPO grows only at a rate of \u00d5(VT), independent of the standard structural complexity measures. Compared with transfer learning in the pure-reward maximization RL, our result is novel in that it exploits the policy coverage property induced by the regularization term in RLHF.\n\u2022 Inspired by the principles in TPO, we further propose an empirical algorithm in Sec. 5 with largely reduced computational overheads, making it scalable to practical RLHF scenarios. In particular, we employ an Upper-Confidence Bound (UCB) subroutine to select transfer policies with the highest win rates competing with the online learning policy. The effectiveness of our approach is demonstrated for fine-tuning T5 models on summarization tasks in Sec. 6."}, {"title": "1.1. Related Work", "content": "We only summarize the most relevant results on sample complexity and transfer RL here due to space limitation. Other closely related topics can be found in Appx. B.2.\nSample Complexity in RLHF Online RLHF emphasizes strategic exploration for sample-efficient learning in tabular and linear settings (Du et al., 2024; Novoseller et al., 2020; Pacchiano et al., 2021; Xu et al., 2020), as well as more general function approximation cases (Cen et al., 2024; Chen et al., 2022; Wang et al., 2023; Xie et al., 2024; Xiong et al., 2024; Ye et al., 2024; Zhang et al., 2024). Our work further improves sample efficiency by leveraging imperfect reward models that are readily available in a variety of practical scenarios. As an alternative, offline RLHF (Huang et al., 2024; Liu et al., 2024; Zhan et al., 2023) focuses on exploiting pre-collected datasets without exploration. What lies in between online/offline RL is hybrid RL (Chang et al., 2024; Gao et al., 2024). These methods harness online feedback, while assuming the reference policy provides good coverage and only engaging in passive exploration.\nTransfer Learning in RL and RLHF Transfer learning in pure-reward maximization RL has been extensively investigated in previous literature (Taylor & Stone, 2009; Zhu et al., 2023), and theoretical guarantees have been established under various conditions (Golowich & Moitra, 2022; Huang & He, 2023; Huang et al., 2022; Mann & Choe, 2013). Unlike previous works, this paper unveils new insights for transfer learning enabled by the KL regularization in RLHF. In particular, it enables us to design a policy-value-based transfer policy selection strategy, and identify a unique regime, i.e., \"self-transfer learning\", that can significantly improve sample efficiency. We defer more discussions to Sec. 3.2.\nMost works on transfer learning in RLHF focus on empirical approaches. For example, (Hong et al., 2024; Wu et al., 2024) investigate the cross-lingual reward transfer. To our knowledge, our empirical algorithm (Alg. 3) is novel in that it studies active transfer policy selection, which is still underexplored in existing literature. We further distinguish our work from RLAIF (Ji et al., 2023; Lee et al., 2023) or reward model selection literature (Nguyen et al., 2024). Their goal is to align LLM policies with surrogate reward models, while we study how to leverage those surrogates to accelerate the alignment with ground-truth human rewards."}, {"title": "2. Preliminary", "content": "In this section, we review the mathematical formulation of RLHF for LLMs and introduce our reward transfer setting.\n2.1. Mathematical Formulation of RLHF\nWe adopt the contextual bandits framework (Lattimore & Szepesv\u00e1ri, 2020; Ouyang et al., 2022), where we treat the prompt space as the state space S and the response space as the action space A. Without loss of generality, we assume that both |S| and |A| are finite. We denote \u03c1\u2208 \u0394(S) as the prompt distribution. An LLM can be modeled by a policy \u03c0: S \u2192 \u2206(A), where, given a prompt s \u2208 S, the responses are generated from the conditional distribution a ~ \u03c0(\u00b7|s). Throughout this paper, we assume that all considered LLM policy \u03c0 have positive support over the entire state-action space, that is, min_{s,a} \u03c0(a|s) > 0. This is ensured in practice by the softmax layer in LLM architectures.\nReward Model and Preference Model A reward model is a function r : S \u00d7 A \u2192 [0, R], where R is a constant indicating the largest possible reward value. In the RLHF setting, the reward r is unobservable and we can only access its induced preference model, denoted by Pr. Pr(y|s, a, \u00e3) denotes the probability that a is preferable to \u00e3 given s (y = 1) or not (y = 0), by reward model r. Following previous works, we consider the Bradley-Terry (BT) model (Bradley & Terry, 1952): Pr(y = 1|s, a, \u00e3) = \u03c3(r(s, a) \u2013 r(s, a')), where \u03c3(x) := 1/(1 + e^{-x}) is the sigmoid function.\nRLHF Learning Setting Given a reward model r, in the context of RLHF for LLM fine-tuning, we are interested in optimizing the following the KL-regularized objective:\n$\\pi \\leftarrow \\arg \\max_\\pi J_\\beta(\\pi;r)$ with $J_\\beta(\\pi; r) :=  \\mathbb{E}_{s \\sim \\rho, a \\sim \\pi}[r(s, a)] - \\beta KL(\\pi||\\pi_{\\text{ref}}),$  (1)\nwhere we use $\u03c0_{ref}$ to denote the pretrained reference policy and $KL(\\pi||\\pi_{\\text{ref}}) := \\mathbb{E}_{s\\sim \\rho,a \\sim \\pi( \\cdot|s)} [\\log \\frac{\\pi(a|s)}{\\pi_{\\text{ref}}(a|s)}]$. The above optimization problem yields a closed-form solution:\n$\\pi(a|s) \\propto \\pi_{ref}(a|s)e^{\\frac{r(s, a)}{\\beta}}.$ (2)\nHere \u03b2 > 0 is a moderate constant and is critical for RLHF in practice, for detailed reasons discussed in Appx. B.1.\nWe use $r^* : S \u00d7 A \u2192 [0, R]$ to denote the unknown true reward function determining the human's preference Pr*. For simplicity, we omit r* and use J\u03b2(\u03c0) as a short note for J\u03b2(\u03c0; r*). Following previous works (Xie et al., 2024; Zhang et al., 2024), we consider the function approximation setting with access to a policy candidates class \u03a0 (|\u03a0| < +\u221e) satisfying standard assumptions as follows:\nAssumption A. The policy class \u03a0 satisfies: (I) Realizability: The optimal policy \u03c0** \u2208 \u03a0. (II) Bounded Policy Ratio\u00b9: \u2200\u03c0\u2208 \u03a0, $\\max_{s,a} |\\log \\frac{\\pi(a|s)}{\\pi_{\\text{ref}}(a|s)}| \\le \\frac{R}{\\beta}$.\nAdditional Notation Given \u03a0 satisfying Assump. A, conv(II) denotes its convex hull, and RI denotes the reward class converted from II, s.t., (1) \u2200r \u2208 RI, r \u2208 [0, R]; (2) \u2203r \u2208 RI, \u03c0* = \u03c0**. We defer to Appx. B.1 for detailed converting process. Besides, we denote [n] := {1, 2, ..., n} and a \u2227 b := min{a,b}. We refer the reader to Appx. A for a table of commonly used notation in this paper.\n2.2. Reward Transfer Setting\nWe assume there are W source reward models available, denoted by $\\{r_w\\}_{w=1}^W$, s.t. $ \\forall w, s, a, r^w (s, a) \\in [0, R]$. As motivated in Sec. 1, those reward models are accessible in many scenes. Given a source reward $r_w$, let $\u03c0_w$ be the corresponding source policy and denote $\u0394(w) := J_\u03b2(\u03c0^{**}) - J_\u03b2(\u03c0^{w})$ as its policy value gap. We define $\u0394_{min} := \\min_{w \\in [W]} \u0394(w)$ to be the minimal gap for all source policies. Note that $\u0394_{min} > 0$ and $\u0394_{min} = 0$ implies $r^* \\in \\{r_w\\}_{w \\in [W]}$. We do not assume prior knowledge on $\\{\u0394(W)\\}_{w \\in [W]}$.\nLLM Policy as Reward Model Eq. (2) implies that there is a one-to-one correspondence between policy and reward model. Given a LLM policy $\u03c0(\\cdot | \\cdot)$ and an arbitrary distribution $\u03c0_0(\\cdot | \\cdot)$, $ \\beta\\log \\frac{\\pi(\\cdot | \\cdot)}{\\pi_0(\\cdot | \\cdot)}$ can be interpreted as a reward model that aligns with given $\u03c0_0$ as the reference policy (Rosset et al., 2024). Therefore, while we consistently use the term \"reward transfer\" throughout the paper for clarity, our framework is general to handle transfer learning from any LLM policy through the underlying reward function it aligns with.\n2.3. Background on Policy Coverability\nWe first introduce the formal definition of the policy coverage coefficient, which measures how well a given policy distribution covers the other.\nDefinition 2.2. Given any $\u03c0,\\tilde{\\pi}$, the coverage coefficient for $\\tilde{\\pi}$ by \u03c0 is defined by $Cov_{\\pi|\\tilde{\\pi}} := \\mathbb{E}_{s \\sim \\rho, a \\sim \\tilde{\\pi}( \\cdot|s)} [\\frac{\\pi(a|s)}{\\tilde{\\pi}(a|s)}]$.\nThe concept of policy coverage originally emerged from offline RL (Chen & Jiang, 2019; Yang et al., 2020; Zhan et al., 2022), where one aims to find a good policy given a fixed dataset. The coverage of the optimal policy by the data-generating policy naturally governs the size of the dataset required to find the optimal policy. Policy coverage was later extended to online RL, inducing novel complexity measures to characterize intrinsic statistical difficulty of the underlying MDP (Amortila et al., 2024; Xie et al., 2022).\nCompared to alternative complexity measures, policy coverage is particularly suited for studying sample complexity in RLHF, since optimizing or exploring directly on the policy (LLM) space is preferred for its computational tractability. We use coverage as an analytical tool for reward transfer, and our results are based on RPO (Liu et al., 2024) and XPO (Xie et al., 2024) for offline and online RLHF, respectively:\nLemma 2.3 (Offline RLHF; Thm. 5.3 in (Liu et al., 2024); Informal). Given a dataset D generated by a policy \u03c0D, running RPO with any R including r* and II yields \u03c0OFF, s.t., \u2200n \u2208 \u00ce\u00d1, $J_\u03b2(\u03c0) \u2013 J_\u03b2(\u03c0_{\\text{OFF}}) = \\tilde{O}(e^{2R}Cov_{\\pi|\\pi^D} |D|^{-\\frac{1}{2}})$.\nLemma 2.4 (Online RLHF; Thm. 3.1 in (Xie et al., 2024); Informal). Running XPO with II satisfying Assump. A for T steps yields a policy sequence {$\u03c0^t\\}_{t=1}^T$, s.t.  $\\sum_{t=1}^{T}J_\u03b2(\u03c0) \u2013 J_\u03b2(\u03c0^t) = \\tilde{O}(R \\cdot e^{2R} \\cdot \\sqrt{ Cov_\\infty(\u03a0)T})$.\nLem. 2.3 states that it is possible to compute an offline policy competitive with any policy well-covered by the dataset. Lem. 2.4 suggests that the sample efficiency of online RLHF can be characterized by the L\u221e coverability Cov\u221e(\u03a0).\nHere, Cov\u221e(II) by Xie et al. (2022) is a worst-case version of our Def. 2.2 as it takes the maximum over s, a and . See Appx. D for the formal definition."}, {"title": "3. The Blessing of Regularization: A Policy Coverage Perspective", "content": "Unlike classical pure-reward maximization RL, the RLHF objective in (1) incorporates regularization with respect to \u03c0ref. We start by identifying distinctive properties associated with such regularization in Sec. 3.1, and discuss their implications on transfer learning in RLHF in Sec. 3.2.\n3.1. Structural Property Induced by Regularization\nLemma 3.1. Under Assump. A, for any policy \u03c0\u2208 conv(\u03a0),\n$Cov_{\\pi^{**}|\\pi} \\le 1 + \\kappa(e^{\\frac{2R}{\\beta}}) \\frac{J_\\beta(\\pi^{**}) - J_\\beta(\\pi)}{\\beta}},$ (3)\nwhere $\\kappa(x) := \\frac{(x-1)^2}{x-1-\\log x} = O(x)$.\nThe key insight of Lem. 3.1 is that: for any prospective policy candidates \u03c0\u2208 conv(II), its coverability of \u03c0**, i.e., $Cov_{\\pi^{**}|\\pi}$, is controlled by its policy value gap. Here we consider the convex hull in order to incorporate all possible uniform mixture policies induced by II. Intuitively, $Cov_{\\pi^{**}|\\pi}$ becomes extremely large or even unbounded if there is a significant distribution shift between \u03c0 and \u03c0**. However, in the presence of regularization (\u03b2 > 0), we should only consider policies with bounded policy ratio relative to \u03c0ref (see Assump. A-(II)), and exclude those (near-)deterministic ones from our policy candidate class II, because none of them can be (near-)optimal. In other words, regularization leverages prior knowledge from \u03c0ref and enables a free policy filtration step before learning begins, ensuring that the remaining policies exhibit a favorable structure (Lem. 3.1).\nTo understand why such property is uniquely arising from regularization, consider a bandit instance with a single optimal arm and multiple suboptimal arms yields rewards R and $R - \\frac{2R}{\\beta}$, respectively. In pure reward maximization RL (\u03b2 = 0), the optimal policy \u03c0** is deterministic. A policy class II satisfying Assump. A may include several suboptimal deterministic policies. The coverage coefficient between any of them and \u03c0** is infinity, while their suboptimal gaps are $\\frac{2R}{\\beta}$ and can be arbitrarily small.\n3.2. New Insights for Transfer Learning in RLHF\nInspired by Lem. 3.1, we identify two novel principles for transfer learning in the context of RLHF in the following, which we will further explore in later sections.\nPrinciple 1: Select Transfer Policies with High Policy Value By Lem. 3.1, exploiting a policy with high value for data collection could \"help\" exploration, because such a policy inherently provides good coverage for \u03c0**. In other words, regularization reconciles the trade-off between exploration and exploitation. This insight allows us to use policy value as a criterion and transfer from the policy achieving the highest value among all candidates. This strategy is also practical given that policy values can be estimated well.\nWe emphasize that this principle is unique in the regularized setting. As exemplified by the bandit instance before, near-optimality does not imply good coverage for \u03c0** in the absence of regularization. To avoid negative transfer in pure reward maximization setting, previous algorithms typically rely on additional assumptions about task similarity and employ sophisticated strategies to balance exploiting good source tasks with exploration (Golowich & Moitra, 2022; Huang & He, 2023), which can be challenging to generalize beyond the tabular setting. In contrast, regularization enables us to filter transfer policies directly with their policy value, facilitating the applicablity beyond the tabular setup.\nPrinciple 2: Transfer from the Offline Policy-the \"Self-Transfer Learning\" We first introduce a key result by combining Lem. 3.1 and offline RLHF result in Lem. 2.3.\nTheorem 3.2. Under Assump. A, given an online dataset D generated by a policy sequence {$\u03c0^t\\}_{t=1}^T \\in conv(\u03a0), w.p. 1 \u2013 \u03b4, running RPO with conv(II), RI and D yields \u03c0OFF:\n$\\mathbb{E}_{s \\sim \\rho, a \\sim \\pi^{*}}[r(s, a)] - \\mathbb{E}_{s \\sim \\rho, a \\sim \\pi_{OFF}}[r(s, a)] \\le \\frac{2R}{\\beta}  (2R \\cdot (1+\\kappa(e^{\\frac{2R}{\\beta}}) \\cdot \\frac{\\sum_{t=1}^{T} J_\\beta(\\pi^{**}) - J_\\beta(\\pi^t))}{\\sqrt{|D|}}).$ (4)\nTo understand the significance of Thm. 3.2, consider the case when {$\u03c0^t\\}_{t\u2208[T]}$ are produced by a no-regret online learning algorithm, such as XPO in Lem. 2.4. As a result, the term"}, {"title": "4. Provably Efficient Transfer Learning", "content": "In this section, we develop provably efficient transfer learning algorithms based on the principles in Sec. 3.2.\nOutline of Main Algorithm Our main algorithm TPO, short for Transfer Policy Optimization, is provided in Alg. 1, which leverages Alg. 2 (TPS, short for Transfer Policy Selection) as a subroutine to select source policies to transfer from. TPO can be regarded as a mixture of standard online learning and transfer learning, balanced through a hyper-parameter \u03b1 \u2208 (0, 1). Motivated by the implication of Thm. 3.2, TPO returns the offline policy computed with all the data collected. For convenience, we divide the total number of iterations T into K = T/N blocks, each containing N sub-iterations. In each block, we first run aN iterations of an OnLine learning algorithm Algor, followed by (1 \u2212 a) N iterations of transfer learning with policy selected by Alg. 2. Here, Algor can be any online algorithm with per-step no-regret guarantees, for example, XPO in Lem. 2.4. To save space, we defer to Appx. D the formal behavior assumption on Algor (Def. D.2) and concrete examples with verifications.\nNow we are ready to take a closer look at the transfer policy selection steps in Alg. 2 in Sec. 4.1, after which, the provable merits of proposed approach are showcased in Sec. 4.2.\nBeyond sample complexity, a regret bound improved to \u00d5(VT) for online RLHF can be established. We defer it to Coro. 4.4 after presenting our main results."}, {"title": "4.1. Details for Alg. 2: The Transfer Policy Selection", "content": "The design of Alg. 2 follows the two principles in Sec. 3.2, which are: (1) transfer the policy with the highest (estimated) policy value, because higher policy value implies better coverage for \u03c0**; (2) include the self-transfer policy as a candidate, because it progressively converges to \u03c0** at a faster rate than the best-known ones for online policies.\nBefore diving into details, we first clarify some notation. Given a dataset D := {(s^i, a^i, \u00e3^i, y^i, \u03c0^i)}_{i\u2264|D|}, LD(r) denotes the average negative log-likelihood (NLL) loss regarding the reward model r:\n$\\mathbb{L}_{D}(r) := -\\frac{1}{|D|} \\sum_{i \\le |D|} y^i \\log \u03c3(r(s^i, a^i) - r(s^i, \\tilde{a}^i)) + (1 - y^i) \\log \u03c3(r(s^i, \\tilde{a}^i) - r(s^i, a^i)).$ (5)\nWe will use $\\mathbb{E}_{\u03c1,\u03c0}[r] := \\mathbb{E}_{s\u223c\u03c1,a\u223c\u03c0}[r(s, a)]$ as a short note. In Line 4 of Alg. 2, N(w; D) := \u2211_{i<|D|} \u0399[\u03c0^i = \u03c0^{w}] denotes the number samples collected with $\u03c0^{w}$ in the dataset, following the convention that 1/N(\u00b7,\u00b7) = +\u221e if N(\u00b7, \u00b7) = 0. In Line 6, we leverage RPO (Liu et al., 2024) to compute the self-transfer policy \u03c0OFF and a reward function rOFF. To save space, we defer the details of RPO to Appx. \u0421.\nNext, we explain our value estimation strategy. Note that in RLHF setting, we cannot access r* directly but only the preference comparison samples following the BT model. Thus, we instead estimate the value gain relative to J\u03b2(\u03c0ref).\nOptimistic Estimation for $J_\u03b2(\u03c0^{w}) - J_\u03b2(\u03c0_{\\text{ref}})$ For policies induced by imperfect source reward models, we adopt UCB-style optimistic policy evaluation to efficiently balance exploration and exploitation. Intuitively, by utilizing the MLE reward estimator $ \\hat{r}_{\\text{MLE}}$, the estimation error $ \\hat{r}_{\\text{MLE}} - r^*$ under the distribution of $\u03c0^{w}$ is related to the number of samples from $\u03c0^{w}$ occurring in the dataset. Therefore, we can quantify the value estimation error as follows.\nLemma 4.1 (Value Est Error for $\\{\u03c0^{w}\\}_{w\u2208[W]}$). Under Assump. A and Def. D.2, w.p. 1 \u2013 \u03b4, in each call of Alg. 2:\n$\u2200w \u2208 [W], J_\u03b2(\u03c0^{w}) \u2013 J_\u03b2(\u03c0_{\\text{ref}}) \u2264 \\widehat{V}(\u03c0^{w}; D) <J_\u03b2(\u03c0^{w}) - J_\u03b2(\u03c0_{\\text{ref}}) +  \\frac{e^{2R}}{\\sqrt{N(w; D)}}.$ (6)\nPessimistic Estimation for $J_\u03b2(\u03c0_{OFF})-J_\u03b2(\u03c0_{\\text{ref}})$ The main challenge in estimating the value of $\u03c0_{OFF}$ is that, $\u03c0_{OFF}$ is not fixed but changing and improving. The previous optimistic strategy is not applicable here, since the coverage of $\u03c0_{OFF}$ by the dataset is unclear, making it difficult to quantify the uncertainty in estimation via count-based bonus term. Fortunately, given that $\u03c0_{OFF}$ is improving over time, it is more important when it surpasses all the other source policies. Therefore, it suffices to construct a tight lower bound for"}, {"title": "4.2. Main Results and Interpretation", "content": "We establish the per-step regret bound for TPO below.\nTheorem 4.3 (Total Regret). Suppose Algor is a no-regret instance satisfying Def. D.2, whose regret grows as $\\tilde{O}(Re^{2R} \\sqrt{a C(II)}T)$ for any intermediate step $ \\tilde{T}$ and some policy class complexity measure $C(II)$. Then, w.p. 1 \u2013 2\u03b4, for any T/K \u2264 t \u2264 T, running TPO yields a regret bound:\n$\\sum_{\u03c4<t}J_\u03b2(\u03c0^{**})-J_\u03b2(\u03c0_{k(\u03c4),n(t)}) =  \\widehat{\\text{Reg}}^{(t)} + \\widehat{\\text{Reg}}_{Trf}^{(t)},$ (7)\n$\\widehat{\\text{Reg}}:= \\tilde{O}(Re^{2R} \\sqrt{a C(II)}t),$ (8)\n$\\widehat{\\text{Reg}}_{Trf}:= \\Big(\\sum_{\u03c4<t:\u03b1N<n(\u03c4)\u2264N}  \\frac{\u0394_{min}}{1_{k(\u03c4),n(\u03c4)}} + e^{2R} \\sqrt{(1-\u03b1)Wt}  \\sum_{\\omega:\u0394(w)>0} \u0394(w)\\Big).$\nHere we denote $k(\u03c4) := \\lfloor \\frac{\u03c4}{N} \\rfloor$ and n(\u03c4) := \u03c4 mod N to be the block index and inner iteration index for step \u03c4; $1_{k(\u03c4),n(\u03c4)} := \\tilde{O}\\Big( \\frac{R e^{2R}  \\sqrt{\\text{Cov}_{mix}^{(k(\u03c4),n(\u03c4))}}}{\\alpha} \\Big)$, where $\\pi_{mix}^{(k(\u03c4),n(\u03c4))}$  is the mixture policy up to \u03c4; \u2206(w) and $\u0394_{min}$ denote value gaps as defined in Sec. 2.2.\nThrought the proof, we follow the convention that 1/0 = +\u221e.\nProof. Since we divide the total budget T to K batches with batch size N, we will use two indices K \u2208 [K] and \u00d1 \u2208 [N] to represent the current iteration number, i.e. the N-th iteration in the K-th batch. We will divide the indices of previous iterations to two parts, depending on whether we conduct normal online learning (the first \u03b1 N samples in each batch) or do transfer learning (the rest (1 \u2212 \u03b1)N samples in each batch):\n$I_{N}:= \\{(k,n)|k < K,n \u2264 \u03b1 N, \\text{ or } k = K, n \u2264 \\tilde{N} \u2227 \u03b1 N\\},$ $I_{Trf}:= \\{(k,n)|k < K,\u03b1 N < n < N, \\text{ or } k = K, \u03b1 N < n < \\tilde{N}\\},$ $I_{K,\\tilde{N}} := I_{N}  \\cup I_{Trf} = \\{(k,n)|k < K, n \u2264 N, \\text{ or } k = K, n < \\tilde{N}\\}$.\nFor the policies generated by online algorithm, under the condition in Def. D.2, w.p. 1 \u2013 \u03b4, for any K \u2208 [K], \u00d1 \u2208 [N] we have:\n$\\sum_{(k,n) \u2208 I_{N}}J_\u03b2(\u03c0^{**}) - J_\u03b2(\u03c0_{k,n}) < COL \\frac{R e^{2R}C(II) \\sqrt{t}}{co}  \\log \\frac{\u03a0 \\tilde{T}}{\u03b4}.$ (18)\nNext, we focus on the performance of transfer policies. We first introduce a few notation for convenience."}, {"title": "5. From Theory to an Empirical Algorithm", "content": "In terms of computational overheads, TPO requires solving multiple minimax optimization problems, which restricts its applicability to fine-tune LLMs in practice. To address this, adhering to the design principles of TPO, we introduce a more computationally efficient alternative in Alg. 3.\nKey Insight: Estimating Win Rates instead of Policy Values As discussed in Sec. 4, several optimization steps are designed to estimate policy values used for transfer policy selection, because they help to identify the policies' coverability for optimal policy (i.e. $Cov_{\\pi^{**}|\\pi}$). The key insight in our empirical algorithm design is to find a more accessible indicator to infer $Cov_{\\pi^{**}|\\pi}$. This leads us to the policy win rates, i.e., the probability that human prefer the generation by one policy over another. Formally, given two policies \u03c0, \u03c0, the win rate of \u03c0 over \u03c0 is defined by: $Pr^{*}(\u03c0 > \u03c0) := \\mathbb{E}_{s \\sim \\rho, a \\sim \\pi, a' \\sim \\tilde{\u03c0}}[Pr^{*}(y = 1|s, a, a')]$.\nWin rates between two policies can be unbiasedly estimated by querying human preferences with their generated responses. Moreover, win rates can be used to construct a lower bound for $Cov_{\\pi^{**}|\\pi}$, as stated in Lem. 5.1 below."}, {"title": "6. Experiments", "content": "In this section", "baselines": "I) vanilla iterative-DPO without transfer learning; (II) purely exploiting the worst source reward; (III) purely exploiting the best source reward. Concretely, baseline (I) removes the transfer learning component in Alg. 3 by assigning \u03c0k,n = $\u03c0_{OL}^1$ for all n \u2208 [N"}]}