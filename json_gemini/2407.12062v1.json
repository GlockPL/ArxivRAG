{"title": "Enhancing Multistep Brent Oil Price Forecasting with a Multi-Aspect Metaheuristic Optimization Approach and Ensemble Deep Learning Models", "authors": ["Mohammed Alruqimi", "Luca Di Persio"], "abstract": "Accurate crude oil price forecasting is crucial for various economic activities, including energy trading, risk management, and investment planning. Although deep learning models have emerged as powerful tools for crude oil price forecasting, achieving accurate forecasts remains challenging. Deep learning models' performance is heavily influenced by hyperparameters tuning, and they are expected to perform differently under various circumstances. Furthermore, price volatility is also sensitive to external factors such as world events. To address these limitations, we propose a hybrid approach combining metaheuristic optimisation and an ensemble of five popular neural network architectures used in time series forecasting. Unlike existing methods that apply metaheuristics to optimise hyperparameters within the neural network architecture, we exploit the GWO metaheuristic optimiser at four levels: feature selection, data preparation, model training, and forecast blending. The proposed approach has been evaluated for forecasting three-ahead days using real-world Brent crude oil price data, and the obtained results demonstrate that the proposed approach improves the forecasting performance measured using various benchmarks, achieving 0.000127 of MSE.", "sections": [{"title": "1. Introduction", "content": "Time series forecasting plays a crucial role in various industries, from finance to logistics to energy, as it provides valuable insights into future trends and helps businesses make informed decisions. However, crude oil forecasting is particularly challenging due to the complex dynamics of the oil market and the non-linear and volatile nature of crude oil prices.\nIn recent years, deep learning techniques have gained prominence for their ability to capture intricate patterns in time series data. Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks have emerged as popular choices for modelling temporal dependencies in such forecasting tasks [1, 2]. Their success is attributed to their inherent ability to handle long-range dependencies, making them particularly suitable for time series data with varying temporal patterns.\nNonetheless, achieving accurate predictions remains a challenge. Individual deep-learning models typically struggle to capture all data patterns, and their performance is heavily dependent on the training configuration. The effectiveness of individual models varies depending on the given dataset and conditions. Furthermore, many external factors influence crude oil prices, including geopolitical events and global economic conditions [3, 4, 5, 6]."}, {"title": "", "content": "In other words, the capacity to enhance the capture of dataset patterns, identify external factors influencing the formation of these patterns, and select the appropriate network architecture while configuring it optimally are critical factors that significantly impact forecasting performance. The optimal choice often varies depending on the dataset's characteristics and the specific forecasting problem at hand.\nTo address these challenges, we propose a hybrid approach to combine the Grey Wolf Optimizer (GWO) with an ensemble of five deep learning networks- LSTM, GRU, CNN-LSTM, CNN-LSTM-attention, and an encoder-decoder-LSTM. GWO is a bio-inspired metaheuristic optimisation algorithm which simulates the leadership hierarchy and grey wolf hunting mechanism in nature. The objective is to make a significant contribution to the advancement of research in the field of crude oil forecasting, building upon the recognition of the challenges mentioned above and the advantages of metaheuristic optimisation and conditional forecast combination. This approach can be particularly beneficial in forecasting tasks where different models of diverse structures and data assumptions are expected to perform differently under various conditions. For example, one neural network can perform better than another in a specific dataset and vice-versa.\nMetaheuristic optimisation techniques present a powerful solution for optimal hyperparameters tuning [7, 8]. Metaheuristics excels in reaching near-optimal solutions with limited information and computational resources. These algorithms generally give better results and converge faster than blind approaches like Exhaustive Grid Search. This makes them particularly valuable in optimising deep learning methods for time series forecasting."}, {"title": "", "content": "Although some researchers have already exploited metaheuristic optimisation for tunning hyperparameters (such as epochs numbers, batch size) of ML models to improve WTI crude oil forecasting, i.e. [9], we believe this exploitation can be extended to optimise the entire process including parameters outside the ML network structures such as feature selection, defining the suitable sliding window, and to fuse multiple forecasts.\nThe main contributions of this work are summarised as follows:\n\u2022 An ensemble model to enhance the prediction accuracy of crude oil price.\n\u2022 Evaluating five different neural network (NN) models with various architectures for multi-step crude oil forecasting.\n\u2022 Identifying an optimal configuration for each model using the GWO meta-heuristic algorithm.\n\u2022 Introducing a novel method to implement weighted ensemble learning through a meta heuristic optimisation algorithm."}, {"title": "2. Related works", "content": "Methods used to forecast crude oil prices can be categorised into three primary groups: statistical, econometric, and machine learning-based approaches. Statistical methods provide a foundation for understanding and analysing historical oil price data. They are handy for identifying trends, seasonal patterns, and cyclical fluctuations in oil prices [10, 11]. Common statistical techniques employed in crude oil forecasting include time series"}, {"title": "", "content": "analysis and statistical forecasting models, such as autoregressive integrated moving average (ARIMA) [11]. Statistical methods are relatively simple to understand and implement. However, statistical methods are limited in capturing the nonlinear and unpredictable nature of oil price movements, especially in response to exogenous events. Econometric methods are grounded in statistical theory and economic principles, and they are particularly useful for understanding the causal relationships between economic variables and oil prices. Vector Autoregression (VAR) is a common econometric model used in crude oil forecasting [12, 13]. However, Econometric models often make assumptions about the underlying data distribution and the relationship between variables. Violations of these assumptions lead to less reliable forecasts.\nDeep learning techniques have become increasingly popular in the field of time series forecasting in general due to their ability to capture non-linear and more complex patterns in the data [14, 15]. Researchers have harnessed various deep learning architectures, including long short-term memory (LSTM) networks [1, 2, 16, 17], gated recurrent units (GRUs) [18, 17, 19], convolutional neural networks (CNNs) [20, 21], and transformer models [22], to forecast crude oil prices accurately.\nIn recent years, GRU (Gated Recurrent Unit) and LSTM (Long Short-Term Memory) networks have emerged as prominent choices for deep learning applications in crude oil price forecasting due to their ability to capture long-term dependencies and handle sequential data effectively. Several studies have demonstrated the superiority of GRU and LSTM networks for crude oil price forecasting [14]. To further enhance their forecasting capabilities,"}, {"title": "", "content": "researchers have explored various hybrid and combination methods, such as integrating convolutional neural networks (CNN) and attention mechanisms into GRU and LSTM architectures [20, 19, 18, 23, 24, 25, 26]. Additionally, incorporating external factors has been a focus, with studies demonstrating the effectiveness of these strategies in refining crude oil price predictions [27, 28, 25, 29].\nHowever, the efficacy of these approaches often hinges on the quality and comprehensiveness of feature engineering and the availability of extensive historical data [27], in addition to the optimal hyperparameters setup. Recently, metaheuristic algorithms have been popular for optimising machine learning models training [7, 30]. These algorithms have been employed to some extent for optimising ML models for crude oil forecasting, i.e. [9], leading to improved prediction accuracy. On the other hand, This work falls within the few research works that utilise the Brent crude oil database, as most research typically relies on the Texas crude oil database."}, {"title": "3. Preliminaries", "content": ""}, {"title": "3.1. Metaheuristic optimisation", "content": "At the core of machine learning models is the process of optimisation, wherein algorithms are trained to perform functions with optimal efficiency. Successful hyperparameter optimisation is essential for achieving precision in a model. The thoughtful selection of model configurations profoundly impacts accuracy and proficiency in handling specific tasks. However, the optimisation of hyperparameters can present a challenging undertaking. Metaheuristics have become powerful optimisation tools, thanks to their simplicity"}, {"title": "", "content": "and capability to effectively address complex NP-hard problems with practical computational resources [31, 32]. Metaheuristic optimisation algorithms are a family of mathematical-concepts-based methods used to find good solutions to complex optimisation problems. This type of algorithm provides efficient solutions in scenarios where traditional methods may struggle due to complex and dynamic environments. They are designed to explore the search space in a way that is likely to find good solutions, even if they do not always find the optimal solution. Metaheuristics are not bound by specific problem structures, do not need gradient information and can adapt to various optimisation challenges. Their efficiency in solving real-world problems with limited resources and uncertainties makes metaheuristics one of the most notable achievements of the last two decades in operations research [30, 33, 34]. Metaheuristic algorithms can be grouped into different families based on the natural phenomena they use to guide the search process, such as, for example, evolution or ant behaviour [33]. Over the past few years, population-based methods have been successfully employed to address a diverse range of real-world challenges, including COVID-19 surveillance and forecasting, cloud-edge computing optimisation, energy-efficient cloud computing, feature selection, global optimisation, credit card fraud detection, air pollution forecasting, network intrusion detection, and optimisation of various machine learning models [7]. However, the choice of an algorithm largely depends on the optimisation problem at hand. There is no agreed guideline for large-scale nonlinear global optimisation problems for how to choose and what to choose [33]."}, {"title": "3.2. Grey Wolf Optimizer (GWO)", "content": "The Grey Wolf Optimizer (GWO) is a bio-inspired swarm intelligence optimisation algorithm inspired by grey wolves, which has been introduced in [35] in 2014. This algorithm mimics the leadership hierarchy and hunting mechanism of grey wolves in nature. To simulate the leadership hierarchy in the GWO, four types of grey wolves\u2014alpha (a), beta (\u03b2), delta (\u03b4), and omega (w)-are employed. These categories simulate the leadership hierarchy, where the optimisation process is guided by three wolves (\u03b1, \u03b2, and \u03b4), while the omega (w) wolves follow. As observed in the hunting process, grey wolves tend to encircle their prey, a phenomenon mathematically modelled by equations (1) and (2).\n$\\begin{aligned} \\overrightarrow{D} &= \\left| \\overrightarrow{C} \\cdot \\overrightarrow{X_{p}}(t)-\\overrightarrow{X}(t)\\right|  \\tag{1}\\\\ \\overrightarrow{X}(t+1) &= \\overrightarrow{X_{p}}(t)-\\overrightarrow{A} \\cdot \\overrightarrow{D} \\tag{2}  \\end{aligned}$\nWhere t is the current iteration, $\\overrightarrow{A}$ and $\\overrightarrow{C}$ are coefficient vectors, $\\overrightarrow{X_{p}}$ is the vector of the prey position, and $\\overrightarrow{X}$ indicates the vector of the grey wolf position. $\\overrightarrow{A}$ and $\\overrightarrow{C}$ vectors can be calculated as illustrated in equations (3) and (4).\n$\\begin{aligned} \\overrightarrow{A} &= 2 \\vec{a} \\cdot \\overrightarrow{r_{1}}-\\vec{a} \\tag{3} \\\\ \\overrightarrow{C} &= 2 \\overrightarrow{r_{2}} \\tag{4} \\end{aligned}$\nThe elements of vector $\\vec{a}$ undergo a linear decrease from 2 to 0 across iterations, with $r_{1}$ and $r_{2}$ being random vectors within the [0, 1] range. In emulating the hunting behaviour of grey wolves, it is assumed that a (the"}, {"title": "3.3. Deep learning architectures for time series forecasting", "content": "LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) are two types of recurrent neural networks (RNNs) that have gained immense popularity in time series forecasting due to their ability to capture temporal dependencies in data effectively. These dependencies, often characterised by patterns that persist over time, are crucial for accurate forecasting. LSTM and GRU have demonstrated superior performance in various time series"}, {"title": "", "content": "forecasting tasks, ranging from energy price prediction to weather forecasting [14]. Their ability to handle complex temporal dynamics and relatively simple architecture compared to more advanced models like transformers and GANs have made them the go-to choice for many time series forecasting applications. Researchers are constantly exploring ways to enhance the forecasting accuracy of LSTM and GRU models by combining them with convolutional neural networks (CNNs) to extract spatial features from time series data, incorporating attention mechanisms to focus on relevant portions of the input sequence, incorporating relevant external factors, and integrating statistical approaches to leverage the strengths of both neural networks and traditional forecasting methods [20, 23, 24, 25, 27, 26]. In this work, we examine the following popular deep-learning architectures:\n\u2022 Bi-LSTM, which is a bidirectional Long Short-Term Memory (Bi-LSTM) model; one LSTM layer followed by two full connected layers with relu activation function.\n\u2022 Bi-GRU, is a bidirectional Gated Recurrent Unit (Bi-GRU) model; one GRU layer followed by two fully connected layers with a relu activation function.\n\u2022 CNN-Bi-LSTM, This model combines two convolutional neural networks (CNNs) to extract input features and two LSTM layers for learning temporal dependencies in the data, followed by a fully connected layer with relu activation function.\n\u2022 CNN-Bi-LSTM-Attention, which adds an attention technique for the previous one.\n\u2022 Encoder-decoder-Bi-LSTM is an encoder-decoder architecture with two LSTMs with 128 units for the encoder and another LSTM with 200 units for the decoder.\nTo sum up, We train five models to predict three ahead-days, using the GWO metaheuristic optimisation to optimise training hyperparameters and incorporating two external factors (USDX and SENT) as described in the dataset section."}, {"title": "4. Proposed approach", "content": "In this work, We propose to combine the Grey Wolf Optimizer (GWO) with an ensemble of five deep learning networks- LSTM, GRU, CNN-LSTM, CNN-LSTM-attention, an encoder-decoder-LSTM for enhancing Brent crude oil price forecasting."}, {"title": "4.1. Multi-aspect metaheuristic optimisation", "content": "The Grey Wolf Optimization (GWO) algorithm is employed at four different stages to optimise crucial aspects of the ensemble forecasting process: feature selection, sliding window size determination, model hyperparameter tuning, and final forecast blending. This systematic approach ensures that the ensemble model leverages the strengths of all constituent models and produces highly accurate predictions"}, {"title": "4.1.1. Feature selection", "content": "Forecasting crude oil prices is significantly influenced by external factors, which play a pivotal role in shaping the market dynamics. Factors such as geopolitical events, economic indicators, global demand-supply imbalances, and environmental policies can substantially impact oil prices [29]. Understanding and incorporating these external factors into forecasting models is crucial for accurate predictions, as they contribute to the volatility and unpredictability of the oil market. USD index rate and sentiment analysis"}, {"title": "", "content": "index are two popular external variables that have been widely applied by researchers in the tasks of forecasting crude oil price [6, 26, 4, 38]. The USDX index, reflecting the performance of the U.S. dollar against a basket of major currencies, offers valuable information on currency strength and global economic conditions. Considering the substantial influence of currency fluctuations on oil prices, the USDX index is a crucial indicator. Simultaneously, sentiment analysis, which gauges market sentiment from news and social media, contributes to understanding market psychology and potential price movements. Integrating eternal factors enhances the model's ability to capture complex relationships and dynamic market influences, consequently improving the accuracy of crude oil price forecasts [27, 28].In this work, we examined integrating both indices, USDX and sentiment score, individually and combined."}, {"title": "4.1.2. Determination of sliding window size", "content": "The determination of the sliding window size plays a pivotal role in time series forecasting. This parameter represents the temporal scope of historical data used to train a model, influencing its ability to capture patterns and trends. Selecting an optimal sliding window size is a delicate balance between incorporating sufficient historical information for meaningful pattern recognition and avoiding overfitting or loss of relevance. A smaller window may lead to overlooking essential patterns, while a more significant window might introduce noise or outdated information. Thus, carefully considering and optimising the sliding window size is crucial to ensure the trained model learns from past data effectively, contributing to more accurate and reliable crude oil price forecasts."}, {"title": "4.1.3. Tuning of model hyperparameters", "content": "Hyperparameter tuning, the process of carefully adjusting the settings of a deep learning model, is crucial for optimal performance of deep learning networks. Finding optimal hyperparameters (such as learning rate, optimiser, hidden and unite number) significantly impacts training efficiency and model accuracy. However, tuning these hyperparameters is challenging due to their effects' interplay and the search space's high dimensionality. Manual exploration can be time-consuming and error-prone, while automated techniques like grid search and Bayesian optimisation can be computationally expensive. The emergence of metaheuristic algorithms like genetic algorithms and particle swarm optimisation offers promising avenues for efficient hyperparameter tuning. These algorithms can explore the search space more effectively and efficiently, leading to better-performing models."}, {"title": "4.1.4. Blending of final forecasts", "content": "Ensemble forecast methods can enhance accuracy by combining forecasts of diverse models that capture different aspects of the data. This often leads to better generalisation and robustness. However, the success of the ensemble methods depends on the individual models' diversity and quality and the nature of the data. In practice, ensemble methods are known to provide performance improvements in many cases, but there are situations where individual models might perform equally well or even outperform an ensemble. Variant-Weight technique is a well-established method for ensemble forecasting, in which multiple models are combined by assigning distinct weights to their respective predictions [24, 23]. The success of this method hinges on identifying the optimal weight allocation for each model. To this end, we"}, {"title": "", "content": "employ GWO to optimise the weight distribution for our target models."}, {"title": "5. Experiments", "content": "This section outlines the dataset, details the experimental setup, and delves into the obtained results."}, {"title": "5.1. Dataset", "content": "The dataset used for the experiments consists of the daily observations of the Brent crude oil price for the period from (January 3, 2012, to April 1, 2021), obtained from Bloomberg, including the period where the COVID-19 pandemic has affected energy and stock markets. It also includes Two external factors: the daily USD closing price (USDX) from Bloomberg and a daily cumulative sentimental score (SENT) obtained by the CrudeBERT model described in [39]. CrudeBERT is a variant of FinBERT that has been fine-tuned towards assessing the impact of market events on crude oil prices, focusing on frequently occurring market events and their impact on market prices according to Adam Smith's theory of supply and demand. Mainly, CrudeBERT dataset used headlines originating from 1034 unique news sources, published on the Dow Jones newswires (approx. 21,200), Reuters (approx. 3,000), Bloomberg (approx. 1,100), and Platts (approx. 870). The sentimental score data is described and publicly available in [39]. The period from 2012 to 2021 was chosen for analysis due to the availability of sentiment scores during this time frame. Figure 3 displays the time-series plot of the daily closing price of Brent for this period. The dataset has been cleaned, normalised, and then split as follows: 80% for training, 10% for validation, and 10% for testing."}, {"title": "5.2. Experimental setup", "content": "As mentioned earlier, our experiments are conducted in two stages. In the first stage, the objective is to identify optimal values for hyperparameters of each model (optimal configuration). The targeted models are described in Section (3.3), and the targeted hyperparameters are described in Table (1)."}, {"title": "", "content": "Subsequently, we train and evaluate each model individually using its corresponding optimal configuration. The forecasts obtained from each model serve as inputs for the second stage. In the second stage, we aim to optimally blend the forecasts obtained individually from each model in the first stage to produce the final forecast.\nIn order to determine the optimal configuration for each model, the Grey Wolf Optimization (GWO) has been executed through five independent runs for each network. The objective function is specifically designed to minimize Mean Squared Error (MSE) loss, aiming to attain the best global solution. The best global solution is simply a vector containing the optimal values identified for the targeted hyperparameters, corresponding to the smallest Mean Squared Error (MSE) at these values.\nGenerally, implementation of the GWO metaheuristic optimisation involves defining the main following procedures: problem preparation, objective function definition, population initialisation, and optimisation process. Firstly, the problem preparation involves specifying the hyperparameters to be optimised and establishing their respective ranges. The objective function is a pivotal element, representing the metric that the metaheuristic aims to either maximise or minimise during the optimisation process. This function typically encapsulates the performance of the deep learning model, such as accuracy or loss. The population in the context of metaheuristic optimisation refers to a collection of candidate solutions simultaneously. The metaheuristic explores and refines this population throughout the optimisation iterations to iteratively improve the model's performance. A larger population generally provides more diversity and allows for a more thorough"}, {"title": "5.3. Objective function and evaluation metrics", "content": "For each model, the GWO has been run multiple independent runs with an objective function to minimise the Mean square error (MSE), which can be calculated according to the equation (5). However, to evaluate the final ensemble and individual models, we used the traditional ML measurements: mean square error (MSE), root mean square error (RMSE) - equation (6), and mean absolute error (MAE)- equation (7):\n$\\text{MSE}(y, \\hat{y}) = \\frac{\\sum_{i=0}^{N-1} (y_i - \\hat{y}_i)^2}{N} \\tag{5}$\n$\\text{RMSE}(y, \\hat{y}) = \\sqrt{\\frac{\\sum_{i=0}^{N-1} (y_i - \\hat{y}_i)^2}{N}} \\tag{6}$\n$\\text{MAE}(y, \\hat{y}) = (\\frac{1}{n}) \\sum_{i=1}^{n} |y_i - \\hat{y}_i| \\tag{7}$\nwhere y indicates to the actual value, while $\\hat{y}$ is the predicted value."}, {"title": "5.4. Weighted ensemble model", "content": "Inspired by the advantages of blending forecasts using conditional methods, we examine a meta-heuristic strategy for blinding the forecasts obtained from individual models. The weighted ensemble method in forecasting involves a mathematical approach to combine the predictions of multiple individual models to improve overall accuracy and robustness. Each model in the ensemble contributes to the final forecast with a certain weight. The final ensemble forecast is then obtained by summing the products of each model's forecast with its corresponding optimal weight. Mathematically, this can be represented as the weighted sum:\n$\\text{Weighted ensemble prediction} = \\sum_{i=1}^{n} M_i W_i \\tag{8}$\nHere, $W_i$ represents an optimal weight assigned to $model_i$, where 0 < $W_i$ \u2264 1, and $\\sum W_i$ = 1. While $M_i$ represents $model_i$ forecasts.\nIn this experiment, a dynamic weighted ensemble method was employed to blend the predictions of the top five forecasts obtained from the SENT-Bi-GRU, SENT-Bi-LSTM, SENT-CNN-Bi-LSTM, SENT-CNN-Bi-LSTM-att, and SENT-USD-encoder-decoder-LSTM. The weights metric W was optimised using the Grey Wolf Optimization (GWO) metaheuristic algorithm. The experiments showed that this metaheuristic-calibration-based weighted ensemble method achieves better results than traditional method or other naive ensemble methods and that the ensemble model outperforms the individual models, achieving a mean square error of (0.000165), with a success"}, {"title": "6. Results and discussion", "content": "In this section, we first present the optimal configuration obtained for each model identified by the GWO algorithm. Then, we discuss the accuracy of the forecast achieved by blending the best forecasts and evaluate the performance of the ensemble model by comparing the MAE, MSE and RMSE values to the individual model's selected benchmark models."}, {"title": "6.1. Models optimal configurations", "content": "Table 2 shows the best solution found by GWO for each selected model. The first column (MSE) shows the best MSE achieved by each model, while the other columns show the corresponding optimal configuration values. The results obtained from the GWO-optimised five benchmark models, in Table 2, showed that the best-performing model is based on the Bi-GRU architecture when incorporating the sentimental score (SENT-Bi-GRU), with the best MSE of 0.000137. The corresponding optimal configuration identified in this model is AdamW as the optimiser with a sliding window of 5 lagged steps, 0.003154571 as the learning rate, 0.0399269013 as the dropout value, and 2 hidden units. Integrating the sentimental score index (SENT) generally gives better results than integrating the USDX index. While combining both factors improved the results only with the encoder-decoder-LSTM architecture. The optimal values for sliding windows are (5, 6, 10, 17). These figures align consistently with typical workdays, as 5 corresponds to the number of working days a week."}, {"title": "6.2. GWO-ensemble model accuracy evaluation", "content": "As shown in the table, the GWO-ensemble model outperformed the other models, achieving a 0.000127 of MSE. The best final forecast has been acquired by blending the forecasts obtained from SENT-Bi-GRU, SENT-Bi-LSTM, SENT-Bi-CNN-LSTM-attention, SENT-encoder-decoder-LSTM and SENT-USDX-encoder-decoder-LSTM models. The optimal weights assigned for each model are illustrated in Table (3).\nFigure 5 illustrates the forecasting performances for all targeted models. The figure shows the plot chart of Actual vs. predicted results. Table (4) shows the overall comparison of the GWO-ensemble model with the other benchmark models."}, {"title": "7. Limitations", "content": "Although metaheuristic optimisation algorithms effectively tune deep learning models, they require significant computational resources. This limitation has forced us to restrict our hyperparameter search to a limited number of parameters. However, expanding the problem dimension to include a broader range of parameters could improve forecasting performance. Suppose we need to run GWO 20 independent runs for the Bi-LSTM model with 30 epochs and a population size of 10. Each Bi-LSTM model will be trained 6,000 times (20 runs * 30 epochs * 10 population size)."}, {"title": "8. Conclusions", "content": "In this study, we have introduced a hybrid approach that combines deep learning networks with metaheuristic optimisation to enhance the forecasting of Brent crude oil prices. The main idea is to harness the capabilities of metaheuristic optimisation to identify optimal configurations at various stages of constructing a forecasting solution, alongside the power of combining forecasts with multiple deep learning models. The well-known Grey Wolf Optimizer (GWO) algorithm was employed to optimise feature selection, dataset preparation, tuning hyperparameters for five individual deep learning networks, and the fusion of their forecasts. The results demonstrated that this approach improved the accuracy for each model and that the presented ensemble model outperforms benchmark models and achieves an MSE of 0.000127."}]}