{"title": "Script-Based Dialog Policy Planning for LLM-Powered Conversational Agents: A Basic Architecture for an \u201cAI Therapist\"", "authors": ["Robert Wasenm\u00fcller", "Kevin Hilbert", "Christoph Benzm\u00fcller"], "abstract": "Large Language Model (LLM)-Powered Conversational Agents have the potential to provide users with scaled behavioral healthcare support, and potentially even deliver full-scale \"AI therapy\" in the future. While such agents can already conduct fluent and proactive emotional support conversations, they inherently lack the ability to (a) consistently and reliably act by predefined rules to align their conversation with an overarching therapeutic concept and (b) make their decision paths inspectable for risk management and clinical evaluation\u2014both essential requirements for an \"AI Therapist\".\nIn this work, we introduce a novel paradigm for dialog policy planning in conversational agents enabling them to (a) act according to an expert-written \u201cscript\u201d that outlines the therapeutic approach and (b) explicitly transition through a finite set of states over the course of the conversation. The script acts as a deterministic component, constraining the LLM's behavior in desirable ways and establishing a basic architecture for an AI Therapist.\nWe implement two variants of Script-Based Dialog Policy Planning using different prompting techniques and synthesize a total of 100 conversations with LLM-simulated patients. The results demonstrate the feasibility of this new technology and provide insights into the efficiency and effectiveness of different implementation variants.", "sections": [{"title": "1 Introduction", "content": "Large Language Model (LLM)-based conversational agents have gained in popularity in recent years, and have demonstrated exceptional proficiency in understanding context and generating responses across various domains (Deng et al. 2023a).\nFollowing their popularity, there is increased interest in academia and industry in using such agents in the behavioral health domain, including assisting therapists and conducting therapy. For instance, Stade et al. (2024) describe the most advanced form of a clinical LLM as an autonomous psychotherapy AI, which we further call \u201cAI Therapist\u201d. Such an LLM agent might feature all aspects of traditional therapy, including conducting comprehensive assessments, selecting appropriate interventions, and delivering a full course of therapy; all without oversight of human providers.\nThis clearly has the potential to address insufficient capacity of mental healthcare systems around the world, and provide more individuals with access to personalized treatment.\nHowever, psychotherapy is a highly complex and high-risk domain; interventions considerably affect patient's health, and require properly handling risk of harm to oneself and others. A full-fledged AI Therapist thus poses considerable ethical questions. With respect to the technical development, LLMs seem a well-suited foundation, but additional technologies may be necessary to ensure sufficient effectiveness and safety of such a system.\nOur aim is to contribute to the development of an effective and safe LLM-based AI Therapist by proposing a basic LLM-based architecture that meets its general requirements.\nIn Section 2, we define five key requirements for an AI Therapist agent and derive technical methods well-suited to implement those requirements drawing from recent literature. We find that our requirements are fulfilled by LLM-Based Proactive Conversational Agents complemented and constrained by a deterministic component inspired by hand-crafted chatbots: A \u201cscript\u201d outlining the conversational flow and instructions of the agent. We introduce our design of the script and the associated technique we call \"Script-Based Dialog Policy Planning\".\nIn Section 3, we propose two specific implementation variants for the AI Therapist powered by Script-Based Dialog Policy Planning based on an exemplary script and patient cases. We also suggest evaluation metrics suitable for comparing the efficiency and effectiveness of both variants.\nWe synthesize 100 dialogs between our AI Therapists and LLM-simulated patients, and outline the results in Section 4. We conclude the general feasibility of our novel approach and discuss the strengths and weaknesses of the design decisions in our implementation variants."}, {"title": "2 Method", "content": ""}, {"title": "2.1 Requirements for an AI Therapist", "content": "We argue that an AI Therapist needs to fulfill the following five requirements.\nThe first two are inspired by recent investigation on proactivity of emotional support agents by Zheng et al. (2023); Deng et al. (2023a) and others."}, {"title": "(1) Conversational Fluency", "content": "Firstly, an AI Therapist needs to be able to hold a \"natural\" conversation, i.e. understand and remember context, as well as generate natural and high-quality responses, including reactions to various user utterances, even highly unexpected ones. We call this conversational fluency."}, {"title": "(2) Proactivity", "content": "LLMs are originally trained to passively follow users' instructions, and prioritize accommodating users' intentions (Deng et al. 2023c).\nWe argue that an AI Therapist needs to act proactively, i.e. be able to take the initiative and control the conversation (Jurafsky and Martin 2024).\nThe next three requirements are directly based on the guidelines for the responsible development of clinical LLMS proposed by Stade et al. (2024)."}, {"title": "(3) Expert Development", "content": "Psychotherapists and behavioral health experts should be directly involved in the development of such agents, i.e. be able to explicitly define the agent's functionality."}, {"title": "(4) Application of Evidence-Based Practices", "content": "The agent should apply evidence-based treatment practices (EBPs) only, with close adherence to its defined functionality and safeguarding practices for expectable risks. Beyond the specific interventions, this includes aligning the current conversation with the requirements of the overall therapeutic rationale, and embedding the current conversation in a larger context that includes a series of past and future conversations."}, {"title": "(5) Inspectability", "content": "Since inspectability and explainability of the agent's behavior are clearly essential for risk management and clinical evaluation of such agents, we need an available trace through some pre-defined (finite) set of possible system states, including decisions made to move from one state to another."}, {"title": "2.2 LLM-Based Proactive Conversational Agents", "content": "LLM-Based Conversational Agents are widely regarded as state-of-the-art technology to fulfill the requirement of (1) conversational fluency (Deng et al. 2023c). The ability to (2) proactively steer the conversation towards specific goals, and thus tackle complicated tasks including strategic targets and motivational interactions requires LLM-Based Proactive Conversational Agents (PCA) investigated in (Wu et al. 2019), (Deng et al. 2023a) and other recent works.\nImplementation Approaches There are two general approaches of implementing such agents (Cheng et al. 2024):\n(1) Corpus-Based learning approaches train the agent on a given set of data to predict the next dialog act. They rely on large datasets and/or human annotation, and are considered as inferior in optimizing for the long-term goal of the conversation. Such approaches are extended with planning techniques such as knowledge graphs (Yang et al. 2022) and stochastic processes (Wang, Lin, and Li 2023). However, they do not suffice for our requirement for explicit formulation and following of expert instructions.\n(2) Prompt-Based approaches prompt an LLM to conduct self-thinking for planning on each turn. They are based on explicit instructions and/or examples inside prompts. We further elaborate on such prompting techniques below."}, {"title": "Proactive Dialog Policy Planning", "content": "The agent's underlying strategy for deciding on the next act is referred to as dialog policy (Harms et al. 2019). And the process of deciding what actions the dialog agent should take to achieve the specified goals during the conversation is called dialog policy planning (DPP) (Deng et al. 2023c)."}, {"title": "Proactive Prompting Techniques", "content": "Multiple proactivity prompting techniques for LLM-Based PCAs have been proposed and investigated in recent works, for instance:\n\u2022 (Basic) Proactive Prompting provides multiple options to the LLM to decide what act to take, and then generate a response based on the selected act (Deng et al. 2023a).\n\u2022 Proactive Chain-of-Thought (ProCoT) is an extension of Proactive Prompting in which the LLM is instructed to perform one or multiple intermediate analysis steps as \"thoughts\" before deciding on one of the given dialog acts and generating a response (Deng et al. 2023b).\n\u2022 Ask-an-Expert prompting scheme is involving another LLM actor, the \"expert\", which on each turn is given the context of the conversation and asked to answer one or multiple questions, helping the main LLM to generate its response to the user (Zhang, Naradowsky, and Miyao 2023).\nFurther approaches are Cue-Based Chain-of-Thought (Wang et al. 2023), Tree-of-Thought (Yao et al. 2023), Graph-of-Thought (Besta et al. 2023), RePrompting (Chen, Koenig, and Dilkina 2024), and others. We can generalize that all proactive prompting approaches run through a similar set of steps on each turn of the conversation:\n1. (Optional) Reasoning and planning steps to prepare the decision and response generation\n2. Deciding on the next act from a given set of options\n3. Generating the response to the user\nWhile step 3 is always performed by the main \"Dialog LLM\", steps 1 and 2 can either be performed by the Dialog LLM (like at ProCoT) or by an additional \u201cActor LLM\" (like at Ask-an-Expert).\""}, {"title": "2.3 Script-Based Dialog Policy Planning", "content": "The latter three requirements - (3) expert development, (4) application of evidence-based practices, and (5) inspectability - demand the following criteria:\n\u2022 There needs to be a set of rules defined by experts,\n\u2022 the agent needs to reliably and constantly follow them,\n\u2022 the agent's decisions and acts must be explicitly and explainably selected from some finite set.\nLLMs are primarily acting based on their training data and the user's immediate requests, i.e. not by explicit expert-defined rules. They also have infinite options for their decisions and acts, with no explainability of the decision process. However, the above criteria happen to be covered by traditional handcrafted / rule-based dialog management approaches used to build chatbots that act deterministically upon a set of rules defined by their developers, and are modeled e.g. as finite-state machines (Harms et al. 2019).\nScript for Policy Planning We introduce a component inspired by the handcrafted dialog management approach: The \"script\", which is a natural language piece of text outlining two kinds of content:\n(1) Conversational Flow of the agent defining a finite set of states for the agent as well as rules on when and how to transition from one state to another, and\n(2) State Instructions, i.e. the instructions the agent should follow in each of the states.\nHence, we arrive at a hybrid dialog management approach, which is known for combining advantages of both, the probabilistic and the rule-based approaches (Harms et al. 2019). Compared to the hybrid DM approaches investigated by Pande, Martin, and Pimmer (2023) and Kelley (2024), we start with a data-driven LLM agent to ensure a high degree of conversational fluency, and add a rule-based component to constrain it.\nThe script can be written and iterated in natural language by domain experts with no background in software development. A JSON-formatted script like in our example below might be easier to interpret for the software, but is not necessary as LLMs can be asked to interpret unformatted text. The concept of a script is supported by the fact that there are a large number of psychotherapeutic manuals, which in a sense serve as prompts for human therapists.\nInjecting Instructions via Prompts Due to the lack of existing therapy transcripts, implementing a corpus-based approach would require a costly process of translating the script into human-written or synthesized dialogs, and training the LLM based on those. At the same time, in a corpus-based approach, it is more difficult to identify which state the agent is in, and check its adherence to the script. (Deng et al. 2023c)\nOn the other hand, prompt-based approaches are well-suited to directly source instructions from the script and provide a higher degree of control over the behavior of the LLM agent. They allow for explicit selection of dialog acts by the agent as shown in the prompting techniques listed above. Therefore, we decide for an approach injecting our script instructions via prompting.\nSection-Level Instructions As discussed above, proactive prompting techniques from recent literature involve selection of the next act and generation of the response to the user in each turn.\nWe assume that providing new instructions to the Dialog LLM on each turn would compromise its ability to develop a natural conversation and to react to unforeseen user utterances. We argue that the Dialog LLM needs visibility to a larger context of instructions and some degree of freedom to do the following:\n\u2022 Choose in what order to fulfill the instructions,\n\u2022 skip a part of the instructions that's already completed,\n\u2022 follow user utterances and temporarily deviate from its instructions, then return to completing them.\nWe therefore propose prompting the Dialog LLM with a set of instructions larger than a single-turn act, allowing it to take multiple turns to complete its instructions while possessing the above degrees of freedom. We define the unit of instructions provided to the Dialog LLM at once as a \"section\" of the script, each section consisting of a series of \"tasks\" and representing a state the agent can be in. I.e. during runtime, the agent repeatedly moves to a state and gets a section of instructions assigned, then takes one or multiple turns to complete those, and subsequently moves to the next section.\nExamples illustrating the need for section-level instructions can be found in the technical appendix of this paper.\nCompleting Section Step The section-level instructions scope requires an additional step on each turn: Assessment on whether the current instructions have already been completed by the agent or not, i.e. if the agent should transition to the next state or remain in the current one. Therefore, our Script-Based Dialog Policy Planning involves the following steps performed by the agent on each turn:\n1. Assessing whether the current instructions have been completed\n2. (Optional & conditional on instructions completion) Reasoning and planning steps to prepare the decision and response generation\n3. (Conditional on instructions completion) Deciding on the next section of instructions based on the script\n4. Generating the response to the user based on the current section of instructions\nSimilarly as with proactive prompt-based dialog policy planning, all the steps can be performed either by a single LLM, i.e. the Dialog LLM, in a single or in multiple inference runs; or by multiple LLM actors which we call Assessor LLM (step 1), Dispatcher LLM (steps 2 & 3), and Dialog LLM (step 4). In the following section, we will test two implementation variants representing this differentiation."}, {"title": "3 Experimental Setup", "content": "The objective of this and the next section is to validate the feasibility of our proposed technique. To this end, we have implemented and tested exemplary AI Therapist agents powered by Script-Based Dialog Policy Planning. We have introduced two distinct implementation variants for comparison, with a particular focus on efficiency and effectiveness metrics, in order to gain insights into the optimal implementation of the proposed technique. In order to conduct our experiments, we created synthetic conversations by utilizing an additional LLM to act as a patient. The tests were conducted without the involvement of human users.\nPseudocode of the experiments implementation is included in the paper's technical appendix. The entire software code, that can be used to reproduce and extend the experiments can be found at https://github.com/robderbob/sbdpp."}, {"title": "3.1 Exemplary Script", "content": "In our experimental evaluation, we were interested in investigating the feasibility of our technique, and not its medical-therapeutic properties (which is future work). Therefore, at this stage, we did not need a real therapeutic manual or a script specifically designed for an AI Therapist.\nInstead, the exemplary script needed to be structured according to our defined requirements: Multi-task sections representing states of the conversation, and including instructions on when and how to transition from one state to another. Moreover, the script was chosen to be sufficiently complex to test the transition across its states, but sufficiently short to be able to run a high number of tests within a manageable amount of time and cost.\nTo fulfill those requirements and still ensure some degree of domain validity, we derived our script from a psychologist-developed chatbot intervention that was part of the study \"Corona Stressfrei\u201d (covid stress-free) at the Humboldt-University of Berlin in 2021 (Langhammer et al. 2021). Our script matches the high-level conversational flow and phrasing of the above mentioned intervention. The script contains 8 sections (states) with the following contents. State transition rules are shown as \"[\u2192 state]\".\n1. Introduction: Greet the patient, ask for their name, clarify questions. [\u2192 2]\n2. Engagement: Ask the patient for their wellbeing, show empathy, ask if they like to proceed exploring their issue [\u21923] or conduct a CBT intervention [\u2192 4].\n3. Exploration: Ask relevant questions to explore the patient's issue. [4]\n4. Selection: Based on the patient's condition, suggest a relevant CBT intervention, decide on intervention with the patient. [\u2192 5/ 6 / 7]\n5. Exercise 1, Thought Record: Introduce intervention and guide the patient through it. [\u2192 8]\n6. Exercise 2, Behavioral Activation: Introduce intervention and guide the patient through it. [\u2192 8]\n7. Exercise 3, Relaxation Technique: Introduce intervention and guide the patient through it. [\u2192 8]\n8. Ending: Summarize the session, clarify questions, and say goodbye to the patient.\nThe entire script is included in the technical appendix."}, {"title": "3.2 Implementation Variants", "content": "Based on the proactive prompting techniques introduced in 2.2 as well as variations discussed in 2.3 with respect to involving single LLM actor vs. multiple LLM actors, we chose two different implementation variants for the investigation.\nVariant A: Single LLM Actor In variant A, all steps of the dialog policy planning were performed by a single \"Dialog LLM\" within a single output generation. We used an adaptation of the ProCoT prompting scheme (Deng et al. 2023b) and assigned the Dialog LLM to perform the following steps on each turn:\n1. Assess whether you have completed the current section of instructions and output result as a thought (not visible to the patient).\n2. (IF 1 = completed) Select next section of instructions from the script and output result as a thought (not visible to the patient).\n3. Generate a message to the patient based on the current section of instructions.\nIn addition to those instructions, the Dialog LLM's system message contains some general acting guidelines as well as the entire script. The Dialog LLM sees the entire dialog history from the assistant perspective, i.e. as the actor talking to the user. The Dialog LLM prompt of variant A is included in the technical appendix.\nVariant B: Multiple LLM Actors In variant B, the steps of the dialog policy planning were performed by three different LLM actors within their individual output generations. We were using an adaptation of the Ask-an-Expert prompting scheme (Zhang, Naradowsky, and Miyao 2023), and assigning the LLM actors with their respective tasks on each turn:\n1. Assessor LLM to assess whether the Dialog LLM has completed the current section of instructions and output result as a thought (not visible to any actor, only used to trigger or skip step 2)\n2. (IF 1 = completed) Dispatcher LLM to select next section of instructions from the script and output result as a thought (only visible to Dialog LLM)\n3. Dialog LLM to generate message to the patient based on the current section of instructions.\nIn addition to those instructions, the Assessor LLM and the Dispatcher LLM both have access to the current section of instructions as well as the entire dialog history from a third-party perspective, i.e. as an \u201cexpert\" watching the therapist-patient conversation. The Dispatcher LLM additionally has access to the entire script. Same as in variant A, the Dialog LLM has acccess to the entire dialog history from the assistant perspective. The three LLM actors' prompts of variant B are included in the technical appendix.\""}, {"title": "3.3 LLM-Simulated Patients", "content": "In order to perform scaled testing, we have synthesized conversations with LLM-simulated patients. An additional LLM actor was prompted to act like a patient talking to a therapist.\nWe selected five patient case examples from the American Psychological Association's Practice Guidelines (American Psychological Association 2008). We used ChatGPT to generate structured summaries of the five patients' situations, and iteratively fed those into the Patient LLM's prompt for the experiments.\nWe prompted the Patient LLM to not only act by their case and follow the therapist, but also to ask questions and challenge the therapist's advice from time to time. The patient case summaries and Patient LLM's prompt are included in the technical appendix."}, {"title": "3.4 Evaluation Metrics", "content": "Proactive conversational agents are usually evaluated on turn- or dialog-level. There are automated as well as human-rated evaluation metrics. This includes metrics like goal completion, user satisfaction, and others (Deng et al. 2023a). There are also metrics particularly proposed for human-centered PCA (Deng et al. 2024) and emotional support agents (Zheng et al. 2023). However, those are not in the focus of this work. In our evaluation, we are particularly interested in dialog-level efficiency and effectiveness of our agent, and define the relevant metrics as follows.\nFor efficiency, we are using the following metrics:\n\u2022 Avg. Duration per Turn: Avg. inference time of the involved LLM actors in ms per turn\n\u2022 Avg. LLM Input Tokens per Turn: Avg. input tokens used by the involved LLM actors per turn\n\u2022 Avg. LLM Output Tokens per Turn: Avg. output tokens used by the involved LLM actors per turn\nIn terms of effectiveness, we are using three metrics that represent the LLM actors' performance in our three specific steps of the dialog policy planning. We used:\n\u2022 Percentage of Correct Section Completions: % of sections that were actually completed when the Dialog LLM (A) resp. the Assessor LLM (B) decided that they are completed - validated by a Validator LLM.\n\u2022 Percentage of Coherent Section Switches: % of sections for which the message of the Dialog LLM directly following a section switch (state transition) is coherent, i.e. making sense in context of the overall conversation and addressing the latest message of the patient - validated by a Validator LLM.\n\u2022 Percentage of Script-Conform Dispatchings: % of sections selected as next section by the Dialog LLM (A) resp. the Dispatcher LLM (B) in accordance with the state transitioning rules outlined in the script.\nThe Validator LLM is an additional actor used to validate correct section completions and coherent section switches every time a section is asserted as completed. To this end, the Validator LLM is given the entire dialog history from a third-party perspective and prompted to assess the involved LLM actors' performance. The Validator LLM is also asked to provide reasons for its decisions. The Validator LLM's prompt is shown in the technical appendix.\nWe hypothesize that variant A with a single LLM actor is more efficient, whereas variant B with multiple LLM actors is more effective with respect to the above metrics."}, {"title": "3.5 Used LLMs", "content": "For the Assessor LLM, Dispatcher LLM, and Dialog LLM, we used the LLM gpt-4o-mini via the OpenAI API which is considered a state-of-the-art general-use pre-trained LLM with advanced conversational abilities at comparatively low inference duration and cost. We set temperature=0 to make the behavior as consistent and comparable as possible.\nFor the Patient LLM, we also used gpt-40-mini; however, we set temperature=1 to increase the variability of patient behaviors and allow for more unexpected patient utterances.\nFor the Validator LLM, we used a more advanced model to allow it to validate the other LLM actors' behavior. To this end, we used gpt-4o via the OpenAI API, setting temperature=0.\nWe exclusively used OpenAI LLMs due to their accessibility and low cost. It remains to be shown in later works if the results of this work can be reproduced with other LLMs."}, {"title": "3.6 Experimental Iterations", "content": "In order to perform our measurements, we have synthesized a total of 100 dialogs: For each of the 2 implementation variants, and for each of the 5 patient cases, we have generated 10 dialogs."}, {"title": "4 Experimental Results", "content": "Table 1 shows the experimental results comparing variants A and B across the selected evaluation metrics based on the 100 generated dialogs.\nWe have included details on the used hardware and software stack in the technical appendix. The entire dataset of 100 generated dialogs along with computation steps performed to derive the evaluation metrics can be found at https://github.com/robderbob/sbdpp/blob/main/Exp....\nFeasibility Reviewing the transcripts of the dialogs, we generally see natural and realistic conversations, the therapist agent leads a coherent discussion with the patient while acting proactively and following the given script. We cannot identify any obvious deficiencies with respect to our outlined requirements. Therefore, we conclude that our proposed architecture is generally feasible. Examples of the synthesized dialogs are shown in the technical appendix.\nEfficiency Our hypothesis with respect to higher efficiency of variant A was confirmed as variant A shows around 20% lower avg. duration and 23% lower avg. input token usage per turn compared to variant B. The difference in avg. output token usage can be neglected.\nAs assumed, the token usage difference is mainly due to the single LLM call per turn in variant A compared to 2-3 calls (depending on the section completion assessment) per turn in variant B. All LLM actors receive the entire dialog history as input, hence increasing the input token usage per LLM call progressively over the course of the dialog. However, input token usage in variant B could be easily reduced by restricting input to only parts of the conversation which are necessary for Assessor LLM and Dispatcher LLM to fulfill their tasks. Moreover, in contrary to variant A, variant B provides the script to the Dispatcher LLM only selectively on turns when instructions have been completed. Hence, a longer script would further decrease avg. token usage and duration per turn of variant B relative to variant A.\nEffectiveness Variant A shows significantly higher % correct section completions and % coherent section switches compared to variant B which is further investigated below.\nWe mainly see the following and similar reasons provided by the Validator LLM on non-correct section completions:\n\"The therapist did not provide a summary of what was discussed and the exercises conducted during the session.\u201d, \"The therapist did not explicitly ask if the patient had any more questions before proceeding.\u201d, \u201cThe therapist did not wait for the patient's confirmation that the problem summary was correct before proceeding to suggest CBT exercises.\u201d\nInvestigating those cases in context of the dialog, we don't find any examples when the LLM actors clearly fail completing their instructions, despite the low % of correct section completions in both variants. Rather, when the Validator LLM assesses a task completion as non-correct, it is mostly the case that the Dialog LLM is following its instructions slightly less accurately for the sake of addressing a user utterance and/or holding a fluent conversation. Hence, we can't clearly conclude that variant A is completing instructions more correctly, and need to conduct further investigations in future works.\nWhen investigating the % coherent section switches, we see the following reasons of non-coherence provided by the Validator LLM: \"The therapist did not address the patient's request to talk about what might be contributing to the feeling of being stuck.\u201d, \u201cThe therapist repeated the question \"How are you doing today?\" which was already answered by the patient.\u201d, \u201cThe therapist did not address the patient's question about how they can work together to help her feel less lonely.", "none of the offered CBT exercises are interesting\u201d to them or that it \u201cwould like to end the conversation": "or some reasons. We conclude that in variant A, the Dialog LLM is more inclined to follow the patient's utterances. This is consistent with our previous observation that variant B LLM actors are following the instructions more strictly.\nIt remains to be evaluated in future works whether it is more desirable to follow the patient's utterance or to strictly adhere to the script, particularly with respect to treatment effectiveness."}, {"title": "5 Conclusion", "content": "In this work, we introduced a new paradigm for dialog policy planning of conversational agents, called Script-Based Dialog Policy Planning, serving the requirements of the behavioral healthcare domain and particularly modeling an AI Therapist. We have outlined the notion of the script and explained the associated process that powers the agent's behavior on each turn.\nWe proposed and implemented two variants of Script-Based Dialog Policy Planning, demonstrating their general feasibility and showing that the variant powered by a single LLM actor is more efficient in terms of token usage, whereas the variant featuring multiple LLM actors is more strictly adhering to its script while compromising following user utterances.\nThe new architecture requires further development and testing. This includes evaluations with respect to usability and conversational quality of the agent, based on more realistic and extensive expert-developed scripts. Once specific treatment programs have been implemented using the proposed architecture, treatment safety and efficacy must be adequately tested.\nIn terms of technical evaluations, future studies should examine different LLMs, prompting techniques, corpus-based vs. prompt-based approaches, and other design variants. Moreover, this work only features automated LLM-based evaluations, and the quality of this technology remains to be validated through interactions with humans."}, {"title": "Conflict of Interest Statement", "content": "R.W. is co-founder and director of Aury, which develops an AI-based chatbot providing mental health support. K.H. is scientific advisor and received virtual stock options of Aury. C.B. is scientific advisor of Aury."}]}