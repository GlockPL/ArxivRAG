{"title": "PTR: PRECISION-DRIVEN TOOL RECOMMENDATION\nFOR LARGE LANGUAGE MODELS", "authors": ["Hang Gao", "Yongfeng Zhang"], "abstract": "By augmenting Large Language Models (LLMs) with external tools, their capac-\nity to solve complex problems has been significantly enhanced. However, de-\nspite ongoing advancements in the parsing capabilities of LLMs, incorporating\nall available tools simultaneously in the prompt remains impractical due to the\nvast number of external tools. Consequently, it is essential to provide LLMs with\na precise set of tools tailored to the specific task, considering both quantity and\nquality. Current tool retrieval methods primarily focus on refining the ranking list\nof tools and directly packaging a fixed number of top-ranked tools as the tool set.\nHowever, these approaches often fail to equip LLMs with the optimal set of tools\nprior to execution, since the optimal number of tools for different tasks could be\ndifferent, resulting in inefficiencies such as redundant or unsuitable tools, which\nimpede immediate access to the most relevant tools. This paper addresses the chal-\nlenge of recommending precise toolsets for LLMs. We introduce the problem of\ntool recommendation, define its scope, and propose a novel Precision-driven Tool\nRecommendation (PTR) approach. PTR captures an initial, concise set of tools\nby leveraging historical tool bundle usage and dynamically adjusts the tool set by\nperforming tool matching, culminating in a multi-view-based tool addition. Ad-\nditionally, we present a new dataset, RecTools, and a metric, TRACC, designed to\nevaluate the effectiveness of tool recommendation for LLMs. We further validate\nour design choices through comprehensive experiments, demonstrating promising\naccuracy across two open benchmarks and our RecTools dataset.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have established themselves as powerful intermediaries, demon-\nstrating remarkable impacts across a variety of downstream tasks, including text generation, code\ndebugging, and personalized recommendations (Brown et al., 2020; Touvron et al., 2023; Nam et al.,\n2024; Chen et al., 2024; Zhao et al., 2024). However, as these models continue to evolve, they still\nstruggle to solve highly complex problems due to limitations arising from their pre-training data\n(Mialon et al., 2023; Mallen et al., 2022; Yuan et al., 2023). To expand the potential of LLMs in man-\naging more complex tasks efficiently, recommendations at various levels have been increasingly ap-\nplied to LLMs. Typically, memory recommendations (Borgeaud et al., 2022) and knowledge-based\nrecommendations (Gao et al., 2023; Hu et al., 2023) enhance consistency and context awareness\nin ongoing tasks for LLMs, while data augmentation recommendations (Xu et al., 2020) facilitate\nthe inclusion of additional data to augment training. Furthermore, architecture recommendations\n(Elsken et al., 2019; Fedus et al., 2022) and prompt recommendations (Shin et al., 2020; Pryzant\net al., 2023; Liu et al., 2023) optimize efficiency and generate more relevant outputs. Simultane-\nously, to reduce the cognitive load on LLMs and enhance their complex problem-solving capabil-\nities by enabling actions beyond natural language processing, it is crucial to augment LLMs with\nrecommendations of optimal external tool sets, an aspect currently lacking in existing recommen-\ndation frameworks for LLMs. Furthermore, this approach will be helpful to address the challenge\nof input length limitations encountered when incorporating a large number of external tools into the\nprompt. Providing LLMs with a precise and dynamically adaptable recommended toolset can help\nto enhance the effectiveness of LLM's task-solving ability."}, {"title": "2 TOOL RECOMMENDATION", "content": "Tool retrieval, as discussed in previous, involves generating a comprehensive list of tools that are\npotentially relevant to a user's query. This approach emphasizes breadth, aiming to maximize the\ninclusion of pertinent tools. While effective in ensuring extensive coverage, tool retrieval often\nprioritizes recall over precision, resulting in the inclusion of extraneous tools that may not be essen-\ntial for the task at hand. Addressing this limitation, we propose a new optimization direction-Tool\nRecommendation-for LLMs. It aims to ensure that the recommended set of tools aligns closely with\nthe ground-truth set of tools for a task, both in quantity and quality. Specifically, given a user query\nwith a ground-truth toolset (A, B, C), tool recommendation aims to identify precisely (A, B, C),\navoiding omissions or the inclusion of redundant tools. Here is the definition of the tool recommen-\ndation task:\nDefinition 1 Tool Recommendation: Given a comprehensive set of tools $T = \\{T_1,T_2, ..., T_n\\}$ and\na query Q, let $T_{ground} \\subseteq T$ denote the ground truth toolset that fully satisfies Q. The objective is to\nrecommend a toolset $T_{recommend} = \\{T_1, T_2, ...,T_k\\}$ from T such that $T_{recommend} = T_{ground}$ and the\ncardinality constraint $|T_{recommend}| = |T_{ground}|$ holds.\nAs discussed in previous, achieving precision in tool recommendation is pivotal for enhancing the\nperformance and reliability of LLMs. By minimizing the inclusion of irrelevant tools, LLMs can\nreduce computational overhead, streamline task execution, and improve the overall quality of re-\nsponses. Addressing precised tool recommendation not only mitigates the drawbacks associated\nwith broad tool retrieval but also paves the way for more sophisticated and user-centric LLM ap-\nplications. This advancement is essential for deploying LLMs in environments where efficiency,\naccuracy, and user satisfaction are crucial."}, {"title": "3 THE PRECISION-DRIVEN TOOL RECOMMENDATION", "content": "We introduce a novel approach, Precision-driven Tool Recommendation (PTR), to address the chal-\nlenges faced by prior research through a three-stage recommendation process: (1) Tool Bundle\nAcquisition, which involves establishing a potentially useful tool bundle by leveraging past usage\npatterns across all tool combinations, as opposed to relying solely on instructions for individual tool\nusage; (2) Functional Coverage Mapping, which entails effectively mapping the tools from the ac-\nquired tool bundle to the functionalities of the original query, thereby identifying which tools should\nbe retained and which should be discarded, resulting in any remaining unsolved sub-problems; and"}, {"title": "3.1 TOOL BUNDLE ACQUISITION", "content": "To obtain a initiate set of tools, we employ an retriever to capture the relevance between historical\ntool combinations and the current query. Unlike existing methods that focus on retrieving single\ntools by analyzing the relationship between a query and individual tools, our approach introduces\ntool bundle retrieval. By leveraging historical tool combinations, we capture a richer contextual\nrelationship between queries and sets of tools that have been used together effectively in the past.\nThis facilitates a more holistic understanding of tool dependencies and synergies, thereby enhancing\nthe relevance of retrieved tool sets for complex queries. Specifically, Let $T = \\{T_1, T_2,...,T_n\\}$\nbe the set of all available tools. Let $D = \\{(Q_i, B_i)\\}_{i=1}^{l}$ represent a set of past queries and their\nassociated tool bundles, where $Q_i$ is a past query, and $B_i$ is the corresponding tool bundle used for\n$Q_i$, with $B_i \\subseteq T$. The collection of unique tool bundles is $B = \\{B_1, B_2, . . ., B_N\\}$. Given a new\nquery Q, we select a tool bundle $B_K = \\{T_1, ...,T_z\\}$ from B that is most relevant to Q through the\nretriever, which ideally contains tools potentially useful. The subsequent recommendations operate\non this obtained tool bundle\u2014either based on sparse representations or dense representations."}, {"title": "3.2 FUNCTIONAL COVERAGE MAPPING", "content": "As illustrated in Figure.3, functional coverage\nmapping presents a structured approach to eval-\nuate and optimize a set of tools in relation to a\nspecific query. By systematically aligning re-\nquired functionalities with the capabilities of\navailable tools, this method ensures that the\ntoolset comprehensively addresses the user's\nneeds while minimizing redundancies and iden-\ntifying any gaps, as each tool may correspond to\nmultiple functionalities. At its core, Functional\nCoverage Mapping aims to determine whether\nan initial set of tools $B_K = \\{T_1, T_2,...,T_z\\}$\nadequately fulfills a query Q with its key func-\ntionalities $F = \\{F_1, F_2,..., F_m\\}$. Specifi-\ncally, Functional Coverage Mapping achieves"}, {"title": "3.3 MULTI-VIEW BASED RE-RANKING", "content": "Addressing the challenge of selecting pertinent tools from an extensive toolset to resolve unresolved\nproblems requires comprehensive consideration. The proposed PTR employs a multifaceted simi-\nlarity evaluation strategy that integrates three essential dimensions of the unresolved problem $U_j$:\n(1) Direct Semantic Alignment, wherein the system quantifies the semantic similarity between\nthe user query and each available tool, ensuring the immediate identification of tools intrinsically\naligned with the query's intent; (2) Historical Query Correlation, which involves analyzing past\nqueries that closely resemble the current one to extract tools previously utilized in similar contexts,\nthereby enriching the current toolset with empirically effective solutions while maintaining unique-\nness through aggregation and deduplication; and (3) Contextual Tool Expansion, which leverages\nthe most relevant tool identified through direct semantic alignment to retrieve additional tools ex-\nhibiting high similarity to this primary tool, thereby uncovering supplementary options that may\noffer complementary or alternative functionalities beneficial to the user's query. The multi-view\nmatching process involves obtaining the tool list L through direct semantic alignment (DSA), his-\ntorical query correlation (HQC), and contextual tool expansion (CTE), respectively. These three\ntool lists are then aggregated and ranked according to their frequency of occurrence, with the most\nfrequent tools being selected. After performing the multi-view-based re-ranking for each unsolved\nproblem, the top-ranked tool in each list is selected and added to the final recommended toolset. In\nsome cases, it is also possible that this tool already exists in the toolset acquired from the second-\nstage recommendation; in such instances, the tool will be ignored. The algorithm for multi-view-\nbased re-ranking is summarized in Algorithm.1."}, {"title": "4 DATASETS AND METRICS", "content": "Datasets. To verify the effectiveness of PTR,\nwe utilize three datasets for tool recommenda-\ntion: ToolLens (Qu et al., 2024a), MetaTool\n(Huang et al., 2023), and a newly constructed\ndataset, RecTools. We randomly select 20% of\neach dataset to serve as the test data. Both Tool-\nLens and MetaTool focus on multi-tool tasks, leading us to select them as the primary datasets for our\nexperiments. While ToolLens uniquely emphasizes creating queries that are natural, concise, and\nintentionally multifaceted, MetaTool is a benchmark designed to evaluate whether LLMs possess\ntool usage awareness and can correctly choose appropriate tools. However, both datasets impose\na low upper limit on the number of tools used per query. As the capabilities of LLMs continue\nto develop, more tools need to be recommended to solve increasingly complex problems, thereby\nlimiting the applicability of these datasets. Additionally, all queries in these two datasets utilize a\nfixed number of tools, which not only fails to fully simulate the dynamic nature of tool usage in real-\nworld scenarios but also introduces bias in the subsequent testing of the method. Most importantly,\nsince tool recommendation focuses on the precision of the recommended toolset, the test datasets\nrequire that each query be exactly solvable by the provided tools (Exact Solving). Using one fewer\ntool leads to partial solving, while using one additional tool results in oversolving. To validate the\neffectiveness of the two datasets, we first employ GPT-4o as an evaluator to determine whether the\nprovided toolset can achieve an \u201cExact Solving\" outcome for each query. Subsequently, for each\nquery, we randomly remove one tool from the corresponding toolset and prompt GPT-40 to assess\nwhether the modified toolset can achieve a \"Partial Solving\" outcome. Queries and their respective\ntoolsets that meet the criteria for both evaluations are considered qualified. The performance of\nthese two datasets is not ideal. Based on these limitations, we constructed a new dataset, RecTools,\nwhere queries do not have a uniform number of tools and have a high upper limit on the number of\ntools used. Additionally, RecTools significantly outperforms ToolLens and Metatool in the GPT-40\n\"Exact Solving\" test. The statistics of the three datasets are summarized in Table.1. Specifically,\nfor all (query, tools) pairs involving the use of two and three tools, the success rates of RecTools\nreached 76% and 89%, respectively.\nMetrics. As evaluation metrics for tool recommendation, following previous work focusing on tool\nretrieval (Gao et al., 2024; Qu et al., 2024b), the widely used retrieval metrics are Recall and NDCG.\nHowever, they do not adequately address the requirements for accuracy in both the number of rec-"}, {"title": "5 EXPERIMENTS", "content": "5.1 IMPLEMENTATION DETAILS\nBaselines. We considered the following baselines: Random, which randomly select from historical\ntools; BM25 (Robertson et al., 2009), a classical sparse retrieval method that extends TF-IDF by\nleveraging term frequency and inverse document frequency of keywords; Contriever (Izacard et al.,\n2021), which utilizes inverse cloze tasks, cropping for positive pair generation, and momentum con-\ntrastive training to develop dense retrievers; SBERT (Reimers & Gurevych, 2019), a library provid-\ning BERT-based sentence embeddings. Specifically, we use all-mpnet-base-v2; TAS-B (Hofst\u00e4tter\net al., 2021), the retriever introduces an efficient topic-aware query and balanced margin sampling\ntechnique; And SimCSE (Gao et al., 2021), a simple contrastive learning framework that greatly\nadvances state-of-the-art sentence embeddings.\nBesides, we initially implement the PTR using the open source model open-mistral-7b, due to its\ncost-effectiveness. Subsequently, we evaluate PTR with the model GPT-3.5-turbo and GPT-40, to\ndetermine its effectiveness when employing a more advanced model. For evaluation metrics, in\naddition to the specifically designed TRACC metric, we also calculate Recall@K and NDCG@K,\nreporting these metrics with K set to the size of the ground-truth tool set."}, {"title": "5.2 EXPERIMENTAL RESULTS", "content": "Table 2 presents the main results of the PTR applied to ToolLens, MetaTool, and RecTools using\nvarious models and unsupervised retrievers. Based on these findings, we draw the following obser-\nvations and conclusions."}, {"title": "5.3 FURTHER ANALYSIS", "content": "In this section, we conduct an in-depth analysis of the effectiveness for PTR, using the same datasets\nand evaluation metrics. The results are presented in Table 3.\nw/o Tool Bundle Acquisition. This variant omits the tool bundle acquisition stage, resulting in\nqueries being exclusively mapped to unresolved problems without any existing recommended tools.\nThe observed decline in performance for this variant further supports the effectiveness of tool bun-\ndles in identifying potential recommended tools, thereby refining the unresolved problems and\nachieving precise tool recommendations. Moreover, as illustrated in Table 3, the random approach\nalone is largely ineffective for tool recommendations. However, as presented in Table 2, when com-\nbined with functional coverage mapping and multi-view-based re-ranking, the final recommendation\nperformance improves significantly. This underscores the importance of the last two recommenda-\ntion stages."}, {"title": "Performance w.r.t to accuracy in quantity.", "content": "Furthermore, to evaluate the performance of PTR in\nterms of tool number precision, we calculate the average length difference between the recom-\nmended tool set and the ground truth tool set for each method and backbone. Figure.4 demonstrates\nthe effectiveness of PTR in maintaining consistency in the number of tools. In the MetaTool and\nToolLens dataset, which exhibits relatively simple and small patterns, PTR clearly shows its effec-\ntiveness. Regarding our RecTools dataset, which has a variable structure and involves a wide range\nof tools for each query, the average length difference is effectively controlled within a considerable\nrange, especially when it comes to the Embedding method."}, {"title": "6 RELATED WORK", "content": "6.1 RECOMMENDATION FOR LLMS\nRecent research has explored a variety of recommendation techniques to enhance Large Language\nModels (LLMs), integrating capabilities across multiple dimensions. Data recommendation (Xu\net al., 2020; Ouyang et al., 2022) is crucial for selecting relevant datasets to fine-tune models for spe-\ncific domains, ensuring ongoing performance improvements. Memory recommendation (Borgeaud\net al., 2022; Gao & Zhang, 2024a) facilitates the retrieval of relevant past experiences or interactions,\nimproving continuity, consistency, and long-term context in multi-turn conversations. Knowledge\nbase recommendation (Gao et al., 2023; Hu et al., 2023; Petroni et al., 2019; Lewis et al., 2020)\nenhances factual grounding by retrieving the most pertinent information from external sources, en-\nsuring that model outputs are accurate and up to date. Architecture recommendation (Elsken et al.,\n2019; Fedus et al., 2022) optimizes model performance by dynamically selecting the most appropri-\nate model components or layers to activate for different tasks, thereby improving efficiency. Lastly,\nprompt recommendation (Shin et al., 2020; Reynolds & McDonell, 2021; Li & Liang, 2021; Wang\net al., 2022; Liu et al., 2023) guides LLMs in utilizing the most effective input prompts, thereby\nenhancing the quality of generated responses through optimized input-output interactions. Together,\nthese recommendation techniques form a comprehensive framework that enhances the adaptability,\nefficiency, and task-specific performance of LLMs. However, there remains a lack of research on\ntool recommendation. In this work, we motivate to seek to provide a clear definition of tool rec-\nommendation and proposes an effective recommendation method. Additionally, new datasets and\nmetrics are created to advance research in this area."}, {"title": "6.2 TOOL RETRIEVAL", "content": "Initially, term-based methods such as BM25 (Robertson et al., 2009) and TF-IDF (Sparck Jones,\n1972) were employed to measure the similarity between queries and tool documents by identifying\nexact term matches. Subsequently, with the development of dense retrievers (Karpukhin et al., 2020;\nGuu et al., 2020; Xiong et al., 2020), the semantic relationships between queries and tool descrip-\ntions have been more effectively captured through neural networks. Recently, novel approaches for\ntraining retrievers have emerged. For example, Confucius (Gao et al., 2024) selects tools by defining\nthree levels of scenarios, ranging from easy to difficult, to train and deepen the LLM's understanding\nof tools. Additionally, execution feedback is iteratively utilized to refine the tool selection process\n(Wang et al., 2023; Xu et al., 2024). Furthermore, ToolkenGPT (Hao et al., 2024) enhances tool\nselection by representing each tool as a token (\"toolken\u201d) and learning an embedding for it, thereby\nenabling tool calls in the same manner as generating regular word tokens. Moreover, some research\nhas focused on addressing the diversity of retrieval (Carbonell & Goldstein, 1998; Gao & Zhang,\n2024b), which can effectively enhance the quality of multiple tools used by query. Despite their\ncomprehensive nature, tool retrieval systems present notable limitations. The inclusion of superflu-\nous tools can introduce noise, thereby interfering with the LLM's performance and task execution,\nand these systems are often unable to dynamically adjust the toolset. In this work, we extend our\napproach beyond getting a rough toolset by ensuring that the tools in the recommended toolset are\nas accurate as possible in terms of both quality and quantity."}, {"title": "7 CONCLUSIONS", "content": "This study presents a novel challenge, tool recommendation, and offers a precise formalization of the\nproblem. In response, we propose a new approach, PTR, designed to improve the accuracy of tool"}]}