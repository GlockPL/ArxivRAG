{"title": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable Compression", "authors": ["Guangda Liu", "Chengwei Li", "Jieru Zhao", "Chenqi Zhang", "Minyi Guo"], "abstract": "Large Language Models (LLMs) have been widely deployed in a variety of applications, and the context length is rapidly increasing to handle tasks such as long-document QA and complex logical reasoning. However, long context poses significant challenges for inference efficiency, including high memory costs of key-value (KV) cache and increased latency due to extensive memory accesses. Recent works have proposed compressing KV cache to approximate computation, but these methods either evict tokens permanently, never recalling them for later inference, or recall previous tokens at the granularity of pages divided by textual positions. Both approaches degrade the model accuracy and output quality. To achieve efficient and accurate recallable KV cache compression, we introduce ClusterKV, which recalls tokens at the granularity of semantic clusters. We design and implement efficient algorithms and systems for clustering, selection, indexing and caching. Experiment results show that ClusterKV attains negligible accuracy loss across various tasks with 32k context lengths, using only a 1k to 2k KV cache budget, and achieves up to a 2x speedup in latency and a 2.5\u00d7 improvement in decoding throughput. Compared to SoTA recallable KV compression methods, ClusterKV demonstrates higher model accuracy and output quality, while maintaining or exceeding inference efficiency.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have been widely deployed in a variety of applications, such as natural language understanding, coding copilots and chatbots. As LLMs are used for more complex tasks, the need for long context length is rapidly increasing to handle tasks such as long-document QA, repository-level code understanding, and complex logical reasoning [1-3]. This drives LLMs to support larger context windows, expanding from the original 4k to 32k [4, 5], 128k [6] and even up to 1M [7]. While models can support larger context window with continual training or extrapolation [5], inference with long contexts poses significant challenges on efficiency. As the size of key-value (KV) cache increases linearly with the context length, it can exceed the capacity of GPU memory, leading to inference failure or extremely high latency. Since the autoregressive decoding stage is typically the performance bottleneck of LLM inference and is memory-bound [8], accessing the KV cache of all previous tokens results in increased latency for long-context inference.\nTo mitigate the issues, recent works compress KV cache by selecting a subset of tokens and utilizing their keys/values to approximate attention computation. The tokens are typically selected with fixed patterns [9] or based on their attention"}, {"title": "II. BACKGROUND AND MOTIVATION", "content": "A. LLM Inference and KV Cache\nLLMs encompass multiple Transformer layers, each containing a multi-head attention (MHA) module and a feed-forward network (FFN) with residual connections and normalization operations [15]. In MHA, the input tensor is linearly projected into query, key and value tensors (Q, K, V \u2208 \u211d^{N\u00d7d}) for each head, where N is the input length and d represents the number of channels or hidden dimensions per head. The MHA output is defined as softmax(QK^T)V, with outputs of all heads concatenated for subsequent FFN and normalization.\nFor generative inference, LLMs produce tokens in an autoregressive manner, appending each generated token to the input to generate the next token. To avoid the recomputation of K and V of previous tokens, these tensors are stored in memory for reuse, which is known as KV cache. The LLM inference includes two stages: prefill and decoding. The prefill stage processes the entire input sequence, computes KV cache and generates the first output token. During decoding, the query vector q of the latest generated token and K, V of previous tokens are used to compute attention for generating the next token, formulated as softmax(q^TK)V, where q \u2208 \u211d^{1\u00d7d}, K, V \u2208 \u211d^{L\u00d7d} and L is the context length of previous tokens.\nB. Long Context Inference and KV Cache Compression\nLong context is an emerging trend of LLM inference, such as long-document QA [1, 16], repository-level code understanding [2] and complex logical reasoning [3]. Moreover, supported context windows of LLMs are extending rapidly, from 4k tokens of GPT3.5 and Llama-2 to 32k [5], 128k [6] and even 1M tokens [7]. However, long-context inference incurs significant memory and computation cost. The size of KV cache and complexity of attention computation during decoding increase linearly with context length, resulting in low inference efficiency or even inference failures.\nRecent works reveal the sparsity of attention computation, i.e., only a small subset of tokens contributes to most of attention outputs [10, 13]. This observation enables KV cache compression by selecting a subset of tokens to approximate attention computation, which is formulated as softmax(qK_s^T)V_s, where K_s, V_s \u2208 \u211d^{B\u00d7d} represents the keys and values of"}, {"title": "C. Related Work and Motivation", "content": "KV cache compression should be recallable. While selecting a subset of KV cache for computation ensures inference efficiency, computing all the attention weights for selection introduces substantial overhead. Therefore, most existing works only compute attention weights over tokens that have been selected, rather than for all previous tokens [10-12]. In this way, keys and values of tokens not selected at one decoding step are permanently evicted from the KV cache and never recalled in later inference steps, as shown in Fig. 1b.\nHowever, we observe that the token importance changes dynamically during inference. Token that are unimportant with low attention weights at one decoding step can become important in later steps, and vice versa. Figure 3a shows changes in attention weight rankings during 64 decoding steps of Llama-3-8B. For instance, token 3200 is initially unimportant but becomes crucial after 20 steps, while the opposite occurs for token 2048. And importance of all tokens can fluctuate throughout inference, as seen with token 7168. Therefore, non-recallable compression inevitably overlooks some tokens with dynamically changing importance, leading to reduced model accuracy and a decline in output quality.\nDefects of existing recallable KV compression methods. However, achieving recallable compression by computing attention weights with all previous tokens incurs an unacceptable cost of O(Ld), which is comparable to attention computation with full KV cache and negates the benefits of compression. To"}, {"title": "III. ALGORITHM DESIGN", "content": "A. Problem Formulation and Design Rationale\nProblem Formulation. For approximated attention computation with selected tokens formulated in Sec. II-B, let Ks to be (k_{i_1}, k_{i_2}, ..., k_{i_B})^T, where \u2110_T = {i_1, i_2, ..., i_B} denotes the indices of selected tokens. Our goal is to select tokens which contribute most to attention weights, thereby approximating the original computation as closely as possible. Specifically, \u2110_T is supposed to be arg max_{\ud835\udcae\u2208\u211d} \u2211_{i\u2208\ud835\udcae} qk_i, which corresponds to selecting tokens with top-B largest attention weights.\nDesign Rationale. Our observation is that tokens which are close in semantic space tend to have similar attention weights for a given q. Therefore, we propose KV selection at the granularity of semantic clusters. We first apply clustering to tokens in the semantic space. Then, we compute attention weights only with respect to the cluster representations, rather than individual tokens, and select clusters with the largest attention weights. Since the number of clusters is typically an order of magnitude smaller than the number of tokens, cluster-based selection significantly reduces recall overhead.\nB. Clustering in the Semantic Space\nSemantic Distance. As for a given q, attention weights are associated only with the key tensors, we measure the semantic distance between tokens by calculating the distance between corresponding key vectors. Furthermore, we find that cosine similarity is more suitable than L2 or inner product distances. This is due to the presence of outlier channels with large magnitudes in key vectors [18], which can cause drastic changes in L2 or inner product distances. Thus, we define distance between token i and token j in the semantic space as\nD(i, j) = 1 - \\frac{(k_i \\cdot k_j)}{||k_i|| ||k_j||}, where distance is smaller for vectors with larger cosine similarity.\nClustering. We apply a simple K-means algorithm for clustering over key vectors as shown in Fig. 4 [19]. We first randomly sample key vectors as the initial centroids. Subsequently, we alternatively perform the assignment and update steps until convergence. In the assignment step, each key vector is assigned to the nearest centroid and given a corresponding cluster label based on the distance D, i.e., assigned to the centroid with the maximum cosine similarity. Then in the update step, the mean of keys assigned to the same centroid are used as the new centroid. The algorithm converges when the assignment no longer changes, and keys assigned to the same centroid form a semantic cluster, with the centroid as the cluster representation.\nDuring LLM inference, we first apply clustering to the key vectors of prompt tokens after the prefill stage. However, we note an exception for the initial tokens, referred to as attention sinks [9]. These tokens typically appear as outliers in the clustering process, as they are distant from other tokens in the semantic space. Therefore, we always retain the first 16 tokens and apply clustering to the subsequent tokens, generating C_0 centroids. We set C_0 = \\frac{L}{80}, as our experiments indicate that for a 32k context, using 400 clusters achieves a balance between efficiency and accuracy. For tokens generated in the decoding stage, clustering is applied every m decoding steps to the key vectors of m generated tokens, creating C_+ new centroids. Since clustering keys of generated tokens together with those from the prefill stage can incur significant overhead, we instead apply clustering within the generated tokens only. To amortize the cost, we set C_+ and m to 4 and 320, respectively.\nC. Selection at the Granularity of Semantic Clusters\nSelection. We denote cluster centroids as \u03bc\u2081, \u03bc\u2082, ..., \u03bc_C \u2208 \u211d^d. To select important tokens for a given query q, we sort those centroids based on their attention weights, i.e., q\u03bc_i, in descending order. While keys are clustered using cosine similarity distance, the distance between query vector and centroids is measured with inner product, as it better aligns with attention weight computation. Intuitively, keys assigned to clusters with centroids that have larger attention weights tend to have larger attention weights for the given q. Therefore, we retrieve the sorted centroids and collect KV of tokens from the corresponding clusters, until the top-B most important tokens are selected, as shown in Fig. 4.\nD. Efficiency Concerns\nCluster-based selection avoids the internal fragmentation of important tokens and can achieve higher accuracy compared to"}, {"title": "IV. SYSTEM DESIGN & IMPLEMENTATION", "content": "A. System Overview\nThe system overview is shown in Fig. 5. During the prefill stage, the key tensors are processed with semantic clustering (SC) on GPU, producing centroids and corresponding metadata. The generated KV tensors are offloaded to CPU memory. During the decoding stage, as described in Section III-C, attention weights of the query vector q and cluster centroids are computed to determine the importance of each cluster. The results, along with clustering metadata, are used to generate indices (\u2110_T) for selected tokens, which are then used to load selected KV (K_s, V_s) from CPU memory to GPU memory for attention computation. A cache for selected KV is maintained on GPU, so only KV not already cached (K'_s, V'_s) need to be loaded. For every m decoding steps, the clustering and KV offloading are performed for the m generated tokens.\nB. Semantic Clustering\nClusterKV optimizes the efficiency of clustering at both system and kernel levels."}, {"title": "V. EVALUATION", "content": "A. Experimental Setup\nWe evaluate ClusterKV on an NVIDIA Ada 6000 GPU. To assess model accuracy and output quality, we conduct experiments on eight datasets from LongBench [1], including 2WikiMQA, TriviaQA, HotpotQA, MultiFieldQA, MuSiQue, NarrativeQA, Qasper and GovReport. These datasets cover a variety of tasks, such as single-doc QA, multi-doc QA, few shot learning and summarization, with context lengths reaching up to 32k. Additionally, we evaluate ClusterKV on the PG19 dataset for the language modeling task [21]. We use the state-of-the-art long-context GLM4-9B-Chat model for evaluation of model accuracy [7], which supports a context window of up to 128k tokens.\nWe compare ClusterKV with two state-of-the-art KV cache compression methods: Quest [14] and InfiniGen [17]. Most configurations of these methods remain as in their original settings, such as page_size for Quest and the partial weight ratio and selection threshold for InfiniGen. To align with settings of Quest which does not apply selection on the first two layers of the model, we also disable selection and use the full KV cache for the first two layers in ClusterKV and InfiniGen, in both model accuracy and inference performance evaluations. As both Quest and InfiniGen only support efficient inference for Llama-architecture models, we use Llama-3.1-8B for evaluation of inference performance.\nB. Model Accuracy\nResults on LongBench. Figure 9 presents the results on LongBench. ROUGE-L is used as the score metric for GovReport and F1 score is used for other tasks. We evaluate methods under the KV cache budgets of 256, 512, 1024 and 2048 tokens. ClusterKV outperforms Quest and InfiniGen in most settings, and achieves accuracy comparable to that of using the full KV cache with budgets as low as 1k to 2k tokens.\nC. Inference Efficiency\nComparison with inference using full KV cache. We evaluate the inference latency of ClusterKV under various\nWe present the comparison between ClusterKV and Quest in Fig. 13b, using Llama-3.1-8B model with a budget of 1k tokens. As shown, ClusterKV achieves performance very close to Quest, with latency deviations up to 5%, while ClusterKV delivers significantly higher model accuracy.\nEffectiveness of caching. We analyze the hit rates of our cluster-granularity cache during inference, using a 32k-tokens sample from the NarrativeQA dataset. The average hit rates are 63% and 74% for R = 1 and R = 2, respectively. Compared to directly loading from CPU memory, the caching mechanism improves decoding throughput by 2.3\u00d7 and 3\u00d7, respectively."}, {"title": "VI. CONCLUSION", "content": "This paper introduces ClusterKV, achieving efficient and accurate KV cache compression, recalling tokens at the granularity of semantic clusters. ClusterKV preserves model accuracy while significantly enhancing inference efficiency."}]}