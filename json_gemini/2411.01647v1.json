{"title": "MedSora: Optical Flow Representation Alignment\nMamba Diffusion Model for Medical Video Generation", "authors": ["Zhenbin Wang", "Lei Zhang", "Lituan Wang", "Minjuan Zhu", "Zhenwei Zhang"], "abstract": "Medical video generation models are expected to have a profound impact on the\nhealthcare industry, including but not limited to medical education and training,\nsurgical planning, and simulation. Current video diffusion models typically build\non image diffusion architecture by incorporating temporal operations (such as\n3D convolution and temporal attention). Although this approach is effective, its\noversimplification limits spatio-temporal performance and consumes substantial\ncomputational resources. To counter this, we propose Medical Simulation Video\nGenerator (MedSora), which incorporates three key elements: i) a video diffusion\nframework integrates the advantages of attention and Mamba, balancing low com-\nputational load with high-quality video generation, ii) an optical flow representa-\ntion alignment method that implicitly enhances attention to inter-frame pixels, and\niii) a video variational autoencoder (VAE) with frequency compensation addresses\nthe information loss of medical features that occurs when transforming pixel space\ninto latent features and then back to pixel frames. Extensive experiments and ap-\nplications demonstrate that MedSora exhibits superior visual quality in generating\nmedical videos, outperforming the most advanced baseline methods. Further re-\nsults and code are available at https://wongzbb.github.io/MedSora/.", "sections": [{"title": "Introduction", "content": "With the development and integration of technologies such as diffusion (Ho et al., 2020; Sohl-Dickstein et al., 2015), multimodal (Radford et al., 2021; Ramesh et al., 2021), pre-trained models (Brown et al., 2020), and model fine-tuning (Hu et al., 2021; Li & Liang, 2021), artificial intelligence generated content (AIGC) has achieved remarkable progress, sparking widespread interest in interdisciplinary medical fields. Ranging from image reconstruction (Liu et al., 2023) and translation (Wang et al., 2024) to the generation of virtual cases and the creation of simulation data (\u00d6zbey et al., 2023), AIGC underpin numerous innovations. This has significantly advanced the frontiers of computer-assisted diagnosis and precision medicine. Recently, advances in video generation models (Blattmann et al., 2023; An et al., 2023; Huang et al., 2024) have propelled AIGC to new heights. However, due to the considerable complexity and significant resource demands of clinical video generation, research in this area remains in its nascent stages. In this context, our work aims to explore whether it is possible to create temporally coherent and realistic clinical medical videos.\nSignificant progress has been made in generating realistic medical images using generative adversarial networks (GANs) (Goodfellow et al., 2014) and diffusion models (Rombach et al., 2022). However, video is composed of pixel arrays spanning both temporal and spatial dimensions, necessitating meticulous attention to fine temporal dynamics and the maintenance of temporal consistency across frames. Consequently, generating stable, high-quality videos is a non-trivial task. To achieve this, most recent works in computer vision attempt to extend the spatial self-attention mechanism of images to spatio-temporal self-attention (Khachatryan et al., 2023; Qi et al., 2023), using pseudo-3D (Singer et al., 2022) or serial 2D+1D convolutions (Xing\net al., 2024a) to apply image diffusion models to video generation. To the best of our knowledge, the sole existing study in the medical field on video generation employs the spatio-temporal self-attention (Li et al., 2024). Those strategies integrate features from patches across different video frames via an expanded attention or convolution module, as illustrated in Figure 1. Despite their effectiveness in cap-\nturing contextual information across\nspace and time, these methods have\nsignificant drawbacks: i) spatio-\ntemporal self-attention requires each\npatch attends to all other patches\nin the video, which not only poses\nthe risk of misdirecting the atten-\ntion mechanism with patches that are\nirrelevant to video generation (Liu\net al., 2022), but also significantly\nincreases the computational com-\nplexity, presenting significant chal-\nlenges in terms of resource alloca-\ntion and maintaining cross-frame co-\nherence, and ii) convolutional meth-\nods focus only on local areas and\ncannot capture the global receptive\nfield, thereby reducing model per-\nformance. To mitigate these chal-\nlenges, some recent works have in-\ncorporated Mamba (Gu & Dao, 2023; Dao & Gu, 2024) into video generation, they typically replace the original convolution or attention modules of GANs or diffusion models with Mamba (Gao\net al., 2024; Mo & Tian, 2024), or merely alter the scanning order of patches (Park et al., 2024)\nwithout thorough exploration. These Mamba-based video generation models exhibit two primary\nlimitations: i) existence of pixel-level bias. Recent advances in diffusion models for image gener-\nation illustrate that Mamba underperforms relative to certain Transformer in generating pixel-level\ndata (Wang et al., 2024). And ii) absence of explicit spatio-temporal modeling. These models either\nmaintain the same structural design of image generation models (Mo & Tian, 2024; Gao et al., 2024)\nor simplistically modify the sequence of video frames (Park et al., 2024), assuming it to suffice for\nspatio-temporal modeling. We aim to address these challenges by advancing our approach with an\noptimized design of spatio-temporal Mamba and attention blocks.\nSpatio-temporal self-attention frameworks typically model the content within each video frame be-\nfore conducting temporal modeling of patches at corresponding positions across frames (Diba et al.,\n2023). However, this approach has a significant limitation as it presupposes linear motion of patches\nalong the temporal axis. This assumption leads to inadequate mutual attention among patches fol-\nlowing non-linear trajectories across frames. To overcome existing drawbacks, we propose a super-\nvised, optical flow representation alignment method seamlessly integrated into the video diffusion\nmodel. By using optical flow (Fleet & Weiss, 2006; Shi et al., 2023) to implicitly control key fea-\ntures, our method addresses the visual consistency problems identified in previous work. An primary\nbenefit is the facilitation of precise information transfer between frames through optical flow guid-\nance, stabilizing the generated visual content throughout the video. Specifically, we initially employ\na pre-trained optical flow estimation model (Teed & Deng, 2020) to determine the optical flow of\nthe source video. Following this, a self-supervised foundation model (Caron et al., 2021) is utilized\nto extract key features of the optical flow from various perspectives. These key features are then\nsampled and leveraged to supervise the training of video diffusion model, without the addition of\nany extra training parameters.\nVideo generation models process high-dimensional videos as cubic arrays, necessitating substan-\ntial memory and computational resources, particularly for high-resolution and extended-duration\nvideos. To reduce the computational load, numerous video diffusion models (Blattmann et al., 2023;\nAn et al., 2023) are not trained on raw pixels, but first employ an autoencoder (Kingma & Welling,\n2013) to project video frames into low-dimensional latent space and then model this latent distribu-\ntion. However, the pre-train time-agnostic image autoencoder can distorts the temporal information\nin the latent space (Zhou et al., 2022), i.e., the change \u25b3z = zi \u2192 zj from the i-th frame to the j-th"}, {"title": "Preliminaries", "content": "2.1 Latent Diffusion Model\nLatent Diffusion Models (LDM) (Rombach et al., 2022) are probabilistic denoising models that\nintegrate autoencoders with predefined posterior distributions, rapidly becoming foundational com-\nponents in the AIGC field. These models consist of two main processes: diffusion and denoising.\nDuring the diffusion process, random noise is progressively added to the latent representation z\nthrough a sequence governed by a Markov chain with T-steps. Specifically, consider a fixed z and\na predetermined signal-noise schedule {at, \u03c3\u03c4}, where the signal-to-noise ratio (SNR), given by\n(\u03b12/\u03c32), decreases monotonically as t increases. Define \u03b1t|s = \u03b1t/\u03b1s and \u03c32|s = \u03c32 \u2013 \u03b12\u03c32 for\nall s < t, we specify a series of latent variables z\u0142 for t = 0,...,T, which adhere to the following\nspecified conditions:\nq(zt|zs) = \\begin{cases}\n\u039d(zt; \u03b1t z, \u03c3tI) & \\text{if } s = 0 \\\\\n[N(zt; \u03b1t|s zs, \u03c3t|sI) & \\text{if } s > 0\n\\end{cases}\n\nwhere I represents the identity matrix. During the denoising phase, LDM train a reverse model\nP\u03b8(zs|zt), which reformulates a denoising objective,\nP\u03b8(zs|zt) = N(zs; \u00b5\u03b8(zt, c, t), \u03a3\u03b8(zt, c, t)),\nhere c represents the conditional input. The mean function \u03bc\u03b5 and the covariance matrix \u03a3\u03b8 of the\nconditional distribution in the inverse process are calculated by training a denoising model \u03b5\u03b8.\n2.2 Scalar State-Space Models\nThe state space consists of a minimal set of variables that fully describe every possible state of the\nsystem. Building upon this concept, State-Space Models (SSM) (Gu et al., 2021a;b; Smith et al.,\n2022) define these variables to represent the system's states and can predict future states based on\nspecific inputs. To summarize, SSM defines a mapping from x \u2208 R(T,P) to y \u2208 R(T,P). The state\ntransition is:\nht = Atht\u22121 + Btxt,yt = Cht,\nwhere Bt, Ct \u2208 R(T,N). ht is referred to as the hidden state, represented by an N-dimensional vec-\ntor, where N is an independent hyperparameter, commonly referred to as the state dimension, state\nexpansion factor, or state size. SSM are classified as regular (unstructured) SSM, diagonal (struc-\ntured) SSM (Gu, 2023; Gupta et al., 2022), or scalar SSM (also known as SSD) (Dao & Gu, 2024),\ndepending on whether the dimensions of At are R(T,N,N), R(T,N), or R(T), respectively. Selective\nSSM permit these parameters to change over time. Mamba(Gu & Dao, 2023), specifically its core"}, {"title": "Methodology", "content": "Consider a dataset of videos D, where each sample x1:F is drawn from an unknown data distribution\nPdata(x1:F). In this context, each x1:F := (x1, \u2026\u2026\u2026 ,xF) corresponds to a sequence of video frames,\nwith each frame sequence having a length F > 1 and a resolution of H \u00d7 W. Specifically, each\nframe x \u2208 R(C,H,W), where C denotes the number of channels. The goal is to learn a model\ndistribution pmodel (x1:F) that precisely matches the original data distribution Pdata (x1:F).\nIn the ensuing sections, we commence by detailing the architecture of diffusion model employed\nin MedSora, with a particular emphasis on components spatio Mamba and temporal Mamba (Sec-\ntion 3.1). Subsequently, we elucidate the implementation of the optical flow representation align-\nment (Section 3.2), followed by delving into the comprehensive fine-tuning strategy for the 3D\ncausal VAE (Section 3.3). Finally, we provide a image and video joint training strategy (Section 3.4).\n3.1 Video Mamba Diffusion Model\nCurrent video diffusion models in the natural imaging typically employ either 3D or 2D+1D op-\nerations, simplifying the model excessively and thus compromising spatio-temporal performance.\nAlternatively, using standard self-attention mechanisms leads to significant computational overhead.\nIn Figure 2, each block of our video diffusion framework consists of modules sequentially connected\nvia spatial and temporal components. Considering Mamba's limitations in local modeling and its\nproficiency in handling long sequences, and given the self-attention mechanism's excellent local\nmodeling capabilities but high computational cost that grows quadratically with sequence length,\nwe propose employing local attention within the spatial component to model local information. To\nexpand the receptive field to cover an entire video frame, we use Mamba to model long sequences\nalong both the width and height axes, as shown in Figure 3. The motivation for this is that both\nthe width and height axes are high-dimensional, and correlations generally weaken with increasing\ndistance, prompting us to selectively scan along these axes.\nGiven a batch of video latent z1:f \u2208 R(b,f,l,d), derived from a batch of original videos x1:F \u2208\nR(B,F,C,H,W) via a VAE, where l denotes the number of tokens in a sequence and d is the dimension\nof each token. The factorized local attention branch of the spatial component initially reshapes z1:f\ninto (bf, l, d) to facilitate spatial attention. For the i-th token in a sequence, define a sliding window\nS(i) that encompasses the current position along with surrounding positions,\nS(i) = {j | j \u2208 [max(1, i \u2212 \\lfloor\\frac{w}{2}\\rfloor), min(f, i + \\lfloor\\frac{w}{2}\\rfloor)]},\nhere w denotes the window size. The token at position i is then processed through multi-head\nattention with other tokens within the window to determine the output for that position,\nScorehij = \\frac{Qih(Kjh)T}{\\sqrt{dk}}, j\u2208S(i), h\u2208\u0397, Aijh = \\frac{exp(Scorehij)}{\\sum_{k\u2208S(i)} exp(Scorehk)}, Attentioni = \\sum_{jES(i)} AijhVjh,\nAttentioni = Concat(Attentioni1,..., AttentioniH)Wo,\nwhere H is the number of heads in multi-head attention. Qh = z1:fWh, Kh = z1:fWh,\nVh = z1:fWh, and each of Wh, W, Wh, Wo is a trainable weight matrix. We expand the input\ntensor z1:f along the width and height axes to perform spatial Mamba in the shapes (bh, fw, d) and\n(bw, fh, d), respectively, where w = h = \u221al. To fuse the selective scanning results from these two\ndirections, we integrate them using a gating mechanism.\nAs shown in Figure 3, prior to performing the selective scan with Mamba, the sequence of fw or fh\ntokens is rearranged according to the spiral direction and its reverse (denoted as \u03a9). Selective scan\nare then performed on both rearranged sequences. Subsequently, the original order of the results is\nrestored using the spiral indexes, and the corresponding tokens at each position are summed (denoted\nas \u2295\u03a9). Without loss of generality, given an input tensor of shape (b, l, d), where b = bh, l = fw for\nheight axis scanning or b = bw, l = fh for width axis scanning. The spiral scanning process is\n{\\tilde{y}_1, \\tilde{y}_2, \u2026, \\tilde{y}_{of}} = SSM(A, B, C)(\\{z^{1:f}_{\u03a9^1},z^{1:f}_{\u03a9^2},\u2026,z^{1:f}_{\u03a9^{of}}\\}),\\n{\\tilde{y}_{\u03a9^{of}}, \\tilde{y}_{\u03a9^{of-1}}, \u2026, \\tilde{y}_{\u03a9^{1}}} = SSM(A, B, C)(\\{z^{1:f}_{\u03a9^{of}},z^{1:f}_{\u03a9^{of-1}},\u2026,z^{1:f}_{\u03a9^{1}}\\}),\\n\\{y_1, y_2, \u2026, y_{of}\\} = \\{\\tilde{y}_{\u03a9^1}, \\tilde{y}_{\u03a9^2}, \u2026, \\tilde{y}_{\u03a9^{of}}\\} \u2295\u03a9^{-1} \\{\\tilde{y}_{\u03a9^{of}}, \\tilde{y}_{\u03a9^{of-1}}, \u2026, \\tilde{y}_{\u03a9^{1}}}\\},\n\nHere zi \u2208 R(b,1,d) denotes the i-th token in the sequence. Following a spiral scan across both\nwidth and height dimensions, gated fusion is utilized to synthesize the outcomes from both axes,\ny = \u03c3(W1yw + b1) \u2299 yw + \u03c3(W2yh + b2) \u2299 yh,\nwhere yw := {y1, y2, \u2026, yfw} and yh := {y1,y2,\u2026\u2026, yf\u0127}. \u2299 is activation function. W1, W2,\nb1 and b2 are trainable parameters.\nFollowing the modeling of individual video frames, we reshape the tensor z1:f to shape (bl, f, d) and\napply the temporal Mamba to selective scan along the frame axis, thereby extending the receptive\nfield to encompass the entire video.\n{y1, y2, \u2026, yf} = SSM(A, B, C)({zi1:f, zi1:f,...,zi1:f}).\nComputation Efficiency. For a video latent representation z1:f \u2208 R(1,f,l,d), the computational\ncomplexities of spatio-temporal self-attention and our method are as follows:\nO(Spatio-temporal self-attention) = f(4ld2 + 2l2d) + l(4fd2 + 2f2d) = 8fld2 + 2fl2d + 2lf2d,\nO(Ours) = f(4ld2 + 2lwd) + 2h(3fw(2d)N + fw(2d)N2) + 2w(3fh(2d)N + fh(2d)N2)\n+2l(2f(2d)N + f(2d)N2) = 4fld2 + 2flwd + 18fldN + 6fldN2.\nN is a fixed parameter of Mamba, which is generally less than l. Eq.(11) demonstrates that the\nproposed video Mamba diffusion model significantly reduces the computational complexity.\n3.2 Optical Flow Representation Alignment\nRecent studies (Chen et al.; Xiang et al., 2023b; Li et al., 2023) have demonstrated that meaningful\ndiscriminative feature representations can be induced within diffusion models during the denois-\ning process, achieved by implicitly learning the representation h as the hidden state of a denoising\nautoencoder that reconstructs the original data x from its corrupted version. Compared to relying\nsolely on the diffusion model to independently learn these representations, this method can enhance\ntraining efficiency and improve the quality of generation (Yu et al., 2024). This insight motivate\nus to enhance generative models by integrating external self-supervised representations. However,\napplying this approach to video generation presents challenges. First, the complex pixel movements\nbetween video frames are not fully considered. Most existing studies focus on image generation\ntasks, and the few that address video generation (Li et al., 2024) usually treat videos as sequences of\nindividual images-inputting single-frame images into self-supervised models to obtain representa-\ntions, and then integrating these representations. Second, there is an input mismatch problem: most\nself-supervised learning encoders are trained on clean images, whereas latent diffusion models typ-\nically use compressed latent representations from VAE with added noise as input. To address these\nproblems, we start by using a pre-trained optical flow prediction model RAFT (Teed & Deng, 2020)\nto track pixel motion across video frames, and employ a self-supervised visual model DINO (Caron\net al., 2021) to extract motion feature representations. We then sample these motion feature rep-\nresentations and compute the covariance with the output of the spatial modeling from the video\nMamba diffusion model, without requiring any additional parameter training.\nOptical flow estimation relies on analyzing consecutive frame pairs. To maintain consistency, we\nsupply the optical flow prediction model with an extra frame, resulting in one more input frame\nthan the VAE receives. Initially, the input x1:F+1 is processed using the optical flow prediction\nmodel RAFT to generate optical flow vector field O1:F. The i-th field encompasses the horizontal\ncomponent Oi and the vertical component Oi, which define the vector transitions from the i-th\nframe to the (i + 1)-th frame and can be expressed as\nO_i^u = \\begin{bmatrix}\nu^i(1,1) & u^i(1,2) & ... & u^i(1,W) \\\nv^i(2,1) & u^i(2,2) & ... & u^i(2,W) \\\n... & ... & ... & ... \\\nv^i(H,1) & u^i(H,2) & ... & u^i(H,W) \\\n\\end{bmatrix}, O_i^v = \\begin{bmatrix}v^i(1,1) & v^i(1,2) & ... & v^i(1,W) \\\nv^i(2,1) & v^i(2,2) & ... & v^i(2,W) \\\n... & ... & ... & ... \\\nv^i(H,1) & v^i(H,2) & ... & v^i(H,W) \\\n\\end{bmatrix},\n\nSince the self-supervised foundation model DINO accepts a three-dimensional RGB image as input,\nthe two-dimensional optical flow field must be further processed. Specifically, for the horizontal\ncomponent ui(x, y) and vertical component vi(x, y) at position (x, y) in the i-th optical flow field,\nwe encode the two-dimensional optical flow vector into an RGB pixel. This representation not only\nintuitively illustrates the direction of motion but also effectively conveys the intensity of motion.\nMathematically,\nMi(x,y) = \\sqrt{u^i(x,y)^2 + v^i(x,y)^2}, A^i(x,y) = arctan(\\frac{v^i(x,y)}{u^i(x,y)}),\\nHi(x,y) = \\frac{A^i(x,y)}{2\\pi}\u00d7 360\u00b0, Si(x, y) = 1, Vi(x,y) = \\frac{M^i(x,y)}{M^i_{max}},\\\n(Ri(x, y), Gi(x, y), Bi(x, y)) = HSVtoRGB(Hi(x, y), Si(x, y), Vi(x, y)).\nEq.(13) initially represents ui(x, y) and vi(x, y) in terms of motion intensity Mi(x, y) and direction\nAi(x, y). Subsequently, these components are mapped to the HSV color space, where Hi(x,y),\nSi(x, y), Vi(x, y) respectively denote hue, saturation, and value. Finally, the HSV pixel is converted\nto an RGB pixel using a standard color space conversion method.\nWe employ the self-supervised foundation model DINO, which exhibits high semantic relevance, to\nextract dense features from optical flow images. Specifically, the optical flow image derived from\nEq.(13) is fed into the DINO model to yield multi-scale representations that span from shallow to\ndeep layers, denoted as [h1, h2,\u2026\u2026,hL]. These representations are then aligned with the reshaped\noutput features [y1, y2,\u2026,yf] of the spatial component. Here hi \u2208 R(\u00d1,K), yi \u2208 R(K,d),\nK = bFl, K = bfl and l is the sequence length of DINO. We utilize the relative distribution\nsimilarity (Tang et al., 2024) to match features,\nr_{i,d_i}^{1:f} = 1 - \\frac{\\sum_{k=1}^{K} (\\<h_{vidi},Sampling(\\hat{h}_{vidi})> - \\mu_{i,d})^2}{\\sum_{k=1}^{K}(Sampling(\\hat{h}_{vidi}) - \\mu_{i,d})^2 \\sum_{k=1}^{K}(h_{vidi} - \\mu_{i,d})^2},\nwhere \\mu_{i,d_i} = \\frac{1}{K}\\sum_{k=1}^{K}Sampling(\\hat{h}_{vidi}), \\mu_{i,d_i}=\\frac{1}{K}\\sum_{k=1}^{K}\\hat{h}_{vidi}"}, {"title": "4 Experiments", "content": "4.1 Implementation Details\nVideo Mamba diffusion model was trained using the AdamW optimizer (Loshchilov, 2017), which\nemploys a phased learning rate adjustment strategy. During the initial 2 epochs, a learning rate of\n1 \u00d7 10-3 and a batch size of 16 was used to accelerate convergence. Subsequently, the learning\nrate was fixed at 1 \u00d7 10-4 and the batch size was set to 2 to achieve more stable and fine-grained\nparameter optimization. In alignment with the settings of the sole existing medical video generation\nwork (Li et al., 2024), we configured the training video to consist of 16 frames, each with a resolution\nof 128 x 128 pixels. In addition, we randomly selected 8 images for joint training, each matching\nthe resolution of the video frame. Data augmentation was performed solely through horizontal\nflipping. Consistent with standard practices in diffusive generative models, we did not modify the\ndecay/warm-up schedule, learning rate, or Adam B1/B2 hyperparameters. Training incorporated\nthe exponential moving average (EMA) (Tarvainen & Valpola, 2017; Wang et al., 2022) with a\ndecay rate of 0.9999. The performance outcomes of the EMA model were reported for final result\nsampling. Each dataset undergoes training for one week using early stopping strategy to prevent\noverfitting, in line with standard practices for video-related tasks. Gradient clipping is initiated at\nthe 25-th epoch, with the clip max norm set to 1.0 to ensure training stability. Unless otherwise\nspecified, the hyperparameter \u03b1 is set to 0.01. Most of our experiments utilized multiple Nvidia\nRTX 3090 Ti GPUs. During the inference stage, videos are generated via DDIM sampling (Song\net al., 2020). Additional training details and network specifications are outlined in Table 1.\nDuring the training of the frequency compensation video VAE, we utilize the Adam optimizer, con-\nducting training across each dataset for 10 epochs, setting the learning rate to 2 \u00d7 10-7, and config-\nuring the momentum parameters B\u2081and B2 to 0.5 and 0.9, respectively."}, {"title": "5 Conclusion", "content": "This paper introduces MedSora, an exploratory framework for medical video generation. Specifi-\ncally, by considering the advantages and disadvantages of attention and Mamba, we propose a video\nMamba diffusion model framework that efficiently models the spatio-temporal information in med-\nical videos. To accurately capture inter-frame pixel motion, we propose optical flow representation\nalignment to assist in the training of the diffusion model. Additionally, we enhance the 3D causal\nVAE for efficient compression of medical videos. In benchmark experiments on medical video\ndatasets, MedSora demonstrated excellent performance across various metrics, computational effi-\nciency, and impressive potential as a medical video simulator for downstream tasks. We anticipate\nthat this work will inspire further research in medical video generation and contribute significant\nbreakthroughs to generative AI in medicine."}]}