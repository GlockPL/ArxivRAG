{"title": "LORTA: LOW RANK TENSOR ADAPTATION OF LARGE LANGUAGE MODELS", "authors": ["Ignacio Hounie", "Charilaos Kanatsoulis", "Arnuv Tandon", "Alejandro Ribeiro"], "abstract": "Low Rank Adaptation (LoRA) is a popular Parameter Efficient Fine Tuning (PEFT) method that effectively adapts large pre-trained models for downstream tasks. LORA parameterizes model updates using low-rank matrices at each layer, significantly reducing the number of trainable parameters and, consequently, resource requirements during fine-tuning. However, the lower bound on the number of trainable parameters remains high due to the use of the low-rank matrix model. In this paper, we address this limitation by proposing a novel approach that employs a low rank tensor parametrization for model updates. The proposed low rank tensor model can significantly reduce the number of trainable parameters, while also allowing for finer-grained control over adapter size. Our experiments on Natural Language Understanding, Instruction Tuning, Preference Optimization and Protein Folding benchmarks demonstrate that our method is both efficient and effective for fine-tuning large language models, achieving a substantial reduction in the number of parameters while maintaining comparable performance.", "sections": [{"title": "1 INTRODUCTION", "content": "The advent of Large Language Models (LLMs) \u2013 billion parameter scale models pre-trained on vast corpora of data \u2013 has enabled unprecedented capabilities across a wide range of tasks. However, as LLM sizes continue to grow exponentially, their computational and memory demands represent significant challenges, particularly for those lacking access to high-performance computing infrastructure (Varoquaux et al., 2024). This has spurred interest in parameter efficient fine tuning (PEFT) techniques (Ding et al., 2023), which facilitate the adaptation of LLMs to specific applications, downstream tasks or user preferences, by using only a small fraction of trainable parameters. Most importantly, they reduce GPU memory requirements, primarily by shrinking optimizer states (Liao et al., 2023). Moreover, they provide greater efficiency in storage and deployment, enabling the management of multiple fine-tuned LLMs with reduced storage footprints and faster load times (Sheng et al., 2023; Wen & Chaudhuri, 2024), which is particularly relevant for applications requiring rapid model switching across numerous task- or user-specific models.\nBeyond computational benefits, PEFT techniques can also mitigate overfitting risks associated with fine-tuning high-capacity LLMs. By constraining model updates, PEFT methods can act as an implicit regularization mechanism, improving generalization (Fu et al., 2023; Sun et al., 2023). Parameter sharing, a well-established technique in deep learning architecture design, has been shown to improve generalization across various tasks such as protein folding (Jumper et al., 2021; Lin et al., 2023), image segmentation (Ronneberger et al., 2015), and generative modeling (Rombach et al., 2022). Incorporating parameter sharing in PEFT methods has also improved performance in specialized applications with limited data, such as in medical domains (Dutt et al., 2023; Zhu et al., 2024).\nLow Rank Adaptation (LoRA) is a popular PEFT approach that uses a low rank parametrization of weight matrix updates (Hu et al., 2021). For instance, these allow to fine tune a 175 billion parameter LLM using only 5 million trainable parameters (Hu et al., 2021) without performance degradation. Since the model updates can be merged with the frozen weights, LoRA incurs no"}, {"title": "2 PRELIMINARIES", "content": "However, the lower bound on trainable parameters often remains substantial for large-scale models. Recent works have aimed to further reduce the number of parameters in LoRA by allocating different ranks across update matrices in different layers (Valipour et al., 2022; Zhang et al., 2023b), using fixed low rank projections (Zhang et al., 2023a; Kopiczko et al., 2023), and parameterizing low rank matrices using a random basis (Koohpayegani et al., 2024).\nIn this work we introduce Low Rank Tensor Adapters (LoRTA), which exploit redundancy in weight updates across different layers, heads and attention matrices by representing updates into a unified 5th-order low-rank tensor model. This holistic approach can not only reduce the number of trainable parameters but also facilitate learning by exploiting the shared information among various components of the model. We also show that the parameter sharing schemes in state-of-the-art methods (Kopiczko et al., 2023; Koohpayegani et al., 2024) are, in fact, implicit low-rank tensor models with fixed random factors, while our explicit tensor model is fully trainable and thus more expressive. Moreover, our higher order low rank tensor model allows greater parameter efficiency than those recently proposed in the context of vision transformers (Jie & Deng, 2023; Edalati et al., 2023).\nThe two main advantages of our proposed method over existing matrix-based approaches can be summarized as follows:\n(A1) It enables a reduction in the number of trainable parameters by using updates with lower tensor rank.\n(A2) It provides finer-grained control of adapter size.\nWe evaluate our method on diverse benchmarks including Natural Language Understanding, Instruction Tuning, Preference Optimization, and Protein Folding. Our experiments demonstrate that LORTA can achieve up to an order of magnitude reduction in the number of trainable parameters compared to state-of-the-art PEFT methods, with minimal performance trade-offs."}, {"title": "2.1 TRANSFORMER ARCHITECTURE", "content": "We focus on the transformer architecture, although it can be naturally extended to other architectures such as Convolutional Neural Networks and Long Short Term Memory networks. We adopt the problem setting presented in (Thickstun, 2021). In the transformer model, an initial embedding layer maps input tokens to d-dimensional vector representations. These embeddings then pass through a series of layers, each performing multi-head attention, normalization and feed-forward operations. The input to the l\u2014th layer of the transformer is a matrix $X^{(l)} \\in \\mathbb{R}^{N\\times d}$, where N is the number of queries, represented in a d-dimensional feature space. A transformer layer with H attention heads is then defined as follows:\n$X^{(l+1)} = f(X^{(l)}) = X^{(l)} + Attn(X^{(l)})$\n$Attn(X^{(l)}) = \\sum_{h=1}^{H} \\sigma( X^{(l)} K_h^{(l)} Q_h^{(l)T} ) X^{(l)} V_h^{(l)} P_h^{(l)T}$\n$MLP (X^{(l)}) = ReLU (X G_1^{(l)} + 1_b b_1^{(l)}) G_2^{(l)} + 1_b b_2^{(l)}$,\nwhere $K_h^{(l)}, Q_h^{(l)}, V_h^{(l)}, P_h^{(l)} \\in \\mathbb{R}^{d \\times d}$ are the key, query, value and projection matrices respectively, for head h and layer l."}, {"title": "2.2 LOW RANK (MATRIX) ADAPTATION", "content": "LORA modifies the pre-trained weights by adding a trainable update. Explicitly, at each layer and head h:\n$K_h = K_h^0 + dK_h, Qh = Q_h^0 + dQ_h, V_h = V_h^0 + dV_h, Ph = P_h^0 + dPh,$\nwhere $K_h^0, Q_h^0, V_h^0, P_h^0$ denote the pre-trained weights and $dK_h, dQ_h, dV_h, dP_h$ the trainable adapters.\nWhile each attention head's MLP contains two trainable matrices, $G_1$ and $G_2$, our focus is on fine-tuning the attention matrices. This has been demonstrated to be effective for LLM adaptation (Hu et al., 2021; Kopiczko et al., 2023). Nevertheless, these methods can be easily extended to other parameters, including the MLP weights.\nLet $W_h \\in \\{Q_h, K_h, V_h, P_h\\}$ for h = 1, . . ., H denote the query, key, value and projection matrices, respectively, for each attention head. After concatenating updates across all attention heads, we get:\n$d\\hat{W} = (dW_1, ..., dW_H) \\in \\mathbb{R}^{d \\times d}$.\nHu et al. (2021) proposed to parametrize the updates using rank-r matrices, which can be expressed as\n$dW = \\frac{\\alpha}{r} A B^T, A, B \\in \\mathbb{R}^{d \\times r},$\nwhere \u03b1 is a constant and r denotes the rank of the update. The scaling factor simply aims to reduce the efforts of re-tuning the learning rate when training adapters of varying rank. It has been shown that while this scaling heuristic works well for smaller ranks, it can be sub-optimal for larger ranks (Kalajdzievski, 2023). Hayou et al. (2024) have also shown that setting the learning rate for the A and B matrices appropriately can further improve convergence and performance.\nAlthough LoRA is an efficient fine-tuning technique, the number of parameters required for each layer is at least $8 \\cdot d \\cdot r$. Thus, the total number of trainable parameters is:\n$\\#parameters (LoRA) = 8 \\cdot d \\cdot L \\cdot r,$\nwhere L is the total number of layers. Even with r = 1, this results in $8 \\cdot d \\cdot L$ parameters. In practice, for LLMs with high dimensionality (d) and many layers (L), this lower bound can still lead to a significant number of trainable parameters.\nLORA has also been combined with model weight quantization (Dettmers et al., 2024), further decreasing resource requirements. Unlike adapter-based PEFT methods (Houlsby et al., 2019; Pfeiffer et al., 2020; R\u00fcckl\u00e9 et al., 2020; He et al., 2021), LoRA does not introduce additional inference time overhead during deployment, as the trainable matrices can be integrated with the fixed weights.\nBuilding upon this foundation, AdaLoRA (Zhang et al., 2023b) expands the LoRA technique by introducing dynamic rank adjustment for low-rank matrices during fine-tuning. The fundamental concept involves optimally allocating the parameter resources by selectively pruning less crucial components of the matrices based on an importance metric. LoRA-FA (Zhang et al., 2023a) reduces the number of trainable parameters by freezing the A matrix to its random initialisation, while achieving similar performance to LoRA."}, {"title": "2.3 TENSOR ALGEBRA", "content": "In the following sections we introduce our proposed LoRTA framework, which is a tensor adaptation model for PEFT. To facilitate the upcoming analysis, we briefly present some tensor algebra prelimi- naries and refer the reader to Appendix A and Sidiropoulos et al. (2017); Kolda & Bader (2009) for further details.\nA N-order tensor $X \\in \\mathbb{R}^{I_1 \\times I_2 \\times \\dots \\times I_N}$ is an N-way array indexed by $i_1,i_2,...,i_n$ with elements $X(i_1,i_2,...,i_n)$. It consists of N types of modes: $X(:,i_2,...,i_n), X(i_1,: ,..., i_N), ..., X(i_1, i_2, . . ., :)$. Any tensor can be decomposed as a sum of N-way outer products as:\n$X = \\sum_{r=1}^R A_1[:, r] \\circ A_2[:, r] \\circ \\dots \\circ A_N[:, r]$,"}, {"title": "3 TENSORS ARE ALL YOU NEED", "content": "where $A_n = [a_{n_1}, a_{n_2}, ..., a_{n_r}] \\in \\mathbb{R}^{I_n \\times R}, n = 1, . . ., N$ are called the low rank factors of the tensor. The above expression represents the canonical polyadic decomposition (CPD) or parallel factor analysis (PARAFAC) (Harshman & Lundy, 1994) of a tensor. A tensor can be fully characterized by its latent factors, so we can represent a tensor by its CPD model as:\n$X = [[A_1, A_2, ..., A_N]].$\nUnlike other tensor models, such as Tucker and Block Term Decomposition (BTD), the CPD model is unique under certain conditions. As a result, the CPD model is often preferred when the goal is to minimize the number of parameters.\nA tensor can also be represented as a set of matrices, by fixing all the modes but two as:\n$X[:, :, i_3, ..., i_N] = A_1 (Diag (A_3 (i_3, :)) \\dots Diag (A_N (i_N, :))) A_2^T,$\nwhere Diag $(A_n (i_n, :))$ is the diagonal matrix with diagonal equal to $A_N (i_n, :)."}, {"title": "3.1 PARAMETER SHARING ACROSS LAYERS", "content": "To further increase the compression ratio in PEFT models, recent works (Kopiczko et al., 2023; Koohpayegani et al., 2024) suggest sharing parameters across layers that usually operate as predefined projection matrices. As we see next, this leads to tensor factorization models with fixed and learnable parameters.\nVector-based Random Matrix Adaptation (VeRA) Kopiczko et al. (2023) have proposed to parameterize updates using two learnable vectors at each layer and fixed random matrices shared across all layers. The update at each layer can be expressed as\n$dW = A Diag (C_D[l, :]) B^T Diag (C_B[l,:]),$\nwhere A, B \u2208 Rd\u00d7r are the random projections, and CD \u2208 RL\u00d7r, CB \u2208 RL\u00d7d are matrices that collect trainable vectors across layers. The model in 10 is a coupled matrix factorization model and is similar to a tensor model. In particular, if we remove the CB term VeRA can be interpreted as a low-rank tensor CPD parametrization with fixed random factors. That is, the weight update W is a rank-r third order tensor $T\\in \\mathbb{R}^{d\\times d\\times L}$. Note that, omitting the CB term has been shown to lead to a small performance degradation unlike omitting CD (Kopiczko et al., 2023).\nRandom Matrix basis Adaptation (NOLA) In a similar manner, Koohpayegani et al. (2024) have proposed to parameterize the weight update by expressing the matrices A and B as linear combinations of fixed random basis matrices, that are shared across all layers. The weight update dW for layer l is then given by:\n$\\delta W_l = \\sum_{i=1}^k\\sum_{j=1}^k\\alpha_{(i,l)}\\beta_{(j,l)} A_i B_j^T,$\nwhere $A_i, B_j \\in \\mathbb{R}^{dxr}$ are fixed random matrices, shared across all layers, and $a_l = \\{\\alpha_{(i,l)}\\}_{i=1}^k$ and $B = {\\beta_{(i,l)}\\}_{i=1}^k$ are the learned coefficients for each layer. If we stack the random matrices $A_i, B_j \\in \\mathbb{R}^{d\\times r}$ into tensors $\\mathbb{A}, \\mathbb{B}$ such that: $\\mathbb{A}[:, :, i] = A_i$ and $\\mathbb{B}[:, :, j] = B_j$, then 11 can be cast as:\n$\\delta W_l = \\sum_{i=1}^k\\sum_{j=1}^k\\alpha_{(i,l)}\\beta_{(j,l)} \\sum_{m=1}^r A[:, m, i]B[:, m, j]^T = \\sum_{m=1}^r A[:, m, :] (\\alpha_l \\beta_l^T) B[:, m, :]^T,$\nand $d\\hat{W_l}$ admits the following factorization. $d\\hat{W_l} = \\sum_{m=1}^r P^{(m)} (\\alpha_l \\beta_l^T) P^{(m)T}$, where $P^{(m)} = A[:, m, :]$, and $P^{(m)} = B[:, m, :]$ are also random projection matrices with different dimensions compared to Ai, Bj. As a result, NOLA can be viewed as the following tensor factorization model:"}, {"title": "3.2 LORTA: A GENERAL TENSOR MODEL", "content": "$\n$\\delta\\hat{W} = \\sum_{m=1}^r [P^{(m)} \\mathbb{A}, P^{(m)} \\mathbb{B}, \\mathbb{I} | \\mathbb{A}[:, 1] = \\alpha_l, \\mathbb{B}[:, 1] = \\beta_l].$\nThe expression in 13 is a a summation of CPD models, also known as Block Term Decomposition, which is a powerful tensor model, but can lack parsimony (Kolda & Bader, 2009).\nIn the previous section, we explored PEFT models that share parameters across layers, highlighting their correspondence to tensor factorization models. Namely, VeRA and NOLA utilize fixed projection matrices shared across layers. However, this strategy can result in models that are larger than necessary relative to their degrees of freedom due to the inclusion of these random matrices. Although these matrices can be generated on the fly by solely storing the pseudo-random number generator seed, this still incurs additional resource demands during training, and increases loading time for inference.\nTo address this issue, we propose modeling the trainable adapters using a low-rank CPD structure. This choice is motivated by the favorable properties of CPD: it is universal, capable of exactly factorizing any tensor, yet remains concise and parsimonious, typically requiring only a small number of parameters to achieve low approximation error (Sidiropoulos et al., 2017). These contrasts with tensor adapters used in vision (Jie & Deng, 2023) which rely on Tucker and Tensor-Train models. In fact, for small ranks, CPD is equivalent to Tucker when the core tensor in Tucker is the identity tensor. However Tucker is always parametrized with a dense tensor and therefore requires a larger number of parameters for the same rank.\nLoRTA represents all weight updates as a 5th-order tensor $T\\in \\mathbb{R}^{d\\times d\\times H\\times L\\times 4}$. By integrating updates of layers, heads and the Q, K, V, P matrices into a unified low-rank CPD tensor model, LORTA exploits redundancy across different modes of the tensor. This approach can thus not only improve parameter efficiency but also facilitate learning by exploiting the shared information among various components of the model. By utilizing a low-rank CPD model, we can express this tensor as:\n$T = [[\\mathbb{A},\\mathbb{B},C_H,C_L,C_M]],$\nwhere $\\mathbb{A} \\in \\mathbb{R}^{d\\times r}$ and $\\mathbb{B} \\in \\mathbb{R}^{d\\times r}$ are factor matrices for the input and output dimensions, respectively, and $C_H \\in \\mathbb{R}^{H\\times r}, C_L \\in \\mathbb{R}^{L\\times r}, C_M \\in \\mathbb{R}^{4\\times r}$ are factor matrices for the attention heads, layers, and the four matrices Q, K, V, P. Each weight matrix update can then be retrieved as:\n$T[:, :, k, l, i] = \\mathbb{A} (Diag (C_H [k, :]) Diag (C_L [l, :]) Diag (C_M[i, :])) \\mathbb{B}^T,$\nwhere k indexes the attention heads, l indexes the layers, and i indexes the matrices Q, K, V, P. Note that, unlike previous implicit tensor models such as NOLA and VeRA, which rely on fixed random projections or parameters and learn only scaling coefficients, our proposed model is fully trainable. All factor matrices (A, B, CH, CL, CM) are learned during training, providing greater expressiveness and forgoing the dependency on pre-defined random bases or projections.\nThe LoRTA update then has the following number of parameters:\n$\\#parameters (LoRTA) = r \\times (d+\\frac{d}{H}+ h + L + 4).$\nTo fairly compare the parameter efficiency of LoRTA with LoRA, we adjust the tensor rank in LORTA to match the effective total tensor rank in LoRA, which is r\u2032 = r \u00d7 4L due to LoRA applying a rank r update to each of the 4L matrices individually. For a given tensor rank, LoRTA reduces the number of parameters from scaling 8dLr in LoRA to 4L(d(1 + 1/h) + h + L + 4)r in LoRTA (usually d \u226b L and d \u226b h), achieving substantial parameter savings without compromising expressive power. For example, this amounts to a 47.6% reduction in a LLaMA2 7B model. A breakdown of the parameter efficiency gains coming from each of the added modes \u2013 i.e., heads, layers and attention matrices \u2013 can be found in Table 2 in Appendix C. In order to illustrate the parameter efficiency gains Figure 1 compares - for a single weight update \u2013 LoRA with a rank one tensor parametrization that adds heads as a mode."}, {"title": "4 EXPERIMENTS", "content": "We evaluate our approach by fine-tuning RoBERTa-base and RoBERTa-large models on the General Language Understanding Evaluation (GLUE) benchmark. Following the setup from Kopiczko et al. (2023), we tune all attention matrices in each self-attention module and fully train the classification head. The number of trainable parameters reported includes only the attention layers, as the classifier is fully fine-tuned by all methods. As in previous works, separate learning rates are used for the classification head and the adapted layers, which are adjusted through a grid search. All other parameters, including batch size, maximum sequence length, and number of epochs, match those used in Kopiczko et al. (2023). Detailed settings can be found in Table 3 in Appendix D.1.\nBaselines We benchmark our method against the following methods:\n\u2022 Full finetuning: all parameters are trained.\n\u2022 Bitfit (Zaken et al., 2021): Only the bias vectors are fine-tuned.\n\u2022 Adapter tuning: Includes several variations:\nAdapter (Houlsby et al., 2019): Adapter layers between self-attention and MLP modules with residual connection.\nAdapterP (Pfeiffer et al., 2020): Adapter layer after the MLP module and LayerNorm.\nAdapterDrop (R\u00fcckl\u00e9 et al., 2020): Omits some layers for efficiency.\n\u2022 LoRA (Hu et al., 2021), LoRA-FA (Zhang et al., 2023a) and VeRA (Kopiczko et al., 2023):\nAs previously described.\nThe results in Table 1 show that LoRTA achieves competitive performance, with less than a 2% average reduction compared to the best-performing methods. This is achieved with over six times fewer parameters than the most efficient method (VeRA) and over a hundred times with respect to LORA."}, {"title": "4.2 INSTRUCTION TUNING", "content": "We fine-tune the 7 billion parameter Llama2 (Touvron et al., 2023) models on the cleaned Alpaca instruction tuning dataset (Taori et al., 2023). We train for one epoch, preceded by a warm-up learning rate sweep as in the standard setting. Other hyperparameters are detailed in Appendix D.2.\nAs shown in Figure 2, LoRTA effectively reduces the number of parameters to a fraction of those required by the lowest rank in LoRA, with only a small performance cost. The loss decreases monotonically with the number of parameters used, both in training and testing, and LoRTA even demonstrates superior performance with fewer parameters for ranks 96 and 192. To further evaluate"}, {"title": "4.3 PREFERENCE OPTIMIZATION", "content": "While various techniques to align LLMs with human preferences on specific tasks exist (see, for example, Kaufmann et al. (2023) and references therein), we utilize Direct Preference Optimization (DPO) (Rafailov et al., 2024) due to its simplicity and effectiveness.\nWe fine-tune the 7 billion parameter Llama 2 model (Touvron et al., 2023) on the cleaned version of the Intel Orca dpo pairs dataset\u00b9. This synthetic preference dataset comprises 6k prompts across various domains and tasks, along with the corresponding outputs from ChatGPT and Llama2-13B. In this version of the dataset, ChatGPT is used to score outputs and the preferred choices are designated based on these scores. Because preference datasets are often small, a KL regularization that penalizes deviations from the pre-trained model's outputs is used to mitigate overfitting. In our experiments, the regularization coefficient \u1e9e was set to 0.1. We use Huggingface Transformer Reinforcement Learning (trl) library\u00b2. For a complete description of hyperparameters see Appendix D.3.\nAs shown in Figure 4 LoRTA exhibited non-monotonic performance across ranks. This suggests that further hyperparameter tuning may be necessary to stabilize its performance. Although we did not tune hyperparameters, most ranks still outperformed LoRA with significantly fewer parameters. We further evaluated the fine-tuned models on the LLM-as-a-judge MT-benchmark. In this setting,\nLORTA consistently outperformed LoRA across all ranks, including at rank 2 where it had shown higher DPO loss on the preference dataset. This improvement suggests enhanced out-of-distribution generalization capabilities for LoRTA adapters since MT-bench differs from the training dataset."}, {"title": "4.4 PROTEIN FOLDING", "content": "Protein folding, the process by which a protein's amino acid sequence determines its three-dimensional structure, is a fundamental problem in molecular biology. Accurate prediction of protein structures from their sequences has significant implications for understanding protein function and designing new proteins for therapeutic purposes. ESMFold (Lin et al., 2023) is a frontier model for this task trained in two stages. First, ESM-2, a BERT-based (Devlin et al., 2019) protein language model, is trained with the masked-language-modeling objective on amino acid sequences. This unsupervised pretraining allows the model to capture complex patterns and relationships within protein sequences. Remarkably, valuable structural information emerges in the model's features during this process (Rao et al., 2020). In the second stage, ESM-2 is frozen, and a model head predicting three-dimensional protein structures is trained on top of language model features.\nWe re-train ESMFold in the second stage \u2013 fine-tuning ESM-2 parameters (we use ESM-2 35M instead of the ESM-2 3B model used in Lin et al. (2023) due to compute constraints) with LoRA and LoRTA instead of freezing them. We evaluate performance with the Local Distance Difference Test for Ca atoms (LDDT-Ca) (Mariani et al., 2013) \u2013 that measures accuracy of predicted protein structures by comparing the distance between alpha carbons in predicted and true structures. LDDT-Ca ranges from 0 (poor accuracy) to 1 (perfect match). See Appendix D.4 for experiment details.\nAs shown in Figure 5, all tested LoRTA ranks outperform rank 1 LoRA on the training set; on the validation set, all tested LoRTA ranks are competitive with rank 1 LoRA. Notably, rank 1 LoRTA is competitive with rank 1 LoRA despite having an order of magnitude fewer parameters."}, {"title": "5 CONCLUSION", "content": "We have introduced LoRTA, a novel approach that employs a low-rank tensor model for LLM updates. By extending low-rank adaptation to higher-order tensors, LoRTA overcomes the inherent lower bounds on the number of trainable parameters while offering finer-grained control over adapter size. Our experiments across various benchmarks demonstrate that LoRTA achieves comparable and sometimes superior performance than baselines at a reduced parameter count.\nFurthermore, we have shown that previous works have implicitly utilized low-rank tensor models with random factors. Nothing precludes our higher-order tensor model from using randomized factors for increased efficiency-a potential direction for future work that could further reduce computational overhead. Lastly, developing more efficient implementations of tensor operations that result in greater memory efficiency also remains a relevant future work direction which could make LoRTA even more suitable for resource-constrained environments."}, {"title": "A TENSOR ALGEBRA", "content": "To facilitate our analysis, we briefly present some tensor algebra preliminaries and refer the reader to Sidiropoulos et al. (2017); Kolda & Bader (2009) for further details.\nA N-order tensor $X \\in \\mathbb{R}^{I_1 \\times I_2\\times\\dots\\times I_N}$ is an N-way array indexed by $i_1,i_2,...,i_n$ with elements $X(i_1, i_2,...,i_n)$. It consists of N types of modes: $X(:,i_2,...,i_n), X(i_1,: ,..., i_N),..., X(i_1, i_2, . . ., :)$.\nA rank-one tensor $Z \\in \\mathbb{R}^{I_1\\times I_2\\times\\dots\\times I_N}$ is the outer product of N vectors defined as:\n$Z = a_1\\circ a_2\\circ... \\circ a_N,$\nwhere $a_1 \\in \\mathbb{R}^{I_1}, a_2 \\in \\mathbb{R}^{I_2},..., a_n \\in \\mathbb{R}^{I_N}$ and $\\circ$ denotes the outer product. The elementwise formula of the above expression is:\n$Z(i_1,i_2,...,i_n) = a_1 (i_1)a_2(i_2)\\dots a_n(i_n)$, for all$i_1, i_2, ...,i_n,$\nAny tensor can be realized as a sum of N-way outer products (rank one tensors), i.e.\n$X = \\sum_{r=1}^R a_1^r \\circ a_2^r \\circ... \\circ a_N^r.$\nThe above expression represents the canonical polyadic decomposition (CPD) or parallel factor analysis (PARAFAC) (Harshman & Lundy, 1994) of a tensor. The CPD elementwise representation is:\n$X(i, j, k) = \\sum_{r=1}^R A_1 (i_1, f) A_2(i_2, f) ... A_N (i_n, f),$\nwhere $A_n = [a_{n_1}, a_{n_2}, ..., a_{n_r}] \\in \\mathbb{R}^{I_n\\times R}, n = 1, . . ., N$ are called the low rank factors of the tensor. A tensor can be fully characterized by its latent factors, so we can represent a tensor by its CPD model as:\n$X = [[A_1, A_2, ..., A_N]].$\nA tensor can be also represented as a set of matrices, by fixing all the modes but two as:\n$X[:, :, i_3, . . ., i_N] =\nA_1 (Diag (A_3 (\u0456\u0437, :))... Diag (A_N (i_N, :))) A_2^T,$\nwhere Diag $(A_n (i_n, :))$ is the diagonal matrix with diagonal equal to $A_N (i_n, :)."}, {"title": "B ADDITIONAL RELATED WORK", "content": "Model Compression While these techniques differ from PEFT in that they focus on reducing the requirements of a trained model rather than efficient adaptation, they offer valuable insights for developing more efficient PEFT approaches. Pruning and quantization are key techniques for compressing neural networks, that have also been extensively applied to LLMs. Pruning removes less important weights, with some methods achieving high compression rates, e.g. (Ma et al., 2023). Quantization reduces weight precision, decreasing model size and also allowing more efficient operations (Lin et al., 2024a). Knowledge distillation is an alternative approach that involves transferring knowledge from a large \"teacher\" model to a smaller \u201cstudent\u201d model (Gu et al., 2024).\nLow Rank Training. Exploiting low rank structure to improve efficiency during both training and inference in deep models has long been studied (Sainath et al., 2013), and also combined with sparsity (Sprechmann et al., 2015). Recent advancements include Cuttlefish (Wang et al., 2023) and ELRT (Sui et al., 2024).\nData efficient fine tuning. An alternative approach to reducing fine-tuning costs is to reduce the amount of data. In this direction, Few-shot and continual learning approaches have been shown to be effective in LLM fine-tuning tasks (Lin et al., 2024b; Wang et al., 2024).\nEfficient Architectures Another relevant direction in resource usage is using more efficient model architectures. Mixture of Experts (MoE) technique, implemented in models like Switch Transform-ers (Fedus et al., 2022) and GLaM (Du et al., 2022), has shown promise in scaling model capacity"}, {"title": "C PARAMETER EFFICIENCY GAINS BREAKDOWN.", "content": "We provide a breakdown of the parameter savings achieved by our proposed method, LoRTA, compared to LoRA, by parameterizing the weight updates using low-rank tensor decompositions at different granularities. The table below summarizes the dimensions of the update tensors, the number of update tensors used, and the corresponding parameter savings when the tensor rank r matches the tensor rank of LoRA rank r. The first row corresponds to LoRA."}, {"title": "D EXPERIMENTAL DETAILS", "content": "In this appendix, we provide further details on the experiments presented in the main paper."}, {"title": "D.1 NLU", "content": "In our GLUE experiments we implemented our method using Huggingface's PEFT and VeRA's codebase, the hyperparameters are detailed below."}, {"title": "D.2 INSTRUCTION TUNING", "content": "For instruction tuning experiments we utilized Lightning AI's LitGPT codebase and training recipe. Hyperparameters are detailed below."}, {"title": "D.3 DPO", "content": "For preference optimization experiments we utilized using Huggingface trl library's dpo implementa- tion and example script. Hyperparameters are detailed below."}, {"title": "D.4 PROTEIN FOLDING", "content": "For protein folding experiments, we utilized OpenFold Ahdritz et al. (2024) training code and datasets. The following modifications were made"}]}