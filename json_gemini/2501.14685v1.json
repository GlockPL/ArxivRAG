{"title": "Rethinking Foundation Models for Medical Image Classification through a Benchmark Study on MedMNIST", "authors": ["Fuping Wu", "Bart\u0142omiej W. Papie\u017c"], "abstract": "Foundation models are widely employed in medical image analysis, due to their high adaptability and generalizability for downstream tasks. With the increasing number of foundation models being released, model selection has become an important issue. In this work, we study the capabilities of foundation models in medical image classification tasks by conducting a benchmark study on the MedMNIST dataset. Specifically, we adopt various foundation models ranging from convolutional to Transformer-based models and implement both end-to-end training and linear probing for all classification tasks. The results demonstrate the significant potential of these pre-trained models when transferred for medical image classification. We further conduct experiments with different image sizes and various sizes of training data. By analyzing all the results, we provide preliminary, yet useful insights and conclusions on this topic.", "sections": [{"title": "1. Introduction", "content": "Foundation models, which are pre-trained on large-scale diverse datasets, have achieved rapid advancement in various areas, such as computer vision (Awais et al., 2023), natural language processing (Zhou et al., 2023), and audio signal analysis (Huang et al., 2024). These models have been proven effective in adapting to numerous downstream tasks, including semantic segmentation (Wang et al., 2024), recommendation systems (Zhao et al., 2024), showcasing remarkable generalizability and adaptability (Touvron et al., 2023; Anil et al., 2023).\nIn recent years, the success of foundation models has triggered significant research into their application in medical image analysis (Zhang and Metaxas, 2024), either by transferring models from other fields (Mazurowski et al., 2023) or training models with large-scale medical data (Butoi et al., 2023). While studies have shown the great potential of foundation models in organ segmentation (Ma et al., 2024), clinical reports generation (Thawakar et al.), and medical Q&A systems (Chen et al., 2024), few have conducted large-scale comparisons of the existing models to provide insights into their performance in specific applications (Huix et al., 2024).\nIn this work, we examine the performance of various foundation models in medical image classification tasks. Although some studies have performed similar comparisons, they"}, {"title": "2. Method", "content": ""}, {"title": "2.1. Pre-trained Model Selection", "content": "With thousands of pre-trained models publicly available, we select representative backbones for validation and comparison, as shown in Figure 1, ranging from CNN-based to ViT-based architectures. For CNN-based models, we include two baseline models: VGG16 (Simonyan and Zisserman, 2014) and DenseNet-121 (Huang et al., 2017), and two models with skip-connection via addition: Efficient Net-B4 (Tan, 2019) and ResNet-18 (He et al., 2016). All these CNN-based models were trained with ImageNet-1k (Russakovsky et al., 2015) for image classification. For ViT-based models, we categorized them into two classes: (1) Four models trained for image classification: Three models, including ViT-B/16 (Steiner et al., 2021), CLIP ViT-B/16 (Ilharco et al., 2021) and SAM-C ViT-B/16 (Chen et al., 2021), are adopted for comparison. These models were trained with different optimization strategies and datasets (Wightman, 2019), all including ImageNet-1k. We also included SAM ViT-B/16 (Kirillov et al., 2023) to compare with SAM-C ViT-B/16. Although it was trained with"}, {"title": "2.2. General Validation Framework", "content": "As shown in Figure 1, to validate the selected foundation models on different datasets for classification tasks, we append a one-layer linear classifier after the encoder of each model. For a fair comparison, each classification model is trained with 15,000 iterations, and optimized with the AdamW optimizer (Loshchilov, 2017) using two optimization strategies, linear probing and end-to-end fine-tuning. In linear probing, the encoder is frozen and only the last classifier layer is optimized. The initial learning rate is set to 0.001, which is reduced by 0.9 every 200 iterations. For end-to-end fine-tuning, we adopt the same updating strategy as in linear probing for the classifier, while keeping the learning rate for the encoder, denoted as $lre$, as a constant. Using a low learning rate for the encoder is a common method to adapt the pre-trained model to downstream tasks (Baharoon et al., 2023). The selection of lre will be discussed in Section 3.2."}, {"title": "3. Benchmark Results", "content": ""}, {"title": "3.1. Datasets", "content": "To evaluate the potential of foundation models in medical image classification tasks, we utilize MedMNIST (Yang et al., 2023) collection for validation. As illustrated in Figure 1, MedMNIST comprises 12 2D datasets, with modality spanning X-ray, ultrasound, CT, and electron microscope. Each dataset is available in four different image resolutions: 28 \u00d7 28, 64 \u00d7 64, 128 \u00d7 128, and 224 \u00d7 224. We primarily use images of 224 \u00d7 224 for testing.\nDuring training, we employ the Cross-Entropy loss for multi-class classification and ordinal regression tasks, and the Binary Cross-Entropy loss for multi-label binary classification problems, such as the multi-label classification task for the ChestMNIST dataset. To maintain consistency with Yang et al. (2023), all experiments are conducted three times on a single NVIDIA A100 GPU. The performance of each model is assessed by the mean \u00b1 std of accuracy (ACC) and the area under the receiver operating characteristic curve (AUC)."}, {"title": "3.2. Results", "content": ""}, {"title": "3.2.1. LEARNING RATE SELECTION FOR END-TO-END FINE-TUNING STRATEGY", "content": "In the end-to-end fine-tuning process, we first determine a suitable learning rate, i.e., lre, for the encoder of each foundation model. For the CNN models, we select lre from {10-3, 10-4, 10-5}, while for ViT-based models, we choose from {10-4, 10-5, 10-6}. Figures 3-14 in Appendix B illustrate the performance of the 12 foundation models across all 12 datasets with various learning rates. Interestingly, the results indicate that for CNN models, lre = 10-4 generally yields optimal performance, while for ViT-based models, lre = 10-5 proves to be the best choice in most cases."}, {"title": "3.2.2. END-TO-END FINE-TUNING vs LINEAR PROBING", "content": "Table 1 and Tables 4-6 in Appendix C present the linear probing and the best end-to-end fine-tuning results for all foundation models across 12 datasets in the MedMNIST collection. Although the 12 models were pre-trained with natural images or languages, they demonstrate significant potential in transfer learning for medical image classification. Notably, end-to-end fine-tuning consistently outperforms linear probing across all datasets, in both accuracy and AUC metrics, except the AUC value on the PathMNIST dataset, where the DINO ViT-B/16 achieves 99.78% AUC with linear probing, 0.03% higher than the former. Particularly, on challenging tasks such as DermaMNIST, OCTMNIST, OrganCMNIST, OrganSMNIST, and TissueMNIST, end-to-end fine-tuning substantially outperforms linear probing."}, {"title": "3.2.3. CNN MODELS US VIT-BASED MODELS", "content": "When comparing the results of CNN models with those of ViT-based models, we observe that most of the best performance is achieved by ViT-based models in both linear probing and end-to-end fine-tuning settings. Notably, DenseNet-121 and EfficientNet-B4 achieve the highest accuracy on PneumoniaMNIST and the highest AUC on BloodMNIST, but only under the end-to-end fine-tuning strategy."}, {"title": "3.2.4. IMPACT OF TRAINING PIPELINE", "content": "It is important to note that Doerrich et al. (2024) adopted a different training pipeline when validating foundation models on MedMNIST, which led to conclusions that diverged from ours. Specifically, they employed the AdamW optimizer with a learning rate of 0.0001 and a cosine annealing learning rate scheduler (Loshchilov and Hutter, 2022) with a single cycle. When comparing the results for the 8 common models, highlighted in Table 3 in Appendix A, we found similar outcomes under the linear probing strategy. However, our results surpass theirs in the end-to-end fine-tuning setting, especially for ViT-based models. For instance, on BreastMNIST, ViT-B/16 achieved an accuracy of 83.76\u00b11.09 and an AUC of 86.18\u00b10.26 of AUC in their setup. In contrast, our setup yielded better results, with an accuracy of 91.67\u00b10.52 and an AUC of 94.09\u00b10.89. Similarly, on DermaMNIST, DINO ViT-B/16 achieved an accuracy of 81.31 \u00b1 1.05 in their work vs 89.39 \u00b10.12 in ours.\nBased on a different training pipeline, Doerrich et al. (2024) delivered several conclusions that could be contradictory to the results in this work. For example, they concluded that self-supervised pretraining models, such as CLIP and DINO, do not always improve medical image classification results in end-to-end fine-tuning while demonstrating enhanced performance with linear probing. Conversely, our results demonstrate that these models not only improve the performance in end-to-end fine-tuning, but also outperform linear probing. Additionally, they concluded that CNN models consistently outperform ViT-based models in accuracy with end-to-end training, while in our work we have an almost opposite conclusion. For example, on the DermaMNIST dataset, DINOv2 ViT-B/14, reached the highest accuracy of 92.02 \u00b1 0.32, while the best-performing CNN model, EfficientNet-B4, obtained an accuracy of 88.13 \u00b1 0.49."}, {"title": "4. Discussion", "content": ""}, {"title": "4.1. Effect of Image Size and Resizing Strategy", "content": "We further investigated the impact of resizing strategies on performance, particularly when handling smaller image sizes, as many foundation models require a fixed input size. Specifically, we compare zero-padding and scaling strategies for both end-to-end fine-tuning and linear probing, focusing on the DermaMNIST dataset with image sizes of 28 \u00d7 28, 64 \u00d7 64 and 128 \u00d7 128. Table 2 and Table 7 in Appendix D present the results with scaling and zero-padding strategies, respectively. The findings reveal that, generally, accuracy improves with larger image sizes when using zero-padding. Similarly, for the scaling strategy, larger image sizes typically result in higher accuracy. However, the differences, particularly among sizes of 64 \u00d7 64, 128 \u00d7 128, and 224 \u00d7 224 are marginal. Comparison plots illustrating these trends can be found in Figures 15-18 in Appendix E. Notably, for end-to-end fine-tuning, scaling tends to yield better accuracy than zero-padding across all image sizes. Conversely, for linear probing, the performance gap between scaling and zero-padding narrows, particularly when the image size is 128 \u00d7 128. For a detailed comparison, please refer to Figures 19-24 in Appendix E."}, {"title": "4.2. Data Efficiency", "content": "We further investigated whether these foundation models can be adapted to medical image classification tasks with limited training data using end-to-end fine-tuning or linear probing. This analysis was conducted on the DermaMNIST dataset, varying the number of training images per class in {20, 40, 60, 80, 100, 200, 400, 600}. As illustrated in Figure 2, the accuracy of DINO VIT-B/16 improves significantly with an increasing number of training images for both end-to-end fine-tuning and linear probing. This indicates the low data efficiency of DINO ViT-B/16 when transferring to this task. Similar trends were observed across the other 11 models as detailed in Figures 25-34 in Appendix F. These findings suggest that few-shot learning for foundation model transferring remains challenging, and requires delicate optimization strategies."}, {"title": "4.3. Transferring Foundation Models Pre-trained on Medical Data", "content": "Zhang et al. (2024) fine-tuned three foundation models pre-trained on medical data, including BioMedGPT (Zhang et al., 2024), BioMedCLIP Zhang et al. (2023) and MedSAM (Ma et al., 2024), on seven datasets from the MedMNIST collection. Their results, presented at the bottom of Table 1 and Tables 4-6 in Appendix C are generally poorer compared to the models we adopted. This suggests the need for further development of foundation models based on medical data. However, the absence of linear probing results for these models limits the completeness of the comparison, which we propose to address in future work."}, {"title": "5. Conclusion", "content": "In this work, we evaluated the performance of foundation models on medical image classification tasks, selecting 12 representative models spanning CNN and ViT-based models. We employed both end-to-end fine-tuning and linear probing strategies for model transfer. Our results demonstrate the significant potential of foundation models in these tasks. Additionally, we explored the impact of image size and resizing strategies, deriving useful insights from our findings. Despite these promising results, several limitations remain: (1) Our validation is limited to the MedMNIST collection. Including additional datasets would provide more comprehensive conclusions; (2) Our analysis of resizing strategies is limited to the DermaMNIST dataset. A broader validation across all 12 datasets in the collection is planned for future work. (3) The current analysis lacks depth, particularly regarding model performance on specific classes within each task. A more detailed discussion will be provided. (4) The biases of each model across different medical image classification tasks are yet to be understood (Alloula et al., 2024)."}]}