{"title": "Exploring Feature-based Knowledge Distillation For Recommender System: A Frequency Perspective", "authors": ["Zhangchi Zhu", "Wei Zhang"], "abstract": "In this paper, we analyze the feature-based knowledge distillation for recommendation from the frequency perspective. By defining knowledge as different frequency components of the features, we theoretically demonstrate that regular feature-based knowledge distillation is equivalent to equally minimizing losses on all knowledge and further analyze how this equal loss weight allocation method leads to important knowledge being overlooked. In light of this, we propose to emphasize important knowledge by redistributing knowledge weights. Furthermore, we propose FreqD, a lightweight knowledge reweighting method, to avoid the computational cost of calculating losses on each knowledge. Extensive experiments demonstrate that FreqD consistently and significantly outperforms state-of-the-art knowledge distillation methods for recommender systems. Our code is available at https://anonymous.4open.science/r/FreqKD/.", "sections": [{"title": "1 Introduction", "content": "With the rapid growth in the variety of items and the gradual di- versification of user preferences, recent studies [16, 18] have begun to increase the model capacity by increasing the dimensionality of user/item embeddings. However, due to the huge share of em- beddings [18, 42] in the overall model parameters, increasing the embedding dimensionality greatly increases the number of parame- ters, which in turn increases the storage cost and inference latency, leading to longer waiting times and lower user satisfaction.\nTo improve the inference efficiency without sacrificing accuracy, many studies [10\u201312, 28] have adopted Knowledge Distillation (KD) to recommender system. KD is a model-agnostic approach for model compression [6, 8]. In knowledge distillation for recommendation, the common process is first to train a large teacher model using the user-item interactions, then train a small student model using the user-item interactions as well as the features in the intermediate layer [10\u201312] and the predictions in the output layer [1, 10, 14, 15] provided by the teacher model. After training, the student trained with KD performs similarly to the teacher and has a much lower inference latency due to its small representation dimensionality. Most KD methods for recommendation [1, 14, 15, 29] are response- based methods that force student models to learn the teachers' logits to improve their performance. Recently, some feature-based KD methods [10, 12] are proposed. They use the teacher's hidden representations as additional supervisory signals and have achieved promising performance. However, unlike the observations in CV and NLP, these feature-based methods behave much worse than response-based methods, as shown in Table 4. This is very counter- intuitive, as the mainstream studies in this regard [5, 7, 30] show that features play a crucial role in recommender systems.\nWe believe the above phenomenon is due to the lack of a funda- mental understanding of the knowledge transfer processes during feature-based distillation. To this end, our work focuses on the feature-based knowledge distillation methods for recommender systems and aims to investigate the following research questions:\n1) What \"knowledge\" in features is transferred from teachers to student recommendation models when minimizing feature-based knowledge distillation loss?\n2) Considering the huge gap between the capacity of the student and teacher, why is the important knowledge in features not being successfully transferred to the student recommendation models?\n3) How to transfer as much important knowledge in features as possible to student recommendation models while avoiding in- creasing the complexity of knowledge distillation?\nTo answer the first question, we first need to define the knowl- edge contained in the features. Therefore, we need a rigorous system for analyzing the features. Thanks to the development of graph signal processing [13, 34] and graph-based collaborative fil- tering [4, 7, 22], we can get a clearer picture of the features from the frequency domain perspective. Specifically, recent studies [4, 22, 34] have demonstrated that the low-frequency component of a feature characterizes the common properties between a node (user or item) and its neighboring nodes, whereas the high-frequency component characterizes the differences between nodes. We, therefore, define various types of knowledge as different frequency components of"}, {"title": "2 Related Work", "content": "2.1 Graph-based Collaborative Filtering\nWith the increasing development of graph neural networks [26, 36, 38, 40], researchers have introduced them into the field of recom- mender systems [24, 32, 35]. Graph-based collaborative filtering (GCF) aims to conduct collaborative filtering with graph processing. It provides a general framework to incorporate high-order informa- tion in the user-item graph to boost the recommendation perfor- mance. NGCF [33] first proposes the graph collaborative filtering framework to make recommendations. LightGCN [7] empirically shows the uselessness of non-linearity and feature transformation. Recently, some works have analyzed and improved graph models from a spectral perspective. For example, GF-CF [27] theoretically shows that LightGCN and some traditional CF methods are essen- tially low-pass filters. FF-G2M [34] factorizes features into low- and high-frequency components, demonstrating they represent com- monalities and differences between nodes, respectively. GDE [22] proposes that low-frequency components are much more important than high-frequency components in recommender systems.\n2.2 Knowledge Distillation\nKnowledge distillation (KD) is a model compression method that distills the knowledge from a well-trained teacher model to a small student model. Existing KD methods mainly fall into three cate- gories [6] based on their knowledge type: response-based methods, feature-based methods, and relation-based methods. Response-based methods [8, 9, 41] take the final prediction of the teacher as soft targets, and the student is trained to mimic the soft targets by min- imizing the divergence loss. Feature-based methods take advantage of the teacher model's ability to learn multiple levels of feature representation with increasing abstraction. They propose to enrich auxiliary signals by matching the learned features of the student and the teacher. These methods leverage various types of knowledge, such as feature activations [25], attention maps [39], probability distribution in feature space [20]. Relation-based methods [2, 19, 21] further explore the relationships between different layers or data samples. A representative example is RKD [19], which calculates the similarity between each pair of samples to obtain a similarity matrix and then minimizes the difference between the teacher's and the student's similarity matrices.\n2.3 Knowledge Distillation in Recommender System\nKowledge distillation has been introduced into recommender sys- tems to reduce the representation dimensionality and inference latency, which falls into the same three categories, i.e., response- based, feature-based, and relation-based.\nResponse-based methods focus on teachers' predictions. For ex- ample, CD [15] samples unobserved items from a distribution asso- ciated with their rankings predicted by students: items with higher rankings are more likely to be sampled. RRD [10] adopts a list-wise loss to maximize the likelihood of teachers' recommendation list. UnKD [1] partitions items into multiple groups based on popularity and samples positive-negative item pairs within each group."}, {"title": "3 Preliminary", "content": "3.1 Top-N Recommendation\nIn this work, we focus on the top-N recommendation with implicit feedback. Specifically, let U and I denote the user set and item set, respectively. Then |U| and |I| are taken as the number of users and items, respectively. The historical implicit feedback can be formulated as a set of observed user-item interactions R = {(u, i)|u interacted with i}. A recommendation model aims to score the items not interacted with by the user and recommend N items with the largest scores. As for the learning objective, most works adopt the BPR loss [23] to make the predicted scores of interacted items higher than randomly sampled negative items.\n3.2 Feature-based Knowledge Distillation\nLet d_s and d_T denote the dimensionalities of student and teacher features, respectively. In this work, we use S \u2208 \\mathbb{R}^{(|U|+|I|) \\times d_s} and T \u2208 \\mathbb{R}^{(|U|+|I|) \\times d_T} to represent all the features of the student model and the teacher model, respectively.\nIn feature-based knowledge distillation methods [10, 12], a pro- jector is used to align the dimensionalities of the student model and the teacher model. Then, the student model is trained to mini- mize the MSE loss to make the student model mimic the teacher's features. Formally, the feature-based knowledge distillation loss is given by:\n$\\mathcal{L}_{FD} = ||\\text{Proj}(S) \u2013 T||_2^2,$\nwhere Proj(.) denotes the projector that aligns the dimensionalities of S and T, and is usually a linear layer.\n3.3 Graph Signal Processing\nIn this paper, we use tools in graph signal processing to define the knowledge in features. Given a graph G = (V, \\mathcal{E}) where V and \\mathcal{E} being the node set and edge set, respectively. We denote"}, {"title": "4 Methodology", "content": "In Section 4.1, we define the knowledge in features and show the dif- ference between various types of knowledge. In Section 4.2, we pro- pose a simple yet effective method to reweight the distillation loss of different knowledge. In Section 4.3, we further propose FreqD, a lightweight knowledge reweight method. The overall training process of FreqD is exhibited in Algorithm 1 and the time com- plexity analysis is provided in Appendix A.3. A further discussion on extending our analytical approach to relation-based knowledge distillation methods is provided in Appendix A.5.\n4.1 Analysis of Knowledge in Feature-based Knowledge Distillation\nFirst, we must define the knowledge in features to analyze the knowledge transfer process in feature-based knowledge distilla- tion. Inspired by recent work in graph signal processing and graph collaborative filtering, we introduce the following definition.\nDefinition 4.1 (Knowledge in features). Given the features $X \\in \\{S, T\\}$, the k-th type of knowledge contained in the features is defined as the k-th frequency component of X; that is\n$X_k = u_k u_k^T X.$\nThen, we have a total of |V| types of knowledge. As we have analyzed in previous sections, different types of knowledge indicate different degrees of commonality among nodes.\nNext, we theoretically show that when minimizing feature-based knowledge distillation losses, the student model must learn all types of knowledge equally.\nTHEOREM 1. Consider the feature-based distillation loss:\n$\\mathcal{L}_{FD} = ||\\text{Proj}(S) \u2013 T||_2^2.$\nThen $\\mathcal{L}_{FD}$ can be decomposed into losses on all types of knowledge. Formally,\n$\\mathcal{L}_{FD} = \\sum_{k=1}^{|V|} ||\\text{Proj}(S_k) \u2013 T_k||_2^2.$\nThe proof is presented in Appendix A.1. Theorem 1 indicates that the feature-based distillation loss aims to force the student"}, {"title": "4.2 Knowledge Reweighting", "content": "Our findings in the previous section suggest that during the knowl- edge distillation process, knowledge representing low frequencies fails to be transferred to student models of low capacity. Unfortu- nately, our experiment next illustrates that this neglected knowl- edge is very important. Specifically, we first rewrite the feature- based distillation loss in Eq.(1) as:\n$\\mathcal{L}_{FD} = \\sum_{i=1}^{4} w_i \\mathcal{L}_{S_i}$"}, {"title": "4.3 Lightweight Knowledge Reweighting Method", "content": "Although the above method allows us to specify the weight for each knowledge, direct computation requires computing each knowl- edge $X_k$, which also requires a complete eigendecomposition of the graph Laplacian matrix to obtain $u_k$. However, this is usu- ally impractical because there are many users and items in real recommendation scenarios. In this section, we propose FreqD, a lightweight weight knowledge reweighting method that defines graph filters and performs graph filtering on the features to assign weights to the losses on different knowledge indirectly.\nFirst, we demonstrate that computing feature-based distillation loss using filtered features is equivalent to reweighting losses on different knowledge.\nTHEOREM 2. Given a graph filter $H(\\tilde{L})$ defined as follows:\n$H(\\tilde{L}) = U \\text{Diag}(h(\\lambda_1), h(\\lambda_2),\u00b7\u00b7\u00b7, h(\\lambda_{|V|}))U^T$\nwhere $\\tilde{L}$ is the graph Laplacian matrix. Then, computing the feature- based distillation loss using the features filtered by $H(\\tilde{L})$ is equivalent to setting weights $g(\\lambda_i) = h^2(\\lambda_i)$ in Eq.(3).\nFormally, we have\n$||H(\\tilde{L})\\text{Proj}(S) \u2013 H(\\tilde{L})T||_2^2 = \\sum_{k=1}^{|V|} h^2(\\lambda_i) || (\\text{Proj}(\\tilde{S_k}) - \\tilde{T_k})||_2^2$\nThe proof is provided in Appendix A.2. Based on Theorem 2, we have now transformed the task of designing the decreasing function g(.) presented in the previous section into designing the decreasing function h(.). However, the direct computation of $H(\\tilde{L})$ using Eq.(5) still requires an eigendecomposition of the graph Laplacian matrix to obtain U. Inspired by work in graph signal processing, we propose to solve this problem by choosing h(\u03bb) as a polynomial function of \u03bb, which has the following general formulation:\n$h(\\lambda) = \\sum_{k=0}^{K} \\theta_k \\lambda^k,$"}, {"title": "5 Experiments", "content": "5.1 Experimental Settings\nDatasets. We conduct experiments on three public datasets, includ- ing CiteULike\u00b9 [11, 12, 31], Gowalla\u00b2 [3, 15, 29], and Yelp2018\u00b3 [14, 15]. We split training and test dataset following the previous method [37]. Specifically, we filter out users and items with less than 10 interac- tions and then split the rest chronologically into training, validation, and test sets in an 8:1:1 ratio. The statistics of the preprocessed datasets are summarized in Table 3.\nEvaluation Protocols. Per the custom, we adopt the full-ranking evaluation to achieve an unbiased evaluation. To evaluate the per- formance of top-N recommendation, we employ Recall (Recall@N) and normalized discounted cumulative gain (NDCG@N) and report the results for N \u2208 {10, 20}. We conduct five independent runs for each configuration and report the averaged results.\nBaselines. We compare our method with all three types of knowl- edge distillation methods:"}, {"title": "5.2 Performance Comparison", "content": "Overall Performance. The results shown in Table 4 indicate that: (i) FreqD outperforms all baselines significantly on all datasets and achieves remarkable improvements over the best baseline for different backbones, consolidating the effectiveness of FreqD.\n(ii) In most cases, response-based knowledge distillation methods are better than other baseline methods, including feature-based methods and relation-based methods. This is at odds with the ex- cellent performance of feature-based distillation methods in CV and NLP, reflecting the importance of a deeper understanding of feature-based distillation methods for recommender systems.\n(iii) On all three datasets, FreqD shows comparable performance to the teacher, making it possible to achieve inference efficiency without sacrificing recommendation accuracy.\nAblation Study. Since FitNet can be regarded as a special case of our method obtained by equally weighting losses on all types of knowledge, here we serve as an ablation experiment by comparing our method with FitNet. By comparing the performance of FitNet and FreqD in Table 4, we find that although FreqD is simple, it significantly outperforms FitNet on all datasets and all backbones. This suggests that by decoupling the knowledge contained in the"}, {"title": "5.3 Training Efficiency", "content": "In this section, we report the training efficiency of our method and comparison methods. All results are obtained by testing with PyTorch on GeForce RTX 3090 GPU.\nIn FreqD, we only need to perform graph filtering on features once before proceeding with regular feature-based knowledge dis- tillation. Moreover, we use linear graph filters, which are extremely sparse. These design choices make our approach extremely effi- cient. To empirically validate the training efficiency of our method, we report the training time and storage cost of our methods and comparison methods. The results are presented in Table 5 and Ta- ble 6. The method Student denotes that we train the student model without knowledge distillation.\nFrom the results, we find that:\n\u2022 Although all knowledge distillation methods inevitably increase training costs, feature-based knowledge distillation methods bring about the least time cost. We believe this is because, in response-based distillation methods for recommender systems, it is often necessary to non-uniformly sample from all items, greatly increasing training time. For relation-based distillation"}, {"title": "5.4 Inference Efficiency", "content": "In this section, we use our method to train students of different sizes. Then, we report their inference efficiency and recommendation performance. All results are obtained by testing with PyTorch on GeForce RTX 3090 GPU.\nFrom the results in Table 7, it is obvious that our method can benefit students of all sizes. Moreover, we achieve comparable per- formance to the teacher with only 10% of the number of parameters and about a quarter of the inference latency. When the student is half the size of the teacher, the student can significantly out- perform the teacher with only half the storage cost and greatly increase the inference efficiency. Moreover, with the same number of parameters, our method significantly outperforms the teacher, demonstrating that FreqD is also effective in improving existing recommendation models."}, {"title": "5.5 Different Choices of h(\u03bb)", "content": "In Eq.(10), we set h(\u03bb) as a linear function for simplicity and effi- ciency. However, it is possible to design it into other higher-order forms. In this section, we verify empirically that the linear form, despite its simplicity, has a good performance. In Table 8, we give the results when h(\u03bb) is designed as a quadratic function, i.e., $h(\\lambda) = a\\lambda^2 + b\\lambda + 1$. Note that we similarly restrict its shape to make it decreasing in the domain of definition. From the results, we find that modifying h(\u03bb) to a quadratic function can slightly improve"}, {"title": "6 Limitations", "content": "Although our method is efficient and effective in highlighting im- portant knowledge, we have used only linear and quadratic filters. We did this design to keep the graph filter as sparse as possible to reduce the complexity of our method. However, this may result in a filter whose performance is not optimal. Therefore, how to design more complex filters to retain important knowledge better while removing unimportant knowledge would be an interesting research direction."}, {"title": "7 Conclusion", "content": "This paper analyses the feature-based knowledge distillation for recommendation from the frequency perspective. By defining dif- ferent types of knowledge as different frequency components of features, we show that regular feature-based knowledge distillation equally minimizes losses on all types of knowledge. We then analyze how this equal loss weight allocation method overlooks important knowledge. Based on this, we propose emphasizing the important knowledge by redistributing knowledge weights and proposing a"}, {"title": "A Appendix", "content": "A.1 Proof of Theorem 1\nIn Theorem 1, we claim that the feature-based distillation loss $\\mathcal{L}_{FD} = ||\\text{Proj}(S) \u2013 T||_2^2$ can be decomposed into losses on different frequency bands. In this section, we will provide the proof.\nPROOF.\n$\\mathcal{L}_{FD} =||\\text{Proj}(S) \u2013 T||_2^2$\n$=||U I U^T (\\text{Proj}(S) \u2013 T) ||_2^2$\n$=||\n$\\sum_{k=1}^{|V|} u_k u_k^T (\\text{Proj}(S) - T)\n||_2^2$\n$\\mathcal{L}_{FreqD} = ||H(\\tilde{L})\\text{Proj}(S) \u2013 H(\\tilde{L})T||_2^2$\n$=||\n$\\sum_{i=1}^{|V|} h(\\lambda_i)u_iu_i^T (\\text{Proj}(S) - T)\n||_2^2$\n\n$\\mathcal{L}_{FreqD} = \\sum_{i=1}^{|V|} h^2(\\lambda_i) ||(\\text{Proj}(\\tilde{S_i}) - \\tilde{T_i})||_2^2$"}, {"title": "A.2 Proof of Theorem 2", "content": "In Theorem 2, we assert that computing distillation loss using filtered features is equivalent to reweighting the distillation loss on each frequency band. In this section, we will provide the proof.\nPROOF. Given a graph filter $H(\\tilde{L})$ defined as follows:\n$H(\\tilde{L}) = U \\text{Diag}(h(\\lambda_1), h(\\lambda_2),\u00b7\u00b7\u00b7, h(\\lambda_{|V|}))U^T.$"}, {"title": "A.3 Time Complexity", "content": "The proposed FreqD is highly efficient. The only additional op- eration that our method performs over FitNet is graph filtering. Thanks to the support for sparse matrix multiplication in various machine learning libraries, the complexity of this filtering opera- tion is only O(|R|(d_S + d_T )). Given batch size |B|, since the com- plexity of FitNet is O(|B|d_S d_T ), the complexity of our method is $O(|B|d_S d_T +|R|(d_S +d_T ))$. In contrast, the complexity of DE using K projectors is O(K|B|d_S d_T ), and the required training time is usually larger than our method."}, {"title": "A.4 Training Process", "content": "The pseudocode of the entire training process is presented in Algo- rithm 1."}, {"title": "A.5 Further Discussion", "content": "In this paper, we focus on feature-based knowledge distillation methods. However, since relation-based knowledge distillation methods (e.g., HTD [11]) are also closely related to features, our theoretical analysis can also be applied to them.\nTHEOREM 3. Given the widely used relation-based distillation loss\n$L_{RD} = ||SS^T - TT^T||$"}, {"title": "A.6 Proof of Theorem 3", "content": "In Theorem 3, we claim that the loss used in some relation-based knowledge distillation methods can also be decomposed into losses on different frequency bands. More specifically, the $L_{RD} = ||SS^T - TT^T||$ consists of distillation loss on each pair of frequency. In this section, we will provide the proof.\nPROOF.\n$L_{RD} = ||SS^T \u2013 TT^T||_2^2 $\n= tr ((SST \u2013 TTT) (SST \u2013 TTT))\n= tr(SSTSST) + tr(TTTTTT) \u2013 tr(TTTSST) \u2013 tr(SSTTTT)"}, {"title": "A.7 Hyperparameter Analysis", "content": "This section analyses the effects of hyperparameters of FreqD in terms of NDCG@20.\nEffects of \u03b1. In Eq.(10), we define the graph filter in FreqD as a linear function of the graph Laplacian matrix and introduce hyperparameter a. The effects of a is reported in Figure 3. The experimental results illustrate that too large or too small a results in performance dropping. This is because too large an a leads to too much loss of high-frequency knowledge, while too small an a can not motivate the student model to focus on low-frequency knowledge. The optimal value of a is usually between 0.4 and 0.5.\nEffects of \u00df. In Eq.(9), hyperparameter \u00df represents the weight of the feature-based distillation loss. The effects of \u00df is shown in Figure 4. Because the types of loss functions of the proposed methods differ from those of the base models, it is important to balance the losses by properly setting \u00df. In our experiments, we find that setting \u00df to 0.1 usually resulted in good performance."}]}