{"title": "How Do Your Code LLMs Perform? Empowering Code Instruction Tuning with High-Quality Data", "authors": ["Yejie Wang", "Keqing He", "Dayuan Fu", "Zhuoma Gongque", "Heyang Xu", "Yanxu Chen", "Zhexu Wang", "Yujia Fu", "Guanting Dong", "Muxi Diao", "Jingang Wang", "Mengdi Zhang", "Xunliang Cai", "Weiran Xu"], "abstract": "Recently, there has been a growing interest in studying how to construct better code instruction tuning data. However, we observe Code models trained with these datasets exhibit high performance on HumanEval but perform worse on other benchmarks such as LiveCodeBench. Upon further investigation, we find that many datasets suffer from severe data leakage. After cleaning up most of the leaked data, some well-known high-quality datasets perform poorly. This discovery reveals a new challenge: identifying which dataset genuinely qualify as high-quality code instruction data. To address this, we propose an efficient code data pruning strategy for selecting good samples. Our approach is based on three dimensions: instruction complexity, response quality, and instruction diversity. Based on our selected data, we present XCoder, a family of models finetuned from LLaMA3. Our experiments show XCoder achieves new state-of-the-art performance using fewer training data, which verify the effectiveness of our data strategy. Moreover, we perform a comprehensive analysis on the data composition and find existing code datasets have different characteristics according to their construction methods, which provide new insights for future code LLMs.", "sections": [{"title": "1 Introduction", "content": "Code pre-trained models have achieved remarkable progress in the era of large language models (LLMs), such as Codex (Chen et al., 2021b), AlphaCode (Li et al., 2022), PaLM-Coder (Chowdhery et al., 2022) and StarCoder (Li et al., 2023a). Training on large code corpora (Kocetkov et al., 2022) has been shown to enhance the coding capabilities of current LLMs (Lozhkov et al., 2024; Rozi\u00e8re et al., 2023). In addition to costly pre-training, recent research has garnered increased interest in code instruction tuning and obtains promising results on several code benchmarks (Chaudhary, 2023; Luo et al., 2023a; Team, 2024; Wei et al., 2023; Yang et al., 2024; Song et al., 2024; Muennighoff et al., 2023; Wang et al., 2024a). Differing from the high demand of pre-training for data quantity, instruction tuning aligns existing model abilities towards a desired direction using high-quality but much smaller datasets. To construct code instruction datasets, earlier research predominantly relies on heuristic automation (e.g. distillation from ChatGPT) or manual selection. For example, Code Alpaca (Chaudhary, 2023) and WizardCoder (Luo et al., 2023a) use distillation signals from ChatGPT via self-instruct and evol-instruct. Other methods such as OctoPack (Muennighoff et al., 2023) and Magicoder (Wei et al., 2023) construct code instructions from pre-training code corpora. Although these code instruction datasets seem excellent on popular code benchmarks like HumanEval, we find some of them dramatically drop on another contamination-free benchmark LiveCodeBench (Jain et al., 2024) which continuously collects new problems over time from on-line contests. As shown in Figure 1, Magicoder Evol-Instruct and Code-Feedback (Zheng et al., 2024) achieve top ranks on HumanEval but drop on LiveCodeBench. We perform a further decontamination process and find that several existing code models achieve abnormally high performance on HumanEval because of the potential use of the benchmark or benchmark-similar data. Thus, it remains unclear what good code instruction data is and how these datasets actually work. Besides, all the data come from different pipelines and have no unified principle to ensure good quality. We need to systematically define what constitutes good examples of data for code instruction tuning and establish an effective principle for achieving competitive performance using only highly valuable samples.\nIn this work, we aim to define the characteristics of good data for code instruction tuning based on a diverse range of existing code datasets. Our goal is to select the most influential samples through a comprehensive and quantitative data assessment measure. Drawing inspiration from Liu et al. (2024); Ni et al. (2024), we propose a paradigm of data-efficient instruction tuning for code capabilities. Generally, we assume good code samples are complex, of high quality, and diverse. For the complexity aspect, we adopt the evolved complexity scorer to predict the complexity of a given instruction. The scorer is trained on evolved samples via the complexity prompt (Luo et al., 2023a) with ChatGPT. For the aspect of quality, we train a verified model to generate multiple test cases given an (instruction, response) pair and evaluate its quality via the pass rate of the generated test cases. For the aspect of diversity, we select the sample with a large distance to a data pool via instruction embeddings. Combining the three measures, our simple but effective data selection strategy pursues valuable code instruction data and achieves more efficient instruction tuning where fewer training samples yield performance on par with, or even surpassing, models trained on significantly larger datasets. Moreover, we also analyze the composition of our selected data mixture and give suggestions for future code instruction tuning research.\nWe present XCoder, a family of models fine-tuned from LLaMA3 using our selected code instruction data mixture. Experiments on LiveCodeBench and HumanEval demonstrate that XCoder is able to outperform or be on par with state-of-the-art code instruction models such as WizardCoder (Luo et al., 2023a), Magicoder (Wei et al., 2023), StarCoder2-Instruct and OpenCodeInterpreter (Zheng et al., 2024) while using fewer automatically selected data examples. For example, XCoder-8B based on LLaMA3-8B achieves 43.66 LiveCodeBench-Easy and 54.9 HumanEval when trained on only 40K data samples. Besides, our XCoder-70B based on LLaMA3-70B achieves top-tier results compared to the state-of-the-art open-source models."}, {"title": "2 Deep Dive into Existing Datasets", "content": "We present mainstream and open-source Code Instruction Tuning datasets in Table 1. And then we select several influential datasets from these for training and test their performance on HumanEval and LiveCodeBench benchmarks, with the results shown in Table 2.\nFrom the results, we observe that different training datasets lead to significant performance differences on HumanEval, but the differences on LiveCodeBench are minimal. This phenomenon leads us to suspect whether the remarkably high performance of some data in HumanEval is due to data leakage. Therefore, we propose the Test Leakage Index (TLI) to detect the degree of data leakage for each dataset in the test set.\nTLI The Test Leakage Indicator is a metric for quantifying the extent of data leakage from a training set to a test set. To compute TLI, n-grams are generated for both datasets, and the overlap between the n-grams of each test sample and those of all training samples is measured. The similarity score S(t\u1d62, r\u2c7c) between a test sample t\u1d62 and a training sample r\u2c7c is calculated as the fraction of common n-grams over the total n-grams in the test samples. For each test sample, the maximum similarity score among all training samples is recorded. The final TLI metric is the average of these maximum similarity scores across all test set. Higher TLI values indicate greater risks of leakage, highlighting significant similarities between the training and test data.\nWe calculate the TLI metrics for different datasets on HumanEval, as shown in Table 2. More dataset can be viewed in Appendix B. we find that most datasets maintain a TLI of around 5% on HumanEval, but Codefuse-Evol-Instruct, Magicoder-Evol-Instruct, and Code-Feedback exhibit TLI indices exceeding 30%. Therefore, we further clean these datasets ensuring that the TLI of all cleaned datasets is controlled at 5%, and then conduct re-experiments with these datasets. From the result we can observe that the cleaned datasets, after filtering only a small portion, show a significant performance drop on HumanEval, but their performance on LiveCodeBench remains almost unchanged or even slightly improved. For example, after filtering out 3.4% samples from the Code-Feedback dataset, its performance on the HumanEval Base-Pass@1 metric drops by 7.3%, but its performance on LiveCodeBench slightly increases. This further substantiates the presence of data leakage. Additionally, we discover numerous cases where the training data are almost identical to the test data in HumanEval, confirming the serious data leakage in these datasets. The leaked cases can be viewed in Appendix B."}, {"title": "3 What Characteristics Do Good Data Have", "content": "In this section, we first define the characteristics of good data for code instruction tuning and then select the most influential samples via data pruning. Inspired by Deita (Liu et al., 2024), we select the samples in the Data Pool from three dimensions: instruction complexity, response quality, and instruction diversity. For a data pool P, we first use the a complexity score C and Unit Test Model U to calculate the complexity score c and quality score q for each data. Then, we use linearly combine c' and q' to obtain a score s representing complexity and quality. Finally, we sort the data pool P and apply the Diversity-based Sampling to iteratively select samples from the data pool into the final training set D, until D reaches the budget size. Our data selection approach is illustrated in the Figure 2 and Algorithm 1. The details of Complexity Score, Unit Test Model and Diversity-based Sampling are as follows."}, {"title": "3.1 Instruction Complexity: Complexity Scorer", "content": "Inspired by Evol Complexity (Liu et al., 2024), which is a complexity measure based on the evolution algorithm. We use evolved instructions to train our complexity scorer. Specifically, we use self-instruct to obtain a small-scale dataset Seed = {S\u2081, S\u2082,..., S\u2099} as the seed for evolution. Then, we apply the in-depth evolving prompting from WizardCoder for M rounds of evolution. This process results in an instruction set where each seed instruction s\u1d62 has M evolved instructions and their corresponding rounds {(S\u1d62, 0), (I\u2081, 1), . . ., (I\u2098, M)}. We then treat the rounds as a complexity measure and train the complexity scorer to predict the complexity score given the input instruction. In multi-turn dialogues, we score each turn separately and use the sum of them as the final score."}, {"title": "3.2 Response Quality: Unit Test Model", "content": "We consider the number of test cases passed as a measure of response quality, which, as demonstrated in our experiments in Section 4.4.3, is an effective way to assess code quality for code generation tasks compared to directly scoring the language model.\nTo obtain test cases for each training sample, we utilize a unit test model that can generate a fully executable unit test program according to the provided instructions and code snippet for testing, which can be formulated as: T = U(I, R), where we denote the instruction as I, the code solution as R, and the generated unit test as T. We collect 6k TACO(Li et al., 2023b) data to train the unit test model based on LLaMA3-70B-Base. During application, we prompt the Unit Test Model to generate 12 test cases for each training sample, and execute the unit testing program. The number of passed test cases is considered as the quality score.\nWe also show some cases output by our unit test model which can be found in Appendix C."}, {"title": "3.3 Instruction Diversity: Diversity-based Sampling", "content": "We use Diversity-based Sampling method to ensure the diversity of the selected data. The iterative method selects samples p\u1d62 one by one from the pool P, and when p\u1d62 contributes to the diversity of the selected dataset D, it is added to D. This process continues until the budget Q is reached or all samples p\u1d62 in P have been enumerated. Specifically, the benefit of the diversity brought by the newly considered sample p\u1d62 can be formulated as an indicator function F(p\u1d62, D) := distance(p\u1d62, D) < \u03c4, which equals 1 only when F(p\u1d62, D) is true, otherwise it is 0. Only when F(p\u1d62, D) equals 1, p\u1d62 will be added to D. We use the embedding distance between the sample p\u1d62 and its nearest neighbor in D to calculate distance(p\u1d62, D). And \u03c4 is a hyperparameter."}, {"title": "4 Experiments", "content": "4.1 Benchmarks\n\u2022 HumanEval: HumanEval (Chen et al., 2021a) is a widely researched benchmark test for code language models, specifically designed to evaluate the ability of code generation. It includes 164 hand-written programming problems, each problem includes a function signature, docstring, body, and several unit tests, with an average of 7.7 tests per problem.\n\u2022 LiveCodeBench: LiveCodeBench (Jain et al., 2024) is a comprehensive and pollution-free benchmark for evaluating Large Language Models in code assessment. It updates new problems in real-time from competitions on three competitive platforms (LeetCode, AtCoder, and CodeForces)."}, {"title": "4.2 Implementaion Details", "content": "Data Pools To construct the best Code Instruction Tuning dataset, we gathered various available open-source datasets, as detailed in Table 1. This resulted in a collection of 2.5M data samples. However, this amount of data is excessively large. To control the size of the Data Pools, we implemented a straightforward filtering process according to the following rules: Firstly, We include datasets proposed by academic work: Magicoder-OSS-Instruct, Magicoder-Evol-Instruct, and Code-Feedback. We also select the longest 200K samples to add to the Data Pools. Following this, we sort the data by complexity score and add the top 200K highest-scoring samples. Finally, we performed deduplication on the Data Pools, resulting in a final dataset of 336K samples.\nComplexity Scorer We use ChatGPT to evolve the dataset over 4 iterations on Code-Alpaca as the training set and train on LLaMA3-8B-Instruct with a learning rate of 2e-5 for 1 epoch.\nUnit Test Model We use 6k TACO data to train our unit test model based on LLaMA3-70B-Base. TACO is a dataset for code generation that each sample contains question, code solutions and test cases. We train the final unit test model using a learning rate of 5e-6 over 3 epochs.\nDiversity We use LLaMA3-8B-Base to get the instruction embedding. We set \u03c4 to 0.945 which means we consider an example p\u1d62 could increase the diversity of selected dataset D when the embedding distance between p\u1d62 and its nearest neighbor is smaller than 0.945."}, {"title": "4.3 Main Results", "content": "To validate the effectiveness of XCoder, we conducted experiments on LLaMA3-8B-Base, with the results shown in Table 3. From the results we can observe that XCoder achieves the best results on LiveCodeBench and BigCodeBench among other open-source dataset. It also also achieves the best level performance on HumanEval among the clean datasets. Additionally, we observe that XCoder is highly efficient with samples, achieving superior performance on LiveCodeBench and BigCodeBench with only 40K data compared to baselines. As the data size increases further, XCoder continues to improve on HumanEval and BigCodeBench. We also notice that Magicoder-Evol-Instruct and Codefuse-Evol-Instruct still achieve leading results on HumanEval. The reason may be that the decontamination algorithm cannot completely filter out all leaked data, so some data leakage still exists within these training sets on HumanEval."}, {"title": "4.4 Analysis", "content": "We also train XCoder-70B based on LLaMA3-70B-Base. Figure 3 shows that XCoder-70B is one of the best open-source Code LLMs."}, {"title": "4.4.1 Ablation Study", "content": "To validate the effectiveness of each data dimension, we conducted ablation experiments with the results shown in Table 4. As observed across both data sizes, the model's final performance on LiveCodeBench improves with the addition of each dimension, indicating the effectiveness of each dimension."}, {"title": "4.4.2 Complexity Dimension", "content": "Table 5 illustrates the performance of models trained on 40K selected data samples using various complexity measures on LiveCodeBench. Our Complexity Scorer measure exhibits the best performance across all measures, surpassing the Random method by 2.1% on Pass@1 and by 3.5% on Easy-Pass@1. The results also indicate that instruction length is a good measure for observing the Code Instruction Tuning data, second only to Complexity Scorer, which contrasts with observations made on general alignment data. Interestingly, perplexity, as an intuitive measure of complexity, performs comparably to the random selection method, consistent with observations by Liu et al. (2024)."}, {"title": "4.4.3 Quality Dimension", "content": "Using Unit Test for Ranking To validate our Unit Test Model's ability to rank the quality of code, we conducted the following experiment. Specifically, we generate 10 candidate solutions for each question in HumanEval, then use our unit test model to generate test cases for each solution, rank-ing them based on the number of test cases passed. We select the best one as the final solution. And we use random selection from the candidate solutions as the baseline. The results are shown in Table 6. Additionally, we consider another method where using LLMs to output the correctness of the code directly. We choose GPT-4-0409 to do that. From the results, we observe that compared to random selection, using the unit test model significantly improves the accuracy of the chosen answers, with an increase of nearly 13.6% in the Base-Pass@1 metric and 10.3% in the Plus-Pass@1 metric. Notably, the unit test model trained on LLaMA3-70B-Base also outperforms GPT-4, with improvements of around 3% in both metrics.\nFrom the results, we can observe that using unit tests improves the BoN-Pass@1 metric by approximately 14%, which is higher than merely using language model judgment. However, we also notice a gap in evaluation accuracy per solution compared to GPT-4. We believe this discrepancy may arise because, for unit tests, a solution must pass all the test cases to be considered correct. Any error in generating a test case can cause the solution to fail. Nevertheless, the effectiveness of unit tests in the Best-of-N metric demonstrates that this approach might be more suitable for ranking the quality of code solutions.\nAccuracy of Generated Test Cases We also experimented with the impact of different model sizes on the accuracy of the Unit Test Model in generat-ing test cases. Specifically, we instructed the model to generate 10 test cases for the golden solutions in HumanEval, execute them, and count the number of passing test cases. The results are shown in Figure 4. Additionally, we evaluated GPT-4's capability in generating test cases.\nWe observed that increasing the model parameters significantly improves the accuracy of generating test cases, from 64.8% to 78.7%. Further, we find that the test case model trained on LLaMA3-70B performs very close to GPT-4 in generating test cases, with a difference of less than 2%."}, {"title": "4.4.4 Data Scaling", "content": "To study the impact of our data selection strategy on data scaling efficiency, we conduct experiments using different data budgets. Table 7 shows that XCoder outperforms randomly sampled data across different data sizes. Surprisingly, XCoder achieves performance comparable to using 160K training samples with only 10K samples, and it matches the performance of using the full dataset at 80K samples. This demonstrates the high efficiency of XCoder's data samples and the effectiveness of XCoder in data selection."}, {"title": "4.5 Data Analysis", "content": "In this section, we analyze the data composition of XCoder, reassess the strengths and weaknesses of different data sources, and develop new insights into different data generation methods.\nComplexity: We sorted all samples according to the Complexity Score and analyzed the source datasets of the top 160K samples. The results are shown in Figure 5(a). We observe that the multi-turn Code-Feedback dataset, which includes code refinement data, contributes the largest amount of samples. And OctoPack, which uses real Git commit information as instructions, results in limited instruction complexity and contributes only 0.1%. However, We also observe that StarCoder2-Self-Align contributes the second largest amount of samples, indicating that, besides Evol-Instruct, converting pre-training data appropriately can also yield complex instructions.\nQuality: Figure 5(b) shows the contribution of different data sources in the top 160K quality score samples. We observe that OctoPack, which uses real code data, contributes the most high-quality samples. Moreover, we notice that Magicoder-Evol-Instruct, which used GPT-4 to evolve instructions and generate responses, contributes almost as many high-quality samples as OctoPack. However, Dolphcoder-Evol-Instruct, which used the same Evol-Instruct method but with GPT-3.5 for response generation, only contributes 11.16% of the samples. And Code-Alpaca, which was generated with text-davinci-003, contributes the fewest high-quality samples, comprising only 2.04% of the total. We assert that in the Evol-Instruct process, responses generated by more capable models tend to have higher quality. Notably, we observe that StarCoder2-Self-Align contributes a considerable amount, which we think is potentially due to its use of self-synthesized test cases and the rejection of samples that do not execute correctly.\nDiversity: The XCoder method relies on the added samples when calculating the diversity of the samples, meaning it dynamically measures the diversity of the samples and cannot independently calculate diversity scores for each sample. Therefore, we present the composition of the top 160K data before and after applying Diversity-based Sampling method, considering the changes as the impact brought by data diversity. Figure 5(c) displays the source statistics of the top 160K samples before using Diversity-based Sampling, while Figure 5(d) illustrates the composition of the data after applying Diversity-based Sampling for the top 160K data. We find that the most notable change is that, after applying the Diversity-based Sampling method, OctoPack jumps from having the lowest contribution to the second highest. We believe this phenomenon may be due to OctoPack directly gathering instructions from the real world, thus possessing better diversity.\nOverall, we find that in terms of complexity: data with more rounds has longer context and higher complexity. Additionally, Evol-Instruct is an effective method for improving instruction complexity. In terms of quality: Code LLMs that deliver accu-rate responses. Data with added test case feedback verification during data synthesis tends to have higher quality. Furthermore, using a stronger model to synthesize data is a simpler, more direct, but effective approach. In terms of diversity: We find that directly sampling from the real world and transforming it results in instructions with better diversity compared to other methods that only expand instructions using fixed seeds."}, {"title": "5 Related Work", "content": "Code Instruction Tuning. Code instruction tuning is a necessary step for models to accurately understand human instructions and generate relevant code responses. Xu et al. (2023) apply the Evol-Instruct method (Xu et al., 2023) to CodeAlpaca (Chaudhary, 2023) dataset and obtain a instruction dataset with high complexity. Muennighoff et al. (2023) take git commits as natural instruction data. They collect 4TB git commits across 350 programming language. Wang et al. (2024b) propose Diverse Instruction Tuning and Multi-Objective Tuning to train Dolphcoder, which proves that more diverse code solutions and code evaluation instruction data are also beneficial for code generation tasks. Considering Evol-Instruct depends on a seed instruction data which is less diversity, Wei et al. (2023) proposes OSS-Instruct, which leverages open-source code cnippets to generate high-diversity instructions. They also propose Magicoder-Evol-Instruct dataset and train Magicoder-S, which is the first 7B model to exceed 70% on HumanEval Pass@1. However, we find this dataset suffers from serious data contamination (Dong et al., 2024b; Xu et al., 2024). Motivated by various works with execution feedback (Cao et al., 2024; Le et al., 2022; Chen et al., 2023; Qiao et al., 2023, 2024; Dong et al., 2024a), OpenCodeInterpreter (Zheng et al., 2024) and AutoCoder (Lei et al., 2024) leverages GPT-4 and Code Interpreter as code feedback to generate multi-turn instruction data which instruct model to refine incorrect code snippets acrroding to feedback information.\nData Selection for Instruction Tuning. While instruction fine-tuning primarily relies on a large volume of data, research such as LIMA (Zhou et al., 2024) indicates that data quality is more critical than quantity. Li et al. (2024a) proposes a novel metric Instruction Following Difficulty(IFD) to assess the challenge of responding to specific instructions. Li et al. (2024b) harnesses the disparity between one-shot and zero-shot scores to calculate a definitive 'gold score' for each instruction. Kung et al. (2023) present Active Instruction Tuning, which introduces the concept of Prompt Uncertainty. Tasks that exhibit higher Prompt Uncertainty are prioritized for instruction tuning. Furthermore, Lu et al. (2023) introduce an automated instruction tagging method (INSTAG), which employs ChatGPT to generate detailed, open-ended labels for instructions. It starts by sorting instructions in descending order of label count and then iteratively adds instructions to a subset based on the uniqueness of their labels. Deita (Liu et al., 2024) integrates a multifaceted approach for selecting instruction data, focusing on complexity, quality, and diversity. Utilizing the WizardLM technique, ChatGPT is employed to augment instructions, which are then evaluated for both complexity and quality by specially trained scorers."}, {"title": "6 Conclusion And Future Work", "content": "Code LLMs have raised great interest in current LLM research and plenty of code instruction datasets are proposed over time. However, although many of them claim that good results are achieved on the popular benchmark HumanEval, we find several datasets may have data leakage by using benchmark samples as seed data in self-instruct or evolveinstruct. In this paper, we aim to identify which dataset genuinely qualifies as high-quality code instruction data and propose an efficient code data selection strategy for selecting valuable samples. Based on three dimensions of assessing data, we present XCoder, a family of models finetuned from LLaMA3 on our selected dataset. XCoder achieves superior performance than the SOTA baselines using fewer training samples. From the composition of our selected data mixture, we find existing code datasets have different characteristics corresponding to their construction methods, which provide new insights for developing better code LLMs."}, {"title": "7 Limitation", "content": "Our limitations are two-fold: (1) We only explore our method on the LLaMA3-Base model. More experiments on different model bases are needed to confirm our conclusions. (2) We only focus on the Code Generation task, and in the future, we need to incorporate data containing more tasks."}, {"title": "8 Broader Impacts", "content": "Similar to the other LLMs, our XCoder could also generate unethical, harmful, or misleading information, which is not considered in our work. Future research to address the ethical and societal implications is needed. XCoder is also susceptible to hallucination in ungrounded generation use cases due to its smaller size. This model is solely designed for research settings, and its testing has only been carried out in such environments. It should not be used in downstream applications, as additional analysis is needed to assess potential harm or bias in the proposed application"}]}