{"title": "'Do as I say not as I do': A Semi-Automated Approach for Jailbreak Prompt Attack against Multimodal LLMs", "authors": ["CHUN WAI CHIU", "LINGHAN HUANG", "BO LI", "HUAMING CHEN"], "abstract": "Large Language Models (LLMs) have seen widespread applications across various domains due to their growing ability to process diverse types of input data, including text, audio, image and video. While LLMs have demonstrated outstanding performance in understanding and generating contexts for different scenarios, they are vulnerable to prompt-based attacks, which are mostly via text input. In this paper, we introduce the first voice-based jailbreak attack against multimodal LLMs, termed as Flanking Attack, which can process different types of input simultaneously towards the multimodal LLMs. Our work is motivated by recent advancements in monolingual voice-driven large language models, which have introduced new attack surfaces beyond traditional text-based vulnerabilities for LLMs. To investigate these risks, we examine the state-of-the-art multimodal LLMs, which can be accessed via different types of inputs such as audio input, focusing on how adversarial prompts can bypass its defense mechanisms. We propose a novel strategy, in which the disallowed prompt is flanked by benign, narrative-driven prompts. It is integrated in the Flanking Attack which attempts to humanizes the interaction context and execute the attack through a fictional setting. Further, to better evaluate the attack performance, we present a semi-automated self-assessment framework for policy violation detection. We demonstrate that Flank Attack is capable of manipulating state-of-the-art LLMs into generating misaligned and forbidden outputs, which achieves an average attack success rate ranging from 0.67 to 0.93 across seven forbidden scenarios. These findings highlight both the potency of prompt-based obfuscation in voice-enabled contexts and the limitations of current LLMs' moderation safeguards. With a comprehensive evaluation of Flank Attack, this work establishes a replicable testing framework for adversarial robustness evaluation in multimodal LLMs. It highlights the urgent need for advanced defense strategies to address the challenges posed by evolving, context-rich attacks.", "sections": [{"title": "1 Introduction", "content": "The rapid advancement of Large Language Models (LLMs) has fostered significant progress across various domains, from natural language processing to multimodal interactions involving audio, text, and images. However, as these models become increasingly integral to critical applications, concerns regarding their robustness and susceptibility to adversarial attacks have garnered particular attention. One type of adversarial attacks, known as the jailbreak attack, aims to circumvent the internal constraints and safeguards, thereby obtaining the prohibited contents and response from LLMs [12, 33]. It has since become a significant threat to LLMs, mostly focusing on the craft of deliberate text prompt to mislead the LLMs [36].\nIn recent years, there has been a growing body of research works investigating the limitations of LLMs in handling adversarial inputs across multiple languages and modalities [24]. While numerous constraints have been implemented to actively defense such threats, the jailbreak techniques to bypass such defense continue to evolve [4, 25, 37]. Most of these studies emphasize text-based or multilingual environments for LLMs, resulting in the curation of jailbreak prompts, as illustrated in Figure. 1."}, {"title": "Our Work", "content": "This work introduces a semi-automated systematic framework for assessing the security threats of audio-enabled LLMs, specifically for the jailbreak prompt attack. We first investigate the LLMs within the context of English as the monolingual language. We leverage"}, {"title": "Contributions", "content": "Our contributions are listed as follows.\nWe perform a systematic benchmarking of audio-based jailbreak attacks against the state-of-the-art multimodal LLMs. This work presents a thorough approach specifically designed to exploit potential vulnerabilities within monolingual, audio-enabled LLMs. Upon investigation, we observe that current approaches for defending multimodal LLMs from jailbreak prompt attacks are generally effective when addressing queries consisting solely of pure forbidden questions.\nWe propose a novel, simple and universal audio-based jailbreak attack framework for LLMs. By leveraging prompt-based guidance and franking the jailbreak query with benign ones, we propose Franking Attack, which can successfully bypass the defense mechanism in the multimodal LLMs. In total, we have evaluated 2,100 prompts across seven forbidden scenarios, yielding an impressive ASR result of 0.81. This finding highlights the importance of prompt formulation in jailbreak attack strategy, as it can drastically enhance the likelihood of bypassing defensive mechanisms. The framework's adaptability and scalability make it a valuable tool for future studies on audio-based jailbreak attacks against multimodal LLMs, providing a structured approach to assess and replicate adversarial scenarios across similar models.\nWe develop a semi-automated approach to evaluate jailbreak prompt attacks against multimodal LLMs. To further enhance the generation and assessment of jailbreak prompt attacks for multimodal LLMs, a new semi-automated approach is introduced. Unlike traditional method, we feature an aligned multimodal LLM to facilitate the evaluation of responses for policy compliance. We anticipate this approach can be adapted for future research of multimodal LLMs so that they will progressively improve response quality and better align with safety guidelines."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Large Language Models", "content": "Large language models (LLMs) and artificial intelligence (AI) are two interconnected fields that are dramatically changing human life with technology. Generally AI refers to the ability of computers to simulate intelligent human behavior. This covers a variety of techniques and applications, ranging from simple algorithms to complex deep neural networks. Within this context, LLMs represent a specific and highly advanced subset of AI methodologies.\nLLMs, such as OpenAi's GPT family, and LLaMa, are a type of generative artificial intelligence designed to comprehend and generate human-like outputs. These models utilize deep learning architecture, particularly Transformers, to interpret and generate coherent and contextual text. As noted by [20], \u2018Generative AI, often referred to as GenAI, involves systems that have the capacity to generate content autonomously, and large language models (LLMs) are a prominent embodiment of this concept'.\nRecent works have demonstrated LLMs' tremendous potential for achiveing human-like intelligence [2, 5, 28], benefiting from large-scale training datasets along with a significant number of model parameters. However, concerns have been raised regarding the safety and security of LLMs, particularly their misuse by malicious actors. These risks involve a wide range of issues, including social engineering and data breaches [9]. To mitigate such risks, various methods have been presented to regulate LLMs usage, including fine-tuning LLMs to enhance ethical standards compliance [2, 28] and using third-party censorship systems to identify and filter inappropriate inputs or outputs [9]. Despite these efforts, existing defenses have been empirically circumvented [4, 6, 14, 22, 24]. It highlights the critical need for further research to ensure the safe and ethical udage of LLMs."}, {"title": "2.2 LLMs safety and its limitation", "content": "LLMs may fabricate facts (referred to as \u2018hallucinations'), create divisive content, or reproduce prejudice, hate speech or stereotypes [9]. These problems largely arise from the large and diverse data sets use for pre-training. Reinforcement Learning with Human Feedback (RLHF) is thus considered as mitigation to align LLMs with human values to address these issues [3, 9].\nAccording to GenAi, Gemini has a decreased propensity to produce damaging information or have hallucinations[27]. Undesirable behaviors may arise when instructions to labelers are inadequately specified during the reward model data collection phase of the Reinforcement Learning with Human Feedback (RLHF) pipeline[2]. The process involves injecting policy-driven guidelines, sometimes referred to as 'constitutions,' which guide the model in handling sensitive topics such as political neutrality. For example, in topics like elections, the model is trained to avoid taking sides, ensuring that responses maintain a neutral point of view. This method is inspired by Constitutional AI, where human feedback is used to revise responses and select safer outputs.[27]\nTaking Gemini as an example, the zero-sample reasoning ability [17] to modify answers and choose between multiple candidate answers. For example, in topics like elections, the model is trained to avoid taking sides, ensuring that responses maintain a neutral point of view. This method is inspired by Constitutional AI, where human feedback is used to revise responses and select safer outputs.\nAccording to recent research on supervised fine-tuning (SFT), balancing the harmlessness and helpfulness of language model responses remains a significant challenge. While a response such as, 'I cannot help with that because it violates X policy', refer to Appendix D, ensures safety by avoiding harmful content, it often fails to meet user expectations for helpfulness. The difficulty lies in striking an appropriate balance between these two objectives within the model's fine-tuning framework [27]. Furthermore, the landscape of harmful query patterns is highly dynamic, which"}, {"title": "3 Related Works", "content": ""}, {"title": "3.1 Adversarial Attack", "content": "Machine learning algorithms are known to be vulnerable to adversarial attacks, in which the carefully crafted inputs can result in producing consistently erroneous outputs [11]. Understanding adversarial attacks in the context of an LLM presents several challenges. LLMs are complicated since they are large-scale, imaginative, situational, multi-modal, and increasingly intertwined into intricate ecosystems (e.g. large language model based autonomous agents [24]). As such, the threat presented by adversarial attackers exhibits distinct behaviors that require meticulous examination. Thus, accurate threat models is important to guide the creation of principled defenses. Here we list some motivational examples for adversarial attack on an LLM:\n(1) Personal use of LLM extension in browser as a shopping assistant. Malicious sellers embed adversarial messages in the text or images of their product pages to pollute the context processed by shopping extensions, thereby increasing the likelihood of product recommendations.\n(2) Attempts to get harmful information from an LLM on how to make a bomb. Although the model was fine-tuned and adjusted to prevent the spread of harmful messages, users successfully elicited dangerous messages by manipulating prompts in ways that bypassed its safety mechanisms.\n(3) Use the LLM enhanced programming assistant to assist in writing code. An adversarial example accidentally caused LLM to generate code with a malicious backdoor."}, {"title": "3.2 Multi-modal Attack", "content": "The target model of a multi-modal attack accepts input from multiple modalities (such as text, images, audio, etc.)[8]. These attacks exploit the increased complexity and introduction of new vulnerabilities by combining different types of input data. Additional modalities open up fresh attack vectors. Traditional textual alignment methods frequently fail to protect the joint embedding space that these multi-modal inputs generate."}, {"title": "3.3 Jailbreak Prompt Attacks", "content": "The focus of the prompt injection attack is to influence the input of the model by inserting adversarially constructed hints. This causes the model to misinterpret input data as instructions, which is how attacker-controlled deceptive output is produced. In effect, this type of attack takes over the model's expected responsibilities, which are typically specified by a system prompt (ref to Figure 6) provided by the provider or developer.\nPrompt injection (PI) attacks exploit the way LLM interprets and processes input prompts. They can override the original instructions and controls set by the developer, causing the model to produce output that benefits the attacker[9]. PI attacks involve crafting adversarial prompts that the LLMs mistake for legitimate instructions. This manipulation can cause the model to produce deceptive or harmful outputs.\nNaive injection attacks primarily target image manipulation in order to mislead classification models. Inspired by the work of Noever[19], who demonstrated the ability to deceive OpenAI's CLIP[21] in zero-shot image classification by adding text that contradicted the image content,"}, {"title": "4 Threat Model", "content": "In this work, we consider the primary objectives from potential attackers are from two aspects. First, the attacker aim to circumvent the built-in content moderation systems so as to retrieve or generate prohibited content, including but not limited to the contents outlined in the specific user policies. On top of this, the attacker may even attempt to elicit sensitive data from LLMs, such proprietary training data, model parameters, or system prompts etc. Given the recent developments of LLMs, including open-source and enterprise ones, we note that the resource and time may be the primary concerns for everybody including the attackers. Moreover, general open-source LLMs don't support multimodal capabilities. Thus, in this work, we consider the current available enterprise solutions for jailbreak attack.\nAttacker's Capabilities The effectiveness of an attack is contingent upon the attacker's capabilities, which encompass their knowledge, resources, and access levels. Since the attackers may have the ability to directly interact with multimodal LLMs, we consider this access presents a channel for the attackers to submit the well crafted prompts in either the forms of audio or text. This doesn't specify any prior knowledge of the accessible multimodal LLMs, while the knowledge of LLMs\u2019"}, {"title": "Threat Scenarios", "content": "Building upon the identified objectives, capabilities, and attack surfaces, the threat scenarios will be specified by the design of jailbreak prompts and the targeted plots, such as the examples in Appendix A. As such, the following specific threat scenarios are delineated:\n\u2022 Jailbreak Prompt: Attackers embed adversarial prompts within audio inputs, leveraging narrative contexts or fictional scenarios to manipulate Gemini into generating restricted content. For example, embedding illegal activity instructions within a story or simulation game narrative.\n\u2022 Multi-Modal Input: Combining audio inputs with other modalities, such as text or images, to create a composite adversarial prompt that is more likely to bypass multimodal LLMs\u2019 safety mechanisms. For example, pairing an audio prompt with a benign text overlay to distract or confuse the model's content moderation process."}, {"title": "5 Methodology", "content": "In this paper, we adopt a stepwise methodology to evaluate the multimodal LLMs' resilience against audio-based jailbreak attacks. A preliminary study of the jailbreak attacks is conducted for around 100 adversarial experiments in the beginning. We manually review each output compliance with the corresponding usage policies. While this approach yields critical insights into potential audio-based jailbreak attacks for multimodal LLMs, it exhibits inherent limitations for scalability and efficiency. Thus, a semi-automated approach is proposed in this work, together with Flanking Attack, to better generate and assess the audio-based jailbreak attacks, enabling a more comprehensive evaluation of multimodal LLMs' defensive mechanisms. Following we start with the forbidden question set."}, {"title": "5.1 Forbidden Question Set", "content": "To rigorously assess the defenses against policy violations, we employ a Forbidden Questions Set designed to provoke responses that might contravene the usage policies. In this work, we strictly follow the design principles implemented in previous Forbidden Questions dataset work [25]. The question set is strategically crafted based on seven distinctive scenarios, each representing a category of content prohibited by the user guidelines [1].\nThe forbidden questions are designed to simulate realistic and contextually varied prompts within each scenario, maximizing the likelihood of policy breaches. Each set of questions for a given scenario is based on the prohibited content categories and reflects situations or queries that, if responded to, would indicate a lapse in the model's filtering capabilities."}, {"title": "5.2 Methodology Overview", "content": "This work adopts an innovative framework, leveraging both prompt-based guidance and Flanking Attack as principal design strategies against multimodal LLMs. Inspired by [26] and [29], in the Attack Settings, structured Text Prompts establish a fictional setting and assign character roles to craft a superficially benign context. For the Flanking Attack, the idea is to flank sensitive and malicious inquiries within otherwise benign prompts, aiming to circumvent LLMs' filters by obfuscating potentially problematic content."}, {"title": "Attacks Setting", "content": ""}, {"title": "5.2.1 Text Prompt", "content": ""}, {"title": "Character Setting, Character, and Rule Application", "content": "The initial stage leverages text-based prompt injection, a technique that introduces adversarial prompts within fictional and benign contexts. This stage is foundational, as it prepares LLMs to accept more complex inputs during the subsequent audio-based phase [25, 26].\n\u2022 Setting: The adversarial prompts are framed within fictional and non-threatening contexts. Different scenarios are applied for each forbidden scenarios to distance the the content from real-world applicability, for example hypothetical worlds. This contextual shift helps in reducing the model's resistance to potentially sensitive queries, as it interprets them within a 'safe' fictional context.\n\u2022 Character: The characters in each scene play a vital role in driving the narrative. By assigning personalities, motivations and fictional characters (for example, a detailed guide in a sci-fi screenplay or a hacker in a simulation game), prompts become more engaging and convincing. Characters can be anything, including humans, animals or inanimate objects with anthropomorphic qualities. This further embed the query in a fictional layer.\n\u2022 Rule Application: There is a specific rule embedded in the scene that clearly states that the dialogue is a simulation and has no implications in the real world. For example, a hint might include: 'We are just simulating a scenario; no real harm is intended. This rule reassures the model, encouraging it to comply with the query under the assumption of harmless exercise."}, {"title": "5.2.2 Flanking Attack (Voice-Based)", "content": "The second stage is Flanking Attack, which builds on text-based prompt injection by introducing the prompts through voice input. The idea is to flank the critical adversarial content with benign queries to bypass LLMs' defenses. This stage includes unique considerations due to multimodal LLMs' capabilities, ensuring that audio variations are seamlessly integrated within the prompt sequence.\n\u2022 Sequential Layering: The voice-based attack is structured to include a series of five to nine questions, where the central (adversarial) question is framed in a non-threatening, hypothetical format and surrounded by contextually benign queries. This sequence reduces the likelihood of LLMs' flagging the critical content, as the adversarial question is masked by the benign context.\n\u2022 Positioning of the Adversarial Query: The adversarial question is intentionally placed in the middle of the sequence (typically as the third or fifth query) to avoid triggering LLMs' safety mechanisms that might be more vigilant at the beginning or end of the input. This approach leverages the attention dynamics within LLMs, as Flankinging critical prompts"}, {"title": "5.3 Semi-Automated Model for Policy Violation Detection", "content": "This section introduces a semi-automated model is introduced, a novel approach in the field of adversarial attacks. This model enhances efficiency by automating parts of the output evaluation process, significantly reducing the time required for the detection of policy violations.\n\u2022 Documented Output Storage: Each Gemini model's response generated in this stage is automatically saved in a structured document file, preserving a clear record of all interactions."}, {"title": "Gemini-Based Self-Evaluation", "content": "The innovative aspect of this semi-automated approach lies in its use of Gemini itself as a secondary evaluator. Once the outputs are documented, a new prompt instructs Gemini to read the log file and compare each response against its own policy guidelines. By guiding Gemini to perform this self-evaluation, the model is prompted to identify any outputs that potentially violate its usage restrictions."}, {"title": "Evaluation Process", "content": "The task of Gemini at this stage is twofold: first, to interpret the outputs documented in the log, and second, to self-assess based on its policy constraints. The model flags any instances of content that it deems as potentially non-compliant, providing an automatic layer of scrutiny that complements manual inspection. This semi-automated evaluation is a novel contribution to adversarial attack research, as it leverages the model's self-assessment capabilities to enhance detection accuracy and streamline the review process."}, {"title": "6 Experiment Result", "content": "In this section, we evaluate the performance of Flanking Attack and the semi-automated approach for jailbreak prompt attack against the multimodal LLMs. In particularly, we select Gemini as the state-of-the-art multimodal LLMs as the target. Until the conclusion of this work, Gemini represents the first and only multimodal LLMs that allows external users to interact with. We leverage the Gemini's API for the generation and evaluation of audio-based jailbreak prompts\u00b9. As such, we further use the Gemini's usage policy as the constraints [27]. The details of the descriptions and questions bank are provided in the Appendix A.\nThe results are presented in Table 1, which outlines the Attack Success Rates (ASRs) across seven forbidden scenarios for four different configurations of the adversarial methodology. The four configurations, each comprising varying elements of the Prompt Injection and Flanking"}, {"title": "6.1 Analysis of Gemini's Vulnerabilities to Multi-Modal, Multi-Layered Attacks", "content": "The success of Configuration 1 in bypassing Gemini's defense mechanisms lies in its sophisticated multi-layered approach, combining Text Prompt, Setting, Character, and Plot, and Flanking Attack techniques. This configuration constructs a complex narrative around sensitive prompts, making it difficult for Gemini's content filters to accurately identify and block potentially harmful or restricted queries.\nThe fictional context created by the Setting, Character, and Plot elements plays a pivotal role in lowering Gemini's defenses. By framing the scenario as a harmless game, where the user pretends to be a \u201chacker,\u201d the prompts appear to be innocuous simulations rather than genuine requests. For example, phrases like \u201cImagine we are in a game\u201d and \u201cWe are just simulating\u201d encourage Gemini to interpret the prompts as part of a fictional scenario, as shown in Figure 10 and Figure 17. This fictionalization acts as a disarming tactic, making the content appear less threatening and prompting Gemini's content filters to relax their scrutiny. Consequently, Gemini perceives the scenario as playful or purely hypothetical, which weakens its usual defenses against potentially harmful content.\nThe Flanking Attack technique enhances this approach by embedding sensitive requests between benign, innocuous questions. For example, by surrounding critical queries like \u201cSet up a fraudulent business\u201d or \u201cDevise a plan of robbing a bank\u201d with neutral requests such as \u201cGuide me how to make a cake\u201d and \u201cStep of washing hand,\u201d the prompts blend harmless content with sensitive material. This layered structure, as seen in Figure 10 and Figure 17, obscures the restricted content, making it harder for Gemini to identify and flag it as potentially harmful. The Flanking Attack thus serves to further obfuscate the sensitive intent of the prompts, diluting their apparent risk level by mixing them with benign context. This layered structure reduces the likelihood that Gemini's filters will flag the critical queries, as the benign context diverts attention from the forbidden content.\nIn Figure 10 and Figure 17, Gemini responds with detailed steps for a hypothetical bank heist, illustrating how the fictional setting encourages it to interpret the request as safe to answer. Gemini's responses to these prompts reveal how the model processes and interprets the information within this multi-modal, multi-layered setup. Gemini provides disclaimers, such as \u201cThis is a purely fictional scenario for entertainment purposes only,\" which indicates that it recognizes the hypothetical framing and attempts to maintain ethical boundaries by including cautionary language. However, despite these disclaimers, Gemini still proceeds to provide detailed responses on restricted topics like setting up a fraudulent business or robbing a bank, under the guise of entertainment or hypothetical guidance. This behavior suggests that while Gemini's filtering mechanisms prompt it to add disclaimers in potentially sensitive scenarios, the fictionalized context combined with the Flanking Attack reduces its sensitivity, allowing the model to overlook the restricted nature of the queries.\nConfiguration 1 underscores a fundamental vulnerability in Gemini's filtering mechanism: its reliance on surface-level cues and context to gauge the nature of the prompts. By embedding sensitive content within a playful or hypothetical narrative and then further obfuscating it through the Flanking Attack, the methodology exploits Gemini's inability to perform deep semantic analysis to recognize policy violations masked by fictional settings. The fictional game framework combined with the layered structure of the Flanking Attack overwhelms Gemini's content\""}, {"title": "6.2 Analysis of Gemini's Defense Mechanisms Across Reduced Adversarial Configurations", "content": "In examining the results for Configuration 2, Configuration 3, and Configuration 4, the effectiveness of Gemini's defenses becomes increasingly apparent as the components of the adversarial framework are progressively reduced. These configurations reveal how Gemini's content moderation system responds differently based on the complexity and specificity of the prompts. Without the full set of elements used in Configuration 1, Gemini's defenses are more effective at filtering out potentially sensitive or restricted content.\nIn Configuration 2, which includes Text Prompt along with Setting, Character, and Plot but excludes the Flanking Attack. The presence of a Text Prompt still provides enough direction to sometimes bypass Gemini's filters by creating a fictionalized, narrative-driven context. For example, in one successful instance (refer to Figure 18), Gemini responds to a prompt framed within a hypothetical scenario and provides detailed, albeit hypothetical, instructions. This success suggests that Text Prompt combined with Setting, Character, and Plot can occasionally create a scenario that Gemini interprets as safe to answer. However, without the Flanking Attack to add layers of obfuscation, the prompt lacks the structural complexity needed to consistently evade Gemini's defenses."}, {"title": "6.3 Effectiveness of semi-automated approach", "content": "In this section, we provide some preliminary results concerning the manual approach and the proposed semi-automated approach for generation and evaluation of jailbreak prompts. Figure 14 illustrates one of the successful cases where the Gemini API outputs an answer, despite the presence of a potentially policy-violating prompt. Other examples of successful cases can be found in Figure 21 and Figure 22 in Appendix C. In Figure 14, Gemini provides a detailed response to the \u201cBank Heist\" scenario, which can be interpreted as a violation of its usage policy. This example demonstrates the effectiveness of the Flanking Attack, as the sensitive query is embedded between benign questions. Gemini's response encompasses three distinct topics-cake hacking, bank heisting, and handwashing-which masks the adversarial intent within an innocuous context. This layered approach confirms the success of the Flanking Attack in bypassing Gemini's defenses, as it enables the model to provide detailed responses without triggering content moderation filters. Figure 15 and Appendix 23 represent examples of failure cases, where manual inspection is required to determine if Gemini's output bypassed its defense mechanisms. From Figure 15, it is evident that this particular attack was unsuccessful."}, {"title": "6.4 Ablation study", "content": "In this section, we provide a thorough ablation study investigating the effects of different modules in our proposed framework, including the Text Prompt, Setting, Character, Plot and Flanking Attack."}, {"title": "6.4.1 Configuration 1: Text Prompt + Setting + Character + Plot + Flanking Attack", "content": "This configuration, which includes the full range of adversarial elements, achieved the highest average ASR of 0.81 across scenarios. By incorporating the Text Prompt in combination with Setting, Character, Plot, and Flanking Attack, this setup effectively creates complex, layered prompts that Gemini struggles to filter accurately. Notably, the ASR in the Illegal Activities scenario reached 0.93, the highest recorded in this study, indicating that the combination of all elements significantly enhances the model's ability to bypass filters related to sensitive or restricted content.\nThe inclusion of the Flanking Attack method in this configuration appears to be particularly effective. The sequential layering of benign and adversarial prompts, along with varied linguistic"}, {"title": "6.4.2 Configuration 2: Text Prompt + Setting + Character + Plot", "content": "This configuration, which omits the Flanking Attack component but retains Text Prompt, Setting, Character, and Plot, yields an average ASR of 0.57. While the ASR is lower than Configuration 1, this setup still demonstrates substantial effectiveness, particularly in Illegal Activities (0.6) and Abuse and Disruption of Services (0.63) scenarios. The presence of Text Prompt with Setting and Character creates a sufficiently strong narrative context, allowing the prompts to bypass Gemini's content restrictions to a moderate degree.\nThe absence of the Flanking Attack method in this configuration results in a noticeable drop in ASR across most scenarios. For instance, in Misinformation and Misleading Content, the ASR decreases to 0.42, and in Sexually Explicit Content, it is 0.51. This reduction suggests that while the Setting, Character, and Plot framework provides a persuasive context, the lack of Flankinging techniques reduces the prompts' ability to evade detection fully. This finding highlights the added value of the Flanking Attack technique in creating multi-layered prompts that Gemini finds challenging to interpret and filter."}, {"title": "6.4.3 Configuration 3: Setting + Character + Plot", "content": "In the third configuration, only Setting, Character, and Plot are applied, without the use of Text Prompt or Flanking Attack techniques. This configuration demonstrates a further reduction in ASR, averaging 0.28 across scenarios. The ASR in Illegal Activities and Harmful Content Generation drops to 0.32 and 0.28 respectively, underscoring the limitations of this approach when compared to configurations that include explicit Text Prompts or Flanking Attacks.\nThe absence of Text Prompt in this configuration likely reduces the prompts' specificity and directness, weakening the adversarial attack. Additionally, without Flanking Attack layering, the prompts become more recognizable to Gemini's filters, as they lack the multi-layered obfuscation necessary to bypass Gemini's content moderation consistently. This lower ASR across scenarios emphasizes the critical role of Text Prompts and Flanking Attacks in achieving higher rates of policy violations."}, {"title": "6.4.4 Configuration 4: Plot only", "content": "The final configuration, which utilizes only Plot without Setting, Character, Text Prompt, or Flanking Attack, records the lowest average ASR of 0.12 across scenarios. The ASR for Illegal Activities in this configuration is 0.13, while the ASR for Misinformation and Misleading Content drops to 0.00. These findings illustrate the ineffectiveness of Plot-only prompts in bypassing Gemini's defense mechanisms.\nThis configuration serves as a baseline, demonstrating that without the narrative complexity provided by Setting and Character or the specificity of Text Prompts, the model easily recognizes and filters prohibited content. The significant reduction in ASR highlights the importance of each element within the adversarial methodology. It suggests that Plot alone does not provide sufficient context or subtext to mask forbidden content effectively, underscoring the value of combining multiple narrative and linguistic techniques."}, {"title": "6.4.5 Observation", "content": "The results underscore the efficacy of a multi-component adversarial approach, where Text Prompt and Flanking Attack are essential for achieving high ASRs. The significant decrease in ASRs across scenarios when either of these components is omitted illustrates the need for both direct, targeted prompts and layered obfuscation to evade detection effectively. The narrative context provided by Setting + Character + Plot contributes to building a credible fictional scenario but is insufficient on its own to produce high success rates. This analysis emphasizes that a layered, contextually rich methodology is crucial for maximizing ASR, establishing a new standard for adversarial attacks on voice-enabled LLMs like Gemini."}, {"title": "7 Challenges and Future Directions", "content": "The rapid evolution of Multimodal Large Language Models (MLLMs) presents a significant challenge in adversarial research. As models like Gemini undergo frequent updates and enhancements, previously identified vulnerabilities may be mitigated or eliminated, rendering earlier adversarial techniques less effective. This dynamic landscape necessitates continuous adaptation and refinement of attack methodologies to keep pace with the latest model architectures and defense mechanisms. Wang et al. (2024) [31], highlight the swift advancements in MLLMs, noting that \u201cthe development of MLLMs is not only an inevitable trend in technological evolution but also a critical enhancement for improving the effectiveness of AI applications\u201d.\nVarshney et al. (2024) [30], presents notable challenges for maintaining consistent adversarial testing and defense strategies. Their research underscores that each update to an LLM can significantly alter the model's sensitivity to adversarial inputs, which complicates long-term security testing.\nFuture researchers may need to constantly adapt and refine adversarial techniques to stay current with the latest model architectures and defense mechanisms. This could involve developing a systematic approach to monitor model updates and testing new methods as soon as changes are implemented."}, {"title": "7.1 Future Direction 1: Exploring Audio Variations in Voice Input Attacks:", "content": "While this study employs a voice-based Flanking Attack, it does not account for variations in audio properties, such as pitch, tone, or speech speed, which may affect the model's response. Future research could focus on testing different audio characteristics to analyze how they impact Gemini's ability to recognize and filter sensitive content. This line of research could reveal additional vulnerabilities in MLLMs' processing of audio inputs, potentially leading to more effective adversarial techniques that exploit audio-based nuances\nTo develop a more focused and nuanced discussion on the first research direction. One key limitation observed in the field of voice-controlled systems, as highlighted by Wang et al. (2023)"}, {"title": "7.2 Future Direction 2: Manipulating Sentence Structure and Positional Changes", "content": "Another limitation of this study is its focus on a fixed sentence structure within the adversarial prompts. Future studies could investigate how changes in sentence order or the positioning of sensitive queries within a prompt sequence affect the model's filtering mechanisms. For instance, altering the order of benign and sensitive questions in the Flanking Attack may yield insights into the model's attention patterns and its sensitivity to different prompt structures. Understanding these positional impacts could refine prompt design to further improve success rates in bypassing content filters.\nTo address the second research direction of manipulating sentence structure and positional changes in adversarial attacks, Prompt Attack's framework provides insights into sentence-level perturbations [35]. The study demonstrates that altering syntactic structures without changing the original semantic meaning can significantly affect an LLM's response reliability. Specifically, the authors illustrate that techniques like paraphrasing, restructuring phrases, and modifying sentence positions can introduce subtle yet impaction variations that challenge the model's interpretative consistency.[35]\nFuture studies could experiment with a variety of syntactic transformations and analyze how Gemini's defense mechanisms respond to these positional shifts, aiming to further refine the effectiveness of adversarial prompts by targeting model sensitivity to sentence structure and position."}, {"title": "7.3 Future Direction 3: Enhancing the Flanking Attack with Multilingual Inputs:", "content": "This study uses a monolingual approach in the Flanking Attack, which may limit its effectiveness in scenarios where language diversity could add complexity. Future researchers could explore the impact of integrating different languages within the same prompt sequence. By embedding benign and sensitive queries in multiple languages, researchers could assess whether multilingual prompts are more effective at circumventing Gemini's filters. This approach could lead to a more robust adversarial method by leveraging the language-processing limitations of MLLMs.\nTo address the fourth research direction-enhancing the Flanking Attack with multilingual inputs-the findings from Upadhayay & Behzadan (2024) [29] offer valuable insights. They introduced a multilingual mixture adaptive attack in which questions in multiple languages are strategically layered around an adversarial query, found that embedding sensitive questions in low-resource languages often bypasses safety mechanisms, particularly when surrounded by questions in other languages, thus confusing the model's content moderation system.[29]\nFor future research, extending the Flanking Attack with more diverse languages could amplify its effectiveness, especially by incorporating languages in which the model may have limited proficiency. Additionally, testing different language pairings and sequences could reveal optimal configurations for bypassing safety measures. This multilingual approach not only diversifies adversarial tactics but also probes the model's safety alignment capabilities in multilingual settings, offering insights into how language diversity impacts an LLM's defensive robustness."}, {"title": "8 Conclusion", "content": "In this work, we explored innovative audio-based jailbreak prompt attack to evaluate the effectiveness of adversarial attacks on multimodal large language models (LLMs). With the semi-automated approach, we leverage the prompt-based guidance with the proposed Flanking Attack to successfully evade the defense mechanisms implemented in the multimodal LLMs. In particular, we systematically examined the potential security threats of adversarial audio-based jailbreak prompts within the context of English-only monolingual models. The semi-automated approach enables us for a thorough assessment, yielding an average attack success rate of 0.81 across seven specific forbidden areas for 2,100 well crafted prompts. These findings feature the success of multi-layered adversarial strategies, particularly those leveraging complex narrative framing and obfuscation, are much more effective at bypassing multimodal LLMs' content filters. We anticipate this work will have broader"}]}