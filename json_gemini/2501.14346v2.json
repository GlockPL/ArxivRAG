{"title": "HorNets: Learning from Discrete and Continuous Signals with Routing Neural Networks", "authors": ["Boshko Koloski", "Nada Lavra\u010d", "Bla\u017e \u0160krlj"], "abstract": "Construction of neural network architectures suitable for learning from both continuous and discrete tabular data is a challenging research endeavor. Contemporary high-dimensional tabular data sets are often characterized by a relatively small instance count, requiring data-efficient learning. We propose HorNets (Horn Networks), a neural network architecture with state-of-the-art performance on synthetic and real-life data sets from scarce-data tabular domains. HorNets are based on a clipped polynomial-like activation function, extended by a custom discrete-continuous routing mechanism that decides which part of the neural network to optimize based on the input's cardinality. By explicitly modeling parts of the feature combination space or combining whole space in a linear attention-like manner, HorNets dynamically decide which mode of operation is the most suitable for a given piece of data with no explicit supervision. This architecture is one of the few approaches that reliably retrieves logical clauses (including noisy XNOR) and achieves state-of-the-art classification performance on 14 real-life biomedical high-dimensional data sets. HorNets are made freely available under a permissive license alongside a synthetic generator of categorical benchmarks.", "sections": [{"title": "1 Introduction", "content": "In recent years, neural network models have shown great promise when modeling complex real-life data sets such as images, texts, and networks [1]. Data sets are most frequently"}, {"title": "2 Related work", "content": "This section overviews some of the related work that impacted the development of HorNets. We begin by discussing binarized neural networks, followed by the body of literature focusing on neuro-symbolic learning.\nExploration of how rule lists can be transformed into networks capable of classification was performed by [7]. Their networks could account for AND, OR and NOT connectives, offering expressions in full propositional logic. The subfield of studying neural networks that also resonates with the proposed approach concerns binary neural networks. For example, the"}, {"title": "3 HorNets Architecture Overview", "content": "We begin the section with an overview of the HorNets architecture, followed by a theoretical analysis of its expressiveness and computational complexity."}, {"title": "3.1 General overview and intuition", "content": "We begin by discussing a high-level overview of the HorNets architecture, followed by a more detailed inspection in the following sections. The architecture consists of three main components: the routing operator, categorical, and continuous blocks. A schematic overview is shown in Figure 1."}, {"title": "3.2 Modeling logical operators via activations", "content": "We first discuss one of the main contributions of this paper, POLYCLIP activation, its formulation and connections to zero-order (propositional) logic. Conventionally used activations surveyed in [34], such as ReLU and Sigmoid, offer truncation of weight values to positive real values and were shown to offer state-of-the-art performance on many tasks. On the other hand, activations such as ELU and LReLU offer the inclusion of negative weights; however, they are not symmetric and thus offer little insight into the effect of positive and negative weights. Activations such as tanH indeed offer symmetry around zero, and incorporate negative values, however, their infinite bounds hinder interpretability/capability to truncate them to logical clauses directly.\nIn this work, we introduce the polyClip family of activation functions. For a given parameter $k \\in N_0$, the corresponding polyClip activation function is defined as follows:\nPOLYCLIP(x,k) = clip(x^{2k+1},-1,1) = \\begin{cases}\n-1;&x < -1 \\\\\n1;&x \\geq 1 \\\\\nx^{2k+1};& else.\\\n\\end{cases}\nHere, the x represents real-valued input and k a parameter, determining the order of the considered odd polynomial.\nNote that for the rest of this paper, if A is a real-valued matrix, we denote with polyClipk (A) the matrix in which the polyClip function is applied to each element. Apart from being efficient in computing, polyClip activation offers a probabilistic means of expressing statements in formal logic, which we will discuss next."}, {"title": "3.3 HorNets Architecture formulation", "content": "We continue the discussion with a more detailed formulation of HorNets architecture. Let X \u2208 \u211d^{b\u00d7d} represent an input batch of data (b being batch size and d the dimension). Let X represent the set of values in this batch. The routing layer, named catRouter, denotes a category routing function that distinguishes between two categories: discrete (linAtt \u2013 Linear Attention) and continuous (catInt \u2013 Categorical Interactions). The catRouter mechanism can be in its general form written as\nCATROUTER(X, Y) = \\begin{cases}\nLINATT(X, Y);& |X| > 2; \\\\\nCATINT(X, Y);& else.\n\\end{cases}\nThe CATROUTER(X) is thus an efficient operation with negligible practical cost. The subsequent possible operations, LINATT(x) and CATINT(x) are discussed next. The LINATT(x) is defined as (recursive formulation aligned with code)\n$\\begin{aligned}\nx_0 &= \\frac{x}{\\max(||x||_p, \\epsilon)} \\\\\nx_1 &= \\text{POLYCLIP}(x_0) x \\\\\nx_2 &= x_1 w + b.\n\\end{aligned}$\nThis part of the architecture consists of three main steps; normalization p norm (\u025b is a small constant), activation and elementwise product (\u00b7), followed by a linear layer.\nThe second part of the architecture CATROUTER(X) can be formulated as follows. Let M \u2208 \u211d^{O\u00d7|C|} represent the interaction factorization matrix, where o represents the order of interaction and C the space of input combinations to be considered. This matrix serves as the basis for representing different feature interactions separately, enabling fine-grained control at interaction level. The forward pass for this part of the architecture, however, is computationally substantially more expensive and can be formulated as\n$\\begin{aligned}\nF &= \\text{COMBACTOP}(M, x, \\text{CombIndices}) \\\\\nx_0 &= \\text{DROPOUT}(F) \\\\\nx_1 &= \\text{SOFTMAX}(x_0) \\\\\nx_2 &= x_1 w + b.\n\\end{aligned}$"}, {"title": "3.4 Pseudovariables and the curse of higher order interactions", "content": "While developing the architecture, we observed that unless specific interactions were retrieved, performance on simple tasks such as modeling of logic gates could have been more consistent. By considering higher-order interactions way beyond what is normally considered in matrix factorization (e.g., order 16 and more), we can force HorNets to sample from larger feature subspaces. However, if e.g., a subspace of 16 interactions contains the two features that govern the output signal, yet the remaining 14 represent pure noise, optimization (backpropagation) was observed to have issues with convergence. To remedy this shortcoming, we introduced the notion of pseudovariables \u2013 artificially added features that, in the"}, {"title": "4 Evaluation", "content": "In this section, we set up the evaluation scenario. We focus on assessing the predictive power of our method \u2013 first on synthetic data, where we want to analyze the logic modeling capabilities and second on real-life downstream classification tasks."}, {"title": "4.1 Empirical evaluation on synthetic data", "content": "The first round of experiments revolves around benchmarking of HorNets' logic modeling capabilities. To systematically evaluate this trait, we designed a synthetic data generator that enables the construction of arbitrary-dimensional binary training datasets, where target space is defined as a logical combination of a given feature subspace. We considered the main logic operators, namely AND, OR, NOT, XOR and XNOR. We refer to these gates as OP. The dimension parameter d parametrizes the generator, and the instance count parameter c.\nIt first generates a binary matrix B \u2208 {0,1}^{c\u00d7d}, where \u2200k \u2208 B\u2758k ~ BERNOULLI(0.5). Note"}, {"title": "4.2 Empirical evaluation on real-life data", "content": "We proceed by an overview of the HorNets performance on real-life biomedical data. The motivation for this is twofold: First, biomedical data features an abundant number of variables but a scarce number of examples, which leads to problematic behavior when MLPs are applied directly. Second, neural network-based approaches such as TabNET and MLPs are hardly interpretable, posing challenges for the real-life application of machine learning models in medicine. The data sets [36] considered represent different classification problems that aim to associate gene expression signals with the target output (e.g. tumor presence). A summary of data sets is shown in Table 1.\nMotivated by the work of [44], we aim to compare our method against various classifiers, including linear, ensemble, neural networks, and AutoML classifiers. Specifically, we have selected the following classifiers:"}, {"title": "5 Results", "content": "This section presents the experimental results of the synthetic and the real biomedical experiments, followed by the analysis of the execution time and impact of hyperparameters on the method."}, {"title": "5.1 Synthetic results", "content": "Table 2 presents the performance of our method on synthetic logic modeling datasets. Using the PolyClip variant, our method achieves a perfect score, demonstrating its ability to model arbitrary Horn clauses. Strong competitors such as Logistic Regression, Random Forest, and MLP could model logical operations like AND, NOT, OR, and XNOR when the number of variables was less than 8. All models, with the exclusion of the Random Forest, failed to model the XOR operator. Interestingly, the attention-based TabNET method failed to successfully model any interactions regardless of the number of variables across all problems. High-dimensional settings posed challenges for most methods, except for the proposed HorNet. These results highlight the ability of HorNets with the PolyClip variant to model arbitrary logical clauses effectively."}, {"title": "5.3 Execution time", "content": "Next, we analyze the execution time of the methods shown in Figure 6. Our results show that both the polyClip and ReLU variants exhibit indistinguishable performance. The proposed method significantly outperforms advanced neural baselines such as LassoNet and TPOT (which exhaustively search for solutions). It also outperforms ensemble methods such as RandomFrost and XGBoost. Furthermore, we find that the execution time of all methods increases with the size of the dataset, both in terms of the number of features and the number of samples."}, {"title": "5.4 Impact of different hyperparameters", "content": "Next, we analyze the impact of different hyperparameters. Recall, we use the following grid to search and evaluate for hyperparameters Activation: ReLU and our proposed PolyClip; Order: 4, 8, 16, 32, 64, 128, 512, 1024, 2048, 4096; Number of Rules: 4, 8, 16, 32, 64, 128, 512, 1024, 2048, 4096; Learning Rate: 0.001, 0.01, 0.1; Batch Size: fixed at 15. For each tuple of parameters, we train on all datasets in the described 5 run 5-fold cross-validation setting. Figure 8 shows the optimization landscape across different hyperparameter settings, showing the interaction of different activation functions, the learning rates with the order and number of rules. The results show that no clear best setting can be found and that a theoretical parameter sweep is required when trying to optimize for the best results. Note that, however, the difference between the best-performing (88.60%) and worst-performing parameter setting (86.10%) on average is 1.98 percentage points, which we hypothesize."}, {"title": "6 Discussion and conclusions", "content": "In this section, we discuss the potential impact of the proposed HorNets based on their performance in synthetic and real-world problems. First, we review the results of the synthetic datasets. This evaluation aimed to empirically test the functional completeness of the proposed HorNets PolyClip capability. The results in Table 2 show that the network can learn any interaction between any number of variables for four neural logic operators: AND, NOT, OR, XNOR, and XOR. In contrast, the newer variants of neural networks - TabNET and MLP - as well as the naive rule learning approaches \u2013 the decision trees and random forests were unable to learn. Since we can learn these combinations of features, we believe that the routing functionality means that we can learn arbitrary combinations of features that interact with each other. This, in turn, shows the potential for using the HorNets applications in many neurosymbolic systems that Garcez et al. [51] see as the next generation of AI. On the other hand, Marconato et al. [52] emphasizes the problem of linking neurosymbolic learners and considers the production of rules about false relations as linking inferences. We show that the HorNets achieve maximum log-likelihood on both training and test data and thus successfully reconstruct the concept of ground truth \u2013 in the case of logical operations.\nUsing the results on the real biomedical datasets, we establish the effectiveness of the learned rules for subsequent application to real classification problems. The statistical tests confirm that the method can keep up with the strong AutoML baseline (TPOT), while generating interpretable rules that experts can use as a basis for further analyses. This experiment shows that our model can a) train neural networks on small, large tabular datasets, [44] and b) achieve interpretable results, something that related work highlights as necessary [53-55] Furthermore, we show that the proposed HorNets are both data- and resource-intensive, making them ideal for applications related to computing on the edge. We presented a novel approach for learning rules using neural networks. Our method learns from both continuous and discrete signals, as evidenced by rigorous evaluations on both synthetic and real biomedical datasets. The architecture excels at obtaining arbitrary Horn clauses by combining a custom polynomial-like activation function with an attention mechanism that allows it to model complex interactions. This method outperforms advanced AutoML-based learners and is well suited for numerical and categorical data with large table density, while requiring significantly less computational resources than comparable architectures. Our results show that deep neural networks augmented with attention mechanisms and user-defined activation functions are very effective in learning rules. We plan to improve our framework with uncertainty-based principles to enable the extraction of probabilistic rules. We also plan to extend our approach to explore spaces with mixed inputs. To further improve the interpretability of the extracted rules, we will perform in-depth analyses with domain experts."}, {"title": "7 Availability and Reproducibility", "content": "The code for the HorNets is freely available via the following GitHub link: https://github.com/bkolosk1/hornets. To ensure reproducibility and replicability, we provide a Singularity container environment, which integrates the code and provides a benchmarking environment."}, {"title": "Appendix: Proof of the Exponentiality of the exact CatInt interactions Using Stirling's Approximation", "content": "Stirling's approximation states that for large n:\nn! \u2248 \u221a(2\u03c0\u03b7) (n/e)^n Given the complexity expression for the combinatorial part of the CatRouter (Comb(|I|, order)), let us consider the combination Comb(|I|, order) which represents the binomial coefficient: Comb(|I|, order) =  (|I| \\atop order) =  |I|! / (order! (|I| - order)!) Using Sterling's approximation for factorials, we have: |I|! \u2248 \u221a(2\u03c0|I|) (|I|/e)^{|I|} order! \u2248 \u221a(2\u03c0 order) (order/e)^{order} (|I| - order)! \u2248 \u221a(2\u03c0(|I| - order)) ((|I| - order)/e)^{(|I| - order)} Substituting these into the binomial coefficient formula, we get: (|I| \\atop order) \u2248 \u221a(2\u03c0|I|) (|I|/e)^{|I|} / (\u221a(2\u03c0 order) (order/e)^{order}  \u221a(2\u03c0(|I| - order)) ((|I| - order)/e)^{(|I| - order)}) =  \u221a(2\u03c0|I|) / (\u221a((2\u03c0)^2) \u221a( order \u221a( |I| - order)))  (1/ e^{|I|}/ ((order/e)^{order}  \u221a(2\u03c0(|I| - order)) ((|I| - order)/e)^{(|I| - order)})) Next, the exponential term simplifies as follows:   (1/ e^{|I|}/ ((order/e)^{order}  \u221a(2\u03c0(|I| - order)) ((|I| - order)/e)^{(|I| - order)})) = (order/e)^{order}. ( \u221a(2\u03c0(|I| - order))/ ((\u221a(I| - order) / e)^((|I| - order))"}]}