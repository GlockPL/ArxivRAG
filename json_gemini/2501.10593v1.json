{"title": "ColorGrid: A Multi-Agent Non-Stationary Environment for Goal Inference and Assistance", "authors": ["Andrey Risukhin", "Kavel Rao", "Ben Caffee", "Alan Fant"], "abstract": "Autonomous agents' interactions with humans are increasingly focused on adapting to their changing preferences in order to improve assistance in real-world tasks. Effective agents must learn to accurately infer human goals, which are often hidden, to collaborate well. However, existing Multi-Agent Reinforcement Learning (MARL) environments lack the necessary attributes required to rigorously evaluate these agents' learning capabilities. To this end, we introduce COLORGRID, a novel MARL environment with customizable non-stationarity, asymmetry, and reward structure. We investigate the performance of Independent Proximal Policy Optimization (IPPO), a state-of-the-art (SOTA) MARL algorithm, in COLORGRID and find through extensive ablations that, particularly with simultaneous non-stationary and asymmetric goals between a \"leader\u201d agent representing a human and a \u201cfollower\" assistant agent, COLORGRID is unsolved by IPPO. To support benchmarking future MARL algorithms, we release our environment code, model checkpoints, and trajectory visualizations at https://github.com/andreyrisukhin/ColorGrid.", "sections": [{"title": "1 Introduction", "content": "The integration of autonomous assistants into real-world tasks with humans necessitates robust guidance and correction. Nascent work is examining how to train autonomous agents to collaborate with humans of diverse skill levels, yet these studies always assumed a fixed objective. In the real world, human goals change \u2013 consider a surgeon encountering unexpected complications during a surgery. A robotic surgery assistant must seamlessly follow suit, adapting on-the-fly. As such, we contend that training collaborative agents to adaptively help humans in real-world tasks requires adapting to the human's hidden and changing goals. We develop a benchmark that tests this aspect of human-AI coordination, creating a test-bed that enables focusing on the core problems of inferring goals in real-time. Agents that can make progress on this benchmark take a step toward helping humans in the real-world.\nMany prior works explore zero-shot coordination with other agents and with humans [5, 31, 13, 21], but their goals are typically modeled as static throughout a given episode. Other works study social agents and their collaboration when augmented with message passing [35, 28], but in reality we seek autonomous agents that do not require explicit messages to seamlessly and quickly transition between goals when assisting humans with arbitrary tasks. In settings such as elder care, it is unrealistic to expect humans to describe new goals in detail to the assistant, since the tasks may be time sensitive and require immediate attention.\nIn competitive Multi-Agent Reinforcement Learning (MARL) domains, self-play [29] has been shown to reach human-level play without human data using model-free reinforcement learning."}, {"title": "2 Related Work", "content": "Zero-shot coordination with humans & Ad-hoc Team-play Multiple approaches exist for zero-shot coordination with humans and ad-hoc team-play. To encourage strategic diversity while still averting the need to collect human data, fictitious co-play [31] trains agents against a population of self-play agents and prior training checkpoints in the game Overcooked [3]. Several works in robotics have also modeled shifting human goals to improve robot-human cooperation. For instance, Jeon et al. [13] develop a shared autonomy method for precise robotic manipulation, reinterpreting user inputs in a latent space based on a confidence-conditioned prediction of the human's goal. In the setting of human-robot cooperation where the two agents may prefer different strategies to achieve a given task, Nikolaidis et al. [21] model the human's adaptability as a latent variable in a POMDP (Partially Observable Markov Decision Process) [30] and adjust the robot's weighting between its and the human's preferred strategies to maximize the team's rewards depending on the human's capability to switch modes. Shared autonomy, a robot estimating operator intent based on operator inputs, can benefit from modeling those intents as Bayesian [12]. Moreover, prior work such as Yell At Your Robot [27] explore real-time corrections of low-level robot actions through natural language. While"}, {"title": "3 Method", "content": null}, {"title": "3.1 Environment", "content": "We introduce COLORGRID, a novel non-stationary environment to evaluate multi-agent reinforcement learning (MARL) without explicit message passing. This environment features complex agent learning interactions and state dynamics and is implemented using PettingZoo [32], a platform that supports creating environments for MARL scenarios."}, {"title": "Customizability", "content": "Users of COLORGRID can configure the environment size, the reward and penalty ascribed by the goal and incorrect blocks, the probability of the goal block changing, whether the goal is asymmetric, the density of blocks filling the grid, and the reward shaping functions applied. The latter two affect reward sparsity, which defaults to filling 10% of the grid with three colors uniformly at random. The is also customizable. Providing options to toggle non-stationarity and asymmetry make COLORGRID a valuable resource for comparing RL algorithms between multiple levels of environment complexity."}, {"title": "Asymmetry", "content": "In the multi-agent setting of COLORGRID, we define one agent to be the \"leader\" and the other to be the \"follower\" to simulate the scenario where a robot follower must assist a human leader. The leader is always informed which block color is the goal, but this is hidden from the follower when the asymmetric mode is toggled. Adding asymmetry makes this environment significantly more challenging, as the follower must learn to infer the leader's goal from the leader's trajectory only. We also highlight that hidden goals play a crucial role in determining the effectiveness of agents using social learning, i.e. learning through observing the actions of other agents in a shared environment [16, 8]. Without considering hidden goals, agents may struggle to achieve certain objectives through individual exploration or adapt effectively to new environments."}, {"title": "State Representation", "content": "We represent states in a grid representation format, with 5 channels of 32\u00d732 matrices. Each channel is a binary mask for every cell on the 32\u00d732 board, representing whether it contains the color or agent (3 block position and 2 agent position masks result in 5 channels). Additionally, the goal color is provided as a one-hot vector where each index represents a color present in COLORGRID."}, {"title": "Cost of Exploration", "content": "We consider three cases of COLORGRID with an expected reward that is either positive, zero, or negative (in expectation, varying the cost of exploration). See A.1 for the specific reward values."}, {"title": "3.2 Agent Implementation", "content": "We implement our agents as Actor-Critic neural networks [7], with a shared portion that computes features using convolutional and linear layers before feeding into the separate policy and value networks. We use stride size of 1 for all convolutions, Leaky ReLU [18] as the activation function following all convolution layers, and TanH [17] following all linear layers. We reference Ndousse et al. [20] for our initial architecture implementation, modifying it to use a stride of 1 for all convolutions (since our state representation is symbolic rather than pixel-based), concatenate a vector to signal the goal block, and add an auxiliary objective network as described in 3.2.1."}, {"title": "3.2.1 Model Architecture", "content": "Shared Convolutional Network:\n\u2022 Conv2d Layers\n Input channels: 5 \u2192 32 \u2192 64\n Kernel size: 3 for all layers\nShared Projection and Feature Network:"}, {"title": "Linear Layers", "content": "Input: 43264 + 3 \u2192 192 \u2192 192 \u2192 192\n The first layer adds a one-hot vector of goal information (+3 to input dimension). For asymmetric followers, zeros are concatenated instead.\n Alternative architecture: Goal information is concatenated at the input of the second layer (see Appendix A.5).\n\u2022 LSTM (if asymmetric)\n Input: 192 \u2192 192"}, {"title": "Value Network:", "content": "Linear Layers\n Input: 192 \u2192 64 \u2192 64 \u2192 1"}, {"title": "Policy Network:", "content": "Linear Layers\n Input: 192 \u2192 64 \u2192 64 \u2192 4"}, {"title": "Auxiliary Network (Goal Color Prediction):", "content": "Linear Layers\n Input: 192 \u2192 64\u21923\nWe now discuss two architecture factors: the auxiliary supervised loss and goal color concatenation."}, {"title": "Auxiliary Supervised Loss", "content": "We add an auxiliary supervised prediction task, in which a two-layer MLP predicts the goal block color from an input of a feature representation output by the shared network. Auxiliary model-based loss objectives have been extensively studied in the RL literature, and in line with previous works we find that this component improves training performance and stability [20, 14, 34, 26, 10, 15, 9]. Unless specified otherwise, we use a default coefficient of \u043a = 0.2 to add the supervised cross-entropy loss to the main PPO loss. With this additional objective, the full loss is described by the below equation, where $c_i = 1$ if the i'th color is the current goal, or 0 otherwise.\n$L = L_{PPO} - \\kappa \\sum_{i=1}^{3} c_i \\cdot log c_i$  (1)"}, {"title": "Goal Color Concatenation", "content": "Specifically, in the forward pass, we append an one-hot vector describing the current color of the goal block. This vector is concatenated either \u201cEarly\u201d, immediately after flattening the output from the convolutional network, or \u201cLate\", after projecting the flattened output into a lower-dimensional space.\nAs our goal is to produce agents which can cooperate effectively, the performance metric is the sum of rewards achieved by each agent. We train agents using Independent Proximal-Policy Optimization (IPPO) [25] to ensure that the individual rewards observed by each agent are correlated with its own actions. For this reason, we don't use Multi-Agent PPO (MAPPO) because the value and policy networks between the leader and follower can't be shared.\""}, {"title": "4 Experiments", "content": "As this work is exploratory in nature, we focus on completing a breadth of experiments with the resources available rather than running fewer experiments with many seeds. While COLORGRID supports an arbitrary number of leader and follower agents, we focus on the 1:1 scenario emulating the simplest human-agent collaboration case. For all experiments, we use the default block density of 10% and goal switch probability of 2% unless otherwise mentioned. We also fix a set of standard hyperparameters for the IPPO algorithm, shown in A.2, maintaining a constant learning rate rather than incorporating a schedule."}, {"title": "4.1 Baselines", "content": "We use the symmetric setting as a baseline environment to the asymmetric case, which is the main subject of our investigation, and have a baseline leader and follower agent that use the A* search algorithm, where the cost of a path is the length of the shortest path to a goal block (ties are broken arbitrarily). The follower also uses a greedy A* policy, but only updates its current goal with the true reward color when the leader collects a block."}, {"title": "4.2 Findings", "content": "Changing the cost of exploration causes poor agent behavior except in neutral mode. Figure 3 shows the effect of training an IPPO follower in asymmetric environments with a positive, zero, and negative expected cost of exploration, learning with a frozen leader policy trained with the best hyperparameters from Table 2. A frozen leader agent operates with a fixed, pre-trained policy and probabilistically switches the goal it seeks rather than with a predictable schedule. When the follower is trained with a fixed leader policy, COLORGRID retains multi-agent dynamics because the follower must adapt to the leader agent's actions.\nWe train the follower without an auxiliary objective or penalty annealing to purely explore the effect of varying the reward structure. Upon visualizing the environments, we observe the follower agent behavior:\nThis is an admissible heuristic, so this implementation of A* is guaranteed to perform optimally.\nThe follower's first actions of an episode are to move to any neighboring empty space until the leader collects a block.\nIt would be interesting to consider the maximum performance which would be an oracle agent that, in addition to knowing the shortest path to blocks, would have knowledge on where, when, and what color new blocks would spawn, and effectively be able to utilize this information."}, {"title": null, "content": "1. In an environment with a positive expected value of exploration, the follower learns to pick up all blocks, whether they are the color of the goal or not.\n2. In an environment with a negative expected value of exploration, the follower learns to avoid picking up any blocks, and does not distinguish which are the goal.\n3. Only in an environment with an expected cost of exploration of zero does the follower learn to pick up the correct block color often, and less frequently collecting incorrect blocks.\nNote that in these experiments, the goal block color switches randomly, implying that the follower in the neutral environment occasionally learned to correctly observe the leader's behavior to inform which goal to collect. We hypothesize the pessimistic follower avoids collecting any blocks due to a sparsity of reward: it is rare that the leader collects the goal block, and the follower randomly chooses the goal block shortly after. To maximize reward in an environment where uninformed block pickup occurs, the follower avoids picking up blocks and incurring the penalty. On the contrary, the optimistic follower collects every block in its path. We infer the follower learns a local optimum that collecting blocks uniformly at random yields positive reward, and thus never learns the global optimum of only collecting the goal block.\nEven in a neutral environment, which we find to be the best environment for learning a follower agent using IPPO, the agent still collects incorrect blocks. In a real-world scenario with high consequences for mistaken actions, this would be unacceptable for a human helper. Though the Negative EV follower never collects incorrect blocks, it also never collects goal blocks, which is harmless but not helpful. We believe the real world is a pessimistic (Negative EV) environment if pursuing random goals, where many actions exist that cause harm, and only a few are helpful. This, combined with IPPO learning poorly in a pessimistic environment, motivates our study of how to get a follower to learn in a pessimistic environment for the rest of the paper. We observe that the frozen leader score is approximately on-par with the baseline A* leader score, showing that IPPO learns a policy close to classical heuristic-based search algorithms but is capable of learning more as it's inherently \"blind\" like a greedy algorithm. The A* copying follower performance, on the other hand, significantly outperforms all follower agents trained with IPPO, demonstrating that even with an expected cost of exploration of zero, IPPO is insufficient to learn to infer the leader's goal."}, {"title": "5 Conclusion", "content": "We present COLORGRID, a novel multi-agent environment with changing and hidden goals. We find that it is a challenging MARL environment that SOTA IPPO cannot solve out-of-the-box, and analyze several architectural, environmental, and algorithmic factors that make the environment easier or harder for agents. Notably, we see that the cost of exploration directly affects an agent's ability to learn while collaborating. By making a simplified gridworld environment to focus on studying the keys aspects of this problem from a cooperation perspective and while abstracting away details like image understanding or low-level motor control, COLORGRID is another step towards human-AI cooperation and social learning. We hope this research helps spur other works to gain more insights into how costlier individual exploration raises the incentive to rely on social learning and enables the development of AI agents that can adaptively help humans without explicit feedback in real-time and realistic settings where goals are constantly changing. This can impact a wide range of fields and industries including robotic surgery and household assistance. Specifically, a robotic surgery assistant can better respond to patient-specific comfort levels, which could lead to improved patient outcomes, or a robot could help another human set the table while not getting in the way."}, {"title": "5.1 Limitations and Future Work", "content": "Apart from running more experiments with different seeds and determining optimal hyperparameter configurations, avenues for future work broadly fall into two categories: environmental and algorithmic design. These support developing multi-agent systems that collaborate more effectively with humans and other agents in zero-shot settings."}, {"title": "5.1.1 Environmental Developments", "content": "There are various environment developments that could further evaluate agents' capabilities to adapt more robust strategies by introducing uncertainty. To examine the effects of goal rarity, non-uniform block distributions and stochastic rewards tied to block color sampled from distributions with varying means and variances, could be implemented. An alternative approach to challenge agents could involve structured mazes or hemispherical designs where blocks collected in one hemisphere respawn in the other. Adopting curriculum-based approaches, like POET [33], could facilitate progressive learning by generating adversarial environments that push agents' abilities. Lastly, real-world scenarios often involve humans outnumbering autonomous agents, requiring followers to prioritize which leaders to assist. Future studies could explore settings where followers assist multiple leaders, each with different goals, or scenarios where a follower faces conflicting objectives from two leaders. These niche but critical challenges are essential for developing robust multi-agent coordination."}, {"title": "5.1.2 Algorithmic Developments", "content": "Advancing algorithm developments beyond IPPO is critical to address the challenges presented by COLORGRID's complex multi-agent scenarios. One potential improvement is warm-starting the follower agent by behavior cloning the A* copying baseline. Another method that can likely surpass behavior cloning is leveraging diverse policy data collected from baseline A* agents for Implicit Q-Learning. An extension of IQL specifically is using a COMA-inspired objective [6], such as $Q(a_F, S) = Q(a_{H},a_{K}, s) \u2013 Q(a_{H}, s)$, which could help the follower isolate its unique contribution to the reward. Online inverse reinforcement learning (IRL) represents another promising direction for goal inference. Techniques like BASIS [1] could enable the follower to infer the leader's current goal explicitly by learning a model of the leader's reward function. This approach could complement or outperform auxiliary supervised objectives by embedding goal inference directly into the agent's policy."}, {"title": "A Appendix", "content": null}, {"title": "A.1 Cost of Exploration Reward Values", "content": "Below are the reward values for the three cases of COLORGRID that are differentiated by having a positive (optimistic), zero (neutral), or negative (pessimistic) expected value for collecting a random block, respectively."}, {"title": "A.2 PPO Parameters", "content": null}, {"title": "A.3 Goal Block Switch Probability", "content": "The default block switching probability parameter is $\\frac{1}{24} \\approx 2.08\\%$. The $\\frac{1}{24}$ is motivated by the $\\frac{1}{3}$ chance to randomly select the same color, while the $\\frac{1}{2}$ gives the leader enough time steps to collect a new goal block color before switching again (in expectation, it takes 32 environment steps before the goal block switches, and 32 steps is enough to cross half the default 32x32 environment)."}, {"title": "A.4 Warmstarting Experiment Parameter Details", "content": "For warmstarting, we apply the distance penalty reward shaping term for a distance threshold of 10, penalty 0.25, for 20M timesteps, and penalty annealing from 10M to 20M timestep. We also try penalty 0.5 for distance threshold 10 for 40M time steps and penalty annealing for 4M to 10M."}, {"title": "A.5 Additional Ablation: Early vs. Late Goal Concatenation", "content": "Concatenating goal information earlier in the forward pass is insignificant. We find that, in an environment with 10% block density, concatenating the one-hot goal information vector immediately after flattening the output from the convolutional network doesn't have a substantially higher shared reward than concatenating the vector after projecting the flattened output into a lower-dimensional space. We hypothesize that the difference is negligible because either way, there are enough layers after appending to appropriately encode the goal information into the feature representation."}, {"title": "A.6 Hardware Details", "content": "COLORGRID takes approximately 2 seconds per 1,000 time-steps on a A40 or L40 GPU. With some additional code improvements, this runtime can be optimized much further."}]}