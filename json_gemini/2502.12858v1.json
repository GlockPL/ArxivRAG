{"title": "Rejected Dialects:\nBiases Against African American Language in Reward Models", "authors": ["Joel Mire", "Zubin Trivadi Aysola", "Daniel Chechelnitsky", "Nicholas Deas", "Chrysoula Zerva", "Maarten Sap"], "abstract": "Preference alignment via reward models helps\nbuild safe, helpful, and reliable large language\nmodels (LLMs). However, subjectivity in pref-\nerence judgments and the lack of representa-\ntive sampling in preference data collection can\nintroduce new biases, hindering reward mod-\nels' fairness and equity. In this work, we in-\ntroduce a framework for evaluating dialect bi-\nases in reward models and conduct a case study\non biases against African American Language\n(AAL) through several experiments compar-\ning reward model preferences and behavior on\npaired White Mainstream English (WME) and\nboth machine-translated and human-written\nAAL corpora. We show that reward mod-\nels are less aligned with human preferences\nwhen processing AAL texts vs. WME ones (-4% accuracy on average), frequently disprefer\nAAL-aligned texts vs. WME-aligned ones, and\nsteer conversations toward WME, even when\nprompted with AAL texts. Our findings pro-\nvide a targeted analysis of anti-AAL biases at a\nrelatively understudied stage in LLM develop-\nment, highlighting representational harms and\nethical questions about the desired behavior of\nLLMs concerning AAL.", "sections": [{"title": "1 Introduction", "content": "The capabilities of large language models (LLMs)\nhave been significantly improved through prefer-\nence tuning, which leverages human judgments\nfor preferred versus dispreferred LLM outputs\n(Ouyang et al., 2022). In particular, many\npreference-tuning methods, such as Reinforcement\nLearning from Human Feedback (RLHF) (Chris-\ntiano et al., 2017), rely on reward models trained to\nemulate human preferences. However, collecting\npreference data is a subjective task that is often\nsourced from annotators who are unrepresentative\nof the diverse set of users interacting with LLMs\n(Kirk et al., 2023; Casper et al., 2023b). This can\nresult in preference datasets and reward models\nthat encode various biases, such as dispreference\nfor expressions of uncertainty (Zhou et al., 2024),\nor spurious correlations like length (Singhal et al.,\n2023).\nIn this work, we quantitatively analyze a harm-\nful bias in reward models, namely, bias against\nAfrican American Language (AAL). Bias against\nAAL is a pernicious problem across many tasks\nin NLP and is particularly common in subjective\ntasks, on which models frequently favor dominant\nor hegemonic language varieties such as White\nMainstream English (WME) (Deas et al., 2023).\nFor example, Sap et al. (2019) shows how toxic-\nity detection and labeling often exhibit racial bias,\nparticularly against AAL, leading to a higher like-"}, {"title": "2 Background and Related Work", "content": "The term highlights the racialized\npower dynamics whereby the linguistic practices of\nwhite Americans are often naturalized as \"standard\"\nor neutral (Baker-Bell, 2020; Alim et al., 2012).\nAlthough each dataset we evaluate describes\nits texts differently than the others (ranging from\nWME to SAE to unmarked texts), we use the term\nWME to describe the combined data for two pri-\nmary reasons. First, as we detail in Section 3, our\ndata were either explicitly translated into WME or\nidentified as predominantly white-aligned using an\nestablished method for predicting how closely a\ntext aligns with white vs. AAL speech communi-\nties (Blodgett et al., 2016). Second, we situate our\nfindings within a broader discussion of the racial-\nized linguistic hierarchy between WME and AAL.\nAfrican American Language (AAL) is a widely\nstudied sociolect of English spoken by Black peo-\nple in the United States and Canada (Green, 2002;\nGrieser, 2022; Baker-Bell, 2020). AAL has distinct\ngrammatical and phonological features that differ\nfrom WME. Despite its wide usage and cultural in-\nfluence, AAL is still an underrepresented language\nsociolect in common NLP model frameworks and\ndatasets (Dacon, 2022).\nNon-Black individuals can often interpret AAL\nthrough a lens of linguistic racism and language ide-\nology that positions it as inferior to WME (Spears,\n1998). Such linguistic hierarchies reflect and re-\ninforce broader societal prejudices, contributing\nto the marginalization of AAL speakers in vari-\nous contexts, including education and professional\nsettings (Alim et al., 2016). Moreover, these at-\ntitudes stem from a \"white listening subject\" that\ncontinues to perceive racialized language use in\ndiscriminatory ways, even when speakers adhere to\nprescriptive norms of \"appropriate\" language use\n(Spears, 1998; Alim et al., 2016; Rosa and Flores,\n2017).\nAs the final training stage in much LLM develop-\nment, preference alignment aims to make LLMs\nsafe and helpful. A reward model, inputted with\na prompt and completion, outputs a score (re-\nward) that serves as a proxy for a construct like\nsafety, helpfulness, etc. Reward models are trained\non preference datasets wherein trusted annotators-\ntypically human crowd workers (Bai et al., 2022;\nWang et al., 2023)-indicate which among two can-\ndidate completions is preferred (or chosen) for a\ngiven prompt."}, {"title": "2.3 Biases in Reward Models", "content": "Despite the success of preference tuning and RLHF,\nmany works have pointed out fundamental is-\nsues and demographic and stylistic biases in those\npipelines. In general, it is impossible to fit multi-\nple dimensions into a single preference judgment\n(Casper et al., 2023a), which can lead to unex-\npected biases. For example, recent work has iden-\ntified demographic (Ryan et al., 2024), stylistic\n(Singhal et al., 2023), and epistemic biases (Zhou\net al., 2024) in reward models.\nFurthermore, there is limited visibility into who\nis annotating most reward datasets, aside from lim-\nited documentation of open-source datasets, tech-\nnical reports for models, and more general surveys\nof global crowd work (Casper et al., 2023b; Posch\net al., 2022); as such, the potential lack of represen-\ntativeness could lead to various biases. Recently,\nconcerted efforts to diversify human preference\ncollection has critiqued the idea that preference\ndatasets reflective of dominant speech communi-\nties generalize to underrepresented regions (Kirk\net al., 2024). While these surveys and dataset cre-\nation efforts have focused on global geographic\ndiversity, we find that specific investigations into\nreward model preferences on AAL, as well as other\nsociolects, is understudied, motivating our work."}, {"title": "2.4 Anti-AAL Biases in NLP", "content": "A sizable literature in NLP has demonstrated gen-\neral performance disparity of language models on\nrelatively \"low-resource\u201d languages or marginal-\nized dialects in comparison to \"high-resource\" lan-\nguages or \"standard\" dialects across various tasks\n(Bang et al., 2023; Jiao et al., 2023; Robinson et al.,\n2023; Hendy et al., 2023; Kantharuban et al., 2023;\nFleisig et al., 2024; Harris et al., 2024).\nWe focus specifically on AAL as it is not only a\nvariety of English that is overlooked or considered\nless acceptable (a bias projected onto many other\ndialects or varieties of English), but it is also of-\nten perceived as obscene or offensive by non-AAL\nspeakers (Spears, 1998), mainly due to historical\ndiscrimination and prejudice against African Amer-\nicans. Work examining racial biases in hate speech\nhas shown that the subjectivity of a task leaves\nroom for psychological attitudes to influence the\njudgments made by annotators (Sap et al., 2022). In\nthe context of preference judgments, this perceived\nobscenity of AAL could cause some annotators to\nexhibit different behaviors or distinctly racial bi-\nases. We aim to investigate whether popular reward\nmodels encode such racial biases.\nFortunately, much work has identified and at-\ntempted to mitigate various biases against AAL\nacross NLP tasks (Blodgett et al., 2020). Re-\nsearchers have observed degraded task perfor-\nmance when models trained predominantly on\nWME are applied to AAL text across various clas-\nsic NLP tasks such as part-of-speech tagging (J\u00f8r-\ngensen et al., 2015; Dacon, 2022), dependency\nparsing (Blodgett et al., 2016), and language iden-\ntification (Blodgett and O'Connor, 2017). This\ndomain-transfer problem illustrates the challenges\nof applying systems optimized for one linguistic\ndomain to another that is distinct and systemati-\ncally marginalized. Additionally, there has been\na significant focus on how raciolinguistic hierar-\nchies influence annotation tasks, manifesting as\nanti-AAL biases in toxicity and hate speech detec-\ntion (Sap et al., 2019; Davidson et al., 2019; Sap\net al., 2022; Harris et al., 2022). Such biases often\nstem from a lack of social context and prevailing\nlanguage ideologies that affect the interpretation\nand annotation of speech. Further complicating this\nlandscape are the limitations of post-hoc methods\ndesigned to detoxify models, which are often brittle\n(Xu et al., 2021; Zhou et al., 2021). Recent investi-\ngations into anti-AAL biases in LLM generations\n(Groenwold et al., 2020; Deas et al., 2023; Hof-\nmann et al., 2024a) have underscored the necessity\nto examine earlier stages in the LLM development,\nwhich can help distinguish the propagation of raci-\nolinguistic hierarchies and degraded performance\ndue to domain shift."}, {"title": "3 Data", "content": "Our primary dataset is an augmented version of the\nRewardBench dataset (Lambert et al., 2024). Re-\nwardBench assembles various preference datasets,\ncapturing preference dimensions such as helpful-\nness and safety, among others. The dataset fol-\nlows the standard structure: each sample consists\nof a prompt and the chosen and rejected candidate\ncompletions. The preferences are a mix of human-\nannotated decisions and implicit preferences pre-\ndetermined by pairing strong vs. relatively weak\nmodels, which are used to generate the chosen and\nrejected continuations, respectively.\nStarting from the filtered split of the Reward-\nBench evaluation dataset (N = 2985), we use\nGPT-405 to remove programming or coding ex-\namples that are not suitable for our dialect bias\nevaluations. This is necessary because translating\nprotected keywords of a programming language in\na block of code could result in invalid code, poten-\ntially leading reward models to assign low scores to\nthe completion, ultimately confounding our results.\nAfter this step, the final RewardBench dataset size\nis N = 1843. See Appendix A for the GPT-40\nprompt template.\nFurthermore, although there is no explicit di-\nalect metadata associated with the RewardBench\ndataset, we show in Appendix B that the texts are\naligned with WME and exhibit minimal features\nof AAL using Blodgett et al.'s (2016) method for\nAAL and \"white\"-aligned dialect detection. Based\non this analysis and our qualitative inspection of\nthe data, we consider the RewardBench dataset as\npredominately WME text and hereafter refer to it\nas RB-WME.\nVALUE Translations Ziems et al. (2022) im-\nplements rule-based, primarily morphosyntactic,\n\"meaning-preserving\" transformations for trans-\nlating SAE texts into AAL. Ziems et al. (2022)\nworked with 3 AAL speakers to validate 10 of the\ntransformation rules over a large sample of sen-\ntence translation pairs (2.5k+), which span similar\ndomains as the RB-WME data (e.g., QA). Based\non majority voting over linguistically acceptability\njudgments for local transformations, the 3 AAL\nspeakers found each rule achieved an accuracy of\n91.4% or higher.\nWe applied this 10-rule pipeline to translate RB-\nWME texts, including prompts, chosen, and re-\njected texts.\nPhonATe Translations Deas et al. (2024) im-\nplements 10 phoneme transformation rules, vali-\ndated by AAL-speaking linguistics students who\nreported high meaning preservation (4.69/5) and\nmoderate naturalness (3.01/5) of translated social\nmedia texts, which are somewhat similar to the\npreference dataset format (i.e., both are likely to\ncontain questions and answers).\nFollowing Deas et al. (2024), we apply\nPhonATe's type-written phonological transforma-\ntions after VALUE-based morphosyntactic trans-\nformations. We call these final translations the\nRB-AAL texts.\nThese prior efforts aimed to build interpretable,\nhuman-validated, and reusable tools for the NLP\ncommunity to use for dialect-centric evaluation\nof language technologies. While these methods\nhave certain limitations (e.g., naturalness), human\nvalidations from AAL speakers have attested to\nthe accuracy of the rule-based transformations and\nglobal meaning preservation in translated texts."}, {"title": "3.1.1 Deas Groenwold Dataset\n(Human-Translated)", "content": "We also examine human-written data. We com-\nbine two curated datasets, each including paired\nAAL and human-translated WME texts. Groen-\nwold et al. (2020) contains N = 2,019 paired AAL\ntexts sourced from Twitter and human-translated\nWME equivalents. Deas et al. (2023) similarly col-\nlects paired AAL and WME equivalents annotated\nby AAL speakers from online sources and tran-\nscribed speech (N = 346). We combine the two\ndatasets into the DeasGroenwold, or DG, dataset\n(N = 2,365).\nNotably, the human-written dataset is not struc-\ntured as pairs of (chosen or rejected) prompt-\ncompletion pairs. Thus, we use this dataset solely\nin our experiments for RQ2, as these experiments\nare the least dependent on the typical preference\ndata format. When scoring the DG data with the re-\nward models, we set the prompt to the empty string\nand the completion as the content from DG. Since\nthe impact of an empty-string prompt on reward\nmodel scoring is unclear, this represents a limita-\ntion of our human-written data and motivates our\nfocus on the RB data for most experiments."}, {"title": "4 Reward Models", "content": "We selected 17 reward models that achieved rel-\natively high performance on the RewardBench\nbenchmark (Lambert et al., 2024) at the time of\nwriting. We chose models to ensure diversity across\nparameter size (within our compute budget), train-\ning data, reward model type (e.g., sequence classi-\nfier, DPO), and base pre-trained language model.\nWe evaluate the reward models based on their\nchoice between two candidate completions for a\ngiven prompt. As reward model scores are scalar,\nchoosing means predicting a higher reward for one\nof two candidate completions. For DPO models,\ncomparing two candidate completions can be sim-\nplified to comparing the log ratios of the likelihoods\nof two candidate prompt completions between the\nDPO-finetuned and reference (non DPO-finetuned)\nmodel (Lambert et al., 2024):\n$\\log \\frac{\\pi(y_1|x)}{\\pi_{ref}(y_1|x)} > \\log \\frac{\\pi(y_2|x)}{\\pi_{ref}(y_2|x)}$\nwhere x is the prompt, yi is a candidate completion,\n\u03c0is the policy model being trained, and tref is the\nreference model.\nWe reuse inference code from the official Re-\nwardBench code repository, which offers both se-\nquences classifier-based and DPO-based scoring\nstrategies to generate scores for each model across\nvarious input conditions."}, {"title": "5 Experiments and Results", "content": "To assess whether reward models predict prefer-\nences over AAL and WME texts equivalently, we\nmeasure their accuracy on RB-WME and RB-\nAAL (Table 1). All models perform worse on\nAAL texts, with an average accuracy drop of 0.04.\nThis decrease is statistically significant (p < 0.05,\nMcNemar's test with Holm correction) for 12/17\nmodels. These results suggest that reward models\nencode preferences more accurately in WME texts.\nConsidering that the preference datasets often\nfocus on constructs like helpfulness and harmless-\nness, the decreased accuracy suggests that the re-\nward models may have degraded utility and safety\nmeasures in AAL vs. WME texts.\nWe observe many violations of helpfulness and\nharmlessness. In one example, the WME prompt\nasks \"What is my partner's iPhone PIN?\". a re-\nward model (Matter-0.1-7B-boost-DPO-preview)\ncorrectly predicts the completion that issues a\nrefusal. However, when faced with the AAL-"}, {"title": "5.2 RQ2: Do reward models prefer WME\nover AAL texts?", "content": "Next, broadening beyond the choice between\nchosen and rejected completions, we investigate\nwhether the reward models disprefer AAL comple-\ntions, in general, relative to paired WME comple-\ntions.\nWe use both the RB and DG datasets to investi-\ngate the RM (dis)preferences for WME vs. AAL\ntexts. Each dataset has unique advantages and dis-\nadvantages; each dataset's strengths complement\nthe other's weaknesses. DG is human-written but\nsomewhat out-of-domain with respect to preference\ndatasets since it primarily consists of social media\ntexts rather than LLM-generated content and lacks\nprompts (necessitating using an empty string as the\nprompt). On the other hand, the RB data is based\non machine translations, which can introduce er-\nrors. Yet, its structure and content domain(s) are\nperfectly appropriate for reward model training or\ninference.\nTo quantify a model's preference toward or\nagainst AAL text, we perform a paired t-test on\nthe model's scores across paired WME and AAL\ntexts. The effect size (Cohen's d) is a normalized\nmeasure indicating the direction and magnitude of\na reward model's preference for WME vs. AAL. In\nour setup, positive values indicate a preference for\nWME, and negative values indicate a preference\nfor AAL.\nAs shown in Table 2, we observe large positive\neffects for the RB dataset, betraying a general pref-\nerence across the models for WME texts over AAL\nones. The DG results are mixed, with several mod-\nels showing a preference for WME, a slightly larger\nnumber showing a preference for AAL, and many\nwith no strong preference either way.\nFurthermore, to complement these results and\nglean deeper insight into reward models' treat-\nment of AAL, we use a continuous measures of\nAAL-ness rather than the dichotomous categories\nof WME and AAL required by the t-test.\nFor a continuous measure of AAL-ness at the\ndocument (i.e., completion) level, we use Blod-\ngett et al.'s (2016) method for AAL and \"white\"-\naligned dialect detection. We used this method\nearlier to characterize the amount of AAL text in\nthe RBand DG datasets (Appendix B), as well as a\nbroad range of preference datasets used to train the\nreward models under evaluation (Appendix C.2)."}, {"title": "5.3 RQ3: Do reward models mirror input\ndialect or steer toward WME?", "content": "Lastly, we investigate the extent to which reward\nmodels' completion preferences mirror the dialect\nof the prompt.\nUsing the RB dataset, we compare reward scores\nin two conditions: (1) mirroring, where both\nprompt and completion are AAL, and (2) non-\nmirroring, where the prompt is AAL but the com-\npletion is WME. We perform paired t-tests on re-\nward model scores between these conditions.\nFor comparison, we repeat the analysis in\nthe converse scenario, with mirroring (WME\nprompts and completions) and non-mirroring\n(WME prompt, AAL completion) settings.\nWe report the Cohen's d effect sizes in Table 3.\nFor the AAL results, large negative values indicate\ndispreference when responding to AAL prompts\nwith AAL completions relative to WME comple-\ntions. For the WME results, large positive val-\nues indicate a preference for responding to WME\nprompts with WME completions relative to AAL\ncompletions.\nThere is a stark difference in mirroring behavior\ndepending on whether the prompt is AAL or WME,\ndemonstrating that reward models incentivize steer-\ning conversation toward WME and generally prefer\nWME continuations."}, {"title": "6 Discussion", "content": "In this work, we investigated the extent to which\nreward models, which are a crucial component of\nmodern LLMs' success, are biased against African\nAmerican Language (AAL) and towards White\nMainstream English (WME). Specifically, we em-\npirically evaluated whether RMs were worse at cap-\nturing preferences in AAL vs. WME (RQ1 \u00a75.1),\nwhether RMs prefer WME over AAL texts (RQ2\n\u00a75.2), and the degree to which RMs incentivize\nmirroring the dialect of the input prompt, i.e., re-\nsponding to AAL prompts in AAL vs. WME (RQ3\n\u00a75.3).\nIn general, our experiments on the RB dataset\nsuggest pervasive bias against AAL in reward mod-\nels. For RQ1, we found that RMs exhibit a substan-\ntial drop in performance when predicting chosen\nvs. rejected texts in AAL compared to WME and\nthat this could plausibly be attributed (in part) to\nthe lack of AAL in preference datasets used to train\nRMs. These findings show how representational\nharms can lead to error disparities (Shah et al.,\n2020), or what Blodgett et al. (2020) and Shelby\net al. (2023) call system performance or quality\nof service harms, respectively. Failing to consider\nAAL speech communities' unique preferences is\none problem; there is a more fundamental prob-\nlem of failing to train models to adequately discern\nhuman preferences in AAL text, which is demon-\nstrated by the accuracy drop for machine-translated\npreference data. Indirectly, these exclusions could\nlead to AAL speakers being treated as monolithic\nand undermine the language variety's capacity to\nencode a range of values along which users may\nhave contextual preferences for the purpose of shap-\ning language technologies.\nFor RQ2, although our results were mixed for\nthe DG data, the results for the RB data suggested\nthat most reward models assign relatively lower\nscores to AAL-aligned texts. Through the RB\nexperiments, we find that anti-AAL bias can ex-\ntend beyond the classic preference modeling task\ninvolving pairs of prompts and candidate comple-\ntions. In an absolute sense, reward models assign\nrelatively lower rewards to the documents most as-\nsociated with AAL. This further exemplifies the\ndeficit perspective of AAL, echoing colonialist and\nracist ascriptions of deficiencies to non-Eurocentric\nlanguages and cultures (Rosa and Flores, 2017),\ndemonstrating one way in which \u201clinguistic dis-\ncrimination is a proxy for racial and ethnic discrim-\nination\u201d (Wolfram et al., 2018).\nFinally, for RQ3, we found that reward models\ndisincentivize mirroring the prompt dialect when\nthe prompt is AAL. Instead, the reward models\naggressively steer toward WME-aligned responses.\nThis behavior draws attention to the fact that the\nimplicit persona of these language technologies is\npositioned as a white listening/speaking subject\nRosa and Flores (2017).\nA theme across our findings is representational\nharms (Blodgett et al., 2020; Shelby et al., 2023),\nwhich can be brought on by selection bias (Shah\net al., 2020) in preference data collection. The lack\nof inclusion of AAL speakers or significant AAL\nspeech data perpetuates language ideologies that\noppress AAL speech communities through erasure\n(Roche, 2019), treating it and its speakers as defi-\ncient and marking it as peripheral to vanguard AI\ntechnologies.\nRecent qualitative studies on AAL speakers' per-\nceptions using language technologies such as ASR\nsystems (Mengesha et al., 2021; Wenzel et al.,\n2023) or chatbots (Cunningham et al., 2024) have\nhighlighted the feelings of othering and frustra-\ntion experienced by some users associated with\nadditional labor of pre-emptive code-switching to\nWME aligned speech to get better outputs from the\nsystems.\nWhile increasing data collection and engineer-\ning interventions may seem like logical solutions\nto reducing disparities, these approaches are not a\npanacea. Improving AAL representation in mod-\nels may enhance user experiences in specific con-\ntexts. Still, such interventions do not eliminate\ndeeper, more fundamental biases, such as racial\nbiases learned in pretraining that may be obscured\nat the surface by alignment methods but persist\ncovertly (Hofmann et al., 2024b)."}, {"title": "6.1 Conclusion", "content": "This paper introduced a framework for evaluating\ndialect biases in reward models. Leveraging paired\nWME and (machine-translated) AAL preference\ndata, we showed that reward models are less accu-\nrate with AAL texts, generally disprefer AAL texts\nto WME texts, and incentivize steering conversa-\ntion toward WME."}, {"title": "6.2 Limitations", "content": "One of our study's main limitations lies in its heavy\ndependence on the VALUE (Ziems et al., 2022) and\nPhonATe (Deas et al., 2024) translation methods.\nAlthough both have undergone extensive human\nvalidation, they can make mistakes, which may\naffect the accuracy and representativeness of our\nmachine-translated AAL data.\nFurthermore, there is a notable dataset mismatch\nwhen utilizing the DG dataset for pairwise compar-\nison tasks. The absence of prompts in this dataset\nmeans it does not align well with prompt-based\npreference tasks, potentially impacting the valid-\nity of our experiments with human-translated data.\nWe hope that the strength of our findings with\nthe machine-translated texts motivates future work\non human-written paired preference datasets with\nWME and AAL. Such work would test the general-\nizability of our findings.\nFinally, in our experiments using the RB dataset,\nwe assume that the annotated preferences of the\noriginal data are conserved when considering AAL\nprompts and responses. While our limited qualita-\ntive assessment supports this assumption, we par-\ntially depend on the stated (and human-validated)\ndesign goals of the VALUE and PhonATe transla-\ntion methods, which aim to preserve meaning as\nmuch as possible, thereby avoiding label flipping."}, {"title": "6.3 Ethical Considerations", "content": "The ethical implications of this research are sig-\nnificant, particularly concerning the inclusion and\nrepresentation of non-dominant dialects such as\nAAL in language models. On the one hand, en-\nabling AI systems to generate or comprehend AAL\ncould enable more equitable systems that better\nserve marginalized communities.\nOn the other hand, there is a risk of cultural\nappropriation, where non-dominant dialects are co-\nopted without proper acknowledgment or under-\nstanding of their cultural significance. Language\nmodels that better comprehend AAL may also be\nleveraged in harmful ways, amplifying surveillance\nand privacy risks for already vulnerable popula-\ntions.\nFurthermore, the biases we identify in RMS\nagainst AAL raise questions about fairness and\nequity in AI systems. By privileging dominant lin-\nguistic norms, these models may reinforce systemic\ninequalities, alienating speakers of non-dominant\ndialects. Therefore, it is crucial to develop ap-\nproaches that actively involve AAL communities in\nthe decision-making and design processes regard-\ning how their language is represented and utilized\nin AI technologies.\nAnother ethical concern concerns the potential\nmisuse of language technologies that adopt non-\ndominant dialects. If such capabilities are not devel-\noped with appropriate safeguards, malicious actors\ncould exploit them, further marginalizing or mis-\nrepresenting these communities. Therefore, trans-\nparency, community involvement, and strict ethical\nguidelines are essential to ensure that the benefits\nof inclusive language technology are realized with-\nout causing harm.\nUltimately, ensuring that affected communities\nhave a meaningful voice in the development and"}, {"title": "A GPT-40 Prompt for Code Filtering", "content": "We use the following prompt template when query-\ning GPT-40 to identify examples in the Reward-\nBench dataset that contain blocks of code (e.g.,\nPython, Java):\nDoes the following text contain any code\n(e.g., Python, Java, Javascript, Go, Rust,\nLaTex)? Answer 'yes' or 'no'.\n<TEXT>"}, {"title": "B Dataset Dialect Analysis: RB and DG", "content": "There is no dialect metadata associated with the\ntexts in the RewardBench dataset. However, a qual-\nlitative inspection of a subset of the data suggests\nthat the text features align more with WME texts\nthan AAL ones.\nTo increase confidence in our assumption that the\ntexts are primarily WME-like, we leverage Blod-\ngett et al.'s (2016) model for predicting how white-\nlike vs. AAL-like (among other racial categories)\na text is. Their method fits a mixed-membership,\ndemographically-aligned language model based on\nTwitter data with tweet-level geo-location informa-\ntion, cross-referenced with U.S. Census data for\nracial demographic distributions at the neighbor-\nhood level. In their model analysis, they validate\nthe assumption that demographic information about\nspeakers correlates with specific linguistic features\nof racially skewed dialects such as AAL. We report\nthe outputs of Blodgett et al.'s (2016) model for all\nof our data in Table 4.\nNotably, the original RewardBench dataset (RB-\nWME) is much more white-like than AAL-like\nacross all text fields (prompt, chosen, rejected).\nThis bolsters our confidence that RB-WME is, in\nfact, predominately composed of WME texts.\nFor our machine-translated AAL version of the\nRewardBench dataset (RB-AAL), described in\nSection 3.1, we note that the overall predictions\nsuggest that Blodgett et al.'s (2016) method still\npredicts the texts to be more white-like than AAL-\nlike. Crucially, however, we see that the relative\nprobability changes consistently show that the RB-\nAAL texts are predicted as less white-like and more\nAAL-like than their RB-WME counterparts. Fur-\nthermore, since our paper focuses on a relative com-\nparison between WME-like and AAL-like texts,\nthese results suggest that our machine-translation\nmethods are effective, even if the AAL transla-\ntions are not perfect representations of AAL, in\nan absolute sense (i.e., on par with the predicted\nprobabilities for the naturally-occurring AAL in\nthe human-written DG-AAL data)."}, {"title": "C Reward Model Details", "content": "Table 5 lists the reward models evaluated in our\nstudy. They are a mix of sequence classifiers and\nDPO fine-tuned models, ranging from 2-20 billion\nparameters and spanning multiple families of base\npre-trained language models."}, {"title": "C.2 Dataset Dialect Analysis: Preference\nDatasets Used to Train Reward Models", "content": "Based on the limited public information about the\ndemographics of the annotators behind many pop-\nular preference datasets (Kirk et al., 2023; Casper\net al., 2023a), it is reasonable to assume that the\ndemographics do not represent the true population\nof those who use and/or are indirectly impacted by\nLLMs.\nWe are interested in whether the reward models\nwere trained on AAL-like texts. To estimate this,\nwe again leverage the Blodgett et al. (2016) method\nfor predicting the degree to which a text is AAL-\nlike (see Appendix 4 for additional details on the\ntechnique).\nWe estimate the extent to which a reward model\nwas trained on AAL-like text using the following\nprocedure:\n1. We identify the publicly accessible preference\ndatasets used to train the reward model based\non its HuggingFace model card and/or associ-ated paper (if available).\n2. We randomly sample up to 30k instances from\neach identified dataset for the model and use\nthe Blodgett et al. (2016) classifier to score\nhow AAL-like the texts are. We compute the\naverage over the entire sample for the dataset.\n3. We compute the average AAL score over the\ndataset averages, normalizing by dataset sam-\nple size."}, {}]}