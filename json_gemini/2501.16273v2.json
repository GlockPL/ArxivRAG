{"title": "Return of the Encoder: Maximizing Parameter Efficiency for SLMs", "authors": ["Mohamed Elfeki", "Rui Liu", "Chad Voegele"], "abstract": "The dominance of large decoder-only language models has overshadowed encoder-decoder architectures, despite their fundamental efficiency advantages in sequence processing. For small language models (SLMs) - those with 1 billion parameters or fewer our systematic analysis across GPU, CPU, and NPU platforms reveals that encoder-decoder architectures achieve 47% lower first-token latency and 4.7x higher throughput compared to decoder-only models on edge devices. These gains may be attributed to encoder-decoder's one-time input processing and efficient separation of understanding and generation phases.\nWe introduce a novel knowledge distillation framework that enables encoder-decoder models to leverage capabilities from large scalable decoder-only teachers while preserving their architectural advantages, achieving up to 6 average performance points improvement across diverse tasks, with significant gains in asymmetric sequence tasks where input and output distributions can benefit from different processing approaches.\nWhen combined with modern advances like Rotary Positional Embeddings (RoPE) and Vision encoders, our systematic investigation demonstrates that encoder-decoder architectures provide a more practical path toward deploying capable language models in resource-constrained environments. Our findings challenge the prevailing trend toward decoder-only scaling, showing that architectural choices become increasingly crucial as parameter budgets decrease, particularly for on-device and edge deployments where computational efficiency is paramount.", "sections": [{"title": "1. Introduction", "content": "The introduction of the Transformer (Vaswani, 2017) as an encoder-decoder architecture revolutionized sequence modeling and proven remarkably robust over time. While encoder-decoder transformers demonstrated strong performance, their inherent information bottleneck between encoder and decoder components ultimately constrained further scaling to hundreds of billions of parameters, as demonstrated in Raffel et al. (2020); Chowdhery et al. (2023).\nThis limitation led the field toward decoder-only architectures, following Sutton's \"bitter lesson\" (Sutton, 2019) that general methods leveraging computation often outperform specialized architectures. Models like GPT (Radford, 2018) and LLAMA (Touvron et al., 2023) have demonstrated impressive scaling properties (Kaplan et al., 2020), achieving state-of-the-art performance through increased parameter counts by eliminating this bottleneck and enabling more flexible parameter utilization at scale.\nDespite this trend, encoder-decoder architectures offer specific advantages through their separation of comprehension and generation stages. The encoder constructs a fixed representation of the input sequence, while the decoder performs targeted attention over encoded information during generation. This separation creates two key benefits: efficient handling of divergent input-output distributions (as in summarization and translation) and elimination of key-value (KV) cache requirements for input sequences, making them particularly efficient for tasks involving large inputs like code or long document reasoning/QA.\nRecent advances in positional encoding, particularly Rotary Positional Embeddings (RoPE), further enhances encoder-decoder architectures in two ways: by enabling precise encoding of relative token distances for improved reasoning over long contexts, and by facilitating efficient parameter utilization through asymmetric scaling. This dual benefit allows the architecture to both handle complex positional reasoning tasks like QA while enabling flexible optimization of encoder and decoder sizes for different tasks. Through pipelined inference, a 2N-parameter encoder-decoder model can match the computational efficiency of an N-parameter decoder-only model (Wang et al., 2022).\nWhile the architecture's inherent information bottleneck"}, {"title": "2. Related Work", "content": "Language model architectures have evolved through the interplay of theoretical advances and practical constraints, particularly in resource-constrained deployments of small-scale language models (SLMs)."}, {"title": "2.1. Evolution of Transformer Architectures", "content": "The Transformer architecture (Vaswani, 2017) revolutionized sequence modeling by replacing recurrent networks with attention mechanisms. Its core design demonstrated remarkable stability, requiring only minimal refinements such as pre-layer normalization (Xiong et al., 2020) and improved positional embeddings (Su et al., 2024). A significant empirical study by (Wang et al., 2022) later validated the advantages of non-causal visibility and masked language modeling, showing encoder-decoder setups consistently outperforming decoder-only architectures.\n(Liu et al., 2024b) demonstrated that architectural choices become crucial at smaller scales. In particular, their findings revealed that deeper, thinner models and grouped-query attention (GQA) mechanisms are particularly effective for sub-billion parameter models. Building on these insights, our architecture incorporates GQA and emphasizes depth over width."}, {"title": "2.2. Efficient Model Design", "content": "Recent approaches to efficient modeling have evolved along several paths. Decoder-only solutions like SmolLM (Allal et al., 2024) and MobileLM (Liu et al., 2024b) focus on higher quality data and architectural optimizations respectively, while Llama 3.2 (Dubey et al., 2024) explores pruning and knowledge distillation. While large models rely heavily on prompt engineering for task adaptation, our work suggests that finetuning smaller, more efficient models may offer a more practical and scalable approach for deploy"}, {"title": "2.3. Architectural Trade-offs", "content": "While decoder-only models excel in zero-shot scenarios (Wang et al., 2022), they struggle with long-sequence generation (Fu et al., 2023) and resource utilization. Encoder-decoder architectures, in contrast, offer significant advantages through their architectural design. The one-time input processing with fixed latent representations enables substantial inference optimization, while the natural separation of phases (i.e., encoder for understanding and decoder for generation) allows for efficient parameter distribution across the model. This architecture also enables task-specific optimization through independent sizing of encoder and decoder components, providing flexible asymmetric scaling capabilities. Furthermore, the bidirectional attention mechanism delivers superior performance at smaller scales (Wang et al., 2022), opening opportunities for advanced optimizations such as pooling and linear attention techniques (Tay et al., 2021). While these benefits traditionally came with engineering challenges in handling variable-length inputs, our work shows that modern approaches like RoPE (Su et al., 2024) can effectively mitigate these limitations while preserving the core advantages of the encoder-decoder."}, {"title": "2.4. Knowledge Distillation Advances", "content": "Knowledge distillation has emerged as a crucial bridge between efficiency and performance (Hinton, 2015; Dubey et al., 2024; Sreenivas et al., 2024), with notable successes like Llama 3.3's 70B student matching its 405B teacher. Unlike previous approaches maintaining decoder-only architecture for both student and teacher, we introduce novel crossarchitecture distillation, enabling small encoder-decoder"}, {"title": "3. Parameter Efficient SLMs", "content": "Building on insights from decoder-only and encoder-decoder architectures, we present a parameter-efficient framework that combines the architectural advantages of encoder-decoders with a novel knowledge distillation approach, enabling smaller models to benefit from larger decoder-only teachers while maintaining their inherent efficiency benefits."}, {"title": "3.1. Architectural Design", "content": "Our encoder-decoder architecture addresses two fundamental challenges in language modeling: efficient handling of variable-length sequences and optimal parameter allocation. While decoder-only models process concatenated input-output sequences uniformly, our approach enables specialized processing and flexible resource distribution based on the distinct roles of understanding (encoder) and generation (decoder).\nCore Architecture The design builds on the encoder-decoder foundation from (Raffel et al., 2020), with several key modifications for improved efficiency. Following (Wang et al., 2022), we maintain consistent training FLOPs across all architectural variants while incorporating modern components: pre-layer normalization (Xiong et al., 2020), Rotary Positional Embeddings (RoPE) (Su et al., 2024), and Grouped-Query Attention (GQA) (Ainslie et al., 2023). GQA proves particularly valuable in our resourceconstrained setting, aligning with findings from (Liu et al., 2024b) on its effectiveness for small-scale deployments.\nSequence Length Management A critical piece in our design lies in the handling of variable-length sequences. Traditional encoder-decoder architectures suffer from two key inefficiencies: separate padding requirements for encoder and decoder components, and complex cross-attention management. We address these challenges through an integrated approach to sequence processing. At the core of our solution is the integration of RoPE with neural tangent kernel (NTK) scaling (Chowdhery et al., 2023), enabling flexible sequence length handling without rigid padding constraints. The encoder produces token-wise representations with fixed dimensionality (while the sequence length varies with input), enabling efficient memory utilization during generation. Furthermore, cross-attention computations leverage these pre-computed encoder representations, eliminating the need for repeated input processing. This approach maintains the architectural benefits of separate encoding and decoding while mitigating traditional efficiency bottlenecks."}, {"title": "3.2. Knowledge Distillation Framework", "content": "A key contribution of our work is a novel knowledge distillation framework that enables encoder-decoder models to learn from larger decoder-only architectures, despite their fundamentally different input-output schemas-decoderonly models use unified attention on concatenated sequences, while encoder-decoder models maintain separate processing stages.\nBuilding upon (Agarwal et al., 2024), we introduce novel sequence alignment strategies specifically designed for distilling encoder-decoder models from decoder-only teachers. For an input sequence x of length |x|, we first generate output sequence y using the student model. We structure the inputs distinctly: the teacher receives a concatenated sequence $[PAD]_{n_e}\\circ x \\circ y \\circ [PAD]_{n_d}$ (where [PAD] denotes padding tokens, $n_e$ and $n_d$ are encoder and decoder padding lengths), while the student model processes $x \\circ [PAD]_{n_e}$ in the encoder and $[BOS] \\circ y \\circ [PAD]_{n_d}$ in the decoder (where [BOS] denotes the beginning-of-sequence token). This arrangement ensures proper alignment through careful offset management, with teacher logits starting at position $(|x| + n_e - 1)$ and student logits coming directly from the decoder output.\nThe complete distillation process follows Algorithm 1, employing a temperature parameter $\u03c4$ and combining reverse KL-divergence with cross-entropy loss, where the mixing ratio is tuned per dataset. The temperature parameter serves dual purposes: softening probability distributions for improved knowledge transfer and scaling gradients through the $\u03c4^2$ term. Further ablation studies (see Appendix) demonstrate the effectiveness of this approach compared to alternative distillation methods."}, {"title": "3.3. Training and Evaluation Methodology", "content": "Training Process Training is done in two stages. We first conduct pretraining using span corruption (Raffel et al., 2020) with a 15% noise ratio and span length k = 3 on a decontaminated 100B token dataset from FineWeb-Edu (Penedo et al., 2024). For downstream tasks, we implement two training strategies: standard sequence-to-sequence learning with cross-entropy loss and knowledge distillation from a Phi-3.5Mini (3.3B parameters) (Abdin et al., 2024) that is fine-tuned on the downstream task.\nTraining Efficiency We train all the models using 16 A100 GPUs. The encoder-decoder (2/3-1/3) model com"}, {"title": "3.4. Hardware Efficiency Analysis", "content": "To demonstrate practical deployment capabilities, we conduct comprehensive efficiency analysis across three representative platforms: an NVIDIA RTX A6000-48GB GPU for high-performance computing, a X1E80100-Snapdragon Qualcomm Oryon CPU for consumer-grade client-side deployment, and a Qualcomm SnapDragon Hexagon neural processing unit (NPU) for mobile/edge applications. To maintain consistency in model execution environments, we compiled both 330M architectures into the ONNX format and executed them across CPU, GPU, and NPU platforms. Our analysis focuses on two critical metrics for inference: first token latency for response time assessment, and subse"}, {"title": "4. Extending to Vision-Language Tasks", "content": "Having demonstrated our architecture's efficiency for text tasks, we extend these benefits to vision-language tasks where modality separation proves even more crucial for computational efficiency."}, {"title": "4.1. Vision-Language Architecture", "content": "Our vision-language model (Figure 3) maintains the core benefits of the text encoder-decoder architecture and incorporating visual processing capabilities. We utilize CLIP's ViT-L-336px (Radford et al., 2021) as our vision encoder, choosing the highest resolution variant to maximize visual understanding. Following (Liu et al., 2023; Li et al., 2024), we employ a 2-layer MLP projection layer to align the vision encoder's output with the text embedding space.\nTo handle high-resolution images efficiently while preserving detail, we follow (Li et al., 2025; Chen et al., 2024) in implementing a high-resolution image processing pipeline. Our approach first partitions input images into sub-images to enable detailed visual analysis. To provide a holistic view with a global visual context, the original image is also resized to a low-resolution thumbnail image that matches the input resolution of the vision encoder (Liu et al., 2024a). These sub-images are then processed independently through the vision encoder, after which the encoded vision tokens are concatenated before being fed to the text encoder-decoder. This architecture maintains our principle of separation be"}, {"title": "4.2. Efficient Training Strategy", "content": "Our process begins with feature alignment, where we train the projection layer $W$ on 600K image-caption pairs for one epoch, establishing the foundation for vision-language integration. We then enhance OCR capabilities using 200K image-OCR examples, which is critical for tasks requiring text extraction from images. This is followed by instruction following training on 700K examples to develop general visual reasoning capabilities. Finally, we fine-tune on specific downstream tasks such as VQAv2 (Goyal et al., 2017) and TextVQA (Singh et al., 2019).\nOur training leverages diverse datasets spanning multiple domains: feature alignment via LLaVA pretraining dataset (Liu et al., 2023), OCR understanding via UReader (Ye et al., 2023) and SynDoG (Kim et al., 2022), diagram understanding through AI2D (Kembhavi et al., 2016), chart comprehension via ChartQA (Masry et al., 2022), document analysis using DocVQA (Mathew et al., 2021), and general visual reasoning through GQA (Hudson & Manning, 2019) and VG (Krishna et al., 2017).\nVision Token Compression Processing high-resolution images presents computational challenges, as they can generate between 5k-10k vision tokens. To address this, we implement a novel efficient token compression strategy. Our primary approach is variance-based selection, a computationally efficient heuristic that reduces vision tokens by 67% (from 3,000 to 1,000) by identifying and filtering lowinformation background regions. We also explored learned token weighting through a trainable selection layer, though our experiments suggest this approach requires additional optimization through auxiliary losses.\nRecent developments offer promising directions for further"}, {"title": "4.3. Experimental Evaluation", "content": "Our evaluation framework for vision-language models maintains the same principles of efficiency and effectiveness established in our text experiments. For comprehensive assessment, we compare three architectural configurations: an 1/2-1/2 encoder-decoder model with 800M parameters (22 encoder, 22 decoder layers) and a parameter-matched decoder-only baseline (48 layers).\nOur evaluation framework for vision-language models maintains the same principles of efficiency and effectiveness established in our text experiments. Building on insights from work like Hinton (2015) and Agarwal et al. (2024), we leverage knowledge distillation to maximize model capabilities within our computational budget. Our evaluation compares two configurations: an encoder-decoder model with 800M parameters (22 encoder, 22 decoder layers), a similarly-distilled decoder-only baseline of 800M parameters (48 layers). Both are distilled from a task-finetuned Phi-3-Vision (4.1B parameters) (Abdin et al., 2024) as a largerscale reference point (teacher). We use knowledge distillation and the same training pipelines consistently across the encoder-decoder and decoder-only variants to ensure a fair comparison that isolates architectural differences while maintaining optimal performance for each configuration.\nThe evaluation spans several key tasks strategically chosen to stress different aspects of multimodal understanding: VQAv2 (Goyal et al., 2017) for complex visual reasoning, TextVQA (Singh et al., 2019) for cross-modal comprehen"}, {"title": "5. Conclusion and discussion", "content": "Rather than focusing on state-of-the-art comparisons with orthogonal SLMs of different training configurations like Mehta et al. (2024) and Allal et al. (2024), our work focuses on the fundamental architectural advantages of encoderdecoder designs in resource-constrained deployments. Recent trends have favored massive decoder-only models following Sutton (2019)'s \"bitter lesson\", but our systematic investigation reveals that architectural choices become increasingly crucial as parameter budgets decrease. Intriguingly, we find that the encoder-decoder information bottleneck - often cited as a limitation for scaling to hundreds of billions of parameters - becomes a valuable inductive bias at smaller scales, effectively constraining the model to learn more efficient representations and processing patterns.\nThe encoder-decoder architecture delivers four key benefits in small-scale deployments:\nOptimized performance for asymmetric tasks and divergent input-output distributions (e.g., summarization, long-context QA)\nFlexible parameter allocation between components for task-specific optimization (i.e., asymmetric scaling)\nTraining & Inference efficiency through one-time input processing and fixed memory footprint\nSuccessful integration of modern advances like ViT (Alexey, 2020) and ROPE (Su et al., 2024), extending benefits to multimodal tasks\nFurther, our knowledge distillation framework effectively bridges large-scale training benefits with efficient deploy"}]}