{"title": "Survey on Hand Gesture Recognition from Visual Input", "authors": ["Manousos Linardakis", "Iraklis Varlamis", "Georgios Th. Papadopoulos"], "abstract": "Hand gesture recognition has become an important research area, driven by the growing demand for human-computer interaction in fields such as sign language recognition, virtual and augmented reality, and robotics. Despite the rapid growth of the field, there are few surveys that comprehensively cover recent research developments, available solutions, and benchmark datasets. This survey addresses this gap by examining the latest advancements in hand gesture and 3D hand pose recognition from various types of camera input data including RGB images, depth images, and videos from monocular or multiview cameras, examining the differing methodological requirements of each approach. Furthermore, an overview of widely used datasets is provided, detailing their main characteristics and application domains. Finally, open challenges such as achieving robust recognition in real-world environments, handling occlusions, ensuring generalization across diverse users, and addressing computational efficiency for real-time applications are highlighted to guide future research directions. By synthesizing the objectives, methodologies, and applications of recent studies, this survey offers valuable insights into current trends, challenges, and opportunities for future research in human hand gesture recognition.", "sections": [{"title": "I. INTRODUCTION", "content": "Hand gesture recognition has captivated researchers' interest for decades, as it offers a natural and intuitive form of human-computer interaction. The ability to accurately detect and interpret hand gestures has numerous applications, from enhancing communication for the hearing impaired through sign language recognition [2], [5], [108] to enabling more efficient human-robot interaction in industrial and medical settings [12], [199] (see Fig. 1). Human hands play a versatile role in non-verbal communication, frequently conveying messages that can replace spoken words when interactions need to be brief and unambiguous. For example, gestures such as a thumbs-up for approval or a wave to beckon someone closer serve as universally understood signals that transcend language barriers and are often more practical than spoken language in certain contexts [43]. Additionally, hand gestures become especially valuable when communicating across physical distances where speech may not be audible, and enhance gaming by providing an intuitive, immersive control method that replaces traditional input devices.\nAs gesture recognition technology continues to advance, it holds the potential to transform both human-computer [40], [162] and human-to-human [128], [188] interactions by enabling contactless control and enhancing accessibility for users with disabilities. Additionally, 3D hand reconstruction [85], [177] is an important aspect of human gesture recognition, offering valuable digital insights into hand movements and supporting applications in fields like 3D animation. The growing importance of this field underscores the need for accurate, real-time recognition systems capable of both interpreting and reconstructing a wide range of gestures across diverse environments.\nThe type of data used for hand gesture recognition influences the accuracy, efficiency, and suitability of hand recognition systems across various application domains. Such data are available in various modalities, collected by infrared sensors [14], [116], motion capture systems, cameras, and wearable devices [46], [48]. Methods relying on infrared sensors or motion capture technology for hand gesture classification and estimation typically involve complex setups that can limit their applicability in real-world scenarios. On the other side, RGB cameras are commonly found in smartphones, tablets, and laptops, making it easier for researchers to test methodologies and to collect large datasets without using specialized equipment. This democratization of technology enables researchers to develop and deploy gesture recognition systems that can be utilized in everyday settings. Based on the above, we focus on works that use RGB images, depth images, and video data collected by cameras, which are broadly available and accessible to researchers and are widely used in common user applications.\nCompared to other recent surveys in the field, this survey offers a more comprehensive yet focused review of research in the field of Hand Gesture Recognition (HGR) using visual input. Unlike [150] that explores various modalities beyond visual input (e.g., EEG, EMG, and audio) with a primary emphasis on Sign Language Recognition (SLR) and limited discussion on hand gesture estimation, our work specifically"}, {"title": "A. Contributions of the Paper", "content": "This survey provides a comprehensive overview of recent advances in hand gesture recognition, aiming to cover multiple aspects, from algorithms and input types to applications and datasets. The main contributions of this paper can be summarized in the following:\n\u2022\n\u2022\n\u2022 An organized presentation of research approaches: We organize the surveyed studies based on input data types (e.g., RGB images, depth images, video), hand capture techniques, and recognition methods used, providing a clear framework for understanding current research directions.\nA survey of applications and datasets: We analyze the applications and datasets employed in recent studies, discussing their main features, limitations, and suitability for different applications.\nA review of current research trends: We examine the latest trends in human gesture recognition and estimation, highlighting emerging approaches and key focus areas within the field."}, {"title": "B. Organization of the Paper", "content": "Section II presents the fundamental concepts and defines the terminology used in the field, and at the same time illustrates the various types of human gestures (e.g., static vs. dynamic, hand, body, and facial gestures). Section III outlines the review methodology used to select and categorize the surveyed papers. Section IV provides an overview of the hand gesture recognition methods and their classifications, whereas Section V surveys the most popular algorithms employed in HGR and explains how they formulate and solve the task. Section VI discusses datasets used across different applications in hand gesture recognition. Section VII identifies trends and challenges highlighted in the literature. Finally, Section VIII summarizes key findings and suggests future research directions for advancements in the field."}, {"title": "II. BACKGROUND AND FUNDAMENTALS", "content": ""}, {"title": "A. Definition of Gesture and Gesture Recognition", "content": "A gesture can be defined as a structured, intentional movement or posture of the human body, often involving the hands, arms, or face, used to convey information and interact with devices, or control systems. Formally, let G represent a gesture, which can be expressed as a temporal sequence of poses:\n$G = \\{P_1, P_2, . . ., P_n\\}, P_i \\in R^d$\nwhere Pi denotes the i-th pose, represented in a d-dimensional feature space capturing spatial or spatio-temporal characteristics such as joint coordinates, velocity, or orientation.\nIn the context of this study, Gesture Recognition is subdivided into two categories; Gesture Classification and Gesture Estimation."}, {"title": "1) Gesture Classification", "content": "Gesture classification refers to the computational process of detecting and interpreting gestures from input data. Given a time series of inputs $X = \\{X_1,X_2,...,X_t\\}$ derived from sensors or imaging devices (e.g., RGB cameras, depth sensors), gesture recognition involves mapping X to a predefined set of gesture labels $C = \\{C_1, C_2,..., C_k\\}$:\n$f: X \\rightarrow C,$\nwhere f is the recognition function, trained to minimize a loss function $L(y, f(X))$, y being the ground truth label."}, {"title": "2) Gesture Estimation", "content": "Gesture estimation refers to the computational process of digitally representing a gesture's structure and configuration. Unlike gesture classification, which maps gestures to predefined labels, gesture estimation aims to extract key spatial or temporal features of the gesture, such as joint positions, angles, or trajectories. Given a time series of inputs $X = \\{X_1, X_2, ..., X_t\\}$ from sensors or imaging devices (e.g., RGB cameras, depth sensors), gesture estimation involves mapping X to a set of gesture pose parameters $P = \\{P_1,P_2,\u2026\u2026,P_n\\}$ that define the gesture's digital representation:\n$g: X \\rightarrow P,$\nwhere g is the estimation function, trained to minimize a reconstruction loss $L(P,g(X))$, P representing the ground truth gesture pose parameters (e.g., 3D joint coordinates or gesture skeleton structure)."}, {"title": "B. Static and Dynamic Gestures", "content": "Gestures can be classified into static and dynamic ones. A static gesture is defined as a single pose P that remains unchanged over time:\n$G_{static} = P, P\\in R^d$.\nIn contrast, a dynamic gesture involves a sequence of poses over time, modeled as a trajectory:\n$G_{dynamic} = \\{P_t | t = 1, 2, . . ., T\\},$\nwhere T represents the duration of the hand gesture. Dynamic gestures require temporal modeling to capture dependencies across the movement sequence."}, {"title": "C. Multimodal Gestures", "content": "Multimodal gestures refer to gestures that involve multiple body parts or the integration of multiple sensory modalities to enhance the richness and clarity of communication. These gestures combine various sources of information, such as hand movements, facial expressions, or even audio cues, to form a more comprehensive understanding of human intent.\nFormally, a multimodal gesture Gm can be expressed as the combination of modalities:\n$G_m = \\{M_1, M_2,..., M_k\\},$\nwhere Mi represents the i-th modality, such as hand motion, facial expressions, or audio signals. Each modality Mi can be represented in its own feature space Rdi, and the fusion of modalities $(G_m)$ combines these features:\n$\\phi(G_m) = \\phi_1(M_1) \\oplus \\phi_2(M_2) \\oplus...\\oplus\\phi_k(M_k),$\nwhere $\\oplus$ denotes the fusion operation, which may involve concatenation, attention mechanisms, or other techniques to integrate information.\nExamples of multimodal gestures include:\n\u2022\n\u2022\n\u2022 Hand and Facial Expressions: Gestures such as a thumbs-up combined with a smile to reinforce positive feedback.\nSpeech and Gestures: Pointing while providing verbal instructions to emphasize spatial references.\nCross-Sensory Integration: Clapping, snapping or vocal tone emphasis to accompany hand gestures, providing auditory reinforcement.\nThe integration of multimodal data often requires specialized frameworks to capture dependencies across modalities. These frameworks aim to improve recognition accuracy by leveraging complementary information and addressing ambiguities that may arise in unimodal systems. Multimodal gestures are particularly relevant in applications such as virtual reality, where body movements and vocal commands are often combined for immersive interactions, or in sign language translation, where facial expressions are critical for conveying grammatical nuances. Fig. 2 explains what a multimodal gesture may comprise using a hierarchical structure, and categorizing the visual and audio modalities into distinct gesture types."}, {"title": "D. Hand Gesture Recognition", "content": "Among the various types of gesture recognition, we focus on hand gesture recognition because it offers a unique combination of advantages. Hand gestures are highly expressive, enabling a wide range of commands, emotions, and intentions to be conveyed, making them versatile for applications such as human-computer interaction, virtual reality, and sign language interpretation. They also operate within a compact interaction zone, which is particularly practical for confined environments like vehicles, mobile devices, or personal workspaces. Additionally, hand gestures provide high precision and fine-grained control, essential for tasks like robotic manipulation, gaming, or medical surgery assistance. Many existing devices are already equipped with hardware optimized for capturing hand movements, reducing the need for specialized infrastructure. Furthermore, hand gestures are culturally and contextually universal, offering a natural and intuitive form of communication compared to other gesture types that may vary in interpretation. Lastly, the strong research foundation in hand recognition, supported by well-defined datasets and algorithms, allows for rapid development and deployment of systems, making it an ideal focus area."}, {"title": "III. REVIEW METHODOLOGY", "content": ""}, {"title": "A. Article Selection", "content": "The selection of articles for this study targets all the recent papers in the computer vision field that specifically focus on hand gesture recognition from images and videos. To ensure a comprehensive review, we began by identifying the top-ranked venues in computer vision as listed under the \"Top Publications - Computer Vision & Pattern Recognition\" section on Google Scholar\u00b9. Using a crawler developed for this purpose\u00b2, we retrieved 2,307 articles from these top venues.\nSubsequently, we conducted a systematic screening process in which the titles and abstracts of all articles were carefully reviewed based on predefined inclusion criteria detailed in Section III-B, which prioritized recent studies focusing on hand gesture recognition techniques, systems, or applications. This rigorous filtering ensured that only the most relevant to hand gesture recognition contributions from Google Scholar's top venues for computer vision were included in the survey, and resulted in 37 articles that were directly relevant to the task. Given the limited number of relevant studies (from Google Scholar), we widened the search base by conducting additional searches in all venues listed in Scopus using specific boolean search queries, which are listed in Table I. The queries were carefully designed to contain the most relevant keywords to hand gesture recognition. To ensure consistency with the Google Scholar approach, we systematically reviewed the titles and abstracts of the matching papers from each query, assessing their alignment with the inclusion/exclusion criteria outlined in Section III-B. Articles were excluded if they lacked relevance to the hand gesture recognition task, its techniques, systems, or applications."}, {"title": "B. Inclusion/Exclusion Criteria", "content": "All selected articles have been rigorously reviewed and carefully analyzed based on the inclusion/exclusion criteria described in the following:\n\u2022\n\u2022 Methodologies with human action recognition (entire body) that did not include hand estimation or classification were excluded,\nStudies were included only if published between 2018 and 2024,\n\u2022 Only research publications available online (i.e., peer-reviewed conference proceedings, book chapters, and journal articles) were included, and\n\u2022 Only studies written in English were considered."}, {"title": "C. Quantitative Analysis", "content": "The search and filtering methodology described above helped us identify 113 studies (37 from Scholar and 76 from Scopus) that propose efficient methodologies for hand gesture recognition. To better understand how these studies are distributed over time and across different forums we performed a quantitative analysis, which is summarized in Table II and Fig. 3. The visualizations provide a comprehensive overview of the research landscape of hand gesture recognition, revealing trends in publication frequency, preferred venues, and topics."}, {"title": "1) Distribution of Articles to Venues", "content": "As shown in Table II, the main forums where research in hand gesture recognition has been published include the major computer vision conferences such as CVPR, ICCV, ICCVW, ECCV, and the International Journal of Computer Vision. These forums list more than 50% of the relevant works that contribute to advancements in this field."}, {"title": "2) Research Timeline by Topic", "content": "As shown in Fig. 3 the main topics discussed in the articles. The methodology for extracting the topics from the collection of articles is detailed in Section II-D that follows. The bar charts depict the annual occurrences of individual topics, while the line plot represents the total number of publications per year. The analysis reveals a substantial increase in publications in 2024, signaling growing research interest and advancements in the field of hand gesture recognition."}, {"title": "D. Topic modeling", "content": "In order to get a better understanding of the articles we retrieved, we decided to group them into topics, applying topic modeling [127]. The main topics are depicted in Fig. 4 and are described in the following.\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022 Hand Gesture Classification: Focuses on categorizing different types of hand gestures into predefined classes.\nHand Gesture Estimation: Involves determining the 3D position and orientation of hand joints.\nSign Language Recognition: Translates hand gestures into a corresponding sign language vocabulary.\nHand/Body Reconstruction: Deals with accurate 3D model creation/estimation of hands or bodies.\nMultimodal Fusion: Combines multiple models and integrating data from diverse sources (e.g., video, depth cameras) to improve accuracy and robustness.\nReal-Time Recognition: Emphasizes systems capable of recognizing and processing gestures in real-time."}, {"title": "IV. OVERVIEW OF HGR METHODS", "content": "Hand gesture recognition (HGR) has been the focus of extensive research, resulting in a wide range of methodologies. This section provides a comprehensive review of the methods found in the current literature, classified based on key factors such as the primary task objective (hand gesture classification or estimation), the type of input devices (e.g., RGB, RGB-D, or video, monocular or multi-view cameras), the type of hand gesture capture and representation (e.g., skeleton or box/filter), and the machine learning techniques employed in the recognition (e.g., SVMs, neural networks). These classifications are also summarized and visualized in Fig. 5."}, {"title": "A. Classify by task objective", "content": "As broadly described in Section II, we can distinguish two main hand recognition tasks: hand gesture classification and hand gesture estimation. While this survey focuses primarily on hand gesture classification, the significance of the hand gesture estimation task cannot be overlooked, since it is supported by a crucial subset of general hand gesture recognition methods. In what follows, we briefly present the main objectives of the two tasks and discuss their main applications."}, {"title": "1) Hand Gesture Classification", "content": "The primary goal of hand gesture classification is to identify and categorize a given gesture into predefined classes [1], [119], [172]. As a result, the respective methods focus on interpreting the semantic meaning of gestures rather than their capturing and predicting their spatial representation. Hand gesture classification methods are widely used in applications like sign language recognition [24], [144], [188] and human-robot communication [12], [40]. In sign language recognition, the classifier distinguishes between subtle variations in gestures to correctly identify words or phrases, as shown in Fig. 6a. Similarly, in HCI, classification methods enable systems to interpret hand gestures to user commands that are then used to control devices or software applications. Hand classification approaches are often evaluated based on metrics such as accuracy, precision, recall, and F1-score, which reflect their ability to correctly classify gestures. The broad applicability of this task makes it an accessible and popular area of research, particularly with the growing availability of related image/video datasets for training and testing [65], [87], [101], [108]."}, {"title": "2) Hand Gesture Estimation", "content": "Hand gesture estimation focuses on understanding and representing the topology or spatial structure of the hand [15], [56], [147]. Unlike classification, which assigns a gesture to a specific category, estimation methods aim to digitally recreate the hand's pose or movement, as shown in Fig. 6b. This is typically achieved by generating a 2D or 3D representation, such as a skeleton with joints [107], [177] or a detailed mesh of the hand's surface [129], [143]. The applications of hand gesture estimation are mainly in areas that require detailed spatial information, such as augmented reality (AR), virtual reality (VR), gaming, and animation. For example, in a VR environment, hand estimation methods allow users to interact with virtual objects through accurate hand tracking [151], [159]."}, {"title": "B. Classify by Input Data type", "content": "The type of input data is an important factor in defining the effectiveness and scope of a hand gesture recognition method. This survey focuses on methods utilizing static images or video, which are widely used input data types due to the widespread availability of cameras and their accessibility to users. One classification is based on the format of the input data, which can be either RGB or RGB-D images, or video. A different classification is based on the type of camera, namely monocular or multi-view. In the following, we provide the main features of each of these input categories."}, {"title": "1) RGB Methods", "content": "RGB methods rely on static RGB images without considering sequential video frames, so the works of this category analyze a single frame to recognize/estimate static hand gestures [82], [126], [196]. Although these methods are limited to processing one frame at a time, they can be easily extended to handle multiple frames for video-based hand gesture recognition. However, such extensions are usually less efficient than methods specifically designed for video gesture recognition."}, {"title": "2) RGB-D Methods", "content": "RGB-D methods utilize depth image data, which combines traditional RGB information with depth data to enhance hand gesture recognition [26], [158], [173]. These methods typically work on a single RGB-D frame at a time to capture gesture details, providing richer spatial information compared to RGB-only approaches."}, {"title": "3) Video Methods", "content": "Video-based methods process sequences of RGB frames [16], [112], [166], or even RGB-D frame sequences [22], [59], [159], [162]. In the case of RGB-D video sequences, depth data accompanies each frame to provide a more comprehensive understanding of hand gestures. The respective methods in this category are designed for analyzing dynamic hand movements in tasks such as sign language recognition or complex gesture analysis."}, {"title": "4) Monocular vs. Multi-View Input", "content": "The type of camera setup\u2014monocular or multi-view\u2014further influences the methodology of hand gesture recognition. Monocular methods rely on a single camera to capture hand gestures, whereas multi-view methods use multiple cameras from different angles to provide a more complete representation of the hand. Monocular methods dominate the field due to their simplicity, cost-effectiveness, and ease of deployment in real-life scenarios [28], [36], [117]. In contrast, multi-view methods, while offering improved accuracy and robustness in capturing complex gestures, require intricate setups involving multiple cameras, which limits their practicality for everyday use.\nDespite their limited prevalence, multi-view setups are worth mentioning, particularly for applications in hand gesture estimation where capturing detailed 3D spatial information is critical [23], [64], [133]. However, as the survey progressed, the distinction between monocular and multi-view methods became less significant, with most articles opting for monocular setups (as also depicted in Fig. 7b) due to their practicality and widespread applicability."}, {"title": "C. Classify by Hand Gesture capture and representation", "content": "Hand capture methods play an important role in hand gesture recognition systems as they determine how hand gestures are acquired for further processing. These methods can be broadly classified into skeleton-based captures and box/filter-based captures, each offering unique advantages and challenges."}, {"title": "1) Skeleton-Based Captures", "content": "Skeleton-based methods represent the hand using a skeletal structure that consists of key joints and connections [33], [105], [149]. These captures typically utilize graphs, where nodes represent specific hand joints or segments such as fingers, and edges denote the relationships or connections between them. This graph-based representation enables the system to focus on the underlying structure and movement dynamics of the hand. By abstracting the hand into a simplified skeleton, these methods are particularly effective in scenarios where the precise articulation of fingers and joints is important."}, {"title": "2) Box/Filter-Based Captures", "content": "Box/filter-based methods follow a different approach, focusing on capturing the hand's region using bounding boxes or regions of interest [4], [24], [131]. These methods involve detecting the hand from the image using a bounding box and once the hand is isolated, applying additional filters or algorithms to extract meaningful features related to the gesture, such as shape, texture, or movement patterns.\nSkeleton-based captures excel in scenarios that require precise motion analysis and are robust against background noise. However, these methods often require more computational resources to represent the hand as a graph, making them more suitable for applications where accuracy and detailed articulation are prioritized over speed or simplicity. In contrast, box/filter-based methods are well-suited for working with 2D data and are easily adaptable to a wide range of environments. These methods are generally more user-friendly and are widely applied in hand gesture classification tasks (as described in Section IV-F, where Bayes' Theorem demonstrates that box/filter methods are predominantly used in classification tasks), where the primary goal is to categorize gestures, rather than in estimation tasks, which require detailed hand representation. Despite their advantages, box/filter-based methods are sensitive to external factors such as lighting conditions, background clutter, and variations in hand appearance, which can negatively impact recognition performance.\nIt is worth noting that box/filter-based approaches have been studied more extensively in the literature compared to skeleton-based methods (as shown in Fig. 9). The greater prevalence of box/filter-based methods can likely be attributed to their relative simplicity, ease of implementation, and broader applicability across various scenarios, making them a preferred choice for researchers in the field."}, {"title": "D. Classify by Recognition Techniques", "content": "Hand gesture recognition employs a variety of techniques to extract gesture-specific information from RGB data. In recent years, the combination of convolutional neural networks (CNNs) with other neural network-based methods has emerged as the dominant approach, leveraging the strengths of multiple models. This trend is evident in the findings of this survey, where 68% of the selected articles employ such hybrid methods for hand gesture recognition. Nonetheless, standalone neural networks or traditional machine learning techniques, such as Support Vector Machines (SVMs) and Random Forests, continue to play a role in specific applications. Based on the methodologies used, the articles can be categorized into three groups: hybrid approaches, neural network-based approaches, and non-neural network approaches as discussed in the following."}, {"title": "1) Neural Network Approaches", "content": "Neural networks, particularly CNNs [21], [153], [176], have been extensively applied to hand gesture recognition due to their ability to learn spatial hierarchies and extract complex features directly from RGB data. Recent advancements, such as transformers [191], have further enriched this field by enabling models to capture temporal and contextual information. This capability is especially valuable for applications like sign language recognition and human-computer interaction (HCI), where understanding the context of previous gestures is crucial."}, {"title": "2) Non-Neural Network Approaches", "content": "Traditional machine learning methods, such as SVMs, remain relevant for specific hand gesture recognition tasks [10], [38], [66]. These methods typically involve pre-processing input images or video frames to extract features, which are then fed into machine learning models for classification or regression. While less common in recent studies, these approaches are still employed in scenarios where computational efficiency or simplicity is a priority."}, {"title": "3) Hybrid Approaches", "content": "Hybrid approaches represent the predominant methodology in hand gesture recognition, combining the strengths of multiple techniques to achieve higher accuracy and robustness. For instance, methods integrating CNNs with LSTMs leverage CNNs for spatial feature extraction and LSTMs for temporal pattern learning [42], [73], [120]. Other hybrid systems blend neural networks with traditional machine learning techniques in ensemble models or integrated frameworks [26], [137]. These approaches are particularly effective in tasks requiring adaptability and precision, as they exploit the complementary strengths of their components, offering a versatile solution for complex hand gesture recognition challenges."}, {"title": "E. Summary of Methodology Overview", "content": "To provide a clear understanding of the groupings, Table III summarizes the classification of the articles based on the criteria described in this section. Specifically, these criteria include:\n\u2022 Type of input data: video, RGB-D, (static) RGB_image\n\u2022 Input configuration: monocular or multiview\n\u2022 Hand capture method: box/filter or skeleton\n\u2022 Goal of hand recognition: estimation or classification"}, {"title": "F. Association between input, input type, capture and hand recognition task", "content": "To better understand the relationships between the hand gesture categories discussed in Section IV and the hand recognition task (classification or estimation), we utilized Bayes' theorem. Bayes' theorem allows us to determine the probability of Y occurring given that X is true. The formula is as follows:\n$P(Y|X) = \\frac{P(X|Y) \\cdot P(Y)}{P(X)}$\nwhere:\n\u2022 P(Y|X) is the conditional probability of Y given X,\n\u2022 P(X|Y) is the likelihood of X given Y,\n\u2022 P(Y) is the prior probability of Y, and\n\u2022 P(X) is the marginal probability of X.\nIn our analysis, X represents the groups: input, input_type, capture, and method, and Y represents the goal, which can be either estimation or classification. For the groups input and method, we extended Bayes' theorem to consider joint probabilities, as these categories include three distinct values each. To simplify the analysis and derive meaningful insights, we merged the values within these categories as follows:\n\u2022 Input groups \u2192 RGB and Video vs. RGB-D: Static RGB images and video data were combined into a single category to compare with RGB-D inputs. This grouping reflects the fact that RGB and video methods both rely solely on color information, whereas RGB-D methods include depth information, making them inherently different in terms of hardware requirements and data richness."}, {"title": "V. OVERVIEW OF ALGORITHMS USED IN HGR TASKS", "content": "The variety of techniques employed in HGR reflects the complexity of the task, ranging from spatial modeling to capturing temporal dynamics. This section explores the prominent algorithms utilized in HGR, emphasizing their unique strengths and applications in diverse real-world scenarios."}, {"title": "A. Support Vector Machines (SVMs)", "content": "Support Vector Machines are a robust classification tool in HGR, particularly effective for distinguishing complex hand poses or subtle variations in gesture patterns. The works in the field employ different feature extraction techniques and representations to capture information from images. Typical examples are Histogram of Oriented Gradients [52], Hu Moment Invariants extracted by the hand contour [194], or positional and trajectory features of the hand joints.\nFollowing the notation described in Section II, from a set of N input images the potential gestures are represented as $X = \\{X_1,X_2,...,X_n\\}$ where each vector $x_i \\in R^d$ is a d-dimensional feature vector extracted using HOG, skeletal tracking, etc. information. The set of corresponding class labels for each image is denoted as $Y = \\{Y_1,Y_2, \u2026\u2026\u2026, Y_N \\}$ where $y_i \\in \\{1,2,..., C\\}$ represents the class of gesture i and C is the total number of distinct gesture classes. The SVM aims to find a hyperplane that separates the classes in the feature space. The SVM optimization problem can be expressed as:\n$min =\\frac{1}{2}||w||\u00b2 + \\sum_{i=1}^{N}\\xi_i$\nw,b\nsubject to:\n$Y_i(w^T x_i + b) \\geq 1 - \\xi_i, i = 1, 2, ..., N$\nwhere w is the weight vector defining the hyperplane, b is the bias term, $\\gamma > 0$ is a regularization parameter controlling the trade-off between maximizing the margin and minimizing classification error, $\\xi_i$ are slack variables that allow for misclassification.\nOnce trained, the SVM model predicts the class label for a new input vector xnew:\n$f(x_{new}) = sign(w^{T}\\phi(x_{new}) + b)$\nThis function outputs $f(x_{new}) = y_j$, where j = 1, 2, ..., C, indicating which class the new gesture belongs to.\nThe integration of SVMs with handcrafted feature descriptors, such as Local Extrema Min-Max Pattern (LEMMP), has led to remarkable recognition accuracies, reaching up to 99% on benchmark datasets [10]. Kernel-based SVMs have proven successful in dynamic gesture recognition systems, achieving over 98% accuracy in tasks like Japanese Sign Language classification [66]. Ensemble methods, where SVMs are part of multi-camera systems, further enhance recognition accuracy by fusing results from multiple classifiers [133]. These attributes make SVMs a versatile choice for both static and dynamic gesture recognition."}, {"title": "B. Hidden Markov Models (HMMs)", "content": "Hidden Markov Models are widely employed for modeling sequential dependencies in HGR tasks [69], [73]. These probabilistic models excel at predicting gesture sequences by representing hidden processes through observable outputs.\nThe process of gesture recognition using HMMs can be formulated with the following steps:\n\u2022\n\u2022\n\u2022\n\u2022 Feature Extraction: Initially, features are extracted from the video input. Techniques such as skin color segmentation may be employed to isolate the hand from the background, followed by tracking methods to monitor hand movements over time. Preprocessing techniques, such as grayscale conversion and edge detection, facilitate the extraction of key gesture regions for HMM-based sequence learning.\nState Representation: Each gesture is represented by a sequence of hidden states. For example, in a sign language recognition system, each state might correspond to a different phase of a gesture (e.g., starting position, mid-gesture, ending position).\nObservation Sequence: Observations are derived from features extracted from video frames that capture relevant aspects of the gesture. Common features include the position of key points (such as fingers and hands) and angles between joints. The continuous observation sequences such as angles or positions are quantized into discrete states suitable for HMM modeling. This quantization process involves mapping continuous values into discrete categories that can be effectively handled by the HMM framework.\nTransition Probabilities: To model the transitions between these hidden states, transition probabilities are defined. For example, let $P(s_t = j | S_{t-1} = i) = A_{ij}$ denote the probability of transitioning from state i at time t-1 to state j at time t. This probabilistic framework allows us to account for the inherent variability in how gestures are performed.\nEmission Probabilities: In addition to transition probabilities, emission probabilities are also established that describe how observations are generated from hidden states. Each state emits observations according to a probability distribution, often modeled using Gaussian distributions. This means that for a given state st, the observation Ot is generated based on a distribution characterized by parameters specific to that state.\nTraining the HMM: The parameters of the HMM -namely, transition and emission probabilities\u2014 are estimated using algorithms such as the Baum-Welch algorithm. This training phase is crucial for enabling the model to learn from example sequences of gestures.\nDecoding Observation Sequences: Once trained, algorithms like Viterbi are used to decode new observation sequences and to determine the most likely sequence of hidden states that corresponds to a given gesture.\nHybrid approaches integrating HMMs with CNNs or LSTMs capitalize on their strengths in spatial and temporal feature extraction, achieving state-of-the-art results in datasets with complex and continuous gestures."}, {"title": "C. Convolutional Neural Networks (CNNs)", "content": "Convolutional Neural Networks are often utilized in HGR due to their ability to process visual data and extract meaningful features from images in real-time [4", "160": ".", "includes": "n\u2022\n\u2022\n\u2022\n\u2022 Convolutional Layers: These layers apply convolution operations to"}]}