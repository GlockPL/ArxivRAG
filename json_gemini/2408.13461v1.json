{"title": "Probing the Robustness of Vision-Language Pretrained Models: A Multimodal Adversarial Attack Approach", "authors": ["Jiwei Guan", "Tianyu Ding", "Longbing Cao", "Lei Pan", "Chen Wang", "Xi Zheng"], "abstract": "Vision-language pretraining (VLP) with transform-ers has demonstrated exceptional performance across numerous multimodal tasks. However, the adversarial robustness of these models has not been thoroughly investigated. Existing multimodal attack methods have largely overlooked cross-modal inter-actions between visual and textual modalities, particularly in the context of cross-attention mechanisms. In this paper, we study the adversarial vulnerability of recent VLP transformers and design a novel Joint Multimodal Transformer Feature Attack (JMTFA) that concurrently introduces adversarial perturbations in both visual and textual modalities under white-box settings. JMTFA strategically targets attention relevance scores to disrupt important features within each modality, generating adversarial samples by fusing perturbations and leading to erroneous model predictions. Experimental results indicate that the proposed approach achieves high attack success rates on vision-language understanding and reasoning downstream tasks compared to existing baselines. Notably, our findings reveal that the textual modality significantly influences the complex fusion processes within VLP transformers. Moreover, we observe no apparent relationship between model size and adversarial robustness under our proposed attacks. These insights emphasize a new dimension of adversarial robustness and underscore potential risks in the reliable deployment of multimodal AI systems.", "sections": [{"title": "I. INTRODUCTION", "content": "Vision and Language Pretraining (VLP) employing trans-formers has recently emerged as a promising approach in understanding the complex relationships between visual and linguistic modalities [1], [2]. These VLP transformers enhance adaptability and acquire extensive knowledge by leveraging paired image and text inputs through self-attention learning. These models are pre-trained on large-scale datasets and subsequently fine-tuned for a variety of specific downstream tasks [3]\u2013[5]. Their performance has demonstrated great gen-eralizability in a variety of multimodal vision and language tasks, including Visual Question Answering (VQA) [6], [7], Visual Commonsense Reasoning (VCR) [8] and Image Cap-tioning [9]."}, {"title": "II. RELATED WORK", "content": "A. Vision and Language Pretraining\n\nVLP transformers typically comprise three essential com-ponents: a text encoder, a visual encoder, and a multimodal feature fusion encoder. The fusion encoder simultaneously acquires modality knowledge from text and visual encoders to effectively learn joint visual and textual feature repre-sentations, enhancing performance on multiple downstream tasks. In VLP fusion, there are two typical types of model architectures. Single-stream architectures, such as VILT [12] and visualBERT [13], interact with modalities through self-attention, merging text and image features into a unified representation. In contrast, recent dual-stream architectures like VLE [14] and LXMERT [15] utilize separate transformers for encoding each modality. These dual-stream models em-ploy cross-attention for more sophisticated information fusion, fostering richer modality-specific representations. As depicted in Fig. 2, these VLP architectures leverage advanced fusion in vision and language tasks. Despite impressive downstream task performances, there is a lack of studies on investigating adversarial vulnerability of fused VLP transformer models. The vulnerability of fusion module can provide an oppor-tunity for adversaries who aim to deteriorate contextualize information by exploiting modality attention influences. In this research, our primary focus is on the adversarial robustness of discriminative VLP trasnformers in relation to classification challenges, leaving autoregressive generation VLP transform-ers like BLIP [16] and METER [17] for future exploration.\n\nB. Single Modality Adversarial Attack\n\nVision Modality Attacks. Szegedy et al. [18] first demon-strate adversarial vulnerability of deep neural networks where this study has been extensively studied for computer vision domains [19]\u2013[24], focusing on optimizing the loss function with respect to single vision modality decision boundaries based on model outputs. Furthermore, feature importance attacks [25], [26] aggregate gradients concerning feature maps to perturb object-related regions. The feature disruptive attack [27] introduces an intermediate loss that alters the local activation of image features within the middle layer hidden outputs in target models, enhancing the transferabil-ity of perturbed features without requiring knowledge of their structures or parameters. Language Modality Attacks.\n\nAdversarial attacks have also emerged in natural language processing, impacting language model predictions through various manipulations at different levels. Character-level per-turbations [28]\u2013[30] involve character substitutions and visual character substitutions using swaps, deletions, insertions, and repetitions. These attacks utilize gradient-based and greedy-search methods. Word-level perturbations [31]\u2013[33] strive to replace key words with synonyms that have similar word and"}, {"title": "III. METHODOLOGY", "content": "In this section, we introduce the JMTFA, a unified adver-sarial attack approach that simultaneously perturbs a pair of image and text. This approach details the adversary's goals and capabilities to search feature importance through aggregated relevance attention scores. The importance of features as a key influential indicator in model predictions is then applied to generate adversarial visual and linguistic modality examples. As shown in Fig. 3, JMTFA enforces forward propagation to produce a composite of relevance attention scores to pinpoint key features in the fusion of vision and language, where important features of one modality are influenced by the other modality in cross-modal interactions. A detailed description is provided below.\n\nA. Recap Aggregated attention relevance scores\n\nWe briefly describe the motivation for selecting aggregated relevance scores of attention in [41]. The attention-aggregated relevance scores involve the Hadamard products of atten-tion scores and their gradients in each layer of multimodal transformers to characterize the influence of tokens on the prediction results.\n\nSingle-Stream Architecture. The aggregated attention rel-evance scores for a single-stream architecture are detailed in Equation 1. In this context, A represents the attention scores (product of queries matrix Q and keys matrix K within each attention layer), n denotes the total amount of attention layers, i represents the current layer, \\nabla A refers to the backpropagated gradients from the outputs to the attention layer, and \\odot denotes element-wise multiplication. The result of the element-wise multiplication is an attention-weight matrix, which is normalized as described in [42] and considers only the positive contributions as outlined in [43]. In addition, \\mathbb{E}_{h} represents the mean head values (average of positive normalized attention weight matrix across all heads for each attention layer), and \\hat{A} is the aggregated attention relevance score. With such accumulation, the aggregated attention relevance scores by a forward pass contribute to the feature importance.\n\n$$ \\hat{A} = \\sum_{i=1}^{n} [ \\mathbb{E}_{h} [(\\nabla A \\odot \\nabla A)^{+} ]_{norm},$$\n\nDual-Stream Architecture. Dual-stream architectures in-corporating both self-attention and cross-attention layers, are designed to extract meaningful image and text representation features by an independent transformer. This architecture enables each modality to be influenced by the other through preceding attention layers via matrix multiplication, and vice versa. Additionally, each modality retains influence from itself in both previous and current layers. It computes aggregated attention relevance scores that contextualize the vision and language modalities, which can be formalized as follows:\n\n$$ \\hat{A} = A_{s} + \\sum_{i=1}^{n} [ \\mathbb{E}_{h} [(A_{c} \\odot \\nabla A_{c})]_{norm}$$\n\nwhere $A_{s}$ represents the aggregated self-attention relevance scores for each modality, as detailed in Equation 2, $A_{c}$ is defined as the cross-attention scores derived from the cross-attention weights of the vision or language modality in each layer, $\\nabla A_{c}$ represents the gradients obtained through back-propagation, and the superscript 'c' indicates the contextually mutual interactive relevance from the other modality. Overall, \\hat{A} combines the aggregated self-attention relevance scores $A_{s}$ with the aggregated cross-attention relevance scores $A_{c}$, providing a comprehensive representation of the attention dynamics in the dual-stream architecture.\n\nB. Vision Modality Attack\n\nFor the JMTFA on the image modality, we utilize aggregated attention relevance scores to highlight object-related regions and mitigate interest in non-object regions. This approach of identifying feature importance through aggregated attention relevance scores motivates us to preserve the disrupted di-rection towards adversarial examples. Given this, we define"}, {"title": "C. Language Modality Attack", "content": "For the JMTFA on the language modality, we adopt BERT-Attack [44] to determine word importance using text ag-gregated attention relevance scores A, rather than relying solely on logits from the victim models. Although gradient-based attacks are efficient for images, they are challenging to implement for textual attacks due to naturalness constraints, including lexical, semantic, and grammatical considerations. BERT-Attack strategically applies the masked language model to generate various adversarial substitute candidate tokens, excelling in strong word replacement using contextual pertur-bations while retaining sentence semantics. Moreover, word-level attacks are more effective than sentence-level attacks and stealthier than character-level attacks [45].\n\nThe text attack of JMTFA comprises two stages: 1) Iden-tifying important words using language modality aggregated attention relevance scores to maximize the chance of incorrect predictions by the victim model, and 2) ensuring semantic integrity by generating top-K candidate tokens that strive to maintain semantic continuity and ensure minimal perturbation levels. The following Equation 5 demonstrates the JMTFA for generating text adversarial examples:\n$$ X'_{t} = arg\\ \\underset{x'_{t}}{max} \\ P(y' \\| x) \\ s.t. \\| x_{t} - x'_{t} \\|_{\\circ} \\leq \\epsilon, \\epsilon \\in \\hat{A} $$\nwhere $x'_{t}$ represents the adversarial text example, $x_{t}$ is the orig-inal text input, and $\\epsilon$ is the maximum perturbation, constrained to K tokens. This allows for the generation of adversarial text examples while maintaining semantic consistency and minimizing the number of modified tokens, as guided by the aggregated attention relevance scores.\n\nD. Vision and Language Modality Attack\n\nIn a white-box scenario, the adversary has complete knowl-edge of architecture and parameters of the victim VLP models, enabling the identification of aggregated relevance attention scores as feature importance. Perturbing the two modalities by combining Formulations in 4 and 5 strives to find vulnerabili-ties across the fused intrinsic feature space. BERT-Attack may not generate appropriate word substitutions or can change the prediction. To overcome this, JMTFA simultaneously targets the vision modality and dynamically update perturbations under the guidance of cross-modal interactions. The overall scheme of JMTFA in an adversarial manner is summarized in Algorithm 1. JMTFA formulates the problem of generating adversarial examples based on either single modality or two modalities. This approach allows for leveraging the interde-pendencies between vision and language modalities, leading"}, {"title": "IV. EXPERIMENTS", "content": "A. Models and Datasets\n\nIn our experiments, we primarily conduct experiments on these four VLP models in our paper: ViLT [12] and Vi-sualBERT [13], representing single-stream architecture, and VLE [14] and LXMERT [15], exemplifying dual-stream archi-tecture. Single-stream architectures integrate visual and textual embeddings into a unified sequence, utilizing self-attention for modality fusion. In contrast, dual-stream architectures use both cross-attention and self-attention layers to facilitate cross-modal fusion. These models differ in their image and text encoding methods. Specifically, ViLT employs the pixel-level visual transformer (ViT) [46] for its image encoding, while VLE utilizes CLIP-ViT [14]. On the other hand, VisualBERT and LXMERT rely on the regional-level pre-trained object detector Faster-RCNN [47]. For textual encoding, ViLT, Visu-alBERT, and LXMERT implement BERT [48], whereas VLE utilizes DeBERTa-v2 [49]. All selected VLP models process image-text pairs as inputs, with the primary distinction lying in their methods of encoding visual features. We use these VLPs from the Huggingface hub to evaluate the JMTFA on different architectures, focusing on fine-tuned public VLP models rather than direct inferences from off-the-shelf pretraining models due to their relatively lower classification performance.\n\nOur evaluation focuses on two critical tasks: Visual Ques-tion Answering (VQA) refers to vision and language under-standing and Visual Spatial Reasoning [50] (VSR) indicates vision and language reasoning. Understanding and reasoning capabilities are pivotal for multimodal AI systems that require deeper, sophisticated interactions between visual and linguistic domains, such as in-field robotics and autonomous driving. The VQA task requires accurately comprehending natural questions and real-world images to respond with precise answers, formulated as a classification problem with 3,129 predefined most frequent responses. Images in VQAv2 are from Microsoft COCO [51] and the dataset contains 65 types of open-ended questions, with answers classified into three types: yes/no, number, and other. Similarly, the VSR task predicts locations based on image details under textual tem-plate instructions, capturing complex relational information within an environment. This task requires recognizing spatial relations among objects in a manner akin to human cognition, articulated through natural language that encompasses intricate lexical and syntactic details. We experiment on the VQAv2 validation set using 5,000 natural language questions with associated images and ground truth answers. The VSR evalu-ation is performed on a test dataset comprising 1,000 image-text pairs and uses a zero-shot model that has no overlapping training location concepts. It is worth noting that no VSR fine-tuned model exists for VLE.\n\nB. Implementation Details and Metrics\n\nTo instantiate image attack, we resize the images to 384 \u00d7 384, except for VLE that uses 576 \u00d7 576 due to its default input settings. We adopt the PGD optimizer with a budget $\\epsilon$ of 4/255 or 8/255, a step size $\\alpha$ of 2/255, and a fixed total iteration number of 40. All perturbations are constrained within a bound of 8/255 under the $l_{\\infty}$ norm, except for VLE, where $\\epsilon$ is set to 14/255 based on empirical observations. For the text attack, we utilize the default settings of BERT-Attack. We adhere to the default number of candidate words as 48 and use the similar-words-filter matrix [32] for antonyms. The maximum perturbation $\\epsilon$ is set to K = 3 tokens. We investigate both single-source modality and dual-source modality attacks on each iteration to explore worst-case permutations.\n\nWe employ two primary metrics to evaluate JMTFA on victim VLP models: i) Attack Success Rate (ASR), which is the proportion of adversarial examples that effectively fool target VLP models among all correct predictions; and ii) Adversarial Accuracy, which is the classification rate on the whole testing dataset under adversarial attacks. We report the best attack performance achieved by adding perturbations to the legitimate image and text pair. These metrics effectively evaluate two-modality adversarial examples and their impact on the performance of the target VLP models.\n\nC. Attack Performance on VQA\n\nThe results of JMTFA on the VQA tasks are shown in Table I. We notice that LXMERT achieves the highest ASR of 68.06% in the JMTFA image attack, while VLE has the highest ASR of 80.50% in the JMTFA text attack. Additionally, we"}, {"title": "D. Attack Performance on VSR", "content": "We report the attack experimental results on the VSR task in Table III. Our observations indicate that the impact of attacking the vision modality is relatively weak for VSR. To address this, we introduce JMTFA-MSE, which adds a loss distance function to AFIA in Equation 3. JMTFA-MSE, designed to measure feature importance and semantic label distance when the vision modality attack, proves more effec-tive than FIA alone. Our findings are as follows: (1) Perturbing two modalities consistently outperforms perturbing a single source modality, aligning with our observations from the VQA challenge. (2) The language modality of JMTFA demonstrates stronger attacks than the vision modality on VLP models, sug-gesting that text perturbations have a more significant impact. (3) VisualBERT exhibits stronger adversarial robustness than ViLT and LXMERT, highlighting the importance of positional encodings in VLPs. This is evident in ViLT's use of patches for positional encodings and LXMERT's incorporation of object regions with position features."}, {"title": "V. ABLATION STUDY", "content": "Image Importance Feature Selection. Our visual attack in JMTFA can be applied to all network layers. We speculate that this behavior significantly impacts feature importance on model predictions. To investigate this, we compare the performance of different visual understanding levels. Fig. 8 (a) shows attack accuracy results by selecting various image feature layers. We distinguish between first self-attention, mid-dle self-attention (6th attention layer), and last self-attention layers in single-stream architectures, as well as last self-attention, first cross-attention, and last cross-attention layers in dual-stream architectures. The overall attack accuracy has dropped significantly compared to the original performance of victim VLP transformers. LXMERT achieves the highest attack accuracy, while VLE is the most vulnerable when the final layer features are used to represent an input image. Empirically, this observation confirms that the early layer preserves low-level information related to the ground truth, while the middle features are not sensitive enough to determine the final output. By contrast, the later layer learns salient semantic features that contribute to classification accuracy. Based on this observation, we select each victim model's final layer representation for image adversarial sample generation. Word Substitution Amount Ranking. We further analyze the influence of selecting K candidate word choices in tex-tual attacks. First, we consider the worst-case scenario of replacing all words in the sentence except for non-contributing"}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "In this paper, we introduce JMTFA, a novel approach that uses important features to execute visual and linguis-tic adversarial attacks on VLP transformers. This research gap raises important considerations regarding the impact of attacking different modalities on various VLP transformers. Extensive experiments demonstrate that the performance of the adversarial attacks with various settings and analyze key factors to enhance the effectiveness of attacks by two-modality. Our findings contribute to the understanding of vulnerabilities VLP transformers and offer valuable insights for developing more robust VLP models, highlighting the importance of considering both visual and linguistic aspects when designing defense mechanisms for multimodal AI systems. While our study focuses specifically on VLP transformer architectures, the underlying principles of our approach may have broader applicability adapting to across more different multimodal architectures, as the concept of extracting feature saliency tokens or patches. In the future, we seek to investigate the adversarial robustness of Multimodal Large Language Models against sophisticated attacks from malicious users."}]}