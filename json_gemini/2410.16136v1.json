{"title": "Modeling dynamic neural activity by combining naturalistic video stimuli and stimulus-independent latent factors", "authors": ["Finn Schmidt", "Suhas Shrinivasan", "Polina Turishcheva", "Fabian H. Sinz"], "abstract": "Understanding how the brain processes dynamic natural stimuli remains a fundamental challenge in neuroscience. Current dynamic neural encoding models either take stimuli as input but ignore shared variability in neural responses, or they model this variability by deriving latent embeddings from neural responses or behavior while ignoring the visual input. To address this gap, we propose a probabilistic model that incorporates video inputs along with stimulus-independent latent factors to capture variability in neuronal responses, predicting a joint distribution for the entire population. After training and testing our model on mouse V1 neuronal responses, we found that it outperforms video-only models in terms of log-likelihood and achieves further improvements when conditioned on responses from other neurons. Furthermore, we find that the learned latent factors strongly correlate with mouse behavior, although the model was trained without behavior data.", "sections": [{"title": "1. Introduction", "content": "Neural activity is influenced both by sensory stimuli and internal, stimulus-independent fluctuations (Stringer et al., 2019; Niell and Stryker, 2010; Reimer et al., 2016), which are often ignored by stimulus-based models. Existing approaches to neural response prediction largely fall into three main categories: (1) deterministic models that predict neural activity from visual stimuli, but neglect uncertainty and variability in responses (Wang et al., 2023a; Turishcheva et al., 2023; Sinz et al., 2018; Vystr\u010dilov\u00e1 et al., 2024; H\u00f6fling et al., 2022); (2) probabilistic models that derive latent embeddings from neuronal responses to predict behavior but do not account for external sensory inputs (Schneider et al., 2023; Gokcen et al., 2022; Sussillo et al., 2016; Yu et al., 2009); and (3) models that combine task inputs and a subset of neural activity to predict the responses of conjugate neurons (Zhou and Wei, 2020; Kim et al., 2023). To the best of our knowledge, few works exist that model the correlated variability of large neuronal populations, that also take visual stimuli as input (Bashiri et al., 2021). Specifically, none of them are designed for dynamic video-based stimuli. The flow based approach of Bashiri et al. (2021) can be computationally too expensive for modeling temporal dependencies in the latent variables. This paper addresses this gap by proposing a predictive model of dynamic neural activity as a function of naturalistic video stimuli and a temporally varying latent variable that captures correlated, stimulus-independent neural variability. We demonstrate that this model improves predictive accuracy of mouse V1 responses and implicitly learns latent variables that are correlated with behavior, which were not provided to the model during training."}, {"title": "2. Models", "content": "Our model predicts time-varying neuronal responses (2-photon calcium traces) $y \\in R^{N\\times T}$ to a video stimulus $x \\in R^{W\\times H\\times T}$, where $N$ is the number of neurons, $T$ the number of time points, and $W$ and $H$ are the width and height of one frame. The prediction additionally depends on a dynamic, stimulus-independent latent factor $z \\in R^{k\\times T}$ with latent dimension $k << N$ (Fig 1). Combining ideas from Bashiri et al. (2021) and Zhu et al. (2022), we model the distribution of neuronal responses conditioned on the stimulus and the latent factor as a Zero-Inflated-Gamma (ZIG) distribution (Wei et al., 2019):\n$P_{ZIG} (y \\mid x, z; \\psi) = \\prod_{i}^{N,T} (1-q_{it} (x, z; \\psi))^{\\delta_{y_{it}, 0}} + q_{it}(x, z; \\psi) (\\frac{y_{it}^{k_i - 1}}{\\Gamma(k_i)\\theta_{it}(x, z; \\psi)^{k_i} } )e^{-\\frac{y_{it}}{\\theta_{it}(x, z; \\psi) }} \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad(1)$,\nwhere $p$ is the mixture separation with no overlap, $k_i$ the shape parameter per neuron $i$, $q_{it}$ the nonzero response probability, and $\\theta_{it}$ the scale parameter for each neuron at time-step $t$. 0 and q are functions of the video-stimulus, the ZIG-model's parameters $\\psi$ and the latent factor while $k_i$ is fitted once per neuron:\n$q_{it} (x, z, \\psi) = sigmoid (f_\\phi^{(q)}(x; \\psi) + w_i^{(q)} \\cdot g (z_t; \\psi) ) \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad(2)$,\n$\\theta_{it}(x, z, \\psi) = ELU (f_\\phi^{(\\theta)}(x; \\psi) + w_i^{(\\theta)} \\cdot g (z_t; \\psi)) + 1. \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad(3)$\nWe model $f_\\phi^{(q)}$ and $f_\\phi^{(\\theta)}$ using a typical core-readout architecture (Fig 1A) where the core extracts features of the video-input, and the readout maps the relevant features from the core-output to the individual neurons (H\u00f6fling et al. (2022); Turishcheva et al. (2023); Vystr\u010dilov\u00e1 et al. (2024); Sinz et al. (2018); Wang et al. (2023a)). We additionally introduce $g$ as a one-layer recurrent neural network with gated recurrent units to capture correlations in the latents across time. The stimulus-independent latent factor is modeled with a prior distribution as a standard Gaussian independently across time $p(z) = N(0, I)$. We model an approximate posterior (Fig. 1C) as a Gaussian with the mean as function of the responses $y$, the encoder parameters $\\phi$ and an independent variance: $q(z|y; \\phi) = N(\\mu(y; \\phi), \\sigma I)$. We fit the model by maximizing its evidence lower bound (see Appendix D).\nBaseline models Additionally, we train (1) a video-only ZIG-model (without latent) that maps video-stimulus $x$ to dynamic responses $y$, and a (2) non-probabilistic model trained with the Poisson loss (Turishcheva et al. (2023)). All models share the same hyperparameters wherever possible (Fig 1). Since we aim to use correlations between latent variables and behavioral variables (Stringer et al., 2019; Bashiri et al., 2021) as external validation for the model, we excluded behavioral data during training for all models - in contrast to previous work (Sinz et al., 2018; Wang et al., 2023a)."}, {"title": "4. Discussion", "content": "In this work we showed that adding latent factors to a video encoding model enables the prediction of a joint dynamic response distribution and captures biological variables by implicitly learning correlations between latent factors and behavior. Future work could investigate modeling the latent state further, such as experimenting with the number of neurons needed, the optimal dimensionality k, and distributional assumptions in the generative model, potentially learning latent variables with temporal dependencies."}, {"title": "Appendix A. Related Works Overview", "content": "We give in 4 an overview of related work, which either predicts neuron responses for given natural stimuli, uses latent representations for encoding neuron responses or takes behavior of the animal into account."}, {"title": "Appendix B. Behavior Analysis", "content": "For the pupil dilation and the treadmill speed of each mouse we performed one CCA analysis each. The CCA analysis was done with 5-fold cross validation. We split the recording time with a 80/20 ratio. This was repeated on five different seeds. The correlations were computed between the CCA combination $\\Sigma_i w_i^{(cca)} z^{(i)}$ on the test time, where $w_i^{(cca)}$ are the CCA weights and $z^{(1)}, ..., z^{(k)} \\in R^T$ are the latent variables. For two selected videos and mice we plotted their normalized pupil dilation and treadmill speed against the corresponding normalized CCA combination of the latent over whole videos time, which correspond to ~ 300 time points (3)."}, {"title": "Appendix C. Training and Marginalization", "content": "Training In order to train our model, we maximize $P_{ZIG}(y|x)$ via its evidence lower bound (ELBO) via variational inference (Kingma and Welling, 2022; Blei et al., 2017):\n$log p_{ZIG} (y|x) \\geq \\langle logp(y|z, x) \\rangle_{z \\sim q_\\phi(z|y)} + D_{KL}(q_\\phi(z|y) || p(z)), \\quad \\quad \\quad \\quad(4)$\nwhere $\\langle \\cdot \\rangle$ represents expected value and $D_{KL}$ Kullback-Leibler divergence.\nMarginalization The marginalized performance of the latent model is obtained by calcu-lating:\n$p(y|x) = \\int p(y, z|x) dz \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad(5)$\n$= \\int p(y|x, z)p(z|x) dz \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad(6)$\n$= \\int p(y| x, z)p(z) dz \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad(7)$\n$ \\approx \\frac{1}{L} \\Sigma_i p(y|x, z^{(i)}) \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad(8)$\nThe last equality is obtained, since x and z are independent (Fig. 1 D). The last integral is approximated via Monte-Carlo sampling. We found 5000 samples to be sufficient for convergence (4)."}, {"title": "Appendix D. Model Architecture and Hyperparmeters Setting", "content": "The video processing part consists of a factorized 3D-CNN block followed by a Gaussian readout (H\u00f6fling et al. (2022)). Each convolutional layer consists of a factorized 3D convolution across spatial and temporal dimension followed by a batch normalization layer and an ELU activation function. We use a variational autoencoding approach for the latent representations. A dropout layer is applied to the neuron responses before they are fed into the encoder. This prevents the model from learning correlations between specific neurons thereby encouraging the learning of global latent representations. The encoder applies a linear layer reducing the dimensionality of neuron responses N \u2248 8,000 (N ranging from 7800 to 8200 depending on the mouse as part of the SENSORIUM dataset (Turishcheva et al., 2024)) to n << N. For each time step the same linear layer is applied independently. The linear layer is followed by a layer normalization and an ELU activation function. An individual linear layer was trained for each mouse. The output is processed by a one-layer recurrent neural network with gated recurrent units producing the means \u03bc(y; \u03c6) of the approximate posterior $q(z|y; \\phi) = N(\\mu(y; \\phi), \\sigma I)$. \u03c3is a learnable model parameter of the encoder. The resulting latent variables are decoded with another one-layer GRU, denoted as g. The GRUs are shared among the mice. g(z; \u03c6) is combined with the outputs from the video encoding part computing the parameters for the probability log p(y|z, x) as in 2, 3. For a graphical illustration see 1. We searched over various hyperparameters (Table 5) using Optuna (Akiba et al., 2019)."}, {"title": "Appendix E. Forecasting Model", "content": "For the forecasting, we consider the latent representations to be a Markov process developing smoothly over the time (Fig. 5). Thus, we trained a forecasting model, where we adapted variational autoencoder architecture to an autoregressive model by shifting the objective from reconstructing the original data point to predicting the subsequent time point as in Wang et al. (2023b). Hence, we minimize:\n$\\Sigma_t (\\langle log p(y_{t+1}|z_{t+1}, x, \\psi)\\rangle_{z \\sim q(z_{t+1}|y_t, \\phi)} + D_{KL}(q(z_{t+1}|y_t, \\phi) || p(z_{t+1})) \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad(9)$\nAgain, p(yt+1|zt+1, x, y) is the ZIG likelihood of responses at time t + 1 given a latent zt+1, a video x and y are the parameters of the video-encoding model (Fig. 1B). $q(z_{t+1}|y_t, \\phi) = N(\\mu(y_t, \\phi), \\sigma I)$ is the approximate posterior of the future latent at time zt+1 given the responses Yt."}]}