{"title": "IMPLEMENTING STREAMING ALGORITHM AND K-MEANS CLUSTER TO RAG", "authors": ["Haoyu Kang", "Yukun Zhong", "Yuzhou Zhu", "Ke Wang"], "abstract": "Retrieval-augmented generation (RAG) has achieved great success in information retrieval to assist\nlarge models because it builds an external knowledge database. However, it also has many problems:\nit consumes a lot of memory because of the huge database. When faced with massive streaming data,\nit is unable to update the established index database in time. To save the memory of building the\ndatabase and maintain accuracy simultaneously, we proposed a new approach combining a streaming\nalgorithm and k-means cluster with RAG. Our approach applies a streaming algorithm to update the\nindex and reduce memory consumption. Then use the k-means algorithm to cluster documents with\nhigh similarities together, the query time will be shortened by doing this. We conducted comparative\nexperiments on four methods, and the results show that RAG with streaming algorithm and k-means\ncluster performs well in accuracy and memory. For massive streaming data, we find that our method\nbehaves better than traditional RAG.", "sections": [{"title": "Introduction", "content": "Retrieval-augmented generation(RAG) is a mechanism to solve the problem of handling queries beyond pre-trained\ndata or requiring current information. [1] Firstly it loads the raw data and splits the input text into chunks, then embeds\nchunks into vectors and stores vectors in the index database. Secondly, it transforms the input query into a vector,\nretrieves the top-k documents that are most similar to the query, and then synthesizes the retrieved documents and the\nquery together into a new prompt as a new input to LLMs. Finally, the model will generate better answers based on\nthe new input prompt that provides extra information to the pre-trained LLMs.\nBut with the growing volume of data, there are two main problems that the traditional RAG model faces: The first\none is memory cost, storing large amounts of data in the index database will cost a huge memory. The second one is\nthe lack of updating the database in time, so if the data like new articles change frequently, the traditional RAG shows\npoor accuracy.\nWhat we are doing is to solve these two problems. We implementing a transformed heavy hitters streaming algorithm\nto RAG to save memory by discarding the documents that are almost impossibly retrieved. Our method filters the\ndatabases, leaving documents that are more likely to be the result of a query. Implementing the streaming algorithm\ncan also update the index database to improve the low precision of traditional RAG when dealing with massive data\nthat are frequently changing. Then we combined the streaming algorithm with the k-means cluster algorithm together\nto further enhance the performance. We use the k-means cluster algorithm to cluster similar documents together\nin advance after the selection of the streaming algorithm, and it optimizes the query time because it can retrieve\ndocuments from K clusters rather than the whole index database while inputting a new query.\nWe ran our experiments on a news dataset with constantly updated messages, and we calculated the precision of\nthe query for the four methods. Our experiments' results show that implementing the k-means cluster algorithm\nand streaming algorithm can effectively save a lot of memory and exhibit greater accuracy when the number of\nqueries is large. Our results demonstrate that the streaming algorithm optimizes the memory space while\nmaintaining the accuracy."}, {"title": "Related Work", "content": ""}, {"title": "The evolution of RAG and its problems", "content": "RAG was first proposed in the article \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" [2]. This\narticle illustrated the limited ability of LLMs to access and precisely manipulate knowledge, so LLMs' performance\non knowledge-intensive taskslags behind task-specific architectures. Plus, pre-trained LLMs can not easily expand\nor revise their memory and they encounter challenges like hallucination, outdated knowledge, and non-transparent,\nuntraceable reasoning processes. [1] So it applies this information retrieval mechanism, Retrieval-augmented gener\nation, to LLMs to solve the problems that pre-trained LLMs face. This model called Naive RAG, builds an\nexternal knowledge database and retrieves relevant documents to help LLMs. However it has some drawbacks\nsuch as the precision of the query being low, leaving out important information, and so on.\nThe emergence of Advanced RAG was aimed at improving those shortcomings of RAG. [3] Advanced RAG enhances\nits indexing techniques by employing a sliding window approach, fine-grained segmentation, and the integration of\nmetadata. Furthermore, it integrates multiple optimization methods to streamline the retrieval process. Each com\nmunity will generate a community answer and then incorporate it into a global answer. They improved the\nRAG's performance on the comprehensiveness and diversity of answers.\nMicrosoft found that Naive RAG is not good at extracting global answers and fails on query-focused summarization\ntasks(QFS). So they proposed an approach that builds the knowledge graph based on the extracted entities and rela\ntionships in the sentence, then divides the graph into communities by the Leiden algorithm.[4] Each community\nwill generate a community answer and then incorporate it into a global answer. They improved the RAG's\nperformance on the comprehensiveness and diversity of answers.\nInspired by previous research, we realized that the proposal of traditional RAG is significant, but there still exists a lot\nof things to improve. What we want to do is to save the huge memory cost and the huge amount of time spent on each\nquery of relevant documents."}, {"title": "Streaming Algorithms", "content": "The streaming algorithm was first regularised and popularised in the paper \"The Space Complexity of Approximating\nthe Frequency Moments\". [5] Streaming algorithms process the streaming data where the input is a boundless and\nordered sequence of items which can usually be handled in a few passes, and only once in most cases.[6] However,\nin most cases the true class label of streaming data is probably unavailable and there is no prior information on the\nnumber of the classes, so the clustering algorithm is a good option to cluster and define the class label of the streaming\ndata. [7]\nGenerally, the streaming algorithm adapts traditional algorithms to function effectively with streaming data when it\ncomes to some limitations such as limited memory, single-pass processing, and real-time analysis. The Streaming\nclustering methods can be classified into five categories: partitioning, hierarchical, density-based, grid-based, and\nmodel-based.[8]\n\u2022 Partitioning based methods Streaming data is categorized into a pre-defined number of clusters according to\nthe distance or similarity to the center of the clusters. Specifically, Incremental k-means[?], StreamKM++[9],\nand SWClustering[10] belong to this method.\n\u2022 Hierarchical based methods These algorithms apply Binary Tree and can be sorted into two categories:\nAggregate and Split algorithms. Aggregate algorithms assume that every item forms a cluster initially, and\ngradually merge the items (subclusters) to form new larger clusters. However, Split algorithms consider that\nall the streaming data comprises a cluster and progressively split it up. Birch[11],CluStream[12],HUEStream\n[13] are typical Hierarchical based methods.\n\u2022 Density-based methods These methods summarise input data into a large number of microclusters, which\nare then updated based on density accessibility and connectivity. DenStream[14], OPTICS-Stream[15] and\nD-Stream[16] are some popular methods of this kind.\n\u2022 Grid based methods The whole workspace is divided into numerous grid units, each of which contains a\ndata item. Then we cluster the grid units according to their density.MR-Stream [17] and CellTree[18] are\ndesigned based on the referred methods.\n\u2022 Model based methods These methods focus on finding the data distribution model that best fits the input\nstreaming data, such as SWEM[19] and GCPSOM[20].\nIdentifying Heavy Hitters, often referred to as finding the top k popular or frequent items, is a well-researched\nand important problem in streaming data. [21] The two main kinds of Heavy Hitters algorithms are methods based\non the counter and those based on the sketch. The former methods keep track of the frequent elements by main-\ntaining an array of counters directly calculating the numbers. Misra-Gries[22], Lossy Counting[23], and Space\nSaving[24] belong to the algorithms based on counters. Methods based on sketch apply probabilistic data struc-\ntures such as Hash Functions and Bitmaps to describe the streaming data, providing frequency estimates with less\nmemory consumption.CountSketch[25], CountMinSketch[26], Multi-stage Bloom Filters[27], and Sketch-guided\nSampling[28] are part of these methods.\nAfter realizing the power of the streaming algorithm, we want to use the heavy hitter algorithm to build and update the\nindex database to solve the problem that RAG consumes a lot of memory."}, {"title": "Method and Algorithm", "content": ""}, {"title": "Dataset", "content": "The Dataset that we use is at Kaggle [29], it contains more than sixteen thousand pieces of news and comments and 23\nfeatures from January 1, 2020-December 31, 2020. We selected three features to help us retrieve relevant documents:\nheadlines, keywords, and abstracts. We combined these three features into one feature and then embedded the new\ncombined feature into a vector. To calculate the accuracy of our input query, we also manually labeled the dataset, we\nmarked the documents that are related to the query we entered as 1 and we marked the rest as 0. The accuracy of our\nmodel is calculated as follows:\nAccuracy = $\\frac{TP}{TP + FP}$\n(1) where\n\u2022 TP (True Positives): The number of retrieved documents labeled as 1, which are indeed relevant.\n\u2022 FP (False Positives): The number of retrieved documents labeled as 0, which are not relevant."}, {"title": "Algorithm", "content": "Traditional RAG costs huge memory because it builds a very large database to store the documents. Therefore, if\nwe want to optimize the memory, it is evident that we can try to reduce the size of the big database. According to\nprevious research[8], streaming algorithms work effectively on reducing memory, so we came up with an idea to apply\na streaming algorithm to RAG to see if it works. Heavy hitters is a streaming algorithm used to filter and retain data\noccurring more frequently. The streaming algorithm in our method is a transformed heavy hitters algorithm, Figure 2\nis a workflow that describeshow our algorithm works. In the preparation of our algorithm, we embed the query into\na vector, split the input data into chunks, and set up a new data structure to store the documents, which only cost 10%\nof the memory of the Naive RAG database.\nD\u2081 = Embedding(d\u2081) \u2208 Rm\nQ = Embedding(q) \u2208 Rm\nwhere:\n\u2022 Di is the result of embedded data\n\u2022 Q is the result of embedded query\nIn the initialization of our algorithm, we read the streaming data to fill the new data structure. Then read the data\ncontinuously and implement the transformed heavy hitter algorithm: For every input chunk, we calculate the highest\ncosine similarity in this chunk and use it to replace the document with the lowest similarity in our new database. When\nthe streaming data ends, we get the final database that stores all the documents which are filtered by the transformed\nheavy hitter algorithm.\nCosSim(Q, D\u2081) = $\\frac{Q. Di}{||Q||||Di ||}$\nPHH (diq) = \u03c3 (\u03b1\u00b7 CosSim(Q, D\u2081) \u2013 \u03b2)\nwhere:\n\u2022 \u03c3(x) = $\\frac{1}{1+e^{-x}}$ is the sigmoid function, and a and \u1e9e are hyperparameters.\nAlgorithm 1 is the pseudo-code of our method. The above describeshow streaming RAG works, but there still exists\na problem, whenever we deal with a new query and retrieve relevant documents, we always need to sort the\nsimilarity of all documents, it is unnecessary to take so much time to do this. So we think about clustering similar\ndocuments together in advance, using the k-means cluster algorithm to cluster n documents intom classes. where:\n\u2022\nn is the number of all the documents in the database.\n\u2022\nmis the number of clustered classes\n\u2022\nn << m\nThe following formulas describe the clustering process and retrieval of the top-k documents:\nEuclideanDistance(Q, D\u2081) = $\\sqrt{2(1-Cos Sim(Q, D\u2081))}$\nCj = $\\frac{1}{ |Hj|} \\sum_{\u00a1EHj} P_i$\nCosSim(Q,Cj) = $\\frac{QCj}{||Q||||C\u0632 ||}$\nPtop-k(diq) = $\\sum_{jetop-k(CosSim(Q,Cj))} PHH(diq) Pkmeans (di Cj)$\nPRAG-Token(y|x) \u2248 $\\prod_{i=1} \\sum_{zEtop-k(p(x))} Pn (Zix)Po (Yi X, Zi, Y1:i-1)$\nwhere:\n\u2022 Hj is the index set of the j-th cluster\nIn practice, you will deal with many queries, and then the time cost of clustering in advance can be ignored. The\ntime complexity of searching of traditional RAG model is O(N\u00b7nlogn), but Streaming algorithm and K-means\nRAG is O(Nmlogm) where:\n\u2022 N is the number of queries."}, {"title": "Experiments", "content": "We did four experiments to verify the good performance of our method. All of these experiments are based on our\ndataset and input query \"Biden decided to run for the Presidential Election of 2020\". And then generate the results.\nOur experiments aim at measuring the memory and accuracy performance of four methods, comparing the effects of\nthe k-means cluster algorithm between two methods with streaming algorithm and without streaming algorithm, and\nexploring the influence of different clusters on accuracy. What's more, we experiment to find the balance between\nmemory and accuracy."}, {"title": "Accuracy of four methods", "content": "In this experiment, We input the query into four methods and calculated the accuracy of the result of retrieving different\nquery numbers. We draw a figure of the variation in the accuracy of four methods with different query numbers from\n1 to 100 to better describe the performance of the four methods."}, {"title": "Comparative experiments on clustering effects", "content": "To illustrate why we combined the streaming algorithm with the k-means cluster algorithm together, we do this exper-\niment to compare the effects of the k-means cluster algorithm between two methods with the streaming algorithm and\nwithout the streaming algorithm. We also plotted the clustering effect and used the contour coefficient to measure the\nmetrics of the clustering algorithm. The formula for calculating the contour coefficient for the clustering effect is as\nfollows:\nA(i) = $\\frac{1}{|Ci|-1} \\sum_{jECi,ji}D(i, j)$\nCk = arg $\\min_{CEC} \\frac{1}{|C|} \\sum_{jEC}D(i, j)$\n$\\frac{1}{|Cemic} CD(i, j)$\nB(i) = $\\frac{1}{Cemic CD(i, j}$\nS(i) = $\\frac{B(i) - A(i)}{max A(i), B(i)}$\nwhere:\n\u2022 A is the mean distance between a sample and all other points in the same cluster.\n\u2022 B is the mean distance between a sample and all points in the nearest cluster that the sample is not a part of.\n\u2022\nCi is the cluster of the point i\nThe silhouette coefficient (s(i)) ranges from -1 to 1, where a higher value that is close to 1 indicates that the sample is\nbetter matched to its cluster and poorly matched to neighboring clusters and a lower value that is close to -1 indicates\nthat the sample is incorrectly divided.[30]"}, {"title": "The effect of different clusters on accuracy", "content": "The number of clusters has a big impact on the results of a query, to find the most appropriate number of clusters, We\ncalculated the accuracy of the number of different clusters for the same number of retrievals. Plus, we fit a curve of\nthe logarithm of accuracy as a function of the number of clusters."}, {"title": "Results", "content": ""}, {"title": "Accuracy of four methods", "content": "Figure 3 shows the result of four methods, we can find that when dealing with a large number of queries, the streaming\nalgorithm with RAG outperforms Naive RAG in accuracy. Table 1 also shows that the streaming algorithm with RAG\nonly occupies 10% memory of Naive RAG. We have discussed that using the k-means cluster algorithm can avoid\nsorting the documents for every query in the method part. However, we can find that only using the k-means cluster\nalgorithm resulted in a particularly low accuracy. We find that combining the streaming algorithm with k-means to\nRAG(SKR) can not only reduce the memory but maintain the accuracy. Our result 5.2 explains why could SKR solve\nthe problem of the deterioration of the accuracy of the k-means cluster algorithm.\nAt the early stage, Naive RAG and streaming RAG have a good performance, when k-means RAG and SKR are not\nvery well because of the clustering. However, SKR went well after that and became closer to the Naive RAG thanks to\nthe streaming process. At the late stage, SKR's and streaming RAG's accuracy are higher than Naive RAG due to their\nadvantage in processing false negative(FN) information, which shows the high quality of stability in a large number of\nqueries."}, {"title": "Comparative experiments on clustering effects", "content": "Figure 4b shows the clustering results when applying the streaming algorithm. By observing Figure 4a, we can\nintuitively see that the clustering results with the streaming algorithm are significantly better than those without it.\nAdditionally, we can confirm this conclusion by calculating the silhouette coefficients of both clustering methods. The\nresults of Figure 3 and Figure 4 together explain why the k-means cluster algorithm should not be used alone and why\nit should be combined with the streaming algorithm."}, {"title": "The effect of different clusters on accuracy", "content": "This experiment explored the relationship between the accuracy of retrieval and the number of clusters. We can observe\nthat as the number of clusters increases, the retrieval accuracy also increases, but the rate of improvement slows down\nand the time cost of k-means clusters increases too. Additionally, we can find that when n > 800, the accuracy grows\nvery slowly. So we choose a number between 600 800 to better balance the accuracy and time efficiency."}]}