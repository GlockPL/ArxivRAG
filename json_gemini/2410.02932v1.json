{"title": "Intrinsic Evaluation of RAG Systems for Deep-Logic Questions", "authors": ["Junyi (Edward) Hu", "You Zhou", "Jie Wang"], "abstract": "We introduce the Overall Performance Index (OPI), an intrinsic metric to evaluate retrieval-augmented generation (RAG) mechanisms for applications involving deep-logic queries. OPI is computed as the harmonic mean of two key metrics: the Logical-Relation Correctness Ratio and the average of BERT embedding similarity scores between ground-truth and generated answers. We apply OPI to assess the performance of LangChain, a popular RAG tool, using a logical relations classifier fine-tuned from GPT-40 on the RAG-Dataset-12000 from Hugging Face. Our findings show a strong correlation between BERT embedding similarity scores and extrinsic evaluation scores. Among the commonly used retrievers, the cosine similarity retriever using BERT-based embeddings outperforms others, while the Euclidean distance-based retriever exhibits the weakest performance. Furthermore, we demonstrate that combining multiple retrievers, either algorithmically or by merging retrieved sentences, yields superior performance compared to using any single retriever alone.", "sections": [{"title": "1 INTRODUCTION", "content": "A RAG system typically consists of two major components: Indexing and Retrieval. The former is responsible for indexing a reference text document before any queries are made to it. The latter is responsible for retrieving relevant data from the indexed document in response to a query and passing that information, along with the query, to a large language model (LLM) to generate an answer. The Retrieval component is typically a framework that supports a variety of retrieval methods, each referred to as a retriever.\nTo assess the effectiveness of a retriever in uncovering the logical relationship for an answer to a query with respect to the reference document, we introduce the Overall Performance Index (OPI). This metric measures both the correctness of the answers generated by an LLM and the accuracy of the logical relations produced by a classifier. The OPI is calculated as the harmonic mean of the BERT embedding similarity between ground-truth and generated answers, and the logical-relation correctness ratio.\nTo demonstrate the effectiveness of the OPI metric, we use the RAG-Dataset-12000 provided by Hug-"}, {"title": "2 PRELIMINARIES", "content": "The technique of RAG was introduced by Lewis et al. (2020) [Lewis et al., 2020] a few years before the widespread adoption of LLMs. The performance of a RAG system relies on the quality of the underlying retriever and the ability of the underlying LLM.\nLangChain is a popular RAG tool, which divides a reference document into overlapping text chunks of equal size. The suffix of each chunk overlaps with the prefix of the next.\nTo the best of our knowledge, no previous research has comprehensively evaluated the performance of RAG systems in the context of deep-logic question answering.\nGiven below are seven common sentence retrievers supported by LangChain:\nDPS (dot-product similarity) converts a query and a text chunk as BERT-based [Devlin et al., 2018] embedding vectors and compute their dot product as a similarity score. It returns k chunks with the highest scores to the query. (DPS in LangChain is referred to as Cosine Similarity.)\nKNN (k-Nearest Neighbors) in LangChain is the normalized dot-product similarity by the L2-norm, which is widely referred to as the cosine similarity. It returns k chunks with the highest cosine similarity scores to the query.\nBM25 [Robertson and Zaragoza, 2009] is a probabilistic information retrieval model that ranks documents based on the term frequency in a chunk and the inverse chunk frequency in the reference document. Let q be a query, T a chunk of text, $f(t_i,T)$ the frequency of term $t_i$ in T, |T| the size of T, avgTL the average chunk length, N the total number of chunks, and n($t_i$) the number of chunks that contain $t_i$. Then BM25(q, T) is defined by\n$BM25(q, T) = \\sum_{i=1}^n \\ln (\\frac{N-n(t_i)+0.5}{n(t_i)+0.5}) \\cdot \\frac{f(t_i, T) \\cdot (\u043a+1)}{f(t_i,T) +\u03ba\\cdot (1-b+b\\frac{|T|}{avgTL})}$,\nwhere k and b are parameters. Return k chunks of text with the highest BM25 scores to the query.\nSVM (Support Vector Machine) [Cortes and Vapnik, 1995] is a supervised learning model that finds the hyperplane that best separates data points in a dataset. To use SVM as a retriever, first represent each chunk of text as a feature vector. This can be done using word embeddings, TF-IDF, or any other vectorization method. Then use the labeled dataset to train an SVM model. Convert the query into the same feature vector space as the chunks. Apply the SVM model to the query vector to produce a score that indicates how similar the query is to each chunk. Extract k chunks with the highest scores.\nTF-IDF [Sammut and Webb, 2011] measures the importance of a word in a chunk of text relative to the set of chunks in the reference document, combining term frequency and inverse chunk frequency. In particular,\nTF-IDF(t, T) = TF(t,T) \u00d7 IDF(t),"}, {"title": "3 OVERALL PERFORMANCE INDEX", "content": "Let A and LR denote, respectively, the ground-truth answer and logical relation to the question with respect to the question Q, the context C, and the retrieved sentences S. Let A' and LR' denote, respectively, the answer and the logical relation generated by a RAG system with an LLM. We represent A and A' using BERT embeddings and compute the cosine similarity of the embeddings.\nFor a given dataset D with respect to a particular logical relation LR, let BERTSimD denote the average BERT similarity scores of all (A,A') pairs and LRCRD (logical-relation correctness ratio) denote the proportion of data points where the predicted logical relation matches LR. Namely,\n$LRCR_D = \\frac{|\\{d \\in D | LR = LR'\\}|}{|D|}$ (1)\nThe OPI for dataset D is defined by the follow-ing parameterized harmonic mean of BERTSimD and LRCRD, similar to defining the F-measure [Lewis and"}, {"title": "4 EVALUATION", "content": "As seen in Table 1, the data points in RAG-Dataset-12000 are unevenly distributed across the 13 logical relations, with significant disparities, such as only 106 data points in Fuzzy Reasoning compared to 6,920 data points in Direct Matching. To fine-tune GPT-40 and construct a classifier for identifying logical relations, a balanced dataset is preferred. To achieve this, we randomly select 100 data points from each logical relation category, forming a new dataset called RAG-QA-1300 that consists of 1,300 data points. This dataset is then split with an 80-20 ratio to create a training set and a test set.\nFine-tuning was performed by combining the context, question, and answer from each data point into a cohesive input text, labeled with its corresponding logical relation. The process involved approximately 800 training steps, resulting in a validation loss of 10-4. This specific checkpoint was selected for its optimal performance.\nThe fine-tuned GPT-40 classifier for logical relations significantly improves the accuracy to 75.77% on the test set, compared to 49.23% when using GPT-40 out-of-the-box without fine-tuning.\nWe used LangChain with the seven common retrievers mentioned in Section 2. We used GPT-40 to generate answers and the fine-tuned GPT-40 classifier to generate logical relations. LangChain supports a wide range of retrievers and allows for the seamless integration of pre-trained LLMs."}, {"title": "4.1 Intrinsic Evaluation", "content": "We set the chunk size to 100 (words) with a chunk overlap of 20 % in the setting of LangChain, where paragraph breaks, line breaks, periods, question marks, and exclamation marks are set to be the separators. These settings were fed into the LangChain function RecursiveCharacterTextSplitter to split a reference document into chunks, where each chunk contains up to 100 words, ending at a specified separator to break naturally such that the chunk is as large as possible, and adjacent chunks have a 20% overlap.\nWe used the default settings for each retriever to return four chunks in the context with the best scores-highest for similarity and ranking measures, smallest for distance measures-from the underlying retriever as the most relevant to the query. We then converted the four chunks extracted by the retriever back into complete sentences as they appeared in the original article. These sentences and the query were then fed to GPT-40 to generate an answer. Moreover, we instructed GPT-40 to determine the logical relationship for the answer with respect to the input text.\nWe consider the accuracy of the generated answers and logical relations to be equally important.\n$OPI_{(B)D} = \\frac{(1+B^2) \\cdot BERTSimp_D \\cdot LRCR_D}{(B^2 \\cdot BERTSimp_D) + LRCR_D}$ (2),\nwhere L is the set of the 13 logical relations, and BERTSime and LRCRe denote, respectively, the cor-responding BERTSim score and LRCR value for the logical relation l.\nAn alternative is to calculate the average OPI-1 score across all logical relations. While this differs slightly from Formula (3), the difference is minimal. We prefer Formula (3) for practical efficiency, as it bypasses the need to compute individual OPI-1 scores for each logical relation when these scores are not needed in applications, streamlining the process and reducing unnecessary computations."}, {"title": "4.2 Extrinsic Evaluation", "content": "The extrinsic evaluation uses a 0-3-7, 3-point scoring system to score A' for each pair (A,A'), where A is the ground-truth answer and A' is the answer generated directly by GPT-40 based on the question Q and the extracted sentences in the corresponding data point. The scoring system works as follows:\n\u2022 A' receives 7 points if it exactly matches the mean-ing of A.\n\u2022 A' receives 3 points if it partially matches themeaning of A, with or without extra informationnot contained in A.\n\u2022 A' receives 0 points if there is no meaningfulmatch.\nThis scoring system was designed to simplify extrinsic evaluation while widening the gaps between per-fect, partial, and no matches, providing a more nu-"}, {"title": "4.3 Combining Multiple Retrievers", "content": "LangChain supports combining multiple retrieversinto a new retriever. We use the default setting to return four chunks for each combination. This approach diversifies the retrieved content from the reference document, potentially improving overall performance.\nAs examples, we combine all seven retrievers, denoted as A-Seven; three retrievers with the highest OPI-1 scores: kNN, DPS, and TF-IDF, plus MMR for its strength in balancing relevance and diversity, denoted as A-Four; and two retrievers with the highest OPI-1 scores: KNN and DPS, denoted as A-Two.\nWe may also combine the sentences retrieved by individual retrievers, removing any duplicates, and use the remaining set of sentences with the corresponding questions to generate answers and logical relations. Let S-Seven, S-Four, and S-Two denote the sets of sentences obtained this way by the correspond-ing retrievers as in A-Seven, A-Four, and A-Two.\nThe experimental results of both types of combinations are shown in Table 4."}, {"title": "5 ANALYSIS", "content": "We first analyze the performance of individual retrievers, followed by examining the combinations of retrievers and the sentences retrieved by multiple retrievers."}, {"title": "5.1 Individual Retrievers", "content": "For each retriever, we first analyze the performance for each logical relation individually and then assess the overall performance across all logical relations."}, {"title": "5.1.1 Individual logical relation", "content": "We use the OPI-1 scores to help identify the strengths and weaknesses of individual retrievers across the 13 logical relations. For example, as seen in Table 2, almost all retrievers tend to perform the worst on adversarial reasoning, followed by fuzzy reasoning. For other logical relations, the performance of retrievers varies, indicating that certain retrievers may be more suited to specific types of reasoning tasks while struggling with others. For example, even for the worst-performing retriever, EDI, which consistently ranks the lowest in both extrinsic and intrinsic evaluations of answer accuracy as seen in Table 3, it still performs best on deductive reasoning. This suggests that while EDI may generally be less effective across various logical relations, it has a particular strength in handling tasks that involve deductive reasoning. This example highlights the nuanced performance of retrievers, where even a generally weaker retriever can excel in specific logical tasks. This variability in performance highlights the importance of selecting the appropriate retriever."}, {"title": "5.1.2 Across all logical relations", "content": "The average OPI-1 scores provide a means to identify, across all 13 logical relations, which retrievers are more suitable for specific tasks and which retrievers should be avoided. For example, as shown in Ta-ble 2, EDI has the lowest and SVM the second-lowest average OPI-1 scores, indicating they should generally be avoided. This is likely due to the limitations of the underlying features used to compute SVM scores and the coarseness of L2-norms when representing text chunks as bag-of-word vectors, which may fail to capture the nuanced relationships required for deep logical reasoning tasks.\nOn the other hand, KNN has the highest and DPS the second-highest average OPI-1 scores, indicating that these retrievers would be the best choices for answering deep-logic questions. kNN (cosine similarity) and DPS are similar measures, with kNN being a normalized version of DPS, which explains their comparable performance. However, kNN takes slightly more time to compute than DPS, as DPS is the fastest among all seven retrievers-dot products are the simplest and quickest to compute compared to the operations used by other retrievers.\nThe MMR retriever allows GPT-40 to generate better answers across all logical relations, as shown in Table 3. However, it does not perform as well in producing the correct logical relations. This discrepancy may be attributed to MMR's focus on balancing relevance and diversity in retrieved content, which improves answer quality but doesn't necessarily align with capturing accurate logical relations.\nBM25 is in general more effective for retrieving longer documents in a document corpus with the default parameter values for k and b. However, to retrieve sentences from an article, it was shown that BM25 would should use different parameter values [Zhang et al., 2021]. This explains why BM25 is the second worse for generating answers as shown in Table 3 by both extrinsic and intrinsic evaluations. It is not clear, however, why it produces a relatively higher LRCR value.\nTF-IDF's performance falls in the middle range, which is expected. As a frequency-based approach, it may struggle to capture deeper semantic information, but it remains relatively effective because it retains lexical information, ensuring that important terms are still emphasized in the retrieval process."}, {"title": "5.2 Performance of Various Combinations", "content": "We first analyze the performance of combinations of retrievers versus individual retrievers, followed by an analysis of combining retrievers algorithmically ver-sus combining sentences retrieved by individual re-trievers within the combination."}, {"title": "5.2.1 Combinations vs. individuals", "content": "It can be seen from Table 4 that A-Seven outperforms A-Four, which in turn outperforms A-Two. A similar ranking is observed with S-Seven, S-Four, and S-Two. Moreover, both A-Seven and A-Four are substantially better than the top performer, kNN, when only a single retriever is used (see both Tables 2 and 4). A similar result is observed with S-Seven and S-Four, where combining more retrieved sentences from different retrievers also enhances performance, reinforcing the benefits of increased diversity in the retrieval process.\nThese results all confirm the early suggestion that combining more retrievers generally enhances performance in both algorithmic and sentence-based com-binations, supporting the idea that diverse retrieval methods contribute positively to the overall effective-ness of the RAG system.\nHowever, we also observe that some combinations of retrievers may actually lead to poorer performance compared to using the individual retrievers alone. This is evident in the case of A-Two and S-Two, the algorithmic and sentence combinations of KNN and DPS, both result in slightly lower average OPI-1 scores than kNN alone. This is probably due to the fact that KNN and DPS are very similar measures, and combining them doesn't significantly increase diversity. Worse, the extra information pro-vided through their combination seems to have led to diminishing returns, negating the potential benefits of combining retrievers to improve performance. This phenomenon warrants further investigation.\nNevertheless, combining retrievers based on different retrieval methodologies could help increase diversity and, consequently, improve overall performance. This is evident in the case of A-Seven and S-Seven, which combine retrievers utilizing diverse retrieval methods, as well as in A-Four and S-Four, where MMR-a retrieval method that balances rel-evance and diversity-complements kNN. By leveraging varied retrieval techniques, we can ensure that a broader range of relevant content is retrieved, potentially leading to greater accuracy and more robust logical reasoning in the generated answers."}, {"title": "5.2.2 Combining algorithms vs. combining sentences", "content": "We compare the outcomes of combining retrievers at the algorithm level versus the sentence level. Combining retrievers at the algorithm level is a feature supported by LangChain, which returns the same default number of chunks before sentences are extracted. In contrast, combining retrievers at the sentence level involves merging sentences retrieved"}, {"title": "6 FINAL REMARKS", "content": "This paper presents an effective intrinsic evaluate method for the performance of RAG systems in connection to question-answering involving deep logical reasoning.\nLangChain supports a wide range of retrievers and allows users to integrate custom retrievers. Additionally, there are numerous large language models (LLMs) such as the Gemini series [Google, 2024], LlaMA series [Meta, 2024], and Claude series [Claude AI, 2024], among others, as well as various retrieval-augmented generation (RAG) tools like LLAMAINDEX [LlamaIndex, 2024], HayStack [Deepset, 2024], EmbedChain [EmbedChain, 2024], and RAGatouille [AnswerDotAI, 2024]. Evaluating the performance of these models and tools, particularly for answering deep-logic questions where identifying logical relations is essential, represents an intriguing direction for future research.\nRegularly reporting the findings of such investigations would significantly contribute to the advancement of RAG technologies. Furthermore, we aim to develop a tool that quantitatively assesses the depth of logical relations in question-answering systems relative to the underlying context. This effort would ne-cessitate the creation of a new dataset that annotates the depth of each logical relation for every triple con-"}]}