{"title": "Deep Learning-Based Facial Expression Recognition for the Elderly: A Systematic Review", "authors": ["F. Xavier Gaya-Morey", "Jose M. Buades-Rubio", "Philippe Palanque", "Raquel Lacuesta", "Cristina Manresa-Yee"], "abstract": "The rapid aging of the global population has highlighted the need for technologies to support elderly, particularly in healthcare and emotional well-being. Facial expression recognition (FER) systems offer a non-invasive means of monitoring emotional states, with applications in assisted living, mental health support, and personalized care. This study presents a systematic review of deep learning-based FER systems, focusing on their applications for the elderly population. Following a rigorous methodology, we analyzed 31 studies published over the last decade, addressing challenges such as the scarcity of elderly-specific datasets, class imbalances, and the impact of age-related facial expression differences. Our findings show that convolutional neural networks remain dominant in FER, and especially lightweight versions for resource-constrained environments. However, existing datasets often lack diversity in age representation, and real-world deployment remains limited. Additionally, privacy concerns and the need for explainable artificial intelligence emerged as key barriers to adoption. This review underscores the importance of developing age-inclusive datasets, integrating multimodal solutions, and adopting XAI techniques to enhance system usability, reliability, and trustworthiness. We conclude by offering recommendations for future research to bridge the gap between academic progress and real-world implementation in elderly care.", "sections": [{"title": "1. Introduction", "content": "The global population is aging at an unprecedented rate, with significant implications for multiple aspects of society, including healthcare, social services, and the economy (Bloom and Luca, 2016). According to the World Population Aging report (WHO, 2024), there will be 265 million persons aged 80 years or older by the mid-2030s, more than the number of infants (1 year of age or less), and by the late 2070s, the number of persons at ages 65 years and higher is projected to reach 2.2 billion, surpassing the number of children (under age 18). This shift in demographics, driven by increasing life expectancy and declining fertility rates, particularly in developed countries, is creating a significant demand for innovations in elderly care. As older adults are more susceptible to physical and cognitive health conditions, including dementia, Alzheimer's disease, and other age-related illnesses (Langa, 2018), they often require specialized care. Providing such care on a large scale poses unique challenges, calling for the development of technologies that can assist with monitoring, diagnosing, and enhancing the well-being of this rapidly growing segment of the population with very specific needs.\nOne area where technology can have a significant impact is in the real-time recognition of emotional states in elderly individuals. Facial expression recognition (FER) systems offer a non-invasive means to monitor emotional well-being and identify potential mental health issues early on (Fei et al., 2019). FER, in particular, has become an important tool in healthcare, enabling caregivers and medical professionals to better understand the emotional states of their patients and respond accordingly. Social robots, for example, have been shown to increase social engagement among the elderly and can help reduce loneliness and depression"}, {"title": "2. Related Work", "content": "In this section, we examine the literature to identify studies that assess the impact of age on FER tasks, as well as prior reviews on FER. Our objective is twofold: first, to underscore the lack of FER reviews focused on specific age groups, particularly the elderly, and second, to provide evidence of the age bias often introduced by age-agnostic approaches. This analysis aims to highlight the need for age-sensitive methodologies in FER and to emphasize the importance of addressing this gap in existing reviews."}, {"title": "2.1. Effects of Aging on Facial Expression Recognition", "content": "Numerous psychological studies have explored the effects of aging on facial expression recognition, demonstrating that observers are influenced by their own age. For example, older individuals tend to exhibit deficits in decoding specific emotions (Isaacowitz and Stanley, 2011; Ruffman et al., 2008). However, aging impacts not only the observers but also the individuals displaying the target expressions. In this regard, F\u00f6lster et al. (2014) examined how age-related changes in the face, such as wrinkles and folds, influence the decoding process of emotional expressions, concluding that the age of the face is a critical factor. This finding is supported by subsequent studies, such as Ko et al. (2021), which identified variations in facial expression intensities and muscle usage across different age groups. For instance, elderly individuals tend to display more negative emotions and engage more muscles in the lower face compared to younger people. Similarly, Grondhuis et al. (2021) used generative adversarial networks (GAN) to investigate the increased difficulty in identifying expressions of older adults and attributed this challenge to the decline in facial muscle function with age. S\u00f6nmez (2019) explored the effect of training FER models with data from different age groups and found that recognizing expressions in elderly faces posed the greatest difficulty. A review by Atallah et al. (2019) on the effect of facial aging on FER identified several open challenges, including the scarcity of images capturing the same individuals across different ages (which hinders the learning of aging patterns), the tendency of models to overlook important facial features, and the considerable variation in aging effects across subjects. Moreover, the training of a Siamese CNN on the ElderReact and EmoReact datasets by Jannat and Canavan (2021) highlighted differences in the expressions of elderly individuals and children. Park et al. (2022) demonstrated the importance of addressing age bias in FER, showing that age-specific training yielded a 22% improvement in accuracy compared to using a non-age-specific dataset.\nAutomatic FER systems are heavily influenced by the age of the faces in the dataset, making the choice of dataset critical. Datasets such as FACES (Ebner et al., 2010a) and LifeSpan (Minear and Park, 2004), are frequently employed (Guo et al., 2013; Mary and Jayakumar, 2016; Wu et al., 2015; Wang et al., 2015; Al-Garaawi and Morris, 2016; Lopes et al., 2018; S\u00f6nmez, 2019; Caroppo et al., 2017, 2019, 2020; Al-Garaawi et al., 2022), largely due to their inclusion of subjects from a broad age range, which facilitates the development of age-invariant models. To address the effects of aging, several strategies have been proposed. One approach involves removing aging features through facial smoothing techniques that eliminate age-related details without compromising essential structural information (Guo et al., 2013; Mary and Jayakumar, 2016). Another method incorporates age information during training using Bayesian networks, while marginalizing over age during testing (Wu et al., 2015; Wang et al., 2015). Al-Garaawi and Morris (2016) first analyzed age-related differences in facial characteristics, then later incorporated age as a key feature in their model (Al-Garaawi et al., 2022), using a weighted combination of age group estimators and age-specific expression recognizers.\nOther studies have focused on specific age groups rather than addressing aging effects comprehensively. Datasets utilized in such approaches include ElderReact (Ma et al., 2019) and Tsinghua (Yang et al., 2020) for elderly subjects, CK+ (Lucey et al., 2010), JAFFE (Lyons et al., 2020a), AFEW (Dhall et al., 2007), and FER-2013 (Goodfellow et al., 2013) for adults, and LIRIS (Khan et al., 2019), CAFE (LoBue and Thrasher, 2014), DEFSS (Meuwissen et al., 2017), and EmoReact (Nojavanasghari et al., 2016) for children, among others."}, {"title": "2.2. Summary of Previous Reviews", "content": "Mining the literature, we identified several existing reviews addressing facial expression recognition. Although most of these do not focus specifically on elderly populations and therefore do not consider the effects of human aging discussed in the previous section, they do provide valuable insights into the datasets and techniques commonly employed in FER. In this section, we analyze relevant reviews and surveys from the past decade, highlighting that many critical aspects addressed in our current study have not been thoroughly examined in prior reviews.\nTo begin with, it is important to consider the methodological rigor of prior review processes. A systematic review methodology promotes a thorough and unbiased selection and analysis of studies by precisely defining information sources, search strategies, study selection criteria, and data extraction procedures. Additionally, quality assessment steps are essential to ensure that only high-quality, relevant studies are included. However, only 12 of the 41 reviews listed in Table 1 followed a systematic review process. Kitchenham and Charters (2007) also emphasize the importance of analyzing existing reviews in the field to identify gaps in the literature and avoid duplicate efforts. Yet, only 11 studies accounted for previous reviews, as illustrated in the table.\nThe main distinction between the current study and earlier reviews lies in our focus on FER applications for the elderly and deep learning tools. Only two previous reviews have concentrated specifically on elderly populations. Labzour et al. (2023) discussed 11 studies on elderly-focused FER, covering both traditional and deep learning methods and their associated datasets. Fei et al. (2019) also reviewed FER in elderly populations, primarily as a tool for early diagnosis of mild cognitive impairment. However, their work lacks coverage of recent studies from the last five years and focuses primarily on traditional computer vision approaches rather than DL-based methods. Consequently, there is a gap in the literature regarding recent DL-based FER research specific to elderly individuals.\nTen reviews were identified that discuss DL in FER (Ghayoumi, 2017; Chinchanikar, 2019; Sari et al., 2020; Li and Deng, 2022; Abdullah and Abdulazeez, 2021; Liang and Dong, 2023; Pinto et al., 2023; Boughanem et al., 2023; Kumari et al., 2024). Of these, only the work by Pinto et al. employed a systematic review process, and none of these reviews examined previous FER reviews, which may have contributed to overlapping lists of studies. While many of these reviews included extensive lists of commonly used datasets-an important consideration in DL-based approaches-none were focused on elderly populations, nor did they address critical aspects for the current work such as deployment in real-world environments, privacy preservation, or explainable AI techniques.\nThree critical aspects-deployment in real-world settings, privacy preservation, and XAI techniques- remain largely unexplored in prior reviews. Deployment in real-world scenarios was examined in only two reviews, which described applications involving mobile apps and robots (Leong et al., 2023) and smart home integration (Aziz et al., 2022). Although ethical concerns regarding privacy were noted in two reviews (Kaur and Kumar, 2024; Almasoudi et al., 2023), only one review explored this issue in depth (Adyapady and Annappa, 2023). Furthermore, despite the growing importance of XAI, this area was not thoroughly investigated in any review, even though some listed it as a future challenge (Mohana and Subashini, 2024; Kaur and Kumar, 2024; Patel et al., 2020).\nGiven the evident lack of reviews focused on elderly populations using DL-based techniques for FER, the present study aims to address this gap. Additionally, as few reviews have examined factors critical for the practical deployment of FER systems in real-world environments, and none have explored XAI in detail, our work may offer valuable insights for the development of practical, reliable applications."}, {"title": "3. Review Questions", "content": "We identified two primary review questions, each associated with multiple secondary questions, as shown in Table 3.\nRQ1 aims to find which deep learning techniques are applied specifically to facial expression recognition in elderly populations, and understand how are they used. The associated secondary questions delve into particular facets of these techniques, such as the prevalent architectures and datasets. Additionally, the questions examine data characteristics (e.g., image or video format, and multimodal integration) and assess the role of facial landmarks and action units, which are common features in FER research.\nIn contrast, RQ2 focuses on practical considerations for deploying FER systems for elderly users in real-world settings. We investigate factors crucial to successful deployment, including the potential impact of age-related biases in training data that could lead to suboptimal performance in real-life scenarios. We also assess privacy and economic considerations. Finally, given the critical need for transparency and trust in DL applications within healthcare settings (Adadi and Berrada, 2018), we explore the extent to which explainable AI techniques are integrated into these systems."}, {"title": "4. Review Methods", "content": "In this section, we provide a detailed description of the systematic review process, adhering to the guidelines outlined by Kitchenham and Charters (2007). First, we present the data sources and search strategy employed to compile the initial list of studies. Next, we explain the quality assessment and study selection procedures used to exclude low-quality and irrelevant works. Lastly, we outline the information extracted from each selected study."}, {"title": "4.1. Data sources", "content": "To ensure a comprehensive collection of relevant studies, we utilized five distinct digital databases. SCOPUS and Web of Science (WOS) were selected for their broad interdisciplinary coverage, while the ACM Digital Library, IEEE Xplore Digital Library, and PubMed were chosen for their focus on computer science, technology, and biomedical research, respectively. Figure 1 illustrates the distribution of studies retrieved from each source. Notably, the ACM Digital Library yielded the highest number of studies among the five databases."}, {"title": "4.2. Search strategy", "content": "To ensure a more precise search in the SCOPUS database, we limited our query to the title, abstract, and keywords, as searching the full text yielded a significant number of irrelevant results. In contrast, full-text searches were performed in the other databases. Although the query strings were adjusted according to the specific requirements of each search engine, they consistently included four key concepts: facial expression recognition, computer vision, elderly population, and deep learning. Each concept was paired with multiple synonyms and, where applicable, the \"*\" wildcard was used to capture various word terminations.\nThe search was constrained to publications from the past ten years, specifically from 2015 through September 2024, inclusive. Figure 2 presents the number of related studies identified by year, demonstrating a clear upward trend, particularly over the last three years."}, {"title": "4.3. Quality assessment", "content": "To assess the quality of the included studies, we followed the guidelines proposed by Kitchenham and Charters (2007). The evaluation was based on four primary criteria:\n\u2022 Type of publication. Since conference proceedings may not always undergo a rigorous peer-review process, unlike journal articles, the type of publication was considered as a quality indicator.\n\u2022 Reproducibility. This criterion assesses whether the work can be replicated, either through detailed explanations of the methods used, by providing access to the programming code, or by making the collected dataset publicly available.\n\u2022 Benchmarking with the state-of-the-art. Whether the proposed methods are compared with prior works under similar conditions, such as using the same performance metrics, dataset splits, and methodologies.\n\u2022 Performance on public datasets. The use of publicly available datasets is important, as it allows other researchers to compare their results with those of prior studies in a standardized manner.\nBy applying these criteria, we were able to filter out studies that exhibited potential bias. This quality assessment was integrated into the study selection process, further detailed in Section 4.4, and the list of extracted data from each study, outlined in Section 4.5."}, {"title": "4.4. Study selection", "content": "After gathering the search results from multiple databases, totaling 285 studies, we identified and removed 16 duplicate entries. The remaining studies were then screened individually to exclude publications that were not directly relevant to the research topics. Additionally, we eliminated works that did not meet our inclusion criteria, including those not written in English or Spanish, those without full-text access, or those that did not meet the quality standards outlined in Section 4.3.\nFigure 3 illustrates the progression of the selection process, from the initial collection of studies to the final screening. Ultimately, 31 studies were selected for inclusion in this review, of which 15 were journal articles, and 16 were conference proceedings. As stated by Kitchenham and Charters (2007), \"publication bias can lead to systematic bias in systematic reviews unless special efforts are made to address this issue\". Since scanning conference proceedings is a standard search strategy to address publication bias, they were retained for analysis among the relevant studies."}, {"title": "4.5. Data extraction and synthesis", "content": "To synthesize the relevant studies, different information was extracted from each one. The gathered fields are shown in Figure 4."}, {"title": "5. Results", "content": "The complete list of relevant studies is provided in Table 3. In this section, we present a detailed analysis of these studies to address the research questions posed. For improved readability, the results have been organized into distinct subsections, each addressing a review question."}, {"title": "5.1. RQ1. DL techniques for FER in elderly populations", "content": "Next, we present the findings related to the first research question, focusing on common DL architectures, datasets, data characteristics, the use of facial landmarks and action units, and related tasks in performing FER for elderly populations."}, {"title": "5.1.1. RQ1.1. Deep learning architectures", "content": "Table 4 provides an overview of the deep learning-based methods employed in the relevant studies. The table highlights the diversity in model architectures and their applications to various tasks within the facial expression recognition pipeline.\nThe methods listed in the table were used for a wide range of tasks. The most common was FER itself, where models included a final classification layer to predict facial expressions. Convolutional models such as VGG, Inception, and ResNet were often employed to extract complex features, which were later used for classification tasks (Zhang et al., 2020; Bi et al., 2022; Huang et al., 2024b). Object detectors, including Faster R-CNN, YOLO, SSD, MTCNN, and SCRFD, were used in pre-processing steps to localize and crop faces before performing FER (Jiang et al., 2022; Gaya-Morey et al., 2024). Notably, Faster R-CNN, YOLO, and SSD were also used by Khajontantichaikun et al. (2022; 2023) to simultaneously perform object detection and FER. In addition to FER, some studies focused on auxiliary tasks such as facial landmark estimation, for which MobileNet, SPIGA, MTCNN, FaceReader, and OpenFace 2.0 were employed (Zhu et al., 2024; Chen and Chen, 2023; Gaya-Morey et al., 2024). Among these, FaceReader and OpenFace 2.0 also supported action unit estimation. Other applications included emotion recognition from text, as demonstrated by Carolis et al. (2024), who used GPT-4 for emotion classification and text generation in a social robot. Gaya-Morey et al. (2024) also explored the integration of multiple tasks in a social robot, among which U2-Net and MiVOLO were used for object segmentation and for age and gender estimation, respectively. To address privacy concerns, Huang et al. (2024a) employed GAN to transform images and FaceQnet to estimate image quality. GAN were also used by Zhu et al. (2024) to generate a FER dataset comprising multiple age groups.\nThree primary families of architectures were frequently employed: CNNs, recurrent neural networks (RNNs), and Transformers. Convolutional neural networks were the most widely used, appearing in 27 models across 22 of the reviewed studies. These feed-forward architectures are highly effective for computer vision tasks and were often used as either end-to-end FER solutions (Petrou et al., 2023; Gaya-Morey et al., 2024) or feature extractors (Caroppo et al., 2020; Sree-"}, {"title": "5.1.2. RQ1.2. Datasets", "content": "Table 5 provides an overview of the datasets used for training in the FER tasks identified in the reviewed studies. As highlighted, these datasets vary significantly in terms of annotations, data type, classes, and sample sizes.\nSeveral datasets were automatically collected from the Internet, including FER-2013, ElderReact, AffectNet, EmoReact, Jafar Hussain, CIFE, and CMU-MOSEI. This collection approach enables the creation of large datasets, such as FER-2013 (35,887 samples) and AffectNet (440,000 samples). However, these datasets often suffer from issues such as poor expression annotations (Mejia-Escobar et al., 2023), the presence of duplicate samples (Manresa-Yee et al., 2023), and the absence of demographic information about users. Conversely, datasets collected under controlled conditions offer advantages such as user selection based on demographic criteria, multiple samples from the same individual, and controlled lighting conditions. For example, FACES and LifeSpan emphasize age diversity, supporting cross-age group studies, while ElderReact and EmoReact target elderly individuals and children, respectively. Additionally, UIBVFED stands out as the only synthetically generated dataset, allowing for precise control over the facial expressions of avatars, as well as their demographic characteristics, including age, gender, and ethnicity.\nAlthough the majority of datasets are in image format, several are video-based (Lucey et al., 2010; Ma et al., 2019; Nojavanasghari et al., 2016; Zhao et al., 2011; Livingstone and Russo, 2018; Bagher Zadeh et al., 2018; Zhalehpour et al., 2017). Notably, MMI (Pantic et al., 2005) offers samples in both image and video formats. Video-based datasets enable the incorporation of temporal information into FER tasks and are particularly valuable for systems requiring real-time processing.\nMost datasets include the six basic facial expressions (happiness, sadness, anger, fear, disgust, and surprise). Additionally, the neutral expression is included in twelve datasets, while contempt is present in three. Other, less common expressions are found in only one dataset each, such as boredom (Zhalehpour et al., 2017), content (Yang et al., 2020), calmness (Livingstone and Russo, 2018), curiosity, uncertainty, excitement, frustration (Nojavanasghari et al., 2016), annoyance, and grumpiness (Minear and Park, 2004).\nInstead of relying on the datasets listed in Table 5, several of the analyzed studies chose to create custom datasets tailored to their specific research needs. These efforts provided unique opportunities to address specialized populations or expressions that were not well-represented in existing datasets. For instance, Uddin et al. (2017a; 2017b) developed a dataset containing 40 close-up RGB-D videos for each of the six basic facial expressions. This approach leveraged depth information to improve facial expression recognition while maintaining user privacy. Similarly, Khajontantichaikun et al. (2022; 2023) curated a dataset of 900 images of Thai elderly individuals, with 150 samples per basic expression. These samples were collected from online media and annotated by five human evaluators. Zhu et al. (2024), on the other hand, employed a generative adversarial network to synthesize 20 images for each age group: children, young adults, middle-aged adults, and elderly individuals.\nSeveral studies focused on populations with specific medical or psychological conditions. For instance, Huang et al. (2024a) introduced the Parkinson's disease facial expression dataset, which comprises RGB images of the six basic expressions from 95 patients with Parkinson's disease. Fan et al. (2022) constructed a dataset of 1,302 videos from stroke patients aged 18-85, capturing not only basic expressions (happiness, sadness, surprise, and anger) but also four condition-specific expressions: painful, strained, tired, and neutral. Additionally, this dataset included facial action coding system (FACS) labels, making it valuable for detailed facial action unit analysis. Sharma et al. (2020) gathered 140 videos from 70 elderly participants aged 65 and above, 28 of whom were identified as apathetic, enabling the study of apathy recognition in older adults. To address the issue of imbalanced data, several techniques are available, such as rebalancing through undersampling or oversampling and employing weighted loss functions. However, despite many studies using imbalanced datasets, such as AffectNet and LifeSpan,"}, {"title": "5.1.3. RQ1.3. Data characteristics", "content": "Seven studies leveraged the temporal dimension of visual data to perform facial expression recognition, while the remaining studies were limited to using static images. Most video-based solutions adopted a common approach: extracting features from individual frames and employing an RNN to capture temporal information. For instance, Zhang et al. (2020) utilized the VGG16 network for feature extraction, followed by a long short-term memory (LSTM) network to process sequential data. Similarly, Rokkones et al. (2019) employed local directional strength patterns as frame descriptors, which were then fed into an RNN. Bi et al. (2022) adopted a comparable methodology, using GoogleNet for feature extraction, applying principal component analysis to reduce data dimensionality, and processing the resulting data with a Bayesian probabilistic framework. In the same line, Sreevidya et al. (2022) extracted features from multiple video frames using a convolutional neural network and utilizing an LSTM to handle the temporal aspects.\nOther studies explored variations of this approach to optimize performance. For example, Sharma et al. (2020) combined visual features extracted by VGG16 with action units and facial landmarks obtained via OpenPose 2.0, processing the sequential data with a gated recurrent unit (GRU). While most studies relied on RNNs to model sequential information, Anand et al. (2023) took a different route by employing a multimodal Transformer architecture (Vaswani et al., 2017) that integrated visual data, audio, and text. Carolis et al. (2024), despite performing FER on still images, incorporated temporal information by calculating the frequency of different expressions over a defined time period. They determined the final expression by selecting the most frequently occurring one, giving greater weight to the most recent frames.\nOnly four studies opted for multimodal solutions, integrating non-visual data into the facial expression recognition pipeline. These approaches demonstrated the potential of combining modalities to enhance predictive performance.\nSharma et al. (2020) investigated apathy detection from a multimodal perspective. For the audio modality, they extracted features using the Geneva minimalistic acoustic parameter set, which includes 18 low-level descriptors based on frequency, energy, and spectral parameters. These features were processed through fully connected layers to learn meaningful representations. The audio features were then concatenated with visual features, action units, and facial landmarks before being passed through additional fully connected layers. Comparing their multimodal model with unimodal approaches, they observed significant improvements in prediction accuracy when leveraging multiple modalities.\nSreevidya et al. (2022) proposed two distinct methods for processing audio signals. The first approach used a one-dimensional representation, extracting features such as prosody, spectral coefficients, and voice quality characteristics (e.g., tenseness and creakiness) and feeding them into a 1-D convolutional neural network. The second approach converted the audio signals into 2-D spectrograms, which were processed using the Inception-V2 (Szegedy et al., 2016) network. For the visual modality, a separate CNN was employed to compute FER. The final multimodal model fused intermediate layers from both modalities, with the fusion layers determined through a grid-based search. Results showed that combining features from both audio and visual data significantly improved overall performance.\nAnand et al. (2023) introduced text as a third modality in their FER pipeline, proposing a multimodal Transformer network. The architecture consisted of modality-specific peer networks that independently learned discriminative features from each modality. These features were then fused by minimizing the Kullback-Leibler divergence between peer networks. This design enabled the network to learn mode-specific patterns while effectively leveraging multiple modalities simultaneously, leading to remarkable improvements in performance, particularly in cross-dataset settings.\nCarolis et al. (2024) focused on integrating vision and text modalities. For the visual modality, they trained an EfficientNet model (Tan and Le, 2020b) to process RGB images. For the text modality, they utilized GPT-4.0 (OpenAI et al., 2024) to predict emotions from textual inputs. A late-fusion strategy was employed for the final prediction, using the confidence scores from both modalities."}, {"title": "5.1.4. RQ1.4. Facial landmarks and action units for FER", "content": "Facial landmarks and action units (AUs) are two fundamental tools frequently employed for facial expression recognition. Facial landmarks are visually identifiable points on the face, such as specific regions around the eyes, nose, and mouth, which help characterize facial geometry. AUs, on the other hand, were introduced in the facial action coding system (Ekman and Friesen, 1978) to describe 46 basic muscle movements, providing a more granular understanding of facial expressions. These features serve as inputs for various computational approaches to facial expression recognition.\nOne notable application of landmarks and AUs is in the multimodal apathy detection solution by Sharma et al. (2020). They integrated landmarks and AUs as features into a multimodal model alongside visual and audio data. They computed these features using the OpenFace 2.0 toolkit (Baltrusaitis et al., 2018), and demonstrated that combining multiple modalities significantly enhanced system performance. In contrast, Caroppo et al. (2020) extracted geometrical features-linear, elliptical, and polygonal-from facial landmarks estimated using an improved version of active shape models (Milborrow and Nicolls, 2014). These features were then used to train various machine learning models. However, their results showed that the performance of geometrical landmark-based features lagged behind that of DL-based features, such as those computed by VGG16 (Simonyan and Zisserman, 2015). A different approach was adopted by Wu et al. (2015), who utilized manually annotated fiducial points provided by Guo et al. (2013) to train a Bayesian network. By incorporating age labels during the training phase, Wu et al. demonstrated significant performance improvements, highlighting the potential of integrating demographic features like age into the FER task.\nOn the other hand, some studies computed facial landmarks and AUs but did not use them directly for facial expression recognition. For instance, Gaya-Morey et al. (2024), Zhu et al. (2024), and Chen and Chen (2023) incorporated these features for other purposes, such as system development and integration, but refrained from leveraging them for FER tasks specifically."}, {"title": "5.1.5. RQ1.5. Other tasks", "content": "Apart from the facial expression recognition task, which is the central focus of this review, several other tasks were computed alongside it in multiple studies.\nFace detection emerged as the most common preprocessing step in the studies reviewed. This step involves cropping the image to focus on the face and removing irrelevant data for the FER task. Several approaches were used for face detection, with the Viola-Jones technique (Viola and Jones, 2001) being the most prevalent due to its simplicity, high speed, and low computational requirements. Another common preprocessing task was facial landmark estimation, which is further addressed in Section 5.1.4.\nAge estimation was frequently employed to address facial expression differences among age groups. For example, Caroppo et al. (2020) automatically labeled images from the CIFE and FER-2013 datasets using landmark-based methods (Wu et al., 2012) to study FER accuracy across different age groups. Similarly, Zhu et al. (2024) utilized FaceReader (Noldus Information Technology b.v, 2021) to classify images into age groups and analyze performance variations. Huang et al. (2024b) not only used age estimation with ResNet18 (He et al., 2016b) for evaluation purposes but also integrated it into their FER system to compute age group-specific features. In contrast, Yang et al. (2018) adopted a multi-tasking approach, simultaneously predicting both age and facial expressions to improve accuracy for both tasks. Because age estimation is included as a means to studying aging effects on facial expression recognition, these works are analyzed in more depth in Section 5.2.2\nFER was also employed as a tool for detecting cognitive impairment (CI), which is often characterized by abnormal emotional patterns. Fei et al. (2022) used the occurrence of facial expressions in consecutive frames as input to an SVM, enabling CI detection in elderly individuals. Jiang et al. (2022) adopted a similar approach, utilizing the VGG19 network (Simonyan and Zisserman, 2015) to predict facial expressions. These predictions were then fed into a linear regression model and an SVM for CI detection. They found that CI participants displayed significantly fewer positive emotions, more negative emotions, and higher facial expressiveness. However, attempts to identify CI subtypes using the same features were unsuccessful. Apathy detection, closely related to FER, was also explored by Sharma et al. (2020), who proposed a multimodal solution that combined RGB data, action units, and audio. Their results demonstrated that using multiple modalities improved performance compared to relying on a single modality.\nSeveral studies integrated FER with additional tasks to create multifunctional systems. Yan et al. (2020) combined FER with facial recognition and fall detection. For facial recognition, they used MobileNetV2 (Sandler et al., 2018), while fall detection was achieved by estimating body pose using OpenPose (Cao et al., 2021) and analyzing shoulder and chest coordinates. This integration aimed to detect and respond to different warning situations, like falls and negative expressions prolonged in time. Similarly, Fahn et al. (2022) performed fall detection using body pose estimation and incorporated gaze, facial expression, and speech recognition into a social robot. Carolis et al. (2024) extended FER by including emotion recognition from text and dialogue generation using GPT-4.0. These tasks, paired with FER, endowed a social robot with empathy capabilities. A user study involving 30 elderly participants revealed that the robot's empathic features improved usability and provided more positive user experiences. Gaya-Morey et al. (2024) took this further by including eight tasks in their system: FER, face and person detection, age and gender estimation, facial recognition, facial landmark estimation, and background subtraction. They also provided multiple options for each task to accommodate different use cases. Their objective was to create a versatile, AI-powered module for any socially interactive agent."}, {"title": "5.2. RQ2. Successful deployment in real-world environments", "content": "We now present the findings related to the second research question, emphasizing framework integration, deployment, the impact of age on FER, privacy considerations, economic costs, and the use of explainable AI techniques."}, {"title": "5.2.1. RQ2"}]}