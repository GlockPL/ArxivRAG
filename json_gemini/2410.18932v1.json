{"title": "ANAVI: Audio Noise Awareness using Visuals of Indoor environments for NAVIgation", "authors": ["Vidhi Jain", "Rishi Veerapaneni", "Yonatan Bisk"], "abstract": "We propose Audio Noise Awareness using Visuals of Indoors for NAV- Igation (ANAVI\u00b9) for quieter robot path planning. While humans are naturally aware of the noise they make and its impact on those around them, robots cur- rently lack this awareness. A key challenge in achieving audio awareness for robots is estimating how loud will the robot's actions be at a listener's location? Since sound depends upon the geometry and material composition of rooms, we train the robot to passively perceive loudness using visual observations of indoor environments. To this end, we generate data on how loud an 'impulse' sounds at different listener locations in simulated homes, and train our Acoustic Noise Pre- dictor (ANP). Next, we collect acoustic profiles corresponding to different actions for navigation. Unifying ANP with action acoustics, we demonstrate experiments with wheeled (Hello Robot Stretch) and legged (Unitree Go2) robots so that these robots adhere to the noise constraints of the environment. See code and data at https://anavi-corl24.github.io/.", "sections": [{"title": "1 Introduction", "content": "Humans are very aware of the noise that their movements create. In homes and offices, we avoid being noticed if a person is having a video call or a child who has just fallen asleep. Home robots need that same social awareness when planning their actions in indoor environments. Unfortunately today robot vacuums may be as loud as 70 decibels, which concerns many people with sensitive ears. The existing solution is to use the \"do not disturb\" mode [1] which completely stops the robot from functioning so that it does not make noise. Integrating more complex robots, e.g., quadrupeds, in homes, needs more sophisticated methods of addressing noise levels while maintaining efficiency.\nAlthough sound is inevitable with robot movement, it can be mitigated. In isolation, e.g. without acoustic reflections or echoes, unoccluded sound intensity decays quadratically with distance. As a robot moves away from humans or sound-sensitive areas, its sounds, such as motor noise or beeps, become less likely to be heard. Thus, a simplistic approach to reducing this noise is for the robot to move slowly and steer clear of people and pets in its environment. While this might initially seem like a sensible strategy, it can lead to increased time and energy consumption for task completion and may not always be practical or feasible. Importantly, sound intensity depends on many other factors beyond distance. The intensity at the same distance of 1 meter will be reduced if separated by a wall or in a carpeted room, whereas, conversely, halls create echoes. Architectural geometry and material properties affect how the sound reflects, diffracts, and is absorbed.\nExisting work on audio for robotics focuses on finding the source of an audio signal, guiding naviga- tion [2, 3]; which leverages realistic 3D simulators to learn how to navigate to the sound of dripping water in the sink or a fire alarm. In contrast, we focus on awareness of self-generated sounds; a related yet orthogonal problem: how loud will the robot's actions be at a listener's location?\nTo this end, we propose Audio Noise Awareness by Visual Interaction (ANAVI), a framework that allows a robot to learn about the unintended noise made by its actions and thereby adapt its actions to"}, {"title": "2 Acoustic Noise Prediction (ANP)", "content": "A robot navigating indoor environments produces sound, either from its motors or its speakers trying to communicate with others in its environment. Given the robot in an environment and a listener elsewhere, we want to predict how loud the robot is at the listener's location. Sound travels from the robot's location to a listener's location depending on the relative distance, architectural geometry, materials, and hearing sensitivity of the listener. In our simulated environments, the most dramatic differences happen at short distances blocked by walls or furniture, versus long echoing hallways.\nSimulator Setup. We create a simulated dataset of sound decibels perceived at the listener's location when the robot is located in different parts of home environments. We use the Matterport3D dataset, which consists of 85 real-world scans of indoor environments. The simulator assumes a sound impulse at the source location and calculates a response (aka Room Impulse Response or RIR) at the listener location. For this, we use Sound-Spaces 2.0 [4] to simulate how audio waves travel from the source to arrive at the listener as an impulse response. To simulate any sound, the room impulse response is convolved with the audio at the source. As convolution is linear, the max decibel of audio at the listener is linearly proportional to the decay of the max db of the RIR from that of the original impulse. We assume that an impulse generates the sound pressure level of $1W/m^2$ at the source and that the listener is within 10 m of the source. More details are in Appendix B.2."}, {"title": "Architecture", "content": "We learn an acoustic noise predictor function $f$ such that $y = f(r_{sl}, \\theta_{sl}, V_s)$. Fig. 3 shows our proposed architecture for Acoustic Noise Prediction (ANP). Our model consists of three components. Image Encoder extracts the visual features using pretrained visual encoders like ResNet-18. We apply adaptive average pooling and normalize it to get visual encoding $e_{visual}$. Direction-Distance Encoder is a simple linear projection to obtain an encoding $e_{dirdis}$. Predictor takes the concatenated vector of $e_{visual}$ and $e_{dirdis}$, and processes it through a series of blocks. Each block contains a linear layer, a batch norm layer, and GeLU activation. The output size of each successive linear layer shrinks to act as an \"information bottleneck\". The final output is either a scalar real number for regression or $m$-binned logits for classification.\nTraining Our code uses PyTorch and torchvision ResNet-18 checkpoint. We apply ResNet-18 pre-processing on $V_s$ for mean 0 and std 1. We normalize the $r_{s}$ and $y$ between 0 and 1. We use Huber Loss [6] with $\\delta = 0.1$ and the AdamW optimizer, which has a learning rate of 0.01 and a step schedule with 0.95 decay after 10 epochs. We use a batch size of 64 on a 24GB RAM GPU."}, {"title": "3 Simulation Experiments", "content": "We assess our acoustic noise prediction model in simulation and demonstrate its applicability to real-world environments. Our simulation experiments are conducted in the Habitat 2.0 simulator [7] on the Matterport3d dataset [8]. We inherit the train/val/test splits for Matterport3d scenes from the PanoIR [4] dataset - 59 maps for train, 11 for val, and 15 for test. For training, we create 5,000 samples per map, where each sample consists of robot location $s$, listener location $l$, the robot's panoramic view $V_s$, and the listener's Impulse Response $w_l$. We post-process the impulse response to calculate the max dB heard at the listener's location as $y$. For val and test, we create 500 samples per map. We evaluate 500 \u00d7 15 = 7500 samples in all simulation experiments.\nMetrics. We compare acoustic noise prediction models with data distribution coverage plots. To understand the coverage of predicted values with respect to the true labels, we include a distance-decibel plot in Fig.4. The higher the overlap of the red overlay over the blue, the better the model's performance. Note, the highly diverse data distribution of decibels $y$ for a given distance $r$.\nAnother, more quantitative, view of the regression analysis is via $\\epsilon$-thresholded accuracy curves. Our $\\epsilon$-thresholded accuracy curves are inspired by Precision-Recall curves used in classification. This comparison allows us to see the effect of cross-entropy style training, both on performance and how it skews the data distribution. See Fig. 5. Let $y$ be the normalized max decibel value of the simulated impulse response (IR)s, and $\\hat{y}$ be the predicted value by the model. Let there be $N$ samples, $\\mathbb{1}(\\cdot)$ is the indicator function, which is 1 if the condition inside is true and 0 otherwise, and $N = 7500$. For $\\epsilon$-accuracy, we compute $\\sum_{i=1}^{N} \\mathbb{1}(|Y_i - \\hat{Y_i}| < \\epsilon)$. An $\\epsilon = 1/128 = 0.007$ captures whether the model can learn to accurately predict within a single decibel. Humans typically can barely differentiate the 1 dB difference for sounds in 1000-5000 Hz frequencies, also known as Just Noticeable Difference (JND) [9]. JND increases to 3-5 dB at very low or very high frequencies and changes with larger initial loudness levels.\nBaselines. We compare the performance of our Acoustic Noise Prediction (ANP) model with a distance-based heuristic and two learned baselines: (i) Heuristic Distance based (Heuristic) Sound intensity decays by a factor of inverse squared distance, that is $\\frac{1}{r^2}$. Thus, the predicted max dB IR can be calculated by $-20 log_{10}(r) + 120$. (ii) Regression with Distance (DisLinReg) We train a linear regression model to match the average shape and curvature of the data as a function of the data. Note that this more accurately captures the shifting mass distribution than the heuristic; however, it is unimodal in its predictions. (iii) MLP Regression with Distance and Direction (DirDisMLP) We train a multi-layer perceptron model with blocks of linear layers, batch norm, and ReLU activation, where layer sizes of 2, 8, 8, and 1. (iv) Visual features with Distance and Direction (VisDirDis) One concern with the DirDisMLP is that it does not incorporate visual information. Therefore, it cannot distinguish between scenarios with a wall between the robot and the listener versus instances in open spaces where sound may travel collision-free. In VisDirDis model, we use a panoramic RGB view at the agent's location as the visual feature of 512-dim. We include direction as input to localize and attend to how the loud listener hears the robot. The direction and distance are encoded by a linear projection to 16-dim. We concatenate and pass the resulting 528-dim vector to the predictor. The predictor consists of 4 blocks with layers sizes: 528, 256, 64, 8 and outputs as a scalar value."}, {"title": "3.1 Results", "content": "We show significant performance improvement by incorporating visual information. Visual informa- tion conveys the architectural geometry and materials used in the space, which affects the perceived sound intensity at the listener's location. Fig. 4 shows the predicted data distribution (in red), over-layed on the true test data (in blue). The ideal model would directly match the blue data distribution, and it is apparent that our model most closely aligns with the ground truth. Additionally, Figure 5 shows $\\epsilon$-accuracy for learned models. On the far left of Figure 4, the geometry spreading-based dis-tance heuristic (Heuristic) only predicts correctly for free space and doesn't account for reflections in corridors or absorption by walls in home environments. Second, we plot the linear regression pre-diction (DisLinReg) based on distance. While it improves the overall $\\epsilon$-accuracy curve in Fig. 5, it doesn't cover the plot. The learned MLP (DirDisMLP) slightly improves performance and begins"}, {"title": "3.2 Ablations", "content": "We evaluate modeling design choices for loss func- tions and visual representations. Refer to Fig. 6 for the data distribution coverage results for all our model variants in this section. (i) Loss function: Re- gression vs Classification. We use m-bins to dis- cretize the prediction and convert it from regression into a classification problem-the granularity of the prediction changes depending on the bin size. For example, a 16-binned model considers 81 and 87 to be in the same bin, while a 128-bin prediction con- siders 87 and 88 as separate bins. We observe that the cross-entropy models train faster to reach their peak performance, especially with coarser granular- ity like 16-binned labels. While in many cases, 16 bins could be enough to tell if the robot is too loud, it may not help with sensitive listeners.\n(ii) Visual input: Ego-centric vs Panoramic view. For ego-centric images, we extract the 90\u00b0FoV from the panoramic image at the robot's location in the direction of the receiver. We use ResNet-18 as the visual encoder for RGB features. We encode the distance and direction with a linear projection to a 16-dim vector. In Fig. 6, we observe that the learned distributions with ego-view miss outliers with very high values for the longer distances. In Fig. 5, the overall $\\epsilon$-accuracy is lower than our Pano- version. While ego-centric is desirable for simplified real-time perception with a camera, it has a limited field of view and lacks the information about architectural geometry in the immediate vicinity of the robot; which can affect how sound travels to the listener."}, {"title": "3.3 Analysis", "content": "To better understand the model's performance, we create visual representations of two scenarios by mapping the expected max dB value at each location. See Appendix B.3 for visualizations. (i) Fixed Robot Acoustics Map: In this scenario, we initialize the agent at a location and compute the model's predictions for different listener locations. The input to the model consists of the same image, with variations in the distance and direction of the listener. This acoustics map helps to predict how many possible listeners in the environment perceive the robot's actions as loud in Sec. 4. (ii) Fixed Listener Acoustics Map: In this scenario, we fix the listener's location and compute the max dB heard by the listener when the robot is at different locations in the environment. We construct this acoustic map to estimate the noise cost incurred at each state for planning in Sec. 4."}, {"title": "4 Real world experiments", "content": "We propose to evaluate how well the sound loudness is perceived at different listener locations in its environment. Unlike the audio simulators, which have privileged information about the scene mesh, material properties, etc., the agent can only see in its immediate vicinity and knows the relative polar coordinates of the listener. Ideally, we would move the robot to multiple different rooms/apartments and then record the audio. However, practically, robots can be hard to move across apartments and this limits data collection. Thus, an easier method to gather audio responses is to record the sound of robot's actions onto a device (e.g. phone or laptop). Then instead of moving the robot to other locations, we take our device and play back the sound. An added benefit of using recorded audio is that we reduce the variability of the source sound across measurements at different listener locations.\nOur experiments use the recorded sounds from a Unitree Go2 and Hello Robot Stretch performing different actions. We use a laptop speaker to play the robot action's audio; acting as the sound source. We use mobile phones to record panoramic images at the robot's location, measure relative distances with Augmented Reality apps, and record audio at the listener location [10].\nHow loud will the robot be in my home? We want to evaluate our model's prediction for the loudness measured at different listener locations from a fixed robot location. Recall in Sec. 3.3;"}, {"title": "5 Related Work", "content": "Acoustics in Learning and embodied AI. Sound and vibrations have been extensively studied in physics, architectural designs, and acoustic material properties. Recent works in multimodal ma- chine learning have studied the alignment of audio and visual observations [11], acoustic synthesis for novel view [12], and how actions sound [13, 14, 15]. Improved acoustic simulation, with bi-"}, {"title": "6 Limitations", "content": "Acoustics is inherently complex and often noisy (pun unintended), whether in simulation or the real world. In simulation, the holes in mesh reconstructions often reduce the ray tracing efficiency. In the real world, ambient noise and microphone sensitivity usually interfere with reliable and reproducible acoustic measurements. Material absorption, scattering, transmission, and dampening properties are complex, and the resulting reflections and reverberations are hard to measure and predict.\nThis work uses simulated environments (Matterport3D scans) and audio impulse response simula- tion (SoundSpaces). Our approach enables controlled experiments and large-scale data collection. However, as discussed in Sec. 4, it may not fully capture the complexities and nuances of real-world environments and acoustic properties. Our current framework does not address the effects of am- bient noise. In this work, we focus on first-order principles that directly involve the robot's audio in relation to the audio. However, in real scenarios, the effect of other ambient noises (e.g. a loud TV on) changes how the robot's noise effects the listener. Incorporating other noise sources requires more complex reasoning like sound source separation and noise processing.\nLoudness is a subjective and psychological sound pressure; differing based on demographics [32] and prior duration of noise exposure. We use max dB from the impulse response w(t); it is an important yet insufficient aspect to truly measure loudness. In the future, we hope to train models that capture frequency, duration, and other characteristics relevant to perceived sound from room impulse response in simulated and real-world environments."}, {"title": "7 Conclusion", "content": "We want our future robots to reason about auditory disturbances when operating in human environ- ments like households, hospitals, and offices. Robot assistants in homes and indoor environments generate sound due to their movements or speech through robot's speakers. For any sound produced, robots should be aware of the loudness perceived by the other listeners in the environment.\nWe present an Acoustic Noise Prediction (ANP) model that uses visual features, along with the listener's distance and directions, to predict the max decibel value of impulse response at the listener. The model enables us to predict the robot's perceived loudness in the environment and to plan routes that lead to less noise perceived by the listener. We show the real-world applicability of our ANAVI framework to mitigate noise to the listeners. With audio noise awareness, we hope that future robot policies can adapt their path, velocity, and speaker's volume to adhere to the environmental noise constraints; thereby becoming well-suited to function in human-inhabited spaces."}, {"title": "A Real-world Experiments", "content": "A.1 More details on ANAVI Framework\nIn Section 4, we provide experiment for robot planning a quieter path. For adapting robot path plans, we detail the ANAVI framework here. See Figure 9.\n(a) Record the environment: Localize and cache the visual observations. This is similar to the Matterport scans of a physical space that records the localization and panoramic views from each location. Let each location be a node in the graph.\n(b) Run ANP: Given known (1) location of listeners and (2) the panoramas of the discrete locations of the map: For each node in the graph, use the panorama and the relative polar coordinates of the listener from this node as inputs to the ANP model to predict the max RIR dB. Use the max RIR dB with the robot's action audio files to estimate max dB for each action.\n(c) Time vs. Audio Noise Tradeoff: Given the social scenario, use LLM prompting or heuristics to decide a weighting factor between the audio noise cost and the time-to-go cost. For example, if a person is having a video call, then the audio noise cost is high with respect to time taken to goal, versus if someone is listening to headphones, then the noise cost is not too high and the robot can optimize for time. With multiple listeners, we can estimate their sound sensitivity, and then incorporate their relative weighted plan into the overall cost.\n(d) Plan: Use the weighted overall cost with an A* planner (or any other cost informed motion planner) to plan over what actions and path and velocity to take. We discretize slow/normal/fast velocities and encode them as actions with corresponding audio and time costs."}, {"title": "A.2 Collecting Real-world Audio", "content": "Setup We compare the model's prediction with how loud the robot would be in the real world in Section 4. We describe how we collected the data and provide real-world acoustic measurements in Table 2. In this experiment, we fix the robot at a location and vary the listener locations.\nWe want accessible commodity hardware to ensure easy replicability for researchers so they can use existing resources. We also want to ensure that an easy setup on a mobile robot for future research. To this end, we use mobile phones to capture images, record audio and measure distances."}, {"title": "A.3 Qualitative Evaluations on Real-World Panoramas", "content": "We collect real-world panorama of indoor environments to visualize the model's predictions. We focus on sim-to-real performance of the audio prediction, as once accurately estimated, these values can be weighted against distance for audio-informed planners.\nSetup To collect real world panoramas, we requested graduate students to contribute these panoramic images, by capturing their surrounding indoor environment for acoustic profiling. Con- tributors used their mobile phones at zoom level 1x and took a single panoramic image which we then resized to 256\u00d71024. The images are then fed into a neural network that predicts the maximum decibel level at a given distance and direction from the robot. The ANP neural network is trained on a dataset of simulated Matterport renderings, and we qualitatively evaluate its performance on a dataset of real-world panoramas. Below we use Figure 12 to explain our visualizations.\nThe first row shows the real-world panorama. The leftmost and rightmost edges correspond to 45\u00b0, and we change angles in clockwise direction from left-to-right. The reason for non-increasing order of angles on x-axis is that panorama's are taken left-to-right, that is, clockwise, whereas angles are measured anti-clockwise. Note that the cardinal directions indicated on the plots does not imply that the panoramo starting direction and are only for illustration purposes.\nThe second row shows heatmap plot with the model's prediction for different distance and direction values. The direction is shown on x-axis, covering 360\u00b0, and the distances on y-axis range from 0 to"}, {"title": "B.1 Training Data Generation", "content": "We generate training data for ANP in simulation using SoundSpaces 2.0. Here we provide additional details for the data generation process: (i) Uniform sampling across the map. We first divide the map into grids to get 100 points. These points serve as circle centers to sample a suitable navigable point within 20 meters radius for the robot's location as the source. We then use the robot's location as the circle center and sample a listener location within 10 meters radius. (ii) Visual panorama for the robot's source location. We sample RGB camera observations 4 times with rotations of 90 degrees at 256 \u00d7 256 resolution and 90 degrees field-of-view. We discuss the audio data generation below as part of room impulse response (RIR)."}, {"title": "B.2 More details on Room Impulse Response (RIR)", "content": "An impulse response is the output signal (sound) that results when an audio system or environment is excited by an idealized impulse signal at a specific location. An impulse signal has a very short duration and theoretically contains all frequencies.\nWe notionally define the simulated impulse response at the listener's location as w(t), a time-domain waveform. We obtain an impulse response using standard techniques that we define below. Let an impulse be generated at the robot's location s = {$s_x, s_y$} and heard by a listener at location l = {$l_x, l_y$}. Then, using bi-directional ray tracing in simulation, we obtain the maximum sound intensity at the listener location. First, we convert the simulated IR waveform w(t) from the time domain to the frequency domain W(f) with Fast Fourier transform. Second, we compute the sound intensity $I(f) = \\frac{W(f)^2}{(\\rho * c)}$, where $\\rho$ is air density and c is the speed of sound in air.\nWe extract the maximum sound intensity as $I_{max} = max I(f)$. Since the faintest sound human ears can hear is considered $I_o = 10^{-12}W/m^2$, the sound intensity is converted into to decibels by $dB_{max} = 10log_{10} I_{max} + 120$. As we make a modeling design choice to assume an impulse of $1W/m^2$ in simulation, we define 120 dB sound pressure level at the source. A large impulse value ensures a good signal-to-noise ratio while avoiding distortion. To ensure that the decibel values are normalized for training, we assume the highest decibel value as 128 and normalize it between 0 and 1 to get target labels y. For simplicity, we assume that the listener captures a mono-channel audio.\nMore concretely, we do the following steps:\n$w(t)$ = SimulateIR(from = $P_{source}$, at = $P_{receiver}$) \t (1)\n$W(f)$ = Fast Fourier Transform($w(t)$) \t (2)\n$I(f) = (W(f)^2)/(\\rho * c)$ \t (3)\n$I_{max} = max I(f)$ \t (4)\n$I_{max}$ = clip($I_{max}$, min = $10^{-12}$, max = $10^{0.8}$) \t (5)\n$dB_{max} = 10 log_{10} I_{max} + 120$ \t (6)\n$y = dB_{max} / 128$ \t (7)\nHere w(t) is the time-domain waveform generated at the receiver's location, W(f) is the frequency domain waveform, I(f) is the frequency domain sound intensity through air computed by root mean square. The $\\rho$ is air density and c is speed of sound in air. We use the maximum sound intensity and convert it to decibels. To ensure that the decibel values are normalized for training, we assume the highest value of decibel as 128 and compute target labels y. Since the faintest sound human ears can hear is considered $I_o = 10^{-12}W/m^2$, we convert the max sound intensity into to decibels by $dB_{max} = 10log_{10}(I_{max}/I_o) = 10 log_{10} I_{max} + 120$."}, {"title": "B.3 Visualization of Fixed Robot and Fixed Listener maps", "content": "In Section 3.3, we discuss the fixed listener and fixed robot prediction maps for analysis. Figure 15 shows a few of the fixed listener and robot maps, and highlights the potential failure cases of the ANP model."}, {"title": "C Discussion and Broader Scope", "content": "With audio-level prediction, we are one step closer to future home robots. Nevertheless, we must develop reliable and calibrated loudness-aware text-to-speech response and navigation policies. Cur- rent acoustic map predictions must be calibrated to real-world audio and appropriate weighting rel- ative to time cost. Learning from visual features is prone to overfitting. Collecting audio-visual measurement data over a larger, diverse set of environments can help alleviate this. This will even- tually facilitate context-aware robot policies for different loudness sensitivities.\nOur current framework does not address the effects of ambient noise. In this work, we focus on first-order principles that directly involve the robot's audio in relation to the audio. However, in"}]}