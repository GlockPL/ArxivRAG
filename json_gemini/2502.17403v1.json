{"title": "Large Language Models are Powerful EHR Encoders", "authors": ["Stefan Hegselmann", "Georg von Arnim", "Tillmann Rheude", "Noel Kronenberg", "David Sontag", "Gerhard Hindricks", "Roland Eils", "Benjamin Wild"], "abstract": "Electronic Health Records (EHRs) offer rich potential for clinical prediction, yet their inherent complexity and heterogeneity pose significant challenges for traditional machine learning approaches. Domain-specific EHR foundation models trained on large collections of unlabeled EHR data have demonstrated promising improvements in predictive accuracy and generalization; however, their training is constrained by limited access to diverse, high-quality datasets and inconsistencies in coding standards and healthcare practices. In this study, we explore the possibility of using general-purpose Large Language Models (LLMs) based embedding methods as EHR encoders. By serializing patient records into structured Markdown text, transforming codes into human-readable descriptors, we leverage the extensive generalization capabilities of LLMs pretrained on vast public corpora, thereby bypassing the need for proprietary medical datasets. We systematically evaluate two state-of-the-art LLM-embedding models, GTE-Qwen2-7B-Instruct and LLM2Vec-Llama3.1-8B-Instruct, across 15 diverse clinical prediction tasks from the EHRSHOT benchmark, comparing their performance to an EHR-specific foundation model, CLIMBR-T-Base, and traditional machine learning baselines. Our results demonstrate that LLM-based embeddings frequently match or exceed the performance of specialized models, even in few-shot settings, and that their effectiveness scales with the size of the underlying LLM and the available context window. Overall, our findings demonstrate that repurposing LLMs for EHR encoding offers a scalable and effective approach for clinical prediction, capable of overcoming the limitations of traditional EHR modeling and facilitating more interoperable and generalizable healthcare applications.", "sections": [{"title": "1 Introduction", "content": "EHRs have become a widespread technology in modern healthcare, providing a comprehensive, longitudinal view of a patient's health status [1]. Machine learning methods can leverage this rich data to perform risk stratification and support clinical decision making [2\u20134]. In recent years, researchers have explored a variety of prediction tasks based on EHRs, including hospital readmission [5, 6], length of hospital stay [6], sepsis onset detection [7, 8], mortality prediction [6, 9], discharge diagnoses [6], and heart failure outcomes [10]. The overarching objective is to harness existing EHR data through machine learning to enhance clinical outcomes and reduce healthcare costs.\nHowever, machine learning on EHR data poses significant challenges due to its inherent complexity. EHR data is characterized by variable-length sequences of patient visits, irregular sampling intervals, missing entries, heterogeneous and noisy information, and a wide range of hierarchical medical concepts [11]. As a result, deep learning models often achieve only modest improvements over traditional methods such as logistic regression or tree-based methods [6, 12, 13]. To mitigate these issues, recent approaches have employed large-scale foundation models that are pre-trained on unlabeled EHR data using unsupervised learning [14]. Many of these models adopt strategies from natural language processing, such as masked-word prediction as in BERT [15] or autoregressive next-word prediction as in GPT [16]. Treating EHR data as sequences of medical codes, enables analogous methods such as masked code prediction [12, 17\u201319] or next code prediction [13]. However, these techniques also face significant limitations: coding standards and healthcare practices differ strongly across sites, and interoperable EHR foundation models would likely need to be trained on a wide variety of EHR datasets, which is difficult to achieve due to the sensitivity of healthcare data. Therefore, the development of EHR-specific foundation models remains constrained by the limited size and restricted availability of EHR data.\nIn contrast, LLMs benefit from training on vast general-purpose text corpora and a broad range of natural-language tasks [20]. This extensive pre-training enables their language comprehension and allows them to capture domain-agnostic patterns that can be adapted for healthcare applications. Consequently, LLMs have demonstrated strong performance in extracting medical concepts [21], summarizing medical texts [22], and predicting medical outcomes [23] even in low-resource settings. However, most modern LLMs, such as GPT [24] or Llama [25], utilize a decoder-only transformer architecture, which complicates the generation of robust text representations. To overcome this limitation, recent work has introduced methods to convert decoder-only LLMs into effective embedding models for downstream prediction tasks [26\u201329]. Additionally, these state-of-the-art models offer an increased context window, making them well-suited for handling long inputs such as serialized EHR data.\nIn this study, we systematically evaluate whether general-purpose LLM-embedding models can effectively encode EHR records for 15 distinct clinical prediction tasks [30] (see Fig. 1). To this end, we first transform the EHR data into a concise text representation capturing the most relevant patient information at prediction time. We then use two LLM-embedding models, GTE-Qwen2-7B-Instruct [29, 31] and LLM2Vec-Llama3.1-8B-Instruct [26, 32], to generate EHR representations. The resulting embeddings serve as inputs to a logistic regression classifier, which is trained"}, {"title": "2 Results", "content": "2.1 EHR Database and Prediction Tasks\nWe used the EHRSHOT database containing adult patients from the Stanford Health Care and Lucile Packard Children's Hospital from 1990 to February 8th, 2023 [30]. This dataset includes comprehensive EHR timelines for 6.739 patients, covering 921.499 visits, and 41.661.637 clinical events (Table 1). The database is part of a rigorous EHR benchmark containing 15 clinical prediction tasks grouped into four task groups. It also provides canonical dataset splits and provides publicly available code. Table 2 presents a detailed breakdown of the task groups, individual tasks, and the corresponding number of labels used in our experiments. Notably, a single patient may contribute multiple labels for a given prediction task, as relevant clinical events can recur over time."}, {"title": "2.2 EHR Text Serialization", "content": "To utilize an LLM-embedding model for EHR representation, the patient records were encoded into a structured text based on the widely used Markdown format. Due to runtime constraints, the maximum serialization length was limited to 4.096 tokens (approximately 16.000 characters). This constraint informed the serialization strategy, prioritizing the inclusion of recent data to ensure critical medical information was preserved. The serialized record is divided into clearly labeled sections (see example in Fig. 2). All dates were normalized relative to a reference prediction date of January 1st, 2024, explicitly stated at the beginning. This is followed by the patient's basic demographic information. Approximately 65% of the recorded values were time-series data of Logical Observation Identifiers Names and Codes (LOINC) concepts. To reduce the volume of this data, the serialization focuses on 24 frequently recorded concepts, grouped into three primary categories: Body Metrics, Vital Signs, and Lab Results. For each selected concept, the last three recorded values were included, along with the corresponding units and a classification as low, normal, or high, where applicable. The serialization then summarizes all patient visits, including visit type, time and duration, followed by any event not associated with specific visits. Detailed visit records are then provided in descending chronological order, beginning with the most recent visit. Each visit entry is further categorized into conditions, medications, and procedures for improved clarity and utility. As a result, the text serialization ensures a structured and concise representation of EHR data, facilitating efficient processing by LLM-based models."}, {"title": "2.3 LLM-Embedding Models and Baselines", "content": "We focus on two LLM-embedding models as part of our experiments to explore their effectiveness in representing EHR data. The first model, GTE-Qwen2-7B-instruct (GTE-Qwen2-7B), is based on the Qwen2-7B LLM [31] and incorporates bidirectional attention and contrastive learning to enhance embedding tasks [29]. The second model, LLM2Vec-Llama-3.1-8B-Instruct (LLM2Vec-Llama-3.1-8B), is built on the Llama-3.1-8B Instruct architecture [32] and employs similar optimization techniques to improve embeddings [26]. Both models were trained using instructions for the embeddings task. Hence, we added a simple prompt for each task, e.g., \"Given a patient's electronic healthcare record (EHR) in Markdown format, retrieve relevant passages that answer the query: has the patient anemia?\" (see Table 8). Our primary goal was to assess how these LLM-based models, trained on publicly available text data, perform in representing EHRS compared to an EHR-specific foundation model. For this, we included CLMBR-T-Base, a 141-million-parameter autoregressive foundation model trained on 2.57 million de-identified EHRs from Stanford Medicine [13, 30]. For each model, we used a logistic regression classification head trained on the training split, with hyperparameters tuned on the validation split. Additionally, we included a counts-based baseline using a Gradient Boosted Machine (GBM) model, which has demonstrated superior performance over logistic regression for EHR tasks [30]."}, {"title": "2.4 Performance Results on 15 Prediction Tasks", "content": "The performance results measured by the Area Under the Receiver Operating Characteristic Curve (AUROC) for all training and validation examples are presented in Table 3. The GTE-Qwen2-7B model demonstrated superior performance over CLIMBR-T-Base in three of the four task categories: operation outcomes, lab test result prediction, and assignment of new diagnoses. However, in predicting chest X-ray findings, the EHR foundation model (CLIMBR-T-Base) outperformed GTE-Qwen2-7B. The LLM2Vec-Llama-3.1-8B embeddings showed slightly lower performance compared to GTE-Qwen2-7B, surpassing CLIMBR-T-Base only in the assignment of new diagnoses task. Both LLM-based embedding models clearly outperformed the counts-based baseline with a gradient-boosted machine head, which is traditionally regarded as a strong baseline for EHR tasks [12, 13]. The only exception was in the assignment of new diagnoses, where the counts baseline marginally outperformed GTE-Qwen2-7B. We also combined the CLMBR-T-Base embedding with the LLM representations by simple concatenation to test whether they encode orthogonal information. The combined embeddings led to a considerable performance boost for both"}, {"title": "2.5 Performance Results in Few-Shot Setting", "content": "To evaluate model performance with limited training data, we conducted experiments in a few-shot setting using small numbers of training examples. The experiments followed the EHRSHOT task definitions [30]. Fig. 4 illustrates the aggregated AUROC across all subtasks within the four task categories for varying numbers of training examples (see Fig. 6 for AUPRC results). Both LLM-embedding models demonstrated strong results in the few-shot setting, indicating that their pretraining on general text can be successfully transferred to serialized EHR data. Notably, GTE-Qwen2-7B consistently outperformed CLIMBR-T-Base across all training example sizes for predicting lab test results and assigning new diagnoses. For operational outcomes, a minimum of 32 training examples was required for GTE-Qwen2-7B to surpass the performance of the EHR-specific foundation model. In contrast, LLM2Vec-Llama-3.1-8B"}, {"title": "2.6 Effect of Different Contexts Sizes", "content": "We evaluated the impact of varying context sizes on the performance of the LLM-embedding models by testing GTE-Qwen2-7B and LLM2Vec-Llama-3.1-8B with input lengths of 512, 1.024, 2.048, 4.096 (the default), and 8.192 tokens. The objective was"}, {"title": "2.7 Effect of Chunked Contexts", "content": "To investigate whether the models can process the full 4.096-token context cohesively, we conducted an experiment in which the serialized EHR input was divided into chunks of 512, 1.024, and 2.048 tokens. For each chunk, separate embeddings were generated and then averaged to create a final representation (see Table 5). For GTE-Qwen2-7B, the performance decrease with smaller chunks was relatively modest, indicating that the information contained within the full 4.096-token input remains effectively used even when segmented. Notably, using 512-token chunks yielded an overall AUROC performance of 0.735, compared to 0.672 when processing a contiguous 512-token context, which suggests that chunking can mitigate input constraints. In contrast, LLM2Vec-Llama-3.1-8B demonstrated improved performance with chunked inputs, consistent with its behavior on shorter context sizes. This enhancement was primarily driven by better lab value prediction, implying that LLM2Vec-Llama-3.1-8B is particularly effective when processing inputs of up to 2.048 tokens."}, {"title": "2.8 Ablations of EHR Serialization", "content": "To assess the impact of different components of the EHR serialization, we conducted a series of ablation studies for both LLM-embedding models, as summarized in Table 6. The removal of instructions had a notable effect on performance for both GTE-Qwen2-7B and LLM2Vec-Llama-3.1-8B, leading to a decrease from 0.774 (0.749 - 0.798) to 0.725 (0.700 - 0.750) and from 0.742 (0.714 - 0.769) to 0.727 (0.700 - 0.755) in averaged AUROC, respectively. The most significant drop was observed in tasks related to operational outcomes and lab result prediction, suggesting that the instructions played a critical role in guiding the model to focus on relevant clinical information. The inclusion of aggregated information, comprising four body metrics, six vital signs, and 14 lab values, also had a major impact on predictive performance. Removing this information substantially reduced the accuracy of lab test predictions, with performance dropping from 0.867 (0.860 - 0.874) to 0.713 (0.701 - 0.726) for GTE-Qwen2-7B and from 0.777 (0.766 - 0.787) to 0.705 (0.692 - 0.718) for LLM2Vec-Llama-3.1-8B. This highlights the importance of the most recent lab values in predicting future lab results. The effect was particularly pronounced for GTE-Qwen2-7B in predicting conditions such as thrombocytopenia, hyperkalemia, and hyponatremia. However, for hypoglycemia and anemia, GTE-Qwen2-7B performed on par with CLIMBR-T-Base, even though the lab values were still part of the serialized representation (see Fig. 7 and Fig. 8 for task specific performance). Interestingly, the removal of aggregated information led to slight performance improvements in predicting operational outcomes and the assignment of new diagnoses, suggesting that in some cases, a more focused representation may be beneficial. Visit-related information proved to be particularly critical"}, {"title": "3 Discussion", "content": "Our study demonstrates that general-purpose LLM-embedding models, originally pre-trained on extensive natural language corpora, can be repurposed as robust foundation models for EHR prediction tasks. Specifically, models like GTE-Qwen2-7B and LLM2Vec-Llama-3.1-8B not only surpassed a strong counts-based baseline but, in several clinical domains, matched or even exceeded the performance of a dedicated EHR foundation model (CLIMBR-T-Base) [13, 30]. Notably, this is despite CLIMBR-T-Base being trained on data from the same hospital system as the EHRSHOT database, underscoring the strong generalization capabilities of LLM-based embeddings. This is especially significant given the inherent challenges of limited and heterogeneous EHR data. Across 15 diverse clinical tasks, including few-shot scenarios, our findings indicate that the transferable knowledge acquired during large-scale text pretraining enables these models to effectively capture and encode complex clinical information.\nFurthermore, our analysis shows that performance scales with both model size and architectural improvements, suggesting that future advancements in general-purpose LLMs may further enhance their applicability in healthcare settings. In contrast, traditional EHR-specific models face scaling limitations due to restricted data availability and the necessity for specialized architectures [34]. Collectively, these results highlight that repurposing LLMs as embedding generators offers a powerful, flexible, and scalable alternative for clinical prediction tasks.\nOur experiments suggest that several key factors contribute to the success of LLM-embedding models for encoding EHR data. First, providing clear, task-specific instructions within the serialized EHR text was crucial, as it guided the models to focus on the most clinically relevant sections, particularly benefiting tasks that require attention to specific input segments. This design leverages the general-purpose pretraining and further instruction-based fine-tuning of LLM-embedding models [26, 29], enabling them to identify and extract meaningful patterns from heterogeneous and complex EHR inputs. Second, the inclusion of aggregated clinical data, especially semantic"}, {"title": "3.1 Limitations", "content": "This study has several limitations. First, our approach relies on a subjectively designed EHR serialization that we deemed to capture the most medically relevant information. This design choice may introduce bias when comparing LLM-based embedding models with dedicated EHR foundation models that operate on raw data. In addition, the performance of the LLM-embedding models is sensitive to the specific content and instructions provided in the serialized text, which may limit reproducibility and generalization ability. Although these models achieve competitive predictive performance across diverse clinical tasks, including few-shot scenarios, their substantially larger parameter counts lead to longer computation times and higher resource usage. Moreover, by relying on LLM embedding methods, we must train a downstream classifier from scratch, thereby forgoing the inherent zero-shot or few-shot prompting capabilities of LLMs. Finally, our serialization was limited to 4,096 tokens to manage runtime constraints, potentially omitting valuable long-range historical information, and our evaluation on a single institutional dataset may limit broader applicability across diverse healthcare systems."}, {"title": "3.2 Future work", "content": "Future research should explore serialization-free approaches that allow LLMs to process raw EHR data directly, thereby reducing potential biases introduced by manual text transformation. Integrating zero-shot and few-shot prompting into the LLM-based embedding framework could further enhance model flexibility and reduce dependency on downstream training. It will also be important to develop strategies to extend the effective context window beyond 4.096 tokens to capture more comprehensive patient histories. Moreover, investigating techniques for distilling large LLMs into smaller, more efficient models may enhance their practical applicability in clinical settings. Finally, expanding evaluations to multi-institutional datasets and examining how complementary insights from both domain-specific EHR models and general-purpose LLMs can be synergistically combined will be critical for advancing the development of robust, scalable EHR foundation models."}, {"title": "4 Methods", "content": "4.1 EHR Database and Prediction Task\nThe EHR data utilized in our experiments is from the EHRSHOT benchmark for few-shot evaluation of EHR foundation models [30]. We obtained version 2.1 of the dataset, which is accessible via gated access under a research data use agreement. This dataset comprises longitudinal records for 6,739 patients, 921,499 visits, and 41,661,637 clinical"}, {"title": "4.2 EHR Text Serialization", "content": "To leverage LLM-embedding models for representing EHR records, we serialized the records into textual formats. We had to limit the length of the serializations to 4.096 tokens (approximately 16.000 characters) to carry out all experiments on the available computing infrastructure (see Section 4.6). Two experimental runs were conducted with serializations extending up to 8.192 tokens. The primary goal was to create a detailed and informative serialization requiring minimal preprocessing while ensuring that medically relevant information appeared early in the text. This approach mitigated truncation risks in lengthy records, preserving critical details even when older entries were omitted. To convert the visits and clinical events in the EHRSHOT dataset into text, we leveraged the semantic information embedded in the dataset. Each clinical event was labeled using the format \"ontology/code\". EHRSHOT provided a set of prepared ontologies for resolving concept codes into their descriptions,"}, {"title": "4.3 Potential Bias of Manually Defining an EHR Text Serialization", "content": "Defining an EHR serialization involved subjective decisions, which may have introduced bias. For instance, awareness of the prediction tasks could influence the prioritization of certain data elements, potentially favoring task-relevant information. To minimize this risk, we aimed to create an objectively defined serialization that encapsulates key aspects of the EHR records. Also, due to computational constraints,"}, {"title": "4.4 LLM-Embedding Models and Baselines", "content": "In this study, we evaluated two LLM-embedding models, GTE-Qwen2-7B and LLM2Vec-Llama-3.1-8B, based on state-of-the-art decoder-only LLMs. These models were selected for their ability to handle the 4.096-token EHR serializations used in our experiments. For comparison, we also tested a smaller variant of both models. As additional baselines we included commonly used encoder-only embedding models with smaller input sizes (512 tokens). To use them with 4,096 token inputs, the EHR serializations were split into up to eight 512-token chunks, and the resulting embeddings were averaged to generate a single representation. For all LLM-embedding models and smaller language models used in this study, we used the mean pooling of the last layer as the final embedding [26, 44]. The LLM2Vec models used a slight variation that only incorporates the tokens that do not belong to the instruction. Below is an overview of all models:\nGTE-Qwen2-7B\nThis LLM-embedding model is based on the Qwen2-7B-Instruct LLM [31] using a decoder-only Transformer architecture with 28 layers, 28 attention heads, and a hidden size of 3.584. It was trained with autoregressive next token prediction and converted into an embedding model using the General Text Embedding (GTE) method [29]. This conversion replaces causal attention with bidirectional attention, enabling the model to attend to both left and right contexts for token embedding, akin to BERT. Contrastive learning was applied using a mixture of private datasets to enhance embedding performance. The model also incorporates instructions tailored for embedding tasks, supporting a context size of up to 32.000 tokens.\nGTE-Qwen2-1.5B\nA smaller variant of GTE-Qwen2-7B, this model is based on Qwen2-1.5B-Instruct, with 28 layers, 12 attention heads, and a hidden size of 1,536. It was also trained using the GTE method [29] and supports a context size of up to 32.000 tokens."}, {"title": "4.5 Instructions for LLM-Embedding Models", "content": "The GTE and LLM2Vec models used instruction-tuned embeddings, requiring task-specific prompts. Hence, we added simple instructions for each prediction task based on their respective instruction templates. For instance, for prediction of anemia we added \"Given a patient's electronic healthcare record (EHR) in Markdown format, retrieve relevant passages that answer the query: has the patient anemia\". The existing EHRSHOT benchmark encoded the EHRs of the same patient and the identical prediction times only once for efficiency reasons. However, to support task-specific instructions, we had to change this default behavior leading to 1.161.412 instead of 406.379 EHRs to encode resulting in longer processing times. The difference between 1.161.412 labels used in our experiments and the total number of labels of 1.178.665 (Table 2) is since some labels even have the same tasks and prediction time and are merged. We list all instructions in Table 8 and perform ablations testing the effect of the instructions."}, {"title": "4.6 Computational Setup and Running Times", "content": "All experiments were conducted on the Charit\u00e9 High-Performance Cluster using Nvidia A100 GPUs with 80 GB memory, configured with one, four, or eight GPUs (DGX"}, {"title": "4.7 Performance Results on 15 Prediction Tasks and Few-Shot Setting", "content": "Following the EHRSHOT benchmark, we evaluated all models across 15 prediction tasks under various few-shot settings. The benchmark includes a modular pipeline designed to execute key tasks, with the flexibility to optionally utilize a Slurm cluster for distributed execution. Running all steps within this pipeline ensures full reproducibility of results. Step four of the pipeline, which generates EHR representations with CLIMBR-T-Base and the counts-based model, was extended to incorporate our method for creating language model-based EHR representations. This adaptation allowed us to reuse significant portions of the existing code, including the task evaluation framework. Additionally, we implemented new functionality for EHR serialization and slightly modified other steps of the benchmark to accommodate our experimental setup. For instance, the label creation process was adjusted (step three) to enable task-specific instructions for the LLM-embedding models. All modifications have been documented and can be tracked in our public GitHub repository."}, {"title": "4.8 Effect of Different Contexts Sizes", "content": "We investigated the impact of varying context sizes in the LLM-embedding models. We wanted to determine whether encoding information from older visits enhances prediction performance and whether longer inputs might dilute critical details, such as laboratory values, in the final embeddings. Specifically, we evaluated GTE-Qwen2-7B and LLM2Vec-Llama-3.1-8B models with input token limits of 512, 1.024, 2.048, and 8.192 tokens. Input tokens exceeding these thresholds were discarded. Due to the design of our EHR serialization process, additional input tokens primarily consisted of medical concepts from past visits. By testing these varying context sizes, we aimed to assess the balance between capturing historical medical data and preserving the clarity of high-priority information within the embeddings."}, {"title": "4.9 Effect of Chunked Contexts", "content": "To further explore whether LLM-embedding models effectively interpret the entirety of their input, we compared the performance of models processing complete inputs versus segmented (chunked) inputs. For this, the 4.096-token inputs were divided into smaller chunks of sizes 512, 1.024, and 2.048 tokens. Separate embeddings were generated"}, {"title": "4.10 Ablations of EHR Serialization", "content": "To better understand the contribution of various components in the EHR serialization process to the performance of the LLM-embedding models, we conducted a series of ablation experiments. First, we assessed the impact of task-specific instructions by removing them entirely to determine their influence on the final embeddings. Subsequently, we performed additional ablations by systematically excluding specific components of the serialization. These included demographic data, aggregated LOINC codes, and both visit summaries and detailed entries. Furthermore, we examined the effect of removing specific fields from the detailed visit entries, such as conditions, medications, and procedures. Throughout these experiments, the rest of the pipeline was kept consistent to isolate the effects of the removed components. This approach allowed us to identify which parts of the serialization process were most critical for generating effective EHR representations, providing insights into how these models leverage structured medical data for prediction tasks."}]}