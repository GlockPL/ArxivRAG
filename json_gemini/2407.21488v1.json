{"title": "Breaking the Hourglass Phenomenon of Residual Quantization: Enhancing the Upper Bound of Generative Retrieval", "authors": ["Zhirui Kuai", "Zuxu Chen", "Huimu Wang", "Mingming Li", "Dadong Miao", "Binbin Wang", "Xusong Chen", "Li Kuang", "Yuxing Han", "Jiaxing Wang", "Guoyu Tang", "Lin Liu", "Songlin Wang", "Jingwei Zhuo"], "abstract": "Generative retrieval (GR) has emerged as a transformative paradigm in search and recommender systems, leveraging numeric-based identifier representations to enhance efficiency and generalization. Notably, methods like TIGER employing Residual Quantization-based Semantic Identifiers (RQ-SID), have shown significant promise in e-commerce scenarios by effectively managing item IDs. However, a critical issue termed the \"Hourglass\" phenomenon, occurs in RQ-SID, where intermediate codebook tokens become overly concentrated, hindering the full utilization of generative retrieval methods. This paper analyses and addresses this problem by identifying data sparsity and long-tailed distribution as the primary causes. Through comprehensive experiments and detailed ablation studies, we analyze the impact of these factors on codebook utilization and data distribution. Our findings reveal that the \"Hourglass\" phenomenon substantially impacts the performance of RQ-SID in generative retrieval. We propose effective solutions to mitigate this issue, thereby significantly enhancing the effectiveness of generative retrieval in real-world E-commerce applications.", "sections": [{"title": "1 Introduction", "content": "In recent years, GR has surfaced as a ground-breaking retrieval paradigm, marking significant advancements in search and recommendation environments including recommender systems (Rajput et al., 2024; Tan et al., 2024; Wang et al., 2024), search question answering (Liu et al., 2023; Qin et al., 2023), and E-commerce retrieval (Tay et al., 2022; Wang et al., 2022). In this paradigm, target items are initially represented as identifiers (e.g., numbers, subwords, n-grams, token IDs, URLs, semantic codes). Subsequently, leveraging input information such as queries and user details, large models are employed to output the final items in an end-to-end manner. This approach not only enhances retrieval efficiency but also improves the model's generalization capability.\nIn generative retrieval, numeric-based identifier representation methods are widely adopted in the industry due to their simplicity, efficiency, and strong generalization, especially in long behavior sequence recommendations. These methods significantly reduce sequence lengths and accelerate the inference process. Notable methods include DSI (Tay et al., 2022), NCI (Wang et al., 2022), TIGER (Rajput et al., 2024), GDR (Yuan et al., 2024), and GenRet (Sun et al., 2024). Among these, the TIGER method generates Semantic Identifiers (SID) through Residual Quantization (RQ) (Lee et al., 2022; Zeghidour et al., 2021), effectively capturing both semantic information and hierarchical structures. This approach is particularly advantageous in item-dominated e-commerce scenarios, where it accurately reflects the complex hierarchical relationships and semantic features inherent in e-commerce data, thereby significantly enhancing recommendation performance.\nIt is important to highlight that the performance upper bound of RQ-based methods critically depends on the generation of SID. However, we have identified a significant \"hourglass\" phenomenon in SID produced via RQ, as illustrated in Figure 1. Specifically, the codebook tokens in the intermediate layers are excessively concentrated, leading to"}, {"title": "2 Preliminary", "content": "a one-to-many and many-to-one mapping structure. This concentration results in path sparsity, where the matching paths for the item constitute a minimal fraction of the total path space and a long-tail distribution of intermediate layer tokens with a majority of SID concentrated in a few head tokens. This hourglass effect is particularly exacerbated in datasets with long-tail characteristics, which substantially constrains the representational capacity of GR methods. The underlying cause of this issue stems from the intrinsic nature of progressively quantizing high-dimensional vector residuals.\nFurthermore, we analyzed the process of generating SID from residuals, demonstrating that sparsity and long-tail distributions are inevitable. To assess the general impact of SID on downstream GR tasks, we trained models of different scales (such as 0.8B, 7B) and types (Qwen1.5 (Bai et al., 2020), Baichuan2 (Yang et al., 2023), LLaMA2 (Touvron et al., 2023)) based on RQ-SID. Through a series of experiments, including altering the distribution of Semantic IDs by interacting with the first and second layers and swapping tokens between the first and second layers, we not only confirmed the existence of the Hourglass effect but also detailed its specific impact on model performance. This analysis provides a robust foundation for future model optimization.\nTo alleviate the hourglass effect, we propose two straightforward yet effective methods: the heuristic approach and the adaptive variable-length token strategy. The heuristic method involves directly removing the second layer, while curtailing the long-tail impact, it may lead to insufficient spatial capacity. The second method implements an adaptive token distribution adjustment to remove the top tokens from the second layer, thereby transforming the semantic ID into a variable-length structure. This strategy ensures that the overall distribution remains consistent while effectively mitigating the hourglass effect by selectively token removal. Extensive experimental results reveal that although both methods are straightforward, they successfully alleviate the impact of the hourglass effect to varying extents. Notably, the adaptive variable-length token strategy method emerges as the most effective.\nThe contributions of this paper can be summarized as follows:\n\u2022 To our knowledge, this is the first study to systematically investigate the deficiencies of residual quantization-based semantic identifiers in generative retrieval, specifically identifying the \"hourglass\" phenomenon where intermediate layer codebook tokens are overly concentrated.\n\u2022 We conduct thorough experiments and ablation studies that reveal data sparsity and long-tail distributions as the primary causes of the \"hourglass\" effect, limiting the representation and performance capabilities of generative models.\n\u2022 We propose and validate a novel method to alleviate the \"hourglass\" effect, which significantly enhances model performance by improving codebook utilization and addressing token long-tail distributions."}, {"title": "2.1 Residual Quantization", "content": "Residual-quantized is a multi-level vector quantizer that applies quantization on residuals to generate a tuple of codewords (i.e., Semantic IDs). Residual-quantized variational AutoEncoder (RQ-VAE) (Rajput et al., 2024; Lee et al., 2022; Zeghidour et al., 2021) is jointly trained by updating the quantization codebook and the encoder-decoder reconstruction parameters.\nSupport that there is a vector \\(x \\in \\mathbb{R}^D\\), we aim to quantize it using L codebooks (L layer) of M elements each, where codebook could be denoted as \\(C \\in \\mathbb{R}^{L \\times M \\times D}\\), D is the dimension of vector. When \\(l = 1\\), the initial residual is simply defined as \\(r_1 = x\\). Then, \\(r_1\\) is quantized by mapping it to the nearest embedding from that layer's codebook \\(C_l \\in \\mathbb{R}^{M \\times D}\\). The index of the closest embedding at this layer could be computed as follows:\n\\[c_l = \\arg \\min_{m \\in M} || r_l - C_{l,m} ||_2 \\]  (1)\nwhere \\(c_l\\) represents the l-th codeword(semantic ID). Note that, at the l-th layer (\\(l > 1\\)), the residual is:\n\\[r_l = r_{l-1} - C_{l,c_{l-1}}\\] (2)\nThe above process is repeated recursively L times to get a tuple of L codewords that represent the Semantic ID for the given x, denoted as \\((C_1, C_2,..., C_L)\\).\nTo reconstruct the raw vector, we sum the corresponding codebook elements as:\n\\[\\hat{x} = \\sum_{1=0}^{L} C_{l,c_l}\\] (3)"}, {"title": "2.2 Generative Retrieval", "content": "Generative retrieval (Wang et al., 2022; Tay et al., 2022; Tang et al., 2023; Bevilacqua et al., 2022; Zhou et al., 2023), has been proposed in the recommendation field, search field and question-answer field. These models advocate generating identifiers of target passages/items directly through the autoregressive language models.\nIn personalized search scenarios, a core task is to provide the most relevant candidates that the user is likely to purchase based on their given query and historical interaction behaviors. In this paper, we re-frame this task as a Next Token Prediction (NTP) problem utilizing LLM and Semantic ID. Specifically, given user u, query q, and the user's historical item sequence, we first convert the sequence into a Semantic ID sequence, denoted as Seq :=\n\\[(C_{1,1}, \\cdots, C_{1,M}); (C_{2,1}, \\cdots, C_{2,M}); ...; (C_{t,1}, \\cdots, C_{t,M})\\]\n\\(\\text{item}_1\\) \\(\\text{item}_2\\)  \\(\\text{item}_t\\)\nwhere \\((C_{i,1}, \\cdot, C_{i,M})\\) denotes the M-length Semantic ID for \\(\\text{item}_i\\). The LLM is then trained to predict the Semantic ID of \\(\\text{item}_{t+1}\\), represented as \\((C_{t+1,1}, \\cdot, C_{t+1,M})\\). The generation objective could be formulated as,\n\\[\\mathcal{L}_{SFT} = - \\sum_{i} \\log p_{\\theta}(i|q, u, Seq, I_{<i}) \\] (4)\nwhere \\(I_{<i} = \\{C_{t+1,1},\u2026\u2026\u2026, C_{t+1,i}\\} \\), \\(p_{\\theta}\\) is the supervised fine-tuning (SFT) model."}, {"title": "3 Problem of GR based on RQ", "content": "3.1 Hourglass Phenomenon\nTo generate the semantic IDs used RQ, we first leverage the query-item data from billions of search logs within the company to train dual-tower models such as DSSM and BERT (Fan et al., 2019; Li et al., 2023b; Karpukhin et al., 2020; Li et al., 2023a; Qiu et al., 2022; Wang et al., 2023). Subsequently, we obtain the embeddings for hundreds of millions of items using the item tower. Finally, we employ RQ to generate semantic IDs for all items.\nUpon the successful generation of semantic IDs, we proceed to aggregate and compute the three-layer distribution maps for all items. As illustrated in Figure 2, it is evident that the second layer of the Semantic ID architecture is concentrated with a substantial number of routing nodes. The overall distribution of the three-layer code exhibits an hourglass phenomenon.\nTo investigate the generalizability of this phenomenon, we conducted multiple visualization experiments under various parameter combinations, e.g., code table size and number of layers. As shown in Figure 6 in the appendix, the results indicate that the hourglass effect is highly pronounced, and the path distribution among the tokens across the three layers of the code table is relatively sparse.\nAdditionally, based on the aforementioned experiments, we conducted statistical analyses of the token distribution in the second layer using three metrics: entropy (Shannon, 1948), Gini coefficient (Yitzhaki, 1979), and standard deviation (Pal et al., 2019), as shown in the Figure 3. The results indicate that the token distribution in the second layer exhibits low entropy, high Gini coefficient, and large standard deviation, suggesting that the distribution is highly skewed and exhibits a long-tail effect.\nOverall, this hourglass phenomenon is statistically evidenced in the code table by path sparsity and a long-tail distribution of tokens. 1) Path sparsity, resulting from the Semantic ID structure, leads to low code table utilization. 2) The long-tail distribution indicates that in the intermediate layer, a predominant number of routes converge on a single token."}, {"title": "3.2 Analysis of Residual Quantization", "content": "To explore the causes of the hourglass phenomenon, we will conduct an in-depth analysis and discussion based on the operating mechanism of the RQ. Without loss of generality, we consider two distributions of raw embedding: un-uniform and uniform, denoted as \\(X = \\{x|x \\in \\mathcal{X}\\} \\in \\mathbb{R}^{N \\times M}\\), N is the size"}, {"title": "3.3 Impact on the GR", "content": "In the above section, we have discussed the long-tail distribution in the second layer of Semantic ID, indicating a one-to-many and many-to-one structure. We argue that this phenomenon significantly impacts the generation of downstream tasks, especially for generative retrieval task.\nTo measure this impact, we conducted various experiments. First, we altered the distribution of Semantic ID by interacting with the first and second"}, {"title": "4 Methods and Experiments", "content": "To alleviate the hourglass effect, we propose two simple yet effective methods."}, {"title": "4.1 Heuristic Method", "content": "One heuristic approach is to directly remove the second layer, eliminating the impact of the long tail. However, it can lead to insufficient spatial capacity, i.e., \\(M^L \\rightarrow M^{L-1}\\). Note that, here needs first to generate an L-layer SID and then remove the second layer, which differs from directly generating a two-layer SID, where large routing nodes may still exist."}, {"title": "4.2 Variable Length of SID", "content": "Another simple method is to adaptively remove the top tokens of the second layer, making the semantic ID a variable-length structure. Here, a top@K strategy is used, with p as a threshold. This approach ensures that the distribution remains unchanged while reducing the impact of the hourglass effect selectively. What's more, the spatial capacity is sufficient, i.e., \\(M^L \\rightarrow M^L + K(M^{L-2} \u2013 M^{L-1})\\)."}, {"title": "4.3 Experiments", "content": "To further validate the effectiveness of the method, experiments are conducted on the LLaMA model. Results indicate that by applying the adaptive token removal strategy, the performance of the model is improved while maintaining a similar computational cost compared to the base model, and several"}, {"title": "4.4 Valid Ratio", "content": "During the autoregressive decoding process, as the model decodes the next token of the target SID, it may predict invalid SIDs, SIDs that are not in the SID's vocabulary, or do not correspond to any item in the full dataset. Therefore, we have calculated the proportion of invalid SIDs on the LLaMA2-0.8B model with RQ3x12. As shown in Figure 5, we can see the base model, the invalid ratio of the proposed method is lower than the base model, indicating that the higher-quality generation items with a lower ratio of hallucination. Furthermore, when the number of recalls is less than 10, the invalid ratio is below 5%. Thus, the effectiveness of generation is to meet practical needs. In other situations, where a higher number of recalls is required (k=50), the invalid ratio is higher. Therefore,"}, {"title": "5 Conclusion", "content": "This study systematically explores the limitations of RQ-SID in GR, particularly identifying the \"hourglass\" phenomenon in the intermediate layer where codebook tokens are overly concentrated, leading to data sparsity and long-tail distribution. Through extensive experiments and ablation studies, we have demonstrated the existence of this phenomenon and conducted an in-depth analysis attributing its root cause to the characteristics of residuals. To alleviate this issue, we propose two methods: a heuristic approach that removes the second layer and a variable-length token strategy that adaptively adjusts token distribution. Experimental results show both methods effectively mitigate the bottleneck effect, with the adaptive token distribution adjustment yielding the best results. To the best of our knowledge, this is the first systematic exploration of the deficiencies of RQ-SID in GR, providing a solid foundation for future model optimizations and significantly enhancing model performance by improving codebook utilization."}]}