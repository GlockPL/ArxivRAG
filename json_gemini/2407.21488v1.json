{"title": "Breaking the Hourglass Phenomenon of Residual Quantization: Enhancing the Upper Bound of Generative Retrieval", "authors": ["Zhirui Kuai", "Zuxu Chen", "Huimu Wang", "Mingming Li", "Dadong Miao", "Binbin Wang", "Xusong Chen", "Li Kuang", "Yuxing Han", "Jiaxing Wang", "Guoyu Tang", "Lin Liu", "Songlin Wang", "Jingwei Zhuo"], "abstract": "Generative retrieval (GR) has emerged as a transformative paradigm in search and recommender systems, leveraging numeric-based identifier representations to enhance efficiency and generalization. Notably, methods like TIGER employing Residual Quantization-based Semantic Identifiers (RQ-SID), have shown significant promise in e-commerce scenarios by effectively managing item IDs. However, a critical issue termed the \"Hourglass\" phenomenon, occurs in RQ-SID, where intermediate codebook tokens become overly concentrated, hindering the full utilization of generative retrieval methods. This paper analyses and addresses this problem by identifying data sparsity and long-tailed distribution as the primary causes. Through comprehensive experiments and detailed ablation studies, we analyze the impact of these factors on codebook utilization and data distribution. Our findings reveal that the \"Hourglass\" phenomenon substantially impacts the performance of RQ-SID in generative retrieval. We propose effective solutions to mitigate this issue, thereby significantly enhancing the effectiveness of generative retrieval in real-world E-commerce applications.", "sections": [{"title": "1 Introduction", "content": "In recent years, GR has surfaced as a ground-breaking retrieval paradigm, marking significant advancements in search and recommendation environments including recommender systems (Rajput et al., 2024; Tan et al., 2024; Wang et al., 2024), search question answering (Liu et al., 2023; Qin et al., 2023), and E-commerce retrieval (Tay et al., 2022; Wang et al., 2022). In this paradigm, target items are initially represented as identifiers (e.g., numbers, subwords, n-grams, token IDs, URLs, semantic codes). Subsequently, leveraging input information such as queries and user details, large models are employed to output the final items in an end-to-end manner. This approach not only enhances retrieval efficiency but also improves the model's generalization capability.\nIn generative retrieval, numeric-based identifier representation methods are widely adopted in the industry due to their simplicity, efficiency, and strong generalization, especially in long behavior sequence recommendations. These methods significantly reduce sequence lengths and accelerate the inference process. Notable methods include DSI (Tay et al., 2022), NCI (Wang et al., 2022), TIGER (Rajput et al., 2024), GDR (Yuan et al., 2024), and GenRet (Sun et al., 2024). Among these, the TIGER method generates Semantic Identifiers (SID) through Residual Quantization (RQ) (Lee et al., 2022; Zeghidour et al., 2021), effectively capturing both semantic information and hierarchical structures. This approach is particularly advantageous in item-dominated e-commerce scenarios, where it accurately reflects the complex hierarchical relationships and semantic features inherent in e-commerce data, thereby significantly enhancing recommendation performance.\nIt is important to highlight that the performance upper bound of RQ-based methods critically depends on the generation of SID. However, we have identified a significant \"hourglass\" phenomenon in SID produced via RQ, as illustrated in Figure 1. Specifically, the codebook tokens in the intermediate layers are excessively concentrated, leading to a one-to-many and many-to-one mapping structure. This concentration results in path sparsity, where the matching paths for the item constitute a minimal fraction of the total path space and a long-tail distribution of intermediate layer tokens with a majority of SID concentrated in a few head tokens. This hourglass effect is particularly exacerbated in datasets with long-tail characteristics, which substantially constrains the representational capacity of GR methods. The underlying cause of this issue stems from the intrinsic nature of progressively quantizing high-dimensional vector residuals.\nFurthermore, we analyzed the process of generating SID from residuals, demonstrating that sparsity and long-tail distributions are inevitable. To assess the general impact of SID on downstream GR tasks, we trained models of different scales (such as 0.8B, 7B) and types (Qwen1.5 (Bai et al., 2020), Baichuan2 (Yang et al., 2023), LLaMA2 (Touvron et al., 2023)) based on RQ-SID. Through a series of experiments, including altering the distribution of Semantic IDs by interacting with the first and second layers and swapping tokens between the first and second layers, we not only confirmed the existence of the Hourglass effect but also detailed its specific impact on model performance. This analysis provides a robust foundation for future model optimization.\nTo alleviate the hourglass effect, we propose two straightforward yet effective methods: the heuristic approach and the adaptive variable-length token strategy. The heuristic method involves directly removing the second layer, while curtailing the long-tail impact, it may lead to insufficient spatial capacity. The second method implements an adaptive token distribution adjustment to remove the top tokens from the second layer, thereby transforming the semantic ID into a variable-length structure. This strategy ensures that the overall distribution remains consistent while effectively mitigating the hourglass effect by selectively token removal. Extensive experimental results reveal that although both methods are straightforward, they successfully alleviate the impact of the hourglass effect to varying extents. Notably, the adaptive variable-length token strategy method emerges as the most effective.\nThe contributions of this paper can be summarized as follows:\n\u2022 To our knowledge, this is the first study to systematically investigate the deficiencies of residual quantization-based semantic identifiers in generative retrieval, specifically identifying the \"hourglass\" phenomenon where intermediate layer codebook tokens are overly concentrated.\n\u2022 We conduct thorough experiments and ablation studies that reveal data sparsity and long-tail distributions as the primary causes of the \"hourglass\" effect, limiting the representation and performance capabilities of generative models.\n\u2022 We propose and validate a novel method to alleviate the \"hourglass\" effect, which significantly enhances model performance by improving codebook utilization and addressing token long-tail distributions."}, {"title": "2 Preliminary", "content": "2.1 Residual Quantization\nResidual-quantized is a multi-level vector quantizer that applies quantization on residuals to generate a tuple of codewords (i.e., Semantic IDs). Residual-quantized variational AutoEncoder (RQ-VAE) (Rajput et al., 2024; Lee et al., 2022; Zeghidour et al., 2021) is jointly trained by updating the quantization codebook and the encoder-decoder reconstruction parameters.\nSupport that there is a vector x \u2208 RD, we aim to quantize it using L codebooks (L layer) of M elements each, where codebook could be denoted as C \u2208 RL\u00d7M\u00d7D, D is the dimension of vector. When l = 1, the initial residual is simply defined as r\u2081 = x. Then, r\u012b is quantized by mapping it to the nearest embedding from that layer's codebook Ci \u2208 RM\u00d7D. The index of the closest embedding at this layer could be computed as follows:\nc\u2081 = arg minm\u2208M || r \u2013 C\u0131,m ||2 (1)\nwhere ca represents the l-th codeword(semantic ID). Note that, at the l-th layer (l > 1), the residual is:\nr = rl-1 \u2013 Cl,ci-1 (2)\nThe above process is repeated recursively L times to get a tuple of L codewords that represent the Semantic ID for the given x, denoted as (C1, C2,..., CL).\nTo reconstruct the raw vector, we sum the corresponding codebook elements as:\nx = \u2211L1=0Cl,c (3)\nThis method could approximate the raw vector from a coarse-to-fine granularity by the norm of residuals decreasing, i.e., ||x \u2212 x||2 < \u0454, \u0454 < 0.001."}, {"title": "2.2 Generative Retrieval", "content": "Generative retrieval (Wang et al., 2022; Tay et al., 2022; Tang et al., 2023; Bevilacqua et al., 2022; Zhou et al., 2023), has been proposed in the recommendation field, search field and question-answer field. These models advocate generating identifiers of target passages/items directly through the autoregressive language models.\nIn personalized search scenarios, a core task is to provide the most relevant candidates that the user is likely to purchase based on their given query and historical interaction behaviors. In this paper, we re-frame this task as a Next Token Prediction (NTP) problem utilizing LLM and Semantic ID. Specifically, given user u, query q, and the user's historical item sequence, we first convert the sequence into a Semantic ID sequence, denoted as Seq :=\n(C1,1, *, C1,M); (C2,1, *, C2,M); ...; (Ct,1,, Ct,M)\nitem1\nitem2\nitemt\nwhere (Ci,1,\u00b7, Ci,M) denotes the M-length Semantic ID for item. The LLM is then trained to predict the Semantic ID of itemt+1, represented as (Ct+1,1, \u00b7, Ct+1,M). The generation objective could be formulated as,\nLsft = \u2212 \u2211Milog po(iq, u, Seq, I<i) (4)\ni\nwhere I<i = {Ct+1,1,\u2026\u2026\u2026, Ct+1,i}, po is the supervised fine-tuning (SFT) model."}, {"title": "3 Problem of GR based on RQ", "content": "3.1 Hourglass Phenomenon\nTo generate the semantic IDs used RQ, we first leverage the query-item data from billions of search logs within the company to train dual-tower models such as DSSM and BERT (Fan et al., 2019; Li et al., 2023b; Karpukhin et al., 2020; Li et al., 2023a; Qiu et al., 2022; Wang et al., 2023). Subsequently, we obtain the embeddings for hundreds of millions of items using the item tower. Finally, we employ RQ to generate semantic IDs for all items.\nUpon the successful generation of semantic IDs, we proceed to aggregate and compute the three-layer distribution maps for all items. As illustrated in Figure 2, it is evident that the second layer of the Semantic ID architecture is concentrated with a substantial number of routing nodes. The overall distribution of the three-layer code exhibits an hourglass phenomenon.\nTo investigate the generalizability of this phenomenon, we conducted multiple visualization experiments under various parameter combinations, e.g., code table size and number of layers. As shown in Figure 6 in the appendix, the results indicate that the hourglass effect is highly pronounced, and the path distribution among the tokens across the three layers of the code table is relatively sparse.\nAdditionally, based on the aforementioned experiments, we conducted statistical analyses of the token distribution in the second layer using three metrics: entropy (Shannon, 1948), Gini coefficient (Yitzhaki, 1979), and standard deviation (Pal et al., 2019), as shown in the Figure 3. The results indicate that the token distribution in the second layer exhibits low entropy, high Gini coefficient, and large standard deviation, suggesting that the distribution is highly skewed and exhibits a long-tail effect.\nOverall, this hourglass phenomenon is statistically evidenced in the code table by path sparsity and a long-tail distribution of tokens. 1) Path sparsity, resulting from the Semantic ID structure, leads to low code table utilization. 2) The long-tail distribution indicates that in the intermediate layer, a predominant number of routes converge on a single token."}, {"title": "3.2 Analysis of Residual Quantization", "content": "To explore the causes of the hourglass phenomenon, we will conduct an in-depth analysis and discussion based on the operating mechanism of the RQ. Without loss of generality, we consider two distributions of raw embedding: un-uniform and uniform, denoted as X = {x|x \u2208 X} \u2208 RN\u00d7M, N is the size of the dataset. Now, we use the RQ to produce the semantic ID for X.\nIn the first layer, all candidate's points are divided into M different cluster buckets. Each cluster bucket contains nm data points and has a radius of em. For the uniform distribution, nm = N/M, and e\u2081 = e2 = . . . = em. Therefore, the in-degree of all tokens in this layer are equal.\nIn the second layer, all input embedding is X', the residual of the first layer. Due to the difference in the magnitude of residual values, the input distribution in this layer is non-uniform. There are a large number of points with smaller magnitudes (points near the cluster centers in each bucket from the previous layer), which is equal to nm * M * p = N * p, p is the ratio. At the same time, there are small points with larger magnitudes, which are considered as outliers. To reduce the clustering loss, the clustering process in this layer focuses on these outliers. As a result, the points with smaller magnitudes will occupy fewer cluster centers, while the outliers will either occupy individual cluster centers or multiple cluster centers. Therefore, this layer's semantic IDs will form large routing nodes, exhibiting a long-tail phenomenon, which is also demonstrated in the second layer of Figure 4.\nIn the third layer, all input point magnitudes become consistent again and relatively uniform. Therefore, the code distribution in this layer is similar to the first layer, with a uniform distribution. As a result, it can be directly observed that the large routing nodes from the second layer diverge into multiple smaller nodes in the third layer, creating a one-to-many situation, as shown in the third layer of Figure 4. At the same time, if the residuals in the second layer tend towards zero, there will still be some clustering in the third layer. However, since all magnitudes are very small at this point, the impact of the clustering effect is limited.\nAs we continue to iterate through the layers, this phenomenon of non-uniform distribution and long-tail clustering followed by uniform distribution will alternate. However, as the number of layers increases, the residuals become smaller (refer to layer 4 of Figure 4), and the clustering effect weakens, so it can be ignored. Ultimately, this leads to the formation of an hourglass-like structure, where the input data is first compressed into a smaller number of clusters, then expands back out into a larger number of clusters, and finally converges to a uniform distribution. Upon the completion of SID construction, the influence of the RQ quantization method, coupled with the dominance of head tokens in the intermediate layer, naturally leads to the sparsity of paths.\nSimilarly, for the un-uniform distribution, such as long-tail distribution, the residual distribution becomes even more uneven, resulting in a more severe phenomenon."}, {"title": "3.3 Impact on the GR", "content": "In the above section, we have discussed the long-tail distribution in the second layer of Semantic ID, indicating a one-to-many and many-to-one structure. We argue that this phenomenon significantly impacts the generation of downstream tasks, especially for generative retrieval task.\nTo measure this impact, we conducted various experiments. First, we altered the distribution of Semantic ID by interacting with the first and second layers. On this basis, we only predicted the tokens of the second and third layers while keeping the tokens of the first layer fixed.\nDuring the evaluation process, we divide the test set into two groups according to the distribution of second-layer tokens: the head token test set and the tail token test set. As shown in Table 1, the performance of the head token test set significantly improved, whereas the performance of the tail token test set was notably poorer. This performance disparity can be attributed to the previously analyzed path sparsity and long-tail distribution of tokens, leading to biased results. This phenomenon has been observed across models of different scales (LLaMA2, Baichuan2, and Qwen1.5) and different parameters of RQ, highlighting the widespread impact of long-tail token distribution and path sparsity on model performance.\nTo further investigate the impact of the hourglass phenomenon on model performance, we conduct two critical experiments: 1) exchange the tokens of the first and second layers, and 2) give the first token of the swapped sequence as input. These approaches aim to mitigate the effects of the long-tail distribution, and results verify a significant improvement. This finding indicates that the hourglass phenomenon has a substantial negative impact on model performance. Through the above experiments, we not only confirmed the existence of the hourglass effect but also elucidated its specific impact on model performance, thereby providing a robust basis for future optimization."}, {"title": "4 Methods and Experiments", "content": "To alleviate the hourglass effect, we propose two simple yet effective methods.\n4.1 Heuristic Method\nOne heuristic approach is to directly remove the second layer, eliminating the impact of the long tail. However, it can lead to insufficient spatial capacity, i.e., ML \u2192 ML-1. Note that, here needs first to generate an L-layer SID and then remove the second layer, which differs from directly generating a two-layer SID, where large routing nodes may still exist.\n4.2 Variable Length of SID\nAnother simple method is to adaptively remove the top tokens of the second layer, making the semantic ID a variable-length structure. Here, a top@K strategy is used, with p as a threshold. This approach ensures that the distribution remains unchanged while reducing the impact of the hourglass effect selectively. What's more, the spatial capacity is sufficient, i.e., ML \u2192 ML + K(ML\u22122 \u2013 ML\u22121).\n4.3 Experiments\nTo further validate the effectiveness of the method, experiments are conducted on the LLaMA model. Results indicate that by applying the adaptive token removal strategy, the performance of the model is improved while maintaining a similar computational cost compared to the base model, and several objective optimizations, such as Focal Loss (Lin et al., 2017) and Mile Loss (Su et al., 2024). Specifically, experimental results showed that the model with top@400 token removal outperforms the baseline model in terms of most evaluation metrics. This suggests that the method effectively reduces the impact of the long-tail effect. As the number of tokens removed increases, the performance improvement of the model encounters a bottleneck. Especially when all tokens are removed, this limitation is particularly pronounced, which is presumed to be due to the absence of long-tail tokens, resulting in a loss of recall. At the same time, removing the second layer directly will cause one SID to correspond to multiple items.\nThis fine-grained analysis provides strong evidence for the effectiveness of the proposed method, which selectively removes less important tokens while retaining the most informative ones, leading to improved model performance even when a substantial amount of data is removed."}, {"title": "4.4 Valid Ratio", "content": "During the autoregressive decoding process, as the model decodes the next token of the target SID, it may predict invalid SIDs, SIDs that are not in the SID's vocabulary, or do not correspond to any item in the full dataset. Therefore, we have calculated the proportion of invalid SIDs on the LLaMA2-0.8B model with RQ3x12. As shown in Figure 5, we can see the base model, the invalid ratio of the proposed method is lower than the base model, indicating that the higher-quality generation items with a lower ratio of hallucination. Furthermore, when the number of recalls is less than 10, the invalid ratio is below 5%. Thus, the effectiveness of generation is to meet practical needs. In other situations, where a higher number of recalls is required (k=50), the invalid ratio is higher. Therefore, it is necessary to employ the retrieval augmented generation (RAG) (Lewis et al., 2020; Ding et al., 2024) for processing during the inference process, such as prefix-tree (Beurer-Kellner et al., 2024), and FM_Index (Herruzo et al., 2021)."}, {"title": "5 Conclusion", "content": "This study systematically explores the limitations of RQ-SID in GR, particularly identifying the \"hourglass\" phenomenon in the intermediate layer where codebook tokens are overly concentrated, leading to data sparsity and long-tail distribution. Through extensive experiments and ablation studies, we have demonstrated the existence of this phenomenon and conducted an in-depth analysis attributing its root cause to the characteristics of residuals. To alleviate this issue, we propose two methods: a heuristic approach that removes the second layer and a variable-length token strategy that adaptively adjusts token distribution. Experimental results show both methods effectively mitigate the bottleneck effect, with the adaptive token distribution adjustment yielding the best results. To the best of our knowledge, this is the first systematic exploration of the deficiencies of RQ-SID in GR, providing a solid foundation for future model optimizations and significantly enhancing model performance by improving codebook utilization."}]}