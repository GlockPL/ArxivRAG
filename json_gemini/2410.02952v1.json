{"title": "Visual Editing with LLM-based Tool Chaining: An Efficient Distillation Approach for Real-Time Applications", "authors": ["Oren Sultan", "Alex Khasin", "Guy Shiran", "Asnat Greenstein-Messica", "Dafna Shahaf"], "abstract": "We present a practical distillation approach to fine-tune LLMs for invoking tools in real-time applications. We focus on visual editing tasks; specifically, we modify images and videos by interpreting user stylistic requests, specified in natural language (\"golden hour\"), using an LLM to select the appropriate tools and their parameters to achieve the desired visual effect.\nWe found that proprietary LLMs such as GPT-3.5-Turbo show potential in this task, but their high cost and latency make them unsuitable for real-time applications. In our approach, we fine-tune a (smaller) student LLM with guidance from a (larger) teacher LLM and behavioral signals. We introduce offline metrics to evaluate student LLMs. Both online and offline experiments show that our student models succeeded in matching the performance of our teacher model (GPT-3.5-Turbo), significantly reducing costs and latency. Lastly, we show that fine-tuning was improved by 25% in low-data regimes using augmentation.", "sections": [{"title": "Introduction", "content": "Videos are a powerful communication and storytelling medium, gaining popularity through social media and video-sharing platforms. This surge has inspired many to create content. However, the complexity of video editing, with its numerous parameters and their interactions, poses significant barriers for beginners (Zhang et al., 2022).\nUsing natural language as an interaction medium for video editing can mitigate this challenge. Text-to-video, diffusion-based models that support instruction-guided video editing have demonstrated impressive results. However, they are computationally expensive, slow, and still lack in visual quality and user control over the generated video (Geyer et al., 2023; Couairon et al., 2023; Qi et al., 2023). This makes them unsuitable for real-time mobile applications, which need to combine high editing quality, low execution cost and fast response.\nWe believe that instead of relying on an end-to-end approach that treats deep learning models as black boxes, it is more beneficial to teach LLMs to use existing, specialized tools. This approach is also more interpretable. We are encouraged by recent advances in LLMs that demonstrated the effectiveness of building AI agents that leverage multiple external tools with LLMs (Schick et al., 2024; Wang et al., 2023; OpenAI, 2023b), in particular for vision or vision-language tasks (Liu et al., 2023; Yang et al., 2024; Wu et al., 2023).\nIn our work, we leverage LLMs to invoke existing, traditional video editing tools that are specialized for our task. Our aim is to implement an AI assistant in our video editing mobile app, democratizing advanced capabilities. As a proof-of-concept, we focused on tonal color adjustments, allowing users to change a video's appearance via textual instructions (e.g., \"golden hour\"; see Figure 1).\nLearning tool chaining through prompt engineering and in-context learning often relies on proprietary LLMs like GPT-3.5 (Yang et al., 2024). These models are expensive, not publicly available, and slow, posing significant challenges for online production systems. We propose a distillation approach based on fine-tuning an open source (smaller) student LLM for tools usage using the output from a (larger) teacher LLM, enhanced by user behavioral signals.\nWe create offline metrics to evaluate model performance, involving the choice of tools to apply and their parameters. This evaluation is challenging due to continuous parameter values and to our creativity-focused use case, with no single correct answer. Finally, we develop a data augmentation scheme and demonstrate a 25% improvement in the common real-life scenario of low-data regimes.\nOur contributions are: (1) We propose a practical distillation method to fine-tune open-source"}, {"title": "Problem Statement", "content": "Our visual editing task deals with color grading a post-processing procedure that alters the appearance of an image or a video by adjusting its tonal colors. Our application features three tonal adjustment tools: global adjust (global color range), selective adjust (selective color ranges), and filters.\nEach tool has up to a dozen parameters, which can be difficult for beginners to set correctly.\nIn our task, the user provides an asset (image/video) and a free-text description of the requested appearance. This raises the following challenges: (1) How to interpret the user's intent, which can be vague or require specific knowledge (e.g., given \"The Matrix\u201d request, it should recognize the distinctive imagery associated with the movie, characterized by a green tint, high contrast, and cyberpunk aesthetic). (2) How to decide which tools to use and with what parameters and values. More formally, the AI Assistant's function, $f : I \\rightarrow O$, maps a user's intent (I) into a tailored configuration of tools and settings (O), interpreting and implementing the user's intent.\nThe output is of the following form:\n$O = \\{(T_i, P_i) | T_i \\in T, P_i \\in P(T_i)\\}$   (1)\nwhere T is the set of the available tools, and $P(T_i)$ is the power set of all possible parameter-value pairs for the tool $T_i$, including the empty set for when the tool is not used. We denote $P_i$ as the set of parameter-value pairs for the i-th tool:\n$P_i = \\{(p_{i1}, v_{i1}), (p_{i2}, v_{i2}), ..., (p_{in}, v_{in})\\}$   (2)\nwhere $p_{ij}$, $v_{ij}$ are $T_i$'s j-th parameter and value.\nFigure 1 shows examples of various input images (top), with intents (middle), and outputs (bottom).\nSee Appendix A.1 for details on tool parameters."}, {"title": "Our Distillation Framework Approach", "content": "Our goal is to automate our visual editing task using LLMs. In our proof-of-concept, we found that proprietary LLMs, like GPT-3.5-Turbo, can solve this task using preliminary prompts (based on an evaluation conducted by five experts from our team, who assessed the results across 20 different inputs). However, their high cost and latency make them unsuitable for real-time industry applications.\nWe employ a distillation framework approach (see Figure 2). We generate data by collecting outputs from a teacher LLM based on user intents. The teacher LLM selects relevant tools and sets their parameters. If multiple users express the same intent, this could result in multiple outputs per intent. We ensure high-quality data by retaining the best results based on user feedback, filtering out those with no engagement. The retained data samples are injected into prompts for fine-tuning and are randomly split into training and test sets (\u00a73.1). Then, we fine-tune a much smaller student LLM on this dataset (\u00a73.2). Since comparing between fine-tuned models in an online A/B test is costly and takes time, we design offline evaluation (\u00a73.3) metrics to predict the model's performance online. To improve fine-tuning in low-data settings, we use augmentation by having another LLM generate similar samples for those the student LLM got wrong during training, then add these to the training set (\u00a73.4). Finally, to compare the actual performance of two fine-tuned student LLMs, we conduct an online A/B test (\u00a73.5)."}, {"title": "Data Collection", "content": "Our goal is to collect high-quality data using a teacher LLM's outputs to existing user intents for fine-tuning a student LLM.\nGathering Teacher LLM Outputs. We use GPT-3.5-Turbo\u00b9 as the teacher due to its cost/performance tradeoff. Initially, it was deployed in our video app, serving users for four months, during which we collected data for fine-tuning. A data row includes: (1) The user's intent with the requested vibe (e.g., \u201cx-ray\u201d). (2) The output of the teacher LLM to this intent, including the tools to use and their parameters. (3) Whether the user exports the result per tool (highly satisfied\nhttps://platform.openai.com/docs/models/gpt-3-5-turbo users export results). We filter out samples with zero exports (~80%) to train on high-quality data. Our teacher LLM can generate different outputs per intent (across different calls); we take as ground truth the result that maximizes the export rate.\nIn our teacher prompts, we included one-shot example for user intent, with an output of the rationale (a free text explanation of the reasoning, how to achieve the intent by adjusting parameters) as well as the output parameters for the tool. Integrating similar Chain-of-Thought (CoT) mechanisms has been shown to enhance LLMs' performance (Wei et al., 2022) and interpretability. Refer to Appendix A.3 for our teacher LLM implementation details, and Appendix A.4 Figures 3, 4, and 5 for the three prompts (one per tool) we used.\nIn total, we collected 9,252 unique user intents, each paired with corresponding teacher outputs for the three tonal adjustment tools, resulting in 27,756 data points. See Appendix A.2 for statistics on the distribution of different parameter's values across the different tools observed in the dataset.\nData Processing for Fine-Tuning. We used the collected data to fine-tune a student LLM, using three prompts. These prompts for the student were more concise those for the teacher, as the student would be fine-tuned on thousands of examples (instead of one-shot). We decided not to request rationale from the student, as we prioritize low latency, and generating the reasoning significantly increases the response time. See Appendix A.5 Figures 9, 10,"}, {"title": "Supervised Fine-Tuning (SFT)", "content": "Our goal is to fine-tune a student LLM to mimic a teacher LLM outputs (filtered using behavioral signals). Using our collected dataset for fine-tuning, $D = \\{(x, y)\\}$, where x is the prompt (user's intent and task instructions) and y is the teacher LLM\u2019s output. We fine-tune two types of LLMs.\nAuto-Regressive Model. We fine-tune a decoder-only LLM to generate y = {$y_1, ..., y_n$} using the auto-regressive LLM objective, which maximizes the expected log-likelihood (Radford et al., 2019):\n$L(\\theta) = \\sum_{t=1}^{T}log P(y_t | y_1, y_2, ..., y_{t-1}; \\theta)$    (3)\nWe aim to maximize the log probability of the target word yt given prior words ($y_1, ..., y_{t\u22121}$) with model parameters \u03b8. We used the Llama-2-7b-chat-hf (Touvron et al., 2023) (see 4.1 for details).\nSequence-to-Sequence Model. We fine-tune an encoder-decoder LLM to generate y = {$y_1,..., y_n$} using the sequence-to-sequence LLM objective, which maximizes the expected log-likelihood (Sutskever et al., 2014):\n$L(\\theta) = \\sum_{t=1}^{T}log P(y_t | y_1, y_2, ..., y_{t-1}, x; \\theta)$    (4)\nWe want to maximize the log probability of the target word yt given the previous target words ($y_1,..., y_{t-1}$) and the source sequence x, using model parameters \u03b8. We explored various sizes of FlanT5 (Chung et al., 2022) aiming to keep high-quality results and reducing latency and GPU costs."}, {"title": "Offline Evaluation", "content": "Our goal is to evaluate the student LLM's performance on our test set. Since online evaluation (A/B testing) is time-consuming and costly, we design offline metrics to compare different student LLMs and predict their performance in online A/B tests.\nOur metrics assess two key elements of the task: (1) Tool-selection: the model's ability to decide correctly whether to use a tool. We measure precision and recall, and report the tool-selection score as the F1-score. (2) Quality: the model's ability to use a tool correctly. For the filter tool, the quality score is the accuracy (proportion of correct predictions between the predicted and ground truth filter names). For the adjust and selective adjust tools, the quality score is the mean cosine similarity across samples, on predicted and ground truth parameter values (where both prediction and ground truth agree the tool should be used). Note that this metric is overly strict, as a desired result might be achievable with different parameter combinations.\nThe final score for a tool is the harmonic mean of the tool-selection score and quality score, emphasizing high performance in both. The overall score is the average of the final scores of all tools.\nFor a reality check, we also analyze the actual generated images/videos by applying the tools' predicted parameters in our app. In this study, we analyze a random sample, with three human annotators per sample (see Section 4.2, RQ1). Our ideas for automatic image evaluation, comparing two student LLMs, are provided in Appendix A.8."}, {"title": "Data Augmentation", "content": "A common industry need is fine-tuning a model with limited data. Here we demonstrate efficient data augmentation to improve this process.\nInspired by Lee et al. (2024), we iteratively run the offline evaluation on the LLM's training set. Each iteration involves two steps: (1) Identifying where the student LLM's predictions differ from the teacher's. For the filter tool, a mistake occurs when the predicted filter name is incorrect. We define a mistake in the adjust or selective adjust tool when a sample's cosine similarity is lower than the tool's mean cosine similarity without data augmentation. (2) Using another LLM to generate similar input user intents where the student LLM made mistakes (e.g., \u201ccool tone\u201d from \"cool morning\"). These new intents, along with the teacher LLM's original answers, are added to the training set.\nWe evaluated the augmentation on different sizes of our training set (using random sampling). To ensure a similar number of augmentations between different subsets of the training set, we always evaluated mistakes on a random sample of 1K. We augmented an intent if a mistake was identified in at least one tool. Using GPT-4 (OpenAI, 2023c), we generated similar user intents. Our implementation showed a 25% performance improvement in low data regimes with just one iteration (Section 4.2). See Appendix A.9 for implementation details and Appendix A.10, Figure 14 for the prompt used."}, {"title": "Online Evaluation", "content": "When our offline evaluation shows it is worthwhile to consider a new student LLM, we recommend confirming this in an online A/B test experiment.\nOur primary metric of interest is the project completion rate, calculated as the number of projects exported divided by the number of projects started. This metric indicates total user satisfaction with the results and the overall experience."}, {"title": "Experiments", "content": "We focus on the following research questions:\nRQ1. How well do student LLMs perform, and do they effectively mimic the teacher LLM?\nRQ2. Is augmentation effective in low-data regimes?"}, {"title": "Models", "content": "Our teacher LLM is GPT-3.5-Turbo. We explored two student LLMs: (1) Llama-2-7b-chat-hf (Touvron et al., 2023) with Low Rank Adaptations (LoRA) (Hu et al., 2021) and 4-bit quantization. Our Llama-2-7b-chat-hf SFT runs on an NVIDIA Tesla A100 GPU. (2) FlanT5-base (250M) (Chung et al., 2022), which is faster and works on an NVIDIA Tesla L4 GPU, which is five times cheaper. We fine-tuned both student LLMs for 10 epochs, selecting the best checkpoint from the last 3 epochs based on the highest final average tool score. See Appendix A.6 for details."}, {"title": "Results", "content": "RQ1 (Performance). We begin evaluating our student LLMs on the test set with our offline evaluation (Section 3.3). We report results using our metrics (tool-selection score, quality score, final score) per tool in Table 2, as well as the overall average final score. We can see both student models achieve comparable performance, despite FlanT5-base being smaller (rows 1, 4).\nWe denote by $r_i$ unique user intents with at least i calls. Interestingly, both models perform better on subsets of the test including more popular intents ($r_5 > r_3 > All$), This is important for production, as these intents cover more traffic.\nNext, we conducted a reality check on a sample of 15 generated images (See Figure 12 and Appendix A.7). Three calibrated team annotators reviewed each sample according to two criteria: (1) is the image relevant to the intent, and (2) does the student model correctly mimic the teacher. After aggregating the majority vote, we got: Relevance of Teacher: 86.7%, Llama-2-7b-chat: 86.7%, FlanT5-base: 93.3%. Both students successfully mimicked the teacher 73.3% times (11 images each, but not the same). These results match Table 2, showing our student LLMs have similar performance.\nThe average latency for running all tools was 1.63s for Llama-2-7b-chat-hf on an A100 GPU and 1.38s for FlanT5-base on an L4 GPU, both significantly faster than GPT-3.5-Turbo.\nA/B tests. In addition to offline evaluation, we conducted two online A/B tests. First, we compared our teacher, GPT-3.5-Turbo (tested on 94,317 projects), with Llama-2-7b-chat-hf (93,495 projects). We measured project completion rates as an indicator of user satisfaction\u00b2 (Section 3.5). The completion rate for the teacher was 96.1% of that of Llama-2-7b-chat-hf (no statistical significance). Thus, we conclude they are comparable.\nIn our second A/B test, we compared our student models. FlanT5-base (tested on 20,294 projects) achieved a completion rate of 99% of that of Llama-2-7b-chat-hf (20,282 projects). Thus, we conclude they are comparable and choose FlanT5-base for its lower latency and cost. Importantly, we are encouraged by the fact that our offline metrics align with the results of the online A/B tests 3.\nRQ2 (Augmentation). We evaluated FlanT5-base"}, {"title": "Related Work", "content": "Pre-LLM Dialogue-Based Image Editing. Natural language instructions for image editing have been explored extensively, particularly through dialogue systems, prior to the advent of LLMs. For instance, Lin et al. (2020) introduced NLIE, a system designed to convert high-level user commands into precise edits, aiding tasks such as object segmentation and action mapping. Similarly, Kim et al. (2022) developed Caise, a conversational agent that integrates image search and editing via natural language dialogue. Despite these advancements, both systems struggled with ambiguous or complex instructions and found it difficult to support detailed artistic edits or fully capture user preferences through language alone.\nLLM-Based Tool Invocation for Multimedia Tasks. Diffusion-based models for instruction-guided video editing still lag behind image models in visual quality and user control (Geyer et al., 2023; Couairon et al., 2023; Qi et al., 2023; Ceylan et al., 2023; Kara et al., 2024). To address this, we drew inspiration from previous research (Liu et al., 2023; Wang et al., 2023; Schick et al., 2024) that used LLMs to invoke tools for complex general and multimedia tasks beyond the LLM's capabilities. The strength of this approach is the LLM's ability to perform diverse visual tasks using tools, which can be integrated into an AI agent at a low development cost.\nTwo main approaches exist for using tools with LLM planners: (1) tool chaining via prompt engineering and in-context learning (Wu et al., 2023; Yang et al., 2023; Caciularu et al., 2024), and (2) instruction tuning of LLMs (Yang et al., 2024; Patil et al., 2023; Lian et al., 2024). Similar to (Patil et al., 2023; Eldan and Li, 2023), we used a strong LLM proficient with tools through prompt engineering and in-context learning as a teacher to create an instruction tuning dataset for smaller open-source models. A distinctive feature of our approach is incorporating users' behavioral signals in the tuning process."}, {"title": "Conclusions and Future Work", "content": "We introduced a novel NLP application for automatic video editing using LLMs, focusing on tonal color adjustment. We fine-tuned a (smaller) student LLM with guidance from a (larger) teacher LLM, while leveraging user behavioral signals. We proposed offline evaluation metrics and showed that our student models succeeded in matching the performance of our teacher model (GPT-3.5-Turbo) in both offline and online experiments. Our solution significantly reduces costs and latency, crucial for real-time industry applications.\nIn the future, we plan to test potential fine-tuning improvements by adding rationale as an additional label for supplementary supervision in a multi-task framework, as in Hsieh et al. (2023). We also aim to quantify the benefits of integrating user signals versus relying solely on unfiltered teacher LLM outputs, and to explore other methods for combining user feedback, including personalization. We also plan to extend our one-hop responses to conversational agent / dialogue system. More broadly, we aim to apply our research to additional tools, features, and applications (e.g., light effects, transition between clips, etc.) We release our code and data, including aggregated user preferences (https://github.com/orensultan/AIRecolor). We hope to inspire researchers to adopt our best practices in developing novel multi-modal real-time applications using tool chaining."}, {"title": "Ethical Considerations", "content": "Our dataset includes only user intent and model responses. We did not collect any asset used by users or any personally identifiable information. In offline evaluation of the images, we used images from the internet or other sources, which were not taken by our users."}, {"title": "Limitations", "content": "\u2022 Isolated tools with fixed sequential order:\nThe current framework employs the three tools independently, without integrated reasoning, which affects the cohesiveness and effectiveness of the editing process. The tools are applied in a fixed sequence (Adjust, Selective Adjust, LUT filters), which may not be optimal for all scenarios. Training the LLM also to consider dependencies between the tools could improve its flexibility.\n\u2022 Overly strict offline metric: We use cosine similarity to a single ground-truth solution when comparing predicted parameters for the Adjust and Selective Adjust tools, even though multiple different combinations might fulfill the user's request.\n\u2022 One-hop responses: Our current implementation supports one-hop responses, where the user provides a stylistic request in natural language and receives an immediate response. Expanding this to conversational agents or dialogue systems could better adapt to the user's specific needs.\n\u2022 Language: Our dataset contains mostly English user intents. We acknowledge that results may differ in other languages."}]}