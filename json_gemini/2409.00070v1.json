{"title": "Learning to Plan Long-Term for Language Modeling", "authors": ["Florian Mai", "Nathan Cornille", "Marie-Francine Moens"], "abstract": "Modern language models predict the next token in the sequence by considering the past text through a powerful function such as attention. However, language models have no explicit mechanism that allows them to spend computation time for planning long-distance future text, leading to a suboptimal token prediction. In this paper, we propose a planner that predicts a latent plan for many sentences into the future. By sampling multiple plans at once, we condition the language model on an accurate approximation of the distribution of text continuations, which leads to better next token prediction accuracy. In effect, this allows trading computation time for prediction accuracy.", "sections": [{"title": "Introduction", "content": "By pretraining on the next-token prediction objective, autoregressive decoder-only models based on e.g. Transformers attain a variety of skills, spending a small amount of compute for each token. As such they can be considered fast, intuitive reasoners (Bengio et al., 2021), analogous to the type 1 reasoning systems found in humans according to the dual-process theory (Evans, 1984; Kahneman, 2011). System 1 allows solving intuitive tasks such as perception and talking, but it is insufficient for tasks that require planning, such as writing coherent, long stretches of text. For planning tasks, humans instead invoke a slow, deliberate reasoning system 2. Most works that attempt to integrate deliberate planning and reasoning ability into LLMs pose the problem as a post-training process: by finetuning on reasoning datasets (Hendrycks et al., 2021; Havrilla et al., 2024), by learning to invoke external task-specific planners (Schick et al., 2023; Nye et al., 2021), or by employing advanced prompting methods like Chain-of-Thought (Wei et al., 2022). However, neuroscientific studies have revealed that predictive coding, the ability to continuously predict, update and draw on multiple hypotheses about future inputs, is central to language learning and production (Casillas and Frank, 2013; Ylinen et al., 2017; Shain et al., 2020; Aitchison and Lengyel, 2017; Kellogg, 2013; Mallahi, 2019). This suggests that the ability to plan originates, at least in part, from learning from unlabeled data and should hence be fostered in LLMs during pre-training. Cornille et al. (2024) propose a pretraining method in which language modeling is factorized into 1) first predicting a high-level latent plan via a separate planner module and 2) then conditioning the language model on generated plans when predicting the next token. However, their method only predicts a single one-step plan, which predicts merely one sentence ahead. As such, it neither performs long-term planning nor allows to draw on multiple hypotheses through variable compute.\nIn this paper, we propose an extension of the framework by Cornille et al. (2024) through two crucial changes (Figure 1): 1) We learn a planner that predicts multiple steps ahead to enable long-term predictive coding. 2) We sample a variable amount of hypotheses from the planner to condition the language model on, allowing to trade off computation time for better prediction accuracy. Our experimental evaluation demonstrates that both changes contribute to improving the language modeling ability."}, {"title": "Related Work", "content": "Predictive coding Multi-step predictions in the form of predictive coding have inspired machine learning algorithms in the past for a long time (Rafols et al., 2005). They often serve as an auxiliary loss to produce better representations for a specific downstream task, e.g., document classification (Trinh et al., 2018), POS tagging (Lan et al., 2021) and sentence representation learning (Araujo et al., 2021, 2023). For language modeling, Gloeckle et al. (2024) recently extended the next-token prediction objective to predicting n tokens ahead. They observe that multi-step prediction yields up to 17% better performance on coding tasks, demonstrating the potential of multi-step prediction for reasoning tasks. However, the improvement only appears with large-scale training. In our work, multi-step predictions are not an auxiliary task, but directly inform the downstream language model in its prediction.\nAdditional inference-time compute Aiming to overcome the computational limitations of the original Transformer architecture, Dehghani et al. (2019) equip it with Adaptive Computation Time (Graves, 2016). Many works attempt to transfer AlphaGo's famous success in Go (Silver et al., 2017) to text by generating and evaluating multiple paths of concrete text to improve performance (Yao et al., 2023; Wang et al., 2023; Zelikman et al., 2024). In contrast, our approach generates paths in an abstract space, which is more akin to MuZero (Schrittwieser et al., 2020)."}, {"title": "Methods", "content": "The key idea of the method is to transform an unlabeled text corpus into sequences of abstract writing actions and use these actions to guide the language model. Our method consists of three steps (cmp. Figure 1): 1: Inferring action sequences from unlabeled texts \u2461: Training a multi-step planner to predict the next actions 3: Sampling multiple paths from the planner to condition the LM."}, {"title": "Training an External Planner", "content": "We briefly review the method of Cornille et al. (2024), who train a planner that can predict only one step into the future. In Section 3.2, we propose a novel multi-step planner. In Section 3.3, we propose a novel way of conditioning the LM on multiple sampled action sequences.\nGiven a training corpus X with articles $X = t_1, t_2,..., t_n$, we first embed each text unit $t_i$ into a low-dimensional vector $z_i = E(t_i)$ using a text encoder E. We then cluster these embeddings into C clusters via k-means. Since the cluster centroids do not represent concrete sentences, Cornille et al. (2024) call them \"abstract writing actions\" $a \\in A$. This labeling process transforms the article $X = t_1, t_2,..., t_n$ into $X' = a_1, t_1, a_2, t_2, ..., a_n, t_n$.\nThe planner module P is composed of two functions: the representation function h and the prediction function f. The function h turns the textual context $t_1, t_2,..., t_{i-1}$ into a set of latent variables $Z_1, Z_2,..., Z_{i-1}$ by using a text encoder E per sentence:\n$Z_{1:i-1} = h(t_1, t_2,..., t_{i-1})$\n$= {E(t_1), E(t_2),..., E(t_{i-1})}$\nThe function f consists of a Transformer encoder that first contextualizes $Z_1, Z_2, ..., Z_{i-1}$, averages them after the last layer, and finally passes the resulting vector into a linear classifier to return a probability distribution over the possible writing actions. The predicted action results as:\n$\\hat{a_i} = \\underset{a\\in A}{\\operatorname{argmax}} f(Z_{1:i-1})$  (1)\nDuring training of the language model, at every sentence boundary the planner module P predicts the next writing action $\\hat{a_i}$ based on the current context $t_1, t_2,...,t_{i-1}$. The language model LM is then conditioned on $\\hat{a_i}$ when generating the next sentence $t_i$. The training objective is to predict the next token based on the previous words and the predicted actions, approximating the distribution $p(x_t | x_{1:t-1}, \\hat{a}_{1:i})$.\nA simple adapter module integrates the action information into the language model through a linear projection, $c_i = W Emb(a_i) + b$. Finally, $c_i$ is added to every token embedding in sentence $t_i$."}, {"title": "Planning Multiple Steps Ahead", "content": "To enhance the planner's capability, we extend it to predict multiple steps into the future. Instead of predicting only the next action $a_i$, the planner now generates a sequence of future actions $\\hat{a}_i, \\hat{a}_{i+1},\u00b7\u00b7\u00b7, \\hat{a}_{i+T}$, where T represents the number of future timesteps considered."}, {"title": "Multi-path Adapter", "content": "During inference at text unit i \u2013 1, instead of using the single best action (argmax) $\\hat{a_i}$ of the first step, we sample K paths $\\hat{a}_{i:i+T}^j, 1 \u2264 j \u2264 K$ from the planner with temperature 7\u00b9. These K paths allow the language model to account for a diverse set of possible futures, enhancing its ability to generate coherent long-term text.\nA straight-forward adaptation of the adapter module by Cornille et al. (2024) can be constructed as follows: For each path j, we simply average the linearly projected action embeddings $\\frac{1}{T} \\sum_{t=0}^T c_{i+t}$ to obtain a representation of the path. Then, a final representation $\u0109_i$ is obtained as $\u0109_i = \\frac{1}{K} \\sum_j c^j$. In the experiment section, we refer to this as Project and Avg. However, this adaptation has several shortcomings: 1) It completely disregards the sequential structure of actions in a path, and 2) it is unable to compute nonlinear interactions between multiple paths.\nTo enable the language model to effectively reason over multiple paths, we introduce a new adapter architecture consisting of a PathTransformer (PT), which is responsible for aggregating a single path into a vector that represents the path, and a SampleTransformer (ST), which aggregates a set of path vectors. Both models are bi-directional encoder-only transformers (Vaswani et al., 2017). For enabling better training stability, we additionally found it necessary to apply a ReZero-inspired (Bachlechner et al., 2021) normalization which initializes the solution close to the naive adapter. Formally, the model is described as:\n$c^j = PT(Emb(\\hat{a}_{i:i+T}^j + P_{1:T}) \\cdot \u03b1_1 + c^j$ (2)\n$c_i = ST(c^1, c^2, ..., c^K) \\cdot \u03b1_2 + \u0109_i$  (3)\n$P_{1:T}$ are absolute position embeddings indicating the order of actions in a path. $\u03b1_1, \u03b1_2 \u2208 R$ are learnable scalars initialized to zero."}, {"title": "Experiments", "content": "The purpose of our experiments is to demonstrate the benefit of our contributions for language modeling: 1) Multi-step planning and 2) conditioning on multiple sampled plans.\nBaselines and metrics Cornille et al. (2024) can be viewed as a special case of our model with T = 1, K = 1 and Project and Avg adapter. It thus serves as our primary baseline. Furthermore, we reproduce the Fixed baseline from Cornille et al. (2024), which reports the LM performance with finetuning with a single, fixed action only, demonstrating the usefulness of conditioning on planner-generated outputs. As is standard practice for language modeling, all models are evaluated via the perplexity metric. For reference only, we report the edit distance metric proposed by Cornille et al. (2024), which indicates how well generated text follows the ground truth in terms of action sequences.\nHyperparameters Following Cornille et al. (2024), all experiments are performed based on GPT-2 small (128M parameters) finetuned on 285310 articles of English Wikipedia. The full set of hyperparameters is reported in Appendix B."}, {"title": "Results", "content": "Table 1 shows the results of our model with various configurations in comparison to the baselines. All models are tested with the same K as in training."}, {"title": "Impact of multi-step predictions", "content": "Considering a fixed amount of path samples K = 10, when moving from T = 1 to T = 5, the perplexity of our model improves substantially by 0.2. When moving from T = 5 to T = 10, the improvement continues albeit relatively small. We attribute this to high uncertainty when modeling long-distance futures."}, {"title": "Impact of conditioning on multiple paths", "content": "Considering a fixed number of time steps T, the performance of our model also improves consistently when conditioned on an increasing number K of sampled paths (Table 1). In order to understand whether this generalizes to larger K than seen during training, in Figure 2 we increase the number of sampled paths K at inference time only. This experiment demonstrates that the performance continues to improve until at least K = 50. Naturally, this comes at the expense of additional compute.\nWhile our best models clearly outperform Cornille et al. (2024), for K = 1, our model performs worse. We explain this with the fact that sampling once is generally worse than argmax."}, {"title": "Discussion", "content": "Our consistent improvements in perplexity indicate that both integrating long-term predictions of the future writing process and modeling multiple future paths provide an LM with information that is valuable even for making local predictions. Consequently, our model outperforms the single-step planner by Cornille et al. (2024).\nMoreover, a core motivation of our work is to allow a language model to spend additional test-time compute to improve its predictions, similar to how AlphaGo (Silver et al., 2017) uses a lot of inference-time compute to achieve superhuman performance in Go. Demonstrating that our model, too, can trade off compute for better performance, we take a first step towards enabling this property for LMs."}, {"title": "Conclusion", "content": "LLMs acquire many skills through the next-token prediction objective, but planning remains a major weakness. We take a step towards learning to plan from pretraining on unlabeled data by predicting long sequences of abstract writing actions. By allowing the LM to condition on an arbitrary amount of sampled sequences, our model can flexibly trade off compute for prediction accuracy. This opens exciting research directions for planning with LMs."}, {"title": "Limitations", "content": "Lack of large-scale experiments Our work is motivated by the promise of integrating a slow, deliberate reasoning system into the framework of standard language models. We validate our proposed approach through controlled experiments that require the training of many models. Therefore, the evaluation in this paper is limited to the relatively small language model GPT2-small with 128M parameters. However, we have two reasons to believe that our approach will generalize to larger scale as well. First, Cornille et al. (2024), who propose the framework on which we build, show that the framework yields improvements for the relatively large LLM OLMo-1B (Groeneveld et al., 2024) as well. Second, Gloeckle et al. (2024) recently showed that their proposed pretraining objective, which, like ours, predicts multiple steps ahead, shows even greater potential at large model sizes starting from 7B. Since our related approach already shows promising results at small scale, we expect it to yield even better performance at larger scale.\nFlexibility of compute-performance tradeoff Inspired by AlphaGo's success, our method is able to trade off inference-time compute for better next-token prediction accuracy. However, this can be quite expensive, especially if the maximum amount of compute is spent every time the planner is called, i.e., at every sentence boundary, limiting the practicality of our method in its current state. To address this limitation in the future, we envision a mechanism similar to Adaptive Computation Time (Graves, 2016) that can learn how much additional compute is needed at any point. Given the success of Universal Transformers (Dehghani et al., 2019) at incorporating this mechanism, we are confident that this limitation will be resolved in the future.\nEdit distance results The purpose of our work is to improve language modeling. As the number of time steps T and the number of drawn samples K are increased, our proposed method consistently improves performance in terms of perplexity, the standard metric for language modeling. However, the performance in terms of edit distance shows no clear trend in either direction. This indicates that the edit distance, proposed by Cornille et al. (2024) to measure how well the model generates articles that adhere to the structure of the ground truth article, is noisy. In fact, some of the edit distance results reported by Cornille et al. (2024) are also in disagreement with the perplexity improvements. Future work interested in measuring the quality of generations in terms of structure should reconsider this choice of metric."}, {"title": "Ethical and Broader Impact", "content": "Our paper is concerned with LMs in general, which can be used to generate data that is within the training distribution. We train our models exclusively on Wikipedia, which is a corpus that contains very little to no content that is directly harmful to the user (e.g. slurs, insults, etc.). However, our developed method can, in principle, be used to enhance any LM, including those trained on harmful data, which is outside our control.\nIn the past, ethical concerns about LLMs have been raised because they are compute-intensive, energy-intensive, and carbon-intensive (Strubell et al., 2019; Bender et al., 2021). Our paper proposes a method that can trade off more compute for better performance, potentially adding to this problem. Therefore, rather than increasing the compute indiscriminately, we advocate for researching methods that can learn when it is necessary to spend more compute. We suspect that this avenue will ultimately lead to more energy-efficient LLMs."}, {"title": "Impact of Softmax Temperature", "content": "In preliminary experiments, we performed a small experiments to test the impact of the softmax temperature \u03c4 that is applied when sampling action paths. As Figure 3 shows, \u03c4 = 1.0 leads to the lowest perplexity. When the temperature is too low, the performance degrades because the diversity of sampled paths goes down, decreasing the amount of effective information passed to the LM. When the temperature is too high, the effective probability distribution converges towards uniform, which means that only uninformative paths are passed to the LM."}, {"title": "Implementation Details", "content": "Our implementation extends upon the source code of Cornille et al. (2024), which was privately shared with us. Once their code is shared publicly, we will release our own extensions as soon as possible thereafter. Unless specified explicitly, all packages use default parameters.\nThe code base makes use of PyTorch (Paszke et al., 2019), the Huggingface 'datasets' (Lhoest et al., 2021) and \u2018transformers' (Wolf et al., 2020) libraries to load and preprocess data and pretrained models (GPT-2 (Radford et al., 2019)), respectively. Furthermore, we used PyTorch-Lightning (Falcon et al., 2020) for model training.\nWe obtain the Wikipedia dataset through the 'datasets' library at https://huggingface.co/\nB.1 Implementation"}, {"title": "Hyperparameters", "content": "datasets/wikipedia (version \u201820220301\u2018 from March 2022). No additional preprocessing is applied. We randomly subsample 285,310 articles for training, and 1,000 for each validation and test set, respectively.\nAbstract writing actions are generated by first splitting every article into sentences using spaCy (Honnibal et al., 2020), and then encoding them into embeddings using MPNet-base-v2 (Song et al., 2020) via the SentenceTransformer library (Reimers and Gurevych, 2019)2 to encode sentences into embeddings. The final clustering step is performed via Scikit-Learn (Pedregosa et al., 2011) with k-means++ initialization (Arthur and Vassilvitskii, 2007). All used libraries are either open source or freely usable for academic purposes.\nWe ran our experiments on a compute grid with NVIDIA P100s (16GB). Only one GPU was used per experiment. Pretraining the planner took at max 48h, with the maximum reached when T = 10, i.e., necessary compute increases the more steps we predict ahead. Finetuning the LM takes another 24 hours. This includes first using the planner to predict writing actions for all data in the training set, and then finetuning the LM conditioned on the actions. Evaluating perplexity takes in total about 1h+0.25h\u00b7 K, while evaluating edit-distance (which requires generation) takes around 3 times as long.\nMost preliminary experiments were ran on a 10x smaller subset of the data, of which we ran roughly 200 experiments (using 10\u00d7 less compute). Every final experiment was run once. We estimate that in total we used around 4000 GPU hours.\nB.2 Hyperparameters"}]}