{"title": "Sample-efficient Unsupervised Policy Cloning from Ensemble Self-supervised Labeled Videos", "authors": ["Xin Liu", "Yaran Chen"], "abstract": "Current advanced policy learning methodologies have demonstrated the ability to develop expert-level strategies when provided enough information. However, their requirements, including task-specific rewards, expert-labeled trajectories, and huge environmental interactions, can be expensive or even unavailable in many scenarios. In contrast, humans can efficiently acquire skills within a few trials and errors by imitating easily accessible internet video, in the absence of any other supervision. In this paper, we try to let machines replicate this efficient watching-and-learning process through Unsupervised Policy from Ensemble Self-supervised labeled Videos (UPESV), a novel framework to efficiently learn policies from videos without any other expert supervision. UPESV trains a video labeling model to infer the expert actions in expert videos, through several organically combined self-supervised tasks. Each task performs its own duties, and they together enable the model to make full use of both expert videos and reward-free interactions for advanced dynamics understanding and robust prediction. Simultaneously, UPESV clones a policy from the labeled expert videos, in turn collecting environmental interactions for self-supervised tasks. After a sample-efficient and unsupervised (i.e., reward-free) training process, an advanced video-imitated policy is obtained. Extensive experiments in sixteen challenging procedurally-generated environments demonstrate that the proposed UPESV achieves state-of-the-art few-shot policy learning (outperforming five current advanced baselines on 12/16 tasks) without exposure to any other supervision except videos. Detailed analysis is also provided, verifying the necessity of each self-supervised task employed in UPESV.", "sections": [{"title": "I. INTRODUCTION", "content": "With the advancement of reinforcement learning (RL), it's possible to train policies that reach expert levels for challenging tasks defined in complex environments [1], [2], [3], [4]. However, the demanding training conditions and low sample efficiency significantly constrain the applicability of RL. To this end, many advanced RL approaches, such as model-based RL [5], [6], [7] and self-supervised RL [8], [3], [9], are proposed for higher sample efficiency. At the same time, some other researchers are no longer limited to online agent experience but try to seek help from offline supervisions. Given enough expert-labeled demonstrations, ideal policies can be obtained through offline-RL [10], [11] or imitation learning [12], [13], without the need of reward-specific environmental interactions. However, similar to expert rewards, these offline supervisions are also not widely available and always require time-consuming collection at a high cost. Different from them, the image-only videos are easily accessible currently thanks to the development of video websites and social media. However, Learning From Videos (LFV) poses a big challenge due to the lack of action labels. Although some attempts, such as video-based RL pre-training [14], [15] and video-based intrinsic rewards [16], have made great progress on RL data efficiency, they still require more or less additional supervisory information, e.g., reward-based finetuning and expert-annotated actions. Inverse RL methods [17], [18], [19] provide effective so-lutions for completely unsupervised policy learning from videos by reward prediction. While they often fail to well balance policy performance and sample efficiency, requiring more interactions than RL with expert rewards [20], [12].\nCan we derive a video-based policy with only a few interactions, without exposure to any other supervisions? Previous advanced methods [21], [22] have proven it possible and achieved considerable results in state-based control (with the state-based demonstrations) and style-unchanged easy video games. However, their performance is unsatisfactory when facing environments with complex visual input and dynamics, which may be attributed to the following two shortcomings: First, they don't make full use of available information, i.e., both the expert videos and reward-free in-teractions. For example, BCO [21] tries to label expert videos only based on non-expert environmental interactions, which means the highly qualified dynamics information contained in expert videos and the distribution difference are ignored. In contrast, ILPO [22] pays much more attention to the above dynamics contained in videos, but their world model is trained without considering the potential assistance provided by ground-truth interactions. Second, they don't provide their"}, {"title": "II. RELATED WORKS", "content": "Interacting with the environment to collect data for policy learning is a time-consuming and expensive process. In some special scenarios, such as autonomous driving [25] and real robot learning [26], the interactions can even lead to danger. To this end, many advanced methods are proposed to improve the RL sample efficiency. Model-based RL methods [6], [7], [27], [28] train extra world models that augment the RL experience, thereby improving sampling efficiency[29]. Self-supervised RL, i.e., RL auxiliary tasks [30], [8], [31], [32] alleviate the lack of supervision signals for representation learning in DRL, accelerating policy learning by improving the upstream representation understanding capabilities. At the same time, many researchers try to completely avoid accessing environments, learning policies through imitation learning [12], [33] and offline RL [11], [10], [14]. However, the required expert supervision signals, including both expert reward and offline expert dataset, are also expensive and unavailable in many scenarios. Different from these expert supervisions, videos are easy to obtain from the internet currently. Our work aims to achieve completely unsupervised policy learning with the easily accessible videos, in the absence of any other expert supervision.\nThanks to the improvement of the internet, it's very easy to acquire massive and different kinds of videos currently. Many videos contain expert demonstrations in different fields, which provides extensive knowledge for both human beings and machines to utilize. However, due to the lack of action information, how to well utilize the videos poses a big challenge. One of the most direct ways is to improve existing strategy learning methods with the help of videos, such as video-based pre-training for RL [34], [35], [36], [37] and video-based intrinsic reward for RL [16]. They indeed improve the sample efficiency of policy learning under enough supervision but still require other expert supervision. Imitation Learning from only Observations (ILO) [17], [21], [22] is proposed to completely decouple expert supervisions from LFV. Most ILO methods try to extract expert rewards contained in the videos, employing RL to optimize the unsu-pervised reward expectation for policy imitation [17], [38], [39]. These inverse RL methods achieve considerable results"}, {"title": "III. METHODOLOGY", "content": "UPESV trains a policy network and a video labeling model jointly. The policy determines the action a based on current observation o, defined as $\\pi(a|o)$. The video labeling model predicts the real expert action between two neighboring expert observations in videos, formulated as a inverse dynamics model $V(a|o, o_{+1})$, where $a$ denotes the predicted action while $o$ and $o_{+1}$ denote neighboring expert observations. V consists of three networks: a feature encoder f, a latent predictor g, and an action projector h. $o$ and $o_{+1}$ are separately encoded by f, concatenated, and processed by g and h to obtain the $a_r$, formulated as:\n$\\hat{a} = h(g(f(o), f(o_{+1}))) $. \nIn practice, we also include a few available historical observations $o_i$ as additional input information. They are processed similar to o and we omit them in the remaining part for clarity. UPESV learns V through three different but related self-supervised tasks over both expert videos $B_v$ and environmental interactions $B_e$, which we describe in the first three subsections. During the training of V, we simultaneously train the policy $\\pi(a|o)$ by behavior cloning V-labeled expert videos, which we describe in Section III.D. The policy $\\pi(a|o)$ collects self-supervised data for V while the improved V further enhances the policy $\\pi(a|o)$. After a sample-efficient and iterative training process, we obtain the well-trained video labeling model V and policy $\\pi$.\nWe provide the framework diagram, as shown in Fig. 2."}, {"title": "A. Self-supervised Task: Visual Shift Contrast", "content": "To infer actions from videos, the video labeling model $V(a|o, o_{+1})$ should be able to capture the difference between neighboring images. In most visual control domains, the relative difference is more related to decision making than the absolute difference in visual inputs. Imagine two particles moving upward at different positions on the plane. They have different absolute positions, but the same relative position changes that corresponds to their same actions. To this end, we want the labeling model to focus on relative position difference, which is achieved by driving it to ignore the absolute position difference. Therefore, a shift matching task that forces the video labeling model to align a random shifted image to its origin in the semantic space, is employed. This task enhances the visual understanding of the V in a target manner, which is necessary to understand complex dynamics through another two tasks (detailed in the next two subsections) simultaneously.\nThe visual shift contrast task forces the video labeling model V to align a randomly shifted image to its origin in the latent semantic space. Concretely, a batch of observations $\\{o_i\\}_{i=1}^N$ are sampled from expert video dataset $B_v$. Each observation is randomly shifted (up to s units in any direction) twice to obtain two shifted images, $\\hat{o}$ and $\\acute{o}$, where the two different hats upon o denote two independent random shifting. They are further encoded by f to obtain two latent features, $f(\\hat{x})$ and $f(\\acute{x})$. Then, we define $(f(\\hat{o}), f(o))$ as positive pairs while $(f(\\hat{o}),f(\\acute{o}))$ as negative pairs. Contrastive learning [41] is conducted on the whole batch to bring the positive pair closer while pulling"}, {"title": "B. Self-supervised Task: Latent Future Reconstruction", "content": "To predict expert actions accurately, the video labeling model $V(a|o, o_{+1})$ should be able to capture and understand which relative differences (between neighboring images) are similar and close, which requires it to understand the expert-related dynamics hidden in the videos. To this end, we introduce a latent future reconstruc-tion task into model training, where another world model is employed. The world model takes the latent feature of the current observation and V-predicted latent action as inputs, trying to reconstruct the latent feature of the next observation. By supporting the world model to reconstruct different future observations, the video labeling model is forced to understand dynamics, refining more action-like information into a small-dimension output.\nThe latent future reconstruction task updates the video labeling model and an extra world model jointly. First, a batch of observation pairs $\\{(o, o_{+1})\\}_{i=1}^N$ are sampled from expert video dataset $B_v$. Each current observation o is encoded by the f to obtain f(o) while each next observation $o_{+1}$ is encoded by the f to obtain $f(o_{+1})$. Then, they are concatenated and further processed by latent predictor g to produce the predicted latent action $z_i$. Then, the f(o) and $z_i$ are concatenated as the input to the world model $M_e(z, o)$ and we can obtain a predicted observation $\\hat{o_{+1}}$ in the latent space. Then we minimize the difference between the latent embeddings of predicted observation $\\hat{o_{+1}}$and ground-truth next observation $f(o_{+1})$, jointly updating the parameters of our video labeling model V and world model $E_w$:\n$L_{LFR} = \\frac{1}{N} \\sum_{i=1}^N ||f(o_{+1}) - \\hat{o_{+1}}||^2$."}, {"title": "C. Self-supervised Task: Ground-truth Action Prediction", "content": "If the video labeling model $V(a|o, o_{+1})$ can understand the expert video well, it should also gener-alize well to demonstrations sampled by other policies. To this end, we train our policy $\\pi(a|o)$ to imitate the V-labeled videos through behavior cloning and utilize it to interact with environments for real transitions collection. Then V is"}, {"title": "D. Unsupervised Policy Cloning", "content": "As mentioned in Section III.C, our policy $\\pi(a|o)$ and video labeling model $V(a|o, o_{+1})$ are trained and im-proved jointly. $\\pi(a|o)$ imitates the V-labeled expert videos through behavior cloning and interacts with environments for self-supervised data collection, while V is improved for a better $\\pi(a|o)$ through three self-supervised tasks ($L_{vs}, L_{lfr},$ and $L_{sm}$) conducted over the expert videos and collected transitions. After a sample-efficient iterative training process, an effective $\\pi(a|o)$ and an advanced labeling model V are both obtained.\nThe behavior cloning process of $\\pi(a|o)$ is described here. For every observation pairs $(o, o_{+1})$ from $B^v$, we use the video labeling model V to obtain a action vector $\\hat{a}$. Then the action-free observation pairs are labeled with $\\hat{a}$, denoted as $(o, \\hat{a}, o_{+1})$. Based on the current observation $o_i$ and the policy $\\pi(a|o)$, we obtain the policy-based action vector $a^{\\upsilon,\\pi}$ and then minimize its difference from label $\\hat{a}$ for policy learning:\n$L_{UPC} = \\frac{1}{N} \\sum_{i=1}^N ||\\hat{a} - a^{\\upsilon,\\pi}||^2$."}, {"title": "IV. EXPERIMENTS & ANALYSIS", "content": "We evaluated our method on all sixteen environments introduced by the Procgen benchmark [46]. Procgen provides a diverse set of procedurally-generated video game environments, each with unique challenges, multiple dynamics, and changing visual style. For example, in Maze task, the screen color, map size, and maze content are all changing dras-tically, which places high demands on both representation learning and dynamics understanding of methods."}, {"title": "B. Sample Efficiency Comparison", "content": "We test the policy learning abilities of all the methods when interaction is limited, which directly reflects their sample efficiency. Each method is allowed to interact with environments for only 100K steps. 100K steps are much less than the 25M employed in previous works [46], [45], requir-ing extremely high sample efficiency. The results across all 16 tasks are shown in Table 1. Compared with three video-based baselines, UPESV performs better on 12/16 tasks and not worse on the others except Starpilot. These advantages demonstrate the UPESV video labeling model has a better understanding of both visual inputs and environmental dy-namics, which can be attributed to the employed three self-supervised tasks. We observe trivial solutions that lead to huge performance drops in ILPO. This is due to its mode collapse in complex environments, which is also observed by [20], [43]. Compared with two advanced RL methods, UPESV's performance advantages are very obvious. In ad-dition, video-based methods BCO and LAPO also overall perform better than pure RL methods. Note that both UPESV and BCO are reward-free. These phenomena demonstrate that the videos may serve as a more efficient teacher than"}, {"title": "C. Ablations: Necessity of Each Self-supervised Task", "content": "In UPESV, we employ three different self-supervised tasks to jointly learn the video labeling model and policy. These three tasks are organically combined, where each one performs its own duties. To demonstrate that they are all indispensable in our framework, we sequentially ablate the latent future reconstruction task (UPESV w/o LFR), ground-truth action prediction task (UPESV w/o GAP), and visual shift contrast task (UPESV w/o VSC), observing the performance changes. In UPESV w/o GAP, we have to train the video labeling model without accessing true action space, so we train a policy decoder to remap the latent actions to real actions based on interactions after the policy learning process, which follows previous works [22]. The results are shown in Fig. 3, demonstrating that (i) every employed self-supervised task is necessary in UPESV, and (ii) the visual shift contrast task is crucial for the model to benefit from the other two tasks for advanced environment dynamics understanding."}, {"title": "D. Hyper-parameter Sensitivity Analysis", "content": "In this section, we test different shift distance s values in the visual shift contrast (VSC) task, as shown in Fig. 4. When s = 0, VSC degenerated into an image discrimination task. It is also effective (much better than UPESV v/o VSC) but worse than adding a few shifts (worse than s = 1, 2, 4. It means that forcing the model to ignore the global observation difference is helpful for understanding dynamics. In addition, we also try to add some time information (usually useful in RL), using two neighboring images instead of two shifted images (having the same origin) to define positive pairs for contrastive learning [48] while finding no improvements. It fits our intuition because the time contrast reduces the model's ability to perceive neighboring distinctions, which is critical for understanding complex environmental dynamics."}, {"title": "E. Robustness: Prediction Accuracy on Unseen Expert Data", "content": "Considering both BCO and our UPESV train video label-ing models to infer real actions contained in expert videos, we compare the two models' prediction accuracy on unseen expert action-labeled datasets. The results in TABLE II show the superiority of UPESV. This is consistent with their policy performance in the few-shot learning task, demonstrating the influence of additional dynamics understanding and represen-tation learning on models' robustness."}, {"title": "V. CONCLUSION & FUTURE WORK", "content": "In this work, we propose UPESV, a novel, sample-efficient, and completely unsupervised policy learning frame-work to imitate policy from videos. With three organically selected and combined self-supervised tasks, UESPV models well understand both visual representation and environmental dynamics, enabling state-of-the-art few-shot policy learn-ing in challenging procedurally-generated environments. The ability of sample-efficient video-based policy learning is a perfect fit for current challenging tasks where interac-tions are expensive, supervision is difficult to acquire, but video demonstrations are plentiful, e.g., robot manipulation. Crossing the gap from video games to real robots is a big challenge, which we leave in future studies."}]}