{"title": "TOWARDS GENERALISABLE TIME SERIES UNDERSTANDING ACROSS DOMAINS", "authors": ["\u00d6zg\u00fcn Turgut", "Philip M\u00fcller", "Martin J. Menten", "Daniel Rueckert"], "abstract": "In natural language processing and computer vision, self-supervised pre-training on large datasets unlocks foundational model capabilities across domains and tasks. However, this potential has not yet been realised in time series analysis, where existing methods disregard the heterogeneous nature of time series characteristics. Time series are prevalent in many domains, including medicine, engineering, natural sciences, and finance, but their characteristics vary significantly in terms of variate count, inter-variate relationships, temporal dynamics, and sampling frequency. This inherent heterogeneity across domains prevents effective pre-training on large time series corpora. To address this issue, we introduce OTiS, an open model for general time series analysis, that has been specifically designed to handle multi-domain heterogeneity. We propose a novel pre-training paradigm including a tokeniser with learnable domain-specific signatures, a dual masking strategy to capture temporal causality, and a normalised cross-correlation loss to model long-range dependencies. Our model is pre-trained on a large corpus of 640, 187 samples and 11 billion time points spanning 8 distinct domains, enabling it to analyse time series from any (unseen) domain. In comprehensive experiments across 15 diverse applications - including classification, regression, and forecasting - OTiS showcases its ability to accurately capture domain-specific data characteristics and demonstrates its competitiveness against state-of-the-art baselines. Our code and pre-trained weights are publicly available at https://github.com/oetu/otis.", "sections": [{"title": "1 INTRODUCTION", "content": "In natural language processing (NLP) or computer vision (CV), generalisable language features, e.g. semantics and grammar (Radford et al., 2018; Touvron et al., 2023; Chowdhery et al., 2023), or visual features, e.g. edges and shapes (Geirhos et al., 2019; Dosovitskiy et al., 2021; Oquab et al., 2024), are learned from large-scale data. Self-supervised pre-training paradigms are designed to account for the specific properties of language (Radford et al., 2018; Touvron et al., 2023; Chowdhery et al., 2023) or imaging (Zhou et al., 2022; Cherti et al., 2023; Oquab et al., 2024), unlocking foundational model capabilities that apply to a wide range of domains and downstream tasks. This potential, however, remains largely unrealised in time series due to the lack of self-supervised pre-training paradigms that account for the heterogeneity of time series across domains.\nTime series are widespread in everyday applications and play an important role in various domains, including medicine (Pirkis et al., 2021), engineering (Gasparin et al., 2022), natural sciences (Ravuri et al., 2021), and finance (Sezer et al., 2020). They differ substantially with respect to the number of variates, inter-variate relationships, temporal dynamics, and sampling frequency (Fawaz et al., 2018; Ismail Fawaz et al., 2019; Ye & Dai, 2021; Wickstr\u00f8m et al., 2022). For instance, standard 10-20 system electroencephalography (EEG) recordings come with up to 256 variates (Jurcak et al., 2007), while most audio recordings have only 1 (mono) or 2 (stereo) variates. Weather data shows high periodicity, whereas financial data is exposed to long-term trends. Both domains encompass low-frequency data recorded on an hourly (278mHz), daily ($12 \\mu$Hz), or even monthly (386 nHz) basis, while audio data is sampled at high frequencies of 44.1 kHz or more. Overall, this heterogeneity across domains renders the extraction of generalisable time series features difficult (Fawaz et al., 2018; Gupta et al., 2020; Iwana & Uchida, 2021; Ye & Dai, 2021)."}, {"title": "2 RELATED WORKS", "content": ""}, {"title": "2.1 SELF-SUPERVISED LEARNING FOR TIME SERIES", "content": "Time series vary significantly across domains, with differences in the number of variates, inter-variate relationships, temporal dynamics, and sampling frequencies. Due to this inherent heterogeneity, most existing works focus on pre-training models within a single domain (Oreshkin et al., 2019; Tang et al., 2020; Wu et al., 2021; Zhou et al., 2021; Wu et al., 2022a; Woo et al., 2022; Yue et al., 2022; Zhang et al., 2022; Li et al., 2023; Nie et al., 2023; Zeng et al., 2023; Dong et al., 2024). To develop more generalisable time series models, recent methods have explored multi-domain pre-training by addressing certain aspects of the heterogeneity, such as differences in variate count and sampling frequency. For instance, Liu et al. (2024) treat each variate in multi-variate time series independently to standardise generative tasks like forecasting, while Goswami et al. (2024) extend uni-variate analysis to discriminative tasks like classification. Similarly, Jiang et al. (2024) and Yang et al. (2024) standardise time series by cropping them into segments of predefined size and resampling them to a uniform frequency, respectively, to enable general classification capabilities in medical domains.\nWhile partially addressing time series heterogeneity, these methods limit model capabilities for general time series analysis. Standardisation techniques like cropping or resampling may distort inter-variate relationships, temporal dynamics, and long-range dependencies. Additionally, many of these approaches are tailored to specific applications, such as generative tasks (Das et al., 2024; Liu et al., 2024; Woo et al., 2024), or focused on particular domains like medicine (Jiang et al., 2024; Yang et al., 2024). Moreover, recent foundational models (Das et al., 2024; Goswami et al., 2024; Liu et al., 2024) focus on uni-variate analysis, ignoring crucial inter-variate relationships essential for real-world applications, such as disease prediction (Schoffelen & Gross, 2009; Wu et al., 2022b). Our study aims to overcome these limitations by fully addressing heterogeneity of multi-domain time series, establishing a foundation for general time series analysis across domains and tasks."}, {"title": "2.2 TIME SERIES TOKENISATION", "content": "Transformers (Vaswani et al., 2017) have emerged as the preferred architecture for foundational models in NLP and CV due to their scalability (Kaplan et al., 2020; Gordon et al., 2021; Alabdul-mohsin et al., 2022), enabling the training of models in the magnitude of 100 billion parameters (Chowdhery et al., 2023; Touvron et al., 2023; Oquab et al., 2024; Ravi et al., 2024). To utilise a Transformer for time series analysis, a tokeniser is required to map the time series into a compact latent space. Current methods (Jin et al., 2023; Nie et al., 2023; Zhou et al., 2023; Das et al., 2024; Goswami et al., 2024; Jiang et al., 2024; Liu et al., 2024; Woo et al., 2024; Yang et al., 2024) follow established techniques from NLP and CV, dividing time series into patches of pre-defined size. These patches are then flattened into a 1D sequence, with positional embeddings used to retain positional information. While uni-variate models (Nie et al., 2023; Das et al., 2024; Goswami et al., 2024; Liu et al., 2024) consider only temporal positions, multi-variate approaches (Woo et al., 2024; Yang et al., 2024; Jiang et al., 2024) account for both temporal and variate positions. However, none of these methods address the unique characteristics of variates, mistakenly assuming that the relationships between variates are identical across domains. Our work seeks to adapt the tokenisation process to preserve the domain-specific relationships between variates."}, {"title": "3 METHODS", "content": "In this work, we present a novel multi-domain pre-training paradigm that enables generalisable feature extraction from large, heterogeneous time series corpora. We introduce a domain-specific tokeniser with learnable signatures to address heterogeneity in multi-domain time series, as described in Section 3.1. We tailor masked data modelling (MDM) for multi-domain time series to pre-train our open model for general time series analysis (OTiS) on a large, heterogeneous corpus, as detailed in Section 3.2. In particular, we propose normalised cross-correlation as a loss term to capture global temporal dynamics in time series, as explained in Section 3.3. Moreover, we introduce a dual masking strategy to capture bidirectional relationships and temporal causality, essential for general time series analysis, as described in Section 3.4. After pre-training, we fine-tune OTiS on limited data to perform a variety of downstream tasks in any - including previously unseen - domain, as outlined in Section 3.5. A graphical visualisation of our method is provided in Figure 2."}, {"title": "3.1 DOMAIN-SPECIFIC TOKENISER", "content": "Overview. Assume a time series sample $X \\in \\mathbb{R}^{V_s\\times T}$ from domain $S$, where $V_s$ denotes the number of variates specific to $S$ and $T$ denotes the number of time points. We randomly crop or zero-pad $X$ to a fixed context length of $T$ time points. We then split it into $T'$ temporal patches of size $P$ along the time dimension, resulting in $V_s \\cdot T'$ patches $x_{v,t} \\in \\mathbb{R}^{1\\times P}$, where $v \\in \\{1, ..., V_s\\}$ and $t \\in \\{1, ..., T'\\}$.\nNext, we embed these patches using a shared patch projector across all variates and domains, resulting in patch embeddings $e^P(x_{v,t}) = e_{v,t} \\in \\mathbb{R}^{1\\times D}$, where $D$ denotes the model dimension. The patch projector consists of a 1D convolutional layer followed by layer normalisation and GELU activation.\nThe permutation-equivariant nature of Transformers (Vaswani et al., 2017) requires the use of positional embeddings to accurately capture the inherent relationships in the input data. Initially introduced for 1D textual token sequences (Vaswani et al., 2017), positional embeddings simply introduce an ordering into the input sequence. Modern implementations further extend their capabilities to encode more complex geometric information, such as 2D spatial (Dosovitskiy et al., 2021) or graph (Kreuzer et al., 2021) structures. For the analysis of any-variate time series, we distinguish between the temporal and variate structure. The temporal structure is equivalent to a sequential 1D structure, such that we use standard 1D sinusoidal embeddings $e^T(x_{v,t}) = e_t \\in \\mathbb{R}^{1\\times D}$.\nThe variate structure exhibits great heterogeneity across domains. In domains with uni-variate and two-variate data, such as mono and stereo audio, the structure is either trivial or only requires a basic distinction between variates. In other domains, however, the variate structure may represent more complex relationships, such as 3D manifolds for electroencephalography (EEG) or electrocardiography (ECG) data, or be of non-spatial nature, such as for financial data. Hence, we introduce learnable domain-specific variate embeddings to adequately address the heterogeneity across domains. These embeddings, denoted as $e^\\mathcal{S}(x_{v,t}) = e^\\mathcal{S},v \\in \\mathbb{R}^{1\\times D}$ for each variate $v$ in domain $S$, are designed to model the unique properties of a domain. They capture the inter-variate relationships and temporal dynamics specific to domain $S$, forming what can be considered as the signature of the very domain.\nFinally, the patch, temporal, and domain-specific variate embeddings are summed to form the input token $e_{v,t} = e_t + e^P_{v,t} + e^\\mathcal{S},v \\in \\mathbb{R}^{1\\times D}$. These input tokens collectively constitute the final input sequence $E \\in \\mathbb{R}^{(V_s\\cdot T')\\times D}$. To support batches of any-variate time series from multiple domains, we"}, {"title": "Definition of (Sub-)Domains.", "content": "The domain-specific tokeniser is designed to integrate different datasets within a domain. Consider two EEG datasets, TDBrain (Van Dijk et al., 2022) and SEED (Zheng & Lu, 2015), which share 19 identical variates but have different sampling frequencies of 500 Hz and 200 Hz, respectively. In this case, a single EEG-specific tokeniser ($V_{EEG} = 19$) is sufficient to accommodate both sampling frequencies, i.e. $E_{EEG-TDBrain} = [e_{EEG,1},..., e_{EEG,19}] \\in \\mathbb{R}^{19\\times D}$, as demonstrated in our experiments in Section 4. Note that while these positional embeddings are agnostic to variate ordering, we simplify processing by aligning the variate order across datasets within the same domain. Consider another EEG dataset, LEMON (Babayan et al., 2019), which includes 62 electrodes. Of these, 15 overlap with the electrodes in TDBrain (Van Dijk et al., 2022) and SEED (Zheng & Lu, 2015), while the remaining 47 are unique to LEMON (Babayan et al., 2019). In this scenario, the EEG-specific tokeniser can be extended by the 47 new variates ($V_{EEG} = 66$), such that $E_{EEG-LEMON} = [e_{EEG,1},..., e_{EEG, 15}, e_{EEG,20},..., e_{EEG,66}] \\in ]\\mathbb{R}^{62\\times D}$. In this way, different datasets can be combined to approximate the underlying data distribution of a domain $S$, e.g. EEG, enabling the creation of large and diverse time series corpora."}, {"title": "Multi-Variate or Uni-Variate Analysis?", "content": "Consider the Electricity dataset (UCI, 2024), which contains electricity consumption data for 321 households recorded from 2012 to 2014. These 321 observations are sampled from an underlying population and are assumed to be independent and identically distributed (i.i.d.). In this scenario, we perform a uni-variate analysis ($V_{Electricity} = 1$) of the data, initialising a single Electricity-specific variate embedding that models the hourly consumption of a household. In contrast, the Weather dataset (Wetterstation, 2024) contains 21 climatological indicators, such as air temperature, precipitation, and wind speed, which are not i.i.d. because they directly interact and correlate with one another. Therefore, a multi-variate analysis ($V_{Weather} = 21$) is conducted to account for the dependencies and interactions between the observations."}, {"title": "3.2 PRE-TRAINING ON MULTI-DOMAIN TIME SERIES", "content": "We pre-train our model using masked data modelling (MDM) (He et al., 2022) to learn generalisable time series features across domains. We mask a subset of input tokens and only encode the visible (i.e. non-masked) tokens using an encoder $f(\\cdot)$. Afterwards, we complement the encoded tokens with learnable mask tokens and feed them to a decoder $g(\\cdot)$, reconstructing the original input tokens.\nMore precisely, we draw a binary mask $m \\in \\{0,1\\}^{V_s \\cdot T'}$, following the dual masking strategy proposed in Section 3.4, and apply it to the input sequence $E \\in \\mathbb{R}^{(V_s\\cdot T')\\times D}$. Thus, we obtain a visible view $E[m] \\in \\mathbb{R}^{N_1\\times D}$, where $N_1 = \\sum_{v, t} m_{v,t}$ and $N_0 = (V_s \\cdot T') - N_1$ denote the number of visible and masked tokens, respectively. The visible view $E[m]$ is then fed to the encoder $f(.)$ to compute the token features $H \\in \\mathbb{R}^{N_1\\times D}$:\n$H = f(E[m]).$ (1)\nTo reconstruct the original input, these token features are fed to the decoder $g(.)$ together with a special, learnable mask token $e^M \\in \\mathbb{R}^{1\\times D}$, that is inserted at the masked positions where $m_{v,t} = 0$:\n$h'_{vt} = \\begin{cases} h_{v,t} & \\text{if } m_{v,t} = 1\\\\ e^M & \\text{if } m_{v,t} = 0 \\end{cases}$ (2)\nsuch that $H' \\in \\mathbb{R}^{(V_s\\cdot T')\\times D}$. The decoder $g(.)$ then predicts the reconstructed input $X \\in \\mathbb{R}^{V_s \\times (T'\\cdot P)}$:\n$\\hat{X} = g(H'),$ (3)\nwhere $(T' \\cdot P) = T$, i.e. the context length specified in time points. Eventually, the domain-specific tokeniser described in Section 3.1, the encoder $f(\\cdot)$, and the decoder $g(\\cdot)$ are optimised end-to-end using the mean squared error (MSE) loss on all reconstructed input tokens:\n$L_{MSE} = \\frac{1}{V_s \\cdot T'}\\sum_{v=1}^{V_s}\\sum_{t=1}^{T'} ||x_{vt} - \\hat{x}_{vt}||_2.$ (4)"}, {"title": "3.3 NORMALISED CROSS-CORRELATION LOSS", "content": "MDM focuses on reconstructing masked parts of the data, emphasising local patterns through the MSE loss (4). However, time series often exhibit long-range dependencies, where past values influence future outcomes over extended periods. To accurately capture these global patterns, we introduce normalised cross-correlation (NCC) as a loss term in MDM for time series:\n$L_{NCC} = \\frac{1}{V_s T}\\sum_{v=1}^{V_s}\\sum_{t=1}^{T'} \\frac{1}{\\sigma_{v}} (x_{v,t} - \\mu_{x_v}) (\\hat{x}_{v,t} - \\mu_{\\hat{x}_v}) \\in [-1,1],$ (5)\nwhere $\\mu$ and $\\sigma$ denote the mean and standard deviation, respectively. Hence, to capture both local and global temporal dynamics, the total loss used to optimise OTiS is defined as\n$L = L_{MSE} + \\lambda\\cdot (1 - L_{NCC}),$ (6)\nwhere $\\lambda$ is empirically set to 0.1 during pre-training."}, {"title": "3.4 DUAL MASKING STRATEGY", "content": "We design the masking strategy to enhance foundational model capabilites in time series analysis. Specifically, we randomly select between two masking schemes during pre-training, namely random masking and post-fix masking. In 75% of cases, we apply random masking, where each $m_{v,t}$ is independently sampled from a Bernoulli distribution with probability $p = 1 - \\bar{p}$, with $\\bar{p}$ denoting the masking ratio (i.e. $m_{v,t} \\sim Bernoulli(1 - \\bar{p})$). This encourages the model to learn complex inter-variate relationships across the entire time series. In the remaining 25% of cases, we employ post-fix masking, which masks the second half of the temporal dimension, leaving only the first half visible (i.e. $m_{v,t} = 1[t<T'/2]$). The prediction of future values solely based on past observations simulates real-world forecasting conditions, helping the model to capture temporal causality. Overall, this dual masking strategy enables OTiS to learn both bidirectional relationships and temporal causality, which are essential for general time series analysis."}, {"title": "3.5 FINE-TUNING & INFERENCE ON (UNSEEN) TARGET DOMAINS", "content": "Inclusion of Unseen Domains. For a new domain $S$, a randomly initialised variate embedding $E_\\mathcal{S} \\in \\mathbb{R}^{V_S \\times D}$ is introduced. The domain-specific tokeniser is then fine-tuned alongside the encoder $f(.)$, and, if required, the decoder $g(\\cdot)$, for the specific downstream task, as described in the following.\nClassification & Regression. We use the encoder $f(\\cdot)$ and the unmasked input sequence $E$ to compute all token features $H = f(E) \\in \\mathbb{R}^{(V_s\\cdot T')\\times D}$. We average-pool these features into a global token $h^* \\in \\mathbb{R}^{1\\times D}$, which we feed through a linear layer to obtain the final model prediction. We optimise a cross-entropy and MSE loss for the classification and regression tasks, respectively.\nForecasting. We apply post-fix masking to generate a binary mask $m \\in \\{0,1\\}^{V_s\\cdot T'}$ for the forecasting task. The encoder $f(\\cdot)$ is used to compute the visible token features $H \\in \\mathbb{R}^{N_1 \\times D}$. We then concatenate the sequence with learnable mask tokens to form $H' \\in \\mathbb{R}^{(V_s\\cdot T')\\times D}$, which is passed through the decoder $g(\\cdot)$ to produce the final output. We optimise the MSE loss together with the NCC loss term over all reconstructed input tokens."}, {"title": "4 EXPERIMENTS & RESULTS", "content": ""}, {"title": "4.1 MODEL VARIANTS AND IMPLEMENTATION DETAILS", "content": "We introduce OTiS in three different configurations, Base, Large, and Huge, with their specific architectures described in Appendix C.1, to explore scaling laws with respect to the model size. We set the patch size and stride to $P = 24$, respectively, to split the time series into $T' = 42$ non-overlapping patches along the time dimension. For pre-training, the context length specified in time points is set to $T = 1008$, resulting in $T' = 42$ sinusoidal temporal embeddings. If longer context lengths are"}, {"title": "4.2 LARGE AND DIVERSE PRE-TRAINING CORPUS", "content": "We aim to develop a general time series model that fully handles the heterogeneity in real-world data. Specifically, our model is designed to handle time series with different variate counts Vs, inter-variate relationships, temporal dynamics, and sampling frequency, ensuring flexibility for downstream tasks. To this end, we pre-train our model on a large and diverse corpus of publicly available data spanning 8 domains, with a total of 640, 187 samples and 11 billion time points, as summarised in Table 1. A detailed description of the datasets included in our pre-training corpus can be found in Appendix A. The time series corpus is split into 612, 394 training and 27, 793 validation samples for pre-training."}, {"title": "4.3 BENCHMARKING ACROSS DOMAINS AND TASKS", "content": "To assess the utility of OTiS in real-world settings, we conduct experiments on three key use cases in time series analysis: classification, regression, and forecasting. For classification, we perform binary epilepsy detection using EEG (Epilepsy 2001), multi-class fault detection in rolling bearings from vibration signals (FD-B 2016), multi-class hand-gesture classification with accelerometer signals (Gesture 2009), and multi-class muscular disease classification using electromyographie (EMG 2000). In regression, we predict five imaging-derived cardiac phenotypes from 12-lead ECG (LVEDV, LVESV, LVSV, LVEF, LVM 2020). For forecasting, we predict electricity transformer temperature (ETT 2021), weather (Weather 2024), and electricity consumption (Electricity 2024). Further dataset details are provided in Appendix B. We adhere to the established data splitting and evaluation procedures for the classification (Zhang et al., 2022), regression (Turgut et al., 2023), and forecasting (Zhou et al., 2021) tasks. All experiments are reported as mean and standard deviation across five seeds set during fine-tuning.\nOTiS demonstrates strong classification capabilities, setting two new benchmarks, as shown in Table 2a. Additionally, OTiS excels in predicting imaging-derived cardiac phenotypes, even surpassing multimodal baselines that incorporate imaging data during pre-training, as summarised in Table 2b. Our model outperforms the baselines in 4 out of 5 regression tasks. As shown in Table 3, OTiS also exhibits strong forecasting capabilities, outperforming current models in 4 out of 6 benchmarks. Notably, all of the forecasting tasks are performed in previously unseen domains. A visualisation of the forecast predictions can be found in Appendix F. Overall, OTiS outperforms both specialised and general state-of-the-art baselines in 10 out of 15 diverse applications across 8 domains, demonstrating its strong utility and generalisability across (unseen) domains and tasks."}, {"title": "4.4 DOMAIN SIGNATURE ANALYSIS", "content": "A key component of OTiS is its use of domain-specific variate embeddings. While these embeddings are randomly initialised, we expect them to capture unique domain characteristics during training, eventually serving as the signature of their respective domain. To validate this hypothesis, we analyse the domain-specific variate embeddings after pre-training using principal component analysis (PCA).\nFirst, we find that OTiS unifies time series from diverse domains into a meaningful latent space, where embeddings of domains with shared high-level semantics cluster together, as depicted in Appendix E.1. For example, embeddings of mono and stereo audio group closely, as do those of banking and economics. Moreover, EEG-specific embeddings are clearly separated and ECG-specific embeddings form a tight cluster.\nSecond, OTiS preserves low-level semantics specific to each domain, such as the relationships between variates. To explore this, we focus on the learned variate embeddings of EEG, the most complex domain in our corpus. EEG variates correspond to actual electrodes, each associated with a 3D position in space or a 2D position on the scalp, which is ideal for studying inter-variate relationships. Our analysis covers both (i) variate embeddings for 10-20 system EEG recordings with 19 electrodes learned during multi-domain pre-training, and (ii) variate embeddings for previously"}, {"title": "4.5 SCALING STUDY", "content": "We analyse the scaling behaviour of OTiS with respect to model and dataset size. To this end, we subsample the pre-training data to 10% and 1% of its original size, ensuring that each subset is fully contained within the corresponding superset. We evaluate the downstream performance of all OTiS variants across classification, regression, and forecasting tasks, as depicted in Figure 4.\nThe experiments demonstrate that downstream performance generally scales with dataset size, achieving the best results with the full pre-training dataset. This trend, however, does not directly apply to model size, which is in line with the scaling behaviour observed in current time series foundational models (Woo et al., 2024; Goswami et al., 2024). Given that performance generally improves across all models with increasing data size, we hypothesise that scaling the model size could prove beneficial with even larger pre-training corpora."}, {"title": "4.6 ABLATION STUDY", "content": "We perform an ablation study to analyse the impact of OTiS' key components: the domain-specific tokeniser, dual masking strategy, and normalised cross-correlation (NCC) loss. As shown in Figure 5, the best and most robust performance is achieved when all components are used during pre-training.\nReplacing the domain-specific variate embeddings with domain-agnostic embeddings (i.e. learnable embeddings shared across all domains) consistently led to inferior performance across all tasks, demonstrating the importance of capturing domain-specific data characteristics during tokenisation. Switching from dual masking to random masking resulted in performance degradation, although the impact was less notable for generative tasks than for discriminative tasks. We hypothesise that the NCC loss already captures temporal causality, which is particularly crucial for generative tasks like forecasting. Overall, removing the NCC loss caused performance declines across all downstream tasks, emphasising the role of long-range dependencies for general time series understanding."}, {"title": "5 DISCUSSION & CONCLUSION", "content": "In this study, we explore the problem of effective pre-training on heterogeneous time series corpora. Time series vary substantially across domains, e.g. with respect to inter-variate relationships and temporal dynamics, rendering generalisable feature extraction from multi-domain time series difficult. To address this issue, we present OTiS, an open model for general time series analysis, specifically designed to handle multi-domain heterogeneity. Our novel multi-domain pre-training paradigm, including a domain-specific tokeniser with learnable signatures, a dual masking strategy, and a normalised cross-correlation (NCC) loss, enables OTiS to extract generalisable time series features.\nIn extensive experiments, we demonstrate that OTiS generalises well across 15 diverse downstream applications spanning 8 distinct domains, achieving competitive performance with both specialised and general state-of-the-art (SOTA) models. In a qualitative analysis, we further show that OTiS unifies time series from diverse domains in a meaningful latent space, while preserving low-level semantics of a domain including the inter-variate relationships. Thereby, our work establishes a strong foundation for future advancements in interpretable and general time series analysis.\nLimitations. While OTiS outperforms SOTA models across 10 tasks, our experiments in low-data regimes suggest that larger pre-training corpora could further enhance its performance. Unlike in NLP and CV, where large datasets are curated from web-crawled data, foundational models in time series, including OTiS, still rely on manually curated datasets. Future work could explore fully automatic pipelines, e.g. using embedding similarity, to filter and rebalance multi-domain time series from the web. OTiS could further benefit from processing domain signatures during inference, potentially unlocking zero-shot capabilities, similarly to those seen in foundational models in NLP and CV."}, {"title": "E.1 INTER-DOMAIN ANALYSIS", "content": "A visualisation of all domain-specific variate embeddings learned during pre-training is provided in Figure 6. We find that OTiS learns a meaningful latent space, where embeddings of domains with shared high-level semantics cluster closely together."}, {"title": "E.2 INTRA-DOMAIN ANALYSIS", "content": "A visualisation of the domain-specific variate embeddings for EEG and ECG are given in Figure 7 and Figure 8, respectively. We find that OTiS preserves low-level semantics specific to each domain, accurately capturing the relationships between variates."}, {"title": "F FORECAST VISUALISATION", "content": "We visualise the performance of our model on 6 forecasting benchmarks in Figure 9."}]}