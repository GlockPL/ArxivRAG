{"title": "Reactzyme: A Benchmark for Enzyme-Reaction Prediction", "authors": ["Chenqing Hua", "Bozitao Zhong", "Sitao Luan", "Liang Hong", "Guy Wolf", "Doina Precup", "Shuangjia Zheng"], "abstract": "Enzymes, with their specific catalyzed reactions, are necessary for all aspects of life, enabling diverse biological processes and adaptations. Predicting enzyme functions is essential for understanding biological pathways, guiding drug development, enhancing bioproduct yields, and facilitating evolutionary studies. Addressing the inherent complexities, we introduce a new approach to annotating enzymes based on their catalyzed reactions. This method provides detailed insights into specific reactions and is adaptable to newly discovered reactions, diverging from traditional classifications by protein family or expert-derived reaction classes. We employ machine learning algorithms to analyze enzyme reaction datasets, delivering a much more refined view on the functionality of enzymes. Our evaluation leverages the largest enzyme-reaction dataset to date, derived from the SwissProt and Rhea databases with entries up to January 8, 2024. We frame the enzyme-reaction prediction as a retrieval problem, aiming to rank enzymes by their catalytic ability for specific reactions. With our model, we can recruit proteins for novel reactions and predict reactions in novel proteins, facilitating enzyme discovery and function annotation.", "sections": [{"title": "1 Introduction", "content": "Enzymes, as catalysts of biological systems, are the workhorses of various biological functions [35, 52, 13] (Fig. 1a). They accelerate and regulate nearly all chemical processes and metabolic pathways in organisms, from simple bacteria to complex mammals [53, 18]. The ability to understand and manipulate enzyme functions is fundamental to numerous scientific and industrial fields, including biosynthesis, where enzymes help to produce complex organic molecules [16, 42], and synthetic biology, where they are engineered to create novel biological pathways [19, 34, 24]. Furthermore, they can break down pollutants, thus playing a significant role in bio-remediation efforts [57, 75]. In the realm of protein evolution, examining enzyme functions across the tree of life enhances our understanding of the evolutionary processes that sculpt metabolic networks and enable organisms to adapt to their environments [31, 20, 11, 54]. As such, gaining insights into enzyme function is not merely an academic pursuit in life sciences but a necessity for practical applications in medicine, agriculture, and environmental management.\n\nThe current methodologies for enzyme annotation primarily rely on established databases and classifications such as KEGG Orthology (KO), Enzyme Commission (EC) numbers, and Gene Ontology (GO) annotations, each with its specific focus and methodology [65] (Fig. 1b). For instance, the EC system categorizes enzymes based on the chemical reactions they catalyze, providing a hierarchical numerical classification [4]. KO links gene products to their functional orthologs across different species [48], whereas GO offers a broader ontology for describing the roles of genes and proteins in any organism [12]."}, {"title": "2 Related Work", "content": "Protein Function Annotation. Protein function annotation is a foundational task in bioinformatics, typically utilizing databases like Gene Ontology (GO), Enzyme Commission (EC) numbers, and KEGG Orthology (KO) annotations [12, 4, 48]. Traditional methods such as BLAST, PSI-BLAST, and eggNOG rely on sequence alignments and similarities to infer function [3, 2, 29]. Recently, deep learning has introduced innovative approaches for protein function prediction [56, 39, 8]. There are 2 types of protein function prediction model, one uses only protein sequence as their input, while the other also uses experimentally-determined or predicted protein structure as input. Generally, these methods typically predict EC or GO information to approximate protein functions, distinct from describing the exact catalysed reaction.\n\nProtein-Ligand Interaction Prediction. Protein-ligand interaction prediction is another related area, with numerous models designed to identify potential bindings between proteins and ligands [10, 25, 73]. Most existing models, such as those for drug-target interaction (DTI), focus on stable bindings critical for therapeutic efficacy [72, 14], which differs from substrate-enzyme interactions where binding does not necessarily result in catalysis. Some models have also tackled the specific challenge of enzyme-substrate prediction, including the ESP model [37, 38]. This area differs from drug-target interactions, underscoring the unique dynamics of enzyme-substrate relationships where the interaction may not always lead to stable binding.\n\nProtein-Ligand Structure Prediction. The protein-ligand structure prediction task, also referred to as ligand docking, has evolved with new methodologies emerging [14, 80, 1, 26]. Traditional docking methods like Vina [63], Gold [70], and Glide [17] have been complemented by deep learning approaches such as EquiBind [60], TankBind [43], E3Bind [81], UniMol [83], and DiffDock [14]. Moreover, recent advances in protein-ligand structure prediction, such as AlphaFold 3 [1], RFAA [36], and Umol [9], provide detailed structural models of protein-ligand complexes, but they do not specifically address the functional interactions between enzymes and substrates. These methods are"}, {"title": "3 Reactzyme Dataset", "content": "3.1 Dataset\nOverview. Our study utilizes a comprehensive dataset compiled from the SwissProt and Rhea databases [7, 5]. SwissProt, a curated subset of the UniProt database, has been selected for its high-quality, human-derived functional annotations of protein sequences. This section of UniProt is particularly valuable for its expert-reviewed entries, which ensure reliable and accurate functional data, making it ideal for our analysis. Rhea is employed for its precise mapping from enzymes to specific catalyzed functions, offering detailed descriptions of biochemical reactions. The Reactzyme dataset can be downloaded via https://zenodo.org/records/11494913.\n\nData Collection. The SwissProt and Rhea dataset are downloaded on January 8, 2024, and includes data entries up to this date, providing the most recent and comprehensive data available for our study. We selectively exclude water molecules and unspecific functional groups that could mask the true molecular structures. Conversely, we keep metal ions, gas molecules, and other small molecules because of their potential to bind to proteins, a characteristic that presents a valuable learning feature for our model. To this end, the total dataset comprises 178, 463 positive enzyme-reaction pairs, including 178, 327 unique enzymes and 7, 726 unique reactions.\n\nCompare to Other Datasets. There are two datasets related to the enzyme-reaction prediction task. The first one is from ESP [37], which used GO annotation database for UniProt dataset, lay emphasis on the substrate binding to the enzyme. The ESP dataset contains 18, 351 enzyme-substrate pairs with experimental evidence for substrate binding, contains 12, 156 unique enzymes and 1, 379 unique molecules. The other dataset is from EnzymeMap [23], which used as training set in CLIPZyme [51]. EnzymeMap is a high-quality dataset of atom mapped and balanced enzymatic reaction, with enzyme information from BRENDA [59]. This dataset contains 46, 356 enzyme-driven reactions, including 16, 776 distinct reactions and 12, 749 enzymes. A comparison is illustrated in Table 1.\n\nReactzyme Limitation. While Reactzyme has the advantage of containing significantly more data than both ESP and EnzymeMap, it has some limitations. Notably, it lacks atom-mapping data, and the number of reactions is smaller than in EnzymeMap. This reduction in reaction count is because some reactions in Reactzyme are represented using functional groups rather than the full substrate. Futhermore, Reactzyme may not include sufficient coverage of the entirety of space of proteins and reactions in practical use. Reactzyme can be developed further for more practical interest in enzyme and substrate design.\n3.2 Data Split\nWe provide three dataset splits based on time, enzyme similarity, and reaction similarity. For each data split, 10% of the training data are randomly sampled for validation.\n\nTime Split. The first data-split method is based on a specific date. We split the training and test samples by selecting enzyme-reaction pairs before 2010-12-31, for training and pairs after this date for testing. This results in 166, 175 training pairs and 12, 287 test pairs, approximately a 93%/7% training/test ratio. The training samples include 166, 084 unique enzymes and 7, 726 unique reactions, while the test samples include 12, 277 unique enzymes and 2, 634 unique reactions."}, {"title": "4 Reactzyme Approach", "content": "We conceptualize the prediction of enzyme-substrate/product as a retrieval task, where it seeks to rank a given list of enzyme proteins according to their catalytic efficacy for a specified chemical reaction [51]. The overarching goal is to understand the intricate interactions between enzymes and chemical reactions. To this end, we formulate strategies for the representation of the reactions and proteins to enhance the generalization capabilities of machine learning models in the retrieval task. More specifically, we highlight the development of representation methods that capture structural and functional subtleties of enzymes and reactions, which play a central role in predicting enzyme- substrate compatibility and catalytic potential. Our approach is visualized in Fig. 2.\n4.1 Multi-View Reaction Representation\nIn representing the substrate and product of catalytic reactions, we employ both string and graph representations to capture the transition from substrates to products. Diverging from the previous enzyme datasets, such as CLEAN [77] and CLIPZyme [51], our dataset uniquely offers a combination"}, {"title": "4.2 Enzyme Representation", "content": "When representing enzymes involved in catalytic reactions, we draw upon advancements in both protein structures and protein language models. This approach shares similarities with CLIPZyme [51], where we utilize a equivariant graph neural network to leverage information of protein structures. However, we are different in the additional use of a structure-based protein language model, where the protein embeddings are computed based on structure-aware sequence tokens.\n\nProtein Language Model Initialization. Each protein is represented as a residue-level point cloud in Euclidean space, denoted as $G_e = (V_e, X_e, S_e)$, where $S_e$ represents the protein sequence and $V_e \\in \\mathbb{R}^{N_e \\times d_e}$ represents residue features. Each residue $v_i \\in V_e$ can be initialized either with a one-hot encoded residue type or using embeddings from a protein language model (PLM). The protein structure is denoted as $X_e \\in \\mathbb{R}^{N_e \\times 3}$, which can be initialized using AlphaFold [32] or by searching"}, {"title": "4.3 Enzyme-Reaction Prediction", "content": "Once we have the reaction and enzyme embeddings r, e, designing models to learn the interactions between enzymes and reactions becomes quite flexible. While approaches like Transformer and attention mechanisms can be used to learn pairwise relationships from positive and negative enzyme- reaction pairs [69, 49], or Bidirectional Recurrent Neural Network (Bi-RNN) can capture enzyme- reaction interactions sequentially [76, 22], we take a more direct approach by employing an MLP network. Consider the input reaction embedding of dimension $d_r$, the reaction encoder is a 4-layer Multi-Layer Perceptron (MLP) as:\n$$z_r = \\text{ReactionEnc}(r) = W_4 (\\text{SiLU}_3(\\text{LN}_3(W_3(\\text{SiLU}_2(\\text{LN}_2(W_2(\\text{SiLU}_1(\\text{LN}_1 (W_1 r + B_1))))) + B_2))) + B_3))) + B_4 \\in \\mathbb{R}^{256},$$\nwhere $W_1 \\in \\mathbb{R}^{d_r \\times 512}$, $B_1 \\in \\mathbb{R}^{512}$, $W_2 \\in \\mathbb{R}^{512 \\times 256}$, $B_2 \\in \\mathbb{R}^{256}$, $W_3, W_4 \\in \\mathbb{R}^{256\\times 256}$, $B_3, B_4 \\in \\mathbb{R}^{256}$. The enzyme encoder, denoted as EnzymeEnc, has a similar architecture, with only modification in the first-layer MLP as $W_1 \\in \\mathbb{R}^{1280 \\times 512}$, $B_1 \\in \\mathbb{R}^{512}$. And the encoded reaction and enzyme representations have the dimension of 256, as $z_r, z_e \\in \\mathbb{R}^{256}$.\n\nThe decoder network is a 4-layer MLP that takes the encoded enzyme-reaction pair and computes the prediction score:\n$$y = \\text{Decoder}(z_r, z_e) = W_4(W_3(\\text{SiLU}(W_2(\\text{SiLU}(W_1([z_r, z_e]) + B_1)) + B_2)) + B_3)) \\in \\mathbb{R},$$\nwhere $W_1 \\in \\mathbb{R}^{512\\times 256}$, $B_1 \\in \\mathbb{R}^{256}$, $W_2 \\in \\mathbb{R}^{256\\times 128}$, $B_2 \\in \\mathbb{R}^{128}$, $W_3 \\in \\mathbb{R}^{128\\times 64}$, $B_3 \\in \\mathbb{R}^{64}$, $W_4 \\in \\mathbb{R}^{64\\times 1}$. In Appendix C, we further compare the simple MLP-decoder network with Transformer- and Bi-RNN-decoder networks (in Tables 9, 10, and 11), showing their retrieval performance."}, {"title": "5 Benchmarking on Reactzyme Dataset", "content": "5.1 Primary Empirical Evaluation\nBaseline Overview. We summarize the baseline models used for the enzyme-reaction retrieval task. For reaction representation, we employ Molecule Attention Transformer-2D (MAT-2D) [49], and UniMol-2D [83] for 2D molecular graphs, as well as MAT-3D and UniMol-3D for 3D molecular conformations. For enzyme representation, we employ ESM [41] and a structure-aware protein language model, SaProt [61]. Additionally, we use an equivariant graph neural network (FANN [55]) to enhance residue-level representations."}, {"title": "5.2 Classic Annotation Method \u2013 BLAST", "content": "Method. To predict the reaction of an enzyme using BLAST, we employ BLASTp with default parameters. The training set sequences are used as the target database, while the test set sequences serve as the query. We use the following commands:\n\nBash Command \u2192 bash makeblastdb -in train.fasta -dbtype prot parse_seqids -out train_db blastp -query test.fasta -db train_db -outfmt \"6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore\" -out results.tsv\n\nIf BLASTp finds a match between the test set and training set sequences, we set the corresponding value to 1, indicating that the sequences likely share the same reaction. If there is no match found, the value is set to 0, indicating no predicted reaction match."}, {"title": "5.3 Potential Strategy", "content": "scores for the reaction similarity- based split. This pattern is similar in the retrieval results, underscoring the heightened difficulty of the reaction similarity-based task."}, {"title": "5.4 Further Evaluation", "content": "We present further experiments in the Appendices for deeper evaluation and comparison. In Appendix C, we compare MLP, Transformer, and Bi-RNN decoder networks. Given the presence of annotated negative samples, we explore a contrastive learning approach in Appendix D. We also compare to the CLIPZyme pseudo-graph approach in Appendix E. And for a better description of chemical environment of reactants and product, we compare with fingerprint features in Appendix F."}, {"title": "6 Conclusion", "content": "In this paper, we introduce Reactzyme, a new benchmark for enzyme-reaction prediction. Unlike previous methods that rely on protein sequence or structure similarity or provide EC/GO annotations to predict reaction, our approach directly evaluates the mapping between enzymes and their catalyzed reactions. These enzyme-reaction prediction methods are able to handle protein with novel reactions and to discover proteins that catalyze unreported reactions.\n\nWe evaluate the performance of several baselines on the Reactzyme. While the baselines demonstrate competitive results on time- and enzyme-similarity-based splits, the reaction-similarity-based split remains particularly challenging. This difficulty may arise from the presence of many unseen reactions in the test set of the reaction-similarity-based split. One potential avenue for improvement is to explore contrastive learning techniques to address this challenge. However, we acknowledge that this remains an open problem for researchers in our community to tackle.\n\nThe Reactzyme benchmark facilitates the evaluation of models working with protein and molecule representations, which requires a comprehensive understanding in both modalities. Models demon- strating high performance in enzyme-reaction prediction can be further leveraged for protein function prediction and enzyme discovery. This includes identifying key enzymes in biosynthesis and dis- covering potent enzymes for degrading emerging pollutants, for these reactions that have not been previously found in enzymes."}, {"title": "B Terminology of enzyme-reaction prediction, Enzyme-function prediction, enzyme-substrate/product prediction, and enzyme annotation", "content": "The terms or the concepts of 'enzyme reaction prediction', 'enzyme function prediction', 'enzyme substrate/product prediction', and 'enzyme annotation' may not be clearly delineated in the main section. In here, we aim to explain and address these concerns. There are indeed different types of annotations for enzyme, with function annotation being one of them. A reaction is part of the function, as not all functions map directly to a reaction. An enzyme reaction includes multiple features, such as substrate, product, and conditions (including the catalyst). This distinction helps clarify the various concepts like enzyme reaction prediction, function prediction, and substrate/product prediction."}, {"title": "C Experiments on Transformer and Bi-RNN Networks", "content": "In Section 4, we choose to use an encoder-decoder network over directly employment of Transformer or Bi-RNN. Here, we explain the intuition behind the use of the encoder-decocoder design over the transformer-like architectures. The encoder network, at the low-hierarchical level, aims to learn individual representations for enzymes and reactions, respectively. And the decoder network, at the high-hierarchical level, aims to learn the contacts or the interactions between any enzyme-reaction pair. Thus, in principle, the decoder could be any network that learns the interactions between enzymes and reactions.\nResults. In Section 4, we choose to use a MLP as the decoder network, here, we employ the Transformer and Bi-RNN as the decoder network for further evaluation. We compare the average results of baseline models by MLP, Transformer, Bi-RNN for time-based, enzyme similarity-based, and reaction similarity-based splits in Tables 9, 10, and 11, respectively."}, {"title": "D Experiments on Contrastive Learning", "content": "Results. In this section, we compare the average results of baseline models and the contrastive learning approach for time-based, enzyme similarity-based, and reaction similarity-based splits in Tables 12, 13, and 14, respectively. For enzyme-reaction prediction, contrastive learning can be used to learn embeddings or representations of enzymes and reactions that are predictive of their interactions. Positive pairs are optimized to have similar representations, while the negative pairs are optimized to be distinct in the embedding space."}, {"title": "E Experiments on Cross-Attention and Pseudo-graph for \u2018Transition State\u2019", "content": "In Section 4, we mention the concept of creating a pseudo-transition state graph for substrates and products introduced in CLIPZyme [51], and we choose to use the cross-attention to describe the transition state. Here, we further evaluate between the pseudo-graph approach in CLIPZyme [51] and our cross-attention approach.\nResults. We compare the average results of baseline models and the pseudo-graph of CLIPZyme for time-based, enzyme similarity-based, and reaction similarity-based splits in Tables 15, 16, and 17, respectively. We observe there is significant performance increase in reaction similarity-based split bu using the pseudo-graphs for transition states. However, the method does not improve the performance or the improvements are incremental on time-based and enzyme-similarity-based splits in comparison with cross-attention of the baseline models."}, {"title": "F Experiments on Fingerprint Features", "content": "In addition to the use of one-hot encoded atomic and bond features, we study the encodings of using fingerprints generated by RDKit to describe the chemical environments of reactants and products.\nResults. We compare the average results of baseline models and the fingerprint features for time- based, enzyme similarity-based, and reaction similarity-based splits in Tables 18, 19, and 20, respec- tively."}, {"title": "G No Significant Improvement with Molecular Conformations: An Intuitive Explanation from 3Di Perspective", "content": "In our paper, we compared models like ESM (without structure) and SaProt (with structure), as well as models with 2D or 3D molecular conformation information. The results showed inconsistent performance when structural features were included in different tasks. We believe that this might be because the fact that SaProt encodes only 3Di information, which lacks the detailed structural features necessary to accurately model enzyme functional sites. For molecules, due to their smaller sizes, the difference between 2D and 3D information might be minimal. This could explain the limited performance gains observed in experiments.\nFurthermore, it is important to consider the scale of the Reactzyme dataset in comparison to previous studies. The dataset comprises more than 100,000 enzyme-substrate pairs, which is an order of magnitude larger than the typical datasets used in similar studies (around 10,000 pairs). The increased size and diversity of our dataset may dilute the impact of molecular conformation information on the overall performance. While the incorporation of this information has resulted in only a modest improvement, it remains a valuable aspect of our work.\nMoreover, we recognize this as a current limitation and believe that there is potential for further optimization in the utilization of molecular conformations and structural data. Future work could ex- plore more sophisticated methods to leverage this information, potentially leading to more substantial performance gains in enzyme-reaction prediction tasks."}, {"title": "H Further Dataset Statistics", "content": "In Section 3, we describe the enzyme-similarity split using the Levenshtein distance, ensuring that enzymes in the training and test sets differ by at least 60% in sequence. While this approach guarantees that the test set enzymes are distinct from those in the training set, it does not necessarily ensure that the test set is representative or meaningfully distinct in terms of enzyme clustering. To work on the concern, we apply MMseqs2 alignment to the test set enzyme sequences to analyze their clustering patterns. The results show that 72.7% of the test enzymes have at least a 30% sequence difference, 40.7% have at least a 50% sequence difference, and 14.5% have at least a 70% sequence difference. These statistics suggest that while there is substantial diversity in the test set, additional considerations may be necessary to ensure that it accurately reflects the broader enzyme landscape rather than being skewed by unrepresentative outliers.\nSimilarly, we introduce the reaction-similarity split using the Needleman-Wunsch algorithm applied to SMILES, ensuring that reactions in the test set are distinct and do not overlap with those in the training set. We apply Needleman-Wunsch algorithm to the SMILES of test set reactions to analyze their clustering patterns. The results show that 92.3% of the test enzymes have at least a 10% SMILES difference, 60.9% have at least a 30% SMILES difference, and 14.5% have at least a 50% SMILES difference. These results indicate a significant level of diversity in the test set reactions, although additional considerations might be necessary to ensure the representativeness and typicality of the test set in capturing the broader reaction space."}]}