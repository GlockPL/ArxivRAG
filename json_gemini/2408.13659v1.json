{"title": "Reactzyme: A Benchmark for Enzyme-Reaction Prediction", "authors": ["Chenqing Hua", "Bozitao Zhong", "Sitao Luan", "Liang Hong", "Guy Wolf", "Doina Precup", "Shuangjia Zheng"], "abstract": "Enzymes, with their specific catalyzed reactions, are necessary for all aspects of life, enabling diverse biological processes and adaptations. Predicting enzyme functions is essential for understanding biological pathways, guiding drug development, enhancing bioproduct yields, and facilitating evolutionary studies. Addressing the inherent complexities, we introduce a new approach to annotating enzymes based on their catalyzed reactions. This method provides detailed insights into specific reactions and is adaptable to newly discovered reactions, diverging from traditional classifications by protein family or expert-derived reaction classes. We employ machine learning algorithms to analyze enzyme reaction datasets, delivering a much more refined view on the functionality of enzymes. Our evaluation leverages the largest enzyme-reaction dataset to date, derived from the SwissProt and Rhea databases with entries up to January 8, 2024. We frame the enzyme-reaction prediction as a retrieval problem, aiming to rank enzymes by their catalytic ability for specific reactions. With our model, we can recruit proteins for novel reactions and predict reactions in novel proteins, facilitating enzyme discovery and function annotation.", "sections": [{"title": "1 Introduction", "content": "Enzymes, as catalysts of biological systems, are the workhorses of various biological functions [35, 52, 13] (Fig. 1a). They accelerate and regulate nearly all chemical processes and metabolic pathways in organisms, from simple bacteria to complex mammals [53, 18]. The ability to understand and manipulate enzyme functions is fundamental to numerous scientific and industrial fields, including biosynthesis, where enzymes help to produce complex organic molecules [16, 42], and synthetic biology, where they are engineered to create novel biological pathways [19, 34, 24]. Furthermore, they can break down pollutants, thus playing a significant role in bio-remediation efforts [57, 75]. In the realm of protein evolution, examining enzyme functions across the tree of life enhances our understanding of the evolutionary processes that sculpt metabolic networks and enable organisms to adapt to their environments [31, 20, 11, 54]. As such, gaining insights into enzyme function is not merely an academic pursuit in life sciences but a necessity for practical applications in medicine, agriculture, and environmental management.\n\nThe current methodologies for enzyme annotation primarily rely on established databases and classifications such as KEGG Orthology (KO), Enzyme Commission (EC) numbers, and Gene Ontology (GO) annotations, each with its specific focus and methodology [65] (Fig. 1b). For instance, the EC system categorizes enzymes based on the chemical reactions they catalyze, providing a hierarchical numerical classification [4]. KO links gene products to their functional orthologs across different species [48], whereas GO offers a broader ontology for describing the roles of genes and proteins in any organism [12]."}, {"title": "2 Related Work", "content": "Protein Function Annotation. Protein function annotation is a foundational task in bioinformatics, typically utilizing databases like Gene Ontology (GO), Enzyme Commission (EC) numbers, and KEGG Orthology (KO) annotations [12, 4, 48]. Traditional methods such as BLAST, PSI-BLAST, and eggNOG rely on sequence alignments and similarities to infer function [3, 2, 29]. Recently, deep learning has introduced innovative approaches for protein function prediction [56, 39, 8]. There are 2 types of protein function prediction model, one uses only protein sequence as their input, while the other also uses experimentally-determined or predicted protein structure as input. Generally, these methods typically predict EC or GO information to approximate protein functions, distinct from describing the exact catalysed reaction.\n\nProtein-Ligand Interaction Prediction. Protein-ligand interaction prediction is another related area, with numerous models designed to identify potential bindings between proteins and ligands [10, 25, 73]. Most existing models, such as those for drug-target interaction (DTI), focus on stable bindings critical for therapeutic efficacy [72, 14], which differs from substrate-enzyme interactions where binding does not necessarily result in catalysis. Some models have also tackled the specific challenge of enzyme-substrate prediction, including the ESP model [37, 38]. This area differs from drug-target interactions, underscoring the unique dynamics of enzyme-substrate relationships where the interaction may not always lead to stable binding.\n\nProtein-Ligand Structure Prediction. The protein-ligand structure prediction task, also referred to as ligand docking, has evolved with new methodologies emerging [14, 80, 1, 26]. Traditional docking methods like Vina [63], Gold [70], and Glide [17] have been complemented by deep learning approaches such as EquiBind [60], TankBind [43], E3Bind [81], UniMol [83], and DiffDock [14]. Moreover, recent advances in protein-ligand structure prediction, such as AlphaFold 3 [1], RFAA [36], and Umol [9], provide detailed structural models of protein-ligand complexes, but they do not specifically address the functional interactions between enzymes and substrates. These methods are"}, {"title": "3 Reactzyme Dataset", "content": "3.1 Dataset\nOverview. Our study utilizes a comprehensive dataset compiled from the SwissProt and Rhea databases [7, 5]. SwissProt, a curated subset of the UniProt database, has been selected for its high-quality, human-derived functional annotations of protein sequences. This section of UniProt is particularly valuable for its expert-reviewed entries, which ensure reliable and accurate functional data, making it ideal for our analysis. Rhea is employed for its precise mapping from enzymes to specific catalyzed functions, offering detailed descriptions of biochemical reactions. The Reactzyme dataset can be downloaded via https://zenodo.org/records/11494913.\n\nData Collection. The SwissProt and Rhea dataset are downloaded on January 8, 2024, and includes data entries up to this date, providing the most recent and comprehensive data available for our study. We selectively exclude water molecules and unspecific functional groups that could mask the true molecular structures. Conversely, we keep metal ions, gas molecules, and other small molecules because of their potential to bind to proteins, a characteristic that presents a valuable learning feature for our model. To this end, the total dataset comprises 178, 463 positive enzyme-reaction pairs, including 178, 327 unique enzymes and 7, 726 unique reactions.\n\n\nCompare to Other Datasets. There are two datasets related to the enzyme-reaction prediction task. The first one is from ESP [37], which used GO annotation database for UniProt dataset, lay emphasis on the substrate binding to the enzyme. The ESP dataset contains 18, 351 enzyme-substrate pairs with experimental evidence for substrate binding, contains 12, 156 unique enzymes and 1, 379 unique molecules. The other dataset is from EnzymeMap [23], which used as training set in CLIPZyme [51]. EnzymeMap is a high-quality dataset of atom mapped and balanced enzymatic reaction, with enzyme information from BRENDA [59]. This dataset contains 46, 356 enzyme-driven reactions, including 16, 776 distinct reactions and 12, 749 enzymes. A comparison is illustrated in Table 1.\n\nReactzyme Limitation. While Reactzyme has the advantage of containing significantly more data than both ESP and EnzymeMap, it has some limitations. Notably, it lacks atom-mapping data, and the number of reactions is smaller than in EnzymeMap. This reduction in reaction count is because some reactions in Reactzyme are represented using functional groups rather than the full substrate. Futhermore, Reactzyme may not include sufficient coverage of the entirety of space of proteins and reactions in practical use. Reactzyme can be developed further for more practical interest in enzyme and substrate design.", "3.2 Data Split": "We provide three dataset splits based on time, enzyme similarity, and reaction similarity. For each data split, 10% of the training data are randomly sampled for validation.\n\nTime Split. The first data-split method is based on a specific date. We split the training and test samples by selecting enzyme-reaction pairs before 2010-12-31, for training and pairs after this date for testing. This results in 166, 175 training pairs and 12, 287 test pairs, approximately a 93%/7% training/test ratio. The training samples include 166, 084 unique enzymes and 7, 726 unique reactions, while the test samples include 12, 277 unique enzymes and 2, 634 unique reactions."}, {"title": "Enzyme Similarity", "content": "The second data-split method is based on enzyme similarity. We ensure that enzymes in the training set do not appear in the test set, using the Levenshtein distance [6] for sequence-based protein sequence comparison, ensuring at least 60% sequence difference between training and test set enzymes. This results in 169, 724 training pairs and 8, 739 test pairs, approxi- mately a 95%/5% training/test ratio. The training samples include 169, 596 unique enzymes and 7,726 unique reactions, while the test samples include 8, 734 unique unseen enzymes and 1,573 unique reactions."}, {"title": "Reaction Similarity", "content": "The third data-split method is based on reaction similarity, calculated by the Needleman-Wunsch algorithm on SMILES. We ensure that reactions in the training set do not appear in the test set. This results in 163, 771 training pairs and 14, 692 test pairs, approximately a 91%/9% training/test ratio. The training samples include 163, 651 unique enzymes and 7, 340 unique reactions, while the test samples include 14, 688 unique enzymes and 386 unique unseen reactions."}, {"title": "Negative Sample", "content": "A common method involves designating all enzymes within a training set that are not annotated for catalyzing a specific reaction as negative samples [51]. Nevertheless, given the extensive size of our dataset, we opt for a strategy centered on enzyme and reaction similarity to construct negative samples. Specifically, for each verified positive enzyme-reaction pair, we identify the top-k enzymes that closely resemble the positive enzyme but do not have annotations for catalyzing the reaction, using them as negative samples. Similarly, we select the top-k reactions that are similar to the positive reaction but are not catalyzed by the positive enzyme, to serve as additional negative samples (k=1000). This method effectively narrows down the size of negative samples while retaining those of significance for both training and testing purposes. Despite our approach, the construction of negative samples still presents an unresolved challenge, remaining as an open question for future development."}, {"title": "4 Reactzyme Approach", "content": "We conceptualize the prediction of enzyme-substrate/product as a retrieval task, where it seeks to rank a given list of enzyme proteins according to their catalytic efficacy for a specified chemical reaction [51]. The overarching goal is to understand the intricate interactions between enzymes and chemical reactions. To this end, we formulate strategies for the representation of the reactions and proteins to enhance the generalization capabilities of machine learning models in the retrieval task. More specifically, we highlight the development of representation methods that capture structural and functional subtleties of enzymes and reactions, which play a central role in predicting enzyme- substrate compatibility and catalytic potential. Our approach is visualized in Fig. 2."}, {"title": "4.1 Multi-View Reaction Representation", "content": "In representing the substrate and product of catalytic reactions, we employ both string and graph representations to capture the transition from substrates to products. Diverging from the previous enzyme datasets, such as CLEAN [77] and CLIPZyme [51], our dataset uniquely offers a combination of graph and geometric data representations. This allows the structural and functional information that is inherent in reactions to be captured in a more fine-grained manner, hence portraying a rich and informative description of the catalytic processes.\n\nSMILES. Following CLEAN [77] and CLIPZyme [51], we continue to use SMILES [71] for representing substrates and products. This method is highly useful for its simplicity and ease of interpretation. Such representation concisely shows the substrate-to-product conversion process and uses some linear notation, which is particularly adept at conveying structural changes in a straightforward manner.\n\nGraph and Conformation. Graph representation for substrates and products can capture the structural and functional information that is not typically included in string representations [33, 40, 74]. In these graphs, atoms are represented as nodes, while bonds are viewed as edges. Formally, consider a molecular graph denoted as $G = (V,E)$, $V \\in \\mathbb{R}^{N\\times d_v}$ represents atom (node) features with each $v_i \\in V$ denotes one-hot encoded atom type, and $\\mathcal{E} \\in \\mathbb{R}^{N\\times N \\times d_e}$ represents edge (bond) features with each $e_{ij} \\in \\mathcal{E}$ denotes one-hot encoded bond type and connectivity. In addition to the graph representations for reactions, we use molecular conformations to incorporate geometric information. Formally, consider a molecular conformation denoted as $G = (V, E, X)$, $X \\in \\mathbb{R}^{N\\times 3}$ denotes additional geometric features, specifically atom positions. These conformations are computed through molecular force field optimization [62].\n\nOnce obtaining the graph representations $G_s = (V_s, E_s, X_s)$, $G_p = (V_p, E_p, X_p)$ for substrates and products, respectively, we proceed to compute reaction embeddings. Consider a graph neural network denoted as $\\phi$, we first use it to separately encode the graph representations as\n\n$V_s, \\hat{\\mathcal{E}}_s = \\phi(V_s, \\mathcal{E}_s, X_s), V_s \\in \\mathbb{R}^{N_s \\times d_r}, \\hat{\\mathcal{E}}_s \\in \\mathbb{R}^{N_s \\times N_s \\times d_e},$ (1)\n\n$V_p, \\hat{\\mathcal{E}}_p = \\phi(V_p, \\mathcal{E}_p, X_p), V_p \\in \\mathbb{R}^{N_p \\times d_r}, \\hat{\\mathcal{E}}_p \\in \\mathbb{R}^{N_p \\times N_p \\times d_e},$ (2)\n\nwhere $V, \\hat{\\mathcal{E}}$ denotes the updated node and edge representations, respectively. It then becomes challenging to formulate 'transitions' between substrates and products. One method to address this challenge is by constructing a pseudo-transition state graph denoted $G_t = (V_t, \\mathcal{E}_t)$, by adding the bond features for edges connecting the same pairs of nodes in the reactants and the products. Then the graph neural network $\\phi$ can be used to update the transition graphs, and final reaction embedding can be computed by taking the aggreagted node features, as $r = Aggregate(V_t) \\in \\mathbb{R}^{d_r}$. The concept of creating a pseudo-transition state graph is adopted in CLIPZyme [51].\n\nHowever, we take a more direct approach by computing cross-attention between substrates and products to formulate the 'transitions', as follows:\n\n$V_s = softmax\\left(\\frac{(V_sW_q)(V_pW_k)^T}{\\sqrt{d_r}}\\right)(V_pW_v) \\in \\mathbb{R}^{N_s \\times d_r}, V_p = softmax\\left(\\frac{(V_pW_q)(V_sW_k)^T}{\\sqrt{d_r}}\\right)(V_sW_v) \\in \\mathbb{R}^{N_p \\times d_r}.$ (3)\n\nIn here, the 'transitions' are learned through an attention mechanism that considers the pairwise relationships between atoms in substrates and atoms in products, and the edge features $\\hat{\\mathcal{E}}_s, \\hat{\\mathcal{E}}_p$ can be additionally used as attention biases in transformers [69]. And the final reaction embedding is computed by taking the average of node features, as $r = Mean([V_s, V_p]) \\in \\mathbb{R}^{d_r}$. In practice, for the choice of graph neural networks to process the structural information of substrate and product graphs $G = (V,\\mathcal{E})$, we choose to use Molecule Attention Transformer-2D (MAT-2D) [49] and UniMol-2D [83]; and with additional geometric features $G = (V, \\mathcal{E}, X)$, we choose to use MAT-3D and UniMol-3D."}, {"title": "4.2 Enzyme Representation", "content": "When representing enzymes involved in catalytic reactions, we draw upon advancements in both protein structures and protein language models. This approach shares similarities with CLIPZyme [51], where we utilize a equivariant graph neural network to leverage information of protein structures. However, we are different in the additional use of a structure-based protein language model, where the protein embeddings are computed based on structure-aware sequence tokens.\n\nProtein Language Model Initialization. Each protein is represented as a residue-level point cloud in Euclidean space, denoted as $G_e = (V_e, X_e, S_e)$, where $S_e$ represents the protein sequence and $V_e \\in \\mathbb{R}^{N_e \\times d_e}$ represents residue features. Each residue $v_i \\in V_e$ can be initialized either with a one-hot encoded residue type or using embeddings from a protein language model (PLM). The protein structure is denoted as $X_e \\in \\mathbb{R}^{N_e \\times 3}$, which can be initialized using AlphaFold [32] or by searching"}, {"title": "GNN Encoding", "content": "In addition to these embeddings, we utilize an equivariant graph neural network to encode the protein graphs $G^{ESM} = (V^{ESM}_e, X_e, S_e)$ and $G^{Sap} = (V^{Sap}_e, X_e, S_e)$. We employ the Frame Averaging Neural Network (FANN), denoted as $\\psi$, to learn $SE(3)$-invariant node features [55]. This approach possesses the effectiveness and efficiency advantage when dealing with large graphs. The frame averaging operation is achieved by first projecting the protein structure $X_e$ onto a set of eight frames $U_e \\in \\mathcal{F}(X_e)$. These frames are constructed using Principal Component Analysis (PCA). Suppose $u_1, u_2, u_3$ denote the three principal components of a covariance matrix $\\Sigma_e = (X_e - \\mu_e)(X_e - \\mu_e)$, where $\\mu_e$ denotes the Center-of-Mass of $X_e$. The frame set $\\mathcal{F}(X_e)$ is defined as $\\mathcal{F}(X_e) = {\\pm u_1, \\pm u_2, \\pm u_3}$. Then the frame averaging operation computes $SE(3)$-invariant node features $V_e$, as follows:\n\n$V_e = \\frac{1}{|\\mathcal{F}(X_e)|}\\sum_{U_e \\in \\mathcal{F}(X_e)} \\psi(v_e, (X_e - \\mu_e)U_e) \\in \\mathbb{R}^{N_e \\times 1280}.$ (4)\n\nAnd the final GNN-encoded protein embedding is computed by taking the average of node features as, $e^{SE3}_{esm} = Mean(V^{ESM}_e) \\in \\mathbb{R}^{1280}$ and $e^{SE3}_{sap} = Mean(V^{Sap}_e) \\in \\mathbb{R}^{1280}$."}, {"title": "4.3 Enzyme-Reaction Prediction", "content": "Once we have the reaction and enzyme embeddings $r, e$, designing models to learn the interactions between enzymes and reactions becomes quite flexible. While approaches like Transformer and attention mechanisms can be used to learn pairwise relationships from positive and negative enzyme- reaction pairs [69, 49], or Bidirectional Recurrent Neural Network (Bi-RNN) can capture enzyme- reaction interactions sequentially [76, 22], we take a more direct approach by employing an MLP network. Consider the input reaction embedding of dimension $d_r$, the reaction encoder is a 4-layer Multi-Layer Perceptron (MLP) as:\n\n$z_r = ReactionEnc(r) = W_4(SiLU(LN_3(W_3(SiLU(LN_2(W_2(SiLU(LN_1(W_1r + B_1)))) + B_2))) + B_3))) + B_4 \\in \\mathbb{R}^{256},$ (5)\n\nwhere $W_1 \\in \\mathbb{R}^{d_r \\times 512}, B_1 \\in \\mathbb{R}^{512}, W_2 \\in \\mathbb{R}^{512 \\times 256}, B_2 \\in \\mathbb{R}^{256}, W_3, W_4 \\in \\mathbb{R}^{256 \\times 256}, B_3, B_4 \\in \\mathbb{R}^{256}$. The enzyme encoder, denoted as EnzymeEnc, has a similar architecture, with only modification in the first-layer MLP as $W_1 \\in \\mathbb{R}^{1280 \\times 512}, B_1 \\in \\mathbb{R}^{512}$. And the encoded reaction and enzyme representations have the dimension of 256, as $z_r, z_e \\in \\mathbb{R}^{256}$.\n\nThe decoder network is a 4-layer MLP that takes the encoded enzyme-reaction pair and computes the prediction score:\n\n$y = Decoder(z_r, z_e) = W_4(W_3(SiLU(W_2(SiLU(W_1([z_r, z_e]) + B_1)) + B_2)) + B_3)) \\in \\mathbb{R},$ (6)\n\nwhere $W_1 \\in \\mathbb{R}^{512 \\times 256}, B_1 \\in \\mathbb{R}^{256}, W_2 \\in \\mathbb{R}^{256 \\times 128}, B_2 \\in \\mathbb{R}^{128}, W_3 \\in \\mathbb{R}^{128 \\times 64}, B_3 \\in \\mathbb{R}^{64}, W_4 \\in \\mathbb{R}^{64 \\times 1}$. In Appendix C, we further compare the simple MLP-decoder network with Transformer- and Bi-RNN-decoder networks (in Tables 9, 10, and 11), showing their retrieval performance."}, {"title": "5 Benchmarking on Reactzyme Dataset", "content": "5.1 Primary Empirical Evaluation\nBaseline Overview. We summarize the baseline models used for the enzyme-reaction retrieval task. For reaction representation, we employ Molecule Attention Transformer-2D (MAT-2D) [49], and UniMol-2D [83] for 2D molecular graphs, as well as MAT-3D and UniMol-3D for 3D molecular conformations. For enzyme representation, we employ ESM [41] and a structure-aware protein language model, SaProt [61]. Additionally, we use an equivariant graph neural network (FANN [55]) to enhance residue-level representations."}, {"title": "Metrics", "content": "In the evaluation of the enzyme-reaction retrieval task, we use several metrics: Top-k Accuracy, Top-k Accuracy-N, Mean Rank, and Mean Reciprocal Rank (MRR). (1) Top-k Accuracy quantifies the proportion of instances where the correct enzyme (or reaction) is ranked within the model's top-k predictions, irrespective of its exact position. (2) Top-k Accuracy-N refines this by assessing the frequency at which the correct enzyme (or reaction) is not only within the top-k predictions but also occupies the precise rank specified by N within this subset. For instance, with k=1, the correct enzyme must be the model's foremost prediction. (3) Mean Rank calculates the average position of the correct enzyme in the retrieval list, with lower values indicating better performance. (4) MRR evaluates how quickly the correct enzyme is retrieved by averaging the reciprocal ranks of the first correct enzyme across all reactions, ranging from 0 to 1, with higher values indicating better performance. More details and implementations can be found in Appendix A."}, {"title": "5.3 Potential Strategy", "content": "In Table 8, we report the accu- racy and AUROC of prediction models on positive and negative samples for enzyme-reaction pre- diction. While these metrics are secondary to the retrieval results discussed in Section 5.1, a strong correlation is evident between the retrieval performance and the ROC scores. Notably, the ROC scores for the reaction similarity- based split are lower compared to those for the time- and enzyme similarity-based splits. This pattern is similar in the retrieval results, underscoring the heightened difficulty of the reaction similarity-based task."}, {"title": "5.4 Further Evaluation", "content": "We present further experiments in the Appendices for deeper evaluation and comparison. In Ap- pendix C, we compare MLP, Transformer, and Bi-RNN decoder networks. Given the presence of annotated negative samples, we explore a contrastive learning approach in Appendix D. We also compare to the CLIPZyme pseudo-graph approach in Appendix E. And for a better description of chemical environment of reactants and product, we compare with fingerprint features in Appendix F."}, {"title": "6 Conclusion", "content": "In this paper, we introduce Reactzyme, a new benchmark for enzyme-reaction prediction. Unlike previous methods that rely on protein sequence or structure similarity or provide EC/GO annotations to predict reaction, our approach directly evaluates the mapping between enzymes and their catalyzed reactions. These enzyme-reaction prediction methods are able to handle protein with novel reactions and to discover proteins that catalyze unreported reactions.\n\nWe evaluate the performance of several baselines on the Reactzyme. While the baselines demonstrate competitive results on time- and enzyme-similarity-based splits, the reaction-similarity-based split remains particularly challenging. This difficulty may arise from the presence of many unseen reactions in the test set of the reaction-similarity-based split. One potential avenue for improvement is to explore contrastive learning techniques to address this challenge. However, we acknowledge that this remains an open problem for researchers in our community to tackle.\n\nThe Reactzyme benchmark facilitates the evaluation of models working with protein and molecule representations, which requires a comprehensive understanding in both modalities. Models demon- strating high performance in enzyme-reaction prediction can be further leveraged for protein function prediction and enzyme discovery. This includes identifying key enzymes in biosynthesis and dis- covering potent enzymes for degrading emerging pollutants, for these reactions that have not been previously found in enzymes."}, {"title": "A Metrics", "content": "The code for evaluation follows:\n1 import torch\n2\n3 def enzyme_reaction_evaluation(logits, labels):\n4 # logits=(n_enzyme, n_reaction); labels=(n_enzyme, n_reaction)\n5 #compute argsort according to logits values\n6 asrt = torch.argsort (logits, dim=1, descending=True, stable=True)\n7 ranking = torch.empty(logits.shape[0], logits.shape [1], dtype\n8 torch.long).scatter (1, asrt, torch.arange(logits.shape[1]).\n9 repeat (logits.shape[0], 1))\n10 ranking = (ranking + 1).to (labels.device)\n11\n12 #compute mean rank\n13 mean_rank = (ranking * labels.float()).sum (dim=-1) / (labels.sum(\n14 dim=-1))\n15 mean_rank = mean_rank.mean (dim=0)\n16\n17 #compute mrr\n18 mrr = (1.0 / ranking * labels.float()).sum (dim=-1) / (labels.sum(\n19 dim=-1)) # (num_seq)\n20 mrr = mrr.mean(dim=0)\n21\n22 top_accs = []\n23 top_accs_n = []\n24 for k in [1, 2, 3, 4, 5, 10, 20, 50]:\n25 #compute top-k acc\n26 top_acc = (((ranking <= k) * labels.float()).sum(dim=-1) > 0).\n27 float()\n28 top_acc = top_acc.mean (dim=0)\n29 top_accs.append(top_acc)\n30\n31 #compute top-k acc-n\n32 top_acc_n = ((ranking <= k) * labels.float()).sum(dim=-1) / k\n33 top_acc_n = top_acc_n.mean(dim=0)\n34 top_accs_n.append(top_acc_n)\n35 return top_accs[0], top_accs [1], top_accs [2], top_accs [3],\n36 top_accs [4], top_accs [5], top_accs [6], top_accs [7], top_accs_n[0],\n37 top_accs_n [1], top_accs_n [2], top_accs_n [3], top_accs_n[4],\n38 top_accs_n [5], top_accs_n[6], top_accs_n[7], mean_rank, mrr\nWe employ Top-k Accuracy, Top-k Accuracy-N, Mean Rank, and Mean Reciprocal Rank (MRR) to evaluate the enzyme-reaction retrieval task.\n\nTop-k Accuracy measures the percentage of cases where the correct enzyme (or reaction) is included within the top-k predictions made by the model; and it does not necessarily have to be the first prediction, as long as it is within the top-k. For Top-k Accuracy, the formula could be:\n\nTop-k Accuracy $=\\frac{\\text{Number of correct enzymes in top-k predictions}}{\\text{Total number of predictions}}$\n\nTop-k Accuracy-N measures how often the correct enzyme (or reaction) is not just within the top-k predictions, but also at the correct rank within those top-k. For example, if k=1, then the correct enzyme must be the model's top prediction. For Top-k Accuracy-N, the formula might look like:\n\nTop-k Accuracy-N  $\\frac{\\text{Number of correct enzymes at correct rank in top-k predictions}}{\\text{Total number of predictions}}$"}, {"title": "B Terminology of enzyme-reaction prediction, Enzyme-function prediction,\nenzyme-substrate/product prediction, and enzyme annotation", "content": "The terms or the concepts of 'enzyme reaction prediction', 'enzyme function prediction', 'enzyme substrate/product prediction', and 'enzyme annotation' may not be clearly delineated in the main section. In here, we aim to explain and address these concerns. There are indeed different types of annotations for enzyme, with function annotation being one of them. A reaction is part of the function, as not all functions map directly to a reaction. An enzyme reaction includes multiple features, such as substrate, product, and conditions (including the catalyst). This distinction helps clarify the various concepts like enzyme reaction prediction, function prediction, and substrate/product prediction."}, {"title": "C Experiments on Transformer and Bi-RNN Networks", "content": "In Section 4, we choose to use an encoder-decoder network over directly employment of Transformer or Bi-RNN. Here, we explain the intuition behind the use of the encoder-decocoder design over the transformer-like architectures. The encoder network, at the low-hierarchical level, aims to learn individual representations for enzymes and reactions, respectively. And the decoder network, at the high-hierarchical level, aims to learn the contacts or the interactions between any enzyme-reaction pair. Thus, in principle, the decoder could be any network that learns the interactions between enzymes and reactions.\n\nResults. In Section 4, we choose to use a MLP as the decoder network, here, we employ the Transformer and Bi-RNN as the decoder network for further evaluation. We compare the average results of baseline models by MLP, Transformer, Bi-RNN for time-based, enzyme similarity-based, and reaction similarity-based splits in Tables 9, 10, and 11, respectively."}, {"title": "D Experiments on Contrastive Learning", "content": "Results. In this section, we compare the average results of baseline models and the contrastive learning approach for time-based, enzyme similarity-based, and reaction similarity-based splits in Tables 12, 13, and 14, respectively. For enzyme-reaction prediction, contrastive learning can be used to learn embeddings or representations of enzymes and reactions that are predictive of their interactions. Positive pairs are optimized to have similar representations, while the negative pairs are optimized to be distinct in the embedding space."}, {"title": "E Experiments on Cross-Attention and Pseudo-graph for \u2018Transition State\u2019", "content": "In Section 4, we mention the concept of creating a pseudo-transition state graph for substrates and products introduced in CLIPZyme [51], and we choose to use the cross-attention to describe the transition state. Here, we further evaluate between the pseudo-graph approach in CLIPZyme [51] and our cross-attention approach.\n\nResults. We compare the average results of baseline models and the pseudo-graph of CLIPZyme for time-based, enzyme similarity-based, and reaction similarity-based splits in Tables 15, 16, and 17, respectively. We observe there is significant performance increase in reaction similarity-based split bu using the pseudo-graphs for transition states. However, the method does not improve the performance or the improvements are incremental on time-based and enzyme-similarity-based splits in comparison with cross-attention of the baseline models."}, {"title": "F Experiments on Fingerprint Features", "content": "In addition to the use of one-hot encoded atomic and bond features, we study the encodings of using fingerprints generated by RDKit to describe the chemical environments of reactants and products.\n\nResults. We compare the average results of baseline models and the fingerprint features for time- based, enzyme similarity-based, and reaction similarity-based splits in Tables 18, 19, and 20, respec- tively."}, {"title": "G No Significant Improvement with Molecular Conformations: An Intuitive Explanation from 3Di Perspective", "content": "In our paper, we compared models like ESM (without structure) and SaProt (with structure), as well as models with 2D or 3D molecular conformation information. The results showed inconsistent performance when structural features were included in different tasks. We believe that this might be because the fact that SaProt encodes only 3Di information, which lacks the detailed"}]}