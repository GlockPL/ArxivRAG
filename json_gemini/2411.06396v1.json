{"title": "A Variance Minimization Approach to Temporal-Difference Learning", "authors": ["Xingguo Chen", "YuGong", "Shangdong Yang", "Wenhao Wang"], "abstract": "Fast-converging algorithms are a contemporary requirement in reinforcement learning. In the context of linear function approximation, the magnitude of the smallest eigenvalue of the key matrix is a major factor reflecting the convergence speed. Traditional value-based RL algorithms focus on minimizing errors. This paper introduces a variance minimization (VM) approach for value-based RL instead of error minimization. Based on this approach, we proposed two objectives, the Variance of Bellman Error (VBE) and the Variance of Projected Bellman Error (VPBE), and derived the VMTD, VMTDC, and VMETD algorithms. We provided proofs of their convergence and optimal policy invariance. Experimental studies validate the effectiveness of the proposed algorithms.", "sections": [{"title": "Introduction", "content": "Reinforcement learning (RL) can be mainly divided into two categories: value-based reinforcement learning and policy gradient-based reinforcement learning. This paper focuses on temporal difference learning based on linear approximated valued functions. Its research is usually divided into two steps: the first step is to establish the convergence of the algorithm, and the second step is to accelerate the algorithm.\nIn terms of stability, Sutton (1988) established the convergence of on-policy TD(0), and Tsitsiklis and Van Roy (1997) established the convergence of on-policy TD(\u03bb). However, \"The deadly triad\" consisting of off-policy learning, bootstrapping and function approximation makes the stability a difficult problem (Sutton and Barto 2018). To solve this problem, convergent off-policy temporal difference learning algorithms are proposed, e.g., BR (Baird et al. 1995), GTD (Sutton, Maei, and Szepesv\u00e1ri 2008), GTD2 and TDC (Sutton et al. 2009), ETD (Sutton, Mahmood, and White 2016), and MRetrace (Chen et al. 2023).\nIn terms of acceleration, Hackman (2012) proposed a Hybrid TD algorithm with the on-policy matrix. Liu et al. (2015, 2016, 2018) proposed true stochastic algorithms, i.e., GTD-MP and GTD2-MP, from a convex-concave saddle-point formulation. Second-order methods are used to accelerate TD learning, e.g., Quasi Newton TD (Givchi and\n\nPalhang 2015) and accelerated TD (ATD) (Pan, White, and White 2017). Hallak et al. (2016) introduced a new parameter to reduce variance for ETD. Zhang and Whiteson (2022) proposed truncated ETD with a lower variance. Variance Reduced TD with direct variance reduction technique (Johnson and Zhang 2013) is proposed by (Korda and La 2015) and analysed by (Xu et al. 2019). How to further improve the convergence rates of reinforcement learning algorithms is currently still an open problem.\nAlgorithm stability is prominently reflected in the changes to the objective function, transitioning from mean squared errors (MSE) (Sutton and Barto 2018) to mean squared bellman errors (MSBE) (Baird et al. 1995), then to norm of the expected TD update (Sutton et al. 2009), and further to mean squared projected Bellman errors (MSPBE) (Sutton et al. 2009). On the other hand, the algorithm acceleration is more centered around optimizing the iterative update the formula of the algorithm itself without altering the the objective function, thereby speeding up the convergence rate of the algorithm. The emergence of new optimization objective functions often lead to the development of novel algorithms. The introduction of new algorithms, in turn, tends to inspire researchers to explore methods for accelerating algorithms, leading to the iterative creation of increasingly superior algorithms.\nThe kernel loss function can be optimized using standard gradient-based methods, addressing the issue of double sampling in residual gradient algorithm (Feng, Li, and Liu 2019). It ensures convergence in both on-policy and off-policy scenarios. The logistic bellman error is convex and smooth in the action-value function parameters, with bounded gradients (Bas-Serrano et al. 2021). In contrast, the squared Bellman error is not convex in the action-value function parameters, and RL algorithms based on recursive optimization using it are known to be unstable.\nIt is necessary to propose a new objective function, but the abovementioned objective functions are all some form of error. Is minimizing error the only option for value-based reinforcement learning?\nThe contributions of this paper are as follows: (1) Introduction of variance minimization (VM) approach for value-based RL instead of error minimization. (2) Based on this approach, we proposed two objectives, the Variance of Bellman Error (VBE) and the Variance of Projected Bellman Er-"}, {"title": "Background", "content": "Markov Decision Process\nReinforcement learning agent interacts with the environment, observes the state, takes sequential decision-making to influence the environment, and obtains rewards. Consider an infinite-horizon discounted Markov Decision Process (MDP), defined by a tuple \u3008S, A, R, P, \u03b3 \u3009, where S = {1, 2, ..., N} is a finite set of states of the environment; A is a finite set of actions of the agent; R : S \u00d7 A \u00d7 S \u2192 R is a bounded deterministic reward function; P : S \u00d7 A \u00d7 S \u2192 [0, 1] is the transition probability distribution; and \u03b3 \u2208 (0, 1) is the discount factor (Sutton and Barto 2018). Due to the requirements of online learning, value iteration based on sampling is considered in this paper. In each sampling, an experience (or transition) (s, a, s', r) is obtained.\nA policy is a mapping \u03c0 : S \u00d7 A \u2192 [0, 1]. The goal of the agent is to find an optimal policy \u03c0\u2217 to maximize the expectation of a discounted cumulative rewards over a long period. For each discrete time step t = 0, 1, 2, 3, ..., State value function V\u03c0(s) for a stationary policy \u03c0 is defined as:\n$$V^{\\pi}(s) = \\mathbb{E}^{\\pi}[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t = s].$$\nLinear value function for state s \u2208 S is defined as:\n$$V_{\\theta}(s) := \\theta^T \\phi(s) = \\sum_{i=1}^{m} \\theta_i \\phi_i(s),$$\nwhere \u03b8 := (\u03b81, \u03b82, . . . , \u03b8m) \u2208 Rm is a parameter vector, \u03c6 := (\u03c61, \u03c62, . . . , \u03c6m) \u2208 Rm is a feature function defined on state space S, and m is the feature size.\nTabular temporal difference (TD) learning (Sutton and Barto 2018) has been successfully applied to small-scale problems. To deal with the well-known curse of dimensionality of large-scale MDPs, the value function is usually approximated by a linear model (the focus of this paper), kernel methods, decision trees, neural networks, etc.\nOn-policy and Off-policy Learning\nOn-policy and off-policy algorithms are currently hot topics in research. The main difference between the two lies in the fact that in on-policy algorithms, the behavior policy \u00b5 and the target policy \u03c0 are the same during the learning process. In off-policy algorithms, however, the behavior policy and the target policy are different. The algorithm uses data generated from the behavior policy to optimize the target policy, which leads to higher sample efficiency and complex stability issues.\nFrom the theory of stochastic methods, the the convergence point of linear TD algorithms is a parameter vector, say \u03b8, that satisfies\n$$b - A\\theta = 0,$$ \nwhere A \u2208 R|S|\u00d7m and b \u2208 Rm. If the matrix A is positive definite, then the algorithm converges."}, {"title": "Theorem 1.", "content": "(The main factor affecting convergence rates (Chen et al. 2024)). Assume the same parameters setting for each algorithm, from the perspective of the expected convergence rate, the main factor that affects the convergence rate is the minimum eigenvalue of the matrix (A + AT ). The larger the minimum eigenvalue, the faster the convergence rate.\nNext, we will compute the minimum eigenvalue of A for TD(0), TDC, and ETD in both on-policy and off-policy settings in a 2-state environment. First, we will introduce the environment setup for the 2-state case in both on-policy and off-policy settings.\n\nThe \"1\"\u2192\"2\" problem has only two states. From each state, there are two actions, left and right, which take the agent to the left or right state. All rewards are zero. The feature \u03a6 = (\u03c61, \u03c62) are assigned to the left and the right state. The first policy takes equal probability to left or right in both states, i.e., $P_1 = \\begin{bmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{bmatrix}$. The second policy only selects action rights in both states, i.e., $P_2 = \\begin{bmatrix} 1 & 0 \\\\ 1 & 0 \\end{bmatrix}$.\nThe state distribution of the first policy is d1 = (0.5, 0.5)T. The state distribution of the second policy is d1 = (0, 1)T. The discount factor is \u03b3 = 0.9. In the on-policy setting, the behavior policy and the target policy are the same, so let P\u00b5 = P\u03c0 = P1. In the off-policy setting, let P\u00b5 = P1 and P\u03c0 = P2.\nThe key matrix Aon of on-policy TD(0) is\n$$A_{on} = \\Phi^T D_{\\mu} (I - \\gamma P_{\\mu})\\Phi,$$ \nwhere \u03a6 is the |S| \u00d7 m matrix with the \u03c6(s) as its rows, and D\u00b5 is the |S| \u00d7 |S| diagonal matrix with d\u00b5 on its diagonal. d\u00b5 is a vector, each component representing the steady-state distribution under policy \u03c0. P\u03c0 denote the |S| \u00d7 |S| matrix of transition probabilities under \u03c0. And Pd\u00b5 = d\u03c0.\nThe key matrix Aoff of off-policy TD(0) is\n$$A_{off} = \\Phi^T D_{\\mu} (I - \\gamma P_{\\pi})\\Phi,$$ \nwhere D\u00b5 is the |S| \u00d7 |S| diagonal matrix with d\u00b5 on its diagonal. d\u00b5 is a vector, each component representing the steady-state distribution under behavior policy \u00b5.\nIn the off-policy 2-state, Aoff = -0.2, which means that off-policy TD(0) cannot stably converge, while, in the on-policy 2-state, Aon = 0.475, which means that on-policy TD(0) can stably converge.\nThe key matrix $A_{TDC} = A_{off}C^{-1}A_{off}$, where $C = E[\\phi\\phi^T]$. In the 2-state counterexample, ATDC = 0.016, which means that TDC can stably converge.\nThe key matrix ATDC of on-policy TDC is\n$$A_{TDC} = A_{on}C^{-1}A_{on}.$$ \nThe key matrix ATDC of off-policy TDC is\n$$A_{TDC} = A_{off}C^{-1}A_{off}.$$"}, {"title": "Variance Minimization Algorithms", "content": "This section will introduce two new objective functions and three new algorithms, including one on-policy algorithm and two off-policy algorithms, and calculate the minimum eigenvalue of A for each of the three algorithms under on-policy and off-policy in a 2-state environment, thereby comparing the convergence speed of the three algorithms.\nVariance Minimization TD Learning: VMTD\nFor on-policy learning, a novel objective function, Variance of Bellman Error (VBE), is proposed as follows:\n$$\\arg \\min_{\\theta} VBE(\\theta) = \\arg \\min_{\\theta} \\mathbb{E}[(E[d_t|S_t] - E[E[d_t|S_t]])^2]$$\n$$ = \\arg \\min_{\\theta, \\omega} \\mathbb{E}[(E[d_t|S_t] - \\omega)^2]$$\nwhere dt is the TD error as follows:\n$$d_t = r_{t+1} + \\gamma \\theta^T \\phi_{t+1} - \\theta^T \\phi_t.$$\nClearly, it is no longer to minimize Bellman errors.\nFirst, the parameter w is derived directly based on stochastic gradient descent:\n$$\\omega_{t+1} \\leftarrow \\omega_t + \\beta_t(d_t - \\omega_t),$$\nThen, based on stochastic semi-gradient descent, the update of the parameter \u03b8 is as follows:\n$$\\theta_{t+1} \\leftarrow \\theta_t + \\alpha_t(d_t - \\omega_t)\\phi_t.$$\nThe semi-gradient of the (2) with respect to \u03b8 is\n$$- \\nabla VBE(\\theta)\n=\n\\mathbb{E}[(E[d_t|S_t] - \\mathbb{E}[\\mathbb{E}[d_t|S_t]])(\\phi_t - \\mathbb{E}[\\phi_t])]\\\\\n\\mathbb{E}[d_t \\phi_t] - \\mathbb{E}[d_t] \\mathbb{E}[\\phi_t],$$\nThe key matrix AVMTD and byMTD of on-policy VMTD is\n$$A_{VMTD} =\n\\mathbb{E}[(\\phi - \\gamma \\phi')\\phi^T] - \\mathbb{E}[\\phi - \\gamma \\phi']\\mathbb{E}[\\phi^T]\\\\\n\\sum_s d(s) \\phi(s) (\\phi(s) - \\sum_{s'} \\gamma [P_{\\pi}]_{ss'} \\phi(s'))^T-\n\\sum_s d(s) \\phi(s) \\sum_s (\\phi(s) - \\sum_{s'} \\gamma [P_{\\pi}]_{ss'} \\phi(s'))\\\n\\Phi^T D_{\\pi}(I - \\gamma P_{\\pi})\\Phi - \\Phi^T d_{\\pi}d_{\\pi}^T(I - \\gamma P_{\\pi})\\Phi\\\n\\Phi^T (D_{\\pi} - d_{\\pi}d_{\\pi}^T) (I - \\gamma P_{\\pi})\\Phi,$$\n$$b_{VMTD} = E(r - E[r]) = E[r\\phi] - E[r]E[\\phi] = \\Phi^T (D - dd^T)r_{\\pi}.$$ \nIt can be easily obtained that The key matrix AVMTD and bvMTD of off-policy VMTD are, respectively,\n$$A_{VMTD} = \\Phi^T (D_{\\mu} - d_{\\mu}d_{\\mu}^T) (I - \\gamma P_{\\pi})\\Phi,$$\n$$b_{VMTD} = \\Phi^T (D_{\\mu} - d_{\\mu}d_{\\mu}^T)r_{\\pi},$$\nIn the on-policy 2-state environment, the minimum eigenvalue of the key matrix for VMTD is greater than that of TDC and smaller than that of TD(0) and ETD, indicating that VMTD converges faster than TDC and slower than TD(0) and ETD in this environment. In the off-policy 2-state environment, the minimum eigenvalue of the key matrix for VMTD is greater than 0, suggesting that VMTD can converge stably, while TD(0) diverges."}, {"title": "Variance Minimization TDC Learning: VMTDC", "content": "For off-policy learning, we propose a new objective function, called Variance of Projected Bellman error (VPBE), and the corresponding algorithm is called VMTDC.\n$$VPBE(\\theta) = E[(d - E[d])\\phi]^T E[\\phi\\phi^T]^{-1} E[(d - E[d])\\phi]$$\n$$ = E[(d - \\omega)\\phi]^T E[\\phi\\phi^T]^{-1} E[(d - \\omega)\\phi],$$\nwhere w is used to approximate E[d], i.e., w = E[d].\nThe gradient of the (6) with respect to \u03b8 is\n$$- \\nabla VPBE(\\theta) =\n-E[(( - ) - E[()])]E[TT]\u22121E[(-)4]\\\\ E[((\u03c6 - \u03b3\u03c6') \u2013 E[(\u03c6 - \u03b3\u03c6')])]E[\u03a6\u03a6T]-1\nE[r + \u03b3\u03c6'T\u03b8 \u2212 \u03c6T\u03b8] \u03c6].$$\nIt can be easily obtained that The key matrix AVMTDC and bVMTDC of VMTDC are, respectively,\n$$A_{VMTDC} = A_{VMTD} C^{-1} A_{VMTD},$$\n$$b_{VMTDC} = A_{VMTD} C^{-1} b_{VMTD},$$\nwhere, for on-policy, $A_{VMTD} = \\Phi^T (D - d_{\\pi}d_{\\pi}^T)(I - \\gamma P_{\\pi})\\Phi$ and $b_{VMTD} = \\Phi^T (D - d_{\\pi}d_{\\pi}^T)r_{\\pi}$ and, for off-policy, $A_{VMTD} = \\Phi^T (D_{\\mu} - d_{\\mu}d_{\\mu}^T) (I - \\gamma P_{\\pi})\\Phi$ and $b_{VMTD} = \\Phi^T (D_{\\mu} - d_{\\mu}d_{\\mu}^T) r_{\\pi}$.\nIn the process of computing the gradient of the (7) with respect to \u03b8, w is treated as a constant. So, the derivation process of the VMTDC algorithm is the same as for the TDC algorithm, the only difference is that the original d is replaced by d \u2212 w. Therefore, we can easily get the updated formula of VMTDC, as follows:\n$$\\theta_{k+1} \\leftarrow \\theta_k + \\alpha_k[(d_k - \\omega_k)\\phi_k - \\gamma \\Phi_k(\\Phi_k \\upsilon_k)],$$\n$$\\upsilon_{k+1} \\leftarrow \\upsilon_k + \\zeta_k[(d_k - \\omega_k)\\phi_k - \\Phi_k \\upsilon_k],$$\n$$\\omega_{k+1} \\leftarrow \\omega_k + \\beta_k(d_k - \\omega_k).$$\nThe VMTDC algorithm (8) is derived to work with a given set of sub-samples in the form of triples (Sk, Rk, S'k) that match transitions from both the behavior and target policies.\nIn the on-policy 2-state environment, the minimum eigenvalue of the key matrix for VMTDC is smaller than that of TD(0), TDC, ETD, and VMTD indicating that VMTDC converges slower than them in this on-policy. In the off-policy 2-state environment, the the minimum eigenvalue of the key matrix for VMTDC is greater than TDC, suggesting that VMTDC converges faster than TDC in off-policy environment."}, {"title": "Variance Minimization ETD Learning: VMETD", "content": "Based on the off-policy TD algorithm, a scalar, F, is introduced to obtain the ETD algorithm, which ensures convergence under off-policy conditions. This paper further introduces a scalar, w, based on the ETD algorithm to obtain VMETD. VMETD by the following update:\n$$F_{t} \\coloneqq \\gamma p_{t-1}F_{t-1} + 1,$$\n$$\\theta_{t+1} \\leftarrow \\theta_t + \\alpha_t(F_t p_t d_t - \\omega_t)\\phi_t,$$\n$$\\omega_{t+1} \\leftarrow \\omega_t + \\beta_t(F_t p_t d_t - \\omega_t),$$\nwhere $p_t = \\frac{\\mu(A_t|S_t)}{\\pi(A_t|S_t)}$ and w is used to estimate E[Fp\u03b4], i.e., \u03c9 = E[E\u03c1\u03b4].\n(12) can be rewritten as\n$$\\theta_{t+1} \\leftarrow \\theta_t + \\alpha_t(F_t p_t d_t - \\omega_t)\\phi_t - \\alpha_t \\omega_{t+1} \\phi_t \\\\\n\\theta_t + \\alpha_t (F_t p_t d_t - \\mathbb{E}_{\\mu}[F_t p_t d_t|\\theta_t])\\phi_t \\\\\n\\theta_t + \\alpha_t F_t p_t (r_{t+1} + \\gamma \\theta_t^T \\phi_{t+1} - \\theta_t^T \\phi_t)\\phi_t-\n\\alpha_t \\mathbb{E}_{\\mu}[F_t p_t d_t] \\phi_t\\\\\n\\theta_t + \\alpha_t\\{(F_t p_t r_{t+1} - \\mathbb{E}_{\\mu}[F_t p_t r_{t+1}])\\phi_t \\\\\n- (F_t p_t \\phi_t(\\phi_t - \\gamma \\phi_{t+1})^T - \\phi_t \\mathbb{E}_{\\mu}[F_t p_t(\\phi_t - \\gamma \\phi_{t+1})^T]) \\theta_t\\}.$$\nTherefore,\n$$A_{VMETD} =\n\\lim_{t \\to \\infty} \\mathbb{E}[A_{VMETD,t}]\\\n\\lim_{t \\to \\infty} \\mathbb{E}_{\\mu}[\\phi_t F_t p_t(\\phi_t - \\gamma \\phi_{t+1})^T] - \\lim_{t \\to \\infty} \\mathbb{E}_{\\mu}[\\phi_t] \\mathbb{E}_{\\mu}[F_t p_t(\\phi_t - \\gamma \\phi_{t+1})^T]\\\n=\n\\lim_{t \\to \\infty} \\mathbb{E}_{\\mu}[\\phi_t F_t p_t(\\phi_t - \\gamma \\phi_{t+1})^T] - \\lim_{t \\to \\infty} \\mathbb{E}_{\\mu}[\\phi_t] \\lim_{t \\to \\infty} \\mathbb{E}_{\\mu}[F_t p_t(\\phi_t - \\gamma \\phi_{t+1})^T]\\\n\\sum_s f(s) \\mathbb{E}_{\\pi}[\\phi_t(\\phi_t - \\gamma \\phi_{t+1})^T|S_t = s] - \\sum_s d_{\\mu}(s)\\phi(s) * \\sum_s f(s) \\mathbb{E}_{\\pi}[(\\phi_t - \\gamma \\phi_{t+1})^T|S_t = s]\\\n\\sum_s f(s)\\phi(s)(\\phi(s) - \\sum_{s'} [P_{\\pi}]_{ss'} \\phi(s')) - \\sum_s d_{\\mu}(s)\\phi(s) * \\sum_s f(s) (\\phi(s) - \\sum_{s'} [P_{\\pi}]_{ss'} \\phi(s'))\\\n\\Phi^T F(I - \\gamma P_{\\pi})\\Phi - \\Phi^T d_{\\mu} f^T (I - \\gamma P_{\\pi})\\Phi\\\n\\Phi^T (F(I - \\gamma P_{\\pi}) - d_{\\mu} f^T (I - \\gamma P_{\\pi}))\\Phi\\\n\\Phi^T (F - d_{\\mu}f) (I - \\gamma P_{\\pi})\\Phi,$$\n$$b_{VMETD} = \\lim_{t \\to \\infty} \\mathbb{E}[b_{VMETD,t}]\\\n\\lim_{t \\to \\infty} \\mathbb{E}_{\\mu}[\\phi_t F_t p_t R_{t+1}] - \\lim_{t \\to \\infty} \\mathbb{E}_{\\mu}[\\phi_t] \\mathbb{E}_{\\mu}[F_t p_k R_{k+1}]\\\n=\n\\lim_{t \\to \\infty} \\mathbb{E}_{\\mu}[\\phi_t F_t p_t r_{t+1}] - \\lim_{t \\to \\infty} \\mathbb{E}_{\\mu}[\\phi_t] \\lim_{t \\to \\infty} \\mathbb{E}_{\\mu}[F_t p_t r_{t+1}]\\\n\\sum_s f(s)\\phi(s) r_{\\pi} - \\sum_s d_{\\mu}(s) \\phi(s) * \\sum_s f(s) r_{\\pi}\\\n=\\Phi^T (F - d_{\\mu}f)r_{\\pi}.$$\nIn both the off-policy 2-state environment and the on-policy 2-state environment, the minimum eigenvalue of the key matrix for VMETD is greater than that of TD(0), TDC, VMTD, and VMTDC and smaller than that of ETD, indicating that"}, {"title": "Theoretical Analysis", "content": "This section primarily focuses on proving the convergence of VMTD, VMTDC, and VMETD.\nTheorem 2. (Convergence of VMTD). In the case of on-policy learning, consider the iterations (4) and (5) with (3) of VMTD. Let the step-size sequences \u03b1k and \u03b2k, k \u2265 0 satisfy in this case \u03b1k, \u03b2k > 0, for all k, $\\sum_{k=0}^{\\infty} \\alpha_k$\n$\\sum_{k=0}^{\\infty} \\beta_k = \\infty, \\sum_{k=0}^{\\infty} \\alpha_k^2 < \\infty, \\sum_{k=0}^{\\infty} \\beta_k^2  < \\infty$, and \u03b1k = o(\u03b2k). Assume that (\u03c6k, rk, \u03c6k) is an i.i.d. sequence with uniformly bounded second moments, where \u03c6k and \u03c6k are sampled from the same Markov chain. Let A =\nCov(\u03c6, \u03c6 \u2013 \u03b3\u03c6'), b = Cov(r, \u03c6). Assume that matrix A is non-singular. Then the parameter vector \u03b8k converges with probability one to A\u22121b.\nProof. The proof is based on Borkar's Theorem for general stochastic approximation recursions with two time scales (Borkar 1997).\nA sketch proof is given as follows. In the fast time scale, the parameter w converges to E[\u03b4k|\u03b8k]. In the slow time scale, the associated ODE is\n$$ \\dot{\\theta}(t) = h(\\theta(t)) = -A\\theta(t) + b. $$ \n$$A = Cov(\\phi, \\phi - \\gamma \\phi')\\\\\nCov(\\phi, \\phi - \\gamma \\phi') + Cov(\\phi - \\gamma \\phi', \\phi - \\gamma \\phi') - Cov(\\gamma \\phi', \\gamma \\phi') \\\\\nCov(\\phi,\\phi) + Cov(\\phi-\\gamma \\phi',\\phi-\\gamma \\phi')-\\gamma^2 Cov(\\phi',\\phi') \\\\\n(1-\\gamma^2)Cov(\\phi,\\phi)+Cov(\\phi-\\gamma \\phi',\\phi-\\gamma \\phi'),$$\nwhere we eventually used Cov(\u03c6', \u03c6') = Cov(\u03c6, \u03c6) 2. Note that the covariance matrix Cov(\u03c6, \u03c6) and Cov(\u03c6 \u2013 \u03b3\u03c6', \u03c6 \u2013 \u03b3\u03c6') are semi-positive definite. Then, the matrix A is semi-positive definite because A is linearly combined by two positive-weighted semi-positive definite matrice (22). Furthermore, A is nonsingular due to the assumption. Hence, the matrix A is positive definite.\nTherefore, \u03b8\u2217 = A\u22121b can be seen to be the unique globally asymptotically stable equilibrium for ODE (21). Let h\u221e(\u03b8) =\nlimr\u2192\u221e h(r\u03b8). Then h\u221e(\u03b8) = \u2212A\u03b8 is well-defined. Consider now the ODE\n$$\\dot{\\theta}(t) = -A\\theta(t).$$ \nThe ODE (23) has the origin of its unique globally asymptotically stable equilibrium. Thus, the assumption (A1) and (A2) are verified.\nPlease refer to the appendix for VMTD's detailed proof process.\nTheorem 3. (Convergence of VMTDC). In the case of off-policy learning, consider the iterations (10), (9) and (8) of VMTDC. Let the step-size sequences \u03b1k, \u03b6k and \u03b2k, k \u2265 0 satisfy in this case \u03b1k, \u03b6k, \u03b2k > 0, for all k, $\\sum_{k=0}^{\\infty} \\alpha_k$\n$\\sum_{k=0}^{\\infty} \\beta_k = \\sum_{k=0}^{\\infty}  \\zeta_k = \\infty, \\sum_{k=0}^{\\infty} \\alpha_k^2 < \\infty, \\sum_{k=0}^{\\infty}  \\zeta_k^2 <\n18, \\sum_{k=0}^{\\infty} \\beta_k^2  < \\infty$, and \u03b1\u03ba = o(\u03b6k), \u03b6k = o(\u03b2k). Assume that (\u03c6k, rk, \u03c6k) is an i.i.d. sequence with uniformly bounded second moments. Let A = Cov(\u03c6, \u03c6 \u2013 \u03b3\u03c6'), b = Cov(r, \u03c6), and C = E[\u03c6\u03c6T]. Assume that A and C are non-singular matrices. Then the parameter vector \u03b8k converges with probability one to A\u22121b.\n\nThe covariance matrix Cov(\u03c6', \u03c6') is equal to the covariance matrix Cov(\u03c6, \u03c6) if the initial state is re-reachable or initialized randomly in a Markov chain for on-policy update."}, {"title": "Proof.", "content": "The proof is similar to that given by (Sutton et al. 2009) for TDC, but it is based on multi-time-scale stochastic approximation.\nA sketch proof is given as follows. In the fastest time scale, the parameter w converges to E[\u03b4k, \u03b8k", "E[\u03b4|\u03b8k": "\u03c6\u03b8k", "E\u00b5[F\u03c1\u03b4|\u03b8k": ".", "I": "d_{\\mu} \\\\=\n(1 \u2013 \\gamma) [\\sum_{t=0}^{\\infty} (\\gamma P_{\\pi})^t \u2013 I", "P_{\\pi})^t)": "d_{\\mu} > 0,$$\n$$1^T(F(I \u2013 \\gamma P_{\\pi}) \u2013 d_{\\mu}d_{\\mu}^T) = 1^T F(I \u2013 \\gamma P_{\\pi}) \u2013 1^T d_{\\mu}d_{\\mu}^T \\\\=\n\\dot{d_{\\mu}} - 1^T d_{\\mu}d_{\\mu}^T\\\\=\n\\dot{d_{\\mu}} - \\dot{d_{\\mu}}\\\\=\n0.$$\n(43) and (44) show that the matrix F(I \u2212 \u03b3P\u03c0) \u2212 d\u00b5d\u00b5T of diagonal entries are positive and its off-diagonal entries are negative. So each row sum plus the corresponding column sum is positive. So AVMETD is positive definite.\nTherefore, \u03b8\u2217 ="}]}