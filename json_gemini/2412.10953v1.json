{"title": "Optimizing AI-Assisted Code Generation: Enhancing Security, Efficiency, and Accessibility in Software Development", "authors": ["Simon Torka", "Sahin Albayrak"], "abstract": "In recent years, the rise of AI-assisted code-generation tools has significantly transformed software development. While code generators have mainly been used to support conventional software development, their use will be extended to power-ful and secure Al systems. Systems capable of generating code, such as ChatGPT, OpenAI Codex, GitHub Copilot, and AlphaCode, take advantage of advances in machine learning (ML) and natural language processing (NLP) enabled by large language models (LLMs). However, it must be borne in mind that these models work probabilistically, which means that although they can generate complex code from natural language input, there is no guarantee for the functionality and security of the generated code.\nHowever, to fully exploit the considerable potential of this technology, the security, reliability, functionality, and quality of the generated code must be guaranteed. This paper examines the implementation of these goals to date and explores strategies to optimize them. In addition, we explore how these systems can be optimized to create safe, high-performance, and executable artificial intel-ligence (AI) models, and consider how to improve their accessibility to make AI development more inclusive and equitable.", "sections": [{"title": "1. Introduction", "content": "Software development is currently undergoing a profound change due to the use of AI-supported code generation tools 1. While earlier systems aimed to reduce the complexity of conventional software development, new systems based on Large Language Models (LLM) have significantly expanded these capabilities. Advances in context understanding, language generation, and multilingualism have contributed significantly to this development through better human-AI interaction [7, 101, 73].\nIn the field of software development, systems such as ChatGPT, OpenAI Codex, GitHub Copilot, and AlphaCode now enable the translation of natural language into executable code, making software development much easier. These tools can autonomously create programs, classes, functions, and test cases in different programming languages and far outperform conventional autocompletion tools [38, 70, 76, 86, 56, 96]. For example, GitHub Copilot generates code that is comparable to human-written code in terms of complexity and readability [1, 76]. This leads to considerable time savings and increases efficiency, effectiveness, and productivity, which also shortens the time to market for new software [40, 57, 76, 86, 39]. As a result, 70% of professional developers2 use or plan to use AI technologies throughout the entire software development cycle\u00b3 [40]. AI-based tools are already being used in areas such as project planning, requirements definition, requirements analysis, code generation, testing, refactoring, change management, optimization, debugging, and documentation as well as in the explanation of code [38, 40, 73, 74, 76]. However, it can also be increasingly observed that these systems are being used for security-related tasks such as the detection and elimination of\nvulnerabilities, coding errors, and security gaps, but also for the development of malware and exploits [4, 40].\nThe rapid development of these technologies and their enormous potential not only increases the productivity of professional developers but also increasingly opens up the possibility for non-AI/code specialists [21], private individuals [84] and non-profit organisations [41] to create programmes and Al models. One outstanding example is ChatGPT, which uses prompting techniques to enable intuitive interaction in natural language, thus transforming conventional programming and AI development in the long term [73].\nThe social potential can be utilized particularly in the area of \u2018AI for Good' (AI4G)4 to promote socially effective projects independently and efficiently [41]. Supporting technologies such as LLMs (Large Language Models) facilitate access to AI and software development and promote the use of software and AI in these areas.\nDespite these advantages, there are also challenges, particularly concerning the security, reliability, and quality of the generated code [76]. For example, many users lack the necessary expertise to sufficiently validate the results generated by LLM. In addition, the misuse of these tools, whether intentional or unintentional, can have serious consequences, especially as they are increasingly being used to develop code in safety-critical environments [40].\nTo overcome these barriers, user-friendly [98] No-/low-code assistance systems [84], AI platforms5 and the introduction of AI ecosystems [71] offer promising approaches. These tools enable non-experts to actively participate in knowledge analysis and AI development and thus promote participation in software development processes [70].\nThis paper aims to analyze current developments in AI-supported code generation, in particular concerning safety aspects, reliability, and code quality. Based on this, a tool is to be developed that generates secure, efficient, and functional code as well as AI models that are also easy to use for non-experts.\nSection 2 provides an overview of current developments in AI-based code generation tools, in particular the role of LLM and its increasing influence on software development. Section 3 focuses on the use of AI systems to generate software code and AI models, while section 4 covers the technical and security risks associated with these tools. Together, these sections explore how AI code generation can be safely integrated into workflows to maximize benefits and minimize risks. Section 5 explores attack vectors such as prompt injection and jailbreaks, as well as mitigation strategies. Section 6 highlights the dual role of AI code generators as tools for attacking and defending systems. Section 7 picks up on these insights and presents a system that generates secure and executable code and optimizes human-AI interactions even for non-experts. Finally, section 8 summarises the most important findings of the paper."}, {"title": "2. Current state of development", "content": "AI-based code generators  often referred to as AI assistants \u2013 support developers in software development by using machine learning (ML) and natural language processing (NLP) [38]. They are usually based on large language models (LLMs) that enable natural language interaction and can understand and process complex, multi-step instructions [11, 38, 76, 86, 101].\nA key technical aspect that significantly shapes the capabilities of these models is their distinction into encoders, decoders, and encoder-decoder (sequence-to-sequence)-models, which perform different tasks in the processing of speech and code [90, P. 83-86]:\n\u2022 Encoder models (like BERT) are specialized in analyzing input sequences and generating context-dependent representations. They are particularly well-suited for analyzing code and detecting patterns or errors (see the left side of Figure 1).\n\u2022 Decoder models (such as GPT-3) focus on generating sequences based on an input, such as a short text or code prompt. They are particularly efficient at creating new code or text by predicting further tokens based on the previous context (see the right side of Figure 1).\n\u2022 Encoder-decoder models (also known as sequence-to-sequence models, such as T5 or BART) combine both approaches by processing input sequences and generating matching output sequences from them. They are particularly useful for transformation tasks, such as translating natural language into code or automating code repairs [76, 38] (see the overall structure in Figure 1).\nAnother interesting development is the increasing use of platforms such as Hugging Face [19] and other LLM hubs, which are central points of contact for pre-trained models and tools for fine-tuning and integrating LLMs [84]. These platforms provide access not only to models but also to the infrastructure to adapt and use them efficiently for different tasks. This enables a broad application of LLMs in software development and beyond, for example for generating models that meet the requirements of specific domains.\nHowever, the increasing use of such LLM-based assistants also presents challenges. For example, their increasing use endangers previously established information sources such as search engines or online communities such as Stack Overflow, which could displace them [40, 76, 8]. However, since these information sources often also serve as the training basis for the LLM models, a decline in the availability and quality of this data could have negative feedback effects on the performance of the LLM models [8, 40]. Nevertheless, the prevalence of such systems is increasing, as evidenced by telemetry data from the Bing Developer Assistant [95, 102]. This data shows that programmers use AI-supported tools even if they already know the solution, as these tools make work faster and more efficient [95, 102]."}, {"title": "3. AI code generation", "content": "In artificial intelligence (AI), code generators have established themselves as powerful tools that can accelerate and automate the development of both conventional software [56, 72, 96] and the creation of AI systems [76, 84, 98].\n3.1. AI code generation of conventional code\nIn software engineering, AI code generators can be used in various ways, e.g. in software de-velopment, data processing and analysis, and as cooperative non-human assistants [73]. AI-supported functions for code generation, completion, translation, optimization, summarisation, verification, error correction, documentation, and clone detection far outperform existing systems [56, 73, 96]. For example, they can generate program code in various programming languages based on prompts [56, 96].\nIn addition, the use of AI techniques allows knowledge and experience to be derived from previous projects [96, 98]. The high performance of these systems results in particular from the use of modern LLMs that have been trained with billions of data points [56]. Due to this performance, LLMs have proven to be an important design method that is potentially accessible to non-experts and makes the use of AI systems more intuitive [101].\nis worse for non-AI experts, as these systems can only be used profitably if expertise in the field of prompt engineering is available [101] 6.\nTo facilitate the accessibility of AI programming assistants for research, business, and private individuals, these systems must be secure, easily accessible, and intuitive to use. Rajamani [72] presents the vision of a futuristic programming environment based on ML models and logical rules that continuously evolve during use, while Sundberg and Holmstr\u00f6m [84] propose the development of AI platforms as a solution. By using such platforms, a bridge can be built between different groups of experts (e.g. technical experts and business experts), overcoming existing technical, linguistic, and interest-related barriers and enabling solutions to complex problems to be reached more quickly and efficiently [84]. In addition, Sundberg and Holmstr\u00f6m [84] show that the democratization of AI and the associated expansion of the user base can lead to new, diversified, and more innovative AI solutions.\nIn addition, the use of AI also gives rise to technical [73, 100], ethical [7, 73], legal [7, 73] and social [7] requirements that must be analyzed and considered in detail. For example, while these models often produce promising results, they also have considerable risks and limitations. A common problem is that models replicate entire solutions or significant parts of them directly from the training data instead of generating new code [76]. This raises questions about the quality and originality of the generated code. Furthermore, even developers' efforts to curate and maintain high-quality datasets do not always guarantee that the output is error-free or optimal [76]. In addition, performance consistency remains a constant challenge [76]. Furthermore, the installation, maintenance, and management of AI systems represent a further obstacle [84]."}, {"title": "3.2. AI code generation of AI models", "content": "An increasingly important application of AI code generators is the automatic creation of ML and AI code by AI systems, making AI accessible to non-AI experts. However, to make AI usable for these non-experts or experts from other domains and to meet the increasing demand for knowledge-intensive processes, the technology for creating such AI models must be easily accessible and secure. Feltus et al. [21] therefore call for AI tools to enter into a symbiotic relationship with the creators and to pursue a user-centric, simple AI generation approach. In particular, this should also promote a balanced interaction between AI-supported innovation and human expertise [73]. The approaches already explained for the synthesis of conventional software apply to systems for the automatic synthesis of AI code, extended by the principles following in this section.\nHowever, the development of AI models is often subject to high entry barriers, which makes accessibility more difficult. Therefore, experts are currently required for development [78]. Another hurdle is the validation of models, which can hardly be guaranteed by experts from other domains and laypersons [98]. As they often only consider the accuracy of the system, problematic or incorrect models could be used [98]. In addition, access restrictions for the development of AI models must be reduced with the help of suitable concepts such as interactive assistance systems to provide the best possible support for these groups [78, 71, 96, 98].\nThe introduction of these systems can ensure the democratization of AI technology while guar-anteeing easy and user-friendly access [84]. This can make AI accessible and affordable not only for large companies but also for small businesses and organizations, academia, and individuals. To enable inexperienced users without coding, security, and AI experience to develop, deploy, operate, and train AI models, these no-/low-code AI platforms must be made accessible to this user group through a combination of AI assistants, automated MLOps, and user-friendly graphical interfaces [84]. This also means that the need for often unavailable experts such as data scientists and ML/AI specialists can be reduced [84].\nYlitalo [100] point out that such AI tools must also be adapted to the target group. They show this using the example of the study \u201cGithub Copilot AI Pair programmer: asset or liability\" [55] which suggests that the GitHub Copilot is a good tool for senior developers, but offers only limited added"}, {"title": "4. Challenges and risks when using AI-based code generation", "content": "With the increasing integration of AI-based code generators into the software development process, it is becoming increasingly important to understand the associated challenges and risks. While high-performance models can have a positive economic impact for the organizations using them [76, 40, 39], inferior models can both reduce productivity and pose significant security risks [39]. In particular, the introduction of programming wizards adapted to end users poses specific challenges that affect professional programmers and amateurs alike [76].\n4.1. Technical challenges\nOn the technical side, there are particular requirements in the areas of reliability, accuracy, bias control, generalization capability, explainability of the models, contextual understanding and safety aspects [73], which in turn can have an impact on the areas of ethics, law and social issues, as the\nexample of incorrect or ethically questionable answers shows. This is in particular due to the training data and the technical properties of LLMs.\nSince collecting training data is a time-consuming and expensive process [39], the datasets used for training are typically collected by crawling billions of lines of source code [11, 56] from publicly available, voluminous, unverified, sometimes incomplete, and not always trustworthy online sources.\nIn addition, it is often unknown how secure or efficient this code is. As a result, the models trained with this data also learn to generate the weaknesses present in the training data, such as not taking into account important edge cases, using outdated and insecure functions and libraries as well as exploiting incomplete, non-executable, insecure, security-critical, faulty or even malware-compromised code [11, 39, 40, 57, 70, 76].\nIn addition, these models can only answer correctly as long as the question is within the scope of their training data, otherwise, the models give unclear or incorrect answers or start to hallucinate [40, 57]. Furthermore, some models include previous questions and answers within a session in the generation of the output. This can cause the quality of the answers to deteriorate over time. In addition to the training data, the LLM technologies themselves also have their weaknesses. For example, the generated answers are based only on statistical abstractions of the training data [10, 57, 92]. These systems can therefore only predict which token is most likely to occur next, and not whether this makes semantic sense [57]. To make matters worse, it is very difficult to detect errors in the training data or the design of the model since the internal process of the LLM/GPT is beyond any quality control [11]. Thus, despite all efforts of the developers to use only high-quality and cleaned data to train these generators, no guarantee can be given for its correctness and consistency [76]. However, improving code quality and security has so far proved difficult, as this requires large amounts of high-quality and, at best, labeled training data [4]. the quality of the answers can deteriorate over time, and complex and non-executable solutions can be output [40].\nThe operation of such extensive AI assistants also requires complex technical knowledge. In addition to DevOps and MLOps, cloud infrastructures are often required to operate modern AI systems. However, the administration and operation of such infrastructures is extremely complex. Ylitalo [100] offer a solution by investigating how generative artificial intelligence can support the management and operation of cloud infrastructure. They rely on an AI-automated \u201cinfrastructure as code\" approach, which can improve service quality and simplify the necessary processes. However, the authors focus on supporting cloud experts in the management of such systems and not on replacing them.\nFor the reasons mentioned above, companies and their developers are still reluctant to make widespread use of this technology [74]. Therefore, future work should focus on further improving the security and accuracy of these tools to ensure safe use [86].\""}, {"title": "4.2. Code quality", "content": "A crucial aspect when evaluating AI code generators, in addition to correct functionality, is the security of the generated code. According to Open Source Security and Risk Analysis (OSSRA), 40% of AI-generated code has security vulnerabilities and is considered unsafe [38, 68, 85]. The performance of AI code generators depends on factors such as prompt, programming language, application domain, and type of vulnerability [68, 69]9. However, it turns out that the use of AI assistance systems only causes 10% more security vulnerabilities than a control group without such systems [75]. In addition, the choice of code generator itself also influences code security [86, 57, 70, 11].\nTo validate code security, Taeb et al. [86] test the CodeBERT, GPT 3.5, and CodeX systems with code compromised by CVE, CWE, NIST, and NVD vulnerabilities, as well as the top 10 OWASP web vulnerabilities, and analyze how well these systems can repair the compromised code and generate bug-free code. The output of the models was then analyzed by static code analyzers such as Clang-Tidy, FlawFinder and VCG. CodeX was found to have the highest code generation capability, producing\naccurate, secure and privacy-compliant code. GPT 3.5 showed lower code generation capabilities compared to CodeX, but was very effective in explaining potential vulnerabilities and commenting on code and log files. CodeBert was also able to generate high-quality code but was weaker in terms of security measures. They also show that the quality and security of the output depends on the complexity of the tasks and the accuracy of the instructions and thus, among other things, on the skills of the operator [86].\nAlthough not directly examined by Taeb et al. [86], the study also suggests that the architecture of the model influences the results. CodeBert is based on BERT's encoder architecture, but has been specifically optimized for processing source code and natural language. This architecture allows CodeBert to extract contextual information from the input text and understand semantic meanings. In contrast, GPT-3.5 and Codex are part of the Transformer architecture, which is designed for sequential text generation. While GPT-3.5 uses a broad and diversified text base, Codex was specifically trained on large-scale programming data. This focus improves Codex's ability to generate high-quality code and answer specific programming queries.\nIn addition to the studies by Perry et al. [70], Sarkar et al. [76] and Klemmer et al. [40] already presented in chapter 4.4, Negri-Ribalta et al. [57], Ylitalo [100] and Improta [39] also analyze the effects of the steadily increasing use of AI assistants on code quality and code security. Based on a systematic literature review, Ylitalo [100] and Negri-Ribalta et al. [57] conclude that current AI code generators have known and easily avoidable vulnerabilities or even malware in their output. Ylitalo [100] also emphasize that some of the code blocks proposed by the AI assistance system \"Github Copilot\" are incomplete or incorrect and also contain serious security vulnerabilities. However, detecting security vulnerabilities in this partially unfinished and non-executable code poses a significant challenge that can only be solved by analyzing complex patterns and distant relationships within the entire generated code [4, 11]. A study by [39] also shows that the code generated by tools such as \"GitHub Copilot\", \"Amazon CodeWhisperer\", \"Salesforce CodeGen\" and \"OpenAI ChatGPT\" often has security issues and insufficient code quality. Insecure encryption methods and other vulnerabilities in the code are particularly problematic as they facilitate potential attacks. It is therefore considered crucial to develop mechanisms that can detect and fix such security gaps at an early stage [39].\nIn addition, Cotroneo et al. [11] point out that the human factor has a significant impact on the quality and security of code generated by code generators. For example, inexperienced users are usually unable to assess the security of AI-generated code [11]. They also note that manual review of AI-generated code cannot be implemented due to the expected increase in its volume 10 [11]. They also point out that the use of static analysis tools is usually not possible when the code is incomplete [11]. Furthermore, the need for manual verification and the potential presence of serious security risks significantly reduces the adoption of this technology [11]."}, {"title": "4.3. Challenges in prompt engineering", "content": "As previously explained, prompt engineering is one of the biggest challenges in the use of AI code generators [76, 40]. Writing effective prompts for AI models is difficult and requires detailed instructions and clear logic [76, 40]. In addition, problems often need to be broken down into sub-problems to be solved by today's AI systems [76]. This means that users have to specify their intentions in detail and at a fine granular level and often refine them afterward to achieve the desired results [76]. This is a key obstacle, especially for inexperienced users who have no programming skills but could be significantly relieved by natural language programming assistants [76]. The interaction interfaces of these systems must therefore be improved in the future in terms of user-friendliness and quality of results [40]."}, {"title": "4.4. Assessing the security and trust of AI code generators based on user studies", "content": "AI code generators support developers in all areas of the software development cycle [70]. It is important that these tools avoid potential security vulnerabilities and generate robust, secure code\n[86]. To assess the advantages and disadvantages of these systems from the user's perspective, we analyse studies by Perry et al. [70], Sarkar et al. [76] and Klemmer et al. [40].\nPerry et al. [70] conducted a laboratory study in which two groups worked on security-related tasks in the areas of encryption, access control, databases, data input and output, data types and web technologies, with one group working with the help of AI code generators and the other without their help. The generated code was then evaluated and interviews were conducted with the participants to determine their trust in the technology. To ensure reproducibility, the course of interaction (queries, interaction behavior, model responses) with the AI system and the subsequent safety assessments were recorded anonymously and can be downloaded at [58]. Analyzing the data showed that participants with AI assistants produced unsafe code even though they thought it was safe. Inexperienced users tend to consider the generated code to be of high quality without scrutinizing it critically. Experienced developers, on the other hand, are better able to assess security and improve code quality by making targeted adjustments to the input, which ultimately increases their productivity. Participants without AI assistants, on the other hand, wrote safer code and had a more realistic assessment of code quality.\nSarkar et al. [76] analyze how LLM-supported programming assistants influence code development based on experience reports from publicly available sources and also found that inexperienced users often tend to blindly trust the results generated by the AI, which poses potential safety risks. In addition, the automation of code generation can lead to less time being invested in testing and understanding.\nKlemmer et al. [40] analyzed 27 interviews and 190 Reddit posts and found that despite safety concerns, the use of AI assistants is increasing in practice. However, the study emphasizes that developers have to subject the code generated by the AI to additional checks, which negates some of the time benefits as this process has been little researched and standardized to date 11. This could lead to a creeping habituation effect in which the generated code is adopted unchecked, which jeopardizes software security in the long term and makes it more difficult to understand and maintain the code [40, 76]. To ensure security, AI generators are required that already have mechanisms for generating secure code.\nDespite security and quality concerns, AI assistants are used by developers almost every day, although more than half fear negative effects on software security [40]. This is particularly because AI code generators promise significant productivity increases [15, 16, 76, 40, 70] and are becoming increasingly crucial for competitiveness [40]. It is estimated that tools such as GitHub Copilot could increase global productivity by 30 % and boost GDP by USD 1.5 trillion by 2030 [15, 16, 40].\nThis transformation will increasingly shift developers' responsibilities to the areas of prompt engi-neering and AI management. However, prompt engineering is a challenge, especially for inexperienced users [40, 76]."}, {"title": "4.5. Social, legal and ethical challenges", "content": "The social, legal, and ethical requirements concern the security and protection of data, the impact on the environment, the transparency and accountability of AI systems as well as the prevention of misuse and the promotion of understandable and legally, ethically and socially acceptable interaction [73].\nBorger et al. [7] analyze in their work the challenges that arise from the use of LLMs such as ChatGPT and Bard in the areas of scientific research and education concerning these three central areas of tension. It must be noted, for example, that these systems can be used immediately by anyone and can in principle also be misused for malicious purposes that were previously inaccessible to the person using them [7]. To protect against misuse, OpenAI, for example, has implemented security measures to prevent malicious use of ChatGPT [7].\nThere are also social implications that can arise from the use of AI-supported systems. Borger et al. [7] show that there are still problems to be solved in the areas of privacy, confidentiality and security. On the other hand, they also show that the use of AI can overcome previous barriers and access restrictions. An example from biology shows the resulting advantages. In the future, biologists"}, {"title": "5. Attack possibilities and protection of AI code generators", "content": "This section examines how LLMs can be attacked, how malicious actors can abuse LLMs for cyberattacks, and what safeguards can be put in place against such abuse. As the prevalence of LLMs continues to grow, understanding the potential threats and safeguards is essential.\n5.1. Options for attacking AI generators\nAs AI-assisted code generation becomes more widespread, the number and variety of attack vectors targeting these systems are also growing. Understanding potential vulnerabilities and taking preventive measures is crucial to ensuring the security and reliability of such models. This chapter presents exemplary attack vectors on LLM-based code generators and their impact on the security of the generated code. This analysis is intended to highlight the importance of proactive defense mechanisms.\nAmong the most significant threats to LLM-based code generators are reverse psychology attacks, prompt injection, and jailbreaks. These techniques make it possible to circumvent the security mechanisms of language models by using specially manipulated inputs to generate unwanted or malicious output [35]. Such manipulations can be subtle and difficult to detect [66]. For example,\nLLM code generators can be abused to exploit existing vulnerabilities [35, 79], to create malware and conduct attacks, or to generate code that contains vulnerabilities [35]. This type of attack emphasizes the need to monitor LLM inputs and outputs carefully.\nFurthermore, these attack vectors enable the undermining of social and ethical constraints as well as data protection mechanisms [35], posing significant risks to both data security and user protection. This also poses the risk of LLM-based code generators unintentionally disclosing sensitive information, as their models usually contain confidential information used during training (so-called model inversion attacks) [23]. Studies show that identifiable (28.2%), private (7.8%), and secret (6.4%) information, such as addresses, medical data, or passwords, can be leaked [57, 63]. In addition, such leaks pose a significant threat to protecting user data, intellectual property, and corporate information, which can compromise infrastructure and financial assets [38].\nIn addition, membership inference attacks pose a further threat. These attacks aim to determine whether certain data points are contained in a model's training dataset, which can have serious consequences, especially when processing sensitive data [80]. Such attacks could reveal confidential code snippets or internal company information that greatly benefits organizations. As LLMs are often trained with large amounts of data from corporate sources or open-source code, there is a risk that trade secrets or internal processes could be inadvertently exposed.\nAnother serious risk to the security and reliability of LLM-based code generators are attacks that specifically target the manipulation of the models, such as poisoning and backdoor attacks. Both types of attacks aim to compromise the functionality of a model by injecting malicious or erroneous data during the training process.\nPoisoning attacks aim to deliberately infect the training set of a model with malicious data to affect the quality of the generated code. This can happen both in the pre-training phase and the fine-tuning phase [39]. Attackers use two main methods here: In white-box attacks, they gain direct access to the training data to manipulate it in a targeted manner [39]. In contrast, black-box attacks, attempt to subvert data collection by web crawlers by injecting erroneous information into the sources used to create the model [39]. Backdoor attacks are a particularly insidious extension of this threat. Here, a kind of 'backdoor' is integrated into the model during the training phase. This backdoor remains hidden during normal use of the model and is only activated by certain specialized inputs. As soon as it is triggered, the model can generate dangerous or malicious code [33]. The challenge in detecting such attacks is that the hidden backdoors are only activated under specific conditions, which makes them extremely difficult to detect. These attacks could compromise LLM-based code generators in such a way that they are later exploited by attackers to generate malicious output [79]. Both poisoning and backdoor attacks pose serious threats to the security and integrity of AI-generated code, as they have the potential to inject malicious elements into the code in subtle and hard-to-recognize ways. Current research shows that LLMs are particularly vulnerable to such attacks as they are made accessible on public interfaces [79].\nAnother significant threat to LLM-based code generators is model extraction. In such attacks, attackers attempt to replicate the underlying model through targeted queries to steal its knowledge and functionality [89]. This also leads to a data leak, as the entire model with all the knowledge it contains is stolen.\nAs a result, the security risks associated with LLM-based code generators are manifold and can have significant consequences for users, organizations, and society. Attacks such as prompt injection,"}, {"title": "5.2. Defense of AI code generators", "content": "The increasing use of AI code generators requires a comprehensive approach to maintaining quality and security. To protect these systems from misuse and the generation of bad code with vulnerabilities, various strategies are required. First, we show how to secure the data generation. Then we look at specific data sets that can be used to fine-tune a secure LLM-based code generator. Then we show how to influence the prompting process to steer models towards generating secure code, followed by a look at the human factor that interacts with the model through these prompts. Next, we look at how to secure the output of the LLM. Finally, we look at other technical security mechanisms.\n5.2.1. Data creation\nAs already explained in section 5.1, there are numerous attack vectors due to the mechanics of data collection, which favor data poisoning. However, this data is essential for training. To rule out the presence of compromised data, AI models should only be trained on verified and high-quality data from trustworthy sources or labeled data [39, 70, 57]. In addition, Negri-Ribalta et al. [57] recommend training the models with both positive and negative labeled examples to teach the system what is good and bad. However, the existence of corresponding labels is often not given. A solution to this problem could be to use the approaches presented in chapter 4.2 for security analysis of the code output by AI code generators to also analyze the training data. In addition, manually checking and labeling this data is a tedious and error-prone activity [14], which is usually impractical.\nOne solution could be to use techniques for automatic labeling or to clean up erroneous data to improve the quality of the models. To this end, the training data used could be analyzed in advance using static analysis tools [39, 70]. Cotroneo et al. [11] mentions various tools and security test suites such as CodeQL [31], Bandit [27], Semgrep [29] and PyT [28] for identifying and evaluating security vulnerabilities in the generated code 12. However, these tools could also be used to examine the training data. In addition, methods should also be developed that draw on knowledge from library documentation13 and \u201cexpert knowledge\u201d to weigh the examples available in the training data accordingly [70].\nAnother solution is offered by Desmond et al. [14]. The authors present a system that offers an interactive, AI-supported data labeling approach. This could be used to add labeled data (e.g. good and bad code examples) to the corpus for the knowledge of LLMs. This system can work both unsupervised and semi-supervised. One possible scenario would be to extract human-labeled data as well as data from sources with known performance values, such as Kaggle competitions14 in the field of AI efficiency or hackathons in the field of code security, and use the results of these competitions as labels. This data could then be used to automatically re-label data from other sources, e.g. repositories, in a semi-supervised way.\nAnother recommended approach to minimize risk is to include different programming languages in the training data and find a balance between the size of the training dataset and the capability of the system [57]."}, {"title": "5.2.2. Optimizing AI Code Generators: The Role of Specialized Data Sets and fine-tuning", "content": "Optimizing and validating the pre-trained base model of an AI code generator requires large, high-quality, and specialized datasets [40", "74": "describes that model fine-tuning involves retraining an existing model with additional training data, for example from the area of security, to influence the quality of the results in this direction. The authors cite the main advantage of this approach as being that users can immediately interact with the model and produce secure code without any further knowledge. However, they see a major disadvantage in that this requires access to the original model. In the case of optimizing code quality and code security, the base model is specifically optimized with tasks from these areas. However, to efficiently test the performance of the model after fine-tuning, these data sets must also be suitable for use in an automated evaluation process. In addition, the creation of domain-specific models requires an explicit knowledge representation that is prepared in a machine-understandable way [21"}]}