{"title": "GRAPH NEURAL NETWORKS Are More Than FILTERS: REVISITING AND BENCHMARKING FROM A SPECTRAL PERSPECTIVE", "authors": ["Yushun Dong", "Patrick Soga", "Yinhan He", "Song Wang", "Jundong Li"], "abstract": "Graph Neural Networks (GNNs) have achieved remarkable success in various graph-based learning tasks. While their performance is often attributed to the powerful neighborhood aggregation mechanism, recent studies suggest that other components such as non-linear layers may also significantly affecting how GNNS process the input graph data in the spectral domain. Such evidence challenges the prevalent opinion that neighborhood aggregation mechanisms dominate the behavioral characteristics of GNNs in the spectral domain. To demystify such a conflict, this paper introduces a comprehensive benchmark to measure and evaluate GNNs' capability in capturing and leveraging the information encoded in different frequency components of the input graph data. Specifically, we first conduct an exploratory study demonstrating that GNNs can flexibly yield outputs with diverse frequency components even when certain frequencies are absent or filtered out from the input graph data. We then formulate a novel research problem of measuring and benchmarking the performance of GNNs from a spectral perspective. To take an initial step towards a comprehensive benchmark, we design an evaluation protocol supported by comprehensive theoretical analysis. Finally, we introduce a comprehensive benchmark on real-world datasets, revealing insights that challenge prevalent opinions from a spectral perspective. We believe that our findings will open new avenues for future advancements in this area.", "sections": [{"title": "1 INTRODUCTION", "content": "Graph Neural Networks (GNNs) have shown remarkable performances in modeling graphs in a plethora of domains, such as social media analysis (Fan et al., 2019; Ying et al., 2018), molecular biology (Wang et al., 2022; Liu et al., 2022; Gasteiger et al., 2021), and cybersecurity (Jin et al., 2020; Zhang & Zitnik, 2020; Tang et al., 2022), to name a few. The huge success of GNNs is generally attributed to its powerful neighborhood aggregation mechanism (Xu et al., 2018; Zhu et al., 2020). Specifically, such a mechanism allows each node to contribute key information to its neighbors across the graph topology (Liu & Zhou, 2022), which enables GNNs to learn informative representations and perform accurate predictions in graph-based learning tasks (Wu et al., 2022a).\nTo gain a deeper understanding of the reason why such a neighborhood aggregation mechanism brings revolutionary performance improvement, recent years have witnessed a huge amount of explorations (Jegelka, 2022; Xu et al., 2018). Currently, a widely acknowledged belief is that neighborhood aggregation mechanism acts as a graph signal filter (Luan et al., 2024), which serves as the dominant module in GNNs (Wang & Zhang, 2022; Bianchi et al., 2021). In most traditional GNNs, such a mechanism acts as a low-passing filter (Chang et al., 2021; Nt & Maehara, 2019) to capture the frequency components encoded with the most task-relevant information in most graph datasets. More recent studies such as (Bo et al., 2021; Bianchi et al., 2021; Luan et al., 2022), have noticed that the most task-relevant information is not necessarily encoded in the lowest frequencies, e.g., in heterophilous graphs (Zheng et al., 2022; Zhu et al., 2021). To facilitate more capable GNNs to han-"}, {"title": "2 PRELIMINARIES", "content": "Notations. Throughout our work, without further specification, italic letters (e.g., X), bold uppercase letters (e.g., X), bold lowercase letters (e.g., x), and ordinary lowercase letters (e.g., x) represent matrices, vectors, and scalars, respectively. For any matrix, e.g., X, we employ $X_i$ and $X^i$ to indicate its i-th row and column, respectively. We denote an undirected graph as $G = (V, \\mathcal{E})$, where $V = \\{1, ..., n\\}$ and $\\mathcal{E}$ are the set of nodes and edges. We denote $A \\in \\mathbb{R}^{N \\times N}$ as the graph adjacency matrix in which $A_{i,j} = 1$ if there exists an edge between node i and node j, otherwise $A_{i,j} = 0$. With graph adjacency matrix A, the graph node degree matrix can be defined as $D = diag(d_1, ..., d_v)$, where $d_i = \\sum_j A_{i,j}$. The normalized graph Laplacian matrix is defined as $L = I - D^{-1/2}AD^{-1}$. Additionally, we define the graph node feature matrix $X \\in \\mathbb{R}^{N \\times F}$, where $X^j$ represents the j-th feature channel and F denotes the number of feature channels. We employ the $\\mathrm{sign}$ as the Hadamard multiplication.\nCurrent Progress of Gnns & Concerns From a Spectral Perspective. There are two mainstream lines of research on GNNs, i.e., spatial- and spectral-based ones. Researchers examining the spatial perspective consider the aggregation process of GNNs as a node attribute aggregator across the graph topology (Kipf & Welling, 2017; Wu et al., 2020; Xu et al., 2018). In contrast, those exploring GNNs from a spectral perspective consider the aggregation process as a filter in the spectral domain, i.e., the eigenspace of the graph Laplacian matrix L, largely considering GNNs as low-passing filters (Nt & Maehara, 2019; Chang et al., 2021; Yu & Qin, 2020). Recently, diverse designs of filters associated with different neighborhood aggregation mechanisms have been proposed to help GNNs capture the key information encoded in different frequency components (Bo et al., 2021; Guo et al., 2023; Dong et al., 2021). In general, these explorations lay a solid mathematical foundation for GNN with spectral-based methods. However, most current methods focus on the filters associated with the neighborhood aggregation mechanisms rather than considering a GNN as a whole. As such, the role of other modules such as non-linear layers in GNNs has been long neglected. In recent years, several works (Balcilar et al., 2021b; Yang et al., 2024) have provided primary evidence that the non-linear layers can effectively shift the behavioral characteristics of GNNs. To further bridge the aforementioned gap, we now formally introduce the problem of Measuring and Benchmarking the Performance of GNNs From A Spectral Perspective below."}, {"title": "Problem 2.1. Measuring and Benchmarking the Performance of GNNs From A Spectral Perspective", "content": "Our goal is to qualitatively understand and quantitatively compare the capabilities of GNNs in capturing the key information encoded in different frequency components of input graphs to perform graph learning tasks."}, {"title": "3 AN EXPLORATORY STUDY", "content": "To properly handle Problem 2.1, the prevalent strategy is to simply analyze the frequency response function associated with the filter resulted from the neighborhood aggregation mechanism (Nt & Maehara, 2019; Wu et al., 2019). However, such a straightforward approach may not be able to handle Problem 2.1, since the behavioral characteristics of GNNs in the spectral domain may not be fully dominated by such filters. Below, we show our preliminary explorations to further clarify such a common misunderstanding.\nResearch Question. Here, we perform preliminary studies to explore whether the behavioral characteristics of GNNs in the spectral domain are dominated by the filters associated with the neighborhood aggregation mechanism (as discussed in Section 1). Specifically, we are particularly interested in revealing whether GNNs can flexibly yield outputs with abundant frequency components that have been significantly weakened or filtered out by the non-learnable filter.\nEvaluation Protocol. In this study, we measure the influence of different frequency components using the commonly adopted notion of Energy (Yang et al., 2022; Tang et al., 2022). We construct our experimental datasets based on real-world graph datasets. First, we compute the normalized Laplacian eigenvectors of the graph and sort them by eigenvalue (i.e., frequency). We then bin these eigenvectors into even-width bins, where each bin is associated with the mean of the eigenvectors"}, {"title": "4 BENCHMARK DESIGN", "content": "In this section, we introduce the benchmark design. Specifically, we first introduce the evaluation protocol, followed by comprehensive analysis to lay a solid theoretical foundation which will directly support the practical significance of our proposed benchmark from a spectral perspective."}, {"title": "4.1 EXPERIMENTAL PROTOCOL", "content": "Downstream Task & Dataset Preparations. While we used node regression for our preliminary study in Section 3, we adopt node classification for our extensive empirical benchmark considering its superior practical significance in graph learning tasks. Specifically, we propose to adopt the same approach to generate continuous values corresponding to each node in the input graph (as in Section 3) followed by an additional discretization process by giving thresholds to determine the ground truth labels for each node. We show in Section 4.2 that the additional discretization process only brings an upper-bounded energy distribution deviation compared with the continuous ground truth values in the node-level regression task, which ensures satisfying consistency. Meanwhile, such an approach ensures that the targets to predict possess the frequency-specific incentives needed to evaluate performance on each bin in the frequency domain. We propose to adopt real-world datasets such that we will perform evaluations on the node attributes and graph topology that bear practical significance across different domains. We further provide more details in Appendix A.\nGNNs for Benchmarking. We conduct evaluation on a total number of 14 GNNs, namely SAGE (Hamilton et al., 2017), GCN (Kipf & Welling, 2017), GCNII (Ming Chen et al., 2020), GAT (Veli\u010dkovi\u0107 et al., 2018), GATv2 (Brody et al., 2022), SGC (Wu et al., 2019), FA (Bo et al., 2021), GIN (Xu et al., 2018), ChebNet (Defferrard et al., 2016), GatedGraph (Li et al., 2016), the Transformer (Vaswani et al., 2017), GPS (Ramp\u00e1\u0161ek et al., 2022), APPNP (Gasteiger et al., 2019), and the 1-WL operator from Morris et al. (2019) (denoted as 1-GNN). These models cover a wide range of popular and state-of-the-art GNNs designed in either spatial or spectral domain.\nReal-World Datasets. We benchmark GNNs on the full versions of the Cora and DBLP citation graphs (Bojchevski & G\u00fcnnemann, 2018), the CS and Physics coauthor datasets by Shchur et al. (2018), and the Amazon-Computers and Amazon-Photo product graphs by (Shchur et al., 2018). These datasets vary in size, structure, spectral energy distribution, and semantic domains, allowing us to evaluate the GNNs' performance across diverse graph types and application areas.\nBenchmark Evaluation Metrics. Following most other works, we adopt node classification accuracy as the primary metric to measure the performance of GNNs. On the basis of this, we now introduce the qualitative and quantitative performance evaluation methods. From a qualitative perspective, we propose the notion of Accuracy Curve in the Spectral Domain. Specifically, as introduced in Section 3, each round of experiments is associated with a bin on the spectral axis, based on which the node-level prediction target (a discrete label for node classification) is generated. When the input graph signal contains all frequency components at the same energy level, the performance under this bin generally reflects the capability of such a GNN model in capturing and leveraging the information encoded in the associated frequency component to perform prediction. Accordingly, the performance across all available bins on the spectral axis form a curve, which generally reflects the tendency of how such capability changes w.r.t. the frequency value. From a quantitative perspective, we propose to utilize the Normalized Area Under the Accuracy Curve, Normalized AUAC, to measure the general capability of each GNN model in capturing and leveraging the information encoded in different frequency components. Specifically, it is calculated as the division between the AUAC under the full frequency range and the largest possible AUAC under the full frequency range. Additionally, we are also interested in the capability of GNNs in capturing the information encoded in a certain range of frequency components. In this case, Normalized AUAC can also be adopted by specifying a particular range of frequencies.\nImplementation Details. All experiments were conducted using PyTorch (Paszke et al., 2017) and PyTorch Geometric (Fey & Lenssen, 2019) libraries. We used 2-layer GNNs with a hidden dimension of 64 for all runs. GNNs were trained for 500 epochs using the Adam optimizer with"}, {"title": "4.2 THEORETICAL ANALYSIS", "content": "We adopt node classification task for our main benchmark considering that it typically fosters a stronger practical significance. To ensure the consistency between node-level regression task and node classification tasks, in this section, we aim to reveal that the additional discretization process in node classification does not bring significant deviation from the ground truth's energy distribution in the spectral domain. Below we present the theoretical analysis revealing such insights.\nWe refer to the matrix of one-hot vectors representing the ground truth node class labels after discritization as the Node Class Label (NCL) matrix for convenience. Below we first define the Energy Distribution Field (EDF) of a node class label distribution.\nDefinition 4.1. (Energy Distribution Field) The energy distribution field, denoted as $F_M$, of an NCL matrix $M \\in \\mathbb{R}^{n \\times k}$ is the set of energy distributions of the unit vectors whose corresponding NCL matrix is M. In other words, $F_M := \\{e(v) \\in \\mathbb{R}^{n \\times 1} | ||v||_2 = 1, \\tau(v) = M \\}$, where $e(\\cdot)$ is the energy distribution function mapping a vector to its energy distribution in the graph spectrum field, $\\tau(\\cdot)$ is the function mapping unit vectors to their NCL matrices.\nWith the concept of EDF, we then formulate the \"closeness\" of the energy distribution between the eigenvector $v$ of the graph Laplacian and its corresponding NCL matrix $\\tau(v)$ as $\\max_{e_u \\in F_{\\tau(v)}} ||e_u - e(v)||_2$. We take two steps to verify that the optimal value of the maximization problem is small enough: (i) We prove that the energy distribution function is Lipschitz, which indicates that the energy distribution function is a \u201csmooth\u201d function where quantifying its function value variations is equivalent to quantifying its variable variations; (ii) The inverse image of the energy distribution function $e(\\cdot)$ on any energy distribution field $F_M$ is small enough.\nFor the first step, since the energy distribution function is essentially a linear function followed by a vector normalization procedure, we can write the energy distribution function as $e(v) = \\frac{(Av)(Av)}{||Av||_2}$, with A being an orthonormal matrix. Actually, the energy distribution function is Lipschitz:\nTheorem 4.2. The energy distribution function $e(v) = \\frac{(Av)(Av)}{||Av||_2}$, with A being orthonormal, is Lipschitz on the unit sphere.\nFor the second step, we prove the inverse image of the energy distribution function $e(\\cdot)$ on any EDF $F_M$ is \u201csmall enough\u201d. Intuitively, we can calculate the area of $e^{-1}(F_M)$ on the unit sphere and prove that the area is \u201csmall enough\". However, since the calculation of the area is rather complex for a high-dimensional sphere, we instead prove that the variation of center angle in $e^{-1}(F_M)$ is small enough. Specifically, we have the following theorem:"}, {"title": "5 EMPIRICAL INVESTIGATION", "content": "5.1 RESEARCH QUESTIONS\nWe are interested in answering four research questions (RQs) below. RQ1: What are the trends of GNNs' performance in capturing the information encoded in different frequency components across the frequency domain? RQ2: How do various GNNs compare in their ability to capture information across different frequency components, both holistically and in specific ranges? RQ3: How will the benchmark show its practical implications to guide a practitioner's choice of GNN? RQ4: How does the depth of GNNs affect the proposed accuracy curves in the spectral domain?"}, {"title": "5.2 QUALITATIVE PERFORMANCE COMPARISON IN THE SPECTRAL DOMAIN", "content": "We first answer RQ1 by analyzing the general trends that GNNs show in our benchmark, and we present the general tendency of the accuracy curves in the spectral domain for different GNNs. We note that only partial results are presented here due to space limit, and see Appendix D for complete results. We have the following observations.\nFirst, from the perspective of a general tendency, we observe that GNNs show V-shaped curves in all cases. Such a phenomenon indicates that GNNs typically have stronger capability in capturing the information encoded in the lowest and highest frequency components (e.g., those associate with the smallest and largest frequency values) compared with the middle frequency components. Specifically, when the task-relevant information is encoded in the low frequency components, the neighborhood aggregation can directly benefit the prediction of each node. This is because nodes with similar labels tend to connect with each other, which can significantly facilitate predictive performance. On the other hand, when the task-relevant information is encoded in the high frequency components, the neighbors of each node can also contribute to its own prediction if the GNNs are able to learn not to predict the same label for connected nodes. However, in contrast to the prevalent opinion, the capability of capturing the task-relevant information encoded in the middle frequency components is the most difficult for all adopted GNNs. A key reason for this phenomenon is that in this case, the label distribution of each node's neighborhood is generally uniform. Therefore, it becomes very difficult for GNNs to learn a general criterion to predict the label of a node based on its neighbors. Based on the discussion above, we argue that the GNNs' weakness in capturing the task-relevant information encoded in the middle frequency components reveals an inherent limitation of neighborhood aggregation mechanism."}, {"title": "5.3 QUANTITATIVE PERFORMANCE COMPARISON IN THE SPECTRAL DOMAIN", "content": "We now answer RQ2 by analyzing the quantitative results of Normalized AUAC score in percentage for 14 GNN models across six real-world datasets. According to Section 4, the quantitative value of the Normalized AUAC score reflects the general capability of GNNs to capture the task-relevant information encoded in different frequency ranges. In addition, we are also interested in analyzing this capability on specific ranges of frequencies. Therefore, we split the range of frequencies evenly into three sections, namely the low frequency frange (the components with a frequency value ranks at the bottom one third), the middle frequency range (the components with a frequency value ranks at the middle one third), and the high frequency range (the components with a frequency value ranks at the top one third). We show the average rankings of GNNs by measuring their Normalized AUAC score in these three different frequency ranges. Notably, the proposed benchmark is able to analyze the performance of both spatial- and spectral-based GNNs from a consistent spectral view. Therefore, we are able to involve both types of GNNs into the comparison at the same time. We have the observations below.\nComparing models focusing on the general spectral domain, we observe that the performance measured by Normalized AUAC score gives a ranking that challenges the traditional assumptions on the relative superiority of these GNNs. Specifically, SAGE, GCNII, and GATv2 are the GNNs showing the best performance in terms of the average ranking across all datasets. In fact, these GNNs are rarely discussed and evaluated from a spectral perspective. The primary reason is that they are mostly designed in the spatial domain and most do not have a well-defined frequency response function. We argue that the advantages of these GNNs in the spectral domain are largely neglected by the current spectral-based studies (Nt & Maehara, 2019; Wu et al., 2019) due to their reliance on an explicit analytical form of frequency response function.\nComparing models focusing on the three specific sections of frequency components, we observe that different GNNs specialize in extracting task-relevant information from different ranges of frequencies. Specifically, GNNs specialize in learning from low (e.g., SAGE and Cheb) and high (e.g., GPS and Transformer) frequency components typically do not show strong superiority over other GNNs in learning from middle frequency components. Instead, GNNs such as GATv2 and GCN achieve clear superiority over other GNNs in learning from middle frequency components, which is often ignored by other studies relying on frequency response analysis."}, {"title": "5.4 CASE STUDY", "content": "In this subsection, we conduct a case study to explore RQ3. Specifically, we propose to simulate the real-world problem of needing to choose from a variety of different GNNs, and we then analyze how the findings from our benchmark can help practitioners. Specifically, we adopt six new real-world datasets, namely Airport-Brazil (AB), Wisconsin (Wisc.), Cornell (Corn.), Squirrel (Squi.), and Chameleon (Cham.). Most of these dataests are reported to be difficult for node classification task with GNNs. We first perform node classification with all 14 GNNs based on these datasets and then derive the ranking of all GNNs as $r_1$. Here $r_1$ serves as the actual ranking we seek to know to pick the best GNNs to use. However, in practice, we may not always be able to perform experiments prior to making a choice. Therefore, we propose to analyze how well other rankings can approximate such an actual ranking. We first analyze the node labels in the training node set to identify the frequency component range (low, mid, or high) where most energy falls in. According to this range, we collect the associated performance ranking $r_2$ from Section 5.3 as the ranking derived from our benchmark. As a comparison, we also collect random rankings $r_3$ and the node classification rankings $r_4$ directly derived from the node classification task on the chosen six datasets.\nWe show the average Kendall-Tau (KT) distance $\\tau$ (total number of inversions for any two positions i and j where i > j) between the actual ranking $r_1$ and the benchmark ranking $r_2$. Here, a smaller KT distance indicates larger similarity between the two rankings. We found that the KT distance between $r_1$ and $r_2$ (Benchmark Ranking $\\tau$) is significantly smaller than that between $r_1$ and $r_3$ (Random Ranking $\\tau$) or $r_4$ (Original Task Ranking $\\tau$) in most cases, which indicates a satisfying approximation of $r_1$ with $r_2$. This reveals the practical significance of the benchmark in understanding the superiority across different GNNs prior to any experiments."}, {"title": "5.5 PARAMETER STUDY", "content": "We finally answer RQ4 by changing the depth of each GNN to explore how the results of the proposed evaluation protocol will change. We note that stacking multiple filters (by adding more iterations of neighborhood aggregation) significantly affects the frequency response. Therefore, existing studies that analyze the performance of GNNs by relying on their aggregators' frequency response functions typically have significantly different conclusions when the layer number of GNNs changes. Specifically, we range the number of GNN layers from two to four, and we present an example of the accuracy curves in the spectral domain across two-, three-, and four-layer cases."}, {"title": "6 RELATED WORKS", "content": "Spectral Graph Neural Network Analysis. Compared to spatial GNNs (Hamilton et al., 2017), spectral GNNs typically offer a sound theoretical basis (Balcilar et al., 2021a; Bo et al., 2023). Recent studies have shown that the neighborhood aggregation mechanism in GNNs typically acts as a low-pass graph signal filter (Chang et al., 2021; Nt & Maehara, 2019), capturing the lowest frequency components that are usually relevant for various graph learning tasks. However, while more flexible filters have been proposed to better capture task-relevant information encoded in different frequency components (Li et al., 2018; Guo et al., 2023; Bianchi et al., 2021), existing research typically overlook the behavioral characteristics of GNNs in the spectral domain as a whole. As such, the roles of other modules, such as the non-linear layers, are often ignored. Recent works (Balcilar et al., 2021b; Yang et al., 2024) have show evidence that these modules other than neighborhood aggregation can significantly alter the frequency components of the GNN's output, This underscores the need for a comprehensive analysis that considers GNNs in their entirety instead of only focusing on their neighborhood aggregation mechanisms. Different from existing works, we propose to consider GNNs as a whole and conduct benchmarking form a spectral perspective.\nGraph Neural Network Benchmarking. Research GNN benchmarks focus on evaluating the performance of GNNs in terms of accuracy (Wu et al., 2022b; Zheng et al., 2021), scalability (Duan et al., 2022), and efficiency (Baruah et al., 2021). For example, various benchmarks have been proposed to evaluate the performance of GNNs in various tasks (Dwivedi et al., 2023), such as recommendations (Wu et al., 2022a) and molecular property predictions (Fung et al., 2021). In terms of the scalability of GNNs, different benchmarks are also proposed, such as LRGB (Dwivedi et al., 2022) for long-range dependency and LS-Bench for training on large-scale graphs. The efficiency of GNNs is also studied in various benchmarking works (Gong & Kumar, 2024). Despite existing efforts of evaluating GNNs, most benchmarks emphasize node classification accuracy, scalability, and computational efficiency without delving into the behavioral characteristics of GNNs in the spectral domain. This hinders a spectral understanding of the strengths and weaknesses of popular GNNs. Different from these benchmarks, we have proposed a comprehensive benchmark to evaluate both spatial and spectral GNNs from a consistent spectral view. To the best of our knowledge, it is a first-of-its-kind work that reveals insights challenging prevalent opinions from a spectral perspective."}, {"title": "7 CONCLUSION", "content": "This paper presents a comprehensive benchmark for evaluating GNNs from a spectral perspective, challenging common opinions about how GNNs process graph data. Through exploratory studies and rigorous experiments on real-world datasets, we demonstrate that GNNs can flexibly generate outputs with diverse frequency components, even when certain frequencies are absent in the input. This finding contradicts the prevailing opinion that neighborhood aggregation mechanisms dominate GNN behavior in the spectral domain. Our novel evaluation protocol, supported by theoretical analysis, provides a fundamental framework to measure the capability of GNNs to capture and leverage information across different frequency components. These findings open new avenues for understanding and improving GNN architectures, emphasizing the need to analyze GNNs in their entirety rather than focusing solely on their aggregation mechanisms. Our work lays the foundation"}, {"title": "A REPRODUCIBILITY", "content": "All experiments were conducted using Python with the PyTorch (Paszke et al., 2017) and PyTorch Geometric (Fey & Lenssen, 2019) libraries. We detail the exact experimental settings for the preliminary study and main benchmark below. We also list all of our experimental hyperparameters in Table 2."}, {"title": "A.1 PRELIMINARY STUDY", "content": "The datasets for the preliminary study are constructed by first taking each graph and computing its eigenvectors based on the normalized graph Laplacian. We then sort the eigenvectors by eigenvalue in ascending order and bin them into 20 uniform-width bins. Each bin contains the mean of the eigenvectors whose corresponding eigenvalues falling into that bin. We then split the bins into 3 uniform intervals corresponding to low, medium, and high frequencies.\nWe then task a randomly initialized GCN and FA with predicting the mean of these low, medium, and high frequency eigenvectors using the mean-squared error (MSE) loss using the eigenvectors of the complementary ranges as features. For each setting (e.g. using low frequencies as features and high frequencies as targets), we split our constructed dataset using an 80%/20%/20% train/validation/test split under a transductive node regression setting where we randomly generate the train/validation/test masks. During training, we standardize the eigenvector targets, subtracting their mean and dividing by their variance, which is a common practice in machine learning to facilitate learning during gradient descent (Hastie et al., 2009). During evaluation, we restore the outputs of the GNN back to the original scale."}, {"title": "A.2 MAIN BENCHMARK", "content": "For the main benchmark, we construct each dataset similar to the preliminary study by collecting the normalized Laplacian eigenvectors of each graph followed by binning. Next, as this benchmark is a node classificaiton task, each entry of each binned eigenvector is assigned a class corresponding to that which bin it falls into between -1 and 1. For example, if the number of provided classes is 5, and a given binned eigenvector's entry is 0.8, then that entry will be assigned a class label of 4, the final class label.\nNext, we conduct experiments where, for every bin b of eigenvectors, we train each GNN to predict the classes of each entry of b using the features from the original dataset, e.g. BOW text features for Cora-Full. We train our models using the cross-entropy loss and use a random 60%/20%/20% train/validation/test split under a transductive node classification setting. We detail our hyperparameter settings for our benchmark."}, {"title": "B PROOFS", "content": "In order to support the solidity of our design of the graph node class labels matrix (NCL matrix), we claim that the energy distribution of an eigenvector of the graph Laplacian matrix is close to that of its corresponding NCL matrix. To verify this claim, we first define the energy distribution field of a node class label distribution."}, {"title": "Definition B.1. (Energy Distribution Field)", "content": "The energy distribution field, denoted as $F_M$, of an NCL matrix $M\\in \\mathbb{R}^{n \\times k}$ is the set of energy distributions of the unit vectors whose corresponding NCL matrix is M. In other words, $F_M := \\{e(v) \\in \\mathbb{R}^{n \\times 1} | ||v||_2 = 1, \\tau(v) = M \\}$, where $e(\\cdot)$ is the energy distribution function mapping a vector to its energy distribution in the graph spectrum field, $\\tau(\\cdot)$ is the function mapping unit vectors to their NCL matrices.\nWith the concept of an energy distribution field (EDF), we formulate the \"closeness\" between the energy distribution between the eigenvector $v$ of graph Laplacian and its corresponding NCL matrix $\\tau(v)$ as $\\max_{e_u \\in F_{\\tau(v)}} ||e_u - e(v)||_2$. We take two steps to validate that the optimal value of the maximization problem is \u201csmall enough\u201d: (i) We prove that the energy distribution function is Lipschitz, which indicates that it is a \u201csmooth\u201d function where quantifying its function value variations is equivalent to quantifying its variable variations; (ii) We prove that the inverse image of the energy distribution function $e(\\cdot)$ on any energy distribution field $F_M$ is \u201csmall enough\u201d."}, {"title": "C COMPLEMENTARY RESULTS OF EXPLORATORY STUDY", "content": "Figures 7, 8, 9, 10, 11 illustrates a comparison between the energy distributions of input and output frequency components of FA and GCN on the CS, Computers, Cora Full, Photo and Physics datasets. The plots depict the energy across 20 frequency bins, highlighting the distribution of input signals (orange), target signals (green), and output signals (blue).\nWe observe that FA and GCN performs similarly across all the examined datasets. Specifically, in each of those figures, the first subplot (top left) shows the FA model receiving an input concen-"}, {"title": "D COMPLEMENTARY RESULTS FOR QUALITATIVE PERFORMANCE", "content": "We show the complete version of Fig. 2 with more GNNs and datasets involved in Fig. 12 and Fig. 13. We have the observations as follows, which are consistent with the conclusion we draw from the Section 5.2. Generally, we observe that the V-shape curves are maintained across all GNNs in almost all datasets. The phenomenon indicates GNNs' strong ability in capturing the information encoded in the lowest and highest frequency components (e.g., those associate with the smallest and largest frequency values). We have provided a detailed rationale in Section 5.2. Note that we also detected a few cases such as 1-GNN on Photo and FA on DBLP which we consider to be outliers as their curves may be noisy in terms of accuracy variance, which hinders their frequency-capturing ability in the high frequency range. As expected, each GNN finds the most difficulty in recovering task-relevant information in the middle frequency components of each dataset."}, {"title": "E COMPLIMENTARY RESULTS FOR QUANTITATIVE PERFORMANCE COMPARISON IN THE SPECTRAL DOMAIN", "content": "To more accurately study the performance of each GNN under our benchmark, we report the average GNN performance in capturing task-related information on different spectrum component areas. Specifically, Tables 3, 4, and 5 report average GNN performance in low, middle, high spectrum component areas. All metrics are multiplied by 100 for readability. We also bolden the highest metrics and underline the second-best metrics. Ties are broken by lower standard error.\nIn the low-frequency range, nearly all models perform well, but GatedGraph, GATv2, and GIN achieve particularly high accuracy, reflecting their strong low-pass filtering capabilities. For the mid-frequency range, ChebNet and GCNII demonstrate superior performance, while models like GATv2 and Transformer also perform consistently across multiple datasets. However, surprisingly, models such as FA and APPNP begin to show weaknesses in this region, with their performance dropping notably. In the high-frequency domain, GCNII once again leads the pack in Photo and Physics as well as the 1-WL graph operator while models such as GAT and FA struggle to maintain"}, {"title": "F COMPLEMENTARY RESULTS FOR PARAMETER STUDY", "content": "To demonstrate the robustness of our results, we run experiments under our benchmark settings using wider and deeper GNNs. Specifically, we re-run our benchmark using twice as many hidden dimensions (128 versus 64) and varying layer depths (up to 4) in order to test whether these hyperparameters can significantly affect each GNN's frequency adaptation abilities. In Figure 14, we present parameter studies across co-author datasets. In all of our ablations, we observe no significant shift in overall frequency adaptation behavior from the original plots in 2, suggesting that layer depth and width are not enough to improve or worsen GNN frequency adaptation capabilities."}]}