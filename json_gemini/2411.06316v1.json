{"title": "Prompts Matter: Comparing ML/GAI Approaches for Generating Inductive Qualitative Coding Results", "authors": ["John Chen", "Alexandros Lotsos", "Lexie Zhao", "Grace Wang", "Uri Wilensky", "Bruce Sherin", "Michael Horn"], "abstract": "Inductive qualitative methods have been a mainstay of education research for decades, yet it takes much time and effort to conduct rigorously. Recent advances in artificial intelligence, particularly with generative AI (GAI), have led to initial success in generating inductive coding results. Like human coders, GAI tools rely on instructions to work, and how to instruct it may matter. To understand how ML/GAI approaches could contribute to qualitative coding processes, this study applied two known and two theory-informed novel approaches to an online community dataset and evaluated the resulting coding results. Our findings show significant discrepancies between ML/GAI approaches and demonstrate the advantage of our approaches, which introduce human coding processes into GAI prompts.", "sections": [{"title": "Introduction", "content": "Inductive qualitative coding, the process of identifying and generating potential themes or codes from data, has been widely adopted by many fields, including education, to create new theoretical insights that help us understand the nuances of human interactions. However, as qualitative researchers know, inductive coding can be highly time-consuming (Kalman, 2019), and it is challenging to meet theoretical expectations. While prior studies have leveraged machine learning (ML) approaches to support qualitative analysis, fewer have touched on inductive coding. Recently, researchers have successfully used Generative AI (GAI) to generate inductive coding results. GAI requires instructions just like humans, with researchers noting the strong impact of instructions on GAI outcomes. However, early GAI studies on inductive coding mostly worked with small individual pieces of data and/or codebooks, making it difficult to compare them effectively.\nWe present an exploratory study that systematically and automatically applies two known ML/GAI approaches to generate inductive codes for an online community dataset. Building on them, we proposed two novel, theory-informed approaches that identified more codes and nuanced insights from the same dataset. We evaluated four generation results regarding size, groundedness, overly broad, and overlapping concepts. Our findings show significant discrepancies between different ML/GAI approaches, suggesting the advantage of introducing human coding processes into GAI prompts. The large number and prevalence of overlapping codes suggest the urgency of our follow-up work, a human-Al approach for measuring inductive qualitative coding results."}, {"title": "Background", "content": "Qualitative analysis theorists from both thematic analysis and grounded theory suggest inductive coding as a key approach to discovering and constructing concepts from a dataset (J. M. Corbin & Strauss, 1990; Fereday & Muir-Cochrane, 2006). Yet, as an open-ended process, inductive coding is suspect to rigor issues (Roberts et al., 2019) and can be very time-consuming (Deterding & Waters, 2021). To do it rigorously, grounded theory asks for line-by-line or incident-by-incident coding, looking for all possible interpretations while grounded in the dataset (Willig & Rogers, 2017). However, the expectations could be challenging to match. Thematic analysis is more flexible but also asks for groundedness, i.e., whether the codes and themes are derived from and closely match the data (Braun & Clarke, 2012).\nResearchers have incorporated various ML/Al methods to help alleviate some of the issues with inductive coding. For example, topic modeling (Wallach, 2006), an unsupervised ML technique to identify groups of semantically similar words (i.e., topics), has been used to analyze qualitative survey data (Baumer et al., 2017), social media posts, and online discussion board data (Saravani et al., 2023), etc. The topics identified were used to focus the researcher's attention on important parts of the dataset for deeper analysis and to automatically code the dataset in preparation for future rounds of analysis. However, as an unsupervised technique, the output of topic modeling can be difficult for researchers to interpret, measure, or evaluate despite previous work, making the method inaccessible to many researchers (e.g., Grootendorst, 2022; Sievert & Shirley, 2014).\nRecently, researchers have found some success in using GAI for inductive coding. To use LLMs for inductive coding, researchers supply a piece of data with relevant instructions (e.g., research questions, coding instructions, desired output format), and the LLM would respond accordingly. Repeating this process, researchers could consistently produce codes that humans found more interpretable and useful (e.g., DePaoli, 2023; Sinha et al., 2024). However, LLMs could still lack nuance and fail to grasp less linguistically straightforward themes (DePaoli, 2023; Hamilton et al., 2023). LLMs could produce results not grounded in the data (Byun et al., 2023; De Paoli, 2023), or remain on a coarser grain size of analysis than desired (Sinha et al., 2024; Zambrano et al., 2023). Studies highlight that careful prompt design and thoughtful prompting strategies are crucial to addressing these limitations when using LLMs for inductive coding (Byun et al., 2024; Sinha et al., 2024). As such, we pose the following research questions:\n\u2022\tWhat codes can we discover from text-based datasets using existing ML/GAI methods?\n\u2022\tHow can we do so more effectively?"}, {"title": "Context and Method", "content": "We developed a system that automatically and systematically runs two known and two novel approaches on text-based datasets for inductive coding. Though we tested on many LLMs, due to page constraints, we only report the results of GPT-40, one of the most powerful models in 2024."}, {"title": "Study Results", "content": "To understand how LLMs could identify emergent insights from unstructured datasets, we used a conversational dataset from Physics Lab's online message groups. The original research question was to understand how Physics Lab's online community emerges, which is an open-ended question suitable for an inductive qualitative analysis. The example dataset covers 127 messages between designers and teachers over the first two months of the community. We separated the conversation flow into chunks for human and machine coders with signal processing techniques (Appendix 1). Four coders independently open-coded the dataset before LLMs. Two of them have worked with the community for years, bringing in-depth knowledge about the conversations; the other two have never worked with the community, bringing fresh perspectives with little preconceptions.\nWe assigned the same roles and tasks to machine coders, such as \"You are an expert in thematic analysis with grounded theory, working on open coding.\" We also provide the research question and dataset information to human and machine coders (See Appendix 2). Then, we instruct each machine coder to follow one approach, such as identifying labels from a list of quotes (as in Topic Modeling). To ensure the process was truly inductive, machine coders received no codebooks or examples beforehand. Due to differences in datasets/research contexts, replicating ML/GAI methods from prior studies is challenging. In Appendix 3, we provide the original LLM prompts and our replication side-by-side to help with transparency.\nTwo authors counted the Al-generated codes and flagged noticeable findings while reading each codebook independently. They independently marked potential groundedness issues (i.e., when humans can not reasonably connect the label and the data) and overly broad codes (i.e., when the code provides little information to the researchers) and reconciled their differences. To avoid selection bias, we provide the four codebooks and examples in Appendix 4."}, {"title": "Topic Modeling", "content": "While earlier studies manually interpret the topics (Baumer et al., 2017), a more recent method, BERTopic, uses LLMs to explain the topic modeling results for better human interpretability. Since BERTopic's prompt intends for general-purpose label-making, we adapted and optimized it (Appendix 3.1), similar to a recently reported method (Katz et al., 2024). Yet, we found the approach only capable of finding content topics and surface-level interpretations.\nThis method resulted in 23 codes (Appendix 4.1), most related to content topics from the dataset, e.g., \"software update process\" or \"user guidance and instructions.\u201d We marked two codes as problematic in groundedness. For both, the clustering algorithm identified too many examples (e.g., one has 34 examples, 26% of the dataset) for LLMs to find an appropriate label. Occasionally, BERTopic made superficial-level interpretations, e.g., the \"informal interaction\u201d code included cases of \u201cHello,\u201d \u201cSorry,\u201d or \u201cYes, yes.\u201d Some labels overlap conceptually, e.g., the \"software update process\" and \"software updates and downloads.\""}, {"title": "Chunk-level LLM Coding", "content": "For the next approach, we replicated a well-documented example from AIED 2024 (Barany et al., 2024), which asked LLMs to provide codebooks for each chunk of an interview. Many more recent studies followed"}, {"title": "Item-level LLM Coding", "content": "Inspired by grounded theorists who suggest line-by-line coding (Gibbs, 2007) and a recent study that asked GPT-4 to find a single code for each item (Sinha et al., 2024), we propose a novel approach that instructs LLMs to write a plan before coding, then identify multiple codes for each item (prompt in Appendix 3.3). This approach seems more capable of identifying nuances, although overly broad and overlapping codes persist.\nOur item-level approach identifies 240 codes, almost six times as many as the chunk-level approach (Appendix 4.3), while also more easily identifying nuances across the text. For example, it identifies nine codes around the concept of \"software update,\" a common topic in the dataset. It presented a much more detailed perspective than previous approaches, which only identified \u201cupdate announcement\u201d and \u201cupdate planning.\" However, it still generates overly broad concepts such as \"user feedback,\u201d or overlapping ones such as \"update inquiry\" and \"update status inquiry.\"\nWe found this approach impressively capable of reading contexts. When it coded \"User: Yes\" as \"professional engagement,\" it seemed to learn from a previous message, where a designer asked for the same user's identity. However, such capability may become a negative asset. In the only problematic case for groundedness (0.83%), GPT-40 labelled a message as \"new user interaction.\u201d It was indeed the user's first interaction, yet the input data did not include that information."}, {"title": "Item-level Coding w/ Verb Phrases", "content": "Inspired by a study using grounded theory analysis (Davis et al., 2020), we improved the item-level approach by asking LLMs to use verb phrases as labels, hoping to distinguish the nuances at the level of micro-actions (Appendix 4.4). The revised approach found 271 codes, a 13% increase, while showing substantial qualitative improvement by eliminating many overly broad codes and identifying more nuances."}, {"title": "Discussions", "content": "Our study reveals clear qualitative differences between codebooks generated from four different ML/GAI approaches. The item-level LLM coding approach with the usage of verb phrases performed the best (Table 1), with a considerable capability of identifying nuanced insights while reducing issues with groundedness or vagueness. Our finding showcases 1) the importance of prompts in using ML/GAI to generate inductive coding results and 2) the potential of applying human qualitative coding processes in GAI prompts.\nExcept for the BERTopic approach, we found few issues of groundedness. LLMs did not always use optimal labels, but they rarely hallucinated labels without connection to the data. One reason is that the LLM was explicitly instructed to ground its generation in the input data, which has been found to reduce hallucinations (e.g., Lewis et al., 2020). Another reason is the nature of inductive coding. An interpretation or construction from the underlying data may be \"shallow,\u201d \u201cirrelevant,\u201d or \u201cincomplete,\u201d but can hardly be \"wrong\" in a strict sense (Terry et al., 2017).\nIt is essential to discuss the implications of our work. For qualitative researchers, inductive coding means more than producing the codes; the process enables them to get closer to the underlying dataset (J. Corbin & Strauss, 2008). From this epistemological standpoint, it seems that machines can only imitate the results of open coding processes. Even when our Item-Level Verb Phrases approach identified codes close to human results, claiming that it (with GPT-40) can closely imitate human results would be far-reaching. To rigorously claim that requires working with more research contexts. Moreover, none of the approaches fully eliminated overly broad or overlapping codes, which may be attributed to the stochastic nature of LLMs.\nWith hundreds of codes and the ambiguity of human language (e.g., could we claim that \u201csoliciting feedback\" covers the same idea as \"eliciting feedback\"), examining where LLMs or humans missed was impractical to do by hand. On the other hand, it also provides rich opportunities for qualitative researchers to identify novel insights from machine coders - particularly when machines identified extra codes than humans.\nA rigorous, computational(-assisted) approach to evaluating qualitative codes is urgently needed. While consistency is widely used to assess deductive coding, it can only address issues after establishing the codebook. Early attempts using consistency or coverage to evaluate codebooks (Zhao et al., 2024) suffer a similar issue: using human results as the \"golden standard\" for machines to match, they miss opportunities for machine approaches to provide novel insights. Hence, a theory-informed human-Al approach becomes"}]}