{"title": "Learning Privacy-Preserving Student Networks via Discriminative-Generative Distillation", "authors": ["Shiming Ge", "Bochao Liu", "Pengju Wang", "Yong Li", "Dan Zeng"], "abstract": "While deep models have proved successful in learning rich knowledge from massive well-annotated data, they may pose a privacy leakage risk in practical deployment. It is necessary to find an effective trade-off between high utility and strong privacy. In this work, we propose a discriminative-generative distillation approach to learn privacy-preserving deep models. Our key idea is taking models as bridge to distill knowledge from private data and then transfer it to learn a student network via two streams. First, discriminative stream trains a baseline classifier on private data and an ensemble of teachers on multiple disjoint private subsets, respectively. Then, generative stream takes the classifier as a fixed discriminator and trains a generator in a data-free manner. After that, the generator is used to generate massive synthetic data which are further applied to train a variational autoencoder (VAE). Among these synthetic data, a few of them are fed into the teacher ensemble to query labels via differentially private aggregation, while most of them are embedded to the trained VAE for reconstructing synthetic data. Finally, a semi-supervised student learning is performed to simultaneously handle two tasks: knowledge transfer from the teachers with distillation on few privately labeled synthetic data, and knowledge enhancement with tangent-normal adversarial regularization on many triples of reconstructed synthetic data. In this way, our approach can control query cost over private data and mitigate accuracy degradation in a unified manner, leading to a privacy-preserving student model. Extensive experiments and analysis clearly show the effectiveness of the proposed approach.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep learning [1] has delivered impressive performance in image recognition [2]\u2013[6] due to the powerful capacity of deep networks on learning rich knowledge from large-scale annotated data. However, the deployment of deep models may suffer from the leakage risk of data privacy. Recent works [7], [8] have shown that the private information in the training data can be easily recovered with a few access to the released model. Thus, many real-world requirements [9], [10] need to provide high-performance models while protecting data privacy. Thus, it is necessary to explore a feasible solution that can address a key challenge for model deployment: how to effectively learn a privacy-preserving deep model without remarkable loss of inference accuracy?\nCompared to traditional learning solutions that directly access to private data and lead to privacy leakage in released model (in red in Fig. 1), the privacy-preserving learning solutions usually add privacy protection strategies or avoid released model (in green in Fig. 1) directly access to private data during training. Towards this end, many existing approaches have been proposed, which are mainly based on differential privacy [11]. According to the privacy-preserving strategy, they can be roughly grouped into two categories: the implicit category and explicit category.\nThe \"implicit\" approaches leverage differentially private learning to train models from private data by enforcing differential privacy on the weights or gradients during training. The prior approach is introducing differential privacy into stochastic gradient descent to train deep networks [12]. Later, Chen et al. [13] proposed a stochastic variant of classic backtracking line search algorithm to reduce privacy loss. Papernot et al. [14] proposed to replace the unbounded ReLU activation function with a bounded tempered sigmoid function to retain more gradient information. Some recent works proposed gradient operations for private learning by denoising [15], clipping [16], perturbation [17] or compression [18]. Typically, these approaches have promising privacy protection, but often suffer from a big drop in inference accuracy over their non-private counterparts. By contrast, the \u201cexplicit\" approaches pretrain models on private data and then use auxiliary public or synthetic data to learn the released models with knowledge transfer by enforcing differential privacy on the outputs of pretrained models [19]-[22]. Papernot et al. [19] proposed private aggregation of teacher ensembles (PATE) to learn a student model by privately transferring the teacher knowledge with public data, while Hamm et al. [20] proposed a new risk weighted by class probabilities estimated from the ensemble to reduce the sensitivity of majority voting. Zhu et al. [22] proposed a practically data-efficient scheme based on private release of k-nearest neighbor queries, which can avoid the decline of accuracy caused by partitioning training set. These approaches generally can improve the model performance while massive unlabeled public data with the same distribution as private data are available. However, these public data are difficult to obtain and the models trained in this way are still at risk of malicious attacks. Recent works [23], [24] trained a private generator and used the generated synthetic data to replace the auxiliary public data. Generally speaking, the most important process in the explicit category is transferring sufficient knowledge from private data to auxiliary data with minimal privacy leakage. Thus, the key issue that needs to be carefully addressed is applying reliable models to extract knowledge from private data and exploring effective auxiliary data to transfer knowledge.\nInspired by this fact, we propose a teacher-student learning approach to train privacy-preserving student networks via discriminative-generative distillation, which applies discriminative and generative models to distill private knowledge and then explores generated synthetic data to perform knowledge transfer (Fig. 1). The objective is to enable an effective learning that achieves a promising trade-off between high model utility and strong privacy protection. As shown in Fig. 2, the student is trained by using two streams. First, discriminative stream trains a baseline classifier on all private data and an ensemble of separate teachers on disjoint subsets, while generative stream takes the baseline classifier as a fixed discriminator and trains a generator in a data-free manner. Massive synthetic data are then generated with the generator and used to train a variational autoencoder (VAE) [25]. After that, a few of the synthetic data are fed into the teacher ensemble to query labels with Laplacian aggregation, while most of the synthetic data are fed into VAE to achieve massive data triples by perturbing the latent codes. Finally, a semi-supervised learning is performed by simultaneously handling two tasks: knowledge transfer via supervised data classification, and knowledge enhancement via self-supervised model regularization.\nIn summary, our approach can effectively learn privacy-preserving student networks by three key components. First, data-free generator learning is incorporated to generate massive synthetic data. These synthetic data are difficult to be identified from appearance but have similar distribution with private data in discriminative space. Therefore, the student learning does not involve any private data and the synthetic data do not expose the information of private data even if they are recovered. Second, differential privacy is incorporated to provide a strong privacy guarantee theoretically. In Laplacian aggregation of teacher ensemble, student's access to its teachers is limited by reducing label queries, so that the student's exposure to teachers' knowledge can be meaningfully quantified and bounded. Third, tangent-normal adversarial regularization is adopted to improve the capacity and robustness of student. In semi-supervised student learning, synthetic data are embedded into the pretrained VAE space and reconstructed from latent codes by adding perturbation along both tangent and normal directions of distribution manifold. Then, the tangent regularization can enforce the local smoothness of the student along the underlying manifold and improve model accuracy, while the normal regularization imposes robustness on the student against noise. In this way, the two regularization terms complement each other, jointly facilitating knowledge transfer from the teacher ensemble to the student. Our approach provides a unified framework to learn privacy-preserving student networks. The data-free generator learning and Laplacian aggregation can protect the private data, and adversarial regularization via VAE reconstruction of the synthetic data can better learn data manifold. Combining them together can protect private data while reducing the impact of noisy labels and instability of the synthetic data.\nOur major contributions are three folds: 1) we propose a discriminative-generative distillation approach to train privacy-preserving student networks that achieves an effective trade-off between high utility and strong privacy, 2) we propose to combine data-free generator learning and VAE-based model regularization which facilitates knowledge transfer in a semi-supervised manner, and 3) we conduct extensive experiments and analysis to demonstrate the effectiveness of our approach."}, {"title": "II. RELATED WORKS", "content": "The approach we proposed in this paper aims to learn privacy-preserving student networks by distilling knowledge from private data and transferring it to synthetic data. Therefore, we briefly review related works from three aspects, including differentially private learning, learning with synthetic data and teacher-student learning."}, {"title": "A. Differentially Private Learning", "content": "Differentially private learning [26] aims to address tasks like healthcare [27] where the data are private and the learning process meets differential privacy requirements. Differential privacy provides a guarantee that two adjacent databases produce statistically indistinguishable results under a reasonable privacy budget.\nPrevious works [28], [29] considered using differential privacy in machine learning settings. Shokri et al. [30] introduced a privacy-preserving distributed stochastic gradient descent (SGD) algorithm which applies to non-convex models. Its privacy bound is decided by the number of model parameters that are related to the representation ability of the model, leading to an inefficient trade-off between privacy and model capacity. Abadi et al. [12] provided a stricter bound on the privacy loss induced by a noisy SGD by introducing moments accountant. Papernot et al. [19] proposed a general framework named private aggregation of teacher ensembles (PATE) for private training. PATE uses semi-supervised learning to transfer the knowledge of the teacher ensemble to the student by using a differentially private aggregation. It uses the assumption that the student has access to additional unlabeled data. To reduce erroneous aggregation results, Xiang et al. [31] proposed a private consensus protocol by returning only the highest voting results above a threshold in aggregation of teacher ensembles, leading to accuracy improvement under the same privacy level. Gao et al. [32] improved PATE to securely and efficiently harness distributed knowledge by using lightweight cryptography, which can achieve strong protection for individual labels. Miyato et al. [33] proposed virtual adversarial training to avoid the requirements of label information, which reduces queries to the privacy model and protects data privacy. Jagannathan et al. [34] combined Laplacian mechanism with decision trees and proposed a random forest algorithm to protect privacy. The idea of differentially private learning can suggest the usage of data for training models under a certain privacy budget."}, {"title": "B. Learning with Synthetic Data", "content": "With the development of generative adversarial networks (GANs) [35], recent works began to use synthetic data in training deep networks. Zhang et al. [36] found that the performance of classifiers trained in a semi-supervised manner using synthetic data could not be guaranteed and proposed Bad GAN to preferentially select the generator, which greatly improves the feature matching of GANs. Dumoulin et al. [37] proposed to jointly learn a generation network and an inference network using synthetic data generated by generation network, achieving a very competitive performance. Salimans et al. [38] presented a variety of architectural features and training procedures, which improves the performance of both classifier and generator. Kumar et al. [39] proposed to estimate the tangent space to the data manifold using GANs and employ it to inject invariances into the classifier, which can greatly improve in terms of semantic similarity of the reconstructed samples with the input samples. Luo et al. [40] introduced smooth neighbors on teacher graphs, which improves the performance of classifier through the implicit self-ensemble of models. Qi et al. [41] presented localized GAN to learn the manifold of real data, which could not only produce diverse image transformations but also deliver superior classification performance. The works [23], [42]\u2013[44] used differentially private stochastic gradient descent (DPSGD) to train GANs, which has been proven effective in generating high-dimensional sanitized data [43]. However, DPSGD relies on carefully tuning of the clipping bound of gradient norm, i.e., the sensitivity value. Specifically, the optimal clipping bound varies greatly with model architecture and training dynamics, making the implementation of DPSGD difficult. In order to solve this problem, Chen et al. [45] used Wasserstein GANs [46], [47] for a precise estimation of the sensitivity value, avoiding the intensive search of hyper-parameters while reducing the clipping bias. Generally, these approaches aim to generate synthetic data to facilitate model learning, while the privacy issue introduced by generated data is less considered."}, {"title": "C. Teacher-Student Learning", "content": "Typically, teacher-student learning applies knowledge distillation [48]\u2013[50] to learn a more compact student model by mimicking the behaviors of a complex teacher model. It is used for model compression while hardly degrading the model performance. In the vanilla knowledge distillation, by using the softmax output of the teacher network as soft labels instead of hard class labels, the student model can learn how the teacher network behaves given tasks in a compact form. Since then, many works [51]\u2013[55] had used and improved this training method. Romero et al. [51] proposed to add an additional linear projection layer. Tian et al. [56] proposed to combine contrastive learning with knowledge distillation. The teacher-student learning manner has been applied in many applications, such as low-resolution face recognition [57], action recognition [58], semantic segmentation [59], data generation [60] and molecular generation [61]. For circumstances when training data for the teacher are unavailable in practical problems such as privacy, Chen et al. [62] proposed a data-free knowledge distillation framework. It regards the pretrained teacher networks as a fixed discriminator and trains a generator to synthesize training samples for the student. To protect the privacy of the data, some works utilize structural improvements [20], such as training a collection of teacher models [63]. Recently, the distillation idea is used to control privacy loss [19], [24]. The key issue in learning privacy-preserving models with distillation is to make knowledge transfer adequately and privately."}, {"title": "III. THE APPROACH", "content": "Given a private dataset D, the objective is learning a privacy-preserving student \\( \\mathcal{S}_s \\) that does not reveal data privacy and has the capacity approximating to the baseline model trained directly on D. To achieve that, we introduce both discriminative and generative models to enforce the knowledge transfer via discriminative-generative distillation with two streams. Discriminative stream partitions D into n disjoint subsets \\( \\mathcal{D} = \\{ \\mathcal{D}_i \\}_{i=1}^n \\) and learns an ensemble of multiple teachers \\( \\Phi_t = \\{ \\phi_{t,i} \\}_{i=1}^n \\) where \\( \\phi_{t,i} \\) is trained on \\( \\mathcal{D}_i \\). Generative stream takes \\( \\Phi_b \\) as a fixed discriminator and learns a generator \\( \\Phi_g \\) to generate massive synthetic data \\( \\mathcal{D} \\). A VAE \\( \\{ \\phi_e, \\phi_d \\} \\) is pretrained on synthetic data, where \\( \\phi_e \\) and \\( \\phi_d \\) are the encoder and decoder respectively. The pretrained VAE is used to obtain data distribution information to facilitate model learning. To reduce the privacy budget, only a few of synthetic data \\( \\mathcal{D}_s \\subset \\mathcal{D} \\) are used to query the teacher ensemble and get the noisy labels \\( \\mathcal{L}_s \\). The other unlabelled data \\( \\mathcal{D}_u = \\mathcal{D} \\backslash \\mathcal{D}_s \\) with \\( |\\mathcal{D}_u| > |\\mathcal{D}_s| \\) are employed to provide manifold regularization, with the help of VAE. Thus, the student learning can be formulated by minimizing an energy function E:\n\n\\( E(\\mathcal{W}_s; \\mathcal{D}) = E_s(\\phi_s(\\mathcal{W}_s; \\mathcal{D}_s); \\hat{\\mathcal{L}}_s) + E_u(\\phi_s(\\mathcal{W}_s; \\phi_d(\\phi_e(\\mathcal{D}_u)))), \\phi_s(\\mathcal{W}_s; \\phi_d(\\mathcal{P}[\\phi_e(\\mathcal{D}_u)]))) \\)"}, {"title": "B. Data-Free Generator Learning", "content": "Knowledge transfer from the model trained on private data or synthetic data generated by GAN pretrained on private data may lead to privacy leakage. Meanwhile, the models trained on public data may cause significant accuracy degradation due to distribution mismatch, since finding public data that match the distribution with private data often is very difficult [19]. Moreover, the resulting models are vulnerable to attacks since adversaries can also access public data. Thus, we aim to learn a generator in a data-free manner to generate synthetic data, which does not compromise privacy to assist in knowledge transfer from private data to learn released models.\nUnlike traditional GAN training where the discriminator is an online learned two-class classifier, our data-free generator learning first pretrains a baseline multi-class classifier \\( \\Phi_b \\) (with parameters \\( \\mathcal{W}_b \\)) on private data D that serves as the fixed discriminator, and then train the generator \\( \\Phi_g \\) without data. It is suggested that the tasks of discrimination and classification can improve each other and the multi-class classifier can learn the data distribution better than the two-class discriminator [62], [64]. Thus, the key of using the multi-class classifier as discriminator is defining a loss to evaluate the generated data. Towards this end, we assess a synthetic example \\( x = \\phi_g(\\mathcal{W}_g;z) \\) generated by \\( \\Phi_g \\) with parameters \\( \\mathcal{W}_g \\), from a random vector z by the following loss:\n\n\\( \\mathcal{L}(x) = \\ell(\\phi(\\mathcal{W}_b; x), \\arg \\max(\\phi_b(\\mathcal{W}_b; x));) + \\alpha \\phi_b(\\mathcal{W}_b; x) \\log \\phi_b(\\mathcal{W}_b; x) + \\beta ||\\phi_b(\\mathcal{W}_b; x)||_1, \\)\n\nwhere \\( \\alpha \\) and \\( \\beta \\) are the tuning parameters to balance the effect of three terms, and we set them as 5 and 0.1 respectively. The first term \\( \\ell(\u00b7) \\) is cross entropy function that measures the one-hot classification loss, which enforces the generated data having similar distribution as the private data. The second term is the information entropy loss to measure the class balance of generated data. The third term uses \\( \\ell_1 \\)-norm || * ||1 to measure the activation loss, since the features \\( \\phi_b(\\mathcal{W}_b;x) \\) that are extracted by the discriminator and correspond to the output before the fully-connected layer tend to receive higher activation value if input data are real rather than some random vectors, where \\( \\mathcal{W}_b \\subset \\mathcal{W} \\) is the discriminator's backbone parameters. Then, using the fixed discriminator, the generator learning is performed iteratively via five steps:\n\n*   randomly generate a batch of noise vectors : \\( \\{ z_i \\}_{i=1}^m \\).\n*   generate synthetic samples \\( \\{ x_i \\}_{i=1}^m \\) for training: \\( x_i = \\phi_g(\\mathcal{W}_g; z_i) \\).\n*   apply the discriminator on the mini-batch: \\( y_i = \\phi_b(\\mathcal{W}_b; x_i) \\).\n*   calculate the loss function with Eq. (2) on mini-batch: \\( \\sum_i \\mathcal{L}(x_i) \\).\n*   update weights \\( \\mathcal{W}_g \\), using back-propagation.\n\nIn this way, the synthetic data \\( \\hat{\\mathcal{D}} \\) generated by the learned generator have a similar distribution to private data without compromising privacy. Fig. 5 shows some examples. The synthetic data are very helpful for student learning, which can greatly improve accuracy compared to using public data and reduce accuracy loss compared to using private data directly. With \\( \\hat{\\mathcal{D}} \\), we train a VAE \\( \\{ \\phi_e,\\phi_d \\} \\), where the encoder \\( \\phi_e \\) with parameters \\( \\mathcal{W}_e \\) and decoder \\( \\phi_d \\) with parameters \\( \\mathcal{W}_d \\) are constructed with convolutional neural networks like [65]."}, {"title": "C. Teacher Ensemble Learning", "content": "Instead of using a single model as teacher that may lead to privacy leakage [63], we learn an ensemble of teachers for knowledge transfer. Towards this end, we partition the private data D into n disjoint subsets \\( \\{ D_i \\}_{i=1}^n \\) and then separately train a teacher \\( \\Phi_{t,i} \\) with parameters \\( \\mathcal{W}_{t,i} \\) on each subset \\( D_i \\), leading to the teacher ensemble \\( \\Phi_t = \\{ \\Phi_{t,i} \\}_{i=1}^n \\).\nIn general, the number of teachers n has an impact on knowledge extraction from private data. When n is too large, the amount of each training subset data gets less and the teachers may be underfitted. When n is too small, it will make the noise of differential privacy more influential and lead to unusable aggregated labels. Thus, the teacher number n should be carefully set in experience.\nThe teacher ensemble serves to label query, where the synthetic data x \u2208 D, is fed and the predicted labels by multiple teachers are privately aggregated:\n\n\\( \\ell = \\arg \\max_k \\{ V_k(\\{\\phi_{t,i}(\\mathcal{W}_{t,i}; x)\\}_{i=1}^n) + \\text{Lap}(2/\\epsilon_0) \\}, \\)\n\nwhere \\( V_k(\u00b7) \\) counts the votes of the query being predicted as class k by all n teachers, the final predicted label \\( \\ell \\) is noisy and used to supervise the student training, a low privacy budget \\( \\epsilon_0 \\) is used to adjust privacy protection and \\( \\text{Lap}(2/\\epsilon_0) \\) denotes the Laplacian distribution with location 0 and scale \\( 2/\\epsilon_0 \\). For student training, each example from the query data \\( \\mathcal{D}_s \\) is fed into the teacher ensemble and then the prediction is privately aggregated via Laplacian aggregation in Eq. (3), leading to \\( \\mathcal{L}_s = \\{ \\ell_i \\} \\). Directly using the maximum value of vote counts as labels may leak privacy, so we add random noise to the voting results to introduce ambiguity. Intuitively, this means that multiple teachers jointly determine the query result, making it difficult for adversary to recover the training data. In addition to this, our approach can provide the same or stronger privacy guarantee than many state-of-the-arts [17]\u2013[19], [23], [24], [45], [66], [67] while reducing accuracy degradation by knowledge enhancement with an extra model regularization. It also means that our approach will have a less privacy cost when delivering student models with the same accuracy."}, {"title": "D. Semi-Supervised Student Learning", "content": "To reduce privacy leakage, we only use a few of synthetic data \\( \\mathcal{D}_s \\) for label query. Thus, the teacher knowledge that transfers from private data to \\( \\mathcal{D}_s \\) is not only noisy due to Laplacian aggregation but also insufficient due to limited data. To enhance knowledge transfer, we learn the student in a semi-supervised fashion by adding another unsupervised pathway. Each synthetic example \\( x_j \\in \\mathcal{D}_u \\) is embedded into VAE space and get a mean vector \\( \\mu_j \\) and a standard deviation vector \\( \\sigma_j \\) with \\( \\{ \\mu_j, \\sigma_j \\} = \\phi_e(\\mathcal{W}_e;x_j) \\) that form a normal distribution \\( \\mathcal{N}(\\mu_j, \\sigma_j) \\). Then, the data is reconstructed from a sampled code \\( e_j \\) as well as its perturbed versions along tangent and normal directions of the distribution manifold, leading to massive data triples \\( \\mathcal{T} = \\{ (x_j, \\hat{x}_j^{\\||}, \\hat{x}_j^{\\bot}) \\}_{j=1}^{|\\mathcal{D}_u|} \\) with:\n\n\\( \\hat{x}_j = \\phi_d(\\mathcal{W}_d; \\mathcal{M}(e_j)),  \\hat{x}_j^{*} = \\phi_d(\\mathcal{W}_d; \\mathcal{M}(e_j + \\eta)), \\)\n\nwhere the mapping operator \\( \\mathcal{M}(\u00b7) \\) projects the code into decoder input, \\( \\eta \\) is random perturbation noise along tangent direction (* =||) or normal direction (* =\u22a5). Then, the semi-supervised student learning is performed with \\( \\{ \\mathcal{D}_s, \\mathcal{L}_s \\} \\) and \\( \\mathcal{T} \\). The supervised energy in Eq. (1) can be formulated as\n\n\\( E_s = \\sum_{i=1}^{|\\mathcal{D}_s|} \\ell(\\phi_s(\\mathcal{W}_s; x_i), \\ell_i), \\quad \\text{s.t.} \\quad x_i \\in \\mathcal{D}_s, \\ell_i \\in \\hat{\\mathcal{L}}_s, \\)\n\nand the unsupervised energy is formulated as\n\n\\( E_u = \\sum_{j=1}^{|\\mathcal{D}_u|} ||\\phi(\\mathcal{W};x_j) - \\phi_b(\\mathcal{W};\\hat{x}_j^{\\||})||^2 + \\sum_{j=1}^{|\\mathcal{D}_u|} ||\\phi(\\mathcal{W};x_j) - \\phi_b(\\mathcal{W};\\hat{x}_j^{\\bot})||^2 + \\sum_{j=1}^{|\\mathcal{D}_u|} \\phi_s(\\mathcal{W}_s; \\hat{x}_j) \\log \\hat{\\phi}_s(\\mathcal{W}_s; \\hat{x}_j), \\)\n\nwhere \\( \\mathcal{W}_c \\subset \\mathcal{W}_s \\) is backbone parameters of the student for extracting features. We can see that the unsupervised energy Eq. (6) includes normal regularization, tangent regularization and entropy regularization. The first two regularization terms enhance model robustness against perturbations along orthogonal and parallel directions to the underlying data manifold respectively, while entropy regularization ensures the student output more determinate predictions. This tangent-normal adversarial regularization by adding perturbation to the latent layer can make the student vary smoothly along tangent space and have strong robustness along normal space [65].\nThe total energy comes from two streams. Discriminative stream employs the supervised energy for knowledge transfer from teacher ensemble with a few of query examples, while generative stream takes the unsupervised energy for knowledge enhancement via model regularization. The differentially private aggregation provides privacy protection, while the usage of VAE embedding and reconstruction can obtain the characteristics of the data in the tangent and normal spaces. In particular, generative stream applies self-supervised learning from massive unannotated data to compensate the knowledge that may miss in discriminative stream."}, {"title": "E. Discussion", "content": "Practical Deployment. To learn a privacy-preserving student, our approach trains it from synthetic data generated with a generator pretrained in a data-free manner. Typically, the learning could be deployed in a single server where the private data are partitioned into several subsets to train an ensemble of teachers. Moreover, the learning is also suitable to deploy for jointly training models from distributed clients via a trusted server as the coordinator. In this case, each client trains a teacher on its private local data and all teachers form the teacher ensemble, while the trusted server aggregates the local data via centered learning or local knowledge via federated learning [68] to pretrain a baseline classifier that is used to train a generator in a data-free manner. Then, the server applies the generator to generate massive synthetic data that are used to pretrain a VAE. After that, the server splits the synthetic data into two parts: a few of them are distributed to local clients to query labels by noisy aggregation in discriminative stream, and most of them are fed into generative stream for VAE reconstruction to get massive synthetic data triples. Finally, the student is trained on noisy labels and synthetic data triples in a semi-supervised manner within the trusted server. By allowing only student to be accessible to adversaries, the trained student could be deployed on practical applications and gives the differential privacy guarantee, introduced next.\nPrivacy Analysis. According to the learning process in two streams, the total privacy budget contains two parts. Discriminative privacy budget is computed as PATE [19], [21], achieving \\( \\epsilon_0 \\)-differential privacy via Eq. (3) and getting \\( (|\\mathcal{D}_s| + \\epsilon_0^{-2}|\\mathcal{D}_s| \\log \\delta, \\delta) \\)-differential privacy over \\( |\\mathcal{D}_s| \\) queries for all \\( \\delta \\in (0, 1) \\) [69]. Generative privacy budget is computed according to the latent code perturbation in VAE construction. By taking the synthetic data in generative stream as a sequence, we achieve \\( \\epsilon_1 \\)-differential privacy by adding Laplacian noise with scale \\( 2c/\\epsilon_1 \\) to the normalized latent codes, where c is the dimension of latent codes. It could be explained as follow. According to Laplacian mechanism and post-processing theorem [69], we have: for any two different images \\( x_j \\) and \\( x'_j \\) as well as possible reconstructed output \\( x_j \\), the VAE reconstruction mechanism A satisfies\n\n\\( \\Pr[\\mathcal{A}(x) = x_j] < e^{\\epsilon_1} \\cdot \\Pr[\\mathcal{A}(x') = x'_j \\) where \\( \\Pr[\u00b7] \\) is the probability function. Then, we have the theorem.\n\nTheorem 1 The sequence of VAE reconstruction mechanism A, denoted as \\( \\mathcal{A}(\\mathcal{D}_u) \\) satisfies \\( \\epsilon_1 \\)-differential privacy.\nProof For any two adjacent datasets \\( \\mathcal{D}_u \\) and \\( \\mathcal{D}'_u \\), where \\( x_j \\in \\mathcal{D}_u \\) and \\( x'_j \\in \\mathcal{D}'_u \\) are the two only different images, we have\n\n\\( \\Pr [\\mathcal{A} (\\mathcal{D}_u) \\subseteq O] = \\Pr [\\mathcal{A} (\\mathcal{D}'_u) \\subseteq O] \\cdot \\Pr [\\mathcal{A} (x_j) = x'_j] \\leq e^{\\epsilon_1} \\cdot \\Pr [\\mathcal{A} (\\mathcal{D}'_u) \\subseteq O] \\cdot \\Pr [\\mathcal{A} (x'_j) = x'_j] = e^{\\epsilon_1} \\Pr [\\mathcal{A} (\\mathcal{D}'_u) \\subseteq O], \\)\n\nwhere O denotes the subset of possible outputs. Eq. (7) indicates that \\( \\mathcal{A}(\\mathcal{D}_u) \\) satisfies \\( \\epsilon_1 \\)-differential privacy according to the definition of differential privacy [69]. Further, according to the composition theorem [69], our approach finally satisfies \\( (|\\mathcal{D}_s| + \\epsilon_0^{-2}|\\mathcal{D}_s| \\log \\delta + \\epsilon_1, \\delta) \\)-differential privacy and gives the differential privacy guarantee."}, {"title": "IV. EXPERIMENTS", "content": "To verify the effectiveness of our proposed discriminative-generative distillation approach DGD, we conduct experiments on four datasets (MNIST [70], Fashion-MNIST [71] (FMNIST), SVHN [72] and CIFAR-10 [73]) and perform comparisons with 13 state-of-the-art benchmarks, including 6 explicit approaches that train models with generative data (DP-GAN [23], PATE-GAN [24], GS-WGAN [45], G-PATE [66] and DP-MERF [67], DataLens [18]), and 7 implicit approaches that train models with differentially private learning (DPSGD [12], zCDP [74], GEDDP [15], DP-BLSGD [13], RDP [75], TSADP [14] and GEP [17]). Here, all explicit approaches but DP-GAN apply teacher-student learning to distill models, while all implicit approaches perform differentially private learning without model distillation. To make the comparisons fair, our experiments use the same experimental settings as these benchmarks and take the results from their original papers. Note that original PATE [19] conducted experiments with private data to simulate public data, thus we just compare to it in component analysis experiment.\nComparisons with 6 Explicit Approaches. In the comparisons, we check the performance under different privacy budget, and report the results in Tab. I. Our approach takes 1300 queries under \\( \\epsilon = 10.0 \\) and 27 queries under \\( \\epsilon = 1.00 \\), respectively. All approaches are under a low failure probability \\( \\delta = 10^{-5} \\). The test accuracy of baseline model is 99.2% on MNIST and 91.0% on FMNIST.\nFrom Tab. I, under the same condition of high privacy budget, we can see that our student achieves the highest test accuracy of 97.4% on MNIST and 88.2% on FMNIST, which remarkably reduces the accuracy drop by 1.80% and 2.80% respectively. It shows that our approach has the best privacy-preserving ability and minimal accuracy drop. Under the same low privacy budget, all approaches suffer from accuracy drop with respect to their counterparts under high privacy budget. However, our student still delivers the highest test accuracy and the lowest accuracy drop on both datasets. These results imply that discriminative stream plays an important role in knowledge transfer from private data. First, discriminative stream provides class identity supervision thus we cannot just use generative stream. Second, it uses certain queries to balance privacy protection and model accuracy.\nAfter the promising performance achieved, we further analyze the impact of each component in our approach, including label query, generator learning, teacher ensemble, noisy aggregation, and VAE-based regularization."}, {"title": "Label Query", "content": "To study query effect on the trade-off between model accuracy and privacy protection, we compare the student learning under 27, 750, 1000 and 3000 queries. We treat the label of a generated example by the teacher ensemble as a query. The query number determines the privacy budget and failure probability, and we use differential privacy with moments accountant [19] as metric. More queries will cost a larger privacy budget and fixed query number will lead to constant privacy cost. The results are as expected where a higher privacy budget leads to a higher model accuracy. Besides, in our approach, the private information that the delivered student can directly access is the noisy teachers' prediction outputs who pass through Laplacian aggregation. The results also reveal that our student learning by dicriminative-generative distillation can be performed robustly and consistently under different label queries and providing a certain number of examples (e.g., 750) can lead to an impressive accuracy of 95.2%. It is very helpful in many practical applications where a few of samples are available for sharing. Therefore, our approach can effectively learn privacy-preserving student models and control accuracy drop."}, {"title": "Generator Learning", "content": "To study the effect of data generation, we conduct student learning on MNIST, FMNIST and CIFAR10 with the raw private data as well as synthetic data generated by four generative approaches, including AC-GAN [76", "46": "InfoGAN [77"}]}