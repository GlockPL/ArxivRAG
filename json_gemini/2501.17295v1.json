{"title": "Mitigating Hallucinated Translations in Large Language Models with\nHallucination-focused Preference Optimization", "authors": ["Zilu Tang", "Rajen Chatterjee", "Sarthak Garg"], "abstract": "Machine Translation (MT) is undergoing a\nparadigm shift, with systems based on fine-\ntuned large language models (LLM) becom-\ning increasingly competitive with traditional\nencoder-decoder models trained specifically for\ntranslation tasks. However, LLM-based sys-\ntems are at a higher risk of generating halluci-\nnations, which can severely undermine user's\ntrust and safety. Most prior research on hallu-\ncination mitigation focuses on traditional MT\nmodels, with solutions that involve post-hoc\nmitigation - detecting hallucinated translations\nand re-translating them. While effective, this\napproach introduces additional complexity in\ndeploying extra tools in production and also\nincreases latency. To address these limitations,\nwe propose a method that intrinsically learns to\nmitigate hallucinations during the model train-\ning phase. Specifically, we introduce a data\ncreation framework to generate hallucination\nfocused preference datasets. Fine-tuning LLMs\non these preference datasets reduces the hallu-\ncination rate by an average of 96% across five\nlanguage pairs, while preserving overall transla-\ntion quality. In a zero-shot setting our approach\nreduces hallucinations by 89% on an average\nacross three unseen target languages.", "sections": [{"title": "Introduction", "content": "LLMs are gaining popularity for various NLP appli-\ncations, including machine translation. Fine-tuning\nLLMs for MT has been proven to be highly data-\nefficient, requiring orders of magnitude less parallel\ndata than large standalone multilingual MT models,\nwhile achieving increasingly competitive perfor-\nmance (Liao et al., 2024; Xu et al., 2024; Alves\net al., 2024). Moreover, there is a significant and\nongoing effort within the research community to\npush the performance limits of foundational LLMs\nand expand their multilingual capabilities (Jiang\net al., 2023; Dubey et al., 2024; Aryabumi et al.,\n2024).\nDespite these advantages, LLM-based models\nare more prone to hallucinations: the models gen-\nerates information that is inaccurate or entirely fab-\nricated. This issue has lead to a growing research\narea, focusing on the causes, detection, and miti-\ngation of hallucinations in LLMs (Tonmoy et al.,\n2024). In the context of MT, hallucinations mani-\nfest as highly pathological translations, which can\nlead to misunderstandings in conversations, poten-\ntially damaging relationships and undermining user\ntrust in the system (Kumar et al., 2023).\nMost of the existing research on hallucination\nmitigation in MT has focused on traditional encode-\ndecoder models, establishing effective post-hoc\nmitigation strategies (Guerreiro et al., 2023c; Dale\net al., 2023a,b). These strategies first detect\nwhether a translation contains hallucination, and\nif so, generate and present a mitigated translation\nto the user. In practical scenarios, using post-hoc\nmitigation has several drawbacks: i) the need for\ndeploying an additional hallucination detector in\nproduction; ii) running the hallucination detector\non every translation, which increases cost and la-\ntency; and iii) re-running inference if a translation\nhallucinates (which often much slower than regular\ninference).\nTo address these issues, we propose a framework\nthat intrinsically integrates hallucination mitigation\nduring the LLM development phase, aiming to min-\nimize hallucinations from the outset. Specifically,\nwe apply post-hoc mitigation strategies offline on a\nlarge-scale monolingual corpus, generating a cor-\npus of model hallucinations alongside their corre-\nsponding mitigated translations. We then fine-tune\nthe LLM using Contrastive Preference Optimiza-\ntion (CPO) (Xu et al., 2024), guiding the model\naway from hallucinations.\nOur approach requires no additional human-\nannotated data, is easily scalable across many lan-\nguage pairs, and is highly effective achieving\na 96% reduction in hallucination rates across five"}, {"title": "Dataset Creation Framework", "content": "One of the techniques for fine-tuning LLMs for\ntranslation is preference optimization (Xu et al.,\n2024) which uses a dataset of triplets, consisting of\na source sentence x, its preferred translation yp, and\na dispreferred translation ya. Preference optimiza-\ntion trains the model to prioritize the generation of\npreferred set of translations over dispreferred ones.\nXu et al. (2024) focus on optimizing general trans-\nlation quality, and hence in their datasets, yp and\nya differ only in quality and do not explicitly con-\nsider the notion of hallucination. For instance, both\ntranslations could be broadly correct, but one might\nbe preferred over the other due to minor errors or\nsubtle differences in style.\nTo address hallucinations, we develop a frame-\nwork for automatically creating a hallucination fo-\ncused preference dataset and propose to fine-tune\nthe LLM on this dataset to effectively mitigate hal-\nlucination generation. In this dataset, the dispre-\nferred translations contain hallucinations, whereas\nthe preferred translations do not. The set of dispre-\nferred translations are derived from the LLM's own\ngenerated outputs. This is particularly important\nas it enables the model to learn from its own errors\nand correct them. Our approach for creating this\npreference dataset is completely unsupervised and\ncan easily scale to multiple languages without any\nhuman annotation. At a high level, the dataset cre-\nation process consists of translating monolingual\ndata using the LLM and automatically detecting\nhallucinations (Section 2.1) and mitigating them\nusing existing post-hoc methods (Section 2.2)"}, {"title": "Hallucination Detection", "content": "In the first step, we construct a set of source sen-\ntences and their corresponding dispreferred trans-\nlations containing hallucinations. To achieve this,\nwe translate publicly available monolingual cor-\npora Dm from the source language into the target\nlanguages using the model M, which we aim to\nfine-tune for reducing hallucinations. We then au-\ntomatically identify translations y (y := M(x))\nthat exhibit hallucination using the state-of-the-\nart hallucination detector model based on BLASER\n2.0-QE (Chen et al., 2023; Dale et al., 2023b).\nBLASER 2.0-QE is a reference-free machine trans-\nlation quality estimation metric that predicts cross-\nlingual semantic similarity between a source sen-\ntence x its translation y. It operates on a scale\nof 1-5, where 1 denotes completely unrelated sen-\ntences and 5 signifies fully semantically equivalent\nsentences. We re-normalize the BLASER score to a\nhallucination score (HS), with a higher value indi-\ncating a greater likelihood of hallucination in y:\n$HS(x, y) = 1 - \\frac{BLASER(x, y)}{5}$                                                           (1)\nAfter fixing a threshold T, we classify a transla-\ntion as containing hallucination if its hallucination\nscore exceeds the threshold. Collecting such in-\nstances where hallucinations are detected provides\nus with a hallucination dataset Dh, which consists\nof source sentences and their corresponding hallu-\ncinated translations as follows:\n$D_h := \\{(x, y) | HS(x, y) \\geq T \\forall x \\in D_m \\}$\n(2)"}, {"title": "Post-hoc Hallucination Mitigation", "content": "The second step involves mitigating the halluci-\nnated translations in Dh to create hallucination-free\nalternatives. Previous works (Dale et al., 2023a;\nGuerreiro et al., 2023a,c) have proposed several\npost-hoc mitigation strategies, though they are typ-\nically applied during test time. In contrast, we ex-\nplore using these strategies offline to build a prefer-\nence fine-tuning corpus. We consider a few notable\nstrategies, outlined below:\nFallback System Guerreiro et al. (2023a)\ndemonstrated that simply switching to a differ-\nent fallback translation system when hallucinations\noccur is an effective mitigation strategy. Follow-\ning this, we employ the NLLB-3.3B model (NLLB\nTeam et al., 2022) as a fallback."}, {"title": "Fine-tuning Using CPO", "content": "We fine-tune the baseline LLM Musing\nour hallucination-focused preference dataset Dp\nthrough CPO, a variant of Direct Preference Opti-\nmization (DPO) (Rafailov et al., 2023), which has\nshown to be effective for fine-tuning LLMs on the\ntranslation task. The CPO objective is formally\ndefined as follows:\n$L_{CPO} = L_{NLL} + L_P$ (4)\nwhere\n$L_P = - E_{(x,y_p,y_d)\\sim D_p} log\\sigma(\\beta log \\frac{\\pi_{\\theta}(y_p|x)}{\\pi_{\\theta}(y_d|x)})$ (5)\n$L_{NLL} = -E_{(x,y_p,y_d)\\sim D_p} log \\pi_{\\theta}(y_p|x)$ (6)\nIn equations above, x, yp and ya represent the\nsource sentence, preferred (hallucination free)\ntranslation and dispreferred (hallucination contain-\ning) translation, respectively, sampled from the\npreference dataset Dp. The policy \\$\\pi_{\\theta}$\\$ refers to the\nconditional probability distribution from the model\nM, \u03c3 is the sigmoid function and \u03b2 is a scaling\nhyperparameter from (Rafailov et al., 2023).\nThe CPO objective combines the standard nega-\ntive log-likelihood NLL loss, which encourages the\nmodel to generate yp, and the preference loss Lp,\nwhich aims to increase the probability gap between\nYp and yd. The preference loss term explicitly in-\nstructs the model to prioritize the generation of Yp\nand reject yd. In Section 6.2 we show that this loss\nterm is crucial for reducing the model's likelihood\nof generating hallucinations.\nIn our dataset, we ensure that Yp always has\nhigher quality than ya, as measured by hallucina-\ntion score (HS(x,yp) < T and HS(x,ya) \u2265 T).\nHowever different preference pairs may exhibit\nvarying quality gaps. To account for this variation\nin quality gaps in the preference fine-tuning, we\nintroduce a scaling term to Lp. A preference pair\n(Yp, yd) with larger quality gap provides a more\ninformative data point, so we design the scaling\nterm to assign greater weight to pairs with a larger\ngaps, proportional to the quality ratio of yp and yd.\nWith this scaling term, the modified preference loss\n(L'p) is defined as follows"}, {"title": "Experimental Setup", "content": "Given a model M, we evaluate it on a monolingual\ndataset D using hallucination rate. Hallucination\nrate (HR) computes the ratio of source sentences\nfor which model produces translations containing\nhallucinations:\n$HR(M, D) = \\frac{|\\{x | HS(x, M(x)) > T \\forall x \\in D\\}|}{|D|}$                                                            (9)\nwhere || counts the number of elements in a set.\nWe split the monolingual corpus Dm into Dtrain\n(train), Ddev (dev) and Dtest (test) sets. The\nhallucination-focused preference dataset (Dp) is\nderived from Dtrain as described as Section 2.\nWe evaluate the baseline and fine-tuned LLMs us-\ning hallucination rates computed against unseen\nset Drest. All the hyperparameters and the best\npost-hoc mitigation strategy for preparing the fine-\ntuning set are selected based on Ddev.\nTo ensure that improvements in hallucina-\ntion mitigation do not come at the expense\nof general translation quality, we also eval-\nuate the baseline and fine-tuned models on\nthe WMT'22 and WMT'23 testsets using\nthree COMET models: wmt22-cometkiwi-da,\nwmt23-cometkiwi-da-xxl, and XCOMET-XXL.\nThis evaluation methodology aligns with that of\nXu et al. (2024)."}, {"title": "Baseline Model and Language Coverage", "content": "We choose ALMA-7B-R as our baseline LLM. Built\nupon LLAMA-2 (Touvron et al., 2023), ALMA-7B-R\nhas been extensively optimized for translation\nthrough multiple rounds of fine-tuning, including\ncontinued pre-training on multilingual data, super-\nvised fine-tuning with parallel corpora and pref-\nerence tuning using CPO. ALMA-7B-R has shown\ncompetitive performance, matching or surpassing"}, {"title": "Hallucination Focused Preference Dataset\nConstruction", "content": "We follow the data creation framework outlined\nin Section 2 to construct a hallucination focused\npreference fine-tuning dataset, as detailed below:"}, {"title": "Monolingual Data", "content": "As our study is restricted to language pairs with En-\nglish as source, we randomly sample English sen-\ntences from the NewsCrawl dataset (Kocmi et al.,\n2022) for Dm. We sample 0.5M sentences each\nfor Ddev and Dtest, and these evaluation sets are\nshared across all language pairs. To create prefer-\nence sets for each language pair, we sample sep-\narate Dtrain sets, with sizes of 2M (en\u2192zh), 5M\n(en\u2192cs, en\u2192is, en\u2192ru), or 10M (en\u2192de) sen-\ntences. The sizes are determined based on halluci-\nnation rates of the baseline model for each language\npair, with larger sets allocated to language pairs ex-\nhibiting lower hallucination rates, ensuring that the\nresulting preference sets are of comparable sizes\nacross all language pairs. All the above datasets\nare cleaned by applying a series of filters to remove\nnoisy samples."}, {"title": "Post-hoc Hallucination Mitigation", "content": "We evaluate the post-hoc mitigation strategies de-\nscribed in Section 2.2 on Ddev. Given a sample\n(x, yd) \u2208 Ddev, where ya contains hallucinations,\neach mitigation strategy S attempts to generate an\nalternative translation \u1ef9 := S(x) which is likely\nfree of hallucinations. We evaluate these strategies\nusing mitigation rate (MR), which is the ratio of sam-\nples where \u1ef9 successfully mitigates hallucinations.\nHigher MR values indicate better performance.\n$MR(S, D_h) = \\frac{|\\{x | HS(x, S(x)) <T \\forall (x,y_a) \\in D_h\\}|}{|D_h|}$                                                           (10)\nFor the Fallback strategy, we use a beam size of 40.\nFor Candidate Generation and Selection approach,\nwe generate n = 40 candidates using temperature\nsampling with t \u2208 {0.8, 1, 1.5} in conjunction with\neither nucleus sampling with p = 0.9 or epsilon\nsampling with e = 0.02. For MCBeam, we generate\ncandidates using a beam size of 5. When using\nCOMET with MBR, we use eamt22-cometinho-da\nwhich is a distilled model that takes as input the\nsource sentence, translation and reference transla-\ntion. For COMET with Re-ranking, we employ the\nwmt20-comet-qe-da, which only takes the source\nsentence and translation as input.\nA detailed comparison of the mitigation strate-\ngies is presented in Section 5.1. We use the best per-\nforming strategy (re-ranking using LaBSE) to con-\nstruct our preferences datasets Dtrain from Dtrain"}, {"title": "Fine-tuning Using CPO", "content": "We present the main results in Table 4.10 Our pri-\nmary baseline, ALMA-7B-R, achieves an average"}, {"title": "Cross-lingual Zero-shot Generalization", "content": "To assess the cross-lingual generalization of our\nfine-tuning approach in reducing hallucinations on\nunseen language pairs, we conducted zero-shot ex-\nperiments comparing baseline ALMA-7B-R with our\nbest fine-tuned model (Mp+a) in a zero-shot set-\nting. In these experiments, we translated our test\nset Dtest from English into three target languages\nFrench (fr), Italian (it), and Spanish (es), none of\nwhich were prominently present in the pre-training\nand fine-tuning stages of ALMA-7B-R.\nTable 5 presents the hallucination rates and\nCOMET scores for both models across these lan-\nguage pairs.11 Notably, both models perform well,\ndespite the target languages being unseen during\ntraining. The baseline model achieves an average\nCOMET score of 83.31, with the fine-tuned model\ntrailing slightly at 83.17. However, in terms of hal-\nlucination rates, the fine-tuned model significantly\noutperforms the baseline, reducing the average hal-\nlucination rate from 0.273% to 0.03%, representing\nan 89% reduction. These results demonstrate that\nour fine-tuning approach generalizes effectively\nto unseen language pairs, substantially reducing\nhallucinations without significant loss in general\ntranslation quality."}, {"title": "Ablation of Loss Function Components", "content": "As shown in equation 8, the CPO loss consists of\ntwo components: i) preference loss and ii) NLL"}, {"title": "Hallucination Characterization", "content": "To gain a deeper understanding of the nature of\nhallucinations, we conducted a detailed analysis of\nthe source sentences and the corresponding halluci-\nnated translations on the test set Dtest\nSource sentences We examined source sentences\nto identify any patterns that might consistently trig-\nger hallucinations when translating to different tar-\nget languages. Table 8 presents these statistics of\nthe overlap of source sentences between halluci-\nnation samples of different language pairs. For\ne.g., in the en\u2192zh language pair, 2178 source sen-\ntences generate hallucinations, however only 5-19\nof source sentences result in hallucinations when\ntranslating other target languages. A similar trend\nis observed across all language pairs. This indi-\ncates that the source sentences do not exhibit strong\npatterns that trigger hallucinations across different\ntarget languages."}, {"title": "Evaluation at Different Hallucination\nScore Thresholds", "content": "Our main evaluation results in Table 4 use a hallu-\ncination score threshold of 0.5. This threshold is\nalso applied to create hallucination focused pref-\nerence datasets. To assess whether our approach\nis biased toward this threshold, we re-evaluated\nboth the baseline (ALMA-7B-R) and our best fine-\ntuned model (Mp+a) at a few lower thresholds.\nIt's important to note that as we lower the thresh-\nold, the distinction between hallucination and non-\nhallucination becomes increasingly blurred. How-\never, a well-tuned model should still show im-\nproved performance over the baseline. Table 11\npresents the evaluation results at different halluci-\nnation score thresholds (0.5, 0.45, and 0.4). While\nour Mp+a consistently outperforms ALMA-7B-R\nacross all thresholds, the performance gap de-\ncreases as the threshold is lowered."}, {"title": "Distribution of Hallucination Scores", "content": "Figure 1 illustrates the distribution of hallucination\nscores for the en\u2192zh pair on Dtest before and after\nfine-tuning. The top plot shows the full scale distri-\nbution from 0-1, while the bottom image provides a\nzoomed-in view focused on the critical range of 0.5-\n1, which highlights the hallucination-prone section.\nIn the top plot, the distribution post-fine-tuning (in\norange) shifts markedly to the left, indicating an\noverall improvement in translation quality across\nthe dataset. In the bottom plot, we observe that the\nremaining hallucinations post-fine-tuning are pri-\nmarily concentrated near the threshold, with fewer\ninstances with extreme hallucination scores. Plots\nfor all language pairs can be found in Appendix 2."}, {"title": "Related Work", "content": "Prior works on hallucination detection include\nidentifying repeated n-gram patterns in transla-\ntions (Raunak et al., 2021), utilizing internal model\ninformation such as attention weights (Lee et al.,"}, {"title": "Conclusion", "content": "In this work, we presented a framework for miti-\ngating translation hallucinations in large language\nmodels (LLMs). To the best of our knowledge,\nthis is among the first works to demonstrate how to\nmitigate translation hallucination in LLMs. In this\nframework, we propose an unsupervised method to\ncreate a hallucination-focused preference dataset,\nwhich is easily scalable across multiple languages.\nFine-tuning LLMs using this dataset through prefer-\nence optimization reduces hallucination rates by an\naverage of 96%, while preserving general transla-\ntion quality. Additionally, our method generalizes\nwell in a cross-lingual zero-shot setting, achieving\nan 89% reduction in hallucination rates across three\npreviously unseen target languages."}, {"title": "Limitations", "content": "\u2022 In this work we explored only en\u2192X language\npairs due to time and resource constraints. We\nleave the exploration of other directions as a\nfuture work.\n\u2022 Since natural translation hallucination is very\nrare, we need to translate huge amount\nof monolingual data to create a reasonable\namount of hallucination focused preference\ndataset, thus making our approach time and\ncompute intensive.\n\u2022 Our approach depends on a hallucination de-\ntector. The language pairs of interest must\nbe supported by the detector, as well as some\nanalysis might be required to decide halluci-\nnation detector threshold."}, {"title": "Ethics Statement", "content": "This work, in our knowledge, does not pose any\nethical concerns. It proposes approaches to make\nAI models safe and trustworthy. Still, our mod-\nels might generate some hallucinations like any\nother AI models. The original data, model, tools,\nand open-source software used in the paper are\npublicly available and has been mentioned in the\ncorresponding sections."}]}