{"title": "EfficientCrackNet: A Lightweight Model for Crack Segmentation", "authors": ["Abid Hasan Zim", "Aquib Iqbal", "Zaid Al-Huda", "Asad Malik", "Minoru Kuribayashi"], "abstract": "Crack detection, particularly from pavement images, presents a formidable challenge in the domain of computer vision due to several inherent complexities such as intensity inhomogeneity, intricate topologies, low contrast, and noisy backgrounds. Automated crack detection is crucial for maintaining the structural integrity of essential infrastructures, including buildings, pavements, and bridges. Existing lightweight methods often face challenges including computational inefficiency, complex crack patterns, and difficult backgrounds, leading to inaccurate detection and impracticality for real-world applications. To address these limitations, we propose EfficientCrackNet, a lightweight hybrid model combining Convolutional Neural Networks (CNNs) and transformers for precise crack segmentation. EfficientCrackNet integrates depthwise separable convolutions (DSC) layers and MobileViT block to capture both global and local features. The model employs an Edge Extraction Method (EEM) and for efficient crack edge detection without pretraining, and Ultra-Lightweight Subspace Attention Module (ULSAM) to enhance feature extraction. Extensive experiments on three benchmark datasets Crack500, DeepCrack, and GAPs384 demonstrate that EfficientCrackNet achieves superior performance compared to existing lightweight models, while requiring only 0.26M parameters, and 0.483 FLOPs (G). The proposed model offers an optimal balance between accuracy and computational efficiency, outperforming state-of-the-art lightweight models, and providing a robust and adaptable solution for real-world crack segmentation.", "sections": [{"title": "1. Introduction", "content": "Cracks are common structural faults found on residential buildings, pavements, and bridges. They often result from inadequate load-bearing capacity, compromising safety. Progressive crack propagation impacts structural integrity and longevity. Therefore, identifying and examining cracks is essential for assessing and maintaining structural security [9, 10]. According to the findings of a research that was carried out in 2006, the cost of traffic accidents that were caused by the state of the roads was estimated to be 217.5 billion. This figure represented 43.6% of the overall cost of accidents that occurred in the United States. [54]. The American Society of Civil Engineers (ASCE) classified US road infrastructure in the 2021 Infrastructure Report Card as a \"D\" grade, indicating poor condition and at risk. An analysis by the U.S. Interstate Highway System reveals that 11% of interstate highways have pavements that are either in poor or average condition. Specifically, 3% of the pavements are categorized as poor, while 8% are categorized as mediocre. Given these critical issues, it is essential to enhance the safety, functionality, and durability of road infrastructure. Effective methods to monitor structural health and assess road conditions enable rapid decision-making and treatment [63]."}, {"title": "2. Related Works", "content": "Most previous studies on crack segmentation are CNN-based models. U-Net is the most well-known CNN-based model, and various architectures of U-Net have been extensively used for crack segmentation [14, 15, 20]. Other CNN based models like FCNs [2, 45], SegNet [7, 37], DeepLab [42], and Mask R-CNN [55] used in crack segmentation in various study. Moreover, various attention modules have also been used with CNN-based models [13]. For example, the attention gate module was integrated into U-Net to enhance the extraction of fracture characteristics by prioritizing important regions and reconstructing semantic features, leading to improved crack segmentation performance [27]. Another study enhanced DeepLabv3+ with a multiscale attention module to combine multiscale crack features [42]. Similarly, an attention module was incorporated into the DCANet backbone network to combine both detailed and abstract features, enhancing the model's overall performance to recover edge information from the crack [38]. However, CNN-based models often struggle to capture explicit long-range dependencies because of their inherently localized nature. Given that most cracks display narrow, elongated structures, it is essential to capture both local and non-local features for accurate crack segmentation [8]. Transformer-based models have shown remarkable performance in crack segmentation [29, 61]. A study developed SegCrack using MMSegmentation and the OHEM strategy to enhance crack segmentation accuracy [41, 46]. Subsequently, another study proposed Crack Transformer [17], a model that integrates elements from the Swin Transformer [32] and SegFormer [50]. However, transformer-based models can be initially challenging to train and are prone to overfitting when working with limited datasets [31]. Recently, there has been increasing attention towards hybrid models that combine CNNs and transformers [23, 34, 43, 52]. Unlike CNNs, transformer has robust long-range modelling capabilities. However, cracks often only occupy a limited part of the picture. Therefore, relying only on a transformer might be susceptible to background interference, resulting in an overall decrease in segmentation performance. Using the hybrid model can compensate for this deficiency [49, 51, 65]. Mobile devices, like drones and phones, often execute crack segmentation activities with limited computational capabilities, memory capacity, and battery support [43]. Hence, our objective is to investigate the use of a compact hybrid model in order to create a network that enhances the precision of crack segmentation with a lightweight structure."}, {"title": "3. Methodology", "content": "This section presents a concise overview of the architectural components of the proposed model. The proposed model's architecture design is based on U-Net. The proposed EfficientCrackNet model consists of three main parts: the encoder, bottleneck, and decoder. The main components of the EfficientCrackNet are the Edge Extraction Method (EEM), Ultra-Lightweight Subspace Attention Mechanism (ULSAM), and MobileViT block."}, {"title": "3.1. Edge Extraction Method (EEM)", "content": "We employed an Edge Extraction Method (EEM) by combining two conventional edge detection methods, Difference of Gaussian (DoG) and Laplacian of Gaussian (LoG), with convolutional layers. This ultimately resulted in a trainable EEM that was able to learn edge features without the need for separate edge label training and with a minimal number of parameters. For edge extraction, the EEM initially applies Gaussian Blur (GB) to the input by convolving it with a Gaussian kernel of size (3, 3), which smooths out high-frequency components and highlights the overall structure of objects in the image by retaining low-frequency features. Secondly, the mathematical formula of DoG is described by Eq. 1, which involves subtracting the blurred input from the original input to extract the boundaries and borders of objects between the complete image and the low-frequency features in the input. This process is analogous to the band-pass filter effect. Furthermore, combining the Gaussian and Laplacian kernels results in a LoG kernel, as formulated in Eq. 2. When this LoG kernel is convolved with the input image, it performs a second-order derivative, thereby extracting only the edges where there are significant changes. The edges extracted by each method are processed through a (1, 1) convolutional layer independently, then multiplied and passed through a (3, 3) max-pooling layer, followed by another (1, 1) convolutional layer [1, 5, 26]. To prevent the loss of edge features, residual connections are incorporated around the EEM, facilitated by the Squeeze-Excitation Module (SEM) [22].\n$DOGI = I * (\\frac{1}{2\\pi\\sigma^2\\epsilon}e^{-\\frac{x^2 + y^2}{2\\sigma^2}} - e^{-\\frac{x^2+y^2}{2\\sigma^2}})$ (1)\n$LOG=I*(-\\frac{1}{\\pi\\sigma^4}(1-\\frac{r^2}{2\\sigma^2})e^{-\\frac{r^2}{2\\sigma^2}})$ (2)\nThis integration of SEM within the residual connections accentuates critical features, ensuring the preservation and effective utilization of edge features for crack segmentation. Consequently, incorporating SEM within the residual connections allows the EEM to enhance and maintain edge features, which are vital for precise crack detection and segmentation. The structure of EEM is illustrated in Fig. 2."}, {"title": "3.2. Ultra-Lightweight Subspace Attention Module (ULSAM)", "content": "The attention mechanism is capable of performing efficient computational modelling of global dependencies and offers an unlimited receptive field. The multi-scale convolution structure has the potential to derive more detailed features; however, interference may result from irrelevant features [48, 60]. In order to resolve this issue, it is imperative to implement attention mechanisms. The current state-of-the-art attention mechanism is not appropriate for our lightweight model due to its high computational and/or parameter overhead. Therefore, this study employs a simple, effective, and lightweight attention mechanism UL-SAM. The ULSAM employs a single attention map for each feature subspace. Initially, ULSAM uses depthwise convolutions (DW) and subsequently applies only one filter during the pointwise convolution (PW) phase to produce the attention maps. This approach significantly reduces computational complexity.\nLet $F \\in R^{m \\times h \\times w}$ denote the feature maps from a convolutional layer, where m is the number of input channels and h and w are the spatial dimensions. In ULSAM, the feature maps $F$ are segmented into g distinct groups $[F_1, F_2,..., F_\\tilde{n}, ..., F_g]$, with each group containing $G$ feature maps. The group $F_{\\tilde{n}}$ represents a specific set of these intermediate feature maps, and the following steps outline the subsequent processing.\n$A_{\\tilde{n}} = softmax(PW^{1 \\times 1}(maxpool_{3 \\times 3,1}(DW^{1 \\times 1}(F_{\\tilde{n}}))))$ (3)\n$F_{\\tilde{n}} = (A_{\\tilde{n}} \\otimes F_{\\tilde{n}}) \\oplus F_{\\tilde{n}}$ (4)\n$F = concat([F_1, F_2, ..., F_{\\tilde{n}}, ..., F_g])$ (5)\nIn Eq. 3, $A_{\\tilde{n}}$ represents an attention map derived from a group of intermediate feature maps $F_{\\tilde{n}}$. Eq. 4 describes the process in which each set of feature maps undergoes refinement to obtain the augmented feature maps $F_{\\tilde{n}}$, utilizing feature distribution; here $\\otimes$ denotes element-wise multiplication and $\\oplus$ signifies element-wise addition. The output $F$ produced by ULSAM is obtained by concatenating the feature maps from each groups, as outlined in Eq. 5. This approach allows ULSAM to capture features at multiple scales and frequencies while also facilitating effective cross-channel feature utilization within the network [39]. The structure of ULSAM is illustrated in Fig. 3."}, {"title": "3.3. MobileViT block", "content": "The MobileViT block is composed of three distinct sub-modules: local feature encoding, global feature encoding, and feature fusion. Each submodule is tasked with either extracting local features, capturing global features, or merging the extracted features. MobileViT excels at efficiently extracting image feature while maintaining a low parameter count, making it an ideal option for applications constrained by limited computational resources.\nGiven an input tensor $X \\in R^{H \\times W \\times C'}$, MobileViT first uses a convolutional layer $n \\times n$, followed by a pointwise (or 1 \u00d7 1) convolutional layer, which outputs $X_L \\in R^{H \\times W \\times d}$. The $n \\times n$ convolution layer captures local spatial features, while the point-wise convolution maps the tensor to a higher-dimensional space (d-dimensional, with d > C) by learning linear combinations of the input channels.\nTo equip MobileViT with the ability to learn global representations incorporating spatial inductive bias, we divide $X_L$ into N non-overlapping flattened patches, denoted as $X_U \\in R^{P \\times N \\times d}$. In this context, $P = wh$, and $N = \\frac{HW}{P}$ represents the number of patches, with h \u2264 n and w \u2264 n being the height and width of each patch, respectively. For every p\u2208 {1,\u00b7\u00b7\u00b7, P}, inter-patch relationships are captured by utilizing transformers to produce $X_G \\in R^{P \\times N \\times d}$ as follows:\n$X_G(p) = Transformer(X_U (p)),  1 \\leq p \\leq P$ (6)\nThe $X_G \\in R^{P \\times N \\times d}$ is folded to produce $X_F \\in R^{H \\times W \\times d}$. Then, $X_F$ is projected to a lower C-dimensional space through point-wise convolution and merged with $X$ through concatenation. A subsequent $n \\times n$ convolutional layer is used to integrate these concatenated features. It is important to note that $X_U (p)$ captures local features from an n \u00d7 n region using convolutions, while $X_G(p)$ captures global feature across P patches for the p-th location. Consequently, each pixel in $X_G$ can encompass features from all pixels in X. Therefore, the overall effective receptive field of MobileViT is H x W.\nThe MobileViT block improves the network's ability to perceive both global and local features, thereby enhancing its feature extraction capability compared to traditional convolution modules. This convolution-like operation allows the transformer to incorporate positional features, meaning fewer transformer modules are required to learn more features, making it lightweight [36]."}, {"title": "3.4. Encoder", "content": "The network is designed to be light, efficient, and robust. To make the model lightweight, in this study, depthwise separable convolutions (DSC) were used. Numerous efficient network architectures utilize DSC as their fundamental components [21, 40, 58]. DSC notably decreases both the computational load and the total number of parameters in the network, leading to improved efficiency.DW and PW serve distinct purposes in feature generation: DW focuses on identifying spatial relationships, while PW emphasizes capturing correlations across channels [18]. Batch normalization (BN) and ReLU activation are used after each DSC layer.\nThe encoder part of the network leverages the EEM, the ULSAM, and the MobileViT block. EEM is incorporated to improve the model's capability in delineating the external boundaries of the cracks. EEM employs a combination of Gaussian and Laplacian filters to extract edges effectively. The module's ability to highlight edges and fine details makes it particularly useful for crack segmentation. ULSAM primarily employs a subspace attention mechanism. This allows the proposed model to capture features at various scales and frequencies while also facilitating effective use of cross-channel features. The convolution operation, characterized by its fixed receptive field, is designed to detect local patterns but inherently struggles to capture global context or long-range dependencies. The MobileViT block addresses this limitation by enabling the model to efficiently encode both local and global features."}, {"title": "3.5. Decoder", "content": "Like the encoder, the decoder part of the network integrates advanced components such as DSC, upsampling, concatenation blocks, ULSAM, and the MobileViT block to achieve a robust and efficient design. The decoder path begins by increasing the resolution of the feature maps and then combines them with matching feature maps from the encoder, maintaining spatial information. DSCs are also utilized in the decoder to maintain the efficiency and lightweight nature of the network. Each upsampling step is followed by concatenation with the corresponding feature maps from the encoder, allowing the decoder to take advantage of both high-level abstract features and low-level detailed features. This skip connection strategy ensures that the decoder retains important spatial features from the encoder. ULSAM is integrated within the decoder to enhance the learning of cross-channel interdependencies and multi-scale features. The MobileViT block is integrated into the decoder to improve the model's capacity for capturing features at both local and global levels."}, {"title": "4. Experiments and analysis", "content": "This section provides detailed information on the datasets, comparative study, and implementation details."}, {"title": "4.1. Datasets", "content": "The Crack500, DeepCrack, and GAPs384 benchmark datasets is used in this study.\nCrack500: The Crack500 dataset consists of 447 images at a resolution of 2560 x 2592 pixels, featuring diverse crack shapes and widths against complex background textures, making segmentation challenging [57].\nDeepCrack: DeepCrack is a widely recognized dataset used for assessing crack detection algorithms. It consists of 537 images, each with a resolution of 384 \u00d7 544 pixels. The dataset features clear differences in intensity between the cracks and the background, which aids in the effective identification of cracks in pavement images [30].\nGAPs384: The GAPs384 dataset features 384 high-resolution images (1080 x 1920 pixels) with diverse noise types and complex road textures, making crack segmentation challenging and crucial for developing advanced algorithms [53]."}, {"title": "4.2. Data Augmentation", "content": "To enhance the generalization of our model, we utilized various data enhancement techniques. The images were augmented using flipping (p = 0.7), rotation (p = 0.7), random brightness and contrast adjustments (p = 0.2), Gaussian blurring (p = 0.2) and shift scale-rotate transformations (p = 0.2). Additionally, Gaussian noise (p = 0.2), color inversion (p = 0.2) were applied. These augmentations simulate various real-world conditions, enhancing the model's robustness and ability to generalize effectively across different scenarios."}, {"title": "4.3. Loss Function", "content": "The Dice Coefficient Loss is used in this study. It is defined as:\n$Dice Coefficient = \\frac{2 |A\\cap B|}{|A| + |B|}$ (7)\nWhere A is the set of predicted pixels and B is the set of ground truth pixels. To use the Dice Coefficient as a loss function, we define the Dice Loss as:\n$Dice Loss = 1 - Dice Coefficient$ (8)\nIn practice, this can be written for continuous variables as:\n$Dice Loss = 1 - \\frac{2\\sum_{i=1}^{N}p_i g_i}{\\sum_{i=1}^{N}p_i^2 + \\sum_{i=1}^{N}g_i^2}$ (9)\nwhere pi and gi represent the predicted and ground truth values for pixel i, respectively, and N is the total number of pixels [25, 59]."}, {"title": "4.4. Evaluation metrics", "content": "Four widely recognized evaluation metrics are employed: Recall (Re), Precision (Pr), F-score (F1), and the mean Intersection-over-Union (mIoU). These metrics are defined as follows:\n$Re = \\frac{TP}{TP + FN}$ (10)\n$Pr = \\frac{TP}{TP + FP}$ (11)\n$F1 = 2 \\times \\frac{Re \\times Pr}{Re + Pr}$ (12)\n$mIoU = \\frac{1}{Nel} \\sum_{i=1}^{Nel} \\frac{n_{ii}}{\\sum_{k}n_{ik} + \\sum_{j}n_{ji} - n_{ii}}$ (13)\nThe quantities TP, FP, FN, and TN represent the following: true positives, false positives, false negatives, and true negatives, respectively. Here, $n_{ij}$ denotes the number of pixels classified as class $j$ in class $i$, $n_{el}$ represents the number of classes, and $t_i$ is the total number of pixels in class i, calculated as $t_i = \\sum_j n_{ij}$."}, {"title": "4.5. Comparison with the lightweight models", "content": "We evaluate the performance of several state-of-the-art and lightweight segmentation models with our model on three datasets. The models compared are EfficientNet [33], DeepCrack [30], ShuffleNetV2 [35], MobileNetV3 [21], LMM [1]. Here, Table 1 shows the results. Fig. 5 compares the segmentation outputs of our model and other lightweight models to the ground truth on the DeepCrack dataset, demonstrating the robustness of the proposed model.\nThe Results on the Crack500 dataset: On the Crack500 test dataset, the proposed EfficientCrackNet demonstrated a mIoU of 81.33%, Re of 78.43%, and Pr of 79.77%. In terms of the F1-score, which evaluates the balance between Pr and Re, EfficientCrackNet achieved the highest score of 79.10%. In comparison to other models, EfficientCrackNet demonstrated significantly superior performance, achieving a mean Intersection over Union (mIoU) that was 8.02% higher than LMM, 17.07% higher than MobileNetV3, 14.07% higher than ShuffleNetV2, 18.79% higher than DeepCrack, and 30.04% higher than EfficientNet.\nThe Results on the DeepCrack dataset: When tested on the DeepCrack dataset, the proposed EfficientCrackNet model achieved a mIoU of 87.10%, a Re of 83.37%, and a Pr of 88.54%. In terms of the F1-score, EfficientCrackNet reached 85.88%. Comparative analysis reveals that EfficientCrackNet substantially outperforms other models, with a notable mIoU improvement of 0.77% over LMM, 11.61% over MobileNetV3, 5.29% over ShuffleNetV2, 12.29% over DeepCrack, and 11.37% over EfficientNet.\nThe Results on the GAPs384 dataset: On the GAPs384 test dataset, EfficientCrackNet achieved a Re of 76.87%, a Pr of 58.43%, and an F1-score of 66.40%. Additionally, it obtained a mIoU of 71.94%, which represents the highest value among the models evaluated. In comparison to other models, EfficientCrackNet demonstrated superior performance, with an mIoU that was 8.44% higher than LMM, 21.71% higher than MobileNetV3, 13.72% higher than ShuffleNetV2, 29.91% higher than DeepCrack, and 22.70% higher than EfficientNet.\nThe consistently better performance of the proposed EfficientCrackNet indicates its robustness and adaptability while maintaining computational efficiency."}, {"title": "4.6. Model Complexity", "content": "Lightweight networks seek to reduce the complexity of models by addressing three key factors: floating-point operations per second (FLOPs), and the number of parameters. The FLOPs and parameters can be quantified using the formulas outlined in Eq. 14 and 15.\n$FLOPs = 2HW(C_{in}K^2 + 1)C_{out}$ (14)\n$Params = (C_{in}K^2 + 1)C_{out}$ (15)\nIn this context, H and W refer to the height and width of the input feature map, respectively. $C_{in}$ represents the number of input channels, and $C_{out}$ signifies the number of output channels. The variable K corresponds to the kernel size.\nTable 2 presents the FLOPs (G) and parameters (M) of the models. Our model, EfficientCrackNet, has the fewest parameters, with 0.61M and 0.80M fewer than LMM and MobileNetv3, respectively, due to its efficient design. EfficientCrackNet has the second-lowest FLOPs (G), which is 0.483, with only MobileNetv3 having fewer FLOPS (G). However, EfficientCrackNet significantly outperforms MobileNetv3 across all three datasets. Despite being lightweight, our model performs well on all three datasets, indicating its suitability for real-world crack segmentation on mobile devices."}, {"title": "5. Ablation Study", "content": "This section discusses the effect of ULSAM, MobileViT block, and EEM on our model. The Crack500 dataset is used as an illustrative case due to its wide range of shapes and widths, as well as the intricate background textures present in most of the images."}, {"title": "5.1. Effect of ULSAM and MobileVit block", "content": "We conducted experiments to evaluate the impact of ULSAM and the MobileViT block within our model. The results, presented in Table 3, are based on the Crack500 dataset. Without ULSAM resulted in a reduction of the F1 score by 1.49% and the mIoU by 1.18%. Similarly, the exclusion of the MobileViT block led to a decrease in the F1 score by 4.79% and the mIoU by 3.40%. ULSAM is integrated within the encoder, bottleneck, and decoder of the model. ULSAM improve the network's ability to understand complex visual patterns, without significantly increasing model parameters. On the other hand, the MobileViT block aids in capturing global features without escalating the model's complexity significantly."}, {"title": "5.2. Effect of EEM", "content": "Edges and object boundaries are crucial for various high-level computer vision tasks, such as image segmentation. In the early layers of a model, feature maps retain more spatial details of the original shapes of objects, making the extraction of these edges particularly important for tasks like crack segmentation. In our proposed model, we employ an EEM in the earlier layers of the encoder. We conducted experiments to evaluate the impact of EEM on our model using the Crack500 dataset. The results, presented in Table 4, show that the exclusion of EEM led to a significant reduction in performance, with the F1 score dropping by 10.71% and the mIoU by 7.48%. This highlights the importance of EEM in enhancing segmentation accuracy. Fig. 6 illustrates the effect of each component of EEM on the segmentation mask."}, {"title": "6. Conclusion", "content": "EfficientCrackNet is a lightweight hybrid model designed for automatic crack detection and segmentation in infrastructure maintenance. It combines DSC and MobileViT blocks to capture global and local features, enhancing segmentation precision. The model uses an innovative EEM with DoG and LoG for feature extraction without additional training and incorporates ULSAM for improved feature representation. EfficientCrackNet achieves state-of-the-art results on three benchmark datasets, outperforming other lightweight models with just 0.26M parameters and 0.483 GFLOPs, making it ideal for real-world applications.\nDespite its strengths, EfficientCrackNet has limitations that require further exploration. It struggles with detecting extremely thin cracks, which may need more advanced feature extraction techniques. Additionally, variations in lighting and background conditions can affect its performance. Future research should refine the model and expand its use in structural health monitoring, enhancing automated inspection for better infrastructure maintenance and safety."}]}