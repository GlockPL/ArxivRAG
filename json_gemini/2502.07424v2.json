{"title": "RomanLens: The Role Of Latent Romanization In Multilinguality In LLMs", "authors": ["Alan Saji", "Jaavid Aktar Husain", "Thanmay Jayakumar", "Raj Dabre", "Anoop Kunchukuttan", "Ratish Puduppully"], "abstract": "Large Language Models (LLMs) exhibit remarkable multilingual generalization despite being predominantly trained on English-centric corpora. A fundamental question arises: how do LLMs achieve such robust multilingual capabilities? We take the case of non-Roman script languages, we investigate the role of Romanization-the representation of non-Roman scripts using Roman characters as a bridge in multilingual processing. Using mechanistic interpretability techniques, we analyze next-token generation and find that intermediate layers frequently represent target words in Romanized form before transitioning to native script, a phenomenon we term Latent Romanization. Further, through activation patching experiments, we demonstrate that LLMs encode semantic concepts similarly across native and Romanized scripts, suggesting a shared underlying representation. Additionally, for translation into non-Roman script languages, our findings reveal that when the target language is in Romanized form, its representations emerge earlier in the model's layers compared to native script. These insights contribute to a deeper understanding of multilingual representation in LLMs and highlight the implicit role of Romanization in facilitating language transfer.", "sections": [{"title": "Introduction", "content": "The majority of modern Large Language Models (LLMs) (Touvron et al., 2023; Dubey et al., 2024; Team et al., 2024) are trained predominantly on English-dominated corpora. Yet, they exhibit strong multilingual generalization across diverse languages (Huang et al., 2023; Zhao et al., 2024a; Zhang et al., 2023). This raises a fundamental question: How do LLMs develop such robust multilingual capabilities despite their English-centric training?\nTo address this, we adopt a mechanistic interpretability perspective. Wendler et al. (2024) suggest that LLMs encode multilingual information within a shared, language-agnostic latent space, albeit with an inherent bias toward English due to training data composition and architectural choices. Further, Kojima et al. (2024) describe distinct phases in multilingual information processing: initial layers map language-specific lexical and syntactic representations to a language-independent semantic space, middle layers maintain this semantic abstraction, and final layers transform these representations into language-specific lexical and syntactic forms.\nIn this study, we focus on languages written in non-Roman scripts. We hypothesize that LLMs leverage romanized forms of non-Roman script languages as an intermediate bridge between their language-agnostic concept space and language-specific output representations. Romanization-the representation of non-Roman scripts using Roman characters\u2014may facilitate this process by aligning non-English languages more closely with English. Supporting this, Jaavid et al. (2024) demonstrated that explicitly romanizing inputs improves model performance on multilingual tasks, suggesting an inherent alignment between romanized text and English representations. We investigate whether LLMs indeed use romanization as a bridge between language-agnostic concepts and language-specific outputs given its potential implications for understanding multilingual processing in LLMs.\nOur primary experiment visualizes next-token generation using the logit lens (Nostalgebraist, 2020), applying the language modeling head to intermediate layers. As illustrated in Figure 1, we prompt the LLaMA-2 7B (Touvron et al., 2023) model with \u201cFrancais: porte - \u0939\u093f\u0928\u094d\u0926\u0940:\u201d to translate \"door\" from French to Hindi. Our results show that in the middle-to-top layers (layers 20\u201329), roman-"}, {"title": "Related Work", "content": "Recent studies have explored various aspects of LLMs' multilingual behavior: examining whether English emerges as a latent language in English-centric LLMs (Wendler et al., 2024), how the composition of training corpus mixtures influences latent representations (Zhong et al., 2024) and how LLMs handle multilingual capabilities(Zhao et al., 2024b). Interpretability tools relevant to this work"}, {"title": "Background", "content": "We give a quick background of the transformer's forward pass as well as the basics of mechanistic interpretability approaches such as logit lens and activation patching which we leverage in this paper."}, {"title": "Transformer's Forward Pass", "content": "Decoder-only transformer models (Vaswani, 2017) employ a residual architecture to process input sequences through multiple layers, producing a sequence of hidden states (latents). These latents, whose dimensionality remains the same are updated iteratively across layers through transformer blocks $f_j$, where $j \\in [0, k]$ indicates the layer index and $k$ is the final layer index. For next-token prediction, the final latent $h_k$ is transformed by an unembedding matrix $U \\in R^{v \\times d}$ to produce logit scores for vocabulary tokens which are then converted to probabilities via the softmax function (c.f. Appendix A)."}, {"title": "Interpretability Tool: Logit lens", "content": "Generally, in a decoder only LLM, the unembedding matrix $U$ is multiplied with the final hidden state and a softmax is taken on the product to produce the token distributions at that token generation step. Since all hidden states of an LLM are in the same shape, it is possible to apply the unembedding matrix and softmax on all layers, thereby generating token distributions at all layers. This method of prematurely decoding hidden states is referred to as logit lens (Nostalgebraist, 2020). Logit lens reveals how the latent representations evolve across layers to produce the final output, providing insights into the progression of computations within the model."}, {"title": "Interpretability Tool: Activation Patching", "content": "Activation patching involves modifying or patching the activations at specific layers during a forward pass and observing the effects on the model's output. In this work, we adopt the activation patching setup introduced in Dumas et al. (2024).\nIn the context of activation patching, let $l$ denote the language of a word, $C$ denote the concept of a word and $w(C_l)$ denote that word. Eg: If $C$ = cow and $l$ = \u2018en' then $w(C_{en})$ = \u2018cow'. Similarly $w(C_{fr})$ = \u2018vache\u2019. We use 5-shots translation prompts to create paired source $S = (l_{ig}, l_{out}, C_S)$ and target prompt $T = (l_{in}, l_{out}, C_T)$, with different concept, input language, and output language. Unless otherwise specified, $l_S$ and $l_T$ refer to the output languages of $S$ and $T$, respectively.\nFor each transformer block $f_j$, we create two parallel forward passes: one processing the source prompt $S = (s_1, ..., s_{n_S},..., s_{n'_S})$ and the other processing the target prompt $T = (t_1,..., t_{n_T},..., t_{n'_T})$. It should be noted that $n_S$, $n_T$ represents the position of the last token of the object to be translated whereas $n'_S$, $n'_T$ represent the last token position of the source and target prompt to be translated. In Figure 3 in the target prompt translating \"sun\" from French to Hindi,"}, {"title": "Methodology", "content": "We design our analysis setup with the intention of addressing the following research questions:\nRQ1: Do LLMs exhibit latent romanization during multilingual text completion tasks? (Section 4.1)\nRQ2: How does the representation of semantic concepts in LLMs compare between native and romanized scripts of non-Roman languages? (Section 4.2)\nRQ3: What are the differences in hidden layer representations when processing the same language in romanized and native scripts? (Section 4.3)\nPrompt design. We design prompts that facilitate next-token ($x_{n+1}$) prediction from the given context ($x_1,...,x_n$). This is adopted across all analysis setups. Appendix B provides a Hindi example of this prompt design, including its English translation and romanized form.\nTranslation task. We prompt the model to translate a word given five in-context examples.\nRepetition task. We prompt the model to repeat a word in the same language given 5 in-context examples.\nCloze task. We prompt the model to predict the masked word in a sentence given two in-context examples."}, {"title": "Latent Romanization Analysis", "content": "Translation, repetition, and cloze tasks are explored by providing the respective prompts as inputs to an LLM to generate the corresponding output word. We romanize the output word, tokenize it, retaining only the tokens present in the model's vocabulary, and analyze the occurrence of these tokens in the latent layers across timesteps of the output word generation. The analysis is done using logit lens by examining whether the probability of a romanized token in the next token distribution at a given layer exceeds 0.1. We refer to this hereafter as the latent romanization condition. The 0.1 threshold is empirically determined to optimize detection accuracy, i.e. minimizing false positives and maximizing true positives (compared to alternative thresholds 0.05 and 0.01). Our analysis focuses on the final 10 layers of an LLM, where coherent romanized representations emerge according to logit lens visualizations (c.f. Figures 1 and 8 - 12).\nWe track romanized tokens using a timestep-specific tokenization scheme optimized for detection accuracy. In the first output generation"}, {"title": "Patching With Romanized Representation Versus Native Representation", "content": "We intend to compare how concepts are encoded in native script versus romanized script using trans-"}, {"title": "Comparing Translations Into Romanized vs. Native Script", "content": "This analysis examines translation task with target languages in native script and their romanized equivalents. We focus on first-token generation of the output word, also considering possible synonyms.\nIn the next token generation step, the probability of target language and latent language (English) (Wendler et al., 2024) at each layer is examined using logit lens. Each probability is computed by summing over probabilities of all possible tokens corresponding to the answer word(s) in that respective language (c.f. Appendix E). Tokens of latent language and target language are derived using tokenization scheme for first token generation timestep mentioned in Section 4.1."}, {"title": "Experimental Settings", "content": "Languages: We focus on five Indic languages: Hindi, Gujarati, Tamil, Telugu, and Malayalam, as well as Chinese and Georgian. Among these, Hindi and Gujarati belong to the Indo-Aryan branch of the Indo-European language family and use scripts derived from the Devanagari and Gujarati scripts, respectively. Tamil, Telugu, and Malayalam, on"}, {"title": "Results", "content": "We now share our observations about the role of Romanization in LLMs."}, {"title": "Latent romanization", "content": "Our analysis demonstrates that LLMs do exhibit latent romanization during text completion tasks in six out of seven quantitatively analyzed languages (c.f. Figure 4a), with Chinese being the only exception where this phenomenon is not observed. In Figure 4a we can see the latent fraction of romanized tokens for the last 10 layers of an LLM. This is across all tokens of the output word. The frequency of romanized tokens tends to increase just before the last layers. Qualitative logit lens analysis done for Greek, Ukrainian, Arabic, Amharic and Hebrew reveals similar patterns (c.f. Appendix H).\nIn Figures 4b and 4c it is observed that the range of the latent fraction of romanized tokens varies from 0 - 0.01 in the first token generation step to 0-0.2 in the last token generation step in most languages. This trend indicates that latent romanization increases progressively from the initial token to the final token of the output across languages. This observation supports our hypothesis that the first token generation involves more intricate decision-making processes compared to the generation of the final token within an output word."}, {"title": "Patching With Romanized Representation vs. Native Representation", "content": "In Figure 5, we analyze two patching scenarios: In the single-source setup, we compare patching from Hindi\u2192Italian with Hindi(romanized)\u2192Italian source prompt, to Malayalam Italian target prompt. In the multi-source setup, we contrast patching from multiple native script prompts (Hindi\u2192Italian, Gujarati Italian ...) against their romanized counterparts (Hindi(romanized)\u2192Italian, Gujarati(romanized)\u2192Italian ...). We compare the probability distributions of source concept in target language $P(C_T^l)$ across adjacent graphs where"}, {"title": "Comparing Translations Into Romanized vs. Native Script", "content": "We quantify the observations from Figure 2 by analyzing next-token predictions across layers using logit lens. In Figure 7, panels 7c and 7d illustrate that for translations into native scripts, target language tokens begin to emerge from layer 40 onward. Conversely, in panels 7a and 7b, where the target language is in romanized script, target tokens appear 1-2 layers earlier. This pattern indicates that when processing non-Roman script languages, the model forms internal represen-"}, {"title": "Conclusion", "content": "Our findings show that LLMs implicitly use Romanization as a bridge for non-Roman scripts, exhibiting Latent Romanization in intermediate layers before switching to native scripts. Layerwise analyses reveal that semantic concepts are encoded similarly across native and Romanized inputs, indicating a shared internal representation. Moreover, when translating to a Romanized script, target words emerge earlier, highlighting Romanization as a structural link between language-agnostic concepts and language-specific output. While our study reveals initial insights into Latent Romanization, future work could focus on applying these"}, {"title": "Limitations", "content": "The handling of multilingual text by large language models (LLMs) remains an active area of research. Although evidence suggests that LLMs process English representations within a language-agnostic space, the specific mechanisms by which these models adjust their interactions over different timesteps during token generation are still not fully understood. In our study, we observe that romanized representations become increasingly prominent in the hidden layers as token generation progresses from the first to the final token. This trend suggests that latent romanization may help the model mitigate differences in token fertility\u2014that is, the average number of tokens required to represent a word-between the output language and its primary latent language, English. This effect appears especially for non-Roman script languages, with high token fertility. However, further research is needed to confirm and generalize these observations.\nThe interpretability of non-Roman scripts at latent layers is limited when models employ tokenization schemes that split non-roman characters into multiple bytes, complicating logit lens analysis. Extending this work to models with alternative tokenization methods would offer a more complete understanding of multilingual capabilities and representations.\nThis work identifies but does not explain the selective occurrence and varying intensity of latent romanization across languages-questions that merit dedicated future investigation."}, {"title": "Ethics Statement", "content": "Through this work, our aim is to democratize access to LLMs and address the issue of limited data availability for low-resource languages. We emphasize that it is not our intention to diminish the value or significance of the native scripts of the languages included in this study.\nThe code and datasets created in this work will be made available under permissible licenses. Generative AI systems were only used for assistance purely with the language of the paper, eg: paraphrasing, spell-check, polishing the author's original content, and for writing boiler-plate code."}]}