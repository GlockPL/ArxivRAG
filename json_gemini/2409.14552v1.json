{"title": "Unleashing the Power of Emojis in Texts via Self-supervised Graph Pre-Training", "authors": ["Zhou Zhang", "Dongzeng Tan", "Jiaan Wang", "Yilong Chen", "Jiarong Xu"], "abstract": "Emojis have gained immense popularity on social platforms, serving as a common means to supplement or replace text. However, existing data mining approaches generally either completely ignore or simply treat emojis as ordinary Unicode characters, which may limit the model's ability to grasp the rich semantic information in emojis and the interaction between emojis and texts. Thus, it is necessary to release the power of emojis in social media data mining. To this end, we first construct a heterogeneous graph consisting of three types of nodes, i.e., post, word and emoji nodes to improve the representation of different elements in posts. The edges are also well-defined to model how these three elements interact with each other. To facilitate the sharing of information among post, word and emoji nodes, we propose a graph pre-training framework for text and emoji co-modeling, which contains two graph pre-training tasks: node-level graph contrastive learning and edge-level link reconstruction learning. Extensive experiments on the Xiaohongshu and Twitter datasets with two types of downstream tasks demonstrate that our approach proves significant improvement over previous strong baseline methods.", "sections": [{"title": "1 Introduction", "content": "Emojis have gained significant attention due to their popularity in digital communication, especially on social media platforms. They convey emotions and reactions, supplementing or replacing text in posts, which adds nuance to conversations. For example, the heart emoji conveys love more strongly than words, and the face-with-tears-of-joy emoji effectively expresses humor. Accurately modeling emojis can enhance natural language processing (NLP) tasks like sentiment analysis and emoji generation."}, {"title": "2 Related Work", "content": "Emojis play multiple roles in people's communication. They not only provide an expressive way to convey emotions or situational information but are also convenient for grasping implicit sentiments as well as emotions. The use of emojis on social media tends to affect the popularity of tweets. However, the importance of emojis in textual studies has not been fully realized by early work. Recently, many studies have revealed the significance of emojis, and these studies could be summarized into four types of research fields: traditional pre-process, description mining, supervised learning, and self-supervised learning methods.\nTraditional Pre-process Methods. These methods directly utilize standard pre-processing methods and then feed the pre-processed emojis to language models. Specifically, (1) Removing emoji strategy treats emojis as noise and remove them from the original text. This strategy neglects the semantic information conveyed by emojis. (2) Keeping emoji strategy retains emojis as tokens, but unfortunately, the tokens fail to convey the accurate meanings carried by the emojis. (3) Translating emoji translates emojis into descriptive texts and learn representations from these descriptions. Though these methods are intuitive, to perform preprocessing on emojis, we have to maintain a dictionary that covers specific types of emojis (e.g., iOS emojis encoded by Unicode). Such a dictionary cannot be generalized to other types of emojis (e.g., Xiaohongshu emojis and user-created emojis).\nDescription Mining Methods. To further capture the emojis' semantics, another line of research investigates methods for learning emoji representations by utilizing the corresponding textual descriptions. introduce an attention model to learn representations for emojis and text, while simultaneously conducting sentiment classification. This approach learns emoji representations by minimizing the similarity between the representation of an emoji and its corresponding textual description. However, the effectiveness of this approach relies on the availability of textual descriptions for emojis, which may not be accessible for certain types of emojis, such as user-created emojis.\nSupervised Learning Methods. The third line of work explores the use of emojis to assist in various downstream tasks, such as sentiment classification, emotion analysis, and emoji prediction."}, {"title": "3 Methodology", "content": "In this section, we first construct the heterogeneous graph with three types of nodes: posts, emojis and words, to capture the information and interactions among them (\u00a7 3.1). Then, based on the heterogeneous graph, we design a graph pre-training framework to model the comprehensive interaction among all types of nodes (\u00a7 3.2). After that, we show how to use our proposed method in various downstream tasks, i.e., popularity prediction, emoji generation and sentiment classification(\u00a7 3.3). Figure 1 illustrates the overview of our pre-training and fine-tuning stages."}, {"title": "3.1 Heterogeneous Graph Construction", "content": "To incorporate the relationship and information between textual content and emojis, we build up a heterogeneous graph, as shown in Figure 1(a), which consists of three types of nodes:\n\u2022 Post node: $T = \\{t_1, t_2,\\ldots,t_N\\}$ represents the set of post nodes, where $t_i (i = 1, 2, \\ldots, N)$ is the i-th post node containing only the textual information in the i-th post, and $N$ is the total number of posts.\n\u2022 Word node: $W = \\{w_1, w_2, \\ldots, w_m\\}$ is the set of word nodes (i.e., the word vocabulary), where $M$ is the vocabulary size. $W$ includes all the words collected from all the posts.\n\u2022 Emoji node: $E = \\{e_1, e_2,\\ldots,e_k\\}$ means the set of emoji nodes, comprising the top $K$ emojis with the highest occurrences in posts.\nAmong these nodes, there exist three kinds of edges to describe their connections.\n\u2022 Post-emoji edge: This type of edge indicates that if an emoji is in/out of the post. In this manner, we can build connections among posts that share the same emojis, and lead to the information flow from post to emoji. For example, posts \u201cI didn't pass the exam and \u201cI finally pass the exam tell totally different emotions with the same emoji. The former is a sad feeling while the latter means an overwhelming joyful feeling opposite. Therefore, connections between posts and emojis will help a lot in an informative representation study for both.\n\u2022 Post-word edge: This edge type shows the existence of a word in the post. With no doubt that building connections between all words from the post and itself will make the graph complex and may introduce uncontrollable noise. To avoid that, we treat the words inside hashtag as keywords, and then filter keywords from posts and only consider links between them. Typically, the hashtag of a post matches its topic. In this way, we can make sure that connected posts will share the same topic, and make the textual semantic information more finely.\n\u2022 Word-emoji edge: This type of edge comes from the emoji and its k-step neighbor words. For instance, in the sentence \"What a nice day!\", words \"a\", \"nice\", and \"day\" are's two-step neighbors. They contribute to a fine-grained semantic knowledge for emojis, and help us to locate emojis' position in a post.\nTo initialize the node embeddings, we use the pre-trained language model (i.e., BERT ) to generate representations for post nodes and word nodes. For emoji nodes, we obtain the initialization by randomization, these will be discussed comprehensively in Section 4."}, {"title": "3.2 Graph Pre-training Framework", "content": "Based on the heterogeneous graph built above, we design a graph pre-training framework to learn the post, emoji, and word-level representations. The framework consists of two self-supervised tasks: node-level sub-graph contrastive learning and edge-level link reconstruction learning. Both of them are designed to enhance the ability to encode comprehensive semantics among posts, emojis, and words by the general GNN encoder model."}, {"title": "Node-level Sub-graph Contrastive Learning Task", "content": "Following, for each node type, we first sample sub-graphs from the heterogeneous graph via random walk starting from a start node in this node type. Then, we regard the sub-graphs sampled from the same start node as positive pairs, and those from different start nodes as negative pairs. Note that the start nodes of a negative pair should be of the same node type. Finally, we apply the contrastive learning technique to train the GNN encoder by minimizing the InfoNCE loss:\n$L_x = -log\\frac{exp(q^Tk+/T)}{\\Sigma^K_{i=1} exp(q^Tki/T)}$   (1)\nwhere $X$ denotes a node set and $X \\in \\{T,E,W\\}$, $q$ and $k+$ are the representations of two sub-graphs sampled from the same node $xq \\in X$, and $\\{k0, k1,..., kk\\}$ is representations collection of other nodes except node $xq$, which are generated by the GNN encoder $f$, denoted by $q = f(xq)$ and $k = f(xk)$. In this way, the GNN encoder is guided to enhance the similarities (and dissimilarities) between positive (and negative) instances."}, {"title": "Edge-level link reconstruction learning task", "content": "Another task focuses on the relationships among different node types. The motivation is that the representations of semantically similar textual content and emojis should also have higher similarity, even though text and emojis might come from the different distributions. Based on the graph encoder $f$, we additionally use a score predictor that outputs the inner product of two nodes' representations as their similarity. If the similarity value exceeds a certain threshold, a edge between the text and emoji nodes is predicted to exist.\nTo construct training samples, we first sample some existing edges in the heterogeneous graph as positive examples and add an equal number of randomly sampled non-existing edges as negative examples. During training, the model is optimized using a cross-entropy loss to learn representations that maximize the similarity for positive examples and minimize the similarity for negative examples. The binary cross entropy loss for positive edges and negative sampled edges is computed as:\n$L+ = \\frac{1}{\\vert P \\vert} \\sum_{(i,j)\\in P} logp_o (Y_{i,j} = 1|z_i, z_j)$  (2)\n$L- = \\frac{1}{\\vert N \\vert} \\sum_{(i,j)\\in N} log(1 - p_o (Y_{i,j} = 1|z_i, z_j))$ (3)\n$L = L+ + L-$ (4)\nWhere P is the set of positive edges, N is the set of negative edges, and $p_o(Y_{i,j} = 1|z_i, z_j)$ is the probability that the edge (i, j) exists.\nFor each edge type, we make the training processes separately, resulting in three sub-tasks, i.e., emoji reconstruction, important word identification and emoji-word position reconstruction."}, {"title": "3.3 Downstream Tasks", "content": "To evaluate our proposed framework, we apply our emoji embeddings to two downstream tasks: popularity prediction and sentiment analysis. For all downstream tasks, the task input is a social-media post $P = \\{T_p; E_p\\}$, where $T_p = \\{W_{p1}, W_{p2},...W_{pm}\\}$ is the text in the post, $W_{p_{i}}$ means the i-th word in the post text $T_p$, and $m$ indicates the number of words in $T_p$. $E_p = \\{e_{p1}, e_{p2}, ..., e_{pn}\\}$ denotes the emojis in P, $e_{j}$ is the j-th emoji, and n represents the number of emojis in $E_p$. The representations of each word $W_{pi}$ and emoji $e_{pj}$ can be initialized to $h_{wpi} \\in R^d$ and $h_{epj} \\in R^d$ via our graph pre-training framework.\nWe first sample a sub-graph of this node from the heterogeneous graph via a random walk starting from the node, and then input the sub-graph into our GNN encoder model to get its representation, which also serves as the embedding of the node. This embedding is then used for downstream tasks.\nPopularity Prediction & Sentiment Analysis\nThese two tasks are both multi-class classification tasks, which take P as input and output the popularity/sentiment prediction (assuming there are c classes). The process can be formulated as:\n$H_{ep} = \\sum^n_{i=1} h_{epi}$ (5)\n$H_e. = h_tET E$ (6)\n$Y_p = W_{pop}(h_t H_{ep} \\oplus H_{eo})^T$ (7)\nwhere $E \\in R^{K\\times d}$ is the matrix embedding of the emoji set $\\varepsilon$ (includes pre-trained embeddings of all emojis). $h_t \\in R^d$ is the embedding of the post text $T_p$, which is obtained from pre-trained language model backbones. $W_{pop} \\in R^{c \\times 3d}$ is the trainable parameters. $Y_p \\in R^c$ denote the predicted probability over all classes.\nFor objective, we choose the multi-class cross entropy, and we use the weighted labels to mitigate the imbalance of labels. The weight of labels is the inverse of their count.\n$L_{pop} = - \\sum^C_{i=1} w_i y_i log(\\hat{y}_{pi})$ (8)\nwhere $w_i$ is the label weight for the i-th category, $\\hat{y}_i$ is the target and $y_{pi}$ is the predicted probabilities for the i-th category."}, {"title": "4 Experiment", "content": "In this section, we first introduce the implementation details of our proposed pre-training framework (\u00a7 4.1), and then evaluate our framework on two downstream tasks, i.e., popularity prediction (\u00a7 4.2) and sentiment classification (\u00a7 4.3). Also we use illustrative exampleS on emoji generation (\u00a7 4.4) for further demonstration. Finally, we conduct ablation studies to show the effectiveness of each pre-training task (\u00a7 4.5)."}, {"title": "4.1 Pre-training Settings", "content": "During pre-training, we use Adam for optimization with a learning rate of 0.005, $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\epsilon = 1 \\times 10^{-8}$, weight decay of le \u2013 4, learning rate warm-up over the first 7,500 steps, and linear decay of the learning rate after 7,500 steps. Gradient norm clipping is applied with range [-1,1]. For MoCo (He et al., 2020), we use a mini-batch size of 32, dictionary size of 16,384, and momentum m of 0.999, the temperature t is set as 0.07, and we adopt GIN with 2 layers and 768 hidden units each layer as our encoders. The GNN encoder trained in two types of pre-training tasks is then used to generate embeddings of post, emoji and text for downstream tasks."}, {"title": "4.2 Popularity Prediction", "content": "Dataset Our first dataset mainly comes from the Chinese social media platform Xiaohongshu*. We collect over 4 million Xiaohongshu posts through web crawling, and all the information are desensitized. In addition to the content of the post, other information mainly includes the author's Xiaohongshu ID, post title (text information), number of likes, number of collects, number of comments, and publishing time. The user information mainly includes the number of followers, the total number of likes, and whether the account is an official certified account. Note that this Xiaohongshu dataset will not be public due to ethical considerations (e.g., the post might contain individuals' privacy). Our popularity label is derived from the \u201cliked count\", \"collected count\", and \"comments count\" fields in the Xiaohongshu dataset. Given the continuous nature of this data, to simplify it into a categorical task, we cluster them based on the distribution trend of popularity according to the data percentile of [0-50), [50-80), and [80-100], resulting in three categories (low, medium, and high). Thus, the ratio of category labels is low:medium:high = 5:3:2, indicating the imbalanced labels in the data. Given this, we use the weighted labels to mitigate the imbalance issue. The weights of the labels are the inverses of their counts, as described in Eq. 8. The statistic of dataset is shown in table 4.\nCompared Baselines To evaluate the performance of our pre-training framework, we also employ two types of emoji processing methods as baselines:\n\u2022 Preprocessing methods. We use the traditional emoji preprocessing methods introduced in section 2 as baselines: (1) Remove emojis: we use vectors of zeroes as the embedding of emojis to eliminate the information contained in them. (2) Keep emojis: we use vectors of numbers randomly generated from Gaussian distribution as the embedding of emojis, meaning that they are different tokens but the model has no prior information on emojis. We generated two versions of embeddings to avoid randomness, which are denoted as Keep V1 and Keep V2. (3) Translate emojis: we translate emojis into their descriptions (e.g., replace \" \" with the word \u201ccar\u201d), and then use our backbone LLMs to encode emojis. In this way, emojis are treated just as normal words.\n\u2022 Emoji representation learning method. These baseline model utilize the Emoji2vec and the Emoji Co-occurence methods to obtain emojis' embedding. For those emojis not included in the model, we initiate them to vectors of zeroes.\nIn all popularity prediction methods, the embedding of text in the post $T_p$ is encoded by corresponding different backbones (i.e., BERT , GPT2, RoBERTa, OPT-1.3B  and Llama2-7B ). With regard to baseline models, we do not use DeepMoji  and SEntiMoji  for they simply use emojis as annotations and do not learn the representation of emojis. Furthermore, the majority of emojis are represented by Unicode characters that LLMs cannot recognize. Therefore, the translate method mentioned above is essentially an enhanced approach of directly inputting the original text containing emojis into an LLM. Consequently, we did not include the latter in the baseline.\nResults & Analyses As the dataset is quite imbalanced on each label, we use the weighted F1 score as an evaluation metric. To get a robust result, we run each task 10 times with different random initiations and summarize its average metrics.\nAs illustrated in Table 1, our method outperforms all four baselines on all three sub-types of popularity prediction tasks. This finding suggests that emojis offer extra information for popularity prediction by comparing our model with the remove baseline. Besides, traditional methods gen"}, {"title": "4.3 Sentiment Classification", "content": "Dataset The dataset used for the sentiment classification task, is a public dataset consisting of Twitter posts generated by authors of Emoji2vec, which is also used in their experiments on Emoji2vec. This dataset inherently includes three types of sentiment labels for each post: negative, neutral, and positive. During data preprocessing, we filtered out tweets without emojis from the dataset, separated emojis and text for the rest, and then used BERT to obtain post embeddings.\nCompared Baselines To evaluate the performance of our pre-training framework, we employ the same baselines as those in section 4.2. Additionally, we also add SEntiMoji and DeepMoji in our baseline models. Although they do not generate emoji embeddings which can be applied in various downstream tasks, they still serve as strong baselines in sentiment classification task.\nResults & Analyses As shown in Table 2, our method outperforms all baselines on the sentiment classification task with three categories, achieving a F1 score of 0.5679. SEntiMoji and Emoji2vec performs well, while other baseline models fail to extract emoji information in the posts."}, {"title": "4.4 Illustrative Example on Emoji Generation", "content": "To further demonstrate that our pre-training framework can effectively learn the relationship between posts, words, and emojis, we also show an illustrative example in a real-world scenario: post emoji generation. Specifically, this scenario receives post P that only contains text Tp as a input, and inserts emojis into the post. The generation process includes three steps: (1) we use an emoji prediction to determine the most relevant emojis $Er = \\{er_1, er_2,..., er_n | eri \\in E\\}$ with the post P. The similarity between each emoji $ei$ and the post P is calculated as $W_{ep}(he_i \\oplus ht)^T$, where $W_{ep}$ is trainable parameters. (2) Then, word prediction will find the most matched $n_w$ words from $T_p$ for each emoji in $E_r$. The similarity between emoji $er_i$ and word $w_{pi}$ is calculated as $W_{ew}(her_i h_{wp})^T$. To train $W_{ep}$ and $W_{ew}$, it can be regarded as a binary classification for every emoji (matched or unmatched with posts and words). (3) In the last step, we turn $E_r$ and the corresponding matched words into a prompt for GPT-40 model to output a post with emojis.\nResults & Analyses We show several cases to demonstrate the efficiency of our model. At each time, we enter the original post without emojis plus the emojis and their correlated words generated by our model to form a prompt for LLMs models. The results are shown in Figure 4. Compared to the output post generated only by prompting GPT to insert emojis, entering the results helps both in the richness of emojis and the accuracy of the position to insert emojis (more cases are given"}, {"title": "4.5 Ablation Study", "content": "The node-level sub-graph contrastive learning task is separately conducted on different types of nodes, and thus includes three sub-tasks, i.e., post-node, word-node and emoji-node contrastive learning. The edge-level link reconstruction also consists of three sub-tasks, i.e., post-emoji, word-post and emoji-word link reconstruction. To evaluate the effectiveness of these sub-tasks, we conduct ablations by removing each of them in our pre-training framework, and evaluate the model performance on popularity prediction (number of likes).\nAs shown in Table 3, the performance of our original framework exceeds every other variant model, indicating that our model gain information from every part of pre-training tasks. In conclusion, the ablation study shows that Emoji representation learning can benefit from all the pre-training sub-tasks, leading to enhanced prediction performance."}, {"title": "5 Conclusion", "content": "In this paper, we propose a novel framework for joint pre-training on text and emojis. To model the relationship between posts, words and emojis, we first construct a heterogeneous graph comprising three node types corresponding to each element. Subsequently, we propose a subgraph-level pre-training framework for representation learning, including node-level graph contrastive learning and edge-level link reconstruction learning."}, {"title": "Limitations", "content": "We discuss the limitations and how we can potentially address them in this section.\nLimitations in Emoji Modalities: While our proposed model could handle the majority of emoji representations, including Unicode encoding and text-based descriptions, it currently lacks the capability to process emoji in image format. This limitation is noteworthy, especially considering the prevalent use of image-based emojis on certain social media platforms. Our future endeavors involve exploring multimodal approaches to enhance our model's versatility.\nData Cleansing Requirements: The efficacy of our model hinges on accurate emoji tokenization in data preprocessing. Under certain extreme conditions, such as processing an extensive volume of emojis that resist correct tokenization, the model's performance may be compromised. Addressing these challenges remains a focus for future improvements."}, {"title": "Ethics Statement", "content": "There are no ethics-related issues in this paper. The Xiaohongshu data is used only for scientific research purposes, and will not be public. The Twitter data and other related resources in this work are open-source and commonly-used by many existing work."}]}