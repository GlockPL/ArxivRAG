{"title": "Real-World Offline Reinforcement Learning from Vision Language Model Feedback", "authors": ["Sreyas Venkataraman", "Yufei Wang", "Ziyu Wang", "Zackory Erickson", "David Held"], "abstract": "Offline reinforcement learning can enable policy learning from pre-collected, sub-optimal datasets without online interactions. This makes it ideal for real-world robots and safety-critical scenarios, where collecting online data or expert demonstrations is slow, costly, and risky. However, most existing offline RL works assume the dataset is already labeled with the task rewards, a process that often requires significant human effort, especially when ground-truth states are hard to ascertain (e.g., in the real-world). In this paper, we build on prior work, specifically RL-VLM-F, and propose a novel system that automatically generates reward labels for offline datasets using preference feedback from a vision-language model and a text description of the task. Our method then learns a policy using offline RL with the reward-labeled dataset. We demonstrate the system's applicability to a complex real-world robot-assisted dressing task, where we first learn a reward function using a vision-language model on a sub-optimal offline dataset, and then we use the learned reward to employ Implicit Q learning to develop an effective dressing policy. Our method also performs well in simulation tasks involving the manipulation of rigid and deformable objects, and significantly outperform baselines such as behavior cloning and inverse RL. In summary, we propose a new system that enables automatic reward labeling and policy learning from unlabeled, sub-optimal offline datasets.", "sections": [{"title": "I. INTRODUCTION", "content": "Offline reinforcement learning (RL) involves learning poli-cies from a pre-collected, potentially sub-optimal offline dataset. As it does not require online interactions with the environment, it is particularly suited to scenarios where such interactions are impractical, e.g., learning policies with real robots. However, a key challenge remains: the need for the dataset to be labeled with the task rewards.\nMost prior work assumes that the offline dataset comes with labeled rewards and focus on algorithm design [2]-\n[5]. While reward labeling can be straightforward for simple tasks or those in simulation where the rewards can be generated based on low-level environment states, it becomes significantly more challenging for complex or real-world tasks where ground-truth states are not easily accessible. In such cases, manual reward labeling is often needed, making it a time-consuming bottleneck in applying offline RL algorithms. In this paper, we propose a system that auto-matically generates reward labels for a given offline dataset, enabling the learning of effective robot control policies from unlabeled, sub-optimal datasets.\nTo automatically generate rewards, we build upon a prior method, Reinforcement Learning from Vision Language Model Feedback (RL-VLM-F) [1], which was originally designed and evaluated to automatically generate a reward function for a given task in the context of online reinforce-ment learning. RL-VLM-F operates by querying a vision-language foundation model (VLM) to provide preference labels over pairs of the agent's image observations, based on a textual description of the task goal. The algorithm then learns a reward function from these preference labels. This approach has demonstrated effectiveness in online RL settings, where the agent interacts with the environment iteratively: gathering new interaction data, querying the VLM for labels, updating the reward function, and relabeling the agent's experiences with the updated reward. However, this method has not been tested in an offline setting, nor has it been applied to real-world robotics tasks.\nIn this paper, we adapt RL-VLM-F for the offline RL set-ting. Specifically, we query a VLM to generate a preference dataset from the given offline dataset. Subsequently, we learn a reward function from the generated preference dataset, and we use the reward function to label the given dataset. The dataset with the labelled rewards can then be utilized with existing frameworks for offline reinforcement learning to learn a control policy. An overview of our system is shown in Figure 1. Furthermore, we demonstrate the effectiveness of the proposed system by applying it to a complex real-world robot-assisted dressing task, learning a point-cloud based reward and policy from a sub-optimal unlabeled dataset.\nWe chose to build our system based on RL-VLM-F due to its ability to learn rewards directly from visual observa-tions without requiring access to low-level state information, making it particularly well-suited for real-world applications. Many other prior works have explored the use of foundation models, e.g., large language models (LLMs), as a substitute for human supervision in generating reward functions [6]\u2013\n[10]. However, most of these efforts have focused on the online RL setting, and express reward functions as code, necessitating access to the environment code and low-level ground-truth state information [6]\u2013[8]. This reliance poses challenges in high-dimensional environments, such as deformable object manipulation, and for real-world tasks where low-level states are often inaccessible. Some other works employ contrastively trained vision-language models"}, {"title": "II. RELATED WORK", "content": "Inverse Reinforcement Learning (IRL). Similar to our system, IRL methods [14], [15] seek to learn a reward function from expert demonstrations, such that the expert policy is optimal with the learned reward. IRL methods usually assume that the given dataset is optimal and perfectly solves the task. In contrast, our method does not assume the given dataset to be optimal when learning the reward, as it only requires a textual description of the task goal and access to visual observations of the task execution. One of the most classic IRL algorithms is Maximum Entropy IRL [16]. Many new algorithms have been proposed in recent years, such as those that leverage adversarial training, and are shown to achieve better performances [17]\u2013[21].\nBehaviour Cloning (BC). Behavior cloning [22] is a classic imitation learning method where an agent learns to perform tasks by mimicking expert demonstrations. The"}, {"title": "III. PRELIMINARIES", "content": "We consider a standard discounted Markov Decision Pro-cess (MDP) formulation of reinforcement learning [31], defined by the tuple $(S, A, P, R, \\gamma)$, where $S$ is the state space; $A$ is the action space; $R$ is the set of possible rewards; $\\gamma \\in [0,1]$ is the discount factor; and $P : S \\times A \\times S \\rightarrow [0,1]$ is the state transition probability function. For a given state $s \\in S$ and action $a \\in A$, the agent transitions to state $s'$ gaining a reward $r \\in R$ with the probability $P(s',r|s,a)$. The Q function $Q(s, a)$ of policy $\\pi$ is defined as the sum of future discounted rewards starting from state s, taking action a, and following policy $\\pi$. The goal is to learn an optimal policy $\\pi^*$ that maximizes the expected reward over time.\nPreference-based Reinforcement Learning. Our work builds upon preference-based RL, in which a reward function is learned from preference labels over the agent's behaviors [32], [33]. Formally, a segment $\\sigma$ is a sequence of states {$s_1,...,s_H$}, where H > 1. In this work, we consider the case where the segment is represented using a single image. Given a pair of segments $(\\sigma_0, \\sigma_1)$, an annotator gives a feedback label y indicating which segment is preferred: $y \\in {\u22121,0,1}$, where 0 indicates the first segment $\\sigma_0$ is preferred, 1 indicates the second segment $\\sigma_1$ is preferred, and -1 indicates they are incomparable or equally preferable. Given a parameterized reward function $r_{\\psi}$ over the states, we follow the standard Bradley-Terry model [34] to compute the preference probability of a pair of segments:\n$P[\\sigma_1 - \\sigma_0] = \\frac{exp(\\sum_{t=1}^{H} r_{\\psi}(s_{1,t}))}{\\sum_{i \\in {0,1}} exp(\\sum_{t=1}^{H} r_{\\psi}(s_{i,t}))} ,$ (1)\nwhere $\\sigma_i > \\sigma_j$ denotes segment i is preferred to segment j. Given a dataset of preferences $D = {(\\sigma_0, \\sigma_1, y)}$, preference-based RL algorithms optimize the reward function $r_{\\psi}$ by minimizing the following loss:\n$L_{Reward} = E_{(\\sigma_0,\\sigma_1,y)~D}[I{y = (\\sigma_0 > \\sigma_1)} log P_{\\psi}[\\sigma_0 - \\sigma_1] + I{y = (\\sigma_1 > \\sigma_0)} log P_{\\psi}[\\sigma_1 - \\sigma_0]]$ (2)\nOffline RL and Implicit Q Learning (IQL) [2]. We consider the standard offline RL setting, in which the agent has no access to the online environment interactions but instead learns from a fixed dataset of transitions $D = {s_i, a_i, r_{\\theta}(s_i, a_i), s_i'}_{i=1}^{N}$ of size N. These transitions can be collected with a behaviour policy $\\pi_{\\beta}$, which might be a mixture of several sub-optimal policies. In contrast to prior work that assumes the reward is given, here we assume the reward is labelled by a learned function $r_{\\psi}$, which in our case is learned using preference labels provided by a VLM (more details in Section IV).\nWe use IQL [2] as the underlying offline RL algorithm for all our experiments, and we briefly review how it works. IQL predicts an upper expectile of the TD-target, which approximates the maximum of $r(s,a) + \\gamma Q_{\\theta}(s', a')$ over actions $a'$ constrained to the dataset actions. IQL trains a separate value and Q function. The value function $V_{\\phi}$ is trained to approximate an expectile purely with respect to the action distribution:\n$L_V(\\psi) = E_{(s,a)~D} [L_\\tau(Q_{\\theta}(s, a) - V_{\\phi}(s))],$ (3)\nwhere $L_\\tau(u) = |\\tau - I(u < 0)|u^2$. This value function is then used to train the Q-function:\n$L_Q(\\theta) = E_{(s,a,s')~D} [(r(s, a) + \\gamma V_{\\phi}(s') - Q_{\\theta}(s, a))^2] .$ (4)\nThe policy $\\pi_{\\varphi}$ is extracted from the Q function via advantage weighted regression (AWR) [35]:\n$L(\\phi) = E_{s,a~D} [exp(\\beta(Q_{\\theta}(s, a) - V_{\\phi}(s))) log \\pi_{\\varphi}(a | s)],$ (5)\nwhere $\\beta$ denotes an inverse temperature."}, {"title": "IV. SYSTEM", "content": "Our system, Offline RL-VLM-F, consists of two phases: the reward labeling phase, which is built on top of RL-VLM-F [1], and the policy learning phase, based on Implicit Q Learning [2]. Figure 1 provides an overview of the system. We describe each of the two phases in detail below.\nA. Reward labeling of the offline dataset\nIn the reward labeling phase, we aim to learn a reward model to label all of the transitions in the offline dataset. We employ RL-VLM-F [1] to perform this labeling. We assume access to a text description of the task and that the"}, {"title": "B. Policy Learning from the labelled dataset", "content": "In the policy learning phase, we first label the entire offline dataset using the learned reward model. From the labelled dataset, we then learn a policy using Implicit Q Learning."}, {"title": "C. Implementation Details", "content": "In the real world robot-assisted upper-body dressing task, we don't have access to the low-level states of the task for two reasons: 1) the cloth is highly deformable and does not have a known compact low-level state; 2) even if we manually define a low-level state as e.g., key points on the cloth, it is challenging to accurately estimate these key points reliably from sensory observations. Therefore, we choose to just represent the cloth, along with the human arm, using point clouds obtained from a depth camera, as was done in prior work [30]. We use this point cloud representation for both learning the reward model and the policy. Note that when querying the VLM, we still provide it with pairs of image observations, because the VLM was trained from images; however; after we store the preference labels from the VLM, we can learn the reward model using the corresponding point cloud observations. In simulation, we learn all the reward models from images, and the policy with low-level ground-truth states provided by the simulator. For the real-world dressing task, both the reward model"}, {"title": "V. EXPERIMENTS", "content": "1) Experiment setting: We first evaluate our method on the following four simulation environments. 1. Cartpole [38]: a classic control task where the goal is to balance a pole on a moving cart. 2. Open Drawer: an articulated object manipulation task from MetaWorld [39] where a Sawyer robot needs to pull out a drawer. 3. Soccer: A dynamic task from MetaWorld where a Sawyer robot must push a soccer ball into the goal. 4. Straighten Rope: a deformable object manipulation task from SoftGym [40] where the goal is to straighten a rope from a random configuration. See Fig. 2 for an illustration of these tasks.\nFor each of the environments, we test our method with three different kinds of dataset optimality: 1. Random: the transitions in the dataset are collected from a policy that just performs random actions; 2. Medium: the transitions in the dataset are collected from a partially trained RL policy with the ground-truth reward. Therefore, the dataset is of medium quality. 3. Expert: the transitions in the dataset are collected from an RL policy trained until convergence with the ground-truth task reward. The trained RL policy can solve the task; therefore this dataset is considered as the expert dataset. We note that there are no reward labels provided in any of these datasets.\nWe compare our method with the following baselines: 1. Simple behavior cloning (BC): we perform standard behavior cloning on the dataset to learn the policy. The policy is represented as a simple MLP. 2. GAIL [17]: an inverse RL algorithm that uses adversarial training to learn a cost and policy from the given demonstrations. 3. Diffusion Policy [24]: a state-of-the-art imitation learning algorithm that represents the policy as a denoising diffusion process. 4. IQL with average reward: this baseline labels all transitions in the dataset with the same reward, which is the average of the ground-truth reward of the dataset. It then performs IQL with the labelled dataset. Such a baseline has been proposed and used in prior work [41]. 5. IQL with ground-truth reward: this baseline performs IQL on the dataset, using ground-truth rewards. This baseline is an oracle and provides an upper bound for the performance. Among these baselines, Simple"}, {"title": "B. Real-World Robot-Assisted Dressing", "content": "1) Setup: We also test our method in a real-world robot-assisted dressing task, where the goal is for a Sawyer robot to dress one sleeve of a garment to a person's shoulder. As in prior work [30], [43], we assume the garment is already grasped and that the person remains static during the dressing process. To ensure a controlled comparison between methods, we use both a manikin and a robot arm (ViperX 300 S) as a substitute for a human limb being dressed. The rightmost two images in Fig. 2 illustrate the real-world setup. The offline dataset we use is collected from a prior human study in Wang et al. [30]. The policy used to collect the dataset is trained using reinforcement learning and policy distillation in simulation and then transferred to the real world. Due to sim2real gaps, the policy is not optimal in the dressing task, and thus the offline dataset is not optimal and contains failure trajectories. There are in total 485 trajectories, which corresponds to 26158 transitions in this offline dataset. As garments do not have a known compact state representation, the logged observation in the dataset is the segmented point cloud of the scene, which contains the garment point cloud, the arm point cloud, and a point that corresponds to the robot end-effector, which we use to train the reward and the policy. We randomly sampled 4000 image pairs to query the VLM for preference labels. The VLM we use for this real-world dressing task is GPT-4o.\nWe compare to a state-of-the-art behavior cloning base-line that takes point cloud as input: 3D diffusion policy (DP3) [25]. The evaluation metric is the arm dressed ratio, which is the ratio between the dressed distance on the arm and the total length of the arm. We test both methods in two scenarios: the first is dressing the manikin, which holds a \"L\" shape arm pose. The second is dressing the ViperX 300 S arm in a pose akin to a human stretching their forearm away from their torso. We note that both of these two arms are not present in the training set, which only includes data collected from real people. The shape and morphology of the manikin arm is similar to those of real people, and the shape and morphology of the ViperX 300 S arm is rather distinct from arms of real people. Each dressing trial stops after 60 steps, or when the sensed force on the Sawyer end-effector is larger than 7 Newtons. We test each method with 3 different garments, as shown in Fig. 3. For both the manikin and the ViperX 300 S arm, we run each method on each garment for 5 times and report the mean and standard deviation of the dressed ratios.\n2) Real-world results: The results are presented in Fig. 4 and Fig. 5. As shown, both methods perform well when dressing the manikin, achieving an average dressed ratio of 0.9. More interestingly, when dressing the ViperX 300 S arm, our method achieves a much higher average dressed ratio of 0.83 compared to 0.32 achieved by DP3. Fig. 6 shows the final dressed states of both methods in these two test cases. When testing DP3 with the ViperX arm, we notice that even though the elbow is extended outwards from the body, the DP3 policy always tries to move forward instead of following the direction of the forearm. As a result, it often gets the garment stuck on the forearm and cannot progress with dressing (see Fig. 6). This could be because the shape and morphology of the ViperX 300 S arm is rather distinct from arms of real people. This might explain why DP3 (behavior cloning) performs poorly in this case, as the offline dataset is not optimal, and behavior cloning (DP3) is known to generalize poor towards out-of-distribution cases. Instead of just imitating the trajectories in the offline dataset that include failure trajectories, our method performs offline RL and optimizes the true task objective of dressing the arm, leading to better performance. Videos of the dressing trials can be found on our project website. This demonstrates that our system can generate effective policies with sub-optimal, unlabeled dataset directly in the real world."}, {"title": "VI. CONCLUSIONS", "content": "In this paper, we propose Offline RL-VLM-F, a new system that enables automatic reward labeling and policy learning from unlabeled, sub-optimal offline datasets. We build on prior work RL-VLM-F to automatically generate reward labels for offline datasets using preference feedback from a vision-language model and a text description of the task. Our method then learns a policy using offline RL with the reward-labeled dataset. We test our system in a complex real-world robot-assisted dressing task, where we first learn a reward function using a vision-language model on a sub-optimal offline dataset, and then we use the learned reward to employ Implicit Q Learning to develop an effective dressing policy. Our method also performs well in simulation tasks involving the manipulation of rigid and deformable objects, outperforming baselines such as behavior cloning and inverse RL. In summary, we propose a new system that enables automatic reward labeling and policy learning from unlabeled, sub-optimal offline datasets."}]}