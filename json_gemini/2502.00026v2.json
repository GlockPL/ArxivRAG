{"title": "Pushing the Limits of BFP on Narrow Precision LLM Inference", "authors": ["Hui Wang", "Yuan Cheng", "Xiaomeng Han", "Zhengpeng Zhao", "Dawei Yang", "Zhe Jiang"], "abstract": "The substantial computational and memory demands of Large Language Models (LLMs) hinder their deployment. Block Floating Point (BFP) has proven effective in accelerating linear operations, a cornerstone of LLM workloads. However, as sequence lengths grow, nonlinear operations, such as Attention, increasingly become performance bottlenecks due to their quadratic computational complexity. These nonlinear operations are predominantly executed using inefficient floating-point formats, which renders the system challenging to optimize software efficiency and hardware overhead. In this paper, we delve into the limitations and potential of applying BFP to nonlinear operations. Given our findings, we introduce a hardware-software co-design framework (DB-Attn), including: (i) DBFP, an advanced BFP version, overcomes nonlinear operation challenges with a pivot-focus strategy for diverse data and an adaptive grouping strategy for flexible exponent sharing. (ii) DH-LUT, a novel lookup table algorithm dedicated to accelerating nonlinear operations with DBFP format. (iii) An RTL-level DBFP-based engine is implemented to support DB-Attn, applicable to FPGA and ASIC. Results show that DB-Attn provides significant performance improvements with negligible accuracy loss, achieving 74% GPU speedup on Softmax of LLaMA and 10x low-overhead performance improvement over SOTA designs.", "sections": [{"title": "Introduction", "content": "The noteworthy success of Large Language Models (LLMs) has revolutionized various fields of artificial intelligence. The emerging LLMs like LLaMA families (Meta 2024) and Mistral families (Jiang et al. 2023) continue to push the boundaries of what these models can achieve, promising even greater capabilities in natural language understanding, generation, and problem-solving across diverse domains. While LLMs have exhibited remarkable performance across a range of tasks, their inference process demands substantial computing power and memory bandwidth, severely hindering their application and implementation.\nVarious approaches have been explored for efficient large model deployment. While quantization (Lin et al. 2024; Ma et al. 2024) and pruning (Frantar and Alistarh 2023; Xia et al. 2023) reduce model size and complexity, they often suffer from accuracy degradation and complex post-training processes. Alternative numerical formats like BF16 and TF32 (Burgess et al. 2019; Choquette et al. 2021) improve efficiency but remain costly for large-scale inference. Block Floating-Point (BFP) (Drumond et al. 2018; Darvish Rouhani et al. 2020) offers a promising solution by sharing exponents within data blocks, providing the dynamic range for DNN inference with minimal hardware overhead.\nWhile BFP research in deep learning has primarily targeted linear operations, such as convolution and fully connected layers, nonlinear operations like Softmax and GELU still depend on floating-point computations, emerging as performance bottlenecks. For instance, Softmax alone consumes over 30% of LLM inference time (Stevens et al. 2021). Analysis of LLaMA3-8B shows that longer sequences lead to super-linear growth in memory and latency (Fig. 1), due to the quadratic complexity of Attention layers.\nIn this paper, we pioneer the application of BFP to nonlinear operations and identify three key challenges: 1 Outlier Sensitivity: accuracy degradation from exponent alignment with outliers. 2 Representation Granularity: uniform alignment of exponent drops the representation granularity, which leads to less accurate results. 3 Hardware Complexity: nonlinear operations involve complex logic (transcendental functions, division, etc.), complicating system optimization (detailed in Tab. 1). To this end, we propose: Dynamic-BFP (DBFP), an advanced variant of BFP, adeptly addresses the challenges of accuracy and efficiency in nonlinear operations. It incorporates two novel strategies: a pivot-focus strategy capable of conforming to various data distributions, and an adaptive grouping strategy that enables a more precise and flexible exponent sharing mechanism. DB-Attn, a DBFP-based framework with software-hardware co-optimization for efficient Attention computation. It accelerates Softmax without floating-point operations and streamlines the dataflow between linear (BFP Matmul) and nonlinear (DBFP Softmax) operations by sharing exponents, eliminating explicit conversions.\n\u2022 Algorithm. We present DH-LUT, a nonlinear operations dedicated lookup table (LUT) algorithm with DBFP format, achieving 74% speedup on the GPU for Softmax while maintaining comparable accuracy to floating-point.\n\u2022 Hardware. We design and implement an RTL-level DBFP-based engine applicable to FPGA and ASIC, delivering 10x throughput over SOTA designs."}, {"title": "Related Work", "content": "Data formats for LLMS\nAs LLMs grow in size and complexity, the standard 32-bit floating-point (FP32) format becomes less practical. Researchers develop low-bit formats such as BF16 (Jouppi et al. 2020) and TF32 (NVIDIA 2022) to address increasing memory and computational demands. Low-bit fixed-point data types convert operations to integer operations (e.g., INT4) by fixing the number of floating-point bits (Nagel et al. 2019; NVIDIA 2020; Mellempudi et al. 2017), risking accuracy drop with large dynamic ranges. BFP formats (Drumond et al. 2018) share exponents within data blocks, enabling efficient GEMM operations through dense integer logic with reduced hardware complexity. While (Song, Liu, and Wang 2018) explores various BFP grouping methods, they are limited by spatial constraints and ignore data distribution characteristics. Although BSFP (Lo, Lee, and Liu 2023) improves upon BFP, its complex hardware design presents implementation challenges. The MXFP format (Rouhani et al. 2023) represents another step towards efficient floating-point computations in deep learning.\nNonlinear Operation Algorithms\nNonlinear operations present unique challenges in efficient temporal memory utilization and computation, requiring multiple passes over input values held in memory (Kim et al. 2023). Calculation methods for these operations typically fall into two categories: LUT-based method (Zhang et al. 2023, 2022; Du et al. 2019) and approximation algorithms (Kim et al. 2021; Xia and Zhang 2023; Wang et al. 2018). While LUTs offer high accuracy, they demand substantial storage. Approximation, on the other hand, generally improves in accuracy with larger computational units."}, {"title": "Methodology", "content": "Dynamic Block Floating-Point\nHere, we introduce the DBFP, including its mathematical model and the corresponding optimization."}, {"title": "BFP Formulation.", "content": "Let F denotes the set of floating-point numbers, and x \u2208 F is represented as: x = (\u22121)^s \u2022 2^e \u2022 m where s \u2208 {0,1} is the sign bit, e \u2208 [e_{min}, e_{max}] is the exponent, and m \u2208 [1,2) is the mantissa. BFP numbers are formally defined as a group: Y = (y_1, y_2,..., y_n), where each y_i shares an exponent e_s, such that y_i = (\u22121)^{s_i} \u00b7 m_i2^{e_s}. BFP numbers can be partitioned into two fields: a shared field S and a private field P. Each s_i \u2208 S represents a shared exponent for multiple elements in P. Each p_i \u2208 P is of the form p_i = (\u22121)^{s_i} \u00b7 m_i.\nTo convert floating-point numbers to BFP, a uniform exponential alignment is applied as Eq.1:\nxi = (\u22121)si \u00b7 2ei \u2022 mi = (\u22121)si \u00b7 2ei\u2212dij. (mi\u00b72dij)\n= (\u22121) 2es mi\n\n, where dij = ei - es; denotes the difference between the exponent of xi and the jth shared exponent. This alignment introduces an error due to the finite precision of mantissa multiplication by 2^{d_{ij}} (shifting d_{ij} bits). The error depends on the distance d_{ij}, determined by the input exponent e_i. Therefore, selecting appropriate shared exponents sj, which are determined by many factors, is crucial to minimize dij.\nProblem Formulation: To this end, we formulate the problem as finding a adaptive grouping function f that partitions a set X of floating-point values into k subsets based on their characteristics: f : X \u2192 x {S_1, S_2, ..., S_k } where each subset S_j is determined within a frame of discernment \u03a9.\nEach subset Sj has a unique BFP representation Bj with shared and private fields. We then explore factors influencing BFP format accuracy, which define the function f."}, {"title": "Observations and Insights.", "content": "Through experimental and theoretical analysis, we explore the limitations of vanilla BFP, providing new insights for improving accuracy.\nObservation 1 Pivot-focus policy: Vanilla BFP formats typically align a group of xi (e.g., every 16 elements) to the maximum exponent within that group, leading to significant accuracy drop. Our experimental findings in Softmax show that setting the alignment direction to the default maximum causes up to 9.6x greater loss than the median one.\nInsight 1: This suggests that using maximum values for alignment is suboptimal. Instead, a more representative value (e.g., median) as an alignment pivot better preserves accuracy across the group.\nObservation 2 Adaptive grouping strategy: Prior work on BFP relied on fixed bounding boxes, which are particularly vulnerable to outliers. Outliers can cause disproportionate shifts in data exponents and lead to substantial accuracy drop. See Tab. 1 for a more detailed analysis.\nInsight 2: If elements with similar magnitude distributions can share exponents within a group, it can reduce the bit shifts (diminish the dij) for individual numbers.\nFig. 2 illustrates the difference between the mentioned formats: (a) represents floating-point data formats such as FP16 or TF32; (b) shows the vanilla BFP format; (c) highlights our DBFP, which employs automatic grouping based on the intervals in which the exponent falls."}, {"title": "Theoretical Analysis.", "content": "For the pivot-focus policy, we introduce directional vectors to characterize the process. Let sj denote the vector representing the direction and magnitude from the origin to the point sj in the vector space. The shift vector vij = (|sj| \u2013 ei)\u00fb determines the shift for each xi relative to the sharing exponent. m'i and mi denote the mantissa's projection before and after conversion, with their Euclidean distance as d^2_{ij} = ||m'i - mi||^2. Hij represents the confidence that xi falls within the interval defined by sj. It's aimed to minimize the following objective function:\narg min J_{DBFP} =\\Sigma_i\\Sigma_{S_j \\subset \\Omega}  \\mu_{ij} d^2_{ij} = \\Sigma_{i=1}^n  \\Sigma_{\\{j / S_j \\neq \\emptyset, S_j \\subset \\Omega\\}} \\mu_{ij} d^2_{ij}\n+ \\Sigma_{i=1}^n \\mu_{i\\emptyset} \\delta^2\n\ns.t. \\Sigma_{\\{j/S_j \\subseteq \\Omega\\}}  \\mu_{ij} + \\mu_{i\\emptyset} = 1, \\forall i=1, n,\nIn Eq. 2, hyperparameter \u03b2 (default 2) regulates pij's importance. An empty set mitigates outlier impact on sj selection, with pig as its confidence. The distance between outliers and sp is \u03b42, related to the distances of all Sj.\n\\delta^2, S_j = 0\nD = \\left\\{\n\\delta^2_{ij}, S_j = \\emptyset\nd_{ij}, S_j = 1\n\\right.\n\nTo minimize JDBFP, we alternately fix one of the variables pij or sj and solve the constrained minimization problem for the other. By introducing n Lagrange multipliers \u03bbi, the Lagrangian is expressed as:\nL(U, S, \u03bb_1,\u2026, \u03bb_n) = J_{DBFP}(U, S) -  \\Sigma_{i=1}^n\\Sigma_{S_j \\subseteq \\Omega} \\lambda_{ij}  \\mu_{ij} (4)\nBy differentiating the Lagrangian and setting each element of gradient VL to zero, specifically \\frac{\\theta \\mu_{ij}}{\\theta J_{DBFP}}, \\frac{\\theta S}{\\theta \\mu_{ij}}, \\frac{\\theta \u03bb}{\\theta J_{DBFP}}, we obtain pij, the necessary conditions for optimality:\n\\mu_{ij} = \\frac{d^{-2/(\u03b2-1)}}{\\Sigma_{k \\neq 0}d^{-2/(\u03b2-1)} + \\delta^{-2/(\u03b2-1)}}\nSimilarly, we can further calculate the s"}, {"title": "DB-Attn Algorithm Design", "content": "In this section, we introduce the algorithmic core of DB-Attn. Unlike the vanilla BFP format, which struggles with nonlinear operations, our proposed DH-LUT, for the first time, completes nonlinear operations in a BFP-like format. We also apply DBFP to linear operations, optimizing Matmul efficiency through cascade operations.\nOptimization for Softmax. The Softmax operation is among the most computationally intensive nonlinear components in Transformers, significantly impacting computational efficiency. Our optimization methodologies using DBFP offer a general approach to efficiently computing nonlinear functions, readily extensible to other nonlinear operations. The Softmax function converts Attention scores into a probability distribution. It can be represented as follows:\nSoftmax (xi) = \\frac{e^{xi}}{\\Sigma_{j=0}^{n} e^{xj}} =  \\frac{e^{xi-X_{max}}}{\\Sigma_{j=0}^{n} e^{xj-X_{max}}}\nwhich is transformed by the down-scaling exponentiation to smooth data distribution and prevent overflow.\nSoftmax's bottleneck stems from the low throughput of exponential functions. Leveraging DBFP's shared exponents, we propose Dynamic Hierarchical LUT (DH-LUT), a lightweight solution. DH-LUT uses a two-dimensional structure of sub-LUTs, dynamically loaded based on DBFP shared exponents and high k bits of mantissa. For Softmax's e^{x-X_{max}} operation, only the [-\u221e,0] range needs fitting, showing non-uniform distribution. DH-LUT adapts to this through pivot-focus policy and non-uniform fitting. We optimize accuracy and memory with a non-uniform hierarchical allocation method (Algo.1), adaptively partitioning to focus resources on nonlinear regions shown in Fig. 3.\nFor the Softmax based on DH-LUT, its computational error stems from the DBFP format and the mapping error of DH-LUT. Eq. 8 introduces the error analysis of vanilla BFP, which is applicable to DBFP. For a BFP block, using the"}, {"title": "Algorithm-driven Hardware Architecture", "content": "We design and implement an RTL-level engine for Softmax with DBFP and evaluate the hardware resources and throughput advantage. Benefiting from the DBFP, the proposed accelerator offers competitive performance with very light design complexity, making it easily adaptable to other general-purpose GPUs and NPUs. Below, We'll first detail the proposed architecture and implementation.\nOverall Architecture. With the alignment to the algorithms presented above, we split the architecture of the accelerator into four pipeline stages: Max finds the maximum value within the input vector; SE performs Shared Exponent calculation and maximum value subtraction; Exp computes exponents and sum using an adder tree; Div executes division operations to obtain the result.\nThe SE stage segments and aligns the exponent of the input vector with the MAX stage's maximum value, followed by subtraction (Fig. 4 a). Simultaneously, it checks if the DH-LUT's current data exponent matches the required to-be-shared exponent, signalling DMA (Direct Memory Access, a module allowing hardware subsystems to access memory for efficient data transfer between memory and devices) to preload new data required for the following computations (Fig. 46). These initial stages prepare for computation. The subsequent three phases complete the exponential function (Fig. 4), denominator summation (Fig. 4 d), and division (Fig. 4 e the main operations in Softmax."}, {"title": "Low-bit Storage Implementation of DBFP.", "content": "Our dynamic hierarchical non-uniform LUT strategy introduced in the previous section enables a compact yet flexible hardware implementation. By extracting n bits from vector elements, we create a 2^n-entry table that balances accuracy and size. This compact design is suitable for Softmax computation, which requires global input information. Our approach utilizes two tables: a value table storing exp values for approximation and a hit bitmap table recording mantissa occurrences (Fig. 4). Input vectors perform lookups in parallel, with each exponent index hitting a DH-LUT interval, setting a corresponding bitmap bit. After recording the input vector, we multiply and sum values from both tables using an adder tree structure. This achieves parallel lookup results without extra hardware resources, leveraging the compact table size.\nEfficient DBFP Computing. DBFP's ability to convert floating-point operations into integer operations is a key advantage. By sharing exponents, most floating-point computations simplify basic exponent operations combined with integer mantissa operations, streamlining arithmetic calculations. Vector addition in neural networks exemplifies this efficiency. Traditional floating-point addition requires aligning exponents for each number pair. DB-Attn pre-aligns exponents within groups, reducing the operation to exponential multiplication with mantissa addition. For cascading structures like adder trees (Fig. 4d), a single initial exponent alignment allows direct mantissa calculations, yielding substantial benefits. Fig. 4(d) demonstrates an experimentally proven 42% latency reduction in a 4-level adder tree.\nDivision operations in Softmax and Layernorm often limit parallelism. DBFP's shared exponents enable efficient parallel integer division approximation (Fig.4e), reducing la-"}, {"title": "Evaluation", "content": "Baselines. As described, DB-Attn involves both software and hardware implementation. Hence, we examined DB-Attn against the SOTA work in both worlds. For the software, we compare DB-Attn with FP32, FP16, vanilla BFP, and FP8, focusing on the tradeoffs between accuracy and precision. To measure the improvement DBFP brings to the hardware, we compare its hardware metrics with SOTA Softmax acceleration architectures: Hyft (Xia and Zhang 2023), TCAS-I'22 (Zhang et al. 2023), ISCAS'23 (Koca, Do, and Chang 2023) and TCAS-II'22 (S 2021).\nModels and Datasets. We evaluate DB-Attn on both LLM and Vision models. For LLMs, we test on LLaMA-7B, LLaMA2-7B, and LLaMA3-8B (Touvron et al. 2023a,b; Meta 2024), using WikiText2 and C4 datasets for perplexity comparison. Zero-shot accuracy is evaluated on PIQA, ARC, BoolQ, HellaSwag, and Winogrande tasks. For Vision tasks, we test image classification using ViT and Swin (Dosovitskiy et al. 2020; Liu et al. 2021) on ImageNet, and object detection using Detr on COCO dataset.\nImplementations. We implement DB-Attn on NVIDIA"}, {"title": "Accuracy Results of DB-Attn", "content": "LLM Tasks. We examine the accuracy of DB-Attn on the language generation task and six zero-shot tasks on LLaMA LLMs, comparing it against vanilla BFP and FP8 format. Tab. 1 presents the perplexity and zero-shot accuracy of LLMs. Wherein FP8 e4m3-S denotes the use of scaling factor (the maximum value that FP8 e4m3 can represent) to rescale the value within the representable range of FP8 e4m3. It can be seen in Tab. 1 that the direct application of vanilla BFP format to Softmax operations leads to substantial accuracy drop, with LLaMA3's average accuracy on zero-shot tasks decreasing by 33.81%. Similarly, FP8 e4m3 is inadequate for Attention calculations due to its inability to represent infinity. DB-Attn outperforms both vanilla BFP and FP8 formats across all evaluated tasks, maintaining nearly the same accuracy as floating-point. These results unequivocally demonstrate the comprehensive excellence of DB-Attn in maintaining model performance while potentially reducing computational overhead.\nVision Tasks. We assess tasks of object detection and image classification (Tab. 2). It is seen that DB-Attn's performance is similar to results on LLMs. DB-Attn can be losslessly integrated into existing Transformer models, showing its generalization and versatility across various distributions.\nPrecision-to-Accuracy Pareto Frontier. We test the computational error, LUT memory usage, and actual model accuracy of Softmax in DB-Attn under different LUT bit widths. To find the Pareto frontier of the LUT bit width configuration, we visualize some results in Fig. 5(a) and (b) when the LUT bit width is 5-7, a balance is achieved among computational error, memory usage, and accuracy."}, {"title": "Hardware Implement Evaluation", "content": "As BFP has been validated on linear operations in previous work (Yeh et al. 2022; Zou et al. 2024), we mainly focus on the performance of hardware deployments for the yet to be well optimized nonlinear operation Softmax.\nDBFP GPU Run-time Analysis. We implement a custom CUDA Softmax operator to emulate DBFP formats on NVIDIA A800 GPU. This operator replaces the Softmax in various models (e.g., LLaMA and ViT) and performs inference. Results in Fig. 5(c) demonstrate that DBFP-based Softmax consistently achieves speed improvements of at least 30% across diverse model architectures. Notably, on the LLaMA series, we reduced latency by 74% on average.\nDBFP Hardware Implement on FPGA. We evaluate designs against SOTA based on Softmax processing latency,"}, {"title": "FPGA resource utilization (LUT and FF), maximum operating frequency, and FOM (a comprehensive metric).", "content": "FOM =  \\frac{F_{max} N X W}{LUT + FF},\n, where W and N denote the precision and numbers of the inputs. A higher FOM value indicates better performance.\nTab. 3 shows our comparison with multiple SOTA designs, including Xilinx FP IP (Koca, Do, and Chang 2023) baseline. While existing Softmax accelerators only support input bandwidths under 16, our design uniquely accommodates the larger bandwidths needed for modern LLMs. Testing with 1024-length sequences, our implementation achieves 54.21% less resource usage while operating at higher frequencies, reduces processing latency by 62.5%, and delivers 128x higher computational bandwidth. Notably, it shows a 10x FOM improvement over ISCAS'23 SOTA, demonstrating the significant potential of our approach for nonlinear operation hardware units.\nDesign Evaluation on ASIC. ASIC implementations can be supplemented with more accurate data on power consumption, maximum clock frequency, and scalability for high-volume applications. We evaluate the hardware design on ASIC using four key metrics: Area-Delay-Product (ADP, Area \u00d7 Latency), Energy-Delay-Product (EDP, Energy \u00d7 Latency), and Throughput (Freq \u00d7 Bandwidth).\nFor scenarios with a uniform input sequence length of 1024, we normalized the experimental results to 28nm. Fig. 5 (d) compares our design's normalized PPA (Power, Performance, and Area) metrics with SOTA designs (Cardarilli et al. 2021; Kouretas and Paliouras 2020). Our design is capable of handling large bandwidth and long input sequences. Hence, there is an average 10% increase in area"}, {"title": "Hardware Scalability.", "content": "The design keeps scalability in mind. To demonstrate this feature, we conducted tuning tests with input sizes ranging from 8 to 4096 elements. Fig. 6 shows the total latency and each part of the processing pipeline. The total computation time grows exponentially with input size due to the quadratic relationship between input length and the size of the processed matrix. The scaled histogram in Fig. 6 shows the balanced growth in the proportion of time consumed by each pipeline level as the input size increases. No single component shows disproportionate latency growth; instead, pipeline allocation becomes more balanced with longer sequences. These observations evidenced the parallelism and scalability of the design."}, {"title": "Conclusion", "content": "We present DBFP, an enhanced BFP variant optimizing nonlinear operations. Our DB-Attn framework, enables efficient Attention, advancing narrow-precision LLM inference.\nLessons learned. Different from the conventional solutions that attempted to optimize nonlinear computation solely through hardware design or software techniques, this work shows that using an algorithm/hardware co-designed approach (DB-Attn), computation latency and memory accesses can be largely improved with light overhead. The construction of DBFP and the algorithm-driven hardware provide key insights and effective means that foster a collaborative environment of hardware and the algorithm for LLMs research that neither discipline could achieve independently."}]}