{"title": "AdvAnchor: Enhancing Diffusion Model Unlearning with Adversarial Anchors", "authors": ["Mengnan Zhao", "Lihe Zhang", "Xingyi Yang", "Tianhang Zheng", "Baocai Yin"], "abstract": "Security concerns surrounding text-to-image diffusion models have driven researchers to unlearn inappropriate concepts through fine-tuning. Recent fine-tuning methods typically align the prediction distributions of unsafe prompts with those of predefined text anchors. However, these techniques exhibit a considerable performance trade-off between eliminating undesirable concepts and preserving other concepts. In this paper, we systematically analyze the impact of diverse text anchors on unlearning performance. Guided by this analysis, we propose AdvAnchor, a novel approach that generates adversarial anchors to alleviate the trade-off issue. These adversarial anchors are crafted to closely resemble the embeddings of undesirable concepts to maintain overall model performance, while selectively excluding defining attributes of these concepts for effective erasure. Extensive experiments demonstrate that AdvAnchor outperforms state-of-the-art methods. Our code is publicly available at https://anonymous.4open.science/r/AdvAnchor.", "sections": [{"title": "1. Introduction", "content": "Text-guided diffusion models (DMs) have garnered significant interest in the research community for their ability to generate high-fidelity images [14, 39] and their widespread applications like medical image reconstruction [20, 42], creative arts [24], and material generation [2, 22, 43]. However, these models also encounter critical security concerns, such as harmful content generation [30] and potential copyright infringement [34]. Furthermore, retraining safe DMs is challenging due to the extensive data cleaning, the high training resource consumption, and the unpredictable effects of training data on model predictions.\nTo remove unsafe behaviors from DMs without retraining, researchers are increasingly exploring Machine Unlearning (MU) techniques. A prominent MU approach directly fine-tunes the weights of pre-trained DMs [8], often by minimizing the prediction differences between text anchors (pre-defined target prompts) and prompts containing unsafe concepts [9, 44]. For instance, to erase the 'Van Gogh' style, Abconcept [23] adjusts the cross-attention module weights in DMs to enhance prediction consistency between predefined prompt pairs, such as \"A picture of a painting\" and \"A picture of a Van Gogh's painting\"."}, {"title": "3. Proposed method", "content": "In this section, we first explore the impact of anchors on DM unlearning. Then, we introduce the proposed AdvAnchor."}, {"title": "3.1. Impact of various anchors on DM unlearning", "content": "We follow previous unlearning approaches [8, 21] to conduct this research, i.e., the undesirable concept cu is erased from DMs by minimizing the prediction difference between Pu and Panchor. Here, pu is a prompt containing Cu and Panchor denotes a target prompt.\n\\begin{equation}\n\\min_{\\theta_{o\\rho}} L_{op} = || f_{de} (x_t, e_{pu}; \\Theta_{op}) - f_{de}(x_t, e_{anchor}; \\Theta_{ori})||_2,  (1)\n\\end{equation}\nwhere op denotes the optimizable model weights for un-learning. fde() means the denoiser in Fig. 2. \u00e6t represents the latent representations of inputs at the timestamp t, which can be obtained through either the diffusion process [8] or the sampling process [23]. epu and eanchor are text embeddings of prompts pu and Panchor, respectively. Oori refers to the fixed weights of original models. ||. ||2 is the l2 norm."}, {"title": "3.1.1. Anchors", "content": "We use various types of pu and Panchor, detailed as follows:\n1) Each anchor is designated as a word, denoted poor, with pu = Cu. ChatGPT [1] is used to determine the anchor for Cu, with potential input prompts structured as follows:\n*   The closest type to Cu;\n*   The type imitated by cu;\n*   The parent class of cu;\n*   The type least similar to cu.\n2) We embed poor and cu into a masked sentence to construct por and pu, respectively. For instance, when removing an artist style from DMS, por and pu can be denoted as \u201cA picture of a {panchor}'s painting\u201d and \u201cA picture of a {cu}'s painting\u201d, respectively.\n3) We prepend a same long sentence to pwordor pword and Cu, to construct plonger and pu, respectively. For example, when erasing the 'Van Gogh' style, the prompt for ChatGPT to generate this long sentence might be:\n    Provide a long sentence that describes an artistic style except for 'Van Gogh'.\n4) We utilize a descriptive sentence as an anchor, called as paeshor, and Pu = Cu. For instance, when erasing an object category, the prompt for ChatGPT may be\ndesc_1.\nPaeshor: Provide a sentence that describes the morphological features of panchor This sentence should include the common attributes between pdecorator and Cu.\npes: {pdescriptor, while excluding defining features of cu."}, {"title": "3.1.2. Settings", "content": "Evaluation metrics. 1) We use Fr\u00e9chet Inception Distance (FID) [13] to measure the distance between images generated with the undesirable concept and the corresponding anchor; 2) Accuracy (ACC): For object classification, we use a pre-trained ResNet50 [12]. For style classification, we fine-tune the fully connected layer of a pre-trained ResNet18 [12] on a dataset generated by original DMs. This dataset includes a blank prompt and nine artist styles: Cezanne, Van Gogh, Picasso, Jackson Pollock, Caravaggio, Keith Haring, Kelly McKernan, Tyler Edlin, and Kilian Eng.\nEvaluation data. 1) Erasure: Generate 1,000 images per undesirable concept, with 200 seeds per concept and 5 images per seed. 2) Object preservation: Generate 1,859 images with prompts from 1,000 categories in the ImageNet dataset\u00b9. 3) Style preservation: Generate 2,000 images with the retained styles as prompts (excluding the erased one), with 50 seeds per style and 5 images per seed.\nOthers. Under identical settings, models from multiple fine-tuning processes exhibit notable performance variation."}, {"title": "3.1.3. Observations", "content": "The observations are as follows:\n*   -01 A higher similarity between pword and Cu often results in better preservation performance, as shown in Fig. 3.\n*   -02 A longer shared sentence between Panchor and pu generally leads to better preservation performance, as demonstrated in Fig. 4, which compares the results for Panchor set to parador, maskor, and Panchor plong.\n*   -03 Unlearning with pesor usually achieves superior erasure and preservation performance than other variants, as illustrated in Fig. 5.\nAdditional experiments supporting these observations are provided in the Appendix. We can conclude that:\n*   To preserve overall model performance, eanchor and epu should retain high similarity (Observations ol and o2).\n*   Undesirable concepts can be erased by excluding their defining attributes from anchors (Observation 03)."}, {"title": "3.2. Proposed AdvAnchor", "content": "Inspired by prior studies that tiny adversarial perturbations can significantly affect model predictions [28, 41], and by our conclusions, we propose AdvAnchor to generate adversarial anchors for DM unlearning.\nSpecifically, as the text space is discrete, AdvAnchor fine-tunes the undesirable concept Cu in the embedding space to generate adversarial anchors eadv The i-th feature element of eadvanchoris expressed as\n\\begin{equation}\ne_{anchor, i}^{adv}=\\begin{cases}\n e_{pu,i} + C_{adv} & \\text{if } e_{pu,i} = e_u\\\\\n e_{pu, i} & \\text{otherwise},\n\\end{cases}\n\\end{equation}\nWhere epu and eu denote the text embeddings of pu and Cu, respectively. Since eu remains fixed during erasure, we designate eadv as a universal perturbation.\nTo exclude defining attributes of undesirable concepts from eanchor, given the ground truth images \u00e6gt of undesirable concepts and the random noise z, we produce eadv by\n\\begin{equation}\n\\max_{e_{adv}} || f_{DM} (z, e_{anchor}^{adv}; \\theta_{ori}) - X_{gt}||_2,\n\\end{equation}\nWhere fDM denotes a pre-trained DM with fixed weights Oori. Eq. (2) aims to degrade the generation quality of undesirable concepts across any visual input. To influence the full denoising process with eadv, we reformulate Eq. (2) as:\n\\begin{equation}\n\\max_{e_{adv}}[L_{adv} (f_{de} (x_t, e_{anchor}^{adv}; \\theta_{ori}), f_{de} (x_t,e_{pu}; \\theta_{ori}))],\n\\end{equation}\nwhere fde (xt, epu; dori) acts as the pseudo target. We design two functions for Lady: Ladv1, which measures the cosine similarity between model predictions for eador and epu,\n\\begin{equation}\n\\max_{e_{adv}}[- cos(f_{de}(x_t, e_{anchor}^{adv}, \\Theta_{ori}), f_{de} (x_t, e_{pu}, \\Theta_{ori}))],\n\\end{equation}"}, {"title": "4. Experiments", "content": "4.1. Experimental Details\nFollowing existing works [8, 21], we conduct experiments using Stable Diffusion [33]. By default, we utilize the stable-diffusion-v-1-4 version. The Adam optimizer is used with a learning rate of 1e-5 for optimizing Oop, and 1e-4 for adjusting eadv. For constructing adversarial anchors, we apply the alternating optimization strategy with a maximum of 30 iterations (S = 30). For DM unlearning, only the cross-attention module weights are fine-tuned, with the un-learning step set to 50, requiring two RTX 3090 GPUs. \u03bb is set to 10. The evaluation metrics include FID, ACC, and Learned Perceptual Image Patch Similarity (LPIPS) [45].\n4.2. Unlearning Evaluation\nFor style and object unlearning, we produce 250 images per evaluation concept, utilizing 50 seeds per concept and generating 5 images per seed.\nStyle unlearning. We evaluate various unlearning methods on nine artist styles described in Section 3.1.2, erasing each style individually and assessing preservation performance across the remaining styles."}, {"title": "5. Conclusions", "content": "In this paper, we address the performance trade-off issue in DM unlearning by exploring the influence of anchor selection. Our analysis reveals that ideal anchors should exclude defining attributes specific to undesirable concepts while remaining close to these concepts. To this end, we propose AdvAnchor, which yields adversarial anchors using specifically designed loss constraints and optimization strategies. Experimental results demonstrate that AdvAnchor effectively removes undesirable concepts while preserving the generative quality of DMs for retained concepts."}]}