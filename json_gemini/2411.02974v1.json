{"title": "Region-Guided Attack on the Segment Anything Model\n(SAM)", "authors": ["Xiaoliang Liu", "Furao Shen", "Jian Zhao"], "abstract": "The Segment Anything Model (SAM) is a cornerstone of image segmentation, demonstrating exceptional performance across various applications, particularly in autonomous driving and medical imaging, where precise segmentation is crucial. However, SAM is vulnerable to adversarial attacks that can significantly impair its functionality through minor input perturbations. Traditional techniques, such as FGSM and PGD, are often ineffective in segmentation tasks due to their reliance on global perturbations that overlook spatial nuances. Recent methods like Attack-SAM-K and UAD have begun to address these challenges, but they frequently depend on external cues and do not fully leverage the structural interdependencies within segmentation processes. This limitation underscores the need for a novel adversarial strategy that exploits the unique characteristics of segmentation tasks. In response, we introduce the Region-Guided Attack (RGA), designed specifically for SAM. RGA utilizes a Region-Guided Map (RGM) to manipulate segmented regions, enabling targeted perturbations that fragment large segments and expand smaller ones, resulting in erroneous outputs from SAM. Our experiments demonstrate that RGA achieves high success rates in both white-box and black-box scenarios, emphasizing the need for robust defenses against such sophisticated attacks. RGA not only reveals SAM's vulnerabilities but also lays the groundwork for developing more resilient defenses against adversarial threats in image segmentation.", "sections": [{"title": "1. Introduction", "content": "The Segment Anything Model (SAM) [1] has emerged as a leading solution in\nimage segmentation, demonstrating remarkable adaptability and performance across\ndiverse datasets and prompts. Its architecture allows for seamless integration with var-\nious inputs, making it a pivotal tool for applications ranging from autonomous driv-\ning [2, 3] to medical imaging [4, 5]. However, this versatility also exposes SAM to\nvulnerabilities, particularly from adversarial attacks that can significantly degrade its\nperformance [6, 7]. These attacks leverage subtle perturbations in the input data, mis-\nleading the model into producing incorrect segmentations, thereby raising concerns\nabout the reliability of SAM in critical contexts.\nPrevious adversarial attack methods, such as the Fast Gradient Sign Method (FGSM) [8]\nand Projected Gradient Descent (PGD) [9], primarily focus on classification models\nand often utilize global perturbations that affect the entire input. These methods can be\nless effective in segmentation tasks, where spatial relationships and contextual informa-\ntion are critical. Recent approaches like Attack-SAM-K [6] and UAD [7] have begun\nto explore the unique challenges associated with adversarial attacks on segmentation\nmodels, but many still rely on external prompts or do not fully exploit the structural\ndependencies inherent in the segmentation process.\nTo effectively address these vulnerabilities, we propose the Region-Guided Attack\n(RGA), a novel adversarial attack strategy specifically designed for SAM. Unlike tradi-\ntional adversarial methods that often rely on external prompts or global perturbations,\nRGA focuses on manipulating segmented regions directly through a Region-Guided\nMap (RGM). This approach allows for targeted adversarial perturbations that divide\nlarge segments into smaller fragments while merging smaller regions into larger areas,\nultimately leading to misclassifications in SAM's outputs. The innovation of RGA lies\nin its ability to exploit the structural dependencies within the segmentation task, lever-\naging the inherent characteristics of SAM to enhance the effectiveness of the attack.\nThe significance of RGA is twofold. First, it provides a deeper understanding of\nthe vulnerabilities inherent in advanced segmentation models like SAM, offering in-\nsights into how adversarial perturbations can be crafted more strategically. Second,"}, {"title": "2. Related Works", "content": null}, {"title": "2.1. Segmentation Models", "content": "Image segmentation is a critical task in computer vision, aiming to partition an\nimage into meaningful segments or objects at the pixel level. Traditional approaches\nrelied heavily on hand-crafted features and were limited in handling complex scenes.\nThe advent of deep learning revolutionized segmentation tasks with models like Fully\nConvolutional Networks (FCNs) [10], which replaced fully connected layers with con-\nvolutional ones to maintain spatial information.\nBuilding upon FCNs, the U-Net architecture [11] introduced an encoder-decoder\nstructure with skip connections, enabling precise localization and context assimilation,\nparticularly in biomedical image segmentation. DeepLab models [12] further enhanced\nsegmentation by incorporating atrous convolution and conditional random fields for\ncapturing multi-scale context.\nThe SAM, introduced by Meta AI in 2023 [1], represents a significant leap in seg-\nmentation models. SAM is designed as a promptable segmentation system that can\ngenerate high-quality object masks from user input prompts, such as points, boxes, or\ntext descriptions. Trained on a massive dataset of over one billion masks, SAM demon-\nstrates remarkable generalization across diverse image distributions and tasks without\nthe need for additional training.\nSAM's architecture comprises three main components: an image encoder, a flexi-\nble prompt encoder, and a fast mask decoder. The image encoder processes the input\nimage to produce an embedding, the prompt encoder transforms user prompts into em-\nbeddings, and the mask decoder combines these embeddings to generate segmentation\nmasks. This design allows SAM to perform zero-shot generalization to new tasks and\nimage domains, making it a foundational model for various segmentation applications.\nSubsequent research has focused on adapting SAM to specific domains and improv-\ning its efficiency. For instance, efforts have been made to fine-tune SAM for medical\nimage segmentation, where domain-specific features are crucial [13]. Other studies\nexplore integrating SAM with text-based prompts to enhance interactive segmentation\ncapabilities [14]."}, {"title": "2.2. Adversarial Attacks", "content": "Adversarial attacks deliberately manipulate input data to deceive machine learn-\ning models into making incorrect predictions. Initially studied in image classifica-\ntion [15, 8], these attacks exploit the vulnerability of deep neural networks to small,\nimperceptible perturbations.\nThe Fast Gradient Sign Method (FGSM) [8] was one of the first techniques intro-\nduced to generate adversarial examples by performing a one-step gradient update along\nthe direction of the gradient's sign of the loss function with respect to the input image.\nIts iterative variant, the Basic Iterative Method (BIM) or I-FGSM [16], applies FGSM\nmultiple times with smaller step sizes, increasing the attack's success rate.\nThe Projected Gradient Descent (PGD) [9] attack extends BIM by adding random\ninitialization within the allowed perturbation radius and projecting the adversarial ex-\nample back onto the feasible set after each iteration. PGD is considered a universal\n\"first-order adversary\" and is widely used due to its effectiveness in finding robust ad-\nversarial examples.\nTo enhance the effectiveness and transferability of adversarial examples, several\nadvanced methods build upon FGSM, BIM, and PGD:\nMomentum Iterative Fast Gradient Sign Method (MIM) [17]: Incorporates a mo-\nmentum term into the iterative attack process, stabilizing update directions and improv-\ning transferability against different models.\nDiverse Input Iterative Fast Gradient Sign Method (DIM) [18]: DIM increases the\ndiversity of adversarial examples by applying random transformations, such as image\nresizing and padding, to the input image at each iteration. This randomization helps\nadversarial perturbations remain effective across models that process inputs of varying\ndimensions or padding schemes.\nTranslation-Invariant Iterative Fast Gradient Sign Method (TIM) [19]: Crafts per-\nturbations invariant to image translations by convolving the gradient with a predefined\nkernel, increasing transferability across models with different architectures and training\ndata.\nScale-Invariant Iterative Fast Gradient Sign Method (SIM) [20]: Averages gradients\nover multiple scaled copies of the input image, capturing scale variations and making"}, {"title": "2.3. Adversarial Attacks on Segmentation Models", "content": "Recent advancements in adversarial attacks on segmentation models have intro-\nduced several methods that challenge model robustness and expose vulnerabilities in\ncomplex feature extraction processes. Attack-SAM-K (ASK) [6] employs a global re-\nduction of feature responses by utilizing K point prompts distributed across the entire\nimage, with K often set to a large value such as 400. This approach is designed to ma-\nnipulate the SAM model's segmentation responses on a broad scale, directly targeting\nits feature extraction pipeline.\nTransferable Adversarial Perturbations (TAP) [23] introduces perturbations that\npush adversarial features away from original features using Minkowski distance. By\nfocusing on perturbation transferability, TAP highlights cross-model vulnerabilities,\nmaking it effective across various model architectures. Building upon TAP, Intermediate-\nLevel Perturbation Decay (ILPD) [24] refines this approach by maintaining an ef-\nfective adversarial direction with an increased perturbation magnitude. ILPD targets\nintermediate-level features, thereby testing the resilience of models at deeper feature\nlayers.\nAnother method, Activation Attack (AA) [25], minimizes the distance between ad-\nversarial features and target image features. By manipulating the activation layers,"}, {"title": "3. Preliminary", "content": "In this section, we establish the foundational concepts and framework for under-\nstanding the SAM and its susceptibility to adversarial attacks."}, {"title": "3.1. Architecture of the Segment Anything Model", "content": "The SAM is a promptable segmentation model designed to handle a wide range\nof image segmentation tasks. Its architecture comprises three primary components, as\nillustrated in the Figure 1:\nImage Encoder $f_{\\theta_{I}}$: The image encoder processes the input image x and extracts\nhigh-dimensional feature representations. These feature embeddings encapsulate es-\nsential visual details required for accurate segmentation.\nPrompt Encoder $h_{\\theta_{P}}$: SAM accepts various forms of prompts, such as points, bound-\ning boxes, and textual descriptions. The prompt encoder transforms these user-defined\nprompts P into embeddings that guide the segmentation process, enabling SAM to\ntackle diverse tasks without necessitating retraining.\nMask Decoder $g_{\\theta_{M}}$: The mask decoder integrates the outputs from the image en-\ncoder and prompt encoder to generate the final segmentation mask. This lightweight\ncomponent ensures efficient and rapid mask generation.\nSAM's ability to adapt to different types of inputs and tasks makes it a powerful\ntool, but it also exposes the model to adversarial vulnerabilities. The model's reliance\non both image and prompt encoders to generate masks means that small perturbations\nto the input image or the prompt embeddings can lead to significant segmentation er-\nrors."}, {"title": "3.2. Adversarial Attacks in Segmentation", "content": "Adversarial attacks in segmentation involve generating small, imperceptible pertur-\nbations to the input image that mislead the model into producing incorrect segmentation\nresults. These attacks, originally studied in classification tasks, have been extended to\nsegmentation models. Common methods like FGSM [8] and PGD [9] target pixel-level\npredictions, making them applicable in segmentation tasks."}, {"title": "3.3. Challenges in Adversarial Attacks on SAM", "content": "Adversarial attacks on SAM present unique challenges compared to standard seg-\nmentation models:\n\u2022 Prompt-Agnostic Attacks: SAM's versatility in handling different types of\nprompts makes it challenging to design adversarial examples that generalize\nacross various prompts. Unlike traditional segmentation models, where the in-\nput is static, SAM's output depends on the specific prompts provided by the user,\nincreasing the complexity of the attack.\n\u2022 Transferability: Attacks designed for SAM must also transfer effectively across\ndifferent segmentation models, including those with varying architectures and\nprompt types. Many existing attacks fail to generalize across different models,\nlimiting their practical impact."}, {"title": "4. Approach", "content": "In this section, we present the details of our proposed Region-Guided Attack (RGA)\ntargeting the Segment Anything Model (SAM). The method is designed to manipulate\nSAM's segmentation capabilities by systematically altering how the model interprets\nregions within an image. By focusing on both large and small segmented areas, we\ncan induce errors in SAM's output with minimal and imperceptible perturbations to\nthe input image. The RGA framework leverages SAM's prompt-based segmentation\narchitecture to achieve targeted adversarial attacks while maintaining a high degree of\ntransferability to other segmentation models."}, {"title": "4.1. Overview", "content": "The Region-Guided Attack (RGA) targets the SAM by manipulating how it seg-\nments image regions, causing segmentation errors through strategic adversarial pertur-\nbations. The attack process involves querying SAM for an initial segmentation, gen-"}, {"title": "4.2. Problem Formulation", "content": "The goal of the RGA is to generate adversarial perturbations for an input image\nx such that the Segment Anything Model (SAM), denoted as $f_{sam}$, produces incorrect\nsegmentation results that align with a predefined guidance map. Given an input image\nx and SAM $f_{sam}$, we want to generate a perturbation $\\delta$ such that when applied to the\nimage x, the perturbed image $x + \\delta$ leads SAM to produce a segmentation output that\ndeviates significantly from the original segmentation output, as defined by the guidance\nmap c. The aim is to guide SAM into splitting large segments into smaller fragments\nand merging smaller regions into larger ones, ultimately compromising segmentation\naccuracy. The optimization objective can be described as follows:\n$\\delta = argmix_{\\delta, ||\\delta||_{\\infty} \\le \\epsilon} \\mathbb{E}_{\\theta_{I},\\theta_{P},\\theta_{M}} [IoU(g_{\\theta_{M}}(f_{\\theta_{I}}(x + \\delta), h_{\\theta_{P}}(P)), g_{\\theta_{M}} (f_{\\theta_{I}}(x), h_{\\theta_{P}}(P)))] $  (1)\nwhere $\\delta$ represents the adversarial perturbation applied to the image x, $\\epsilon$ denotes the\nperturbation bound that ensures the perturbation remains imperceptible to human ob-\nservers, $f_{\\theta_{I}}$ is the image encoder function used in SAM to generate feature embeddings,\n$F_{2d}(\u00b7)$ refers to the 2D transformation function that augments the perturbed image to in-\ncrease its diversity, and c is the guidance map generated using the SAD strategy, which\nserves as a target for directing the segmentation output towards incorrect boundaries.\nThe goal is to find an adversarial perturbation $\\delta$ that maximizes the loss function\nL, which measures the divergence between SAM's original segmentation and the tar-\nget segmentation defined by c. This ensures that the segmentation output is altered\naccording to the desired adversarial effect."}, {"title": "4.3. Segmentation and Dilation Strategy", "content": "In this work, we introduce the Segmentation and Dilation (SAD) strategy for ef-\nficiently processing binary mask arrays and applying random colorization based on\nthe size of the segmented regions. This strategy addresses varying scales of seg-\nmented areas through two distinct approaches: grid-based segmentation for large\nregions and dilation-based enhancement for smaller regions, ensuring accurate and\nnon-overlapping color application across the mask."}, {"title": "4.4. 2D Transformations", "content": "Previous research has shown that increasing the diversity of input images during the\ngeneration of adversarial examples enhances both the effectiveness and transferability\nof the attacks. Techniques such as DIM [18], TIM [19], SIM [20], and RSTAM [21]\nhave introduced methods to manipulate input images, enabling adversarial examples to\ngeneralize across different models and tasks.\nIn this work, we implement the Random Similarity Transformation Strategy from\nRSTAM [21, 22], which applies random translations, rotations, and scaling to input\nimages. This method ensures that the generated perturbations remain effective against\ngeometric transformations. Additionally, we adopt the Scale-Invariant Strategy from\nSIM [20], which averages gradients across multiple scaled versions of the image. By\nintegrating these strategies, we improve the robustness and generalization of our adver-\nsarial examples, enhancing their transferability across various models and real-world\napplications."}, {"title": "4.5. Loss Function", "content": "To guide the adversarial attack, we define the following loss function L, which\naims to reduce the similarity between the adversarial image and the source image while\nencouraging its similarity to a guidance map:\n$L =  \\frac{<y_{a}, y_{s}>}{||y_{a}||^{2}||y_{s}||^{2}} - \\lambda \\frac{<y_{a}, y_{g}>}{||y_{a}||^{2}. ||y_{g}||^{2}}$ (6)"}, {"title": "5. Experiments", "content": null}, {"title": "5.1. Experiment Settings", "content": "Datasets: For evaluation, we selected the SAM-1B dataset [1], which comprises a\ndiverse and extensive collection of images that closely resemble real-world scenarios,\nincluding various environments, object types, and lighting conditions. For our study,\nwe focused on a subset of the first 1,000 images, labeled from sa_1.jpg to sa_1000.jpg.\nThis carefully chosen subset includes a total of 98,875 masks, representing a substantial\nquantity that enhances the robustness and statistical significance of our findings."}, {"title": "5.2. Quantitative Evaluation", "content": "We evaluate the performance of our proposed RGA method against several state-\nof-the-art adversarial attack techniques, including ASK [6], TAP [23], ILPD [24],\nAA [25], PATA [26], PATA++[26], and UAD[7]. The evaluation metrics used are\nmean Intersection over Union (mIoU) (lower is better), Attack Success Rate at thresh-\nolds 50% (ASR@50), and 10% (ASR@10) (higher is better). The experiments are"}, {"title": "5.3. Qualitative Evaluation", "content": "We conducted qualitative evaluations to assess the effectiveness of the proposed\nRGA in a black-box setting, specifically targeting Meta AI's online SAM model and"}, {"title": "5.4. Ablation Studies", "content": "To comprehensively evaluate the contributions of various components in the RGA\nframework, we conduct a series of ablation studies focusing on three main aspects:\nthe individual impact of the proposed components, the integration of RGA with tra-\nditional adversarial attack methods, and the evaluation of different adversarial target\ntypes. The ablation experiments are conducted on SAM-B (white-box), SAM-L, and\nSAM-H models, and the results are presented in three tables."}, {"title": "5.4.1. Component Analysis", "content": "To better understand the impact of different components in our RGA framework,\nwe evaluate the following: Region-Guided Map (RGM), Momentum Iteration (MI),\nRandom Similarity Transformation (RST), and Scale-Invariance (SI). Table 2 provides\na summary of the results, highlighting the effectiveness of each component when ap-\nplied individually or in combination.\nAs observed in Table 2, the baseline version of RGA, where none of the compo-\nnents (RGM, MI, RST, SI) are applied, demonstrates relatively low Attack Success\nRates (ASR) and high mIoU values, indicating limited effectiveness in degrading the\nsegmentation performance of the SAM models. Specifically, the baseline achieves\na mIoU of 49.98 for SAM-B, 71.49 for SAM-L, and 75.24 for SAM-H, with low\nASR@10 values.\nAdding the RGM significantly improves the performance, resulting in decreased"}, {"title": "5.4.2. Integrating RGM with Traditional Methods", "content": "We further evaluate the impact of integrating the RGM component with traditional\nadversarial attack methods, such as MIM [17] and DIM [18]. Table 3 presents the\nresults, demonstrating the effectiveness of RGM in enhancing the transferability and\nrobustness of these traditional methods.\nFrom Table 3, it is evident that adding RGM to both MIM and DIM results in\nsubstantial performance gains across all metrics on SAM-B (white-box), SAM-L, and\nSAM-H models. For example, the mIoU for MIM on SAM-B decreases from 54.64\nto 46.59 when integrated with RGM, while ASR@50 and ASR@10 improve from\n40.92% to 49.70% and 19.01% to 32.04%, respectively. Similarly, DIM+RGM achieves\na reduction in mIoU to 28.32 and improvements in ASR metrics, underscoring the ef-\nfectiveness of incorporating RGM to enhance traditional attack approaches."}, {"title": "5.4.3. Evaluating Different Target Types", "content": "To further evaluate the versatility of our RGA framework, we conduct experiments\nusing different target types for adversarial perturbations, including black, white, ran-\ndom noise, and randomly selected samples from the SA-1000 dataset. The results are\nsummarized in Table 4, which shows that our RGM approach consistently outperforms\nother target types in terms of mIoU and ASR metrics across all models.\nAs shown in Table 4, RGM achieves the lowest mIoU and highest ASR values\nacross SAM-B, SAM-L, and SAM-H models. For example, on the SAM-B (white-\nbox) model, RGM achieves a mIoU of 26.87, which is significantly better than the\nother target types, such as SA-1000, which yields a mIoU of 30.05. The ASR@50 and\nASR@10 values for RGM are also notably higher, demonstrating the superior ability\nof RGM to degrade segmentation quality effectively.\nIn summary, the ablation studies conducted across the three experiments highlight\nthe importance of each component in the RGA framework, the effectiveness of integrat-\ning RGM with traditional adversarial methods, and the superior performance of RGM\nover other target types. The results validate the strength of our approach in improving\nthe transferability and effectiveness of adversarial attacks against segmentation models."}, {"title": "5.5. Sensitivity Analysis of Hyper-parameters", "content": "In this section, we conduct a sensitivity analysis to evaluate the impact of key hyper-\nparameters on the performance of our RGA framework. The hyper-parameters exam-\nined include the perturbation bound $\\epsilon$, the number of adversarial iterations T, and the"}, {"title": "5.5.1. Perturbation Bound $\\epsilon$", "content": "The perturbation bound $\\epsilon$ plays a critical role in defining the maximum allowable\ndistortion applied to the input images. As shown in Figure 6, we evaluate various val-\nues of $\\epsilon$ to determine how the extent of perturbation affects the attack's effectiveness.\nLower values of $\\epsilon$ generally result in more subtle perturbations, which can help main-\ntain the perceptual quality of the input while still achieving a significant degradation\nin segmentation performance. Conversely, higher values of $\\epsilon$ can lead to more aggres-\nsive attacks, potentially compromising the model's integrity more effectively but also\nrisking the visibility of the perturbations."}, {"title": "5.5.2. Number of Adversarial Iterations T", "content": "The number of adversarial iterations T refers to the count of optimization steps\ntaken during the perturbation generation process. As shown in Figure 7, increasing T"}, {"title": "5.5.3. Granularity Parameter for Grid Size y and Number of Dilation Iterations n", "content": "Figure 8 presents the sensitivity analysis results for the granularity parameter for\ngrid size y and the number of dilation iterations n. The granularity parameter for grid\nsize y and the number of dilation iterations n are used to generate the Region-Guided\nMap (RGM), which plays a crucial role in misleading the SAM model. These param-\neters determine how the segmentation results are manipulated to create the adversarial\neffect.\nThe parameter y controls the size of the grid blocks used for dividing large seg-"}, {"title": "6. Discussion", "content": "The Region-Guided Attack (RGA) employs a powerful approach to adversarial at-\ntacks on segmentation models by generating the Region-Guided Map (RGM) through\nits Segmentation and Dilation (SAD) strategy. While RGA demonstrates considerable\nstrength in degrading segmentation quality by manipulating regions based on size, cer-\ntain limitations restrict its effectiveness in more specialized segmentation scenarios.\nHere, we discuss two key limitations: challenges with overlapping regions and the\ndifficulty of achieving desired errors with subtle perturbations.\n1. Limitations with Overlapping Regions: In tasks where regions frequently\noverlap-such as medical imaging, multi-layered materials in industrial inspections,\nor semi-transparent objects in computer vision-segmentation often requires intricate,\nmulti-layered outputs that capture the depth and interaction of overlapping structures.\nRGA's SAD strategy, which applies binary transformations (dilation for small regions\nand fragmentation for large ones), is not inherently equipped to manage these complex,\noverlapping relationships within the segmentation output.\nOverlapping regions require an adversarial approach that can distinguish between\nintersecting areas and selectively perturb them without compromising the multi-layered\nrepresentation. However, SAD's current design lacks the granularity to handle over-\nlapping regions effectively. When dealing with complex intersections, SAD's dilation\nand fragmentation may lead to coarse distortions that fail to influence the segmented\nregions in a meaningful way. For example, in a layered tissue sample in medical imag-\ning, disrupting the representation of one tissue layer without affecting the layers above\nor below requires finer control than SAD's region-specific transformations currently"}, {"title": "2. Subtle Perturbations May Not Yield Desired Errors", "content": "RGA's perturbation\napproach, while effective for disrupting larger segmented regions or inducing shifts\nin boundaries, may struggle in cases where minimal yet precise boundary modifica-\ntions are necessary to degrade the model's output meaningfully. In domains like high-\nresolution medical imaging or satellite analysis, segmentation accuracy hinges on the\nprecise identification of fine-grained boundaries, where even small misclassifications\ncan have significant implications. However, the SAD strategy's binary handling of\nregions lacks the subtlety needed to create these precise boundary distortions.\nSAD's transformations\u2014dilating small regions and fragmenting large ones are\nrelatively coarse and may introduce perceptible yet ineffective changes in the segmen-\ntation output. When applied to highly detailed regions, these transformations could\nresult in visible but inconsequential alterations that do not meaningfully impact the\nmodel's accuracy. For example, in tasks that require distinguishing between intricate,\nclosely related textures or subtle edge boundaries, RGA's SAD-based perturbations\nmay produce overly generalized errors that fail to impact model predictions at the de-\nsired level of detail.\nAddressing this limitation would require an enhancement to SAD's perturbation\nmethods, allowing it to achieve fine-grained distortions capable of subtly shifting bound-\naries without creating overtly visible artifacts. Future research could explore integrat-\ning more sophisticated techniques such as texture-sensitive perturbations or boundary-\npreserving modifications to better control the precision of attacks. These enhancements\nwould enable RGA to more effectively target detailed segmentation tasks that demand\nhigh fidelity and minimal perceptual distortion."}, {"title": "7. Conclusion", "content": "In this work, we introduced the Region-Guided Attack (RGA), an innovative ad-\nversarial attack method designed specifically for segmentation models like the SAM.\nRGA stands out by generating a Region-Guided Map (RGM) through the Segmenta-\ntion and Dilation (SAD) strategy, enabling a prompt-agnostic approach that disrupts\nSAM's segmentation accuracy. SAD tailors the perturbation strategy according to the\nsize and structure of each segmented region, dilating small regions to exaggerate their\npresence and fragmenting larger regions to destabilize their boundaries. The resulting\nRGM acts as a blueprint that guides adversarial perturbations in a spatially targeted\nmanner, enhancing the effectiveness and precision of the attack.\nOur quantitative and qualitative evaluations demonstrate that RGA significantly\ndegrades segmentation performance across multiple models and scenarios, achieving\nhigh attack success rates in both white-box and black-box settings. By leveraging the\nfeature-based structure of SAM, RGA ensures that perturbations are highly transfer-\nable and effective even without prompt-based guidance, highlighting a fundamental\nvulnerability in segmentation models that rely on spatial coherence.\nThe success of RGA underscores the need for future research to address region-\nspecific adversarial threats within segmentation frameworks. Defensive measures should\nfocus on enhancing segmentation model robustness against feature-guided perturba-\ntions, which are particularly challenging to detect and mitigate. Overall, RGA presents\na significant advancement in adversarial attack strategies, providing valuable insights\ninto the structural vulnerabilities of segmentation models and setting a foundation for"}]}