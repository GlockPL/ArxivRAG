{"title": "FiCo-ITR: bridging fine-grained and coarse-grained image-text retrieval for comparative performance analysis", "authors": ["Mikel Williams-Lekuona", "Georgina Cosma"], "abstract": "In the field of Image-Text Retrieval (ITR), recent advancements have leveraged large-scale Vision- Language Pretraining (VLP) for Fine-Grained (FG) instance-level retrieval, achieving high accuracy at the cost of increased computational complexity. For Coarse-Grained (CG) category-level retrieval, prominent approaches employ Cross-Modal Hashing (CMH) to prioritise efficiency, albeit at the cost of retrieval performance. Due to differences in methodologies, FG and CG models are rarely compared directly within evaluations in the literature, resulting in a lack of empirical data quantifying the retrieval performance-efficiency tradeoffs between the two. This paper addresses this gap by introducing the FiCo-ITR library, which standardises evaluation methodologies for both FG and CG models, facilitating direct comparisons. We conduct empirical evaluations of representative models from both subfields, analysing precision, recall, and computational complexity across varying data scales. Our findings offer new insights into the performance-efficiency trade-offs between recent representative FG and CG models, highlighting their respective strengths and limitations. These findings provide the foundation necessary to make more informed decisions regarding model selection for specific retrieval tasks and highlight avenues for future research into hybrid systems that leverage the strengths of both FG and CG approaches.", "sections": [{"title": "1 Introduction", "content": "Cross-Modal Retrieval (CMR) involves using one type of data, such as text, to search for another type of data, such as images. Unlike general multi-modal tasks, CMR specifically focuses on bridging the gap between different modalities to enable retrievals across modalities. CMR has gained prominence over the past decade due to its success in various applications, including e- commerce [1, 2], content-based retrieval [3, 4], video surveillance [5], and recommendation systems [6]. When the retrieval task specifically involves images and text, it is referred to as Image-Text Retrieval (ITR). There are two distinct subfields within ITR: Fine-Grained (FG) and Coarse-Grained (CG) ITR.\nFG ITR aims to find instance-level matches, retrieving the image that directly corresponds to a detailed text query, and vice versa. State-of- the-art FG methods employ large-scale Vision- Language Pretraining (VLP) followed by retrieval"}, {"title": "2 Related works", "content": ""}, {"title": "2.1 Fine-grained image-text retrieval", "content": "FG methods aim to map visual and textual infor- mation to a joint space where relevant samples are aligned at the instance level. The alignment of samples is achieved through learning shared feature representations from image-text pairs dur- ing training. Establishing the joint space enables retrieval across modalities, where the relative dis- tance of items in the shared space determines their relevance. While FG methods share the same fun- damental principles as CG ones, they differ in the level of scrutiny with which the processed sam- ples are analysed. Specifically, FG methods focus on low-level features and object-level relationships to achieve an in-depth understanding of the scene within a given sample. The format of the encoded samples is typically real-valued continuous embed- dings. These embeddings enable calculating the similarity between samples using vector distance measurements such as Cosine distance [14]."}, {"title": "2.2 Coarse-grained image-text retrieval", "content": "CG methods, similarly to FG methods, also aim to learn joint visual and textual representations. However, in contrast to the instance-level align- ment of FG methods, CG methods aim to align relevant samples at a broader semantic category level. By using a broader criterion, CG methods can place more emphasis on computational effi- ciency. The most prominent approach within CG ITR is Cross-Modal Hashing (CMH) [8], which is primarily characterised by the use of bit hash codes to represent their encoded data. The use of hash codes aims for lower storage costs and faster retrieval speeds, due to the bit hash format being inherently lightweight and the associated bitwise operations being less computationally complex than continuous embedding operations. Recent CMH methods use deep neural networks to learn hash functions that map image and text samples to binary hash codes. During retrieval, the hash codes are compared using Hamming distance, an efficient similarity measure for bit strings which counts the number of differing bits between two equal-length bit hash codes.\nSupervised cross-modal hashing. Super- vised CMH methods leverage multi-category labelling to train the hash function for each modality. DCMH [35] pioneered this approach by proposing an end-to-end deep learning CMH framework. Leveraging Generative Adversarial Networks (GAN), SSAH [36] implements label information as network input to strengthen cat- egory alignment in the hash space. AGAH [37] uses label information directly in its loss function, implementing a multi-labeling map. DADH [38] adopts a weighted cosine triplet-margin con- straint for ranking-based similarity preservation. DCHUC [39] introduces a four-step iterative opti- misation process that allows simultaneous learn- ing of unified hash codes for database samples and modality-specific hashing functions for unseen queries. DCHMT [40] employs a dual transformer tower network and a differentiable hashing mod- ule, enabling location-aware image encoding and continuous, gradient descent-optimised modality representation. LDSAH [41] integrates label-wise semantic alignment with a dissimilarity-penalising strategy using a combination of Jensen-Shannon divergence loss and attention-driven sample re- weighting.\nUnsupervised cross-modal hashing. Unsupervised CMH methods use image-text pair coupling information to learn the modality hash functions, avoiding the reliance on category labelling information. This makes unsupervised CMH methods more analogous to typical FG methods, which also do not rely on labelling. Unsupervised UGACH [42] uses GANs to exploit the underlying manifold structure of cross-modal data with a max-margin ranking loss."}, {"title": "3 Proposed library and toolkit", "content": "We propose FiCo-ITR, a comprehensive library and toolkit that unifies the evaluation of the FG and CG ITR subfields, facilitating direct compar- isons for both image-to-text (i \u2192 t) and text- to-image (t\u2192 i) retrieval tasks. As illustrated in Figure 2, the library's framework comprises five key components: 1) Data Pre-Processing, 2) Model Encoding, 3) Similarity Metrics, 4) Retrieval Tasks, and 5) Evaluation Metrics, form- ing a complete pipeline for image-text retrieval evaluation.\n1) Data pre-processing. To enable both instance-level and category-level retrieval evalua- tions, the selected benchmark datasets must have full-sentence captions as well as semantic category labelling. For datasets which do not have seman- tic category labelling, we employ the Query2Label (Q2L) [48] classifier to label them with synthetic semantic category labels. We adopt pre-processing steps which most closely adhere to each subfield's standard approaches [10, 18]: For CG models, FiCo-ITR provides the toolkit to extract 4096D VGG-19 features for images and 300D Doc2Vec features for text. For traditional FG models, we refer to the Bottom-Up Attention [49] repository for Faster RCNN 1024x36D features for images, whereas the raw text is given as input. VLP models process raw data using their respective transformer encoder stacks.\n2) Model encoding. Given the pre-processed data as input, the samples chosen for each split must be consistent across all evaluated models to ensure the most directly comparable evaluation. To this end, the FiCo-ITR toolkit provides func- tionality to split samples into pre-defined splits along their IDs to ensure consistency of results when comparing different models. In the case of extending the use of FiCo-ITR to other datasets not included in this study, FiCo-ITR provides the tools to stratify, record ID designation, and align splits across models. Three embedding output types are handled: Binary hash codes, continuous embeddings, or in the case of models which employ query-time attention, directly computed similarity matrices.\n3) Similarity measures. Given a set of evaluation embeddings, we implement four simi- larity measures for similarity rank ordering: For traditional binary hash codes (Os and 1s), we implement Hamming distance which calculates the number of differing bits between two hash codes efficiently on CPUs. However, the library also supports hash codes represented as -1s and 1s, enabling the use of inner product, cosine similarity, and Euclidean distance. This design choice allows users to leverage GPU acceleration for hash code comparisons, potentially boosting performance in GPU-centric environments. For continuous embeddings, all measures except Ham- ming distance are applicable. The consistency in similarity computation within this step ensures the output similarity matrices are aligned for the subsequent tasks.\n4) Retrieval tasks. Given a similarity matrix, FiCo-ITR implements two retrieval task evaluations: Instance-level retrieval and category- level retrieval. For the instance-level retrieval task, the objective is to search for the retrieval sample which directly corresponds to the query sample. Given an image with multiple captions, for the (it) task, retrieving any one of the multiple captions of the query image is considered a 100% recall. For the (t\u2192 i) task, the specific image cor- responding to the query caption must be retrieved"}, {"title": "4 Experiment methodology", "content": "The following is the experiment methodology for the comparative experiments conducted in Section 5."}, {"title": "5 Comparative experiments", "content": ""}, {"title": "5.1 Instance-level retrieval results", "content": "This experiment aims to empirically assess the comparative performance of CG and FG models in instance-level retrieval tasks. Through the use of the proposed FiCo-ITR library, the instance-level Recall@k evaluation results for the selected models on the Flickr30K and MS-COCO datasets are reported in Table 2, with additional results at higher top-k levels for the CG models being reported in Table 3. From these results, the fol- lowing observations can be made:\nWhere CG succeeds. The model ADV in its 64-bit CG setting achieves moderate success, with R@10 scores of 75.0% for (i \u2192 t) and 60.5% for (t\u2192 i) retrieval. The improvement of ADV over DADH and UCCH can primarily be attributed to adopting instance-level matching as the primary objective function. In its 2048-bit hash code FG setting, ADV achieves retrieval performance com- parable to other continuous embedding-based FG models (Table 2). The extended top-k results in Table 3-where ADV 64bit attains R@100 scores of 96.4% and 90.2% for the (i \u2192 t) and (t \u2192 i) tasks suggest that CG models have room to be used as initial retrieval candidate screening steps, provided the efficiency gained by employing such a strategy outweighs the information that is lost in this initial screening step.\nCG limitations. The CG models DADH and UCCH were unable to properly capture instance- level relationships compared to the FG ones, as evidenced by their achieved R@1 scores (Table"}, {"title": "5.2 Category-level retrieval results", "content": "This experiment aims to empirically compare CG and FG models in category-level retrieval tasks, where their relative performance is not well- established in the literature. The category-level mAP@k evaluation results for the selected mod- els on the Flickr30K and MS-COCO datasets are reported in Table 4. The 11-point interpolated precision-recall curves of the evaluated models for both datasets are shown in Figure 3. From these results, the following observations can be made:\nmAP@10 performance comparison. Given the majority of the queries within both the Flickr30K and MS-COCO evaluation sets have hundreds to thousands of retrieval candidates which are considered relevant at the category level, scoring well on the mAP@10 metric is the least challenging aspect of this task. Nevertheless, the no fusion variant of BLIP-2 achieves an (i \u2192 t) mAP@10 score of 96.2% and a (t\u2192 i) mAP@10 score of 95.9% on the MS-COCO dataset, whereas the coarse model UCCH achieves scores of 86.2% and 86.6%, respectively (Table 4). The superior performance of FG models on the mAP@10 met- ric can be attributed to instance-level matches being retrieved at the top ranks, which inherently share category labels with the query.\nmAP@N challenge and model size. In contrast to the relative ease of scoring well on the mAP@10 metric, the mAP@N metric presents a more substantial challenge. Unlike instance-level retrieval, performance on this metric does not appear to align with the size of the model being used. For example, on the MS-COCO dataset, the state-of-the-art FG VLP BLIP-2NF model achieves the second lowest category-level mAP@N score of 47.9% on the (i \u2192 t) task, while the coarse model UCCH attains the highest score of 60.7% (Table 4). This suggests that FG low-level seman- tics may introduce noise when high-level category semantics is all that is needed for the task.\nPrecision-recall trade-offs. The precision- recall curves in Figure 3 illustrate that the CG unsupervised model UCCH and, in particular, the supervised model DADH trained with learning objectives targeting category-level retrieval better maintain their precision as the proportion of rele- vant samples to be retrieved increases. In contrast, FG models, which are not optimised for retrieving large numbers of broadly relevant samples, show a sharper decline in precision with increases in recall requirement.\nDataset difficulty comparison. Across all models and tasks, the average mAP@N was 92.9% for Flickr30K compared to 56% for MS- COCO, indicating that the MS-COCO category- level retrieval benchmark presents a more chal- lenging task. This difference in difficulty can be attributed to the datasets' composition. MS- COCO features more diverse and less overlapping labels, resulting in a more complex retrieval task at the category level. In contrast, Flickr30K has"}, {"title": "5.3 Scaling encoding, storage, and attention", "content": "This experiment aims to evaluate the scalabil- ity of the selected models by measuring encoding time and embedding storage cost across progres- sively larger test sets. We additionally measure query-time attention costs of applicable models to assess the trade-off in real-time performance which attention-based models face. We incrementally duplicate the Flickr30K evaluation set, generating four test sets: 1K/5K, 10K/50K, 100K/500K and 1M/5M image/text samples. By conducting these experiments on an NVIDIA A100 80GiB GPU and Intel Xeon Platinum 8480+ CPU, we aim to pro- vide insights into the practical trade-offs between model complexity, computational resources, and retrieval performance at scale.\nEncoding Time and Storage Cost. Table 5 shows the encoding time and storage require- ments for increasing data volumes for the selected models. The experiments were constrained by an 80GiB GPU memory threshold; runs exceed- ing this limit due to the size of the final or intermediate embeddings did not finish (DNF). For instance, BEIT-3, despite having the most compact final embeddings among VLP models, required 8.12GiB per 1K/5K images/captions to compute intermediate embeddings, causing its 10K/50K encoding run to exceed available GPU memory. These experiments used original model implementations without custom batching to ensure consistency and reflect practical limita- tions users might encounter with similar hardware. This consideration highlights the storage efficiency of cross-modal hashing models, which require only 0.36GiB to store 1M/5M image/text embeddings. For query-time attention models, Table 5 includes storage costs for original image and text features, which must be retained for the attention step; an important consideration when assessing the practicality of query-time attention in memory- constrained applications. Encoding time generally increased by a factor of 10 from CG to FG meth- ods and again to VLP models, with X-VLM and ADV as exceptions. The rapid encoding times of UCCH and DADH demonstrate the computa- tional efficiency of hash function encoders.\nAttention time. Attention in this context refers to cross-attention in the case of IMRAM"}, {"title": "5.4 Scaling similarity search with FAISS", "content": "The final step in retrieval, similarity search, is typically independent of the encoding model. Assuming no query-time attention or reranking, the model's task is complete once samples are encoded. These encoded samples are then typi- cally passed to a separate specialised similarity search implementation. In that case, the only factors affecting similarity search time are the embedding type (real-valued continuous or bitwise hash code embeddings) and the embedding dimen- sions. To explore the practical implications of using CG binary hash codes compared to FG con- tinuous embeddings, we employ Facebook AI Sim- ilarity Search (FAISS) [52], a robust and widely adopted [53-55] similarity search implementa- tion offering various indexes for both exhaus- tive search and Approximate Nearest Neighbour Search (ANNS). By using FAISS, we transition from model-specific comparisons to a generalised evaluation of embedding types within an industry- standard framework. The experiments were con- ducted using the following four indexes:\ncontinuous- a\nIndexBinaryFlat IndexHNSW (Hierarchical Navigable Small is a graph-based index that organises"}, {"title": "6 Key findings and discussion", "content": "Based on the results obtained from the conducted comparative experiments, the following are the main key findings and recommendations derived from this study:\nPerformance comparison. FG models con- sistently outperformed CG models in instance- level retrieval tasks. CG models, however, demon- strated competitive performance in category-level retrieval, especially when retrieving large num- bers of relevant samples. These findings challenge"}, {"title": "6.1 Limitations", "content": "The scalability experiments were conducted through synthetic duplication of standard bench- mark datasets. While this approach offers insights into the computational scalability of the evaluated models, it does not account for potential degra- dation in retrieval performance as the retrieval candidate pool increases.\nOur study primarily focused on existing mod- els and search implementations, aiming for a comprehensive view of the current practical state of the field. This approach aligns with the expe- riences of practitioners who may rely on publicly available tools and implementations, providing insights into the performance characteristics of widely accessible models and search techniques. However, the application of customised batching implementations within the selected models, as well as custom hardware-level optimisations of CG bitwise operations both aspects likely to be implemented in production-level use cases could offer more insightful perspectives on the perfor- mance of proprietary production-level pipelines."}, {"title": "7 Conclusion", "content": "While the conceptual differences in performance between FG and CG image-text retrieval models are well-documented in the literature, empiri- cal data quantifying these differences has been sparse. This study introduced the library and toolkit FiCo-ITR, which provides a standardised toolkit for evaluating both FG and CG image-text retrieval models, which addresses this empirical gap. The results within this paper indicate that while FG methods excel in instance-level retrieval, CG approaches demonstrate competitive perfor- mance in category-level tasks where a large num- ber of relevant samples must be retrieved. Despite the potential for computational efficiency offered by CG models, practical evaluations showed com- parable query latencies between FG continuous embeddings and CG bitwise hash code embed- dings, highlighting the practical impact of recent hardware optimisations. These results highlight that the notion of FG models offering more robust retrieval performance while CG models are more efficient is not straightforward; instead, these char- acteristics depend on the specific retrieval task and implementation. Future work will focus on two key areas: First, the development of a large- scale image-text retrieval benchmark to enable scalability experiments on real data. Second, the exploration of hybrid coarse-to-fine approaches that leverage insights from this study to bal- ance retrieval performance and efficiency. These directions aim to further bridge the gap between FG and CG methodologies, potentially leading to more robust and scalable image-text retrieval systems."}, {"title": "Declarations", "content": "Code availability. The source code for the FiCo-ITR library and toolkit can be found in the project's GitHub repository: https://github.com/MikelWL/FiCo-ITR.\nConflict of interest. The authors declare no Conflict of interest.\nEthical approval. This article contains no data or other information from studies or experimenta- tion involving human or animal subjects."}, {"title": "R@k =", "content": "\\frac{number \\; of \\; relevant \\; items \\; in \\; top \\; k \\; results}{total \\; relevant \\; items}"}, {"title": "P@k =", "content": "\\frac{number \\; of \\; relevant \\; items \\; in \\; top \\; k \\; results}{number \\; of \\; retrieved \\; items}"}]}