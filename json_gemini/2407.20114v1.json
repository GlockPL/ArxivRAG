{"title": "FiCo-ITR: bridging fine-grained and coarse-grained\nimage-text retrieval for comparative performance analysis", "authors": ["Mikel Williams-Lekuona", "Georgina Cosma"], "abstract": "In the field of Image-Text Retrieval (ITR), recent advancements have leveraged large-scale Vision-\nLanguage Pretraining (VLP) for Fine-Grained (FG) instance-level retrieval, achieving high accuracy\nat the cost of increased computational complexity. For Coarse-Grained (CG) category-level retrieval,\nprominent approaches employ Cross-Modal Hashing (CMH) to prioritise efficiency, albeit at the cost\nof retrieval performance. Due to differences in methodologies, FG and CG models are rarely com-\npared directly within evaluations in the literature, resulting in a lack of empirical data quantifying the\nretrieval performance-efficiency tradeoffs between the two. This paper addresses this gap by introduc-\ning the FiCo-ITR library, which standardises evaluation methodologies for both FG and CG models,\nfacilitating direct comparisons. We conduct empirical evaluations of representative models from both\nsubfields, analysing precision, recall, and computational complexity across varying data scales. Our\nfindings offer new insights into the performance-efficiency trade-offs between recent representative FG\nand CG models, highlighting their respective strengths and limitations. These findings provide the\nfoundation necessary to make more informed decisions regarding model selection for specific retrieval\ntasks and highlight avenues for future research into hybrid systems that leverage the strengths of both\nFG and CG approaches.", "sections": [{"title": "Introduction", "content": "Cross-Modal Retrieval (CMR) involves using one\ntype of data, such as text, to search for another\ntype of data, such as images. Unlike general\nmulti-modal tasks, CMR specifically focuses on\nbridging the gap between different modalities\nto enable retrievals across modalities. CMR has\ngained prominence over the past decade due to\nits success in various applications, including e-\ncommerce [1, 2], content-based retrieval [3, 4],\nvideo surveillance [5], and recommendation sys-\ntems [6]. When the retrieval task specifically\ninvolves images and text, it is referred to as\nImage-Text Retrieval (ITR). There are two dis-\ntinct subfields within ITR: Fine-Grained (FG) and\nCoarse-Grained (CG) ITR.\nFG ITR aims to find instance-level matches,\nretrieving the image that directly corresponds to\na detailed text query, and vice versa. State-of-\nthe-art FG methods employ large-scale Vision-\nLanguage Pretraining (VLP) followed by retrieval"}, {"title": "Related works", "content": ""}, {"title": "Fine-grained image-text\nretrieval", "content": "FG methods aim to map visual and textual infor-\nmation to a joint space where relevant samples\nare aligned at the instance level. The alignment\nof samples is achieved through learning shared\nfeature representations from image-text pairs dur-\ning training. Establishing the joint space enables\nretrieval across modalities, where the relative dis-\ntance of items in the shared space determines their\nrelevance. While FG methods share the same fun-\ndamental principles as CG ones, they differ in the\nlevel of scrutiny with which the processed sam-\nples are analysed. Specifically, FG methods focus\non low-level features and object-level relationships\nto achieve an in-depth understanding of the scene\nwithin a given sample. The format of the encoded\nsamples is typically real-valued continuous embed-\ndings. These embeddings enable calculating the\nsimilarity between samples using vector distance\nmeasurements such as Cosine distance [14]."}, {"title": "Traditional fine-grained methods.", "content": "Early\ndeep learning-based FG methods typically\nemployed Long Short-Term Memory (LSTM)\nnetworks for text encoding and convolutional\nneural networks (CNNs) such as AlexNet and\nVGGNet for image feature extraction [15].\nVSE++ [14] built on this approach by experi-\nmenting with VGG19 [16] and ResNet152 [17]\nfor image encoding along with a GRU-based\ntext encoder and implementing hard negative\nmining and reranking loss functions. SCAN [18]\nachieved a breakthrough in performance through\nits proposed bottom-up attention module align-\ning image regions and words based on Faster\nR-CNN object detection [19]. CAMP [20] aggre-\ngates salient messages between image regions\nand words via attention to handle negative\npairs before directly predicting matching scores.\nVSRN [21] introduced hierarchical reasoning to\ncapture region-concept interactions and was later\nupgraded to VSRN++ [22] using BERT [23] text\nfeatures. KASCE [24] expands image concepts\nusing \"common-sense\" relationships from scene\ngraphs and selects the most relevant expansions.\nCAAN [25] employs context-aware attention to\nselectively attend to fragments based on inter- and\nintra-modal semantics. IMRAM [9] progressively\naligns fragments through a recurrent attention\nunit to capture different semantics at each step,\nalongside a memory unit to accumulate cues."}, {"title": "Fine-grained vision-language pretrain-\ning methods.", "content": "Following the success of the atten-\ntion mechanism in BERT [23], the transformer\narchitecture has been extensively adapted for\nvision-language tasks. This adaptation predomi-\nnantly involves leveraging pretraining on large-\nscale data to obtain task-agnostic features for sub-\nsequent fine-tuning onto downstream tasks such\nas ITR. Initial VLP methods are categorised into\ntwo main encoder architectures: Dual-encoder and\nfusion-encoder architectures. Dual-encoders, such\nas ALIGN [26] and BEIT-3 [27], employ separate\nimage and text encoders, allowing the indepen-\ndent computation and offline storage of embed-\ndings for each modality. This approach eliminates\nthe need to compute embeddings at query time."}, {"title": "Coarse-grained image-text\nretrieval", "content": "CG methods, similarly to FG methods, also aim\nto learn joint visual and textual representations.\nHowever, in contrast to the instance-level align-\nment of FG methods, CG methods aim to align\nrelevant samples at a broader semantic category\nlevel. By using a broader criterion, CG methods\ncan place more emphasis on computational effi-\nciency. The most prominent approach within CG\nITR is Cross-Modal Hashing (CMH) [8], which\nis primarily characterised by the use of bit hash\ncodes to represent their encoded data. The use of\nhash codes aims for lower storage costs and faster\nretrieval speeds, due to the bit hash format being\ninherently lightweight and the associated bitwise\noperations being less computationally complex\nthan continuous embedding operations. Recent\nCMH methods use deep neural networks to learn\nhash functions that map image and text samples\nto binary hash codes. During retrieval, the hash\ncodes are compared using Hamming distance, an\nefficient similarity measure for bit strings which\ncounts the number of differing bits between two\nequal-length bit hash codes."}, {"title": "Supervised cross-modal hashing.", "content": "Super-\nvised CMH methods leverage multi-category\nlabelling to train the hash function for each\nmodality. DCMH [35] pioneered this approach\nby proposing an end-to-end deep learning CMH\nframework. Leveraging Generative Adversarial\nNetworks (GAN), SSAH [36] implements label\ninformation as network input to strengthen cat-\negory alignment in the hash space. AGAH [37]\nuses label information directly in its loss function,\nimplementing a multi-labeling map. DADH [38]\nadopts a weighted cosine triplet-margin con-\nstraint for ranking-based similarity preservation.\nDCHUC [39] introduces a four-step iterative opti-\nmisation process that allows simultaneous learn-\ning of unified hash codes for database samples\nand modality-specific hashing functions for unseen\nqueries. DCHMT [40] employs a dual transformer\ntower network and a differentiable hashing mod-\nule, enabling location-aware image encoding and\ncontinuous, gradient descent-optimised modality\nrepresentation. LDSAH [41] integrates label-wise\nsemantic alignment with a dissimilarity-penalising\nstrategy using a combination of Jensen-Shannon\ndivergence loss and attention-driven sample re-\nweighting."}, {"title": "Unsupervised cross-modal hashing.", "content": "Unsupervised CMH methods use image-text\npair coupling information to learn the modality\nhash functions, avoiding the reliance on category\nlabelling information. This makes unsupervised\nCMH methods more analogous to typical FG\nmethods, which also do not rely on labelling.\nUnsupervised UGACH [42] uses GANs to exploit\nthe underlying manifold structure of cross-\nmodal data with a max-margin ranking loss."}, {"title": "Proposed library and toolkit", "content": "We propose FiCo-ITR, a comprehensive library\nand toolkit that unifies the evaluation of the FG\nand CG ITR subfields, facilitating direct compar-\nisons for both image-to-text (i \u2192 t) and text-\nto-image (t\u2192 i) retrieval tasks. As illustrated\nin Figure 2, the library's framework comprises\nfive key components: 1) Data Pre-Processing,\n2) Model Encoding, 3) Similarity Metrics, 4)\nRetrieval Tasks, and 5) Evaluation Metrics, form-\ning a complete pipeline for image-text retrieval\nevaluation.\n1) Data pre-processing. To enable both\ninstance-level and category-level retrieval evalua-\ntions, the selected benchmark datasets must have\nfull-sentence captions as well as semantic category\nlabelling. For datasets which do not have seman-\ntic category labelling, we employ the Query2Label\n(Q2L) [48] classifier to label them with synthetic\nsemantic category labels. We adopt pre-processing\nsteps which most closely adhere to each subfield's\nstandard approaches [10, 18]: For CG models,\nFiCo-ITR provides the toolkit to extract 4096D\nVGG-19 features for images and 300D Doc2Vec\nfeatures for text. For traditional FG models, we\nrefer to the Bottom-Up Attention [49] repository\nfor Faster RCNN 1024x36D features for images,\nwhereas the raw text is given as input. VLP\nmodels process raw data using their respective\ntransformer encoder stacks.\n2) Model encoding. Given the pre-processed\ndata as input, the samples chosen for each split\nmust be consistent across all evaluated models to\nensure the most directly comparable evaluation.\nTo this end, the FiCo-ITR toolkit provides func-\ntionality to split samples into pre-defined splits\nalong their IDs to ensure consistency of results\nwhen comparing different models. In the case of\nextending the use of FiCo-ITR to other datasets\nnot included in this study, FiCo-ITR provides the\ntools to stratify, record ID designation, and align\nsplits across models. Three embedding output\ntypes are handled: Binary hash codes, continuous\nembeddings, or in the case of models which employ\nquery-time attention, directly computed similarity\nmatrices.\n3) Similarity measures. Given a set of\nevaluation embeddings, we implement four simi-\nlarity measures for similarity rank ordering: For\ntraditional binary hash codes (Os and 1s), we\nimplement Hamming distance which calculates\nthe number of differing bits between two hash\ncodes efficiently on CPUs. However, the library\nalso supports hash codes represented as -1s and\n1s, enabling the use of inner product, cosine\nsimilarity, and Euclidean distance. This design\nchoice allows users to leverage GPU acceleration\nfor hash code comparisons, potentially boosting\nperformance in GPU-centric environments. For\ncontinuous embeddings, all measures except Ham-\nming distance are applicable. The consistency in\nsimilarity computation within this step ensures\nthe output similarity matrices are aligned for the\nsubsequent tasks.\n4) Retrieval tasks. Given a similarity\nmatrix, FiCo-ITR implements two retrieval task\nevaluations: Instance-level retrieval and category-\nlevel retrieval. For the instance-level retrieval task,\nthe objective is to search for the retrieval sample\nwhich directly corresponds to the query sample.\nGiven an image with multiple captions, for the\n(it) task, retrieving any one of the multiple\ncaptions of the query image is considered a 100%\nrecall. For the (t\u2192 i) task, the specific image cor-\nresponding to the query caption must be retrieved"}, {"title": "Experiment methodology", "content": "The following is the experiment methodology for\nthe comparative experiments conducted in Section\n5."}, {"title": "Instance-level retrieval results", "content": "This experiment aims to empirically assess the\ncomparative performance of CG and FG models\nin instance-level retrieval tasks. Through the use\nof the proposed FiCo-ITR library, the instance-\nlevel Recall@k evaluation results for the selected\nmodels on the Flickr30K and MS-COCO datasets\nare reported in Table 2, with additional results\nat higher top-k levels for the CG models being\nreported in Table 3. From these results, the fol-\nlowing observations can be made:\nWhere CG succeeds. The model ADV in its\n64-bit CG setting achieves moderate success, with\nR@10 scores of 75.0% for (it) and 60.5% for\n(t\u2192 i) retrieval. The improvement of ADV over\nDADH and UCCH can primarily be attributed to\nadopting instance-level matching as the primary\nobjective function. In its 2048-bit hash code FG\nsetting, ADV achieves retrieval performance com-\nparable to other continuous embedding-based FG\nmodels (Table 2). The extended top-k results in\nTable 3-where ADV 64bit attains R@100 scores\nof 96.4% and 90.2% for the (i \u2192 t) and (t \u2192 i)\ntasks suggest that CG models have room to be\nused as initial retrieval candidate screening steps,\nprovided the efficiency gained by employing such\na strategy outweighs the information that is lost\nin this initial screening step.\nCG limitations. The CG models DADH and\nUCCH were unable to properly capture instance-\nlevel relationships compared to the FG ones, as\nevidenced by their achieved R@1 scores (Table"}, {"title": "Category-level retrieval results", "content": "This experiment aims to empirically compare\nCG and FG models in category-level retrieval\ntasks, where their relative performance is not well-\nestablished in the literature. The category-level\nmAP@k evaluation results for the selected mod-\nels on the Flickr30K and MS-COCO datasets are\nreported in Table 4. The 11-point interpolated\nprecision-recall curves of the evaluated models for\nboth datasets are shown in Figure 3. From these\nresults, the following observations can be made:\nmAP@10 performance comparison.\nGiven the majority of the queries within both the\nFlickr30K and MS-COCO evaluation sets have\nhundreds to thousands of retrieval candidates\nwhich are considered relevant at the category\nlevel, scoring well on the mAP@10 metric is the\nleast challenging aspect of this task. Nevertheless,\nthe no fusion variant of BLIP-2 achieves an (i \u2192\nt) mAP@10 score of 96.2% and a (t\u2192 i) mAP@10\nscore of 95.9% on the MS-COCO dataset, whereas\nthe coarse model UCCH achieves scores of 86.2%\nand 86.6%, respectively (Table 4). The superior\nperformance of FG models on the mAP@10 met-\nric can be attributed to instance-level matches\nbeing retrieved at the top ranks, which inherently\nshare category labels with the query.\nmAP@N challenge and model size. In\ncontrast to the relative ease of scoring well on the\nmAP@10 metric, the mAP@N metric presents a\nmore substantial challenge. Unlike instance-level\nretrieval, performance on this metric does not\nappear to align with the size of the model being\nused. For example, on the MS-COCO dataset,\nthe state-of-the-art FG VLP BLIP-2NF model\nachieves the second lowest category-level mAP@N\nscore of 47.9% on the (i \u2192 t) task, while the coarse\nmodel UCCH attains the highest score of 60.7%\n(Table 4). This suggests that FG low-level seman-\ntics may introduce noise when high-level category\nsemantics is all that is needed for the task.\nPrecision-recall trade-offs. The precision-\nrecall curves in Figure 3 illustrate that the CG\nunsupervised model UCCH and, in particular, the\nsupervised model DADH trained with learning\nobjectives targeting category-level retrieval better\nmaintain their precision as the proportion of rele-\nvant samples to be retrieved increases. In contrast,\nFG models, which are not optimised for retrieving\nlarge numbers of broadly relevant samples, show a\nsharper decline in precision with increases in recall\nrequirement.\nDataset difficulty comparison. Across all\nmodels and tasks, the average mAP@N was\n92.9% for Flickr30K compared to 56% for MS-\nCOCO, indicating that the MS-COCO category-\nlevel retrieval benchmark presents a more chal-\nlenging task. This difference in difficulty can\nbe attributed to the datasets' composition. MS-\nCOCO features more diverse and less overlapping\nlabels, resulting in a more complex retrieval task\nat the category level. In contrast, Flickr30K has"}, {"title": "Scaling encoding, storage, and\nattention", "content": "This experiment aims to evaluate the scalabil-\nity of the selected models by measuring encoding\ntime and embedding storage cost across progres-\nsively larger test sets. We additionally measure\nquery-time attention costs of applicable models to\nassess the trade-off in real-time performance which\nattention-based models face. We incrementally\nduplicate the Flickr30K evaluation set, generating\nfour test sets: 1K/5K, 10K/50K, 100K/500K and\n1M/5M image/text samples. By conducting these\nexperiments on an NVIDIA A100 80GiB GPU and\nIntel Xeon Platinum 8480+ CPU, we aim to pro-\nvide insights into the practical trade-offs between\nmodel complexity, computational resources, and\nretrieval performance at scale.\nEncoding Time and Storage Cost. Table\n5 shows the encoding time and storage require-\nments for increasing data volumes for the selected\nmodels. The experiments were constrained by\nan 80GiB GPU memory threshold; runs exceed-\ning this limit due to the size of the final or\nintermediate embeddings did not finish (DNF).\nFor instance, BEIT-3, despite having the most\ncompact final embeddings among VLP models,\nrequired 8.12GiB per 1K/5K images/captions\nto compute intermediate embeddings, causing\nits 10K/50K encoding run to exceed available\nGPU memory. These experiments used original\nmodel implementations without custom batching\nto ensure consistency and reflect practical limita-\ntions users might encounter with similar hardware.\nThis consideration highlights the storage efficiency\nof cross-modal hashing models, which require only\n0.36GiB to store 1M/5M image/text embeddings.\nFor query-time attention models, Table 5 includes\nstorage costs for original image and text features,\nwhich must be retained for the attention step;\nan important consideration when assessing the\npracticality of query-time attention in memory-\nconstrained applications. Encoding time generally\nincreased by a factor of 10 from CG to FG meth-\nods and again to VLP models, with X-VLM and\nADV as exceptions. The rapid encoding times\nof UCCH and DADH demonstrate the computa-\ntional efficiency of hash function encoders.\nAttention time. Attention in this context\nrefers to cross-attention in the case of IMRAM\nand SCAN and transformer fusion-based rerank-\ning for BLIP-2 and X-VLM, both processes which\nare computed at query time. Due to the cross-\nattention mechanism used by IMRAM and SCAN\nattending to all possible image-text matches,\nassuming an equal rate of increase in the number\nof queries and retrieval candidates, the computa-\ntional complexity of the mechanism approximates\nO(n\u00b2). The k value for the reranking shard thresh-\nold for BLIP-2 and X-VLM was kept at k\n128 irrespective of the increase in sample size,\nwhich leads to a O(n) increase in attention com-\nputation time. However, if the k value were also\nscaled to the sample size, the computational com-\nplexity would approximate O(n\u00b2). Such attention\nmechanisms, therefore, may be impractical for\nlarge-scale applications where query latency is\ncritical."}, {"title": "Scaling similarity search with\nFAISS", "content": "The final step in retrieval, similarity search, is\ntypically independent of the encoding model.\nAssuming no query-time attention or reranking,\nthe model's task is complete once samples are\nencoded. These encoded samples are then typi-\ncally passed to a separate specialised similarity\nsearch implementation. In that case, the only\nfactors affecting similarity search time are the\nembedding type (real-valued continuous or bitwise\nhash code embeddings) and the embedding dimen-\nsions. To explore the practical implications of\nusing CG binary hash codes compared to FG con-\ntinuous embeddings, we employ Facebook AI Sim-\nilarity Search (FAISS) [52], a robust and widely\nadopted [53-55] similarity search implementa-\ntion offering various indexes for both exhaus-\ntive search and Approximate Nearest Neighbour\nSearch (ANNS). By using FAISS, we transition\nfrom model-specific comparisons to a generalised\nevaluation of embedding types within an industry-\nstandard framework. The experiments were con-\nducted using the following four indexes:\nIndexes. IndexFlatIP is continuous-\nembedding brute-force index that exhaustively\nsearches the entire dataset via inner product\nand serves as our baseline for evaluating the\nperformance of the other three indexes against\nit. IndexHNSW (Hierarchical Navigable Small\nWorld) is a graph-based index that organises"}, {"title": "Key findings and discussion", "content": "Based on the results obtained from the conducted\ncomparative experiments, the following are the\nmain key findings and recommendations derived\nfrom this study:\nPerformance comparison. FG models con-\nsistently outperformed CG models in instance-\nlevel retrieval tasks. CG models, however, demon-\nstrated competitive performance in category-level\nretrieval, especially when retrieving large num-\nbers of relevant samples. These findings challenge\nthe conventional notion of FG models having uni-\nversal superiority in retrieval performance over\nCG models; while FG models excel in specific\ninstance-level retrieval, their performance advan-\ntage diminishes in broader, category-level tasks.\nThis suggests that, when retrieval performance\nis the main concern, the choice between FG and\nCG models should be task-dependent rather than\nalways defaulting to FG models.\nHybrid coarse-to-fine potential. An intu-\nitive integration of CG and FG models could\ninvolve a two-step approach, where a CG model\nselects top-k candidates to reduce the computa-\ntional load of a subsequent fine-grained rerank-\ning step. Our evaluation results showed that\nCG models trained specifically with instance-level\nloss functions could potentially serve as this ini-\ntial screening step within a coarse-to-fine ITR\npipeline. However, traditional CG models trained\non category-level loss functions are not suitable\nfor this purpose. Despite their effectiveness in\nbroadly retrieving samples of the same category\nas the query, these models do not consistently\nrank the exact instance-level match high enough\nin the retrieval rank to serve as an effective initial\nscreening step.\nAttention mechanisms. State-of-the-art\nbenchmark recall results are achieved by mod-\nels such as BLIP-2, which employ fusion-encoder\nreranking to refine search results. However, the\ncomputational load associated with such query-\ntime attention mechanisms is difficult to justify\nfor practical use. This inefficiency stems from\nattention mechanisms attending to every possible\nimage-text pair. In scenarios where the increase\nof images and text samples scale at the same\nrate, the computational complexity of such atten-\ntion mechanisms approximates O(n\u00b2). Therefore,\nfor large-scale retrieval applications where stor-\nage concerns can be addressed, dual-encoder VLP\nmodels which do not have query-time attention\nrepresent the most practical architecture type for\ninstance-level retrieval.\nSearch scalability insights. Scalability\nexperiments revealed that the potential efficiency\nadvantages of CG models, particularly in terms\nof bitwise operations, do not always translate\ninto practical performance gains for query latency.\nThis was evidenced when applying FAISS-based\nANNS indexing, where query latency of CG and\nFG embeddings was equalised, yet FG embeddings\nachieved considerably higher recall performance.\nThis outcome highlights the significant impact\nof hardware optimisations and similarity search\nimplementations on search performance. Without\ncustom implementations of hardware-level opti-\nmised bitwise operations, CG embeddings do not\nimprove query-time latency over FG ones when\nusing a standard similarity search implementation\nsuch as FAISS. Further research into standardised\noptimisations of CG search for modern hardware\narchitectures is recommended to fully leverage\ntheir potential efficiency advantages.\nStorage efficiency. In terms of storage effi-\nciency, CG models offer significant advantages.\nThe 64-bit embeddings used for the evaluated CG\nmodels were over 15.8 times smaller than the most\ncompact FG embeddings evaluated (256D). For\ncategory-level retrieval tasks with large amounts\nof relevant retrieval samples where FG models\ndo not yield improvements in retrieval perfor-\nmance, the storage savings of CG models become\nparticularly compelling."}, {"title": "Limitations", "content": "The scalability experiments were conducted\nthrough synthetic duplication of standard bench-\nmark datasets. While this approach offers insights\ninto the computational scalability of the evaluated\nmodels, it does not account for potential degra-\ndation in retrieval performance as the retrieval\ncandidate pool increases.\nOur study primarily focused on existing mod-\nels and search implementations, aiming for a\ncomprehensive view of the current practical state\nof the field. This approach aligns with the expe-\nriences of practitioners who may rely on publicly\navailable tools and implementations, providing\ninsights into the performance characteristics of\nwidely accessible models and search techniques.\nHowever, the application of customised batching\nimplementations within the selected models, as\nwell as custom hardware-level optimisations of\nCG bitwise operations-both aspects likely to be\nimplemented in production-level use cases-could\noffer more insightful perspectives on the perfor-\nmance of proprietary production-level pipelines."}, {"title": "Conclusion", "content": "While the conceptual differences in performance\nbetween FG and CG image-text retrieval models\nare well-documented in the literature, empiri-\ncal data quantifying these differences has been\nsparse. This study introduced the library and\ntoolkit FiCo-ITR, which provides a standardised\ntoolkit for evaluating both FG and CG image-text\nretrieval models, which addresses this empirical\ngap. The results within this paper indicate that\nwhile FG methods excel in instance-level retrieval,\nCG approaches demonstrate competitive perfor-\nmance in category-level tasks where a large num-\nber of relevant samples must be retrieved. Despite\nthe potential for computational efficiency offered\nby CG models, practical evaluations showed com-\nparable query latencies between FG continuous\nembeddings and CG bitwise hash code embed-\ndings, highlighting the practical impact of recent\nhardware optimisations. These results highlight\nthat the notion of FG models offering more robust\nretrieval performance while CG models are more\nefficient is not straightforward; instead, these char-\nacteristics depend on the specific retrieval task\nand implementation. Future work will focus on\ntwo key areas: First, the development of a large-\nscale image-text retrieval benchmark to enable\nscalability experiments on real data. Second, the\nexploration of hybrid coarse-to-fine approaches\nthat leverage insights from this study to bal-\nance retrieval performance and efficiency. These\ndirections aim to further bridge the gap between\nFG and CG methodologies, potentially leading\nto more robust and scalable image-text retrieval\nsystems."}, {"title": "Declarations", "content": "Code availability. The source code for\nthe FiCo-ITR library and toolkit can be\nfound in the project's GitHub repository:\nhttps://github.com/MikelWL/FiCo-ITR.\nConflict of interest. The authors declare no\nConflict of interest.\nEthical approval. This article contains no data\nor other information from studies or experimenta-\ntion involving human or animal subjects."}]}