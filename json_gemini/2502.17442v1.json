{"title": "Thinking Before Running! Efficient Code Generation with Thorough Exploration and Optimal Refinement", "authors": ["Xiaoqing Zhang", "Yuhan Liu", "Flood Sung", "Xiuying Chen", "Rui Yan"], "abstract": "Code generation is crucial in software engineering for automating the coding process efficiently. While test-time computation methods show promise, they suffer from high latency due to multiple computation rounds. To overcome this, we introduce ThinkCoder, a framework that combines thorough exploration with optimal refinement. The exploration phase diversifies the solution space by searching for potential solutions, followed by a refinement phase that enhances precision. This approach allows us to select the best solution through careful consideration before taking action, avoiding excessive trial and error. To further minimize test-time computation overhead, we introduce preference-driven optimization with Reinforced Self-Training (ReST), which uses exploration trajectories from ThinkCoder to guide LLM's evolution. By learning preferences, this approach improves LLM's exploration efficiency, reducing computational costs while maintaining accuracy. ThinkCoder boosts the performance of multiple base LLMs, excelling on benchmarks like HumanEval and MBPP. Compared to SOTA models, it improves Pass@1 by 1.5% over MapCoder with just 21.7% of the computation cost. Against AgentCoder, ThinkCoder achieves a 0.6% higher Pass@1 after 2 rounds, outperforming Agent-Coder's 5 rounds. Additionally, ReST with success trajectories enhances efficiency, allowing models like LLaMA2-7B to achieve competitive results using only 20% of the computational resources. These results highlight the framework's effectiveness and scalability. The code will be released upon acceptance.", "sections": [{"title": "1 Introduction", "content": "Recent research advances indicate that large language models (LLMs) have demonstrated remarkable capabilities in various programming-related domains, such as code generation (Zheng et al., 2023; Chaudhary, 2023; Dong et al., 2024; Li et al., 2024a), code refinement (Chen et al., 2023; Guo et al., 2024; Zheng et al., 2024; Ridnik et al., 2024; Liu et al., 2024; Zhang et al., 2024), and software testing (Li et al., 2023; Jalil et al., 2023; Wang et al., 2024; Jones et al., 2024). By utilizing extensive training data, LLMs can understand complex programming tasks, generate syntactically accurate code, and even enhance the quality of their solutions through iterative improvement.\nIn the context of code generation, researchers have made significant progress in improving prompts to encourage LLMs to generate higher-quality answers. This includes two main directions: optimizing the path for answer exploration and incorporating rich reflection information. In the first direction of path optimization, for example, Self-Planning (Jiang et al., 2024) reduces problem complexity in code generation by introducing a planning phase, which refines the process of solution generation. Similarly, CodeCoT (Huang et al., 2023a) improves the exploration by employing a Chain-of-Thought prompt, enabling models to reason through each step systematically. In the second direction, the integration of reflection, test cases, and feedback is progressively being used to steer and enhance the generation process. For instance, RethinkMCTS (Li et al., 2024b) integrates verbal feedback into its Monte Carlo Tree Search framework to correct errors and improve code generation. Similarly, AgentCoder (Huang et al., 2023b) boosts efficiency and accuracy by using interactions between multiple agents to provide valuable input.\nDespite their impressive capabilities, a critical challenge remains in fully exploiting the potential of LLMs for high-quality code generation, as their performance is still constrained by issues such as high computation costs and limited effectiveness in integrating iterative feedback. To enhance efficiency and minimize trial-and-error costs, it's essential to define the improvement direction"}, {"title": "2 Related Work", "content": "Test-time Compute. Test-time computing methods use self-play and additional verifiers to enhance performance, despite the computational overhead. For example, training a Verifier with Preference Reward Model data and combining strategies like Best-of-N Weighted, Beam Search, and Lookahead Search can help (Snell et al., 2024). ToolLLM (Qin et al., 2023) evaluates tool usage by comparing success rates and solution quality to ChatGPT-ReACT, employing ToolEval as a Verifier for optimal API calls using depth-first search. However, API-based Verifiers are limited by training data and struggle with generalization. In contrast, our approach employs a task-based testing tool as a Verifier, offering a simpler and more effective solution that is closely aligned with the specific task requirements.\nMulti-Agent Code Generation. Deriving multiple agents by mimicking the different stages of human programming can effectively improve both the quality and efficiency of code generation. For instance, AgentCoder (Huang et al., 2023b) introduces programmer agents, test design agents, and test execution agents, integrating code generation, test case generation, and test feedback into the code generation process. Similarly, MapCoder (Islam et al., 2024) introduces retrieval agents, planning agents, coding agents, and debugging agents, iterating on code generation quality through more frequent agent communication and reflection. However, MapCoder incurs higher computational overhead and relies solely on sample I/O for code execution, limiting test case coverage and robustness. In contrast, our framework, ThinkCoder, addresses these limitations by self-verification efficiently without excessive computational costs.\nInstruction Tuning for Code. High-quality code generation models often require instruction fine-tuning based on large-scale pre-trained language models. The fine-tuning data generally comes from real-world collections or is artificially synthesized. For example, Code Alpaca (Chaudhary, 2023) combines the SELF-INSTRUCT strategy with code fine-tuning, CODELLAMA (Roziere et al., 2023) generates questions, solutions, and test cases through prompts, and OctoPack (Muennighoff et al., 2023) directly collects data from Git. LLMs fine-tuned with instructions possess stronger code generation capabilities. However, existing methods often lack comprehensive guidelines for data collection, volume management, and fine-tuning strategies. To address this gap, we present a detailed fine-tuning strategy for Llama2-7B using instruction-tuning data collected via CodeQwen (Bai et al., 2023), providing a transparent and reproducible methodology that supports the advancement of instruction-tuned"}, {"title": "3 ThinkCoder", "content": "Figure 1 illustrates the structure of ThinkCoder. Given an instruction $I = [p, prompt]$ for problem $p$, where prompt is the task description for code generation, the Exploration Agent produces $k$ candidate solutions. Each solution is paired with $m$ test cases, and the temperature $t$ of the LLM's generation is adjusted to promote diversity. These test cases are then aggregated into a Testing Pool $TP$. We define the exploration results as $E = [G, TP]$, where $G = \\{g_i | g_i = LLM(p, t) \\text{ for } i \\in \\{1, 2, ..., k\\}\\}$ , and $TP \\in \\\u00b6^{k \\times m}$, where $T$ represents each test. The Execution Agent sequentially executes each candidate solution in $G$ using the test cases from $TP$ to compute pass rates. The code $g_s$ with the highest pass rate is selected for further refinement. Here we define the pass rate of $g_i$ as $r_{g_i} = \\frac{\\text{number of tests passed by } g_i}{TP}$ and $g_s = \\text{arg max}_i r_{g_i}$. Feedback $f$ from failed test case $f_t$, along with the current code $g_s$, generated test cases $tests$, and problem $p$, is used to update the instructions $I = [p, g_s, f_t, f, tests, prompt]$ for the next exploration. This iterative process allows the Exploration Agent to generate improved solutions and test cases over $n$ iterations. To avoid unnecessary computation on well-solved problems, if $r_{g_s}$ is greater than 0, the refinement process is terminated, and the code $g_s$ is returned as the final solution. Please refer to the ThinkCoder algorithm 1 for more detailed information on these operations. Below, we detail the implementation of Thorough Exploration, Optimal Refinement, Exploration&Execution Agent, and Testing Pool."}, {"title": "3.2 Thorough Exploration", "content": "Before running the generated code and performing corrections, ThinkCoder conducted extensive thinking during the Thorough Exploration phase. By generating $k$ answers at once, it expanded the search area. The search for multiple answers was achieved by increasing the LLM's temperature. With the increasing temperature, the probability distribution of the LLM becomes flatter, reducing the differences between word probabilities and increasing the entropy of the distribution, which enhances the diversity of the generated text. The first iteration of exploration aims to generate diverse"}, {"title": "3.3 Optimal Refinement", "content": "To solve the problem efficiently, it is essential to first identify the optimal solution through careful consideration before making any code corrections. Therefore, Optimal Refinement involves selecting the best solution via self-verification and gathering the necessary information for deeper exploration and improvement in the next exploration. During self-verification, code quality is evaluated by the pass rate, determined by the Execution Agent running candidate code against all test cases. The code with the highest pass rate is selected for further refinement. From this, we randomly select an unresolved error to target for the current round of improvements. This ensures all errors are eventually addressed. The refinement instruction is created using the following information: the problem, the best solution identified so far, feedback, and test cases. After each refinement, the solution's pass rate is evaluated. If it surpasses any previous solution, it is marked as the best. Through multiple iterations, the best solution is continuously corrected and optimized, leading to a gradual increase in the pass rate within the testing pool. For more details, please refer to the Appendix A.3."}, {"title": "3.4 Exploration & Execution Agent", "content": "ThinkAgent consists of two agents: the exploration agent and the execution agent. The exploration agent uses LLMs to search for diversity code and test cases, while the execution agent runs the provided code snippets and test cases in a local environment to generate feedback. To improve the efficiency of the execution agent, we leverage multiprocessing to validate the code in parallel."}, {"title": "3.5 Testing Pool", "content": "The Testing Pool contains the test cases generated by the exploration agent which contains significantly more test cases than the ground truth tests. These test cases are categorized into three types: regular testing, boundary testing, and performance testing. A higher pass rate in the testing pool reflects stronger code robustness, making it essential to ensure the diversity and accuracy of the test"}, {"title": "4 Preference-driven Optimization", "content": "In ThinkCoder, the generated code evolves as the pass rate on the testing pool improves. Each increase in the pass rate signifies the development of better codes. This progression demonstrates that the exploration agent is gradually producing higher-quality outputs. By collecting the trajectory of these improvements, we aim to fine-tune the model, enabling the LLM to generate superior code and test cases more effectively. This approach helps to refine the exploration process, clarify its direction, and reduce the time required for exploration. Figure 2 describes the process of trajectory collection and fine-tuning of the LLM. We will explain this process from the perspectives of Data Collection, the Model Training and Inference."}, {"title": "4.1 Data Collection", "content": "The process of data collection for LLM fine-tuning involves two key steps. First, an exploration agent, denoted as $M_O$, generates diverse low-probability data $D_{M_O}$ by sampling at various temperatures. Second, this data is further refined using reflection to produce an enhanced dataset $R_{M_0}$. In our data generation task, offline data collection starts with a training set $D = \\{x_i\\}_{i=1}^N$, where $N$ is the number of instances in the dataset. We generate high-quality preference data $D_{M_O} \\cup R_{M_0}$ for LLM fine-tuning through the temperature-based and reflection-based generation process.\nTemperature-based Generation. During temperature-based generation, we obtain $D_{M_O}$ to enhance the diversity of responses. For a given problem $x$, the language model $M_O$ is used to explore $k$ times, generating $\\{y_i\\}_{i=1}^k$, where $y_i \\sim M_O(y|x)$. During exploration, a relatively large temperature value $t$ is set for $M_O$. The generated outputs are then evaluated using"}, {"title": "4.2 Model Training and Inference", "content": "Generating high-probability text does not necessarily align well with human preferences, which may lead to biases in various tasks (Perez et al., 2022). Reinforced Self-Training(ReST) mitigates this issue effectively by collecting offline data aligned with human preferences for online training (Gulcehre et al., 2023). We train the base model M with the success trajectory dataset $D_{M_0} \\cup R_{M_0}$:\n$L_{MLE}(\\theta_M) = -E_{(x, y) \\sim D_{M_0} \\cup R_{M_0}} log P_{\\theta_M}(y | x)$\nIn the inference process, two distinct configurations are used. The first configuration utilizes the fine-tuned base model, where the input $x$ is fed directly into the model to generate text with a temperature setting of $t = 0$. This setup is designed to assess whether the data collected through ThinkCoder enhances the quality of the generated output. The second configuration regards the fine-tuned base model as the exploration agent within the ThinkCoder framework. This approach generates and refines solutions using $k$ explorations and $n$ refinements, with various temperature settings $t$. It aims to investigate the framework's ability to improve the generated solutions through the success trajectories and the ThinkCoder framework. Comprehensive experimental results are presented in the evaluation section."}, {"title": "5 Experiments", "content": "We utilized the MBPP (Austin et al., 2021) and HumanEval (Chen et al., 2021) datasets to evaluate the effectiveness of our approach. To ensure a comprehensive assessment, we tested their enhanced versions\u2014MBPP-ET and HumanEval-ET (Dong et al., 2023)\u2014which include 80 and 35 times more test cases than the original datasets, respectively. Regarding the test data, the original problem set sizes for MBPP and HumanEval are 257 and 164, while their extended versions contain 378 and 164 problems, respectively. For training, we started with 374 MBPP training examples and 90 validation examples, combining them as the initial dataset. Using temperature-based and reflection-based generation methods, we expanded this set to over 2,000 examples for ReST training."}, {"title": "5.2 Baselines", "content": "In terms of LLM base model selection for the application of ThinkCoder, we considered a range of advanced instruction-tuned models, including the general-purpose models LLama2-7B-Chat, Kimi\u00b9,"}, {"title": "5.3 Implementation details", "content": "We set different parameter combinations for different experimental settings. To validate the improvement of the base LLMs, we set the exploration budget $k = 5$, the temperature received by the exploration agent $t = 0.5$, the number of optimal refinements $n = 5$, and the budget control threshold $\\theta = 0.8$. The exploration agent gen-"}, {"title": "6 Evaluation", "content": "Table 1 illustrates the performance changes of various base LLMs on the MBPP and HumanEval datasets as the Optimal Refinement iteration $n$ in-"}, {"title": "6.1 Performance on Code Generation", "content": "Table 1 shows that ThinkCoder achieves significant improvements in scenarios with fewer standard test cases, with performance on MBPP and HumanEval improving by 4%-70% after 5 iterations of refinement. In contrast, gains in more extensive scenarios, like MBPP-ET and HumanEval-ET, are more modest, ranging from 1%-35%. ThinkCoder's generated test cases effectively verify code, reducing the need for manual tests, but its performance saturates in complex scenarios, making it better suited for simpler tasks. Table 3 further demonstrates the potential of ThinkCoder. When the number of exploration iterations $k$ is extended to 20, ThinkCoder surpasses AgentCoder's performance, which needs 5 refinement rounds, in just 2 iterations."}, {"title": "6.2 Performance with ReST Training", "content": "From the results presented in Table 2, it is clear that ReST fine-tuning on LLama2-7B using successful trajectories collected by ThinkCoder significantly enhances the model's core capabilities. Firstly, LLama2-7B(ReST) demonstrates superior performance compared to both LLama2-7B(SFT) and LLama2-7B at iteration 0, highlighting that ReST training improves the exploration agent's"}, {"title": "6.3 Evolution of Generated Code and Tests", "content": "After each round of optimal refinement, we evaluated the generated solutions and testing pools. Using the ground truth test set, we analyzed the performance changes of the solutions in each round. Meanwhile, the ground truth code was used to assess the changes in the testing pools. As shown in Figure 4, the accuracy of both the solutions and testing pools improved simultaneously. This demonstrates that ThinkCoder is capable of generating continuously optimized test cases while also refining the quality of the solutions."}, {"title": "6.4 Hyperparameters study", "content": "We conducted hyperparameter selection experiments on the MBPP dataset. As shown in Figure 3(a), models exhibit optimal performance at different temperature settings. LLama2-7B-Chat, CodeQwen1.5-7B-Chat and Kimi improve in Pass@1 as temperature increases, while GPT-4-Turbo and GPT-40 perform better at lower temperatures. As depicted in Figure 3(b), the Pass@k performance of various base LLMs improves with an increasing exploration budget k, highlighting the models' ability to find optimal solutions. However, the performance eventually levels off, suggesting that the Pass@k performance of these models will eventually stabilize. As shown in Figure 3(c), Pass@1 performance of base LLMs improves"}, {"title": "6.5 Cost Analysis", "content": "In Figure 4, we compare the computational overhead between MapCoder and ThinkCoder. On the HumanEval and MBPP datasets, a single refinement with ThinkCoder outperforms the result of five iterations of refinement with MapCoder, while using fewer tokens. This is because MapCoder utilizes four agents that are all powered by LLMs, whereas our framework uses a single LLM to handle both code and test generation, while debugging is directly based on feedback from the Python environment, significantly reducing token usage."}, {"title": "7 Conclusion", "content": "In this paper, we introduce ThinkCoder, a framework designed to improve the code generation capabilities of LLMs. The proposed approach incorporates an extensive exploration to generate diverse solutions, followed by an optimal refinement stage that integrates the solutions, tests, and feedback as reflections to enhance performance. To minimize computational costs during exploration, we capture successful trajectories through ThinkCoder, which are then leveraged to fine-tune the LLM. This process not only boosts the model's performance in code generation but also reduces the number of refinement iterations needed to achieve optimal so-"}, {"title": "Limitations", "content": "In our current approach to managing the testing pool, we primarily focus on selective expansion but lack effective strategies for cleaning noise from the test set. Moreover, the diversity of the test set heavily depends on the generation capabilities of LLMs. Our work highlights the importance of tests and identifies the gap between generated and annotated cases, so we plan to address the enhancement of the testing pool in future work. Furthermore, while ReST training has demonstrated excellent results in enhancing the code generation capabilities of foundational LLMs, validation in ThinkCoder reveals rapid convergence. This indicates that fine-tuning teaches the LLM to find good answers, but the improvement in exploring the overall solution space remains limited. To address this, we look forward to introducing on-policy training methods in future work, combining data collection with fine-tuning to enable the model to evolve continuously during training, generating more diverse answers and further boosting the LLM's online exploration capabilities."}, {"title": "A Appendix", "content": "A.1 Exploration Budget Control"}, {"title": "A.2 Exploration for Tests", "content": "We randomly selected 50 tasks from the MBPP test data to observe the performance improvement and code execution overhead as the number of test"}, {"title": "A.3 Prompts for Exploration Agent", "content": "Figure 6 illustrates the input received by the exploration agent, including the problem, the best solution, the test cases from the ground truth, the generated test cases from testing pooling, and feedback on test failures."}, {"title": "A.4 Prompts for Execution Agent", "content": "Figure 7 shows the input of the executable Python environment, including the code and all test cases from the testing pool."}, {"title": "A.5 Example of Thorough Exploration", "content": "Figure 8 illustrates multiple solutions explored by the exploration agent."}, {"title": "A.6 Example of Optimal Refinement", "content": "To better understand the process of optimal refinement, Figure 9 illustrates a single refinement iteration. Before refinement, we leveraged the best solution output from the previous iteration, along with failed tests and unmet feedback sampled from the testing pool. Using this reflection information, the solution was updated and revalidated on the test cases in the testing pool. The success rate improved from 0.217 to 0.356, leading us to select the current solution as the best solution."}, {"title": "A.7 Example of Testing Pool", "content": "Figure 10 illustrates the test cases in the testing pool generated through multiple explorations by ThinkCoder."}]}