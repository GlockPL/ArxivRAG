{"title": "Warped Diffusion: Solving Video Inverse Problems with Image Diffusion Models", "authors": ["Giannis Daras", "Weili Nie", "Karsten Kreis", "Alexandros G. Dimakis", "Morteza Mardani", "Nikola B. Kovachki", "Arash Vahdat"], "abstract": "Using image models naively for solving inverse video problems often suffers from flickering, texture-sticking, and temporal inconsistency in generated videos. To tackle these problems, in this paper, we view frames as continuous functions in the 2D space, and videos as a sequence of continuous warping transformations between different frames. This perspective allows us to train function space diffusion models only on images and utilize them to solve temporally correlated inverse problems. The function space diffusion models need to be equivariant with respect to the underlying spatial transformations. To ensure temporal consistency, we introduce a simple post-hoc test-time guidance towards (self)-equivariant solutions. Our method allows us to deploy state-of-the-art latent diffusion models such as Stable Diffusion XL to solve video inverse problems. We demonstrate the effectiveness of our method for video inpainting and 8\u00d7 video super-resolution, outperforming existing techniques based on noise transformations. We provide generated video results in the following URL: https://giannisdaras.github.io/warped_diffusion.github.io/.", "sections": [{"title": "1 Introduction", "content": "Diffusion models (DMs) [79, 39, 82] can synthesize photorealistic imagery [73, 64, 66, 5, 60, 26]. They can be conditioned easily, through explicit training or guidance [25, 41], and have also been widely used to solve inverse problems [19, 86, 15, 16, 80, 84, 47, 56], in particular for image processing applications like inpainting and super-resolution [40, 72, 74, 66].\nHow do these methods extend to video processing and solving inverse problems on videos? Although video DMs are seeing rapid progress [38, 78, 9, 29, 8, 30, 7, 11], general text-to-video synthesis has not yet reached the level of robustness and expressivity comparable to modern image models. Moreover, no state-of-the-art video generative models are publicly available [11], and most video DMs are computationally expensive. To circumvent these challenges, a natural research direction is to leverage existing, powerful image generative models to solve video inverse problems."}, {"title": "2 Functional Video Generation", "content": "The basis of our approach, summarized in Figure 2, is to structure the generative model so that it is equivariant with respect to spatial deformations and apply these deformations successively to the input noise. Each deformation effectively warps the noise and the equivariance guarantees that each output image will be similarly warped. By using an optical flow from a real video to define a sequence of such deformations, a new video can be generated. To introduce our method, we first conceptualize both images and noise as functions on a domain and the generator as a mapping between two function spaces."}, {"title": "2.1 Functional Generative Modeling and Videos", "content": "Each video frame can be seen as a single image, and an image as a discretization of a vector-valued function on a rectangular domain. Consider the domain as the 2-D unit square $D = [0, 1]^2$, defining an image as a function $f : D \\rightarrow R^3$. For each location $x \\in D$, the value $f(x) \\in R^3$ represents an RGB color. We assume images have infinite resolution. To formulate a model that generates such images, we must have a notion of a space containing all possible images. We'll use the separable Hilbert space $H = L^2(D; R^3)$, with pointwise formulas interpreted almost everywhere with respect to the Lebesgue measure.\nWe assume that there exists a probability measure $\\mu$ on $H$ whose support is the set of photorealistic images and denote by $\\eta$ a known reference probability measure on $H$. In our case, $\\eta$ will be a Gaussian measure on $H$; for details, see Section 3.1. A generative model, or transport map, is then a mapping $G: H \\rightarrow H$ such that the pushforward of $\\eta$ under $G$ is $\\mu$ which we denote as $G_\\sharp \\eta = \\mu$. In particular, this implies that any random variable $\\xi \\sim \\eta$ will satisfy $G(\\xi) \\sim \\mu$. For diffusion models, $G$ can be defined by the probability flow ODE; see Section 3.2.\nGiven an image $f_0 \\in H$, a video with $n+1 \\in \\mathbb{N}$ frames is the sequence of functions $(f_0, f_1,..., f_n) \\in H^{n+1}$, where each subsequent function is obtained, at least partially, from the previous one by a deformation. Specifically, a sequence of bounded, injective maps $(T_j : D \\rightarrow D_j)_{j=1}^n$ exists such that\n$f_j(x) = f_{j-1}(T_j^{-1}(x)), \\quad \\forall x \\in D \\cap D_j, j = 1, ..., n,$  (1)\nwhere $D_j := T_j(D)$ and we assume that the sets $D \\cap D_j$ have positive Lebesgue measure. In video modeling, the sequence $(T_j)_{j=1}^n$ is usually referred to as the optical flow as it specifies how each pixel in the previous frame moves to the next frame. While the frames can also be conceptualized as a continuum in time, we work with a discrete set of frames for simplicity. We consider $D$ to always represent our fixed frame of vision and we allow each $T_j$ to move pixels outside of this frame. Therefore (1) determines $f_j$ only on the set $D \\cap D_j$ which contains pixels that remain within our field of vision."}, {"title": "2.2 Video Generation and Equivariance", "content": "Given our notion of a video and a generative model, we now describe how such a model can be used to generate new videos. Suppose we want to create a two-frame video given an initial frame $f_0 \\in H$ and a deformation map $T_1 : D \\rightarrow D_1$. Assume we have a generative model $G: H \\rightarrow H$ and an initial noise image $\\xi_0 \\in H$ such that $G(\\xi_0) = f_0$. From definition, the new frame of our video is"}, {"title": "2.3 White Noise", "content": "It is common practice to train generative models assuming the reference measure $\\eta$ is Gaussian white noise. Specifically, a draw $\\xi \\sim \\eta$ on the grid points $E_k = {x_1,...,x_k } \\subset D$ is realized as $\\xi(x_i) = \\chi_i$ for an i.i.d. sequence $\\chi_i \\sim N(0, 1)$ for $i = 1, . . ., k$. However, this approach is incompatible with our goal of having the generative model perform interpolation. For most deformations $T$ encountered in practice, none of the points in $T^{-1}(E_k)$ will match those in $E_k$. Consequently, each new evaluation $\\xi(T^{-1}(x_i))$ will be independent of the sequence {$x_i$}$_{i=1}^k$, making $\\xi(T^{-1}(E_k))$ appear as a new noise realization unrelated to $\\xi(E_k)$. This incompatibility arises because white noise processes are distributions, not regular functions, meaning realizations are almost surely not members of H [18]. [14] proposes a stochastic interpolation method to address this issue (see Appendix C for details and comparison). We generalize this idea and propose using generic Gaussian processes on H."}, {"title": "3 Method: Warped Diffusion", "content": "In Section 1, we formulated the problem of video generation as the computation of a series of functions warped by an optical flow and proposed the use of a generative model for inpainting and interpolating the warped functions. The main challenges which remain are defining a functional noise process which can be evaluated continuously and a generative model which is equivariant with respect to warping. We propose to use Gaussian processes for our functional noise and a guidance procedure within the sampling step of a diffusion model to overcome these challenges."}, {"title": "3.1 Gaussian Processes (GPs)", "content": "A Gaussian Process (GP) $\\eta$ is a probability measure on H completely specified by its mean element and covariance operator. For a mathematical introduction, see Appendix B. We identify Gaussian processes with positive-definite kernel functions $K : R^2 \\times R^2 \\rightarrow R$. Recall that $E_k = {X_1,...,X_k }$ denotes the grid points where we know the values of an image $f \\in H$. To realize a random function $\\xi \\sim \\eta$ on these points, we sample the finite-dimensional multivariate Gaussian $N(0, Q)$, where $Q \\in R^{k \\times k}$ is the kernel matrix $Q_{ij} = k(x_i, x_j)$ for $i, j = 1,..., k$.\nOnce sampled, given the fixed values $\\xi(E_k)$, $\\xi$ can be evaluated at any new point $x^* \\in D$ by computing the conditional distribution $\\xi(x^*) | (E_k)$ [65]. This approach allows us to realize"}, {"title": "3.2 Function Space Diffusion Models and Equivariance Self-Guidance", "content": "We will now focus on the generative model that needs to be equivariant to the noise transformations. Specifically, in this section, i) we introduce function space diffusion models, ii) we prove that if every prediction of the diffusion model is equivariant then the whole diffusion model sampling chain is equivariant to the underlying spatial transformations, and, iii) we describe equivariance self-guidance, our sampling technique for enforcing the equivariance assumption.\nFor ease of notation, we will present everything for the case of unconditional video generation. However, our method seamlessly incorporates any addition conditioning information that may be available. If $c_0,..., c_n \\in R$ is a sequence of known conditioning vectors then these can simply be passed into a conditional score model at the appropriate frame without any other change to our method; see Algorithm 1. Conditioning vectors could be, for example, low resolutions versions of a video or an original video with regions masked. In Section 4, we focus on such conditional tasks.\nFunction Space Diffusion Models. Typically, diffusion models are trained with white noise. As explained in Section 2.3, a principled continuous evaluation of the noise requires a functional process. We briefly describe diffusion models in the context of sampling using the Gaussian processes of Section 3.1. We show in Section 4.1 (Table 1) that a model trained with white noise can be fine-tuned to GP noise without any loss in performance.\nWhile it is possible to formulate diffusion models on the infinite-dimensional space H e.g. [52], we will proceed in the finite-dimensional case for ease of exposition. In particular, we will define the forward and backward process as a flow on a vector $u \\in R^k$, thinking of the entries as the values of a scalar function evaluated on the grid $E_k$ and recall that $Q$ is the kernel matrix on $E_k$.\nWe consider forward processes of the form,\n$du_t = (2 \\sigma(t) \\delta(t) Q)^{\\frac{1}{2}} dW_t, \\quad u(0) = u_0 \\sim \\mu,$ (3)\nwhere $W_t$ is a standard Wiener process on $R^k$ and $\\sigma$ is a scalar-valued, once differentiable function. This process results in conditional distributions $p(u_t|u_0) = N(u_0, \\sigma^2(t)Q)$, see [46]. Let $p(u_t, t)$ denote the density of $u_t$ induced by (3). Then the following backward in time ODE,\n$\\frac{du_t}{dt} = - \\sigma(t) \\delta(t) Q \\nabla_u \\log p(u_t, t)$ (4)\nstarted at $u(\\tau)$ distributed according to (3) has the same marginal distributions $p(u_t, t)$ as (3) on the interval [0, $\\tau$]; see [46]. Approximating $N(u_0, \\sigma^2(\\tau)Q)$ by $N(0, \\sigma^2(\\tau)Q)$, we may then define the generative model $G$ by the mapping $u(\\tau) \\leftrightarrow u(0)$ with reference measure $\\eta = N(0, \\sigma^2(\\tau)Q)$.\nSolving (4) requires knowledge of the score $\\nabla_u \\log p(u_t, t)$. Instead of learning the score, we opt for directly learning the weighted score $Q \\nabla_u \\log p(u_t, t)$. This design choice leads to faster sampling since we do not need to perform any expensive matrix multiplication with $Q$ at inference time.\nA generalized version of Tweedie's formula (for proof see Appendix A.2) implies:\n$Q \\nabla_u \\log p(u_t, t) = \\frac{E[u_0 | u_t] - u_t}{\\sigma^2(t)}.$ (5)\nWe approximate $E[u_0|u_t]$ with a neural network $h_\\theta$ by minimizing the denoising objective:\n$E_{t \\sim U(0, \\tau)} E_{u_0 \\sim \\mu} E_{u_t \\sim N(u_0, \\sigma^2(t)Q)} |h_\\theta(u_t, t) - u_0|^2.$ (6)"}, {"title": "4 Experimental Results", "content": "For all our experiments, we use Stable Diffusion XL [60] (SDXL) as our base image diffusion model. We start by finetuning SDXL on conditional tasks. We choose super-resolution and inpainting as the tasks of interest since they are both commonly used in the inverse problems literature and they represent two distinct scenarios: in super-resolution, the input condition is strong and in inpainting, the model needs to generate new content. For super-resolution, we choose a downsampling factor of 8. For inpainting, we create masks of different shapes at random, following the work of [57]. During the finetuning, we train the model to predict the uncorrupted image given the following inputs: i) the encoding of the noised image, ii) the noise level, and, iii) the encoding of the corrupted (downsampled/masked) image. To condition on the corrupted observation, we concatenate the measurements across the channel dimension. We train models with and without correlated noise on the COYO dataset [12] for 100k steps. We show realizations of independent and correlated noise in Figure 6. Additional implementation details are in Section F.2, including the parameters for the GP introduced in Section 3.1."}, {"title": "4.1 Training with correlated noise", "content": "The first step is to assess the quality of the trained models. To do so, we take images from a test split of the COYO dataset, we corrupt them (either by masking or downsampling) and we measure the conditional performance of the trained models. We use a diverse set of metrics that are commonly used in the inverse problems literature: CLIP Text Score [62], CLIP Image Score [62], SSIM [85], LPIPS [91], MSE, Inception Score [76] and FID [37]. The first five metrics measure point-wise restoration performance. Inception Score measures the quality of the generated distribution (without an explicit reference distribution). Finally, FID measures restoration performance in a distributional sense, i.e. it measures how close is the distribution after restoration to the ground truth distribution.\nWe report our results for the super-resolution and inpainting models in Table 1. The main finding is that finetuning with correlated noise does not compromise performance, i.e. SDXL models finetuned with correlated noise perform on par with SDXL models that are trained with independent noise. Particularly for inpainting, the GP models slightly outperform models trained with independent noise across all metrics. We provide qualitative results for our all models in Figure 1 and in Appendix Figures 7, 8.\nWe remark that the advantages of using an initial distribution other than white noise have been explored in prior work [20, 6, 42]. Our new finding is that a model initially trained with white noise can be easily fine-tuned to work with correlated noise. To the best of our knowledge, ours is the first work that shows that Stable Diffusion XL can be fine-tuned to work with correlated noise.\nWe underline that prior to fine-tuning Stable Diffusion XL produces unrealistic images when the sampling chain is initialized with correlated noise. Our experiments show that post-finetuning, the model can handle spatially correlated noise in the input without compromising performance. Our GP Warping mechanism requires models that can handle correlated noise. Hence, these fine-tunings are essential for the rest of the paper."}, {"title": "4.2 Noise Warping and Equivariance Self Guidance", "content": "In the previous experiments, we measured the restoration performance of the trained models for a single image and we established that models trained with correlated noise perform on par (or even outperform) models trained with independent noise. The next step is to measure the temporal behavior of the models, i.e. how well they work for videos."}, {"title": "4.3 Effect of Sampling Guidance for more general transformations", "content": "We proceed to evaluate our method on realistic videos. We measure performance on 600 captioned videos from the FETV [55] dataset. Since baseline inpainting methods fail even for very simple temporal transformations, we focus on 8\u00d7 super-resolution for our comparisons on FETV.\nFor our video results, we could not provide comparisons with the How I Warped Your Noise paper. At the time of this writing, there was no available reference implementation as we confirmed with the authors by direct communication. In any case, the authors acknowledge as a limitation of their work that their proposed method has \u201climited impact on temporal coherency\u201d when applied to latent models and that \u201call the noise schemes produce temporally inconsistent results\u201d [14]. Once again, we attribute this to the non-equivariance of the denoiser, which we mitigate with our guidance algorithm.\nWe proceed to evaluate our method and the baselines with respect to temporal consistency and mean restoration performance across frames, as we did for our inpainting experiments. We present our results in Table 3 and additional results in Figures 5, 8 of the Appendix and in the following URL as videos: https://giannisdaras.github.io/warped_diffusion.github.io/. As shown in Table 3, there is a trade-off between temporal consistency and restoration performance. Methods that perform better in terms of temporal consistency often have significantly worse performance across the other metrics. Our Warped Diffusion achieves a sweet spot: it has the lowest warping error by a large margin and it still maintains competitive performance across all the other metrics. On the contrary, methods that are based solely on noise warping, such as GP Warping and the simple interpolation methods, lead to significant performance deterioration for a small improvement in temporal consistency.\nNoise Warping Speed. We measure the time needed for a single noise warping. Our GP Warping mechanism takes 39ms per frame Wall Clock time, to produce the warping at 1024 \u00d7 1024 resolution. This is 16x faster than the reported 629ms number in [14]. If we use batch parallelization, our method generates 1000 noise warpings in just 46ms (at the expense of extra memory).\nNo Warping? A natural question is whether we can omit completely the noise warping scheme since equivariance is forced at inference time. We ran some preliminary experiments for super-"}, {"title": "5 Limitations", "content": "Our method has several limitations. First, the guidance term increases the sampling time, as detailed in the Appendix, Section F.3. For reference, processing a 2-second video takes roughly 5 minutes on a single A-100 GPU. Second, even though in our experiments we observed a monotonic relation between the warping error in latent space and warping error in pixel space, it is possible that for some transformations the decoder of a Latent Diffusion Model might not be equivariant. We noticed that this is a common failure for text rendering, e.g. in this latent video the model seems to be equivariant, but in the pixel video it is not. Third, the success of our method depends on the quality of the flow estimation - inconsistent flow estimation between frames will lead to flickering artifacts. For real videos, there might be occlusions and the estimation of the flow map can be noisy. We observed that in such cases our method fails, especially for challenging tasks such as video inpainting. The correlations obtained by following the optical flow field obtained from real videos might lead to a distribution shift compared to the training distribution. For such extreme deformations, our method produces correlation artifacts. This has been observed in prior work (see this video), but it also appears in our setting (e.g. see this video). Finally, our method cannot work in a zero-shot manner since it requires a model that is trained with correlated noise."}, {"title": "6 Conclusions", "content": "Warped Diffusion is a novel framework for solving temporally correlated inverse problems with image diffusion models. It leverages a noise warping scheme based on Gaussian processes to propagate noise maps and it ensures equivariant generation through an efficient equivariance self-guidance technique. We extensively validated Warped Diffusion on temporally coherent inpainting and superresolution, where our approach outperforms relevant baselines both quantitatively and qualitatively. Importantly, in contrast to previous work [14], our method can be applied seamlessly also to latent diffusion models, including state-of-the-art text-to-image models like SDXL [60]."}, {"title": "G Broader Impact", "content": "Our method allows the use of image diffusion models to solve video inverse problems. There are both positive and negative societal implications of such a method. On the positive side, our method does not require training of video models which is typically expensive and contributes to increasing the AI carbon footprint. Further, democratizes access to video editing tools. The average practitioner can now leverage state-of-the-art image models to solve video inverse problems. To illustrate the effectiveness of our method, we trained powerful text-conditioned inpainting models that work on arbitrary images from the web. On the negative side, these models can be used for adversarial image and video editing. Further, our method can be used for the generation of deepfakes."}, {"title": "A Theoretical Results", "content": null}, {"title": "A.1 Convolutions and Equivariance", "content": "To better understand (2), we consider the following example. We will assume that $\u03be : R^2 \u2192 R$ is a scalar-valued field (grayscale image) defined on the whole plane. Furthermore, we let G be given by a continuous convolution and $T_1^{-1}$ be a translation. In particular, for all $x \u2208 R^2$,\n$G(\u03be)(x) = \\int_{R^2} \u03ba(x - y)\u03be(y) dy, \\quad T_1^{-1}(x) = x - a$\nfor some compactly supported kernel $\u03ba : R^2 \u2192 R$ and a direction $a \u2208 R^2$. We then have\n$G(\u03be \u25e6 T_1^{-1})(x) = \\int_{R^2} \u03ba(x - y)\u03be(y \u2013 a) dy = \\int_{R^2} \u03ba((x \u2013 a) - y) \u03be(y) dy = G(\u03be)(T_1^{-1}(x))$\nby the change of variables formula. This shows that G is equivariant to all translations. This is a well-known property of the convolution and, in particular, it shows that convolutional neural networks"}, {"title": "A.2 Tweedie's Formula", "content": "In Section 3.2, we show a diffusion model can be trained and sampled from using Gaussian process noise instead of white noise. Our result depends on the following lemma which is a simple generalization of Tweedie's formula.\nLemma A.1. Let $x$ be a random variable with positive density $p_x \u2208 C^1(R^k)$. Let $\u03c3 > 0$ and $z \u223c N(0, Q)$ for some positive definite matrix $Q \u2208 R^{k\u00d7k}$ and assume that $x \\perp z$. Define the random variable\n$y = x + \u03c3z$\nand let $p_y \u2208 C^\u221e(R^k)$ be the density of $y$. It holds that\n$\\nabla_y \\log p_y(y) = Q^{-1} (E[x|y] \u2013 y).$\nProof. First note that by the chain rule,\n$\\nabla_y \\log p_y(y) = \\frac{1}{p_y(y)} \\nabla_y p_y(y) = \\frac{1}{p_y(y)} \\nabla_y \\int_{R^k} p(y, x) dx$\nwhere $p(y, x)$ denotes the joint density of $(y, x)$. Let $p(y|x)$ denote the Gaussian density of the conditional $y|x$. Since $p_x \u2208 C^1(R^k)$,\n$\\nabla_y \\int_{R^k} p(y, x) dx = \\int_{R^k} \\nabla_y p(y|x) p_x(x) dx.$\nTherefore, by the chain rule,\n$\\nabla_y \\log p_y(y) = \\frac{1}{p_y(y)} \\int_{R^k} p(y|x) p_x(x) \\nabla_y \\log p(y|x) dx.$\nSince $p(y|x)$ is the density of $N(x, \u03c3^2Q)$, a direct calculations shows that\n$\\nabla_y \\log p(y|x) = \u03c3^{-2} Q^{-1} (x - y).$\nFurthermore, Bayes' theorem implies\n$p(y|x) p_x(x) = p(x|y) p_y(y).$\nTherefore,\n$\\nabla_y \\log p_y(y) = \u03c3^{-2} Q^{-1} \\int_{R^k} (x - y) p(x|y) dx = \u03c3^{-2} Q^{-1} (E[x|y] \u2013 y)$\nas desired."}, {"title": "A.3 Flow Equivariance", "content": "In Section 3.2, we claim that if the score network $h_\\theta$ is equivarient with respect to a deformation $T^{-1}$, then the Euler scheme approximation of the map $u(\u03c4) \\leftrightarrow u(0)$ is equivarient with respect to $T^{-1}$. It is easy to see that this results holds so long as it holds for the single step $u_t \u2192 u_{t-\u2206t}$ defined by (7). We will assume that $h_\\theta$ safisfies (8) written as\n$h_\\theta(u_t \u25e6 T^{-1}, t) = h_\\theta(u_t, t) \u25e6 T^{-1}.$\nWe make sense of this equation by using RFF to define $u_t$ as a function on the plane and similarly bilinear interpolation to define $h_\u03b8(u_t, t)$ as a function. It follows by linearity of composition that\n$u_{t-\u2206t} \u25e6 T^{-1} = u_t \u25e6 T^{-1} - \\frac{\u2206t \u03c3(t)}{\u03c3(t)} (h_\u03b8(u_t, t) \u25e6 T^{-1} - u_t \u25e6 T^{-1}) = u_t \u25e6 T^{-1} - \\frac{\u2206t \u03c3(t)}{\u03c3(t)} (h_\u03b8(u_t \u25e6 T^{-1}, t) - u_t \u25e6 T^{-1})$\nwhich is the requisite equivariance of the map $u_t \u2192 u_{t-\u2206t}.$"}, {"title": "B Gaussian Processes", "content": "A probability measure \u03b7 on \u0397 is called Gaussian if there exists an element m\u2208 H and a self-adjoint, non-negative, trace-class operator Q : H \u2192 H such that, for all h, h' \u2208 H,\n$\\langle h, m \\rangle = \\int_H \\langle h, f \\rangle d\u03b7(f), \\quad \\langle Qh,h' \\rangle = \\int_H \\langle h, f - m \\rangle \\langle h', f - m \\rangle d\u03b7(f)$,\nwhere $\\langle , \\rangle$ denotes the inner product on H. The element m is called the mean while the operator Q is called the covariance. It is immediate from this definition that white noise is not included since the identity operator is not trace-class on any infinite dimensional space. This definition ensures that any realization of a random variable \u03be \u223c \u03b7 is almost surely an element of H. When the domain D of the elements of H is a subset of the real line, \u03b7 is often called a Gaussian process. We continue to use this terminology even when D is a subset of a higher dimensional space i.e. $R^2$ but remark that the nomenclature Gaussian random field is sometimes preferred.\nSince we working on a separable space, each such field on H has associated to it a unique reproducing kernel Hilbert space [18, Theorem 2.9] which is associated to a unique positive definite kernel [4]. In particular, there exists a positive definite function $\u03ba : D \u00d7 D \u2192 R$ for which Q is its associated integral operator. It follows that a Gaussian process can be uniquely identified with a positive definite kernel. Sampling and conditioning this process can then be accomplished via the kernel matrix.\nTo make this explicit, suppose that $X = {x_1,...,x_n} \\subset D$ and $Y = {y_1,..., y_m} \\subset D$ are two sets of points in D. We will slightly abuse notation and write\n$Q(X,Y)_{ij} := k(x_i, y_j), \\quad i = 1,..., n \\text{ and } j = 1, ..., m$\nfor the kernel matrix between X and Y and similarly Q(Y, X), Q(X, X), Q(Y, Y). Suppose that \u03be \u223c \u03b7 is a random variable from the Gaussian process with kernel \u039a and mean zero. To sample a realization of \u00a7 on the points X, we sample the finite dimensional Gaussian N (0, Q(X, X)). This can be written as\n$\u03be(X) = Q(X, X)^{1/2}Z$\nwhere $Z \u223c N(0, I_n)$. Suppose now that the points in Y are distinct from those in X and we want to sample \u03be on Y given the realization \u00a7(X). This can be done by conditioning [65]\n$\u03be(Y)|\u03be(X) \u223c N(Q(Y, X)Q(X, X)^{-1}\u03be(X), Q(Y, Y) \u2013 Q(Y, X)Q(X, X)^{-1}Q(X,Y)).$\nWhile the above formulas fully characterize sampling \u03be, working with them can be computationally burdensome. It is therefore of interest to consider a different viewpoint on Gaussian processes, in particular, through the Karhunen-Lo\u00e8ve expansion. The spectral theorem implies that Q possesses a full set of eigenfunctions $Q\u03c6_j = \u03bb_j \u03c6_j$ for $j = 1,2,...$ with some decaying sequence of eigenvalues $\u03bb_1 \u2265 \u03bb_2 \u2265 . . .$. The random variable \u03be \u223c N (0, Q) can be written as\n$\u03be = \\sum_{j=1}^\\infty \\sqrt{\u03bb_j} X_j \u03c6_j$\nwhere $X_j \u223c N(0, 1)$ is an i.i.d. sequence and the right hand side sum converges almost surely in the norm of H [18]. By truncating this sum to a finite number of terms, computing realizations of \u03be becomes much more computationally manageable. This inspires the random features approach to Gaussian processes which is the basis of our computational method; for precise details, see [65, 63, 87]."}, {"title": "C Brownian Bridge Interpolation", "content": "We show in Section 2.3 that a white noise process is not compatible with the idea of using a generative model to interpolate deformed functions. A potential way of dealing with this issue is to treat the original realizations of the white noise \u03be(Ek) as the fixed nodal points of a function \u03be and obtain the rest of the values via interpolation. It is shown in [14] that common forms of interpolation yield a conditional distribution \u03be(T\u22121(Ek))|\u03be(Ek) that is too dissimilar from the training distribution N(0, Ik) and thus the generative model produces blurry or disfigured images."}, {"title": "D Additional Results", "content": "In this section, we provide additional results that did not fit in the main paper. We visualize the difference between independent noise and noise from our GP in Figure 6. We present inpainting results from our SDXL inpainting model fine-tuned with GP noise in Figure 7. We present super-resolution results from our SDXL super-resolution model fine-tuned with GP noise in Figure 8. We further present warping errors with respect to the previous frame in Figure 4 for the inpainting results and warping errors for super-resolution for real videos in Figure 5. Finally, we present additional comparisons for super-resolution in Figure 10."}, {"title": "E Related Works", "content": "Our work is primarily related to three recent lines of research about the utility of diffusion models in inverse problems, video editing, and equivariance in function space diffusion models as elaborated below.\nDiffusion Models for Inverse Problems. Diffusion models have been recently received widespread adoption for solving inverse problems in various domains. Diffusion models can solve inverse problems in a few different ways. A simple way is to train or finetune a conditional diffusion model for each specific task to learn the conditional distribution from the degraded data distribution to the clean data distribution [71, 53, 77]. Some popular examples include SR3 [75] and inpainting stable diffusion [67]. We leverage stable diffusion inpainting in the present work. While successful, they however need to be trained (or finetuned) separately for each individual task that is computationally complex. Also, they are not robust to out of distribution data. To mitigate these challenges, plug-and-play methods have been introduced that utilize a single foundation diffusion model (e.g., stable diffusion) as a (rich) prior to solve many inverse problems at once [44, 70, 15, 47]. The crux of this approach is to modify the sampling post-hoc by either: (i) add guidance to the score function"}, {"title": "F Experimental Details", "content": null}, {"title": "F.1 Dealing with Optical Flows", "content": "We use the RAFT model to predict the optical flows [83"}]}