{"title": "COMPUTING VOTING RULES WITH IMPROVEMENT FEEDBACK", "authors": ["Evi Micha", "Vasilis Varsamis"], "abstract": "Aggregating preferences under incomplete or constrained feedback is a fundamental problem in social choice and related domains. While prior work has established strong impossibility results for pairwise comparisons, this paper extends the inquiry to improvement feedback, where voters express incremental adjustments rather than complete preferences. We provide a complete characterization of the positional scoring rules that can be computed given improvement feedback. Interestingly, while plurality is learnable under improvement feedback\u2014unlike with pairwise feedback-strong impossibility results persist for many other positional scoring rules. Furthermore, we show that improvement feedback, unlike pairwise feedback, does not suffice for the computation of any Condorcet-consistent rule. We complement our theoretical findings with experimental results, providing further insights into the practical implications of improvement feedback for preference aggregation.", "sections": [{"title": "1 Introduction", "content": "Classical social choice theory assumes that voters provide complete rankings of all candidates, which are then aggregated by a voting rule to select a winner [Brandt et al., 2016]. However, this assumption becomes impractical in many real-world scenarios involving a large number of candidates, as voters may be unable or unwilling to rank all of them. For example, in deliberation platforms like Polis [Small et al., 2021], users provide pairwise comparisons over a limited subset of opinions rather than full rankings. Similarly, in Reinforcement Learning from Human Feedback (RLHF)\u2014a methodology widely used to fine-tune large language models (LLMs)\u2014feedback often takes the form of comparison queries between pairs of outputs [Ouyang et al., 2022, Christiano et al., 2017].\nDespite their widespread use, pairwise or t-wise comparisons-rankings of t candidates-face fundamental limitations. Recent work by Halpern et al. [2024] shows that, even under ideal conditions where the preferences of the population over every t-wise query are fully known (i.e., for every ranking of t candidates, the proportion of the population that agrees with it is known), the winner under common voting rules cannot be determined. For example, the plurality winner cannot be reliably identified, and even randomized algorithms fail to perform better than random guessing.\nMotivated by these limitations, we explore an alternative type of feedback known as improvement feedback. Unlike elicitation methods based on rankings or pairwise comparisons, improvement feedback enables agents to iteratively refine an initial suggestion. For instance, in RLHF applications, improvement feedback could involve a user modifying a draft generated by an LLM to better align with their preferences Schick et al. [2022], Jin et al. [2023]. Similarly, in robotics, users might iteratively adjust a robot's trajectory or behavior to achieve their desired outcome Bajcsy et al. [2018], Yang et al. [2024]. In this setting, users typically refine queried options through targeted adjustments rather than identifying the optimal candidate outright [Shivaswamy and Joachims, 2015, Tucker et al., 2024].\nA simple observation is that improvement feedback offers a promising pathway to address some challenges posed by t-wise comparisons. For example, unlike pairwise feedback-which struggles to compute the plurality winner-improvement feedback enables users to iteratively refine a suggested candidate. As users provide targeted adjustments, they gradually reveal their top preferences, ultimately enabling the identification of the plurality winner through this iterative process. This raises the following questions:"}, {"title": "1.1 Our Contribution", "content": "We model improvement feedback as a process in which, when a user or agent is queried with a candidate ranked at position i in their preference order or ranking, they return, with some probability, a candidate ranked at position j, where $j < i$. The likelihood of returning the candidate at position j decreases as the distance $i - j$ increases, reflecting the agent's tendency to provide localized improvements. To formalize this, we introduce the concept of t-improvement feedback, where a user refines a queried option by selecting a better candidate from the t-above neighborhood of the queried candidate in their preference ranking-i.e., a candidate within the t positions above the queried candidate. The t-improvement feedback distribution specifies the probabilities of selecting a candidate from this neighborhood. The parameter t defines the size of this neighborhood, capturing how much better the returned candidate can be relative to the queried one. In practice, t is typically much smaller than the total number of candidates, m, reflecting the limited cognitive and practical effort users are willing to expend.\nOur goal is to investigate whether winners under various voting rules can be identified using t-improvement feedback queries over the underlying preferences of the agents, which are unknown to the algorithm. We consider an idealized setting, similar to Halpern et al. [2024], where the algorithm has access to the full statistical distribution of feedback responses. Specifically, with a sufficiently large number of t-improvement feedback queries, the algorithm knows the exact probability of receiving candidate b as feedback when querying candidate a, given the population's preferences and the underlying t-improvement feedback distribution. This idealized assumption strengthens our impossibility results and models scenarios where such statistical information is available or can be effectively estimated.\nIn Section 4, we study the well-known family of positional scoring rules and provide a complete characterization of the rules that are learnable under t-improvement feedback queries, for all practical values of t. We show that beyond plurality and a specific positional scoring rule uniquely determined by the t-improvement feedback distribution, as well as any linear combination of the two, no other positional scoring rule is learnable using t-improvement feedback queries for any value of $t < m/2 \u2013 2$. In fact, we show that even randomized algorithms cannot reliably identify the correct winner with a probability greater than 1/m in this case. As discussed in the introduction, in practical settings $t \\ll m$, making these findings particularly relevant. We also extend these negative results for every $t \\in [m - 1]$ under the uniform t-improvement distribution, where when a candidate a is queried, a candidate from its t-above neighborhood is returned uniformly at random.\nIn Section 5, we turn our attention to Condorcet-consistent rules, which select the Condorcet winner whenever one exists. While pairwise comparisons suffice to identify the Condorcet winner, we surprisingly show that under t-improvement feedback, no (randomized) algorithm can determine the Condorcet winner with probability greater than 1/m. This result applies under the uniform t-improvement feedback model for all values of t and extends to all $t < m/2 - 2$ for almost every t-improvement feedback distribution. We also discuss the one specific exception to this result in detail.\nThese theoretical results indicate that while t-improvement feedback is particularly effective for identifying the plurality winner, it is insufficient for computing many other widely studied rules in social choice theory, such as Kemeny, Copeland, or Borda, which, by contrast, can be identified using pairwise comparison feedback.\nLastly, in Section 6, we compare the two types of feedback through simulations. Interestingly, contrary to the theoretical results, t-improvement feedback queries turn to be more efficient in some cases for implementing rules\u2014such as Copeland or Borda\u2014that are learnable from pairwise comparison feedback but are not implementable by t-improvement feedback in the theoretical worst case."}, {"title": "1.2 Related Work", "content": "Our work contributes to the growing body of research on decision-making with partial access to votes. Filmus and Oren [2014] and Oren et al. [2013] studied t-top queries, where each agent reports their top t-candidates, and investigated how large t must be to reliably identify the correct winner under various voting rules. Similarly, Bentert and Skowron [2020] examined t-wise comparison queries, analyzing which voting rules can be implemented with this feedback. More recently, Halpern et al. [2024] provided a complete characterization of positional scoring rules that can be computed using t-wise comparison feedback.\nThese works build on foundational results regarding pairwise comparisons, which are often represented as (weighted) tournament graphs. Tournament graphs serve as the primary input for many well-known voting rules, including Borda count and several Condorcet-consistent rules, such as Copeland, Kemeny, and Minimax Brandt et al. [2016]. Our work builds on and extends the results of Halpern et al. [2024], which we discuss in greater detail later. To the best of our"}, {"title": "2 Model", "content": "For $k \\in \\mathbb{N}$, let $[k] = \\{1, 2, ..., k\\}$. We consider a set $M = \\{a_1,...,a_m\\}$ of m candidates. A ranking or preference $\\sigma$ over the candidates is a bijection $\\sigma : [m] \\rightarrow M$, where $\\sigma(i)$ returns the candidate that is the i-th most preferred candidate in $\\sigma$ and $\\sigma^{-1}(a)$ returns the position of candidate a in $\\sigma$. We denote by $\\mathcal{L}(M)$ the set of all m! possible rankings over M. We use $a \\succ_{\\sigma} b$ to denote that candidate a is ranked above candidate b under ranking $\\sigma$.\nLet $\\Delta(\\mathcal{L}(M))$ be the set of probability distributions over $\\mathcal{L}(M)$. A preference profile corresponds to a distribution $D \\in \\Delta(\\mathcal{L}(M))$ which represents the proportion of users in the population that have each ranking. For example, if $Pr_{\\sigma \\sim D}[\\sigma = a_1 \\succ ... \\succ a_m] = 1/3$ this means that 1/3 of the population holds the ranking $a_1 \\succ ... \\succ a_m$.\nGiven a permutation over the candidates $\\pi : M \\rightarrow M$, we define $\\pi \\circ \\sigma$ as the ranking obtained by permuting the candidates in $\\sigma$ according to $\\pi$. The special case $\\pi_{ab}$ swaps only candidates a and b, such that $\\pi_{ab}(a) = b, \\pi_{ab}(b) = a$, and $\\pi_{ab}(c) = c$ for all $c \\neq a, b$. For preference profiles, we denote by $\\pi \\circ D$ the preference profile induced by sampling $\\sigma \\sim D$ and outputting $\\pi \\circ \\sigma$. Lastly, for swapping a and b, $D^{a\\leftrightarrow b} = \\pi_{ab} \\circ D$.\nVoting Rules. A voting rule is a function $f : \\Delta(\\mathcal{L}(M)) \\rightarrow M$ that takes as input a preference profile and outputs a candidate as the winner. We call the winner of f the f-winner. In this work, we focus on two main families of voting rules.\nThe first family is the family of positional scoring rules, which are defined using a scoring vector $s = (s_1, s_2, ..., s_m) \\in \\mathbb{R}^m$ with $s_i \\geq s_{i+1}$ for all $i \\in [m - 1]$ and $s_1 > s_m$. Without loss of generality, we usually assume $s_m = 0$. The score of a candidate $a \\in M$ under a positional scoring rule $f_s$, parametrized by a scoring vector s, given a preference profile $D \\in \\Delta(\\mathcal{L}(M))$, is defined as follows:\n$sc_s(a, D) = \\sum_{i=1}^m Pr_{\\sigma\\sim D}[\\sigma^{-1}(a) = i] \\cdot s_i$.\nThe rule $f_s$ then returns the candidate with the highest score as the winner, breaking ties arbitrarily. Some well-known positional scoring rules are plurality, parametrized by $s_{plu} = (1, 0, ..., 0)$; veto, parametrized by $s_{veto} = (1, ..., 1, 0)$; and Borda count, parametrized by $s_{Borda} = (m \u2013 1, m \u2013 2,..., 0)$.\nThe second family is the family of Condorcet-consistent rules. A candidate $a \\in M$ is called the Condorcet winner if they beat every other candidate in pairwise majority comparisons, i.e., $Pr_{\\sigma \\sim D}[a \\succ_{\\sigma} b] > 0.5$ for all $b \\in M \\setminus \\{a\\}$. A Condorcet-consistent voting rule f selects the Condorcet winner whenever one exists. Famous examples of Condorcet-consistent rules include Copeland's Rule, Kemeny's Rule, the Minimax Rule, Ranked pairs and many others Brandt et al. [2016].\nImprovement Feedback Distribution. In the improvement feedback setting, when an agent is queried with a candidate a ranked at position i in the agent's preference ranking $\\sigma$, the agent returns an improved candidate b ranked at position j, where $j < i$. The candidates ranked in positions $j \\in [max(i \u2013 t, 1), i - 1]$ are referred to as the t-above neighborhood of $\\sigma(i)$, representing the set of candidates that are strictly preferred but within t positions above the queried candidate. The only exception occurs when a is the agent's first choice (i.e., $i = 1$), in which case no improvement is provided, and the agent returns a.\nMore formally, for a fixed parameter $t < m \u2013 1$, the t-improvement feedback distribution defines the probability of returning a candidate $\\sigma(j)$ when querying $\\sigma(i)$, denoted as $p_{i,j}^t$. The distribution satisfies the following properties:"}, {"title": "3 Maint-Indistinguishable Preference Profiles", "content": "We begin by constructing a family of preference profiles, denoted $D_{i,j,l,e}$, which will serve as the foundation for our impossibility results.\nLemma 3.1. Let $D_{i,j,l,e}$ be a preference profile defined with respect to two candidates a and b, as follows:\n1. With probability p, candidate a is fixed at position i (where $i > 1$), and candidate b is fixed at position j (where $j - i > t$).\n2. With probability $1 - p$, candidate a is fixed at position m, and candidate b is fixed at position l, where $1 < l < m - t$.\n3. Select a uniformly random ranking $\\tau_{S-ab}$ of all remaining candidates (excluding a and b). This ranking determines their relative order, and they are then assigned to the unoccupied positions.\nFor any $m \\geq 4$ and any $t \\in [m \u2013 1]$, the preference profiles $D_{i,j,l,e}$ and $D_{i,j,l,e}^{a\\leftrightarrow b}$ are t-indistinguishable when $p = \\frac{P_{i}^{t}}{P_{i}^{t}-P_{l}^{t}+P_{j}^{t}} $.\nProof. Let a, b be the candidates of the statement.\nSince $j - i > t$ and $l < m - t$, a, b are more than t positions apart in both rankings. Therefore, it holds that $Pr_{\\sigma\\sim D_{i,j,l,e}}[q_{t}^{0}(a) = b] = Pr_{\\sigma\\sim D_{i,j,l,e}}[q_{t}^{0}(b) = a] = 0$. Moreover, since $i > 1$ and $l > 1$, it holds that $Pr_{\\sigma\\sim D_{i,j,l,e}}[q_{t}^{0}(a) = a] = Pr_{\\sigma\\sim D_{i,j,l,e}}[q_{t}^{0}(b) = b] = 0$.\nFurthermore, for every $x \\in S-ab$, the following holds:\n$Pr_{\\sigma\\sim D_{i,j,l,e}}[q_{t}^{0}(a) = x] = p \\cdot \\sum_{k=max(i-t,1)}^{i-1} Pr[\\tau_{S-ab}^{0}(k) = x] \\cdot Pr_{\\sigma\\sim D_{i,j,l,e}}[q_{t}^{0}(a) = x | \\tau_{S-ab}^{0}(k) = x]$\n$+ (1-p) \\cdot \\sum_{k=m-t-1}^{m-2} Pr[\\tau_{S-ab}^{0}(k) = x] \\cdot Pr_{\\sigma\\sim D_{i,j,l,e}}[q_{t}^{0}(a) = x | \\tau_{S-ab}^{0}(k) = x]$\n$= p \\cdot \\frac{1}{m-2} \\sum_{k=max(i-t,1)}^{i-1} Pr_{\\sigma\\sim D_{i,j,l,e}}[q_{t}^{0}(a) = x | \\tau_{S-ab}^{0}(k) = x]$\n$+ (1-p) \\cdot \\frac{1}{m-2} \\sum_{k=m-t-1}^{m-2} Pr_{\\sigma\\sim D_{i,j,l,e}}[q_{t}^{0}(a) = x | \\tau_{S-ab}^{0}(k) = x]$\n$= p \\cdot \\frac{1}{m-2} \\sum_{k=max(i-t,1)}^{i-1} \\frac{t}{P_{i,k}^{t}}$\n$+ (1-p) \\cdot \\frac{1}{m-2} \\sum_{k=m-t-1}^{m-2} \\frac{t}{P_{m,k+1}^{t}}$\n$= p \\cdot \\frac{1}{m-2} \\frac{P_{i}^{t}}{t}$\n$+ (1-p) \\cdot \\frac{1}{m-2} \\frac{P_{m}^{t}}{t}$\n$= \\frac{1}{m-2},$\nwhere the second equality holds because $\\tau_{S-ab}$ is chosen uniformly at random and the second last equality holds because the sum of probabilities of returning a candidate ranked above the queried candidate, over all valid positions within the t-above neighborhood, equals 1.\nSimilarly,\n$Pr_{\\sigma\\sim D_{i,j,l,e}}[q_{t}^{0}(b) = x] = p \\cdot \\sum_{k=max(j-t,1)-1}^{j-2} Pr[\\tau_{S-ab}^{0}(k) = x] \\cdot Pr_{\\sigma\\sim D_{i,j,l,e}}[q_{t}^{0}(b) = x | \\tau_{S-ab}^{0}(k) = x]$\n$+ (1-p) \\cdot \\sum_{k=l}^{l+t-1} Pr[\\tau_{S-ab}^{0}(k) = x] \\cdot Pr_{\\sigma\\sim D_{i,j,l,e}}[q_{t}^{0}(b) = x | \\tau_{S-ab}^{0}(k) = x]$\n$= p \\cdot \\frac{1}{m-2} \\sum_{k=max(j-t,1)-1}^{j-2} Pr_{\\sigma\\sim D_{i,j,l,e}}[q_{t}^{0}(b) = x | \\tau_{S-ab}^{0}(k) = x]$\n$+ (1-p) \\cdot \\frac{1}{m-2} \\sum_{k=l}^{l+t-1} Pr_{\\sigma\\sim D_{i,j,l,e}}[q_{t}^{0}(b) = x | \\tau_{S-ab}^{0}(k) = x]$\n$= p \\cdot \\frac{1}{m-2} \\sum_{k=max(j-t,1)-1}^{j-2} \\frac{t}{P_{j,k+1}^{t}}$\n$+ (1-p) \\cdot \\frac{1}{m-2} \\sum_{k=l}^{l+t-1} \\frac{t}{P_{l,k}^{t}}$\n$= p \\cdot \\frac{1}{m-2} \\frac{P_{j}^{t}}{t}$\n$+ (1-p) \\cdot \\frac{1}{m-2} \\frac{P_{l}^{t}}{t}$\n$= \\frac{1}{m-2}.$\nNext, we prove that for $p = \\frac{P_{i}^{t}}{P_{i}^{t}-P_{l}^{t}+P_{j}^{t}}$, and every $x \\in S-ab$, it holds that $Pr_{\\sigma\\sim D_{i,j,l,e}}[q_{t}^{1}(x) = a] = Pr_{\\sigma\\sim D_{i,j,l,e}^{a\\leftrightarrow b}}[q_{t}^{1}(x) = b]$. Note that\n$Pr_{\\sigma\\sim D_{i,j,l,e}}[q_{t}^{1}(x) = a] = p \\cdot \\sum_{k=i}^{i+t-1} Pr[\\tau_{S-ab}^{0}(k) = x] \\cdot Pr_{\\sigma\\sim D_{i,j,l,e}}[q_{t}^{1}(x) = a | \\tau_{S-ab}^{0}(k) = x]$\n$= p \\cdot \\frac{1}{m-2} \\sum_{k=i}^{i+t-1} Pr_{\\sigma\\sim D_{i,j,l,e}}[q_{t}^{1}(x) = a | \\tau_{S-ab}^{0}(k) = x]$\n$= p \\cdot \\frac{1}{m-2} \\sum_{k=i}^{i+t-1} \\frac{P_{i+1,1}^{t}}{t}$\n$= p \\cdot \\frac{t}{m-2} \\frac{P_{i}^{t}}{t},$\nwhere the last equality is by definition of $P_{i}^{t}$.\nSimilarly,\n$Pr_{\\sigma\\sim D_{i,j,l,e}^{a\\leftrightarrow b}}[q_{t}^{1}(x) = b] = p \\cdot \\sum_{k=j-1}^{j+t-2} Pr[\\tau_{S-ab}^{0}(k) = x] \\cdot Pr_{\\sigma\\sim D_{i,j,l,e}}[q_{t}^{1}(x) = b | \\tau_{S-ab}^{0}(k) = x]$\n$+ (1-p) \\cdot \\sum_{k=l}^{l+t-1} Pr[\\tau_{S-ab}^{0}(k) = x] \\cdot Pr_{\\sigma\\sim D_{i,j,l,e}}[q_{t}^{1}(x) = b | \\tau_{S-ab}^{0}(k) = x]$\n$= p \\cdot \\frac{1}{m-2} \\sum_{k=j-1}^{j+t-2} Pr_{\\sigma\\sim D_{i,j,l,e}}[q_{t}^{1}(x) = b | \\tau_{S-ab}^{0}(k) = x]$\n$+ (1-p) \\cdot \\frac{1}{m-2} \\sum_{k=l}^{l+t-1} Pr_{\\sigma\\sim D_{i,j,l,e}}[q_{t}^{1}(x) = b | \\tau_{S-ab}^{0}(k) = x]$\n$= p \\cdot \\frac{1}{m-2} \\frac{P_{j}^{t}}{t}$\n$+ (1-p) \\cdot \\frac{1}{m-2} \\frac{P_{l}^{t}}{t}$\n$= \\frac{P_{i}^{t}}{t}$\n$\\sqrt{m-2}$.\nObserve that we can make the two probabilities above equal by setting $p = \\frac{P_{i}^{t}}{P_{i}^{t}-P_{l}^{t}+P_{j}^{t}}$. Lastly, for any $x, y \\in S\u2212ab it holds that $Pr_{\\sigma\\sim D_{i,j,l,e}}[q_{t}^{0}(x) = y] = Pr_{\\sigma\\sim D_{i,j,l,e}^{a\\leftrightarrow b}}[q_{t}^{0}(x) = y]$, since the two preference profiles differ only with respect to a, b. From all the above, we get that $D_{i,j,l,e}$ and $D_{i,j,l,e}^{a\\leftrightarrow b}$ are t-indistinguishable by setting p as stated in the lemma."}, {"title": "4 Positional Scoring Rules", "content": "In this section, we consider the family of positional scoring rules and provide a complete characterization of the rules that are learnable under t-improvement feedback queries. For general t-improvement feedback distributions, this characterization applies for all $t < m/2 - 2$, while for the uniform t-improvement feedback distribution, it extends to all $t \\in [m - 1]$.\nIn Section 4.1, we show that for any value of $t < m/2 \u2013 2$, the only learnable scoring vectors are linear combinations of $s_{t}$and $s_{plu}$, where $s_{t} = (P_{1}^{t}, ..., P_{m}^{t})$. In fact, we prove that even randomized algorithms cannot identify the correct winner for any scoring vector s outside the $span(s_{t}, s_{plu})$ with a probability greater than 1/m. In Section 4.2, we extend these negative results to cover all values of t under the uniform t-improvement feedback distribution.\nFor showing our negative results, we start with the following lemma, which establishes the conditions under which a family of t-indistinguishable profiles, as described in Lemma 3.2, can be constructed. This lemma closely resembles Lemma 4.1 of Halpern et al. [2024].\nLemma 4.1. Fix a scoring vector s. Let D and $D^{a\\leftrightarrow b}$ be two indistinguishable preference profiles and let a, b \u2208 M be two candidates such that $sc_s(a, D) \\neq sc_s(b, D)$. Then, there exists a family of preference profiles $\\{D^c\\}_{c \\in M}$ such that all profiles are t-indistinguishable from one another, but each candidate $c \\in M$ uniquely maximizes $sc_s(c, D^c)$.\nProof. Given D, where, without loss of generality, $sc_s(a, D) > sc_s(b, D)$, we construct a family of preference profiles $\\{D^c\\}_{c \\in M}$ in the same way as in Lemma 4.1 of Halpern et al. [2024]. Briefly, we sample a permutation $\\pi$ over the candidates uniformly at random. If $\\pi(b) = c$, we sample a ranking from $\\pi \\circ D^{a\\leftrightarrow b}$; otherwise, if $\\pi(b) \\neq c$, we sample a ranking from $\\pi \\circ D$. When $sc_s(a, D) > sc_s(b, D)$, Halpern et al. [2024] show in Lemma 4.1 that c is the unique score maximizer in $D^c$.\nIt remains to show that $\\{D^c\\}_{c \\in M}$ are t-indistinguishable from one another. Note that if D and $D^{a\\leftrightarrow b}$ are t-indistinguishable, then $\\pi \\circ D$ and $\\pi \\circ D^{a\\leftrightarrow b}$ are also t-indistinguishable for all $\\pi$. Consequently, for every $D^c$ and $D^{c'}$ and for all $x, y \\in M$, we have:\n$Pr_{\\sigma\\sim D^c}[q_{t}^{0}(x) = y] = \\sum_{\\pi \\in \\mathcal{L}(M): \\pi(b)\\neq c} \\frac{1}{m!} Pr_{\\sigma\\sim \\pi \\circ D}[q_{t}^{0}(x) = y] + \\sum_{\\pi \\in \\mathcal{L}(M): \\pi(b) = c} \\frac{1}{m!} Pr_{\\sigma\\sim \\pi \\circ D^{a\\leftrightarrow b}}[q_{t}^{0}(x) = y] \\cdot \\sum_{\\pi \\in \\mathcal{L}(M)} Pr_{\\sigma\\sim \\pi \\circ D}[q_{t}^{0}(x) = y]$"}, {"title": "4.1 General t-Improvement Feedback Distribution", "content": "Here, we consider general t-improvement feedback distributions. To provide a complete characterization of the scoring rules that are learnable under t-improvement feedback queries, we first prove the following lemma.\nLemma 4.2. For any $m \\geq 6$, any $t \\leq m/2 \u2013 2$, any pair of candidates a, b \u2208 M and any scoring vector $s \\notin span(s_{plu}, s_t)$, there exists a preference profile D such that $sc_s(a, D) \\neq sc_s(b, D)$, and D and $D^{a\\leftrightarrow b}$ are t-indistinguishable.\nProof. Consider the preference profile $D_{i,m,i+1}$ as defined in Lemma 3.1. From Lemma 3.1, we have that for all $i \\in \\{2, ..., m \u2212 t \u2212 2\\}$, $D_{i,m,i+1}$ and $D_{i,m,i+1}^{a\\leftrightarrow b}$ are t-indistinguishable for every $t \\leq \\frac{m}{4}$, when\n$p = \\frac{P_{i+1}^{t}}{P_{i}^{t}-P_{m}^{t}+P_{i+1}^{t}}$.\nIf it holds that $sc_s(a, D_{i,m,i+1}) \\neq sc_s(b, D_{i,m,i+1})$ for some $i \\in \\{2, ..., m \u2212 t \u2013 2\\}$, then setting $D = D_{i,m,i+1}$ satisfies the lemma.\nNow, suppose that for every $i \\in \\{2, ..., m \u2212 t \u2212 2\\}$, we have\n$sc_s(a, D_{i,m,i+1}) = sc_s(b, D_{i,m,i+1})$.\nThis implies that\n$p \\cdot s_i = (1-p) \\cdot s_{i+1}$\n$\\Rightarrow p = \\frac{s_{i+1}}{s_{i}+s_{i+1}}$\n$\\Rightarrow \\frac{s_{i+1}}{s_{i}+s_{i+1}} = \\frac{P_{i+1}^{t}}{P_{i}^{t}-P_{m}^{t}+P_{i+1}^{t}} \\Rightarrow \\frac{s_{i}}{P_{i}^{t}} = \\lambda,$\nfor all $i \\in \\{2, ..., m \u2212 t \u2212 2\\}$.\nAssuming $s_i/P_{i}^{t} = \\lambda$ for all $i \\in \\{2,...,m - t - 1\\}$, we next consider the preference profile $D_{2,i,2}$ as defined in Lemma 3.1. From Lemma 3.1, we have that for all $i \\in \\{m - t, . . ., m \u2013 1\\}$, $D_{2,i,2}$ and $D^{a\\leftrightarrow b}$ are t-indistinguishable for every $t < \\frac{m}{2} \u2013 2$, when\n$p = \\frac{P^{t}}{2P^{t}-P_{2}^{t}}$.\nIf it holds that $sc_s(a, D_{2,i,2}) \\neq sc_s(b, D_{2,i,2})$ for some $i \\in \\{m \u2013 t, . . ., m \u2013 1\\}$, then setting $D = D_{2,i,2}$ satisfies the lemma.\nNow, suppose that for every $i \\in \\{m - t,..., m 1\\}$, we have\n$sc_s(a, D_{2,i,2}) = sc_s(b, D_{2,i,2})$.\nThis implies\n$p \\cdot s_2 = (1-p) \\cdot S2 + p \\cdot Si \\Rightarrow p = \\frac{s_{2}}{2s_2-s_i}$.\nThus, we obtain\n$\\frac{P^{t}}{P+ P_{2}^{t}} = \\frac{s_{2}}{2s_2-s_i}$.\nFrom Equation (1), we know that $\\frac{s_{i}}{P_{i}^{t}} = \\lambda$. Then by combining Equation (1) and the above equality, we get that $\\frac{s_{i}}{P_{i}^{t}} = \\lambda$ for all $i \\in \\{2,...,m \u2013 1\\}$. This means that, unless $\\frac{s_{i}}{P_{i}^{t}} = \\lambda$ for all $i \\in \\{2,..., m 1\\}$, which implies that $s \\in span(s_{plu}, s_t)$, there exists a preference profile D satisfying the conditions of the lemma and the lemma follows."}, {"title": "4.2 Uniform t-Improvement Feedback Distribution", "content": "Here, we turn our attention to the uniform t-improvement feedback distribution. In this setting, we extend Lemma 4.2 to cover all values of t. To derive this result, we introduce an additional set of preference profiles, distinct from those in Lemma 3.1, and carefully analyze the resulting cases.\nLemma 4.5. Under uniform t-improvement feedback distribution, for any $t \\in [m \u2013 1]$, any pair of candidates a, b \u2208 \u041c and any scoring vector $s \\notin span(s_{plu}, s_t)$, there exists a preference profile D such that $sc_s(a, D) \\neq sc_s(b, D)$, and D and $D^{a\\leftrightarrow b}$ are t-indistinguishable.\nFrom the above lemma, we immediately derive the following theorem in a manner similar to Theorem 4.4.\nTheorem 4.6. Under uniform t-improvement feedback distribution, for any $m \\geq 6$, any $t \\in [m \u2013 1]$, and any scoring vector s:\n1. If $s \\in span(s_{t}, s_{plu})$, then using t-improvement feedback queries, the candidate that maximizes the score under s for any input profile D can be found.\n2. If $s \\notin span(s_{t}, s_{plu})$, then no randomized algorithm with access to t-improvement feedback queries can output the candidate with the maximum score under s for any input profile D with probability greater than 1/m."}, {"title": "5 Condorcet Consistent Rules", "content": "In the previous section, we showed that t-improvement feedback queries do not help us overcome the impossibility results associated with positional scoring rules and pairwise comparison queries, except for plurality and a specific positional scoring rule determined by the t-improvement feedback distribution. In this section, we extend these negative results to the family of Condorcet-consistent rules.\nIn Section 5.1, we demonstrate that for every $t \\in [m-1"}]}