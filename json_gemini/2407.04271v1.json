{"title": "Variational Partial Group Convolutions for Input-Aware Partial Equivariance of Rotations and Color-Shifts", "authors": ["Hyunsu Kim", "Yegon Kim", "Hongseok Yang", "Juho Lee"], "abstract": "Group Equivariant CNNs (G-CNNs) have shown promising efficacy in various tasks, owing to their ability to capture hierarchical features in an equivariant manner. However, their equivariance is fixed to the symmetry of the whole group, limiting adaptability to diverse partial symmetries in real-world datasets, such as limited rotation symmetry of handwritten digit images and limited color-shift symmetry of flower images. Recent efforts address this limitation, one example being Partial G-CNN which restricts the output group space of convolution layers to break full equivariance. However, such an approach still fails to adjust equivariance levels across data. In this paper, we propose a novel approach, Variational Partial G-CNN (VP G-CNN), to capture varying levels of partial equivariance specific to each data instance. VP G-CNN redesigns the distribution of the output group elements to be conditioned on input data, leveraging variational inference to avoid overfitting. This enables the model to adjust its equivariance levels according to the needs of individual data points. Additionally, we address training instability inherent in discrete group equivariance models by redesigning the reparametrizable distribution. We demonstrate the effectiveness of VP G-CNN on both toy and real-world datasets, including MNIST67-180, CIFAR10, ColorMNIST, and Flowers102. Our results show robust performance, even in uncertainty metrics.", "sections": [{"title": "1. Introduction", "content": "Convolutional Neural Networks (CNNs) have demonstrated remarkable success in numerous computer vision tasks, owing to their ability to capture hierarchical features in an equivariant manner. Other approaches, such as Group Equivariant CNNs (G-CNNs) (Cohen & Welling, 2016; 2017; Weiler & Cesa, 2019; Romero et al., 2022), extend equivariance to various symmetry groups, enhancing model robustness across different transformations. However, a limitation arises from the rigidity of these models, as the choice of the equivariance group is fixed a priori.\nIn real-world scenarios, datasets often exhibit equivariance to diverse types of transformations, and the nature of equivariance might not be the same across all data instances. For example, in the classification of handwritten images like MNIST, images of 6 or 9 may be described more naturally by invariance to partial rotations between -90\u00b0 and 90\u00b0, while a 180\u00b0 rotation might distort the classification between 6 and 9, as shown in Fig. 1a. In contrast, the other digits, 0, 1, 2, 3, 4, 5, 7, 8, may possess full equivariance to rotation. The challenge then lies in developing a neural network architecture that adapts the level of equivariance to the specific needs of the data.\nExisting efforts have addressed this issue, such as Partial G-CNN (Romero & Lohit, 2022), which learns varying levels of equivariance at different layers, or Relaxed G-CNN (Wang et al., 2022; van der Ouderaa et al., 2022), which incorporates relaxed kernel design. In particular, Partial G-CNN restricts the distribution of the output group space to break full equivariance. They introduce a convolution layer with a distribution whose support domain does not cover all group elements, effectively breaking equivariance. While this method has shown promising results, it imposes the same level of equivariance for all data points.\nIn this paper, we introduce a new group equivariant convolution that captures different levels of partial equivariance in a data-specific manner. We redesign the distribution of output group elements in Partial G-CNN to be conditioned on the input. For efficient computation, the data-dependent conditional distribution refers to features extracted from the previous layer, as these contain information about the input data. To train the conditional distribution without overfitting, we adopt Variational inference, treating the group elements in each layer as random variables. Thus, the problem becomes maximizing the evidence lower bound (ELBO), consisting of the log-likelihood for classification and the Kullback-Leibler (KL) divergence between the conditional distribution and a certain prior for regularization. Therefore, while the conditional distribution is regularized towards full equivariance, if full equivariance is harmful for the given data, it modifies the distribution to provide partial equivariance. Additionally, we address the unstable training issue in discrete group equivariance, which Partial G-CNN suffers from, by redesigning the reparametrizable distribution of the group elements. Our method, called Variational Partial G-CNN (VP G-CNN), shows promising results in terms of test accuracy and uncertainty metrics. It also demonstrates the ability to detect different levels of equivariance for each data point in one toy dataset, MNIST67-180, and three real-world datasets: CIFAR10, ColorMNIST, and Flowers102.\nTo sum up, our contributions can be summarized as follows:\n1. We propose input-aware partially equivariant group convolutions, which capture different levels of equivariance across data based on variational inference.\n2. We resolve the unstable training issue of discrete group equivariance involved in Partial G-CNN by redesigning the reparametrizable distribution for the discrete groups.\n3. We demonstrate promising results on real-world datasets: CIFAR10, ColoredMNIST, and Flowers102, alongside demonstrating strong calibration performance."}, {"title": "2. Preliminaries", "content": ""}, {"title": "2.1. Group Equivariance and Partial Equivariance", "content": "A representation of a group G on a Euclidean space $R^n$ can be defined as a function $\\rho$ mapping G to the general linear group on $R^n$ (i.e., the group of invertible n x n matrices with matrix multiplication as group composition and identity matrix as identity element), ensuring that $\\rho$ preserves the composition operator and the identity element of the group. When we possess representations of a group G in Euclidean spaces X and Y, denoted as $\\rho_X$ and $\\rho_Y$ respectively, a function $\\Phi : X \\rightarrow Y$ is termed equivariant to G if, for all $g \\in G$ and $x \\in X$, the following condition holds:\n$\\Phi(\\rho_X(g)(x)) = \\rho_Y(g)(\\Phi(x)).$   (1)\nIn simpler terms, this condition implies that $\\Phi$ does not actively utilize information that can be altered by group elements g.\nAs a more general concept, partial group equivariance, or partial equivariance, can be defined as follows:\nDefinition 2.1 (($S, \\epsilon, G$)-Partial Equivariance). Let $\\Psi : X \\rightarrow Y$ be a function and G be a group acting on X. The function $\\Psi$ is partially G-equivariant with respect to a subset $S \\subset X$ and an error threshold $\\epsilon > 0$ if the following holds,\n$\\underset{g \\in G}{sup} || \\Psi(\\rho_X(g)(x)) - \\rho_Y(g) (\\Psi(x)) || = 0, \\qquad x \\in S,$   (2)\n$\\underset{g \\in G}{sup} ||\\Psi(\\rho_X (g)(x')) - \\rho_Y(g) (\\Psi(x')) || \\leq \\epsilon, \\qquad x' \\in X \\setminus S,$\nthat is, it is equivariant on a given subset S and approximately equivariant outside S.\nThe set S is determined with respect to the given dataset and group, typically defined as a subset of X that excludes certain inputs known to possess specific symmetries. For example, in the MNIST dataset with respect to the SO(2) group, subset S includes digit images other than 6 and 9. Notice that for $x \\in S$, the function $\\Psi$ must exhibit full equivariance, while for $x \\notin S$, it must exhibit $\\epsilon$-approximate equivariance. This definition ensures that equivariance is enforced on a specific subset S of the domain, while allowing for $\\epsilon$-approximate equivariance with respect to inputs outside S.\nDefinition 2.2 (($C, \\epsilon, G$)-Partial Equivariance on Feature Map). Let G be a group acting on F and $\\Phi : F \\rightarrow F$ be a map between functions $f : G \\rightarrow R^d$ representing input feature maps on group G. The function $\\Phi$ is partially G-equivariant with respect to a subset $C \\subset F$ and an error threshold $\\epsilon > 0$ if for all $u \\in G$, it satisfies that:\n$\\underset{g \\in G}{sup} ||(\\mathcal{L}_g f)(u) - (\\mathcal{L}_g \\Phi(f))(u)|| = 0, \\qquad f \\in C,$   (3)\n$\\underset{g \\in G}{sup} ||(\\mathcal{L}_g f')(u) - (\\mathcal{L}_g \\Phi(f'))(u)|| \\leq \\epsilon, \\qquad f' \\in F \\setminus C$"}, {"title": "2.2. G-CNN and Partial G-CNN", "content": "The convolutional layers of CNNs for an image can be described in terms of a function $f : R^2 \\rightarrow R^3$ that maps the position of a pixel to its RGB vector and represents the input image, and a kernel $k : R^2 \\rightarrow R^{3\\times d}$, where d is the output feature dimension. They output $(k * f) : R^2 \\rightarrow R^d$ defined by $(k * f)(y) = \\int_{R^2} k(x - y) f(x) dx$. The convolutional neural network exhibits translation equivariance due to the property $\\mathcal{L}_g(k * f) = k * \\mathcal{L}_g f$, where $\\mathcal{L}_g$ denotes a translation (shift) operation of image pixels: $\\mathcal{L}_g f(x) = f(x - t)$. Likewise, the convolutional layers of G-CNN utilize the equivariance property of the convolution operation on an extended space defined on a certain group G, which may include a translation group.\nLifting convolution. We want to do the group convolution on a group G, but the input like an image is typically a map defined on a space $E \\subset R^m$ and so it needs to be lifted to a map from the group G. The lifting convolution performs this lifting. If there is an embedding of E to G so that E can be regarded as a subgroup of G, we have, for an input feature map $f : E \\rightarrow R^3$ and a kernel map $k : G \\rightarrow R^{3 \\times d}$, the following lifting convolution $k *_{lift} f : G \\rightarrow R^d$: for all $u \\in G$,\n$(k *_{lift} f)(u) = \\int_{v \\in E} k(vu) f(v) d\\mu_E(v)$   (4)\nwhere elements in E are viewed as group elements in G, and $\\mu_E$ is the restriction of the left Haar measure of G to the subgroup E. Under an appropriate condition, the lifting convolution defined on group G is equivariant to the group G, i.e. for all $g \\in G$,\n$(\\mathcal{L}_g(k *_{lift} f))(u) = \\mathcal{L}_g(k *_{lift} f)(u),$   (5)\nwhere $\\mathcal{L}_g f(u) = f(g^{-1}u)$.\nGroup convolution. The group convolution generalizes the regular convolution for equivariances with respect to general groups. Once the inputs are feature maps from G, the group equivariant convolution for an input feature map $f : G \\rightarrow R^d$ and a kernel $k : G \\rightarrow R^{d\\times n}$, where n is the output feature dimension, is defined as follows:\n$(k * f)(u) = \\int_{v \\in G} k(v^{-1}u) f(v) d\\mu_G(v),$   (6)\nwhere $\\mu_G$ is the left Haar measure of the group G. Similarly to the regular convolution, the group convolution is G-equivariant, that is, $k * \\mathcal{L}_g f = \\mathcal{L}_g(k * f)$.\nPartial group convolution. Inspired by Augerino (Benton et al., 2020), Partial G-CNN (Romero & Lohit, 2022) introduced a partially equivariant group convolution whose output feature space is determined by a distribution q(u), where $u \\in G$. It modified the group convolution as follows:\n$(k * f)(u) = \\int_{v \\in G} q(u) k(v^{-1}u) f(v) d\\mu_G(v).$   (7)\nFor instance, when G is the 2-dimensional rotation group SO(2) with radian values in $[-\\pi, \\pi]$, the distribution q(u) can be defined as the push forward of the exponential map $exp: \\mathfrak{g} \\rightarrow G$ of the distribution $Unif[R(-\\theta), R(\\theta)]$ on the Lie algebra $\\mathfrak{g}$, where $\\theta$ is a learnable parameter on radian space and $R: R \\rightarrow so(2)$, and represents the maximum possible rotations in $R^2$. That is,\n$u = exp(t), \\qquad t \\sim Unif [R(-\\theta), R(\\theta)].$   (8)\nIf the full equivariance (i.e. $\\theta = \\pi$) is harmful for training, the model modifies the $\\theta$ to be less than $\\pi$. However, Partial G-CNN fails to guarantee the partial equivariance for a non-empty S in Definition 2.1, when $\\theta < \\pi$. This is because, when $\\theta$ becomes less than $\\pi$, Partial G-CNN loses equivariance to G for all $x \\in X$. This departure from equivariance violates the condition specified for a subset S if $S \\neq 0$. The model either exhibits full equivariance when $\\theta = \\pi$ or broken equivariance when $\\theta < \\pi$. For convenience, we omit the exponential map and mapping R when we describe the distribution of group elements, and write $q(u; \\theta) = Unif[-\\theta, \\theta]$ or $q(u) = Unif[-\\theta, \\theta]$.\nColor equivariance $H_m$. We aim to achieve equivariance not only with respect to the standard group SE(2), but also concerning color shifts. In (Lengyel et al., 2023), color equivariance is defined as being equivariant to changes in hue. It is explained that the Hue-Saturation-Value (HSV) color space represents hue using an angular scalar value, and shifting hue involves a straightforward additive adjustment followed by a modulo operation. When translating the HSV representation into the three-dimensional RGB space, a hue shift corresponds to a rotation along the (1, 1, 1) diagonal vector. Color equivariance is established in terms of a group by defining $H_m$, which consists of multiples of 360/m\u00b0 rotations around the (1, 1, 1) vector in $R^3$. $H_m$ is a subgroup of SO(3), the group of all rotations about the origin in $R^3$. The group operation is matrix multiplication, acting on the continuous space of RGB pixel values in $R^3$. Consequently, color-equivariant convolutions can be constructed using discrete SO(3) convolutions when the RGB pixels of an image are treated as $R^3$ vectors forming three-dimensional point clouds."}, {"title": "3. Variational Partial G-CNN", "content": ""}, {"title": "3.1. Input-Aware Partial Convolution", "content": "In order to achieve partial equivariance defined in Definition 2.1, we need to make the distribution q(u) input-aware, and design q(u|x) for each input x. One approach is to put q(u|x) for every layer, but doing so would be memory-inefficient, especially for the continuous group convolutions. This approach requires retaining the group elements sampled from q(u|x) for all convolution layers during feed-forwarding.\nTherefore, for partial equivariance, our new convolution at layer l + 1 uses $q(u|f^{(l)})$ where $f^{(l)}$ is the output of the previous layer l. Since as a feature, $f^{(l)}$ contains information about the input data, this scheme has a potential to identify data-specific equivariance, while being memory-efficient. Concretely, we modify the convolutions in Eqs. 4 and 6 as follows:\n$(k *_{lift} f)(u) = \\int_{v \\in E} q(u|f)k(v^{-1}u) f(v) d\\mu_E(v),$   (9)\n$(k * f)(u) = \\int_{v \\in G} q(u|f)k(v^{-1}u) f(v) d\\mu_G(v).$\nThe distribution q(u|f) here must be partially equivariant in order to achieve partial equivariance in these convolutions. For example, if the input f is the image of digit 7 or 8, which require full equivariance to SO(2), q(u|f) can be just the uniform distribution for all rotations in $R^2$: $q(u|f) = Unif[-\\pi, \\pi]$. Note that in this case, q(u|f) is equivariant to SO(2) in the following sense: $q(u|f) = q(gu|\\mathcal{L}_g f)$ for all $g \\in SO(2)$. On the other hand, for the images of digit 6 or 9, which require only partial equivariance to SO(2), q(u|f) can be a uniform distribution with a narrower range, such as $Unif[-\\pi/2, \\pi/2]$, or just a dirac-delta distribution $\\delta(u)$. Note that in this case, q(u|f) may fail to satisfy the equivariance condition, i.e., $q(u|f) \\neq q(gu|\\mathcal{L}_g f)$ for some $g \\in G$. The next proposition gives one sufficient condition for ensuring partial equivariance of our convolutions:\nProposition 3.1. Assume that the conditional distribution q(u|f) is partially equivariant with respect to a group G and an equivariant subset $C \\subset F$ in the following sense:\n$\\underset{g \\in G}{sup} ||q(u|f) - q(gu|\\mathcal{L}_g f)|| = 0, \\qquad f \\in C,$\n$\\underset{g \\in G}{sup} ||q(u|f') - q(gu|\\mathcal{L}_g f')|| \\leq \\epsilon, \\qquad f' \\in F \\setminus C,$   (10)\nwhere $\\mathcal{L}_g f(u) = f(g^{-1}u)$, and kernel k and input f of the group convolutions defined in Eq. 9 are bounded. Then, the group convolutions are also partially equivariant to G and C.\nThe proof is presented in Appendix A.1. For continuous groups, the integrals in the convolutions are intractable, so we typically employ Monte Carlo approximation to estimate the convolution operation by uniformly sampling from the Haar measure $d\\mu_G$. Thus, the approximate partially equivariant group convolution is determined as follows:\n$(k * f) (u_j) = \\sum_{v_i} q(u_j|f)k(v_i^{-1}u_j) f(v_i).$   (11)\nNow, we describe how the distribution q(u|f) can be trained and implemented using variational inference with the reparametrization trick."}, {"title": "3.2. Variational Inference of q(u|f)", "content": "If we train q(u|f) with only the classification loss, since it encompasses all features f, it may overfit by tending to become another classifier itself, leading to a trivial distribution. To prevent this situation, we adopt variational framework to train the distribution q(u|f). Our goal is to maximize the log-likelihood log p(y|x) for x, y from a dataset D and it can be described as follows:\nlog p(y|x)  (12)\nwhere $x = f^{(0)}$, L is the number of layers of the model, and $u^{(l)}$ is the output group elements at layer l.\nTo estimate the approximate posterior $q(u^{(l)}|f^{(l)})$ at layer l, we maximize the evidence lower bound (ELBO) of the log-likelihood in Eq. 12:\n$\\mathcal{L}_{VP} = \\mathbb{E}_{\\{u^{(l)}\\}_{l=1}^L} [log P(y|f^{(0)}, \\{u^{(l)}\\}_{l=1}^L)] - \\sum_{l=1}^L D_{KL} [q(u^{(l)}|f^{(l)})||p(u^{(l)})],$   (13)\nwhere the expectation is over ${\\{u^{(l)}\\}_{l=1}^L} \\sim \\prod_{l=1}^L q(u^{(l)}|f^{(l)})$. Then, $\\mathbb{E}_D [log p(y|x)] \\geq \\mathcal{L}_{VP}$ and by maximizing $\\mathcal{L}_{VP}$, we can maximize the log-likelihood indirectly. The approximate posterior $q(u^{(l)}|f^{(l)})$ is the partially equivariant distribution shown in Eq. 9."}, {"title": "Rotation SO(2) (continuous).", "content": "Similar to Partial G-CNN (Romero & Lohit, 2022), we can define $q(u|f)$ a uniform distribution $Unif[-\\theta, \\theta]$ but $\\theta$ is calculated from encoding of the input feature, $\\theta = e_{\\phi}(f), \\theta \\in [0,1]$, then $r_{\\theta}(f, \\epsilon)$ is described as\n$r_{\\phi}(f, \\epsilon) = \\epsilon \\pi \\cdot e_{\\phi}(f), \\qquad \\epsilon \\sim Unif[-1,1].$   (17)\nIf $\\theta = 1$, the probabilities of all group elements are the same, while if $\\theta = 0$, the distribution becomes a dirac-delta distribution whose value is non-zero only at zero-rotation. This distribution is reparametrizable so we can estimate the gradient as in Eq. 16 with low variance.\nColor-shift $H_m$ (discrete). The color-shift group $H_m$ has m number of group elements and each represents 360/m\u00b0 rotations around the (1, 1, 1) vector in the three-dimensionl RGB vector space. To sample group elements in such a discrete group, Partial G-CNN utilizes Gumbel-Softmax trick (Maddison et al., 2017) with Straight-Through estimation but it suffers from unstable training (Romero & Lohit, 2022). We observe that the distribution $p(u)$ with learnable parameters irregularly change their distribution during training and this may be due to the multi-modality of Gumbel-Softmax. Therefore, we propose another probability distribution that samples the discrete group without Gumbel-Softmax and mimick the distribution described in the continuous group.\nFor sampling, we first encode the input feature to $\\theta = e_{\\phi}(f), \\theta \\in [0,\\infty)$ and sample ${\\{u_i\\}}_{i=1}^m$ from a discrete uniform distribution $Unif\\{1, 2, ..., m\\}$, corresponding to the uniform distribution in the continuous group. Then, we compute importance weights for each $u_i$ as\n$w_i = \\frac{exp(u_i / \\theta)}{\\sum_{i=1}^m exp(u_i/\\theta)}.$   (18)\nHere, $\\theta$ determines smoothness of the softmax function across each ith component; if $\\theta$ is large enough, $w_i$ converges to almost uniform. Now using Straight-Through estimator, we select which group element in ${\\{u_i\\}}_{i=1}^m$ should be non-zero.\nq(u_i/f) = {\n    1,\n    if\n    wi>\n    1 m\n    \u2212\n    \u03b7,\n    0,\n    otherwise,\n}   (19)\nwhere $\\eta \\in [0,1/m]$ is a hyperparameter that determines how easy to be selected as non-zero. As $\\theta$ increases, the difference in magnitude between $w_i$ decreases, and more elements surpass the threshold. Conversely, as $\\theta$ decreases, the difference in magnitude between $w_i$ increases, and fewer elements surpass the threshold. This principle is analogous to the distribution of continuous groups. For example, if $\\eta$ is zero, $w_i$ should be greater than 1/m to be non-zero so it always select only one group elements, whereas if $\\eta$ is 1/m, it always select every elements in the group. The model trains value of $\\theta$ so that it decides how many group elements are appropriate for given input. For instance, $m = 3, \\eta = 7/12, \\{u_i\\} = \\{3,2,1\\}$, and then the threshold 1/m - $\\eta = 0.25$. For $\\theta = 1, \\{w_i\\} = \\{0.67,0.24, 0.09\\}$ and 0.67 is the only value larger than 0.25, thereby only $u_1$ is selected. For $\\theta = 3, \\{w_i\\} = \\{0.45,0.32, 0.23\\}$ and 0.45, 0.32 are above the threshold, thus $u_1$ and $u_2$ are selected. Since at least one of the softmax result in Eq. 18 for m candidates should be greater than 1/m, Eq. 19 always selects at least one group elements."}, {"title": "3.3. Implementation", "content": "Utilizing the input-aware partial group convolution for every layers would be the best strategy to gain performance. However, there are limitations to performance improvement compared to the increase in parameters. Hence, throughout the experiments we set a portion of layers to be the input-aware partial convolution in a network. In fact, once at least one of the convolutional layers exhibits input-aware partial equivariance, the entire network becomes partially equivariant.\nProposition 3.2. If at least one of the convolutional layers in a G-CNN is partially equivariant to a group G and an equivariant subset $C \\subseteq F$, and its activation functions are equivariant with respect to G and L-Lipschitz continuous, and its kernel functions are bounded, then the entire G-CNN is also partially equivariant to G and C.\nIts proof is described in Appendix A.2. For example, in the CIFAR10 dataset, we apply the input-aware partial group convolution in the lifting convolution and the last group convolution only. In the Flower102 dataset, we apply it in the last two group convolution only. In addition, we use light-weighted encoder $e_{\\phi}$, which calculate $\\theta$ as in Eqs. 17 and 18, consisting of two global average pooling layers, two one-dimensional convolution, and one linear layer. The detailed architecture is described in Appendix C."}, {"title": "4. Related Work", "content": "Group equivariant networks. G-CNN (Cohen & Welling, 2016) proposed a convolutional neural network architecture ensuring equivariance to a group of input transformations, including translation, rotation, and reflection, thereby enhancing the model's ability to learn and generalize from data with inherent symmetries in a given dataset. Steerable CNN (Cohen & Welling, 2017) introduced a framework for constructing rotation-equivariant convolutional neural networks, enabling efficient and flexible modeling of rotational symmetries in image data by leveraging the theory of group representations. E(2)-CNN (Weiler & Cesa, 2019) demonstrated constraints based on group representations, simplifying them to irreducible representations and providing a general solution for E(2), thereby covering continuous group equivariance for images. CEConv (Lengyel et al., 2023) extended equivariance from geometric to photometric transformations by incorporating parameter sharing over hue shifts, interpreted as a rotation of RGB vectors, offering enhanced robustness to color changes in images.\nApproximate equivariance. RPP (Finzi et al., 2021) involved placing one equivariant neural network (NN) and one non-equivariant NN in parallel, with a prior imposed on the parameters of each NN. In contrast, PER (Kim et al., 2023) replaced the two components with a single non-equivariant NN and introduced a regularizer to drive the non-equivariant NN towards equivariance. Relaxed G-CNN (Wang et al., 2022) introduced a small linear kernel to G-CNN, which slightly breaks the group equivariance of the model. In Partial G-CNN (Romero & Lohit, 2022), a distribution of group elements in the output was adopted, allowing group convolutions to consider only a subset of group elements in the hidden space.\nInput-aware automatic data augmentation. MetaAugment (Zhou et al., 2021) presents an efficient approach to learning a sample-aware data augmentation policy for image recognition by formulating it as a sample reweighting problem, where an augmentation policy network adjusts the loss of augmented images based on individual sample variations. AdaAug (Cheung & Yeung, 2022) learns adaptive data augmentation policies in a class-dependent and potentially instance-dependent manner, addressing the limitations of methods like AutoAugment (Cubuk et al., 2019) and Population-based Augmentation (Ho et al., 2019) by efficiently adapting augmentation policies to specific datasets. InstaAug (Miao et al., 2023) learns input-specific augmentations automatically by introducing a learnable invariance module that maps inputs to tailored transformation parameters, facilitating the capture of local invariances. Singhal et al. (2023) designed a method to capture multi-modal partial invariance by parameterizing the distribution of instance-specific augmentation using normalizing flows."}, {"title": "A. Proofs", "content": ""}, {"title": "A.1. Proof of Proposition 3.1", "content": "Using change of variables, we expand the convolution integral when the group action $\\mathcal{L}_g$ acts on feature f.\n$(k * \\mathcal{L}_g f)(u) = \\int_{v \\in G} q(u|\\mathcal{L}_g f)k(v^{-1}u) f(g^{-1}v) d\\mu_G(v)$   (20)\n$= \\int_{v' \\in G} q(u|\\mathcal{L}_g f)k(v'^{-1}g^{-1}u) f(v') d\\mu_G(v').$   (21)\nOn the other hand, the convolution when $\\mathcal{L}_g$ acts on the output of the convolution is\n$\\mathcal{L}_g(k * f)(u) = \\int_{v \\in G} q(g^{-1}u|f)k(v^{-1}g^{-1}u) f(v) d\\mu_G(v).$   (22)\nThe equivariance error is represented by the difference between the group action on the input and on the output. Then, we bound the $l_2$-norm of the equivariance error using the Cauchy-Schwarz inequality:\n$|| (k * \\mathcal{L}_g f)(u) - \\mathcal{L}_g(k* f)(u)||_2 = ||\\int_{v \\in G} [q(u|\\mathcal{L}_g f) - q(g^{-1}u|f)]k(v^{-1}g^{-1}u) f(v) d\\mu_G(v) ||$   (23)\n$\\leq \\int_{v \\in G} ||q(u|\\mathcal{L}_g f) - q(g^{-1}u|f)||^2 d\\mu_G(v) \\int_{v \\in G} ||k(v^{-1}g^{-1}u) f(v)||^2 d\\mu_G(v)$   (24)\n$\\leq {\\begin{cases} 0, & f \\in C, \\\\ \\epsilon^2 \\cdot \\int_{v \\in G} ||k(v^{-1}g^{-1}u) f(v)||^2 d\\mu_G(v), & f \\in F \\setminus C \\end{cases}}$   (25)\nAccording to the partial equivariance of q(u|f) as in Proposition 3.1, the error becomes zero when $f \\in C$. Conversely, when $f \\in F \\setminus C$, since the kernel k and input f are bounded, the equivariance error is bounded by a certain value $\\epsilon'$. The proof for the lifting convolutions is the same because the integral in Eq. 25 is still bounded even when integrated over E instead of G"}, {"title": "A.2. Proof of Proposition 3.2", "content": "By mathematical induction", "as": "n$||(k_2 * \\sigma(k_1 * \\mathcal{L"}, "g f))(t) - \\mathcal{L}_g(k_2 * \\sigma(k_1 * f))(t)||_2$   (26)\n$= ||(k_2 * \\sigma [k_1 * \\mathcal{L}_g f \u2013 (k_1 * \\mathcal{L}_g f)"], "f$": "n$|| (\\mathcal{L}_g(k_1 * f) - (k_1 * \\mathcal{L}_g f)) (v)||_2,$   (31)\nwhich means if the equivariance error of the partially equivariant network is zero, the equivariance error of the whole network is also zero.\nOn one hand, for partially G"}