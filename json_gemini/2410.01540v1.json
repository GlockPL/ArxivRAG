{"title": "EDGE-PRESERVING NOISE FOR DIFFUSION MODELS", "authors": ["Jente Vandersanden", "Sascha Holl", "Xingchang Huang", "Gurprit Singh"], "abstract": "Classical generative diffusion models learn an isotropic Gaussian denoising process, treating all spatial regions uniformly, thus neglecting potentially valuable structural information in the data. Inspired by the long-established work on anisotropic diffusion in image processing, we present a novel edge-preserving diffusion model that is a generalization of denoising diffusion probablistic models (DDPM). In particular, we introduce an edge-aware noise scheduler that varies between edge-preserving and isotropic Gaussian noise. We show that our model's generative process converges faster to results that more closely match the target distribution. We demonstrate its capability to better learn the low-to-mid frequencies within the dataset, which plays a crucial role in representing shapes and structural information. Our edge-preserving diffusion process consistently outperforms state-of-the-art baselines in unconditional image generation. It is also more robust for generative tasks guided by a shape-based prior, such as stroke-to-image generation. We present qualitative and quantitative results showing consistent improvements (FID score) of up to 30% for both tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Previous work on diffusion models mostly uses isotropic Gaussian noise to transform an unknown data distribution into a known distribution (e.g., normal distribution), which can be analytically sampled (Song and Ermon, 2019; Song et al., 2021; Ho et al., 2020; Kingma et al., 2021). Due to the isotropic nature of the noise, all regions in the data samples x0 are uniformly corrupted, regardless of the underlying structural content, which is typically distributed in a non-isotropic manner. During the backward process, the model is trained to learn an isotropic denoising process that ignores this potentially valuable non-isotropic information. In image processing literature (Elad et al., 2023), denoising is a well studied topic. Following the work by Perona and Malik (1990) structure-aware guidance has shown remarkable improvements in denoising. Since generative diffusion models can also be seen as denoisers, we ask ourselves: Can we enhance the effectiveness of the generative diffusion process by incorporating awareness of the structural content of the data samples in the underlying dataset?\nTo explore our question, we introduce a new class of diffusion models that explicitly learn a content-aware noise scheme. We call our noise scheme edge-preserving noise, which offers several advantages. First, it allows the backward generative process to converge more quickly to accurate predictions. Second, our edge-preserving model better captures the low-to-mid frequencies in the target dataset, which typically represent shapes and structural information. Consequently, we achieve better results for unconditional image generation. Lastly, our model also demonstrates greater robustness and quality for generative tasks that rely on shape-based priors.\nTo summarize, we make the following contributions:\n\u2022 We present a novel class of content-aware diffusion models and show how it is a generalization of DDPM (Ho et al., 2020).\n\u2022 We conduct a frequency analysis to better understand the modeling capabilities of our edge-preserving model.\n\u2022 We run extensive qualitative and quantitative experiments across a variety of datasets to validate the superiority of our model over existing models.\n\u2022 We observed consistent improvements in pixel space diffusion. We found that our model converges faster to more accurate predictions and better learns the low-to-mid frequencies of the target data, resulting in FID score improvements of up to 30% for unconditional image generation and most remarkably a more robust behaviour and better quality on generative tasks with a shape-based prior.\nOur source code and other supplemental supporting results will be released in the public domain tobe-declared.github.io."}, {"title": "2 RELATED WORK", "content": "Most existing diffusion-based generative models (Sohl-Dickstein et al., 2015; Song and Ermon, 2019; Song et al., 2021; Ho et al., 2020) corrupt data samples by adding noise with the same variance to all pixels. These generative models can generate diverse novel content when the noise variance is higher. On the contrary, noise with lower variance is known to preserve the underlying content of the data samples. Rissanen et al. (2023) proposed to use an inverse heat dissipation model (IHDM), which can be seen as an isotropic Gaussian blurring model. The idea is to isotropically blur the images to corrupt them. Hoogeboom and Salimans (2023) proposed an improved version of IHDM, where they not only blur but also use isotropic noise to corrupt data samples. The resulting model gives far better quality improvements compared to IHDM. Recently, Huang et al. (2024a) introduced correlated noise for diffusion models (BNDM). They proposed to use blue noise which is negatively correlated and showed noticeable improvements both in visual quality and FID scores. While IHDM and BNDM also consider a form of non-isotropic noise, they do not explicitly take into account the structures present in the signal.\nVarious efforts (Bansal et al., 2023; Daras et al., 2023) were made to develop non-isotropic noise models for diffusion processes. Dockhorn et al. (2022) proposed to use critically-damped Langevin diffusion where the data variable at any time is augmented with an additional \"velocity\" variable. Noise is only injected in the velocity variable. Voleti et al. (2022) performed a limited study on the impact of isotropic vs non-isotropic Gaussian noise for a score-based model. The idea behind non-isotropic Gaussian noise is to use noise with different variance across image pixels. They use a non-diagonal covariance matrix to generate non-isotropic Gaussian noise, but their sample quality did not improve in comparison to the isotropic case. Yu et al. (2024) developed this idea further and proposed a Gaussian noise model that adds noise with non-isotropic variance to pixels. The variance is chosen based on how much a pixel or region needs to be edited. They demonstrated a positive impact on editing tasks."}, {"title": "3 BACKGROUND", "content": "Generative diffusion processes. A generative diffusion model can be described as a parameterized Markov chain where the next (perturbed) state of a data sample xt+1 will only depend on the current state xt at time t. Previous works on generative diffusion models (Song and Ermon, 2019; Song et al., 2021; Ho et al., 2020; Kingma et al., 2021; Rissanen et al., 2023; Hoogeboom and Salimans, 2023) are typically formulated as a Markovian Gaussian process defined within a time interval t \u2208 [0; T] and denoted by the following equation:\n$X_t = \\gamma_t x_0 + \\sigma_t \\epsilon$\n(1)\nhere, xt is the data sample diffused up to time t, xo stands for the original data sample, et is a standard normal Gaussian noise, and the signal coefficient \\gamma_t and noise coefficient \\sigma_t determine the signal-to-noise ratio (SNR) (\\gamma_t/\\sigma_t). The SNR refers to the proportion of signal retained relative to the amount of injected noise. Note that \\gamma_t and \\sigma_t are both scalars. Previous work on diffusion models has made several different choices for \\gamma_t and \\sigma_t respectively, leading to different variants, each with their own advantages and limitations. Typically, a noise schedule \u03b2t is employed to govern the rate at which \\gamma_t and \\sigma_t vary over time (Ho et al., 2020). Let us first define at = 1 \u2212 \u03b2t and \u0101t = \u03a0i=1 \u03b1i. Song and Ermon (2019) then defined their forward diffusion process with \\gamma_t = \u0101t and \\sigma_t = \u221a1 \u2212 \u0101t, leading to a process with exploding variance. Instead, Ho et al. (2020) chose \\gamma_t = \u221a\u0101t and \\sigma_t = \u221a1 \u2212 \u0101t which better preserves variance. Rissanen et al. (2023), on the other hand, consider a forward process that serves as a low-pass filter to corrupt the information in the data, i.e. blurring in the context of image data. Their forward heat-dissipation process is formulated as:\n$x_t = A_t x_0 + \\sigma_t \\epsilon$\n(2)\nwhere At = VT ZVT is a blurring operator based on the discrete cosine transform VT. Note that in Eq. (2) the noise coefficient \\sigma is kept constant, in contrast to previous works following Eq. (1). This formulation is based on the observation that the solution to the heat dissipation equation:\n$\\frac{\\partial x}{\\partial t} = c(x,t) \\Delta x,$\n(3)\ncan be formulated as the solution to an eigenvalue problem in the frequency domain. In Eq. (3), \u0414x represents the image Laplacian and c(x, t) is a scalar diffusion coefficient, implying isotropic diffusion. Hoogeboom and Salimans (2023) showed that the forward heat-dissipation process (isotropic blurring) in the spatial domain is equivalent to a non-isotropic diffusion process in the frequency domain. In particular, a different \\gamma_t is assigned to each discrete frequency component of VT.\nForward and backward process posteriors. The isotropic diffusion process formulated in Eq. (1) has the following marginal distribution (see Kingma et al. (2021) for more details):\nq(xt|x0) = N(\u03b3t x0, \u03c32tI)\n(4)\nMoreover, it has the following Markovian transition probabilities:\nq(xt|xs) = N(\u03b3t|s xs, \u03c32t|s I)\n(5)\nwith the forward posterior signal coefficient \u03b3t|s = \u03b3t/\u03b3s and the forward posterior variance (or square of the noise coefficient) \u03c32t|s = \u03c32t \u2212 \u03c32s and 0 < s < t < T. For a Gaussian diffusion process, given that q(xs|xt, x0) \u221d q(xt|xs)q(xs|x0), one can analytically derive a backward process that is also Gaussian, and has the following marginal distribution:\nq(xs|xt, x0) = N(\u03bct\u2190s, \u03c32t\u2190sI).\nThe backward posterior variance \u03c32t\u2190s has the following form:\n$\\sigma^2_{t \\gets s} = \\frac{1}{\\frac{\\gamma^2_t}{\\sigma^2_t} + (\\frac{\\gamma^2_s}{\\sigma^2_s})^{-1}}$\n(7)"}, {"title": "4 EDGE-PRESERVING NOISE", "content": "Inspired by Perona and Malik (1990), we aim to design a linear diffusion process that incorporates edge-preserving noise. To obtain a content-aware linear diffusion process, we apply the idea of edge-preserved filtering (anisotropic diffusion) to the noise term of Eq. (1). More specifically, we define an edge-preserving forward process as follows:\n$x_t = \\sqrt{\\alpha_t} x_0 + \\frac{\\sqrt{1-\\alpha_t}}{\\sqrt{1 + \\lambda ||\\bigtriangledown x||}} \\epsilon$\n(12)\nObserve that formally, our forward process looks very similar to the isotropic forward process introduced in Eq. (1): we made the same choice \u03b3t = \u221a\u03b1t for the signal coefficient as DDPM (Ho et al., 2020), and only modified the noise coefficient \u03c3t. In particular, we multiply DDPM's choice for \u03c3\u03c4 = \u221a1 \u2212 \u03b1t by a non-isotropic diffusion coefficient c(x0, t) defined in Eq. (11). This results in the noise coefficient for our process becoming a tensor \u03c3t instead of a scalar \u03c3t. Intuitively, we suppress noise on the edges and leave noise unchanged in more uniform image regions. In our formulation, we also consider \u03bb to be time-varying (more details in section Section 4.2). Furthermore, it is important to note that we do not consider the original anisotropic diffusion as presented in Eq. (10). Instead, our diffusion coefficient only takes into account the gradient magnitude of x0. In the original formulation, the diffusion coefficient c(x, t) is time-dependent (it depends on the state of the data sample xt at each point in time) and therefore the resulting forward process would no longer be linear."}, {"title": "4.1 FORWARD HYBRID NOISE SCHEME", "content": "The forward edge-preserving (anisotropic) process described in Eq. (12) in its pure form is not very meaningful in our setup. This is because if the edges are preserved all the way up to time t = T, we end up with a rather complex distribution pT(x) that we cannot analytically take samples from. Instead, we would like to end up with a well-known prior distribution at time t = T, such as the standard normal distribution. To achieve this, we instead consider the following hybrid forward process:\n$x_t = \\sqrt{\\alpha_t} x_0 + \\frac{\\sqrt{1-\\alpha_t}}{\\sqrt{(1 - \\tau(t)) \\frac{1}{\\sqrt{1+\\lambda ||\\bigtriangledown x||}} + \\tau(t)}} \\epsilon$\n(13)\nThe function \u03c4(t) now appearing in the denominator of the diffusion coefficient is the transition function. When \u03c4(t) < 1, we obtain edge-preserving noise (the edge-preservation is stronger when \u03c4(t) \u2248 0). The turning point where \u03c4(t) = 1 is called the transition point t0. At the transition point, we switch over to isotropic noise with scalar noise coefficient \u03c3\u03c4 = \u221a1 \u2212 \u0101t. This approach allows us to flexibly design noise schedulers that start off with edge-preserving noise and towards the end of the forward process fall back to an isotropic diffusion coefficient. Practically, one can choose any function for \u03c4(t), as long as it maps to [0; 1] and \u03c4(t) = 1 for t in proximity to T. We performed an ablation for different transition functions in Section 5.2. We would like to draw attention to how our model is a generalization of DDPM. If we set the transition function to be a constant \u03c4(t) = 1, we end up with exactly the forward process used in DDPM. Choosing any other non-constant function for \u03c4(t) leads to a hybrid diffusion process that consists of a mix of an edge-preserving stage and an isotropic stage (at \u03c4(t) = 1)."}, {"title": "4.2 TIME-VARYING EDGE SENSITIVITY \u03bb(t)", "content": "The edge sensitivity parameter \u03bb controls the granularity of the content preservation on image edges. By setting \u03bb very low, all levels of edges and fine details are preserved. The higher we increase \u03bb, the less details will be preserved. By choosing a very high \u03bb (e.g. \u03bb > 1e\u22121), the edge-preserving stage of the diffusion process almost resembles an isotropic process because the noise suppression on the edges is unnoticeable. We study this impact in detail in the ablation study in Section 5.2.\nWe found that choosing constant values for \u03bb has negative impact on sample quality. Selecting a \u03bb-value that is too low results in unrealistic, \"cartoonish\" outputs, while a \u03bb-value that is too high diminishes the effectiveness of the edge-preserving diffusion model, making it nearly indistinguishable from an isotropic model.\nTo overcome this, we instead consider a time-varying edge sensitivity \u03bb(t). We set an interval [\u03bbmin; \u03bbmax] that bounds the possible values for the time-varying edge sensitivity. The function that governs \u03bb(t) within this interval can in theory again be chosen freely. We have so far experimented with a linear function and a sigmoid function. We experienced that a linear function for \u03bb(t) resulted in higher sample quality and therefore used this function for our experiments. Additionally, we have attempted to optimize the interval [\u03bbmin; \u03bbmax], but this led to unstable behaviour."}, {"title": "5 EXPERIMENTS", "content": "Implementation details We compare our method against three baselines, namely DDPM (Ho et al., 2020), IHDM (Rissanen et al., 2023) and BNDM (Huang et al., 2024a). The motivation for comparing with the latter two works is that they also consider a non-isotropic form of noise.\nWe perform experiments on two settings: pixel-space diffusion following the setting of Ho et al. (2020); Rissanen et al. (2023) and latent-space diffusion following (Rombach et al., 2022) noted as LDM in Table 1, where the diffusion process can be driven by any method but runs in the latent space. We use the following datasets: CelebA (1282, 30,000 training images) (Lee et al., 2020), AFHQ-Cat (1282, 5,153 training images) (Choi et al., 2020), Human-Sketch (1282, 20,000 training images) (Eitz et al., 2012) (see Appendix) and LSUN-Church (1282, 126,227 training images) (Yu et al., 2015) for pixel-space diffusion. For high-resolution image generation with latent-space diffusion (Rombach et al., 2022), we use on CelebA (2562), AFHQ-Cat (5122).\nWe used a batch size of 64 for all experiments in image space, and a batch size of 128 for all experiments in latent space. We trained AFHQ-Cat (1282) for 1000 epochs, AFHQ-Cat (5122) (latent diffusion) for 1750 epochs, CelebA(1282) for 475 epochs, CelebA(2562) (latent diffusion) for 1000 epochs and LSUN-Church(1282) for 90 epochs for our method and all baselines we compare to. Our framework is implemented in Pytorch (Paszke et al., 2017). For the network architecture we use a 2D U-Net from Rissanen et al. (2023). We use T = 500 discrete time steps for both training and inference, except for AFHQ-Cat128, where we used T = 750. To optimize the network parameters, we use Adam optimizer (Kingma and Ba, 2014) with learning rate 1e-4 for latent-space diffusion models and 2e-5 for pixel-space diffusion models. We trained all datasets on 2x NVIDIA Tesla A40.\nFor our final results in image space, we used a linear scheme for \u03bb(t) that linearly interpolates between \u03bbmin = 1e-4 and \u03bbmax = 1e-1. We used a transition point t0 = 0.5 and a linear transition"}, {"title": "5.1 FREQUENCY ANALYSIS OF TRAINING PERFORMANCE", "content": "To better understand our model's capacity of modeling the target distribution, we conducted an analysis on its training performance for different frequency bands. Our setup is as follows, we create 5 versions of the AFHQ-Cat128 dataset, each with a different cutoff frequency. This corresponds to convoluting each image in the dataset with a Gaussian kernel of a specific standard deviation \u03c3, representing a frequency band. For each frequency band, we then trained our model for a fixed amount of 10000 training iterations. We place a model checkpoint at every 1000 iterations, so we can also investigate the evolution of the performance over this training time. We measure the performance by computing the FID score between 1000 generated samples (for that specific checkpoint) and the original dataset of the corresponding frequency band. A visualization of the analyzed results is presented in the inline figure on the right. We found that our model is able to learn the low-to-mid frequencies of the dataset significantly better than the isotropic model (DDPM)."}, {"title": "5.2 ABLATION STUDY", "content": "Impact of transition function \u03c4(t). We have experimented with three different choices for the transition function \u03c4(t): linear, cosine and sigmoid. While cosine and sigmoid show similar performance, we found that having a smooth linear transition function significantly improves the performance of the model. A qualititative and quantitative comparison between the choices is presented in the inline figure below.\nImpact of transition points t0. We have investigated the impact of the transition point t0 on our method's performance by considering 3 different diffusion schemes: 25% edge-preserving - 75% isotropic, 50% isotropic - 50% edge-preserving and 75% edge-preserving - 25% isotropic. A visual example for AFHQ-Cat (1282) is presented in the inline figure on the right. We have experienced that there are limits to how far the transition point can be placed without sacrificing sample quality. Visually, we observe that the further the transition point is placed, the less details the model generates. The core shapes however stay intact. This is illustrated well by Fig. 7 in Appendix A. For the datasets we tested on, we found that the 50%-50% diffusion scheme works best in terms of FID metric and visual sharpness. This again becomes apparent in Fig. 7: although the samples for t0 = 0.25 contain slightly more details, the samples for t0 = 0.5 are significantly sharper.\nImpact of edge sensitivity \u03bb(t). As shown in the above inline figure, lower constant \u03bb(t) values lead to less detailed, more flat, \"water-painting-style\u201d results. The intuition behind this is that a lower \u03bb(t) corresponds to stronger edge-preserving noise and our model is explicitly trained to accordingly better learn the core structural shapes instead of the high-frequency details that we typically find in interior regions. Our time-varying choice for \u03bb(t) works better than other settings in our experiments, by effectively balancing the preservation of structural information across different granularities."}, {"title": "6 CONCLUSION", "content": "We introduced a new class of edge-preserving generative diffusion models that extend DDPMs with negligible overhead. In practice, we didn't notice any increase in training/inference time. Our linear diffusion process has two stages: edge-preserving and isotropic. The edge-preserving stage maintains core shapes, while the isotropic stage fills in details. This decoupled approach captures low-to-mid frequencies better and converges to sharper predictions faster. It improves performance on both unconditional and shape-guided tasks, outperforming several state-of-the-art models. For future work it would be interesting to extend our non-isotropic framework to the temporal dimension for video generation to improve time-consistency of important image features.\nBroader impact. We recognize that releasing generative models can open doors for malevolent actors to exploit and make misuse of them. However, by being transparent about the workings of our method and releasing the source code, we hope to support the research communities that are working on methods to better detect machine-generated content and warn or protect against it."}]}