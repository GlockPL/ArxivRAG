{"title": "Hierarchical End-to-End Autonomous Driving: Integrating BEV with Deep Reinforcement Learning", "authors": ["Siyi Lu", "Lei He", "Shengbo Eben Li", "Yugong Luo", "Jianqiang Wang", "Keqiang Li"], "abstract": "End-to-end autonomous driving offers a streamlined alternative to the traditional modular pipeline, integrating perception, prediction, and planning within a single framework. While Deep Reinforcement Learning (DRL) has recently gained traction in this domain, existing approaches often overlook the critical connection between feature extraction of DRL and perception. In this paper, we bridge this gap by mapping the DRL feature extraction network directly to the perception phase, enabling clearer interpretation through semantic segmentation. By leveraging Bird's-Eye-View (BEV) representations, we propose a novel DRL-based end-to-end driving framework that utilizes multi-sensor inputs to construct a unified three-dimensional understanding of the environment. This BEV-based system extracts and translates critical environmental features into high-level abstract states for DRL, facilitating more informed control. Extensive experimental evaluations demonstrate that our approach not only enhances interpretability but also significantly outperforms state-of-the-art methods in autonomous driving control tasks, reducing the collision rate by 20%.", "sections": [{"title": "I. INTRODUCTION", "content": "End-to-end autonomous driving has garnered significant attention for its ability to unify perception, prediction, and planning into a single, integrated model, offering an alternative to the traditional modular approach [1]. In contrast to the classic pipeline, where separate modules for perception, prediction, and planning are prone to error propagation and high computational complexity [2], [3]. Since it can significantly reduce the writing of manual rule codes, end-to-end has gradually become the mainstream trend in the intelligent development of smart connected vehicles [4].\nRecent advancements have seen Deep Reinforcement Learning (DRL) applied to end-to-end autonomous driving [5], where the system encodes environmental and vehicular state information into high-dimensional latent feature representations. From these representations, a DRL agent outputs driving strategies for autonomous navigation. However, existing research has often treated feature extraction as an isolated component, without explicitly connecting it to the perception module, which is crucial in traditional driving systems. In this paper, we bridge this gap by mapping DRL's feature extraction network to the perception phase, and more importantly, utilizing semantic segmentation decoding to interpret the extracted features in a structured manner.\nBird's-Eye-View (BEV) representations have emerged as an effective means for capturing driving scenarios, particularly in urban settings [6], [7], [8], [9]. BEV consolidates multi-sensor inputs in a unified three-dimensional space, providing a comprehensive understanding of the vehicle's surroundings. In our framework, BEV features serve as the input to the DRL policy, which outputs driving control signals. While BEV offers a robust means of feature representation, the process of extracting and translating these features into abstract states suitable for DRL remains challenging. To overcome this, we propose an expressive yet efficient neural network to extract relevant features from BEV inputs and map them directly to the perception phase of the autonomous driving pipeline. By incorporating semantic segmentation into the feature decoding process, we aim to provide a clearer interpretation of the environment, making the DRL agent's"}, {"title": "II. RELATED WORK", "content": "The end-to-end algorithm framework has attracted great attention in the field of autonomous driving due to its more concise algorithm process and stronger generalization performance. Building on the previous end-to-end autonomous driving approach [36], [37], we use multiple cameras mounted on the car as the output of the end-to-end autonomous driving algorithm, and output control signals that control the accelerator, brake, and steering wheel corner."}, {"title": "A. Traditional Modular Approach", "content": "The traditional modular approach to autonomous driving consists of four main modules: perception, prediction, planning, and control [10], [11]. Each of these modules impacts overall performance. The early perception module relied on traditional computer vision algorithms, such as edge detection [12], corner detection [13], and target tracking [14]. Vehicle trajectory prediction methods based on traditional machine learning include the Kalman filter [15], [16], Bayesian networks [17], and Markov methods [18]. In contrast, deep learning approaches often use long short-term memory (LSTM) encoder-decoder structures [19], [20]. The planning module is divided into global and local path planning, calculating trajectory points for the vehicle's low-level controller [21], [22]. The control module generates safe and reliable real-time instructions based on the driving trajectory [23]. A key advantage of this modular design is its interpretability; it breaks down a complex system into independent yet interrelated modules, each focusing on a specific task, making understanding and analysis easier."}, {"title": "B. Deep Reinforcement Learning for Autonomous Driving", "content": "Deep Reinforcement learning is a powerful and effective method to obtain end-to-end autonomous driving policies with superior performance. There are many works on end-to-end autonomous driving using reinforcement learning [24], [25]. Ref.[26] proposed a framework designed to facilitate model-free deep reinforcement learning within complex urban autonomous driving environments. Ref. [27] proposed reinforcement learning within a simulation environment to develop a driving system capable of controlling a full-scale real-world vehicle. The driving policy utilizes RGB images captured from a single camera along with their semantic segmentation as input data. Ref. [28] proposed a comprehensive framework for decision-making, amalgamating the principles of planning and learning. This fusion leverages Monte Carlo tree search and deep reinforcement learning to address challenges such as environmental diversity, sensor information uncertainty, and intricate interactions with other road users."}, {"title": "C. Explainability of Autonomous Driving", "content": "Autonomous driving is a high-stakes, safety-critical application. Explainability, which combines interpretability (human comprehensibility) and completeness (exhaustive explanations) [29], is essential for users and traffic participants to trust and accept autonomous systems [30], [31]. Researchers also rely on explainability to optimize and enhance the performance of driving algorithms [32], [33]. As end-to-end autonomous driving develops, the need for interpretability becomes increasingly important. Deep reinforcement learning models, consisting of multiple layers and complex neural networks, often make their decision-making processes and feature representations difficult to understand [34]. Visual analysis is a key method for enhancing the interpretability of these models [35]. This paper proposes a deep reinforcement learning feature extraction network from a bird's eye view (BEV) that integrates perception tasks with feature decoding and visualization, improving both the performance and interpretability of end-to-end autonomous driving algorithms."}, {"title": "III. APPROACH", "content": "The end-to-end algorithm framework has attracted great attention in the field of autonomous driving due to its more concise algorithm process and stronger generalization performance. Building on the previous end-to-end autonomous driving approach [36], [37], we use multiple cameras mounted on the car as the output of the end-to-end autonomous driving algorithm, and output control signals that control the accelerator, brake, and steering wheel corner."}, {"title": "A. Problem Formulation", "content": "Our work focuses on designing end-to-end autonomous driving algorithms aimed at efficiently reaching a target location while avoiding collisions with other traffic participants. This problem can be modeled as Partially Observable Markov Decision Processes (POMDPs) [38]. A POMDP can be represented by a tuple < A,S,R, P,O,Z,\u03b3 >, where A denotes the action space, S denotes the state space, R represents the reward function, P is the state transition function, O is the observation space, Z is the observation function, and \u03b3 is the discount factor. In the context of autonomous driving, the state transition function P and the observation function Z are often not available in closed-form, rendering this a model-free POMDP problem. The following sections will outline the POMDP formulation for the autonomous driving task.\n\u2022 State Space S: we use the CARLA driving simulator [39] as the environment for the agent. The state space S is defined by CARLA and cannot be directly obtained by the agent. At time step t, the state of the environment is represented by $S_t$.\n\u2022 Observation Space O: similar to [36], the observation of the agent at time step t is $o_t$ = $\\bigcup_{m=1}^{M} {\\{I, R, V, N\\}_m}$, where I is a 6 \u00d7 3 \u00d7 128 \u00d7 352 image obtained by six RGB camera (front view and rear view, a total of six cameras). R is a 9-dimensional vector representing road features, V is a 4-dimensional vector embedding vehicle features, and N is a 5-dimensional vector containing navigational features.\n\u2022 Action Space A: the actions of the agent include acceleration or deceleration (braking) values and steering angle. Their value range are all [-1,1].\n\u2022 Reward Function R: the reward function is designed as follows:\n$R \\left(S_{t}, a_{t}\\right)=r_{t}=\\begin{cases}-k_{c} & \\text { if collision; } \\\\\n\\frac{v_{m}-v_{c}}{4 v_{c}}+\\frac{v_{s}}{\\left|\\left|p_{c}-p_{w}\\right|\\right|^{2}}, & \\text { if } v_{c}v_{m}>0; \\\\\n0, & \\text { otherwise, }\\end{cases}$                                                                                                                  (1)\nwhere $v_m$, $v_c$, and $v_s$ are respectively the maximum speed limit of the vehicle, the current speed of the vehicle and the similarity of the vehicle with next waypoint w. $k_c$ is the penalty value when a collision occurs. $p_c$ and $p_w$ represent the current position of the vehicle and the position of waypoint w respectively."}, {"title": "B. Deep Reinforcement Learning-Based Autonomous Driving", "content": "Reinforcement learning has proven to be a powerful technique for solving Partially Observable Markov Decision Processe.By modeling the autonomous driving process as a POMDP, reinforcement learning can be leveraged to derive optimal driving strategies. In this paper, we adopt the Proximal Policy Optimization (PPO) [40] algorithm as the core reinforcement learning method. PPO is known for its stability and efficiency in continuous control tasks, making it suitable for autonomous driving applications. The network architecture in our approach adopts the Actor-Critic architecture, and specific details are shown in Fig. 2. The input to the deep reinforcement learning system includes not only road features (such as road conditions, lane markings, etc.), vehicle features (such as speed, direction, etc.), and navigation feature, but also images from the surround-view camera. Each observation has a separate channel to extract features, and the RNN captures the temporal dependency of the features. Finally, the features of each channel are concentrated and handed over to the Critic network and the Actor network for decision and estimation.\nThe feature extractor network of road features and vehicle features is based on the MLP backbone architecture and the feature extractor network of images from the surround-view camera is based on a BEV feature extraction network named SC Block. By integrating the BEV feature extraction network into the actor network, our DRL-based autonomous driving system gains a clearer and more comprehensive understanding of its surroundings, which significantly enhances decision-making performance. The next section will discuss the details of the BEV feature extraction network and its implementation."}, {"title": "C. BEV Feature Extraction Network", "content": "Traditional image feature extraction algorithms usually process in the same coordinate frame as the input image without coordinate transformation. However, other inputs in the perception space of the autonomous driving algorithm are in the BEV space coordinate system. Different coordinate systems will cause errors in the feature fusion process. The core idea behind this network is to transform raw image data into a 3D representation and project it into the BEV grid similar to [41]. The process can be divided into two main steps: Lift and Splat.\nLift Step. In the Lift step, the transformation from a 2D image to a 3D representation is achieved by predicting a depth distribution for each pixel. Given an image I \u2208 $R^{3\u00d7H\u00d7W}$, where H and W are the height and width of the image, we learn a depth distribution $a_{h,w} \u2208 \u2206_{|D|\u22121}$ for each pixel at location (h, w). This distribution is over a predefined set of depth bins D = {$d_1,d_2,\u00b7\u00b7\u00b7, d_n$}. The transformation can be formulated as\n$I(h, w, d) = \\sum_{d \\in D} a_{h,w} (d) \u00b7 f_{h,w}(d)$,                                                                                                                  (2)\nwhere $f_{h,w}(d)$ is the context vector associated with pixel (h,w) at depth d, and $a_{h,w}(d)$ is the learned probability that the pixel corresponds to that depth. This step effectively produces a frustum of points for each pixel in the image across multiple depth values, leading to a 3D point cloud. The depth distribution $a_{h,w}(d)$ is learned through a supervised or self-supervised approach, typically using depth ground truth or monocular depth estimation techniques. This"}, {"title": "D. Semantic Segmentation of Latent Feature", "content": "The input feature extraction network in deep reinforcement learning significantly impacts the algorithm's overall performance, but its relationship with the final results is often unclear. To address this, we decode and visualize intermediate features using semantic segmentation to evaluate the performance of our proposed EV Feature Extraction Network.\nSemantic segmentation is a key perception task in autonomous driving. It provides essential contextual information, allowing the system to understand road layouts and identify pedestrians, vehicles, and obstacles. In deep reinforcement learning, the input feature extraction network processes information to derive additional environmental features, aligning with the goals of the traditional perception module. We propose a decoding mechanism that utilizes semantic segmentation to transform the latent features from the extraction network into interpretable outputs. In this article, we leverage semantic segmentation to visualize the performance of the BEV Feature Extraction Network.\nWe use the pre-trained ResNet as the decoder backbone network for semantic segmentation. The latent features output by the BEV Feature Extraction Network are first processed by the convolutional layer for simple feature extraction, and then further processed by the backbone network to obtain higher-level features. Then, through upsampling and feature concatenation, the low-level features are combined to preserve spatial information. This network can effectively generate bird's-eye view semantic segmentation results."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "In order to verify the performance of representation features in the bird's-eye view space on the autonomous driving method based on reinforcement learning, we tested two of our proposed autonomous driving algorithms based on BEV space-based representation features and reinforcement learning (Ours-3 and Ours-6), using three cameras and six cameras, respectively. It is also compared with other autonomous driving algorithms based on reinforcement learning (DRL and DRL-pan) [36]."}, {"title": "A. Experimental Setup", "content": "We use CARLA as a simulator for training and testing autonomous driving algorithms, and autonomous vehicles are equipped with RGB cameras to perceive their surroundings. The DRL method is equipped with three cameras with a 60-degree field of view (FOV), which can observe the image within 180 degrees in front of it. Based on the DRL method, the DRL-pan use three cameras with a FOV of 120 degrees, enabling a 360-degree view of the vehicle's surroundings. The camera setup of the Ours-3 is exactly the same as that of the DRL-pan. Ours-6 uses six cameras with a FOV of 60 to view a 360-degree view of the vehicle's surroundings. Fig. 3 shows the transformation of the reward function during the training process of the DRL method with three cameras as input and our proposed method.\nWe selected the Town03 map in CARLA and the traffic flow with low congestion to train four autonomous driving algorithms based on reinforcement learning (DRL, DRL-pan, Ours-3, Ours-6), with 50 pedestrians and 50 cars in the low congestion traffic. The test was conducted in CARLA's Town01 Town07 with a combination of low congestion traffic and high congestion traffic. During autonomous driving, if there is a collision, the mission fails, and conversely, if there is no collision within 128 steps, the mission succeeds.\nThe evaluation indicators of autonomous driving are Collision Rate, Similarity, Timesteps and Waypoint Distance.Collision Rate refers to the probability of collision while driving, Similarity refers to the average value of the cosine similarity between the direction of vehicle movement and the current vehicle position pointing to the direction of the next planned route waypoint during driving, the Timesteps refers to the driving time before the success or failure of the driving task, and the Waypoint Distance refers to the average value of the distance between the vehicle position and next planned route waypoint during driving."}, {"title": "B. Evaluation of autonomous driving in different maps", "content": "In order to comprehensively evaluate the performance of our proposed autonomous driving algorithm, we trained the reinforcement learning algorithm on the Town03 map, and verified the performance of the algorithm on seven maps from Town01 to Town07. The results are shown in Table II, and our method achieves the best results on most maps and averages for the three indicators Collision Rate, Similarity, and Timesteps. On the average of the 7 maps, ours-6 reduces collision rate by 22%, improves similarity by 3%, and increases timesteps by 11.92 compared to the DRL method. On the Waypoint Distance, our method also achieves the best results on 5 maps. This strongly proves that the feature representation in the BEV space enhances the spatial understanding ability of the reinforcement learning agent, which greatly improves the performance of autonomous driving.\nUnexpectedly, DRL and DRLpan use the same feature extraction network and the same number of cameras, and the DRLpan method using cameras with a larger field of view can obtain more information in the environment to assist autonomous driving decision-making than DRL. However, the DRLpan method is much worse than the DRL method in three indicators: Collision Rate, Similarity, and Timesteps.The experiment indirectly proves that the expressive power of the feature extraction network shufflenet is limited, thus limiting the performance of the overall reinforcement learning.On the contrary, when our proposed method uses the input of 3 cameras and 6 cameras respectively, the increase in the number of cameras will greatly improve the overall autonomous driving algorithm performance."}, {"title": "C. Evaluation of autonomous driving in high-congestion environments", "content": "In high-congestion environments, the performance of autonomous driving systems faces greater challenges due to the increased number of dynamic traffic participants. To evaluate the robustness of our proposed BEV-based autonomous driving algorithms under such conditions, we conducted tests using both low and high traffic densities across various CARLA maps. High-congestion scenarios involve 100 pedestrians and 100 vehicles, doubling the complexity compared to the low-congestion settings.\nThe results of these experiments are shown in Table II, where we compare our algorithms (Ours-3 and Ours-6) with the baseline methods (DRL and DRL-pan). As expected, the collision rate increases under high traffic densities for all methods due to the higher likelihood of encountering obstacles and unpredictable behaviors of other traffic participants. However, our proposed methods, particularly Ours-6, demonstrate significantly better collision avoidance capabilities. Ours-6 reduces the collision rate by an average of 18% compared to DRL across the tested maps, proving that the enhanced spatial understanding from the BEV feature representation is effective even in high-traffic situations.\nIn addition to collision rate, other evaluation metrics such as similarity, timesteps, and waypoint distance further illustrate the superior performance of our methods in handling congestion. Ours-6 maintains a high degree of similarity, even when surrounded by numerous other vehicles, ensuring the vehicle follows the planned route more precisely. The timesteps achieved by Ours-6 are consistently longer, indicating that the algorithm can successfully navigate through congested environments for extended periods without collisions. Lastly, the waypoint distance remains low for our method, proving that the vehicle stays closer to the optimal route in complex traffic situations."}, {"title": "D. Interpretability", "content": "To assess the interpretability of To implement our proposed framework, we conducted experiments using several randomly selected sampled frames. Fig. 4 shows the results of semantically segmented (bird's eye view) decoding of the latent variables obtained by the BEV feature extraction network. Due to the domain gap between the CARLA simulator and the nuScenes dataset, pre-training on nuScenes alone does not generalize well to the simulated environment in CARLA. The differences in sensor configurations, traffic scenarios, and environment dynamics between these datasets may lead to poor decoding accuracy when transferring to a new domain. Fig. 4 shows that the decoding quality is significantly improved after fine-tuning the model using deep reinforcement learning. Fine-tuning adapts the model to the specific features of the CARLA environment, enabling it to generate clearer and more accurate BEV masks. These masks effectively capture the spatial layout of objects and obstacles, providing interpretable insights into the decision-making process."}, {"title": "V. CONCLUSION", "content": "In this paper, we present a novel end-to-end control framework for autonomous driving that utilizes a DRL-based approach to integrate perception and control. Our method employs a BEV feature extraction network to convert visual input into latent features, which are then decoded using semantic segmentation for improved interpretability. We tackle the challenges of partial observability by framing the problem as a partially observable markov decision processe, enhancing the system's ability to make informed control despite incomplete environmental data. Our approach demonstrates significant advancements in autonomous driving by providing a robust feature extraction and explanation mechanism. It not only improves the interpretability of end-to-end control strategy but also contributes to making autonomous systems more transparent and reliable. Future work will focus on refining depth prediction and camera parameter integration to enhance the accuracy and robustness of BEV feature extraction. Additionally, we plan to explore real-world implementations to evaluate the practical viability of our approach in diverse driving environments."}]}