{"title": "Chattronics: using GPTs to assist in the design of data acquisition systems", "authors": ["Jonathan Paul Driemeyer Brown", "Tiago Oliveira Weber"], "abstract": "The usefulness of Large Language Models (LLM) is being continuously tested in various fields. However, their intrinsic linguistic characteristic is still one of the limiting factors when applying these models to exact sciences. In this article, a novel approach to use General Pre-Trained Transformers to assist in the design phase of data acquisition systems will be presented. The solution is packaged in the form of an application that retains the conversational aspects of LLMs, in such a manner that the user must provide details on the desired project in order for the model to draft both a system-level architectural diagram and the block-level specifications, following a Top-Down methodology based on restrictions. To test this tool, two distinct user emulations were used, one of which uses an additional GPT model. In total, 4 different data acquisition projects were used in the testing phase, each with its own measurement requirements: angular position, temperature, acceleration and a fourth project with both pressure and superficial temperature measurements. After 160 test iterations, the study concludes that there is potential for these models to serve adequately as synthesis/assistant tools for data acquisition systems, but there are still technological limitations. The results show coherent architectures and topologies, but that GPTs have difficulties in simultaneously considering all requirements and many times commits theoretical mistakes.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLM) have proven to be capable of achieving impressive milestones, such as passing the bar exam [1], medical diagnosis [2] and code optimisation [3]. Applications such as these show that the adoption of LLMS has been faster in fields in which text-based outputs can be directly used. In fields such as Electrical and Electronic Engineering, additional steps are required to properly leverage natural language and allow LLMs to reliably help.\nThe application is packaged as a command line interface (CLI) and follows a pre-established conversational structure, which requests the user to answer questions and interact with the model. The goal of such flow is to limit the scope and help the model understand better the project requirements, in order to improve the final proposed solution.\nOne of the main challenges of basing applications on LLMs is automating the testing process. The non-deterministic characteristic of these models makes it difficult to run multiple iterations of the same scenario and expect the same output. Given that the tool requires user interaction - as the model will ask questions and request feedback - automated tests have to be able to sufficiently emulate human behaviour. For that, two approaches were developed, one of which uses a second GPT model with its own set of prompts.\nFor each user emulation method and testbench which consists of projects based on data acquisition [4] and instrumentation textbooks [5] - 20 iterations of the tests were run, totalling 160 iterations. All iterations were manually analysed, in order to identify the different types of errors and how many times they appear across iterations.\nCurrently, system and block-level design of data acquisition systems is mostly manual. Although some blocks, such as filters, can be designed using specific tools, high-level decisions and the specification translation from system-level to block-level is performed by experienced designers. Both for educational and industrial purposes, the use of automated strategies can benefit parts of the design flow. In education, such tools can help students see how system-level specifications are converted to block-levels and understand the rationale of the design flow. For the industry, this type of tool can be used in non-demanding projects, saving time for experienced designers and improving the time-to-market of products.\nLLMs were already used to assist the synthesis of digital chips [6], in which a chip design was co-authored by a transformer model, in this case, ChatGPT-4.\nChatGPT has also been used to assist in the field of embedded systems [7], using the model to provide specifications on the controller, sensor models, synthesis flow and helpful literature. The conclusion is that much of the answers still require validation by an expert given eventual inconsistencies.\nThese papers manually use and manipulate GPT in order to get their answers. In order to develop a tool that can be packaged and distributed, however, more prompt engineering and conversational flow techniques have to be incorporated into the application. These are very common in open-source projects [8][9], which use GPTs conversational back-and-forth capabilities to build a solution piece by piece.\nThis paper incorporates frequently used techniques in LLM based tools, such as personas, requesting questions from the model and templates, in order to develop an application for Data Acquisition System project assistance. Specific testing techniques, which propose methods to emulate the user, and testbenches are also presented. The results were manually analysed and the main takeaways will be discussed."}, {"title": "II. METHODOLOGY", "content": "The application was developed in Go using the go-openai library, which acts as an SDK (Software Development Kit) to OpenAI official API (Application Programming Interface).\nThe model used for both GPTs in this paper was OpenAI's GPT-4-Turbo [10], officially named gpt-4-1106-preview, as it was in preview version at the time. The supposed better performance matched with the lower cost of this model made it a viable option, despite not being an official major version of GPT. The only configured parameter was temperature, which was set to 0.8 for the main GPT and 0.2 for the user emulating model.\nThe conversational flow, i.e. the planned back and forth between the model and the client (be it a user or an emulator), aims to reduce superficial answers by the model that might not satisfy all project requirements. As the application was developed with the purpose of being an interactive tool, it must consider the case in which the user does not initially provide enough information to form a satisfactory solution.\nA high level diagram showing the stages included in the application's chat flow can be seen in Figure 1. Given that the chosen approach was system level synthesis with top- down methodology, the first step is to establish the architecture of the proposed solution. Following it, each block should be individually analysed in order to establish lower level requirements and details. The stages named Categorisation and Revision are mostly for internal purposes and will be explained in the next sections."}, {"title": "1) Architectural Stage", "content": "This stage is the application's entry point and aims to gather the main system requirements of the project in order to design a block diagram representing the architecture. Figure 2 shows the internal steps of this stage, which starts with the user input that describes the desired project. The next step is an exchange of questions and answers, in which the model will ask up to 5 questions to further understand the provided project description. After the user (or emulator) provides the answers or leaves the answers empty, given that in some cases the user may not have the requested information -, the model will generate a .DOT string of the architecture, which the application visually presents to the user."}, {"title": "2) Categorisation Stage", "content": "This stage is entirely internal, i.e. without interaction with the client, and the main purpose is to set up the application for the detailing phase. Given that each individual block has to go through its own detailing step, the prompts would have to be very generic given that the block in question could be either a sensor, an analogue-digital converter (ADC) or a low-pass filter, etc.\nHence the categorisation stage, that attributes a category to each block of the designed architecture. This enables each category to have it's own set of prompts, allowing the ADC stage to specifically request the model to provide a sampling rate, for example. Each block can be assigned to one of the following categories:"}, {"title": "3) Detailing Stage", "content": "The detailing stage follows a similar flow as to the architectural one, with the only difference being that the input text that will prompt the model to ask the questions is not user generated, but rather triggered by the application of a specific prompt for each category. Other than that, the question and answer steps still remain, as well as the feedback loop, which depends on the users satisfaction with the provided detailing of the block.\nIn some cases, the amount of blocks in the architectural diagram can be large, which makes the conversation history scale up when adding up all interactions in the detail phase. This is not only a problem cost wise, but it might also affect the attention given to specific parts of the input, lowering the value of otherwise important information. This resulted in the decision not to share information across detail stages, i.e. the conversation history of every detail stage is only the architectural conversation and does not contain the detailing of previous blocks. This proved to significantly shape the outcome of the solution, as will be discussed in later sections of the paper."}, {"title": "4) Revision", "content": "The final stage is where the user will be able to see the proposed solution as a whole and request corrections if necessary. During preliminary tests, it was observed that each block generated almost a pages worth of detailing, causing the final solution to be excessively large. Despite the attempts to mitigate GPT's prolix writing, a simple concatenation of all generated text would be too long and impractical. Therefore, after all blocks are finished being detailed, the model compiles everything into a summary, but is instructed to keep all numerical values, equations, parameters and variables that were generated, to reduce the chance of losing important information.\nThe user is presented with this summary and is asked whether something must be altered, following the same feed- back loop method used in the previous stages. This is the last chance to request changes to the solution, as it marks the end of the process as a whole."}, {"title": "C. Automated Testing", "content": "To run automated tests on the application it was necessary to find ways to emulate the user. Given that there is no way of knowing what questions the model would ask and what phrasing will be used, hardcoding the answers was not an option. An exception to this is if the emulated user actually does not provide any answers, making the initial project description the only information provided to the model. This will be called the Direct Context approach.\nThe other alternative is to instruct another GPT model to emulate the user, providing it with a set of information that it can answer from if asked. If the list says that the output voltage must be between 0 and 20 Volts, for example, the user-emulating model can provide this as an answer if - and only if that's the questions asked by the application. This will be called the Open Context approach and it not only tests the application's ability to generate adequate solutions, but also its ability to extract the required restrictions to build such solutions.\nTo allow a fair comparison of both approaches, the list pro- vided to the open context is included in the project description used by the direct context, as if all project requirements are passed straight away to the model and excusing it of the need to ask the questions."}, {"title": "D. Testbenches", "content": "To test different capabilities of the application, a set of 4 testbenches were chosen, each of which requires a specific expertise to solve. They were based on data acquisition projects [4] and instrumentation exercises [5]. Each testbench is composed of 2 prompts: a project description and a list of possible answers. With them it's possible to build all required testing prompts."}, {"title": "1) Angular Position Project", "content": "This testbench [5] does not require extensive calculations, nor does it tackle more ad- vanced forms of sensoring: it's a potentiometer, whose output voltage must be measured by a Data Acquisition device. The main challenge of this project is to satisfy two important requirements, the maximum input voltage on the DAQ must not exceed the recommended limit and frequencies near the 50 and 60 Hz must be dampened."}, {"title": "2) Thermometry Project", "content": "This testbench [5] focuses on the qualitative aspects of the projects, without requesting specific numeric values. Unlike the previous testbench, this one requires more conditioning blocks in order to enable a viable implementation of the solution. Given that the sensor is an NTC, which is non-linear, a linearization step is mandatory as well as a Wheatstone bridge. There are alternatives to this, but complicated solutions also impact viability."}, {"title": "3) Portable Accelerometry for Low-Frequency Vibrations", "content": "This project [4] differs from the other testbenches as it focuses on the calculations and numeric values associated to a project. In the book, the author recognises that the use of a piezoelectric sensor might not be ideal for such applications, but provides the step by step guide to build such a solution.\nThe information inside the answers list - which is directly provided in the direct context approach indicates the nu- meric values that the solution should satisfy. Therefore, the requirements list validates whether the component, parameters and project choices lead to such values. The architecture can contain only a charge amplification stage between the sensor and the signal acquisition, or use an additional voltage gain block. An important note to this testbench is that the signal acquisition stage is mostly overlooked, and the application is instructed by the description to focus on the conditioning."}, {"title": "4) Machinery Pressure and Temperature Monitor", "content": "The only testbench to include more than one sensing principle, this project [4] is unique as it contains multiple channels and requires the application to provide specific conditioning for the strain gauges and for the non-linear temperature sensors. This testbench contains the most project requirements, a conse- quence of the larger block diagram that must be developed for it."}, {"title": "B. Manual Validation", "content": "Despite there being 160 iterations, many mistakes and decisions follow patterns that are caused by similar problems. For all testbenches, especially for the accelerometry project, the lack of sharing the detailing stage across blocks hindered the consistency across the solution. This was observed in the best solution in the thermometry project when the gain was set twice: in the instrumentation amplifier and in the output stage. Despite the block diagram (provided by the tool) already specifying the use of an instrumentation amplifier to adjust the gain, the lack of communication across blocks allows for such mistakes.\nIn the case of the accelerometry project, the decision to not share the chat history does not allow the blocks to share parameters, many of which must be used in multiple stages in the architecture to successfully apply the equations. Even though many mistakes were still observed in the calculations, regardless of this detail, it's undeniable that the blocks require shared information to better formulate the solution.\nIn all cases, the application is instructed to provide com- ponent models (as can be seen in snippets 1 to 4), non- optimal solutions were detected. In 24 of the 40 iterations of the angular position project, the solution proposed the use of TL072 Operational Amplifiers (OPAMP). These are not usually used for precision applications and are considered outdated, but that's not enough to disregard the solution, as no requirement was set for such.\nOn the other hand, the angular position testbench contained theoretical errors regarding the non-inverting amplifier. In 7 of the 40 iterations, the solutions used such topology with the aim of achieving a gain inferior to 1, which is mathematically impossible. In these cases, the step-by-step of the calculations show that the solution describes an inverting topology - which would be acceptable given that the description does not specify polarity, but calls it non-inverting.\nStill referring to this testbench, 14 iterations of the solutions provided the component values to build the Butterworth low- pass filters and in 11 of those the values were correct. In the remaining three, the values result in a cutoff frequency 10 times greater or lower than expected. Preliminary tests also support the evidence that many calculation mistakes are due to the scale.\nThe angular position project also shows that GPT has difficulties in implicitly satisfying requirements. The testbench explicitly asks for 50 and 60 Hz to be dampened, but instead of using a single low-pass filter, which would solve both requirements, the model prefers to use two Notch filters. In 3 iterations, the model used three filters, the two Notches and an anti-aliasing. This decision is not incorrect, but outlines a characteristic in how the model provides the solution.\nWhen comparing the direct and open contexts, an error observed in 7 iterations of the open context is the value of the cutoff frequency of the anti-aliasing or low-pass filter, such as setting it to 100 Hz to dampen 50 and 60 Hz. This is believed to be the case when the model did not manage to ask the correct questions in order to get the requirement that requested the dampening of those frequencies. Another inconsistency found in 4 iterations was the positioning of the anti-aliasing filter not being directly before the ADC and in another 4, the cutoff frequency of said filter was close to the desired sampling rate, which is incorrect.\nIn terms of the block diagram for this testbench, most of the iterations provided a sufficiently acceptable architecture, but some, such as in Figure 7, are incorrect. A mistake found in all testbenches was the incorrect usage of double and single ended arrows, but compared to other mistakes this can be considered less important."}, {"title": "IV. CONCLUSION", "content": "The goal of the paper was to develop a tool that provides solutions to real project examples and evaluate its feasibility. Despite not all testbenches and iterations resulting in flawless solutions (rather the opposite), many of the limitations are due to GPTs capability - so far - to reliably execute such tasks.\nThe tool proposes generating the entire synthesis at the system level, creating both the architecture and the implemen- tation of each block. For this to happen, a series of interac- tions with GPT must be conducted, making the conversation extensive. It is known that language models struggle when the context of the message is too large, which may have worsened the design choices made by the model.\nThe methodology used for the project was designed to develop a general-use tool (in the field of signal acquisition systems). The goal was not to test GPT's abilities specifically for the four selected testbenches, as overfitting scenarios were to be avoided. The prompts and conversational flow was not developed to be ideal for these four projects but for any project, regardless of whether the user's criteria were more quantitative or qualitative.\nIt is understood that the code implementation fulfilled its function well and managed to extract GPT's functionalities, even if it has limitations. However, the conversation history strategy caused consistency issues between blocks in multiple iterations. If it were possible to maintain the previous details in the conversation history without prohibitively increasing the context and losing information, some solutions would have likely achieved better results. Investigating how to mitigate this error could improve the tool.\nWhen comparing the results between the direct context and open context, it's noticeable that the direct context provides more consistent solutions. However, that's to be expected given that it guarantees that all restrictions will be provided to the model, while the open context depends on the model's ability to ask the appropriate questions at the right stage. Therefore, the open context should not be disregarded, but rather future research should work on improving the model's efficiency in asking questions.\nOverall, the project was successful in structuring flows that enable LLMs (especially, GPT) to translate project requests in design decisions, even though its main conclusion is that much work is needed to turn LLM models into reliable tools to use in the data acquisition field. Many of these limitations are still in regard to the model, but newer models are continually being released."}]}