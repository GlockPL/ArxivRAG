{"title": "Connecting Joint-Embedding Predictive Architecture with Contrastive Self-supervised Learning", "authors": ["Shentong Mo", "Shengbang Tong"], "abstract": "In recent advancements in unsupervised visual representation learning, the Joint-Embedding Predictive Architecture (JEPA) has emerged as a significant method for extracting visual features from unlabeled imagery through an innovative masking strategy. Despite its success, two primary limitations have been identified: the inefficacy of Exponential Moving Average (EMA) from I-JEPA in preventing entire collapse and the inadequacy of I-JEPA prediction in accurately learning the mean of patch representations. Addressing these challenges, this study introduces a novel framework, namely C-JEPA (Contrastive-JEPA), which integrates the Image-based Joint-Embedding Predictive Architecture with the Variance-Invariance-Covariance Regularization (VICReg) strategy. This integration is designed to effectively learn the variance/covariance for preventing entire collapse and ensuring invariance in the mean of augmented views, thereby overcoming the identified limitations. Through empirical and theoretical evaluations, our work demonstrates that C-JEPA significantly enhances the stability and quality of visual representation learning. When pre-trained on the ImageNet-1K dataset, C-JEPA exhibits rapid and improved convergence in both linear probing and fine-tuning performance metrics.", "sections": [{"title": "Introduction", "content": "Unsupervised learning of visual representations has recently seen remarkable progress, primarily due to the development of innovative architectures and strategies that exploit unlabeled imagery. Among these advancements, the Joint-Embedding Predictive Architecture (JEPA) [1, 2, 3] has distinguished it-self as a powerful approach. I-JEPA [2] leverages a masking strategy to extract visual features, facilitat-ing significant strides in understanding and utilizing unlabeled visual data.\nHowever, despite its successes, certain limitations within the JEPA framework have become apparent, particularly concerning its components I-JEPA Exponential Moving Average (EMA) and I-JEPA predic-tion capabilities. Specifically, I-JEPA EMA has been\nfound to be inadequate in preventing the issue of entire collapse [4, 5], while the I-JEPA prediction mechanism struggles to accurately learn the mean of patch representations. These challenges not only hinder the performance of JEPA but also limit its applicability in broader contexts.\nTo address these limitations, we introduce a novel contrastive self-supervised learning framework\nbased on JEPA, namely C-JEPA, which aims to address the aforementioned challenges by incor-porating the principles of Variance-Invariance-Covariance Regularization (VICReg) [6]. VICReg's methodology is adept at learning variance and covariance to avert entire collapse and ensure invari-ance for the mean of augmented views. By integrating VICReg with the Image-based JEPA, C-JEPA\nis designed to achieve faster and better convergence, as shown in Figure 1. In this paper, we aim to detail the theoretical underpinnings and empirical validations that substantiate the superiority of C-JEPA over previous self-supervised learning methods.\nOur contributions are manifold and significant. Firstly, we identify and articulate the limitations inherent in the I-JEPA framework, specifically its EMA and prediction mechanisms. Secondly, we propose the C-JEPA framework as a novel solution that synergizes JEPA with VICReg to address these limitations effectively. Thirdly, through rigorous empirical and theoretical evaluations, we demonstrate that C-JEPA not only mitigates the issues identified but also achieves superior performance metrics when compared to existing frameworks. Particularly notable is C-JEPA's performance when pre-trained on the ImageNet-1K dataset, where it shows fast and improved convergence in linear probing and fine-tuning scenarios. These results highlight C-JEPA's potential to\nset a new benchmark in unsupervised visual representation learning, thereby contributing significantly to the field's advancement."}, {"title": "Related Work", "content": "Self-supervised Learning. In the self-supervised literature, researchers aim to exploit the internal characteristics of data and leverage pretext tasks to train a model. Recently, an unsupervised framework that learns effective views with data augmentation was proposed by Tian et al. [7] to reduce the mutual information between views. CMC [8] introduced a multi-view contrastive learning framework with any number of views to learn view-agnostic representations. Another pretext task of solving jigsaw puzzles was developed in PIRL [9] to improve the semantic quality of learned image representations, achieving better object detection results than supervised pre-training. More recently, Masked image modeling (MIM) has been explored in many previous works [10, 11, 12, 13, 14, 15, 16, 17, 18] to reconstruct the masked image patch given the unmasked\ncounterpart as clues. Some MIM approaches [10, 11, 12, 19] designed customized masking strategies\n(i.e., random, block-wise) as pre-text tasks during pre-training. For example, block-wise masking was introduced in BEiT [10] to learn transferrable visual representations by recovering discrete tokens of masked image patches. Given features extracted from the 25% unmasked patches, the seminal work,\nMAE [12] directly reconstructed missing pixels of 75% masked patches. SimMIM [14] randomly\nmasked the input image with a large square patch size (i.e., 32) and used a one-layer prediction\nlayer after the encoder to predict RGB values of raw pixels. Other researchers [20, 21, 22, 23]\nstarted to leverage a teacher network like CLIP [24] or adversarial learning to generate the mask and\nsupervision target.\nContrastive Learning. In the past years, contrastive learning has shown its effectiveness in self-supervised learning, where various instance-wise contrastive learning frameworks [25, 26, 27, 28, 29,\n30, 31, 32, 4, 33, 34] and prototype-level contrastive methods [35, 36, 37, 38, 39, 40] were proposed.\nThe general idea of the instance-wise contrastive learning is to close the distance of the embedding of\ndifferent views from the same instance while pushing embeddings of views from different instances\naway. One common way is to use a large batch size to accumulate positive and negative pairs in the\nsame batch. For instance, Chen et al. [25] proposed a simple framework with a learnable nonlinear projection head and a large batch size to improve the quality of the pre-trained representations. To\nmake the best use of a large amount of unlabelled data, they present a bigger unsupervised pre-training\nnetwork and introduce distillation with unlabeled data in SimCLR v2 [26] to improve the performance\nin downstream tasks. Without involving negative instances, BOYL [27] trains the online network\nfrom an augmented view of an image to predict the target network representation of the same image\nunder a different augmented view (positive instance). Another broadly-used approach [28] in the ICL\nliterature is to apply a momentum encoder to update negative instances from a large and consistent\ndictionary on the fly. The dynamic dictionary was used with a moving-averaged encoder in MoCo\nseries [29, 28] to build a dynamic dictionary to update negative instances in a queue of considerable\nsize. The Variance-Invariance-Covariance (VICReg) [6] regularization strategy has been proposed to address shortcomings in self-supervised learning by enforcing stability through variance and covariance constraints."}, {"title": "Methodology", "content": "In this section, we present a novel masked modeling framework designed for the joint-embedding\npredictive architecture to avoid entire collapsing and improve the mean of patch representations. Our\nkey idea is to integrate VICReg into the JEPA framework for alleviating entire model collapse and\nimproving the invariance across different views of the same image. We first provide preliminaries in\nSection 3.1, then provide the theoretical and empirical connection between I-JEPA and SimSiam in\nSection 3.2, and finally connect I-JEPA with VICReg in Section 3.3 to show the benefit of reducing\nentire collapsing and learning the mean of patch representations."}, {"title": "Preliminaries", "content": "In this section, we first describe the problem setup and notations, and then revisit I-JEPA [2],\nSimSiam [4], and VICReg [6] for self-supervised image modeling.\nProblem Setup and Notations. Given a dataset \\( \\mathcal{X} = {x_i}_{i=1}^N \\) with images \\( x_i \\in \\mathbb{R}^{c \\times h \\times w} \\), our goal\nis to learn a neural network \\( f_\\theta(\\cdot) \\) to extract unsupervised representations from these visual samples.\nRevisit I-JEPA [2]. Taking the self-supervised modeling as a joint-embedding predictive architecture,\nI-JEPA [2] utilized \\( f_\\theta(\\cdot) \\) as a context encoder, a pair of neural network \\( f_\\phi(\\cdot) \\) as a target encoder.\nA predictor \\( g_\\sigma(\\cdot) \\) is applied to predict the target representations from \\( M \\) masked block patches\n\\( b_y(1), ..., b_y(M) \\). For a target block \\( b_y \\) corresponding to a target mask \\( B_i \\), the predictor \\( g_\\sigma(\\cdot, \\cdot) \\)\ntakes as input the output of the context encoder \\( b_x \\) and a mask token for each patch to predict\n\\( {m_j}_{j \\in B_i} \\), and outputs the patch-level prediction \\( {b_{y_j}}_{j \\in B_i} \\), that is, \\( {b_{y_j} }_{j \\in B_i} = g_\\sigma({m_j}_{j \\in b_i}) \\)."}, {"title": "Connecting I-JEPA with SimSiam", "content": "In the realm of self-supervised learning, both I-JEPA [2] and SimSiam [4] frameworks aim to extract robust representations from images, yet they approach the problem with distinct architectures and objectives. Here, we draw theoretical and empirical parallels between these methodologies, leveraging their unique approaches to enhance the understanding of joint-embedding architectures.\nRegarding theoretical connections, I-JEPA [2] uses a predictive model where the encoder and predictor work together to forecast masked parts of the input, relying on partial views. Conversely, SimSiam [4] operates without explicit masking, utilizing dual augmentations of the same image to enforce consistency between the independently processed views. Both models share the underlying principle of minimizing the distance between certain representations. As shown in Figure 2, I-JEPA focuses on the distance between predicted and actual masked patch representations, whereas\nSimSiam minimizes the distance between the two augmented views of an image, thereby encouraging consistency across different transformations of the same data.\nMeanwhile, empirical studies such as Tian et al. [5] have shown that linear predictors in frameworks like BYOL [27], which are closely related to SimSiam, tend to learn alignment with the correlation"}, {"title": "Connecting I-JEPA with VICReg", "content": "VICReg [6] introduces variance and covariance regularization to prevent the collapse of representa-tions in Siamese networks, ensuring that the model learns informative and diverse features across different dimensions. By integrating VICReg into I-JEPA, our objective is to tackle the common challenges of model collapse and enhance the mean representation learning of image patches.\nVICReg's variance regularization ensures that all dimensions of the embedding space contain mean-ingful variance, which is crucial for preventing the model from collapsing to trivial solutions. I-JEPA, which aims to learn diverse patch representations, can benefit from such a mechanism to ensure that each masked patch contributes informatively to the overall representation. By minimizing the off-diagonal elements of the covariance matrix, VICReg encourages the features to be uncorrelated, enhancing the diversity of the learned features. This aspect can be particularly beneficial for I-JEPA, where diverse patch predictions are essential for effective representation learning. Literature on\nnon-contrastive self-supervised learning [43] suggests that implicit variance regularization, as seen in\nVICReg, can facilitate the learning dynamics in joint-embedding architectures. Such regularization helps maintain a balance between similarity and diversity in learned representations, which is crucial for effective self-supervised learning.\nIn the following, we consider linear predictor \\( W_p \\in \\mathbb{R}^{M \\times M} \\) in I-JPEA, the masking objective\nwith the average \\( l_2 \\) distance between the predicted patch-level representations \\( z_{y_j} \\) and the target\npatch-level representations \\( z_{y_j} \\) as\n\\[ \\mathcal{L} = \\frac{1}{|M|} \\sum_{i=1}^M \\sum_{j \\in B_i} ||W_p z_{y_j} - SG(z_{y_j}^\\prime)||_2^2, \\] \nwhere \\( |M| \\) denotes the total number of target blocks \\( M \\), and \\( B_i \\) is the generated mask corresponding to the i-th target block during training. By connecting I-JEPA with SimSiam in Section 3.2, we\ncan can diagonalize correlation matrix of \\( z_{y_j} \\) over R: \\( C_{z_{y_j}} = UD_c U^T \\), where \\( U \\) is an orthogonal\nmatrix whose columns are the eigenvectors of \\( C_{z_{y_j}} \\) and \\( D_c \\) is the real-valued diagonal matrix\nof the eigenvalues \\( s_k, k \\in [1,K] \\). Given this eigendecomposition, the predictor is directly set\nto \\( W_p = UD_c^\\alpha U^T \\), where \\( \\alpha \\) is a positive constant exponent applied element-wise to \\( D_c \\). The\neigenvalues \\( \\lambda_k \\) of the predictor matrix \\( W_p \\) are then \\( \\lambda_k = s_k^\\alpha \\). Assume \\( z_{y_j} \\) the representations\nexpressed in the predictor's eigenbasis, the asymmetric loss \\( \\mathcal{L} \\) can be formulated as:\n\\[ \\mathcal{L} = \\frac{1}{|M|} \\sum_{i=1}^M \\sum_{j \\in B_i} \\sum_k ||\\lambda_k \\tilde{z}_{y_j, k} - SG(\\tilde{z}_{y_j}^\\prime)||_2^2, \\] \nFollowing non-contrastive self-supervised learning [43], we can use NTK [44, 45] to characterize the\nlearning gradient dynamics of neural networks as \\( \\nabla_{z_{y_j}} \\mathcal{L} = (D z_{y_j} - z_{y_j}^\\prime) D \\), and the representational\ndynamics of each mode k independently follow gradient of the loss \\( -\\nabla_{\\tilde{z}_{y_j}} \\mathcal{L} \\), and decouple as K\nindependent differential equations:\n\\[ \\frac{d\\tilde{z}_{y_j,k}}{dt} = -\\eta \\frac{\\partial \\mathcal{L}}{\\partial \\tilde{z}_{y_j,k}}(t) = \\eta \\lambda_k (z_{y_j,k}^\\prime - \\lambda_k \\tilde{z}_{y_j,k}), \\] \nBy taking the expectation over the same masking blocks, we can have the dynamics for each mask\npatch \\( y_j \\) as\n\\[ \\frac{d\\tilde{z}_{y_j,k}}{dt} = \\eta \\lambda_k (1 - \\lambda_k) \\tilde{z}_{y_j,k}. \\] \nNote that when \\( \\lambda_k < 1 \\), \\( \\frac{d\\tilde{z}_{y_j,k}}{dt} \\) has the same sign as \\( \\tilde{z}_{y_j,k} \\) and they have opposite sign at \\( \\lambda_k > 1 \\). These convergent dynamics will push an eigenvalue \\( \\lambda_k \\) of one to prevent the collapse of each\nmode k in representation dynamics. When removing the stop-grad operator in \\( \\mathcal{L} \\), we can have"}, {"title": "Experiments", "content": "Following previous methods [12, 2], we use ImageNet-1K [46] for image classification,\nMS-COCO [47] for object detection and instance segmentation, and ADE20K [48, 49] for semantic\nsegmentation. We closely follow previous work [50, 51], and adopt the Mask R-CNN [52] as the\ndetector. The ViTs [53] backbone weights are initialized with weights pre-trained on ImageNet-\n1K using our C-JEPA. Following the settings in [12], we use the Semantic FPN and UPerNet\napproach [54] based on our ImageNet-1K pre-trained ViTs for evaluation. For a fair comparison, we\nfine-tune the detector with the same learning rate in [12]. For video object segmentation, we use\nDAVIS-2017 dataset containing 60 training, 30 validation, and 60 testing videos. For low-level tasks,\nwe follow the previous work [2] and use Clevr/Count and Clevr/Dist on Clevr [55] dataset.\nFor image classification, we follow previous masked image modeling meth-ods [12, 2] to report the classification accuracy of linear probing and fine-tuning. For object detection\nand instance segmentation on MS-COCO, we apply \\( Ap^{box} \\) and \\( Ap^{mask} \\) as metrics for the bounding\nboxes and the instance masks. mIoU results are reported to evaluate semantic segmentation on\nADE20K. For video object segmentation on DAVIS-2017, we use Jabri-based (J&F)m, Im, Fm as\nmetrics to evaluate the quality of frozen representations of image patches by segmenting scenes with\nthe nearest neighbor between consecutive frames. For object counting and depth prediction tasks on\nClevr, we use object counting and depth prediction to evaluate the linear probing performance of our\nmodel.\nImplementation. For input images, we resized the resolution to 224 \u00d7 224, i.e., H = W = 224.\nFollowing prior work [12, 2], we apply a patch size of 16, i.e., P = 16. We use the tiny, small,\nbase, and large models of ViT [53] architecture for experiments. We set the embedding dimension\nof the predictor to 384, and keep the number of self-attention heads the same as the backbone\ncontext-encoder. For the ViT-T/16, ViT-S/16, and ViT-B/16 context-encoder, we set the depth of the\npredictor as 6. For ViT-L/16 context-encoders, we set the depth of the predictor to 12. Following\nI-JEPA [2], we use AdamW to optimize the context-encoder and predictor weights. We train our\nmodel using the default batch size of 2048, and the learning rate linearly increased from 1e-4 to 1e-3\nduring the first 15 epochs of pre-training, and decay to 1e-6 following a cosine schedule. The weight\ndecay is linearly increased from 0.04 to 0.4, and the target-encoder weights are initialized the same as\nthe context-encoder weights, and updated via an exponential moving average. We use a momentum\nvalue of 0.996, and linearly increase this value to 1.0. For masking, we use the same strategy and settings as I-JEPA [2] for 4 possibly overlapping target block masks."}, {"title": "Experimental comparisons", "content": "In this work, we propose a novel and effective framework for connecting Joint-Embedding Predictive\nArchitecture with VICReg in non-contrastive self-supervised learning. In order to demonstrate the effectiveness of the proposed C-JEPA, we comprehensively compare it to previous baselines [51, 10,\n12, 56, 57, 2] on non-contrastive self-supervised learning and mask image modeling.\nFor image classification on the ImageNet-1K benchmark, we report the quantitative comparison results on linear evaluation and fine-tuning results in Table 1. We\ncan observe that our C-JEPA achieves the best results in terms of ViT-B/16 compared to previous"}, {"title": "Experimental analysis", "content": "In this section, we performed ablation studies to demonstrate the benefit of the proposed Vari-ance/Covariance and Invariance terms from VICReg. Here, we conducted extensive experiments on ImageNet-1k pre-trained ViT-B/16 to explore the impact of representation mean and collapse, and learned meaningful qualitative patch-level representations."}, {"title": "Conclusion", "content": "In this work, we introduced C-JEPA, a novel enhancement to the Joint-Embedding Predictive\nArchitecture that integrates the robust features of Variance-Invariance-Covariance Regularization (VICReg) to address critical limitations in the existing I-JEPA model. By refining the EMA and prediction components of I-JEPA, C-JEPA successfully prevents entire model collapse and more effectively learns the mean of patch representations, thereby advancing the state of unsupervised visual representation learning. Our theoretical analysis and empirical results have demonstrated the effectiveness of C-JEPA, particularly when pre-trained on the ImageNet-1K dataset. The framework\nshows marked improvements in convergence rates and performance in both linear probing and fine-tuning scenarios, outperforming existing methods. This underscores C-JEPA's capability to not only\naddress the weaknesses of its predecessor but also to provide substantial improvements in learning quality and stability."}, {"title": "Broader Impact", "content": "Our C-JEPA's adoption of VICReg principles enhances its versatility and applicability across various\nunsupervised learning contexts, making it a valuable tool for tasks requiring robust and reliable visual representations. The implications of this research are significant, offering a pathway for future studies\nto explore and expand upon the integration of contrastive and joint-embedding techniques. As we\nmove forward, further refinements and explorations into the scalability of C-JEPA in more diverse\nand challenging datasets will be crucial. Additionally, investigating the adaptability of C-JEPA to\ndifferent domains and its effectiveness in real-world applications will be essential to fully realize its\npotential. C-JEPA represents a significant step forward in the field of machine learning, particularly in the unsupervised learning of visual representations. It sets a new benchmark for future research\nand development in this area, promising enhanced performance and broader applicability in tackling complex visual understanding tasks."}, {"title": "Appendix", "content": "In this appendix, we provide the following materials:\n\u2022 proofs on the loss \\( \\mathcal{L} \\) in the predictor eigenspace and representation dynamics under \\( \\mathcal{L} \\) Section A,\n\u2022 derivation of learning dynamics without the stop-grad and predictor in Section B,\n\u2022 detailed loss functions and algorithm in Section C,\n\u2022 additional experimental results in small-scale models and VICReg hyperparameters in Section D,\n\u2022 additional visualizations on the learned attention maps in Section E."}, {"title": "Proofs", "content": "We consider linear predictor \\( W_p \\in \\mathbb{R}^{M \\times M} \\) in I-JPEA, the masking objective with the average \\( l_2 \\) dis-\ntance between the predicted patch-level representations \\( z_{y_j} \\) and the target patch-level representations\n\\( z_{y_j}^\\prime \\) as\n\\[ \\mathcal{L} = \\frac{1}{|M|} \\sum_{i=1}^M \\sum_{j \\in B_i} ||W_p z_{y_j} - SG(z_{y_j}^\\prime)||_2^2, \\] \nAssume \\( z_{y_j} \\) the representations expressed in the predictor's eigenbasis. Since \\( W_p \\in \\mathbb{R}^{M \\times M} \\) is a symmetric matrix with eigendecomposition \\( W_p = UD_c U^T \\), where \\( U \\) is an orthogonal matrix and\n\\( UU^T = I \\), we can substitute \\( W_p \\) in Eq. 9 as\n\\[ \\mathcal{L} = \\frac{1}{|M|} \\sum_{i=1}^M \\sum_{j \\in B_i} ||UD_c U^T z_{y_j} - SG(z_{y_j}^\\prime)||_2^2 = \\] \n\\[ \\frac{1}{|M|} \\sum_{i=1}^M \\sum_{j \\in B_i} ||UD_c U^T z_{y_j} - SG(UU^T z_{y_j}^\\prime)||_2^2 = \\] \n\\[ \\frac{1}{|M|} \\sum_{i=1}^M \\sum_{j \\in B_i} ||D_c \\tilde{z}_{y_j} - SG(\\tilde{z}_{y_j}^\\prime)||_2^2 = \\] \n\\[ \\frac{1}{|M|} \\sum_{i=1}^M \\sum_{j \\in B_i} \\sum_k ||\\lambda_k \\tilde{z}_{y_j, k} - SG(\\tilde{z}_{y_j}^\\prime)||_2^2 \\] \nFollowing non-contrastive self-supervised learning [43], we can use NTK [44, 45] to characterize the\nlearning gradient dynamics of neural networks as \\( \\nabla_{z_{y_j}} \\mathcal{L} = (D z_{y_j} - z_{y_j}^\\prime) D \\), and the representational dynamics of each mode k independently follow gradient of the loss \\( -\\nabla_{\\tilde{z}_{y_j}} \\mathcal{L} \\), and decouple as K independent differential equations:\n\\[ \\frac{d\\tilde{z}_{y_j,k}}{dt} = -\\eta \\frac{\\partial \\mathcal{L}}{\\partial \\tilde{z}_{y_j,k}}(t) = \\eta \\lambda_k (z_{y_j,k}^\\prime - \\lambda_k \\tilde{z}_{y_j,k}), \\] \nBy taking the expectation over the same masking blocks, we can have the dynamics for each mask patch \\( y_j \\) as\n\\[ \\frac{d\\tilde{z}_{y_j,k}}{dt} = \\eta \\lambda_k (1 - \\lambda_k) \\tilde{z}_{y_j,k}. \\] \nSuppose the parameters of the neural network are denoted as @ with weights \\( W \\in \\mathbb{R}^{M \\times M} \\), we follow Lemma 3 proposed in [43] and have the empirical NTK)(Y, Y) in the orthogonal eigenbasis is equal"}, {"title": "Representation dynamics under L", "content": "Following non-contrastive self-supervised learning [43], we can use NTK [44, 45] to characterize the\nlearning gradient dynamics of neural networks as \\( \\nabla_{z_{y_j}} \\mathcal{L} = (D z_{y_j} - z_{y_j}^\\prime) D \\), and the representational dynamics of each mode k independently follow gradient of the loss \\( -\\nabla_{\\tilde{z}_{y_j}} \\mathcal{L} \\), and decouple as K independent differential equations:\n\\[ \\frac{d\\tilde{z}_{y_j,k}}{dt} = -\\eta \\frac{\\partial \\mathcal{L}}{\\partial \\tilde{z}_{y_j,k}}(t) = \\eta \\lambda_k (z_{y_j,k}^\\prime - \\lambda_k \\tilde{z}_{y_j,k}), \\] \nBy taking the expectation over the same masking blocks, we can have the dynamics for each mask patch \\( y_j \\) as\n\\[ \\frac{d\\tilde{z}_{y_j,k}}{dt} = \\eta \\lambda_k (1 - \\lambda_k) \\tilde{z}_{y_j,k} \\] \nSuppose the parameters of the neural network are denoted as @ with weights \\( W \\in \\mathbb{R}^{M \\times M} \\), we follow\nLemma 3 proposed in [43] and have the empirical \\( \\text{NTK}_\\Theta(Y, Y) \\) in the orthogonal eigenbasis is equal\nto the empirical \\( \\text{NTK}_\\Theta(Y, Y) \\) in the original basis. Each block of the full empirical NTK is denoted\nas \\( \\Theta_t(y_j, Y_q) = (y_j^T y_q) I_M \\), where \\( y_j, y_q \\) denote the high-dimensional inputs from mask patches\n\\( j \\) and \\( q \\) in the training samples. \\( I_M \\in \\mathbb{R}^{M \\times M} \\) is the identity. Based on this and the central limit theorem for an i.i.d standard Gaussian distribution, we can formulate the representational dynamics\nfor each mask patch \\( y_j \\) under the \\( \\mathcal{L} \\) as\n\\[ \\frac{d\\tilde{z}_{y_j}}{dt} = -\\eta \\nabla_{\\Theta_t(y_j, Y)} \\mathcal{L} \\] \n\\[ = -\\eta \\Theta_t(y_j, Y_j) \\nabla_{\\tilde{z}_{y_j}} \\mathcal{L} - \\eta \\sum_{q \\neq j} \\Theta_t(y_j, Y_q) \\nabla_{\\tilde{z}_{y_q}} \\mathcal{L} \\] \n\\[ = -\\eta (y_j^T y_j) \\nabla_{\\tilde{z}_{y_j}} \\mathcal{L} - \\eta \\sum_{q \\neq j} (y_j^T y_q) \\nabla_{\\tilde{z}_{y_q}} \\mathcal{L} \\] \n\\[ = -\\eta \\nabla_{\\tilde{z}_{y_j}} \\mathcal{L} - \\eta \\frac{\\nabla_{\\tilde{z}_{y_j}} \\mathcal{L}}{\\eta_T}(t) \\] \n\\[ = -\\eta \\nabla_{\\tilde{z}_{y_j}} \\mathcal{L} = -\\eta \\nabla_{\\tilde{z}_{y_j}} \\] \n\\[ = -\\eta (Dt \\tilde{z}_{y_j} - \\tilde{z}_{y_q}) Dt \\] \nwhere \\( D_t \\) is a diagonal matrix composed of the eigenvalues \\( \\lambda_{yk} \\). For kth entry of \\( \\frac{d\\tilde{z}_{y_j}}{dt} \\), we can formulate the representational dynamics under the \\( \\mathcal{L} \\) as\n\\[ \\frac{d\\tilde{z}_{y_j,k}}{dt} = \\frac{\\partial \\mathcal{L}}{\\partial \\tilde{z}_{y_j,k}} = \\] \n\\[ -\\eta \\nabla (\\lambda_k \\tilde{z}_{y_j, k} - \\tilde{z}_{y_j, k}^\\prime) \\lambda_k \\] \n\\[ = -\\eta (\\lambda_k \\tilde{z}_{y_j, k} - \\tilde{z}_{y_j, k}^\\prime) \\lambda_k \\] \nSince the expectation over the same masking blocks is \\( \\tilde{z}_{y_j,k} \\), we can have the dynamics for each mask patch \\( y_j \\) as\n\\[ \\frac{d\\tilde{z}_{y_j,k}}{dt} = \\eta \\lambda_k (1 - \\lambda_k) \\tilde{z}_{y_j,k} \\] \n\\[ = \\eta \\lambda_k (1 - \\lambda_k) \\tilde{z}_{y_j,k} \\]"}, {"title": "Representation learning dynamics without the stop-grad and predictor", "content": "Recall we consider linear predictor \\( W_p \\in \\mathbb{R}^{M \\times M} \\) in I-JPEA, the masking objective with the\naverage \\( l_2 \\) distance between the predicted patch-level representations \\( z_{y_j} \\) and the target patch-level\nrepresentations \\( z_{y_j}^\\prime \\) as\n\\[ \\mathcal{L} = \\frac{1}{|M|} \\sum_{i=1}^M \\sum_{j \\in B_i} ||W_p z_{y_j} - SG(z_{y_j}^\\prime)||_2^2, \\] \nWhen removing the stop-grad operator in \\( \\mathcal{L} \\), we can have the masking objective as\n\\[ \\mathcal{L} = \\frac{1}{|M|} \\sum_{i=1}^M \\sum_{j \\in B_i} ||W_p z_{y_j} - z_{y_j}^\\prime||_2^2 = \\] \n\\[ \\frac{1}{|M|} \\sum_{i=1}^M \\sum_{j \\in B_i} \\sum_k || \\lambda_k \\tilde{z}_{y_j, k} - (z_{y_j,k}^\\prime)||_2^2 \\]"}, {"title": "Algorithm", "content": "In this section, we provide detailed algorithmic and implementation aspects of the Contrastive-JEPA\n(C-JEPA) framework, elaborating on the loss functions, pseudo-code, and experimental setup used\nthroughout our studies. These details aim to enable reproducibility and deeper understanding of the\nmodifications introduced in the C-JEPA compared to its predecessors."}, {"title": "Loss", "content": "The loss function for C-JEPA is a combination of the original JEPA loss and the VICReg-inspired\nregularization terms", "follows": "n\\[ \\mathcal{L"}, "mathcal{L}_{JEPA} + \\beta_{vicreg} * \\mathcal{L}_{VICReg} \\"], "as": "n\\[ \\mathcal{L}_{VICReg} = \\beta_{sim} * \\mathcal{L}_{sim} + \\beta_{std} * \\mathcal{L}_{std} + \\beta_{cov} * \\mathcal{L}_{cov} \\] \nwith \\( \\beta_{sim} = 25 \\), \\( \\beta_{std} = 25 \\), and \\( \\beta_{cov} = 1 \\), which scale the similarity, standard deviation, and\ncovariance"}