{"title": "Agent-Based Uncertainty Awareness Improves Automated Radiology Report Labeling with an Open-Source Large Language Model", "authors": ["Hadas Ben-Atya (MS)", "Naama Gavrielov", "Zvi Badash", "Gili Focht (MSc)", "Ruth Cytter-Kuint (MD)", "Talar Hagopian (MD)", "Dan Turner (MD PhD)", "Moti Freiman (PhD)"], "abstract": "Purpose: To improve the reliability and performance of Large Language Models (LLMs) in extracting structured data from radiology reports, particularly in domains with complex and non-English texts (e.g., Hebrew), by incorporating agent-based uncertainty-awareness method to achieve trustworthy predictions in medical applications.\n\nMaterials and Methods: This retrospective study analyzed 9,683 Hebrew radiology reports from Crohn's disease patients (2010-2023) across three medical centers. A subset of 512 reports was manually annotated for six gastrointestinal organs and 15 pathological findings, while the remaining data was automatically annotated using the HSMP-BERT model.\n\nStructured data extraction used Llama 3.1 (Llama 3-8b-instruct), an open-source LLM, with Bayesian Prompt Ensembles (BayesPE) leveraging six semantically equivalent prompts for uncertainty estimation. An Agent-Based Decision Model synthesized multiple prompt outputs into five confidence levels for calibrated uncertainty, compared against three entropy-based models. Performance was assessed via accuracy, F1 score, precision, recall, and Cohen's Kappa, both before and after filtering high-uncertainty cases.\n\nResults: The agent-based model outperformed the baseline across all metrics, achieving an F1 score of 0.3967, recall of 0.6437, and Cohen's Kappa of 0.3006. After filtering high uncertainty cases (\u2265 0.5), the F1 score increased to 0.4787 and Kappa to 0.4258. Uncertainty histograms", "sections": [{"title": "Introduction:", "content": "Radiology reports contain critical clinical information, essential for healthcare decision-making, retrospective studies, and radiologic image annotation. Despite ongoing efforts to implement structured reporting [1], unstructured free-text reports remain dominant, posing challenges for large-scale data analysis due to their variability and lack of standardization. Automating the extraction of structured data addresses these issues by streamlining workflows, reducing manual effort, and improving data consistency and accessibility. This approach not only enhances patient care but also enables large-scale research through comprehensive meta-analyses, accelerated scientific discoveries, and stronger evidence-based practice [2-4].\n\nLarge language models (LLMs) can automate structured data extraction from radiology reports by processing unstructured text to efficiently yield clinically relevant insights [5-7]. However, privacy concerns around sensitive medical data have prompted interest in open-source LLMs, which offer greater transparency, adaptability, and control. Models like Llama 3.1 can extract meaningful insights from medical texts while mitigating risks associated with proprietary systems [8].\n\nDespite their potential, applying LLMs to radiology remains challenging. Overconfidence in predictions undermines reliability in high-stakes medical contexts [9], highlighting the need for robust methods to assess and manage uncertainty. Additional obstacles arise in underrepresented languages like Hebrew, where limited high-quality training data hampers performance. Innovative strategies to enhance LLM adaptability and reliability are essential for effective deployment across diverse linguistic and clinical settings.\n\nRecent progress in uncertainty quantification enhances LLM reliability and interpretability in high-stakes domains like healthcare and radiology. Techniques such as Confidence Elicitation (CE) [10-12], Token-Level Probabilities (TLP), and Sample Consistency (SC) address overconfidence by providing uncertainty measures. However, these approaches treat all responses with equal"}, {"title": null, "content": "weight may overlook variations in quality, relevance, or coherence. Refining these methods is crucial for meeting the demands of increasingly complex applications.\n\nBayesian Prompt Ensembles (BayesPE) [13] enhance uncertainty estimation in LLMs by combining semantically equivalent prompts and optimizing their weights via variational inference, improving prediction reliability and calibration. However, its reliance on a parametric model for prompt weights may limit flexibility in complex or dynamic settings.\n\nAgent-based approaches [14] offer adaptive decision-making for LLM outputs, potentially addressing BayesPE's limitations by incorporating more sophisticated uncertainty estimation processes. This study aims, therefore, to introduce and evaluate an agent-based method for uncertainty-aware, LLM-based structured data extraction from real-world Hebrew radiology reports of Crohn's disease patients, comparing its performance and robustness to the probabilistic aggregation methods of BayesPE."}, {"title": "Materials and Methods:", "content": "Data Collection\n\nThe retrospective multicenter study was approved by the institutional review board. Informed consent was waived due to the retrospective nature of the study.\n\nFree-text Hebrew radiology reports of Crohn's disease patients were sourced from the EPI-IIRN study [15]. The dataset comprises 9,683 reports, each corresponding to a patient visit, collected from 8,093 unique patients across three medical institutions. These reports document Magnetic Resonance Enterography or CT Enterography exams conducted between 2010 and 2023.\n\nManual Data Annotation"}, {"title": null, "content": "We randomly selected 512 reports from the dataset, each manually annotated by a trained radiologist (T.H.) for six gastrointestinal organs (Jejunum, Ileum, Cecum, Colon, Sigmoid, and Rectum). Each report included up to 15 distinct pathological findings (e.g., Inflammation, Wall thickening, Ulceration) [16], resulting in 90 organ-finding combinations. Labels were assigned as 0 (negative), 1 (positive), 2 (organ absent due to surgery), or 9 (organ not visible on MRI). In the main experiment, we treated labels 2 and 9 as negative, leaving two possible outcomes per organ-finding combination. We excluded combinations with \u226415 positive cases, yielding 23 final organ-finding combinations. Fig. 1 shows the distribution of all organ-specific findings and the subset with at least 15 positive samples. Detailed information about the prevalence of the different labels in the test set is provided in Table S2 in the supplementary materials.\n\nNLP Based Annotations\n\nWe used HSMP-BERT, an in-house BERT-based NLP model for structured data extraction from Hebrew radiology reports of Crohn's disease patients [17,18], to annotate the entire dataset of 9,683 reports. The model was trained on the manually annotated subset, focusing on 23 prevalent organ-finding combinations. Performance metrics for these labels appear in Appendix A. To ensure high-quality data for the BayesPE-based methods, we retained only labels with a Cohen's Kappa score above 0.7, resulting in eight final labels: lleum comb sign, Ileum inflammation, Ileum pre-stenotic dilation, Ileum stenosis, lleum wall enhancement, Ileum wall thickness, Rectum wall thickness, and Sigmoid comb sign."}, {"title": "Large Language Model (LLM) utilization for structured data extraction", "content": "We used the Llama 3.1 model (Llama 3-8b-instruct) [19], part of Meta\u2019s multilingual LLM collection, \nto extract structured data from Hebrew radiology reports. Although it supports several languages \n(English, German, French, Italian, Portuguese, Hindi, Spanish, Thai), Hebrew is not among them. \nLlama 3.1 utilizes an auto-regressive transformer architecture [20], further refined via supervised \nfine-tuning (SFT) [21] and reinforcement learning from human feedback (RLHF) [22-24] to better \nalign with human preferences. Given its limited Hebrew support, we carefully adapted the model \nfor this context.\n\nWe prompted the model with various organ\u2013finding combinations to extract structured data from \nHebrew radiology reports, asking whether a specific finding was indicated. We also included a \nself-explanation step for each answer, aiding model validation and improving accuracy, as \npreviously demonstrated. Specifically, we used the following prompt:"}, {"title": null, "content": "Uncertainty Estimation with Bayesian Prompt Ensembles\n\nWe employed the Bayesian Prompt Ensembles (BayesPE) method [13] to estimate uncertainty in \nLLM-generated predictions by leveraging an ensemble of semantically equivalent prompts, \nwithout modifying the LLM\u2019s architecture or requiring retraining. In alignment with their findings, \nwe selected six prompts to balance the benefits of prompt diversity with computational efficiency,"}, {"title": "LLM Prompt example:", "content": "\"Does the following radiology report indicate that the patient has {label}? Here is the report: \n{{}}. Please provide a concise response in JSON format, adhering to the schema: {{'Answer': \n'Yes/No', 'Explanation': 'str'}}. Important: Only return a single valid JSON response. For \ninstance: {{'Answer': 'Yes', 'Explanation': 'The report states that the patient has a severe case \nof the disease.'}}\""}, {"title": null, "content": "1. Agent Decision Model\n\nWe propose an Agent Decision Model to consolidate the outputs of multiple prompts, derive\na final decision, and quantify its uncertainty. This model synthesizes responses and\nexplanations from an ensemble of prompts, producing a unified decision with a rationale for\nits conclusion. The agent evaluates response consistency, assesses the clarity and\ncoherence of explanations, and identifies indications of uncertainty. Based on these factors,\nit categorizes the final decision into one of five distinct confidence levels:"}, {"title": null, "content": "To ensure interpretability and structure, the agent outputs its decision and reasoning in a\npredefined JSON format: {\"Decision\": \"str\",\"Explanation\": \"str\"}. This formatting requirement\nensures that the output is consistent and machine-readable. The agent receives as input a\nset of answers (\"Yes\" or \"No\") and their corresponding explanations generated by the LLM\nwith the different prompts. The agent evaluates these inputs based on the consistency of the\nanswers, the clarity of their explanations, and the degree of ambiguity to produce its final\ndecision. In our implementation, we utilized the Llama 3 - 70B model as the agent. Its outputs\ninclude:"}, {"title": null, "content": "The agent's uncertainty level is derived based on the confidence category of its decision:\n\nThis semi-quantitative approach enables the seamless integration of uncertainty measures\ninto downstream analyses, enhancing the interpretability of the decision-making process. For\nthe \"Uncertain\" response, a final \"Yes\" or \"No\" decision is determined by aggregating the\nprobabilities for each option from the previous step and selecting the option with the highest\noverall score."}, {"title": null, "content": "2. Entropy-based Decision Models\n\nAlongside our agent-based decision model, we implemented three entropy-based models for\ncomparison. These models determine the final prediction and its entropy by applying different\nweighting methods to the prompt outputs, as follows:\n\na) Uniform weights.\n\nb) Linearly optimized weights.\n\nc) Learnable weights using MLP.\n\nWe describe each in detail below."}, {"title": "Uniform Weights:", "content": "In this approach, all prompts are assigned equal weights:\n\n$w_i = \\frac{1}{\\text{len(Prompts)}}$\n\nThis method operates under the assumption of no prior knowledge regarding the efficacy of\nindividual prompts, thus distributing trust uniformly across all prompts. Such an approach\nenhances simplicity and mitigates potential biases in weight assignment."}, {"title": "Linearly Optimized Weights:", "content": "In this approach, we optimized the weights assigned to\ndifferent prompts using a small validation set. We determine the weights by minimizing the\nfollowing objective function:\n\n$L(w_{raw}, \\text{log}(P(y^*|a_i, x))) = \\sum_{i=1}^N w_i \\cdot \\text{log}(P(y^*|a_i, x)) - \\sum_{i=1}^N w_i \\text{log}(w_i)$\n\nwhere $y^*$ denotes the correct answer (e.g., \"Yes\" or \"No\"), $a_i$ represents the $i^{th}$ prompt\nstructure, and x is the radiology report. The weights $w_i$ are computed by applying a softmax\ntransformation to the raw weights $w_{raw}$:\n\n$w_i = \\frac{\\text{exp}(w_{raw,i})}{\\sum_{j=1}^N \\text{exp}(w_{raw,j})}$\n\nThe first term $\\sum_{i=1}^N w_i \\text{log}(p(y^*|a_i, x))$ represents the log-likelihood of the validation data,\nwhile the second term, $\\sum_{i=1}^N w_i \\text{log}(w_i)$, acts as an entropy-based regularizer, discouraging\noverconfidence in a single prompt.\n\nDuring inference, model uncertainty was estimated from the weighted entropy of the ensemble\nof prompts, representing the LLM's confidence in its predictions."}, {"title": null, "content": "We performed linear optimization of prompt weights for each organ-finding label. We utilized\na fixed subset of 50 samples from the manually annotated dataset. We conducted this process\nindependently for each label, resulting in a distinct, fixed weight combination tailored to each\norgan-finding label."}, {"title": "Learnable Weights Using MLP:", "content": "The uniform and linearly optimized weighting methods\nassume fixed or simple relationships between prompts, limiting their ability to capture complex\ninteractions. To address this, we introduce a Multi-Layer Perceptron (MLP) for adaptive\nprompt weight optimization. The MLP dynamically adjusts weights based on prompt behavior\nacross the dataset, enabling more accurate weighting in scenarios with intricate dependencies\nbetween prompts. The forward pass for the model is defined as follows:\n\n$w = \\text{softmax}(MLP(h))$\n\nwhere h represents the fixed prompt embeddings, and w are the normalized weights.\n\nThe training objective combines the binary cross-entropy loss between the ground truth labels\n$y_{gt}$ and the weighted prompt probabilities $\\hat{y}$, with an entropy regularization term to encourage\nwell-calibrated uncertainty:\n\n$\\mathcal{L} = BCE(\\hat{y}, y_{gt}) - \\lambda \\cdot \\sum_i w_i \\text{log}(w_i)$\n\nwhere $\\lambda$ is a hyperparameter that controls the strength of the entropy regularization term.\n\nThe MLP is trained using the Adam optimizer, with gradient clipping applied to ensure stable\nupdates. During inference, the final predicted probabilities for each label (yes/no) are the\nweighted average of the individual prompt probabilities:\n\n$\\hat{y} = \\sum_i w_i p_i$"}, {"title": null, "content": "where $w_i$ are the optimized weights, and $p_i$ are the probabilities associated with each prompt.\nThe final decision is the label with the highest probability. This adaptive approach offers a \ndistinct advantage over simpler methods by dynamically adjusting the prompt weights, \npotentially improving performance and yielding better-calibrated uncertainty estimates.\n\nWe leveraged the automatically labelled cases for the 8 selected labels to train the MLP model. \nWe used 60% of the automatically labelled dataset, comprising 5794 cases, for training, 20% \nfor validation and 20% for internal test of the weights. We trained the MLP model across all \nlabels simultaneously, producing a single model applicable to all labels. During inference, we \nused the trained MLP model to predict prompt weights dynamically for each case and each of \nthe 23 organ-finding label combinations."}, {"title": "Evaluation Setup", "content": "We evaluated our approach on the manually annotated test dataset (462 reports, with 23 organ\u2013\nfinding combinations), excluding the 50 cases used to tune the linear weights. To establish a \nbaseline, we simulated using any single prompt from our set of six, computing performance \nmetrics for each prompt independently and then averaging the results. We then compared various \nBayesian aggregation methods with this baseline, both before and after filtering high-uncertainty \ncases. For each organ\u2013finding label, we measured accuracy, F1, and Cohen\u2019s Kappa at two \nuncertainty thresholds: excluding samples above 0.5 and excluding up to 20% of samples if their \nuncertainty exceeded 0.5. We also generated uncertainty histograms for each model, separating \ncorrect and incorrect predictions, and reported the median uncertainty in each group to assess \nhow well uncertainty levels distinguish between them."}, {"title": "Results:", "content": "Uncertainty-aware prediction without filtering"}, {"title": null, "content": "Table 1 shows that aggregating multiple prompts significantly outperforms using a single prompt.\nThe baseline (single-prompt) model has the lowest metrics, particularly F1=0.2699 and\nKappa=0.1790. In contrast, the agent-based approach achieves the best overall balance, with\nF1=0.3967, recall=0.6437, and Kappa=0.3006, suggesting it handles ambiguity effectively.\nAlthough the MLP model attains the highest accuracy (0.8605) and precision (0.3772), its recall\nis lower (0.3977). Both uniform and linear methods also surpass the baseline, with linear showing\nslightly higher accuracy (0.8454) for similar F1, emphasizing the benefits of weight-optimized\nprompt ensembles."}, {"title": "Uncertainty Histograms", "content": "Figures 3 and 4 show the uncertainty histograms for two representative labels from the four\nmethods, with the Agent model providing the clearest separation between correct and incorrect\npredictions. Additional histograms appear in Appendix D. Table 2 presents the average median\nuncertainty across all 23 labels, illustrating that a well-calibrated model should exhibit low\nuncertainty for correct predictions and high uncertainty for incorrect ones. The Agent model yields\nan average median uncertainty of 0.0 for correct predictions and about 0.5 for incorrect ones,\nindicating superior calibration compared to the other methods."}, {"title": "Uncertainty-aware prediction with filtering", "content": "Tables 3 and 4 present model performance after filtering out cases with uncertainty \u22650.5. Table\n3 imposes no cap on excluded cases, while Table 4 limits exclusion to at most 20%. In both\nscenarios, removing high-uncertainty cases improves accuracy, F1, and Cohen's Kappa.\nAppendix C provides additional tables summarizing the result per label and model.\n\nWith no exclusion cap (Table 3), the MLP model achieves the highest accuracy (92.42%),\nwhereas the Agent approach yields the best F1 (47.87%) and recall (66.14%). Under the 20%\ncap (Table 4), MLP again leads in accuracy (90.87%), and the Agent method retains the highest\nF1 (44.94%) and recall (72.95%). These results highlight the Agent method's strong focus on\nidentifying true positives while maintaining robust calibration, even with constrained exclusions."}, {"title": "Discussion:", "content": "In this study, we evaluated prompt-ensemble-based uncertainty awareness for structured data\nextraction from radiology reports using LLMs. Aggregating predictions from multiple prompts and\nfiltering high-uncertainty cases outperformed single-prompt methods, notably boosting F1 and\nCohen's Kappa.\n\nAmong the tested approaches, the agent-based method provided the best overall balance in F1,\nrecall, and Cohen's Kappa by integrating multiple prompts and accounting for their consistency.\nMeanwhile, the MLP model attained the highest precision and accuracy, useful for scenarios\ndemanding fewer false positives, but its lower recall indicates some missed positive cases. This\ntrade-off underscores the importance of selecting models based on task needs.\n\nAdditionally, the agent approach demonstrated superior calibration, distinguishing correct and\nincorrect predictions more effectively than other methods. Entropy-based metrics further\nconfirmed the agent's robust handling of complex prompt outputs, enhancing prediction reliability."}, {"title": null, "content": "Excluding high-uncertainty cases improved accuracy, F1, and Cohen's Kappa, showing the value\nof uncertainty-aware filtering for better alignment between model confidence and prediction\ncorrectness. Setting class-specific thresholds at the output layer can improve recall for minority\nclasses while preserving precision for majority classes, thus optimizing performance for the target\napplication.\n\nLarge language models (LLMs) efficiently automate structured data extraction from radiology\nreports by converting unstructured text into clinically relevant insights [5-7]. However, ensuring\ntrustworthy application through robust uncertainty quantification remains an open challenge.\nWhile Zeng et al. [26] demonstrated the potential of multiple interacting LLM agents with\nspecialized tasks to improve performance in medical applications, their role in quantifying\nuncertainty for reliable use remains underexplored. This study is the first to introduce a\ngeneralizable agent-based approach that quantifies uncertainty and fosters the trustworthy use\nof LLMs for structured data extraction from radiology reports.\n\nOur study has limitations. First, while the focus was on radiology reports, extending these\nmethods to LLM-based analysis of other medical reports, such as pathology reports or electronic\nmedical records, requires additional validation. Second, optimizing ensemble techniques for\nhighly imbalanced datasets remains a challenge. Future work should explore tailored approaches\nto address these limitations and investigate the application of uncertainty quantification in broader\nclinical scenarios.\n\nIn conclusion, our findings demonstrate the effectiveness of prompt ensembles-based uncertainty\nawareness in enhancing LLM performance for structured data extraction in radiology. The agent-\nbased approach emerged as particularly robust, achieving superior results. Incorporating\nuncertainty quantification not only improves reliability but also facilitates interpretability, paving\nthe way for more impactful and trustable Al applications in healthcare."}]}