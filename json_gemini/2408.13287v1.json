{"title": "Abstract Art Interpretation Using ControlNet", "authors": ["Rishabh Srivastava", "Addrish Roy"], "abstract": "Our study delves into the fusion of abstract art interpretation and text-to-image synthesis, addressing the challenge of achieving precise spatial control over image composition solely through textual prompts. Leveraging the capabilities of ControlNet, we empower users with finer control over the synthesis process, enabling enhanced manipulation of synthesized imagery. Inspired by the minimalist forms found in abstract artworks, we introduce a novel condition crafted from geometric primitives such as triangles.", "sections": [{"title": "1. Introduction", "content": "Visual inspiration often strikes unexpectedly, prompting a desire to immortalize fleeting mental imagery into tangible forms. With the advent of text-to-image diffusion models [14], such aspirations have become increasingly attainable through the simple act of textual description. However, the inherent limitations of these models in providing precise spatial control over image composition persist, presenting challenges in accurately conveying complex layouts, poses, shapes, and forms solely through textual prompts.\nWhile existing methodologies primarily rely on textual prompts to guide image generation, the integration of additional image conditions has emerged as a promising approach to empower users with finer control over the synthesis process. One major breakthrough in this domain was the ControlNet architecture [21]. ControlNet revolutionized the field by enabling the integration of diverse conditioning inputs with Stable Diffusion, ranging from edge maps and human pose skeletons to segmentation maps and depth information. By treating these additional images as conditions for the image generation process, ControlNet paved the way for enhanced spatial control and nuanced manipulation of synthesized imagery.\nBuilding upon the foundation laid by the ControlNet architecture introduced in the previous year, our project focuses on training ControlNet with a new condition. Inspired by the simplicity and versatility of geometric shapes in abstract artworks, our new condition is crafted from geometric primitives like triangles. These primitives are meticulously arranged to approximate the original image in the training dataset, yet imbue it with the essential qualities of abstract artistry.\nThe concept stems from the recognition that abstract art often relies solely on shapes to convey its message, emphasizing the power of minimalist forms in eliciting diverse interpretations. Such artistry, characterized by its reliance on geometric elements, serves as the foundation for our new condition image. Moreover, the subjective nature of human imagination further fuels this endeavor, as diverse textual prompts can yield a myriad of representations from the same abstract artwork. Thus, our pursuit is grounded in the rich tapestry of abstract expressionism, where geometric shapes serve as the catalyst for exploring the boundless realm of artistic interpretation and creativity.\nIn summary, our project presents a pioneering exploration into the intersection of abstract art interpretation and text-to-image synthesis. We aim to train a novel condition and conduct qualitative assessments of the outcomes, condition fidelity, and the overall quality of the generated images."}, {"title": "2. Related Work", "content": "Diffusion models belong to a category of probabilistic generative models. They operate by gradually altering data through noise injection and subsequently learn the ability to reverse this process for generating samples. Sohl-Dickstein and colleagues pioneered Image Diffusion Models [16], a concept subsequently adapted to image generation [3]. Latent Diffusion Models (LDM) [14] streamline computation by conducting diffusion steps in the latent image space [4]. Text-to-image diffusion models, utilizing pre-trained language models such as CLIP [11], excel in generating images by translating textual inputs into latent vectors. Stable Diffusion [20], a large-scale implementation of latent diffusion, is notable in the field.\nControlling Image Diffusion Models empowers users with the ability to personalize, customize, and generate images tailored to specific tasks. Methods for guiding image"}, {"title": "3. Methodology", "content": ""}, {"title": "3.1. Dataset Preparation", "content": "The dataset utilized in this study was meticulously curated from a subset of the Wikipedia-based Image Text (WIT) Dataset [17], a colossal multimodal multilingual repository comprising an astounding 37.6 million image-text examples.\nTo construct our dataset, we utilized the image URLs sourced from the WIT dataset to download the original images. These images served as the target images when we trained our ControlNet model. While the WIT dataset offers image descriptions, their specificity and granularity were not ideally suited for training our model. Thus, cap-"}, {"title": "3.2. ControlNet Architecture", "content": "Fig. 2 above helps to visualize the setup for ControlNet where we can see that ControlNet integrates additional conditions into the main neural network block (for example, it can be resnet block, multi-head attention block, transformer block, etc.). Consider the trained neural network block to be represented by $F(\\cdot; \\Theta)$ where the $\\Theta$ refers to the parameters of the block. Let, $x \\& y$ denote the input & output of this block. Then,\n$y = F(x; \\Theta)$\nTo inject a ControlNet into this block, we lock the parameters $\\Theta$ of the pre-trained original block and duplicate the"}, {"title": "3.3. Training", "content": "Once the dataset is prepared, the next step is to load the pre-trained model. The model architecture is defined in a YAML configuration file, encapsulating its structure and parameters. Loading the pre-trained model weights initializes the model with prior knowledge learned from extensive training on large datasets. This initialization kickstarts the model's ability to generate meaningful images based on textual input.\nWith the dataset and model in place, the training process is initialized. This involves setting up a DataLoader to efficiently feed batches of data into the model during training. The DataLoader handles tasks such as batching, shuffling, and parallel data loading, optimizing the training workflow.\nThe model is then trained to learn to generate images by iteratively processing image-text pairs and optimizing its parameters to minimize the discrepancy between generated and ground truth images like in Stable Diffusion. During each iteration, the model adjusts its internal representations, gradually improving its ability to translate textual prompts into realistic images. The use of GPU acceleration speeds up this process significantly, allowing for efficient exploration of the high-dimensional parameter space."}, {"title": "3.4. Inference", "content": "Once the ControlNet parameters are trained, the model becomes capable of generating new images. Inference becomes straightforward by inputting a sample abstract image along with a prompt. While inference can technically function without prompts, it is important to note that our model's performance without prompts may be sub-optimal. This limitation arises from the fact that our model has not been extensively trained on datasets lacking prompts."}, {"title": "4. Experimental Results", "content": "We applied the ControlNet architecture on Stable Diffusion and trained the resultant model using 14000 images from our dataset. The model was trained on an Nvidia T4 GPU, and a batch size of 2 was employed to ensure compliance with the memory constraints of the GCP Compute Engine instance. The code can be found at [19].\nAs stated in the ControlNet paper [21], we also observed the sudden convergence phenomenon (Fig. 3). As a result of zero convolutions, ControlNet consistently generates high-quality images throughout the training process. However, at a specific stage in training, the model undergoes a sudden learning event wherein it begins to adhere closely to the input condition, which is noted as the sudden convergence phenomenon.\nFig. 4 illustrates the results generated by our trained model for a given test image. This same image can be perceived in multiple ways, as evidenced by the diverse prompts showcased. Each prompt breathes life into its own distinct imaginative space, showcasing the richness and depth of creative interpretation."}, {"title": "5. Discussion", "content": "Successfully training a ControlNet model on a novel control image concept marks a significant achievement in our research endeavor. To facilitate this, we meticulously curated a custom dataset tailored to our specific requirements.\nDuring the training process, our ControlNet exhibited remarkable adaptability and proficiency, effectively learning to generate diverse images in accordance with the provided prompts. Notably, it adeptly preserved the approximated object locations, underscoring its ability to maintain spatial consistency while exploring the creative possibilities inherent in the input conditions.\nHowever, a notable limitation was observed: while the trained model exhibited proficiency in determining object placements and aligning with the desired prompt descriptions and input control image, it struggled to accurately replicate colors sometimes. This shortfall likely stems from the need for further training. Regrettably, due to constraints on compute resources, we were unable to conduct more extensive training sessions to address this deficiency."}, {"title": "6. Conclusion", "content": "In conclusion, our exploration delves into the confluence of abstract art interpretation and text-to-image synthesis, addressing the intrinsic challenges of achieving precise spatial control over image composition solely through textual prompts. By extending the capabilities of pre-trained large diffusion models, ControlNet empowers users with finer"}]}