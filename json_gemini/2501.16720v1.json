{"title": "One Head Eight Arms: Block Matrix based Low Rank Adaptation for CLIP-based Few-Shot Learning", "authors": ["Chunpeng Zhou", "Qianqian Shen", "Zhi Yu", "Jiajun Bu", "Haishuai Wang"], "abstract": "Recent advancements in fine-tuning Vision-Language Foundation Models (VLMs) have garnered significant attention for their effectiveness in downstream few-shot learning tasks. While these recent approaches exhibits some performance improvements, they often suffer from excessive training parameters and high computational costs. To address these challenges, we propose a novel Block matrix-based low-rank adaptation framework, called Block-LoRA, for fine-tuning VLMs on downstream few-shot tasks. Inspired by recent work on Low-Rank Adaptation (LoRA), Block-LoRA partitions the original low-rank decomposition matrix of LoRA into a series of sub-matrices while sharing all down-projection sub-matrices. This structure not only reduces the number of training parameters, but also transforms certain complex matrix multiplication operations into simpler matrix addition, significantly lowering the computational cost of fine-tuning. Notably, Block-LoRA enables fine-tuning CLIP on the ImageNet few-shot benchmark using a single 24GB GPU. We also show that Block-LoRA has the more tighter bound of generalization error than vanilla LoRA. Without bells and whistles, extensive experiments demonstrate that Block-LORA achieves competitive performance compared to state-of-the-art CLIP-based few-shot methods, while maintaining a low training parameters count and reduced computational overhead.", "sections": [{"title": "Introduction", "content": "In recent years, research on Foundation Models has made significant progress, demonstrating strong generalization capabilities and remarkable performance across various downstream tasks. In particular, Vision-Language Foundation Models (VLMs) have shown impressive performance in Computer Vision tasks, such as CLIP"}, {"title": "Related Work", "content": null}, {"title": "Vision-Language Models", "content": "Vision-Language Models (VLMs) have gained significant attention in recent years, driving substantial advancements in integrating visual and textual data. A typical example is CLIP (Contrastive Language-Image Pre-training), which employs a contrastive learning approach to jointly train an image encoder and a text encoder. CLIP learns to align the features of images and their corresponding text descriptions feature, enabling zero-shot transfer to various downstream tasks by comparing the similarity between between encoded image and text representations. Based on CLIP, ALIGN leverage a large-scale noisy dataset without requiring expensive filtering or post-processing steps during the data-gathering phase. By training on massive multi-modal datasets in an end-to-end manner, VLMs have demonstrated remarkable performances on various downstream tasks by effectively leveraging pre-trained knowledge. However, despite their strong transfer capabilities, efficiently adapting VLMs to few-shot learning tasks while keeping training costs manageable remains a significant challenge."}, {"title": "Few-Shot Learning", "content": "Few-shot learning aims to enable models to learn novel concepts from only using the limited labeled samples, mimicking human-like learning abilities. Conventional few-shot learning methods typically rely on an additional base dataset and the meta-training strategy to pre-train the models for the generalization capabilities. Recent advancements in pre-trained VLMs have led to a new paradigm in few-shot learning, where meta-training and base datasets are no longer required. For example, CoOP directly fine-tunes CLIP with the learnable context prompts to improve few-shot performance. CoCoOP extends CoOp by incorporating the input-conditional tokens, combined with the learnable context vectors. MaPle and PromptSRC both employ the multi-modal prompt learning, applying prompting techniques to both the image and text encoder to enhance few-shot adaptation. Another line of research focuses on adapter-based techniques to fine-tune CLIP for few-shot tasks. For instance, CLIP-Adapter integrates a learnable adapter with a bottleneck layer to refine feature transformations. GraphAdapter introduces a graph learning based adapter to capture the structure knowledge. More recently, CLIP-LORA directly applies LoRA to fine-tune CLIP. Despite the promising results achieved by these methods, most suffer from high computational costs and large parameter counts, making efficient fine-tuning of VLMs for few-shot tasks an ongoing challenge."}, {"title": "Preliminaries", "content": null}, {"title": "Contrastive vision-language pretraining", "content": "Contrastive vision-language pre-training has demonstrated impressive transfer performance sur-"}, {"title": "CLIP-based Few-Shot Learning", "content": "Although CLIP enable zero-shot Classification without training, its performance degrades significantly in the presence of domain shifts or uncertain concepts in downstream tasks. Consequently, fine-tuning CLIP with few available samples from downstream datasets is vital. Formally, in CLIP-based Few-Shot Learning, a collection of known images {xili \u2208 NK} from a downstream dataset is given, where N denotes the total number of categories and K is the number of labeled images (shot) per category, which is typically small (e.g., K\u2264 16). The goal of few-shot learning is to predicted the labels of the remained unknown images in the dataset, using only these NK samples, a setting commonly referred to as K-shot learning. Inspired by recent progresses of Parameter-Efficient Fine-Tuning (PEFT) in NLP, most proposed CLIP-based few-shot learning methods utilize the PEFT paradigms to fine-tune the pre-trained CLIP, showing better performance and parameter-efficient compared to the full fine-tuning. They will only fine-tune a small portion of parameters in CLIP or introduce the additional learnable modules, while keeping the most portion of parameters in CLIP frozen. And the fine-tuned visual feature and textual feature with few-shot data are denoted as vi and ti, respectively. And the training process of PEFT with CLIP is formulated as vi and ti, respectively. The training process is formulated as:"}, {"title": "Method", "content": "In this section, we first introduce the Low-Rank Adaptation (LoRA), and then details the Block Matrix based Low-Rank Adaptation (Block-LoRA)."}, {"title": "Low-Rank Adaptation (LoRA)", "content": "Low-Rank Adaptation (LoRA) is a recently proposed PEFT method, which simulates the update of the pre-trained model weights by injecting trainable low-rank decomposition matrices into each layer, while keep the original model weights frozen. As illustrated in Fig 2 (a), LoRA only optimizing the low-rank decomposition matrices, when adapting to a downstream task. Specifically, given a pre-trained weight W \u2208 Rk\u00d7d, the updated weight matrix \u0174 by LORA is formulated as: \u0174 = W+AW = W+BA, where a low-rank decomposition \u25b3W = AB, A \u2208 Rkxr,Be Rrxd models this weight update during training, and the rank r < min(d,k). Consequently, the forward pass with LoRA is formulated as:"}, {"title": "Block Matrix based Low-Rank Adaptation", "content": "Though using LoRA to fine-tune CLIP achieves the promising few-shot performance, it ignore the existed redundancy in the vanilla LoRA structure. To alleviate this, we introduce Block-LoRA to further reduce the redundancy for few-shot adaptation while maintaining effectiveness. Similar to LoRA, we assume the low-rank parameter update follows: W = W + \u2206W = W + AB. As detailed in Fig 2 (b), the proposed Block-LoRA first partitions the two vanilla low-rank matrices A \u2208 Rk\u00f6r and B\u2208 Rr\u00d7d into multiple sub-matrix multiplications along the dimension of rank r, without modifying other dimensions:"}, {"title": "Theoretical Analysis", "content": "We define W = {W1}{=1 as the all L parameter matrices of a pre-trained model. Let I C {1,..., L} be a set of the parameter matrice index to be fine-tuned. Given a labeled training set S\u2208 Z with i.i.d. training examples from unknown real data distribution \u00b5, we analyze the adaptation process. Let r denote the rank in both LoRA and Block-LoRA. We define the number of training as #S and suppose each tuned parameter is quantized to q bits. We define the fine-tuning based adaptation frameworks by using an adaptation matrices set as \u2206W = {W1}{1, leading to the fine-tuned parameters: W = {\u0174_1}{=1 with \u0174\u2081 = W\u2081 + \u2206W\u2081 for l \u2208 I. We formally define the LoRA and Block-LoRA adaptation as follows:"}, {"title": "Experiments", "content": null}, {"title": "Datasets and Settings", "content": "We evaluate our method on 11 publicly available datasets widely used in few-shot classification and cross-dataset transfer tasks, strictly following prior works. These datasets include:"}, {"title": "Main Results", "content": "Few-shot Classification. This section mainly shows the results of the proposed Block-LoRA on few-shot image classification tasks, comparing to the recent methods, including COOP , MaPLe , PromptSRC  and CLIP-LORA . The few-shot image classification results on 11 datasets are summarized in Fig 3. Firstly, Block-LoRA demonstrates highly competitive few-shot performance across all 11 datasets. For example, Block-LoRA outperforms CLIP-LORA in average accuracy on 11 datasets,"}, {"title": "Model Analysis", "content": "Hyperparameter analysis. As mentioned above, to ensure consistency with the default rank in CLIP-LORA, Block-LORA(2, 2) is as the default configuration in our experiments, where the rank r and the number of blocks nare both 2 by default. To further investigate the impacts of these hyperparameters on downstream performance, we evaluates the performance of different rank r and number of blocks n in few-shot image classification task across the above 11 datasets. The results are summarized in Table 3, which contains the classification accuracy under the 1-shot setting. Notably, Block-LORA(r, n) is equivalent to the vanilla LoRA when n = 1. All experimental settings are consistent with those in the few-shot classification task, and results are averaged over three runs. From Table 3, we observe that: First, increasing the rank slightly improves accuracy at most cases. For example, the average accuracy increases from 72.86% for Block-LORA(2, 2) to 73.07% for Block-LORA(4, 2) and 73.44% for Block-LoRA(8,2). However, a higher rank also leads to an increase in the number of trainable parameters and computational complexity. Secondly, using the matrix Block sharing operation and choosing the appropriate number of blocks n will improve the accuracy compared to vanilla LoRA, while"}, {"title": "Conclusion", "content": "The recent Vision-Language Foundation Models (VLMs) based few-shot learning have achieved promising results. However, these models often suffer from excessive training parameters and high computational costs. To address these challenges, we propose a novel Block matrix-based low-rank adaptation framework, Block-LoRA, for fine-tuning VLMs on downstream few-shot tasks. Block-LoRA partitions the original low-rank decomposition matrix of LoRA into multiple sub-matrices while sharing all down-projection sub-matrices. This structure not only reduces the number of trainable parameters but also simplifies certain complex matrix multiplications into more efficient matrix additions, significantly lowering the computational cost of fine-tuning. Extensive experiments demonstrate that Block-LoRA achieves competitive performance compared to SOTA CLIP-based few-shot methods, while maintaining a low training parameters count and reduced computational overhead. The effectiveness of Block-LORA on other visual or textual tasks deserves further exploration."}, {"title": "Appendix", "content": null}, {"title": "Proof of Lemma 1", "content": "We first proof the generalization upper bound of vanilla LoRA [Hu et al., 2022]. Following previous works [Xu and Raginsky, 2017; Zhu et al., 2024], we have the bound as follows:\nTheorem 1. The LoRA-based fine-tuning algorithm has an\nL\nadaptation matrices set as \u2206W = {\u2206W1}{=1, trained by\ndataset S. Assume that the loss (W(\u2206W, Z) is o-sub-\nGaussian under (\u2206W, Z) ~ P\u25b3w|w \u00d7 \u03bc. Then,\nNext, we proof the generalization upper bound of the proposed Block-LoRA in this paper. Similar with LoRA, we have:\nL\nTheorem 2. The Block-LoRA-based fine-tuning algorithm (Block for short) has an adaptation matrices set as\nAW = {\u2206W1}{=1, trained by dataset S. Assume that\nthe loss lW (AW, Z) is o-sub-Gaussian under (\u2206W, Z) ~ \nPow\\w \u00d7 \u00b5. Then,"}, {"title": "B Details of Datasets", "content": null}, {"title": "Few-shot learning dataset", "content": "This paper employs 11 publicly available image datasets for few-shot learning tasks, consistent with previous works [Zhou et al., 2022b; Zanella and Ben Ayed, 2024]. These datasets include:\nDatasets for Generic Object Recognition Tasks: The ImageNet [Russakovsky et al., 2015] and Caltech101 [Fei-Fei et al., 2004] datasets. ImageNet is a large-scale dataset widely used in the field of computer vision for general object recognition, containing a vast number of images across 1000 object categories. Caltech101, also serves as a benchmark for general object recognition tasks, with 101 object categories.\nDatasets for Fine-Grained Image Classification Tasks:\nThe OxfordPets [Parkhi et al., 2012], StanfordCars [Krause et al., 2013], Flowers102 [Nilsback and Zisserman, 2008], Food101 [Bossard et al., 2014], and FGVCAircraft [Maji et al., 2013] datasets. OxfordPets focuses on classifying pet images, which requires fine-grained discrimination between different breeds. StanfordCars is dedicated to classifying car models, where the differences between classes are often subtle. Flowers102 is designed for flower classification, and the fine-grained nature of flower species demands high precision classification. Food101 is used for classifying different types of food images, and FGVCAircraft is specialized in fine-grained classification of aircraft images.\nDataset for Scene Recognition Task: SUN397 [Xiao et al., 2010] dataset is specifically designed to assist in scene recognition tasks, covering a wide range of natural and man-made scenes, enabling models to learn the characteristics of different scenes.\nDataset for Action Recognition Task: The UCF101 [Soomro, 2012] dataset contains a large number of video sequences related to 101 different human actions.\nDataset for Texture Classification Task: The DTD [Cimpoi et al., 2014] dataset is focused on texture classification, with a collection of images that represent various textures, allowing models to learn to distinguish between different texture patterns.\nDataset for Satellite-based Land Use and Cover Classification Task: The EuroSAT [Helber et al., 2019] dataset consists of satellite images for land use and cover classification, helping to analyze and classify different types of land use from satellite-based imagery."}, {"title": "Domain generalization dataset", "content": "For the domain generalization task, this paper uses 4 publicly available domain generalization datasets, all of which are variants of the ImageNet dataset. These include:\nImageNetV2 [Recht et al., 2019] dataset is a newly re-collected test data, ensuring its difference from previous"}, {"title": "Ablation Study", "content": "As shown in Eq (9), the proposed Block-LoRA introduces a learnable shared down-projection matrix As. Recent studies on LoRA consistently suggest that down-projection matrix matrices A are generally less important than up-projection matrix matrices B [Zhu et al., 2024; Zhang et al., 2023]. To investigate this further, we evaluate a variant of Block-LoRA, denoted as w/o A, where A is randomly initialized and remains fixed during training (i.e., it is not updated). We con-"}]}