{"title": "BACKTRACKING IMPROVES GENERATION SAFETY", "authors": ["Yiming Zhang", "Jianfeng Chi", "Hailey Nguyen", "Kartikeya Upasani", "Daniel M. Bikel", "Jason Weston", "Eric Michael Smith"], "abstract": "Text generation has a fundamental limitation almost by definition: there is no taking back tokens that have been generated, even when they are clearly problem-atic. In the context of language model safety, when a partial unsafe generation is produced, language models by their nature tend to happily keep on generating similarly unsafe additional text. This is in fact how safety alignment of frontier models gets circumvented in the wild (Andriushchenko et al., 2024), despite great efforts in improving their safety. Deviating from the paradigm of approaching safety alignment as prevention (decreasing the probability of harmful responses), we propose backtracking, a technique that allows language models to \"undo\" and recover from their own unsafe generation through the introduction of a special [RESET] token. Our method can be incorporated into either SFT or DPO train-ing to optimize helpfulness and harmlessness. We show that models trained to backtrack are consistently safer than baseline models: backtracking Llama-3-8B is four times more safe than the baseline model (6.1% \u2192 1.5%) in our evaluations without regression in helpfulness. Our method additionally provides protection against four adversarial attacks including an adaptive attack, despite not being trained to do so.", "sections": [{"title": "1 INTRODUCTION", "content": "Remarkable progress has been recently made in building capable and helpful large language mod-els (Touvron et al., 2023). As capabilities become more powerful, these models also have more potential to cause real societal harms (Kumar et al., 2023). The de facto standard in safety align-ment focuses on prevention: training language models to generate safe responses while minimiz-ing the likelihood of unsafe ones, through techniques including SFT (Ouyang et al., 2022) and RLHF (Christiano et al., 2017). Prevention-based safety tuning goes a long way towards building safe language models (Bai et al., 2022a), and yet the safety of production models which have under-gone substantial safety tuning (e.g., Claude 3.5 and GPT-4) can still be compromised in the wild by simple attacks (Andriushchenko et al., 2024).\nThe core challenge of safety alignment seems to be that the attack surface induced by a text interface is practically infinite, and model developers have to rely on the generalization of safe behavior from a relatively small (often predominantly in English) safety tuning dataset to prevent every failure case. To illustrate just how big this attack surface truly is, two recent papers jailbreak GPT-4 by encoding malicious instructions in base64 (Wei et al., 2023) and in low-resource languages such as Zulu (Yong et al., 2024). It is plausible that, through pre-training, GPT-4 picked up capabilities to understand a variety of encodings and languages, but that safety tuning failed to cover these \u201cedge\u201d cases. In this work, we ask: how can we meaningfully improve language model safety, given that models will likely always produce some unsafe generations? We hypothesize that it can be easier for a model to verify safety after some tokens have been generated, instead of relying on prevention of unsafe generations solely. To this end, we introduce backtracking, a technique that allows language models to verify and \u201cundo\u201d prior unsafe generations and start anew. Specifically, we allow language models to emit a special token, [RESET], upon which the generation API discards tokens prior to [RESET]. During training, we supervise language models to properly backtrack by constructing contrastive safety demonstrations, generations that are initially unsafe, but then backtrack into a"}, {"title": "2 RELATED WORK", "content": "Generation refinement. Language model generations are prone to errors (Holtzman et al., 2019), and a large body of work focuses on correcting generation errors through a critique-and-refine pro-cess (Pan et al., 2024). Critique can be provided by the language model itself (Saunders et al., 2022; Madaan et al., 2023), or from an external critic model (Yasunaga et al., 2021; Welleck et al., 2022). This feedback can be distilled back into the model (Bai et al., 2022b; Yuan et al., 2024) to further improve model quality. Various algorithms such as best-of-k (Nakano et al., 2022) and majority-voting (Wang et al., 2022) can also be applied to improve generation quality at inference time. This need for post-generation refinement is necessitated by the fact that generation cannot be undone, and thus corrections have to be applied post-hoc. The goal of our work is precisely to introduce a mechanism for backtracking into a language model, thus enabling it to recover from an unsafe gen-eration and removing the need for post-hoc correction. Outside of the domain of generation safety, but similar in spirit to our work, Cundy & Ermon (2023) proposes token-level backtracking which defines a special token for removing a single token, and Ye et al. (2024) introduces a backspace action that removes an entire sentence to undo mistakes in math reasoning.\nGeneration safety. Safeguarding model generation is a key issue for model providers (Bai et al., 2022a; Inan et al., 2023; Dubey et al., 2024), and numerous alignment techniques have been devel-oped to ensure that language models produce harmless content by steering them away from harmful behavior (Christiano et al., 2017; Bai et al., 2022a; Rafailov et al., 2023). However, manually crafted"}, {"title": "3 TEACHING LANGUAGE MODELS TO BACKTRACK", "content": "Our approach enables training of a language model that backtracks after an unsafe generation is produced. In this work, we define backtracking as the behavior of recognizing a partial unsafe re-sponse through the production of a special [RESET] token and then re-generating a safe response from scratch. Note that the partial unsafe response remains in the context window of the language model during the generation of tokens after backtracking which can aid the model in the produc-tion of a safer response. When generation is complete, all tokens up to and including the [RESET] token would then be discarded (if they appear) when returning the final output to the user. We further assume that the model backtracks at most once in a single generation. This definition is no-tably different from prior work (Cundy & Ermon, 2023) that approaches backtracking in language models through the introduction of a backspace token that indicates the removal of the preceding token, which can occur an arbitrary number of times. By restricting our definition to one-shot back-tracking, we can avoid framing text generation (with backtracking) as a Markov decision process as in Cundy & Ermon (2023), which is not obviously compatible with language model post-training algorithms including RLHF. In other words, our design choice makes backtracking learnable via ex-isting algorithms for language model post-training. Empirically, we observe that model generations after backtracking are almost always safe, suggesting that backtracking just once is sufficient in the context of safety (Section 4.2).\nFollowing a standard post-training recipe (Dubey et al., 2024), we fine-tune a pre-trained language model to backtrack through supervised fine-tuning (SFT) on instruction-following data, followed by direct preference optimization (DPO) on pairwise preference data. We provide an overview of our method in Figure 1."}, {"title": "3.1 SUPERVISED FINE-TUNING", "content": "In a standard supervised fine-tuning setup (Sanh et al., 2021; Wei et al., 2021), pre-trained language models are further fine-tuned to follow user instructions (Ouyang et al., 2022) to make them more useful. With backtracking demonstrations added into the instruction tuning dataset, we could use standard instruction tuning to supervise language models to imitate backtracking. We note that SFT alone does not improve model safety by much empirically, but it crucially serves as warm-up for backtracking preference tuning which leads to substantial safety gains (Section 4.2).\nWe start by assuming access to a standard safety tuning dataset $D_{SFT} = \\{(x_i, y_i^+) \\vert i \\in [n]\\}$, where $x_i$ is a prompt and $y_i^+$ is the ideal safe response. Because safety tuning datasets are commonly constructed by sampling model generations followed by human annotation (Bai et al., 2022a; Dai et al., 2023), it is not uncommon in safety tuning that we have an additional unsafe response $y_i^-$. These unsafe responses are discarded in SFT, because maximizing likelihood over them simply makes the fine-tuned model less safe. However, we can leverage these unsafe responses to construct backtracking demonstrations. Intuitively, when an unsafe response is produced, we supervise the model to backtrack and produce a safe response instead.\nSpecficially, given a prompt x, safe response $y^+$ and unsafe response $y^-$, we first use a safety classifier, such as Llama Guard 2 (Team, 2024), to perform rejection sampling over random prefixes of $y^-$ until we identify an unsafe prefix of $y^-$. The reason to perform rejection sampling (as opposed to random sampling or using the full unsafe response) is that it is desirable for the model to backtrack"}, {"title": "3.2 PREFERENCE TUNING", "content": "RLHF has been an essential component in the post-training of frontier large language models (Ope-nAI, 2023; Dubey et al., 2024; Gemma Team et al., 2024) to make them helpful and harmless (Bai et al., 2022a). RLHF algorithms in general involve optimizing language models so that their behavior is consistent with human judgements in the form of preferred and dispreferred response pairs (Chris-tiano et al., 2017; Azar et al., 2024).\nWe construct backtracking preference pairs and teach language models when or when not to back-track through RLHF. To construct a positive example where backtracking is desirable, we create a preference pair with $prefix(y^-) \\oplus \\text{[RESET]} \\oplus y^+ > y^-$. By contrast, if the model already produces a safe response, there is no need for the model to backtrack at all. To discourage the model from unnecessary backtracking, we introduce negative examples by sampling a random prefix of the safe response $y^+$, and create a preference pair with $y^+ > prefix(y^+) \\oplus \\text{[RESET]} \\oplus y^-$. It's possible to introduce other types of negative examples, for example $y^+ > prefix(y^+) + \\text{[RESET]} \\oplus y'^+$, to pre-vent backtracking from an already safe generation. However, we do not explore alternative methods of constructing negative examples for simplicity. To elicit desirable backtracking behavior, we apply DPO off-the-shelf for preference optimization (Rafailov et al., 2023) over the constructed preference pairs. Similar to SFT, we mix in general utility data during DPO to improve model helpfulness.\nGeneration from a backtracking language model is straightforward. After sampling a generation $y \\sim p_\\theta(x)$, we simply check whether the [RESET] token is present in y. If [RESET] is not present, we simply return all tokens. If [RESET] is present, we discard all tokens generated before and including [RESET], and return all tokens after."}, {"title": "4 BACKTRACKING IMPROVES GENERATION SAFETY", "content": "In this section, we evaluate the ability of backtracking to improve the safety of fine-tuned models, and we analyse how backtracking may impact the generation efficiency of the language model."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "Training data. In language model post-training, it is crucial to balance the safety and helpfulness of models. We use the OpenAssistant-2 (OA) dataset (K\u00f6pf et al., 2023) for general utility training. We filter for English data only and take the top two ranked responses to construct preference pairs. For safety training, we use the harmless subset of the HH-RLHF dataset (Bai et al., 2022a). These data remain noisy: occasionally the chosen response is unsafe, and there is a substantial fraction of non-safety-related prompts. Thus, we use Llama Guard 2 as a safety classifier to filter for preference pairs such that the chosen response is safe and the rejected response is unsafe. Targeting a 90:10 mixture of utility and safety data, we end up with 12,260 pairs from OA and 1,367 pairs from HH-RLHF. (In preliminary experiments, we had explored a 50:50 mix of utility and safety data, which did not lead to meaningful safety gains but substantially degraded model helpfulness.)\nModels. Post-training relies heavily on the quality of pre-trained models, and we experiment with backtracking on top of two state-of-the-art pre-trained text models at different sizes, Gemma-2-2B (Gemma Team et al., 2024) and Llama-3-8B (Dubey et al., 2024). We specifically choose to use base versions of these models because their instruction-tuned variants have already undergone heavy safety tuning on proprietary data, which confounds the true performance of the tuning methods themselves. We compare our backtracking method with baseline models fine-tuned on the same data without backtracking, to measure the effect of backtracking on model safety. The best baseline and backtracking models are chosen by their safety on the HH-RLHF test set after hyperparameter tuning (details in Appendix A.2).\nSafety and utility evaluation. We use the existing open-source safety evaluation datasets AdvBench (Zou et al., 2023b, AB), MaliciousInstructions (Bianchi et al., 2023, MI), SimpleSafetyTests (Vidgen et al., 2024, SST), and StrongREJECT (Souly et al., 2024, SR) for evaluation. Our entire safety evaluation set contains a total of 1,033 prompts (see sizes of individual datasets as well as examples in Appendix A.1). We report the safety of models for the four sets of prompts by generating responses with greedy decoding, and we use Llama Guard 2 to provide safety labels. We further evaluate model helpfulness on MT-Bench (Zheng et al., 2023), a challenging benchmark of multi-turn questions that evaluates capabilities including math and writing."}, {"title": "4.2 RESULTS", "content": "Backtracking improves model safety. We report safety evaluation results of backtracking and baseline models in Table 1. For both models, we observe that the backtracking model (with SFT + DPO) is substantially safer than all other setups. Backtracking improves the safety of Gemma over the DPO baseline on three out of four safety benchmarks, and we observe a -42% relative reduction in overall safety violations (from 10.6% to 6.1%). The gains on the Llama-3-8B model are even more substantial: we observe safety improvements across all four benchmarks and a -72% relative reduction in overall safety violations (from 5.3% to 1.5%). Crucially, both Llama and Gemma models trained with backtracking do not degrade model utility, as demonstrated by similar levels of performance on MT-Bench. We also observe that the models virtually never backtrack during non-safety evaluations. We report qualitative examples of backtracking in Appendix B.1.\nAn interesting observation is that backtracking DPO is very effective in improving model safety, while backtracking SFT is only marginally effective. The reason seems to lie in how often these models actually backtrack across the safety evaluation sets: both models backtrack rarely after SFT training (2.4% for Gemma, 0.2% for Llama), but much more frequently after DPO training (60.6% for Gemma, 49.9% for Llama)."}, {"title": "Trading off efficiency for safety.", "content": "The high backtracking frequencies after DPO training indicate that there are false positives: sometimes the models backtrack after a safe partial generation is produced. This could potentially lead to users experiencing higher latency, since the first effective token would come later in the generation process. In addition, all tokens generated before [RESET] are essentially wasted, and so the system overall will provide lower effective throughput.\nWhile it is arguable that any reasonable amount of reduction in efficiency is acceptable for building a safer language model, we nevertheless want to understand the extent to which backtracking reduces generation efficiency. By applying a logit bias to the [RESET] token at inference time, we can explicitly tune the likelihood of backtracking, which also allows us to quantify this tradeoff between generation safety and efficiency. Specifically, we apply a logit bias in {0, -5, -10, -15, -20} to the [RESET] token. We run inference on the safety evaluation set and compute relevant safety and efficiency metrics using VLLM (Kwon et al., 2023) to simulate a production environment on a single H100 GPU. Results in Figure 3 confirm that more backtracking does improve overall model safety: safety of the backtracking model decreases consistently with more negative logit bias. With a logit bias of -20, the backtracking model becomes 2X as unsafe as the same model without logit bias applied.\nBacktracking (without logit bias) does lead to an increased system latency perceived by the user: it takes an additional second on average for the user to observe the first effective token. The impact on throughput is more mild: the effective throughput decreases by approximately -12%. By applying a small logit bias (e.g., -5), these decreases in generation efficiency can be mostly mitigated, with the model maintaining a similar level of safety. We note that these results are on a safety evaluation set where the model backtracks frequently. For the average, non-safety-related use case, backtracking"}, {"title": "Better safety under sampling.", "content": "In practice, model providers could employ a best-of-k sampling strategy to improve model safety at the cost of efficiency: they could simply resample from the model if the first generation is unsafe (assuming access to a safety classifier such as Llama Guard 2), with up to k tries total. On the other hand, a malicious user could adopt a worst-of-k strategy, namely, resampling from the model up to k times, hoping to get at least one unsafe response. In Figure 4, we explore model safety under best-of-k and worst-of-k sampling, with and without backtracking.\nOne intriguing observation is that sampling just once under unit temperature (as opposed to greedy decoding, used in the earlier experiments) compromises safety. While baseline models become much more unsafe (Gemma: 10.8% \u2192 25.2%, Llama: 5.2% \u2192 14.4%), backtracking model be-come only marginally less safe (Gemma: 6.1% \u2192 7.6%, Llama: 1.5% \u2192 2.8%). If we allow the baseline models to generate twice, and pick the safer generation out of two (which makes the base-line model much less efficient than backtracking), the responses are still less safe than sampling from the backtracking model only once.\nUnder simple worst-of-k sampling, the baseline models become dramatically unsafe, and backtrack-ing models are asymptotically safer than the baselines: unsafe rates grow much slower as a function of k. In fact, if we sample 10 times for every prompt, the baseline Gemma model produces at least one unsafe generation for >80% of the prompts, but backtracking reduces this number to 39%. Similarly for Llama, backtracking decreases the worst-of-10 unsafe rate to 16% from 66% for the baseline model. While these gains are encouraging, the overall safety levels under sampling are still far from user expectations of safety. In light of Huang et al. (2023), which shows that strategically choosing the decoding algorithm can circumvent the alignment of many large language models, fu-ture work should investigate methods of safeguarding language model generation regardless of the sampling algorithm."}, {"title": "5 SAFETY AGAINST ADVERSARIAL ATTACKS", "content": "Our results demonstrate that backtracking improves model safety in the \"standard\" case, but what about in the adversarial case, where an attacker tries to actively overcome the safety alignment of the models? In this section, we evaluate the robustness of backtracking models against 4 jailbreaking attacks, including an adaptive attack designed specifically to prevent backtracking."}, {"title": "5.1 EVALUATION", "content": "Threat model. For these jailbreaking attacks, we assume the extreme case that the adversary has white-box access to a language model (i.e., access to model architecture and weights) behind a generation API, including knowledge of the backtracking method and the [RESET] token. We assume that [RESET] is treated as a privileged token (similar to BOS/EOS tokens), so that the attacker cannot use [RESET] in the prompt or modify its probability by applying logit bias. Given"}, {"title": "Algorithm 1", "content": "The adaptive attack algorithm.\nInput: language model M, prompt p, (partial) target behavior t, initial adversarial suffix ao\nOutput: optimized adversarial suffix a\n1: procedure ADAPTIVEGCG\n 2:  \u03b1 \u2190 ao\n 3:  repeat\n 4:   a \u2190 GCG(M, p, t,a)\n  Optimize Ladaptive with GCG\n 5:   s\u2190 M(pa)\n  Sample from model using the prompt and adversarial suffix\n 6:   if s starts with target t then\n 7:   Attack is partially successful when model produces target behavior but backtracks\n 8:   if s contains [RESET] then\n 9:   t  prefix of s before [RESET]\n 10:   else\n 11:   Attack is successful when generation completes without reset\n 12:   break\n 13:   end if\n 14:   end if\n 15:  until time out\n16: end procedure"}, {"title": "If the generation is unsafe.", "content": "In other words, the attacker has to prevent [RESET] from being generated at every token, assuming that the generation after backtracking is always safe.\nOur objective is to find an adversarial suffix a such that, when concatenated to a prompt p, simulta-neously elicits an unsafe response and prevents backtracking:\n$L_{adaptive} (p, t, a) = \u2013 log (t | p + a) + \\sum_{i} (5+log([reset]|p\\oplus a \\oplus t<i))^+$. (2)"}, {"title": "5.2 RESULTS", "content": "In Table 2, we report attack success rates of adversarial attacks on models after baseline and back-tracking DPO training. We report effectiveness of the adaptive attack on baseline models as well, which is equivalent to GCG with additional search budget.\nNotably, the baseline models are very vulnerable to jailbreaking attacks, and all attacks are success-ful >50% of the time. This result seems to suggest that standard post-training on safety data is likely"}, {"title": "6 CONCLUSION", "content": "In this paper, we propose backtracking, a technique that allows language models to recover from unsafe generations by discarding the unsafe response and generating anew. Over the same datasets and base models, we demonstrate that backtracking training leads to substantial gains over standard safety tuning, without degradation in model helpfulness. Evaluations against strong jailbreaking attacks, including an adaptive attack targeting backtracking, shows that the method provides addi-tional protection against jailbreaking despite not being trained to do so. However, we note that, even with backtracking, all models tested are far from providing adequate adversarial robustness against jailbreaking. Viewing backtracking (specifically, the production of [RESET]) as a classi-fication problem, a natural extension of our work would be to apply time-proven techniques such as adversarial training (Madry et al., 2018) as well as representation steering (Zou et al., 2024) to improve the robustness of backtracking.\nThe backtracking technique can, in principle, be applied to generation refinement beyond correcting unsafe generations. However, backtracking relies on recognizing a \"bad\" generation: unsafe gener-ations are often salient enough to identify post-hoc, while recognizing other generation errors (e.g., hallucinations, or reasoning errors) could be more challenging (Agrawal et al., 2024; Tyen et al., 2024). Future work should explore the limit of backtracking as a technique for improving language model generation in contexts beyond safety."}, {"title": "B.1 MODEL GENERATIONS", "content": "In Table 7, we show three prompts and generations by the Llama-3-8B model that are representative of different kinds of backtracking behaviors we observe at inference time. In the first example, the model correctly backtracks from an unsafe generation. In the second example, the model backtracks from an (already) safe generation and produces another safe generation. In the third example, the model does not backtrack from an unsafe generation."}, {"title": "B.2 GENERATION EFFICIENCY ON A HELPFULNESS DATASET", "content": "Backtracking has virtually no impact on generation throughput, and a small impact on latency that can be mitigated with logit bias. Figure 5 reports generation efficiency metrics on the validation set of OpenAssistant."}, {"title": "B.3 ROBUSTNESS OF SFT MODELS TO ADVERSARIAL ATTACKS", "content": "In Table 8, we report robustness of the baseline and backtracking models after SFT to adversarial attacks. Backtracking SFT does effectively reduce success rates of the prefilling attack, but is mostly ineffective against optimization-based attacks: reset rates are less than 10%, and safety levels are similar to baseline models. We speculate the reason is that optimization-based attacks create attack prompts that are out-of-distribution for the language model which interfere with the model's ability to backtrack precisely. Future work should explore how to make language models robust to unknown styles of prompts."}]}