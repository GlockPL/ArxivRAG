{"title": "NDP: Next Distribution Prediction as a More Broad Target", "authors": ["Junhao Ruan", "Abudukeyumu Abudula", "Xinyu Liu", "Bei Li", "Yinqiao Li", "Chenglong Wang", "Yuchun Fan", "Yuan Ge", "Tong Xiao", "Jingbo Zhu"], "abstract": "Large language models (LLMs) trained on next-token prediction (NTP) paradigm have demonstrated powerful capabilities. However, the existing NTP paradigm contains several limitations, particularly related to planned task complications and error propagation during inference. In our work, we extend the critique of NTP, highlighting its limitation also due to training with a narrow objective: the prediction of a suboptimal one-hot distribution. To support this critique, we conducted a pre-experiment treating the output distribution from powerful LLMs as efficient world data compression. By evaluating the similarity between the n-gram distribution and the one-hot distribution with LLMs, we observed that the n-gram distributions align more closely with the output distribution of LLMs. Based on this insight, we introduce Next Distribution Prediction (NDP), which uses n-gram distributions to replace the one-hot targets, enhancing learning without extra online training time. We conducted experiments across translation, general task, language transfer, and medical domain adaptation. Compared to NTP, NDP can achieve up to +2.97 COMET improvement in translation tasks, +0.61 average improvement in general tasks, and incredible +10.75 average improvement in the medical domain. This demonstrates the concrete benefits of addressing the target narrowing problem, pointing to a new direction for future work on improving NTP.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) are predominantly trained using the next-token prediction (NTP) paradigm. However, this approach has been subject to criticism, primarily focusing on two key issues: (1) the inability to perform tasks requiring advanced planning, such as look-ahead tasks (Kambhampati et al. 2024b; Bachmann and Nagarajan 2024), and (2) error propagation during inference. These critiques have prompted various improvements, including methods to incorporate planning for future tokens during training or inference (Kambhampati et al. 2024a; Monea, Joulin, and Grave 2023; Chen et al. 2023; Gloeckle et al. 2024; Cai et al. 2024).\nWe posit that the NTP paradigm not only suffers from short-term thinking in the temporal dimension but also from a narrow candidate issue. During training, the model treats the successor token for a specific prefix as the sole correct target, attempting to approximate a one-hot distribution. This approach contrasts with human cognition, where multiple potential successor words are considered. Consequently, we argue that the probability distribution of successors should be non-one-hot.\nDrawing inspiration from Huh et al. (2024), who proposed that a model's ultimate representation should be a statistical model of underlying reality, we consider that the ideal target for model learning is the statistical distribution over a comprehensive world data. We suggest that an n-gram statistical language model trained on such world data could provide this distribution efficiently. Furthermore, we hypothesize that even when trained on the same dataset as NTP use, n-gram LM can generate more efficient distributions than their NTP counterparts. To validate our hypothesis, we use LLM distributions as a proxy for the ideal statistical distribution of the world data, since LLM can be seen as a efficient compression of world data (Deletang et al. 2024). By comparing the similarities between n-gram distribution and one-hot distribution with LLM distribution on the same specific datasets, we demonstrate that n-gram distribution serves as a superior learning target since it aligns more with LLM distribution.\nWe introduce Next Distribution Prediction (NDP), an approach that draws inspiration from the n-gram LM concept to guide LLM training. This method calculates separate n-gram distributions for instruction and answer components, yielding supervised and causal language modeling (CLM) distributions, respectively. These distributions are then combined to replace the one-hot distribution during training.\nOur extensive experiments across various models, tasks, and evaluation metrics demonstrate significant performance improvements. Moreover, NDP enables the simultaneous use of supervised and unsupervised data for training, effectively allowing for continued pre-training during fine-tuning. This feature is particularly advantageous for domain adaptation and language transfer scenarios. NDP outperforms NTP, showing improvements of up to 2.97 COMET points in translation tasks, an average gain of 0.61 points in general tasks, and a remarkable average increase of 10.75 points in the medical domain.\nOur primary contributions are:\n\u2022 A novel critique of NTP, highlighting its limitations in both temporal and candidate space dimensions, provid-"}, {"title": "Related Work", "content": "Both Knowledge Distillation (KD) and NDP employ non-one-hot distributions as targets. These two approaches, however, can be considered as parallel dimensions of work. Fundamentally, the teacher models employed in neural network-based knowledge distillation methods must still be obtained through statistical NTP or NDP training paradigms. This inherent dependency means that neural network-based knowledge distillation cannot supersede NTP or NDP; rather, they are complementary. In practical applications, the typical LLM training process involves NTP-based pre-training and instruction fine-tuning, optionally followed by teacher network-based knowledge distillation. These are distinct processes that can be applied sequentially to enhance the student network's quality. From a training cost perspective, teacher network-based knowledge distillation introduces a teacher model with typically more than 10 times the parameters of the student model, significantly increasing both training time and hardware requirements.\nNDP and NTP can be conceptualized as forms of dataset distillation, differing in their level of granularity: token-level for NDP and sentence-level for NTP. This perspective illuminates NTP's limitations. Yuan et al. (2023) demonstrated that in knowledge distillation, student models more readily assimilate soft labels compared to one-hot labels. Wei et al. (2024) observed that the efficacy of sentence-level versus token-level distillation correlates with student model size, with larger models benefiting more from token-level approaches. Empirically, most research utilizing black-box Large Language Models (LLMs), such as instruction data synthesis (Xu et al., 2023), employs sentence-level distillation. While effective, sentence-level distillation alone has not enabled open-source LLMs to match the performance of GPT-4-turbo/GPT-4 (OpenAI, 2024). Conversely, Gemma2-9B (Gemma Team, 2024) achieved performance comparable to LLaMA3-8B (Dubey et al., 2024) with only 9T pre-training tokens, attributable to its use of token-level distillation. These findings support NDP's superior performance over NTP."}, {"title": "Calibration During Training", "content": "Our work shares similarities with output probability calibration methods, as both aim to mitigate overconfidence and align output probabilities with true probabilities. Prominent calibration techniques during training include loss function modification (Ren et al., 2024; Li et al., 2020; Lin et al., 2018) and label smoothing (Liang et al., 2024; Wei et al., 2022; Malagutti et al., 2024).\nResearch on loss function modification often attributes the discrepancy between predicted and real-world probabilities to maximum likelihood estimation. This has led to efforts to replace cross-entropy (based on negative log-likelihood) with alternative loss functions, introducing significant computational overhead and sensitive parameters. In contrast, NDP can be easily integrated into existing training frameworks without incurring additional training costs, yielding substantial improvements.\nWhile NDP supports smoothing, its primary advantage stems from addressing the issue of narrow candidates rather than smoothing per se. NDP guarantees a non-one-hot distribution, allowing for multi-discrete value distributions rather than only continuous ones. Given the expanding vocabulary sizes in modern language models, the correct next token candidates cannot span the entire vocabulary range. For large language models requiring high-precision alignment, introducing noise across the entire vocabulary can result in downstream task performance inferior to that achieved with one-hot distributions from N\u039d\u03a4\u03a1."}, {"title": "Improvement on Next Token Prediction", "content": "Earlier criticisms of the NTP training paradigm were all focused on the time dimension, which led to many improvements. Monea, Joulin, and Grave (2023) was inspired by Speculative Sampling (Chen et al. 2023), using the LLMs itself as a draft model, thus allowing the LLMs to output multiple tokens at once during the inference stage, implicitly achieving long-term planning and alleviating the short-term issues to some extent. Gloeckle et al. (2024) achieved consistent improvements in efficiency and performance on code tasks by training shared model backbones and multiple independent output heads and adopting speculative decoding with Medusa-like tree attention (Cai et al. 2024) during inference, indicating that this training paradigm has advantages in large-scale models.\nThese works are completely orthogonal to our perspective. We primarily focus on the issues brought by narrow candidates, with the hope of jointly optimizing the NTP process."}, {"title": "Preliminary Experiments", "content": "We aim to define the strengths of learning targets by exploring the similarity between targets and world data distributions. We use the distribution of LLM to approximate the representation of real-world data distributions, as LLM can be seen as an efficient compression of world data (Deletang et al. 2024). The learning targets we explore are primarily the NTP distribution and the n-gram distribution, which is derived by statistically obtaining global information on specific datasets.\nWe explore the cosine similarity between the distributions generated by n-gram LM and LLM, as well as generated by NTP and LLM. We conduct experi-"}, {"title": "Next Distribution Prediction Paradigm", "content": "In this section, we will provide a detailed description of how our method, NDP, incorporates the aforementioned statistical n-gram concept into the actual model training process.\nAlmost all datasets can be categorized as either unsupervised or supervised datasets. Let's take supervised datasets as an example, since self-supervised datasets can be regarded as a special case of supervised datasets where the instruction/input is empty. Based on the training forms of pre-training and instruction fine-tuning, we aim to extract two distributions from the dataset: the Causal Language Modeling (CLM) distribution and the supervised distribution.\nThis process can be divided into three sub-processes: First, learn the n-gram table through statistical analysis of the dataset (Figure 2(a)). Second, convert value counter from the n-gram table to distribution (Figure 2(b)). And third, replace the training targets in the original dataset from one-hot distributions to non-one-hot distributions (Figure 2(c))."}, {"title": "Learning n-gram Tables", "content": "The specific process is illustrated in Figure 2(a). Given a sentence, we use all its n-grams as keys and the corresponding successor tokens as values to form several key-value pairs. Across the entire training set, the key-value pairs formed by different sentences are merged based on the identical key, and corresponding values will collectively form a frequency Counter. The statistical processes for the supervised table and the CLM table are independent. The supervised table gathers supervised information and starts counting n-gram from the question part, while the CLM table requires only unsupervised information, so it counts only from the answer part.\nIt is easy to observe that the one-hot distribution derived from NTP is actually a special form of supervised table. When none of the keys overlap in the table generated from the entire dataset, the distribution derived from the supervised table will be completely equivalent to NTP distribution. At the same time, considering that the key length in CLM is always shorter than in supervised, the corresponding values will be significantly more frequent after mixing different sentences compared to the supervised distribution. To mitigate this potential effect, where overly abundant language modeling information might dilute the information from supervised data, we empirically begin to compile the CLM table from 5-gram based on some simple experimental results. When performing n-gram statistics solely on an unsupervised dataset, we only compile the CLM table and start from 1-gram.\nWe employ two metrics: one is the proportion of elements required for the distribution to reach a probability p. For a very sparse one-hot distribution, its proportion will always be 1/|V|, where V is the vocabulary of the model. The other is kurtosis, which we demonstrate in Appendix A.1."}, {"title": "Converting Distributions from n-gram Tables", "content": "In this subsection, we will introduce how to complete Figure 2(b). Figure 2(b) shows the process of converting each element in the value counter into a distribution on the model vocabulary dimension. For each counter, We create a tensor with the dimensions of the vocabulary, extracting the indices and corresponding counts from the frequency counter and setting the tensor accordingly. Then, we convert this tensor into a probability distribution using softmax.\nIt is important to note that the vocabulary's dimension is typically much larger than the number of items in the Counter, for instance, 256k vs. 10. If we directly form a probability distribution on such a frequency vector, it would result in a uniform distribution, diluting the information derived from the dataset. We solve this problem by controlling the probability allocated to the zero regions of the tensor.\nWithout loss of generality, we rearrange a vocab tensor v = [a\u2081,a\u2082...a|V|] with |V| elements into two contiguous regions based on whether the elements are zero or non-zero. This rearrangement results in v' = [\u03b1\u2081...\u03b1\u2096, \u03b1\u2096\u208a\u2081...\u03b1|V|], where [a\u2081...a\u2096] represents the non-zero elements region, and [\u03b1\u2096\u208a\u2081...\u03b1|V|] represents the zero elements region. Therefore, the softmax process on v' can be described as Equation 1.\nSoftmax(v') = $\\frac{\\begin{array}{c}\\sum_{i=1}^{k} e^{a_i}\\end{array}}{\\begin{array}{c}\\sum_{j=1}^{k} e^{a_j} + \\sum_{i=k+1}^{VI} e^{a_i}\\end{array}}$\n(1)\nwhere the second item in Equation 1 is the probability value allocated to the entire zero elements region. To control its value and make it equal to our preset probability p, we introduce a temperature coefficient t, transforming it to solve Equation 2.\nf(t) = $\\frac{\\begin{array}{c}\\sum_{i=k+1}^{VI} e^{a_i/t}\\end{array}}{\\begin{array}{c}\\sum_{j=1}^{k} e^{a_j} \\end{array}}$ = p (2)\nObtaining an exact solution for Equation 2 is quite challenging; however, we can easily obtain its approximate solution through numerical computation methods. For instance, the root-finding methods provided in scipy\u00b3, or the simpler bisection method, can efficiently locate t within the [0, 100] interval, with the error easily controlled within 1e-6."}, {"title": "Replacing Origin One-hot Target", "content": "After properly handling the token-level distribution, we can simply traverse the original dataset to replace the training targets. In Figure 2(c), we provide an example with a sentence. First, we decompose a sentence into corresponding keys as in Figure 2(a). Then, we use the keys to look up the corresponding table and obtain the distribution that we transformed in Figure 2(b). Because the CLM table starts collecting statistics only after the first token, the first token itself can only yield a blank distribution. We\nFor the supervised and CLM distributions of the same token, we employ a simple linear weighted fusion. Fusion allows us to combine the strengths of both distributions and avoids the overfitting and extra training costs in separate training phases. We illustrate this process in Equation 3.\nDmix = \u03b1Dsupervised + (1 \u2212 \u03b1)DCLM (3)"}, {"title": "General Tasks for Large Language Models", "content": "In this subsection, we aim to explore the impact of using NDP for instruction fine-tuning (IFT) on the base model for general tasks. The specific experimental setup is as follows.\nModel & Baseline We conducted experiments on Gemma-2B (Team et al. 2024) and LLaMA3-8B (Dubey et al. 2024). We used LoRA (Hu et al. 2022) to train LLaMA3-8B. We mainly compare NDP with NTP, label smoothing and knowledge distillation (KD). In KD we choose Gemma2-27B as the teacher model to teach Gemma-2B. The comparative experiments between NDP and knowledge distillation(KD) are listed in Appendix A.2.\nDataset We selected a mixture of Alpaca-GPT4 (Peng et al. 2023), Math (Hendrycks et al. 2021b), and Code (Zheng et al. 2024) as the instruction fine-tuning (IFT) dataset. This combination is similar to the typical mix of general text, code, and math used in pretraining, with a total dataset size of 220K instances. The evaluation comprised 11 benchmarks that broadly cover the model's general reasoning, knowledge Q&A, mathematics, coding, fact, and instruction following capabilities. More detailed benchmark information can be found in Appendix A.2.\nEvaluation framework Our evaluation process primarily leveraged the Ilm-evaluation-harness framework (Gao et al. 2023), with the exception of coding tasks, for which we utilized the evaluation scripts from the OpenAI/HumanEval repository. The evaluation setting closely follow those outlined in the LLaMA3 evaluation protocol4.\nResults The experimental results are summarized in Table 1. From the result, we observe some phenomena:\n\u2022 Consistent Improvements of NDP. NDP outperforms NTP by +0.61 points on Gemma and by +0.47 points on LLaMA, respectively. These improvements validate the efficacy of the modifications introduced to address the inherent limitations of the NTP paradigm.\n\u2022 Failure of Label Smoothing. Contrary to expectations, the label smoothing technique did not yield performance-"}, {"title": "Translation Task for Encoder-Decoder Models", "content": "In this subsection, we aim to answer the following questions: (1) Does our method work effectively for models with smaller parameter sizes? (2) Can our method benefit specific downstream tasks? Although we have demonstrated that NDP can benefit general, broad tasks, further discussion on adaptation to specific task can still be argued.\nModel & Baseline The T5 model (Raffel et al. 2023) is an excellent choice because we will select a 400M decoder-only LLaMA in the latter experiment. Using T5 would allow us to observe the impact on encoder-decoder models as well. We selected three sizes of the T5 1.1 version models: small (77M), base (248M), and large (783M). Here we only compare with NTP, since in preliminary experiments, label smoothing has already shown a similar drop in performance as general tasks.\nDataset & Metric We selected 200k bilingual sentence pairs in the en-de direction from IWSLT17 (Cettolo et al. 2017) as the training set. Both IWSLT17 and WMT22 (Kocmi et al. 2022) were used as test sets, as IWSLT17 consists of TED talk utterance transcripts while WMT22 comprises news articles. We used WMT22 to observe the generalization performance on out-of-domain data. We use rule-based SacreBLEU (Post 2018) and neural network based COMET22 (Rei et al. 2022) as evaluation metrics.\nResults Translation result is shown in Table 2. Overall, NDP consistently outperformed NTP in both in-domain and out-of-domain performance except COMET on T5-small. This suggests that NDP also has considerable potential in small models and downstream-specific tasks."}, {"title": "Unifying Continue Pre-training and Fine-tuning", "content": "Post-training of large language models often includes continued pertaining (CPT), IFT, and RLHF. Here, we focus on CPT and IFT since they involve NTP. Post-training is usually a delicate and complex process because the goal is not only to adapt to a specific domain or align with humans but also to ensure that the knowledge learned during pre-training is minimally disrupted. The NDP offers an optional-"}, {"title": "The Convergence Endpoints of NDP and NTP", "content": "NDP demonstrates a notable advantage over NTP, however, the source of this superiority, whether from faster convergence or a superior convergence endpoint remains unclear. To investigate long-term convergence behavior, we extended training to 10,000 epochs.\nDrawing from scaling law principles (Kaplan et al. 2020), we use small-scale scenarios to infer large-scale behavior. We trained a randomly initialized 438M LLaMA-like model (Ren et al., 2024) on a custom dataset devoid of real-world semantics. This approach eliminates pre-training knowledge effects, allowing pure comparison of NDP and NTP methods. The dataset comprises target items, noise items sharing prefixes with targets, and unrelated items."}, {"title": "Conclusion", "content": "Our work offers a novel critical perspective on the NTP training paradigm, and this hypothesis was validated through preliminary similarity experiments. Based on addressing this issue, we proposed a new training paradigm inspired from n-gram LM called NDP, which achieved good gains in various tasks such as general capability baselines for LLMs, translation, language adaptation, and domain adaptation. Nevertheless, we believe that NDP is merely a simple solution to the narrow candidate problem, and there remains a broad so-"}]}