{"title": "FAITHUN: Toward Faithful Forgetting in Language Models by Investigating the Interconnectedness of Knowledge", "authors": ["Nakyeong Yang", "Minsung Kim", "Seunghyun Yoon", "Joongbo Shin", "Kyomin Jung"], "abstract": "Various studies have attempted to remove sensitive or private knowledge from a language model to prevent its unauthorized exposure. However, prior studies have overlooked the complex and interconnected nature of knowledge, where related knowledge must be carefully examined. Specifically, they have failed to evaluate whether an unlearning method faithfully erases interconnected knowledge that should be removed, retaining knowledge that appears relevant but exists in a completely different context. To resolve this problem, we first define a new concept called superficial unlearning, which refers to the phenomenon where an unlearning method either fails to erase the interconnected knowledge it should remove or unintentionally erases irrelevant knowledge. Based on the definition, we introduce a new benchmark, FAITHUN, to analyze and evaluate the faithfulness of unlearning in real-world knowledge QA settings. Furthermore, we propose a novel unlearning method, KLUE, which updates only knowledge-related neurons to achieve faithful unlearning. KLUE identifies knowledge neurons using an explainability method and updates only those neurons using selected unforgotten samples. Experimental results demonstrate that widely-used unlearning methods fail to ensure faithful unlearning, while our method shows significant effectiveness in real-world QA unlearning.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are trained on a vast corpus of text, enabling them to achieve outstanding performance across various tasks (Radford et al., 2019; Chowdhery et al., 2023; Gemma et al., 2024). However, LLMs may present privacy risks, as sensitive or private information could unintentionally be included in the large text corpus used for training. Therefore, prior studies have investigated unlearning undesirable knowledge in language models. To assess unlearning, most studies have examined whether a model successfully forgets the targeted knowledge while retaining irrelevant knowledge (Shi et al., 2024; Li et al., 2024a; Maini et al., 2024; Jin et al., 2024).\nHowever, they are limited since they have overlooked the complex and interconnected nature of knowledge, which requires careful investigation of related knowledge. Specifically, these studies have examined only the independent knowledge and failed to evaluate whether an unlearning method effectively erases interconnected knowledge that should be removed, while retaining knowledge that appears relevant but exists in a completely different context. Figure 1 presents an example of faithful unlearning in the real-world knowledge setting. Unlearning methods should also remove paraphrased and multi-hop questions, as they involve knowledge interconnected with the target question being unlearned. Conversely, unlearning methods should retain knowledge of other questions with the same answer as the target, if they actually contain different knowledge despite appearing relevant.\nTo address this gap, we first define superficial unlearning, which refers to the phenomenon where an unlearning method either fails to erase the interconnected knowledge it should remove or unintentionally erases irrelevant knowledge. Based on the definition, we introduce FAITHUN (A Faithful Unlearning Evaluation Benchmark for Real-world Knowledge Question Answering), a new benchmark to examine three types of unlearning challenges: generalization, multi-hop unlearning, and shortcut unlearning to investigate superficial unlearning. Generalization (Anil et al., 2022; Yang et al., 2024a; Albalak et al., 2024), the multi-hop reasoning (Zhong et al., 2023; Li et al., 2024b; Yang et al., 2024b), and shortcut learning (Du et al., 2023; Tang et al., 2023; Zhou et al., 2023) are crucial challenges in machine learning research. Since the unlearning process typically relies on fewer data instances than general training, these challenges can be further amplified. Therefore, we construct three types of new datasets\u2014paraphrased, multi-hop, and same-answer datasets to examine superficial unlearning. These datasets address generalization, multi-hop knowledge unlearning, and shortcut unlearning, respectively. We demonstrate that existing unlearning methods do not ensure faithful unlearning, which raises new research questions for knowledge unlearning.\nFurthermore, we propose a novel method, KLUE, which stands for Knowledge-Localized UnlEarning, to achieve faithful unlearning by precisely identifying and updating neurons related to the target knowledge. Specifically, we use attribution (Yang et al., 2023), an explainability method, to determine which neurons should be updated by quantifying how much information each neuron contributes to predicting the answer to a given question. However, the quantified score may include superficial knowledge that simply affects the target output's probability without considering contextual meaning. Therefore, we propose a robust knowledge regularization method that accurately quantifies each neuron's knowledge score, mitigating the superficial contribution of neurons. After identifying knowledge neurons, our method selectively unlearns the target knowledge while preserving other knowledge by updating only knowledge-related neurons with selected unforgotten samples. In our experiments, our method significantly outperforms the baselines in the FAITHUN setting, demonstrating that knowledge-localized unlearning effectively achieves faithful unlearning."}, {"title": "2 Unlearning in Large Language Models", "content": "Machine unlearning has been used as a solution to address privacy and copyright issues in the text generation process of language models. Notable examples include gradient ascent-based methods (Jang et al., 2023; Yao et al., 2023; Barbulescu and Triantafillou, 2024), preference optimization approaches (Rafailov et al., 2024; Zhang et al., 2024; Jin et al., 2024), and representation learning techniques (Li et al., 2024a; Yao et al., 2024).\nHowever, the effectiveness of these methods has not been clearly demonstrated, prompting prior studies to introduce benchmarks in the field of unlearning to assess them. Eldan and Russinovich (2023); Shi et al. (2024); Tian et al. (2024) have aimed to unlearn the knowledge of copyrighted texts (e.g., BBC News and Harry Potter book) in a language model. Li et al. (2024a) have introduced a benchmark dealing with hazardous knowledge in various professional domains (e.g., biosecurity and cybersecurity). Maini et al. (2024); Jin et al. (2024) have proposed benchmarks for unlearning various entities. Specifically, Maini et al. (2024) have created synthetic entity profiles and removed their knowledge from a language model. Jin et al. (2024) have tried to unlearn the knowledge about real-world entities and evaluated the knowledge memorization in various forms of assessment (e.g., cloze test and question answering). However, existing studies remain limited as they have only examined independent knowledge and overlooked the intricate nature of world knowledge. World knowledge is highly complex and interconnected, which means that unlearning the target knowledge requires examining related knowledge carefully. Our research focuses on this aspect, examining and facilitating faithful unlearning."}, {"title": "3 The FAITHUN Benchmark", "content": "The FAITHUN task evaluates unlearning algorithms under real-world knowledge QA settings. Formally, given a language model \\(P_{\\theta}(y|x) = \\prod_{t=1} P_{\\theta}(y_t|x, y_1, ..., y_{t-1})\\) with parameters \\(\\theta\\), an unlearning algorithm f updates \\(\\theta\\) to \\(\\theta'\\), erasing the target knowledge from \\(P_{\\theta}\\). FAITHUN includes various question-answer pairs (q, a) \\(\\in\\) C, where C is a question-answer pair set. Our task provides forget set \\(C_f\\), which contains target question-answer pairs to be forgotten, where \\(C_f \\subset C\\). FAITHUN also provides retain set \\(C_r \\subset C\\backslash C_f\\) and test set \\(C_t \\subset C\\backslash (C_f \\cup C_r)\\). \\(C_r\\) is used in the unlearning process as training samples to maintain the original knowledge of \\(P_{\\theta}\\), and \\(C_t\\) is used as unseen data to evaluate an unlearned model \\(P_{\\theta'}\\) to reveal whether the unlearned model maintains the original knowledge. Furthermore, FAITHUN provides other new types of test sets (i.e., paraphrased, multi-hop, and same-answer sets) to assess the faithfulness of unlearning methods. Before introducing the other datasets, we first define key aspects of our benchmark.\nA world knowledge graph K is a directed multi-graph where nodes are entities and edges are labeled with relations, i.e., elements of two sets \\(\\mathcal{E}\\) and \\(\\mathcal{R}\\), respectively. We define K as a collection of triples (s, r, o) \\(\\subseteq\\) \\(E \\times R \\times E\\), where s, r, o denote the subject, relation, and object, respectively (Ruffinelli et al., 2020; Loconte et al., 2024). We assume that a world knowledge question is mapped to triples of K; thus, we also define a knowledge mapping function, \\({\\tau}: Q \\rightarrow \\mathcal{P}(K)\\), where Q is a set of questions and \\(\\mathcal{P}(K)\\) represents the power set of K. For example, the knowledge of a multi-hop question, \\(q_i\\) = \"Which continent is Tom Cruise's country in?\", can be denoted as a set of triples like \\(k_i = {\\tau}(q_i)\\) = {(\"Tom Cruise\", \"country\", \"U.S.A\"), (\"U.S.A\", \"continent\", \"North America\")}.\nTo quantify memorization after unlearning, we define knowledge memorization of a language model following the general QA task, as follows:\nLet \\(P_{\\theta}\\) be a language model, and let a be the correct answer to the question q. Then, knowledge memorization \\(M_{\\theta}: Q \\times A \\rightarrow {0, 1}\\) is defined as\n\n\\(M_\\theta(q, a) = \\begin{cases} 1 & \\text{if } \\arg \\max_{a'\\in A} P_\\theta(a'|\\iota, q) = a \\\\ 0 & \\text{otherwise} \\end{cases}\n \\)\nwhere \\(\\iota\\) is an input prompt template for the language model \\(P_{\\theta}\\), and Q and A are question and answer sets, respectively. From the definition, \\(M_{\\theta}(q, a) = 1\\) indicates that the language model retains the knowledge of (q, a), while \\(M_{\\theta}(q, a) = 0\\) signifies that it does not.\nFurthermore, we define Superficial Unlearning using Knowledge Memorization as follows:\nLet g: \\(\\Theta \\rightarrow \\Theta\\) be an unlearning algorithm, and \\(\\tau\\) represent the knowledge mapping. Assume there is a forget set \\(C_f\\), where \\(M_{\\theta}(q, a) = 1\\) holds for all \\((q, a) \\in C_f\\), and that \\((q_j, a_j) \\notin C_f\\) with \\(M_{\\theta}(q_j, a_j) = 1\\). Furthermore, suppose we unlearn the knowledge of \\(C_f\\) using g from a language model \\(P_{\\theta}\\), and finally get an unlearned model \\(P_{\\theta'}\\). Then, g is called a superficial unlearning algorithm for \\(C_f\\) if\n\\(\n((\\mathcal{K}_f \\cap \\mathcal{K}_j \\neq \\emptyset) \\wedge M_{\\theta'}(q_j, a_j) = 1) \\\\ \\lor ((\\mathcal{K}_f \\cap \\mathcal{K}_j = \\emptyset) \\wedge M_{\\theta'}(q_j, a_j) = 0),\n\\)\nwhere \\(\\mathcal{K}_f = \\bigcup_{(q,a)\\in C_f}{\\tau}(q)\\) and \\(\\mathcal{K}_j = {\\tau}(q_j)\\).\nFor example, suppose that an unlearning algorithm g unlearns the knowledge of the question \\(q_i\\) = \"Which country is Tom Cruise from?\", but it does not unlearn the multi-hop question \\(q_j\\) = \"Which continent is Tom Cruise's country in?\". Then, the knowledge of two questions can be denoted as a set of knowledge triples like \\(k_i = {\\tau}(q_i)\\) = {(\"Tom Cruise\", \"country\", \"U.S.A\")} and \\(k_j = {\\tau}(q_j)\\) = {(\"Tom Cruise\", \"country\", \"U.S.A\"), (\"U.S.A\", \"continent\", \"North America\")}. In this case, g is called a superficial unlearning algorithm since \\(\\mathcal{K}_i \\cap \\mathcal{K}_j \\neq \\emptyset\\) and \\(M_{\\theta'}(q_j, a_j) = 1\\) is true; thus, the equation 2 is satisfied.\nBased on the definition of superficial unlearning, we construct three new types of datasets: paraphrased, multi-hop, and same-answer sets to investigate the phenomenon of superficial unlearning. The paraphrased set \\(C_p\\), multi-hop set \\(C_m\\), and same-answer set \\(C_s\\) is matched with each question-answer pair \\((q_i, a_i) \\in C\\). The paraphrased set includes the same context questions with varying textual forms to the matched target question; thus, we should unlearn \\(C_p\\) if a matched question-answer pair \\((q_i, a_i)\\) is included in the forget set \\(C_f\\). The multi-hop set includes multi-hop question-answer pairs interconnected with the target question. Therefore, we should also unlearn \\(C_m\\) if a mapped pair \\((q_i, a_i)\\) is included in \\(C_f\\). The same-answer set includes question-answer pairs where the questions are from different contexts but share the same answer as \\(a_i\\); thus, we should maintain the knowledge of the same-answer set, although a matched pair \\((q_i, a_i)\\) is included in \\(C_f\\)."}, {"title": "4 Method: KLUE", "content": "Unlearning methods should erase only the knowledge associated with the target knowledge while preserving all other knowledge. In this section, we describe the method, KLUE, that identifies neurons contextually related to the target knowledge and updates only them during the unlearning process."}, {"title": "4.1 Quantifying Knowledge Relevance", "content": "We utilize an attribution method (Shrikumar et al., 2016) to extract the importance of neurons for specific world knowledge from language models. It is usually used to derive the importance of the input features (i.e., pixel, token) for performing a specific task, but Yang et al. (2023) expands the attribution formula to the importance of intermediate neurons in language models. Formally, suppose we have \\(P_{\\theta}(y|x) = \\prod_{t=1} P_{\\theta}(y_t|x, y_1, ..., y_{t-1})\\) that represents a language model. The contribution of an i-th neuron to the representation h in a particular layer, in predicting an answer a given a question q using \\(P_{\\theta}\\), is defined as follows:\n\\[A^{\\theta}_{q, a}(h^i) = h^i \\times \\frac{\\partial P_{\\theta}(a|q)}{\\partial h} \\times \\frac{A^{\\theta}_{q, a}(h)}{\\sum A^{\\theta}_{q, a}(h')},\\]\nwhere \\(h^l\\) means l-th token representation of h, and \\(\\frac{\\partial P_{\\theta}(a|q)}{\\partial h}\\) is the gradient of \\(P_{\\theta}(a|q)\\) with respect to h. In this study, we use transformer variants for experiments; thus, activation scores and gradients of a specific layer are computed for each input token representation. Therefore, if an input text includes L tokens, we have L attribution scores for each neuron; thus, we aggregate attributions of tokens by using max aggregation to acquire a single neuron knowledge attribution \\(A^{\\theta}_{q, a}(h)\\)."}, {"title": "4.1.2 Superficial Knowledge Regularization", "content": "Equation 3 computes the knowledge relevance of each neuron for a specific (q, a) pair. However, this equation may include undesirable information that only serves to increase the likelihood of the answer a regardless of the given context. To eliminate undesirable information from the computed attribution, we construct synthetic mismatched QA pairs (q', a) \\(\\in C'\\), where the answers remain the same as the target answer a, while the questions are randomly sampled independently of the answer. Then, we compute the attribution score for each mismatched pair and average them. Since a question and an answer included in mismatched pairs are contextually irrelevant, the computed attribution corresponds to the degree that unconditionally increases the likelihood of the answer regardless of the context (superficial knowledge). Therefore, we can compute the final knowledge attribution, I, containing only contextual knowledge by excluding the information of the mismatched attribution from the basic knowledge attribution as follows:\n\\[S(q, a)(h) = \\sum_{(q', a) \\in C'} A(q', a)(h),\\]\n\\[I(q, a)(h) = A(q, a)(h) - \\alpha \\times \\frac{1}{N} \\times S(q, a)(h),\\]\nwhere \\(C'\\) is a set including mismatched question and answer pairs. N is the number of mismatched samples, and \\(\\alpha\\) is a hyper-parameter to determine the magnitude of knowledge exclusion. A means a negative value of A is converted to the zero value. Since the negative values of the attribution score are negative contributions to a specific knowledge, it is reasonable to eliminate that unnecessary information. We use \\(C_f\\) and \\(C_r\\) as a pool to sample mismatched questions. Notice that this regularization enhances the quantification of contextual knowledge; thus, it can improve multi-hop reasoning and mitigate shortcut unlearning."}, {"title": "4.2 Unforgotten Sample-localized Unlearning", "content": "If we repeatedly unlearn samples that have already been sufficiently unlearned, it leads to overfitting in language models. Therefore, we select only the samples that are not completely forgotten in the unlearning process to preserve the generalization performance. Specifically, in each epoch's unlearning process, we select and unlearn only questions that satisfy the knowledge memorization criteria (Described in Section 3.1)."}, {"title": "4.3 Knowledge Neuron-localized Unlearning", "content": "After selecting unforgotten samples, we localize and update only the knowledge neurons corresponding to those selected samples in the language model. Specifically, we first compute gradients of parameters for the selected unforgotten samples. Then, we quantify the knowledge relevance of each neuron by using the equations 3 and 4, and sort neurons of the whole target layers by the knowledge relevance scores; then, we select the top-n knowledge neurons. We finally mask gradients of the parameters for knowledge-irrelevant neurons to exclude them from the unlearning process. Suppose that a weight matrix W \\(\\in \\mathbb{R}^{d \\times k}\\) is a linear matrix multiplication parameter of a language model, and the gradient computed for the parameter is \\(\\nabla_W \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial W}\\). Then, the gradient of i-th neuron (i.e., column) of the weight matrix after masking is denoted as \\(\\nabla_{W_{\\iota}} \\mathcal{L} = \\gamma \\odot \\nabla_{W_{\\iota}} \\mathcal{L}\\), where \\(\\gamma \\in {\\mathbb{O}^d, \\mathbb{1}^d}\\) and \\(\\odot\\) means the Hadamard product. We also can mask bias terms similar to the weight matrix. Notice that this method is model-agnostic since all neural networks consist of linear transformation layers."}, {"title": "5 Experiments", "content": "We adopt the instruction-tuned Gemma-2 (Gemma et al., 2024) models (2B & 9B) and the Llama-3.2 (Dubey et al., 2024) model (3B) to evaluate unlearning methods since they are among the latest open-source language models showing excellent performance.\nWe sample 5% as the forget set and 10% as the retaining set from the Base QA dataset C since there are generally fewer samples to unlearn than to retain in real-world scenarios. More experiments on varying numbers of samples for the forget set are shown in Appendix B.5. We select 70% of C as the test set, guaranteeing it is completely separate from \\(C_f\\) and \\(C_r\\). For the MCQA evaluation (Section 3.4), we manually select the instruction and randomly sample two false answer options from possible answers for each relation r. The details of an example of the MCQA format and selecting false answer options are shown in Appendix B.1 and B.2, respectively. We also conduct experiments on various prompt templates, and the results are described in Appendix B.6.4.\nWhen unlearning is applied to a language model, there is often a trade-off between unlearning knowledge (i.e., UA, UA+, and \\(MA_f\\)) and retaining the model's overall knowledge (i.e., TA, SA, and \\(MA_t\\)). Therefore, choosing the optimal model in the unlearning process is challenging since unlearning and retention are both important. For a fair comparison, we early stop the training procedure when UA\u2264 0.33 is satisfied (random sampling from three options) to select the optimal model. More detailed experimental settings can be found in Appendix B.3."}, {"title": "5.2 Baselines", "content": "We adopt widely-used unlearning methods to evaluate the superficial unlearning: Gradient Ascent (GA), Gradient Ascent with a Retaining Loss (GAret), two Direct Preference Optimization variants (DPOmis and DPOrej), NPO (Zhang et al., 2024), and RMU (Li et al., 2024a). More details for the baselines are described in Appendix B.3. For KLUE, we select only 5% of neurons from Feed-forward networks for the knowledge neuron localization, and update them using general gradient ascent with retention loss. We also use \\(\\alpha\\) = 10 and N = 5 for the Superficial Knowledge Regularization term. The experiments analyzing various hyper-parameters are shown in Section 5.5 and Appendix B.6."}, {"title": "5.3 KLUE Mitigates Superficial Unlearning", "content": "We investigate superficial unlearning on all baselines with Gemma-2 (2B) in the FAITHUN setting, as shown in Table 2. First, the default Gemma-2 model can correctly answer most questions, validating that FAITHUN is well constructed. After the unlearning process, all baselines reach UA\u2264 0.33, which validates that all methods can unlearn target knowledge. However, they fail to reliably remove implicit and interconnected knowledge, suggesting that their unlearning process is superficial. However, our method mitigates superficial unlearning and achieves faithful unlearning compared to other baselines, without significantly damaging the other knowledge to maintain (i.e., TA, SA, and MA). These results demonstrate that our method accurately identifies neurons relevant to contextual knowledge and successfully erases this knowledge. In addition, experiments on Gemma-2 (9B) and Llama-3.2 (3B) reveal that our method outperforms baselines, with results presented in Appendix B.4. We also conduct ablation studies for KLUE and demonstrate the validity of our proposed methods, as detailed in Appendix B.7."}, {"title": "5.4 KLUE is Robust to Unlearning Trade-off.", "content": "We demonstrate how the unlearning process affects other knowledge by plotting all scores from the Gemma-2 (2B) unlearning process against UA. As the UA score represents the progress of unlearning target knowledge (decreasing with unlearning), we can observe each method's impact on other knowledge in Figure 2. All methods' impact on the paraphrased questions (UA+) shows a strong correlation with the UA score, suggesting that all methods pose robustness in dealing with different lexical forms (but hold the same meaning) of the questions. However, the baselines struggle to maintain other knowledge (TA and SA) and to forget interconnected knowledge (MA). In contrast, KLUE demonstrates robust unlearning performance by effectively forgetting interconnected knowledge and preserving other knowledge."}, {"title": "5.5 The Impact of Neuron Localization", "content": "We adopt varying ratios of neuron selection p \\(\\in\\) {0.01, 0.05, 0.1} to investigate the effect of the knowledge neuron on Gemma-2 (2B). Also, we conduct experiments for the random neuron selection (i.e., p \\(\\in\\) {0.01, 0.05}). As a result, we reveal that a neuron ratio of 0.05 or 0.1 contributes to achieving faithful unlearning, showing that random neuron selection more significantly triggers superficial unlearning."}, {"title": "5.6 Qualitative Analysis", "content": "We conduct a qualitative analysis for KLUE and GAret on Gemma-2 (2B). Both KLUE and GAret successfully unlearn the paraphrased question (Cp), degrading label logits to 0.33. However, GAret has difficulty in unlearning the multi-hop question (Cm), while mistakenly unlearns the same-answer questions (Cs). On the other hand, KLUE faithfully unlearns them, mitigating superficial unlearning."}, {"title": "6 Conclusion", "content": "Our research identifies the limitation of existing unlearning benchmarks, which have not explored the interconnectedness of knowledge. To overcome this issue, we define superficial unlearning and propose a new benchmark, FAITHUN, for evaluating generalization, multi-hop knowledge unlearning, and shortcut unlearning. Using this benchmark, we empirically demonstrate that existing unlearning methods are vulnerable to superficial unlearning. Furthermore, we propose a novel knowledge-localized unlearning method, KLUE, and demonstrate that it outperforms existing unlearning methods, effectively mitigating superficial unlearning. Our paper first illuminates the phenomenon of superficial unlearning and raises a new research question for a deeper analysis of the unlearning field."}, {"title": "Limitations", "content": "FAITHUN is constructed based on Wikidata and is designed to investigate the unlearning of knowledge about famous people for application in various language models. Although knowledge is more interconnected for well-known individuals, our benchmark does not examine a broader range of people. Additionally, our study focuses solely on erasing the target label, leaving the issue of hallucinations in the unlearning process as future work, in line with prior studies."}, {"title": "Ethical Considerations", "content": "Our benchmark includes the private information of famous people, retrieved from Wikidata. Although the information of famous people is prevalent on the World Wide Web, the misuse of these data may raise ethical concerns regarding privacy."}]}