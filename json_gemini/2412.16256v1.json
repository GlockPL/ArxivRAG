{"title": "Aria-UI: Visual Grounding for GUI Instructions", "authors": ["Yuhao Yang", "Yue Wang", "Dongxu Li", "Ziyang Luo", "Bei Chen", "Chao Huang", "Junnan Li"], "abstract": "Digital agents for automating tasks across different platforms by directly manipulating the GUIs are increasingly important. For these agents, grounding from language instructions to target elements remains a significant challenge due to reliance on HTML or AXTree inputs. In this paper, we introduce Aria-UI, a large multimodal model specifically designed for GUI grounding. Aria-UI adopts a pure-vision approach, eschewing reliance on auxiliary inputs. To adapt to heterogeneous planning instructions, we propose a scalable data pipeline that synthesizes diverse and high-quality instruction samples for grounding. To handle dynamic contexts in task performing, Aria-UI incorporates textual and text-image interleaved action histories, enabling robust context-aware reasoning for grounding. Aria-UI sets new state-of-the-art results across offline and online agent benchmarks, outperforming both vision-only and AXTree-reliant baselines. We release all training data and model checkpoints to foster further research at https://ariaui.github.io.", "sections": [{"title": "1 Introduction", "content": "The rapid expansion of graphical user interfaces (GUIs) across web, desktop and mobile platforms has made them indispensable for digital interactions. From completing daily tasks like shopping or booking tickets to complex professional workflows, GUI agents play a critical role in automating these processes. As illustrated in Figure 2, a typical GUI agent operates in two stages: planning and grounding. In the planning stage, the agent generates action decisions to accomplish the user's task based on the current screen state as its observation. In the grounding stage, the agent is tasked with locating and interacting with the target element as referred in the instructions provided by planning, thus make actions truly happen in the environment."}, {"title": "2 Method", "content": "Aria-UI is designed to seamlessly integrate into the latest general-purpose multimodal GUI agent framework (Zheng et al., 2024; Xie et al., 2024; Koh et al., 2024; Rawles et al., 2024a), serving as a robust grounding model. We outline a solution to the challenges from a scalable, data-centric approach, as shown in Figure 3. In Section 2.1.1, we detail the synthesizing of diverse grounding data. Section 2.1.2 discusses building grounding samples with task context for dynamic scenarios, and Section 2.2 explains Aria-UI 's training details."}, {"title": "2.1 Large-scale Diverse GUI Data Synthesizing", "content": "As summarized in Table 1, several existing methods have collected diverse corpus for GUI grounding. However, these corpora fail to effectively address GUI grounding for LMMs. They are either not open-source, too small, or lack coverage of all the major platforms. Moreover, they rely on rigid instruction sources and formats, from HTML extraction or specifically formatted referring caption. Additionally, they overlook the importance of the contextual information for grounding during dynamic task performing. We present how to solve these challenges by a data-centric approach with diverse data scaling from multiple platforms and context-aware data extension with trajectories."}, {"title": "2.1.1 Diverse Data Scaling from Multiple Platforms", "content": "We propose a two-stage pipeline to transform raw samples into high-quality and diverse element instructions for grounding training. At the first stage, we utilize a strong LMM (GPT-4o or Qwen2-VL-72B (Wang et al., 2024a)) that takes element screenshots and text extracted from HTML as input for accurate and detailed element descriptions. To enhance accuracy and reduce hallucination, the model perceives two screenshots: (1) an isolated image of the element and (2) a zoomed-in view, where the element is highlighted with a red bounding box. Additionally, the HTML text and the screen position of the element are provided for reference. The model is then prompted to generate a detailed"}, {"title": "2.1.2 Context-aware Data Extension from Trajectories", "content": "Accurately and efficiently performing grounding tasks within the dynamic context of real-world environments is a crucial capability for GUI agents. Despite its importance, existing approaches largely focus on grounding tasks under a single-step setting, where LMMs are trained to infer grounding results based only on the current state and instruction. Such approaches overlook the dynamic nature of GUI grounding and the critical role of context in real-world scenarios. For example, after executing a TYPE action, the next grounding step is likely associated with an ENTER or SUBMIT button. Similarly, in multi-step tasks that involve navigating through a multi-layered menu to locate a target entry, there is a strong contextual relationship between consecutive grounding actions. Leveraging such contextual information enriches the grounding context and aids the model in avoiding bias, thereby enhancing grounding performance.\nWe utilize publicly available agent trajectories to simulate grounding tasks with contexts. We focus on constructing two types of contextual setups: (1) textual action history and (2) text-image-interleaved history. The text-based setup incorporates the ultimate task along with prior action histories, and the text-image-interleaved setup extends this by including N historical screen state images, providing richer contextual cues and training the model to understand multimodal interaction history. Notably, most trajectory data only includes basic sequential information, such as the click coordinates, thus lacks comprehensive stepwise instruction semantics. To address this, we augment all grounding steps within the trajectory data using the proposed data pipeline to generate detailed stepwise instructions. For non-grounding actions,"}, {"title": "2.2 Model Architecture", "content": "We build Aria-UI with the state-of-the-art multimodal MoE model, Aria (Li et al., 2024a). We leverage two strengths from Aria for GUI agents: 1) Aria is multimodal-native, built for better understanding of complex and interleaved contexts; 2) with only 3.9B activated parameters, Aria shows even faster inference speed than 7B dense models."}, {"title": "2.2.1 Ultra Resolution Support", "content": "With the shift from 1080p to 2K resolutions on computers and mobile devices, training grounding LMMs at high resolutions has become essential. Aria originally supports high-resolution images up to 980x980, which we extend to a maximum of 3920\u00d72940 on Aria-UI by splitting the image into smaller blocks, significantly increasing the range of image sizes to handle. To maintain positional accuracy, we take inspiration from NaViT (Dehghani et al., 2024) to place padding before resizing for keeping the original screenshot ratio."}, {"title": "2.3 Training and Inference Paradigm", "content": "We train Aria-UI following a two-phase procedure. We first leverage all the single-step grounding data to train the foundation GUI grounding capability of Aria-UI. Specifically, Aria-UI is tasked with generating grounding answers given the prompt \"Given a GUI image, what are the relative (0-1000) pixel point coordinates for the element corresponding to the following instruction or description: [...]\". We follow (Gou et al., 2024) to group all the samples for the same GUI image into a multi-turn conversation format. Then, context-aware data with both text-based and text-and-image-interleaved history settings are fed into the model to further enhance the grounding capability under the dynamic setting. For this phase, we add extra 20% samples from the single-step data to keep the generic grounding capability and avoid over-fitting.\nDuring inference, Aria-UI outputs the grounded pixels coordinates normalized to [0, 1000]. Since"}, {"title": "3 Experiments", "content": "We testify the performances of Aria-UI via extensive experiments including single-step grounding, grounding under offline agent trajectories and grounding in dynamic online agent environments."}, {"title": "3.1 GUI Grounding Evaluation", "content": "We first examine Aria-UI's foundational GUI grounding capabilities on ScreenSpot (Cheng et al., 2024). The benchmark compasses six subsets spanning over two types of elements and three major platforms. Each test entry provides a unique GUI image and a human-annotated instruction for locating a specific element. The typical resolution for mobile and web subsets is 2k, and for desktop samples it is 540p. We include the state-of-the-art UGround (Gou et al., 2024), with previous grounding models SeeClick (Cheng et al., 2024) and CogAgent (Hong et al., 2024) as baselines. We also include generic LMMs \u2013 GPT-4, GPT-4o and Qwen2-VL (Wang et al., 2024a).\nFrom the results in Table 2, Aria-UI achieves the highest average accuracy (82.4%) across all subsets, demonstrating its superior grounding performance. Aria-UI achieves a significant margin over the state-of-the-art UGround, particularly excelling in tasks for textual elements. The results showcase Aria-UI's robustness and generalizability across diverse platforms and element types."}, {"title": "3.2 Offline Agent Evaluation", "content": "Mobile Agents. We further testify how Aria-UI performs under an offline dynamic setting, where the model is required to provide grounding coordinates in agent task trajectories. We employ AndroidControl-Low (Li et al., 2024b), GUI-Odyssey (Lu et al., 2024) and AndroidControl-High, the first two has human-annotated or generated stepwise instruction, while the last one only provides the user task, and needs an additional planner for stepwise instructions. We follow (Li et al., 2024b; Gou et al., 2024) to utilize GPT-4o as the planner. We report element accuracy and the task success rate in Table 3. Specifically, we evaluate Aria-UI and the baselines on both zero-shot and training split-included settings. As we"}, {"title": "3.3 Online Agent Evaluation", "content": "Mobile and Web. We use AndroidWorld (Rawles et al., 2024a) for online mobile agent evaluation in an Android emulator environment. The evaluation is fully based on success of the task by checking the system state of the virtual device. We also include the MobileMiniWob++ task collection provided by AndroidWorld, which adpats the Web agent environment MiniWob++ (Liu et al., 2018) to AndroidEnv (Toyama et al., 2021), the same environment as AndroidWorld. We evalute Aria-UI with the strongest baseline, UGround under the same M3A agent framework, compared with SoM and Choice methods that require AXTree input. We report task success rate, the most important metric for real agents in Table 5. Our observations are:\n\u2022 In AndroidWorld, our approach achieves the best performance to date, with a task success rate of 44.8%, achieved by Aria-UITH. This surpasses the previous state-of-the-art method, UGround, as well as non-pure vision methods such as SoM and Choice, which rely heavily on AXTree input. The results highlight Aria-UI's superior ability to handle diverse element instructions in real-world settings, demonstrating its robustness and adaptability for pure-vision GUI agents.\n\u2022 On MobileMiniWob++, Aria-UI outperforms UGround, and choice-based methods. Due to\nthe simplicity of MiniWob++ layouts, GPT-4-Turbo with SoM achieves the highest performance. However, Aria-UI still demonstrates the highest scores with pure-vision input.\nOSWorld. We further evaluate Aria-UI on the most up-to-date and complex computer use simulator benchmark, OSWorld (Xie et al., 2024). Following the pure-vision agent framework in OS-World, we place Aria-UI as the grounding model to work collaboratively with GPT-4o on the 369 real tasks provided. We compared Aria-UI with previous SOTA methods and summarize the task success rate in Table 6. With GPT-4o as planner and Aria-UITH as the grounding model, we achieve the highest average task success rate of 15.15%, outperforming previous methods across all computer-use scenarios in OSWorld. Notably, it excels in tasks like VLC (30.06%), Chrome (23.80%), and Impress (15.32%), highlighting Aria-UI's strong performance in diverse, complex GUI tasks."}, {"title": "3.4 Ablation Study", "content": "We further testify how Aria-UI performs with ablation settings of the proposed components through the following perspectives:\nModel Components.\n\u2022 (-) Ultra Resolution. We remove the ultra resolution support for Aria-UI."}, {"title": "5 Conclusion", "content": "In this paper, we introduced Aria-UI, a robust LMM for GUI grounding across diverse environments. We designed a two-stage data pipeline for high-quality and diverse GUI grounding data from multiple platforms. We further incorporated dynamic action history as effective cues for stronger grounding capabilities in real-world environments. As a scalable and data-centric method, Aria-UI outperforms existing methods on all evaluated benchmarks, with both offline and online agent tasks. The model demonstrates strong zero-shot generalization across platforms, establishing Aria-UI as a powerful solution for universal GUI grounding."}, {"title": "6 Limitations", "content": "While Aria-UI demonstrates strong performance in grounding target elements based on instructions provided by a planner, it currently lacks the ability to autonomously perform both planning and grounding for a given task. This reliance on the planner model introduces a dependency on the quality and effectiveness of the planner, which can affect overall performance for complex tasks. Additionally, Aria-UI 's training does not yet incorporate error correction for planner-generated instructions, limiting its ability to correct mistakes made by the planner during dynamic tasks. Future work will focus on enabling Aria-UI to perform integrated planning and grounding, as well as enhancing its ability to handle and correct planner errors in real-time."}]}