{"title": "NewsInterview: a Dataset and a Playground to Evaluate LLMs' Grounding Gap via Informational Interviews", "authors": ["Michael Lu", "Sriya Kalyan", "Hyundong Cho", "Weiyan Shi", "Jonathan May", "Alexander Spangher"], "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in generating coherent text but often struggle with grounding language and strategic dialogue. To address this gap, we focus on journalistic interviews, a domain rich in grounding communication and abundant in data. We curate a dataset of 40,000 two-person informational interviews from NPR and CNN, and reveal that LLMs are significantly less likely than human interviewers to use acknowledgements and to pivot to higher-level questions. Realizing that a fundamental deficit exists in multi-turn planning and strategic thinking, we develop a realistic simulated environment, incorporating source personas and persuasive elements, in order to facilitate the development of agents with longer-horizon rewards. Our experiments show that while source LLMs mimic human behavior in information sharing, interviewer LLMs struggle with recognizing when questions are answered and engaging persuasively, leading to suboptimal information extraction across model size and capability. These findings underscore the need for enhancing LLMs' strategic dialogue capabilities.", "sections": [{"title": "1 Introduction", "content": "Recent research has shown that LLMs struggle to engage in emotional (Shaikh et al., 2024) or strategic (Wongkamjan et al., 2024) dialogue. For example, Shaikh et al. (2024) examined LLM-generated responses to dialogues and found fewer occurrences of \u201cgrounding language\u201d (Clark, 1996; ?), like acknowledgements or affirmations, that humans typically use to foster comfort and trust. This can impede an LLM's ability to serve in a variety of situations: e.g. education (Kasneci et al., 2023), mental health (Carlbring et al., 2023) or conflict resolution (Argyle et al., 2023). However, prior efforts to ameliorate such gaps face limitations: existing large datasets (1k-10k transcripts) are generated via crowdsourcing and are inherently unnatural (Rashkin et al., 2019; Wang et al., 2019; Liu et al., 2021). More natural transcripts, of educational (Caines et al., 2020) or therapeutic environments (Gratch et al., 2014), are difficult to collect due to privacy concerns (Casey, 2004) and are small-scale (100-1k transcripts).\nIn this work, we directly address these limitations by focusing on an area where grounding communication is required but plentiful data exist: journalist interviews. Journalistic, or informational interviews, are typically conducted between an \"interviewer\u201d and a \u201csource\", and the goal is to obtain information. Sources are often anxious or unclear (Harcup, 2015), and human interviewers are constantly evaluating: (1) Are my questions getting fully addressed? (2) Do I need to more effectively engage or persuade a source (Sedorkin, 2015)? This makes news interviews an ideal setting to observe grounding, strategic, and persuasive dialogues.\nTo study how LLMs perform in journalistic contexts, we start by collecting interview transcripts from two major news sources: National Public Radio (NPR) and Cable News Network (CNN), filtering to over 40,000 dyadic informational interviews. Next, we show that LLMs in news interview settings suffer from the same lack of grounding as in other dialogue settings (Shaikh et al., 2024). We find that significant discourse differences exist in the kinds of questions asked by LLMs and human interviewers: for example, LLMs are 50% less likely to make acknowledgements, and 30% less likely to pivot back to higher-level questions"}, {"title": "2 Dataset Processing", "content": "In this section, we describe the collection and processing of our dataset."}, {"title": "2.1 Data Collection", "content": "We aggregate, clean and condense multiple publicly available datasets of interview transcripts from National Public Radio (NPR) and Cable News Network (CNN) in order to build a high-quality interview dataset of 45k source-to-interview transcripts. These transcripts are published records of live interviews conducted between a journalist and sources invited on the program. They provide a rich resource for analyzing natural language interactions in informational interviews."}, {"title": "2.2 Data Filtering for Interview Analysis", "content": "We want to focus on one-on-one informational interviews between a journalist and a single source."}, {"title": "3 Analysis", "content": "In this section, we analyze how humans conduct informational interviews and compare this behavior to that of pretrained LLMs. Our aim is to explore whether LLMs face similar grounding problems in interviews as observed in educational and therapeutic settings (Clark, 1996; ?)."}, {"title": "3.1 Generating Counterfactual Utterances", "content": "One way to assess how an LLM would behave in an interview setting offline is to perform a counterfactual simulation (?). Specifically, given a human interview consisting of $n$ interviewer-source conversational turns $(q_1, a_1)...(q_n, a_n)$, we feed $t < n$ turns into the LLM along with a prompt instructing the LLM to generate the next question. This generates a counterfactual, $g_t$ to what the human would have said, $h_t$. We experiment with different variations: (1) Baseline: The LLM is simply asked to produce the next question. (2) Chain-of-Thought: The LLM is instructed to reason about the information already provided in the interview, consider what might be left to ask, and then generate the next question. (3) Outline: In addition to a chain-of-thought prompt, the LLM is also provided with an outline of the interview goals (described in Section 4.2) to incorporate into its chain-of-thought reasoning."}, {"title": "3.2 Evaluating LLM Counterfactuals", "content": "To analyze how similar LLM questions, $g_t$ are to human questions, $h_t$, we perform two analyses:\nConsistency Analysis: We aim to assess how similar $g_t$ is to $h_t$ across different comparison categories Saha et al. (2024), specifically: Informational consistency (i.e. $g_t$ and $h_t$ seek similar informational objectives); Motivational, (i.e. similar outcomes); Style, (i.e. similar tone); Contextual consistency (i.e. similar appropriateness given the context); Discourse consistency (i.e. similar purposes in the overall conversation). Putting these together, we assess an Exact match. We ask an LLM, GPT-4, to perform this assessment and manually inspect its outputs and reasoning threads.\nDiscourse Analysis: We aim to assess whether $g_t$ plays a similar function as $h_t$ does. We develop a schema to describe the role of each question. This schema includes the following elements: Follow-up Question (e.g. \u201cCan you tell us more?\"), Outline-Level Question (e.g. \"Moving on, can we discuss the next event?\"), Acknowledgement Statement (e.g., \"I see, that sounds scary.\"), Opinion/Speculation (e.g. \u201cWhat do you think will happen?\"), Broadening Question (e.g. \u201cHow does this fit into the broader trend?\"), Verification Question (e.g. \" So to confirm...\") and Challenge Question (e.g. \u201cThese dates don't line up.\u201d). See Table 5 in the Appendix for definitions of each discourse role.\""}, {"title": "3.3 Findings", "content": "Our analysis yielded several key findings.\nInsight #1: Acknowledgement statements are virtually absent from all LLM variations. As shown in Figure 2, grounding gaps exist in journalistic interviewing similar to those observed by (Shaikh et al., 2024). While human journalistic interviewers tend to make Acknowledgement statements in about 9% of their utterances, all prompting variations that we experimented with madeclose to zero acknowledging statements. This lack of acknowledgement is paired with not mirroring the source's speaking style; human journalists, as shown in Appendix C.6, bring character and voice.\nInsight #2: LLMs do not engage in strategic multi-turn questioning Even in settings where LLMs are exposed to interview outlines, they are still undirected in their questions. As shown in Figure 2, LLMs are significantly more likely to ask follow-up questions than humans across all prompting variations. Introducing chain-of-thought and outline variations increases the rate at which the LLM asks outline-level questions. However, the rate remains significantly below human levels. Additionally, they are also more likely to ask either Opinion questions or Broadening questions. In fact, in Figure 1b, we observe that LLMs tend to ask increasing amounts of Opinion Questions and Broadening Questions over time, which humans do not. As can be seen in Table 8, these questions can be vague and open-ended and reflect a lack of planning. Together, these findings suggest an inability to direct an interview in a desired direction and engage in multi-turn planning.\nInsight #3: LLMs are capable of understanding context, but fail in all other categories of similarity to humans. Comparing the content and style of LLM interviews to human interviews in Table 1, we note that, overall, LLMs are broadly dissimilar to humans in style, motivation and information-seeking. One area where the LLMs succeed, relatively, is understanding the context of the interview beforehand. This is not a new observation \u2013 much recent work, e.g. in dialogue-tracking, has found LLMs to perform well (Ou et al., 2024). The fact that LLMs can preserve context over multiple turns and do not drift away from the topic indicates that models might one day be able to engage in multi-turn goal-oriented dialogue, given the right reward signals and learning environment.\nTaken together, these findings suggest that journalistic dialogue is suitable for studying effective communication patterns, and also highlight significant gaps in current language modeling objectives."}, {"title": "4 NewsInterview: An Interview Game", "content": "As shown, LLM counterfactual questions exhibit several shortcomings: they are less likely to acknowledge the interviewee and focus excessively on follow-up questions. However, questions remain: do they lack strategic multi-turn planning? In human dialogue, grounding exists ultimately strategic purposes (Cialdini, 2009). Our goal for the remainder of the paper is to set up a realistic simulated game-environment with a delayed reward signal to test this."}, {"title": "4.1 Game Design Overview", "content": "We first introduce our game on a high level, explain our game design, as illustrated in Figure 3, and then describe our implementation. We draw on two journalism textbooks: Interviewing: A Guide for Journalists and Writers, which explains how to conduct effective interviews and speak to reluctant, defensive, or poor-explaining sources (Sedorkin, 2015); and Journalism: Principles and Practice, which describes how to build source trust (Harcup, 2015).\nOur gameplay proceeds in a loop, shown in Algorithm 1. The \"player\" in our game plays the role of an interviewer and is able to ask questions to a source, based on the conversational history and the interview objectives (the $Interviewer()$ step). The source is given a set of informational items, and assesses whether any of these items are relevant to the question (the $getRelevantInfo()$ step); they then decide how persuaded or comfortable they are based on the conversational history (the $getPersuasionLevel()$ step); based on this, we determine the subset of relevant items they return (the $getItemsToReturn()$). They respond with these items. The reward, obtained at the end of the game, is that amount of information items the source disclosed."}, {"title": "4.2 Gameplay Design", "content": "We first start by describing our data processing, and then we will describe the functions introduced in Algorithm 1 in more detail.\nDataset Preparation for Simulation To prepare our dataset for use in the simulated game environment, we group together: (1) source responses and ask an LLM to summarize a set of specific informational items and (2) interviewer questions and ask an LLM to summarize them into a set of high-level objectives. The sources' informational items mimic the knowledge a source likely had goinginto the interview and the interviewer's objectives represent the agendas they had prior to the conversation. Both of these summaries are represented in Figure 3 as Given, and are designed to give the interviewer-LLM and the source-LLM a basis for communication. For further examples of both, seeTables 7 and 6 in the Appendix.\nSource Design: Personas and Persuasion Now, we introduce the design of the source. We focus attention on this construction to build a robust game environment that accurately mimics human interactions. To make gameplay varied and challenging, we draw from Sedorkin (2015) to design eight different personas: Anxious, Avoidant, Adversarial, Defensive, Straightforward, Poor Explainer, Dominating and Clueless. To see detailed descriptions of each persona, as well as example responses that the persona might give, see Table 3. Different personas give us the ability to study how interviewers perform in a wider array of scenarios and are designed to more carefully capture different challenges that journalists face.\nThe following three functions, in sequence, power our gameplay: $getRelevantInfoItems \\rightarrow getPersuasionLevel \\rightarrow getItemsToReturn$. The first, getRelevantInfoItems, takes the interviewer's question and determines which of the sources' information items are most relevant; it is simply a retrieval function that we implement using an LLM. getPersuasionLevel is a function that determines the selected source's level of comfort or persuasion (on a 5 point scale) in the current conversation. getItemsToReturn is a stochastic engine: it randomly selects, based on the persuasion level, the number of relevant information items to return: the more persuaded a source is, the more likely they are to return more information.\nThe persuadability component to our gameplay increases the multi-turn strategy: because persuasion is assessed with reference to the entire interview, the interviewer gets more reward overall for spending words (or even whole turns) making the source feel comfortable early in the interview, rather than directly asking for information. However, is it sound for the source-LLM to assess its own level of persuasion? As recent research has found, LLMs are poor detectors of when they are being persuaded (Sakurai and Miyao, 2024) and can even unknowingly persuade themselves (Zeng et al., 2024). Furthermore, persuadability varies from person to person (Wang et al., 2019; Hirsh et al., 2012). Luckily, source-persuasion is a well-studied field in journalism. As a starting point, we draw from Sedorkin (2015), and carefully design"}, {"title": "4.3 Game Simulation Results", "content": "We run our simulation for 200 interviews with four models as the interviewer: gpt-4, gpt-4-mini, Llama-3.1-70b and Llama-3.1-8b. For a source LLM, we used gpt-4 across all personas. Table 2 compares the performance of LLMs across three conditions: the full game, a version without persuasion, and a version where sources do not withhold information. In the full game, where sources' responsiveness depends on persuasion and persona, the gpt-4 model performs the best, at 50.4%. However, when persuasion is removed, performance only marginally improves across all models (e.g., Llama-3.1-70b reaches 45.5%, while gpt-4 remains stable at 49.8%), indicating that other aspects of the game (i.e. inferring which information the source has witheld) also pose a challenge. In the easiest condition, where no information withholding occurs, all models perform significantly better, with reward percentages reaching over 80%, showing that withholding strategies are a major obstacle for current LLMs.\nFigure 4a highlights the performance of gpt-4 across different source personas. The model achieves the highest information extraction from straightforward personas, while adversarial and defensive personas are the most challenging. Despite being harder to extract information from, adversarial sources are easier to persuade, as shown in Figure 4b.\nFigure 5a explores how the reward (information extraction) changes over the course of an interview. The results show a declining trend in reward per conversational turn. However, the total reward accumulated over time (Figure 5b) increasesalmost linearly, showing that the LLMs continue to extract information, albeit at a slower rate. Together, these findings highlight the limitations of current LLMs in engaging with persuasive and strategic multi-turn interviews. While larger models like gpt-40 outperform smaller ones, they still exhibit significant gaps in persuasion and adaptive questioning, particularly when dealing with difficult personas."}, {"title": "5 Discussion", "content": "Our results from Sections 3.3 and 4.3, taken together, show that news interview transcripts are a valuable and real-world dataset for learning about persuasion, grounding dialogue and multi-turn strategy. Our work builds off previous work identifying grounding gaps in language models (Shaikh et al., 2024) and an emerging thread in gameplay (Wongkamjan et al., 2024; Liu et al., 2024), by extending both directions into the news domain, where real-world data is plentiful.\nIn Section 3.3, we show that grounding persuasive dialogue exists in human interviewers, which LLMs fail to mimic. In Section 4.3 we show that LLMs struggle to extract information from sources embodying a variety of personas. Interestingly, particular personas, like adversarial or avoidant sources, are noticeably more difficult for the LLM extract information from. Figure 4b, for instance, might suggest that the LLM models are more responsive to engagement tactics when sources display hostility rather than indifference or avoidance. In the context of prior work (Liu et al., 2024; Chawla et al., 2021), which tends to examine all interactions from a singular persona, these distinctions can point to further investigations illuminating why certain personas are worse than others.\nThe ultimate solution for a propensity towards multi-turn strategies and grounding language in LLMs, we feel, is to incorporate longer-range reward signals into the training process (Li et al., 2016). After all, grounding exists for a purpose: to influence an outcome borne out over many conversational turns (Clark, 1996; Cialdini, 2009) (e.g. in therapeutic environments, the patient is more open and makes more progress (Bohart, 1999); in educational environments the student is more engaged and learns more (Brown, 1994)).\nOur NewsInterview effort provides a game-playing framework to provide such a reward and test such strategies in a real-world scenario. This game-playing environment is complicated: throughout an interview, the LLM has to reason about what information it already has, what likely information the source has, and how to persuade the source to divulge this information. Each of these components is likely challenging for present-day LLMs. Figure 5a suggests that LLMs struggle to maintain effective information extraction as the conversation progresses, possibly due to difficulties in adjusting their questioning strategies over time. Furthermore, we see only marginal gains with large modeling sizes, suggesting that scale alone may not make meaningful progress.\nDespite the considerable challenges this game imposes, NewsInterview is still simpler than other game-playing environments: agent-based strategy games typically require the agent to develop a strategy while other players are also developing strategies (Chawla et al., 2021; Liu et al., 2024; Wongkamjan et al., 2024). In NewsInterview, the source is not typically trying to persuade the journalist, simply to decide how much information to divulge. These components, taken together, suggest that NewsInterview is an ideal real-world starting place to start to incorporate multi-turn strategic dialogue from LLMs. We imagine that agents built off of this training environment will one day be able to: (1) identify persona characteristics easily in other game-players (2) develop strategy books, similar to those humans use (Sedorkin, 2015; Harcup, 2015) (3) reason about early-vs-late stage conversational tactics. These agents can not only help in practical tasks (e.g. by automating interviewing or by providing students an environment to learn in) but also give us more fundamental insights into the strategic approaches that work well.\nOur work has to be viewed with the following fundamental limitations. getPersuasionLevel serves as the core component in our game-design that affects the strategy an agent can learn and deploy; thus, any flaw in this function can lead us to a suboptimal agentic strategy. Despite observing high correlations between humans and LLM assessments of persuasion, there could be many common scenarios we do not observe. For instance, there may be pathological degenerate cases: if, for instance, getPersuasionLevel returns a persuasion level of 5 for agents that say \"thank you\", then agents will learn to say this at every turn, and will not develop any other strategy. We implemented our personas using prompts outlining what different personas would say and what they would be persuaded by, as described in Section 4.2, under the expectation that the source-LLM would be able to generalize. Table 2 does indicate that the interviewer-agent did not simply overfit to certain words. However, if, in future work, we observe collapsing interviewer-agent behavior especiallyin the presence of trained approaches, we must be prepared to do more work to validate or improve getPersuasionLevel.\nAdditionally, future work could also focus on introducing additional reward signals into the gameplay. Some pieces or information are more important than others. By analyzing how much time is spent discussing each of a source's pieces of information, we might reward the agent more for the more important extractions. Another reward signal could be the quotability of the response. Often, an interviewer will seek to obtain specific, memorable lines to write about throughout the interview. There is line of research that tries to predict which parts of a text or speech are the most quotable (Bendersky and Smith, 2012; Lee et al., 2016; MacLaughlin et al., 2018). We believe both of these signals might be an interesting lens through which to improve NewsInterview."}, {"title": "6 Related Work", "content": "Recent research on large language models (LLMs) has highlighted a lack of grounding language and strategic dialogue (Shaikh et al., 2024; Wongkamjan et al., 2024). Grounding communication, which involves the use of acknowledgments and affirmations to foster understanding and trust (Clark, 1996; ?), is essential in various conversational settings such as education (Kasneci et al., 2023), mental health (Carlbring et al., 2023), and conflict resolution (Argyle et al., 2023). Prior efforts to study and improve LLMs' grounding dialogue capabilities have faced limitations due to the scarcity of large-scale, naturalistic datasets (Kasneci et al., 2023). Existing datasets are either generated via crowdsourcing, which can result in unnatural dialogues (Rashkin et al., 2019; Wang et al., 2019; Liu et al., 2021), or are small-scale due to privacy concerns, as in educational (Caines et al., 2020) and therapeutic settings (Gratch et al., 2014; Casey, 2004).\nGrounding dialogue exists to foster longer-term goals. LLMs have been shown to lack multi-turn strategic thinking and planning, which are critical for effective dialogue (Chawla et al., 2023). While some studies have explored the use of game-playing environments to improve these aspects (Li et al., 2016; Wongkamjan et al., 2024), and the development of 'gamified' persona-based subjects has a long history in dialogue agent research (Colby, 1981), there is still a need for realistic datasets and simulations that can facilitate the development of agents with longer-horizon rewards.\nResearch has increasingly focused on using game-playing environments to train agents in strategic dialogue. Lewis et al. (2017) introduced a negotiation task where agents learn to negotiate over resources through dialogue, demonstrating the potential for agents to develop complex negotiation strategies. Similarly, He et al. (2018) proposed a decoupled policy optimization approach for strategic dialogues in games. More recent works have extended strategic dialogue to complex, multi-agent environments. Gray et al. (2020) developed agents capable of playing the game Diplomacy at a human level by engaging in strategic planning and negotiation with other agents. Perolat et al. (2022) introduced a model for playing Stratego, showcasing multi-agent reinforcement learning in adversarial settings. Chawla et al. (2021) presented CASINO, a contextualized dialogue dataset designed for building negotiation agents, emphasizing the importance of context and strategy in dialogue systems. These works highlight the effectiveness of simulated game environments in training agents for strategic multi-turn dialogues, underscoring the potential of such approaches in enhancing LLMs' strategic dialogue capabilities.\nDespite these advancements, several gaps remain. Most existing models focus on task-specific dialogues with singular personas and lack the flexibility to handle a variety of conversational contexts and topics. Another gap is the scarcity of large-scale, naturalistic datasets to create a broader variety of settings for these games, hampering the development of more generalized and robust models (Liu et al., 2021). Addressing these gaps is crucial for advancing the development of agents capable of engaging in sophisticated strategic dialogues akin to human interactions.\nOur work addresses these gaps by introducing a large-scale dataset of 40,000 two-person informational interviews from NPR and CNN, providing a rich resource for studying grounding communication in a naturalistic setting. Additionally, we develop a realistic simulated environment, NewsInterview, which incorporates source personas and persuasive elements to evaluate and improve LLMs' strategic dialogue capabilities."}, {"title": "7 Conclusion", "content": "In this paper, we have introduced a high-quality dataset of 40,000 two-person informational interviews from NPR and CNN, addressing the scarcity of large-scale dialogue data necessary for studying grounding communication. Our detailed discourse analysis reveals significant differences between LLM-generated dialogues and human interviewers, particularly in the use of grounding language and question types. Motivated by observation that longterm objectives guide turn-level grounding, we develop a realistic game environment, NewsInterview, to test and improve dialogue agents in informational interviews. Our experiments demonstrate that while source LLMs can mimic human behavior in information sharing, interviewer LLMs struggle with recognizing when questions are answered and engaging persuasively, leading to suboptimal information extraction. These findings underscore the need for enhancing LLMs' strategic dialogue capabilities, and we believe that our dataset and simulation environment provide valuable resources for future research in this area."}, {"title": "8 Limitations", "content": "8.1 Privacy and Ethical Considerations\nAll data used in this study are publicly available and do not contain personally identifiable information beyond what has been already made public by the news organizations. We adhere to ethical guidelines for data use and ensure that our processing respects the rights and privacy of individuals involved as well as the news organizations that collected this data. Since the dataset we create is derived from interviews that have already been published in academic settings, we believe we are infringing upon the copyright of the news organizations this data originally belonged it. Aside from ownership questions, there are still inherent risks in the use of real-world interview data. Some interviews might involve sensitive topics, and the ethical implications of using such data for model evaluation warrant careful consideration.\n8.2 Reproducibility\nAll experiments are conducted using publicly available models and datasets. Part of our simulation does rely on high-performing language models and to serve this we used gpt-40. This brings us into territory where we are inherently not reproducible, as closed models can be changed without notice. However, we believe we are not out of the norm in the academic community in our usage.\n8.3 Simulated Environment Limitations and Risks\nThe simulated game-playing environment used to evaluate the LLM agents is a simplification of real-world interviewing processes. We might be inducing a bias in agents that could perpetrate and ultimately lead development in the wrong direction. Or, we also might be opening up a sandbox for potential dual-use. The design of our game, to extract information from sources, might one day be used to persuade users to divulge sensitive information.\n8.4 Annotators\nWe worked with multiple professional journalists throughout the summer who were either colleagues or students who signed up to work with us. They were all volunteering their time and efforts to help with the research.\n8.5 Computational Resources\nAll experiments were run either with OpenAI resources (we spent a total of $300 running simulations) or open source Llama-3.1-70b models. These models were run on the university cluster, which consisted of either 4xA40s or 2xA100s NVIDIA GPUS."}]}