{"title": "On Pre-training of Multimodal Language Models Customized for Chart Understanding", "authors": ["Wan-Cyuan Fan", "Yen-Chun Chen", "Mengchen Liu", "Lu Yuan", "Leonid Sigal"], "abstract": "Recent studies customizing Multimodal Large Language Models (MLLMs) for domain-specific tasks have yielded promising results, especially in the field of scientific chart comprehension. These studies generally utilize visual instruction tuning with specialized datasets to enhance question and answer (QA) accuracy within the chart domain. However, they often neglect the fundamental discrepancy between natural image-caption pre-training data and digital chart image-QA data, particularly in the models' capacity to extract underlying numeric values from charts. This paper tackles this oversight by exploring the training processes necessary to improve MLLMs' comprehension of charts. We present three key findings: (1) Incorporating raw data values in alignment pre-training markedly improves comprehension of chart data. (2) Replacing images with their textual representation randomly during end-to-end fine-tuning transfer the language reasoning capability to chart interpretation skills. (3) Requiring the model to first extract the underlying chart data and then answer the question in the fine-tuning can further improve the accuracy. Consequently, we introduce CHOPINLLM, an MLLM tailored for in-depth chart comprehension. CHOPINLLM effectively interprets various types of charts, including unannotated ones, while maintaining robust reasoning abilities. Furthermore, we establish a new benchmark to evaluate MLLMs' understanding of different chart types across various comprehension levels. Experimental results show that CHOPINLLM exhibits strong performance in understanding both annotated and unannotated charts across a wide range of types.", "sections": [{"title": "1 Introduction", "content": "In today's data-driven world, visualizations like bar and pie charts are crucial for deciphering complex datasets. However, the increasing diversity and complexity of these charts highlights the need for advanced tools to enhance human capabilities in data analysis. Artificial Intelligence (AI), particularly Multimodal Large Language Models (MLLMs), is increasingly used to automate the understanding of scientific charts, promising more efficient and accurate analysis. Robust benchmarks are also essential, setting standards and metrics that drive the development and evaluation of these AI tools.\nPrior studies have introduced end-to-end neural models aimed at enhancing chart comprehension [29, 31, 67], such as masked table prediction [67], chart question answering [38], and chart de-rendering [31]. These models each is specialized in handling one task within the domain of chart analysis. Furthermore, advancements in Multimodal Large Language Models (MLLMs), exemplified by LLaVA [33, 34] and miniGPT [68], have showcased their versatility in vision-language tasks."}, {"title": "2 Related works", "content": "Large language model Large Language Models (LLMs) have seen remarkable advancements in recent years, primarily driven by transformers [54] that have significantly scaled in model size and training data [6, 9, 11, 17, 19, 23, 49, 51]. These models excel in generalized reasoning and exhibit robust chain-of-thought reasoning [55, 57, 65] across a variety of tasks, largely attributed to extensive pre-training [3, 13, 66] and fine-tuning strategies [10, 43, 45]. The availability of using powerful LLMs with specialized capabilities \u2013 ranging from general assistance [2, 20, 44, 53] to coding [21, 25, 50] - has fueled diverse applications such as data augmentation [14], data generation [46, 60], and providing training training guidance [28, 61]. These developments have markedly accelerated research and practical applications in the field.\nMultimodal large language model Building on the success of LLMs, recent research has expanded their application to multimodal tasks, including image [34, 35, 40, 64], video [7, 63], audio or speech [4, 12, 18], mixed-modal [52], various tool and API usages [41, 47, 48], and robotics [5, 62]. In extending LLMs to image modalities, early studies combined LLMs with external vision models to convert visual information into text, enhancing image comprehension [30, 59]. Others have integrated visual encoders directly within LLM frameworks, developing end-to-end systems that transform images into textual tokens [1, 8, 16, 33, 34, 68]. While maintaining capabilities like reasoning and chain-of-thought processing across various tasks, these models often fall short in domain-specific tasks like chart analysis [37, 42]. This prompts further research into specialized data collection and fine-tuning for distinct domains.\nChart understanding Current approaches to chart understanding fall into two main categories: models specifically designed for chart-related tasks [29, 31, 36, 38, 67], and those that utilize pre-trained LLMs and MLLMs [22, 32, 39, 58]. The first group involves models trained exclusively on chart-specific data, often limited by the scope of the training datasets thus cannot be applied to diverse chart scenarios. The second group, which involves adapting existing LLMs and MLLMs through fine-tuning [34] or integration with external models [30], shows promising versatility across various questions and scenarios. Yet, there is a scarcity of research on MLLMs' pre-training, crucial for deep chart understanding and adaptability to multiple chart types in practical settings. Typically, chart understanding models are evaluated against benchmarks focused on tasks like data extraction [26, 37], summarization [27], and basic mathematical reasoning [42], which predominantly feature basic chart types (e.g., bar, line, pie charts) and lack nuanced differentiation in QA levels to thoroughly assess models' understanding capabilities. Addressing these gaps, our work not only explores effective pre-training strategies for MLLMs on chart data but also introduces a new benchmark with a variety of chart types and differentiated QA levels (e.g., literal, inferential, reasoning) to evaluate MLLMs' comprehensive abilities. Concurrently, CharXiv [56] is proposed for evaluating general understanding of real-world scientific charts, including complex compositions with multiple subplots. In contrast, our benchmark focuses on single-plot chart images, evaluating the raw data understanding and mathematical reasoning of an MLLM."}, {"title": "3 Generating data for chart understanding", "content": "To build a chart understanding MLLM and study its fundamental training process, a comprehensive dataset containing chart images paired with captions and raw data is essential for pre-training, alongside different types of question-answer pairs for end-to-end fine-tuning. However, no existing dataset provides the necessary variety of chart types, topics, and styles. To bridge this gap, we introduce a novel data generation pipeline for large-scale chart data generation (Sec. 3.1) and QAs generation (Sec. 3.2). With the data at hand, we then explore various training strategies in the later sections, including feature alignment pre-training and end-to-end fine-tuning for LLMs. Figure 2 presents an overview of our framework."}, {"title": "3.1 Efficient data generation with quadratic scaling", "content": "Our data generation leverages the promising text content generation and coding abilities of large language models, e.g., GPT-4, to generate chart images and data. Specifically, LLMs allow us to synthesize raw data for chart images, and then the generated Python script turns the raw data into a chart image. In this way, we can produce image data without accessing costly multimodal LLMs like GPT-4V. Unlike previous and concurrent works [22, 58] that prompt LLMs to iteratively generate CSV data, QAs, and Python script for each chart image \u2013 a process that is costly to massively scale \u2013 our pipeline features parallel code and data generation through shared templates and READMES for consistent definitions and formats across the same chart types. Most importantly, since all code script and data share the same structure, our generated data can be universally applied to any generated code and vice versa, significantly enhancing scalability without exhaustedly prompting LLMs. We detail the pipeline further below.\nShared template and README As shown in Fig. 2 (a), given a chart type (e.g., line) sampled from a predefined chart type database, the JSON expert GPT-4 first generates a JSON template for the given chart type, along with a README file. In detail, the JSON template contains general information for the chart image, including the title, x-axis, y-axis information, and raw data. The README contains the definition of the chart type and the meanings of the keys and values to enhance understanding of the JSON template. Please refer to Appendix F for some examples. We note that the JSON template, together with the README, ensures the consistency of data generation so that further data and code generation can follow the explicit format and definition guidance of the template data. Note that we choose JSON as our primary data representation format, in contrast to previous works [22, 37, 42, 58], which used CSV. The JSON format allows us to incorporate not only numerical data but also additional chart information, such as titles and the scales of x and y axes, which is beneficial for pair-wise pre-training tasks. Moreover, JSON data is structured, and when paired with a README file, it minimizes ambiguity in data descriptions, which is particularly valuable for complex chart types. For instance, in candlestick charts, we can clearly define a data point as a dictionary containing \u201copen\u201d, \u201cclose\u201d, \u201chigh\u201d, and \u201clow\u201d values, rather than a list where the meaning of each number might be unclear.\nOrthogonal data and code generation With the template files at hand, we can generate data and code independently. For the data generation branch, to ensure the generated data covers diverse topics, we jointly input the produced template files (i.e., JSON template and README) and a topic sampled from a pre-defined topic set (e.g., energy production and market share) into a data expert GPT-4 module. For the complete topic list, please refer to Appendix G. We require the data expert GPT-4 to follow the definitions in the template files and generate M JSON data along with different kinds of questions and answers (e.g., summary QA) based on the raw data. As for code generation, another code expert GPT-4 is utilized to produce N Python code based on the given chart type, data template, and Python library. Note that to prevent generating simple code repeatedly for the given chart type, we explicitly ask the code expert GPT-4 to introduce visual variations in aspects such as color, legend, grid, font, and mark texture, etc. More details can be found in the appendix."}, {"title": "3.2 Diverse QA synthesis", "content": "Based on the parallel data generation pipeline, we are able to collect massive amount of chart image and JSON raw data pairs for the feature alignment pre-training. Now, we details how we generate different types of QAs for end-to-end fine-tuning. Specifically, having each JSON data as input, we use text-only LLM to generate question-answer (QA) pairs. To cover various question-anwser for chart data, we include general QAs, containing not only description and summary QA but also three different level of QAs: literal QAs, inferential QAs, and reasoning QAs (as illustrated in Fig. 3). Furthermore, to enhance the training of chart understanding, we introduce two additional augmented QAs (for training only): text-only QAs and data-driven QAs. We detail each QA type as follows:\n\u2022 Description QAs: Generate objective descriptions based on the chart data.\n\u2022 Summary QAs: Summarize the chart, highlighting key findings.\n\u2022 Literal QAs: Extract specific values directly from the data.\n\u2022 Inferential QAs: Infer global insights, such as identifying extreme values.\n\u2022 Reasoning QAs: Perform calculations to derive answers from chart data.\n\u2022 JSON-only QAs: Replace images with JSON raw data to augmented previous QAs.\n\u2022 Data-driven QAs: Prompt the model to extract JSON raw data before answering the question.\nThese QAs encompass a range of questions for chart images, covering abilities from basic data understanding and global concept comprehension to advanced reasoning, allowing us to further assess the abilities of MLLMs. Note that, for each QA pair, we use GPT-4 to generate both long and short answers. The long answer, generated first, includes a step-by-step explanation to derive the answer, while the short answer, generated later, contains only the final answer derived from the long explanation. Short answers contain only numerical values or Yes/No response for convenient evaluation purpose. For more examples of generated chart and QAs, please refer to Appendix J.\nComposition for quadratically scaled data As shown in Fig. 3 (a), we consider 18 different chart types. For each chart type, we collect 400 different Python codes (N = 400) and 1000 different JSON data files (M = 1000) covering various topics. Note that we exclude bad data based on predicted file structure's correctness, Python code execution errors, and OCR tools. Please refer to the supplementary materials for detailed information. After filtering, we have approximately 5 million images, with the distribution for each chart type displayed in Fig. 3 (a). For each chart image, we collect the raw data in JSON format, a shared README file, the corresponding Python script, 17 general question-answer (QA) pairs: one description QA, one summary QA, five literal QAs, five inferential QAs, five reasoning QAs, 2 augmented QAs: 1 JSON-only QA, and 1 data-driven QA."}, {"title": "3.3 A new benchmark for comprehensive chart understanding", "content": "Existing chart benchmarks [37, 42] contains only a limited range of chart types (e.g., line, bar, and pie charts) and lack of comprehensive QAs to access a model's understanding of charts from various perspectives, including raw data comprehension, inferential abilities, and mathematical reasoning capabilities. To bridge this gap, we propose a comprehensive benchmark derived from the aforementioned synthetic dataset. It covers 18 different chart types, three different levels of QAs (literal, inferential, and reasoning QAs), and provides both long and short answers. Notably, the chart images in the benchmark are not all annotated, allowing assessment of the model's ability to understand the underlying data of a chart as humans do. To ensure the quality of the images in the benchmark, we employed human evaluations to filter the data and obtain a high-quality test set. The evaluations are based on two criteria: Validity: Whether the essential components of the images (e.g., title, x, y-axis, labeling) are clearly visible and not missing. Extractability: Whether the evaluator can extract the raw data from the given chart image, and whether the extracted data matches the generated one. After human evaluation and filtering, we obtain a test set consisting of \u2248 300 image-QA paris for each chart type, resulting in a total of \u2248 5k pairs. Note that these QAs equally cover literal, inferential, and reasoning questions for measuring chart understanding of MLLMs."}, {"title": "4 Experiments and model analysis", "content": "4.1 Experimental setup\nBenchmark Our evaluation utilizes four classical benchmarks to compare our model against previous works. We specifically use the ChartQA dataset [37], which includes 1.5k chart images in its test set, divided into human-written and machine-generated questions with 1.2k QA pairs each. The human-written questions often require mathematical reasoning. ChartQA also provides CSV data for each image, enabling us to conduct a Chart-to-Table (or Chart Extraction) task to assess the ability of Multimodal Large Language Models (MLLMs) to extract raw data from charts, following previous studies [22, 30]. Additionally, we use the PlotQA dataset [42] where images generally lack numerical value annotations, necessitating value inference relative to the Y-axis. For evaluating the models' capability to capture global concepts, we assess on the Chart-to-Text task using the Pew and Statista splits from the dataset [27]. The Pew split contains 9k images accompanied by descriptions written by professional editors, while the Statista split includes 33k images that often feature descriptive text within the charts themselves, making it an easier split than Pew.\nMetrics For ChartQA and PlotQA, we adopt the relaxed accuracy metric for numeric answers, allowing a 5% margin of error from the exact value, and use exact match for non-numeric answers as per the standard in previous studies [22, 37]. In the Chart-to-Table task, we measure performance using F1 score of Relative Mapping Similarity (RMS) and Relative Number Set Similarity (RNSS) to evaluate numeric accuracy and table similarity, respectively. For the Chart-to-Text task, we use BLEU-4, an N-gram matching score, following the previous work [27].\nA 3-stage training process Unlike previous approaches that convert a general MLLM into a chart-specific expert by only applying LoRA fine-tuning on limited high-quality data [22], training CHOPINLLM unfolds in three stages, illustrated in Fig. 2 (b). The 3-stage training enables our model not only to understand chart QAs and downstream tasks but also to capture the underlying data, thereby achieving a fundamental understanding of charts. In the initial pre-training stage, we fix the ViT and LLM while training the projector from scratch using original LLaVA data alongside our newly generated chart-description and chart-json pairs. The second stage involves freezing ViT and jointly fine-tuning the projector and LLM with both original LLaVA QA pairs and our generated chart QA pairs, enabling the LLM to comprehend visual tokens and facilitate chart question answering. Finally, we apply LORA fine-tuning to align the LLM's response distribution with the target downstream dataset. Each stage is carefully studied and the results are presented in the following subsections. In the following study, we ablate 1 stage at a time and use the full-training setting for the other 2 stages."}, {"title": "4.2 Stage 1: Pre-training for chart feature alignment", "content": "In the first training stage, the goal is to align visual and linguistic features so that visual data can be seamlessly translated into the textual domain for LLM comprehension. Employing a strategy from Liu et al. [34], we use a projector to translate visual features from ViT [15] into the textual domain, training it with pairwise image-caption data to enhance its capability to capture visual information. We explore three configurations: utilizing only LLaVA CC3M Pretraining data, combining LLaVA data with chart-description pairs, and using LLaVA data with both chart-description and chart-raw data pairs. The data for stage two training remains consistent across these settings, summary QAs, description QAs, three-level QAs, text-only QAs, and data-driven QAs, as depicted in Fig. 2 (b). In stage three, all models undergo LoRA fine-tuning on the downstream dataset, using LLaVA-7B as the baseline for this comparison. Results are detailed in Table 1.\nDense data alignment is beneficial for both chart data comprehension and reasoning. For chart images, chart-description pairs act as standard image-caption pairs. However, to more effectively bridge the visual-textual gap, we also utilize chart-json pairs that encompass the underlying numerical data and its schema of the charts. This approach not only aligns visual features with textual descriptions but also significantly enhances model performance, as demonstrated by improvements of approximately 2% in literal QAs and about 1% in reasoning skills, according to results in Table 1."}, {"title": "4.3 Stage 2: End-to-end fine-tuning", "content": "The second stage, end-to-end fine-tuning, trains the MLLM to actually understand the aligned visual tokens so that it follows the user instruction and reason about the answer, on top of the inherent language capability from the original LLM. We utilize a significant number of image-QA pairs to jointly tune the LLM and the projector. To evaluate the effectiveness of incorporating chart QAs during fine-tuning, we conduct ablation studies starting with a baseline that uses only LLaVA Instruct-150K data, incrementally adding extra QA pairs. All methods leverage the same pre-training weights, derived from training on LLaVA data with both chart-description and chart-raw data pairs (the best setting in Sec. 4.2. In stage three, all models undergo LoRA fine-tuning on the downstream dataset. Table 2 presents the result.\nJSON-only QAs allow transferring pure text reasoning abilities to multimodal chart understanding. The chart understanding of MLLMs can be seen as two stages: visual and text raw data alignment (which is done in the training of the first stage) and question answering with reasoning ability on the raw textual data (JSON). Thus, with a well-aligned first stage training, we hypothesize that re-blending some pure textual QAs, preserving the ability of reasoning on text raw data, can also benefit the reasoning abilities in visual-text scenarios. As detailed in Sec. 3.2, for JSON-only QAs, rather than utilizing chart images and QAs, we replace the chart image with JSON data and a README, resulting in purely text-based QAs for training. Table 2 demonstrates the effectiveness of each QA type. We discover that re-blending JSON-only data during the end-to-end fine-tuning stage improves chart reasoning skills, matching the assumption.\nData-driven QAs in the fine-tuning stage enable MLLMs to enhance prediction accuracy through data prompting. As detailed in Sec. 3.2, data-driven QAs are multi-turn QAs, which require models to extract raw data before answering given questions. Combined with the raw data reasoning abilities enhanced via JSON-only QAs, the model can perform data prompting during inference, where models achieve better reasoning robustness by first extracting raw data and then answering the given question based on the data. Please refer to Appendix I for some examples. As shown in Table 2, data-driven QAs significantly enhance the model's ability to capture visual information. Furthermore, leveraging data prompting in inference significantly improves performance across all downstream tasks."}, {"title": "4.4 Stage 3: Downstream fine-tuning", "content": "We build CHOPINLLM with the best setting based on the previous observation (the data used in each stage can be refered to Fig. 2 (b)), and we compare CHOPINLLM with existing chart understanding approaches, including Pix2struct [29], Matcha [31], Unichart [38], Deplot [30], LLaVA [34], and ChartLlama [22]. The results are shown in Table 3.\nClassical question-answering on ChartQA. We find that CHOPINLLM achieves the best performance on the human split of ChartQA, as shown in Table 3, with \u2248 5% improvement, while achieving comparable accuracy on the augmented split with the previous state-of-the-art model, ChartLlama, making CHOPINLLM the best model in terms of average accuracy. We note that the human split in ChartQA is more challenging than the augmented split, as it contains more reasoning questions, suggesting that CHOPINLLM is better at performing reasoning tasks."}, {"title": "5 Conclusion", "content": "In this paper, we explore the impact of fundamental training strategies in adapting generalist Multimodal Large Language Models (MLLMs) to chart understanding. We offer practical guidance for optimizing feature alignment pre-training and end-to-end fine-tuning. Leveraging these enhanced training strategies, we introduce a specialized chart MLLM, named CHOPINLLM, capable of interpreting diverse chart types independently of numerical annotations. Extensive experiments confirm that CHOPINLLM surpasses the previous state-of-the-art across four benchmarks, validating our framework's effectiveness. Additionally, we present a new benchmark specifically designed to evaluate MLLMs' comprehension across various chart types and multiple levels of understanding."}, {"title": "D Social impact", "content": "Our model is capable of chart understanding and can interpret the raw data of a chart like a human, without relying on annotations, while also performing various levels of QA tasks. Thus, our model can be used in many data analysis scenarios, such as market research, healthcare trend analysis, and other data science areas. With the help of our model, humans can process large volumes of chart data more efficiently, make informed decisions, and enhance reporting accuracy. While our model provides benefits in chart understanding and analysis, there are potential negative impacts. For instance, it could be employed to create misleading data visualizations or generate false narratives when combined with other LLM tools. These fake charts and pieces of information can negatively affect decision-making processes."}, {"title": "E Limitation", "content": "In this paper, we propose an MLLM model for chart understanding, fundamentally trained on synthetic data. However, since the synthetic data generated by LLMs cannot be perfect, sometimes incorrect data can be introduced into the dataset and may not be filtered out by our filtering process. These data can result in misalignments and incorrect mappings during pre-training and fine-tuning, potentially leading to incorrect responses and hallucinations. Thus, the performance of our chart MLLMs is limited by the LLMs' generation capabilities. We can potentially include more advanced LLMs in the data generation pipeline to reduce the occurrence of incorrect data. Moreover, another limitation of our model is that it currently supports understanding only 18 chart types. However, there are many more chart types in the real world. Developing an open-domain, versatile chart understanding MLLM remains a task for future work."}]}