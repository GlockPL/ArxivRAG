{"title": "On Pre-training of Multimodal Language Models Customized for Chart Understanding", "authors": ["Wan-Cyuan Fan", "Yen-Chun Chen", "Mengchen Liu", "Lu Yuan", "Leonid Sigal"], "abstract": "Recent studies customizing Multimodal Large Language Models (MLLMs) for domain-specific tasks have yielded promising results, especially in the field of scientific chart comprehension. These studies generally utilize visual instruction tuning with specialized datasets to enhance question and answer (QA) accuracy within the chart domain. However, they often neglect the fundamental discrepancy between natural image-caption pre-training data and digital chart image-QA data, particularly in the models' capacity to extract underlying numeric values from charts. This paper tackles this oversight by exploring the training processes necessary to improve MLLMs' comprehension of charts. We present three key findings: (1) Incorporating raw data values in alignment pre-training markedly improves comprehension of chart data. (2) Replacing images with their textual representation randomly during end-to-end fine-tuning transfer the language reasoning capability to chart interpretation skills. (3) Requiring the model to first extract the underlying chart data and then answer the question in the fine-tuning can further improve the accuracy. Consequently, we introduce CHOPINLLM, an MLLM tailored for in-depth chart comprehension. CHOPINLLM effectively interprets various types of charts, including unannotated ones, while maintaining robust reasoning abilities. Furthermore, we establish a new benchmark to evaluate MLLMs' understanding of different chart types across various comprehension levels. Experimental results show that CHOPINLLM exhibits strong performance in understanding both annotated and unannotated charts across a wide range of types.", "sections": [{"title": "Introduction", "content": "In today's data-driven world, visualizations like bar and pie charts are crucial for deciphering complex datasets. However, the increasing diversity and complexity of these charts highlights the need for advanced tools to enhance human capabilities in data analysis. Artificial Intelligence (AI), particularly Multimodal Large Language Models (MLLMs), is increasingly used to automate the understanding of scientific charts, promising more efficient and accurate analysis. Robust benchmarks are also essential, setting standards and metrics that drive the development and evaluation of these AI tools.\nPrior studies have introduced end-to-end neural models aimed at enhancing chart comprehension, such as masked table prediction, chart question answering, and chart de-rendering. These models each is specialized in handling one task within the domain of chart analysis. Furthermore, advancements in Multimodal Large Language Models (MLLMs), exemplified by LLaVA and miniGPT , have showcased their versatility in vision-language tasks."}, {"title": "Related works", "content": "Large Language Models (LLMs) have seen remarkable advancements in recent years, primarily driven by transformers [54] that have significantly scaled in model size and training data [6, 9, 11, 17, 19, 23, 49, 51]. These models excel in generalized reasoning and exhibit robust chain-of-thought reasoning [55, 57, 65] across a variety of tasks, largely attributed to extensive pre-training [3, 13, 66] and fine-tuning strategies [10, 43, 45]. The availability of using powerful LLMs with specialized capabilities \u2013 ranging from general assistance [2, 20, 44, 53] to coding [21, 25, 50] - has fueled diverse applications such as data augmentation [14], data generation [46, 60], and providing training training guidance [28, 61]. These developments have markedly accelerated research and practical applications in the field.\nBuilding on the success of LLMs, recent research has expanded their application to multimodal tasks, including image [34, 35, 40, 64], video [7, 63], audio or speech [4, 12, 18], mixed-modal [52], various tool and API usages [41, 47, 48], and robotics [5, 62]. In extending LLMs to image modalities, early studies combined LLMs with external vision models to convert visual information into text, enhancing image comprehension [30, 59]. Others have integrated visual encoders directly within LLM frameworks, developing end-to-end systems that transform images into textual tokens [1, 8, 16, 33, 34, 68]. While maintaining capabilities like reasoning and chain-of-thought processing across various tasks, these models often fall short in domain-specific tasks like chart analysis [37, 42]. This prompts further research into specialized data collection and fine-tuning for distinct domains.\nCurrent approaches to chart understanding fall into two main categories: models specifically designed for chart-related tasks [29, 31, 36, 38, 67], and those that utilize pre-trained LLMs and MLLMs [22, 32, 39, 58]. The first group involves models trained exclusively on chart-specific data, often limited by the scope of the training datasets thus cannot be applied to diverse chart scenarios. The second group, which involves adapting existing LLMs and MLLMs through fine-tuning or integration with external models , shows promising versatility across various questions and scenarios. Yet, there is a scarcity of research on MLLMs' pre-training, crucial for deep chart understanding and adaptability to multiple chart types in practical settings. Typically, chart understanding models are evaluated against benchmarks focused on tasks like data extraction , summarization , and basic mathematical reasoning , which predominantly feature basic chart types (e.g., bar, line, pie charts) and lack nuanced differentiation in QA levels to thoroughly assess models' understanding capabilities. Addressing these gaps, our work not only explores effective pre-training strategies for MLLMs on chart data but also introduces a new benchmark with a variety of chart types and differentiated QA levels (e.g., literal, inferential, reasoning) to evaluate MLLMs' comprehensive abilities. Concurrently, CharXiv [56] is proposed for evaluating general understanding of real-world scientific charts, including complex compositions with multiple subplots. In contrast, our benchmark focuses on single-plot chart images, evaluating the raw data understanding and mathematical reasoning of an MLLM."}, {"title": "Generating data for chart understanding", "content": "To build a chart understanding MLLM and study its fundamental training process, a comprehensive dataset containing chart images paired with captions and raw data is essential for pre-training, alongside different types of question-answer pairs for end-to-end fine-tuning. However, no existing dataset provides the necessary variety of chart types, topics, and styles. To bridge this gap, we introduce a novel data generation pipeline for large-scale chart data generation (Sec. 3.1) and QAs generation (Sec. 3.2). With the data at hand, we then explore various training strategies in the later sections, including feature alignment pre-training and end-to-end fine-tuning for LLMs. Figure 2 presents an overview of our framework."}, {"title": "Efficient data generation with quadratic scaling", "content": "Our data generation leverages the promising text content generation and coding abilities of large language models, e.g., GPT-4, to generate chart images and data. Specifically, LLMs allow us to synthesize raw data for chart images, and then the generated Python script turns the raw data into a"}, {"title": "Diverse QA synthesis", "content": "Based on the parallel data generation pipeline, we are able to collect massive amount of chart image and JSON raw data pairs for the feature alignment pre-training. Now, we details how we generate different types of QAs for end-to-end fine-tuning. Specifically, having each JSON data as input, we use text-only LLM to generate question-answer (QA) pairs. To cover various question-anwser for chart data, we include general QAs, containing not only description and summary QA but also three different level of QAs: literal QAs, inferential QAs, and reasoning QAs (as illustrated in Fig. 3). Furthermore, to enhance the training of chart understanding, we introduce two additional augmented QAs (for training only): text-only QAs and data-driven QAs. We detail each QA type as follows:\n\u2022 Description QAs: Generate objective descriptions based on the chart data.\n\u2022 Summary QAs: Summarize the chart, highlighting key findings.\n\u2022 Literal QAs: Extract specific values directly from the data.\n\u2022 Inferential QAs: Infer global insights, such as identifying extreme values.\n\u2022 Reasoning QAs: Perform calculations to derive answers from chart data.\n\u2022 JSON-only QAs: Replace images with JSON raw data to augmented previous QAs.\n\u2022 Data-driven QAs: Prompt the model to extract JSON raw data before answering the question.\nThese QAs encompass a range of questions for chart images, covering abilities from basic data understanding and global concept comprehension to advanced reasoning, allowing us to further assess the abilities of MLLMs. Note that, for each QA pair, we use GPT-4 to generate both long and short answers. The long answer, generated first, includes a step-by-step explanation to derive the answer, while the short answer, generated later, contains only the final answer derived from the long explanation. Short answers contain only numerical values or Yes/No response for convenient evaluation purpose. For more examples of generated chart and QAs, please refer to Appendix J.\nAs shown in Fig. 3 (a), we consider 18 different chart\ntypes. For each chart type, we collect 400 different Python codes (N = 400) and 1000 different\nJSON data files (M = 1000) covering various topics. Note that we exclude bad data based on\npredicted file structure's correctness, Python code execution errors, and OCR tools. Please refer to the\nsupplementary materials for detailed information. After filtering, we have approximately 5 million\nimages, with the distribution for each chart type displayed in Fig. 3 (a). For each chart image, we"}, {"title": "A new benchmark for comprehensive chart understanding", "content": "Existing chart benchmarks contains only a limited range of chart types (e.g., line, bar, and pie charts) and lack of comprehensive QAs to access a model's understanding of charts from various perspectives, including raw data comprehension, inferential abilities, and mathematical reasoning capabilities. To bridge this gap, we propose a comprehensive benchmark derived from the aforementioned synthetic dataset. It covers 18 different chart types, three different levels of QAs (literal, inferential, and reasoning QAs), and provides both long and short answers. Notably, the chart images in the benchmark are not all annotated, allowing assessment of the model's ability to understand the underlying data of a chart as humans do. To ensure the quality of the images in the benchmark, we employed human evaluations to filter the data and obtain a high-quality test set. The evaluations are based on two criteria: Validity: Whether the essential components of the images (e.g., title, x, y-axis, labeling) are clearly visible and not missing. Extractability: Whether the evaluator can extract the raw data from the given chart image, and whether the extracted data matches the generated one. After human evaluation and filtering, we obtain a test set consisting of \u2248 300 image-QA paris for each chart type, resulting in a total of \u2248 5k pairs. Note that these QAs equally cover literal, inferential, and reasoning questions for measuring chart understanding of MLLMs."}, {"title": "Experiments and model analysis", "content": "Our evaluation utilizes four classical benchmarks to compare our model against\nprevious works. We specifically use the ChartQA dataset, which includes 1.5k chart images in its\ntest set, divided into human-written and machine-generated questions with 1.2k QA pairs each. The\nhuman-written questions often require mathematical reasoning. ChartQA also provides CSV data for\neach image, enabling us to conduct a Chart-to-Table (or Chart Extraction) task to assess the ability of\nMultimodal Large Language Models (MLLMs) to extract raw data from charts, following previous\nstudies [22, 30]. Additionally, we use the PlotQA dataset where images generally lack numerical\nvalue annotations, necessitating value inference relative to the Y-axis. For evaluating the models'\ncapability to capture global concepts, we assess on the Chart-to-Text task using the Pew and Statista\nsplits from the dataset. The Pew split contains 9k images accompanied by descriptions written\nby professional editors, while the Statista split includes 33k images that often feature descriptive text\nwithin the charts themselves, making it an easier split than Pew.\nFor ChartQA and PlotQA, we adopt the relaxed accuracy metric for numeric answers,\nallowing a 5% margin of error from the exact value, and use exact match for non-numeric answers as\nper the standard in previous studies [22, 37]. In the Chart-to-Table task, we measure performance\nusing F1 score of Relative Mapping Similarity (RMS) and Relative Number Set Similarity (RNSS)\nto evaluate numeric accuracy and table similarity, respectively. For the Chart-to-Text task, we use\nBLEU-4, an N-gram matching score, following the previous work [27]."}, {"title": "Experimental setup", "content": "Unlike previous approaches that convert a general MLLM into a\nchart-specific expert by only applying LoRA fine-tuning on limited high-quality data , training\nCHOPINLLM unfolds in three stages, illustrated in Fig. 2 (b). The 3-stage training enables our model\nnot only to understand chart QAs and downstream tasks but also to capture the underlying data,\nthereby achieving a fundamental understanding of charts. In the initial pre-training stage, we fix the\nViT and LLM while training the projector from scratch using original LLaVA data alongside our newly\ngenerated chart-description and chart-json pairs. The second stage involves freezing ViT and jointly\nfine-tuning the projector and LLM with both original LLaVA QA pairs and our generated chart QA\npairs, enabling the LLM to comprehend visual tokens and facilitate chart question answering. Finally,\nwe apply LORA fine-tuning to align the LLM's response distribution with the target downstream\ndataset. Each stage is carefully studied and the results are presented in the following subsections. In\nthe following study, we ablate 1 stage at a time and use the full-training setting for the other 2 stages."}, {"title": "Stage 1: Pre-training for chart feature alignment", "content": "In the first training stage, the goal is to align visual and linguistic features so that visual data can be\nseamlessly translated into the textual domain for LLM comprehension. Employing a strategy from\nLiu et al. , we use a projector to translate visual features from ViT into the textual domain,\ntraining it with pairwise image-caption data to enhance its capability to capture visual information.\nWe explore three configurations: utilizing only LLaVA CC3M Pretraining data, combining LLaVA\ndata with chart-description pairs, and using LLaVA data with both chart-description and chart-raw\ndata pairs. The data for stage two training remains consistent across these settings, summary QAs,\ndescription QAs, three-level QAs, text-only QAs, and data-driven QAs, as depicted in Fig. 2 (b). In\nstage three, all models undergo LoRA fine-tuning on the downstream dataset, using LLaVA-7B as\nthe baseline for this comparison.\nFor chart\nimages, chart-description pairs act as standard image-caption pairs. However, to more effectively\nbridge the visual-textual gap, we also utilize chart-json pairs that encompass the underlying numer-\nical data and its schema of the charts. This approach not only aligns visual features with textual\ndescriptions but also significantly enhances model performance, as demonstrated by improvements of\napproximately 2% in literal QAs and about 1% in reasoning skills"}, {"title": "Stage 2: End-to-end fine-tuning", "content": "The second stage, end-to-end fine-tuning, trains the MLLM to actually understand the aligned visual\ntokens so that it follows the user instruction and reason about the answer, on top of the inherent\nlanguage capability from the original LLM. We utilize a significant number of image-QA pairs to\njointly tune the LLM and the projector. To evaluate the effectiveness of incorporating chart QAs\nduring fine-tuning, we conduct ablation studies starting with a baseline that uses only LLaVA Instruct-\n150K data, incrementally adding extra QA pairs. All methods leverage the same pre-training weights,"}, {"title": "Stage 3: Downstream fine-tuning", "content": "We build CHOPINLLM with the best setting based on the previous observation (the data used in each\nstage can be refered to Fig. 2 (b)), and we compare CHOPINLLM with existing chart understanding\napproaches, including Pix2struct , Matcha, Unichart , Deplot , LLaVA , and\nChartLlama . The results are shown in Table 3."}, {"title": "Classical question-answering on ChartQA", "content": "We find that CHOPINLLM achieves the best perfor-\nmance on the human split of ChartQA, as shown in Table 3, with \u2248 5% improvement, while achieving\ncomparable accuracy on the augmented split with the previous state-of-the-art model, ChartLlama,\nmaking CHOPINLLM the best model in terms of average accuracy. We note that the human split\nin ChartQA is more challenging than the augmented split, as it contains more reasoning questions,"}, {"title": "More model analysis", "content": "We provide a qualitative comparison of chart-to-text and chart-to-table tasks,\nwith results depicted in Fig. 4. In the chart-to-table task, our model accurately captures values from\nchart images, unlike LLaVA and ChartLlama. It is important to note that the gold data tables for\nChartQA are not always directly accessible, leading to the use of existing models or OCR tools for\ndata extraction. This process can introduce errors, such as misreporting the value 91.5 for the UK as\n915.0, which can adversely affect the performance of MLLMs fine-tuned on such data. Despite these\ndataset inaccuracies, our model remains robust, correctly outputting values where ChartLlama does\nnot. In the chart-to-text comparison, both ChartLlama and our model grasp the overall concept of the\ncharts, but our model excels at accurately capturing and summarizing exact numerical values.\nwe emphasize preserving human-like multi-turn conversation\nabilities. Figure 5 presents a qualitative comparison on chart images with multi-turn QAs. Although\nChartLlama extracts accurate numerical values, it fails to provide coherent explanations or reasonable\ntext outputs. In contrast, CHOPINLLM not only extracts accurate data but also provides logical\nreasoning and coherent explanations, showcasing the effectiveness of our training approach."}, {"title": "Qualitative examples", "content": "Our model, trained extensively across a variety of\nchart types, was evaluated to assess its performance against the previous state-of-the-art model on\nthe same chart types. For an unbiased comparison, we focused on the short answer format in QA\npairs to avoid variations in output preference. The results, detailed in Table 4, reveal that our model\nconsistently outperforms the state-of-the-art across both overlapping and basic chart types. Notably,\nour benchmark, which features unannotated images, poses a greater challenge than ChartQA. The\nsubstantial performance improvement indicates that our model is adept at inferring data directly from\ncharts and demonstrates superior reasoning capabilities."}, {"title": "Performance across different chart types", "content": "In this paper, we explore the impact of fundamental training strategies in adapting generalist Multi-\nmodal Large Language Models (MLLMs) to chart understanding. We offer practical guidance for\noptimizing feature alignment pre-training and end-to-end fine-tuning. Leveraging these enhanced\ntraining strategies, we introduce a specialized chart MLLM, named CHOPINLLM, capable of inter-\npreting diverse chart types independently of numerical annotations. Extensive experiments confirm\nthat CHOPINLLM surpasses the previous state-of-the-art across four benchmarks, validating our\nframework's effectiveness. Additionally, we present a new benchmark specifically designed to\nevaluate MLLMs' comprehension across various chart types and multiple levels of understanding."}, {"title": "Conclusion", "content": "This work was funded, in part, by the Vector Institute for AI, Canada CIFAR AI Chairs, NSERC\nCRC, and NSERC DGs. Resources used in preparing this research were provided, in part, by the\nProvince of Ontario, the Government of Canada through CIFAR, the Digital Research Alliance of\nCanada, companies sponsoring the Vector Institute, and Advanced Research Computing at the\nUniversity of British Columbia. Additional hardware support was provided by John R. Evans Leaders\nFund CFI grant and Compute Canada under the Resource Allocation Competition award."}, {"title": "Acknowledgement", "content": "In this paper, we propose an MLLM model for chart understanding, fundamentally trained on\nsynthetic data. However, since the synthetic data generated by LLMs cannot be perfect, sometimes\nincorrect data can be introduced into the dataset and may not be filtered out by our filtering process.\nThese data can result in misalignments and incorrect mappings during pre-training and fine-tuning,\npotentially leading to incorrect responses and hallucinations. Thus, the performance of our chart\nMLLMs is limited by the LLMs' generation capabilities. We can potentially include more advanced\nLLMs in the data generation pipeline to reduce the occurrence of incorrect data. Moreover, another\nlimitation of our model is that it currently supports understanding only 18 chart types. However,\nthere are many more chart types in the real world. Developing an open-domain, versatile chart\nunderstanding MLLM remains a task for future work."}, {"title": "Social impact", "content": "Our model is capable of chart understanding and can interpret the raw data of a chart like a human,\nwithout relying on annotations, while also performing various levels of QA tasks. Thus, our model\ncan be used in many data analysis scenarios, such as market research, healthcare trend analysis,\nand other data science areas. With the help of our model, humans can process large volumes of\nchart data more efficiently, make informed decisions, and enhance reporting accuracy. While our\nmodel provides benefits in chart understanding and analysis, there are potential negative impacts. For\ninstance, it could be employed to create misleading data visualizations or generate false narratives\nwhen combined with other LLM tools. These fake charts and pieces of information can negatively\naffect decision-making processes."}, {"title": "Limitation", "content": ""}]}