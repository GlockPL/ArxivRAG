{"title": "VSC-RL: Advancing Autonomous Vision-Language Agents with Variational Subgoal-Conditioned Reinforcement Learning", "authors": ["Qingyuan Wu", "Jianheng Liu", "Jianye Hao", "Jun Wang", "Kun Shao"], "abstract": "State-of-the-art (SOTA) reinforcement learning (RL) methods enable the vision-language agents to learn from interactions with the environment without human supervision. However, they struggle with learning inefficiencies in tackling real-world complex sequential decision-making tasks, especially with sparse reward signals and long-horizon dependencies. To effectively address the issue, we introduce Variational Subgoal-Conditioned RL (VSC-RL), which reformulates the vision-language sequential decision-making task as a variational goal-conditioned RL problem, allowing us to leverage advanced optimization methods to enhance learning efficiency. Specifically, VSC-RL optimizes the SubGoal Evidence Lower Bound (SGC-ELBO), which consists of (a) maximizing the subgoal-conditioned return via RL and (b) minimizing the subgoal-conditioned difference with the reference policy. We theoretically demonstrate that SGC-ELBO is equivalent to the original optimization objective, ensuring improved learning efficiency without sacrificing performance guarantees. Additionally, for real-world complex decision-making tasks, VSC-RL leverages the vision-language model to autonomously decompose the goal into feasible subgoals, enabling efficient learning. Across various benchmarks, including challenging real-world mobile device control tasks, VSC-RL significantly outperforms the SOTA vision-language agents, achieving superior performance and remarkable improvement in learning efficiency.", "sections": [{"title": "1. Introduction", "content": "In recent years, the vision-language model (VLM) has shown its significant ability of multimodal understanding and commonsense reasoning (Yang et al., 2023a; Shen et al., 2024; Hong et al., 2023), and has achieved impressive achievements in various real-world applications, such as visual question answering and visual captioning (Bai et al., 2023; Chen et al., 2024). Therefore, it shows great potential to solve real-world complex decision-making problems (e.g., mobile device control (Toyama et al., 2021) and web control (Zhou et al., 2023)) via building intelligent vision-language agents through the advanced vision-language model (Yang et al., 2023c; Zheng et al., 2024). In the past decade, reinforcement learning (RL) has achieved impressive achievements in various sequential decision-making tasks including board games (Silver et al., 2017) and video games (Berner et al., 2019). Some recent works (Bai et al., 2024; Qi et al., 2024) suggest that training vision-language agents with the RL methods for addressing sequential decision-making tasks.\nOverall, based on the specific training paradigm, vision-language agents can be categorized into three main types: prompting-based, imitation-based, and RL-based agents. Directly leveraging VLMs (e.g., Gemini-1.5-Pro (Team et al., 2024) and GPT-4V (OpenAI, 2023)) to capture the critical information from the multimodal content, prompting-based agents aim to generate action via prompting engineering and retrieving techniques (Yang et al., 2023c;b). The performance of the prompting-based agents is usually poor as the weights of these VLMs can not be updated for challenging tasks. Some works (Zhang & Zhang, 2023; Hong et al., 2024) suggest using imitation learning techniques to fine-tune the open-source VLMs on the human demonstration. However, the performance of imitation-based agents is highly dependent on the quality of the demonstration, thus may lead to suboptimal performance or even hinder the model's ability to solve the novel tasks out of distribution. Recently, the most promising method is the RL-based agent which enables the VLM to address complex sequential decision-making tasks by applying RL approaches in training VLM-based policy (Bai et al., 2024; Qi et al., 2024)."}, {"title": "2. Related Works", "content": "In real-world complex control tasks requiring capacities in reasoning, planning and content understanding, it is necessary to enable agents with the vision-language models. In particular, the vision-language model (VLM) can process and abstract the image and language content for challenging decision-making tasks, especially in mobile device control tasks. Existing vision-language agents can be categorized as prompting-based, imitation-based, and RL-based agents based on the corresponding learning paradigms. Additionally, recent advances explore using VLM as subgoal generator to decompose the complicated goal in long-horizon tasks."}, {"title": "2.1. Vision-language Agents for Decision-making", "content": "Prompting-based Agent. Leveraging the inherent reasoning and planning abilities of prosperity VLMs (e.g., Gemini-1.5-Pro (Team et al., 2024) and GPT-4V (OpenAI, 2023)), the prompting-based agent makes decision via prompting engineering and retrieving techniques. For instance, AppAgent (Yang et al., 2023c) first introduces a unified prompting-based agent method to enable the vision-language model to directly interact with mobile applications by providing the prompts with details of actions. Set-of-Marks (Yang et al., 2023b) proposes a new prompting method to enhance the visual grounding ability of VLM. However, the performance of these prompting-based agents is always sensitive to prompts required manually and carefully designed. Therefore, it is challenging for the prompting-based agent to directly output the correct and desired actions to address real-world complex control problems.\nImitation-based Agent. The imitation-based agent learns to mimic the expert behaviors by fine-tuning the policy on human demonstration. Recently, Android in the Wild (AitW) (Rawles et al., 2024) establish large-scale datasets of mobile device control tasks, enabling agents to directly learn from human experience. AutoUI (Zhang & Zhang, 2023) and CogAgent (Hong et al., 2024) fine-tune the VLM-based policies with the AitW dataset, remarkably outperforming the prompting-based agent. In order to adapt the fine-tuned agent to the online environment, Filtered BC (Pan et al., 2024b) introduces online imitation mechanisms to learn from successful online experiences. Unfortunately, these methods rely heavily on high-quality human demonstrations and often struggle to generalize to unseen tasks, limiting their application in diverse real-world scenarios.\nRL-based Agent. Different to prompting-based and imitation-based agents, the RL-based agent can autonomously optimize the policy through trial-and-error interactions with environments, without human supervision. DigiRL (Bai et al., 2024) introduces a unified offline-to-online RL framework that enables agents to learn directly from real-time interactions in dynamic environments, improving performance without the need for curated datasets. DistRL (Wang et al., 2024) builds an asynchronous distributed RL system, allowing training multiple agents in parallel across different environments, thus significantly enhancing scalability and convergence speed. However, these RL-based agents still fundamentally suffer from the learning efficiency issue in challenging sequential decision-making tasks with the sparse reward and long-horizon.\nEnhancing RL with VLM. Recent works have shown that VLM can enhance the RL method via its remarkable capacities of reasoning, planning, and content understanding. Recent works suggest adopting VLM in reward-shaping for RL. For instance, VLM-RMs (Rocamonde et al., 2023) demonstrate that VLMs can serve as effective reward models for learning complex skills. VLM can also generate the subgoals to guide the learning process for autonomous driving (Pan et al., 2024a) and robot (Yang et al., 2024) tasks. Nonetheless, it is still an open problem how to effectively integrate the VLM-generated subgoals into RL."}, {"title": "2.2. Goal-conditioned and Variational RL", "content": "We introduce the common RL methods in addressing the control tasks, including goal-conditioned RL and variational RL.\nGoal-conditioned RL. Sequential decision-making tasks can be viewed as the goal-conditioned RL probelm (Liu et al., 2022a). Based on the current state, the agent aims to find the optimal policy that guides progress toward the given goal for maximizing the return. Hindsight experience replay (Andrychowicz et al., 2017) introduces an implicit curriculum learning method to enhance learning efficiency and robustness. With the perspective of divide-and-conquer, some approaches suggest that guiding the agent with subgoals as intermediate reward signals via imagination (Chane-Sane et al., 2021; Nair & Finn, 2019) and tree-search (Jurgenson et al., 2020; Parascandolo et al., 2020).\nVariational RL. The RL problem can be viewed as the variational inference problem (Levine, 2018) which can be resolved by utilizing extensive optimization tools, thus effectively improving the learning efficiency. Applying the expectation-maximization algorithm in the actor-critic method in RL, VIP (Neumann, 2011) presents a unified variational inference framework. MPO (Abdolmaleki et al., 2018a;b) proposes a series of off-policy RL with entropy regulation in the manner of expectation-maximization. VDPO (Wu et al., 2024) and CVPO (Liu et al., 2022b) apply the variational inference techniques in addressing RL problem with delayed signals and safety constraints, respectively.\nThis paper aims to show how to formulate the control problem as the variational subgoal-conditioned RL problem from the perspective of variational inference which allows us to resolve the complicated control task by utilizing extensive optimization tools."}, {"title": "3. Preliminaries", "content": "Finite-Horizon Goal-Conditioned MDP. We formulate the RL problem as the finite horizon goal-conditioned Markov Decision Process (MDP), denoted by the tuple < G,S,A,R,T,H > where G is the goal set, S is the state space, A is the action space, $T : S \\times A \\times S \\rightarrow [0, 1]$ is the dynamic function, R is the reward function and H is the horizon. At each timestep t, the mobile agent takes action $a_t \\in A$ (e.g., typing text, press button or slide the screen) based on its policy $\\pi : S \\times G \\times A \\rightarrow [0, 1]$, the current screenshot $s_t \\in S$, and a specific goal $g\\in G$ (e.g., search a new TV at Best Buy) selected in the beginning of each episode. The agent only receives the reward $r_t = 1$ if the goal g is accomplished, otherwise the reward $r_t = 0$. The objective of the mobile agent is to find the policy $\\pi$ which can accomplish all goals from the goal set G within the finite horizon H.\nVariational RL. RL can be viewed as a variational inference problem. We denote the optimality of a trajectory $\\tau$ is the event O, and the corresponding probability of the trajectory optimality is denoted as $p(\\mathcal{O}|\\tau) \\propto \\exp(\\frac{J(\\tau)}{\\alpha})$ where $\\alpha$ is the temperature. Therefore, the objective transforms to finding a policy with the highest log evidence: $\\max_{\\pi} \\log p_{\\pi} (\\mathcal{O})$. Furthermore, the Evidence Lower Bound of the objective is:\n$\\mathbb{E}_{\\tau\\sim q(\\tau)}[\\log p(\\mathcal{O}|\\tau)] - KL(q(\\tau)||p_{\\pi}(\\tau)),$ (1)\nwhere $q(\\tau)$ is the prior trajectory distribution and KL is the Kullback-Leibler divergence. Thus, the objective of Variational RL is maximizing the ELBO (Equation (1)).\nSubgoal Generator. For challenging control tasks with sparse and long-term reward signals, it is difficult to learn the useful policy arriving at the final goal within the finite horizon. Therefore, subgoal generation is particularly useful in providing the intermediate signals to facilitate learning. Then, we introduce the assumption of the existence of subgoals for the given goal, aiming to bring the goal-conditioned RL problem to the subgoal-conditioned RL problem as follows.\nAssumption 3.1 (Existence of Subgoals). Given a trajectory $\\tau$ and the corresponding goal g, it always exists a sequence of sub-trajectories and corresponding subgoals $\\{\\tau_i, s_{g_i}\\}_{i=1}^N (1 \\leq N \\leq H)$ induced from the $\\tau$ and g.\nCommonly adopted in literature (Sutton et al., 1999), the above assumption is mild and usually holds. For instance, when N = 1, the subgoals and sub-trajectories are the original goal and trajectory, respectively. When N = H, each sub-trajectory is composed of one single transition-tuple $(s_t, a_t, r_t, s_{t+1})$ with its corresponding subgoal."}, {"title": "4. Our Approach: VSC-RL", "content": "In this section, we present our approach, Variational Subgoal-Conditioned Reinforcement Learning (VSC-RL) for enhancing vision-language agents. First, we formulate the sequential decision-making task as the variational goal-conditioned RL problem (Section 4.1). Next, we derive the new subgoal-conditioned optimization objective, SGC-ELBO consisting of (a) maximizing the subgoal-conditioned RL return (Proposition 4.1) and (b) minimizing the subgoal-conditioned difference (Proposition 4.2). We also theoretically show the equivalence between the original and derived objectives, ensuring both improved efficiency and performance guarantees. In Section 4.3, we demonstrate that VLM can effectively generate feasible subgoals form the complex goal for VSC-RL. The practical implementation is illustrated in Section 4.4. We present the overall pipeline of VSC-RL in Figure 1, and the pseudo-code of VSC-RL is summarized in Algorithm 1."}, {"title": "4.1. Problem Formulation", "content": "We first formalize the real-world sequential decision-making as the variational goal-conditioned RL problem for vision-language agents. In this context, similar to Equation (1), our objective is finding goal-conditioned policy $\\pi$ with the highest log evidence: $\\max_\\pi \\log p_\\pi(\\mathcal{O}|g)$ for a given goal g. Then, we can derive the Goal-Conditioned ELBO (GC-ELBO) of $\\log p_\\pi(\\mathcal{O}|\\tau, g)$ as follows:\n$\\mathbb{E}_{\\tau \\sim P(\\tau |g)}[\\log p(\\mathcal{O}|\\tau, g)] - KL(p_\\pi(\\tau|g)||P_{\\pi_{ref}} (\\tau|g)),$ (2)\nwhere $P_{\\pi_{ref}} (\\tau|g)$ is the prior trajectory distribution of the goal-conditioned reference policy $\\pi_{ref}$ for the given goal g. Therefore, from Equation (2), the objective becomes maximizing the GC-ELBO: $\\max_{\\pi} GC\\text{-}ELBO(\\pi, \\pi_{ref}, g)$."}, {"title": "4.2. Variational Subgoal-Conditioned RL", "content": "With the assumption of the subgoals (Assumption 3.1), we demonstrate that the former term of GC-ELBO (Equation (2)) is equivalent to the maximizing subgoal-conditioned RL objective (Proposition 4.1) and the latter term of GC-ELBO can be transformed to the minimizing subgoal-conditioned difference (Proposition 4.2)."}, {"title": "4.2.1. SUBGOAL-CONDITIONED RL EQUIVALENCE", "content": "Based on Equation (2), we show that the former term, $\\mathbb{E}_{\\tau\\sim p_\\pi (\\tau|g)}[\\log p(\\mathcal{O}|\\tau,g)]$, can be reformulated in the subgoal-conditioned RL objective with shorter-horizon in the following Proposition 4.1.\nProposition 4.1 (Subgoal-Conditioned Optimization Objective, Proof in Proposition B.1). Given a goal g with corresponding subgoals $\\{s_{g_i}\\}_{i=1}^N$ and a subgoal-conditioned target policy $\\pi$, the objective of\n$\\max_{\\pi} \\mathbb{E}_{\\tau\\sim P(\\tau|g)} [\\log p(\\mathcal{O}|\\tau, g)]$\nis equivalent to the objective of\n$\\max_{\\pi} \\sum_{i=1}^N \\mathbb{E}_{[\\tau_i \\sim P_\\pi (\\tau_i | s_{g_i})]} [\\log p(\\mathcal{O}|\\tau_i, s_{g_i})]$\nIn the above proposition, we have successfully transformed the original goal-wise objective into the subgoal-conditioned objective which is composed of N subgoals with corresponding shorter sub-trajectories. Thus, the agent can learn from the reward signals from the subgoals, thus effectively improving the learning efficiency (Jiang & Agarwal, 2018)."}, {"title": "4.2.2. SUBGOAL-CONDITIONED DIFFERENCE BOUND", "content": "Next, we show that the latter term in Equation (2), $KL(\\rho_\\pi(\\tau|g)||P_{\\pi_{ref}} (\\tau|g))$, has the subgoal-conditioned upper bound in the following proposition.\nProposition 4.2 (Subgoal-conditioned Difference Bound, Proof in Proposition B.2). Given goal-conditioned reference policy $\\pi_{ref}$ and subgoal-conditioned target policy $\\pi$, the goal-conditioned KL divergence of a given goal g has the upper bound of subgoal-conditioned KL divergence of corresponding subgoals $\\{s_{g_i}\\}_{i=1}^N$ as followed:\n$KL(p_\\pi(\\tau|g)||P_{\\pi_{\\omega f}}(\\tau|g)) \\leq \\sum_{i=1}^N [KL(p_\\pi(\\tau_i|s_{Gi})||P_{\\pi_{\\eta S}}(\\tau_i|g))].$\nTherefore, from Proposition 4.2, we can directly minimize the N subgoal-conditioned KL divergences which is the upper bound of the goal-conditioned KL divergence."}, {"title": "4.2.3. SUBGOAL-CONDITIONED ELBO", "content": "Based on Proposition 4.1 and Proposition 4.2, we have successfully transformed the optimization objective, GC-ELBO($\\pi, \\pi_{ref}, g$) into the SubGoal-Conditioned ELBO (SGC-ELBO) as follows:\n$\\mathbb{E}_{\\tau_i \\sim P_\\pi (\\tau_i | s_{g_i})} [\\log p(\\mathcal{O}|\\tau_i, s_{g_i})] - KL(p_\\pi(\\tau_i|s_{g_i})||P_{\\pi_{ref}}(\\tau_i|g)).$ (3)\nEquation (3) consisting of two separate learning sub-objectives: (a) maximizing the subgoal of the target policy $\\pi$ and (b) minimizing the subgoal-conditioned difference with the reference policy $\\pi_{ref}$. Therefore, the target agent can directly learn to resolve the subgoal $s_{g_i}$ requiring a much shorter horizon requirement g, effectively improving the learning efficiency. Additionally, we also demonstrate the equivalence between GC-ELBO (Equation (2))and SGC-ELBO (Equation (3)), thus improving learning efficiency without compromising performance guarantees."}, {"title": "4.3. Autonomous Vision-Language Subgoal Generation", "content": "For real-world complex decision-making, it is challenging to handcraft and design the subgoals for each goal manually. VLM has exhibited a unique reasoning ability in image captioning, visual question answering, and multimodal reasoning via integrating and interpreting visual and textual information to derive meaningful insights for vision-language agents. Therefore, we use VLM as the subgoal generator which autonomously decomposes the given goal g into the feasible subgoals $\\{s_{g_i}\\}_{i=1}^N$. As demonstrated in the AitW task example (Figure 2), VLM can autonomously decompose the goal: \"What's the US dollar exchange rate against the Euro?\u201d into more specific and easily solvable"}, {"title": "4.4. Practical Implementation", "content": "As a unified RL-based agent framework, most existing RL-based methods can easily be embedded in VSC-RL. In this paper, we mainly consider the mobile device control task to evaluate the VSC-RL, a representative real-world challenging sequential decision-making task which has dramatically drawn attention recently. Specifically, the reference agent $\\pi_{ref}$ and target agent $\\pi$ are both initialized as the AutoUI-Base agent (Zhang & Zhang, 2023) which is pre-trained on the Android in the Wild (AitW) datasets. To maximize the subgoal-conditioned RL objective in Equation (3), VSC-RL uses the Advantage-Weighted Regression (AWR) algorithm (Peng et al., 2019) modified by DigiRL (Bai et al., 2024) as follows:\n$\\arg \\max_{\\pi} \\mathbb{E}_{s, a, s_{g_i} \\sim D} [\\log \\pi(a|s, s_{g_i}) \\exp(\\frac{A(s, a, s_{g_i})}{\\beta})]$ (5)\nwhere D is the replay buffer, $\\beta$ is the hyperparameter and $A(s, a, s_{g_i}) := R_i - V(s, a, s_{g_i})$ is the advantage function which aims to predict the return $R_i$ of the subgoal $s_{g_i}$ as follows:\n$\\arg \\min_V \\mathbb{E}_{s,a,s_{g_i}, R_i \\sim D} [||R_i - V(s, a, s_{g_i})||^2],$ (6)\nwhere $R_i$ is the binary return evaluated by the VLM (Pan et al., 2024b) and $V(s, a, s_{g_i})$ is the subgoal-conditioned value function. VSC-RL minimize the subgoal-conditioned KL divergence in Equation (3) via imitation loss as follows:\n$\\arg \\max_{\\pi} \\mathbb{E}_{a_{ref}\\sim\\pi_{ref}(s, g)\\atop s, s_{g_i},g\\sim D} [\\log \\pi(a_{ref}|s, S_{g_i})],$ (7)\nwhere $a_{ref}$ is the reference action. Similar to DigiRL (Bai et al., 2024), VSC-RL also additionally learns the instruction-level value function for filtering the sub-trajectories and accelerating the learning process.\nVSC-RL adopts Gemini-1.5-Pro (Team et al., 2024) as the subgoal generator. Specifically, we in-context prompt the VLM to generate the subgoals for a given goal including human demonstration as examples. The prompt example is provided in Appendix C. Overall, the pseudo-code of VSC-RL is summarized in Algorithm 1."}, {"title": "5. Experiments", "content": "In this section, we first show that our VSC-RL can effectively improve learning efficiency and performance, on the toy vision-language tasks. Then, we mainly demonstrate that our VSC-RL can achieve better sample efficiency and a higher success rate than various state-of-the-art (SOTA) agents in addressing challenging and complex mobile device control tasks. We also evaluate the generalization ability of VSC-RL on unseen mobile device control tasks. We also investigate the improvement and verification of the VLM-based subgoal generator of the VSC-RL."}, {"title": "5.1. Experimental Settings", "content": "Benchmarks. We first evaluate our VSC-RL on the toy vision-language tasks, MiniGrid (Chevalier-Boisvert et al., 2023). For the more complex and challenging problem, we mainly consider AitW General and Web Shopping tasks (Rawles et al., 2024), two kinds of the most challenging device control tasks for evaluation. The horizon of General and Web Shopping tasks are set to 10 and 20 steps, respectively. The success of the task is autonomously evaluated by the Gemini-1.5-Pro (Team et al., 2024) via the in-context prompting approach.\nBaselines. For the MiniGrid, we select the PPO (Schulman et al., 2017) as the baseline, and we apply VSC-RL in the PPO for a fair comparison. For the mobile device control tasks, we compare our VSC-RL with various SOTA baselines including prompting-based agents (Set-of-Marks (Yang et al., 2023b) and AppAgent (Zhang et al., 2023)), imitation-based agents (AutoUI (Zhang & Zhang, 2023), CogAgent (Hong et al., 2024) and Filtered BC (Pan et al., 2024b)) and RL-based agents (DigiRL (Bai et al., 2024)). In MiniGrid, each method is evaluated across 5 random seeds. For AitW tasks, each method is tested on 3 independent runs, consistent with existing works."}, {"title": "5.2. Experimental Results and Analysis", "content": "Toy vision-language tasks on MultiRoom. Overall, as shown in Figure 3, our VSC-RL outperforms the baseline in all tasks remarkably, especially in the difficult task with the increasing number of rooms. From the result of MultiRoom-N2-v0 shown in Figure 3(a), we can tell that although PPO and VSC-RL both successfully reach 100% success rate, our VSC-RL shows a better sample efficiency. For MultiRoom-N4-v0 (Figure 3(b)) and MultiRoom-N6-v0 (Figure 3(c)) where PPO is not able to learn any useful policy, while VSC-RL exhibits strong performance of 100% and 80% success rate respectively.\nGeneral and Web Shopping. The learning curves of AitW General and Web Shopping are summarized in Figure 4. Overall, our VSC-RL outperforms other baselines significantly in both General and Web Shopping tasks. The RL-based agents (DigiRL and VSC-RL) both show leading performance in the AitW General task. After reaching a similar performance of 0.65 success rate with DigiRL in 250 trajectories, our VSC-RL outperforms all baselines significantly, arriving at the best final performance of 0.75 success rate. Similarly, RL-based agents (DigiRL and VSC-RL) dominate all other types of agents remarkably in the Web Shopping task. Specifically, our VSC-RL can finally achieve around 0.6 success rate significantly outperforming 0.5 success rate of DigiRL.\nGeneralization Evaluation. We also evaluate the generalization of our VSC-RL on train and test datasets including a range of unseen tasks, respectively. The results summarized in Section 5.2 tell us that our VSC-RL shows significant superiors in both train and test datasets. Especially, in the general tasks, VSC-RL performs approximately 13.9% and 7.7% better than the second best baseline on the train and test datasets, respectively. Similarly, VSC-RL also achieve the best performance on both the train and test datasets of web shopping tasks. Overall, VSC-RL can exhibit consistent performance on new tasks, showing remarkable generalization ability.\nImprovement of Subgoal Generator. We investigate the importance of the subgoal generator in our VSC-RL on the Web Shopping subsets with different horizon lengths (short, medium and long). We implement VSC-RL with the original goal instead of the subgoals generated from VLM. As shown in Figure 5, the subgoal generator can effectively improve the performance across all types of Web Shopping tasks via autonomously decomposing the original goal to the subgoals. Especially, the subgoal generator can effectively enhance the 50% and 32% performance in the Web Shopping medium and long tasks, respectively.\nVerification of Subgoal Generator. To investigate the quality and feasibility of the generated subgoals, we manually verify the results of subgoal generator on 135 trajectories from the AitW human demonstration. There are 135(100%) goals are decomposed into feasible subgoals successfully, and the final goal can be accomplished by reaching these subgoals sequentially. Specifically, there are 123(91.1%) goals that are decomposed into subgoals completely aligning with the human demonstration. For the remaining 12(8.9%) goals, the subgoal generator provides alternative subgoals different from human demonstration, but still can successfully arrive at the final goal."}, {"title": "5.3. Limitations and Challenges", "content": "We have empirically demonstrated that our VSC-RL can effectively address the learning efficiency issue commonly existing in complex sequential decision-making tasks. However, there still exists some limitations and challenges in VSC-RL as discussed as follows.\nFine-tuning VLM as Subgoal Generator. Benefiting from the general reasoning ability of the proprietary VLM, we empirically found that the performance of VSC-RL is improved by the feasible subgoals. However, for the control task from a specific domain, it is worth fine-tuning the open-source VLM as the subgoal generator."}, {"title": "Hierarchical RL Approaches.", "content": "Additionally, the VLM in VSC-RL can not only be viewed as the subgoal generator, but also as the high-level policy in the context of the hierarchical RL. It is valuable to investigate how to enhance VSC-RL with the advanced hierarchical RL approaches."}, {"title": "Future Challenging Applications.", "content": "In this work, we mainly consider the mobile device control task, a representative complex control problem as the evaluation benchmark. The theoretical and empirical results presented in this work imply that VSC-RL has great potential in addressing other challenging open problems such as embodied agent learning and real robotics learning."}, {"title": "6. Conclusion", "content": "This work investigates vision-language agents in resolving real-world complex sequential decision-making tasks. Existing promising RL-based agents often suffer from the learning efficiency issue in solving tasks with complicated goals and sparse reward signals. To address this issue, we propose VSC-RL, which can autonomously decompose the goal to subgoals and resolve them efficiently. Formulating the sequential decision-making task as a variational subgoal-conditioned RL problem with the optimization objective of SGC-ELBO. We also provide a theoretical guarantee of the performance of VSC-RL. In various benchmarks, especially in challenging mobile device control tasks, we empirically show that VSC-RL exhibits significant performance improvement and learning efficiency, remarkably outperforming existing methods."}, {"title": "A. Implementation Details", "content": "As shown in Table 2, we summarize VLMs used in VSC-RL. For MiniGrid, we built our VSC-RL on the open repository of babyAI (Chevalier-Boisvert et al., 2019), hyper-parameter settings are listed in Table 3. For AitW tasks, we built our VSC-RL on the open repository of DigiRL (Bai et al., 2024), hyper-parameter settings are listed in Table 4."}, {"title": "B. Theoretical Analysis", "content": "Proposition B.1 (Subgoal-Conditioned Optimization Objective). Given a goal g with corresponding subgoals $\\{s_{g_i}\\}_{i=1}^N$ and a subgoal-conditioned target policy $\\pi$, the objective of\n$\\max_{\\pi} \\mathbb{E}_{\\tau\\sim P(\\tau|g)} [\\log p(\\mathcal{O}|\\tau, g)]$\nis equivalent to the objective of\n$\\max_{\\pi} \\sum_{i=1}^N \\mathbb{E}_{[\\tau_i \\sim P_\\pi (\\tau_i | s_{g_i})]} [\\log p(\\mathcal{O}|\\tau_i, s_{g_i})]$\nProof. We have\n$\\logp(O\\|\\tau,g) \\propto \\exp (\\frac{J(\\tau|g)}{\\alpha}) = \\exp(\\frac{\\sum_{i=1}^N J(\\tau_i,S_{g_i})}{\\alpha}).$\nSo, we have\n$\\mathbb{E}_{\\tau \\sim P(\\tau|g)}[\\log p(\\mathcal{O}|\\tau, g)] \\propto \\mathbb{E}_{\\tau \\sim P(\\tau|g)}[J(\\tau, g)] = \\mathbb{E}_{\\tau \\sim P(\\tau, g)}[\\frac{J(\\tau, g)}{\\sum_{i=1}^N \\Pi_{i=1}^{N_i} P_{\\pi}(\\tau_i, s_{g_i})}]= \\mathbb{E}_{\\tau \\sim \\Pi_{i=1}^{N_i} P_{\\pi}(\\tau_i, s_{g_i})}[\\sum_{i=1}^{N_i} J(\\tau_i, s_{g_i})]= \\sum_{i=1}^N\\mathbb{E}_{\\tau_i \\sim P_{\\pi}(\\tau_i, s_{g_i})}[J(\\tau_i, s_{g_i})].$\nDue to the fact that\n$\\log p(O\\|\\tau_i, S_{g_i}) \\propto \\exp(\\frac{J(\\tau_i, s_{g_i})}{\\alpha}).$\nTherefore, we have\n$\\mathbb{E}_{\\tau \\sim P(\\tau|g)}[\\log p(\\mathcal{O}|\\tau, g)] \\Rightarrow \\sum_{i=1}^N \\mathbb{E}_{\\tau_i \\sim P_\\pi (\\tau_i | s_{g_i})} [\\log p(\\mathcal{O}|\\tau_i, s_{g_i})].$\nProposition B.2 (Subgoal-conditioned Difference Bound). Given goal-conditioned reference policy $\\pi_{ref}$ and subgoal-conditioned target policy $\\pi$, the goal-conditioned KL divergence of a given goal g has the upper bound of subgoal-conditioned KL divergence of corresponding subgoals $\\{s_{g_i}\\}_{i=1}^N$ as followed:\n$KL(p_\\pi(\\tau|g)||P_{\\pi_{\\omega f}}(\\tau|g)) \\leq \\sum_{i=1}^N [KL(p_\\pi(\\tau_i|s_{Gi})||P_{\\pi_{\\eta S}}(\\tau_i|g))].$\nProof. We have\n$P_\\pi(\\tau|g) = p(s_0) \\prod_{i=1}^H P(s_{t+1}|s_t, a_t)\\pi(a_t|s_t, g),$\n$\\Pi_{i=1}^N \\rho_\\pi(\\tau_i s_{gi}).$\n< $ \\rho_\\pi(\\tau_i s_{gi})(i = 1,\u2026\u2026\u2026, N)$"}, {"title": "C. Prompt Example", "content": "We provide the prompt example of the subgoal generator in our VSC-RL for a given goal and corresponding decomposed subgoals in Figure 6 and Figure 7."}]}