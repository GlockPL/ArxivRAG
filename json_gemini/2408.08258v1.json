{"title": "Snuffy: Efficient Whole Slide Image Classifier", "authors": ["Hossein Jafarinia", "Alireza Alipanah", "Danial Hamdi", "Saeed Razavi", "Nahal Mirzaie", "Mohammad Hossein Rohban"], "abstract": "Whole Slide Image (WSI) classification with multiple instance learning (MIL) in digital pathology faces significant computational challenges. Current methods mostly rely on extensive self-supervised learning (SSL) for satisfactory performance, requiring long training periods and considerable computational resources. At the same time, no pre-training affects performance due to domain shifts from natural images to WSIs. We introduce Snuffy architecture, a novel MIL-pooling method based on sparse transformers that mitigates performance loss with limited pre-training and enables continual few-shot pre-training as a competitive option. Our sparsity pattern is tailored for pathology and is theoretically proven to be a universal approximator with the tightest probabilistic sharp bound on the number of layers for sparse transformers, to date. We demonstrate Snuffy's effectiveness on CAMELYON16 and TCGA Lung cancer datasets, achieving superior WSI and patch-level accuracies.", "sections": [{"title": "1 Introduction", "content": "The emergence of whole slide images (WSIs) has presented significant opportuni- ties to leverage machine learning techniques for essential tasks of cancer diagnosis and prognosis [27,29,34]. Nevertheless, integrating contemporary deep learning advancements into these areas faces notable challenges. Primarily, the sheer size of WSIs, with usual dimensions of approximately 150,000 \u00d7 150,000 pixels, ren- ders them unmanageable for processing and training with existing deep learning frameworks on current hardware [30].\nA common strategy to address this challenge is to divide WSIs into smaller patches followed by the application of Multiple Instance Learning (MIL) [11,30, 35, 43, 46, 49]. MIL, a variant of weakly supervised learning, considers instances as elements of sets involving an embedding phase and a pooling operation (MIL- pooling). The embedding phase often employs a pre-trained vision backbone, fre- quently with a self-supervised learning technique applied on the patches, trans- forming these patches into embeddings. Subsequently, MIL-pooling aggregates these embeddings, deriving scores at both the patch and WSI levels [26].\nRecent advancements in WSI classification have achieved significant out- comes but face challenges like data-hungriness, high computational and mem- ory requirements. These issues hinder the deployment and development of deep learning technologies in clinical and research settings. For example, RNN-MIL [6] and HIPT [8] require tens of terabytes of data, while DSMIL [31] and DGMIL [42] require several months for pre-training phases. DTFD-MIL [54] uses an efficient MIL-pooling strategy but demands over 100 gigabytes of system memory for training, which is only feasible in some settings. Conversely, the absence of pre- training or insufficient pre-training degrades performance because of the domain shift from natural image datasets such as ImageNet-1K to WSIS (CLAM [33] and KAT's [56] low AUC as shown in Fig. 1).\nThis work presents an approach that significantly reduces the computational demands required for training the embeddings by orders of magnitude. Then, empower its expressivity in a novel MIL-pooling to compensate for performance loss due to limited pre-training. Snuffy makes continual few-shot pre-training possible and a competitive option in this field by balancing efficiency and per- formance.\nOur framework comprises two key components. First, we propose using self- supervised continual pre-training with Parameter Efficient Fine Tuning (PEFT) in the pathology domain, specifically utilizing Adaptformer [9] due to its effective and straightforward design. While PEFT has been applied in various fields, its use in pathology imaging is novel. Our results indicate that transition- ing from natural images to histopathology is feasible, allowing us to leverage PEFT methods effectively.\nSecond, inspired by the complex biology of cancer and the importance of the tissue microenvironment in cancer detection, we introduce the Snuffy MIL- pooling architecture, which features a new sparsity pattern for sparse trans- formers. We demonstrate that the Snuffy sparsity pattern acts as a universal approximator, with the number of layers constrained to a linear relationship"}, {"title": "2 Related Work", "content": "Parameter-Efficient Fine-Tuning for Vision Parameter-Efficient Fine-Tuning (PEFT), initially successful in NLP [24, 25], especially with Transformers [22, 24, 25, 40], has recently been applied to Vision Transformers (ViTs) for super- vised tasks [9, 19, 47]. Techniques like Adapters [24] and LoRA [25] help mit- igate overfitting during fine-tuning. The high computational demands for self-supervised pre-training, along with SSL pre-training benefits on domain-specific datasets [31], highlight PEFT's potential in SSL contexts. Recent studies [14,55] have proposed continual pre-training from general datasets like ImageNet-1K to domain-specific ones. Our study is the first to apply this approach specifically from ImageNet-1K to pathology datasets, advancing the field.\nMIL for WSI Classification The MIL-pooling operator must be permutation- invariant [26]. Traditional methods like Mean-pooling and Max-pooling have had some success, but parameterized strategies are more effective [26, 31]. Recent"}, {"title": "3 Background", "content": "3.1 Notation\nFor any positive integer a, we represent the set as [a] = {1,2, ..., a}. If we have a matrix A \u2208 Rdxn, we refer to its j-th column as Aj, and As denotes the submatrix comprising columns of A indexed by \u2286 [n]. The softmax operator \u03c3[. ] processes each column of a matrix, resulting in a column stochastic matrix. In terms of asymptotic behavior, f = O(g) implies that there exists a con- stant C > 0 such that f(n) < Cg(n) for all but finitely many n. Conversely, f = \u03a9(g) signifies that g = O(f). We use X to conceal poly-logarithmic factors of X, like 2(n).\n3.2 MIL Formulation\nIn a binary classification problem, the dataset D = {(X1,Y1), ..., (Xn, Yn)} con- sists of bags X, where each bag X = {X1,...,Xk} contains instances x, and Y \u2208 {0, 1} represents the bag's label. The individual instance labels (y1,..., Yk} with with y \u2208 {0,1}, are unknown during training. This is modeled as:"}, {"title": null, "content": "Y = { 0,  iff \u2211i Yi = 0 1,  otherwise.(1)\nor equivalently:\nY = max{y}.(2)\nTo address the complexities in learning, to navigate this, it is proposed to train the MIL model by optimizing the log-likelihood function:\nP(Y|X) = \u03b8(X)Y (1 \u2013 \u03b8(X))1\u2212Y,(3)\nwhere \u03b8(X) \u2208 [0, 1] is the probability of Y = 1 given X.\nGiven that MIL assumes no ordering or dependency of instances, \u03b8(X) must be a permutation-invariant (symmetric) function. This is achieved through the Fundamental Theorem of Symmetric Functions, with monomials [53] and a sim- ilar Theorem by [41] leading to:\n\u03b8(X) = g(\u03c0(f(x1), ..., f(xk))),(4)\nwhere f and g are continuous transformations, and \u03c0 is a permutation- invariant function (MIL-pooling). There are two approaches, the instance-level approach where f is an instance classifier and g is identity function, and the embedding-level approach, where f is a feature extractor and g maps its input to a bag classification score. The embedding-based approach is preferred due to its superior performance [26].\nIn Deep MIL, f typically uses pre-trained vision backbones to extract features from bag instances [31,33, 42, 45,54,56]. The aggregation function \u03c0 ranges from non-parametric methods like max-pooling to parametric ones using attention mechanisms, as detailed in Section 2. Finally, g is often implemented as a linear layer with o to project aggregated instance representations into a bag score.\nFor multiclass classification, g's output dimension is adjusted to the number of classes, and the softmax function is used instead of \u03c3 to distribute probabil- ities across classes."}, {"title": "3.3 Sparse Transformers", "content": "In the full-attention mechanism, each attention head considers interactions be- tween every patch in an image. This requires significant memory and compu- tational resources, with complexity scaling quadratically with the number of patches. However, in a sparse attention mechanism, each attention head only attends to a particular subset of patches instead of the entire set of patches. Drawing upon the formalism presented in [50], the ith sparse attention head output for a patch k in the layer l is articulated as follows:\nSHeadi,l(X)k = WAVA\u03c3(WKWQXk),(5)"}, {"title": null, "content": "When calculating the attention scores, the query vector of the ith head, WoXk of the kth patch interacts exclusively with the key vectors WKWXA from patches belonging to its specific subset, A. This means that attention is computed only between the kth patch and the patches in its assigned subset. Consequently, the output of each attention head for the patch k, SHeadi,l(X)k is a result of combining columns from the patch representations WVA within its assigned subset, rather than considering the entire sequence of patches [50]. The collection of these subsets A across all layers l \u2208 [L] and patches k \u2208 [n], is termed the sparsity patterns.\nSparse attention mechanisms significantly reduce the time and memory com- plexities of transformers. However, this efficiency comes at the expense of loss of accuracy in attention matrix predictions depending on how sparse the sparsity patterns are. Expanding beyond this limitation, prior studies have identified di- verse sparse patterns with O(n) connections (compared to O(n\u00b2) in full-attention mechanisms), effectively approximating any full attention matrices [4,12,21,52]. Notably, [50] established adequate conditions to ensure that any collection of sparsity patterns adhering to these criteria, alongside a probability map such as softmax, can serve as a universal approximator for sequence-to-sequence func- tions (Theorem 1).\nTheorem 1. A sparse transformer with any set of sparsity patterns {A} sat- isfying these conditions:\n1. \u2200k \u2208 [n], \u2200l \u2208 [L], k\u2208 {A}\n2. There is a permutation \u03b3 such that \u2200i \u2208 [n \u2212 1], \u03b3(i) \u2208 \u222al=1U{A(i+1)}\n3. Each patch attends to every other patch, directly or indirectly.\ncoupled with a probability map generating a column stochastic matrix that closely approximating hardmax operator, is a universal approximator of sequence-to- sequence functions [50].\nPut simply, criterion 1 mandates that each patch attends to itself. This con- dition guarantees that even if contextual attention embeddings are not computed within the self-attention framework for a patch, the patch's original embedding remains intact in the output. Additionally, criteria 2 and 3 state that when all patterns in a set of sparsity patterns are combined and an adjacency matrix is created, the resulting graph G, possesses both a Hamiltonian path and strong connectivity. From now on, we refer to any sparsity patterns that meet the con- ditions outlined in Theorem 1 as universal approximator sparsity patterns."}, {"title": "4 Method", "content": "4.1 Continual Few-shot Efficient Self-Supervised Pre-training\nTo avoid extensive training on large domain-specific datasets, we propose using continual few-shot self-supervised pre-training with AdaptFormer [9] on ViTs pre-trained on ImageNet-1K [55]."}, {"title": "5 Universal Approximation of Snuffy", "content": "In this section, we demonstrate that our sparse transformer serves as a universal approximator for sequence-to-sequence functions. By applying Theorem 1 and validating the Snuffy sparsity patterns defined in Definition 1, we confirm that our transformer, utilizing softmax as the probability map, satisfies all condi- tions given in the theorem. Furthermore, we illustrate that our transformer does"}, {"title": null, "content": "not necessitate (n) layers, as previously suggested in studies [52]. Instead, it requires only O(nlog2) layers to ensure universal approximation with high prob- ability, achieving the most stringent probabilistic limit of the layer count to our knowledge.\nSnuffy sparsity patterns unquestionably satisfy the criteria 1 and 3 outlined in Theorem 1. The first condition is fulfilled through the inclusion of k \u2208 A, as defined in Definition 1. Moreover, the presence of at least one global attention patch within the patterns ensures connectivity among all patches, with a maxi- mum path length of 2, thus meeting condition 3.\nTo satisfy the criterion 2, we must demonstrate the existence of a Hamiltonian path in the graph corresponding to the union of patterns in the Snuffy sparsity patterns. Initially, we introduce Proposition 1 from graph theory to facilitate the proof. We employ the proposition and demonstrate that covering half of the patches in all layers, overall satisfies its properties. This leads to the formation of a Hamiltonian path, thus fulfilling the desired proof.\nProposition 1. Every graph G(E,V) with E and V as the set of edges and nodes, with |V|\u2265 3 and \u03b1(G) \u2264 \u03c7(G) has a Hamiltonian cycle. Where \u03b1(G) is the maximum independent set, and \u03c7(G) is the chromatic number of G.\nProof. see Supplementary Material 1.\nLemma 1. For Gs, the graph representing Snuffy sparsity patterns, we guaran- tee that there exists a Hamiltonian path if\n| \u222ale[L] A | > n\u22121\n2.(7)\nProof. The maximum independent set of patches in Gs is equal to [n]\\\u039b, where A := Ul\u2208[L] A\u00b9 is the set of all covered patches. Conversely, the minimum num- ber of colors needed to color the vertices of Gs is |A|+1. Therefore, to sat- isfy \u03b1(Gs) \u2264 \u03c7(Gs), we must demonstrate that |[n]\\A|>|\u039b|+1, which implies |A|>n\u22121\n2. (for more details see supplementary 1)\nConsidering Lemma 1, we make the keen observation that we only need to ensure that after a finite number of layers, we have covered at least n/2 of the patches. This observation resembles a generalized version of the coupon collector problem, where the goal is to collect half of the coupons in a Ar-uniform group setting. In this scenario, at each step, we can collect \u039br number of cards simultaneously. This leads us to our main theorem:\nTheorem 2. If \u03bbr = O(n), where n is number of patches, and \u039br = |\u039br|, then the number of layers L needed to prove that the Snuffy sparsity pattern (defined in 1) is a universal approximator sparsity pattern is concentrated around nlog2 . More precisely, we have:\nlim n\u2192\u221e P(L < c\u221an nlog2 \u03bbr ) \u2192 1 \u2013 \u0424(c)(8)"}]}