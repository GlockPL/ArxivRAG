{"title": "Can adversarial attacks by large language models be attributed?", "authors": ["Manuel Cebrian", "Jan Arne Telle"], "abstract": "Attributing outputs from Large Language Models (LLMs) in adversarial settings such as cy-berattacks and disinformation presents significant challenges that are likely to grow in importance. We investigate this attribution problem using formal language theory, specifically language identi-fication in the limit as introduced by Gold and extended by Angluin. By modeling LLM outputs as formal languages, we analyze whether finite text samples can uniquely pinpoint the originating model. Our results show that due to the non-identifiability of certain language classes, under some mild assumptions about overlapping outputs from fine-tuned models it is theoretically impossible to attribute outputs to specific LLMs with certainty. This holds also when accounting for expressivity limitations of Transformer architectures. Even with direct model access or comprehensive monitoring, significant computational hurdles impede attribution efforts. These findings highlight an urgent need for proactive measures to mitigate risks posed by adversarial LLM use as their influence continues to expand.", "sections": [{"title": "Introduction", "content": "The challenge of attributing outputs from LLMs in the context of adversarial attacks or disinformation campaigns is emerging as a concern for both cybersecurity and information integrity [3, 9, 21, 17, 24]. In this context, attribution involves identifying the specific model responsible for generating harmful outputs. This step is essential not only for conducting further investigations and determining if the implicated model should be restricted or decommissioned but also for mitigating future risks and ensuring accountability in the deployment of LLM-based agents [18, 19, 2].\nThis attribution task is, interestingly, closely related to formal language theory, particularly the problem of language identification in the limit. This theory, introduced by Gold [10] and extended by Angluin [1], has a rich body of work spanning theoretical computer science and cognitive science [11]. Language identification provides a formal framework for understanding how, given a set of outputs, one can determine the source language\u2014or, in this context, the specific LLM responsible for generating those outputs. By framing LLM outputs as formal languages, this theory offers a structured approach to explore the feasibility of attribution.\nWe can represent the outputs of an LLM as strings over a finite alphabet, where the set of all possible outputs defines a formal language. Thus, the attribution problem can be framed as determining whether there exists a unique LLM whose generated language includes the observed outputs.\nTo formalize this, we start by revisiting the concept of language identification in the limit, which underlies this notion of unique model identification.\nDefinition 1 (Gold's Identification in the Limit). A class of languages L is said to be identifiable in the limit if there exists a learning algorithm A such that, for any target language L \u2208 L, given an infinite sequence of examples(S1,S2,...) where each si \u2208 L and each string in L appears at least once, the algorithm A produces a sequence of hypotheses (H1, H2,...) satisfying:\n(1) For all but finitely many n, Hn = L.\n(2) Each Hn \u2208 L is consistent with the observed data up to time n, i.e., {81, 82, ..., Sn} \u2286 Hn.\nThis framework was extended by Angluin, who provided necessary and sufficient conditions for iden-tification from positive data."}, {"title": "Angluin's Theorem and Its Implications", "content": "Theorem 1 (Angluin's Theorem). An indexed family of recursive languages {Li}ien is identifiable in the limit from positive data if and only if there exists a recursively enumerable set of finite subsets {Ti}i\u2208N such that:\n(1) For each i, Ti \u2286 Li.\n(2) For all i \u2260 j, Ti \u2286 Lj implies L\u2081 = Lj.\nHowever, there are classes of languages where these conditions are not met, leading to non-identifiability in the limit. Specifically, this holds for a class C with an infinite language L\u221e \u2208 L such that for every finite subset S CL\u221e, there exists a language Lj \u2208 L where S \u2286 Lj C L\u221e. Note the latter inclusion is strict. This result is formalized in the following corollary of Angluin's Theorem [1].\nCorollary 1 (Non-Identifiability Due to Infinite Languages). Let L be a collection of recursively enu-merable languages satisfying:\n(i) L contains an infinite language Lo.\n(ii) For every finite subset S C L\u221e, there exists L \u2208 L such that S \u2286 L CL\u221e.\nThen, L is not identifiable in the limit.\nProof. Assume, for contradiction, that L is identifiable in the limit. By Angluin's Theorem, there must exist a finite tell-tale set TL CL\u221e that distinguishes L\u221e from all its proper subsets in L. However, condition (ii) ensures that for every finite TL, there exists L\u2208 L such that TL\u221e \u2286 L CL\u221e. This contradicts the existence of a tell-tale set that uniquely identifies L\u221e. Therefore, L is not identifiable in the limit.\nFrom this corollary, it is easy to construct a very simple class of languages, over an alphabet consisting of a single character, that is not identifiable in the limit.\nObservation 1. For each positive integer k, let Lk be the set of all strings of length at most k over the alphabet \u2211 = {x}, and let L = {Lk}k\u2208NU {L}, where L\u221e \u03a3* is the set of all finite strings over \u03a3. Then L is not identifiable in the limit.\nProof. Take any finite subset S C L\u221e and let the longest string in S have length k. Then SC Lk CL\u221e with Lk \u2208 L, so the conditions of Corollary 1 apply."}, {"title": "Attribution Algorithm", "content": "In the context of LLM attribution, the family of languages L corresponds to the languages generated by a set of LLMs M = {M1, M2,...}. Each model Mi defines a language L(M\u2081) \u2286 \u03a3*, where L(M\u2081) consists of all sequences that Mi can generate with non-zero probability. The goal is to determine whether a finite set of observed outputs S \u2264 \u2211* can uniquely identify a specific model Mk such that S \u2286 L(Mk).\nDefinition 2 (Attribution Algorithm). An Attribution Algorithm for a set of LLMs M is a procedure that takes as input a stream of observed outputs S = (81,82,...) generated by a single LLM Mk \u2208 M and outputs a sequence of hypotheses\u3008H1, H2,...) where each Hn \u2208 M\u222a{unknown} represents the algorithm's guess of which model generated the outputs observed up to timen. The algorithm satisfies the Identification in the Limit property if there exists a time point t such that for all n \u2265 t, Hn = Mk. In other words, after observing sufficient outputs, the algorithm will consistently and correctly identify the generating model.\nThis reframes the problem of LLM attribution as designing an algorithm capable of identifying unique, model-specific subsets within observed outputs S to reliably determine the generating model. Angluin's result implies that for attribution to be feasible, a finite set of outputs a tell-tale set must uniquely identify each LLM's language. Importantly, this means that the attacker must eventually produce all elements of the language, including the tell-tale set, as we are considering identification in the limit. How-ever, while such tell-tale sets may theoretically exist, Angluin's framework does not provide a constructive"}, {"title": "Theoretical Considerations and Limitations", "content": "or practical method for discovering them. The Attribution Algorithm may need to enumerate all pos-sible subsets T\u2081 and verify their uniqueness against observed data, an approach that is computationally infeasible given the vast number of models and their fine-tuned variants (as explored below).\nWhile these theoretical considerations suggest challenges in attributing outputs to specific models, one might consider the inherent expressivity limitations of Transformer architectures which underlie most LLMS as a potential aid in attribution. These limitations, as discussed by Strobl et al. [20], restrict Transformers' ability to represent certain language classes, especially those requiring unbounded counting, or hierarchical structures, such as the Dyck languages of balanced parentheses and parity-checking over binary strings [4, 13]. Additionally, Peng et al. [16] highlight Transformers' limited capacity for recursive function composition, essential for languages with deep structural dependencies. These expressivity constraints might reduce the overlap in output distributions across models, potentially aiding attribution by introducing more distinct and bounded patterns in LLM outputs.\nHowever, despite these expressivity limitations, fine-tuning can create significant overlaps in model outputs, complicating attribution. Specifically, under a mild assumption about the power of fine-tuning the impossibility of attribution satisfying Identification in the Limit can be proven formally.\nObservation 2 (Fine-Tuning Leading to Non-Identifiability). Assume there exists an LLM M\u221e with infinite language such that for any finite subset S C L(M\u221e), we can fine-tune the base model M\u221e to produce a model Ms where S \u2286 L(Ms) \u2282 L(M\u221e). In that case, attribution satisfying identification in the limit is not possible for the class of LLMs M = {Ms | SCL(M\u221e) \u2229 |S| < \u221e}\u222a{M}.\nProof. Consider the class of LLMs M consisting of M\u221e and, for each finite subset S of L(M), the fine-tuned model Ms. Note that the class of languages L = {L(M) : M \u2208 M} will then satisfy the conditions of Corollary 1:\n(i) L contains the infinite language L(M\u221e).\n(ii) For every finite subset SCL(M\u221e), there exists L(Ms) \u2208 L such that S \u2286 L(Ms) CL(M\u221e).\nTherefore, by Corollary 1, L is not identifiable in the limit, and thus attribution with identification in the limit is not possible for M."}, {"title": "Computational Feasibility", "content": "In a different scenario, we may have direct access to a majority of the LLMs Mi themselves, enabling us to compute the likelihood of an observed output s under each model Mi. Using these likelihoods one can hope to design attribution algorithms with a probabilistic flavor. However, evaluating the likelihood PM; (s) of an observed output s under each available LLM M\u2081 involves significant computational resources.\nWe conducted our analysis using a dataset representing the ecosystem of LLMs [6] to understand the growth in model sizes over time and evaluate the computational feasibility of attribution calculations based on likelihood. The dataset consists of several LLMs developed over recent years, detailing key parameters including model type, creation date, and parameter size in billions, including a total of 271 models with known sizes (i.e., parameter counts).\nTo evaluate the practicality of running attribution calculations across all models, we consider a hy-pothetical adversarial attack involving a 100,000-token output. For each model, we need to calculate the log-probabilities of these tokens to estimate the likelihood of the output originating from that model. Given the cumulative parameter size across all models, the total floating-point operations (FLOPs) re-quired for this calculation grows significantly.\nWith the current cumulative model size in our dataset, processing a 100,000-token adversarial at-tack would require approximately 8.7 \u00d7 1020 floating-point operations (FLOPs). To assess the feasi-bility of such a task, we compare this with the capabilities of the Frontier supercomputer-the world's fastest achieving a peak of 1.7 exaFLOPS (1.7\u00d71018 FLOPs per second) [15]. Using Frontier's capacity,"}, {"title": "National Monitoring Framework", "content": "a single attribution calculation across 100,000 tokens could theoretically complete in around 8 minutes for a single attack, underscoring the difficulty of scaling attribution in an expanding landscape of LLMs.\nIn the final analysis, we assess the feasibility of attributing adversarial outputs to specific LLMs within a comprehensive, national monitoring framework based on U.S. usage data a extreme scenario. Using realistic simulations and data from Bick et al. (2024) [5], we assume the total U.S. population is approximately 334 million, with 39.4% of adults actively using LLMs daily. This results in an estimated 131.6 million daily users. With an average generation rate of 10,000 tokens per user per day, the annual token volume reaches approximately 4.8 \u00d7 1015 tokens. Assuming a data footprint of 4 bytes per token, this volume equates to about 17.45 petabytes of data generated annually.\nProcessing this extensive dataset demands significant infrastructure. Using a high-performance com-puting setup with 10,000 cores and estimating each record requires approximately 10 microseconds per token, the baseline processing time to analyze the year's data is approximately 132.8 hours, assuming optimal parallel processing.\nWhile the core processing time provides a baseline, real-world conditions introduce additional delays due to data aggregation, preprocessing, and network latency. Assuming an overhead for data collection and preprocessing, and applying a multiplier of 2 to account for additional logistical complexities, the total estimated time for attribution increases to approximately 265.6 hours (or roughly 11 days) to attribute a single adversarial output. Table 1 highlights that even under idealized conditions, attributing a single adversarial output from vast data volumes is a complex and resource-intensive task."}, {"title": "Network Dynamics and Conclusion", "content": "Beyond these challenges of direct identifiability, also issues stemming from network dynamics [14, 8] impact attribution feasibility. In many real-world cases, harmful content or attacks propagate through complex networks, complicating efforts to trace them back to their origin. Attackers can exploit these network structures by rewiring connections or introducing counterfeit nodes, leveraging the diffusion properties of networks to evade detection and mask their identity [22, 23, 7].\nFurthermore, Kleinberg and Mullainathan's surprising findings [12] reveal that while attribution re-mains challenging, generation in the limit is in fact achievable. In a framework similar to Definition 1, their work shows that it is possible for a computational agent to generate new, valid strings from a target language without identifying it explicitly an ability fundamentally different from language identification- as it bypasses the need for complete model attribution. This capability suggests that, theoretically, defenders could match adversarial capabilities by reproducing attack outputs in the limit, creating a scenario where attackers and defenders are locked in a cycle of confusion, unable to find the true origin, and mutual escalation.\nWhether with limited access to adversarial outputs, direct access to models, or even within a com-prehensive monitoring framework, attributing specific outputs to individual LLMs remains profoundly challenging. As LLMs become increasingly powerful and widespread, the complexities of attribution raise significant concerns. This underscores the urgent need for robust safety protocols and regulatory measures to mitigate risks before LLMs are made broadly accessible to the public."}]}