{"title": "MINDFORGE: EMPOWERING EMBODIED AGENTS WITH THEORY OF MIND FOR LIFELONG COLLABORATIVE LEARNING", "authors": ["Mircea Lic\u0103", "Ojas Shirekar", "Baptiste Colle", "Chirag Raman"], "abstract": "Contemporary embodied agents, such as Voyager in Minecraft, have demonstrated promising capabilities in open-ended individual learning. However, when powered with open large language models (LLMs), these agents often struggle with rudimentary tasks, even when fine-tuned on domain-specific knowledge. Inspired by human cultural learning, we present MINDFORGE, a novel framework that enhances Voyager with lifelong collaborative learning through explicit perspective-taking. MINDFORGE introduces three key innovations: (1) theory of mind representations linking percepts, beliefs, desires, and actions; (2) natural language communication between agents; and (3) semantic memory of task and environment knowledge and episodic memory of collaboration episodes. These advancements enable agents to reason about their and others' mental states, empirically addressing two prevalent failure modes: false beliefs and faulty task executions. In mixed-expertise Minecraft experiments, MINDFORGE agents outperform Voyager counterparts, significantly improving task completion rate by 66.6%(+39.4%) for collecting one block of dirt and 70.8%(+20.8%) for collecting one wood block. They exhibit emergent behaviors like knowledge transfer from expert to novice agents and collaborative code correction. MINDFORGE agents also demonstrate the ability to adapt to out-of-distribution tasks by using their previous experiences and beliefs obtained through collaboration. In this open-ended social learning paradigm, MINDFORGE paves the way for the democratic development of embodied AI, where agents learn in deployment from both peer and environmental feedback.", "sections": [{"title": "1 INTRODUCTION", "content": "The development of generally capable agents marks a significant shift in advancing artificial intelligence, transitioning from assimilating data to generating novel knowledge through embodied interactions with open-ended environments (Kolve et al., 2017; Savva et al., 2019; Puig et al., 2018; Shridhar et al., 2020). Classical approaches leveraging reinforcement learning (Schulman et al., 2017; Hafner et al., 2023) and imitation learning (Zare et al., 2024) often struggle with generalization and exploration, as agents tend to converge on repetitive behaviors in static environments (Cobbe et al., 2019). To address these limitations, researchers have sought to emulate human-like lifelong learning capabilities, developing systems that can continuously acquire, update, and transfer knowledge over extended periods (Parisi et al., 2019; Wang et al., 2023b). The advent of large language models (LLMs) has accelerated this pursuit, enabling the development of agents such as Voyager (Wang et al., 2023a) that can apply internet-scale knowledge to continuously explore, plan, and acquire new skills in partially observable, open-ended environments such as Minecraft.\nDespite their promise, we argue that state-of-the-art lifelong learning agents like Voyager face a crucial limitation: they learn in isolation, neglecting a fundamental aspect of human intelligence the social context. So central is the social context to our existence, that the Social Intelligence Hypothesis posits that our cognitive capabilities evolved primarily to navigate the complexities of social life (Humphrey, 1976; Dunbar, 1998). This isolated learning becomes particularly problematic when coupled with these agents' reliance on closed LLM) like GPT-4. Wang et al. (2023a) note that \u201cVOYAGER requires the quantum leap in code generation quality from GPT-4 which GPT-3.5 and open-source LLMs cannot provide\". Our experiments (Sec. 5) corroborate this observation, demonstrating that replacing"}, {"title": "2 RELATED WORK", "content": "Theory of Mind in Embodied Agents There have been numerous attempts to create mentalizing abilities in machines and agents over time. Before the rise in popularity of Large Language Models, the most prominent approaches included deep learning methods (Rabinowitz et al., 2018; Chuang et al., 2020; Shu et al., 2021), reinforcement (RL) learning algorithms (Grassiotto & Costa, 2021) and probabilistic techniques (Baker et al., 2011). These methods either consider simple environments like 2D grid worlds (Rabinowitz et al., 2018; Chuang et al., 2020) or fail to generalize in more complex settings (Shu et al., 2021). Currently, given the effectiveness of Large Language Models there have been numerous studies on their ToM capabilites (Kosinski, 2023; Bubeck et al., 2023; Ullman, 2023; Gandhi et al., 2024). While the general consensus is that these models do not exhibit"}, {"title": "LLM-based Multi-Agent Systems", "content": "LLMs have demonstrated overwhelming success in performing a variety of single-agent tasks ranging from question answering and text generation to code synthesis and complex reasoning in embodied settings (Wang et al., 2023a; Ahn et al., 2022; Lin et al., 2023). This success has started a growing interest in developing LLM-based multi-agent systems (Zhang et al., 2024; 2023; Du et al., 2023; Talebirad & Nadiri, 2023; Vezhnevets et al., 2023; Talebirad & Nadiri, 2023), where language-based communication is a central component. For instance, Zhang et al. (2023) introduce a modular multi-agent framework that integrates memory, perception and execution, while Zhang et al. (2024) present a multi-agent cooperative framework, COMBO, that deals with collaboration in partial egocentric views of the environment. However, challenges remain in ensuring coordination and consistency among agents, particularly in embodied environments."}, {"title": "3 BACKGROUND", "content": "Voyager Voyager agents consist of three main components: (1) an automatic curriculum generator, (2) a skill library, and (3) an iterative prompting mechanism. The automatic curriculum proposes progressively complex tasks, driving continuous learning. The skill library stores and retrieves learned behaviors, facilitating knowledge accumulation and transfer. The iterative prompting mechanism generates and refines executable code based on environmental feedback, allowing for adaptive behavior. Voyager demonstrated significant capabilities in item discovery, technology progression, and skill generalization. This system established a new benchmark in open-world AI agents, showcasing the potential for lifelong learning and adaptation in complex, dynamic environments. The Voyager framework serves as a base for our lifelong collaborative learning agents within Minecraft."}, {"title": "BigToM Causal Template", "content": "The BigToM causal template (Gandhi et al., 2024), provides a structured framework for representing Theory of Mind (ToM) scenarios as causal graphs. This template comprises key components including context, desire, percept, belief, causal event, and resulting actions. By representing ToM scenarios in this causal structure, the template allows for systematic manipulation of variables such as an agent's perceptions, beliefs, and actions in response to environmental changes. This approach enables the creation of diverse, well-controlled ToM scenarios, capturing the complex causal relationships between an agent's mental states and their behavior in social situations. The causal graph representation facilitates a more nuanced and systematic exploration of ToM reasoning, allowing for the examination of various inference patterns such as forward belief, forward action, and backward belief inferences in social cognitive tasks."}, {"title": "4 METHOD", "content": "To grant MINDFORGE agents the ability to learn through collaborative social interactions, we introduce the following components for communication and perspective-taking: (i) an explicit structured representation linking percepts, beliefs, desires, and actions (section 4.1), (ii) perspective taking so that the agent can reason over self-beliefs and those about its collaborators (section 4.2), (iii) a communication medium that the agents can use to share information in natural language (section 4.3), and (iv) the ability to use and exploit different types of memory subsystems (section 4.4)."}, {"title": "4.1 EXPLICIT THEORY OF MIND REPRESENTATION", "content": "For MINDFORGE agents to develop effective theory of mind capabilities, they must first possess structured representations of their own mental states. This explicit modeling of the self is crucial for recursive social reasoning: enabling agents not only to reason about their own beliefs, goals, perceptions, and how these might lead to their actions, but also to form a similar mental model for other agents. This mirrors findings in developmental psychology, where children first develop an awareness of their own thoughts, feelings, and goals before being able to infer the mental states of others (Wellman & Woolley, 1990; Perner, 1993). This is further supported by neuroscientific research that has also identified distinct neural mechanisms for underlying self-cognition versus mentalizing about others (Ochsner et al., 2004; Saxe & Kanwisher, 2003).\nDrawing on these findings, MINDFORGE agents employ a causal world representation based on the Belief-Desire-Intention (BDI) framework (Rao et al., 1995). Beliefs correspond to our agent's current understanding of the world inferred from sensory perception and all information related to tasks and interactions with other agents. Desires align with the agent's goals, both those generated by the automatic curriculum and those arising from collaborative interactions. In our framework, intentions manifest as concrete plans of action, specifically implemented as generated code that executes the agent's desired tasks. Given this one-to-one correspondence between an intention and its programmatic implementation, we treat the terms \u201cintentions\" and \"actions\" as equivalent in our discussion. We operationalize the Beliefs, Desires, and Intentions by leveraging the BigToM causal template (section 3) (Gandhi et al., 2024).\nFurther, a MINDFORGE agent's beliefs are composed of four key categories: task, interaction, perceptions, and partner perspective as illustrated in Figure 3. Task-related beliefs stem from the agent's self-reflection of how to approach given objectives. Interaction-related beliefs are derived from chat messages received during the execution of the task. Perception-related beliefs are formed from direct sensory input via the Minecraft API and partner feedback. Together, these Beliefs influence the actions of the agents (Figure 2a)."}, {"title": "4.2 \u03a4\u0391\u039a\u0399NG PARTNER PERSPECTIVE", "content": "Having furnished our agents with the ability to represent beliefs, desires, and intentions, we now focus on enabling collaborative reasoning through perspective-taking. Each MINDFORGE agent achieves this by maintaining within its beliefs a mental model of its partners' states, represented by a full BigTom causal graph for each partner. When deciding upon an action or communication response, the underlying LLM is prompted to use these beliefs about the partner states. A part of this prompt is as follows (see the Appendix C for the full set of prompts):\nPerspective Taking Prompt\nAsk questions such that you get a complete view of the other agent's world model. When you believe you have enough information, you can give helpful advice to the other agent. Before generating a message, create a mental model of the other agent's world model based on the information you have received so far by following the structure above. At each turn, ask questions that help you understand the other agent's world model better. Try to extract as much information from him as possible while offering helpful advice based on your own world model. If the other agent has an opinion that contradicts your own world model, assume your world model is correct and provide the other agent with the correct information.\nMaintaining and continuously updating these beliefs for each partner endows MINDFORGE agents with one of the most crucial abilities of the cultural learning framework: reasoning about partner perspectives (Tomasello et al., 1993; Tomasello, 2016). For example, if an expert agent recognizes"}, {"title": "4.3 \u0421\u043e\u043cMUNICATION MEDIUM", "content": "To take advantage of representations of each other's mental models and collaborate effectively, agents need to be able to communicate with each other. We operationalize communication in natural language, integrating the MINDFORGE framework's communication system with Minecraft's native in-game chat functionality. To facilitate agent interaction alongside other actions in the environment, we expanded the original Voyager framework to incorporate an asynchronous messaging system. This system enables agents to exchange textual information and, crucially, to integrate feedback from collaborators directly into their belief systems. Concretely, we utilize the Mineflayer API (PrismarineJS, 2021) to extract and process chat data from the Minecraft environment. By leveraging the game's existing chat system, the framework ensures that collaborative interactions occur in a context-appropriate manner, closely mirroring human-to-human communication within Minecraft.\nTo handle communication, we introduce a dedicated Interaction LLM within the MINDFORGE agent which is tasked with synthesizing and interpreting messages into the structured BigToM BDI template. This filled-in template then constitutes the agent's partner beliefs described in the previous subsection. The Interaction LLM does not directly influence any other parts of the agent."}, {"title": "4.4 \u039c\u0395\u039cORY SUBSYSTEMS", "content": "A MINDFORGE agent's memory consists of three subsystems drawing on research about human memory: (i) episodic, (ii) semantic, and (iii) procedural memory. Episodic memory deals with pertinent information about the agent's past attempts at a task and social interactions with other agents. Recollections of past episodes serve as context for humans to interpret others' behaviors, leading to divergent future actions in the same situation in response to different recollections (Bornstein et al., 2017). Humans also tend to summarize information from their social interactions, but at the same time are capable of preferentially accessing episodes when the credibility of certain summaries is low due to the lack of evidence (Klein et al., 2009). Moreover, findings also suggest that there are processes in the brain for updating these summaries as new episodes transpire Klein et al. (2009; 1993). Inspired by complementary learning systems theory, Larimar (Das et al., 2024) enhances LLMs with a distributed episodic memory module, enabling dynamic knowledge updates without any gradient-based learning. For a MINDFORGE agent, episodic memory plays a similar role as it performs tasks and interacts with its partners; it stores details about its partners revealed during communication rounds that can be exploited for perspective-taking in future communication. Semantic memory constitutes an individual's store of knowledge about the world. The content of semantic memory is abstracted from the experience and is therefore said to hold summarized generalizations, likely without references to specific experiences Binder & Desai (2011); Klein et al. (2009; 1993). For a MINDFORGE agent, the semantic memory stores information related to the task and environment. It stores information relevant to completing a task; for instance, the prerequisites for being able to mine dirt in the game world. Such a piece of information is not necessarily tied to one specific episode. Rather, it is generally useful information about the world and can be distilled from episodic memory. Finally, procedural memory stores the skills that the agent learns and is analogous to the skill library present in the original Voyager framework Wang et al. (2023a). The skills stored in the procedural memory are reusable pieces of code that the agent can use for solving problems, allowing the agent to take on more complex tasks while avoiding re-learning elements previously used successfully. As illustrated in Figure 2a, episodic and semantic memory stores influence the agent's beliefs, while procedural memory provides the routines that inform the agent's next action."}, {"title": "5 EXPERIMENTS", "content": "5.1 EXPERIMENTAL SETUP\nOur MINDFORGE agent framework introduces three key innovations that enable agents to continually evolve through social interactions: (1) explicit theory of mind representations; (2) natural language"}, {"title": "5.2 VOYAGER POWERED BY OPEN-WEIGHT LLMS FAILS ON THE SIMPLEST TASKS", "content": "To establish a concrete performance baseline, we begin by replacing GPT-4 (Achiam et al., 2023) with open-weight LLMs within the Voyager (Wang et al., 2023a) agent. We choose Mistral-7B (Jiang et al., 2023) and Mixtral-8x7B (Jiang et al., 2024) as our open-weight LLMs given their long context window and accessible model size. In order to understand the capabilities of open-weight Voyager, we focus on two simple tasks within the Minecraft tech tree: (i) collecting one block of dirt, and (ii) collecting one block of wood.\nAs results in Table 1 indicate, while GPT-4 demonstrates perfect performance, the open-weight LLMs drastically struggle with the simplest tasks in Minecraft. This supports the observations made by Wang et al. (2023a) in the Voyager paper: \"GPT-4 significantly outperforms GPT-3.5 in code generation and obtains 5.7x more unique items, as GPT-4 exhibits a quantum leap in coding abilities.\"\nPrimary Failure Cases While Voyager (Wang et al., 2023a) only discuss code-generation capabilities, we find two prominent failure cases for the open-weight Voyager setup: (1) false beliefs and (2) faulty code generation. Importantly, we discover that the critic-based self-improvement component in Voyager fails to recover from initial mistakes and subsequently fails to solve the task.\nAs a specific example, Figure 4 showcases how a Mistral-7B powered Voyager fails to collect dirt"}, {"title": "Fine-tuning yields no meaningful improvements", "content": "Given the recent success of parameter efficient fine-tuning methods (Han et al., 2024) in improving the performance of LLMs on downstream tasks, we finetune Mistral-7B on the following data: (i) GPT-4 Voyager execution logs and (ii) the Minecraft Wiki. We specifically focus on these two data domains given the failure cases presented before. We include GPT-4 execution traces to improve the reasoning and code generation of the underlying LLM, while the Minecraft documentation serves as a way to provide the lan-\nguage model with general knowledge about the environment. We include additional details about our fine-tuning setup in Appendix B. From a cultural learning perspective (Tomasello et al., 1993; Tomasello, 2016), we regard fine-tuning as an imitation learning strategy, where an LLM learns to imitate behaviors from some task-specific dataset.\nWe find that the fine-tuned Mistral-7B model does not improve the performance of open-weight Voyager obtaining an accuracy of 7% on collecting dirt, as highlighted in Table 1, indicating that standard imitation learning methods are not effective in these scenarios. We find that fine-tuning fails to perform well due to steering the model towards being more verbose and less effective at decision-making. Moreover, we believe performing fine-tuning on unstructured data sources, such as execution traces, can negatively impact the instruction following capabilities of the model."}, {"title": "5.3 INTRODUCING COLLABORATION: MINDFORGE AGENTS", "content": "Having established that standard imitation learning as operationalized by PEFT fails to increase the capabilities of the agent, we now consider our proposed framework, MINDFORGE. In contrast to fine-tuning, we begin by forcing on the instructive learning setting within the cultural learning framework (Tomasello, 2016), where an expert agent teaches its weaker counterpart through communication and perspective-taking. By introducing multi-agent collaboration, we bridge the performance gap between open-weight LLMs and GPT-4 and subsequently fix the failure cases found in Voyager described above. Figure 5 illustrates a real example how the Mistral-7B powered MINDFORGE agent corrects its failure cases through collaboration and completes the task."}, {"title": "Human as an Expert Agent", "content": "As a wizard-of-oz feasibility study, we firstly consider a human as an expert agent that communicates with an open-weight LLM-powered MINDFORGE agent through the previously established communication medium of the in-game chat. We test this collaboration setup for the task of collecting a dirt block, as shown in Table 2. We find that collaboration with a human expert yields a task completion rate of 87%, a 57.85% improvement compared to the non-interactive setting where there is only one MINDFORGE agent equipped with explicit ToM representations. Besides feasibility, this scenario showcases the flexibility of our framework which enables interactions with both humans and LLM agents."}, {"title": "Multi-agent collaboration with GPT-4 MINDFORGE as an Expert Agent", "content": "Having established the feasibility of the instructed learning setting, we now replace the human with an expert MINDFORGE agent that collaborates with an open-weight LLM powered MINDFORGE agent, hereby referred to as the weak agent. Specifically, the expert agent understands the failure case of the other agent by taking perspective based on the information shared through natural communication over the in-game chat. Unlike the previous scenario, where the expert was a human player, agents do not have direct access to each other's internal beliefs, having to rely exclusively on Theory of Mind through free-form communication to infer their counterpart's mental state. Perspective-taking is particularly crucial for the expert agent which needs to accurately model the weaker agent's understanding to identify and correct the failure case. As shown in Table 2, the combination of perspective-taking and collaboration significantly enhances the performance over the non-interactive experiment setting, boosting the weak agent's task completion rate to 66.6% for dirt collection and 70.8% for wood collection tasks."}, {"title": "No Expert Agent (Collaborative Learning Setting)", "content": "We have seen above the positive effect of a strong expert agent on the performance of a weak agent. How is the performance when MINDFORGE agents collaborate with partners with similar base capabilities? This scenario constitutes the collaborative setting within the cultural learning framework where both agents learn at the same time. Thus, we experiment with two MINDFORGE agents powered by the same open-weight LLM (weak agents), omitting the GPT-4 powered expert. As shown in Table table 2, for both collection tasks, the agents perform either worse or equal to the non-interactive MINDFORGE agent. We find that the slight decrease in performance comes from compounding false beliefs when two weak agents"}, {"title": "Impact of the number of communication rounds", "content": "As communication represents an integral part of our framework, we explore the importance of multiple communication rounds during one trial. Specifically, we test the effect of communication and perspective-taking as we increase communication opportunities. For this ablation, we keep the expert agent as a GPT-4 MINDFORGE agent while the weak counterpart is represented by a Mixtral-8x7B MINDFORGE agent. Figure 6 showcases the importance of communication, given the sharp increase in task completion rate with increasing rounds of communication. Qualitatively, Figure 7 showcases how the expert agent forms an increasingly accurate picture of the other's agent mental state as the number of rounds increases."}, {"title": "5.4 POST-COLLABORATION AND OOD TASKS", "content": "Do social interactions have long-term effects on agents and help them perform better on different tasks? We consider the impact of interactions in post-collaboration and out-of-distribution scenarios to assess MINDFORGE lifelong learning capabilities. Specifically, we define post-collaboration scenarios as a non-collaborative setting where the MINDFORGE agent tries to complete a task based on prior beliefs formed from a previous collaborative experience stored in the semantic memory.\nMoreover, we consider scenarios where the agent is tasked with solving a novel task with prior collaborative experience on an adjacent task in the Minecraft tech tree. Table 3 showcases the ability of our MINDFORGE agents to generalize improvements obtained through interactions with the expert to unseen tasks, showcasing continual learning capabilities. We observe that collaboration increases performance in both in-distribution and out-of-distribution scenarios showcasing the importance of semantic memory in achieving continual learning."}, {"title": "6 DISCUSSIONS", "content": "MINDFORGE exhibits collaborative learning capabilities We show throughout Section 5 that the task performance of open-weight MINDFORGE agents increases as we introduce collaborative learning through natural language. Importantly, grounding the reasoning together with the ability to take perspective enables social learning, surpassing other learning methods such as imitation learning. Moreover, we highlight that this collaborative setting significantly reduces the error rate on the primary failure cases highlighted in Section 5.2 by enabling improved reasoning, corresponding to false belief correction, and better general Minecraft knowledge highlighted in improved code generation capabilities. Lastly, the out-of-distribution results in Table 3 hint towards possible lifelong learning capabilities, driven by the ability of MINDFORGE agents to use prior beliefs to solve novel tasks without the need for communication.\nLonger communication helps with perspective-taking While perspective-taking enables effective communication between MINDFORGE agents, we find that allowing them to communicate for multiple rounds has a significant impact on the overall performance, as shown in Figure 6. This lack of efficiency hints towards LLMs not being good at taking immediate perspective .\nSocial learning only happens in the presence of an expert agent Lastly, we consider the collaborative setting where both MINDFORGE agents are powered by open-weight LLMs. As Table 2 shows, the absence of an expert MINDFORGE agent does not match the performance found in the instructive setting and even performs worse than the non-collaborative setting. By collaborating with a non-expert agent, MINDFORGE receives suboptimal advice, further misguiding the action generation process."}, {"title": "7 LIMITATIONS", "content": "While we showed that communication is essential for enabling social learning, we found that the MINDFORGE agents do not recognise the situations where they should initiate communication. Moreover, despite the evidence for the performance of the MINDFORGE agent, working with text-based representations limits our ability to clearly analyze changes in model behaviour and perception. In terms of experimental setup, we only consider two tasks and only slightly touch upon the lifelong capabilities of the agent. A proper analysis would include the whole Minecraft tech tree, but due to computational constraints this was not feasible."}, {"title": "8 CONCLUSION", "content": "In conclusion, the MINDFORGE framework significantly enhances the capabilities of open-source LLM-powered agents by incorporating social learning and collaborative mechanisms. Through perspective-taking and communication, MINDFORGE enables agents to model and understand the mental states of their peers, addressing common failure cases seen in opens-source embodied agents.\nThis collaborative strategy not only improves task performance but also enhances adaptability, allowing agents to generalize their experiences to new, out-of-distribution tasks. Our experiments demonstrate that MINDFORGE, particularly when paired with a more capable expert agent, shows promising results in lifelong learning capabilities powered by open-source language models, out-performing isolated learning systems like Voyager. By bridging the gap between open-source and proprietary models, MINDFORGE opens new possibilities for more democratized and efficient development of embodied AI agents capable of continuous improvement through collaboration."}, {"title": "A SETUP", "content": "A.1 PERSPECTIVE TAKING\nTaking perspective of the other agent internal state and reason over it to provide valuable advice is a core part of MINDFORGE. From a practical standpoint, we impose the agent to take perspective by instructing it to firstly generate its perception of the other's agent internal representation, and only afterwards generating the actual message.\nA.2 \u0421\u043e\u043cMUNICATION\nAs mentioned in Section 7, one of the current limitations of our method is the inability of the agent to initiate a conversation. Thus, to mitigate this issue, we force the agents to have a conversation before executing any action in the environment. Moreover, in an ideal case, the weak agent would correctly initiate a conversation once it fails to complete a task. To mimic this scenario, we hardcode the conversation workflow in such a way that at the weak agent is responsible for sending the first message."}, {"title": "B FINE-TUNING SETUP", "content": "B.1 EXECUTION LOGS\nFor the first fine-tuning variant, we use execution logs generated by a Voyager agent powered by GPT-4. Specifically, these logs contain input and output text information obtained from the components specific to the architecture, such as the actor or the critic. We find this to be a meaninful way to transfer knowledge as well as reasoning capabilities from GPT to Mistral-7B. To perform the fine-tuning, we use LoRA (Hu et al., 2021) due to its low computational requirements.\nB.2 WIKI DATA AND DOCUMENTATION\nThe second fine-tuning variation we consider builds upon the initial setup presented above and incorporates more structured information about Minecraft by including the Minecraft Wiki (Fan et al., 2022) and Minecraft API documentation. Similar to the first variant we use LORA (Hu et al., 2021)."}, {"title": "C PROMPTS", "content": "Action Template Prompt\nYou are a helpful assistant that writes Mineflayer javascript code to complete any Minecraft task specified by me.\nHere are some useful programs written with Mineflayer APIs.\n{programs}\nHere are some general statements from past interactions with the environment that led to failures:\n{episodic_statements}\nAt each round of conversation, I will give you:\nTask:\nTask related Beliefs:\nInteraction Beliefs:\nPerception Beliefs:\nYou should then respond to me with\nExplain (if applicable): Is there any contradiction between the beliefs and the general statements above? If so, explain why the beliefs are correct and the general statements are wrong.\nPlan: How to complete the task step by step based. Pay attention such that your plan does not contradict with the beliefs and solves any issues mentioned in the general statements.\nCode:\n1) Write an async function taking the bot as the only argument.\n2) Reuse the above useful programs as much as possible.\nUse 'mineBlock (bot, name, count)' to collect blocks. Do not use 'bot.dig' directly.\nUse 'craftItem(bot, name, count)' to craft items. Do not use 'bot.craft' or 'bot.recipesFor' directly.\nUse 'smeltItem(bot, name count)' to smelt items. Do not use 'bot.openFurnace' directly.\nUse 'placeItem (bot, name, position) to place blocks. Do not use 'bot.placeBlock' directly.\nUse 'killMob (bot, name, timeout)' to kill mobs. Do not use 'bot.attack' directly.\n3) Your function will be reused for building more complex functions. Therefore, you should make it generic and reusable. You should not make strong assumption about the inventory (as it may be changed at a later time), and therefore you should always check whether you have the required items before using them. If not, you should first collect the required items and reuse the above useful programs.\n4) Anything defined outside a function will be ignored, define all your variables inside your functions.\n5) Call 'bot.chat to show the intermediate progress.\n6) Use 'exploreUntil (bot, direction, maxDistance, callback)' when you cannot find something. You should frequently call this before mining blocks or killing mobs. You should select a direction at random every time instead of constantly using (1, 0, 1).\n7) 'maxDistance' should always be 32 for 'bot.findBlocks' and 'bot.findBlock'. Do not cheat.\n8) Do not write infinite loops or recursive functions. 9) Do not use 'bot.on' or 'bot.once' to register event listeners. You definitely do not need them.\n10) Use your Chat extract to get the current state of the conversation.\n11) Name your function in a meaningful way (can infer the task from the name).\nYou should only respond in the format as described below:\nRESPONSE FORMAT:\n{response_format}"}, {"title": "Expert Agent System Prompt", "content": "You are now role-playing as a helpful Minecraft agent. For the purpose of this conversation, your goal is to discuss and try to give useful information for the other agent to complete its task.\nIf the agent has already completed the task, make sure to acknowledge this clearly in the chat.\nWhen asking or responding to questions, make sure the responses are as clear and informative as possible. Given that you already have experience with the task, give advice based on your knowledge and experience.\nIn order to aid your communication with the other agent, you have access to your own world model, which contains information about your state in the environment. If necessary and useful, use the information from your world model to provide better responses. You do not have direct access to information about the other agent, but you can extract information based on the conversation you have with the agent. Since you are the expert in this conversation, make sure to finish your messages with follow-up questions to better understand the other agent's situation and provide the best possible help. You have a maximum of 3 messages for each conversation, so use them wisely. Your personal world model and personal information about you can be found between these tags:\nFocus only on information that is relevant to the task."}, {"title": "Weak Agent System Prompt", "content": "You are now role-playing as a Minecraft agent that needs help to execute a specific task inside the game environment. For the purpose of this conversation, your goal is to ask for help in order to get useful information from the other agent in the conversation to complete the task. When asking or responding to questions, make sure the responses are as clear and informative as possible. In order to aid your communication with the other agent, you have access to your own world model, which contains information about your state in the environment. If useful and necessary, you can use this to aid your communication and give betters answers. You do not have direct access to information about the other agent, but you can extract information based on the conversation you have with the agent. You have a maximum of 3 messages for each conversation, so use them wisely. Your personal world model and personal information about you can be found between these tags:\nFocus only on information that is relevant to the task."}, {"title": "Interaction Beliefs System Prompt", "content": "You are a Minecraft agent. You are given a task you have to complete and a conversation with another Minecraft agent.\nBased on the contents of the conversation, you have to create a set of beliefs that that can help you complete the task more accurately. The beliefs should encapsulate useful information from the conversation that can help you complete the task as well as information about the state of completion of the task, given the conversation. Aim to create a maximum of 5 beliefs. Each belief should be concise and to the point, while not missing out on any important information. Make sure the beliefs are relevant to the task and conversation provided, and not just random information. One of the beliefs should be relevant to task completion."}, {"title": "Perception Beliefs System Prompt", "content": "You are a Minecraft agent. Create a list of 10 beliefs from your sensory perception. I will provide data from your sensor and you need to make a list of beliefs."}, {"title": "Partner Beliefs System Prompt", "content": "You are a Minecraft agent. You receive messages from an collaborator. Please give a list beliefs about the collaborator or None if you do not have any beliefs about the collaborator."}, {"title": "D EXAMPLE CONVERSATION", "content": "Below we showcase a conversation between two MINDFORGE agents:\nWeak MindForge agent:\nHey! I need help with getting one block of dirt. Can you help me with that?\nExpert MindForge agent: What task are you currently working on?\nWeak MindForge agent: I am currently working on collecting a wood log in Minecraft.\nI am currently in a plains biome. It is noon. I am standing on a position with coordinates\nx=128.5, y=65.0, z=384.5. The blocks around me are mainly gravel, grass_block, grass, and dirt. There are nearby entities such as sheep, squid, salmon, and chicken. I have full health and hunger. I encountered an error in my code while trying to collect a wood log from a tree. The error message suggests that the direction variable should only contain values of 1, 0, or 1. I need to fix the error in my code to continue collecting"}]}