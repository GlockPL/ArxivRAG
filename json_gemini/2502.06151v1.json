{"title": "Powerformer: A Transformer with Weighted Causal Attention for Time-series Forecasting", "authors": ["Kareem Hegazy", "Michael W. Mahoney", "N. Benjamin Erichson"], "abstract": "Transformers have recently shown strong performance in time-series forecasting, but their all-to-all attention mechanism overlooks the (temporal) causal and often (temporally) local nature of data. We introduce Powerformer, a novel Transformer variant that replaces noncausal attention weights with causal weights that are reweighted according to a smooth heavy-tailed decay. This simple yet effective modification endows the model with an inductive bias favoring temporally local dependencies, while still allowing sufficient flexibility to learn the unique correlation structure of each dataset. Our empirical results demonstrate that Powerformer not only achieves state-of-the-art accuracy on public time-series benchmarks, but also that it offers improved interpretability of attention patterns. Our analyses show that the model's locality bias is amplified during training, demonstrating an interplay between time-series data and power-law-based attention. These findings highlight the importance of domain-specific modifications to the Transformer architecture for time-series forecasting, and they establish Powerformer as a strong, efficient, and principled baseline for future research and real-world applications.", "sections": [{"title": "Introduction", "content": "With the recent popularity of Transformer models [36], large language models (LLMs), and foundation models [4], the Transformer architecture gained popularity in time-series forecasting, leading to a number of Transformer based state-of-the-art models: Autoformer [42], FEDformer [48], and PatchTST [27].\nThe multihead attention (MHA) mechanism is at the core of these models, which is an all-to-all process that enriches each embedding via a weighted combination of other embeddings based on a similarity calculation. For natural language processing (NLP) tasks, each embedding represents a word. Here, an all-to-all embedding mechanism is natural since correlations between words can be only weakly dependent on their ordering and the number of words between them. However, for time-series data, where each embedding is a temporal measurement, the relation between embeddings is both causal and dependent on the time delay. This disparity between the attention mechanism's all-to-all structure motivates the open question: how well-suited is the Transformer's attention mechanism for time-series data? For example, [44] shows that linear models outperform Transformers on common time-series benchmarks. Still, Transformers have seen success in time-series forecasting.\nPrevious methods sought to impose causal and local implicit biases on Transformer. Before Transformer's widespread adoption, methods such as WaveNet [34] and [12] used causal convolutions. Imparting local and causal implicit biases to MHA requires altering its architecture. Reformer [19] calculates the attention weights via a causal and locality-aware hash map. LogSparse self-attention [20] uses convolutions to encode local variations and feeds them into a causally sparse self-attention mechanism. The work most similar to ours is ETSformer [39], which develops exponential smoothing attention that replaces the similarity-based attention weights with exponential decaying temporal weights. These temporal weights are no longer dependent on a similarity metric like the key-query projection. ETSformer also employs a frequency attention scheme alongside their exponentially weighted attention.\nWhen enforcing a locality bias, previous methods imposed temporal length scales on the dynamics and/or the functional form of the temporal correlations. However, the temporal correlations between data points vary in both functional form and time scale between datasets. Temporal convolutions, and their dilations, are examples of imposing such a time scale. Exponentially weighted attention imposes exponentially decaying temporal correlations but lacks the similarity-weighted MHA that captures varying temporal dependencies. Previous successes from adding local and causal biases [39, 19, 20, 34, 12] show their importance, but the application method is crucial.\nIn this work, we develop Powerformer (illustrated in Fig. 1), a Transformer-based model that uses Weighted Causal Multihead Attention (WC\u041c\u041d\u0410) to learn temporal dependencies unique to each dataset. Our model is motivated by the observation that many physical systems exhibit heavy-tailed autocorrelations, e.g., the pairwise correlation strength may decay as a power law distribution, as the time delay grows [9]. Our goal is to impose this power-law temporal dependence as an inductive bias, while allowing Powerformer the flexibility to perturb it sufficiently to capture the temporal pairwise correlations unique to each dataset. To do so, we add a temporally decaying mask to the attention mechanism, specifically to the key-query overlap (referred to as the similarity score). The mask decays attention weights and pairwise dependencies to resemble a power law, but its additive nature maintains the key and query similarity score. This maintained similarity score allows the model to learn data-dependent perturbations to the power-law pairwise dependency. The mask acts as a regularizer that guides the attention weight temporal dependence towards a power law, but still maintains sufficient expressivity for MHA to learn pairwise dependencies unique to each dataset. Moreover, Powerformer's simplicity promotes interpretability studies that provide evidence of learned heavy-tailed dependencies in the attention weights.\nWe build Powerformer by combining WCMHA, the commonly used encoder-only Transformer architecture, and patching [27]. Powerformer outperforms previous state-of-the-art models on typical tasks despite its simplicity compared to other attention mechanisms. This suggests that these biases and our implementation better align Transformer with natural time-series structures.\nOur main contributions are the following.\n\u2022 We develop Powerformer, a Transformer-based model for time-series, with WCMHA to impose causal and local implicit biases that shape attention's pairwise correlation to better respect the data's unique temporal structure.\n\u2022 Powerformer is powered by various weighting schemes: power-law decays and Butterworth filters. The former resembles naturally occurring time-series, while the latter acts analogously to a step function.\n\u2022 Our empirical results demonstrate that Powerformer obtains state-of-the-art performance when compared to similar Transformer models on a range of public times-series benchmarks. Moreover, we provide interpretability studies that show how our implicit biases shape Powerformer's learned attention representations.\n\u2022 Powerformer can reduce the attention complexity from quadratic to linear by defining a cutoff time after which the attention weighting removes their contribution."}, {"title": "Related Work", "content": "The importance of time-series data has led to decades of work, resulting in a myriad of methods: early statistical methods [17, 18, 33, 38] (such as ARIMA [5]); multilayer perceptions (MLP) [44, 21, 10, 46] (such as NHITS [7], TSMixer [8], and N-BEATS [28]); convolutional neural networks (CNNs) [15, 12, 3] (such as TimesNet [40] and SCINet [22]); recurrent neural networks (RNNs) [31, 16] (such as DeepAR [30]); Transformers [49, 13, 39, 48, 26, 41, 43, 23, 47, 19, 20, 32, 11] (such as PatchTST [27], Autoformer [42], FEDformer [48], and iTransformer [24]); and state space models (SSMs) [1, 37, 45] (such as MAMBA [14]). Of these methods, we focus on Transformers.\nThe success of LLMs and the introduction of powerful foundation models, like GPT-2 [29], inspired models built atop them and others looking to improve upon the Transformer architecture. For example, Time-GPT [13] uses the Transformer encoder-decoder structure, and One-Fits-All [49] is built atop a pretrained GPT-2 and fine-tunes the attention layer normalization and temporal embedding. Some models like AutoTimes [25] leverage LLM's in-context learning capabilities and employ textual prompt engineering to feed into LLMs. Other models have adapted the attention mechanism to time-series data: some models strongly rely on, or partially include, Fourier representations when calculating attention weights [48, 39]; AutoFormer [42] leverages temporal autocorrelations as a similarity metric; Reformer [19] uses a locality-sensitive hashing mechanism to reduce the attention's complexity; Informer [47] selects only high attention weight values, increasing prediction capacity; FlowFormer [41] uses flow conservation to achieve linear complexity attention without imposing locality implicit biases; and iTransformer [24] inverts the temporal and embedding dimensions, applying attention along the embedding.\nTransformers were developed for discrete inputs, unlike the continuous valued time-series inputs. Patching along the time domain (PatchTST) shortens the context window which allows for longer lookback windows and significantly improves performance over more complicated Transformer methods [27]. This simple yet powerful improvement has made patching ubiquitous. Other works look to convert time-series into discretized tokens similar to word vectors. Chronos [2] normalizes the input sequences and matches these continuous values to tokens made from discretizing a sufficiently large domain. TOTEM [32] similarly discretizes the input time series, but instead VQVAE [35] encodes the input and matches it with the most similar token."}, {"title": "Method", "content": "We propose Weighted Causal Multihead Attention (WCMHA) as a foundation for Powerformer, a Transformer-based architecture designed for time-series forecasting. In this section, we review standard Multihead Attention (MHA) and demonstrate how we introduce locality and causality into it. We then present various weighting functional forms and insights on how WCMHA modifies attention distributions. We then introduce the Powerformer architecture that builds upon these principles. Figure 1 provides a high-level illustration of WCMHA and its effect on attention weights."}, {"title": "Weighted Causal Multihead Attention", "content": "Transformers typically leverage MHA to compute new embeddings from weighted sums of input embeddings. Let $X \\in \\mathbb{R}^{T \\times d}$ be a sequence of $T$ input vectors, each of dimension $d$. For each attention head $h$, MHA first projects $X$ into query, key, and value matrices, respectively denoted by $Q_h, K_h,$ and $V_h$. Concretely,\n$Q_h = XW_Q^{(h)}, K_h = X W_K^{(h)}, V_h = X W_V^{(h)}$,\nwhere each projection matrix $W \\in \\mathbb{R}^{d \\times d_k}$ is learnable and $d_k$ is the dimensionality of each head. The parameters $Q_h, K_h$, and $V_h$ thus each have shape $T \\times d_k$.\nThe unnormalized attention similarity scores $S_h$ measure how each query interacts with all keys:\n$S_h = \\frac{K_h^T Q_h}{\\sqrt{d_k}}$.\nThe factor of $1/\\sqrt{d_k}$ helps stabilize gradients by normalizing the dot products according to the head dimension. We then apply a softmax function to obtain the normalized attention weights:\n$C_h = \\text{Softmax}(S_h)$,\nwhere each row of $C_h$ sums to 1. Finally, each head's output $X_h$ is computed as a weighted sum of the values:\n$X_h = C_hV_h$.\nTo form the final MHA output, we concatenate the outputs from all $H$ heads and apply a linear projection:\n$\\hat{X} = [X_1,..., X_H] W^{(A)}$,\nwhere $W^{(A)} \\in \\mathbb{R}^{(Hd_k)\\times d}$ is another learnable matrix. The result $\\hat{X}$ has the same dimensionality as the input $X$ (i.e., $T \\times d$), allowing it to be passed into subsequent Transformer layers or decoded for downstream tasks."}, {"title": "Enforcing Causality", "content": "To ensure that information at time $t$ does not leak from future time steps $t' > t$, we apply a causal mask $M^{(C)}$:\n$M_{i,j}^{(C)} = \\begin{cases} -\\infty & j > i, \\\\ 0 & j \\leq i. \\end{cases}$\nThis mask is added to the attention score $S_h$, forcing any attention weight for $j > i$ to be zero after the softmax:\n$S^{(C)} = S_h + M^{(C)}, C^{(C)} = \\text{Softmax}(S^{(C)})$.\nHence, $C_{t,t'} = 0$ for all $t' > t$, ensuring causal structure for time-series data."}, {"title": "Incorporating Locality via Decaying Masks", "content": "Time-series often exhibit stronger correlations between nearby time steps compared to distant ones [9]. To bias the model toward local dependencies, we introduce a second mask $M^{(D)}$ that decays attention weights as the time gap $|\\Delta t|$ increases:\n$M_{i,j}^{(D)} = \\begin{cases} 0 & j > i, \\\\ f(t_{i} - t_{j}) & j \\leq i, \\end{cases}$\nwhere $f(\\Delta t) < 0$ is a non-positive function of the time difference. Adding $M^{(D)}$ to $S^{(C)}$ yields\n$S^{(C,D)} = S_h + M^{(C)} + M^{(D)}.$"}, {"title": "Reweighting Functions for Locality", "content": "We explore multiple choices for the reweighting function $f(\\Delta t)$:\n\u2022 Similarity Power Law: $f^{(SPL)}(t)(\\Delta t) = -(\\Delta t)^a$.\n\u2022 Weight Power Law: $f^{(PL)}(t)(\\Delta t) = -a \\log(\\Delta t)$.\n\u2022 Butterworth Filter: A frequency-based filter with a smooth transition resembling a step function, parameterized by its cutoff time $t_c$ and order $n$.\nEach function has a tunable hyperparameter (e.g., $a$ or $t_c$) that governs the decay rate of attention as $|\\Delta t|$ grows."}, {"title": "Insights into WC\u041c\u041d\u0410", "content": "Before integrating WCMHA into a state-of-the-art Transformer-based model, we investigate how causal and local masks reshape attention distributions in a vanilla Transformer. See Appendix E.3 for details. Figure 3 compares attention score and weight distributions (encoder self-attention, decoder self-attention, and decoder cross-attention) on the Electricity dataset. We observe:\n\u2022 Encoder Self-Attention (modified by WCMHA) shifts toward higher positive values, emphasizing local contexts.\n\u2022 Decoder Cross-Attention (unchanged) retains a heavy-tailed distribution, indicating some long-range dependencies remain relevant.\n\u2022 Decoder Self-Attention (already causal) exhibits minimal distributional change.\nThese shifts consistently improve forecasting performance on real-world datasets (see Appendix E.3). In many cases, WCMHA significantly outperforms standard MHA, especially for datasets with slowly decaying long-range dependencies. Interestingly, when WCMHA outperforms MHA it does so by much larger margins than when it underperforms. For example, on the ETTh2 dataset, WCMHA improves MSE and MAE by 16% and underperforms by 2%."}, {"title": "Powerformer: Locality, Causality, and Patching", "content": "Having shown that WCMHA provides a powerful mechanism for time-series, leveraging its full potential in practical scenarios requires an effective approach to represent temporal data. While incorporating WCMHA into modern Transformer time-series models, we wish to isolate the effects of WCMHA and its local and causal biases. To do so requires a simple architecture, and we focus on univariate time-series. Thus, we introduce Powerformer (see Fig. 1), a model that integrates Transformer, WCMHA, and PatchTST's [27] successful patching framework.\nPatching is a common tool in modern time-series models, and similar to M(D) it acts as a regularizer to remove high-frequency noise components. We decompose multivariate time-series into univariate channels before applying convolutional patching, similar to PatchTST [27]:\n$X^{(Patched)}_{s,w} = X * p$,\nwith patch length $w$ and convolution stride $s$. Patching captures local temporal contexts by converting segments of the raw series into compact embeddings for the Transformer.\nMany state-of-the-art models only use the Transformer encoder [27, 49, 32] with a linear decoder head. Powerformer combines these insights with our previous Transformer experiments by using only the Transformer encoder with WCMHA and a similar linear readout head. Powerformer integrates two modern design choices while injecting causal masks and smoothly decaying attention weights, allowing it to better model nearby past time points.\nOur design integrates two complementary strengths:\n\u2022 Locality and Causality: WCMHA introduces both temporal ordering and tailored decay, making it naturally suited for real-world time-series with power-law or exponential correlations.\n\u2022 Robust Patch Embeddings: Patching [27] efficiently captures short-term trends and reduces input length, mitigating quadratic complexity issues that can impede Transformers on long time-series.\nPowerformer is expressive, computationally efficient, and sufficiently simple to interpret. Appendix C.3 provides further details. Our empirical results confirm that imposing explicit locality and causality boosts forecasting accuracy and the interpretability of learned temporal dependencies."}, {"title": "Empirical Evaluation", "content": "We evaluate Powerformer\u00b9, investigating the effects of WCMHA on the attention score and weight distributions, using the mean squared error (MSE) and mean absolute error (MAE) to quantify aggregate performance."}, {"title": "Datasets", "content": "We evaluate Powerformer on 7 popular publicly-available real-world datasets [47, 42]: ETT (4 subsets), Weather, Electricity, and Traffic. We are particularly interested in the ETT subsets which measure the electricity transformer oil temperature at 2 different time scales (every 15 minutes and hourly) for two regions. Analyzing the same measurement in different locations and different time scales will illustrate how sensitive WCMHA is to sampling changes and distribution shifts. Section B provides a detailed description of these datasets, available at [42]. We note that Weather, Electricity, and Traffic have much larger timescales, spanning from one to multiple years (Table A1). These 7 datasets are large enough to capture slowly decaying temporal correlations, shown in Appendix B. Illness [47] is traditionally used as a long-term forecasting benchmark. We omit this dataset as its temporal measurements (Table A1) and evaluation look-back window are more than an order of magnitude smaller than the other datasets. This short look-back window is typically the size of our filters."}, {"title": "Evaluation setup", "content": "Following previous works [27, 42, 48, 39, 24, 32, 19, 47], we train for multiple prediction and input lengths. The prediction lengths are 96, 192, 336, and 720, while the input sequence lengths are 336 and 512. Each model is trained three times for each setting. Further details on our experimental process and hyperparameters are provided in Appendices E.1 and C.4, respectively."}, {"title": "Empirical Results", "content": "Powerformer achieves state-of-the-art performance as we compare it against other state-of-the-art models in Table 1. Powerformer achieves the best performance in 47 forecasting tasks compared to the next best model (PatchTST) which achieves 17. We note that Powerformer outperforms ETSformer in all tasks, further supporting our claim that how and where locally decaying attention weights are applied is crucial. Notably, Powerformer outperforms One-Fits-All [49] which is more than twice Powerformer's size and is built from pre-trained GPT-2 [29].\nPowerformer is sensitive to the mask type and aligns f(t) with the dataset's natural pairwise correlation distribution, which we observe as $f^{(PL)}(t)$ and $f^{(SPL)}(t)$ consistently outperforming $f^{(BW)}(t)$. See Tables A5, A6, A7, and A8 for full $f^{(PL)}(t)$, $f^{(SPL)}(t)$, $f^{(BW)}(t)$, and $f^{(BW)}(t)$ evaluation results. The $f^{(BW)}(t)$ filters resemble step functions and similarly weight all the values within the window. Alternatively, $f^{(PL)}(t)$ and $f^{(SPL)}(t)$ allow for longer-time couplings but significantly diminish their contribution; see Fig. 1c. The superior performance of $f^{(PL)}(t)$ and $f^{(SPL)}(t)$ over $f^{(BW)}(t)$ further supports that the method for enforcing locality is important and that power-law correlation distributions are naturally suited to our datasets. We perform further ablation studies and report detailed results in Appendix F.\nIn addition to distinguishing between masks that naturally align with the data distribution, Powerformer is also sensitive to the pairwise correlation timescale. The ETTh and ETTm datasets test Powerformer's ability to adapt the timescale $a$ of M(D) for datasets sampled from the same distribution but with different temporal resolution. Since ETTh and ETTm are sampled on the hour and minute timescales, respectively, ETTm requires a slowly decaying mask to access the same information as a quickly decaying mask for ETTh. As expected, Powerformer with $f^{(SPL)}(t)$ (Table A6) favors quickly decaying masks on ETTh while selecting slowly decaying masks on ETTm. This illustrates Powerformer's sensitivity to both the temporal information timescale, the timescale covering information needed to accurately forecast, and the temporal sampling rate.\nTo learn the dataset's temporal information timescale, we consider a learnable $a$ instead of treating it as a hyperparameter. Section F.3 outlines method and experimental details, as well as the full results. During training, we observe that $a$ consistently and monotonically drops. This is expected, as it facilitates overfitting by providing the model with extraneous information. Ultimately, we observe very little difference between learnable and constant $a$ (see Table A9). This indicates that WCMHA acts as a regularizer during training, removing superfluous information to avoid overfitting and improve generalization."}, {"title": "Interpretability", "content": "The simplicity of Powerformer is a key strength and indicator that causal and local biases are important for time-series Transformer-based models. Previous models employ more complicated attention schemes using autocorrelations [42] or Fourier decompositions [48]; or they induce causal and local implicit biases via various mechanisms [39, 20, 19]. Some models, like FEDformer [48], purposely avoid such implicit biases. The success and simplicity of WCMHA and Powerformer over other models and their attention techniques bolsters the importance of prioritizing embeddings with causal correlation structures natural to the dataset. Moreover, Powerformer's simplicity, along with the local and causal biases, promotes interpretability and provides insight into the dependencies of each forecast.\nBy leveraging Powerformer's simplicity, we interpret it's behavior through the attention weight and score distributions. As the power-law dampening grows (Fig. 4), an emergent bimodal distribution in attention weights appears. The score distributions (inset in Fig. 4) shift towards lower values, while the positive heavy-tail increases as the decay constant grows. This shift in scores to larger magnitude values produces an emergent bimodal distribution in the attention weights. This bimodal behavior is well aligned with our implicit biases as the right mode (larger weights) corresponds to our amplified local interactions and the left mode (smaller weights) corresponds to the diminished non-local interactions. The bimodal $C^{(C,D)}$ distributions in Fig. 4 indicates that M(C,D) achieves our stated goal of amplifying local couplings, removing causally inconsistent couplings, and diminishing unphysically long-time couplings.\nRemarkably, we observe Powerformer further enforcing our local bias beyond M(D), and we consider this a key insight into the local bias' importance. Because our masks are constant, Powerformer has the potential to mitigate the effects of M(D) by learning to reweight the attention scores. Instead, we observe the opposite, Powerformer further enforces our locality bias during training by imposing a bimodal attention weight distribution before M(C,D) is applied. Figure 5 shows the attention score and weight distributions with and without applying M(C,D). The dashed-dotted line in the attention weight indicates that Powerformer imposed a local bias before the locally biasing M(C,D) mask is applied. This learned effect amplifies the impact of M(C,D) as seen by the further separation in modes between the solid and dashed-dotted distributions in Fig. 5. Ultimately, Powerformer learns embeddings that further strengthen the effects of our implicit bias. We consider Powerformer's learned amplification of our causal and local implicit biases to be strong evidence these biases are crucial in time-series forecasting.\nOur local biasing power-law $M^{(C,D)}$ endows Powerformer with an implicit temporal multiscale structure as its embeddings change as the timescale increases. Let us consider the case when M(D) mask has a long tail (e.g., $f^{(PL)}(t)$ for $a \\leq 0.5$ in Fig. 2). The mask weighting in this tail begins to asymptote and similarly scales the attention weights at large $\\Delta t$. Summing over many similarly weighted time steps washes out high-frequency information and amplifies dynamic information on the time scale of the summation. For example, if one added a fast oscillating sinusoid to a line and summed this signal over many periods of the sinusoid, the high-frequency oscillations would cancel and one would see a linear signal. Conversely, in temporally local regions the mask quickly changes to amplify local high-frequency components. These local high-frequency components thus play a more prominent role than high-frequency components from longer windows. The mask imposes a temporal multiscale attention structure by capturing low-frequency information from longer-time summations and high frequency from local interactions. In Appendix D, we observe this phenomena as Powerformer better predicts higher frequency contributions fast-varying signals than PatchTST.\nBy removing longer-term pairwise contributions, WCMHA has the potential to improve the attention complexity cost from quadratic to linear. Moreover, since multiple timescales can provide similar results (Tables A5, A6, A7, and A8), such speed improvements may come at a small cost to accuracy. Each mask type and timescale has a cutoff length $\\tau$, after which the contributions from data are negligible: e.g., for $f^{(PL)}(t)$ with $a=1$, points beyond $\\tau = 100$ are diminished by two orders of magnitude and may be considered negligible. When WCMHA is only applied within $\\tau$, it scales linearly $O(T\\tau)$ with input sequence length T, instead of quadratically ($O(T^2)$) like MHA. Because $a$ varies between datasets and can be lowered to trade speed for small changes in accuracy, we expect a large variance in potential speedup times, and so we do not provide results here. For example, $f^{(PL)}(t)$ with $a = 1$ provides a factor of 3 and 5 speedup for typical look-back window of 336 and 512, respectively.\nWCMHA leverages the strengths of both statistical models and MHA. The induced power-law decay in attention weights, and thus temporal correlations, resembles previous statistical models. However, the WCMHA attention mechanism is not fully overwhelmed by this decay and still amplifies important longer-time correlations, as shown in Fig. 1b and Fig. 1c where the attention mask clearly selects regions of strong correlations near and past the tail of the power-law decay. We believe this balance between power-law temporal correlations and maintaining WCMHA's ability to attend to highly correlated input is key to WCMHA's success."}, {"title": "Conclusion", "content": "We develop Powerformer to impose and investigate the importance of causal and local biases for Transformer-based time-series models. Powerformer outperforms state-of-the-art time series models on standard tasks often with a simpler architecture. Powerformer's simplicity promotes interpretability studies that elucidate the effects of our biases. Powerformer strengthens our local bias by learning to decrease attention scores for long-time interactions.\nIn addition to outperforming previous models on forecasting accuracy, Powerformer also has the potential to significantly increase attention efficiency. The decaying nature of $f(t)$ determines a cutoff time after which M(D) effectively removes contributions. Evaluating within this cutoff time will result in $O(T\\tau)$ rather than $O(T^2)$ complexity."}]}