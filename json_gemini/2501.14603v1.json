{"title": "Age and Power Minimization via Meta-Deep Reinforcement Learning in UAV Networks", "authors": ["Sankani Sarathchandra", "Eslam Eldeeb", "Mohammad Shehab", "Hirley Alves", "Konstantin Mikhaylov", "Mohamed-Slim Alouini"], "abstract": "Age-of-information (AoI) and transmission power are crucial performance metrics in low energy wireless networks, where information freshness is of paramount importance. This study examines a power-limited internet of things (IoT) network supported by a flying unmanned aerial vehicle (UAV) that collects data. Our aim is to optimize the UAV's flight trajectory and scheduling policy to minimize a varying Aol and transmission power combination. To tackle this variation, this paper proposes a meta-deep reinforcement learning (RL) approach that integrates deep Q-networks (DQNs) with model-agnostic meta-learning (MAML). DQNs determine optimal UAV decisions, while MAML enables scalability across varying objective functions. Numerical results indicate that the proposed algorithm converges faster and adapts to new objectives more effectively than traditional deep RL methods, achieving minimal Aol and transmission power overall.", "sections": [{"title": "I. INTRODUCTION", "content": "The future of wireless communication involves the deploy- ment of large-scale and complex scenarios such as vehicle platooning, smart factories, and connected and autonomous vehicles (CAVs) [1], [2]. These applications often come with stringent quality of service (QoS) requirements, including ultra- low latency, high reliability, limited transmission power and information freshness [3], [4]. Age of information (AoI) is a metric that measures the freshness of data, i.e., the time elapsed since the last received packet was generated. Minimizing the Aol is a key challenge across a wide range of wireless com- munication applications [5], particularly for the uplink of low energy devices [6], [7].\nRecently, unmanned aerial vehicles (UAVs) have been ex- plored as flying base stations (BSs) to optimize Aol and transmission power [8]. UAVs offer remarkable flexibility by positioning themselves closer to the devices they serve, thus improving line-of-sight (LoS) probability and increasing the likelihood of successful transmission. Additionally, by reducing the distance between the UAV and the devices, energy con- sumption is minimized, leading to longer battery life for the devices. UAVs can optimize their flight trajectories and device scheduling policies to jointly minimize Aol and transmission power jointly [9].\nIn this regard, future wireless networks, especially UAV- based networks, must incorporate intelligent decision-making algorithms. Machine learning and artificial intelligence (ML/AI) are pivotal for the success of 5G and beyond and future 6G networks, providing the necessary flexibility and scalability to manage large and complex wireless environ- ments [10]. Among ML techniques, reinforcement learning (RL) stands out for its effectiveness in decision-making and control tasks. RL typically frames problems as Markov decision processes (MDPs), where an agent observes the current state of the environment, takes an action, transitions to a new state, and receives a reward that corresponds with the effectiveness of the chosen action in that state. Given the sequential nature of wireless networks, UAV trajectory planning can be framed as an MDP and solved using efficient RL algorithms to jointly minimize both the Aol and transmission power [11], [12].\nAlthough conventional RL has shown promising potential in UAV networks, it faces two main challenges. First, the UAV system is typically more complex and high-dimensional, particularly in terms of the size of the state and action spaces. Second, the policies designed for a specific network setup, de- fined by factors such as the number of devices, their locations, the channel model, and the traffic patterns of the applications they serve, are often not transferable to networks with different configurations or characteristics. To address these challenges, deep RL and meta-learning have emerged as two promising frameworks designed to overcome these limitations.\nDeep RL has emerged as a promising solution to address the limitations of traditional RL. By combining deep neural networks with conventional RL techniques, deep RL offers an explicit approach to handling complex, high-dimensional environments. One notable deep RL algorithm is the deep Q-Network (DQN) [13], which revolutionized the RL field due to its ability to master complex video games without prior knowledge. DQNs use deep neural networks as function approximators to estimate the Q-function, which evaluates the quality of each action in a given state. Moreover, DQNs rely on off-policy learning, where experiences from different policies are used to improve the current policy. Deep RL, specifically"}, {"title": "A. Literature Review", "content": "Recent advancements in UAV-assisted wireless networks have focused on minimizing Aol to ensure real-time data fresh- ness in dynamic applications. The use of UAVs for enhancing wireless communication coverage and data collection has been studied in various challenging environments in [16]. This adapt- ability enables UAVs to maintain real-time connectivity in areas without infrastructure and supporting applications. In [17], the authors proposed a UAV trajectory optimization framework to minimize Aol in wireless sensor networks (WSNs) by balancing data transmission and flight times for timely data collection. Alternatively, a deep deterministic policy gradient (DDPG) algorithm was proposed in [18] to balance AoI, energy consumption, and data collection efficiency in UAV networks, highlighting the necessary trade-offs in real-time decision- making. This approach uses a multi-objective framework that allows the UAV to adjust its behavior based on priorities, optimizing performance across competing demands.\nRecent studies have explored RL and deep RL techniques to enhance UAV performance in wireless networks. A recent survey in [19] highlights the extensive applications of RL in multi-UAV wireless networks, discussing its potential for trajectory planning and dynamic resource allocation to support complex and collaborative missions. Authors in [20] developed a novel RL approach using a double deep Q-network (DDQN) to optimize UAV path planning for data collection in urban IoT networks, overcoming challenges such as limited flight time and the presence of obstacles. Building on existing rein- forcement learning applications in wireless networks, authors in [21] went one step further. They proposed a deep RL-based trajectory planning algorithm to minimize the Aol in UAV- assisted IoT networks. By framing the trajectory planning as a MDP, their approach dynamically adjusts to unpredictable traffic patterns, demonstrating enhanced data freshness and robustness in real-time UAV operations. Another trajectory planning algorithm is discussed in [22] for energy consumption minimization in UAV-assisted WSNs using deep RL. This approach prioritizes energy-efficient UAV paths by dynamically adjusting the trajectory in response to network conditions, extending operational time, while maintaining effective data collection. The study in [23] focuses on using deep RL to optimize resource allocation, including UAV positioning and communication management, to enhance network performance and throughput in cooperative UAV systems.\nMeta-learning has shown significant promise in wireless communications by enabling fast adaptation to new environ- ments with minimal data. The survey in [24] reviews various meta-learning techniques, emphasizing their ability to optimize wireless policies such as beamforming, resource allocation, and channel estimation, even when the underlying network conditions change dynamically. Authors in [25] propose a meta-learning approach to optimize non-convex problems in large-scale wireless systems, highlighting how meta-learning can significantly reduce the complexity of conventional opti- mization algorithms. Meta-reinforcement learning (Meta-RL) leverages meta-learning techniques to boost the adaptability of RL models, enabling faster learning and improved performance in dynamic environments with limited data. Meta-RL can be utilized for trajectory optimization of UAVs as the meta- learning algorithm tunes the hyperparameters of an RL solution, allowing the UAV to adapt quickly to new environments [26]. Another meta-RL approach was introduced by authors in [27] to optimize unmanned aerial base stations trajectory planning. This method adapts quickly to new traffic patterns by trans- ferring knowledge from previous configurations, reducing the number of training episodes needed to optimize its policy for new scenarios."}, {"title": "B. Contributions", "content": "This paper presents a meta-deep RL framework combining DQN with MAML for efficient adaptation to dynamic multi- objective optimization in UAV-aided IoT networks. The main contributions of this paper are summarized as follows:\n\u2022 We consider the problem of optimizing the UAV trajectory and its scheduling policy to jointly minimize the Aol and transmission power of low energy IoT devices. We solve the problem using DQN, a well-known deep RL algorithm, which estimates the optimum Q-function using a neural network.\n\u2022 We propose combining DQN with MAML, a meta- learning algorithm, to ensure scalability when changing the network configurations.\n\u2022 We consider a multi-objective problem that targets min- imizing the weighted Aol and weighted power. In this"}, {"title": "C. Outline", "content": "The rest of the paper is organized as follows: Section II introduces the system model. The proposed meta-RL algorithm is presented in Section III. Section V evaluates the proposed algorithm through numerical analysis. Finally, conclusions and future work are discussed in section VI. Table I summarizes the list of notations used in the paper."}, {"title": "II. SYSTEM MODEL AND PROBLEM FORMULATION", "content": "Consider an L \u00d7 L 2D grid populated with stationary limited- power IoT devices (termed simply devices in what follows), represented by D = {1,2,\u2026\u2026\u2026,D}. Each device d \u2208 D is randomly deployed and positioned at a specific coordinate $c_d$ = ($x_d$, $y_d$) on the grid. We consider a UAV flying above this grid at a fixed altitude, whose task is to gather data from the devices. The UAV starts flying from an initial random position within the grid, and its movement is constrained, ensuring it does not egress from the grid's boundaries. The UAV can move in four cardinal directions (north, east, south, and west) or hover in its current position.\nIn this model, we consider an episodic environment, where an episode is segmented into discrete time intervals, [t, 2t,\u2026\u2026], where t specifies the duration of each time step the UAV moves from the center of a cell to the center of an adjacent cell, according to the selected movement direction, and receive information from a device. The position of the UAV at time t is fully described by its 2D projection $C_u$ = ($x_u$, $y_u$) on the plane and its altitude $h_u$.\nThe scheduling policy determines which device can transmit data during each time slot. Scheduling policy $w_u(t) = d$ indicates that device d is set to transmit during time slot t. Only one device transmits per time slot to avoid interference. We assume a line-of-sight (LoS) communication between the devices and the UAV, where the channel gain between the UAV and device d at time slot t is given by [28]\n$g_{u,d}(t) = \\frac{g_0}{|h_u|^2 + r_{u,d}^2(t)},$ (1)\nwhere $g_0$ is the channel gain at the reference distance of 1 m, $h_u$ is the altitude of the UAV, and $r_{u,d}(t)$ is the Euclidean distance between the device and the UAV at time t. Hence, the transmit power $P_d$ of device d at time t is calculated as follows\n$P_d(t) = \\frac{(2^M - 1)^2 \\sigma^2}{g_{u,d}(t)} = \\frac{(2^M - 1)^2 \\sigma^2}{g_0}(r_{u,d}^2(t) + h_u^2)$ (2)\nwhere M is the packet size of the received message, $B_W$ is the signal bandwidth and $\\sigma^2$ the noise power.\nWe use the Aol as a performance metric to quantify the freshness of data collected from IoT devices [21]. The Aol for a device d at a given time t, denoted as $A_d(t)$, is defined as the time elapsed since the last successful time update received from device d was generated. Assuming instant transmission, the evolution of Aol for device d at the next time step t + 1 is given by\n$A_d(t+1) = \\begin{cases} 1, & \\text{if } w_u(t) = d, \\\\ min\\{A_{max}, A_d(t) + 1\\}, & \\text{otherwise,} \\end{cases}$ (3)\nwhere $A_{max}$ denotes the maximum practically allowed Aol in the model."}, {"title": "A. Problem Formulation", "content": "This problem aims to optimize the UAV trajectory and its scheduling policy to minimize the Aol and the power consump- tion of the IoT devices using a few shots of interactive data with the environment. The objective function varies depending on the trade-off between Aol and power consumption, which is controlled by a parameter to address different optimization priorities. We formulate the problem as\nP1: $\\underset{C_u(t), S(t)}{\\text{min}} \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{d=1}^{D} \\lambda w A_d(t) + P_d(t),$ (4a)\ns.t. $C_i \\in E_{max}, \\forall i \\in \\{1, 2, ..., I\\},$ (4b)\nwhere $\\lambda$ is a scaling factor that controls the trade-off between Aol and transmission power. $w_d$ denotes the weight assigned to each device d reflecting its relative importance of AoI. I represents the number of tasks. A higher value of $\\lambda$ increases"}, {"title": "III. BACKGROUND", "content": "In this section, we introduce deep reinforcement learning and meta-learning fundamentals. These materials will help introduce the proposed deep meta-RL algorithm presented in the next section."}, {"title": "A. Markov Decision Processes Formulation", "content": "The RL problem is formulated as a Markov Decision Process (MDP), which is composed of the tuple (s, a,r,p), where s is the state, a is the action, r denotes the reward function and p is the state transition probability. At time instant t, the agent observes the current state s(t) from the environment, selects an action a(t) following a policy \u03c0, which describes the selected action at each particular state, receives an immediate reward r(t) for selecting that action at that state, and transits to the next state s(t+1) according to the transition probability p(s(t+1)|s(t), a(t)). The agent aims to find the optimum policy that maximizes the return G(t), which is the accumulative discounted reward\n$G(t) = \\sum_{k=0}^{\\infty} \\gamma^k r(t + k + 1),$ (5)\nwhere \u03b3 is the discount factor that controls how long we care about future rewards compared to immediate rewards. Setting \u03b3 \u2192 0 means we focus on immediate rewards more than future rewards, whereas \u03b3 \u2192 1 considers future rewards in the return function. Next, we define each of these elements in the context of the formulated problem and the target is to optimize both UAV trajectory and its scheduling in order to minimize the Aol and transmission power.\n1) State space: The state space represents all the observa- tions available to the agent at a given time t. The state space of the system at time slot t is defined as s(t) = ($C_u(t)$, A(t)) where $C_u(t)$ is the current 2D location of the UAV at time slot t, and A(t) = ($A_1(t), A_2(t), ..., A_D(t)$) is a vector that contains the individual Aol of each IoT device, while $A_d(t) \u2208 \\{1, 2, ..., A_{max}\\}$. The cardinality of the state space is |s| = 2 + D.\n2) Action space: The action space represents the set of available decisions that the agent can take based on the current state. The action space at time slot t is defined as a(t) =\n($w_u(t)$, $m_u(t)$), where $w_u(t)$ is the scheduling policy of the UAV (i.e., $w_u(t)$ = d means that the UAV chooses device d to serve at time t), and $m_u(t)$ is describes the movement direction of the UAV (i.e., north, south, east, west, or hovering). The number of possible actions is given by D \u00d7 5.\n3) Reward function: The reward function is defined by the weighted average Aol values and energy consumption of all the devices for each action. A scaling factor, \u03bb, is used to evaluate the trade-off between energy consumption relative to Aol. The overall goal of the UAV is to maximize its reward over time by making choices that efficiently balance these two"}, {"title": "B. Deep Reinforcement Learning", "content": "The agent's objective is to learn the optimal policy \u03c0* that maximizes the accumulative discounted rewards. The Q- function Q(s(t), a(t)) maps the expected reward of being at a state s(t), taking an action a(t), and transiting to state s(t + 1). Hence, to find the optimal policy, we need to find the optimal Q-function Q*(s(t), a(t)), which is achieved using a Q-learning algorithm that updates the current Q-function iteratively using the Bellman equation\n$Q(s(t), a(t)) = Q(s(t), a(t)) + \\alpha (r(t) + \\gamma \\underset{a}{\\text{max}} Q(s(t+1), a) - Q(s(t), a(t))),$ (8)\nwhere \u03b1 is the learning rate and \u03b3Q (s (t+1), a (t + 1)) is the discounted state-action value at time instant t + 1. The agent employs an \u03f5-greedy exploration policy, where it primarily selects actions that maximize the Q-function with a probability 1 - \u03f5 and explores suboptimal actions with a small, yet dynamic probability \u03f5 to ensure sufficient exploration of the environment. Frequently, \u03f5 is set to a large value at the first episodes to ensure enough exploration and then decays with time [30].\nTo this end, Q-learning stands short in complex and large-dimension environments as it needs to visit many state-action pairs. Deep RL extends the traditional reinforcement learning methods with deep neural networks. DQN is one family of deep RL that estimates the Q-function using deep neural networks. In addition, DQNs utilize key techniques to stabilize the learning process by using a target network and replay memory buffer. Specifically, DQNs adopt two neural networks that work together to improve the learning process. The first network estimates the current Q-values based on the agent's actions and"}, {"title": "C. Meta-Learning", "content": "Meta-learning is a framework designed to equip models with the capability to rapidly adapt to new tasks using minimal data. MAML builds on this concept by training models to learn the initial parameters that can be optimized for fast adaptation, allowing standard models to quickly adjust to new tasks with minimal updates [31]. Utilizing learning across functions, we can find the initial parameters (model weights) \u03b8 that can reach convergence through a few SGD steps on new unseen tasks. To implement the adaptation and evaluation process during training, each task is divided into a support and query set. The support set is used to fine-tune the model to a specific task, while the query set evaluates the performance of the adapted model on the same task.\nFirst, the weights are randomly initialized and assigned to each task. Then, we update the weights for each task as follows\n$\\theta_i = \\theta - \\alpha\\nabla_\\theta L_{T_i}(\\theta),$ (9)\nwhere \u03b1 is the adaptation learning rate, $L_{T_i}(\\theta)$ is a chosen task- specific loss function and $\u2207_\u03b8L_{T_i}(\u03b8)$ is the gradient of a chosen loss function for task $T_i$ After adapting to individual tasks, we calculate the individual losses using the adapted weights for each task\n$L(\\theta) = \\sum_{T_i}L_{T_i}(\\theta),$ (10)"}, {"title": "IV. THE PROPOSED META-RL", "content": "In this work, we propose a meta-RL approach where the UAV interacts with diverse tasks, which are generated by varying the trade-off parameter, \u03bb in the reward function, which controls the trade-off between Aol and transmission power and network configurations. Each corresponds to a different environment with a different objective. We target utilizing learning across tasks to adapt quickly to new unseen tasks. We adopt DQNs to optimize the UAV policy for each task and MAML to find the optimal Q-network initial weights that enable fast"}, {"title": "V. EXPERIMENTAL ANALYSIS", "content": "In this section, we discuss the simulation results to demon- strate the effectiveness of the proposed Meta-RL algorithm and compare it with the baseline RL model with random Q-network initialization. The environment is described by a grid world of 1000 m x 1000 m, which is divided into 10 x 10 cells. We consider a UAV serving 5 devices and 10 devices, respectively. The Q-network consists of three layers with two hidden layers of 256 neurons. We adopt ReLU as the activation function and Adam optimizer to update the neural network weights. The replay buffer has a size of size 100000. Meta-training comprises 500 epochs, which are trained using the Pytorch framework on the NVIDIA Tesla V100 GPU. The simulation parameters are defined in Table II.\nWe define 10 different meta-training objective functions with varying \u03bb values, which adjusts the priority assigned for power consumption in the reward function allowing the model to learn and adapt to various setups. We consider equal importance of Aol across all devices, i.e. $w_d$ = 1."}, {"title": "VI. CONCLUSIONS", "content": "In this paper, we introduced a meta-deep RL algorithm to optimize the UAV's trajectory and scheduling policy to minimize Aol and transmission power. We first developed a DQN algorithm to determine the optimal UAV policy. Then, we integrated this DQN with MAML to identify the best initial weights of the Q-network, enabling rapid adaptation to dynamic networks with varying objectives. Simulation results highlighted the advantages of using MAML over conventional deep RL methods, demonstrating faster convergence within a few training episodes. Additionally, the proposed approach achieved the lowest combined Aol and transmission power for different objective functions. Deploying the proposed algorithm in a massive network served via multiple UAVs using multi- agent meta-RL (MAMRL) is left for the future."}]}