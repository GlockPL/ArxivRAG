{"title": "Emergent effects of scaling on the functional hierarchies within large language models", "authors": ["Paul C. Bogdan"], "abstract": "Transformer architectures usually involve repeated transformer layers of the same size. However, the architecture is often described as functionally hierarchical: Early layers process syntax, middle layers begin to parse semantics, and late layers integrate information across a wide scale. The present work revisits these ideas within large language models (LLMs).", "sections": [{"title": "Introduction", "content": "Modern LLMs are generally based on the transformer architecture, consisting of identical transformer layers stacked in sequence. Despite layer size being constant, many researchers hypothesize that these layers develop a functional hierarchy during training: Early layers process syntax, middle layers begin to handle semantics, and deeper layers integrate information more broadly. 1,2 This hypothesized hierarchical structure is pervasive in descriptions of LLM functioning, and it presumably stems from the prevalence of abstraction hierarchies across machine learning models on different domains.\nYet empirical investigation of this posed functional hierarchy remains limited, particularly for modern LLMs. Most studies providing layer-by-layer analyses have instead relied on older LLMs (often BERT), which may not reflect the emergent properties of contemporary models.2-4 In addition, prior studies have usually focused on carefully understanding just one form of information \u2013 e.g., sentence-level semantic similarity \u2013 while not actually mapping out a gradient of semantic abstraction.5\nTo investigate functional hierarchies in modern LLMs and the roles of different layers, I conducted several experiments. Each experiment follows a common structure: A dataset of tiny or medium-length texts was prepared (e.g., \u201cAn apple\"). Each text was submitted to an LLM (Llama-3.2-3b or Llama-3.3-70b-Instruct). The LLMs' resulting activation patterns were extracted, separately for each layer. These activations were submitted to support vector machines (SVMs) or ridge regressions, attempting to predict some property of the input text (e.g., whether the object in \"An apple\u201d is edible). By varying the input text and its labels, my analyses attempt to tease out the information encoded (linearly) at different levels."}, {"title": "Experiment 1: Item-level semantics", "content": "Analyses first investigated item-level semantic coding. This experiment referenced public data from a human study on semantic processing (https://cslb.psychol.cam.ac.uk/propnorms). In the study, participants were shown objects and asked to come up with features describing each one (e.g., when shown an \u201capple,\" a participant may respond \"is edible\u201d or \u201cis a fruit\u201d). The present analyses were based on 20 of the most common features and 300 of the objects associated with the most features.\nFor each object, Llama-3.2-3b was fed a two-word text (e.g., \u201cAn apple\u201d) and activations corresponding to the object were extracted; for words split across multiple tokens, the tokens were averaged; loose analyses suggested this was better than taking the activations of just the last token. Activations were extracted separately for (i) the residual stream input into each transformer layer, (ii) the outputs of the attention layer added to the stream, e.g., the outputs added to the \u201capple\u201d tokens based on all prior tokens, and (iii) the outputs of the feed-forward network layer added to the stream. Separately for each of these components each layer, SVMs were fit for each of the 20 human-reported features. Each SVM attempted to predict whether an object had (1) or did not have (0) each feature, using standard 6-fold cross-validation. Three potentially interesting patterns emerge. First, the representation of item-level semantics in the residual stream and feed-forward network (FFN) output peaks in early layers (layers 2-7) before descending but turning upwards again in the final layers. This rise, fall, and ultimate brief"}, {"title": "Experiment 2: Two-item relations", "content": "The experiment 2 analyses investigated how transformers encode the relations between two items, across three branches of tests. The tests examined how an LLM encodes: (A) whether an object is likely to be found in a scene, (B) broadly, whether two items are semantically related in some way, and (C) more narrowly, whether a herbivore/carnivore would eat a plant/meat. Each of these tests followed a similar structure as in Experiment 1, where texts are crafted and submitted to Llama-3.2-3b, and then the resulting activations of different layers are used to fit classifiers."}, {"title": "Experiment 2A", "content": "This experiment examined how a model encodes object-scene relationships and used data from a study by my research group. In the study, participants were shown 114 objects, each paired with one of 114 scenes. For example, participants may be shown a tractor on a farm (likely to be found), a bug in a desert (moderately likely), or a poker table in a prison (unlikely to be found). Overall, 342 object-scene pairs were designed and administered to participants. For each object-scene pair, participants used a 4-point scale to report how likely they perceived it to be for the object to be found in the scene (1 = \u201cVery unlikely\"; 4 = \u201cVery likely\u201d). For the present LLM research, each object-scene pair's likeliness score was averaged across all participants, and I investigated how the LLM encodes this likeliness.\nThe object-scene pairs were used to produce texts (\"In a {SCENE}, an {OBJECT}\"), which were submitted to Llama-3.2-3b, and activations were extracted corresponding to the object embedding. The activations were used to fit Ridge regressions predicting the object-scene's likeliness score. Group-6-fold cross-validation was used such that scene-object examples were grouped by object, and these groups were preserved across the folds.  shows the cross-validated accuracies. The representation of the relation in the residual stream peaked at layer 11 \u2013 noticeably deeper than the item-level semantics peak in Figure 1. This analysis was also done while defining the texts as \u201cA {SCENE} and {OBJECT}\", which yielded similar patterns. Interestingly, there was less of a dip following this peak, which may reflect scene/context words having a type of global relevance represented in deep layers, but this was not otherwise tested. Most relevant though for the present work was this two-item semantic relation peak being deeper than the one-item semantics earlier. This is consistent with model layers progressively increasing in abstraction.\""}, {"title": "Experiment 2B", "content": "This next experiment examined two-item relations between pairs of objects. The experiment leveraged human data from a study where participants used a 4-point scale to rate the relatedness words in pairs (e.g., \u201cchurch\u201d and \"organ\") (https://osf.io/5q6th/). Participants rated this for sixty word pairs, which were used here to produce the texts \u201cAn {ITEM 1} and {ITEM 2}\" and the flipped \u201cAn {ITEM 2} and {ITEM 1}\u201d. These texts were submitted to Llama-3.2-3b. Activations were extracted and then submitted to an SVM, which yielded the accuracies shown in Figure 2b. The peak is consistent with those of Figure 2a. Interestingly, the drop in later layers is much starker, although this result overall provides more evidence that two-item relation representation peak proceeds the one-item semantic representation peak."}, {"title": "Experiment 2C", "content": "A final test of two-item relations studied this topic more narrowly than the type of general \"relatedness\" examined thus far. Here, I prepared two-item texts where one item was always a food (plant or meat) and the other was an animal (herbivore or carnivore) (e.g., \u201cA {FOOD} and {ANIMAL}\"). The texts were submitted to Llama-3.2-3b, activations were extracted, and SVMs were fit, predicting whether the text's animal would eat the food. Cross-validation was structured using 2-group folds, where training was always done on herbivores then testing on carnivores or vice versa; this required z-normalizing the activations with respect to the food examples, as each food would flip labels between the training and testing sets. This procedure was done using texts where the food was mentioned first (e.g., \u201cA leaf and a deer\") to generate accuracy estimates, and the procedure was also independently done using texts where the food was mentioned second (e.g., \u201cA deer and a leaf\") to generate accuracy estimates. The accuracies were averaged across these two versions, and those are reported in Figure 2C. Again, we see a peak layer 10, providing further evidence of two-item relations being parsed following the item-level semantic effects of Figure 1.\""}, {"title": "Experiment 3: Four-object analogies", "content": "To reach yet deeper layers, the next analyses considered four-object analogies. Using public LLM websites (e.g., Claude and ChatGPT), I generated fifty analogies and prepared texts for them (e.g., \"Like a seed and a tree, an egg and a chicken\"). For each analogy (AB:CD), three analogous valid variants were produced (BA:DC, CD:AB, DC:BA). For the classification, the analogies were also used to prepare invalid variants that an SVM would distinguish. There were two types of invalid analogies: Easy invalids were produced where the second and fourth words were flipped (e.g., \u201cLike a seed and a chicken, an egg and a tree\u201d). Distinguishing these from the valid analogies is easy because it can be done either while noticing a discrepancy between just the first two-item relation, just the second, or both. Hard invalids were produced by flipping just the third and fourth words (e.g., \"Like a seed and a tree, a chicken and an egg\"); thus, both first and second halves of the text still contain two-item relations and invalidity only emerges if the two halves are contrasted analogically.\nThe accuracy patterns suggest analogical processing occurs deeper than two-item relational processing. To serve as a reference point, Figure 3a provides new two-item relation results using the present dataset. SVMs classified the valid versus easy invalid examples based on the activations at the second word \u2013 similar to the object-object relation classification of Experiment 2B; the peak of the two-item relation was seen around layer 10 as before. next shows the valid/easy-invalid classification results based on the four word's activations, which produced a peak slightly later.  demonstrates another similarly later peak for valid/hard-invalid classification. These differences relative to two-item-relation representation are more subtle than the earlier differences with item-level representation. These patterns nonetheless are consistent with a functional abstraction hierarchy linked to layer progression. However, the next analyses would challenge a simplistic perspective of this hierarchy.\""}, {"title": "Experiment 4: Burying text", "content": "In an attempt to manipulate the layers in the back half of Llama-3.2-3b, the initial texts were lengthened, and classification attempted to distinguish texts based on early content. The texts were lengthened by burying the target content behind roughly 100 words of filler. This was performed for Experiments 1, 2, and 3. For example, the Experiment 2B texts, which consisted of \"An {ITEM 1} and {ITEM 2}\", were expanded to:\n\u201cAn {ITEM 1} and {ITEM 2} are here. I thought about this for a long while. The more I pondered, the clearer it became that my initial reaction was just the tip of the iceberg. There were layers to this issue, complexities that I hadn't considered at first glance. Each new angle brought a different perspective, challenging my assumptions and making me question what I thought I knew. It was like peeling an onion, revealing not just answers, but more questions, more nuances to explore\u201d\nsuffix was held constant for every experiment, except for the \u201care here\u201d portion at the beginning, which was changed as necessary to produce grammatically proper sentences from the start. Activations were extracted from the last word (\u201cexplore\u201d). Thus, an SVM trained on the activations would make predictions about that initial sentence (e.g., whether items 1 and 2 are related) based on its signature that is preserved through the filler text. Despite the challenges that the filler text presumably creates, shows that SVM accuracy was still considerably above chance. However, representations are consistently shifted into deeper layers.\u00b9 This clear pattern is consistent with later layers capturing more global document-wide information. Yet, whether this coding should be seen as true abstraction is less obvious, as the classification reflects just basic information from only the first sentence. Further experiments would dig deeper into this idea using a much larger model and produce more peculiar deviations from an abstraction hierarchy conception."}, {"title": "Experiment 5: Large llama", "content": "An extended last experiment probed a larger model: Llama-3.3-70b-Instruct, which contains 80 layers; Llama-3.3, which only provides an \u201cInstruct\u201d variant, as of December 2024.\u00b2 This larger model was submitted to several of the same experiments as before (Figure 5). In terms of classification accuracy, the larger model performed about as well as the smaller one for the easier classification tasks but achieved better accuracy in the difficult ones. That is, the larger model yielded similar high points in accuracy for object-scene likelihood and object relatedness. By contrast, the larger model accuracy was improved for animal/food classification which may reflect this being a less obvious property than generic relatedness. Larger model accuracy was also greater for the analogy-hard classification, which may reflect the difficulty. Additionally, the larger model found greater success for each of the buried-concept classifications, which is consistent with generally improved long-context capabilities.\nAlong with these sensible improvements in accuracy, there was also one more surprising glaring pattern, which emerged for the two-item relation and four-item analogy experiments: All four experiments show two robustly distinct peaks, which are most evident in the attention and FFN layer outputs. The first peak emerges around layers 12-16 and the second around layers 25-33. In other words, these semantic properties are most represented in two distinct spots. In between these peaks, there is a dip. If the peaks are taken to shed light on abstraction, this may reflect an increase of abstraction (producing the first peak then dip), then a decrease (producing the second peak), and then a further increase in abstraction (producing the second-half descent). None of the experiments on the smaller Llama-3.2-3b produced double peaks suggesting that this is an emergent property of scaling. Next, analyses turned to understanding what the model is accomplishing through this valley."}, {"title": "Analyses 1", "content": "To better understand the dip-valley-dip representational profile, I examined the attention output representations more carefully, as this component showed the largest swings. For this component, all seven of the test experiments' accuracies were z-scored (across the different layers for a given experiment). Then, the z-scored accuracies were overlaid, which is shown in Figure 6a. Remarkably, the valley in the two/four-item experiments coincides with a rise in representation seen in the buried-concept experiments (the rising blue lines in Figure 6\u0430). \nThis is consistent with a steady rise in abstraction through the start of the valley. The overlay also shows how the buried concepts' peak representation overlays with the second peak of the non-buried concept; this is also evident looking back to Figures 5g-i, where the residual stream's representation of buried concepts is strongest around layer 35.\nIt is unclear what interpretation can unify these results. Potentially, there exist two functional hierarchies: the first hierarchy processes"}, {"title": "Analyses 2", "content": "One interesting final pattern noticed in the large model data is the large fluctuation in the back halves of the attention-layer outputs (notice zigzags in layers 40-70 of ). That is, the information that the attention layers add to the residual stream shifts each layer between strongly representing the two/four-item semantic information to weakly representing said information. This can be quantified using the accuracy series' first derivatives \u2013 i.e., the differences between neighboring layers' accuracies.  shows these first derivatives and the prominent fluctuations. This can be quantified as the first derivative's (Spearman) autocorrelation, and this is consistently negative (mean: \u03c1 = -.33 [standard deviation = .12]), which is a phenomenon referred to as anti-persistence. This is specifically an effect between directly adjacent layers, as the autocorrelations between the accuracy difference of layers two, three, or four layers apart are all near zero (-.08 < \u03c1s < .09). However, studying a yet larger model, such as the 405b-parameter variant of Llama, could produce multi-layer patterns. Regardless, this anti-persistence is a curious pattern.\nAdding further peculiarity, the precise high/low layers in this fluctuation are largely consistent across experiments: Notice the first-derivative accuracy overlap across experiments in along with the strong correlations between experiments in (correlation matrix mean: \u03c1 = .31 [SD = .20]). This consistency suggests that thy\ne attention layers are not repeatedly overshooting some desired spot in latent space, but rather there is a type of coordination between adjacent layers. To a degree, these fluctuations seem to be emergent properties of scaling. Examining Llama-3.2-3b's attention output accuracies also shows anti-persistence (mean: \u03c1 = -.41 [SD = .15]), but these are not consistent across experiments (correlation matrix mean: \u03c1 = .06 [SD = .20]). Returning to the initial focus on functional abstraction hierarchies, these fluctuations are not necessarily inconsistent with that idea. However, these fluctuations represent a separate aspect of LLM's functional architectures."}, {"title": "Conclusion", "content": "This research advances our understanding of how transformer layers encode semantic information, revealing both expected hierarchical trends and deviations from these trends.4 These findings refine prevailing assumptions about transformer dynamics and open avenues for exploring how architectural innovations and scaling influence the emergent properties of modern LLMs.\nOne of the novel components of the present research was the investigation of how \u201cburied\" concepts are represented in Experiments 5 and 6. For this, the input text consisted of the target of the research briefly (e.g., \u201cAn {item 1} and {item 2}\"), followed by ~100 words of filler that is constant across every example. Then, analyses examined whether a model's activations after parsing the filter still represent the target despite the addition filler \u2013 i.e., asking whether the target representation persists. This was the case, but representation shifted starkly toward later layers. Potentially, this reflects a general compression of all information from past tokens, even if the past tokens are entirely unrelated to future ones. If so, deeper layers are not only identifying abstractions but rather, depth enhances the window of content processed even if there is little form to the information being compressed.\nThese results on buried concepts suggest that manipulating activations deep in a layer can simulate noticing a word in the past. This may be useful for steering and may open the door to \u201canalogical steering\u201d, where the goal is to loosely encourage a model to approach some concept. For instance, prior work has shown how middle-layer representations of a concept can be identified by examining the activation patterns associated with the most recent concept (e.g., the activation patterns of the \u201cGolden Gate Bridge\"). By stimulating the middle layer components activated by the concept, researchers can cause a model's outputs to sharply discuss said concept (e.g., have the model discuss the Golden Gate Bridge). However, the present work shows how late layer representation of \u201cburied\" concepts can be extracted \u2013 i.e., the representation of the \u201cGolden Gate Bridge\" mentioned 100 filler words in the past. Steering as so may allow aspects of the representation to color the model's output without causing a fixation on the concept itself. This type of late-layer steering may allow a model to be gently nudged along a long-context window. In addition, this type of steering may be useful for implementing broadly relevant safety-related goals.\nThe large model (larger Llama-3.3-70b-Instruct) showed one stark deviation from a steady abstraction hierarchy. For the representation of both two-item relations and four-item analogies in simple texts, the model displayed two distinct and consistent representation peaks in its middle layers. That is, the representation of this local information first peaked in layers 12-16, then dipped, and then rose again and displayed a second peak in layers 25-33. Analysis of the buried concepts helped unpack this phenomenon, showing how representation of two/four-item relational information historic in the context window peaked around layer 33. The double-peak pattern points to how scaling introduces new emergent behaviors. While this may enhance model capabilities, it also complicates interpretability, making it harder to anticipate or control model outputs.\"\n    }"}]}