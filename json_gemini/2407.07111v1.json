{"title": "Diffusion Model-Based Video Editing: A Survey", "authors": ["Wenhao Sun", "Rong-Cheng Tu", "Jingyi Liao", "Dacheng Tao"], "abstract": "The rapid development of diffusion models (DMs) has significantly advanced image and video applications, making \"what you want is what you see\" a reality. Among these, video editing has gained substantial attention and seen a swift rise in research activity, necessitating a comprehensive and systematic review of the existing literature. This paper reviews diffusion model-based video editing techniques, including theoretical foundations and practical applications. We begin by overviewing the mathematical formulation and image domain's key methods. Subsequently, we categorize video editing approaches by the inherent connections of their core technologies, depicting evolutionary trajectory. This paper also dives into novel applications, including point-based editing and pose-guided human video editing. Additionally, we present a comprehensive comparison using our newly introduced V2VBench. Building on the progress achieved to date, the paper concludes with ongoing challenges and potential directions for future research.", "sections": [{"title": "INTRODUCTION", "content": "RECENT years have witnessed remarkable development in Artificial Intelligence Generated Contents (AIGC), catalyzed by high-quality datasets and efficient computational infrastructure. Particularly in computer vision, AI's impressive capability to generate high-dimensional perceptual data has attracted interest from researchers and exerted a profound significance on industrial products and daily life. Diffusion models [1] have emerged as a leading approach for vision generation tasks, including Text-to-Image (T2I) generation [2], [3], [4], [5], [6], [7], [8] and Image-to-Image (I2I) translation [9], [10], [11], [12], [13], [14], [15], [16]. These methods can generate high-quality semantically accurate images from text descriptions or modify input images based on specified conditions.\nConsidering that videos are primarily composed of continuous image sequences, there is a natural inclination to extend the success of diffusion models from 2D image generation to 3D video generation and editing, which has sparked considerable research interest. However, extending these architectures to videos introduces challenges, mainly in adapting designs for static images to handle videos' dynamic and temporal aspects. The scarcity of high-quality video datasets adds further technical difficulties. Researchers must tackle effective video dataset curation or consider alternative surrogate tasks and other solutions.\nDespite these challenges, recent advancements have been witnessed in diffusion model-based video generative methods [17], [18], [19], [20], [21], [22]. Among these methods, the most influential areas are generating videos from text inputs and editing existing videos generatively. Furthermore, editing existing videos does not require prohibitively expensive video pre-training and allows fine-grained control of the source video, leading to diverse applications, as illustrated in Figure 1.\nAs the field of diffusion-based video editing continues to expand, there is a growing need for a detailed survey that helps readers stay informed about recent advancements. However, existing surveys on diffusion models primarily focus on other aspects or tasks, such as the theoretical formulation of diffusion processes [23], [24], [25], [26], image generation [27], [28], image editing [29], and 3D object generation [30], [31]. While some surveys mentioned video editing [30], [32], they only provide a brief overview, missing influential methods, detailed narration, and the underlying linkages within these emerging approaches.\nThis paper provides a thorough review of diffusion model-based video editing technologies. It covers a broad spectrum of editing tasks, methodologies, and evaluations, offering valuable insights. This survey not only deepens the understanding of the current landscape but also outlines potential developments and applications, highlighting directions for future research.\nThis paper is structured as follows: In Section 2, we review the preliminaries, including the mathematical formulation of diffusion models, related approaches in 2D image generation, and video generation methods. Subsequently, Section 3 categorizes diffusion model-based video editing approaches into five primary classes based on their underlying technologies. Section 4 introduces a new benchmarking, V2VBench, encompassing four text-guided video editing tasks, along with a detailed evaluation and analysis. Finally, in Section 5, we summarize the open challenges and outline emerging research trends."}, {"title": "BACKGROUND", "content": "This section presents an overview of the essential preliminaries for diffusion model-based video editing methods. We start with the mathematical framework of diffusion models (Section 2.1), followed by an exploration of image generation and editing techniques (Section 2.2). We then shift to the video domain, providing an overview of diffusion models for video generation and consolidating motion representation in Section 2.3. We focus on the discrete formulation of the diffusion process [39] and highlight the core contributions of representative works to maintain focus and conciseness.\n2.1 Mathematicial Framework\nDiffusion models [1] fall within the family of latent probabilistic models, alongside variational autoencoders (VAEs) [40], [41], normalization flows [42], etc. They are distinguished by possessing an analytical form of the latent distribution. Specifically, consider a sample zo randomly drawn from the training dataset, with each sample i.i.d. following an underlying distribution q(zo). The posterior distribution is defined by a Markov chain $q(z_{1:T}|z_0)=\\prod_{t=1}^Tq(z_t|z_{t-1})$, known as the forward diffusion process, which gradually adds Gaussian noise into the data:\n$q(z_t|z_{t-1}) := N(\\sqrt{\\alpha_t}z_{t-1},\\sigma^2I)$.\n(1)\nThe Markov chain hyperparameters $s_t = \\sqrt{\\alpha_t/\\alpha_{t-1}}$ and $\\sigma_t = \\sqrt{1-\\alpha_t/\\alpha_{t-1}}$ can be assigned to facilitate the variance preservation formulation [39], [43]. Here, the {$\\alpha_t$}$_{t=0}^T$ denote a predefined set of hyperparameters that adhere to 0 < $\\alpha_T$ < . . . < $\\alpha_0$ = 1.\nThe underlying data distribution q($z_0$), is estimated with the form p($z_0$)=$\\int$p($z_{0:T}$)d$z_{1:T}$. The joint distribution pe (zo:T), known as the reverse diffusion process, is commonly assumed to follow a Markov chain [1], [39] with learnable transition distribution:\n$P_\\theta(z_{t-1}|z_t) := N(\\mu_\\theta(z_t, t), \\Sigma_\\theta(z_t, t))$,\n(2)\nwhere $\\theta$ represents the trainable parameters.\n2.1.1 Diffusion Model Properties\nGaussian Marginal Distribution. The marginal distribution of the forward diffusion process follows Gaussian [1]:\n$q(z_t|z_0) := N(\\sqrt{\\bar{\\alpha}_t}z_0, (1 - \\bar{\\alpha}_t)I)$.\n(3)\nThis implies that the forward diffusion process can be achieved by directly interpolating between noise $\\epsilon_t$ ~ N(0, I) and the initial latent (clean sample) $z_0$ without considering intermediate latents. When $\\bar{\\alpha}_t$ is sufficiently closed to 0, q($z_T$|$z_0$) converges towards a standard Gaussian distribution N(0, I) for arbitray zo.\nScore. The score function is the gradient of the log-density with respect to the data vector [44]. Assuming a Gaussian distribution, the score takes the following form [43]:\n$\\nabla_{z_t} log q(z_t|z_0) = \\frac{z_t - \\sqrt{\\bar{\\alpha}_t}z_0}{1-\\bar{\\alpha}_t} = \\frac{\\epsilon_t}{\\sqrt{1 - \\bar{\\alpha}_t}}$\n(4)\nwhere $\\epsilon_t$ ~ N(0, I) is a standard Gaussian random variable.\n2.1.2 Diffusion Model Optimization\nThe diffusion models can be trained by optimizing the Evidence Lower Bound (ELBO) [39]:\n$ELBO = E_q [log \\frac{p_\\theta(z_{0:T})}{q(z_{1:T}|z_0)}]$.\n(5)\nAfter parameterization, the training is represented as an $\\epsilon$-prediction task [39], [45]:\n$L = E_{t,z_t,\\epsilon_t~N(0,1)} [\\lambda(t) || \\epsilon_t - \\epsilon_\\theta(z_t, t)||^2]$,\n(6)\nwhere $\\lambda$(t) > 0 serves as a weighting term dependent on t. Ho et al. [39] discovered that setting $\\lambda$(t) = 1 in Equation (6) enhances network learning by emphasizing the more challenging instances with larger t. Consequently, they simplified Equation (6) to:\n$L_{simple} = E_{t,z_t,\\epsilon_t} [|| \\epsilon_t - \\epsilon_\\theta(z_t, t)||^2]$.\n(7)\n2.1.3 Reverse Diffusion Sampling\nThe formulation of reverse diffusion sampling is:\n$z_{t-1} = \\sqrt{\\frac{\\alpha_{t-1}}{\\bar{\\alpha}_t}} (z_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\theta(z_t, t)) + \\sigma_t\\nu_t$,\n(8)\nwhere $z_{t0} = \\frac{1}{\\sqrt{\\alpha_t}} (z_t - \\sqrt{1 - \\alpha_t}\\epsilon(z_t, t))$,\n(9)\nand $\\sigma_t = \\sqrt{(1-\\bar{\\alpha}_{t-1})/(1-\\bar{\\alpha}_t)} \\cdot \\sqrt{1-\\alpha_t/\\alpha_{t-1}}$.\nHere $\\nu_t$ ~ N(0, I) denotes standard Gaussian random variable that is independent of $z_t$.\nOne practical limitation of Equation (8) is its requirement for T denoising interactions, which is typically set to a large value (e.g. T = 1000) to conform to the Gaussian assumption. To accelerate the reverse sampling process, DDIM [46] proposes representing the diffusion process through a non-Markov chain, while retaining the identical objective function as in Equation (6). This non-Markov process allows for the flexibility to sample fewer steps (e.g. T = 50) without altering the pre-trained weights. DDIM sampling follows a similar sampling formulation as Equation (8), but it is a deterministic process with $\\sigma_t$ = 0:\n$z_{t-1} = \\sqrt{\\frac{\\bar{\\alpha}_{t-1}}{\\bar{\\alpha}_t}} z_{t0} + \\sqrt{1 - \\bar{\\alpha}_{t-1}}\\epsilon_\\theta(z_t, t)$.\n(10)\n2.2 Image Diffusion Models\nThis section primarily discusses the image diffusion methods. In practical implementations, the diffusion process often occurs in a semantically equivalent space rather than the original data space (e.g. the VAE latent space versus the pixel space of an image dataset). To avoid ambiguity, we denote the original image samples as xo and the noise-free samples in the diffusion process as zo throughout the paper.\nEarly image diffusion models [39], [45], [47], [48], [49], [50] directly generate images at the desired resolution, i.e. $z_0$ = $x_0$. As a result, early image diffusion models typically produced images at low resolutions. Generating high-resolution images is prohibitively expensive.\nTo address the high-resolution challenge, Cascaded Diffusion Models (CDMs) [3], [4], [51] propose initiating generation at lower resolutions and then upsampling by super-resolution diffusion models. In this scenario, the samples for diffusion models training are derived from the downsampled pixel-level images $z_0$ = DownSample($x_0$).\nMore recently, Latent Diffusion Models (LDMs) [5], [52] can generate high-resolution images at affordable computational cost by operating within the latent space of a pre-trained VAE [40], [41], [53]. This approach capitalizes on the observation: VAEs excel at perceptual compression by effectively removing and reconstructing high-frequency details; diffusion models excel at capturing the semantic and conceptual composition of the data [5]. Specifically, people train diffusion models in the VAEs' lower-dimensional latent space: $z_0$ = $\\varepsilon$($x_0$), where $\\varepsilon$ represents a VAE encoder. During inference, the diffusion model outputs are mapped back to the pixel space $x_0$ = D($z_0$) by the VAE decoder D.\n2.2.1 Conditioning and Guidance\nConditional generation has emerged as one of the most important tasks for image generation. Common conditions include class labels [39], [43], [47], [48], text [2], [3], [4], [5], [6], [51], [52], depth maps [54], [55], etc. Then, we survey the architecture design to incorporate conditions and the guidance mechanisms. To avoid confusion, we explicitly declare the conditional input in the noise predictor: $\\epsilon_\\theta(z_t, t, y)$ represents the noise prediction with condition y; $\\epsilon_\\theta(z_t, t, \\varnothing)$ represents the unconditional prediction.\nSpatial-Aligned Conditions. Many condition variants are spatial aligned with the target image, including depth maps, segmentation maps, and spatial layouts. One straightforward solution is to concatenate these conditions with the noisy latent state zt and pass them as input to the noise predictor $\\epsilon_\\theta$. Various I2I translation tasks, such as inpainting and colorization, can be treated as conditional image generation and addressed using this approach [5], [10].\nResearchers later propose to use the adapter to efficiently handle various spatial alignment conditions without modifying the pre-trained diffusion model parameters. Two iconic works, T2I-Adapter [55] and ControlNet [54], offer flexible solutions by adding hyper-networks to the pre-trained model. Specifically, they propose duplicating the encoder blocks of the pre-trained UNet noise predictor and reintegrating their outputs. T2I-Adapter adds the multi-scale hyper-network outputs to the UNet encoder blocks. Conversely, the ControlNet outputs are connected to the decoder blocks by zero convolutions to ensure stability in the initial training steps. Since the parameters remain unchanged, a single pre-trained network can be integrated with multiple adapters for various tasks.\nAdaptive Normalization. Another approach to incorporate conditions into diffusion models is applying the feature-wise transformation [56]: AdaNorm(h) = sy Norm(h) + by, where sy \u2208 R and by \u2208 R are derivated from condition y. Group Normalization [57] and Instance Normalization [58], [59] are common options for hidden feature normalization. This approach has empirically demonstrated promising results for the time steps and class labels [47], [48].\nConditioning by Cross Attention. Many advanced diffusion methods [2], [4], [5] incorporate transformer blocks [60] in their networks and use cross-attention to integrate conditions. A domain-specific encoder $T_\\theta$(\u00b7) first encodes the condition y: $T_\\theta$(y) \u2208 $R^{d+\\times L}$. Then, the cross-attention layers Attn(Q, K, V) = softmax($Q^T \\cdot K/\\sqrt{d}$)\u00b7 $V^T$ can be formulated as:\nQ = WQ \u00b7 h, K = WK \u00b7 $T_\\theta$(y), V = Wy\u00b7 $T_\\theta$(y),\n(11)\nwhere h \u2208 $R^{d\\times N}$ denotes the flattened hidden feature of the noise predictor $\\epsilon_\\theta$(\u00b7). Wq \u2208 $R^{d\\times d}$, WK \u2208 $R^{d\\times d_t}$, and Wv \u2208 $R^{d\\times d_t}$ denote learnable projection matrices. Text descriptions and reference images are common cross-attention conditions.\nClassifier Guidance. In addition to the model inputs, Sohl-Dickstein et al. [1] and Song et al. [43] propose using pre-trained classifier C(\u00b7), which predicts the class probability of the intermediate diffusion latents, to guide the generation process of diffusion models. They formulate the estimated score function as follows:\n$\\nabla_{z_t} log p_\\theta (z_t, y) = \\nabla_{z_t} log p_\\theta (z_t) + \\nabla_{z_t} log C(y|z_t)$\n(12)\n$\\equiv (\\frac{1}{\\sqrt{1-\\bar{\\alpha}_t}}(\\epsilon_\\theta(z_t, t, \\varnothing) - \\sqrt{1 - \\bar{\\alpha}_t}\\nabla_{z_t} logC(y|z_t))$,\nThus, a new classifier-guided noise predictor takes the following form:\n$\\epsilon_\\theta(z_t, t, y) \\leftarrow \\epsilon_\\theta(z_t, t, \\varnothing) - w\\sqrt{1 - \\bar{\\alpha}_t} \\nabla_{z_t} log C (y|z_t)$,\n(13)\nwhere a scale w is added to control the guidance strength.\nClassifier-Free Guidance. Ho & Salimans [49] propose guiding the diffusion generation process without pre-trained classifiers. They apply Bayes' theorem to formulate the classifier gradient $\\nabla_{z_t} logC(y|z_t)$ using the conditional score function and unconditional score function:\n$\\nabla_{z_t} log C(y|z_t) = \\nabla_{z_t} log p_\\theta (z_t|y) - \\nabla_{z_t} log p_\\theta (z_t)$\n(14)\n$\\equiv \\frac{1}{\\sqrt{1 - \\bar{\\alpha}_t}}: (\\epsilon_\\theta(z_t, t, y) - \\epsilon_\\theta(z_t, t, \\varnothing))$,\nwhich is substituted into Equation (13), resulting in a new classifier-free predictor:\n$\\epsilon_\\theta(z_t, t, y) + \\epsilon_\\theta(z_t, t, \\varnothing) - w\\sqrt{1 - \\bar{\\alpha}_t} \\nabla_{z_t} log C (y|z_t)$\n(15)\n$\\equiv \\epsilon_\\theta(z_t, t, \\varnothing) + w(\\epsilon_\\theta(z_t, t, y) - \\epsilon_\\theta(z_t, t, \\varnothing))$.\nDuring training, the condition y is randomly set to empty $\\varnothing$, allowing a single predictor to handle both conditional and unconditional inputs.\nThis classifier-free guidance can naturally extend to multiple conditions with a hierarchical structure. Here, we illustrate the scenario with two conditions; additional scenarios can be similarly extended:\n$\\epsilon_\\theta (z_t, t, y_1, y_2) \\leftarrow \\epsilon_\\theta(z_t, t, \\varnothing, \\varnothing)$\n+ $w_1 (\\epsilon_\\theta (z_t, t, y_1, \\varnothing) - \\epsilon_\\theta(z_t, t, \\varnothing, \\varnothing))$\n+ $w_2 (\\epsilon_\\theta(z_t, t, y_1, y_2) - \\epsilon_\\theta(z_t, t, y_1, \\varnothing))$.\n(16)\n2.2.2 Editing and Customization\nClosely related to conditional generation, image editing concentrates on generating images while maintaining specific attributes of a source image, such as the identity of subjects, background, spatial layout, etc. Here, we discuss image editing techniques that extend beyond traditional conditioning mechanisms.\nLatent State Initialization. Image diffusion models typically initialize the latent states with Gaussian noise [1], [39], [45], [50]. To preserve low-frequency structures while editing high-frequency details, SDEdit [61] perturbs the source image through the forward diffusion process, as indicated in Equation (3), then generates the target image from the partially perturbed latent state. Many feasible images can correspond to the same initial latent state, and conditions and guidance techniques from Section 2.2.1 are employed to direct the editing process [9].\nInstead of perturbing the source image with randomness, the latent state can be initialized using the reverse diffusion process [46]. Specifically, the sampling formula in Equation (10) is algebraically manipulated as follows:\n$z_t \\approx \\frac{\\sqrt{\\bar{\\alpha}_t}}{\\sqrt{\\bar{\\alpha}_{t-1}}}z_{t-1}+ (+\\sqrt{1-\\bar{\\alpha}_t} \\sqrt{1-\\frac{\\bar{\\alpha}_t}{\\bar{\\alpha}_{t-1}}} - \\bar{\\alpha}_t)\\epsilon_\\theta (z_t, t,y)$.\n(17)\n$z_t$ appears on both sides of Equation (17). Assuming Gaussian transitions and small step sizes, $z_t$ can be approximated by replacing $\\epsilon_\\theta(z_t, t)$ on the right-hand side of Equation (17) with $\\epsilon_\\theta(z_{t-1}, t)$, resulting in the following approximation:\n$z_t \\approx \\frac{\\sqrt{\\bar{\\alpha}_t}}{\\sqrt{\\bar{\\alpha}_{t-1}}}z_{t-1}+ (+\\sqrt{1-\\bar{\\alpha}_t} \\sqrt{1-\\frac{\\bar{\\alpha}_t}{\\bar{\\alpha}_{t-1}}} \\bar{\\alpha}_t)\\epsilon_\\theta (z_{t-1}, t,y)$.\n(18)\nThe source image is inverted into latent states by iteratively applying Equation (18). Note that the inversion process can be made unconditional by setting y = \u00d8. Then, reverse diffusion sampling on the inverted latent states leads to deterministic reconstruction with negligible error when using the inversion condition y, and enables editing when using the target condition $\\hat{y}$.\nWhen incorporating classifier-free guidance [49], the w > 1 guidance scale amplifies the approximation error, which further accumulates throughout the diffusion process. Null-text inversion [62] addresses this issue by regressing the 'null text' embedding at each denoising step. For each timestep t = T, . . ., 1, the null-text embeddings can be iteratively optimized using the following formula:\n$\\varnothing_t = arg \\underset{\\varnothing_t}{min} ||z_{t-1} - z_{t-1} (z_t, t, y, \\varnothing_t)||^2$,\n(19)\nwhere $z_{t-1}^u$ represents the unconditional inversion. And $z_{t-1}(z_t, t, y, \\varnothing_t)$ is the latent obtained from the inversion process described in Equation (18), with its noise predicted using classifier-free guidance as described in Equation (15).\nAttention Feature Injection. The network attention maps and hidden features possess abundant semantic and spatial information [63]. Several methods focus on denoising with dual branches in parallel and selectively inject attention maps and hidden features from one branch to another, ensuring consistency and editability simultaneously. Specifically, this paradigm employs a reconstruction branch for source images and an editing branch for target images as illustrated in Figure 3.\nPrompt-to-Prompt (P2P) [13] emphasizes the efficacy of text prompts in T2I diffusion models. It injects the cross-attention map of unchanged tokens from the reconstruction branch into the editing branch, thereby preserving the unaltered area of source images. Plug-and-Play (PnP) [14] and MasaCtrl [15] emphasize the importance of self-attention layers as structural descriptors. PnP injects self-attention query, key, and each block's output from the reconstruction branch to the editing branch, ensuring the preservation of semantic layout. On the other hand, MasaCtrl maintains the query features unchanged while transferring key and value features. It uses the injection masks by thresholding the cross-attention maps of edited words [64], [65] to address foreground-background confusion. Feature injection methods can start by inverting the source image through latent inversion, as shown in Equations (18) and (19). Subsequently, the inverted latent states are used for both branches.\nText Inversion. Text conditions are pivotal in denoising when noisy images offer limited information during earlier reverse diffusion steps [7]. Textural Inversion [66] optimizes a token embedding, serving as an identifier to maintain the protagonist's identity. DreamBooth [67] employs an identifier alongside the subject's class name, capturing semantic information and finer details by fine-tuning the base and super-resolution components of Imagen [4] using 3-5 images. Continuing in this direction, subsequent studies [68], [69], [70], [71], [72], [73] have achieved precise attribute control and interesting applications such as conceptual interpolation.\nAmong these image editing methods, latent state initialization methods are proficient in large features and pose changes [74]. Feature injection methods excel in stylization, background, and object replacements, while text inversion methods are more suitable for object customization and personalization. Many orthogonal methods can be collaboratively employed to address diverse editing tasks.\n2.2.3 Efficient Adaptions\nLow-Rank Adaptation (LoRA) [75] is a widely recognized matrix reparameterization technique. For a pre-trained parameter matrix W \u2208 $R^{dxk}$, LoRA introduces two low-rank factorized matrices A\u2208 $R^{dxr}$, B\u2208 $R^{rxk}$, and r < min (d, k) to represent its training updates \u2206W = A \u00b7 B \u2208 $R^{dxk}$. Only the factorized matrices, A and B, are trainable: W \u2190 W+A\u00b7B. LoRA can be intuitively applied to many network layers, including attention and MLP. It offers significant advantages in lower memory consumption and faster convergence.\nToken Merging (ToMe) [76], [77] is an efficient adaptation to increase the throughput of Vision Transformer (ViT) [78] architecture. ToMe reduces the number of tokens processed by merging similar tokens before attention layers. This merging operation uses bipartite soft matching, where tokens are alternatively partitioned into two sets, and the top r most similar tokens between these sets are merged. ToMe later extends to dense prediction tasks, such as image generation. For StableDiffusion [5], ToMe happens before self-attention layers, with outputs unmerged to maintain the token count. This approach doubles the image generation speed and reduces memory consumption by up to 5.6\u00d7 with negligible quality degradation.\n2.3 Video Generation and Motion Representation\nVideo generation aims to create videos from scratch, sharing many underlying technologies with video editing. Recent advancements in diffusion models [17], [18], [19], [20], [21], [79], [80], [81], [82], [83], [84] have accelerated video generation progress, surpassing previous GANs [85], [86], [87], [88], [89], [90], [91]. We next review key technologies in video generation (Section 2.3.1). Following that, Section 2.3.2 introduces optical flow [92], [93], a motion representation technique widely used in various video-related tasks.\n2.3.1 Diffusion Models for Video Generation\nPre-trained image diffusion models are well-known for their 2D modeling capabilities. Since video frames are sequential images, extending these image priors to video tasks is a logical progression. The seminal work, Video Diffusion Models (VDM) [17], adopts a factorized 3D architecture that separately processes the 2D spatial and 1D temporal dimensions. illustrates two variants of 3D blocks: 1D temporal convolutions are incorporated through residual connections after each 2D spatial convolution; attention blocks apply a similar architecture. This design facilitates cross-frame interaction at a relatively low cost, mitigating the computational burden imposed by the curse of dimensions. also illustrates the distinction between spatial attention, temporal attention, and full 3D spatial-temporal attention, with the two spatial dimensions flattened for clarity. From the training data perspective, VDM facilitates image-video joint learning to tackle the challenges posed by video datasets, which typically exhibit lower quality and quantity than image datasets.\nSubsequent studies extend the success of CDM and LDM in the Text-to-Video (T2V) domain on larger data and model scales. Models such as ModelScope [80], Show-1 [81], VideoCrafter [82], [95], and LaVie [83] serve as large-scale open-source baselines, leveraging pre-trained T2I diffusions for efficient training. GenTron [84] and LATTE [21] utilize a pure transformer backbone in video diffusion models. Additionally, Latent-Shift [96] explores the latent-shift attention (as shown in Figure 5) in video generation tasks as an alternative implementation for spatiotemporal modeling. AnimateDiff [20] showcases the orthogonal learning of appearance and motion. It freezes the original spatial weights while learning adaptable temporal layers on video datasets. Once the temporal layers are trained, they can serve as plugins, enabling various customized image diffusion models from the community to gain animation capabilities without retraining on video data.\nIn addition to T2V, many works primarily focus on Image-to-Video (I2V) [22], [97], [98], [99], [100], [101] and video completion [102], [103] tasks, where a single frame or short video clip is provided, intending to generate longer videos. Other conditional modalities include audio [104], [105], [106], structure [107], [108], and even MRI signals [109]. Multi-modal guided video generation [110], [111], [112] is also actively pursued.\n2.3.2 Motion Representation\nDense optical flow [92], [93] is an iconic representation of video motion information, which depicts each pixel's horizontal and vertical offset. It finds extensive utility in point tracking [93], [113], video compression [114], evaluation metrics [115], [116], and more. Many optical flow estimation models [117], [118], trained and validated using simulated ground-truth datasets [119], [120], [121], [122], exhibit robust generalization to realistic scenarios.\nLet us denote the optical flow from the source frame $x^{f_1}$ \u2208 $R^{3\\times W \\times H}$ to the target frame $x^{f_2}$ \u2208 $R^{3\\times W \\times H}$ as $F_{f_1\\rightarrow f_2}$ \u2208 $R^{2\\times W \\times H}$. Certain regions may lack accurate correspondence, often indicated by occlusion masks $O_{f1\\rightarrow f2}$ \u2208 {0,1}$^{W\\times H}$. The operation of estimating the target frame $x^{f_2}$ from an optical flow $F_{f_1\\rightarrow f_2}$ and a source frame $x^{f_1}$ is known as warping $F_{f_1\\rightarrow f_2}$ ($x^{f_1}$)."}, {"title": "DIFFUSION MODEL-BASED VIDEO EDITING", "content": "Building on the generation concept", "163": "pioneers the concept of one-shot tuning in video editing. It transforms the spatial self-attention layers of a T2I StableDiffusion into the sparse-casual attention layers. As illustrated in Figure 5", "follows": "n$Q^f = W_Q \\cdot h^f$", "h^{f-1}": "n(20)\n$V^f = W_V \\cdot [h^1"}, {"h^{f-1}": "nwhere [", "46": "to initiate the generation", "164": "analyzes the covariate shifting caused by the temporal attention layers in Tune-A-Video [163", "replacement": "n$IC(h) = h - \\frac{1"}, {"h[i,j": ".", "168": "is incorporated to mitigate covariate shifts. Furthermore", "165": "aims to extend T2I StableDiffusion to T2V by parameter efficient fine-tuning. It utilizes an adapter comprising two learnable fully connected (FC) layers: $W_{down"}], "L(\u00b7)": "nAdapter(h) = h + $W_{up}$\u00b7L($W_{down}h$).\n(22)\nThe first FC layer maps the inputh \u2208 $R^{d\\times F\\times W \\times H}$ to a lower-dimension vector space (d' < d), while the second layer maps it back. SimDA incorporates spatial adapters to learn appearance transferability and temporal adapters to model temporal information. The intermediate layers utilize GELU activations [169] and depth-wise 3D convolutions for spatial and temporal adapters, respectively. Additionally, SimDA implements the temporally latent-shifted self-attention [96], [170], shown in Figure 5, to enhance temporal consistency. This approach thus benefits from both computational efficiency and"}