{"title": "Trustworthy Al in practice: an analysis of practitioners' needs and challenges", "authors": ["Maria Teresa Baldassarre", "Domenico Gigante", "Azzurra Ragone", "Marcos Kalinowski", "Sara Tibid\u00f2"], "abstract": "Recently, there has been growing attention on behalf of both academic and practice communities towards the ability of Artificial Intelligence (AI) systems to operate responsibly and ethically. As a result, a plethora of frameworks and guidelines have appeared to support practitioners in implementing Trustworthy AI applications (TAI). However, little research has been done to investigate whether such frameworks are being used and how. In this work, we study the vision Al practitioners have on TAI principles, how they address them, and what they would like to have - in terms of tools, knowledge, or guidelines - when they attempt to incorporate such principles into the systems they develop. Through a survey and semi-structured interviews, we systematically investigated practitioners' challenges and needs in developing TAI systems. Based on these practical findings, we highlight recommendations to help AI practitioners develop Trustworthy AI applications.", "sections": [{"title": "1 INTRODUCTION", "content": "Artificial intelligence (AI) systems increasingly exert an extensive impact on various facets of our existence, encompassing the realm of healthcare and the quality of education we receive, the determination of which news articles or social media posts we encounter, the allocation of employment opportunities, the detention decisions, and the intensification of policing efforts in some areas, just to name a few. With this expansion, the risk of AI increasing social inequities has generated escalating attention across several communities, including the media. Indeed it is common to observe reports in mainstream media of systemic dangerous behaviors observed in widely used Al systems, such as a smart algorithm guiding assistance for tens of millions of people biased against dark-skinned patients\u00b9, or an AI chatbot suspended for making homophobic slurs and leaking user information\u00b2. These risks are even more pronounced with the recent advent of Generative AI and the impact these systems have on various societal aspects [5].\nIn this context, the concept of Trustworthy Artificial Intelligence (TAI) has been defined: \"Trustworthy AI has three components, which should be met throughout the system's entire life cycle: (1) it should be lawful, complying with all applicable laws and regulations (2) it should be ethical, ensuring adherence to ethical principles and values and (3) it should be robust, both from a technical and social perspective since, even with good intentions, Al systems can cause unintentional harm. Each component in itself is necessary but not sufficient for the achievement of Trustworthy AI\" [25].\nSeveral public and private organizations have responded to these societal fears by developing different kinds of resources: ethical requirements, principles, guidelines, best practices, tools, and frameworks [8, 37]. As the field progresses, integrated toolkits are being developed with the objective of rendering these methods more broadly accessible and usable (e.g., Enisa's Machine Learning Security [16], Aequitas [2] or Google's What-if-Tool [47])."}, {"title": "2 BACKGROUND", "content": "National and global entities have established specialized expert committees in the field of Artificial Intelligence (AI) to address the associated risks stemming from AI development. These committees often have the mandate of formulating policy documents. Prominent examples of such organizations include the High-Level Expert Group on Artificial Intelligence initiated by the European Commission [26], the UNESCO Ad Hoc Expert Group (AHEG) tasked with the Recommendation on the Ethics of Artificial Intelligence [45], the Advisory Council on the Ethical Use of Artificial Intelligence and Data in Singapore [28], the NASA Artificial Intelligence Group [36] and the UK AI Council [44], among others.\nThese committees bear the crucial role of generating comprehensive reports and guidelines about Trustworthy AI (TAI). A parallel endeavour is observable within the commercial landscape, particularly among enterprises heavily reliant on AI technologies. Corporations such as Sony\u00b3 and Meta\u2074 have made their Al policies and principles publicly accessible. Concurrently, professional organizations and non-profit entities, such as UNI Global Union\u2075 and the Internet Society\u2076, have issued statements and recommendations.\nThe substantial efforts of this diverse spectrum of stakeholders in crafting TAI principles and policies not only underscore the imperative need for ethical guidance but also exemplify their vested interest in shaping AI ethics to align with their specific objectives [23]. It is noteworthy that the private sector's engagement in the gap of AI ethics has undergone a thorough check, with contentions suggesting that high-level soft policies may be employed to either transform a social issue into a purely technical one [23] or to potentially circumvent regulatory measures [1, 30].\nNevertheless, a set of research endeavors has brought attention to the divergent nature of these proposals, giving rise to a complex challenge often referred to as \"principle proliferation\"\u2077 [30]. Consequently, efforts have been undertaken to address this challenge. For instance, Jobin et al. [30] conducted a comprehensive study, that culminated in the identification of a global convergence around five ethical principles: transparency, justice and fairness, non-maleficence, responsibility, and privacy. Jobin et al. [30] also observed that, while no single document they reviewed encompassed all of these ethical principles, these five principles were mentioned in over half of the sources examined. Furthermore, their detailed thematic analysis unveiled significant semantic and conceptual variations in the interpretation of these principles and the specific recommendations or areas of concern derived from each one."}, {"title": "2.2 Trustworthy AI principles definitions", "content": "As set out in Section 2.1, a notable degree of ambiguity and subtlety exists in demarcating the principles that predominantly characterize Trustworthy AI (TAI). Notably, TAI is sometimes used interchangeably with Responsible or Ethical AI. In our investigation, we confront the challenge of principle proliferation by choosing to focus on a specific subset of principles that characterize TAI. Specifically, we concentrate on the most recurrent four principles identified by Jobin et al. [30], while opting to exclude the principle of responsibility due to its infrequent occurrence and lack of a clear, universally accepted definition.\nFurthermore, in this work, we have decided to adopt the definitions put forth by the High-Level Expert Group on Artificial Intelligence (AIHLEG) an entity established by the European Commission - explained in their \"Ethics guidelines for trustworthy AI\" [25]."}, {"title": "3 METHOD", "content": "The goal of our study is to investigate the state of the practice to understand common practices as well as challenges and difficulties encountered by AI practitioners in implementing TAI systems through the entire SDLC.\nSince traditional phases of the SDLC do not necessarily map well with the activities required to develop an AI-enabled system, we have extended each phase with one or more activities mentioned by Zhengxin et al. [49].\nGiven our main goal, and based on the analysis of the shortcomings and common practices that emerged from the mapping study carried out in our previous work [8], we defined the following Research Questions (RQs):\n\u2022 RQ1: What vision/opinion do practitioners have about untrustworthiness issues and how often do they encounter them?\n\u2022 RQ2: How do practitioners address untrustworthiness issues?\n\u2022 RQ3: What tools/support do practitioners desire to address untrustworthiness problems better?"}, {"title": "3.1 Data collection", "content": "For our study, we recruited practitioners working on AI products and services through a combination of purposive and snowball sampling [24]. To recruit participants for our study, we chose to not widely distribute our survey online, but rather to conduct a brief screening procedure to define the inclusion criteria and target suitable participants for the study. The target participants for the survey were Al practitioners involved in the development of AI-based systems, with at least some basic knowledge of TAI principles and/or who had previously addressed TAI in their professional work. We started by sending personal emails to contacts within our network, working in industry or academia, explaining the purpose of the study and the inclusion criteria, we asked them to help us recruit other participants by spreading the invitation through their networks. Next, we verified that the inclusion criteria were met by asking some specific questions in the demographics section of the survey.\nThe invitation to participate in the study was sent by email and included an explanation of the study's purpose. In order to meet all needs (e.g., restrictions related to tight schedules, time zones, commitments), we gave participants the chance to choose between (a) survey (asynchronous interaction) or (b) semi-structured interviews (synchronous interaction) mode. This allowed us to include a broader amount of subjects and collect a higher number of answers. All participants answered the same set of questions and had the opportunity to add any non-listed practices or suggestions/feedback related to the closed-ended question in the open-text fields (asynchronous mode) whereas, interviewed participants (synchronous) could \"discuss their answers out loud\" and further elaborate their considerations with the interviewers. The answers were all transcribed to be included later in the thematic analysis.\nParticipation was on a voluntary basis and not rewarded by any means. Overall, we obtained 23 answers for the survey and 11 participants attended the interview.\nSection 4.5 provides details about participants' demographics and their relevant experience. Specific details about their companies and working environment have been abstracted to preserve anonymity."}, {"title": "3.2 The survey", "content": "The survey contains six main sections [7]:\n1. Informed consent request. This page asks the participants to provide their informed consent and explains the purpose of the research, the participants' requirements, confidentiality rules, participation on a voluntary basis, and the time needed to complete the survey.\n2. Preliminary concepts knowledge. We clarified the semantics and interpretation of each TAI principle for participants by providing a definition for each principle. By listing a definition, we wanted to build a shared understanding of each principle to answer the remaining questions. In addition, we asked the participants about their vision and previous experience with TAI.\n3. Practices in preventing untrustworthiness in Al. We inquired participants about the main strategies they adopt to prevent TAI issues (e.g., balancing the dataset or choosing a specific algorithm).\n4. Practices in discovering untrustworthiness issues in AI. We asked participants to share their strategies to find possible sources of untrustworthiness (e.g., do auditing tasks, compute metrics, learn from user feedback).\n5. Practices in addressing trustworthiness issues. We investigated the different approaches used to address TAI issues (e.g., dataset augmentation, instance weighting)."}, {"title": "3.3 Interview Study Protocol", "content": "Before scheduling the interviews with participants, to understand the challenges and requirements for conducting remotely semi-structured interviews, as well as to help us refine our study protocol, we conducted two pilot interviews. These preliminary interviews helped us define the protocol and the setting we applied to the final, larger sample.\nAll interviews were conducted via Microsoft Teams; after asking each participant for their consent, we enabled Recording & Transcription Teams features. Once the interview ended, we proof-checked the transcription in order to correct any misspellings, anonymized any Personal Identifiable Information (PII) and finally deleted the recording.\nThe interview study consisted of think-aloud semi-structured interviews, each one lasting between 45 and 90 minutes. During the live interview, we periodically asked participants to elaborate on their responses, especially for the open ones. We also encouraged participants to \"think aloud\" [32, 42] and discuss the information that was being displayed and how their understanding of the question was developing.\nTo give a standard structure to each interview, we used the survey as a canvas."}, {"title": "3.4 Data Analysis", "content": "In this step, we extracted all relevant data using quantitative and qualitative data analysis techniques to summarize and interpret the collected data. For quantitative data, we used descriptive statistics [22], and for qualitative data, we used thematic analysis [15].\nWe used an inductive thematic analysis approach [11, 12] to analyze about 11.5 hours of video recordings and their corresponding (automatically generated and manually proof-checked) transcripts. The entire analysis was done through Atlas.ti\u00b9\u2070. Two authors worked independently and used the tool to conduct an open coding of the transcripts for each quotation. Next, they manually reviewed each code and decided which to include/exclude annotating any comments. Once this step was completed, they joined to compare and discuss results. The total number of analyzed quotations was 23. The calculated Cohen's Kappa [4] is 0.259. All the details about the coding procedure and the generated codes are provided in the online appendix [6].\nOnce finalized, the codes were shared with the entire research team and grouped into higher-level themes concerning the practitioners' knowledge and practices. In Section 4 we discuss the findings identified from these codes and themes, together with implications for future TAI developments."}, {"title": "4 RESULTS AND FINDINGS", "content": "We present findings from our think-aloud interviews study and the survey answers, divided into three main sections,\n\u2022 Practices in preventing untrustworthiness in AI\n\u2022 Practices in discovering untrustworthiness issues\n\u2022 Practices in addressing untrustworthiness issues\nAcross all three phases, we discovered different nuances of practitioners' needs around TAI issues. We supplement data from the closed-ended questions (quantitative results) with the thematic analysis performed on the answers from all the open-ended questions (qualitative results). We performed analysis on the disaggregated data with respect to subgroups such as company size, gender, education, and number of projects deployed. To understand if the differences were statistically significant, we conducted pairwise comparisons using Fisher's exact test coupled with the Benjamini-Hochberg [9] correction to obtain the adjusted p-values. Since there is no statistical significance in any of the cases except for company size, detailed in Section 4.2, the graphs in the paper report the results of the analyses in aggregated form."}, {"title": "4.1 Preliminary concepts knowledge", "content": "All discussed tables and graphs, from now on, bring together the answers from both the interviews (11) and the survey (23). In the survey section \"2. Preliminary concepts knowledge\", we observed that the TAI principle participants have encountered most frequently in their projects is Privacy (20 answers), followed by Transparency (18 answers) and Security (17 answers), while the least experienced is Fairness (13 answers). This answer should be further investigated because perhaps sometimes practitioners may not recognize or be aware of the need to address some issues related to these principles.\nIn addition, the most agreed reasons were Avoid violating legal requirements and Improve the overall quality. While, the least agreed one was Retain users/avoid losing the activity, with eight disagreements.\nOther important factors related to the reasons for caring about TAI emerged: i) \"need to solve mission-critical tasks\"; ii) \"[need to provide] models usable in real-world contexts\", and this demonstrated that black box models are not allowed in some specific contexts; iii) \"the robustness of the Al explanations themselves\", which shows consciousness about the fact that all TAI aspects contribute to making the model more robust; and iv) \"[need to] desire to commercially assemble Al systems to improve society\".\nFinally, that most of the participants address TAI principles during the Design and Development SDLC phases. In contrast, very few participants reported that they had addressed TAI principles during the Requirements Elicitation and, especially, Deploy phases."}, {"title": "4.2 Practices in Preventing untrustworthiness in AI", "content": "In the survey section \"3.Practices in preventing untrustworthiness in AI\", as Fig. 2 shows, the most recurrent strategy employed by our participants to ensure trustworthiness is \"algorithm that can best explain the decision\". On the other hand, the least employed practice appears to be \"inject malicious data points\". In analyzing the disaggregated data, we found a statistically significant difference only in the responses related to the strategy \"Algorithm that can best explain the decision\" for the company size subgroup. Indeed, for medium-sized companies, we found more positive responses than for small and large enterprises. For medium ones, no negative answers were given and 63% of the participants chose \"Always\", reflecting the wide use of this strategy in medium-sized companies.\nSome participants mentioned other strategies, such as \"[conduct an] in-depth study of the state of the art [prior to start designing the system]\" and \"[use the] post-processing phase [...] to apply human-friendly deterministic rules to check whether a result is in line with the sense of the application domain\".\nAdditionally, when we asked the participants to rate the utility of various hypothetical tools assuming their team had access to them, the participants rated as the most valuable the tool able to \"[...] generate an explanation of a model after its creation [...]\". On the other hand, they rated as least useful the tool to \"decide how much data you need for particular subgroups/subpopulations\". These results are shown in Fig. 3.\nThe answers to this section show practitioners are prone to use techniques and tools to prevent trustworthiness issues, focusing mainly on ensuring Transparency (a.k.a. Explainability)."}, {"title": "4.3 Practices in Discovering Untrustworthiness Issues", "content": "In the survey section \"4.Practices in Discovering untrustworthiness in Al\", we investigated which strategies participants mainly employed to discover TAI issues.\nThe data shows that the most used strategies are \"Metrics/KPIs\", \"learn from user feedback\", and \"examine AI/ML model's input features\" (see Fig.4). Examples of Metrics/KPIs related to fairness are, just to cite a few, Demographic Parity, Accuracy, F1-Score. Whereas,"}, {"title": "4.4 Practices in Addressing Untrustworthiness Issues", "content": "Regarding survey section \"5.Practices in Addressing Untrustworthiness in AI\", participants reported that after finding a TAI issue only in 35% of the cases (12/34) the team addressed it directly, while 15% (5/34) of the participants stated that it was not addressed by them, but handled by a third party. Worth noting is the fact that in 50% (17/34) of the cases participants reported that they did not fix the issue after finding it. The reasons why participants did not solve the issue after finding it are asked in a subsequent question (see Table 3).\nWhen participants addressed any issues found, they declared the most implemented strategies were \"improving the quality of the dataset (e.g., removing spurious samples, paradoxical values)\" (8 answers) and \"augmenting the dataset (e.g., with artificial, manually generated data points)\" (6 answers). On the other hand, the less implemented strategy was \"searching for a tool which automates a specific trustworthiness issue-fixing process\" (1 answer). One interviewee also mentioned that they usually approach explainability by \"using [only] white box models\".\nRegarding the reasons why participants did not solve a TAI issue after finding it, Table 3 shows the most frequent reasons are \"the issue solution required too much time to be implemented\" (58.3%) and \"the issue solution was likely to decrease the performance of the system (e.g., decreasing accuracy)\" (50%). On the other hand, none of the participants answered: \"no one had an idea on how to solve the issue\"; this is a positive result since demonstrates practitioners are conscious of untrustworthiness problems and can formulate hypotheses on how to address them. During the interviews, one participant also mentioned \"data availability\" as an impediment.\nFinally, when we again asked the participants to rate the utility of various hypothetical tools - assuming their team had access to them - the participants rated as the most valuable a tool able to \"[...] help [...] monitoring the AI model after its release to the public\", followed by \"best practices that can actively guide your team through the model's SDLC\", \"tools to help the team in the data pre-processing steps (e.g., decide whether one needs to add/remove data points from your training set, and what kind of data you need to add/remove)\", and \"a knowledge book in which are mapped trustworthiness problems and [...] solutions\". On the other hand, they rated as least useful tools \"[...] to help your team doing an ex-post TAI audit\" and tools able to [...] help your team deciding which Al model best respects the TAI principles [...]."}, {"title": "4.5 Demographics and background information", "content": "In terms of demographics of the sample, the study participants' gender is represented by 68% male, 21% female, 3% non-binary or gender diverse, and 9% preferred not to respond. Concerning academic qualifications, 56% of the participants have a master's degree, 41% have earned a Ph.D., and 3% have completed their bachelor's studies in computer science-related fields. The notable prevalence of advanced education within the respondent pool aligns with our expectations, reflecting the elevated cognitive expertise required by the complexities of this particular field.\nThe vast majority of the participants are employed in medium-large companies, specifically, 32.3% (11/34) work for companies with less than 50 employees (small), 23.5%, (8/34) for companies between 50 and 500 employees (medium), while 44.1%, (15/34) are employed in large companies with more than 500 employees.\nParticipants have on average five years of experience in their role and two years of experience in the Al field.\nRegarding the technology area that best describes with which Al products/services the participants work, the four most prevalent are \"Decision support\" (15 answers), \"Natural Language Processing\" (13 answers), \"Computer Vision / Image Analysis\" (12 answers), and \"Recommender Systems\" (9 answers).\nFinally, regarding the number of AI-enabled projects developed and deployed into a production environment, we observed that most of the participants (19/34, 56%) declared that just a small percentage of the developed projects from 1 to 30% are deployed in a production environment, while only 3% of the participants declared that most of the projects - from 90 to 100% are deployed into a production environment. This reveals the fact that most of these types of projects are still in an experimental stage.\nDue to space constraints, we have not included tabular representation of demographics in the paper which are, however, all available in the online appendix [6]."}, {"title": "4.6 Summary of key findings", "content": "Here we summarize some key findings from our study.\nF1. The study reveals that participants care a lot about Privacy and Transparency. Indeed, among the most used strategies to ensure trustworthiness are \"post-hoc explainability\" and \"algorithm that can best explain the decision\" (Fig. 2). In addition, tools that \"generate an explanation\" and that help in deciding \"the more clear and explainable model\" are among the tools perceived as most useful (Fig. 3).\nF2. Acting on the dataset is one of the most used strategies to solve the found TAI issues. Indeed the most implemented strategies are \"improving the quality of the dataset (e.g., removing spurious samples, paradoxical values)\" and \"augmenting the dataset (e.g., with artificial, manually generated data points)\".\nF3. Business constraints like the time required to implement the solution or the unacceptable performance drop often represent impediments to implementing trustworthy AI applications."}, {"title": "5 DISCUSSION", "content": "Our findings reveal that the most addressed principle is Privacy, probably because it is contained in various regulations that exist and must necessarily be complied with (e.g., in Europe the GDPR [18]). Transparency is also often taken into consideration, probably because there are domains where it is a fundamental and unavoidable feature required by the law, such as in Healthcare and Financial Services. On the other hand, the one less addressed is Fairness, perhaps because there are still no clear and shared regulations for this dimension of trustworthiness and everything is left to the initiative and ethical values of those implementing these systems. As a result, even large and well-established companies in the industry are often caught up in scandals that damage their reputation and show how even the most popular and widely used algorithms suffer from unfairness\u00b9\u00b2.\nIt is notable that when we asked practitioners why their team cares about trustworthiness in Al, the motivation \"retain users/avoid losing business\" found most disagreement among them. This may reveal that they believe that TAI issues do not lead to losing users and/or business. Moreover, during interviews, it emerged that addressing TAI is, in some cases, even mandatory and not an option.\nFinally, our study suggests that TAI is mainly addressed in the early stages of the SDLC. While this is good - since the earlier certain decisions are made, the more effective they are in the design of the final model - this also reveals that practitioners are most likely not aware of tools and best practices to be used in the final stages of the lifecycle. In fact, Deploy is one of the least addressed phases. Indeed, one interviewee also mentioned the need for guidelines on the best cloud provider compliant with TAI practices. These elements allow us to infer that the choice of deployment infrastructure is often left to chance or, in the best case, to routines and/or trust in a specific cloud provider.\nBased on the answers, it is clear that when practitioners want to solve TAI issues they mainly act on the dataset, as the most implemented strategies are related to \"improving the quality of the dataset\" and \"augmenting it with artificial data points\". Answers to question 22\u00b9\u00b3 sustain this trend and highlight the \"Tools for ex-post TAI audit\" and \"Tools to decide which Al model best respect the TAl principles\" \"Tools for monitoring the model\",  \"Tools for the data pre-processing steps\""}, {"title": "5.1 Practical implications and recommendations", "content": "Based on the previous discussion, we summarize some key practical implications and recommendations for the AI industry and the AI research community.\nP1. Practitioners should take TAI principles into account throughout the entire SDLC and not just in the early stages, such as the Design phase. Even if valuable mitigation measures can be put in place early in SDLC, there is a huge amount of mitigations that can only be implemented in the latter stages of the SDLC. For instance, using a tool that helps to choose the best cloud provider to be TAI-compliant.\nP2. Our study reveals that, on numerous occasions, even when TAI issues are identified, they are not addressed by practitioners either directly or by third parties due to business constraints such as limited time, financial constraints, or declining performance. Nevertheless, the oversight of these issues may result in significant economic and reputational consequences, incurring substantial costs for companies. Hence, it is imperative for the industry to commit to addressing TAI issues detected at various stages of the SDLC. In addition, such actions are crucial for compliance with emerging regulations, such as the AI Act [19].\nP3. As a general remark, there is a pressing need for guidelines, knowledge bases, and tools that can help practitioners implement TAI principles throughout the entire SDLC. They need guidance and practical advice on which tools to use at each stage of SDLC and to address which principles. Often, as pointed out in our study, although these tools exist, practitioners may not be aware of them. Moreover, as pointed out in our previous work [8], there is a significant gap that should be filled between high-level AI ethics principles and low-level concrete practices for practitioners. For this reason, as a research community, we should rethink how to design these guidelines and best practices, so that they are readily available and usable by professionals and provide actionable guidelines that can be put into practice while implementing trustworthy AI applications."}, {"title": "6 THREATS TO VALIDITY", "content": "In this section, we discuss the threats to the validity [48] of our study. We delineate the threats to validity and constraints on the outcomes of our study arising from the research methodology we employed.\nConstruct validity The thematic analysis was executed by two researchers, introducing the potential for subjective judgment. To address this concern, we implemented the negotiated agreement technique [43] between the first and second researcher, fostering consensus, which was achieved after a careful examination of 25 comments.\nInternal validity. Threats to internal validity pertain to unconsidered factors that might impact the variables and relationships under scrutiny. In our investigation, we conducted interviews with AI practitioners to gain insights into their perspectives on Trustworthy AI (TAI) issues. Each practitioner possesses a distinct background and ethical standpoint, potentially diverging from the practices of their peers. We sought to mitigate this issue by interviewing practitioners from diverse companies and different countries. Furthermore, interviewees' viewpoints may be influenced by additional factors, such as existing literature on TAI, potentially leading to social desirability bias [20], or practices adopted in company projects they are involved in. To counteract this, we consistently reminded interviewees that the discussion focused specifically on the TAI issues they encounter in their daily work. At the conclusion of the interviews, we encouraged them to freely express their broader thoughts on TAI.\nGeneralizability \u2013 Transferability. One notable threat lies in the generalizability of findings, as our sample, albeit diverse, may not fully represent the broader population of professionals engaged in Al development. To mitigate this threat, we solicited opinions from a heterogeneous participants sample: we took practitioners from corporates, characterized by different sizes with different years of experience - both in their current role and in AI-enabled systems development; with different genders, level of education, and job roles; working in diverse application domains and technology areas. All details can be found in the online appendix [6]."}, {"title": "7 CONCLUSION", "content": "In this study, we conducted think-aloud interviews and a survey with 34 AI practitioners to explore how they handle and implement Trustworthy AI applications. The study highlights how, among the TAI principles considered, practitioners mainly focus on Privacy at the expense of Fairness, even if many practitioners acknowledge to be aware of how important this last dimension is. Noteworthy is the fact that half of the participants stated that they did not fix TAI issues after discovering them in their projects. Indeed only half of them declared to have addressed the issue, either by themselves (35%) or revolving to a third party (15%). From the study it also emerged that TAI is mainly addressed in the initial phases of the SDLC (mainly during the Design and Development) and few practitioners declared to also address it in later stages, like Deploy.\nThe strategies most employed to build TAI applications focus on data quality enhancement or choosing the most self-explanatory algorithm. Moreover, data and algorithm design are conducted by exploiting manual analysis, without using any automated tool. As one could expect, the most common cause of obstruction in implementing TAI is business constraints (e.g., time, money).\nFinally, we also identified that practitioners feel the need for tools to monitor the model after production deployment, and knowledge bases and actionable guidelines to help them implement trustworthiness throughout the entire SDLC. While different frameworks, tools, and guidelines may exist [8], these are either composed of high-level statements that are sometimes difficult to translate into concrete implementation strategies or may be unknown to AI practitioners (as shown by all the answers to the questions related to the need/usefulness of tools to address TAI issue). As future directions, we intend to complement and expand this work with more interviews with a larger sample of AI practitioners to investigate some of the critical points that emerged in this study more in-depth."}]}