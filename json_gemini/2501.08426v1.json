[{"title": "Causal vs. Anticausal merging of predictors", "authors": ["Sergio Hernan Garrido Mejia", "Patrick Bl\u00f6baum", "Bernhard Sch\u00f6lkopf", "Dominik Janzing"], "abstract": "We study the differences arising from merging predictors in the causal and anti- causal directions using the same data. In particular we study the asymmetries that arise in a simple model where we merge the predictors using one binary variable as target and two continuous variables as predictors. We use Causal Maximum Entropy (CMAXENT) as inductive bias to merge the predictors, however, we expect similar differences to hold also when we use other merging methods that take into account asymmetries between cause and effect. We show that if we observe all bivariate distributions, the CMAXENT solution reduces to a logistic regression in the causal direction and Linear Discriminant Analysis (LDA) in the anticausal direction. Furthermore, we study how the decision boundaries of these two solutions differ whenever we observe only some of the bivariate distributions implications for Out-Of-Variable (OOV) generalisation.", "sections": [{"title": "1 Introduction", "content": "A common problem in machine learning and statistics consists of estimating or combining models or (expert opinions) of a target variable of interest into a single, hopefully better, model [14]. There are several reasons of why this problem is important. For example, experts might have access to different data when creating their models, but might not have access to the data available to other experts, while there might be a modeller who can access the expert's opinions and put them together into a single model. Furthermore, experts might specialise in certain areas of the support of the input space, so that a modeller with access to the expert's opinions could potentially produce a single model exploiting the strengths of each modeller. This problem is commonly known as \u201cmixture of experts\u201d, \"expert aggregation\u201d, \u201cmerging of experts\" or \"expert pooling\" [40, 22, 6, 31, 30].\nThe merging of experts problem is usually ill-defined, in the sense that there are multiple joint models (that is, models that include all covariates) that after marginalisation would render the same prediction as the individual experts (that is, those which include only some of the covariates). This ill-definedness of the problem requires strong inductive biases. One way to provide this inductive bias in a principled way is through the Maximum Entropy (MAXENT) principle [20]. In brief, MAXENT suggests finding the distribution with maximum Shannon entropy subject to moment constraints. This turns out to be the same as choosing the distribution closest to the uniform distribution having the same moments as those given by the constraints. In Section 2.2 we introduce MAXENT and Causal MAXENT (CMAXENT) in more detail, the latter being an extension that allows to include causal information when available [17]."}, {"title": "2 Notation and preliminaries", "content": "2.1 Notation\nLet $Y$ be a binary random variable taking values in $Y = \\{-1,1\\}$, and $X = \\{X_1, X_2\\}$ be a pair of continuous variables, so that $x_i \\in \\mathbb{R}$. Let $f : Y \\times \\mathbb{R}^2 \\to \\mathbb{R}$ be a measurable function, $P$ a measure on $Y \\times \\mathbb{R}^2$, and $p$ the density of the distribution of a random variable with respect to the Lebesgue measure in the case of real valued random variables, and with respect to the counting measure in the case of discrete random variables. To be precise, $p(Y, X)$ is a density with respect to the product of the Lebesgue measure and the counting measure. We denote $E_p [f(Y, X)]$ the expectation of $f$ with respect to $p$. We restrict ourselves to the scenario with two continuous variables and one binary outcome given that we can already observe asymmetries in the merging of experts, and can visualise such asymmetries without having to project such space into 2 dimensions. The results here can be easily generalised into a discrete outcome variable (and indeed we do, in Corollary 7). Throughout the article we will care about finding a predictor of $Y$ using $X$ as covariates. That is, we are interested in the density $p(Y | X)$.\n2.2 Maximum Entropy and Causal Maximum Entropy\nThe Maximum Entropy (MAXENT) principle was born in the statistical mechanics literature as a way to find a distribution consistent with a set of expectation constraints [20]. That is, given observed sample averages $f = \\sum_{i=1}^N f(y_i, x_i)$ we find the density $p(Y, X)$ so that the expectations with respect to $p(Y, X)$ are equal to those observed.\nNotice that MAXENT does not attempt to find the 'true' distribution of the data, but instead the distribution closest to the uniform distribution so that the expectation constraints are satisfied. We will see examples of such optimisation problems in subsequent sections. Using the Lagrange multiplier formalism for constrained optimisation, one can prove that the solution to the MAXENT problem belongs to the exponential family. The MAXENT distribution and its properties have been studied widely, see Gr\u00fcnwald and Dawid [10] and Wainwright et al. [38] and references therein.\nIn Causal MAXENT (CMAXENT, Janzing [17]), the optimisation is performed in an assumed causal order; that is, we first find the MAXENT distribution of causes and then the Maximum Conditional Entropy of the effects given the inferred distribution of the causes. As argued in [36] this typically results in distributions that are more plausible for the respective causal direction. One can think of CMAXENT as usual MAXENT with the distribution of the cause as additional constraint, where the latter has been obtained via separate entropy maximization."}, {"title": "3 Known predictor covariances", "content": "We will begin by studying the solution of the CMAXENT problem when we observe all the bivariate distributions and summarise them with first and second moments. The restriction to first and second moments has several reasons: First, these simple constraints are already sufficient to explain the interesting asymmetries between causal and anticausal. Second, including higher order moments makes the problem computationally harder and increases the risk of overfitting on noisy finite sample results. Last, including more moments decreases the asymmetries between the causal directions. Mathematically, we have the following (estimated) expectations and their respective sample averages:\n$\u00ca[Y] = q$,\n$\n[X] = x = \\begin{bmatrix}\nX_1 \\\\\nX_2\n\\end{bmatrix}$\n,\n$\\begin{aligned}\n    [XY] &= \\phi = \\begin{bmatrix}\n    \\phi_1 \\\\\n    \\phi_2\n    \\end{bmatrix} \\\\[5pt]\n    [XX] &= \\Sigma_X = \\begin{bmatrix}\n    \\sigma^2_1 & \\sigma_{1,2} \\\\\n    \\sigma_{1,2} & \\sigma^2_2\n    \\end{bmatrix}\n\\end{aligned}$\n where we assumed the mean of X is zero.\n3.1 The causal direction\nConsider the causal graph in Figure la and the expectations given in Equations (1) and (2). As mentioned on Section 2.2, CMAXENT suggests finding the density $p(X)$ with maximum entropy"}, {"title": "3.2 The anticausal direction", "content": "Now consider the graph in Figure 1b. In this scenario, covariates of our predictor of interest are the effects of our target variable. Following the CMAXENT principle, we first find the density $p(Y)$ with maximum entropy and is consistent with first moment of Y and then find the density $p(X | Y)$ with maximum conditional entropy consistent with the moments that involve X and $p(Y)$ found in the previous step. After this two-step process, we are left with the joint density $p(Y, X)$ from which we can derive a predictor of Y, $p(Y | X)$ using Bayes' Theorem (Section 3.3). The whole procedure can be summarised with the following optimisation problems. For the cause, we have\n$\\max\\limits_{P(y)} H (Y) = - \\sum_y p(y) \\log p(y)$\n$s.t. E[Y] = q$\n$\\sum_y p(y) = 1.$\nAnd for the effects,\n$\\max\\limits_{p(x|y)} H(X|Y) = -\\sum_y\\int_{\\mathbb{R}^2} p(x|y)p(y) \\log p(x | y)dx$\n$s.t. \u0395[YXi] = \u03c6\u2081, with i \u2208 {1,2}\n$E[Xi] = \\rho_i, with i \u2208 {1, 2}\n$E[X_i^2] = \\varsigma_i, with i \u2208 {1,2}\n$E[X1X2] = \\varsigma_{1,2}$\n$\\int_{\\mathbb{R}^2} p(x | y)dx = 1, for each y.$\nProposition 3 (Resulting predictor in the anticausal direction). Using the Lagrange multiplier formalism for the optimisation problems in Equations (8) and (9), we obtain a Bernoulli distribution for Y with $p(y = 1) = q$, and $p_X(x | y)$ given by\n$\\begin{aligned}\np_X(x | y) &= exp[\\lambda_{1y}x_1 + \\lambda_{2y}x_2 + \\lambda_3x_1 + \\lambda_4x_2 \\\n&+ \\lambda_5x_1^2 + \\lambda_6x_2^2 + \\lambda_7x_1x_2 + \\beta(y)] \\\n&= exp\\Big[\\sum_k \\lambda_k h_k(x, y) + \\beta(y)\\Big]\\\n\u03b2(y) &= \\log \\int_{\\mathbb{R}^2} exp\\Big[\\sum_k \\lambda_k h_k(x, y) \\Big]dx,\n\\end{aligned}$\nwhere $h_k$ are the different functions for which we have the sample averages. The density $p_X(X | Y)$ is a mixture of multivariate Gaussian distributions. Both components $p_X(X | y = -1)$ and $p_X(X | y = 1)$ have the same covariance matrix.\nFor the following sections, we introduce the following notation for the expectations of the mixture of Gaussians.\n$E[X | y] = \\mu_y = \\begin{bmatrix}\n\\mu_{y,1} \\\\\n\\mu_{y,2}\n\\end{bmatrix}$\n$E[XX^T | Y] = \\Sigma_{XY}$\nIn addition, we will include the subscripts \u201ccausal\u201d and \u201canticausal\" where it might be ambiguous (e.g., $E_{X|Y, causal}$ represents the conditional covariance in the causal scenario and $E_{X|Y, anticausal}$ in the anticausal scenario). As mentioned in Propostion 3, the conditional covariance $E_{XY}$ is the same for both values of y. However, we keep the conditional notation to distinguish it from the marginal covariance of X, $E_X$ introduced in Equation (2). In Appendix A we derive the conditional expectations in Equation (13) and the marginal expectations used as constraints.\nRemark 4 Even though the causal graph in the anticausal direction implies that the conditional covariance $E_{XY}$ is diagonal, the CMAXENT solution does not result in a diagonal conditional"}, {"title": "3.3 The predictor of Y in the anticausal direction", "content": "Recall that our main goal is to produce a predictor of Y as a function of the covariates X. In Section 3.1 we obtain the predictor of Y directly as a result of the CMAXENT principle, given that the predictor is already in the direction of the causal mechanism. On the other hand, in Section 3.2, we have to derive the predictor of Y using the found conditional distributions and Bayes' rule. The main result of this section is that with the constraints we have used, CMAXENT in the anticausal direction is equivalent to Linear Discriminant Analysis [14, Section 4.3]. Furthermore, we generalise this result to Quadratic Discriminant Analysis, and to an exponential family version of discriminant analysis.\nTheorem 5 (Predictor of Y using Bayes' rule). Using the results from Propostion 3, the density $p_X(Y = y | X)$ is the ratio of the product of the Gaussian component with $p_X(Y = y)$ and the mixture of Gaussians resulting from Propostion 3. Minimising the expected 0-1 loss, the optimal decision rule arising from this density is equivalent to Linear Discriminant Analysis (LDA).\nCorollary 6 (Quadratic Discriminant Analysis (QDA)). Quadratic Discriminant Analysis can be interpreted as CMAXENT in the anticausal direction. This is achieved by replacing\n$E[X_i^2] = \\varsigma_i$ with $i \\in \\{1,2\\}$, and $E[X_1X_2] = \\varsigma_{1,2}.$\nin Equation (9) with the following constraints:\n$E[X_i^2 | y] = \\varsigma_{i,y}$ with $i \\in \\{1,2\\}$, and $E[X_1X_2 | y] = \\varsigma_{1,2,y}.$\nWe will now extend this idea, where instead of modelling $p(X)$ as a mixture of Multivariate Gaussians (with equal covariance in LDA or unequal covariance in QDA), $p(X)$ now becomes a mixture of distributions, each coming from an exponential family of distributions corresponding to a more general set of constraints.\nCorollary 7 (Exponential family discriminant analysis). Let $f_i$ be an arbitrary measurable function and $\\bar{f}$ its corresponding sample average. In the general case where Y is a discrete variable and we have d covariates X in the anticausal direction, the CMAXENT problem with constraints of the form:\n$E[f_i(X) | y] = f_{i,y},$\nwhere $f_{i,y}$ are the sample averages of $f_i$ for a specific y as in Section 2.2, results in $p_X(X | Y)$ being a mixture of exponential family distributions which then can be inverted (using Bayes' rule) to a predictor of Y.\nRemark 8 In the previous corollary, the functions $f_i$ can be constant on any of the variables in X.\nThis idea has been extended to use kernels as a way to map X into more complex feature spaces. The resulting algorithm is called Kernel Fisher discriminant analysis [26, 32, 9]."}, {"title": "3.4 The geometry of the decision boundaries", "content": "Hastie et al. [14, Chapter 4.4.5] conclude that the log-posterior odds of the logistic regression and LDA are both linear in x, but with different parameters defining the linear relation. In this section, we revisit these results in more detail and explore whether the CMAXENT solution in causal direction differs from the solution in anticausal direction. From a statistical decision theory perspective, the log-posterior odds correspond to the Maximum A Posteriori (MAP) rule, the optimal decision boundary of a classifier when minimising the expected 0-1 loss [3, Ch. 4.3.3].\nProposition 9 (Normal vector to the decision boundaries in causal and anticausal direction). Under the 0-1 loss, the normal vector to the decision boundary of the CMAXENT predictor is proportional to"}, {"title": "3.5 What are the differences?", "content": "In the previous sections we found that the slopes of the decision boundary of CMAXENT in both the causal and anticausal direction are linear and agree, whenever we have the first and second moments as in Equations (1) and (2). This implies that, if the test data will come from the same distribution as the training data, either algorithm will work equally well. Previous research has studied the advantages and disadvantages [14, 33, Chapter 4.4.5] of each method and their properties such as asymptotic relative efficiency [7], parameter bias [13], asymptotic error under label noise [4] and online learning performance [1]. All of these analyses base their results on the fact that the logistic regression does not make an assumption on how the covariates X are distributed, whereas LDA does.\nAn alternative way of viewing this distinction is through the lens of generative and discriminative models. LDA is a generative model since it models both covariates and target variable and logistic regression only models the target as a function of the input. Ng and Jordan [28] analyse the difference in efficiency between the Naive Bayes algorithm (a generative model similar to LDA) and logistic regression, and find that both models have regimes in which they perform better than the other. Using the same models, Bl\u00f6baum et al. [5] and data from [35], find empirically that generative models perform better in anticausal than in causal direction."}, {"title": "4 Partially known covariances", "content": "In this section we explore variations of the solution of the CMAXENT solution in causal and anticausal direction when some of the sample averages are not known. In Section 4.1 we explore the case where the covariance between a particular predictor and the target is not known, and in Section 4.2 the case where we do not know the covariance between the predictors. In both cases we will see that the models we can infer (that is, $p(Y | X)$) with CMAXENT will depend on the underlying causal assumptions.\n4.1 Unknown predictor-target covariance\nWithout loss of generality, suppose we do not have the sample covariance between X2 and Y, that is, we do not know $\\phi_2$ in Equation (1).\nIn the causal direction, the CMAXENT solution of the distribution of the causes X will still be a multivariate normal distribution with expectations given by the constraints relating X. The conditional density of the effects is the logistic-like regression of Equation (7), however, $\\lambda_2$ will be 0, as this is the parameter corresponding to the covariance between X2 and Y. In other words, X2 becomes irrelevant in the estimation of our target predictor.\nIn the anticausal direction, the distribution of the cause Y is unchanged because $P(Y)$ is determined by the constraints and thus does not depend on $\\phi_2$. However, using the fact that the Gaussian distribution maximises the entropy over all distributions with the same variance [37, Theorem 8.6.5.], we can derive a bound on $\\phi_2$. We use the entropy of the Gaussian distribution because we do not know a closed form expression for the conditional covariance of $p(Y | X)$ as given by Theorem 5."}, {"title": "4.2 Unknown predictor covariance", "content": "Now suppose we observe all the sample averages in Equations (1) and (2) but we do not observe $\\varsigma_{1,2}$.\nIn the causal direction this implies that the multivariate Gaussian distribution resulting from the MAXENT problem on X is diagonal, that is, X are marginally independent. The exponential form of $p_X(Y | X)$ does not change, as we still observed q and $\\phi$, nevertheless, the parameters of the exponential family do change, as the density of X changed so that the resulting $p_X(Y | X)$ needs to adapt in order to match $\\phi$.\nNow we will explore the anticausal case. We will proceed as in Sections 3.1 and 3.2. First we find $p(Y)$ by maximising the entropy subject to the empirical average of Y, which is trivial because $p(Y)$ is already determined by its moments, and then we find $p(X | Y)$ subject to all the moments in Equations (1) and (2) with the exception of $\\varsigma_{1,2}$. We obtain the following result from solving the CMAXENT optimisation problem\nProposition 12 (Diagonal conditional covariance in the anticausal direction with unknown predictor covariance). The density $p(X | Y)$ that maximises the conditional entropy subject to the following constraints:\n$\\begin{aligned}\n    E[XY] &= \\begin{bmatrix}\n    \\phi_1 \\\\\n    \\phi_2\n    \\end{bmatrix}, \\\n    E[X] &= \\begin{bmatrix}\n    [X_1^2] = \\varsigma_1, [X_2^2] = \\varsigma_2,\n    \\end{bmatrix}\n\\end{aligned}$\nand $p(Y)$ inferred on the first step of CMAXENT, is independent after choosing a value of y; that is, X is conditionally independent given Y.\nRemark 13 This result is reassuring given that under these moment constraints, $p(X | y)$ turns out to be Markov relative to the DAG in the anticausal direction. Contrary to Remark 4, where we concluded that CMAXENT is not always Markov relative to a DAG.\nIn Appendix E we derive the slopes of the decision boundaries in the causal and anticausal direction when we do not know the covariance between the predictors. We also find necessary and sufficient conditions for which the slopes are the same. From this simple example, we have learned the following: in causal direction, our inductive bias tells us that the covariates are not correlated and hence, the decision boundary depends only on the marginal variance of each $X_i$ and the covariance between Y and X. In the anticausal direction, CMAXENT infers $X_1$ and $X_2$ to be marginally correlated because they need to be conditionally independent (this fact can proved using the law of total covariance). Hence, the marginal covariance of X, $E_X$ is different in both scenarios. This is something we did not observe in the case with full information (Section 3).\nIn addition, we derive the expressions of the decision boundaries in the causal and anticausal direction (see Appendix E). That is, as proved in Propostion 9, we have that the decision boundaries of the predictors in causal and anticausal direction will differ with the same moments, but different causal assumptions. In Figure 2, we showcase this phenomenon with synthetic data."}, {"title": "5 Discussion", "content": "In this article we have studied the differences arising from merging of predictors in the causal and anticausal directions. In particular, we have studied a simple case with a binary target variable and"}, {"title": "A Relation between the expectations of the Mixture of Gaussians and the known marginal expectations", "content": "In Propostion 3 we proved that the distribution resulting from the constraints in Equations (1) and (2) and the anticausal optimisation problem in Equation (9) result in a mixture of Gaussian distributions. Now we are going to explore the relation between the moments of the resulting distribution and the constraints used in the MAXENT optimisation problem.\nWe have the following expectations under Gaussian mixture model\n$\u0395 [XY] =q\\mu_1 \u2212 (1 \u2212 q)\\mu_{-1}$\n$\u0395 [X] =q\\mu_\u2081 + (1 \u2212 q)\\mu_{-1}$\n$\u0395 [XX] =\u03a3_X$\n$=E [Var(X | Y)] + Var(E [X | Y]).$\nWhere Equation (22) follows from the law of total covariance. We have\n$\\begin{aligned}\nE [Var(X | Y)] &=q\u03a3_{X|Y} + (1 \u2212 q)\u03a3_{X|Y} \\\n&=\u03a3_{X|Y},\\\\\nVar(E [X | Y]) &=E[E[X | Y]^2] \u2013 E[E[X | Y]]^2\\\\nE[E[X | Y]^2] &=q\\mu_1\\mu_1 + (1 \u2212 q)\\mu_{-1}\\mu_{-1}\\\\\nE[E[X | Y]]^2 &=(q\\mu_1 + (1 \u2212 q)\\mu_{-1})(q\\mu_1 + (1 \u2212 q)\\mu_{-1})^T.\n\\end{aligned}$\nSo that\n$\\begin{aligned}\nE [XX] &=\u03a3_{X|Y} + q\\mu_1\\mu_1 + (1 \u2212 q)\\mu_{-1}\\mu_{-1} \\\n&\u2212 (q\\mu_1 + (1 \u2212 q)\\mu_{-1})(q\\mu_1 + (1 \u2212 q)\\mu_{-1})^T\\\\\n&=\u03a3_{X|Y} + (1 \u2212 q)q\\mu_1\\mu_1 + (1 \u2212 q)q\\mu_{-1}\\mu_{-1} \u2212 (1 \u2212 q)q\\mu_{-1}\\mu_{\u22121} \u2013 (1 \u2013 q)q\\mu_{1}\\mu_{1}\\\\\n&=\u03a3_{X|Y} + (1 \u2212 q)q[\\mu_1\\mu_1 + \\mu_{-1}\\mu_{-1} \u2013 \\mu_{-1}\\mu_{\u22121} \u2013 \\mu_{1}\\mu_{1}]\\\\\n&=\u03a3_{X|Y} + (1 \u2212 q)q(\\mu_1 \u2013 \\mu_{-1})(\\mu_1 \u2013 \\mu_{-1})^\u03a4.\n\\end{aligned}$\nRecall that the empirical averages used as constraints in the maximum entropy optimisation problem are coincide with the expectations under the resulting exponential family distribution. Then, using the equations above and the constraints, the means of the multivariate Gaussian distribution are\n$\\mu_1 = \\frac{x + \\phi}{2q}$\n$\\mu_{-1} = \\frac{x - \\phi}{2(1-q)}$"}, {"title": "B Predictor in the anticausal direction", "content": "Theorem 5 (Predictor of Y using Bayes' rule). Using the results from Propostion 3, the density $p_X(Y = y | X)$ is the ratio of the product of the Gaussian component with $p_X(Y = y)$ and the mixture of Gaussians resulting from Propostion 3. Minimising the expected 0-1 loss, the optimal decision rule arising from this density is equivalent to Linear Discriminant Analysis (LDA)."}, {"title": "C Derivation of the decision boundary", "content": "In the following two sections we give the proof of Propostion 9 for the causal and anticausal direction separately. First, we restate the proposition\nProposition 9 (Normal vector to the decision boundaries in causal and anticausal direction). Under the 0-1 loss, the normal vector to the decision boundary of the CMAXENT predictor is proportional to\n1. $\u03a3_X^{-1} \\phi_{causal}$ in the causal direction.\n2. $\u03a3_{XY, anticausal}^{-1} \\phi$ in the anticausal direction.\nAs mentioned on the proposition, we frame these results within the statistical decision theory framework [3], choosing a particular loss function $L(h(x), y)$, where h(x) is the predictor of y we want to evaluate. We consider the 0-1 loss function. That is, $L(h(x), y) = 1$ if $h(x) = y$, and 0 otherwise. The optimal decision rule for this loss is the well-known Maximum A Posteriori (MAP) rule from which we can derive our decision boundary.\nC.1 Proof of Propostion 9 in the causal direction\nIn the causal direction, the Maximum A Posteriori (MAP) rule, results in the decision boundary given by the following equation\n$p(y = 1 | x) = p(y = \u22121 | x)$\n$\\frac{1}{2} (1 + tanh(\\lambda_0 + \\lambda_1x_1 + \\lambda_2x_2)) = \\frac{1}{2} (1 + tanh(-\\lambda_0 \u2013 \\lambda_1x_1 \u2013 \\lambda_2x_2))$\n$\\lambda_0 + \\lambda_1x_1 + \\lambda_2x_2 = -\\lambda_0 \u2013 \\lambda_1x_1 \u2013 \\lambda_2x_2$\n$\\lambda_0 + \\lambda_1x_1 + \\lambda_2x_2 = 0.$\nIn words, the decision boundary in the causal direction is a linear function of the covariates. Using this result, we proceed to prove the relation between the marginal covariance matrix and the normal to the decision boundary as in Item 1 of Propostion 9\nWe want to prove $\u039b \u221d \u03a3_X^{-1}\u2211_{X,Y} = \u03a3_X^{-1}\u2211_{X}\u03c6.$\nFirst, we define the random variable $Z := X_1X_1 + X_2X_2$. We can write $p(y = 1|x)$ entirely as function of Z, thus X | Y | Z.\nTo continue with the proof, we consider the Hilbert space of centered random variables with ba- sis given by span (X) and covariance as inner product. Following this geometric interpretation, we define $W_i := X_i \u2212 \u03b1_jZ$, where $\u03b1_jZ$ is the projection of $X_i$ onto the span of $Z$. That is,"}, {"title": "C.2 Proof of Propostion 9 in the anticausal direction", "content": "The normal to the decision boundary using the MAP rule in anticausal direction is derived in a similar way to Appendix C.1. In particular, the normal is given by the points of x where we are indifferent between choosing y = 1 and y = -1. To find such a vector, we solve for x in\n$p(y = 1 | x) = p(y = \u22121 | x)$\n$p(x | y = 1)p(y = 1) = p(x | y = \u22121)p(y = \u22121).$\nIn the second line of the above equation, we used $p(Y = y | x) \u221d p(x | Y = y)P(Y = y)$.\nWe have\n$\\begin{aligned}\n&\\frac{q}{exp^{\\frac{1}{2}(x \u2013 \\mu_1)^T \u03a3_{X|Y}^{-1} (x \u2013 \\mu_1)}} = (1 - q)exp^{-\\frac{1}{2}(x \u2013 \\mu_{-1})^T \u03a3_{X|Y}^{-1} (x \u2013 \\mu_{-1})}, \\\n&log \\frac{q}{1-q} = -\\frac{1}{2}((x \u2013 \\mu_{-1})^T \u03a3_{X|Y}^{-1} (\\mu_{-1}) - (\\mu_1)^T \u03a3_{X|Y}^{-1} (x \u2013 \\mu_1) (52)\n\\end{aligned}$\nThe above equation is linear in x, giving us a linear decision rule, and we would choose Y = 1 if\n$x^T \u03a3_{X|Y}^{-1} (\\mu_1 \u2013 \\mu_{-1}) > \\frac{1}{2} \\mu_{-1}^T \u03a3_{X|Y}^{-1} \\mu_{1} \u2013 \\frac{1}{2} \\mu_1^T \u03a3_{X|Y}^{-1} \\mu_{-1} + log \\frac{q}{(1-q)} (53)$\n$x (- \u2013 \u2081) > (\u2013 \u2081) \u2013 log (54)\nRemark 14 As mentioned in Theorem 5, the decision rule in Equation (53) is known as the Gaussian discriminant analysis [14]. This is a special case of Linear Discriminant Analysis. The family of LDA algorithms also contains Naive Bayes (if all the $x_i$ are conditionally independent) and Quadratic Discriminant Analysis (QDA) (if the covariance matrices for each Y are not equal, giving a curved decision rule).\nNow we will prove that if we have use all the moments in Equations (1) and (2) as constraints, the slope of the two decision boundaries are the same.\nTheorem 10 (Slope of the decision boundary is the same in causal and anticausal direction). Using the constraints in Equations (1) and (2), the slope of $p_X(Y | X)$ inferred using CMAXENT is the same in causal and anticausal direction.\nProof. In Propostion 9 we proved that in the causal direction, the normal vector to the decision boundary in the causal direction is $\u03a3_X^{-1}\u03c6$. Furthermore, using the law of total covariance (and the assumption that z = 0), we can write $\u03a3_X = \u03a3_{X|Y} + \u0441\u0444\u0444^T$, where $\u0441 = \\frac{1}{2\u03a9q(1 \u2013 q)}$ (see Equation (38)). Using the Sherman-Morrison formula [2], we can write\n$\u03a3_X^{-1} =(\u03a3_{X|Y} + \u0441\u0444\u0444^T)^{-1}  = \u03a3_{X|Y} - \u0441(\u03a3_{X|Y}\u03c6 \u03c6^T  \u03a3_{X|Y} / 1 + \u0441\u03c6^T  \u03a3_{X|Y}) $\nApplying this operator to $\u03c6$, and noticing that $\u03c6^T  \u03a3_{X|Y}$ is a scalar, we obtain\n$\u03a3_X^{-1}\u03c6 = \u03a3_{X|Y}\u03c6 + k\u03a3_{X|Y}\u03c6$\nwhere k = (- \u03a3x,y)/(1 + \u0441\u03c6 \u03a3x,y\u03c6). Thus \u03a3-\u00b9 \u03c6\u03b1 \u03a3,, as required."}, {"title": "D Missing covariance between the outcome variable and one of the covariates", "content": "Suppose we do not observe E[YX2", "theory": "The entropy of a distribution with given first and second moments is always less than the entropy of a multivariate Gaussian given the same first and second moments [37", "8.6.5": ".", "Ex|y": "n$\\begin{aligned}\ndet(\u03a3_{X|Y}) =&\n\\frac{\\varsigma_1\\varsigma_2}{(\\frac{\\varsigma_1^2(2q \u2013 1)^2 x_1^2}{\\Omega^2})}{2q(1-q)} \u2013 \\frac{\\varsigma_{1,2}^2}{(\\frac{\\varsigma_{1,2}^2(2q \u2013 1)^2 x_1^2}{2q(1-q)})} \\\\\n&+ {\\frac{2 (2q \u2013 1) x_1\\varsigma_1}{(2q(1-q))2}} +  + {\\frac{2 (2q \u2013 1) x_1 \\varsigma_{2,2} \\varsigma_1}{(\\frac{x_1 + {\\frac{\\varsigma_1\\varsigma_2}{(\\frac{\\varsigma_1^2(2q \u2013 1)^2 x_1^2}{\\Omega^2})"}]}, {}]