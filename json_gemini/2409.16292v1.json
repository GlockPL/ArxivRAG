{"title": "EXPLAINING HUMAN COMPARISONS USING ALIGNMENT-IMPORTANCE HEATMAPS", "authors": ["Nhut Truong", "Dario Pesenti", "Uri Hasson"], "abstract": "We present a computational explainability approach for human comparison tasks, using Alignment Importance Score (AIS) heatmaps derived from deep-vision models. The AIS reflects a feature-map's unique contribution to the alignment between Deep Neural Network's (DNN) representational geometry and that of humans. We first validate the AIS by showing that prediction of out-of-sample human similarity judgments is improved when constructing representations using only higher-scoring AIS feature maps identified from a training set. We then compute image-specific heatmaps that visually indicate the areas that correspond to feature-maps with higher AIS scores. These maps provide an intuitive explanation of which image areas are more important when it is compared to other images in a cohort. We observe a correspondence between these heatmaps and saliency maps produced by a gaze-prediction model. However, in some cases, meaningful differences emerge, as the dimensions relevant for comparison are not necessarily the most visually salient. To conclude, Alignment Importance improves prediction of human similarity judgments from DNN embeddings, and provides interpretable insights into the relevant information in image space.", "sections": [{"title": "1 INTRODUCTION", "content": ""}, {"title": "1.1 THE QUESTION: EXPLAINING HUMAN COMPARISONS", "content": "Work in recent years has shown that DNNs learn feature spaces whose geometry has some similarity to that of humans. This is convincingly shown by the fact that human similarity judgments (HSJs) for pairs of words or images are often quite well predicted by the distances between image-pairs or word-pairs in vision-DNNs or language models (for reviews, see Battleday et al., 2021; Roads & Love, 2024; Sucholutsky et al., 2023). These models therefore naturally extract features relevant for modeling HSJs when trained on standard tasks such as image classification or word prediction. While the object-embeddings of such pretrained machine learning models approximate HSJs quite well, it has been further shown that these predictions can be considerably improved using downstream operations.\nOne such operation is to learn a reweighting of the products of feature values, which improves pre-diction of HSJs for both images (e.g., Peterson et al., 2018; Kaniuth & Hebart, 2022) and words (e.g., Richie & Bhatia, 2021). Another approach is to use supervised pruning to assess features' impor-tance in the context of estimating a set of similarity judgments (Tarigopula et al., 2023; Flechas Man-rique et al., 2023). Pruning does not alter the activation weights of the retained features, but instead removes a subset of features from the embedding matrix. Pruning has also been used to identify sub-spaces in language models that optimize particular classification tasks (e.g., Cao et al., 2021).\nWhile prior work has shown that pruning of nodes in a DNN's penultimate layer can improve predic-tion of similarity judgments, here we are interested in its potential to explain what parts of an image matter for the judgment itself. Understanding which information is used as a basis for comparison"}, {"title": "1.2 LOGIC OF THE CURRENT STUDY", "content": "We present the logic here, with a complete formal presentation provided in Section 2.1. Our ap-proach relies on evaluating how pruning changes the alignment between human and computer-model representational spaces. Both spaces are operationalized using pairwise distances between images. One set of distances is derived from human behavior (HBdist), the other is computed from a computer model (Modeldist). We define the baseline isomorphism between the two spaces as the correlation between these two vectors.\nIn the next step, a perturbation is introduced to the feature representations of an image. Specifically, a feature map in the last convolutional layer is masked. Therefore, the information from that fea-ture map is not encoded in the model, and not propagated onwards to the fully connected layer from which we obtain image embeddings. Subsequently, $Model_{dist}$ is recomputed, as is the isomorphism between the representations. Note that only the target image is affected, and not the other images. Furthermore, HBdist remains unchanged. There are two possible outcomes: i) if the encoded infor-mation from the feature map is cognitively irrelevant or even confounding, its removal could alter $Model_{dist}$ in a way that improves the isomorphism with human similarity judgments. Conversely, ii) if the encoded information from the feature map is cognitively-relevant (e.g., masking a feature map representing an animal's face in context of similarity judgments between animals), its removal will alter $Model_{dist}$ in a way that decreases the isomorphism with human judgments. This occurs because the way that images stand in relation to each other in the DNN representation is now lack-ing information that underlies human judgments. By iteratively masking all feature maps in the last convolutional layer, each feature map is linked with a perturbation score indicating its importance.\nSimilar logic was presented in the previous works, but masking was applied on the image space rather than the latent feature space. For instance, Tarigopula et al. (2023) used this approach with human neuroimaging data to explain which parts of an image contain information relevant to the representational space of various brain regions. In other work, Palazzo et al. (2020) masked image patches to evaluate how masking impacted the compatibility between vision-DNN embeddings and EEG data."}, {"title": "1.3 CURRENT AIMS AND CONTRIBUTION", "content": "The current study's aims advances over prior studies in three respects: it directly studies human comparison processes, it introduces an advantageous masking procedure, and it evaluates the results against typical saliency maps. The aforementioned studies operationalized representational spaces from multivariate fMRI and EEG recordings but have not studied human comparison processes. Furthermore, the technique they use, namely, mask-sweep over an image, presents several major limitations: 1) the mask size is arbitrary, requiring the use of multiple sizes; 2) an arbitrary deci-sion is required regarding how to combine information from different mask sizes; 3) the process is computationally costly, as masks are ideally applied with each pixel being in the mask center; 4) a theoretical weakness is that the mask is not informed by prior information contained in the model.\nDeparting from these prior studies, here we directly model human comparison judgments, and use a different, more efficient approach to masking images, which uses information already present in the DNNs own feature space. Specifically, we focus on the feature maps in a deep convolutional layer, and use them to define the masks. Our approach is inspired by Score-CAM (Score-weighted Class Activation Maps; Wang et al., 2020) which is an explanatory method that generates heatmaps indicating which sections of a target image are relevant for its classification. Score-CAM takes the information in each feature map, upscales it to the original input resolution, uses it as an information selector for the original input image, and computes the activation for correct class (pre-softmax confidence) when using that feature map alone. After repeating this process for all feature maps,"}, {"title": "2 METHODS", "content": ""}, {"title": "2.1 PRELIMINARIES", "content": "\u2022 Architecture and datasets: In the main analysis, We use VGG-16, a deep neural network (Si-monyan & Zisserman, 2014), pre-trained on ImageNet (Deng et al., 2009) and another trained on Ecoset\u00b9 (Mehrer et al., 2021). VGG-16 was used because Ecoset was trained on that model. It is also a common architecture used for predicting human similarity judgements (Peterson et al., 2018; Kaniuth & Hebart, 2022) and has been shown to be a good candidate for behavior or brain align-ment (Schrimpf et al., 2018). As images we used a dataset provided by Peterson et al. (2018), which consists of 720 images divided into six categories of 120 images. The categories were: Animals, Fruits, Furniture, Various, Vegetables and Automobiles (the latter effectively including any means of transportation including horses, sleds, cranes; Transportation henceforth). Images had a native resolution of 500 x 500 which was downscaled to 224 x 224 to fit the model.\n\u2022 Human Similarity Judgments: Let H be a matrix representing the similarity judgments provided by human assessors for n objects. Each entry $H_{ij}$ in the matrix corresponds to the similarity judg-ment between objects i and j. We use the upper triangle of matrix H, denoted as $H_u$.\n\u2022 Object distances in feature space: Let C be a matrix representing the embeddings of n images onto d features of the penultimate layer of the pre-trained computer vision model, denoted as $C \\in R^{nxd}$. Specifically, we use VGG-16 with d = 4096, and the number of images in each Peterson's category is n = 120. Matrix C is obtained by considering all parameters of the pre-trained model, and specifically all 512 feature maps of the deepest convolutional layer. $Z_u$ is the upper triangle of image-pair similarity matrix Z, computed from the Spearman correlation for each row pair in C.\n\u2022 Subspaces in matrix C: We produce two variants of C (all with dimension n \u00d7 d). The first variant (\u201cremove 1\"), denoted as $C^{(\\neg k)}$, is constructed by excluding feature map k where $k \\in {1,2,..., 512}$. The second variant is produced when using only a subset S of feature maps in the model. Let $S \\subset {1, 2, . . ., 512}$ be a set of selected feature-map indices, and let $C^{(S)}$ be the matrix representing the embedding of n images onto d nodes in the penultimate layer, but when using the subset of feature-maps corresponding to S. Note that in all cases, the (one or more) feature-map activations are propagated to the penultimate layer using the pre-trained weights.\n\u2022 From the variants of C we derive matching similarity matrices. The first, $Z^{(\\neg k)}$, is obtained by computing the cosine similarity for each pair of rows in $C^{(\\neg k)}$. The second, $Z^{(S)}$ is formed using the selected feature indices in $C^{(S)}$.\n\u2022 As indicated, $Z_u$ and $H_u$ denote the vectorized upper triangles of matrices Z and H respectively. The Spearman correlation coefficient between the two is denoted as $\\rho(Z_u, H_u)$. We refer to this value as a Baseline Second-Order-Isomorphism (2OI) between the two domains. Analogously, in some cases we compute $\\rho(Z^{(\\neg k)}, H_u)$ and $\\rho(Z^{(S)}, H_u)$.\""}, {"title": "2.2 AIM 1: IDENTIFYING A SUBSET OF FEATURE MAPS THAT OPTIMIZES PREDICTION OF HUMAN SIMILARITY JUDGMENTS", "content": "We define the Alignment Importance Score (AIS) of each feature map in terms of its predictive capacity for the human representation $H_u$. Intuitively, we aim to determine how the removal of each feature map $k \\in {1, 2, . . ., 512}$ affects the baseline isomorphism, $\\rho(Z_u, H_u)$. The removal of each feature map produces a modified 2OI score, $\\rho(Z^{(\\neg k)}, H_u)$. Finally, The AIS of feature map k is defined in Equation 1, with positive values indicating a relatively important feature map, and negative values a less important one. After computing AIS for all feature-maps, we rank-order them based on their AIS.\n$AIS_k = \\rho(Z_u, H_u) \u2013 \\rho(Z^{(\\neg k)}, H_u)$\nWe then identify an optimal subset of feature maps for predicting $H_u$. In each iteration, one fea-ture map is added to the subset S in descending order of AIS rank, and we recompute the 2OI, $\\rho(Z^{(S)}, H_u)$ using that subset of feature maps alone. After these 512 iterations, subset $S^*$ ulti-mately selected is the one that maximizes 201.\nTo validate AIS, we use an 80:20 cross-validation framework where 80% of the entries in $H_u$ are assigned to a training set, and the remaining 20% constitute the test set. The optimal subset of feature map indices, $S^*$, is determined from the training set using sequential features selection as described above. For statistical significance testing, we repeat the entire cross-validation process eight times with different dataset shuffling. This produces 40 Full vs. Retained value-pairs for each relevant comparison. To evaluate generalization, we use only this $S^*$ set of feature maps to predict HSJs on the test set. Prediction performance is compared against a baseline where all 512 features are used for predicting HSJs in the test set. Statistical significance testing, per dataset, is based on the 40 value-pairs produced via cross-validation, which are analyzed using paired two-tailed T-tests (12 tests in all, non-corrected for multiple comparisons). Success of Aim 1 is determined if $\\rho(Z^{(S^*)}, H_u)$ surpasses $\\rho(Z_u, H_u)$, indicating superior prediction compared to the baseline using a subset of feature maps.\nAs an additional baseline, we used LPIPS (Learned Perceptual Image Patch Similarity, Zhang et al., 2018), which is a method for obtaining a cognitively-relevant similarity metric between image pairs. LPIPS fine-tunes a computer vision CNN so that the image distances in the network, calculated as differences between embedding vectors, align with human similarity judgments. LPIPS is fine-tuned using human decision data regarding which of two slightly altered images are closer to an original image, and is based on reweighting all layers of the network. LPIPS has shown to closely match human behavior in 2-Alternative Forced Choice tasks involving minor image distortions and a ref-erence image. To evaluate whether LPIPS is at all viable for our materials and similarity judgments, we applied LPIPS to all images in each dataset to compute pairwise distances between images, and computed the Pearson correlation between the LPIPS distance matrix and the human similarity judgments. Note that the LPIPS method does not allow integration with pruning, as its reweighting function achieves a parallel goal. We use the pre-trained LPIPS weights provided by the original au-thors as these have been trained on a large set of human judgments and have been argued to predict human behavior in multiple domains."}, {"title": "2.3 AIM 2: EXPLAINING HUMAN SIMILARITY JUDGMENTS", "content": "Our goal is to identify which image patches, in image space, are relevant to comparisons between a target image t and other images in the set. This is visualized by creating a heatmap for t identifying those image sections, as follows. We begin by defining a baseline 2OI for t as the Spearman correla-tion between the n - 1 similarity judgments associated with t, as quantified from the model, and the corresponding set of human similarity judgments. As in Aim 1, we define the AIS of feature map k by computing a value that reflects the departure from baseline, as indicated in Equation 1.\nWe iterate over all 512 feature maps, producing 512 AIS values that indicate the relative impor-tance of each feature map for the alignment between DNN-derived distances and human similarity judgments for target image t. This produces an n \u00d7 k matrix (120 [AIS] x 512 [feature map]) for each dataset containing 120 images. We then compare these distributions between the ImageNet and"}, {"title": "2.4 AIM 3: CROSS-REFERENCING HEATMAPS AGAINST SALIENCY MAPS", "content": "We compare the heatmaps produced by our method to those produced by TranSalNet Lou et al. (2022), which is a state-of-the-art DNN that identifies salient image sections and accurately predicts human gaze patterns (see Figure A.2 in Appendix). We cross-reference TranSalNet against our method (AIS) using two approaches: Precision-Recall curves and Subset analyses."}, {"title": "2.4.1 PRECISION-RECALL CURVES", "content": "First, we evaluate how well a pixel's salience predicts its inclusion in an AIS heatmap. When the salience and AIS maps are thresholded at a specified level to form binarized maps, the relationship between them can be understood in terms of precision and recall. The binarized AIS map is treated as the target variable, and the binarized saliency map is the predicting variable. In this case, we have:\n$Precision = \\frac{|TranSalNet \\cap AIS|}{|TranSalNet|}$\nand\n$Recall = \\frac{|TranSalNet \\cap AIS|}{|AIS|}$\nWe describe this relationship using a Precision-Recall curve. The curve is generated by threshold-ing the AIS map at a fixed level and then plotting precision versus recall as the saliency map is thresholded across a range of levels.\nThe following steps were performed for each image: first, we created a heatmap as described in Aim 2 and generated a corresponding saliency map using TranSalNet. We kept the same aspect ratio of the images input to both VGG-16 and TranSalNet for compatibility in later comparisons. We conducted four separate analyses, where we created a binary mask for the AIS map at each of the following percentiles: $P = {60, 70, 80, 90}$. In each analysis we thresholded the saliency maps at all percentiles between 1 and 99, with a step size of 2. Percentiles were calculated separately for each image."}, {"title": "2.4.2 CONDITIONAL PROBABILITY ANALYSIS", "content": "In this analysis we aim to identify whether an image section (specifically, a pixel) identified as salient (Sal) is more likely to also be identified as comparison-relevant (CR; that is, warm-colored in our analysis). To do this we threshold both maps to select the top 5% of Salient and CR pixels, producing $Sal, \\neg Sal, CR$ and $\\neg CR$ partitions of the image pixels. We then compute the Relative Risk (RR) ratio as in Equation 2.\n$RR = P(CR|Sal) \u00f7 P(CR|\\neg Sal)$"}, {"title": "2.5 AIM 4: GENERALIZATION TO OTHER ARCHITECTURES AND TRAINING OBJECTIVES", "content": "In Aims 1, 2 and 3 the image embeddings used were obtained from VGG-16. VGG-16, and a later variant VGG-19, are somewhat unique in that after the deepest convolutional layer, they also include two very large fully connected layers. These layers perform non-linear, abstract interactions over the information in the deepest feature map layer, and are essential for linking this information to the classification task.\nMany other computer-vision architectures do not include such layers, and instead use the deepest feature maps, relatively directly, for classification. This is done by implementing global average pooling, which reduces each of these feature maps into a single value, followed by learning a linear combination of these values for classification. Thus, in these architectures, the final layer before classification receives an input corresponding to the number of feature maps (after global pooling), and produces an output corresponding to the number of classes to be learned.\nTo evaluate the applicability of the AIS-based analysis to other architectures, we applied the analysis developed for Aim 1, with several modifications, to the following models: Inception-V3 (Szegedy et al., 2015), ResNet-152 (He et al., 2016), DenseNet-161 (Huang et al., 2016), EfficientNet-B3 (Tan & Le, 2019), RegNetY-400MF (Radosavovic et al., 2020), and ResNeXt-50-32x4d (Xie et al., 2017). The deepest layers of these architectures contain varying numbers of feature maps: Inception-V3, ResNet-152, and ResNeXt-50-32x4d each have 2,048 feature maps, DenseNet-161 has 2,208, EfficientNet-B3 has 1,536, and RegNetY-400MF has 440.\nWe note that all these architectures learn features in the context of supervised classification tasks. To evaluate feature maps produced by non-supervised learning, we used a ResNet-50 architecture trained with the Barlow Twins self-supervised learning framework (Zbontar et al., 2021). In this ap-proach, the objective of the the model is learn representations by maximizing the similarity between two augmented versions of the same image. In this way, training extracts general visual features, ignoring small visual distortions.\nFor each of these architectures we performed five-fold Cross validation, as detailed for Aim1. For all architectures except VGG-16 and VGG-19, object embeddings were generated by applying global pooling to the feature maps from the deepest convolutional layer. For VGG-16 and VGG-19, em-beddings were constructed from the penultimate, fully connected layer."}, {"title": "3 RESULTS", "content": ""}, {"title": "3.1 AIM 1: IDENTIFYING A SUBSET OF FEATURE MAPS THAT OPTIMIZES PREDICTION OF HUMAN SIMILARITY JUDGMENTS", "content": "As shown in Figure 1, by computing AIS it was possible to identify a subset of 512 feature maps for each dataset, which produced improved out-of-sample predictions compared to a baseline condition where all feature maps were used. This was consistent for models trained on Ecoset or ImageNet, with less than 50% of the 512 feature maps being used in 5/12 cases. Paired T-tests indicated that in all 12 cases, predictions from Full features were less accurate than those from features learned via pruning (p-values < 0.01). The performance metrics of ImageNet and Ecoset were quite similar."}, {"title": "3.2 AIM 2: EXPLAINING HUMAN SIMILARITY JUDGMENTS", "content": "Figure 2 shows examples of heatmaps produced by alignment importance scoring. Given that each dataset contained 120 images, we selected 4 images from each dataset according to the principle that two of the images produced apparently sensible results, and the two others were less sensible. It can be seen that the method can identify image-sections that are relevant for inter-category comparisons, such as the faces of animals, central parts of fruits and vegetables, and discriminating elements of artifacts and man made objects. As we will see later, these are not necessarily the most salient aspects of images.\nTo assess the similarity of heatmaps produced by Ecoset and ImageNet, for each image we calculated the correlation between the heatmaps produced by the two methods. The median correlation values were as follows: 0.80 \u00b1 0.16 for Animals, 0.64\u00b10.19 for Transportation, 0.73 \u00b1 0.22 for Fruits,"}, {"title": "3.3 AIM 3: CROSS-REFERENCING HEATMAPS AGAINST SALIENCY MAPS", "content": ""}, {"title": "3.3.1 PRECISION-RECALL CURVES", "content": "For each image, we thresholded the AIS-produced heatmap at a given threshold to form a binary pre-diction target with AIS-related image sections (after thresholding) constituting the positive class. We then evaluated the extent to which these could be predicted by the saliency maps, using a Precision-Recall curve. In this analysis, the target variable is thresholded at a fixed level (e.g., 90th percentile),"}, {"title": "3.3.2 CONDITIONAL PROBABILITY ANALYSIS", "content": "We observed that areas identified as comparison-relevant by AIS heatmaps were much more likely to be associated with salient image sections than with non-salient image sections, as indicated by"}, {"title": "3.4 AIM 4: GENERALIZATION TO OTHER ARCHITECTURES AND TRAINING OBJECTIVES", "content": "We find that quantifying alignment importance improved out-of-sample prediction of human simi-larity judgments across all architectures and all six categories tested (see Figure 6)."}, {"title": "4 DISCUSSION", "content": "Understanding what information is used in human comparisons is important not only for a bet-ter understanding of the comparison process itself, but also for comprehending how people form memories and make decisions (Roads & Love, 2024). We introduced and validated a feature-map's Alignment Importance as a meaningful parameter relevant to such explanations. We first showed that AIS values generalize to improve prediction of human similarity judgments. This complements current approaches that achieve improvements by using reweighting or pruning of nodes in a DNN's penultimate layer (e.g., Peterson et al., 2018; Attarian et al., 2020; Kaniuth & Hebart, 2022; Jha et al., 2023; Tarigopula et al., 2023).\nWe then used AIS to produce explanations for those judgments via heatmaps. These heatmaps offered some correspondence to state-of-the-art saliency maps, in that when saliency maps were thresholded at high percentiles, the resulting representation could sometimes predict (binarized) AIS heatmaps quite well, especially for Animals. However, instances where saliency and AIS-reduced maps diverged are of major theoretical importance as they show it is possible to dissociate visually salient image elements from those that are important for comparison.\nBecause the method we present is based on mapping, or aligning a DNN's representational space to a human one via pruning, the feature space of the pretrained-DNN is of fundamental importance. For this reason, in Aim 1 we studied DNNs trained on both ImageNet and Ecoset datasets. We found that AIS scores improved out-of-sample prediction for models trained on either of the training datasets. Thus, both models learn feature maps particularly relevant for accounting for the representational space of specific categories. For both Ecoset and ImageNet, category-specificity was shown in the fact that the relative ranking of AIS scores varied greatly across categories. Interestingly, Ecoset appears to distribute the AIS scores slightly more uniformly across feature-maps than ImageNet, which is a topic that requires further investigation.\nFurther speaking to generalization across both training sets, the heatmaps were, for the most part, quite similar when created from Ecoset or ImageNet AIS scores, with average correlations between the heatmaps exceeding 0.75 for the Animals category. However, some images showed low correla-tions, and these tended to be associated with more uniform post-softmax distributions in the DNN's categorization layer. This means that divergence in heatmaps produced by the two models were more prevalent for images that one of the models found difficult to classify. In practice, we recom-"}]}