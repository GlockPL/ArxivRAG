{"title": "Seg-CycleGAN : SAR-to-optical image translation\nguided by a downstream task", "authors": ["Hannuo Zhang", "Huihui Li*", "Jiarui Lin", "Yujie Zhang", "Jianghua Fan", "Hang Liu"], "abstract": "Optical remote sensing and Synthetic Aperture\nRadar(SAR) remote sensing are crucial for earth observation, of-\nfering complementary capabilities. While optical sensors provide\nhigh-quality images, they are limited by weather and lighting\nconditions. In contrast, SAR sensors can operate effectively\nunder adverse conditions. This letter proposes a GAN-based\nSAR-to-optical image translation method named Seg-CycleGAN,\ndesigned to enhance the accuracy of ship target translation by\nleveraging semantic information from a pre-trained semantic\nsegmentation model. Our method utilizes the downstream task\nof ship target semantic segmentation to guide the training\nof image translation network, improving the quality of out-\nput Optical-styled images. The potential of foundation-model-\nannotated datasets in SAR-to-optical translation tasks is re-\nvealed. This work suggests broader research and applications for\ndownstream-task-guided frameworks. The code will be available\nat https://github.com/NPULHH/.", "sections": [{"title": "I. INTRODUCTION", "content": "OPTICAL remote sensing and Synthetic Aperture\nRadar(SAR) remote sensing are important means to cap-\nture images for earth observation. Although images acquired\nby optical remote sensing feature high quality, this method\nbecomes ineffective under conditions like clouds, fog and\nnighttime. However, Synthetic Aperture Radar, as an active\nsensor, can function normally under nighttime and cloudy\nconditions. As remote sensing technology is a crucial measure\nfor ensuring the safety of ships, effective monitoring and\nsupporting rescue operations, transforming ship targets from\nSAR images to optical-styled images using Generative Ad-\nversarial Networks (GANs) [1] has significant research value\nbut also faces certain challenges. According to dependence\non paired datasets, current GAN-based methods for SAR-\nto-optical image translation can be divided into paired and\nunpaired methods [2]. Using paired datasets and corresponding\npaired image-to-image translation methods [3] can impose\nmore strict constraints on generator modules, which makes\nit more likely to achieve excellent translation results. How-\never, for moving targets like ships, obtaining paired datasets\nis extremely difficult, leaving unpaired translation methods\nthe only choice. Among unpaired methods, U-GAT-IT [4]\nintroduces attention modules and an adaptive normalization\nfunction to achieve image translation requiring large shape\nchanges. NICE-GAN [5] achieves a compact model structure\nand more effective training by reusing the first few layers\nof its discriminator module as an image encoder. CUT [6]\napplies contrastive learning to gain great performance on\ndata that does not meet bijective relationship required by\ncycle-consistency loss [7]. However, in the field of SAR\nto optical translation, among unpaired methods, CycleGAN\n[7] has relatively better performance [2]. As for target-level\nSAR-to-optical image translation, [8] studied SAR-to-optical\nimage translation for aircrafts, but they used CAD models to\ngenerate rendered optical-styled data paired with SAR images\nfor training. Due to the existence of Sim2Real gap, application\nvalue of this algorithm is limited. Besides, current translation\nmethods are carried out independently of downstream tasks\nsuch as semantic segmentation, detection, or recognition of\nships. Thus the abscence of constraints from downstream tasks\nin training makes it difficult for image translation models to\nlearn to capture semantic information that distinguishes ship\ntargets from background. This leads to poor performance in\nimage translation tasks targeting specific objects like ships.\nAdditionally, there are no suitable evaluation metrics to assess\nimage translation performance from the perspective of down-\nstream tasks.\nOur contributions to address the above issues are as follows:\nA GAN-based SAR-to-optical image translation method driven\nby a downstream task is proposed to perform image translation\nfor ship targets. Our method uses a pre-trained semantic seg-\nmentation module as part of the objective function. This mod-\nule can guide the training of image translation with semantic\ninformation gained from the downstream task on optical data.\nBy utilizing Segment Anything Model [9] to annotate optical\ndata [10], we obtained accurate ship segmentation labels to\ntrain a ship segmentation module, which makes it possible\nto guide the training of the SAR-to-optical translation model\nwith semantic information related to ship targets. Experiments\nconducted on DIOR [11] and HRSID [12] datasets proves our\nmethod to be effective. Experiments conducted on WHU-OPT-\nSAR dataset [13] further demonstrate that our method is also\neffective in scenes of different scales.\nIn the remaining parts of this letter, we first introduce\nour proposed Seg-CycleGAN method in section II. Results\nof experiments are presented in Section III. And finally, the\nconclusion is drawn in section IV."}, {"title": "II. METHOD", "content": "It is challenging to find a semantic segmentation dataset for\nship targets, but ship target detection datasets do exist. SAM\n[9], a Foundation Model in the field of semantic segmentation,\nhas excellent zero-shot capability. According to the processing\nmethod mentioned in SAMRS [10], the HBB annotations\nfrom the ship detection dataset are used as prompts for\nSAM, and output of SAM is used as segmentation labels\nfor ship targets. Due to significant differences in color and\ntexture between ships and their background(such as water\nsurface or shores), the segmentation annotations are relatively\naccurate. This method is applied to the HRSC2016-MS [14]\nand DIOR datasets to obtain semantic segmentation datasets\nof ship targets. After appropriate data augmentation, the ship\nsegmentation dataset is used to train a ship-specific binary\nsemantic segmentation module. Images in acquired dataset\nare also used in training and testing of SAR-to-optical image\ntranslation models."}, {"title": "B. Network Architectures", "content": "Since there is no paired SAR-optical ship target dataset, and\nin the field of SAR-to-optical image translation, performance\nof CycleGAN is relatively better than other unpaired methods\n[2], CycleGAN architecture is adopted as the backbone net-\nwork. Thus the backbone network contains 2 generator mod-\nules and 2 discriminator modules as Fig. 1 shows. The gener-\nators are used to perform image style transferring, taking real\nSAR images as input and outputting optical-styled images, or\ntaking real optical images as input and outputting SAR-styled\nimages, while the purpose of discriminators is to distinguish\nwhether received data is real or generated. The generators and\ndiscriminators are trained alternately. In their competition-like\ntraining process, the ability of discriminators to distinguish"}, {"title": "C. Loss Functions", "content": "Adversarial loss [1] $L_{GAN}$ of classical generative adver-\nsarial networks and cycle-consistency loss $L_{cyc}$ are adopted.\n$D_{OPT}$ is trained to distinguish the authenticity of optical-\nstyled data, while $D_{SAR}$ is used to distinguish authenticity\nof SAR-styled data. $r_s$ represents real SAR images, and $r_o$\nrepresents real optical data. $G_{OPT}$ is used to generate optical-\nstyled data, and $G_{SAR}$ is trained to output SAR-styled images.\nThe definitions of $L_{GAN}$ and $L_{cyc}$ are as follows:\n$L_{GAN} (G_{OPT}, D_{OPT}) =\\mathbb{E}_{r_o} [log D_{OPT} (r_o)] + \\mathbb{E}_{r_s}[log(1 \u2013 D_{OPT}(G_{OPT}(r_s)))]$ (1)\n$L_{GAN} (G_{SAR}, D_{SAR}) =\\mathbb{E}_{r_s} [log D_{SAR}(r_s)] + \\mathbb{E}_{r_o}[log(1 \u2013 D_{SAR}(G_{SAR}(r_o)))]$ (2)"}, {"title": null, "content": "$L_{cyc}(G_{OPT}, G_{SAR}) =\\mathbb{E}_{r_s} [||G_{SAR}(G_{OPT}(r_s))||_1] + \\mathbb{E}_{r_o}[||G_{OPT}(G_{SAR}(r_o))||_1]$ (3)\nDuring the alternating training proccess, $D_{OPT}$ and $D_{SAR}$\nare optimized to maximize $L_{GAN}$, while $G_{OPT}$ and $G_{SAR}$ are\noptimized to minimize $L_{GAN}$. $L_{cyc}$ requires that the generated\ndata inputted to another generator produces an output that\nhighly resembles the original input. While $L_{GAN}$ facilitates\nbidirectional style transferring, $L_{cyc}$ ensures that the image\ntranslation process retains information contained in the input\nimage. Under the influence of $L_{cyc}$, the SAR-to-optical gener-\nator used in experiments effectively preserves different regions\nin input SAR images, including land, sea and ships. One major\nadvantage of GANs is using generators and discriminators\nto replace explicitly defined complex loss functions. Based\non this idea, the pre-trained semantic segmentation module\ncan be viewed as a loss function highly adapted to the data\ndistribution of the target optical domain, e.g., segmentation\nloss $L_{seg}$. This module is trained with a SAM-annotated\nship target semantic segmentation dataset before the training\nof other modules of Seg-CycleGAN. When used to guide\nthe training of $G_{OPT}$, parameters of the semantic segmen-\ntation module are fixed. During training, cross-entropy loss of\ngenerated optical-styled data and corresponding SAR image\nsegmentation labels is calculated, and the obtained gradients\nduring back propagation are used to update the weights of the\nSAR-to-optical generator. The pre-trained semantic segmen-\ntation module contains information for recognizing whether\na pixel belongs to a ship or other scenes in optical images,\nincluding shape, color, and edge. Hence using it as a loss\nfunction encourages the generator working with unpaired data\nto convert ship regions in SAR images into ship regions in\nthe optical-styled output, with other regions correspondingly\nconverted to optical-styled sea or land, suppressing appearance\nof unreal targets and textures. The segmentation loss is defined\nas follows:\n$L_{seg}(G_{OPT}) = Cross\\_Entropy(gt_{seg}, S_\\theta(G_{OPT}(r_s)))$ (4)"}, {"title": null, "content": "where $gt_{seg}$ represents the semantic segmentation labels for\nships and $S_\\theta$ represents the pre-trained semantic segmentation\nmodule parametrized by $\\theta$. The definition of overall objective\nfunction $L(G_{OPT},G_{SAR}, D_{OPT}, D_{SAR})$ for the 2 generator\nmodules in Seg-CycleGAN is as follows, composed of $L_{GAN}$,\n$L_{cyc}$, and $L_{seg}$:\n$L(G_{OPT}, G_{SAR}, D_{OPT}, D_{SAR}) =L_{GAN} (G_{OPT}, D_{OPT}) + L_{GAN}(G_{SAR}, D_{SAR})+\\alpha L_{cyc} (G_{OPT}, G_{SAR})+\\beta L_{seg}(G_{OPT})$ (5)\nwhere $\\alpha$ and $\\beta$ are hyperparameters used to balance different\nterms. In the experiments, $\\alpha$ is set to 10.0, and $\\beta$ is set to 0.3."}, {"title": "D. Training Details", "content": "Before all experiments, we preprocessed datasets, excluding\nimages without ships in the DIOR dataset and cropping\nsamples from the HRSID and DIOR datasets. Original size"}, {"title": "III. EXPERIMENT RESULTS", "content": "In this section, we evaluate experiments conducted on\nHRSID, DIOR, and refined WHU-OPT-SAR dataset. HRSID\nand DIOR together form an unpaired SAR-optical image\ndataset, which we refer to as HRSID-DIOR. On HRSID-\nDIOR, effectiveness of our proposed Seg-CycleGAN method\nin translating remote sensing data containing ships is proved.\nPerformance of Seg-CycleGAN on land use classification\ndatasets is also evaluated with WHU-OPT-SAR dataset."}, {"title": "A. Datasets", "content": "Experiments are conducted on subsets of the HRSID, DIOR,\nand WHU-OPT-SAR datasets. The HRSID dataset contains\n5604 SAR images with ship targets, including 16951 ship\ninstances, covering resolutions of 0.5m, 1m, and 3m, and is\ndivided into nearshore and offshore parts. To obtain a higher\nproportion of data with complex scenes, we selected 1031\nimages from the nearshore part, 662 samples of which used as\nthe training set for experiments on various unpaird image-to-\nimage translation models. The DIOR dataset contains 23463\nremote sensing images with 190288 target instances annotated\nin HBB format, covering 20 common object categories with\nspatial resolutions ranging from 0.5m to 30m.We selected data\nfrom 1302 training samples containing ship targets for training\nand 1900 testing samples containing ship targets for testing.\nThe WHU-OPT-SAR dataset is a land use classification dataset\ncontaining 100 pairs of 5556x3704 pixel SAR-Optical pixel-\nregistered images with a resolution of 5m, and it has segmen-"}, {"title": "B. Qualitative Evaluation", "content": "As shown in Fig. 2, the proposed Seg-CycleGAN performs\nwell on both offshore data with simple images and nearshore"}, {"title": "C. Quantitative Evaluation", "content": "To evaluate different algorithms from the perspective of\nimage similarity, we use 4 metrics: PSNR, SSIM, cosine"}, {"title": "IV. CONCLUSION", "content": "This letter proposes a SAR-to-optical image translation\nmethod driven by a downstream task, named as Seg-\nCycleGAN. Its advantage lies in using a pre-trained semantic\nsegmentation module to enable the generator of GAN-based\nalgorithms to perform accurate translation of SAR images\ncontaining ship targets. Additionally, this letter demonstrates\nthe high application value of datasets annotated by SAM in\ntasks of semantic segmentation and image-to-image transla-\ntion. Experiments on a land use classification dataset and ship\ntarget datasets prove that Seg-CycleGAN can be applied to\nSAR-to-optical image translation task of different resolutions\nand scenarios. We hope this letter can promote the research\nand application of downstream-task-guided translation frame-"}]}