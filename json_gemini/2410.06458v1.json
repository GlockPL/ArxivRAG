{"title": "LLM Self-Correction with DECRIM: DECOMPOSE, CRITIQUE, AND REFINE for Enhanced Following of Instructions with Multiple Constraints", "authors": ["Thomas Palmeira Ferraz", "Kartik Mehta", "Yu-Hsiang Lin", "Haw-Shiuan Chang", "Shereen Oraby", "Sijia Liu", "Vivek Subramanian", "Tagyoung Chung", "Mohit Bansal", "Nanyun Peng"], "abstract": "Instruction following is a key capability for LLMs. However, recent studies have shown that LLMs often struggle with instructions containing multiple constraints (e.g. a request to create a social media post \"in a funny tone\" with \"no hashtag\"). Despite this, most evaluations focus solely on synthetic data. To address this, we introduce REALINSTRUCT, the first benchmark designed to evaluate LLMs' ability to follow real-world multi-constrained instructions by leveraging queries real users asked AI assistants. We also investigate model-based evaluation as a cost-effective alternative to human annotation for this task. Our findings reveal that even the proprietary GPT-4 model fails to meet at least one constraint on over 21% of instructions, highlighting the limitations of state-of-the-art models. To address the performance gap between open-source and proprietary models, we propose the DECOMPOSE, CRITIQUE, AND REFINE (DECRIM) self-correction pipeline, which enhances LLMs' ability to follow constraints. DECRIM works by decomposing the original instruction into a list of constraints and using a Critic model to decide when and where the LLM's response needs refinement. Our results show that DECRIM improves Mistral's performance by 7.3% on REALINSTRUCT and 8.0% on IFEval even with weak feedback. Moreover, we demonstrate that with strong feedback, open-source LLMs with DECRIM can outperform GPT-4 on both benchmarks.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated impressive instruction following ability across various tasks, such as creative writing, coding, and arithmetic reasoning (Brown et al., 2020; Wei et al., 2021; Mishra et al., 2022; Sanh et al., 2022; Ouyang et al., 2022; Wang et al., 2022). Despite their remarkable success, recent studies and benchmarks have highlighted significant limitations in LLMs' ability to adhere to user-defined rules (termed as constraints henceforth) within instructions (Mu et al., 2023; Zhou et al., 2023b; Lu et al., 2023; Sun et al., 2023; Zhou et al., 2023a; Jiang et al., 2024; Yao et al., 2024a; Qin et al., 2024). Based on analysis of real user instructions to AI assistants, we estimate that 30% of real-user requests to LLMs require satisfying some constraints (estimation details in Appendix D.1). This highlights the importance of evaluating and enhancing LLMs' ability to follow real-world multi-constrained instructions.\nBenchmarking on Real-World Requests. These benchmarks often rely on synthetic data either to address data scarcity or to facilitate automatic rule-based evaluation (Zhou et al., 2023a; Jiang et al., 2024; Yao et al., 2024a). In this work, we argue that synthetic constraints may not accurately capture the complexity and nuances of real-world scenarios, being sometimes artificially difficult. As a result, focusing on synthetic benchmarks may push research in the wrong direction, as improvements on these may not translate to real-world performance and could even degrade it.\nTo address this gap, we introduce the REALINSTRUCT benchmark, which evaluates LLMs using real user requests to AI assistants. It assesses LLMs' response on individual constraints at a time, as illustrated in Figure 1. Our analysis reveals that even strong proprietary model GPT-4 fails to meet at least one constraint in over 21% of the instructions, highlighting the limitations of current LLMs in handling user's constrained instructions.\nLLM-based evaluation. Evaluating real-world instructions is challenging due to their open-ended nature. Drawing on recent research on LLM-based evaluation (LLM-as-a-Judge) (Zheng et al., 2023;"}, {"title": "2 REALINSTRUCT Dataset", "content": "We introduce REALINSTRUCT, a novel dataset consisting of original user-generated instructions, each decomposed into a task and a set of constraints (see Figure 1 for an example). The task represents the user's main objective \u2013 the outcome they expect from LLM \u2013 and may optionally include context to aid the model's understanding. Constraints are conditions or limitations that guide the LLM's generation. Formal definitions of these terms are provided in Appendix C. Since users rarely specify constraints in a structured list format, the decomposition breaks instructions into manageable items, providing the necessary granularity for evaluating LLMs' ability to follow constraints.\nUsing this dataset, we develop a benchmark protocol that evaluates LLM performance on each constraint at a time, generating constraint-level scores"}, {"title": "2.1 Data Description", "content": "The REALINSTRUCT dataset is divided into two splits: test and validation. The test split, intended for LLM evaluation, was human-validated and contains 302 instructions with 1,055 constraints. The validation split, used for method validation such as training judges, was not human-validated and includes 842 instructions with 2,500 constraints. Table 2 compares REALINSTRUCT with existing benchmarks for evaluating LLMs' ability to follow multi-constrained instructions. Unlike other datasets that rely on synthetic or crowdsourced instructions, REALINSTRUCT uniquely captures real user interactions with AI assistants, offering a more realistic representation of real-world use cases.\nTo better understand constraint characteristics in REALINSTRUCT, we manually categorized them into 22 distinct groups. Detailed dataset statistics are presented in Appendix E. When comparing REALINSTRUCT with IFEval, one of the popular benchmark for constraint following that consists solely of synthetic constraints, we found that only 6.3% of REALINSTRUCT constraints overlap with the 25 types in IFEval, and 11 IFEval constraint types never appear in REALINSTRUCT. This discrepancy highlights the gap between synthetic datasets and real LLM use cases, with synthetic constraints failing to adequately represent the challenges users may pose to LLMs."}, {"title": "2.2 Data Construction", "content": "We built REALINSTRUCT dataset using prompts from a public dataset of conversations between users and AI assistants. We filtered and processed the dataset in five steps: 1) removed AI responses, retaining only the first user turn (instruction); 2) excluded non-English instructions; 3) filtered out code-related instructions using an open-source LLM as a zero-shot classifier; 4) kept only instructions containing constraints, again using a LLM classifier; and 5) manually validated the remaining data for relevance, clarity, and safety. See Appendix D.1 for details on the data filtering.\nFollowing filtering, we decomposed the instructions into tasks, contexts, and constraints using GPT-4 (Achiam et al., 2023). In our tests, we found that this three-part decomposition (task, context, constraints) provided more robust outcomes"}, {"title": "3 Self-correction with DECOMPOSE, CRITIQUE, AND REFINE (DECRIM)", "content": "In this section, we present our proposed DECRIM self-correction pipeline, designed to enhance LLM responses to follow user constraints. Given a multi-constrained user instruction, the pipeline iteratively refines the LLM's response through four key steps: Initial Response, Decompose, Critique, and Refine, iterating until the response meets all constraints or a maximum number of iterations $\\mathbb{N}_{max}$ is reached. Figure 2 provides an overview of the proposed pipeline and we detail each of these steps below:\n1. Initial Response is generated directly from the original user instruction using a strong prompt.\n2. Decompose the original instruction into a list of granular constraints to be followed. This task is similar to the instruction decomposition performed in REALINSTRUCT, but here, the decomposer model focuses solely on listing the constraints, simplifying the task. A prompt example is provided in Appendix G.1.\n3. Critique the response using the Critic model to identify any unsatisfied constraints. If all constraints are satisfied, the response is considered final. Otherwise, the Critic provides feedback in natural language, specifying which constraints were not satisfied.\n4. Refine the response using the Critic's feedback to address unsatisfied constraints. The underlying LLM generates an improved response using a refinement prompt that incorporates the feedback, along with the original instruction and previous response.\nThe Critique and Refine steps can be repeated iteratively until the response satisfies all constraints or the specified maximum number of iterations is reached.\nTo our knowledge, DECRIM is the first approach specifically designed to tackle the challenge of following instructions with multiple open-ended constraints. Unlike previous methods, which assume constraint independence or focus on specific constraint types, our approach makes no assumptions about the nature of user constraints. In Section A.2 of Related Work, we compare our pipeline with other works for constrained generation, includ-"}, {"title": "4 Experimental Setup", "content": "This section outlines the experimental setting to evaluate our proposed methods. We first validate the use of LLM-as-a-Judge for the REALINSTRUCT benchmark (Sec. 4.1). Next, we benchmark models on instructions with multiple constraints (Sec. 4.2) and, finally, test our proposed self-correct DECRIM pipeline to improve open-source models in this task (Sec. 4.3). All experiments with open LLMs were performed with HuggingFace's Transformers library (Wolf et al., 2020) on an AWS instance with 8 V100 32GB GPUs. Times are reported in this configuration."}, {"title": "4.1 Validating LLM-as-a-Judge for Constraint Satisfaction", "content": "Given the open-ended nature of REALINSTRUCT benchmark instructions, rule-based or reference-guided evaluation is not feasible. Drawing from recent work showing the effectiveness of LLMs as evaluators, particularly GPT-4 (Zheng et al., 2023; Zeng et al., 2024), we adopt an LLM-as-a-Judge evaluation protocol for REALINSTRUCT. To assess LLM-as-a-judge reliability compared to human evaluators and identify the most cost-effective approach, we introduce EvalJudge, a test set of instruction-constraint-response triples with ground-truth labels (Sec. 4.1.2). We benchmark both proprietary and open-source models using four adaptation strategies (Sec. 4.1.1) against human judgments on this set. More specifically, we investigate whether open-source models can match the performance of high-cost proprietary models in the LLM-as-a-Judge role."}, {"title": "4.1.1 Adaptation strategies", "content": "Models We evaluate three proprietary and three open-source LLMs as candidates for LLM-as-a-Judge for Constraint Satisfaction. The proprietary models include GPT-4 (gpt-4-0314), GPT-4-Turbo (gpt-4-turbo-2024-04-09), and GPT-3.5-Turbo (gpt-3.5-turbo-0125), ordered by decreasing API cost. For the open-source models, we experiment with Mistral 7B Instruct v0.2 (Jiang et al., 2023) (hereafter referred to as Mistral v0.2), Vicuna 7B v1.3 (Chiang et al., 2023), and Zephyr 7B \u03b2 (Tunstall et al., 2024), all top performers on the Open LLM Leaderboard as of February 2024 (Beeching et al., 2023).\nWe explore four adaptation strategies, including three In-Context Learning (ICL) approaches and one weakly supervised fine-tuning for open-source model. Some strategies use the Chain-of-Thought (CoT) prompt, known to improve LLM reasoning (Wei et al., 2022; Zheng et al., 2023), and we also investigate whether to evaluate constraints individually or collectively. The strategies are as follows:\na) Instruction-wise Eval (ICL-Inst.): All constraints within an instruction are evaluated simultaneously. The LLM-as-a-Judge is presented with an instruction and a list of constraints, and using a CoT prompt with an in-context example containing an instruction and two constraints, it generates reasoning and predictions for all constraints at once.\nb) Constraint-wise Eval (ICL-Const.): Each constraint is evaluated independently. For every response-constraint pair, the LLM-as-a-Judge directly predicts \"Constraint followed\" or \"Constraint not followed.\" Two constraint-response pairs serve as in-context examples.\nc) Constraint-wise Eval + CoT (ICL-Const.+CoT): The LLM-as-a-Judge is prompted to generate reasoning for each constraint, followed by a prediction of \"Constraint followed\" or \"Constraint not followed.\" Evaluation is also performed for each response-constraint pair independently, using two constraints as in-context examples.\nd) Weakly Supervised Open LLM (Supervised): We fine-tuned Mistral for the LLM-as-a-judge task. We construct a training data using the weak instruction annotations from REALINSTRUCT's validation set. We generate Mistral responses to these instructions, and weak constraint satisfaction annotations with reasoning trails from GPT-4-Turbo with ICL-Const.+CoT prompt. We fine-tune Mistral v0.2 using LoRA adapters (Hu et al., 2022) on this dataset, guiding model to mimic GPT's reasoning. Further details can be found in Appendix H.4.\nAll prompt templates are provided in Appendix H.1. Costs and processing times for each configuration are detailed in Table 3."}, {"title": "4.1.2 The EvalJudge Dataset", "content": "Since no public dataset exists for the task of evaluating whether a given response satisfies a specific constraint or not, we create EvalJudge dataset, derived from the test split of the REALINSTRUCT dataset. To ensure diversity, we divided the instructions into two subsets, generating one"}, {"title": "4.2 Benchmarking LLMs on REALINSTRUCT", "content": "We benchmark models on REALINSTRUCT for the task of following multi-constrained instructions. The evaluation includes two proprietary models-GPT-4 and GPT-3.5-Turbo-and three open-source LLMs: Mistral v0.2, Vicuna v1.3, and Zephyr \u03b2, selected based on their performance on Chatbot Arena (Chiang et al., 2024). The judge is GPT-4-Turbo with ICL-Const+CoT prompt. Following Zhou et al. (2023a); Saha et al. (2024), we report accuracy at both the instruction and constraint levels."}, {"title": "4.3 Evaluating DECRIM pipeline", "content": "To validate the effectiveness of our DECRIM pipeline, we conduct experiments using Mistral v0.2 as the underlying LLM. We use $\\mathbb{N}_{max} = 10$. We also investigate the contribution of different Decomposer and Critic models for the pipeline, and compare the performance against the proprietary model GPT-4. We present extra comparisons on Appendix B.\nDatasets We evaluate model performance on the REALINSTRUCT dataset and the IFEval dataset (Zhou et al., 2023a). While IFEval is based on synthetic constraints, it serves as a valuable benchmark for validating our pipeline, as it is popularly used for evaluating constrained instruction-following.\nBaselines To measure the improvements introduced by our method, we establish the following baselines with Mistral v0.2:\n1. Conventional: Only the instruction as input.\n2. \"Make sure\": Appends the text \u201cMake sure to follow all the provided constraints\u201d to the instruction, creating a strong and fair baseline.\n3. Self-Refine: Adapts the Madaan et al.'s (2023) approach for the case of multi-constraint instructions. While Self-Refine uses the model itself as its critic without additional context, our DECRIM pipeline employs a critic with fine-grained evaluation, assessing each constraint individually. This baseline helps quantify the value added by this modeling.\nDecomposer We use a Self-Decomposer, where the LLM itself lists relevant constraints, simplifying the decomposition from Section 2.2 by omitting the request for task and context. The prompt for this approach is detailed in Appendix G.1.\nCritic Model We explore two Critic models based on the underlying LLM:\na) Self-Critic: Uses the model itself as the Critic, with the best ICL-based adaptation (from Section 4.1.1), specifically the ICL-Const+CoT prompt.\nb) Supervised Critic: the Mistral weakly supervised for LLM-as-a-judge, as described in 4.1.1.\nAblations with Oracle and GPT-4 To understand the impact of strong Decomposer and Critic mod-"}, {"title": "5 Results and Discussion", "content": "We discuss our results, particularly the Reliability of LLM-as-a-Judge for Constraint Satisfaction (Section 5.1), the ability of various open-source and proprietary LLMs on follow multi-constrained instructions (Section 5.2), and the efficacy of our"}, {"title": "5.1 Reliability of LLM-as-a-Judge", "content": "Results from several model and baseline judges on the EvalJudge dataset are presented in Table 3. Human inter-rater reliability (Human 1, Human 2, and Expert) is moderate (Krippendorff's $\\alpha = 0.61$), with lower agreement between Humans 1 and 2 ($\\kappa = 0.44$). This highlights the inherent challenges in verifying constraint satisfaction, as the task can be subjective and ambiguous, when involving multiple sub-constraints, despite efforts to minimize these issues during data annotation (see Appendix D.2.2).\nGPT-4-Turbo with CoT is Reliable and More Cost-Efficient. We observe that GPT-4, a widely used LLM-as-a-Judge, shows lower performance compared to humans while maintaining moderate correlation with humans (k = 0.42). Using GPT-4-Turbo, evaluating constraints individually yields better results than evaluating them all at once, despite a 37% cost increase. GPT-4-Turbo with CoT prompt offers a more performant and cheaper alternative to GPT-4. It reduces costs by 57% while improving overall performance (Macro F1) by +7.0% and in detecting unmet constraints (F1 Negative) by +19.0%. Its correlation with Expert annotation is similar to that of Human 2 (0.58 vs. 0.60). We thus adopt GPT-4-Turbo with CoT as the standard evaluation for REALINSTRUCT.\nOpen-source LLMs are unreliable judges. ICL-based configurations of open-source LLMs (Mistral, Vicuna, Zephyr) exhibit poor performance in all scenarios, particularly in detecting unmet constraints. Vicuna and Zephyr closely mirror the \"All Satisfied\" baseline, suggesting they are lenient judges. Mistral, despite similar Macro-F1, diverges in other metrics, indicating more random than lenient decisions (similar to GPT-3.5-Turbo). When weakly supervised with GPT-4-Turbo's CoT reasoning trails, Mistral significantly improves over-all performance and the ability to detecting unmet constraints (+12.9 in Macro F1 and +28.1 in F1 Neg.). This reduces the macro F1 gap with GPT-4-Turbo by about 50%, but the model still shows poor agreement with humans, indicating that open-source LLMs are not yet reliable judges."}, {"title": "5.2 LLMs' ability to follow multi-constrained instructions on REALINSTRUCT", "content": "Benchmarking results for all models on REALINSTRUCT are presented in Table 5. We observe"}, {"title": "5.3 Effectiveness of DECRIM Self-Correction", "content": "We present the results of our experiments with Mistral v0.2 using the DECRIM Self-Correction pipeline in Table 4. A slight improvement is observed with the Make Sure prompt compared to the conventional prompt on both datasets, demonstrating that it serves as a strong baseline for first-generation responses. Consequently, we adopt Make Sure as the baseline for further comparisons.\nWe classify the DECRIM configurations into three categories: (1) Fairly Comparable, where"}, {"title": "6 Conclusion", "content": "In this work, we benchmarked LLMs' ability to follow real multi-constrained user requests with REALINSTRUCT. We showed that even strong proprietary models like GPT-4 fail to meet at least one constraint in over 21% of instructions, demonstrating REALINSTRUCT's challenging nature and the need for improvement across both proprietary and open-source models. To address this, we proposed the DECRIM self-correction pipeline, which decomposes instructions into granular requirements, critiques responses, and refines outputs. Extensive experiments showed that DECRIM significantly improves open-source LLM performance, with stronger feedback allowing them to surpass GPT-4. Overall, our work highlights the underexplored problem of following real-world user requests, as well as advances System 2 techniques with DECRIM. Future work could refine the DECRIM's components and integrate the pipeline to other System 2 approaches, such as self-consistency and generate-and-rank, to enhance its effectiveness in tasks where spending more time on generation-refinement iterations would improve performance."}, {"title": "7 Limitations", "content": "Model-based vs. Rule-based Evaluation. Using model-based evaluation over rule-based introduces two key challenges. First, it reflects the precision/accuracy trade-off, where rule-based methods offer higher precision but are less accurate due to synthetic scenarios, while model-based ones, though less precise, better align with real-world tasks (see discussion on Section A.1.3). Second, evaluating the REALINSTRUCT dataset relies on the proprietary GPT-4-Turbo API, making it costly. To address these, techniques like multi-prompting (Mizrahi et al., 2024) and panel of juries (Verga et al., 2024) can improve precision in model-based evaluation, including with open-source models, while future advancements in open-source LLMs may provide cost-effective evaluation alternatives.\nData Contamination and Intra-Model Scoring Bias. Developing benchmarks with publicly available data that does not overlap with LLM training data is challenging, as pre-training and instruction-tuning datasets are often undisclosed. For example, reliable information on Mistral's training data is unavailable. However, while Vicuna v1.3 reported instruction tuning on a dataset overlapping with REALINSTRUCT, no significant intra-model bias from data contamination was observed, as seen in its poor performance in Table 5. This is likely due to its instruction tuning procedure not ensuring constraint satisfaction in target responses. However, GPT-4's relatively high constraint-level accuracy could indicate scoring bias, as previous studies suggest GPT-4 tends to favor its own outputs (Zheng et al., 2023; Panickssery et al., 2024; Verga et al., 2024). Further investigation into data contamination and intra-model scoring bias is left as future work.\nComputation Overhead. The DECRIM pipeline introduces additional computational time compared to single-pass generation. Table 4 provides the running time for each configuration explored. To mitigate this, we have designed the pipeline to trigger the refinement step only when the Critic model detects unsatisfied constraints, which occurred in about 25% of instructions, minimizing unnecessary computation when the model performs well initially. This is an improvement upon other System 2 approaches (see Section A.2.2), such as generate-and-rank, which typically generate multiple outputs for further ranking. Additionally, we observed that most revisions occur in the first iteration, resulting in a sublinear time increase with more iterations. Also, higher-quality feedback further reduces the need for revisions, improving DECRIM's efficiency.\nOptimization Considerations. Due to high computational costs, we did not optimize hyperparameters for training the weakly supervised LLM-as-a-Judge or exhaustively tune the prompts for the adaptation strategies. Additionally, we did not explore using a dedicated LLM as a Decomposer in the DECRIM pipeline, as this is primarily an implementation-focused task, being not critical for demonstrating our core claims. These aspects are left for future work."}, {"title": "8 Ethical Considerations", "content": "Crowdsourcing. For the EvalJudge Human Annotation task, we recruited native English speakers through Amazon Mechanical Turk. Compensation was based on the number of constraints per instruction, with an estimated average payment of 16.90 USD per hour, which exceeds the highest U.S. minimum wage in 2024 (16.30 USD per hour in Washington State), aligning with ethical guidelines discussed by Huang et al. (2023b).\nData from real users. Constructing a dataset from real user requests presents some ethical challenges:\n\u2022 Personally Identifiable Information (PII): Some user interactions with AI assistants may contain PII. During data validation, we actively sought to remove instances containing PII from the dataset. See Appendix D.1.3 for further details.\n\u2022 Harmful Content: The underlying data source is uncensored, and users may produce or request toxic or harmful content. Apart from flagrant cases, we did not actively remove such instances from the dataset.\nSocietal Impact. The DECRIM pipeline improves LLMs' ability to follow user-requested constraints, contributing to a broader societal impact of advancing LLM capabilities. When it comes particularly to user requests, it is important to note that some user constraints may conflict with system constraints set by developers, such as requests to generate harmful or toxic content. Although our study"}, {"title": "A Related Work", "content": "This section situates our work within broader research directions, highlighting intersections with current studies. Section A.1 focuses on benchmarking and evaluating LLMs' generative abilities, while Section A.2 discusses approaches for enhancing LLM responses."}, {"title": "A.1 Evaluating LLMs' Generative Abilities", "content": "Traditional language model benchmarks, such as HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), MMLU (Hendrycks et al., 2021), GSM-8K (Cobbe et al., 2021), and BIG-Bench (Srivastava et al., 2023), primarily assess LLMs on tasks like commonsense reasoning and standardized exams. These benchmarks evaluate models using multiple-choice questions (MCQs) to objectively measure internal reasoning capabilities. However, recent advancements in language models have demonstrated emergent capabilities in generating high-quality open-ended text generation (Wei et al., 2021; Chung et al., 2024; Ouyang et al., 2022; Taylor et al., 2022; Bubeck et al., 2023).\nThis shift presents new challenges, as the number of possible responses are virtually infinite, requiring more subjective evaluation rather than strict reference matching. While MCQ-based benchmarks fall short in assessing these generative abilities, human annotation, though reliable, is limited by cost and scalability. To address this, some research directions sacrifice the question quality to be able to use rule-based evaluation methods, while others explore model-based approaches."}, {"title": "A.1.1 LLM-as-a-judge", "content": "Early model-based efforts like BERTScore (Zhang et al., 2020) sought to improve on traditional n-gram metrics by recognizing high-quality responses that differ from the reference. For more open-ended generation, where references are soft or nonexistent, recent work has introduced the concept of LLM-as-a-Judge (Zheng et al., 2023; Liu et al., 2023), using strong proprietary LLMs like GPT-4 (Achiam et al., 2023) to evaluate responses. These models have shown they can approximate the depth and consistency of manual human evaluation, but also provide better consistency and stability.\nRecent research has begun exploring open-source LLMs for LLM-as-a-Judge, aiming to reduce reliance on proprietary models. Although open-source models have shown limited capability with in-context learning, fine-tuning them for specific evaluations is a promising direction (Huang et al., 2024; Kim et al., 2024a). Contemporaneous work Prometheus-2, an open LLM-as-a-Judge, has shown strong correlation with human evaluation, even surpassing GPT-4 in some cases, though it still lags in out-of-domain cases (Kim et al., 2024b). In our work, we assess both proprietary and open-source models for evaluating user constraint satisfaction in LLM responses. Our results indicate that while proprietary models outperform, open models can improve significantly when weakly supervised with proprietary model evaluations and reasoning trails, making them viable as Critic models in a self-correction pipeline."}, {"title": "A.1.2 Fine-grained evaluation", "content": "Some studies have explored approaches inspired by the divide-and-conquer paradigm, breaking multi-faceted tasks into fine-grained components (Lee and Kim, 2023). This can be successful given a complex compositional characteristic of the Natural Language (Manino et al., 2022; Dankers et al., 2022; Zhong et al., 2024). For evaluation, this strategy not only provides detailed insights into model performance across different aspects but also makes evaluations more objective and less ambiguous, as models may excel in some areas while underperforming in others. This approach seems promising for LLM-as-a-Judge, given that LLMs prompting techniques such as Chain-of-Thought (Wei et al., 2022), Tree-of-Thought (Yao et al., 2023), and Recursive Thinking (Qi et al., 2023), have demonstrated LLM performance improvements by breaking complex tasks into simpler sequential steps. We discuss these methods on Section A.2.2.\nSpecific works on evaluation by Min et al. (2023); Li et al. (2023b); Jing et al. (2023); Hu et al. (2023); Song et al. (2024); Huang et al. (2024); Zhang et al. (2024b) have shown the benefits of decomposing tasks into atomic facts for tasks such as fact-checking against cross-modality references. Kim et al. (2024a); Magister et al. (2023); Ke et al. (2024) have demonstrated that fine-grained evaluation from diverse sources enhances fine-tuned open evaluators by making the task more objective. Additionally, weak fine-grained evaluation during generation time has been shown to improve LLM self-correction performance (Shridhar et al., 2023, 2024; Wang et al., 2024).\nIn our work, we implement a similar approach by decomposing the task of evaluating multi-constrained instructions into individual constraint evaluations. This \"instruction decomposition\" simplifies and makes more objective the instruction evaluation task for LLMs and provides more informative insights through constraint-level accuracy metrics. We also argue that existing overall instruction satisfaction metrics fail to detect unmet constraints due to the ambiguity caused by the lack of granularity, as also highlighted by Sun et al. (2023). Our results demonstrate the effectiveness of fine-grained evaluation for this task and show that incorporating it into a self-correction pipeline enhances performance, even with weak Critic and Decomposer models."}, {"title": "A.1.3 Benchmarking Instruction-Following Abilities", "content": "The ability of LLMs to follow user instructions in open-ended text generation has only recently gained attention. New benchmarks like AlpacaEval (Li et al., 2023a) and the test splits of Natural-Instructions (Mishra et al., 2022) and Self-Instruct (Wang et al., 2023b) address the evaluation in this aspect by using LLM-as-a-Judge to compare with reference responses or provide overall instruction satisfaction scores. Recent studies have shown that models often follow instructions only partially, frequently failing to adhere to specific constraints provided by users (Sun et al., 2023; Zhou et al., 2023a; Yao et al., 2024a; Jiang et al., 2024; Qin et al., 2024; Wen et al., 2024; He et al., 2024; Zhang et al., 2024a).\nTo evaluate this, the few existing benchmarks focus on a set of specific constraint categories and/or use synthetic constraints that can be easily verified through rule-based methods (Zhou et al., 2023a; Yao et al., 2024a). The trade-off between rule-based and model-based evaluation falls into the famous precision/accuracy dilemma about static instrument characteristics (sometimes referred as bias/variance dilemma) in the Statistics of measurements (Morris, 2001; Taylor, 1997; British Standards, 2022). Rule-based evaluation offers high-to-perfect precision (low variance), but it is usually required to be done on unrealistic scenarios, being less accurate (high bias). The use of model-based evaluation loses some precision compared to rule-based due to inherent variability introduced by LLMs (lower precision, higher variance), but aligns more with the task objective of evaluating more realistic scenarios (higher accuracy, lower bias).\nTo the best of our knowledge, our REALINSTRUCT benchmark is the first to evaluate LLMs using real-user instructions, offering a more realistic and comprehensive assessment. This approach closely"}, {"title": "A.2 LLM Self-Correction for open-ended text generation", "content": "Self-correction has emerged as an effective approach for enhancing LLM responses during generation by refining them during generation time (Pan et al., 2024; Kamoi et al., 2024). However, Kamoi et al. (2024); Huang et al. (2023a) demonstrated that the ability of LLM to self-correct alone is limited to tasks where responses can be decomposed and rely on verifiable components. For harder tasks, LLM self-correction may require additional modeling, new data, or even external tools.\nIn this sense, Self-correction approaches can be categorized based on the feedback source. Intrinsic Self-Correction uses carefully crafted prompts or in-context examples to enable the model to identify issues in its output. Self-Correction with External Feedback leverages external tools or more advanced LLMs to provide feedback, while Self-Correction with Fine-Tuning uses external feedback (from humans, stronger LLMs, external tools) to fine-tune the LLM for better feedback and/or response refinement. Kamoi et al. (2024) emphasizes that each self-correction category should be validated using comparable cases specific to its context.\nIn the case of multi-constrained instructions, the constraints are neither independent nor ordered, making it difficult to guarantee that all responses are decomposable. For example, constraints such as length and style do not have a specific part of the response to be followed, they should be followed in the whole text. Moreover, some constraints are subjective and harder to evaluate, and the instruction decomposition process may introduce noise, further complicating self-correction with the model itself. In our work, we explore both Intrinsic Self-Correction and Self-Correction with Critic Fine-Tuning. As ablation exploration, we also play with External Feedback (referred as Oracle Critic), as an estimation of upper bound performance but recognizing its limited generalization."}, {"title": "A.2.1 Constrained Generation", "content": "Recent work has explored constrained generation via self-correction, we show some representative work on Table 6. Some approaches are validated only on small, specific constraint sets, limiting their general applicability (Schick et al., 2023; Saunders et al., 2022; Lee et al., 20"}]}