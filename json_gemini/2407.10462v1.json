{"title": "BandControlNet: Parallel Transformers-based Steerable Popular Music Generation with Fine-Grained Spatiotemporal Features", "authors": ["Jing Luo", "Xinyu Yang", "Dorien Herremans"], "abstract": "Controllable music generation promotes the interaction between humans and composition systems by projecting the users' intent on their desired music. The challenge of introducing controllability is an increasingly important issue in the symbolic music generation field. When building controllable generative popular multi-instrument music systems, two main challenges typically present themselves, namely weak controllability and poor music quality. To address these issues, we first propose spatiotemporal features as powerful and fine-grained controls to enhance the controllability of the generative model. In addition, an efficient music representation called REMI_Track is designed to convert multitrack music into multiple parallel music sequences and shorten the sequence length of each track with Byte Pair Encoding (BPE) techniques. Subsequently, we release BandControlNet, a conditional model based on parallel Transformers, to tackle the multiple music sequences and generate high-quality music samples that are conditioned to the given spatiotemporal control features. More concretely, the two specially designed modules of BandControlNet, namely structure-enhanced self-attention (SE-SA) and Cross-Track Transformer (CTT), are utilized to strengthen the resulting musical structure and inter-track harmony modeling respectively. Experimental results tested on two popular music datasets of different lengths demonstrate that the proposed BandControlNet outperforms other conditional music generation models on most objective metrics in terms of fidelity and inference speed and shows great robustness in generating long music samples. The subjective evaluations show BandControlNet trained on short datasets can generate music with comparable quality to state-of-the-art models, while outperforming them significantly using longer datasets.", "sections": [{"title": "I. INTRODUCTION", "content": "MUSIC is an essential form of art that profoundly affects human existence and has a power that makes us feel, helps us heal, and connects with others well [1], [2]. However, music creation requires strong musical skills [3], making it difficult for non-musically trained people to compose new music. Recent progress in generative artificial intelligence opens up many possibilities for faster, easier, and more personalized music composition [4]\u2013[6]. As the number of musical creative applications and algorithms increases, it is crucial to consider how to facilitate interaction between users and systems so that we can leverage models to generate musical samples that contain properties desired by the user. In such a scenario, these controllable generative music systems empower the users to become composers, without them having much music theory knowledge.\nVarious deep generative models have been proposed for the controllable generation of different types of symbolic music. Global controls [7]\u2013[16] and fine-grained controls [17]\u2013[26] are two main forms of control signals imbued in these conditional models. Global controls typically use high-level music descriptions or abstract musical labels, such as composers' styles [10], genre labels [11], and emotional labels [16]. Fine-grained controls are achieved as a set of features varying in instruments [17], [18] or time [19]\u2013[23], providing more powerful inductive bias than global controls. However, when trying to generate multitrack popular songs, the kind of music with massive musical elements interweaving across time and instruments, the current conditional models face two challenges corresponding to controllability and music quality respectively: how to support users with maximum controls, and how to create long music with good attributes in structure and inter-track harmony.\nFirstly, depending on the type of features used for conditioning, the models face different issues. For instance, global conditioning is always presented as some constant values and reflects music attributes only from a macro perspective, the conditional models easily forget them, and their effect is weakened as the length of the music sequence goes long. As for current fine-grained control signals, they are mainly extracted from two aspects, i.e. differing in track from the spatial dimension and varying across time from the temporal dimension. Track-differed features usually occur at multitrack music generation task, they are calculated across the whole time scope but still play a global conditional role in each instrument, though they behave differently between tracks. In contrast, time-varying features extracted at various time scales, e.g., at bar or phrase levels, are more appropriate for diverse music genres. However, these features become ambiguous and less instructive for multi-track music to some degree, since the minimum time scale of this type of music is always filled with diverse music elements from various tracks, resulting in weak control for a concrete track.\nSecondly, the sequence length of popular music with multiple instruments typically is very long, which can lead to an exponential increase in the computational complexity of the Transformer-based model [27] and a corresponding decrease in the inference speed. The scalability for conditional generation models also degrades as the sequence grows since the control signal has to cover larger and more complex music sequences. On the other hand, popular music typically has a distinct structure and intricate harmony, involving respectively temporal relationships among different bars or phrases and polyphonic relationships organized by notes from different tracks. Such relationships maintain the overarching coherence and consonance of the songs but are not explicitly considered in current controllable models.\nTo improve the controllability and music quality for popular music with multiple instruments, we propose BandControl-Net (as shown in Figure 1), a novel conditional generation model based on spatiotemporal control features and a parallel Transformers framework. Specifically, we expand the expert and learned features first proposed by FIGARO [21] to spatiotemporal ones, thus providing fine-grained control and strong inductive bias for every track and every bar generated by BandControlNet. In the encoding phase, a set of parallel Transformer encoders is introduced to handle the spatiotemporal features.\nMoreover, to decrease the length of the music sequence, we design a music representation based on multiple sequences, REMI_Track, which represents the notes of each track as a separate sequence and leverages the Byte Pair Encoding (BPE) technique to further shorten it. Subsequently, we propose three decoding modules, namely Bottom Decoders, Cross-Track Transformer (CTT), and Top Decoders, to fuse the spatiotemporal conditions and improve the generated music's quality. We design structure-enhanced self-attention (SE-SA) modules for Bottom Decoders and Top Decoders to facilitate the structure modeling of music sequences. In addition, we propose the CTT module to explicitly strengthen the inter-track dependency modeling.\nWe conducted thorough experiments on two popular music datasets of different lengths for our methods. According to the experimental evaluation of tokenization, we show that the proposed REMI_Track music representation combined with the BPE methods could largely shorten the sequence length of a single track. Furthermore, the objective evaluations show that our proposed BandControlNet surpasses other conditional models across most metrics in terms of fidelity and inference speed. In particular, BandControlNet demonstrates the least decline in most metrics when applied to longer datasets, indicating its robustness in modeling long sequences. Subjective evaluations show BandControlNet trained on a short dataset performs comparable to the state-of-the-art [21], but our model trained on a longer dataset significantly outperforms all benchmarks in all listening criteria.\nThe remainder of this paper is as follows: Section II provides a detailed literature review on controllable music"}, {"title": "II. RELATED WORK", "content": "In this section, we provide a brief overview of symbolic music generation in terms of controllable music generation and multitrack music generation."}, {"title": "A. Controllable Music Generation", "content": "Controllability, regardless of whether music is generated in a symbolic or waveform format, constitutes an essential challenge that, when successfully implemented, allows human composers to manipulate the musical creation process. Some emerging studies focus on generative music audio from text descriptions [28], [29] or various acoustical conditions [26], [30], which are different bodies of work than ours.\nAmong recent studies for symbolic music, various control signals are proposed for generative models. These control signals could be either from different modes such as images [31], [32], video [24], [25], [33], audio [34], and nature languages [35], [36] or more commonly, from music features and musical elements. According to the level of controllability, we mainly discuss two categories of control signals here, namely global controls and fine-grained controls.\nA prevalent form to achieve control is by using global conditioning. Global conditioning sets constant control parameters that are applied to the entire song. In some studies, global conditioning is encoded into parts of music representation and serves as prompts to influence the creation process [7], [11]. Other research considers global control as a supervision signal and then injects these conditions into the training procedure of generative models such as generative adversarial network (GAN) [8], [9], [12], variational autoencoder (VAE) [13]\u2013[15] or diffusion models [10]. However, global conditioning has some limitations: for instance, control signals tend to be forgotten or weakened by the model, especially when facing complex music or requirements for generating long sequences [8], [13], [15].\nIn addition to global controls, several studies have introduced fine-grained control signals that vary across instruments or time. Predefined music descriptors [17], [18], [20], [21], [23] such as chord progressions, note density, polyphony rate, rhythmic intensity, are commonly used conditions. This is due to the fact that these attributes are easy to extract and offer great musical interpretability. Musical features can also be learned from deep neural networks [19], [21]. Such learned features can serve as an alternative complement for fine-grained control. The abovementioned fine-grained control signals are usually extracted at the track or bar level, but both such features only cover a certain part of the attributes of multitrack music. Track-level features summarize the single track but only provide track-wise differences, while bar-level features lose the difference among various tracks.\nIn this paper, to support users with maximum controls when they generate multitrack music, we introduce spatiotemporal features as the control signals, which imbue the generated music with fine-grained controls at every track and every bar."}, {"title": "B. Multitrack Music Generation", "content": "Creating popular music with multiple tracks/instruments is a multitrack music generation task in the symbolic music generation domain. Current approaches for multitrack music generation can be broadly classified into image-based and sequence-based models based on different music representations [5], [6].\nSince multitrack music can be visualized as multi-channel images, many studies approach it as an image generation problem. They first transform each track of music into a pianoroll or its advanced version, and then introduce a generative model such as recurrent neural networks (RNN) [37], GAN [38], [39], VAE [40], [41], Flow-based [42] and diffusion models [43]\u2013[45] to reconstruct each pixel (i.e., note) of the pianoroll.\nSymbolic music like MIDI consists of a set of discrete music events, which can be considered as a kind of \u201clanguage\u201d. Therefore, multitrack music can be represented as single or multiple sequences of discrete tokens. For instance, an RNN-based model, BandNet [46], encodes the Beatles' songs with three tracks into one sequence track-by-track. Later, many studies [17], [18], [21] introduce instrument-related events into the existing token-based representations [47], [48] and organize the final event sequences with different positioning strategies. Despite the single sequence representation methods mentioned above, some studies [37], [49] transform multitrack music into multiple sequences corresponding to various instruments where each track is encoded individually instead of mixing them into one sequence. In these studies, parallel RNNs or Transformer models are introduced to generate these multitrack sequences.\nNo matter whether an image-based or sequence-based representation is used, long sequence modeling is a ubiquitous challenge for multitrack music generation. Long sequence generation using image-based representation is nearly equal to high-resolution image generation. Specifically, the pianoroll of multitrack music will extend into a large and sparse image, especially when representing songs of a few minutes duration. On the other hand, Some studies compress sequences by tuple encapsulation [50]\u2013[52], token combinations [53], [54] and data compression techniques [55], [56]. Nonetheless, generating popular songs still results in a lengthy encoded multitrack music sequence.\nStructure and inter-track harmony modeling are other challenges for multitrack music generation. Only two unconditional models, BandNet [46] and Museformer [57], pay attention to multitrack music generation with reasonable structure. BandNet uses a predefined section template as a structure constraint, while Museformer proposes the fine-grained attention mechanism based on a prior assumption, which popular"}, {"title": "III. METHODS", "content": "In this section, we first introduce two sets of fine-grained spatiotemporal features automatically extracted from the music sequence. Then we present the tailored sequence-based music representation proposed to efficiently generate multitrack music. Finally, details of the architecture of BandControlNet (as shown in Figure 1) are presented."}, {"title": "A. Spatiotemporal Features", "content": "We introduce the expert features $F^{exp}$ and learned features $F^{vq}$, the two sets of fine-grained spatiotemporal features, as control signals. To improve the interpretability and high fidelity for complex multitrack music, both sets of features are calculated bar-by-bar on the temporal axis and track-by-track on the spatial axis. Hence, $F^{exp}$ and $F^{vq}$ are stored in a 2-dimensional matrix $F \\in R^{I \\times B}$, where $I$ represents the instrument (i.e., track) numbers, $B$ represents the bar numbers, and each matrix entry is a tuple of several musical features as discussed below.\n1) Expert Features: Expert features are statistical descriptions extracted using musical knowledge. These features are easy to understand by human composers and commonly adopted in many conditional music generation tasks [18], [20], [21], [23]. We define seven types of musical features in set $M$: Chord Types (CT), Drum Types (DT), Drum Density (DD), Note Density (ND), Mean Pitch (MP), Mean Duration (MD), and Mean Velocity (MV). These features are described in detail in Appendix A. In particular, we design Drum Types and Drum Density for drum tracks alone.\nAll expert features excluding CT are extracted bar-wise on every track, while CT is a 4-element tuple {CT, CT, CT, CT} where each element is inferred per beat during the given bar (only consider the time signature of 4). Each entry of the expert features $F^{exp}$ can be written as:\n$f_i^{i,b} = \\begin{cases} \\{ \\{DT_{i,b}, DD_{i,b}\\} & \\text{for } i \\text{ is drum;} \\\\ \\{ \\{ND_{i,b}, MP_{i,b}, MD_{i,b}, MV_{i,b}\\} \\oplus CT & \\text{otherwise.} \\end{cases}$ (1)\n$CT = \\{CT_1, CT_2, CT_3, CT_4\\}$\nwhere $i$ denotes the $i^{th}$ track, $b$ represents the $b^{th}$ bar and $\\oplus$ is a concatenation operator for two tuples. The expert features of the drum tracks are organized as a 2-element tuple containing DT and DD, while that of other instruments consists of ND, MP, MD, MV, and the shared 4-element CT.\n2) Learned Features: In recent studies on image generation [58] and music generation [21], the Vector Quantized Variational AutoEncoder (VQ-VAE) [59] is used to learn powerful and robust representations for generating high-quality samples. Following FIGARO [21], we consider that learned features can be extracted from the latent space of a pre-trained VQ-VAE model.\nThe VQ-VAE model mainly consists of an encoder block, a decoder block, and a vector quantization (VQ) block. The VAE encoder first maps the bar-level token sequence of an individual track to the latent space. For finer-scale quantization, the encoder output $z_e \\in R^{d_1}$ is decomposed into 8 groups $z_e = \\{z_{e_1}, z_{e_2},...,z_{e_8}\\}$, where $z_{e_n} \\in R^{d_1/8}$, $d_1$ denotes the latent dimension. Then, 8 groups of VQ codes $z_{vq} = \\{z_{vq,n}\\}_{n=1}^{8} \\in R^K$ are produced with a shared codebook of size $K$ after the VQ block. These codes are finally fed to Transformer decoders to reconstruct the original musical sequence. Such the 8-code grouping is considered as the learned features as follows.\n$f^{i,b}_{vq} = \\{z^{i,b}_{vq,n}\\}_{n=1}^8$ (2)\nwhere $f^{i,b}_{vq}$ is the entry of $F^{vq}$ at $i^{th}$ track and $b^{th}$ bar and is the set of all eight VQ codes."}, {"title": "B. REMI_Track Representation", "content": "To improve the computational complexity and inference efficiency when generating long sequences, we develop a new sequence-based music representation called REMI_Track, specifically designed for multitrack music. In the REMI_Track representation, all notes from each track are represented in an independent sequence of discrete tokens. Additionally, we ensure that each sequence is padded to the same length to account for the common scenario where varying numbers of notes play in the different instrument tracks.\nWe design different event tokens to represent the musical sequence with REMI_Track, including metric-related tokens as well as note-related tokens. Metric-related tokens determine the instrument information and the metrical structure of music. Note-related tokens specify a note and will be aggregated by BPE techniques [56]. The detailed description of each type of event token in REMI_Track is as follows.\nInstrument. Instrument tokens are added at the beginning of each event sequence, specifying the instrument used in a track. Here, we consider six types of common-used instruments in popular music, they are Drum, Piano, Guitar, Bass, Strings, and Square Synthesizer (playing Melody).\nBar. We introduce Bar tokens to mark the beginning of a musical measure. Unlike previous representations, our bar tokens only contain [Bar_Empty] and [Bar_Normal], indicating whether the current bar is empty or not.\nPosition. Position tokens indicate the note's onset position within a bar, providing an explicit metrical structure to ensure that notes play in the correct order. The temporal resolution is set to 48-time steps per quarter note in this paper.\nNote. Typically, a note is represented as three or two consecutive tokens in a common-adopted token-based representation [17], [21], [50], [60]. Different from some previous studies, we improve note-related tokens in two ways.\n\u2022 Firstly, some previous studies delete drum tracks [52] or represent drum notes with shared pitch vocabulary of other instruments [21]. In this paper, we identify drum notes as an independent token (i.e. [Pitch_D] token), indicating the drum type. To reduce the length of the drum sequence after tokenization, we drop the duration tokens and the velocity tokens for the drum track since both are fixed to a default value of 16th note and 64 respectively. Notes from other instruments are represented as three consecutive tokens (representing the pitch, duration, and velocity of a note respectively).\n\u2022 Secondly, BPE (Byte Pair Encoding), a data compression technique largely employed in the Natural language processing (NLP) field, is introduced to further decrease each track's sequence length in this paper. In existing studies in the symbolic music domain, BPE has been used before. For instance, MusicBPE [55] has a limited scope as it only covers the notes of chord. Fradet et al. [56] on the other hand, apply BPE over all tokens and aggregate the position tokens with other tokens, resulting in a high number of predicted position mistakes during inference. In contrast, in this work, we consider a middle scope, where BPE is learned on a corpus consisting of note-related tokens that share the same note onset position, the metric-related tokens are excluded from the learning corpus. In this way, the final token sequence after BPE still has an explicit metrical order.\nCompared to original REMI+ representation [21], we discard Tempo, Time Signature, and Chord tokens, since the tempo signature and time signature of the songs in our dataset are fixed and chord information is added to the spatiotemporal features sequence. We consider 128 pitch values of non-drum notes and 31 drum types for note-related tokens. Note velocity is quantized to 32 intervals and note duration is quantized to a hybrid mesh which is the same as in the REMI+ representation. The raw vocabulary size of the proposed REMI_Track is 284, including all metric-related tokens, note-related tokens, and 3 special tokens (i.e. [PAD] for padding, [BOS] for the beginning of the sequence, and [EOS] for the ending). After BPE learning, the total vocabulary size of REMI_Track is expanded to 10,000.\nExamples of a music sequence encoded in REMI+, REMI+ (BPE), and REMI_Track (BPE) are shown in Figure 2. In our proposed tokenization method, the five notes on four instruments are presented as four parallel sequences with a maximum length of 5 tokens, much shorter than 20 tokens and 13 tokens in REMI+ representation [21] with or without BPE techniques, respectively."}, {"title": "C. Model Architecture", "content": "We utilize a Transformer-based seq2seq model as the backbone of our proposed BandControlNet to achieve controllable music generation. As shown in Figure 1, BandControlNet contains two main blocks, namely the feature encoders block and the decoders block which includes three decoding modules. In the encoding phase, the fine-grained spatiotemporal features are encoded by a set of parallel Transformer encoders where the individual encoders process the bar-varying conditions of the corresponding track independently. During the decoding phase, we proposed three decoding modules: Bottom Decoders, Cross-Track Transformer (CTT), and Top Decoders. This allows us to fully combine the encoded conditions and facilitate the quality of generated multitrack music.\n1) Feature Encoders: Both expert features $F^{exp}$ and learned features $F^{vq}$ are stored in the 2-dimensional matrix $F \\in R^{I \\times B}$, we first convert $F^{exp}$ and $F^{vq}$ into an embedding vector. Given an expert feature $f_i^{i,b}$ for which is a tuple extracted from the $i^{th}$ track and $b^{th}$ bar, as defined in Equation 1. We then embed each component feature $firm$ and concatenate them to obtain their embedding vector $C^{exp}_{i,b}$. We can obtain the embedding vector $C^{vq}_{i,b}$ of the learned features in a similar way. Finally, we combine the two embedding vectors to get $C_{i,b} \\in R^d$, where $d$ is the embedding dimension. The above computational procedure can be formulated as follows.\n$C^{exp}_{i,b} = Concat([FE^{exp}(f^{exp,m}_{i,b}), m \\in M])$\n$C^{vq}_{i,b} = Concat([FE^{vq}(f^{i,b}_{vq,n}), n = 1,..., 8])$ (3)\n$C_{i,b} = Concat([C^{exp}_{i,b}, C^{vq}_{i,b}])$\nwhere $Concat$ denotes the vector concatenation operation, $FE^{exp}$ represents the embedding layer for the expert feature of the type $m$, and $FE^{vq}$ is the shared embedding layer for the learned feature of every group $n$.\nAfter embedding, we get $C = \\{ \\{C_{i,b}\\}_{b=1}^{B}\\}_{i=1}^{I} \\in R^{I \\times B \\times d}$ as the input fed to the parallel encoders. Each encoder $Enc_i$ aims to handle the individual bar-varying vectors $\\{C_{i,b}\\}_{b=1}^{B}$, and consists of a vanilla Transformer encoder. Here, we add the bar index $b$ as the position information into the vanilla sinusoidal position encoding PE. The final encoder output $E_{i,b}$ corresponding to the input $C_{i,t}$ can be obtained by:\n$E_{i,b} = Enc_i(C_{i,t} + PE(b))$ (4)\n2) Bottom Decoders: To accommodate multiple music sequences $X = \\{ \\{x_{i,t}\\}_{t=1}^{T}\\}_{i=1}^{I} \\in R^{I \\times B}$ in the REMI_Track representation, where $T$ is the length of the music, we first obtain the embedded input sequences $X = \\{ \\{X_{i,t}\\}_{t=1}^{T}\\}_{i=1}^{I} \\in R^{I \\times B \\times d}$ as:\n$X_{i,t} = TE(x_{i,t}) + PE(t) + BE(b) + IE(i)$ (5)\nwhere TE, BE, and IE are the embedding layers for the token, bar index, and instrument types respectively. Then, we utilize $I$ parallel Transformer decoders to constitute the Bottom Decoders, they are utilized to model the relationships between tokens within each individual track autoregressively. The architecture of each bottom decoder $Decotm_i$ is similar to the vanilla Transformer decoder, except that replace the original self-attention module with our proposed structure-enhanced self-attention (SE-SA) module to enhance the structure modeling of every track.\nThe SE-SA module is designed based on the assumption that the spatiotemporal features offer sufficient summary information to characterize the notes on the bar-level scope of an individual track. Hence, we could learn internal interrelations among various bars for each track from the encoded spatiotemporal features $E_i = \\{E_{i,b}\\}_{b=1}^{B}$, and then use these learned interrelations to guide the structure modeling of the overall sequence of an individual instrument.\nConcretely, we first compute the similarity score $S_i$ between different bars for each track as follows:\n$S_i = LayerNorm(Softmax(\\frac{Q_i^{(e)} K_i^{(e)T}}{\\sqrt{d}}))$ (6)\nwhere $Q_i^{(e)} = W_i^{(Q)} E_i, K_i^{(e)} = W_i^{(K)} E_i$, and $W_i^{(Q)}, W_i^{(K)}$ are trainable parameters to project $E_i$ to queries and keys respectively. Subsequently, as Figure 3 shows, we expand the matrix $S_i \\in R^{B \\times B}$ of each track to a larger $\\tilde{S_i} \\in R^{T \\times T}$ by tiling the similarity score of a bar to all tokens belonging in this bar. Finally, we define the computation of our structure-enhanced self-attention for $x_i = \\{x_{i,t}\\}_{t=1}^{T}$ in $X$ as:\n$Attn(x_i, S_i) = Softmax(\\frac{Q_i K_i^T}{\\sqrt{d}}) V(x_i) \\odot \\tilde{S_i}$ (7)\nwhere $Q(x_i) = W_i^{(Q)} x_i, K(x_i) = W_i^{(K)} x_i, V(x_i) = W_i^{(V)} x_i$, and $W_i^{(Q)}, W_i^{(K)}, W_i^{(V)}$ are trainable parameters; The symbol $\\odot$ denotes the element-wise multiplication operation. For simplicity, we omit the computation of multi-head concatenation here.\nThe cross-attention module in each bottom decoder is kept the same as the vanilla Transformer decoder to fuse the encoded spatiotemporal features $E_i = \\{E_{i,b}\\}_{b=1}^{B}$ of each track. After the parallel Bottom Decoders with the SE-SA module, we obtain the output $Obtm_{i,t}$ of each token as:\n$Obtm_{i,t} = Decotm_i(X_{i,t}, E_i)$ (8)\n3) Cross-Track Transformer (CTT): Our parallel Bottom Decoders are independent of each other, and each of them is only responsible for a certain track, resulting in the loss of inter-track harmony between various instruments. To compensate for this situation, we propose a novel CTT module to construct the connections between different tracks by enhancing the attention of bar-related tokens.\nFirst, we group the bar-related tokens $Obtm_{i,t}$ from $Obtm$ within every track when a bar token is located at position k. Given all bar-related tokens set $\\{ \\{Obtm_{i,t}\\}_{t=1}^{B}\\}_{i=1}^{I} \\in R^{I \\times B \\times d}$, after all grouping operations, we subsequently transpose them to a new set $\\{ \\{Obtm_{i,k}\\}_{i=1}^{I}\\}_{k=1}^{B} \\in R^{B \\times I \\times d}$, where the instrument axis is considered as the sequential dimension. Later, we use a vanilla Transformer encoder $Enc_{ctt}$ to learn the relationships between bar tokens from various instruments as:\n$O^{ers}_{i,k} = Enc_{ctt} (Obtm_{i,k})$ (9)\nFinally, the output $O^{ers}_{i,k}$ of $Enc_{ctt}$ is restored to the same shape as the original $Obtm_{i,t}$. In addition, the final output $Obtm_{i,t}$ is obtained by updating bar-related tokens as follows.\n$Obtm_{i,t} = \\begin{cases} O^{ers}_{i,k} & t = k, \\\\ Obtm_{i,t} & otherwise. \\end{cases}$ (10)\nFor better comprehension, the visual procedure of CTT is shown in Figure 4.\n4) Top Decoders: The architecture of Top Decoders is the same as that of the Bottom Decoders, and equally includes the SE-SA module. Similar to the Equation 8, given each top decoder $Decop_i$ and encoded conditions $E_i$ of each track, we obtain the output $O^{top}_{i,t}$ as:\n$O^{top}_{i,t} = Dector_i(Obtm_{i,t}, E_i)$ (11)\n5) Training and Inference: Considering that the distribution of notes on each track varies, we adapt an independent linear layer MLP for the $i^{th}$ track to project the output $O^{top}_{i,t}$ to obtain its prediction value $\\hat{y}_{i,t}$ as follows.\n$\\hat{y}_{i,t} = Softmax(MLP(O^{top}_{i,t}))$ (12)\nIn training, let Y be the ground truth, which is the right-shifted version of input sequences $X = \\{ \\{x_{i,t}\\}_{t=1}^{T}\\}_{i=1}^{I}$. The training objective of our proposed BandControlNet can be formulated as the reconstruction loss of multiple sequences between Y and $\\hat{Y} = \\{ \\{\\hat{y}_{i,t}\\}_{t=1}^{T}\\}_{i=1}^{I}$ as:\n$L(Y, \\hat{Y}) = \\sum_{i=1}^{I}\\sum_{t=1}^{T} CELoss(y_{i,t}, \\hat{y}_{i,t})$ (13)\nwhere CELoss denotes cross-entropy loss function.\nDuring inference, we select a piece of music from the testing dataset as the reference and extract the spatiotemporal features F from it. Then, we feed these conditions F to the encoder block of BandControlNet and feed a sequence that begin with the indicator [BOS] for each track to the decoders block. Finally, each musical token is sampled from the predicted value $\\hat{y}_{i,t}$. In addition, we adopt a top-k sample strategy with the k set to 2% of the vocabulary size. The sampling process is stopped when the number of bars increases that of the reference music."}, {"title": "IV. EXPERIMENTS", "content": "A. Experiment Settings\n1) Dataset: We conduct our experiments on a popular music subset of the LakhMIDI dataset (LMD) [61], which is the largest publicly available symbolic music dataset that contains multiple instruments. To ensure data quality, we perform several data cleaning and processing steps, including genre selection, melody extraction [62], instrument compression, and data filtering. Please refer to Appendix B for details.\nAfter preprocessing, a total of 5,787 musical pieces remain in our final dataset. We reduce the maximum number of tracks for each song to four, only containing Melody, Drum, and the other two tracks as accompaniment instruments. Due to the large variation in song lengths and the limitation of computing resources, we opt not to train our model on the whole songs. Instead, we convert the final dataset into 32-bar and 64-bar datasets to evaluate the performance of models in generating short-term and long-term music, respectively.\nTo create the 32-bar dataset, each song is split using a flexible window that spans from 16 to 32 bars and a constant stride of 8 bars. Similarly, to create the 64-bar dataset we utilize a larger flexible window ranging from 32 to 64 bars and the same constant stride. The detailed statistics of the two datasets are shown in Table I. Since we covert the BPM of all songs to 120 and only select songs with a time signature of 4 during the preprocessing, the average duration of pieces in the 32-bar and 64-bar datasets is 49.4 seconds and 96.6 seconds respectively. We randomly hold out 10% of songs and their corresponding segments in both the 32- and 64-bar datasets for testing and use the remaining songs for model training.\n2) Benchmark Models: We compare our proposed BandControlNet model with four benchmark models. Two of these are unconditional models, and the other two are state-of-the-art methods for steerable music generation. The details of the selected benchmark models are as follows.\n\u2022 Music Transformer [64]: unconditional Transformer-based baseline with a relative attention mechanism.\n\u2022 BandControlNet-base: another unconditional baseline, It descards the feature encoders, SE-SA, and CTT modules of BandControlNet and only has multiple Transformer decoders in parallel, where each Transformer has the same architecture as the Music Transformer and is responsible for generating notes in the corresponding instrument.\n\u2022 MuseMorphose [20]: a conditional model that has a Transformer and VAE paired together. It introduces two bar-level musical features namely rhythmic intensity and polyphony for fine-grained music generation and includes latent space style transfer.\n\u2022 FIGARO [21]: another conditional model based on a vanilla seq2seq framework, where the source sequence provides the time-varying controls with a combination of bar-level expert features and learned features. Our BandControlNet extends both features to spatiotemporal ones.\nAmong these benchmark models, only the BandControlNet-base model employs parallel transformers like our proposed BandControlNet. The rest of the benchmark models use a single Transformer.\n3) Model Configurations: The encoder layers, decoder layers, attention heads, and hidden size of the FFN for all of the benchmark models (if they have those modules) are set to 4, 6, 8, and 1,024 respectively. Our BandControlNet has the same parameter settings as the benchmarks except that the number of layers of the Bottom Decoders, Top Decoders, and Cross-Track Transformer are 3, 3, and 2 respectively. The hidden size and embedding size of all models are set to 256. We utilize four parallel feature encoders, Bottom Decoders, and Top Decoders for BandControlNet since the number of tracks in our dataset is fixed to four. In addition, each component within the parallel modules shares the same parameters so as"}, {"title": "B. Tokenization Evaluation", "content": "To verify that our proposed REMI_Track music representation can alleviate the challenge of long sequence modeling, we compare REMI_Track with three previous methods proposed"}]}