{"title": "SELF-SUPERVISED PRE-TRAINING WITH DIFFUSION MODEL FOR\nFEW-SHOT LANDMARK DETECTION IN X-RAY IMAGES", "authors": ["Roberto Di Via", "Francesca Odone", "Vito Paolo Pastore"], "abstract": "In the last few years, deep neural networks have been extensively applied in the medical domain for\ndifferent tasks, ranging from image classification and segmentation to landmark detection. However,\nthe application of these technologies in the medical domain is often hindered by data scarcity, both in\nterms of available annotations and images. This study introduces a new self-supervised pre-training\nprotocol based on diffusion models for landmark detection in x-ray images. Our results show that the\nproposed self-supervised framework can provide accurate landmark detection with a minimal number\nof available annotated training images (up to 50), outperforming ImageNet supervised pre-training\nand state-of-the-art self-supervised pre-trainings for three popular x-ray benchmark datasets. To our\nknowledge, this is the first exploration of diffusion models for self-supervised learning in landmark\ndetection, which may offer a valuable pre-training approach in few-shot regimes, for mitigating data\nscarcity.", "sections": [{"title": "Introduction", "content": "Landmark detection is the task of identifying anatomical keypoints of interest in images [1]. Detecting key points can\ndirectly assist in angle measurements [2], skeletal measurements [3], and surgical planning [4].\nIn recent years, deep neural networks have become extensively utilized for the landmark detection task [5, 6], and\nmany fully supervised methods have been proposed [7, 8, 9, 10, 11, 12, 13, 14, 15]. However, the limited availability\nof annotated data and the reliance on medical experts for annotations present challenges in applying deep learning\nto this domain. Specifically, annotating landmarks is labor-intensive, costly, and demands expert precision [8, 7, 9],\nparticularly for critical tasks such as surgical treatment planning. Consequently, real-world datasets typically contain\nvery few annotated images, requiring the design of label-efficient training solutions [16].\nA transfer learning framework, typically fine-tuning ImageNet pre-trained models on medical downstream tasks, has\nbeen a common strategy to tackle this challenge [17, 18, 19]. Whether specific in-domain pre-training may benefit the\nmodel performance on the downstream task is still debated, and recent works provide controversial results depending\non the specific task and domain [20, 9]. Nevertheless, involving in-domain data may be an effective alternative to\nImageNet pre-training, especially in a self-supervised fashion to overcome the limited availability of annotations.\nRecent advances in self-supervised learning (SSL) methods such as MoCoV3 [21], SimCLRV2 [22], and DINO [23]\nhave shown significant potential. These approaches enable models to learn robust representations from unlabeled data,\nthereby reducing the dependency on large labeled training datasets. In this context, our main contribution is the proposal"}, {"title": "Related Works", "content": "The problem of few-shot learning is particularly relevant in the medical application domain, as obtaining both images\nand high-quality, unbiased annotations is costly and time-consuming. Consequently, several works leverage self-\nsupervised pre-training for label efficiency [24, 25, 26, 27].\nZhu et al.[27] propose UOD (Universal One-shot Detection), a domain-adaptive framework for one-shot anatomical\nlandmark detection across multiple medical imaging domains. UOD uses contrastive learning and a domain-adaptive\ntransformer, while reducing annotation burden. Rousseau et al. [24] introduce a pre-training method using diffusion\nmodels for dental radiography segmentation. Baranchuk et al. [25] use diffusion models to capture high-level semantic\ninformation for semantic segmentation. Brempong et al. [26] propose Decoder Denoising Pretraining (DDeP) to\nenhance label efficiency in semantic segmentation models by emphasizing the importance of pre-training both encoder\nand decoder components with limited labeled examples.\nIn the specific context of landmark detection, the existing works have mainly tackled the data scarcity issue by leveraging\ntransfer learning techniques [28, 8, 9]. Tiulpin et al. [28] use transfer learning from low-budget annotations for knee\nx-ray landmark localization in osteoarthritis stages. Di Via et al. [9] study whether in-domain data provide benefits over\nImageNet pre-training for landmark detection in x-ray images. Zhu et al. [8] propose GU2Net, a universal model for\nanatomical landmark detection in medical images, addressing limitations in specialized, dataset-dependent methods.\nUnlike the previously cited works, in this paper, we examine the impact of self-supervised pre-training with diffusion\nmodels on landmark detection for x-ray images, focusing on a realistic and typical scenario where the number of\navailable annotations and images is limited. To our knowledge, this is the first application of diffusion models for SSL\nin this specific task."}, {"title": "Approach", "content": "Fig. 1 illustrates our two-step methodology: initially, a DDPM is pre-trained on the training set, in a self-supervised\nfashion (with no annotations). Subsequently, this model is fine-tuned on the labeled training set for the downstream"}, {"title": "Experimental Setup", "content": "In this section, we describe the datasets used, the metrics employed, and the details of the implementation of our DDPM\narchitectures and training procedures for the different pre-training methods discussed above. Afterwards, we show the\nresults that have been achieved by evaluating the approach presented."}, {"title": "Datasets Overview", "content": "This section outlines the datasets employed in our study, including Chest x-rays, Cephalometric x-rays, and Hand\nx-rays. We describe the dataset composition, annotation protocols, and the division into training, validation, and testing\nsets to ensure clarity and reproducibility in our experiments.\nChest dataset: This dataset comprises 279 Chest x-ray images sourced from the China set, as documented\nin [32], and is accessible through the U.S. National Library of Medicine's public repository. The original image\ndimensions are approximately 3000x3000 pixels. Following the procedures provided in [8, 9], we split the dataset\nwith the first 195 images for training, the subsequent 34 for validation and the final 50 for testing. We evaluate our\nmodel's performance by pixel distance using 6 manually annotated landmarks per image as all the PNG dataset has no\ninformation on physical spacing.\nCephalometric dataset: The Cephalometric dataset [33] is an open-source resource containing 400 cephalo-\nmetric x-ray images of individuals aged between 7 and 76 years. Each image has a resolution of 0.1mmx0.1mm\nand dimensions of 2400x1935 pixels. In our research, the first 150 images are used for training purposes, while the\nremaining 250 images are set aside for testing, following the methodology in [8]. To ensure accurate anatomical\nlandmark annotations, two orthodontists identified 19 landmarks on each image, and the mean coordinates were\ncalculated to address inter-observer variability.\nHand dataset: This open dataset entails 909 Hand x-ray images, each at an average size of 1563x2169 pix-\nels. Consistent with the approaches taken by [8, 9], we allocate the first 550 images for training, the subsequent 59\nfor validation, and the final 300 images for testing. To enable a direct comparison with other approaches in terms of\nphysical distances we take the estimate made by Payer et al.[34] that the width of the wrist is 50mm. Then, physical\ndistance is computed as $\\frac{p}{50}q$, where p and q are the two ends of the wrist, determined by first and fifth landmarks in\nthe set of 37 manually labelled by Payer et al. [34] as well."}, {"title": "Implementation Details", "content": "This section provides the technical specifics of the implementation and experimental setup. The experiments were\nconducted on a single NVIDIA A30 GPU with 24 GB of RAM, using Python 3.10 and PyTorch 2.1.0."}, {"title": "Evaluation Metrics", "content": "In our research, we adopt the same evaluation metrics chosen by [8, 28, 9, 7], namely Mean Radial Error (MRE) and\nSuccess Detection Rate (SDR), which are widely used to assess the performance of landmark detection algorithms.\nThe MRE measures the accuracy of predicted landmarks by calculating the average Euclidean distance to\nthe corresponding ground truth landmarks. For n landmarks, the MRE is defined as:\n$MRE = \\frac{1}{n} \\sum_{i=1}^n \\sqrt{(x_i - \\hat{x_i})^2 + (y_i \u2014 \\hat{y_i})^2}$\nwhere $(x_i, y_i)$ represent the ground truth landmark coordinates and $(\\hat{x_i}, \\hat{y_i})$ represent the predicted coordinates.\nSDR, in contrast, evaluates robustness by indicating the percentage of predicted landmarks that fall within a\ncertain threshold distance from the actual landmarks. For n landmarks with a threshold t, the SDR is calculated as:\n$SDR = \\frac{1}{n} \\sum_{i=1}^n I(d_i \\leq t)$\nwhere $d_i$ is the Euclidean distance between the ground truth and the predicted landmark, and I is an indicator function\nthat returns 1 if $d_i < t$ and 0 otherwise."}, {"title": "Results and Analysis Discussion", "content": "In this section, we present the experiments conducted to evaluate our proposed methods and the corresponding results.\nWe detail the tuning of DDPM pre-training iterations, assess the performance for landmark detection with few labeled\ndata, and analyze the impact of different pre-training datasets on the downstream task."}, {"title": "Tuning the DDPM pre-training iterations", "content": "The number of training iterations during the pre-training of the DDPM is a fundamental hyperparameter of our proposed\napproach. To evaluate the robustness and select the number of training iterations, we perform an experiment running\nthree times the entire pipeline, considering 4k, 6k, 8k, and 10k training iterations for the DDPM pre-training step.\nFigure 3 visualizes the landmark detection results obtained in the validation sets for the Chest, Cephalometric, and\nHand datasets, in terms of MRE. Our findings suggest that the pre-training is quite robust, except for the 4k iterations\nwhere the model is likely still learning crucial data features (as shown by the high standard deviation). Excessive\ntraining iterations beyond 8k may result in overfitting, and a slightly degrading performance on the downstream task.\nConsidering these results, we decided to select the optimal number of training iterations for DDPM pre-training for\nsubsequent experiments, utilizing 6k iterations for the Chest dataset, 8k iterations for the Cephalometric dataset, and 8k\niterations for the Hand dataset."}, {"title": "Downstream task performance evaluation", "content": "In this section, we assess the effectiveness of our DDPM self-supervised pre-training method, benchmarking it with\nsupervised ImageNet pre-training, and self-supervised state-of-the-art methods including MoCoV3, SimCLRV2, and\nDINO, across different numbers of labeled training samples (up to 50) in the Chest, the Cephalometric, and the Hand\ndatasets. Tables 1, 2, and 3 report the average test results for three independent runs on the three datasets (a bar chart\nversion of the tables is provided in the Supplementary Material). With the exception of our approach, which implements\na DDPM as described in the original paper [29], all reported results utilize DenseNet161 as the encoder backbone. We\nopt for this backbone due to its performance in the landmark detection task across all the datasets tested, based on a\nhold-out validation comparison with VGG19 and ResNeXt50_32x4D. Our approach outperforms both ImageNet and the\nexplored alternative SSL approaches for all the tested datasets and quantities of training images in the downstream task.\nThese results are qualitatively supported by Figure 4, providing a visual representation (i.e., the prediction heatmaps) of\nthe methods' performance with 10labels, for each of the datasets included in our work.\nPerformance gains are particularly pronounced in low-data regimes, crucial for medical imaging applications where\nlabeled data is often scarce."}, {"title": "Impact of different pre-training datasets", "content": "In this section, we explore whether a different in-domain dataset can still boost the performance in the potential situation\nwhere both the number of available images and annotations are limited. To this purpose, we select the best DDPM model\npre-trained on the larger dataset (Hand), and we fine-tune it on the other two smaller datasets (Chest and Cephalometric),\nwith the same protocol as the previous experiments (three runs and different numbers of annotated training images"}, {"title": "Conclusions", "content": "Despite the widespread application of deep neural networks for medical image analysis, data scarcity in images and\nannotations represents a fundamental challenge for this domain. In this research, we propose and evaluate the impact\nof a pre-training approach based on self-supervision with DDPM for label-efficient landmark detection tasks in x-ray\nimages, benchmarking it against the typically adopted supervised ImageNet pre-training and self-supervised MoCoV3,\nSimCLRV2, and DINO. Specifically, given an x-ray image dataset, we leverage the training set in a self-supervised\nfashion for pre-training a DDPM based on a Unet, further fine-tuning it on the available annotated training images.\nOur results on three popular benchmark datasets show that the proposed method outperforms ImageNet supervised\npre-training and MoCoV3, SimCLRV2, DINO self-supervised pre-training in the context of few annotations available\nfor the landmark detection task. Moreover, such improvement seems to be preserved when pre-training on a different\nin-domain dataset and fine-tuning on the downstream one, simulating the interesting case where both annotations\nand images are limited for the downstream task. We believe that the proposed method showcases the potential of\nself-supervised diffusion-based pre-training for the landmark detection task, potentially opening new research directions\nto effectively address data scarcity challenges in this domain."}]}