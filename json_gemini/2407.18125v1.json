{"title": "SELF-SUPERVISED PRE-TRAINING WITH DIFFUSION MODEL FOR FEW-SHOT LANDMARK DETECTION IN X-RAY IMAGES", "authors": ["Roberto Di Via", "Francesca Odone", "Vito Paolo Pastore"], "abstract": "In the last few years, deep neural networks have been extensively applied in the medical domain for different tasks, ranging from image classification and segmentation to landmark detection. However, the application of these technologies in the medical domain is often hindered by data scarcity, both in terms of available annotations and images. This study introduces a new self-supervised pre-training protocol based on diffusion models for landmark detection in x-ray images. Our results show that the proposed self-supervised framework can provide accurate landmark detection with a minimal number of available annotated training images (up to 50), outperforming ImageNet supervised pre-training and state-of-the-art self-supervised pre-trainings for three popular x-ray benchmark datasets. To our knowledge, this is the first exploration of diffusion models for self-supervised learning in landmark detection, which may offer a valuable pre-training approach in few-shot regimes, for mitigating data scarcity.", "sections": [{"title": "Introduction", "content": "Landmark detection is the task of identifying anatomical keypoints of interest in images [1]. Detecting key points can directly assist in angle measurements [2], skeletal measurements [3], and surgical planning [4].\nIn recent years, deep neural networks have become extensively utilized for the landmark detection task [5, 6], and many fully supervised methods have been proposed [7, 8, 9, 10, 11, 12, 13, 14, 15]. However, the limited availability of annotated data and the reliance on medical experts for annotations present challenges in applying deep learning to this domain. Specifically, annotating landmarks is labor-intensive, costly, and demands expert precision [8, 7, 9], particularly for critical tasks such as surgical treatment planning. Consequently, real-world datasets typically contain very few annotated images, requiring the design of label-efficient training solutions [16].\nA transfer learning framework, typically fine-tuning ImageNet pre-trained models on medical downstream tasks, has been a common strategy to tackle this challenge [17, 18, 19]. Whether specific in-domain pre-training may benefit the model performance on the downstream task is still debated, and recent works provide controversial results depending on the specific task and domain [20, 9]. Nevertheless, involving in-domain data may be an effective alternative to ImageNet pre-training, especially in a self-supervised fashion to overcome the limited availability of annotations.\nRecent advances in self-supervised learning (SSL) methods such as MoCoV3 [21], SimCLRV2 [22], and DINO [23] have shown significant potential. These approaches enable models to learn robust representations from unlabeled data, thereby reducing the dependency on large labeled training datasets. In this context, our main contribution is the proposal"}, {"title": "Related Works", "content": "The problem of few-shot learning is particularly relevant in the medical application domain, as obtaining both images and high-quality, unbiased annotations is costly and time-consuming. Consequently, several works leverage self- supervised pre-training for label efficiency [24, 25, 26, 27].\nZhu et al.[27] propose UOD (Universal One-shot Detection), a domain-adaptive framework for one-shot anatomical landmark detection across multiple medical imaging domains. UOD uses contrastive learning and a domain-adaptive transformer, while reducing annotation burden. Rousseau et al. [24] introduce a pre-training method using diffusion models for dental radiography segmentation. Baranchuk et al. [25] use diffusion models to capture high-level semantic information for semantic segmentation. Brempong et al. [26] propose Decoder Denoising Pretraining (DDeP) to enhance label efficiency in semantic segmentation models by emphasizing the importance of pre-training both encoder and decoder components with limited labeled examples.\nIn the specific context of landmark detection, the existing works have mainly tackled the data scarcity issue by leveraging transfer learning techniques [28, 8, 9]. Tiulpin et al. [28] use transfer learning from low-budget annotations for knee x-ray landmark localization in osteoarthritis stages. Di Via et al. [9] study whether in-domain data provide benefits over ImageNet pre-training for landmark detection in x-ray images. Zhu et al. [8] propose GU2Net, a universal model for anatomical landmark detection in medical images, addressing limitations in specialized, dataset-dependent methods.\nUnlike the previously cited works, in this paper, we examine the impact of self-supervised pre-training with diffusion models on landmark detection for x-ray images, focusing on a realistic and typical scenario where the number of available annotations and images is limited. To our knowledge, this is the first application of diffusion models for SSL in this specific task."}, {"title": "Approach", "content": "Fig. 1 illustrates our two-step methodology: initially, a DDPM is pre-trained on the training set, in a self-supervised fashion (with no annotations). Subsequently, this model is fine-tuned on the labeled training set for the downstream"}, {"title": "Experimental Setup", "content": "In this section, we describe the datasets used, the metrics employed, and the details of the implementation of our DDPM architectures and training procedures for the different pre-training methods discussed above. Afterwards, we show the results that have been achieved by evaluating the approach presented."}, {"title": "Datasets Overview", "content": "This section outlines the datasets employed in our study, including Chest x-rays, Cephalometric x-rays, and Hand x-rays. We describe the dataset composition, annotation protocols, and the division into training, validation, and testing sets to ensure clarity and reproducibility in our experiments.\nChest dataset: This dataset comprises 279 Chest x-ray images sourced from the China set, as documented in [32], and is accessible through the U.S. National Library of Medicine's public repository. The original image dimensions are approximately 3000x3000 pixels. Following the procedures provided in [8, 9], we split the dataset with the first 195 images for training, the subsequent 34 for validation and the final 50 for testing. We evaluate our model's performance by pixel distance using 6 manually annotated landmarks per image as all the PNG dataset has no information on physical spacing.\nCephalometric dataset: The Cephalometric dataset [33] is an open-source resource containing 400 cephalo- metric x-ray images of individuals aged between 7 and 76 years. Each image has a resolution of 0.1mmx0.1mm and dimensions of 2400x1935 pixels. In our research, the first 150 images are used for training purposes, while the remaining 250 images are set aside for testing, following the methodology in [8]. To ensure accurate anatomical landmark annotations, two orthodontists identified 19 landmarks on each image, and the mean coordinates were calculated to address inter-observer variability.\nHand dataset: This open dataset entails 909 Hand x-ray images, each at an average size of 1563x2169 pix- els. Consistent with the approaches taken by [8, 9], we allocate the first 550 images for training, the subsequent 59 for validation, and the final 300 images for testing. To enable a direct comparison with other approaches in terms of physical distances we take the estimate made by Payer et al.[34] that the width of the wrist is 50mm. Then, physical distance is computed as $\\frac{p}{50}q$, where p and q are the two ends of the wrist, determined by first and fifth landmarks in the set of 37 manually labelled by Payer et al. [34] as well."}, {"title": "Implementation Details", "content": "This section provides the technical specifics of the implementation and experimental setup. The experiments were conducted on a single NVIDIA A30 GPU with 24 GB of RAM, using Python 3.10 and PyTorch 2.1.0."}, {"title": "Evaluation Metrics", "content": "In our research, we adopt the same evaluation metrics chosen by [8, 28, 9, 7], namely Mean Radial Error (MRE) and Success Detection Rate (SDR), which are widely used to assess the performance of landmark detection algorithms.\nThe MRE measures the accuracy of predicted landmarks by calculating the average Euclidean distance to the corresponding ground truth landmarks. For n landmarks, the MRE is defined as:\n$MRE = \\frac{1}{n}\\sum_{i=1}^{n} \\sqrt{(x_i - \\hat{x}_i)^2 + (y_i - \\hat{y}_i)^2}$\nwhere $(x_i, y_i)$ represent the ground truth landmark coordinates and $(\\hat{x}_i, \\hat{y}_i)$ represent the predicted coordinates.\nSDR, in contrast, evaluates robustness by indicating the percentage of predicted landmarks that fall within a certain threshold distance from the actual landmarks. For n landmarks with a threshold t, the SDR is calculated as:\n$SDR = \\frac{1}{n}\\sum_{i=1}^{n} I(d_i \\leq t)$\nwhere $d_i$ is the Euclidean distance between the ground truth and the predicted landmark, and I is an indicator function that returns 1 if $d_i < t$ and 0 otherwise."}, {"title": "Results and Analysis Discussion", "content": "In this section, we present the experiments conducted to evaluate our proposed methods and the corresponding results. We detail the tuning of DDPM pre-training iterations, assess the performance for landmark detection with few labeled data, and analyze the impact of different pre-training datasets on the downstream task."}, {"title": "Tuning the DDPM pre-training iterations", "content": "The number of training iterations during the pre-training of the DDPM is a fundamental hyperparameter of our proposed approach. To evaluate the robustness and select the number of training iterations, we perform an experiment running three times the entire pipeline, considering 4k, 6k, 8k, and 10k training iterations for the DDPM pre-training step.\nFigure 3 visualizes the landmark detection results obtained in the validation sets for the Chest, Cephalometric, and Hand datasets, in terms of MRE. Our findings suggest that the pre-training is quite robust, except for the 4k iterations where the model is likely still learning crucial data features (as shown by the high standard deviation). Excessive training iterations beyond 8k may result in overfitting, and a slightly degrading performance on the downstream task. Considering these results, we decided to select the optimal number of training iterations for DDPM pre-training for subsequent experiments, utilizing 6k iterations for the Chest dataset, 8k iterations for the Cephalometric dataset, and 8k iterations for the Hand dataset."}, {"title": "Downstream task performance evaluation", "content": "In this section, we assess the effectiveness of our DDPM self-supervised pre-training method, benchmarking it with supervised ImageNet pre-training, and self-supervised state-of-the-art methods including MoCoV3, SimCLRV2, and DINO, across different numbers of labeled training samples (up to 50) in the Chest, the Cephalometric, and the Hand datasets. Tables 1, 2, and 3 report the average test results for three independent runs on the three datasets (a bar chart version of the tables is provided in the Supplementary Material). With the exception of our approach, which implements a DDPM as described in the original paper [29], all reported results utilize DenseNet161 as the encoder backbone. We opt for this backbone due to its performance in the landmark detection task across all the datasets tested, based on a hold-out validation comparison with VGG19 and ResNeXt50_32x4D. Our approach outperforms both ImageNet and the explored alternative SSL approaches for all the tested datasets and quantities of training images in the downstream task.\nThese results are qualitatively supported by Figure 4, providing a visual representation (i.e., the prediction heatmaps) of the methods' performance with 10labels, for each of the datasets included in our work.\nPerformance gains are particularly pronounced in low-data regimes, crucial for medical imaging applications where labeled data is often scarce."}, {"title": "Impact of different pre-training datasets", "content": "In this section, we explore whether a different in-domain dataset can still boost the performance in the potential situation where both the number of available images and annotations are limited. To this purpose, we select the best DDPM model pre-trained on the larger dataset (Hand), and we fine-tune it on the other two smaller datasets (Chest and Cephalometric), with the same protocol as the previous experiments (three runs and different numbers of annotated training images"}, {"title": "Conclusions", "content": "Despite the widespread application of deep neural networks for medical image analysis, data scarcity in images and annotations represents a fundamental challenge for this domain. In this research, we propose and evaluate the impact of a pre-training approach based on self-supervision with DDPM for label-efficient landmark detection tasks in x-ray images, benchmarking it against the typically adopted supervised ImageNet pre-training and self-supervised MoCoV3, SimCLRV2, and DINO. Specifically, given an x-ray image dataset, we leverage the training set in a self-supervised fashion for pre-training a DDPM based on a Unet, further fine-tuning it on the available annotated training images.\nOur results on three popular benchmark datasets show that the proposed method outperforms ImageNet supervised pre-training and MoCoV3, SimCLRV2, DINO self-supervised pre-training in the context of few annotations available for the landmark detection task. Moreover, such improvement seems to be preserved when pre-training on a different in-domain dataset and fine-tuning on the downstream one, simulating the interesting case where both annotations and images are limited for the downstream task. We believe that the proposed method showcases the potential of self-supervised diffusion-based pre-training for the landmark detection task, potentially opening new research directions to effectively address data scarcity challenges in this domain."}]}