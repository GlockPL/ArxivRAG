{"title": "Culture-TRIP: Culturally-Aware Text-to-Image Generation with Iterative Prompt Refinement", "authors": ["Suchae Jeong", "Inseong Choi", "Youngsik Yun", "Jihie Kim"], "abstract": "Text-to-Image models, including Stable Diffusion, have significantly improved in generating images that are highly semantically aligned with the given prompts. However, existing models may fail to produce appropriate images for the cultural concepts or objects that are not well known or underrepresented in western cultures, such as 'hangari' (Korean utensil). In this paper, we propose a novel approach, Culturally-Aware Text-to-Image Generation with Iterative Prompt Refinement (Culture-TRIP), which refines the prompt in order to improve the alignment of the image with such culture nouns in text-to-image models. Our approach (1) retrieves cultural contexts and visual details related to the culture nouns in the prompt and (2) iteratively refines and evaluates the prompt based on a set of cultural criteria and large language models. The refinement process utilizes the information retrieved from Wikipedia and the Web. Our user survey, conducted with 66 participants from eight different countries demonstrates that our proposed approach enhances the alignment between the images and the prompts. In particular, C-TRIP demonstrates improved alignment between the generated images and underrepresented culture nouns.", "sections": [{"title": "1 Introduction", "content": "To date, many Text-to-Image Models (Ramesh et al., 2022; Rombach et al., 2022a; Ruiz et al., 2023) have demonstrated remarkable improvements. Despite the outstanding performance in the text-to-image models, the models fail to align the images with the culture nouns in the prompts, such as 'ao dai' (a Vietnamese clothing) or \u2018hangari' (a Korean utensil). Most of these issues stem from the large training datasets gathered by crawling the Internet without paying attention to the details of the cultural elements (Yun and Kim, 2024). Furthermore, Internet access varies significantly across countries (Birhane et al., 2023; Luccioni et al., 2024), leading to challenges in appropriately aligning culture nouns due to insufficient data, as shown in Figure 1 (b). Representation is important in AI applications. Appropriate representation can positively affect viewers, while inappropriate ones can negatively and can even be harmful (Casta\u00f1eda, 2018). Culture nouns are essential elements that often represent the identity and the uniqueness of the given culture. The misrepresentation of culture nouns by existing text-to-image models may cause dissatisfaction in the corresponding countries. Moreover, the models may reinforce harmful stereotypes about particular cultures. In this paper, we introduce a new approach that generates more culturally aligned images for the given culture nouns, called Culturally-Aware Text-to-Image Generation with Iterative Prompt Refinement (C-TRIP). C-TRIP focuses on refining the prompt to ensure that the culture nouns are appropriately represented in the generated images, as shown in Figure 1 (c). Our goal is to generate culturally-aware images by appropriately aligning culture nouns with images from underrepresented countries. Our research question is, How can we refine the prompt so that text-to-image models generate images that appropriately align with culture nouns? The Culture Capsules (Taylor and Sorensen, 1961) is an educational approach designed to help learners who have not directly experienced a culture gain the proper understanding. This approach explains cultural contexts and visual details, enabling learners to gain comprehensive understand unfamiliar cultures. Through this process, learners can develop a deeper profound awareness of different cultures. Inspired by the Culture Capsules, C-TRIP first retrieves cultural contexts and visual details related to the culture nouns in the prompt and then iteratively refines and evaluates the prompt against the criteria used in culture education. Large Language Models (LLMs) are used for this iterative refinement and evaluation process, guiding the text-to-image model in generating images for culture nouns. We conducted experiments across eight countries, refining a total of 10,000 prompts by using 50 prompt templates for each of the 25 culture nouns per country. To evaluate our results, we recruited participants who were a native of the corresponding country and had a high familiarity with the culture to rank images generated by Stable Diffusion 2 (Rombach et al., 2022a), with or without the proposed iterative prompt refinement. The 66 participants across eight countries evaluated the 990 generated images, and C-TRIP received ratings for cultural alignment that were 18.84% on average higher than the baseline's. In particular, our approach demonstrated more improvement for relatively the Unrecognized/Underrepresented Culture nouns (UC nouns for short) than for the Recognized/Common Culture nouns (RC nouns for short). Our contributions are as follows:\n1.  We introduce culturally-aware image generation with a prompt that improves the representation of 'culture nouns'-cultural concepts or objects often overlooked by existing text-to-image models.\n2.  We propose a novel approach, C-TRIP (Culturally-Aware Text-to-Image GeneRation with Iterative Prompt Refinement), which iteratively refines the prompt in order to improve the alignment of culture nouns in the images generated by text-to-image models.\n3.  Human evaluations by representatives across eight countries demonstrate that our refined prompts enhance the alignment between generated images and culture nouns. In particular, C-TRIP demonstrated more improvement for UC nouns than RC nouns."}, {"title": "2 Related Work", "content": "2.1 Cultural Text-to-Image Generation Previously, various methods have been proposed to address the cultural bias in text-to-image models (Cho et al., 2023; Friedrich et al., 2023; Luccioni et al., 2024). Liu et al. (2024) collected a cultural dataset called CCUB across nine cultural categories for five countries and proposed a training technique named SCOFT to address cultural bias. Similarly, Kannen et al. (2024) collected the CUBE across three cultural categories for eight countries. However, these approaches are highly resource-intensive, demanding significant time and cost for data collection. Other works (Basu et al., 2023; Bansal et al., 2022) attempted to mitigate cultural bias by modifying prompt. However, merely adding contextual information such as country names to prompt is proven insufficient in mitigating cultural bias, particularly for the concepts from underrepresented countries. Unlike the previous approaches, our approach refines prompt based on cultural information and visual details to improve the alignment of text-to-image models with culture nouns, which are significant for representing unique concepts and objects across cultures.\n2.2 Prompt Engineering in Text-to-Image Generation The prompt as input to the text-to-image generation guides the images created by the models. Many studies (Oppenlaender, 2023; Liu and Chilton, 2022; Brade et al., 2023) on prompt engineering aimed at optimizing user-desired images by text-to-image models."}, {"title": "3 Method", "content": "Inspired by the Culture Capsules (Taylor and Sorensen, 1961), an educational method that introduces unfamiliar cultures through cultural contexts and visual details, our method refines the prompt for text-to-image models by incorporating cultural contexts and visual details relevant to the culture noun. In order to include only information essential for cultural expressions in the image, external information is retrieved, and an iterative refinement and evaluation process is conducted based on the criteria derived from both cultural contexts and visual details. Our overall architecture is depicted in Figure 2. We first retrieve cultural information from Wikipedia and Web content (Section 3.1), then second iteratively refine and evaluate the prompt based on scores (Section 3.2) until the stop condition is satisfied.\n3.1 Cultural Information Retrieval In obtaining raw information related to culture nouns, we use two sources. Given a culture noun, we first retrieve the Wikipedia content in order to leverage cultural contexts and visual details. Second, for certain culture nouns that lack sufficient information on Wikipedia, we perform additional retrieval from the Web. By making use of both Wikipedia and the Web, we can collect sufficient raw data, even for relatively uncommon UC nouns.\n3.2 Iterative Prompt Refinement The iterative refining process consists of 3 key steps: Refine, Scoring, and Feedback. The Refine step refines the raw information using the prompt and feedback from the previous Feedback step. The Scoring step evaluates the refined prompt based on five cultural criteria. Finally, the Feedback step proposes revisions in the prompt based on the refined prompt and the evaluation score. All three steps were implemented using LLaMA-3-70B (Dubey et al., 2024), as iterative refinement processes benefit from larger LLM (Madaan et al., 2024) with the latest open-source model. Refine The retrieved raw information $I$ for the culture noun $K$ is used to refine the prompts in the Refine step. In this step, the refined prompt $RP$ is typically generated based on feedback $F$. For the first step, only the raw information $I$ and the prompt $P$ are used. In the equations, $| |$ denotes concatenation throughout the paper.\n$RP = \\begin{cases}\nRefine(K||I||P) & \\text{if } i = 0, \\\\\nRefine(K||I||RP_{i-1}||F_{i-1}) & \\text{if } i > 0.\n\\end{cases}$\nThrough the Refine step, only the information essential for cultural education is extracted from the raw data. The final refined prompt guides the text-to-image model in generating images for culture nouns. Scoring In the Scoring step, the refined prompts are evaluated based on five cultural criteria: Clarity, Background, Purpose, Visual Elements, and Comparable Objects. If the total score does exceed the threshold (thresh.) or a specified maximum iteration $i$, the process stops; otherwise, it proceeds to the Feedback step for further refinement.\n$score_i = Scoring(K||RP_i)$\nThe five scoring criteria are structured based on the Culture Capsules (Taylor and Sorensen, 1961), which organize the criteria into two primary aspects: cultural contexts and visual details. Specifically, Clarity, Background, and Purpose are categorized as cultural contexts, while Visual Elements and Comparable Objects fall under visual details. Feedback In the Feedback step, each score is reviewed based on the criteria, and feedback is provided along with suggestions to improve the scores.\n$F_i = Feedback(K||RP_i||score_i)$"}, {"title": "4 Experiments", "content": "4.1 Data Preparation Culture Nouns The culture nouns are phonetically transcribed into English based on the original pronunciation in their respective languages (e.g., hangari, pronounced/ha:nga:ri/). However, when an English equivalent exists, they are represented in the format of 'Adjective form for the country + English expression' (e.g., Korean pagoda) to signify culture nouns. Setup Based on Basu et al. (2023), which addresses cultural bias in text-to-image generation by modifying prompts, we selected eight countries representing diverse cultural backgrounds: India, Pakistan, China, Japan, South Korea, Vietnam, the United States, and Germany. Drawing on research by Liu et al. (2024) on culturally-aware text-to-image models, we focused on eight specific categories that typically represent culture in visual expressions: architecture, city landmarks, clothing, dance & music, visual arts, food & drink, religion & festivals, and utensils and tools. We generated 10,000 prompts using 50 prompt templates for each of the 25 culture nouns per country. These templates, generated by GPT-4o, incorporate culture nouns into typical scenarios, enabling consistent prompt generation for experimentation without relying on real-world prompts.\n4.2 Baselines for Ablation Study To effectively analyze the specific contributions of cultural contexts and visual details based on the Culture Capsules approach, the baseline and ablation configurations were established in order to systematically examine the roles of cultural contexts (Clarity, Background, Purpose) and visual details (Visual Elements, Comparable Objects). In this study, we evaluated three configurations of the C-TRIP, each with different criteria for prompt refinement: C-TRIP0, C-TRIP3, and C-TRIP5. For C-TRIP3 and C-TRIP5, the refinement process was performed up to a maximum of 5 iterations, ensuring that the prompts reached the desired level of cultural and visual alignment. These configurations were applied to 10,000 Base Prompts, resulting in 40,000 refined prompts. Subsequently, 80,000 images were generated, with two images created for each prompt using Stable Diffusion 2 (Rombach et al., 2022b). C-TRIP0 (No Iterative Prompt Refinement). C-TRIP0 utilizes prompts augmented with raw cultural information without applying the Iterative Prompt Refinement. This configuration is used to evaluate the baseline effect of unrefined cultural information, enabling an assessment of how much alignment can be achieved without iterative refinement. C-TRIP3 C-TRIP3 refines the prompts based solely on the cultural context criteria (Clarity, Background, and Purpose). This setup evaluates the contribution of cultural context alone without incorporating visual details. This enables assessing how effectively the refined cultural information enhances alignment with the intended culture nouns in the generated images. C-TRIP5. C-TRIP5 incorporates both cultural context and visual details, refining prompts according to all five criteria: Clarity, Background, Purpose, Visual Elements, and Comparable Objects. This configuration assesses whether adding visual details improves the alignment. By comparing C-TRIP3 and C-TRIP5, we can evaluate how much visual details enhance cultural alignment in the generated images.\n4.3 Evaluation User Survey. The alignment of images with culture nouns is inherently subjective and can only be appropriately evaluated by participants of the respective cultural groups based on country. Accordingly, surveys were distributed to individuals who were either native to the respective countries or had at least three years of cultural experience to evaluate our approach. Participants were provided with survey questions based on their chosen country. Each survey page presented 4 images of a randomly selected culture noun. Each survey page contains 4 evaluation questions to rank images: (a) Cultural Representation, (b) The Naturalness of the Keyword, (c) Offensiveness, and (d) Description and Image Alignment. Participants were asked to evaluate a randomly ordered set of images for each question. The image ranked first was considered the most appropriately represented and least offensive, while the image ranked fourth was deemed the most inappropriate and offensive. We employed the Matrix Mean-Subsequence Reduced (MMSR) model (Ma and Olshevsky, 2020), an established algorithm (Majdi and Rodriguez, 2023) for noise label aggregation provided by crowd-kit (Ustalov et al., 2021), to quantitatively estimate subjective performance perception. Using MMSR, the labels from all respondents were aggregated through weighted majority voting based on the assessment of their reliability. Subsequently, the MMSR+Vote method was applied, in which"}, {"title": "5 Results", "content": "Qualitative Comparison Figure 3 compares the images generated from each C-TRIP configuration and the Base Prompt described in Section 4.2. The refined prompt generated by C-TRIP provides cultural knowledge to Stable Diffusion 2, contributing to the generation of culturally-aware images. C-TRIP5, which includes the visual details criteria, demonstrated higher quality representation. That is, Stable Diffusion 2 can produce better images when the prompts include appropriate cultural contexts and visual details. With these enhancements, C-TRIP effectively improves the model's ability to generate culturally relevant images."}, {"title": "6 Ablation Study for UC Nouns", "content": "The Culture Capsule approach is a method that teaches learners who have not experienced a particular culture. With this idea in mind, we aimed to apply a similar approach to words that Stable Diffusion 2 is not familiar with. In this section, we analyzed and compared the Unrecognized/Underrepresented Culture nouns (UC nouns) and the Common/Recognized Culture nouns (RC nouns). UC and RC noun groups We categorized culture nouns into UC and RC noun groups according to their frequency within the training dataset used for Stable Diffusion 2. To achieve this, we analyzed Re-LAION-2B-en-research, a filtered true subset of the LAION-2B-en (Schuhmann et al., 2022), using a $\\textit{punsafe} > 0.95$ threshold and keyword-based filters to remove potentially suspicious content. Our analysis focused on culture nouns within the dataset captions, classifying them based on their frequency of appearance. These nouns were then grouped using a quartile-based approach (Q1 to Q4), with Q1 and Q2 representing UC nouns and Q3 and Q4 representing RC nouns. This grouping provided a structured means to evaluate the C-TRIP's capacity to generate culturally aligned images across varying levels of representation."}, {"title": "7 Conclusion", "content": "In this paper, we introduced C-TRIP (Culturally-Aware Text-to-Image Generation with Iterative Prompt Refinement), a novel approach that iteratively refines prompts to improve the alignment of culture nouns with images generated by existing text-to-image models without any fine-tuning. Experiments across eight countries demonstrated that C-TRIP significantly improves the alignment of culture nouns in generated images, particularly for underrepresented UC nouns. User surveys and automatic evaluations consistently present C-TRIP's superior performance in cultural representation and the semantic consistency."}, {"title": "8 Limitations", "content": "Sources like Wikipedia and general Web content contain cultural biases (Miquel-Rib\u00e9 and Laniado, 2018; Baeza-Yates, 2018), which can affect the refinement process and C-TRIP's capacity to provide balanced cultural representation. Future work should focus on enhancing the information retrieval process through developing culturally diverse datasets, thereby ensuring high-quality, relevant data for effective prompt refinement. The limited scope of prompts utilized in our experiments and human evaluations presents a current limitation and suggests an important avenue for future research. Additionally, our work is constrained by the perceptual biases of human annotators from eight countries. To improve the reliability of evaluation outcomes, future work will emphasize the inclusion of annotators from a broader range of cultural backgrounds."}]}