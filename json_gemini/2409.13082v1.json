{"title": "AutoVerus: Automated Proof Generation for Rust Code", "authors": ["Chenyuan Yang", "Xuheng Li", "Md Rakib Hossain Misu", "Jianan Yao", "Weidong Cui", "Yeyun Gong", "Chris Hawblitzel", "Shuvendu Lahiri", "Jacob R. Lorch", "Shuai Lu", "Fan Yang", "Ziqiao Zhou", "Shan Lu"], "abstract": "Generative Al has shown its values for many software engineering tasks. Still in its infancy, large language model (LLM)-based proof generation lags behind LLM-based code generation. In this paper, we present AUTOVERUS. AUTOVERUS uses LLM to automatically generate correctness proof for Rust code. AUTOVERUS is designed to match the unique features of Verus, a verification tool that can prove the correctness of Rust code using proofs and specifications also written in Rust. AUTOVERUS consists of a network of LLM agents that are crafted and orchestrated to mimic human experts' three phases of proof construction: preliminary proof generation, proof refinement guided by generic tips, and proof debugging guided by verification errors. To thoroughly evaluate AUTOVERUS and help foster future research in this direction, we have built a benchmark suite of 150 non-trivial proof tasks, based on existing code-generation benchmarks and verification benchmarks. Our evaluation shows that AutoVerus can automatically generate correct proof for more than 90% of them, with more than half of them tackled in less than 30 seconds or 3 LLM calls.", "sections": [{"title": "Introduction", "content": "Generative AI (GenAI) techniques have shown to be effective for many software engineering tasks. GenAI-based code-synthesis tools, like GitHub Copilot, have been widely used in practice. It would be great if GenAI could synthesize not only code but also code-correctness proof. Indeed, recent research [6, 14, 29, 32, 33, 40, 50] has shown the potential of GenAI in synthesizing proofs in proof-oriented languages like LEAN [10], F* [42], Dafny [26]. However, the state of the art of GenAI-for-proof is still far behind that of GenAI-for-coding both in terms of benchmark building and generation capability. In this paper, we explore using LLM to automatically generate proof annotations that allow Verus [25] to formally verify the correctness of Rust code.\nWe choose to focus on Verus for two main reasons. First, the usage opportunity: Verus is a state-of-the-art SMT-based verifier for Rust with a particular focus on practical usage [24, 41, 51]."}, {"title": "Background: Verus", "content": "Verus verification incurs no run-time checks. Instead, Verus statically analyzes the Rust executable code and the Verus annotations, which are referred to as ghost code, and constructs SMT formulas to query the underlying SMT solver. Based on the query result, Verus knows if the Rust code can satisfy some specifications for all possible executions. After the verification, Verus can erase all the ghost code to reproduce the original Rust implementation, allowing easy integration with other Rust code that developers may choose not to verify. This workflow is unlike many other verification tools that require the verification to be discharged in a different project as the implementation.\nVerus has made small extensions on Rust syntax through macros, including new abstract data types. Since fixed-bit-width integer types in Rust (e.g., u64, 64-bit unsigned integer) are not easy for SMT solvers to reason about, Verus additionally supports nat for any natural numbers and int for any integer numbers. By default, int data type is used in all Verus specifications. Similarly, Verus provides some collection types (Seq, Map, Set) that can be used to abstract Rust collections.\nGenerally speaking, Verus specification annotations include function pre-condition (requires), function post-condition (ensures as in Figure 1b), and spec functions. The spec function in Figure 1a looks very similar to native Rust functions. However, being ghost code, it is required to be purely functional without mutations of any variables. This feature allows it to be easily transformed into a function in the SMT solver and can be called from other ghost code, such as in the function post-condition on Line 20, and in the loop invariant on Line 28. Note that, executable functions cannot be called in ghost code a mistake that LLM makes a lot.\nVerus proof annotations generally include loop invariants, assert statements, and proof functions. Loop invariants are needed for the verifier to reason about a loop. asserts are generally used to help SMT solvers reason about complicated properties involving quantifiers, collections, etc. For example, a lot of commonly used Verus data-structures (e.g., Seq, Set) and their specification APIs (e.g., subrange, take, filter, push) require accompanying axioms for SMT solvers to reason about. To avoid complexity explosion in SMT solver, many of these axioms are not automatically triggered in Verus and need to be explicitly added through asserts, like that in Figure 1b.\nJust like spec functions, proof functions are ghost code and need to be purely functional. Proof functions can come with its own premises, specified using requires, and conclusions, specified using ensures. The body of the proof function contains ghost code used to assist Verus in constructing SMT formulas to prove the conclusions. A proof function can be called by other ghost code. As long as the premise of the function is satisfied beforehand, the conclusion of the proof function can help prove properties after its call site. We will see an example of proof function later in \u00a7 4.3."}, {"title": "Unique Challenges and Opportunities for AutoVerus", "content": "Training data and benchmarks. At the time of writing, there are fewer than 10 projects on GitHub developed under Verus. In comparison, there are hundreds of GitHub projects with thousands to hundreds of thousands of files developed under each of those more established verification tools like LEAN [10], Coq [5], Dafny [26], Isabelle [35], and F* [42]. Making things worse, Verus has been designed to verify large system projects (e.g., storage system [31], VM hypervisor component [51], operating system modules [9]). Even with our best efforts, extracting single-function/file benchmark programs from these projects failed due to complicated code dependency and specifications.\nLanguage syntax. Unlike most verification tools, Verus allows LLMs to work with a widely used language, Rust. However, this also produces challenges, as LLMs easily stumble at those subtle syntax extensions made by Verus upon Rust.\nOne such challenge is about the integer data type. Verus uses its abstract int type by default in specification functions and hence often requires type casting, such as the \u02bbi as int' on Line 28, 33-36 in Figure 1. This type of casting is difficult for LLMs like GPT that are used to Rust syntax.\nAs another example, since Rust executable functions cannot be called in ghost code, misuse of Rust APIs in Verus annotation is very common for inexperienced users and LLMs. In lucky cases, like Line 26 of Figure 1, Verus provides a spec-function len() for the Rust collection type Vec. Therefore, text.len() can be used as both a valid Rust API call and a valid spec call in ghost code. Unfortunately, many other Rust APIs do not have corresponding spec. For example, there is no subrange spec function for Rust Vec type. In Figure 1b, if we use text.subrange inside the loop invariants or the assert statement, compilation errors will rise. Instead, text@.subrange is used, where text@ turns text from Rust Vec into Verus Seq, which has a subrange spec function."}, {"title": "Design", "content": "Terminology. The input and output of AUTOVERUS, as well as each of its agents, are Verus programs that contain Rust code and Verus annotations. The input program to AUTOVERUS contains no proof annotations; the output of AutoVerus contains not only Rust code, specification, but also proof annotations added by it. We refer to a Verus program as unsafe, if it modifies the Rust code and/or specifications under proof; as invalid, if it leads to Verus compilation errors; as correct, if it is safe, valid, and can be completely verified by Verus.\nIn the section, we present each of the three phases of AUTOVERUS in detail regarding how the LLM agents are designed, how they are coordinated, and how their output is processed."}, {"title": "Phase 1: preliminary proof generation", "content": "Verus proof typically includes loop invariants, asserts, and sometimes proof functions/blocks. Since asserts and proof functions/blocks are typically added during the proof debugging to provide extra hints to SMT solver, at this first phase of proof generation, AUTOVERUS focuses on loop invariants. An agent is designed to generate loop invariants for every loop in the target Rust code.\n4.1.1 Agent design. In the prompt to LLM, we ask it to \u201cadd loop invariants to the given Rust code, so that Verus can verify the given function behaves exactly what is described in the specifications.\u201d Furthermore, we teach LLM three things that we believe are the most important in writing loop invariants in Verus: 1) to describe the initial value of every variable read in the loop; 2) to describe the assigned value of every variable written in the loop; and 3) to leverage spec and proof functions when needed. We have intentionally kept this agent simple and leave it to later refinement/repair agents to correct any mistakes or oversights of this first agent.\nWe also include three toy Verus examples written by us in the prompt to teach LLM about basic Verus features like invariant, quantifiers (i.e., forall and exists), and abstract data structures (e.g., vstd:: Seq). Each example contains less than 30 lines of Rust code and 10\u201320 lines of proof annotations. We stick to these same examples across all proof tasks.\n4.1.2 Post-processing of agent output. By default, we configure the agent to produce 5 outputs. If one of these 5 outputs is correct, AUTOVERUS's job is done. Otherwise, AUTOVERUS tries to create a correct proof by stitching different LLM outputs together. If that also fails, AUTOVERUS will pick an imperfect yet promising Verus program to start the refinement phase.\nAUTOVERUS first filters out LLM output that is unsafe, like the one in Figure 4, through static analysis. It also filters out invalid LLM output, unless AutoVerus can fix the compilation error through simple code editing, like changing v[i] to be v[i as int] when Verus complains that \"type int is expected for the expression i\u201d. If any of the remaining valid and safe Verus programs allows verification to succeed, the whole proof task is done. Otherwise, AUTOVERUS moves on.\nAUTOVerus then ranks remaining LLM-generated Verus programs based on a score tuple {V, E}, with V being the number of functions successfully verified by Verus, and E being the number of verification errors reported by Verus. For any two programs that contain the same executable Rust code, AUTOVERUS considers the one with higher V score as the better one (i.e., more functions proved), with the tie-breaker being having a lower E score (i.e., fewer verification errors).\nFinally, given a list of K valid and safe programs, P1, P2, ... Pk, AUTOVERUS checks if merging some of them might produce a better proof. Since merging takes time, AUTOVERUS only explores linear, instead of exponential, number of merging schemes: AUTOVERUS starts with the highest ranked program, considered as best-so-far PB, and goes down the ranked list. When merging the best-so-far program PB with a program Pi produces a higher-scored program P', AUTOVERUS updates PB to be P' and continues to check the next program on the list Pi+1 until all the k programs are checked. If AutoVerus fails to identify a perfect proof throughout this process, the final best-so-far program PB will become the input to the next phase of AUTOVERUS."}, {"title": "Phase 2: generic proof refinement", "content": "This phase aims to refine the loop invariants in the best-so-far Verus program PB generated by phase-1 through common and generic Verus-verification tips.\n4.2.1 Refinement-agent design. The refinement phase consists of a series of LLM agents, each aiming one common mistake in the writing of loop invariants in Verus:\nConstant-Propagation agent checks if the function pre-condition includes properties about a read-only parameter, and adds these properties as invariants to every loop in the function. For example, loop invariants N > 0 and N < 1000 in Figure 5c are part of the function pre-condition in that example. If they are missing, this agent will add them.\nArray-Length agent checks if the length information of every array/container used in a loop is specified as a loop invariant, and adds such information if not. For example, if the invariant sum.len()==1 is missing from Figure 5c, it is be added here.\nQuantifier agent checks if quantifier-related invariants are used correctly. For example, if Line 9 of Figure 5c mistakenly has forall |k:int| 0<=k<i ==> b[k]==1, it would be corrected here.\nConditional-Loop-Invariant agent urges LLM to check if any loop invariant may only apply to some, instead of all, of the loop iterations and make adjustment accordingly. For example, this would help LLM handle the example in Figure 3.\n4.2.2 Agent composition and output post-processing. AutoVERUS invokes every refinement agent sequentially, starting with the simplest, Constant-Propagation, and progressing to the more complex ones. Each agent takes the best-so-far Verus program PB as its input and generates a new refined program P. If P turns out to contain the correct proof, the whole task is done; otherwise, AUTOVERUS replaces the original best-so-far program PB with P when two conditions are met: 1) P is a valid and safe Verus program after AUTOVERUS's type-casting edits, if needed; and, 2) P does not lower the number of functions successfully verified by Verus in PB.\nThe post-processing here is simpler than that of Phase 1: each agent only generates one, instead of five, proof candidate, and hence there is no merging or ranking needed. The rationale is that each agent in this phase conducts a relatively straightforward action, and, although every agent is useful in general, it may or may not be necessary for every Verus program given to it."}, {"title": "Phase 3: error-driven proof debugging", "content": "Proof construction is similar to code writing in that debugging is inevitable for realistic and complicated tasks. Human experts expect to see verification errors and are experienced at repairing proofs to fix those errors one by one. Therefore, we have designed this debugging phase for AUTOVERUS to mimic human experts. After the first two phases, if the best-so-far proof still incurs verification errors, AUTOVERUS starts its proof debugging.\n4.3.1 Repair-agent design. AutoVerus has 10 repair agents, with each agent At designed to fix one particular type t of Verus verification errors. The prompt to At includes not only the Verus program under repair, but also 1) the detailed information about an error Et to be fixed, which has the verification-error type t, and 2) the instruction about how to fix errors of type t, which is sometimes customized with code expressions extracted from the code snippets related to Et. It could be boring to go through all the agents one by one, as each agent basically summarizes how human experts handle a type of verification errors. Instead, we use a proof task fibonacci as an example to see what are some of the main agents and how they actually perform in practice.\nFigure 6a contains a Rust function that returns the first n numbers in the fibonacci sequence. It sets the first two numbers in this sequence to be 0 and 1, and then computes the remaining ones through a while loop (Lines 16-36). The spec function fibo in Figure 6b provides a strict mathematical specification of the Fibonacci sequence. What we need to prove for the Rust function fibonacci(n) is that every number in its result vector matches that produced by the spec fibo under the premise that the nth fibonacci number fits in the i32 data type.\nIteration 1: All the gray-background code in Figure 6a, including the red line fib.len() == n, comes from the best-so-far proof PB from AUTOVERUS's refinement phase. Verus only reports one error for this version: invariant fib.len() == n on Line 24 is not satisfied before the loop.\nOur agent dedicated to invariant-not-satisfied-before-loop is invoked. Its prompt includes these repair strategies: 1) to add an assert about this invariant right before the loop; 2) in case of nested loops, to add this invariant to its outer loop when applicable; 3) to modify or delete this loop invariant when it is incorrect or unnecessary. One of this agent's outputs took both the first and the third strategy, as highlighted by -1 and +1 in Figure 6a: it deleted this invariant from Line 23, and added an assert(fib.len() == 2) right before the loop on Line 15, accompanied by a comment explaining its action on Line 14. In fact, deleting this invariant is the right repair action: the length of fib array matches the loop index i in the loop, not always n. Of course, fib.len() == 2 is correct right before the loop. So, this repair succeeds.\nIteration 2: After the first repair, Verus now reports two new errors: 1) an arithmetic-overflow for fib[i-1] + fib[i-2] on Line 33; 2) the invariant on Line 21 that states fib[j] == fibo(j) for all j between 0 and i does not hold at the end of the loop.\nAfterVerus decides to fix the arithmetic-overflow first. The corresponding agent is instructed to add loop invariants or asserts to state the upper/lower bound of the arithmetic operations that Verus complains about. The prompt also includes tips about using parametric bound or approximated bound when needed, and reasoning the monotonicity of a data series when needed. The output of this repair agent is highlighted by the +2 heading in Figure 6a. As we can see, this agent's repair includes several parts: 1) it made the right decision that an assert, instead of a loop invariant, is needed here to state the bound on Line 26 (again accompanied by a comment explaining its action); 2) it figured out the right bound 0x8000_0000 for the expression fib[i-1] + fib[i-2], probably based on the bound expressed in the fibo_fits_i32 spec-function in Figure 6b; 3) it realized that proving the monotonicity of the fibonacci sequence is needed and hence synthesized a new proof function lemma_fibo_monotonic and also used this function to support the bound assertion very nicely on Lines 28\u201331.\nIteration 3: With all this effort, a simpler problem like the one in Figure 1 would have been proved at this point. However, for this example, Verus now reports that the while-loop in Figure 6a is completely verified, but there is a new error the post-condition fibo(n) <= fibo(m) of the new proof function lemma_fibo_monotonic shown in Figure 6c is not satisfied at the end of the function. The agent dedicated to function-post-condition-unsatisfied error is now called. This agent's instruction suggests adding asserts (or loop invariants) that correspond to the failed post-condition to the function exit where the post-condition does not hold (or a relevant loop). Therefore, the agent added the two green-highlighted lines in Figure 6c. Interestingly, LLM does not simply assert fibo(n) <= fibo(m),the failed post condition. Instead, it added two asserts that describe the transitive relationship of fibo(n) <= fibo(m-1), which matches the post-condition of the proof function right before it on Line 7, and fibo(m-1) <= fibo(m), which is quite nice.\nIteration 4 & 5: Unfortunately, this seemingly perfect repair is still incorrect. Verus reports an assertion-failure on the newly added Line 9 of Figure 6c - fibo(m-1) <= fibo(m) cannot be verified to always hold. Therefore, AutoVerus calls upon its assertion-failure repair agent. This agent is given some general options: 1) if the assert expression is related to Verus data structure APIs like Seq:: subrange, Seq::filter, and so on, a set of off-the-shelf lemma functions can be used; 2) change or delete any loop invariants that might be related to the assert; 3) add more assert statements; 4) add more proof functions if needed. This fix is not easy. In fact, the first time this repair agent is called upon, no good repair is produced. At the second time, the repair agent comes up with the repair shown in Figure 6d: it added two assert statements to help the SMT solver reason why fibo(m-1) cannot be larger than fibo(m); and it also added the if m > 1 condition to avoid negative indexing, as explained by its own comment.\nFinally, Verus reports that the whole proof task was successfully done!\nWhat we described above is the output from a particular run of AUTOVERUS. Since this is a difficult task, we have noticed that AUTOVERUS runs into different situations every time, and can generate correct proof roughly every other time (and the correct proof is not always the same).\nAUTOVERUS also contains several other agents that are dedicated to repair other major type of Verus errors, like unsatisfied function pre-condition, index-bound violation, loop-invariant-failed-at-the-end, etc. We skip them here due to space constraints.\n4.3.2 Repair-agent composition and output post-processing. It is challenging to coordinate all the agents because a Verus program often contains more than one verification error and each error may take multiple attempts to get fixed. Furthermore, fixing one error may introduce another error(s), and it could be difficult to tell whether the repair has made the proof better or worse.\nWith these challenges in mind, AUTOVERUS conducts its debugging in iterations, starting with the best-so-far Verus program PB generated by the refinement phase. In each iteration, AUTOVERUS collects all the verification errors E in PB from Verus, picks one error E from E to fix next, and prompts a repair agent that suits E. AUTOVERUS then examines the agent's output one by one to decide whether to accept a repair to replace the best-so-far proof PB. Finally, AUTOVERUS prepares for its next iteration based on whether the target error E has been fixed or not. AUTOVERUS keeps running these iterations until a correctly proved program is found or a pre-configured threshold is reached (10 iterations by default). We explain these three steps in more detail below.\nHow to pick the error to fix next? We have designed an ordered list of Verus error types and AUTOVERUS ensures that a verification error is picked only when higher-priority errors no longer exist for the program under proof. The top one is type error, because they have to be fixed before Verus can compile the program and conduct verification attempt. The next two are bound-related"}, {"title": "Implementation", "content": "We implemented AutoVerus as a command line Python tool using GPT-40 invoked by Azure OpenAI APIs. In the following, we discuss some implementation details.\nHoudini alg. Given a set of proof annotations A that fail the verification, if a subset of A is correct and sufficient to prove, Houdini algorithm guarantees to identify this subset in linear number of verifier-invocations. More details of this long-established algorithm can be found in previous work [18]. This algorithm has been used for GenAI-for-C [23] and we apply it here for Verus.\nLynette. To add discipline into AUTOVERUS, we have implemented Lynette using the Verus front-end parser in Rust to post-process the LLM-generated code at the AST level. To check whether a LLM output P is safe regarding the input Rust program and its specification Po, Lynette checks if P and Po can be compiled to the same executable by comparing the pure Rust AST of the files after erasing the ghost code; Lynette compares the pre- and post-condition of P and Po so the LLM cannot tweak them to change the goal of the verification; Lynette also searches for debugging ghost function, such as admit() or assume(), in P.\nAnother use of Lynette is in program merging. Naively using a text-merging tool does not work for proof candidates, P\u2081 and P2 of a complicated proof task, as P\u2081 and P2 may differ quite a lot. Lynette first erases all of the ghost code in the program to obtain the pure Rust AST, which is the same for P\u2081 and P2, and uses it as the anchor to merge the ghost code in P\u2081 and P2. The minimum merging unit is a ghost expression, i.e. a Verus assertion, a proof block, or an invariant.\nThe third use of Lynette is to support the Houdini algorithm in deleting cannot-be-proved proof annotations from the proof candidate. Since Lynette is AST-aware, it makes sure that the deletion honors the boundary of ghost expression, and guarantees to produce syntactically correct result.\nVerus configuration. Verus has a loop-isolation configuration that can affect the difficulty of proof. When it is set to be false, Verus will consider facts (e.g., function pre-conditions, asserts) from outside a loop body in its proof inside the loop body. By default, this configuration is set to be"}, {"title": "Experimental setup", "content": "Hardware and software All our experiments are run on a machine with Ubuntu 22.04.1 LTS, 24-core CPUs, and 64 GB RAM. By default, AutoVerus uses GPT-40 (model version 2024-05-13). For comparison, we will also run AUTOVERUS on two other models, GPT-4-turbo and GPT-3.5-turbo.\nAlternative designs in comparison To the best of our knowledge, no prior GenAI-for-proof work focuses on Rust or Verus. Therefore, we will compare AUTOVERUS with a baseline that repeatedly invokes GPT-40 to generate Verus proof. To make this baseline competitive, we designed its prompt to essentially be a summary of all the prompts used in AUTOVERUS agents. It uses the same LLM-temperature and 5-output per invocation setting as AUTOVERUS, and has more sophisticated prompt and examples than any AutoVerus agent.\nThe baseline does fine in generating loop invariants for simple loops, succeeding for similar number of tasks from Clover, MBPP, Misc as Phase 1, 2 of AUTOVERUS. Of course, the baseline took much longer and many more LLM calls to achieve this than AutoVerus, which we discuss below.\nEvaluation metrics We will mainly evaluate three metrics: number of correctly verified tasks, time and number of LLM calls required to come up with the correct proof. Due to the randomness of LLMs, the number of correctly verified tasks may change over runs. By default, we report the number of correctly verified tasks after running AUTOVERUS (the whole three phases) for three times; the same applies for various variants of AUTOVERUS. When comparing with the baseline, we will show the number of verified tasks under the same time and LLM-call budget to be fair. Keep in mind that since Verus can automatically tell whether a proof is correct, there is no manual effort involved even if we invoke LLM or execute the AuTOVERUS workflow for multiple times. The difference between verifying a task in one attempt and in multiple attempts is the time cost and the number of LLM-calls, which we will report."}, {"title": "Experimental Results", "content": "Overall results\nTable 2 shows the overall results of AUTOVERUS on Verus-Bench. AUTOVERUS successfully proves 137 out of 150 (91.3%) benchmark problems. That is, provided with the Rust implementation and specification, AutoVerus is able to generate proof annotations that allow Verus to prove the specification is guaranteed to hold for 137 out of 150 tasks. Notably, AUTOVERUS verifies all of the problems from the CloverBench and Diffy benchmarks. Even though the default setting allows up to three attempts for AUTOVERUS, AUTOVERUS was able to prove 122 tasks in its first attempt, with 10 more proved in its second attempt and 5 more on its third attempt.\nAs also shown in Table 2, Phase-1 is able to finish slightly over half of the 150 proof tasks, while Phase-2 refinement and Phase-3 debugging also contribute to 22 and 37 additional proven"}, {"title": "Comparison with baseline", "content": "Table 2 also shows that the baseline scheme of repeatedly invoking LLM (i.e., GPT-40) was only able to prove 67 out of the 150 tasks (44.7%), even though it was given 10 minutes for each task. Note that, this baseline scheme uses the same LLM-temperature and 5-output per invocation setting as AUTOVERUS, and has more sophisticated prompt and examples than any AutoVerus agent.\nThe baseline does fine in generating loop invariants for simple loops, succeeding for similar number of tasks from Clover, MBPP, Misc as Phase 1, 2 of AUTOVERUS. Of course, the baseline took much longer and many more LLM calls to achieve this than AutoVerus, which we discuss below.\nHowever, the baseline cannot handle more complicated loop invariants, and hence succeeded for only 5 out of 38 Diffy tasks. In comparison, AUTOVERUS's Phase-1 alone generated correct proof for 26 Diffy tasks. Furthermore, the baseline scheme is poor in generating more complicated proof annotations, and failed many more tasks in MBPP and Misc than AUTOVERUS.\nTo understand how efficient AUTOVERUS and the baseline are, we plotted how many tasks can be correctly proved by each of them under fixed time budget (up to 10 minutes per task) and the number of LLM-call budget (up to 60 calls per task), as shown in Figure 7a and Figure 7b. As we can see, AUTOVERUS consistently outperforms the baseline, finishing about twice as many tasks, under the same time and LLM-call budget constraints. In order for the baseline to succeed in 67 tasks, it needs a budget of 458 seconds per task or 59 LLM-call per task; in contrast, 22-second per-task or 2 LLM-call per task is sufficient for AUTOVERUS to generate correct proof for at least 67 tasks. Furthermore, if we give each task only 30 seconds, AUTOVERUS can generate correct proof for 81 tasks, while the baseline cannot even finish 40 tasks. Again, keep in mind that the baseline has the same temperature setting and 5-output per LLM-call as AUTOVERUS."}, {"title": "Detailed results of AutoVerus components", "content": "7.3.1 The effectiveness of merging, ranking, filtering, and Houdini. For 14 tasks, although none of the direct output of the Phase-1 agent is correct, merging some of them immediately provides correct proof just like the example in Figure 5. Also during Phase-1, ranking LLM's output based"}, {"title": "Ablation study", "content": "Phase-1 and Phase-2 of AutoVerus. As shown in Table 3, we conducted a number of ablation studies to understand different design choices for Phase-1 and Phase-2 of AUTOVERUS. For this study, we used two subsets of proof tasks from Verus-Bench. The Inference Dataset includes all the 78 tasks that can be correctly proved by AUTOVERUS using Phase-1 alone; the Refinement Dataset includes all the 22 tasks that require both Phase-1 and Phase-2 to be proved by AUTOVERUS.\nAs shown in Table 3, the number of tasks that can be proved drops if we remove any one of the three examples used by AUTOVERUS's Phase-1 agent. The third example stands out as particularly influential, resulting in the most reduction in proved tasks when removed.\nIf we combine all five agents in Phase-1 and Phase-2 together into one agent with a combined prompt, 7 tasks that originally can be proved now cannot. This includes 3 tasks that originally can be proved with just Phase-1. Notably, we maintain the same post-processing procedures, including filtering, ranking, merging, and the application of the Houdini algorithm, for the fused version.\nAs also shown in the table, when we discard our ranking scheme, which is based on Verus results, and instead relies on LLM to tell us the best output, AUTOVERUS verifies 12 fewer tasks.\nPhase-3 of AutoVerus. We have designed an alternative generic debugging agent. This agent is equipped with three few-shot examples that contain examples of asserts and proof functions,"}, {"title": "LLM choice in AutoVerus", "content": "Due to resource constraints, instead of using the whole Verus-Bench, we randomly sampled from Verus-Bench a smaller dataset of 30 problems, preserving the original benchmark-source distribution (i.e., 2 from CloverBench, 8 Diffy, 15 MBPP, and 5 Misc).\nWhile trying three different LLMs, we observe that GPT-4-turbo delivers competitive performance compared to our default setting with GPT-40, with both models power AUTOVERUS to prove 26 out of the 30 tasks. In contrast, GPT-3.5-turbo only allows AUTOVERUS to prove 18 tasks. However, upon further analysis, we find that GPT-4-turbo requires significantly more time in AUTOVERUS to figure out the correct proof an average of 229.8 seconds per task, compared to just 70.0 seconds for AUTOVERUS with GPT-40. Finally, while setting the temperatures of GPT-40 to be 0.1, 0.4, 0.7, and 1.0, AUTOVERUS proves 21, 24, 25, and 26 tasks, respectively. This aligns with our design intuition that a higher temperature setting is beneficial in fostering LLM's creativity and exploring a wider range of potential solutions, as long as human experts and discipline are also applied."}, {"title": "Threats to Validity", "content": "There is an inherent randomness in AUTOVERUS and our evaluation given the probabilistic nature of LLM. We have made our best effort to make Verus-Bench an unbiased and meaningful benchmark suite. However, it is still just a collection of 150 tasks. It cannot represent many real-world proof tasks in large system code; it also cannot guarantee to represent all small-scale proof tasks. We have tried our best to not tune AutOVERUS based on features of specific proof tasks in our Verus-Bench, and have our team that sets up the majority of the Verus-Bench not participating in the design of AUTOVERUS. However, the authors who participated in the design of AutoVerus are already familiar with some of the tasks in the Diffy benchmark and in the Misc set. Finally, our evaluation does not yet cover some of the newest Rust features supported by Verus, such as iterators."}, {"title": "Related Work", "content": "Verification for Rust. As the popularity of Rust continues to grow, a variety of tools have been developed to verify Rust programs [1, 12, 19, 20, 22, 25, 30, 43"}]}