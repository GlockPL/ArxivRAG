{"title": "Scaling Data-Driven Building Energy\nModelling using Large Language Models", "authors": ["Sunil Khadka", "Liang Zhang"], "abstract": "Building Management System (BMS) through a data-driven method always faces data and model scalability issues. We\npropose a methodology to tackle the scalability challenges associated with the development of data-driven models for BMS\nby using Large Language Models (LLMs). LLMs' code generation adaptability can enable broader adoption of BMS by\n\"automating the automation,\" particularly the data handling and data-driven modeling processes. In this paper, we use\nLLMs to generate code that processes structured data from BMS and build data-driven models for BMS's specific\nrequirements. This eliminates the need for manual data and model development, reducing the time, effort, and cost\nassociated with this process. Our hypothesis is that LLMs can incorporate domain knowledge about data science and BMS\ninto data processing and modeling, ensuring that the data-driven modeling is automated for specific requirements of different\nbuilding types and control objectives, which also improves accuracy and scalability. We generate a prompt template\nfollowing the framework of Machine Learning Operations so that the prompts are designed to systematically generate Python\ncode for data-driven modeling. Our case study indicates that bi-sequential prompting under the prompt template can achieve\na high success rate of code generation and code accuracy, and significantly reduce human labor costs.", "sections": [{"title": "INTRODUCTION", "content": "Buildings contribute to approximately 30% of the world's total energy consumption and 26% of global greenhouse gas\nemissions (IEA 2023). Effective building operation and control strategies are crucial for optimizing energy usage (Costa,\nKeane et al. 2013). One way to achieve this is through a data-driven Building Management System (BMS) (Maddalena, Lian\nand Jones 2020) that uses machine learning and statistical modeling techniques to extract insights and develop predictive\nmodels to inform decision-making processes for building operations and control. These systems rely on various data sources,\nincluding weather data, occupancy patterns, energy consumption, and indoor environmental conditions (Amasyali and El-\nGohary 2018). However, the widespread adoption of data-driven BMS faces scalability challenges. Each building has\ndifferent characteristics and extensive data requirements, along with the need for customized data-driven models, which make\nmodeling difficult for building operations. Additionally, the development and deployment of these systems often require\nsignificant manual effort and specialized expertise.\nTo address these challenges, LLMs (Vaswani, Shazeer et al. 2017) have emerged as a powerful technology that can\ngenerate human-like text, code, and data representations. LLM is a deep-neural network-based natural language processing\nmodel to understand and generate contextual and coherent outputs based on input prompts. LLMs achieve excellence by\nundergoing extensive training on diverse datasets (Brown, Mann et al. 2020), (Izacard, Lewis et al. 2022), that encompass a\nwide range of programming languages, code examples, technical guides, and discussion forums. This comprehensive training"}, {"title": "METHODOLOGY", "content": "The proposed methodology includes the development of prompt engineering templates, serving as a foundational bone\nstructure, in an organized storyline to generate code successfully for data-driven modeling of building energy use. The\nprompt template is designed in accordance with MLOps workflow (AmazonWebServices 2019), to provide a standardized\nprocedure of data-driven/machine-learning modeling."}, {"title": "CASE STUDY", "content": "Virtual Building Settings\nThe case study takes place in a DOE reference building (small office) with an area of 511m2 (5500sqft). This building\nwas developed by the National Renewable Energy Laboratory and is simulated using EnergyPlus software for modelling.\nMore details about the building can be found in the work done by (Deru, Field et al. 2011). The weather file for this\nmodelling is USA_AZ_Tucson.Intl.AP.722740_TMY3.epw. The following is the variables of the simulation: zone\ntemperature setpoint (ZTSP), zone air temperature (ZSF1_ZT), outdoor air dry-bulb temperature (OADT), direct solar\nradiation rate per area (SR_DIR), occupancy (OCC), and single zone cooling rate (Clg_Rate). The modeling is a regression to\npredict zone cooling rate (output), using the remaining variables (input). The generated data are saved in two csv files\n'input_fx.csv' and 'output_fx.csv', which are generated after data processing which contains the input variables and output\nvariables respectively for BMS.\nLarge Language Model Settings\nWe choose ChatGpt-4, March 14, 2023 version, released by OpenAI (OpenAI 2023) as the LLM for the case study."}, {"title": "RESULTS", "content": "Using the prompt template, we carried out the experiment among three prompting strategies. The performance is\ndefined by two factors: 1) prompt completion rate: the success rate for LLM to generate all the code for each step without\ntermination, 2) code accuracy: the success rate of the generated code to run error-free. Since our objective is to reduce human"}, {"title": "One-shot prompting strategy", "content": "The strategy known as the one-shot prompting strategy has been specifically designed for the purpose of obtaining a\nresponse through the use of a single prompt. In order to evaluate the effectiveness of this strategy, the experiment was carried\nout a total of 35 times, resulting in a prompt completion rate of 71.42% (23 out of 35 attempts)."}, {"title": "Bi-Sequential prompting strategy", "content": "In this strategy, we split the prompts into two sequential parts, termed bi-sequential prompting strategy. The first part\nincluded the first three steps, while the second part consisted of the remaining four steps. The outcomes achieved by\nimplementing the sequential prompting approach surpassed those obtained from the one-shot prompting method, exhibiting a\nprompt completion rate of 100% and a code accuracy of 97.14% (34/35 attempts)."}, {"title": "Step-wise sequential prompting strategy", "content": "In this strategy, each step is performed sequentially, one at a time, leading to successful code generation with a prompt\ncompletion rate of 100%. However, the code accuracy was significantly lower at 60% (21/35). The stark contrast between the\ncompletion rate and code accuracy can be attributed to code repetition; subsequent prompts read the initial prompt\ninstructions and repeat the same steps, causing the model to execute identical steps multiple times."}, {"title": "Comparison with human coding", "content": "The mean squared value is 0.437 and 0.0533 for linear regression and random forest in manual coding, whereas that by\nLLM is 0.313 and 0.047. The reduction in mean squared error for both linear regression (28.38%) and random forest\n(11.82%) when using LLM instead of manual coding indicates that LLM provide more accurate predictions, reflecting a\nsignificant improvement in model performance. The manual approach required over 2 days for the subject to achieve the\ndesired efficient results, showcasing how LLM can reduce both time and human costs, with higher probability of accuracy."}, {"title": "DISCUSSION", "content": "We compare three prompting strategies: one-shot prompting, step-wise sequential prompting, and bi-sequential\nprompting. Bi-sequential prompting performs the best because its prompt completion rate is 100%, better than or equal to the\nother two (71.42% and 100%); the code accuracy is 97.14%, which is also better than the other two (57.14% and 60%).\nThis indicates the chunk size of prompts matters in the code generation. If the chunk size is too large (meaning we put\neverything into one prompt), the LLM will lose track of long instructions in the prompts; If the chunk size is too small, the\nLLM tends to repeat code as it revisits previous prompts; its premature prediction of future code steps based on its training on\ndiverse datasets, attempting to anticipate the next step before it was actually requested. This inclination of LLM to forecast\nfuture steps and codes adversely affected the accuracy of the step-wise sequential results. To mitigate these challenges, the\nbi-sequential prompt strategy finds a balancing point of chunk size to achieve high accuracy in code generation tasks.\nA beginner's manual coding approach for MLOps tasks like feature selection and hyperparameter tuning can be time-\nconsuming due to their limited understanding and experience. In contrast, LLM have been trained on vast amounts of data,\nincluding code repositories and machine learning algorithms, giving them a comprehensive knowledge base. For example, in\nour case study, LLM considers the grid parameters like n_estimators, max_depth, min_samples_split, and min_samples_leaf,\nwhereas a beginner takes time to select between the hyperparameters that might be good for this operation, leading to more\ntime for project completion and higher probability of less accurate results. Additionally, the task of replicating the same\nprocess for different buildings poses a challenge due to the unique characteristics of each building, requiring adjustments or\ncomplete rewrites of the code."}, {"title": "CONCLUSION", "content": "The paper proposes a prompt template following the framework of Machine Learning Operations so that the prompts\nare designed to systematically generate Python code for data-driven modeling. The prompt template's consistency and\nreliability in the case study shows its effectiveness and potential for a wider range of data-driven modeling in buildings. Our\nfurther investigation into various prompting strategies, including one-shot prompting, step-wise sequential prompting, and bi-\nsequential prompting. We find bi-sequential prompting delivered the best performance in prompt completion rate and code\naccuracy. The study also highlights the critical importance of prompt length and sequence. Excessively detailed prompts can\noverwhelm LLMs, leading to suboptimal solutions and repetitive code generation as in the case of step-wise sequential\nprompting. Also, when compared with the manual coding, it reduces code development time from hours/days to seconds.\nIn terms of limitations, since we have used Chat-GPT-4, March 14, 2023 version as LLM for case study, the\nperformance of the proposed prompt template running in other LLMs is unknown. Second, the specific needs or customized\nrequirements are not fully incorporated within our standard template. We only incorporate one placeholder for modeling\npurpose, but in the future, we can add more details about building types, system types, and other information that might\nimpact the selection of features and machine learning algorithms. Moreover, the use of multi-agent workflow is not\nconsidered in this paper. By enabling parallel processing, these systems significantly accelerate problem-solving by allowing\nmultiple agents to work on different aspects of a problem simultaneously. This approach may speed up the trial-error cycle\nbut also optimizes resource allocation by dynamically assigning tasks based on each agent's capabilities and workload.\nAdditionally, our study is limited by the absence of comparable research in the field of prompt engineering in other\ndisciplines, highlighting this as future exploration work."}, {"title": "NOMENCLATURE", "content": "BMS\n=\nBuilding Management System\nLLM\n=\nLarge Language Model"}]}