{"title": "Variable Assignment Invariant Neural Networks for Learning Logic Programs", "authors": ["Yin Jun Phua", "Katsumi Inoue"], "abstract": "Learning from interpretation transition (LFIT) is a framework for learning rules from observed state transitions. LFIT has been implemented in purely symbolic algorithms, but they are unable to deal with noise or generalize to unobserved transitions. Rule extraction based neural network methods suffer from overfitting, while more general implementation that categorize rules suffer from combinatorial explosion. In this paper, we introduce a technique to leverage variable permutation invariance inherent in symbolic domains. Our technique ensures that the permutation and the naming of the variables would not affect the results. We demonstrate the effectiveness and the scalability of this method with various experiments. Our code is publicly available at https://github.com/phuayj/delta-lfit-2.", "sections": [{"title": "Introduction", "content": "Dynamic systems exist in every aspect of our world. Understanding and being able to model such dynamic systems allow us to predict and even control the outcomes of such systems. Learning from Interpretation Transition (LFIT) [10] is a framework that allows automatic construction of a model in the form of logic programs, based solely on the state transitions observed from dynamic systems. LFIT has been largely implemented with symbolic algorithms [20] that utilize logic operations. These algorithms are therefore interpretable and independently verifiable. The resulting model, a logic program that describes the dynamics of the system, is also interpretable. However, these symbolic algorithms treat all data as equally valid and thus are vulnerable to ambiguous, conflicting or noisy data. Symbolic LFIT algorithms also require all state transitions to be observable, and thus cannot generalize to or predict unobserved data.\nRecent advances in deep learning and neural network represent a good opportunity to address the above issues. The field of combining neural"}, {"title": "Related Work", "content": "Recent advancements in deep learning models have renewed interests in NSAI. In particular, as LLMs gain attention and are used widely, the lack of reasoning ability [25] starts to become critical.\nExtraction-based Methods CIL2-P [7] introduced the foundation for extracting symbolic knowledge from neural networks. While simple, the algorithm placed heavy constraints on the architecture of the neural network. Related to LFIT, NN-LFIT [8] proposed a method that trains a minimal neural network to model the dynamic system, then extracts symbolic rules based on the weights of the neural network.\nDifferentiable Programming-based Methods The Apperception Engine [3], SILP [2] and Logic Tensor Network [1] proposed methods that leverage gradient descent to learn a matrix that can later be transformed into symbolic knowledge. These methods can be integrated with CNN or other neural network modules to further process continuous data. D-LFIT [6] uses gradient descent to optimize a learnable matrix that semantically mirrors the Tp operator. Compared to SLFIT2, these methods require optimization for every problem instance. Scalability issues also remain, as the size of the learnable matrix scales together with the problem size.\nIntegration-based Methods NeurASP [28], Deepstochlog [26] and other similar works propose methods that either uses symbolic reasoning engines to drive neural network models or vice versa. Recent works that combine LLMs and reasoning such as [17] work similarly.\nInvariance in Deep Learning An increasing amount of work have started to focus on the permutation invariance inherent in the problem domain, leading to an increase in performance [14] [12] [23]. While most works focus on spatial invariance, this work focuses primarily on the invariance within the semantics."}, {"title": "Background", "content": "Normal Logic Program for State Transitions A normal logic program for state transitions (NLP) P is a set of logical state transition rules R that are of the following form:\nR: A(t+1) \u2190 A1(t) \u2227\u2026\u2227 Am(t)\u2227\u00acAm+1(t)\u2227\u2026\u2227\u00acAn(t) (1)\nwhere A(t + 1) is the head of the rule h(R) and everything to the right of \u2190 is known as the body b(R). This represents a rule where A(t + 1) is true if and only if b+(R) = {A1(t),..., Am(t)} are true and b\u00af(R) = {Am+1(t),..., An(t)} are false. If we consider a system with a set of variables {A, A1, ..., Am, Am+1, . . ., An }, and A(t) represents the state of the atom A at time t, then the above is also a dynamic rule. Thus, the above rule can be described in plain English as, the state of the variable A at time t + 1 is true if and only if the state of A1,..., Am is true and Am+1,..., An is false at time t.\nNote however, that even though the rule has time arguments for each atom, t + 1 only appears in the head while t only appears in the body. Therefore, we can also equally express it with the following propositional rule:\nA \u2190 A\u2081\u2227\u2026\u2227Am\u2227\u00acAm+1\u2227\uff65\uff65\uff65\u2227\u00acAn\nIn this case, A can be any of A1, ..., An and will not be considered as a cyclic rule because of the implicit time parameter attached to the atom.\nLFIT Given an NLP P of such propositional rules, we can simulate the state transition of a dynamical system with the Tp operator.\nAn Herbrand base B represents all variables involved within a dynamic system. An Herbrand interpretation I is a subset of the B representing a snapshot of the state of the dynamic system. For an NLP P and an Herbrand interpretation I, the immediate consequence operator (or Tp operator) is the mapping Tp : 2B \u2192 2B:\nTp(I) = {h(R) | R \u2208 P, b\u207a(R) \u2286 I, b\u00af(R) \u2229 I = 0}. (2)"}, {"title": "Proposed Method", "content": "The loss function for SLFIT+ is not sufficiently smooth and thus very difficult to optimize. This is mainly caused by the non-continuous inputs of l and v that change the meaning of the output nodes. In particular, with the same set of state transitions, the neural network model needs to learn different sets of rules depending on I and v. Another issue was the permutation of the variables in the dynamic system. While &LFIT+ utilized Set Transformer to ensure the invariance in the ordering of the input of state transitions, the ordering of the variables within the state itself still causes the model to output different results.\nIn this section, we describe our proposed method SLFIT2 which addresses the issues mentioned above."}, {"title": "Variable Assignment Invariance", "content": "In 8LFIT+, the state transitions are tokenized based on the lexicographical ordering of the variable names. However, while variable names in real"}, {"title": "Dynamic Rule Heads", "content": "In 8LFIT+, an \"output node reuse\" technique was employed to address the scalability issue. The naive way is to assign an output node for all possible minimal rules, which leads to 3n number of nodes. This grows very quickly as n gets larger. To address this, the rules are partitioned by body length and the neural network is constructed such that it covers the body length that consists of the most number of rules. For example, for Herbrand base with 3 variables, the body length that consists of the most number of rules is 2 with 12 such rules. The corresponding rules and the position of output nodes assigned is shown in table 2. Some output nodes utilized more (node 0 is used 4 times) and some less (nodes 8-11 are only used once), leading to an imbalance in the training data.\nIn SLFIT2, we construct separate output layers for each of the body lengths and dynamically load them into memory when required. This means that while the other layers are shared between each body length, the final layer is constructed separately. By dynamically loading and unloading the final layer, the memory usage of the model will not increase, even compared to that of 8LFIT+.\nIn addition to each of the rules, the final layer also includes one extra output node, which indicates that there is no rule applicable for that"}, {"title": "Overview of LFIT2", "content": "The overall of 8LFIT2 from the input of state transitions to the output of the probability of each rule is depicted in Figure 1. Contrasting to SLFIT+ in Figure 2, we can see that the core neural network model now only has to be concerned with 1 input, which is the transformed set of state transitions Sv.\nThe model architecture of SLFIT2 consists of an embedding layer, which converts the tokens of the elements of Su, into a sequence of embedding vectors. The sequence of embedding vectors is then fed into the Set Transformer, which produces a latent vector. We utilize both the encoder and the decoder part of the Set Transformer. The latent vector is then fed through several feed forward layers and finally, the result is fed to the final layer which then produces the probability of each rule being present or not.\nIn summary, 8LFIT2 takes as input the head of the rule v, the body length I and the observed state transitions S, then outputs a set of rules R of body length I and has v as the head that partly explains S. So do we have to decide v and l? The answer is that we do not. We would iterate v over the entire Herbrand base and I from 0 to the maximum body length, which is the number of variables in the Herbrand base. The rules predicted for each v and I are then specialized if they subsume each other and then combined into a final set of rules P which is then the logic program that explains S."}, {"title": "Scaling to Larger Systems", "content": "Due to having to enumerate all possible rules at the output, 8LFIT2 suffers from scalability issues in the architecture and the spatial dimension. This is in contrast to symbolic methods, where the scalability issue is in the time dimension.\nA SLFIT2 model is trained for a fixed number of variables. This means that a model trained on 3 variables cannot be directly applied to learn a system with 5 variables that may have a rule with a body length of 5. However, especially in real world scenarios, rules that involve a large number of variables are incredibly rare. This is because a rule that involves many variables will remain inactive for most of the time. In the PyBoolNet repository [15], even a large system such as grieco_mapk, a real world system from [9], that has 53 variables, the longest rule only involves 10 variables.\nTo apply 8LFIT2 to a larger system, consider the subsets of the Herbrand base of size n, where n is the number of variables that the SLFIT2 model is trained for. For each of these subsets, we can define a mapping function from the subset to an index. This index is then used as the variable name for the SLFIT2 model. Once we obtain the rules, we can use the inverse of the mapping function to map from the index to the original variables.\nBy using the assumptions of the synchronous semantics, where every state deterministically only transitions to one state, it is possible to discard subsets of variables that produce transitions to different states. If all subsets for a particular variable are discarded, we know that the body length must be larger than the length of the subset. This can be used to determine whether a SLFIT2 model trained on a larger system is required when the maximum body length is unknown."}, {"title": "Experiments", "content": "To verify the effectiveness of the above proposed improvements, we report the results of the experiments on various dynamic systems in this section.\nDatasets The boolean networks used in the experiments are taken from the PyBoolNet repository [15]. The boolean networks in the repository contain both synthetic systems and real world systems. The boolean networks are converted into logic programs, and state transitions are obtained by applying the Tp operator on all possible states 2B. The training dataset is randomly generated as described in Section 4.3.\nExperimental Methods We use the Mean Squared Error (MSE) as the evaluation metric. Given the original program P and the predicted program P', every possible interpretation of the Herbrand base 2B, the MSE is calculated by the difference between the next states provided by the original program {Tp(I) | I \u2208 2B} and the next states provided by the predicted program {Tp' (I) | I \u2208 2B}. This metric is chosen because 2 different programs can generate the same exact state transitions. Therefore, a direct comparison of the rules will not reflect that both programs are equally valid semantically. All results presented are averaged across 3 runs.\nWe compare our proposed method 8LFIT2 with NN-LFIT [8], 8LFIT [18] and 8LFIT+ [19]. We denote the number of variables n that dLFIT2 is trained on by a superscript SLFIT2n.\nResults The experimental results are shown in Table 3. We trained a 3-variable model noted as SLFIT23. We have also performed experiments with a model trained on 4 variables noted as SLFIT24, however due to time and resource constraints we were not able to fully train the model,"}, {"title": "Conclusion", "content": "In this paper, we proposed a technique that leverages another invariance in the symbolic domain, namely the variable assignment invariance, and showed its effectiveness. We have also improved the architecture of the neural network model to make it easier to optimize. We combined these improvements and proposed a new method named SLFIT2 which greatly improves upon prior methods. We also showed that it is possible to apply SLFIT2 to a larger dynamic system. Future work can consider extending SLFIT2 to various asynchronous semantics and dynamic systems with delay or memory. Extending the SLFIT2 model to first order logic also represents an interesting avenue to explore. We hope this work will inspire more Neuro-Symbolic works that employ techniques to exploit variable name invariance in semantics."}, {"title": "Implementation Details", "content": "The model was implemented in PyTorch 2.1 with Python 3.10. The model architectures are as follows. All models include 3 components, i.e., the Set Transformer encoder, the Set Transformer decoder and a feed-forward layer. The activation function used is ReLU."}, {"title": "SLFIT23", "content": "We did not do any grid search for the following hyperparameters because we were happy with the performance. A smaller model can possibly be constructed.\nSet Transformer Encoder\n\u2022 Input dimensions: 256\n\u2022 Output dimensions: 256\n\u2022 Layers: 3\n\u2022 Number of heads: 8\n\u2022 Number of indices: 64\nSet Transformer Decoder\n\u2022 Input dimensions: 256\n\u2022 Hidden dimensions: 256\n\u2022 Output dimensions: 256\n\u2022 Layers: 1\n\u2022 Number of heads: 8\nFeed-forward Layer\n\u2022 Hidden dimensions: 1024\n\u2022 Layers: 3\nLearning rate: 1 \u00d7 10-4\nWeight decay: 1 \u00d7 10\u22124\nOptimizer: SGD\nDropout: 0.2\nLayer normalization: yes\nTraining data percentage: 95%\nTest data percentage: 0.5%\nValidation data percentage: 4.5%"}, {"title": "SLFIT24", "content": "We did not do any grid search for the following hyperparameters due to the training time required. A more optimized hyperparameter possibly still exists.\nSet Transformer Encoder\n\u2022 Input dimensions: 1024\n\u2022 Output dimensions: 1024\n\u2022 Layers: 5\n\u2022 Number of heads: 8\n\u2022 Number of indices: 64\nSet Transformer Decoder\n\u2022 Input dimensions: 1024\n\u2022 Hidden dimensions: 1024\n\u2022 Output dimensions: 1024\n\u2022 Layers: 3\n\u2022 Number of heads: 8\nFeed-forward Layer\n\u2022 Hidden dimensions: 2048\n\u2022 Layers: 4\nLearning rate: 1 \u00d7 10-5\nWeight decay: 1 \u00d7 10-6\nOptimizer: SGD\nDropout: 0.2\nLayer normalization: yes\nTraining data percentage: 75%\nTest data percentage: 15%\nValidation data percentage: 5%"}, {"title": "Algorithm for Applying to Larger Systems", "content": "Inputs: State transitions S, Herbrand base B, Trained model SLFIT2\"\nOutput: Logic program P\nT = All subsets of 2B with elements equal to n\nfor each v \u2208 B do\nfor each s\u2208T do\n\u03bd = \u03a9(s)\nS = \u03a9(S)\nif Sv is not consistent then\ncontinue \u25b7 // if the states are not consistent after mapping, skip it\nend if\nP = SLFIT2\"(Sv)\nP = \u03a9-1(P)\nP=PUP\nend for\nend for\n\n\u03a9 is the mapping function that maps variables into a new assignment of n variables."}]}