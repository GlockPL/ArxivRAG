{"title": "Quantum Hamiltonian Embedding of Images for Data Reuploading Classifiers", "authors": ["Peiyong Wang", "Casey R. Myers", "Lloyd C. L. Hollenberg", "Udaya Parampalli"], "abstract": "When applying quantum computing to machine learning tasks, one of the first considerations is the design of the quantum machine learning model itself. Conventionally, the design of quantum machine learning algorithms relies on the \"quantisation\" of classical learning algorithms, such as using quantum linear algebra to implement important subroutines of classical algorithms, if not the entire algorithm, seeking to achieve quantum advantage through possible run- time accelerations brought by quantum computing. However, recent research has started questioning whether quantum advantage via speedup is the right goal for quantum machine learning [1]. Research also has been undertaken to exploit properties that are unique to quantum systems, such as quantum contextuality, to better design quantum machine learning models [2]. In this paper, we take an alternative approach by incorporating the heuristics and empirical evidences from the design of classical deep learning algorithms to the design of quantum neural networks. We first construct a model based on the data reuploading cir- cuit [3] with the quantum Hamiltonian data embedding unitary [4]. Through numerical experiments on images datasets, including the famous MNIST and", "sections": [{"title": "1 Introduction", "content": "As a key application area of quantum computing, quantum machine learning [6] has received considerable attention as an area that may achieve a potential quantum advantage compared to classical machine learning/deep learning algorithms through runtime acceleration. The quest for achieving such acceleration has become a standard motivation while developing quantum machine learning algorithms, evidenced by the use of efficient quantum subroutines that could accelerate linear algebra calculations, such as the quantum principal component analysis algorithm (qPCA), which involves calculations of the eigenvalues and eigenvectors of a covariance matrix by quantum phase estimation [7].\nUnlike principal component analysis and kernel methods, which are often referred to as statistical learning algorithms, neural networks, with their ability to discover hidden patterns in large-scale unstructured datasets such as image and natural lan- guage, have gained momnentum since the invention of AlexNet [8], and have become the foundation of modern artificial intelligence applications such as ChatGPT-4 [9]. However, since time complexity is rarely the first priority when designing novel deep neural network architectures, which often rely on intuition and even inspirations from biological neural networks, it becomes less obvious that quantum computing should find any advantage or utility in deep learning and AI. Although recent research has attempted to integrate properties that are unique to quantum systems, such as contex- tuality, into the design of quantum machine learning models for specific types of tasks that could lead to quantum advantage [2], few studies have taken the intuition behind successful deep learning models into account and how to integrate them into quantum machine learning models. In this paper, we aim to bridge this gap by bringing such intuition to the design of quantum machine learning models, especially quantum neu- ral networks, via numerical experiments for the design of a quantum machine learning model for benchmarking image processing tasks.\nOur main contributions in this paper are as follow:\n\u2022 Construction of a quantum classifier based on the quantum Hamiltonian embedding approach and the data reuploading circuit;\n\u2022 Results from numerical experiments show that the proposed model could outperform the baseline quantum convolutional neural network model [5];\n\u2022 Based on the model design process and the numerical experiments, we lay out a set of principles for future quantum machine learning (QML) model design.\nThe results of our paper further emphasise the importance of heuristics during the design of quantum machine learning models, especially heuristics and empirical knowledge that found in the extensive classical deep learning literature.\nThis paper is organised as follows: in the rest of this section, we briefly introduce the relevant research in applying quantum machine learning to image processing, as well as common quantum data embedding approaches. In Section 2, we propose a quantum classification model based on the data reuploading circuit [3] and quantum Hamiltonian embedding method [4]. In Section 3, we demonstrate the effectiveness of this model by evaluating the classification performance on different datasets, including the famous MNIST [10] and FashionMNIST [11] datasets. In Section 4, we discuss the results obtained through numerical experiments which inform our proposed six basic principles for the design of quantum neural networks."}, {"title": "1.1 QML for Image Processing", "content": "As an illustration of quantum neural network (QNN) design, we consider one of the most important tasks in modern artificial intelligence - image processing, including image classification, segmentation, and generation. Since the success of AlexNet [8] at the ImageNet Large-Scale Visual Recognition Challenge 2012 (ILSVRC 2012), deep neural networks, especially deep convolutional neural networks (CNN), have domi- nated image-related tasks. Recently, the vision transformer (ViT) [12] and its variants are trending in image-related tasks due to its structural compatibility with large language models and the potential to build a single unified multi-modal model. An important step in the vision transformer is to cut the image into patches, following the same inductive bias as convolutional neural networks. From the history of image processing with deep neural networks, we can see that there is a central principle in the network architectures designed throughout the years, which is the locality of infor- mation and translation invariance. This is reflected in both the convolution kernels in CNNs and image patches in ViTs.\nIn the quantum context, a number of approaches have been developed as a direct analogue to the classical CNN, namely the quantum convolutional neural network (QCNN) [5], which borrows the idea of localised operators, shared parameters and down-sampling from its classical counterpart. It has been benchmarked for binary classification with classical image data [13, 14] and its effectiveness demonstrated through experiments. Variational circuits other than QCNNs can also be applied to image processing, but often require classical methods to reduce the dimension of the input data [15, 16]. A popular choice is to use a pre-trained classical neural network, such as ResNet [17], to preprocess the original images and extract features [18, 19]. This approach often involves a quantum-classical hybrid neural network, where the output layer of the classical neural network is replaced with a parameterised quantum circuit. However, the necessity of such an approach remains unclear, as the last layer of a classical neural network has a great similarity to logistic regression, which, by itself, is a simple machine learning model. When using a classical neural network for dimension reduction, the \"heavy lifting\" of feature extraction is off-loaded to the classical neural network, and the extracted features are often classified by a simple machine learning model. There is also research that involves the implementation or mimicking of classical"}, {"title": "1.2 Quantum Data Embedding", "content": "One of the most important steps when applying quantum machine learning to classical data is loading the data into the quantum computer. For example, it is difficult to find the classical counterpart of quantum data embedding for the loading of images, where classically images can easily be stored as rank-3 tensors and matrices in computer memory. Appropriate data embeddings are crucial to the success of both classical and quantum machine learning models. Similar data embedding processes, where the original data structure is not suitable to be directly processed by the machine learning model (mostly neural networks), could be found in research and applications that involve graph and natural language data. In each case, the data embedding method, as well as the machine learning model that follows data embedding, need to reflect the intrinsic properties of the data. Such intrinsic properties can sometimes be described simply as translation, rotation, and permutation symmetry [25]. However, most of the time it lies more on a semantic level and is hard to describe via mathematical relations. There are three widely adopted data embedding methods for quantum machine learning [4]:\n\u2022 BASIS EMBEDDING. In basis embedding, a length-n binary string is directly embed- ded as one of the basis states of an n-qubit quantum system by applying Pauli-X operators to the quantum bits that are supposed to encode the classical bit \"1\". For example, to encode the binary bit string '0101' as a quantum state, one only needs to apply Pauli-X gates to the initial state |0000) on the second and fourth qubits:\n$0101\\rangle = X_2 X_4 |0000\\rangle$;"}, {"title": "2 Data Reuploading Classifier with Quantum Hamiltonian Embedding", "content": "In this section we will discuss the quantum neural network classifier model based on the discussions from the previous section. We opt for Hamiltonian image embedding (Section 2.1) for data encoding and the data reuploading classifier [3] (Section 2.2), for both their simplicity in terms of implementation with linear algebra libraries such as JAX [30], and the intuitive connection with classical neural networks for the data reuploading circuit. While looking for quantum advantage in terms of training and inference speedups is a legitimate aim, it is not the primary goal here. Instead, we seek to integrate heuristics from deep learning into quantum machine learning model design."}, {"title": "2.1 Hamiltonian Image Embedding", "content": "As mentioned in the previous section, one of the most important components of a quantum machine learning model is how to embed classical data into the quantum computer. Since we are working with image data, there is preference for data embed- ding methods that preserve two-dimensional structures of images and transform image pixels with the same non-linearity function (activation functions) to avoid unwanted bias on the decision boundaries.\nIn this paper, we adopt the Hamiltonian embedding method [4, 31] for image data encoding. First, we \"Hermitianise\" our square, greyscale, real-valued image matrix $M$ by:\n$H_M = \\frac{M + M^T}{2}$;\nThis is the only classical preprocessing required for our model, in addition to padding the image with zeros for the MNIST and FashionMNIST datasets. Here, the embedding unitary for the input image data is simply the matrix exponentiation of $H_M$:\n$W(t; M) = e^{\\frac{-iH_M t}{2}}$,\nwhere $t$ is a trainable parameter instead of the physical time. If we expand $W(t; M)$ in a Taylor series, we have:\n$W(t; M) = 1 - \\frac{iH_M t}{2^1} - \\frac{H_M^2 t^2}{2! \\times 2^2} + \\frac{i H_M^3 t^3}{3! \\times 2^3} + \\cdots $\nWe can see that by simply time-evolving the (Hermitianised) image, a (matrix) poly- nomial function is applied on the whole image level, bring \"cheapter\" nonlinearity compared to angle embedding with single-parameter rotation gates. Later in Section 3, we demonstrate that with the quantum Hamiltonian embedding approach, our model could outperform QCNN for various datasets. In our model, the Hamiltonian embed- ding of image $M$, parameterised by a single parameter $t$, will act as the data encoding unitary for our quantum machine learning model, which will be discussed in the following subsections."}, {"title": "2.2 Data Re-Uploading", "content": "The data reuploading variational quantum circuit, first proposed in [3], is derived from the guiding principles from classical neural networks that the data is reused multiple times in classical deep neural networks. Variational circuits representing quantum versions of neural networks can be written as (before measurement):\n$|\\psi(x; \\theta)\\rangle = V(\\theta)U(x) |0\\rangle^{\\otimes n}$,\nwhere $V(\\theta)$ are the variational layers parameterised by $\\theta$, and could be absorbed in the measurement observables $O$, becoming $O(\\theta) = V^{\\dagger}(\\theta)OV(\\theta)$:\n$\\langle \\psi(x;\\theta)| O |\\psi(x;\\theta)\\rangle = \\langle 0|^{\\otimes n} U^{\\dagger}(x)V^{\\dagger}(\\theta)OV (\\theta)U(x) |0\\rangle^{\\otimes n} = \\langle 0|^{\\otimes n} U^{\\dagger}(x)O(\\theta)U(x) |0\\rangle^{\\otimes n}$,\n$x$ is the input data, and $U(x)$ is the data encoding unitary, and could be parameterised with some other set of parameters $\\varphi$. In this form, the input data only appears once in the model, while in classical neural networks, one input neuron can be accessed by more than one neuron in the hidden layer. Motivated by this difference, the data reuploading circuit can be written as follows:\n$|\\Psi(x; \\Xi)\\rangle = \\prod_{i=1}^L [V(\\omega_i)U_{\\phi}(x)] |0\\rangle^{\\otimes n}$.\nIn this definition the data encoding unitary $U(x)$, together with the parameterised layer $V$, are repeated $L$ times with the same data encoding unitary but with dif- ferent parameters for $V$, $\\Xi = {\\omega_1,\\omega_2,\\dots,\\omega_L}$. Also it has been proved that data reuploading circuits in principle exhibit a quantum advantage in terms of function approximation [32]."}, {"title": "2.3 The Model", "content": "Combining the data reuploading circuit and Hamiltonian embedding, we have the following quantum machine learning model (prior to measurement, also shown in Fig. 1):\n$|\\gamma(t, \\Omega; M)\\rangle = \\prod_{i=1}^L [V(\\omega_i)W(t_i; M)] |+\\rangle^{\\otimes n}$.\nHere we set the circuit to begin with an equal superposition of all basis states $(|+\\rangle^{\\otimes n})$. There is flexibility in the structure of the parameterised layer $V$. For small datasets, we opt for a parameterised layer composed of SU(4) unitary gates in a brick wall layout with different parameters. Generally, a SU(N) gate, where $N = 2^n$, $n$ being the number of qubits the gate acts on, can be written as:\n$SU(N)(\\Theta) = exp(\\sum_i^m i \\theta_i G_i)$,\nwhere $m = 4^n - 1$ and $G_i \\in {I, X, Y, Z}^{\\otimes n}\\{I^{\\otimes n}}$. $\\Theta = {\\theta_1,\\dots,\\theta_{4^n-1}}$.\nIn our model, the classical data pass through a nonlinear operation first (time evolution), then followed by a parameterised unitary layer. Although this is not common in deep learning practice, where nonlinear operations (activation functions) normally occur after linear and convolutional layers, it could be viewed as a form of pre-activation, which enabled training of a 1001-layer ResNet in [33].\nFor a three-qubit circuit, there could be two different configurations, as shown in Figure 2a. These two different configurations generally do not have a noticeable impact\nFor the classification of $K$- classes, the probability for each label $i \\in {0,1,..., K-1}$ can be obtained by measuring the $[\\log_2 K]$- qubit projection operator $P_i = |i\\rangle \\langle i|$:\n$p(M; i) = \\langle \\phi(t, \\omega; M)| (P_i \\otimes I_{n- \\lceil\\log_2 K\\rceil}) |\\phi(t, \\omega; M)\\rangle$,\nwhere $I_{n- \\lceil\\log_2 K\\rceil}$ is the $(n - \\lceil\\log_2 K\\rceil)$-qubit identity operator, and $n$ is the total number of qubits in the circuit. The loss function for training is the cross-entropy cost function:\n$Cross\\text{-}Entropy Loss (M) = - \\sum_{i=0}^{K-1} y_i \\log_2 p(M; i)$,\nwhere $y_i$ is the true probability of class $i$ for the input $M$, which, in the case of one-hot encoding, is 1 for the true class and 0 for all others. For a more detailed explanation of the cross-entropy function, readers could refer to deep-learning-related textbooks, such as Chapter 5.7 in [34]."}, {"title": "3 Simulation Experiments and Results", "content": "To avoid taking the log of 0, we use Softmax(p(M; i)) to replace p(M; i):\n$Softmax(p(M; i)) = \\frac{e^{p(M;i)}}{\\sum_{k=0}^{K-1} e^{p(M;k)}}$;\nThe purpose of the Softmax function is to convert a real-valued vector to another real-valued vector, but with values in (0,1) and sum to one[34]."}, {"title": "3.1 Baseline Model and Datasets", "content": "Our baseline model for comparison is the quantum convolutional neural network pro- posed in [5]. The structure and implementation of the baseline model follow [35]. For the baseline model, since amplitude embedding is used, all input images were flattened into vectors and padded. The size and number of parameters of the model depend on the size of the input and the number of classes.\nWe trained and tested our model on four different datasets:\nThe Kaggle CT Medical Images dataset\nThis is a small subset of images from [36], obtained from the Kaggle website [37]. This dataset contains 100 CT medical images that have binary labels \"True\" or \"False\" for \"Contrast\". The original dimension of the images is 512 by 512. To reduce the simulation cost, the images were resized to 32 by 32 using the Python package of OpenCV [38]. The resized images were randomly divided into train (80 images) and test (20 images) datasets.\nSubset of the Sklearn digits dataset\nThis data [39] is obtained through the machine learning package \u201cScikit-learn\" [40]. The dimensions of the images are 8 by 8. Only images with labels 0 to 7 (eight classes in total) were sampled when splitting train (1200 images) and test (100 images) datasets.\nSubset of the MNIST dataset\nThe data [10] is obtained through the corresponding data loading module in Torchvi- sion [41]. Only images with labels 0 to 7 (eight classes in total) were sampled when constructing the train datasets (48200 images) and the test datasets (8017 images). The original dimension of the images in the MNIST dataset is 28 by 28. The images were padded to 32 by 32 with zeros.\nSubset of the FashionMNIST dataset\nData [11] is obtained through the corresponding data loading module in Torchvision [41]. Only images with labels 0 to 7 (eight classes in total) were sampled when con- structing the train datasets (48000 images) and the test datasets (8000 images). The"}, {"title": "4 Discussion", "content": "In the previous sections we designed a quantum neural network model based on the data reuploading circuit [3], using the quantum Hamiltonian embedding [4] approach as the data encoding unitary, demonstrating that our model could achieve reasonably better performance than the well-known QCNN model without extensive architecture and hyperparameter search on multiple datasets, or dedicated pre-trained variational circuits to approximate quantum-embedded classical images [43].\nIt should also be noted that our numerical experiments use larger datasets com- pared to previous quantum machine learning research, since data scaling is also an important research question in different areas of deep learning, such as large language models [44, 45]. It is common for machine learning models that have good performance on a small subset of common datasets, such as MNIST and FashionMNIST, to per- form badly on a larger scale, both in terms of number of labels and number of datum in each label.\nIn this section we point out similarities between our model and classic neural network designs. We followed heuristic techniques when choosing the data embedding methods and the structure of the QNN model. In this section we will dive into the details of these heuristics and propose six essential principles for quantum neural network design to inspire future research in this direction."}, {"title": "4.1 Resemblances to Classical Neural Network Design", "content": "Possible connection to pre-activation in classical neural networks: In [33] a modified version of the original ResNet [17], the activation function (ReLU), was placed before the convolutional layers (and after the batch normalisation layer). This modification [33] has been shown to increase the trainability of a 1000-layer ResNet and reduce overfitting. In the proposed model presented here, we can see from the expansion of the Hamiltonian embedding in Eqn. 6 that our training data first go through a non-linear transform, then a parameterised layer. This implicit transform of the input data could be the reason that during training, the proposed model did not suffer from any obvious barren plateau with different random initialisations. However, as this stage this is just a conjecture and still requires further investigation.\nPossible connection to the Gated Linear Unit [46]: Generally, the Gated Linear Unit (GLU) follows the form\n$GLU(x) = f(x) \\cdot \\sigma(g(x))$,\nwhere both $f$ and $g$ are linear transformations (such as the linear layer in an MLP or the convolution layer in a CNN), $\\sigma$ is usually a non-linear activation function, such as ReLU or the tanh function. Recall the mathematical form of our model from Eqn. 10:\n$|\\gamma(t, \\Omega; M)\\rangle = \\prod_{i=1}^L [V(\\omega_i)W(t_i; M)] |+\\rangle^{\\otimes n}$,\nwhich can be rewritten as:\n$v(x;t, V) = \\prod_{i=1}^N [V_i \\cdot \\sigma_{t_i}(x)] v_0$,\nwhere $V_i \\cdot \\sigma_{t_i}(x)$ can be viewed as a gated linear unit parameterised by weight matrix $V_i$ and parameter $t_i$. Although it is hard to say that there is a one-to-one correspondence between GLU and the proposed model, the implicit data transform in the quantum Hamiltonian embedding unitary could contribute to the superior performance of our model compared to other QCNN schemes.\nWe observed a performance degradation in the proposed model occurring when the complexity of the data increases (Sklearn digits \u2192 MNIST \u2192 FashionMNIST). An intuitive explanation is that the images in the FashionMNIST dataset contain more details than those in the MNIST dataset. Most pairs of MNIST digits have been shown to be well distinguished by using just a single pixel [11]. Our result for the FashionMNIST brings us close to the performance achieved by classical machine learning algorithms on the benchmark provided in [11]. Since we did not extensively search for better structures, there is ample room for improvement when it comes to model architecture. For example, our model encodes the entire image as a whole, whereas modern deep learning practice show that even without convolution, we should still divide the image into patches, allowing the model to focus more on local spatial features, such as the embedding layer in the vision transformer [47]."}, {"title": "4.2 Principles of Quantum Machine Learning Model Design", "content": "In this paper, we assume that, as with their classical counterparts, quantum machine learning with quantum neural network (QNN) models also need to process input data"}]}