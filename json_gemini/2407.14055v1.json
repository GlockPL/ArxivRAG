{"title": "Quantum Hamiltonian Embedding of Images for Data Reuploading Classifiers", "authors": ["Peiyong Wang", "Casey R. Myers", "Lloyd C. L. Hollenberg", "Udaya Parampalli"], "abstract": "When applying quantum computing to machine learning tasks, one of the first considerations is the design of the quantum machine learning model itself. Conventionally, the design of quantum machine learning algorithms relies on the \"quantisation\" of classical learning algorithms, such as using quantum linear algebra to implement important subroutines of classical algorithms, if not the entire algorithm, seeking to achieve quantum advantage through possible runtime accelerations brought by quantum computing. However, recent research has started questioning whether quantum advantage via speedup is the right goal for quantum machine learning [1]. Research also has been undertaken to exploit properties that are unique to quantum systems, such as quantum contextuality, to better design quantum machine learning models [2]. In this paper, we take an alternative approach by incorporating the heuristics and empirical evidences from the design of classical deep learning algorithms to the design of quantum neural networks. We first construct a model based on the data reuploading circuit [3] with the quantum Hamiltonian data embedding unitary [4]* Through numerical experiments on images datasets, including the famous MNIST and", "sections": [{"title": "1 Introduction", "content": "As a key application area of quantum computing, quantum machine learning [6] has received considerable attention as an area that may achieve a potential quantum advantage compared to classical machine learning/deep learning algorithms through runtime acceleration. The quest for achieving such acceleration has become a standard motivation while developing quantum machine learning algorithms, evidenced by the use of efficient quantum subroutines that could accelerate linear algebra calculations, such as the quantum principal component analysis algorithm (qPCA), which involves calculations of the eigenvalues and eigenvectors of a covariance matrix by quantum phase estimation [7].\nUnlike principal component analysis and kernel methods, which are often referred to as statistical learning algorithms, neural networks, with their ability to discover hidden patterns in large-scale unstructured datasets such as image and natural language, have gained momnentum since the invention of AlexNet [8], and have become the foundation of modern artificial intelligence applications such as ChatGPT-4 [9]. However, since time complexity is rarely the first priority when designing novel deep neural network architectures, which often rely on intuition and even inspirations from biological neural networks, it becomes less obvious that quantum computing should find any advantage or utility in deep learning and AI. Although recent research has attempted to integrate properties that are unique to quantum systems, such as contextuality, into the design of quantum machine learning models for specific types of tasks that could lead to quantum advantage [2], few studies have taken the intuition behind successful deep learning models into account and how to integrate them into quantum machine learning models. In this paper, we aim to bridge this gap by bringing such intuition to the design of quantum machine learning models, especially quantum neural networks, via numerical experiments for the design of a quantum machine learning model for benchmarking image processing tasks.\nOur main contributions in this paper are as follow:\n\u2022 Construction of a quantum classifier based on the quantum Hamiltonian embedding approach and the data reuploading circuit;\n\u2022 Results from numerical experiments show that the proposed model could outperform the baseline quantum convolutional neural network model [5];\n\u2022 Based on the model design process and the numerical experiments, we lay out a set of principles for future quantum machine learning (QML) model design."}, {"title": "1.1 QML for Image Processing", "content": "As an illustration of quantum neural network (QNN) design, we consider one of the most important tasks in modern artificial intelligence - image processing, including image classification, segmentation, and generation. Since the success of AlexNet [8] at the ImageNet Large-Scale Visual Recognition Challenge 2012 (ILSVRC 2012), deep neural networks, especially deep convolutional neural networks (CNN), have domi-nated image-related tasks. Recently, the vision transformer (ViT) [12] and its variants are trending in image-related tasks due to its structural compatibility with large language models and the potential to build a single unified multi-modal model. An important step in the vision transformer is to cut the image into patches, following the same inductive bias as convolutional neural networks. From the history of image processing with deep neural networks, we can see that there is a central principle in the network architectures designed throughout the years, which is the locality of information and translation invariance. This is reflected in both the convolution kernels in CNNs and image patches in ViTs.\nIn the quantum context, a number of approaches have been developed as a direct analogue to the classical CNN, namely the quantum convolutional neural network (QCNN) [5], which borrows the idea of localised operators, shared parameters and down-sampling from its classical counterpart. It has been benchmarked for binary classification with classical image data [13, 14] and its effectiveness demonstrated through experiments. Variational circuits other than QCNNs can also be applied to image processing, but often require classical methods to reduce the dimension of the input data [15, 16]. A popular choice is to use a pre-trained classical neural network, such as ResNet [17], to preprocess the original images and extract features [18, 19]. This approach often involves a quantum-classical hybrid neural network, where the output layer of the classical neural network is replaced with a parameterised quantum circuit. However, the necessity of such an approach remains unclear, as the last layer of a classical neural network has a great similarity to logistic regression, which, by itself, is a simple machine learning model. When using a classical neural network for dimension reduction, the \"heavy lifting\" of feature extraction is off-loaded to the classical neural network, and the extracted features are often classified by a simple machine learning model. There is also research that involves the implementation or mimicking of classical"}, {"title": "1.2 Quantum Data Embedding", "content": "One of the most important steps when applying quantum machine learning to classical data is loading the data into the quantum computer. For example, it is difficult to find the classical counterpart of quantum data embedding for the loading of images, where classically images can easily be stored as rank-3 tensors and matrices in computer memory. Appropriate data embeddings are crucial to the success of both classical and quantum machine learning models. Similar data embedding processes, where the original data structure is not suitable to be directly processed by the machine learning model (mostly neural networks), could be found in research and applications that involve graph and natural language data. In each case, the data embedding method, as well as the machine learning model that follows data embedding, need to reflect the intrinsic properties of the data. Such intrinsic properties can sometimes be described simply as translation, rotation, and permutation symmetry [25]. However, most of the time it lies more on a semantic level and is hard to describe via mathematical relations. There are three widely adopted data embedding methods for quantum machine learning [4]:\n\u2022 BASIS EMBEDDING. In basis embedding, a length-n binary string is directly embed-ded as one of the basis states of an n-qubit quantum system by applying Pauli-X operators to the quantum bits that are supposed to encode the classical bit \"1\". For example, to encode the binary bit string '0101' as a quantum state, one only needs to apply Pauli-X gates to the initial state |0000) on the second and fourth qubits:\n$0101 \\rangle = X_2X_4 |0000\\rangle;$\n\u2022 ANGLE EMBEDDING. In angle embedding, classical floating-point data is embedded as rotation angles of parameterized quantum gates, such as the Pauli rotation gates Rx, Ry and Rz. For $x = (x_1, x_2)^T$, one could use angle embedding to encode x as:\n$x\\rightarrow R_x(x_1)R_z(x_2) |0\\rangle;$\n\u2022 AMPLITUDE EMBEDDING. In amplitude embedding, the normalised padded data vector $x = (x_0,x_1,\\cdots,x_{2^N-1})^T$ is embedded as a quantum state of an N- qubit system with real amplitudes:\n$x \\rightarrow |x\\rangle = \\sum_{i=0}^{2^N-1} x_i|i\\rangle$.\nThere are also special embedding methods designed for image data, such as the flexible representation of quantum images (FRQI) [26, 27], which embeds the data in a quantum state that takes spatial information into account. Since these methods still require specific amplitudes for the embedded quantum states, they could be viewed as an extension of the amplitude embedding method. To make amplitude embed-ding feasible with current quantum hardware, several approximate heuristic-based state preparation methods have been proposed, such as the GASP algorithm [28], in which the authors applied genetic algorithms to discover relatively low-depth quan-tum circuits for approximate state preparation, as well as the variational circuit-based approach to approximately prepare the FRQI states shown in [29].\nIn this paper, we combine the quantum Hamiltonian embedding method, described in Section 2.1, and the data reuploading approach, described in Section 2.2, for image classification tasks. Compared to angle embedding, embedding the image as a Hamil-tonian puts the pixels in the image on a more equal footing, meaning that they go through the same type of mathematical operation. Also, the matrix representation of a quantum Hamiltonian for a qubit system is naturally \"two-dimensional\", in the sense that it has the same shape as an (grey-scale) image, making it more suitable for modelling images."}, {"title": "2 Data Reuploading Classifier with Quantum Hamiltonian Embedding", "content": "In this section we will discuss the quantum neural network classifier model based on the discussions from the previous section. We opt for Hamiltonian image embedding (Section 2.1) for data encoding and the data reuploading classifier [3] (Section 2.2), for both their simplicity in terms of implementation with linear algebra libraries such as JAX [30], and the intuitive connection with classical neural networks for the data reuploading circuit. While looking for quantum advantage in terms of training and inference speedups is a legitimate aim, it is not the primary goal here. Instead, we seek to integrate heuristics from deep learning into quantum machine learning model design."}, {"title": "2.1 Hamiltonian Image Embedding", "content": "As mentioned in the previous section, one of the most important components of a quantum machine learning model is how to embed classical data into the quantum computer. Since we are working with image data, there is preference for data embed-ding methods that preserve two-dimensional structures of images and transform image pixels with the same non-linearity function (activation functions) to avoid unwanted bias on the decision boundaries.\nIn this paper, we adopt the Hamiltonian embedding method [4, 31] for image data encoding. First, we \"Hermitianise\" our square, greyscale, real-valued image matrix M by:\n$H_M = \\frac{M + M^T}{2}$\nThis is the only classical preprocessing required for our model, in addition to padding the image with zeros for the MNIST and FashionMNIST datasets. Here, the embedding unitary for the input image data is simply the matrix exponentiation of HM:\n$W(t; M) = e^{\\frac{-iH_Mt}{2}},$\nwhere t is a trainable parameter instead of the physical time. If we expand W(t; M) in a Taylor series, we have:\n$W(t; M) = 1 - \\frac{iH_Mt}{2^1} - \\frac{H_M^2 t^2}{2! \\times 2^2} - \\frac{iH_M^3 t^3}{3! \\times 2^3} + \\cdots$\nWe can see that by simply time-evolving the (Hermitianised) image, a (matrix) poly-nomial function is applied on the whole image level, bring \"cheapter\" nonlinearity compared to angle embedding with single-parameter rotation gates. Later in Section 3, we demonstrate that with the quantum Hamiltonian embedding approach, our model could outperform QCNN for various datasets. In our model, the Hamiltonian embed-ding of image M, parameterised by a single parameter t, will act as the data encoding unitary for our quantum machine learning model, which will be discussed in the following subsections."}, {"title": "2.2 Data Re-Uploading", "content": "The data reuploading variational quantum circuit, first proposed in [3], is derived from the guiding principles from classical neural networks that the data is reused multiple times in classical deep neural networks. Variational circuits representing quantum versions of neural networks can be written as (before measurement):\n$|\\psi(x; \\theta)\\rangle = V(\\theta)U(x) |0\\rangle^{\\otimes n},$\nwhere $V(\\theta)$ are the variational layers parameterised by $\\theta$, and could be absorbed in the measurement observables O, becoming $O(\\theta) = V^{\\dagger}(\\theta)OV(\\theta)$:\n$\\langle \\psi(x;\\theta)| O |\\psi(x;\\theta)\\rangle = \\langle 0|^{\\otimes n} U^{\\dagger}(x)V^{\\dagger}(\\theta)OV (\\theta)U_\\varphi(x) |0\\rangle^{\\otimes n} = \\langle 0|^{\\otimes n} U^{\\dagger}(x)O(\\theta)U_\\varphi(x) |0\\rangle^{\\otimes n},$\nwhere x is the input data, and $U_\\varphi(x)$ is the data encoding unitary, and could be parameterised with some other set of parameters $\\varphi$. In this form, the input data only appears once in the model, while in classical neural networks, one input neuron can be accessed by more than one neuron in the hidden layer. Motivated by this difference, the data reuploading circuit can be written as follows:\n$|\\psi(x; \\vec{\\omega})\\rangle = \\prod_{i=1}^{L} [V (\\omega_i)U_{\\varphi}(x)] |0\\rangle^{\\otimes n}.$\nIn this definition the data encoding unitary $U_\\varphi(x)$, together with the parameterised layer V, are repeated L times with the same data encoding unitary but with different parameters for V, $\\vec{\\omega} = {\\omega_1,\\omega_2, \\dots , \\omega_L\\}$. Also it has been proved that data reuploading circuits in principle exhibit a quantum advantage in terms of function approximation [32]."}, {"title": "2.3 The Model", "content": "Combining the data reuploading circuit and Hamiltonian embedding, we have the following quantum machine learning model (prior to measurement, also shown in Fig. 1):\n$|y(t, \\vec{\\omega}; M)\\rangle = \\prod_{i=1}^{L} [V (\\omega_i)W(t_i; M)] |+\\rangle^{\\otimes n}.$\nHere we set the circuit to begin with an equal superposition of all basis states ($|+\\rangle^{\\otimes n}$). There is flexibility in the structure of the parameterised layer V. For small datasets, we opt for a parameterised layer composed of SU(4) unitary gates in a brick wall layout with different parameters. Generally, a SU(N) gate, where N = $2^n$, n being the number of qubits the gate acts on, can be written as:\n$SU(N)(\\theta) = exp(\\frac{-i}{\\sqrt{2}} \\sum_{i=1}^{m} \\theta_i G_i),$"}, {"title": "4 Discussion", "content": "In the previous sections we designed a quantum neural network model based on the data reuploading circuit [3], using the quantum Hamiltonian embedding [4] approach as the data encoding unitary, demonstrating that our model could achieve reasonably better performance than the well-known QCNN model without extensive architecture and hyperparameter search on multiple datasets, or dedicated pre-trained variational circuits to approximate quantum-embedded classical images [43].\nIt should also be noted that our numerical experiments use larger datasets com-pared to previous quantum machine learning research, since data scaling is also an important research question in different areas of deep learning, such as large language models [44, 45]. It is common for machine learning models that have good performance on a small subset of common datasets, such as MNIST and FashionMNIST, to per-form badly on a larger scale, both in terms of number of labels and number of datum in each label.\nIn this section we point out similarities between our model and classic neural network designs. We followed heuristic techniques when choosing the data embedding methods and the structure of the QNN model. In this section we will dive into the details of these heuristics and propose six essential principles for quantum neural network design to inspire future research in this direction."}, {"title": "4.1 Resemblances to Classical Neural Network Design", "content": "Possible connection to pre-activation in classical neural networks: In [33] a modified version of the original ResNet [17], the activation function (ReLU), was placed before the convolutional layers (and after the batch normalisation layer). This modification [33] has been shown to increase the trainability of a 1000-layer ResNet and reduce overfitting. In the proposed model presented here, we can see from the"}, {"title": "4.2 Principles of Quantum Machine Learning Model Design", "content": "In this paper, we assume that, as with their classical counterparts, quantum machine learning with quantum neural network (QNN) models also need to process input data"}]}