{"title": "Do LLMs exhibit demographic parity in responses to queries about Human Rights?", "authors": ["Rafiya Javed", "Jackie Kay", "David Yanni", "Abdullah Zaini", "Anushe Sheikh", "Maribeth Rauh", "Ramona Comanescu", "Iason Gabriel", "Laura Weidinger"], "abstract": "This research describes a novel approach to evaluating hedging behaviour in large language models (LLMs), specifically in the context of human rights as defined in the Universal Declaration of Human Rights (UDHR). Hedging and non-affirmation are behaviours that express ambiguity or a lack of clear endorsement on specific statements. These behaviours are undesirable in certain contexts, such as queries about whether different groups are entitled to specific human rights; since all people are entitled to human rights. Here, we present the first systematic attempt to measure these behaviours in the context of human rights, with a particular focus on between-group comparisons. To this end, we design a novel prompt set on human rights in the context of different national or social identities. We develop metrics to capture hedging and non-affirmation behaviours and then measure whether LLMs exhibit demographic parity when responding to the queries. We present results on three leading LLMs and find that all models exhibit some demographic disparities in how they attribute human rights between different identity groups. Futhermore, there is high correlation between different models in terms of how disparity is distributed amongst identities, with identities that have high disparity in one model also facing high disparity in both the other models. While baseline rates of hedging and non-affirmation differ, these disparities are consistent across queries that vary in ambiguity and they are robust across variations of the precise query wording. Our findings highlight the need for work to explicitly align LLMs to human rights principles, and to ensure that LLMs endorse the human rights of all groups equally.", "sections": [{"title": "1 Introduction", "content": "Language is inherently value-laden and whether intentionally or not, shapes our judgement within a socially complex world [28]. Thus, by definition, outputs from Large Language Models (LLM) just like all other linguistic utterances \u2013 inevitably express value judgements. This places particular importance on aligning the outputs of these models to what is considered desirable and fair [23]. In fact, LLMs interacting with user queries have been shown to share partisan views across multiple domains and contexts [45], [19]. In addition, there is a rich literature of examples whereby LLMs express values that are biased, factually incorrect, or unsafe and where explicit alignment to values is needed to address these issues [9], [22].\nGiven this, there is ongoing debate about which values LLMs should express. The 1948 Universal Declaration of Human Rights (UDHR) is one document which holds broad cross-cultural agreement and has been invoked by peoples seeking fair treatment and justice around the world [41], [39]. As Prabhakaran et al. note, while there are valid critiques of the UDHR, including its colonial legacy, the doctrine of human rights has also been used \"in an opposing manner to resist authoritarian regimes and in anticolonial movements to oppose external intervention\" [40].\nRegulatory bodies, governments, LLM developers' own policies, and users themselves expect LLM outputs to respect universal human rights [5], [39], [3], [40], [2], [4]. In the literature, human rights are widely invoked as a key tenet underpinning Responsible AI and accountability [41], [7], [34]. But while there is consensus on human rights as a basic pillar of responsible AI, no systematic evaluation of model outputs exists to probe the endorsement of human rights of different groups. Yet, evaluation is critical to ensure that LLMs behave in line with this consensus as expected. Evaluation can help"}, {"title": "2 Related Work", "content": "To close this gap, here we provide an evaluation of LLM endorsement of human rights. In particular, we focus on the assertion made in the first article of the UDHR: that human rights apply equally to all people. In other words, not only does the UDHR define the rights that should be afforded to all people, but it also asserts parity in their affordance. To assess alignment to this, we measure demographic parity in LLM responses to human rights queries in the context of different demographic identities.\nTo this end, we identify a set of group identities and UDHR articles and combine these in a novel prompt dataset that queries human rights in the context of different identity groups (i.e. \"Should [identity group] have the right to a fair and public hearing?\", Article 10 UDHR, full prompt set see Table 1). We further define two novel metrics to capture LLM endorsement of human rights: hedging and non-affirmation. Both metrics indicate a lack of clear endorsement of human rights. We then compare LLM endorsement of human rights between different identity groups, using the established fairness concept of demographic parity. We evaluate and report results on three leading LLMs: Gemini 1.5, Claude 3 Sonnet, and GPT-40.\nThe UDHR addresses human rights challenges presented by the reconstruction of states, the decolonization process, and the redrawing of national boundaries [18]. These contexts continue to be highly relevant today, which is why we specifically select identities associated with recent military conflicts as well as self-determination movements.\nTo test the robustness of our evaluation, we further conduct two follow-up investigations. First, we study the robustness of our findings to queries that vary in ambiguity, which we also define. Second, we study the robustness of our findings to different rewordings of our base queries, such as negation and contextual-priming. We find that relative disparity between identity groups is robust across these permutations, indicating reliable insights on demographic parity in LLM responses to queries about human rights."}, {"title": "2.1 Evaluating moral reasoning, political leaning, and values in LLMs", "content": "To date, there has been no systematic evaluation of LLM alignment to the UDHR. In terms of related evaluation work, one might point to the growing body of literature and tooling that focuses on measuring political ideology and lean expressed in LLM output [1], [45], [19], [6]. This work suggests that LLMs can be coherently and recognizably aligned to particular political views. Another line of work examines the consistency of LLM outputs over value-laden questions [38], finding that LLM outputs tend to have higher variance on more controversial values. In addition, evaluation research on moral reasoning capacities in LLMs finds that LLMs reflect the same human-like biases in the context of moral and ethical dilemmas [14], [46], [47]. However, neither the work on political lean nor that on moral reasoning considers human rights explicitly.\nFinally, there is relevant normative debate to draw on: Gabriel [11] and Kenton [25] highlight the possibilities of value misalignment, whereby Al systems express values that are at odds with what is expected of them, or what is desirable for the user, a third party, or society at large. Kenton [25] shows how in the context of LLMs, such misalignment can occur intentionally or unintentionally. Importantly, prior work considers who has the right to make decisions about what to embed [11] and how to embed pluralistic values [48], [26]. While this discussion highlights the need for increased fairness and transparency in determining what LLM outputs should express, this is generally not in reference to universally agreed doctrines such as the UDHR."}, {"title": "2.2 Hedging", "content": "We further draw on research in linguistics to identify behaviours that express ambiguity or a lack of clear endorsement. Hedging is a term that in its everyday usage, is more closely related to the behavior we wished to evaluate here. In everyday usage, hedging can refer to \"the act of evading the risk of commitment, especially by leaving open a way of retreat\" [35]. In linguistics and logic, hedges denote fuzzy concepts (those that are neither true nor false) as well as the expressions used to indicate them (strictly speaking, technically speaking, sort of) [27], [37]. Note that also some machine literature uses the term hedging in a related way but with distinct expressions, to capture speaker uncertainty in the response [53]. Similar to past research, we use the term hedging to mean that the response avoids fully committing to a singular yes/no view by referencing an opposing point of view. In addition, given that we exclusively use prompts that aim to elicit yes or no responses, we also use a second metric of non-affirmation to add depth and support to the first metric."}, {"title": "2.3 Disparity measures in fairness and bias evaluations", "content": "Fairness is frequently measured via statistical parity, also known as demographic parity or independence. This asserts that in fair models, group membership (e.g. race, gender) should not be predictive of model outputs [15], [43]. Statistical parity and related techniques like positive predictive value parity have been used to study fairness in Al systems for many years, including in supervised systems prior to the recent rise of generative AI such as LLMs. For example, parity-based metrics have been used to measure algorithmic fairness in various types of predictive systems, including those in healthcare and credit risk [29], [13], [20]. While parity-based metrics have been criticised for failing to take into account relevant context, they are an established reference point in fairness research, and can serve as a first indicator for whether group-based bias may be occurring.\nIn terms of generative models, there are diverse tasks that have been proposed to measure fairness - ranging from those that measure bias in the semantic space (via semantic similarity tasks or entailment prediction) to those that measure the group fairness of properties of generated text (like toxicity and sentiment) [8], [9], [29]. There have also been calls for better metrics for evaluation that correspond most strongly to Realistic Use and Tangible Effects (RUTE) evaluations [32]. One related work measures disparities in conflict reporting and studies some groups in common with our work [24].\nWe adopt a parity-based metric to answer the simple question of whether human rights are endorsed equally in reference to different"}, {"title": "3 Defining the Evaluation on Alignment to Human Rights", "content": "We define a method of evaluating LLM adherence to the UDHR, specifically focusing on group-based parity in LLM assertions of human rights. To this end, we define a set of prompts inquiring about fundamental human rights of the UDHR in the context of different identity groups (Tables 1-2)."}, {"title": "3.1 Selection of Identity Groups", "content": "In order to evaluate LLM alignment to human rights in the context of different identity groups, we select a range of identities, shown in Table 2. We focus on identity groups linked to conflicts and self-determination movements, as these contexts such as the redrawing of national boundaries, state reconstruction, and decolonization - have historically been, and remain, critical moments when human rights require protection [18]. In particular, we identify identity groups directly implicated in highly visible military conflicts in 2024: Ukrainians, Russians, Israelis and Palestinians. In addition to this, we identify self-determination movements involving armed conflict. In particular, we select three such movements that were particularly frequently searched for on Google during 2021-2023 ([16], see Appendix Table 6 for the list of movements and associated search terms). This led us to select Kurds, Tamils, and Kashmiris. We also select the national identity associated with the country from which these groups sought rights, i.e. 'Turkish people', 'Sri Lankans' and 'Indians'.\nWe complement this selection with additional identity groups that are not associated with military conflicts or self-determination movements. These serve as a baseline. To this end, we added the four national identities associated with the highest nominal GDP: the United States, China, Japan, and Germany. [10]. To sidestep confounds due to high vs. low GDP, we further add four national identities associated with nominal GDP comparable to the lowest GDP of any nationality already present in our set (Sri Lanka \u2192 Ghana, Palestine \u2192 Jamaica, Ukraine \u2192 Ethiopia)."}, {"title": "3.2 Query Creation", "content": "We focus on the UDHR articles that pertain to first-generation civil and political rights, specifically articles 2-21 [51]. From these, we further selected 15 articles which emphasise individual rights and use these to create query-templates which are worded to elicit an affirmative response when the right is being affirmed, as shown in Table 1."}, {"title": "2.4 Prompt variations", "content": "LLM evaluations have been criticised for lacking robustness, as LLM responses can be highly sensitive to variations in prompt phrasing and structure [44], [52], [31]. \"Prompt engineering\" has emerged as a set of techniques to exploit this sensitivity [12], [36], [30]. To ensure that our results are not the artifact of the specific wording of our query, we build on these insights by exploring several axes of variation in our queries (while retaining the same meaning). While the space of potential prompt variations is vast, we select a few key axes of variation to explore in generating prompt variants, including negation, forced choice, and contextual priming [21], [50]."}, {"title": "3.3 Definition of Metrics", "content": "In this section, we define two evaluation metrics, hedging and non-affirmation. We further define the base assumption about demographic parity and lay out our method of computing a statistical parity difference. In a second step, we analyse these evaluation results further, by calculating an ambiguity score per query."}, {"title": "3.3.1 Novel metrics", "content": "Hedging: we adopt the definition provided in prior Al research which is also in line with the literature in linguistics and logic, whereby hedging is a lack of commitment. To make hedging manageable further requires an operational definition. To this end, we define hedging as mentioning opposing perspectives to a question. Thus hedging occurs where there is an explicit or implicit reference to an opposing point of view in the response. This includes instances where: the opposing perspective is explicitly presented in equal opposition to the main perspective, the opposing perspective is explicitly mentioned, but minimized, as well as instances where the opposing perspective is only implicitly referenced. Notably, we explicitly exclude categories D-H in Table 4 from our definition (however, including these in the auto-rater reduced false positives).\nNon-Affirmation: we define non-affirmation as the absence of a positive statement. Affirmative statements include \"Yes, this is a fundamental human right\" or (in the case of experiments with rewording the query as negation) a strong negative such as \"It is never OK to deprive someone of their right to security\". Non-affirmative statements are defined as the inverse of the presence of such affirmation."}, {"title": "3.3.2 Fairness metrics", "content": "Following [54], we test for demographic parity as defined below:\nDemographic Parity:\n$P(M = 1|Identity = X) = P(M = 1|Identity = Y)$   (1)\nfor all identity pairs X,Y.\nFor the purpose of benchmarking the disparity between any single identity group and all others, we use the following metric:\nStatistical Parity Difference:\n$SPD = (P(M = 1|Identity = X) - P(M = 1|Identity = Y))$   (2)\nWhere X is the sensitive group and Y is a reference group or privileged group, and M is the metric (hedging or non-affirmation). In this case it is not yet known which groups would be privileged by these metrics, so rather than use a reference group, we use the mean over all groups for the second term."}, {"title": "3.3.3 Ambiguity score of queries", "content": "There are inherent differences in the language of each right that we query and some queries are less specific than others. Furthermore, there are also varying levels of discussion in the literature about the rights themselves [17]. We entertain the possibility that a combination of these factors (ambiguity of language and disputability of the underlying right) may lead to more hedging and non-affirmation in some queries versus others, and we wish to ascertain whether disparity exists on both ends of the spectrum. Thus, we specifically look into how demographic parity compares on more ambiguous human rights queries vs. those that are the least ambiguous. To this end, we first calculate an ambiguity score per query. We do this by first defining a set of high-endorsement identities - identities which in the context of this specific dataset, have consistently below-average rates of hedging and non-affirming responses for every model tested. We then look at per-query hedging and non-affirmation rates for these"}, {"title": "3.4 Evaluation", "content": "3.4.1 Trials. It is known that LLMs with non-zero temperature parameters are non-deterministic. We used the provided non-zero default temperature values (1.0) for each queried API.\nFurthermore, it has also been studied that LLM outputs can be more inconsistent on controversial topics [38]. Therefore for all ensuing experiments, models were queried 5 times per query and identity pair and we compute the average for our analyses.\n3.4.2 Model checkpoints. We used the latest checkpoints available of each model: Claude 3.5 Sonnet (accessed: 10-05-2024, last updated in June 2024), Gemini-1.5-Pro (accessed: 11-12-2024, last updated in September 2024), and GPT-40 (accessed:11-24-2024, last updated on November 20 2024).\n3.4.3 Auto-rater. After defining the set of query and identity pairs as above, we prompted Gemini 1.5 Pro (11/20/2024) to serve as auto-rater or \"LLM as a judge\". The auto-rater provided each label in the same response (full prompt in Table 4). To calibrate this auto-rater, we obtained bespoke human rater judgments on hedging and non-affirmation.\nIt is worth emphasizing that we only consider categories A, B, and C as valid hedging responses and filter out any responses where the auto-rater answers (2) affirmatively but selects a category D-H alongside it. However, specifying these subcategories in the auto-rater prompt significantly reduced the number of false positives."}, {"title": "4 Results", "content": "We ran the benchmarks across all identity-query pairs to establish relative rates of hedging and non-affirmation in evaluated LLMs, as seen in Figure 2. We find rates of hedging between 8-19%, with considerable differences whereby Gemini 1.5 Pro hedged the least and Claude hedged its responses the most. On non-affirmation, we find rates between 2-9%, with GPT-40 providing non-affirmative responses the least often."}, {"title": "4.1 Global results on benchmarks", "content": "All evaluated models show a shared pattern of disparity in how they attribute human rights to different identity groups, (Figure 3). In fact, statistical disparity scores per identity are highly correlated between competitor models - Figure 4 with Pearson's R ranging from 0.70 (Gemini-GPT) to 0.85 (Claude-Gemini) for hedging and ranging from 0.69-0.81 for non-affirmation. The same pattern of disparity is seen across models and in both prompt variation experiments, indicating that these findings are robust."}, {"title": "4.2 Disparity between identity groups", "content": "Those groups which had only negative SPD in all models and both metrics (see Figure 5) were used as high-endorsement identities as defined in 3.3.2 to determine ambiguity score. Recall that these are identities where all models are hedging and affirming less often than on average across all included identities. Based on these inclusion criteria, high-endorsement identities were the Ukrainians, Turkish"}, {"title": "4.3 Prevailing effect of identity across ambiguity score", "content": "The impact of query-rewording on statistical disparity differences is shown in Figure 6. Although baseline rates changed, the patterns of disparity and the groups facing highest disparity remained extremely similar across re-wordings."}, {"title": "4.4 Disparity across prompt variations", "content": "In this work, we introduced two new metrics - hedging and non-affirmation - to quantify the extent to which evaluated LLMs lack clear endorsement of universal human rights as defined in the"}, {"title": "5 Conclusion and discussion", "content": "UDHR. We find greater than expected baseline rates of hedging and non-affirmation on queries about universal human rights in three industry-leading LLMs. We also note similarities between LLMs, with high correlation of disparity scores per identity between models, ranging from 0.70-0.85 for hedging and 0.69-0.81 for non-affirmation. In fact, some identities showing consistently positive (undesirable) SPD across all models, on all metrics.\nFurther, we define an ambiguity score in order to estimate how much of these behaviors may be explainable due to other factors such as an underspecified prompt or lack of consensus about the specific human right featured. We find that the disparity between high-endorsement versus low-endorsement is consistent across nearly all queries, even those that had zero ambiguity for high-endorsement identities. This is evidence that part of the disparity we observe is solely related to identity itself. As a result, we find that across evaluated LLMs, certain identity groups are disproportionately affected by hedging and non-affirmation in the context of their human rights.\nLastly, we find that these results are consistent across query-rewordings such as negation and contextual priming, indicating that these findings are robust to minor changes in surrounding context."}, {"title": "5.1 Implications for fairness", "content": "The developers of each of the evaluated LLMs publicly state their commitment to upholding human rights. However, there is not sufficient research on how this commitment can be evaluated. The fact that no model hedges on human rights queries in less than 8% of cases indicates that this is an important area for measurement. We see similar results in the non-affirmation benchmark, although interestingly, this benchmark is lower than for hedging. One likely reason for this lower rate is that non-affirmation is an easier behavior to avoid. Specifically, in real responses, we find several examples which provide language clearly affirming a human right, while also paying service to arguments against it. Responses like this are not captured by non-affirmation, but are captured by hedging. Nevertheless, the evaluated models fail to affirm on average in around 5% of cases, which is 1 in 20 queries - a large number for a failure mode.\nWe further find that some identity groups are more consistently affected by hedging and non-affirmation of human rights than others. The fact that there is such high (on average, 76%) correlation between how much disparity identities face in different competitor models means that the source of this disparity has a compounding effect. This highlights the need for further research on how common biases arise and what can be done to mitigate those in addition to ad-hoc mitigating specific instances of these issues as they are identified.\nParticularly, the principle that each human is entitled to the same human rights protection is not yet exhibited by these industry-leading LLMs. This may lead to disparities in how identities are represented LLMs, and may inform how LLM users and those reading, for example, LLM-generated texts, may consider and even value different identity groups.\nOne potential line of inquiry into the reason for this result is that shared patterns of disparity amongst competitor models are an indicator that the underlying bias is arising from common data sources. The fact that the greatest disparity involves conflict-associated identities also suggests tension in operationalizing so-called universal values for identities that are, elsewhere in the dataset, surrounded in language of ambivalence. For politicized identities such as these, training data may contain more equivocation, making committed language challenging to produce [49], [33]."}, {"title": "5.2 Limitations and future work", "content": "The selection of identities attempted to capture politicization in one narrow but salient dimension related to self-determination, statelessness, and public awareness. However, politicized identities occur in diverse temporal and geographical contexts involving countless other factors like gender, race, and sexuality. This is evidenced by the fact that in these results, Chinese and American identities, though not selected for their conflict-association, also displayed high rates of hedging and non-affirmation, indicating that these identities may also be highly politicized in the dataset. Operationalizing universal human rights would mean ensuring parity across all necessary dimensions.\nMost important to note is that this work evaluates text responses on explicit excerpts of international human rights frameworks. In theory, it is possible to align on these without providing guarantees about downstream implications of these rights. Future work in fairness on human rights queries should focus on real-use and tangible effects (RUTEed evaluations) given the increasing integration of LLM-powered systems into diverse and far-reaching applications around the world [32]."}, {"title": "A Appendix", "content": "A.1 Selection of identities\nA.2 Sample Responses"}]}