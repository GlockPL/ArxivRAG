{"title": "Toward Large Language Models as a Therapeutic Tool: Comparing Prompting Techniques to Improve GPT-Delivered Problem-Solving Therapy", "authors": ["Daniil Filienko", "Yinzhou Wang", "Caroline El Jazmi", "Serena Xie", "Trevor Cohen", "Martine De Cock", "Weichao Yuwen"], "abstract": "While Large Language Models (LLMs) are being quickly adapted to many domains, including healthcare, their strengths and pitfalls remain under-explored. In our study, we examine the effects of prompt engineering to guide Large Language Models (LLMs) in delivering parts of a Problem-Solving Therapy (PST) session via text, particularly during the symptom identification and assessment phase for personalized goal setting. We present evaluation results of the models' performances by automatic metrics and experienced medical professionals. We demonstrate that the models' capability to deliver protocolized therapy can be improved with the proper use of prompt engineering methods, albeit with limitations. To our knowledge, this study is among the first to assess the effects of various prompting techniques in enhancing a generalist model's ability to deliver psychotherapy, focusing on overall quality, consistency, and empathy. Exploring LLMs' potential in delivering psychotherapy holds promise with the current shortage of mental health professionals amid significant needs, enhancing the potential utility of AI-based and AI-enhanced care services.", "sections": [{"title": "Introduction", "content": "Numerous studies have demonstrated the potential of Large Language Model (LLM) usage in medical applications, ranging from question-answering services to producing medical notes [1, 2]. Deployment of LLMs in AI-supported care services holds the potential to mitigate healthcare costs and broaden access to care. Companies are already offering products that attempt to undertake the roles of administrative and medical clinicians through the use of LLMs. One of the fields that could greatly benefit from additional resources is psychotherapy, with as many as 20% of people worldwide needing mental health care [3] in the context of a global scarcity of mental health professionals [4].\nPreviously, studies have demonstrated the advantages of using relevant and empathetic responses in mental health dialogues [5, 6]. Althoff et al. [5], in particular, showed that more successful human counselors use fewer templated replies and produce varied responses to similar questions. LLMs' ability to generate coherent and contextually appropriate responses may provide an ideal tool for simulating such behavior, though concerns remain about their safety and utility as patient-facing tools. In this paper, we describe a study of LLMs' ability to provide such relevant and empathetic responses in real-time psychotherapy dialogues.\nIn this study, we explore the ability of off-the-shelf LLMs to deliver Problem-Solving Therapy (PST) for family caregivers, targeting common caregiving symptoms, such as fatigue and anxiety, through a dialogue system that provides caregivers the tools to self-monitor symptoms, problem-solve, and take appropriate actions. PST is an effective form of cognitive behavioral therapy with a specific protocol, making it a good test case for LLM performance, and providing the model with specific guidelines to follow.\nWe demonstrate that non-medical LLMs can display a reasonable performance in PST due to their remarkable ability to use the information provided in the prompt to enhance their performance [7]. We adapt an LLM flow that already showed good performance for medical Q&A [1] to medical dialogue generation. We explore the extent to which a general-purpose LLM such as GPT-4 can be improved by methods not involving modifying the model's weights and compare it with a previously developed human-curated rule-based system [8] in the context of PST for family caregivers with standardized patients portrayed by actors. Actors were provided sample responses and narratives describing the personas that they were playing in the interactions with the model to ensure consistent behavior. To compare the LLM performance on this domain-specific task meaningfully, we recruited multiple clinicians to evaluate the dialogues without knowing how the dialogues were created. Hence, our work has two primary objectives. The first is to investigate the use of prompt engineering methods to improve a general-purpose LLM's ability to deliver steps of"}, {"title": "Related Work", "content": "Existing work to improve the accuracy of LLMs for medical applications leverages (1) fine-tuning the model's weights through further training on application-specific data and/or (2) developing novel ways to query the model to trigger a high-quality response by altering the prompt or including relevant information or examples in the prompt, a method called prompt engineering. Since for fine-tuning, the end results are a reflection of the quality of the fine-tuning corpora used, effective fine-tuning typically necessitates expensive manual review and in-domain expert curation to produce meaningful improvements in the model's responses [9], in addition to highly expensive hardware necessary to run fine-tuning algorithms. In the present study, we focus on prompt engineering, a promising option that has been shown to produce comparable improvements without the resource-intensive fine-tuning [1]. To our knowledge, this is the first study on improving the accuracy of LLM delivering PST through prompt engineering [10]\nUsage of Large Language Models as Therapeutic Chatbots. Literature on the usage of LLMs in psychotherapy is nascent. A few published studies have explored LLMs' ability to lead a therapeutic conversation with a user [11, 12, 10]. Fu et al. described utilizing an LLM to augment a human therapist, providing helpful suggestions rather than singularly leading the conversation, i.e., keeping a human in the loop who can detect hallucinations generated by the LLM and increase the controllability of the system [12]. While such a study provides a setup well-suited for current LLMs, our work explores the current limits of the models' performances in fully autonomous settings to explore the potential for full deployment. Wang et al. performed fine-tuning of a GPT-2 model to generate suitable PST responses [10]. However, their work did not explore prompt engineering methods that recently arose to prominence due to their effectiveness with larger models [1], producing results comparable to those of fine-tuned models [13]. A recent study [11] explored zero-shot prompt engineering to facilitate a diagnostic conversation based on the Diagnostic and Statistical Manual of Mental Disorders-5 standard [14]. They explored similar questions to ours, achieving an increase in the model's empathy in responses via the use of zero-shot prompting. In this study, we expand on prompt engineering methods by using recently emerged techniques including Zero-Shot [15], Few-Shot [7], and Zero-Shot Chain-of-Thought [16], which we will refer to as Chain-of-Thought (COT) hereafter.\nZero-shot prompting: in this setup, a model receives only a natural language instruction to perform a task, without prior demonstrations. Relying on the model's pre-trained knowledge, this approach maximizes convenience and potential for robustness, albeit typically being the most ineffective due to the absence of examples that could clarify the task's format or expected output [15]. Few-shot prompting: in few-shot, a model is given a small number of input-output pairs as a guide. This approach enables LLMs to continue generating appropriate output for similar inputs without fine-tuning the model's weights [7]. Optimal example selection is key in few-shot prompting. Brown et al. [7] demonstrated that by incorporating up to eight high-quality examples in prompts, GPT-3's performance on various natural language benchmarks significantly improved [7]. Chain-of-Thought (CoT): CoT leads LLMs through a step-by-step reasoning process, effectively encouraging them to \"think out loud\". Although the specific style can vary without drastically affecting performance, its presence is crucial for improving problem-solving capabilities [16]. This technique encourages the model to consider intermediate tokens, leading to more robust responses. CoT has been shown to significantly boost the problem-solving performance of LLMs across complex tasks, from arithmetic to commonsense challenges [16]."}, {"title": "Methods", "content": "We used various prompt engineering techniques and their combinations to improve LLM performance when delivering a PST session as a therapist bot. We were guided by findings from Nori et al. who showed that by utilizing various standard prompt engineering techniques, a generalist model, in particular state-of-the-art GPT-4, can respond to medical questions with accuracy on par with models specifically fine-tuned for this task [1]. We utilize the same methods and examine whether their insights can be applied to psychotherapy. While in this study we used GPT-4 through Microsoft Azure's Application Programming Interface (API), at the time of writing, GPT-4 is also available to a wide audience as the backend of OpenAI's ChatGPT conversational agent.\nIn preliminary experiments, our team evaluated multiple prompting strategies in delivering PST and found large inconsistencies in expected model behavior. Models produced at times low quality or generally incoherent responses"}, {"title": "Designing the prompts", "content": "We started with a naive prompt shown in Table 1 and used it as a baseline. We then included the three prompt engineering techniques. We also experimented with combinations of these techniques for better downstream performance."}, {"title": "Model Development", "content": "We developed the models gradually, each step aimed at mitigating the faults noticed at the previous stage (Table 2). We produced models of various complexity, which we then evaluated for their ability to achieve the intended objectives.\nModel 0: A baseline model, referred to as Model 0, introduces the chatbot in its role of guiding caregivers through step 3, \"Identify and Assess Symptom,\" and step 4, \"Goal Setting,\" following the principles of PST. Model 1 and Model 2: Expanding on Model 0, Models 1 and 2 improve the chatbot's ability to follow PST structure by introducing key prompt engineering techniques. Model 1 builds upon Model 0 by integrating a set of structured guidelines into the prompt via zero-shot prompting. This approach systematically guides the chatbot into assessing caregiver's challenges and stressors in a specific order. Model 2 further advances Model 0 through the integration of few-shot learning, which leverages selected single-turn and multi-turn example dialogues from previously recorded sessions to set guidelines for the conversation flow. Model 3 through 6: In our further explorations, inspired by the methodology of Nori et al. [1], we utilize various combinations of prompt engineering techniques across Models 3 to 6. Model 3 integrates zero-shot's structured symptom assessment with few-shot learning to produce more robust output that more closely follows the intended response style. Model 4 incorporates CoT prompting to improve the models' problem-solving results. Models 5 and 6 adjust these components to explore their combined effects on the chatbot's PST performance, with a focus on more effective goal setting. Model 7: While not a model per se, we also curated four dialogues from the rule-based chatbot baseline to compare against our methods following our previous work [8].\nGenerating Dialogues for Evaluation\nTo achieve a higher level of consistency and protect caregivers' privacy, we used personas to generate dialogues. We first crafted four caregiver personas with exemplary replies based on our prior work with family caregivers. We recruited three research team members with direct professional experiences interacting with family caregivers. They were asked to portray one or two of the personas and converse with each bot therapist (Model 0 through 6 in Table 2). They were instructed to use consistent and exemplary replies as much as possible. We collected 28 PST dialogues (4 personas * 7 models). We randomized the sequence of the models and did not disclose which specific models they"}, {"title": "Evaluating Dialogues - Human Evaluation", "content": "We recruited seven clinicians, namely four nurses and three clinical psychologists, to evaluate the quality of the therapy responses. The evaluators were unaware of the models and evaluated the dialogues in random sequence. Evaluators evaluated each dialogue on two aspects, conversational quality and perception of relational capacity, which were adopted from the chatbot evaluation mechanisms by Zhang et al. [19]. For conversational quality, since the generated dialogues specifically focused on the \"symptom assessment\" and \"goal setting\" steps of PST, the evaluators were asked to evaluate the quality of these two steps. To evaluate the symptom assessment step, we asked the evaluators to consider if the therapist assessed all five aspects of symptoms. To evaluate the goal-setting step, evaluators were asked to assess the appropriateness of the goals suggested by the therapist in the dialogue. For the perception of relational capacity, we focused on therapist empathy, which was shown to be a predictor for therapy outcome [20]. We employed the three communication mechanisms to measure empathy developed by Sharma et al. [6]: Emotional Reactions (ER), Interpretations (IN), and Explorations (EX). Strong empathetic communication expresses emotions reacting to what the user said (Emotional Reaction), communicates an understanding of the user's feeling or experience (Interpretation), and explores the user's feelings and experiences that are not stated (Exploration) [6]. Evaluators evaluated all three aspects. Additionally, we asked evaluators to provide a rating based on the overall impression of the therapy session. Details about the evaluation components and questions are included in Table 3. Moreover, to gain a deeper understanding of the ratings and what factors contributed to the ratings, we also asked the evaluators to provide a brief rationale for each rating.\nEvaluating Dialogues - Automatic Evaluation\nIn addition to human evaluation, we adapted the algorithm from Sharma et al. [6] to rate the conversations' empathy."}, {"title": "Discussion", "content": "In this study, we used various prompting techniques to improve GPT-delivered PST for family caregivers and used both human and automatic algorithms to evaluate the therapy dialogues. We found that by using prompt engineering techniques, we were able to improve the quality of the therapy conversations beyond the baseline prompt, but with considerable limitations. Empathy evaluated by both human and the algorithm did not vary significantly across models, despite improving over the baseline in the emotional reaction and interpretation dimensions. Below, we discussed specific prompt engineering techniques and their performances.\nZero-shot learning did not perform as well as the other techniques that we evaluated. Our finding that explicit directions may not be sufficient to adapt a model to a domain-specific task is in line with the literature [1, 7]. In our scenario, the"}, {"title": "Conclusion", "content": "In this paper, we adapt multiple novel prompt engineering approaches to improve an LLM's ability to deliver part of a psychotherapy session. Consistent with previous findings [11], we demonstrate that the model's capability to deliver protocolized therapy can be improved with the proper use of prompt engineering methods, albeit with limitations. Through both automatic and human evaluation, we show an improvement over the baseline model after applying our methods to PST, demonstrating that some prompt engineering techniques are better at improving the performance of the models than others. Hence, while the current models cannot be deployed directly in psychotherapy settings without human oversight, this work contributes to the effort in exploring the potential of LLMs as a therapeutic tool. As such, this work represents an important step toward using LLMs to address the limited availability of human therapists in the context of an escalating need for mental health services."}]}