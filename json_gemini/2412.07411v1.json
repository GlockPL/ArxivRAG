{"title": "DSFEC: Efficient and Deployable Deep Radar Object Detection", "authors": ["Gayathri Dandugula", "Santhosh Boddana", "Sudesh Mirashi"], "abstract": "Deploying radar object detection models on resource-constrained edge devices like the Raspberry\nPi poses significant challenges due to the large size of the model and the limited computational\npower and the memory of the Pi. In this work, we explore the efficiency of Depthwise Separable\nConvolutions in radar object detection networks and integrate them into our model. Additionally, we\nintroduce a novel Feature Enhancement and Compression (FEC) module to the PointPillars feature\nencoder to further improve the model performance. With these innovations, we propose the DSFEC-L\nmodel and its two versions, which outperform the baseline (23.9 mAP of Car class, 20.72 GFLOPs)\non nuScenes dataset: 1). An efficient DSFEC-M model with a 14.6% performance improvement\nand a 60% reduction in GFLOPs. 2). A deployable DSFEC-S model with a 3.76% performance\nimprovement and a remarkable 78.5% reduction in GFLOPs. Despite marginal performance gains, our\ndeployable model achieves an impressive 74.5% reduction in runtime on the Raspberry Pi compared\nto the baseline.", "sections": [{"title": "Introduction", "content": "In recent years, Radar systems have become indispensable in enhancing the perception abilities of autonomous vehicles,\nespecially in challenging weather conditions. The Radar data plays a critical role in enabling accurate object detection,\ncollision avoidance and adaptive cruise control, thus ensuring safe navigation and consistent performance across various\ndriving scenarios. However, current state-of-the-art radar-based algorithms require significant GPU computing power\nand frequently encounter challenges in achieving real-time object detection. Yet, achieving real-time processing is\ncrucial for autonomous vehicles to enable timely decision-making and responses to dynamic changes in the environment,\nensuring safe and efficient navigation by processing sensor data, detecting obstacles and executing control commands\nwith minimal latency.\nState-of-the-art object detection models, designed for Image-based [1][2] and Lidar-based [3][4] methods have demon-\nstrated high accuracy in various performance metrics. Initially, radar-based methods lagged behind due to the sparsity\nof the data compared to other sensor modalities.However, recent studies have improved our understanding of how radar\ncan be beneficial for autonomous vehicles. [5] provides a concise overview of the radar-based object detection methods.\nRecent studies have primarily focused on enhancing feature extractors rather than backbone architectures, often utilizing\nsimilar backbones[3][6]. Moreover, the emphasis has been on improving accuracy rather than deployability. In our\nwork, we are exploring modifications to the backbone of [3][7] along with the feature extractor to emphasize the\nsignificance of real-time processing in autonomous vehicles. Our goal is to develop deployable models suitable for\nedge devices while maintaining high accuracy. The significant contributions of our work are as follows:\n\u2022 Introduction of the FEC (Feature Enhancement and Compression) module to the feature extraction process,\nfacilitating learning and mitigating memory bottlenecks at the initial stages of the network.\n\u2022 We investigated the efficacy of depthwise separable convolutions and integrated them into the backbone of the\nradar-based object detection model [7]. This integration is aimed to enhance the model's efficiency, enhancing\nperformance while minimizing latency.\n\u2022 Designing a deployable version of the proposed model specifically tailored for ARM-based Raspberry Pi."}, {"title": "RELATED WORK", "content": ""}, {"title": "Feature Encoder for Point Clouds", "content": "Sensor data, unlike images, often arrives in irregular formats like point clouds, where each point signifies a measurement\nfrom the environment. Analyzing such data requires specialized techniques such as Point-based and Grid-based\narchitectures to extract meaningful information effectively due to its irregular nature. Point-based architectures [8],[6],\n[9],[10],[11] utilize raw point cloud data to extract features directly. PointNet[6], a foundational work in this field,\nprocesses point clouds without requiring any pre-processing and PointNet++[9] builds upon this by hierarchically\ngrouping points and extracting features to improve performance.\nIn grid-based approaches, point clouds are initially transformed into either a 2D bird's eye view (BEV) or a 3D voxel\ngrid using hand-crafted or learned feature encoders. Hand-crafted feature extraction methods [12],[13],[14],[15] rely\non manual algorithm design based on domain knowledge or heuristics. However, in complex data like point clouds,\nthese methods have limitations to capture all intricate feature relationships. Conversely, learned feature encoders, such\nas PointPillars [16] and VoxelNet[4], employ CNNs or point cloud processing networks to automatically learn these\nrelationships from 2D BEV or 3D voxel grid points, potentially uncovering nuances missed by hand-crafted approaches.\nVoxelNet[4] employs voxelization to transform point clouds into evenly distributed 3D voxels. It utilizes Stacked Voxel\nFeature Encoding (VFE) layers to extract features. On the other hand, the PointPillars[16] utilizes a CNN pillar feature\nnet to extract features from raw point clouds and efficiently converts them into a pseudo-image format. We chose the\nfeature encoder from the PointPillars network for its effective pseudo-image representation in terms of memory and\ncomputation. We then implemented our innovative approach within this feature encoder."}, {"title": "Object Detection for Radar point cloud", "content": "Feature Pyramid Networks(FPNs)[17] were introduced as single-stage object detection models, utilizing a top-down\narchitecture with lateral connections to construct a feature pyramid from a backbone network. This design addresses\nthe challenge of object detection in images with varying scales. RetinaNet[18], an image-based object detector,\nutilizes FPNs on top of ResNet[19] to generate features at different pyramid levels. Following a similar single-stage\ndense object detection approach, Pixor[3] extends these concepts to real-time LiDAR-based 3D object detection by\nadjusting input representation, network architecture and output parameterization. Several radar-based object detection\nmethods[7][20][21] have adopted the Pixor[3] framework for their work with three-headed network to address the\nchallenge of detecting objects at different scales.\nAll of these networks employ ResNet[19]'s residual blocks as the fundamental building blocks for their CNN backbones.\nWhile these residual blocks are renowned for their high accuracy in computer vision tasks due to the innovative skip-\nconnection mechanism in standard convolutional networks and demand higher computational resources and memory.\nConsequently, this results in heightened latency on edge devices. Moreover, as more of these blocks are added in each\nstage, the latency increases proportionately. To tackle this issue, we conducted an analysis of various convolution\ntypes, including Dilated[22], Quantized[23] and Depthwise Separable[24] convolutions, implemented across diverse\napplications. Particularly, the Depthwise Separable Convolution, introduced in XceptionNet[24], substitutes a standard\nconvolution with a depthwise and a pointwise convolution, rendering them nine times faster and suitable for building\nlightweight architectures like [25], [26], [27], [28], [29] for mobile applications.\nFurthermore, MobileNet[29] and its subsequent versions [30], [31] leveraged depthwise separable convolutions,\ndemonstrating superior performance in the ImageNet classification challenge. Additionally, Object detection methods\n[32], [33], [34] have adopted depthwise separable convolution-based backbones, showcasing improved latency and\naccuracy on edge devices. These findings prompted us to delve deeper into this approach, leading to the adoption of\nDepthwise Separable Convolution(DSConv) as a replacement for the default residual block in our work as outlined in\n[7]."}, {"title": "METHOD", "content": "In this section, we present our innovative approach to enhancing the feature encoder and backbone of the network. We\nalso discuss, how we designed our efficeint model and deployable model. An overview of the proposed radar object\ndetection network is shown in Fig.1."}, {"title": "FEC Module", "content": "The PointPillars [16] explains the pseudo-images formation from 3D point clouds, which are then utilized within the 2D\nbackbone. This approach involves several steps: discretizing the point cloud, stacking non-empty pillars, applying a 1x1\nconvolution and subsequently incorporating Batch Normalization and ReLU activation. However, due to the presence\nof a single convolution layer, the PointPillars network is constrained to either feature enhancement or compression,\ndepending on the number of filters employed in that convolution. Emphasizing feature enhancement alone leads to an\nincrease in the number of features, thereby causing the stem to become a runtime and memory bottleneck due to its\nutilization of 3x3 convolution on those resulting features. On the other hand, solely applying feature compression leads\nto the loss of information at the beginning of the network. This limitation hampers its ability to provide optimal feature\nencoding essential for the 2D backbone.\nGiven this analysis, we introduced a Feature Enhancement and Compression (FEC) module, which consists of two\nadditional 1x1 convolution layers after the existing convolution layer, as illustrated in the Fig.2. The first convolution\nis dedicated to feature enhancement by enabling the utilization of a greater number of filters(f2) to enhance feature\nrepresentations. Subsequently, the second convolution, equipped with a limited number of filters(f3), focuses on feature\ncompression. This approach allows the stem to process the compressed features efficiently with fewer filters, resulting\nin reduced runtime and memory footprint. Thus, our FEC module ensures both feature enhancement and compression"}, {"title": "Base Network Architecture", "content": "We consider the network proposed in the paper[7] with the PointPillars feature encoder as our baseline and conducted\nbenchmark experiments atop it. Inspired by the efficiency gains observed in cutting-edge methodologies[24][33][29]\nutilizing depthwise separable convolutions, We replaced the residual block utilized in [7] with the depthwise separable\nconvolution (DSConv) block. This involves employing a combination of 3x3 depthwise convolution and 1x1 pointwise\nconvolution to replace the standard 3x3 convolution. This replacement enhances the model's efficiency in terms of both\naccuracy and latency by reducing the number of parameters and FLOPs. In addition, adopting this approach enables the\nutilization of only a single convolution layer instead of two layers in the stem, similar to [29]. Our DSConv block-based\nnetwork, in conjunction with the FEC module discussed in Sec.3A, forms our new architecture, termed DSFEC, as\nillustrated in Fig. 1. Our initial model DSFEC-L, adopts a configuration with the number of blocks per stage as n\u2081=3,\nn2=6, n3=6 and n\u2081=3, inspired by prior works [3][7]. This serves as the foundation for further model development and\ncomparison."}, {"title": "Designing DSFEC-M & DSFEC-S Models", "content": "From our ablation study Sec.5B focusing on DSFEC-L configuration, we observed that a network with less number\nof blocks could yield comparable performance with reduced runtime latency. This way, We designed an Efficient\nDSFEC-M Model by adjusting the number of blocks in the second and third stages to 3 and 2, respectively, resulting in\nn\u2081=3, n2=3, n3=2 and n\u2081=3. This architecture allowed us to a 60% reduction in GFLOPs while maintaining higher\nmAP than the base architecture.\nAlthough the DSFEC-M model demonstrated superior performance compared to the base architecture, it was not ideally\nsuited for deployment on resource-constrained edge devices like the Raspberry Pi. Hence, we iteratively redesigned\nthe model, leading to the creation of the DSFEC-S Model optimized for edge deployment. By further adjusting the\nnumber of blocks in each stage, we reduced runtime while preserving performance. The final configuration comprised\nn\u2081=1, n2=1, n3=3 and n\u2081=2, achieving an 80% reduction in GFLOPs with a slight mAP improvement from 23.9 to\n24.8 compared to the baseline.\nAdditionally, our experimentation highlighted the impact of activation functions on performance and runtime. While\ncomplex functions like Mish and Swish enhanced performance, they also introduced additional runtime overhead\ncompared to simpler activations like ReLU and Leaky ReLU. To strike a balance between accuracy and efficiency, we\npropose employing different activation functions for various components of the backbone: ReLU for FEC module and\nthe VRU head, Leaky ReLU for the stem and the backbone and Swish for the first two heads (Car & Truck) in both our\nDSFEC-M and DSFEC-S models."}, {"title": "EXPERIMENTS", "content": ""}, {"title": "Experimental setup", "content": "All experiments were conducted using the TensorFlow framework. Our model was trained with a standard Adam\nOptimizer with a momentum of 0.9 and a weight decay of 0.01, over a duration of 30 epochs. The initial learning\nrate was fine-tuned to 0.00348 and updated using a Piecewise Constant Decay scheduler with 400 warm-up iterations,\nemploying a batch size of 24. The experiments and comparisons presented were performed on the nuScenes public\nRadar object detection dataset. A grid-like structure of point cloud has been generated using X range of [0,80], Y range\nof [-40,40], Z range of [-2.5,2.5] and cell size of 0.5."}, {"title": "Evaluation Metrics", "content": "We evaluate our experiments using performance metrics defined by the nuScenes detection benchmark dataset. Although\nour network can detect Car, Truck, Pedestrian and Bicycle classes, we specifically emphasize the Average Precision(AP)\nat a distance of 4 meters and the overall mean AP(mAP) across all four distances (0.5,1,2,4 meters) for the Car class\nto align with previous radar object detection literature. Additionally, we utilize the number of model parameters"}, {"title": "Results", "content": "Experimental results for various network configurations are summarized in Tab.1, focusing on performance metrics\nmentioned in above section. The Baseline model, which utilizes Residual Blocks, achieves an mAP of 23.9 but requires\nsignificant computational resources. Conversely, our proposed baseline model, DSFEC-L improves performance and\nefficiency over the Residual model as mentioned in Tab.1. The DSFEC-M model stands out for its optimal balance\nof high performance, retaining an mAP of 27.4 and an AP@4 of 41.9, while achieving a notable runtime reduction\non Raspberry Pi. Meanwhile, the DSFEC-S model achieved good mAP and is mainly distinguished for its utmost\ndeployability with the smallest model size, delivering a 74.5% runtime reduction on Raspberry Pi. This model is\nespecially suited for resource-constrained environments, emphasizing its practical application in real-world scenarios."}, {"title": "Ablation Studies", "content": "In this section, we aim to validate the impact of each design choice and hyperparameter setting of [3][7]."}, {"title": "Stem Filters Analysis", "content": "We found that processing high-resolution input from PointPillars with the default stem (utilizing standard convolution\nwith a 3x3 kernel and 32 filters) was causing runtime and memory bottlenecks. To address this issue, we utilized a\nconfiguration of 32, 128 and 12 filters in the FEC module, resulting in only 12 filters being required in the stem. Our\nexperiments, as demonstrated in Tab.2, revealed that reducing the stem filters to 12 resulted in a reduction of 0.72\nGFLOPs, runtime by 20 m.sec and halved the memory usage to 2.35 MB while maintaining mAP, highlighting the\neffectiveness of the introduced FEC module and it's configuration. The highlighted row for filters = 12 underscores its\nsuperiority in balancing mAP, speed and resource consumption."}, {"title": "Blocks per Stage Analysis", "content": "We noticed that the blocks in the early stages(1&2), which handle high-resolution feature maps, incur higher FLOPs,\nleading to increased runtime compared to the final stages. Tab.3 illustrates one such analysis: reducing the number of\nblocks in stage 2 from 6 to 3 results in an 8.2% reduction in GFLOPs and a 9.6% decrease in runtime, saving 109 msec.\nAdditionally, the later stages(3&4) have minimal impact on GFLOPs and runtime since the input resolution decreases\nfor them. This flexibility allows us to adjust the number of blocks in these stages as needed to improve mAP."}, {"title": "CONCLUSION", "content": "In this work, we successfully achieved deployability of radar object detection on non-GPU-based device Raspberry\nPi. This was accomplished by introducing the Feature Enhancement and Compression module to the feature encoder,\nenabling efficient feature learning and mitigating memory bottlenecks in the early stages and incorporating Depthwise\nSeparable Convolutions into the network backbone, ensuring accuracy is maintained while minimizing the latency.\nThrough extensive investigations of different design choices, we proposed two models: DSFEC-M and DSFEC-S. We\nhave demonstrated the performance of these models in terms of accuracy, model size and speed. Our results suggests\nthat the DSFEC-M model prioritizes efficiency with higher accuracy and decent latency, making it suitable for scenarios\nwhere accuracy is paramount. Conversely, the DSFEC-S model is optimized for deployability with low latency and\ndecent accuracy, making it ideal for resource-constrained devices."}]}