{"title": "Kozax: Flexible and Scalable Genetic Programming in JAX", "authors": ["Sigur de Vries", "Sander W. Keemink", "Marcel A. J. van Gerven"], "abstract": "Genetic programming is an optimization algorithm inspired by natural selection which automatically evolves the structure of computer programs. The resulting computer programs are interpretable and efficient compared to black-box models with fixed structure. The fitness evaluation in genetic programming suffers from high computational requirements, limiting the performance on difficult problems. To reduce the runtime, many implementations of genetic programming require a specific data format, making the applicability limited to specific problem classes. Consequently, there is no efficient genetic programming framework that is usable for a wide range of tasks. To this end, we developed Kozax, a genetic programming framework that evolves symbolic expressions for arbitrary problems. We implemented Kozax using JAX, a framework for high-performance and scalable machine learning, which allows the fitness evaluation to scale efficiently to large populations or datasets on GPU. Furthermore, Kozax offers constant optimization, custom operator definition and simultaneous evolution of multiple trees. We demonstrate successful applications of Kozax to discover equations of natural laws, recover equations of hidden dynamic variables and evolve a control policy. Overall, Kozax provides a general, fast, and scalable library to optimize white-box solutions in the realm of scientific computing.", "sections": [{"title": "Introduction", "content": "Genetic programming (GP) is an evolutionary algorithm which automatically generates the structure of computer programs that map inputs to an output value [22]. GP does not require a fixed structure to be pre-selected for the solution, which allows the algorithm to flexibly discover general structures of solutions with less human bias. The computer programs are often represented by parse trees, consisting of functions and variables. GP discovers interpretable solutions that provide understanding about the data or model itself, and consequently GP has become one of the main pillars in the field of automated scientific discovery [46, 3]. In scientific discovery, GP has been used in (re-)discovery of natural laws [39, 25], symbolic regression of dynamical systems [3, 11], learning symbolic control policies [17, 44, 32] and evolving learning rules [19].\nMost such GP applications are based on separate implementations, each individually tailored to a specific task. Ideally, one unifying framework would exist that allows users to apply GP to their target problem. However, a major issue in GP is the high computational requirements needed to perform the fitness evaluation [16], especially when applied to difficult problems or large datasets. This complicates the development of a GP framework that both generalizes to arbitrary problems and runs efficiently. Variations of GP have been proposed that are computationally more efficient, such as Cartesian GP [30] and linear GP [6], however these variants have limitations in turn, such as reduced interpretability and inefficient evolution. Another approach for reducing computation time, is to improve the parallelization of the evaluation of different candidate solutions. Due to the inherently parallel nature of evolutionary algorithms, candidate solutions can be evaluated independently [16]. Nonetheless, this remains difficult for GP, as the individual solutions in the population may have different structures and sizes."}, {"title": "Related Work", "content": "Genetic programming (GP) was introduced by Koza in [22], including many applications such as symbolic regression [22], robot movement optimization [24], planning, solving equations and finding a control strategy [23]. More recently, new tools have been developed that focus on symbolic regression using GP, either for dynamical systems or natural laws, including Eureqa [3, 39], DEAP [14], Operon [9] and PySR [12]. Another relevant method for symbolic regression is SINDy [7], which seeks expressions for dynamical systems through sparse regression. This method has seen great successes, and has been further developed to extend to partially observed data [20], control problems [8] and learn robust ensembles [13]. Other applications of GP include control policy optimization [17, 44, 32], evolving learning rules [19] and objective functions [37].\nHowever, most GP frameworks were developed for specific applications, as there is still a lack of a unifying framework for GP. HeuristicLab [45] attempted to provide GP software that allows users to tune the fitness evaluation to their problems, but the software was limited by high computational requirements. More broadly used GP libraries, such as Eureqa and PySR, apply tricks like partitioning and the finite difference method to the input data to improve"}, {"title": "Genetic programming", "content": "GP is a variant of evolutionary algorithms that evolves the structure of computer programs [22]. In Kozax, we focus on evolving symbolic expressions represented by parse trees, a subset of computer programs. Parse trees consist of mathematical operators as interior nodes and variables and constants as the leaf nodes. Parse trees are executed recursively, where child nodes have to be computed before their parent node. An example of the parse tree representation is presented in Figure 2a.\nIn GP, a population of solutions is optimized on a specified task through stochastic optimization. See Algorithm 1 for an overview of the GP algorithm. To evaluate the fitness of a candidate solution, it is transformed into a executable program and tested on a problem. The performance of a candidate solution is expressed by a fitness score, computed with a fitness function. The fitness scores are used to select individuals for reproduction, where fitter individuals produce more offspring."}, {"title": "Kozax", "content": "Kozax is a GP library for general and efficient problem optimization, implemented in JAX [5]. In this section, the major features of Kozax are described in more detail. Afterwards, we will explain the vectorized representation of solutions adopted in Kozax, and how this representation allows for parallelization of fitness evaluation."}, {"title": "Features", "content": "Table 1 shows a comparison between the most important features in PySR [12] and Kozax. PySR provided an extensive comparison with other frameworks for GP and symbolic regression [12]. PySR has some features that are currently not present in Kozax, such as placing constraints on symbolic operators that improve interpretability and reduce the search space. In the remainder of this section, the major features present in both libraries are described, followed by the advantages of the features exclusively available in Kozax.\nFirst of all, Kozax and PySR make use of just-in-time (JIT) compilation to minimize computational overhead and improve the computation speed. In PySR, compilation is used to combine multiple operators into a single compiled operator, which speeds up fitness evaluation. In Kozax, the fitness evaluation and reproduction functionality are compiled, resulting in large speed improvements, as the complete population is evaluated and evolved in parallel. Both libraries allow users to define custom operators and fitness functions, which remains compatible with JIT compilation.\nFurthermore, both PySR and Kozax keep track of the best solutions in a Pareto front throughout evolution. The Pareto front stores the best solution in terms of fitness at every complexity level, and finally includes solutions only when the fitness is improved over less complex solutions. One feature only present in PySR is that the best solutions from earlier generations are randomly reintroduced at later stages in evolution. In Kozax, the Pareto front is solely used for keeping track of the best solutions. Both libraries use a SymPy interface [29] to represent the individual solutions. In Kozax, the discovered expressions after evolution, but PySR also applies simplification throughout the optimization process."}, {"title": "Tree representation", "content": "Kozax implements the GP algorithm using JAX [5]. Typical applications of GP include symbolic regression, policy optimization and other problems that benefit from interpretable solutions. As these are challenging problems, being able to evaluate large populations efficiently is advantageous. JAX offers features for faster computation, high scalability and high-performance machine learning, like vectorization of functions, just-in-time compilation and automatic differentiation. Accordingly, Kozax makes use of these features to improve runtime, scale to large populations and evolve accurate solutions.\nParallelization of different tree structures in GP is not trivial, as the trees in a population have varying structures. To this end, the trees are represented as matrices with a fixed size in Kozax. In cartesian and linear GP, the matrix"}, {"title": "Results", "content": ""}, {"title": "Experiments", "content": "We tested Kozax in six experiments, both to demonstrate its ability to find accurate solutions and the variety of problems that can be studied. PySR showed to outperform other GP libraries in their paper [12], therefore we chose PySR as a competitive baseline for Kozax. Experiments and hyperparameters used in each experiment are presented in Table 2. The number of generations and population size are used in both Kozax and PySR, but the number of constant optimization steps is only relevant in Kozax. In each experiment, except for evolving the loss function, the genetic algorithm is used for constant optimization. The experiments include symbolic regression of laws and dynamic systems, optimizing a symbolic control policy and evolving a loss function. The two libraries are evaluated on ten different seeds, which influence both the data generation and the initial population. Both Kozax and PySR return a Pareto front, containing the best solution at every complexity level that was discovered throughout evolution. The results are reported as the fraction of runs in which a successful solution was included in the Pareto front, where the determination of a successful solution are explained in the experiments respectively. Results are shown in Table 3. The raw data and code to reproduce the results are available at https://github.com/sdevries0/kozax_paper."}, {"title": "Law discovery", "content": "The first three experiments entail symbolic regression of Kepler's third law [21], Newton's law of universal gravitation [33] and Bode's law [4]. To accomplish the task, the correct equation structure and parameters should be discovered that represent each law. A symbolic expression receives the inputs corresponding to each law, which are mapped to a predicted value. The fitness function is computed as the absolute mean error between the predictions and the targets. PySR generated the datasets for each law based on the original papers or given realistic ranges for the variables [12].\nBoth libraries are able to evolve the correct expressions in the majority of the runs, although Kozax failed to evolve the correct equation for Kepler's third law in one run. Overall, both libraries quickly find correct equation structures and parameter values for these laws."}, {"title": "Symbolic regression of dynamical systems", "content": "The next experiment is symbolic regression of the Lotka-Volterra equations [15], a dynamical system governing the population of preys and predators. As methods such as PySR and SINDy make use of the finite difference method, we opted to use this method in Kozax as well. One trajectory of the Lotka-Volterra dynamics is integrated, given a randomly sampled initial condition. Afterwards, the true derivative of the state is computed for every time step. Symbolic regression is then performed with the states as the inputs and the derivatives as the target outputs. The fitness function is again the mean absolute error between the predicted and true derivatives. A successful solution recovers the equations for both variables, including the correct coefficients. PySR and Kozax are able to successfully rediscover the equations of both the prey and the predator in all runs.\nThe finite difference method allows symbolic regression of dynamical systems with PySR. However, this only works when all dimensions of the system are observed. In the fifth experiment, only the prey is observed in the Lotka-Volterra model, which requires GP to find two equations and integrate them as a system of ordinary differential equations. The evolved equations are integrated from the true initial condition, and the fitness is computed as the mean absolute error between the predicted and true prey populations, plus an additional penalty when either population is negative at any timepoint. This is not possible in PySR without involving external methods, but in Kozax the fitness function can be adjusted to integrate differential equations with latent variables. Kozax recovers the equations of both the prey and predator populations in eight out of ten runs."}, {"title": "Symbolic control policy optimization", "content": "In this experiment, a symbolic control policy was evolved to solve the acrobot swing-up task [41]. The fitness function is a sparse reward function, returning the first time point at which the swing-up was satisfied. The proposed policies return continuous values, but these values are mapped to [-1, 1] given the sign of the policy's output. A policy observes the sine and cosine of the angles of the first and second link, and the angular velocities of the two links. The acrobot task is simulated for a fixed number of time steps using Gymnax [28], a JAX implementation of Gym's control environments. A successful evolutionary run entails that a solution is discovered that solves the acrobot swing up within the specified number of time steps.\nPySR could discover a symbolic policy by distilling a pre-trained black-box policy, however the accuracy heavily depends on the quality of the teacher policy, and information might be lost in the distillation stage. Therefore, it is advantageous to optimize a symbolic policy through direct interaction with the control environment. In Kozax, it is possible to evaluate solutions on control tasks directly, and Table 3 shows that Kozax evolves successful policies in all runs, while PySR is not compatible with the task."}, {"title": "Evolving an objective function", "content": "Learning the objective function is a direction within the field of meta-learning that can improve convergence speed and robustness [2]. Previously, GP has been used to learn a symbolic expression for the loss function in various"}, {"title": "Runtime analysis", "content": "The results show that Kozax competes with PySR on the symbolic regression tasks in terms of performance, and that Kozax can handle a wider range of tasks. However, another important aspect of Kozax is the computational efficiency. In Figure 5, the average wall clock time of PySR and Kozax is presented on a symbolic regression task. The task was repeated with different datasets, where the number of data points that have to be evaluated by each candidate solution increases. As Kozax can be deployed on both CPU and GPU, the runtime is measured for each device. Note that the accuracy of the found solutions does not matter in this experiment, and that the wall clock time was measured given a fixed number of generations and population size.\nFigure 5 shows that PySR is faster than Kozax on CPU for each dataset size. Kozax requires more time for the just-in-time compilation, which explains the relatively large difference in runtime between PySR and Kozax for 1000 data points. The difference decreases relatively for larger datasets, as the compilation time is a smaller fraction of the total runtime. When the dataset contains 1000 items, both PySR and Kozax on CPU are faster than Kozax on GPU. However, when the number of data points increases to 10000, Kozax on GPU is slightly faster than Kozax on"}, {"title": "Discussion", "content": "In this paper, we introduced Kozax, a library for genetic programming (GP) built on JAX. By representing trees as matrices, while following the standard GP algorithm, Kozax parallelizes the evaluation of trees with different structures. This way, the fitness evaluation is sped up compared to evaluating candidate solutions sequentially. Additionally, Kozax runs op GPU, therefore the fitness evaluation scales efficiently to large populations or datasets.\nGP has shown promising performance in several domains, such as symbolic regression [3, 12] and symbolic policy optimization [17, 44]. Most GP libraries are designed specifically for a certain class of problems, however a general framework that can handle arbitrary problem classes was still missing. We developed Kozax to give users much freedom in their problem definition. In the results, we showed that Kozax performs competitively compared to PySR on symbolic regression tasks. Yet, Kozax can also be applied to evolve control problems and objective functions, which is impossible with PySR without using external methods. Besides the scalability, another advantage of Kozax is that it flexibly learns trees for desired functionality.\nAs demonstrated, the matrix representation of trees in Kozax improves the computation speed of fitness evaluation and reproduction. The representation of the solution space has resemblances to related algorithms like grammar evolution [34], Cartesian GP [30] and linear GP [6]. In Kozax, the matrices implicitly adhere to the hierarchical tree structure in the standard GP algorithm. A big advantages of Kozax is that only rows with active nodes can be changes during evolution, while grammar evolution, Cartesian GP and linear GP suffer from the inefficiency that inactive nodes are changed during evolution. To our knowledge, Kozax is the first implementation that uses a matrix representation for the standard GP algorithm.\nAlthough Kozax demonstrated efficient evolution of accurate solutions, there are still features that could further improve the applicability of Kozax to complex problems. PySR showed that integrating simplification of expressions throughout evolution and adding constraints of symbolic functions improve the interpretability of the discovered expressions [12]. Another improvement would be allow for higher-dimensional inputs and outputs in the trees, which relates to strongly typed GP [31]. Processing higher-dimensional inputs would enable Kozax to be applied to visual data or vector operations. Being able to evolve a tree with multiple outputs allows to learn the same functionality for multiple variables, which could for example evolve compact neural network structures. Extending GP with automatically defined functions (ADF) supports learning useful building blocks that may be included repeatedly in other trees. Especially when evolving multiple trees simultaneously, it would be efficient to reuse functionality in different trees. The additional value of ADFs increases even more when operators with more than two inputs could be evaluated in Kozax, as complex functions structure can be evolved.\nIn fields such as scientific discovery [46] and explainable artificial intelligence [36], it is beneficial to have interpretable white-box models. As GP automatically generates interpretable computer programs, it is a fundamental approach to optimize white-box models. By utilizing GP, the resulting models may provide knowledge about the underlying system, as seen in symbolic regression, or transparency in decision with symbolic control policies. With the development of Kozax, we hope to contribute to the creation of trustworthy artificial intelligence."}]}