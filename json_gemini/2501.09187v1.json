{"title": "Patch-aware Vector Quantized Codebook Learning for Unsupervised Visual Defect Detection", "authors": ["Qisen Cheng", "Shuhui Qu", "Janghwan Lee"], "abstract": "Unsupervised visual defect detection is essential across various industrial applications. Typically, this involves learning a representation space that captures only the features of normal data, and subsequently identifying defects by measuring deviations from this norm. However, balancing the expressiveness and compactness of this space is challenging. The space must be comprehensive enough to encapsulate all regular patterns of normal data, yet without becoming overly expressive, which leads to wasted computational and storage resources and may cause mode collapse-blurring the distinction between normal and defect data embeddings and impairing detection accuracy. To overcome these issues, we introduce a novel approach using an extended VQ-VAE framework optimized for unsupervised defect detection. Unlike traditional methods that apply a constant and uniform representation capacity across an image, our model employs a patch-aware dynamic code assignment scheme. This approach trains the model to allocate codes of varying resolutions based on the context richness of different image regions, aiming to optimize code usage spatially for each sample in a learnable fashion. We also leverage the learned strategy for code allocation in inference, to enlarge the discrepancy between normal and defective samples, thereby improving detection capabilities. Our extensive testing on the MVTecAD, BTAD, and MTSD datasets demonstrates that our model achieves state-of-the-art performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Visual defect detection for quality control has been a significant focus in industry [1], [2]. With advancements in deep learning, numerous applications have emerged, such as detecting defective parts in electronics manufacturing [3], [4], surface inspection in fabric or metal processing [5], [6], dangerous events monitoring in traffic [7], [8]. Unlike common scenarios where sufficient image samples are available for training, defect samples are often extremely rare or even nonexistent in production environments [2], [9].\nTo address the challenge of limited defect data, deep learning-based visual defect detection is often approached as an unsupervised learning task, known as one-class defect detection. This method assumes no defect data is available for training and instead focuses on creating a \"memory of normality,\" which records the regular patterns of normal samples. Defects are then treated as anomalies that deviate from these memorized patterns, effectively turning defect detection into an outlier detection problem. The core idea is that the greater an input's deviation from established normal patterns, the more likely it is to be identified as a defect. Previous research has demonstrated that neural networks can significantly improve the memory of normality by generating detailed, compact representation spaces. A common approach involves training networks like auto-encoders to reconstruct normal samples, enabling the automatic learning of their representations [10]. However, this approach often encounters the \"mode collapse\" issue, where the model fails to differentiate between normal and defect samples during inference due to an overly expressive representation space [11]. An alternative strategy avoids training representation learning altogether, opting to use a pre-trained model as a feature extractor. These models store normal features in a memory bank, and defects are identified by measuring how much a sample's representation deviates from the stored features using methods such as k-nearest neighbors [12], [13]. However, these methods often require substantial memory capacity to capture the diversity of normal patterns, leading to higher computational and storage costs [14]. Both strategies face the challenge of designing an effective representation learning mechanism, which involves a trade-off. On one hand, higher-capacity representation spaces can capture more detailed information about normal data. On the other hand, this increases the risk of mode collapse, where the model reconstructs inputs too accurately, regardless of whether they are normal or defective, thereby weakening its ability to detect anomalies. Recent approaches have sought to constrain the representation space to mitigate this issue. For example, MEMOAE uses a memory matrix for normality [15], and vector quantization (VQ) methods like HVQ-Trans employ discrete codebooks rather than continuous spaces [14]. While these methods show promise, they still require careful manual selection of representation capacity, which is difficult to optimize. Specifically, in vector quantization, coarse codes might overly restrict the latent space, leading to poor normal sample representation, whereas finer codes can improve normal sample representation but may introduce excessive expressiveness, code redundancy, and inefficiency.\nTo address the challenges in representation learning for unsupervised defect detection, we propose the Patch-aware Vector Quantized Autoencoder (PVQAE). This model enhances the conventional VQ-VAE framework by introducing a redesigned codebook learning mechanism, as shown in Figure 1. Typically, VQ assigns a fixed resolution and quantity of latent codes uniformly across an image, which limits the flexibility of representation. This uniform allocation does not account for the varying levels of detail across different regions of an image, resulting in suboptimal performance. Our approach solves this limitation by employing a dynamic code assignment scheme that adapts the resolution of codes based on the richness of context in each region. Finer codes are allocated to areas with detailed visual patterns, while coarser codes are assigned to regions with simpler, less informative patterns. This approach ensures efficient use of codes, allowing the model to reconstruct normal images with fewer, lower-resolution codes, while applying higher resolution only where necessary. During training, the model learns code budgets-referred to as normal budget priors-which are fixed and used during testing. These priors prevent the model from utilizing excessive reconstruction capabilities, particularly in regions with unexpected detailed patterns, such as defects. For instance, a crack with sharp, irregular edges on the smooth surface of a glass cup would require more detailed reconstruction in those unexpected areas. By limiting the model's ability to reconstruct such patterns, the resulting higher reconstruction error makes defect detection more effective. Moreover, the flexibility in allocating latent codebook capacity allows our PVQAE model to handle a wide range of visual patterns across multiple objects with a single model. This eliminates the need for training separate models for each product type, which has been a labor-intensive practice in prior work [12], [13].\nOur method achieved superior performance in both image and pixel-level evaluations on several defect detection benchmarks, compared to existing state-of-the-art baselines. These experiments validate the effectiveness of our method. To summarize, this paper makes three key contributions:\n1. To the best of our knowledge, we are the first to explore how a one-class model can find optimal representation capacity during training, addressing a key challenge in defect detection.\n2. We introduced two novel approaches to enhance unsu-pervised defect detection performance: patch-aware dynamic code allocation and normal budget prior learning.\n3. Our PVQAE model demonstrated superior performance across multiple industrial defect detection datasets.\nThe rest of the paper is structured as follows: Section 2 covers related work, Section 3 details our codebook learning and detection method, Section 4 presents experimental results, and Section 5 concludes the paper."}, {"title": "II. RELATED WORK", "content": "Most existing work uses a one-class approach, relying on learning representations of normal data and identifying defects as outliers. Early methods used minimal-volume spheres [16] or predefined kernels [17] to simplify the representation space, which are later improved by adding geometric transformations [18], patch-wise embeddings [19], hybrid model architectures [20] and contrastive learning [21]. Generative models like autoencoders [22], [23] and GANs [24] have also been used for more complex data. Some other techniques introduced attention-guided models [25], Gaussian Mixture Models [26], normalizing flows [27], [28], and knowledge distillation [29], [30] to learn condensed representations. Some recent works avoid training altogether by leveraging pre-trained models [31] with local distribution fitting [12] and Coreset sampling [13]. However, the heavy reliance on pre-trained datasets may be sub-optimal for domain-specific tasks like industrial defect detection."}, {"title": "B. Vector Quantization", "content": "Vector quantization (VQ), introduced in the 1980s for signal compression [32], has later been combined with deep generative models like VQ-VAE [33], [34], VQ-GAN [35] and LLM [36], [37]. Standard VQ encodes input data as a codebook of discrete codes, enabling expressive representations while preventing mode collapse. This approach has inspired unsuper-vised defect detection methods [14], [38]. PVQAE also uses VQ but improves on it by introducing a dynamic codebook learning process with adjustable code capacity, addressing the limitations of fixed codebooks for defect detection."}, {"title": "III. METHODOLOGY", "content": "Our PVQAE model is illustrated in Figure 2. It extends the VQ-VAE framework with a patch-aware dynamic codebook learning scheme and a normal budget prior learning step, and is trained in an end-to-end fashion. In this section, we will first revisit the standard VQ-VAE framework and then describe our proposed method in detail."}, {"title": "A. Preliminary", "content": "1) Vector Quantized Variational AutoEncoder (VQ-VAE): A VQ-VAE model consists of a pair of encoder (E) and decoder (G), and a latent representation structure. The latent representation structure is formulated as a codebook $Q \\in R^{(K\\times n)}$, which contains $K$ number of discrete codes (i.e. embeddings) $q_k, k \\in 1,2,...,K$, each of dimension $n$. To learn the codebook, an input image $x$ is first encoded by E to a matrix of discrete embeddings $Z\\in R^{(h\\times w\\times n)}$, where each embedding $z_{i,j}$ correspond to one image constituent (i.e. patch). Then for each image patch, a code $q_{i,j}$ is assigned to it by using a look-up function $l(\\cdot)$, which selects a certain code from the codebook Q that has the smallest euclidean distance from patch embedding $z_{i,j}$:\n$q_{i,j} = l(z_{i,j},Q) = argmin_m ||z_{i,j} - q_m||_2$\nThe matrix of assigned codes, denoted as q, is then passed to the decoder network to generate a reconstructed image (2):\n$x = G(z_q) = G(q(E(x)))$\nThe model is trained with 2 main objectives: 1) reconstruc-tion loss measuring the difference between reconstructed im-age and the input, and 2) discrete representation learning losses encouraging the alignment between the selected latent codes and the encoder outputs. Since the argmin operand used in the look-up function is not differentiable during backpropagation, the gradient of this step is approximated by using the straight-through estimator (STE) with stop-gradient operand (sg) [39]. The overall loss is summation of the objectives with $\\beta$ as a hyper-parameter:\n$L_{VQ}(E, G,Q)) = ||x - x||_2 + ||sg[E(x)] - q||_2 + \\beta ||sg[q] - E(x)||_2$\n2) Adversarial Training: Adversarial training also benefits representation learning as reported in [35]. We hence incorpo-rate it enhance the representations to fully express regular pat-terns. Specifically, we add a patch-wise discriminator network D, and introduce an additional classification loss to distinguish between reconstructed and real patches:\n$L_{ADV} ((E, G, Q), D) = log(D(x)) + log(1 - D(x))$"}, {"title": "B. Patch-aware Vector Quantized Codebook Learning", "content": "1) Multi-resolution dynamic code allocation: Existing methods typically use a codebook with a single static res-olution [38] or a combination of static resolutions [14], assigning a constant number of latent codes to an image. In contrast, our PVQAE dynamically assigns latent codes of varying resolutions and quantities. We define a hierarchy of candidate resolutions $R = {r_1 < r_2 < ... < r_k}$, where $r_i \\in (0,1)$, ordered by the area of the image each resolution represents. For each resolution r, the encoder E encodes an input image $I \\in R^{H\\times W\\times 3}$ into a matrix of feature embeddings $Z_r = {z_{1,1}, z_{1,2}, ..., z_{1/r,1/r}}$, where each $z^r_{i,j} \\in R^{(H\\times r)\\times (W\\times r)\\times d}$ corresponds to an image patch $I_{i,j}$. This creates a hierarchy of embeddings at multiple resolutions $Z = {Z_1, Z_2, ..., Z_R}$.\nRecognizing that different regions of an image contain varying levels of contextual information, we select the most appropriate resolution for each region. Inspired by [40], we use a Dynamic Routing Module to determine the optimal resolution. As shown in 3, the module applies average pooling to feature embeddings at different resolutions, except for the finest level, ensuring all embeddings are resized to match the dimensions of $(H \\times r_1) \\times (W \\times r_1) \\times d$. These pooled embeddings are concatenated and passed through an MLP layer, which acts as a gating mechanism, outputting logits $g^{i,j} \\in R^R$ for the resolution levels of each image patch $I^{i,j}$ at the coarsest resolution. The Gumbel-Softmax technique [41] is applied to these logits, providing a soft, differentiable approximation of the argmax operation. For each patch $I^{i,j}$, a score b is generated for the resolution levels:\n$b^r_{i,j} = \\frac{exp(g^{r}_{i,j}+\\delta^{r}_{i,j})/\\tau}{\\Sigma_{r\\prime=1}^{R} exp(g^{r\\prime}_{i,j}+\\delta^{r\\prime}_{i,j})/\\tau}$\n$\\Sigma_{r=1}^{R} b^r = 1$\nwhere $\\delta$ is random noise sampled from Gumbel distribution, and $\\tau > 0$ is the temperature adjusting the sharpness of scoring. As the temperature approaches 0, the score becomes closer to an one-hot hard score. At the selected resolution level, discrete latent codes are assigned to the feature embeddings as in the standard vector quantization. Consequently, each image are represented with codes of different length.\n2) Progressive budget learning: Our goal is to ensure the model represents each sample with the most economical representation capacity. This means assigning fine-resolution codes to regions with rich context and coarse-resolution codes to areas with simple patterns, such as smooth product surfaces with uniform textures. To achieve this, we introduce a budget loss to guide resolution selection, penalizing the use of finer codes by assigning them a higher cost. Specifically, we apply discrete wavelet transformation (DWT) to each image patch at the coarsest resolution level and compute the normalized entropy ($\\hat{H}$) of the DWT coefficients to estimate context richness. The inverse of $\\hat{H}$ is then used as the base cost, making it context-dependent\u2014cheaper for regions with richer context. As code resolution increases, the cost rises by a factor of $c > 1$, i.e. $c = 2^{R-1}$. The overall budget loss is the summation of costs across all patches $I^{i,j}$ at the coarsest level:\n$L_{Budget} = \\Sigma_{i,j=1}^{H/r_k, j=W/r_k} \\frac{\\hat{H}(I^{i,j})}{c^{k^{i,j}}}$\n$\\hat{H}(I^{i,j}) = \\frac{H(DWT(I^{i,j}))}{\\Sigma H(DWT(I^{i,j}))}$\nThen the overall training objective is:\n$L = L_{VQ} + L_{ADV} + \\lambda L_{Budget}$\nwhere $\\lambda$ is a hyper-parameter to adjust the weight on budget learning. To prevent the model falls to a situation where it avoids using fine resolution codes at all, we schedule $\\lambda$ to lin-early increase from 0 to the maximum value. This encourages the model to learn expressive latent representations and proper reconstruction of normal images first, and then progressively pivot towards refining the representation capacity."}, {"title": "C. Defect Detection with PVQAE", "content": "1) Learning Normal Budget Priors: The learned budget represents the allocation of code resolutions in different re-gions, capturing the regular distribution of normal patterns. We leverage it as an additional indicator of normality for defect detection. For instance, in the MVTecAD dataset, normal images of drug capsules likely have finer resolution codes to regions with intricate details, e.g. the red half shell with white printings. During inference, if more budget is needed for unexpected regions, or less to the detailed areas, it may indicate defects. To record the learned budget priors, we first flatten the resolution level matrix into a sequence in a clockwise order: $B_{norm} = {b_{1,1}, b_{1,2},..., b_{H/r_k,W/r_k}}$. Inspired by VQ-GAN [35], we then train a standalone Budget Prior Transformer to capture these normal sequences. The task is framed as predicting a masked token (resolution level) within the sequence by referencing all other tokens, enumerating the masks to obtain the fully predicted sequence $B_{norm}$. This approach relies on the observation that defects are typically localized, while most image regions remain normal, meaning the normal budget for a region can be inferred from the surrounding areas. To accommodate object-class-specific budget learning, we prepend a CLS token to $B_{norm}$. During training, we freeze the rest of the model to obtain $\\hat{B}_{norm}$ from the Dynamic Routing Module and train the Budget Prior Transformer using Cross Entropy (CE) loss:\n$L_{Prior} = \\Sigma_{i,j}CE(b^{norm}_{i,j}, \\hat{b}^{norm}_{i,j})$\n2) Scoring for Defect Detection: Given an input image x, PVQAE detects defects by estimating a defect score S for each pixel. This score consists of two components: $S_{prior}$ and $S_{Recon}$. $S_{prior}$ measures the Cross Entropy between the budget B, dynamically determined by the Dynamic Routing Module based on the input image's context, and the normal budget prior $\\hat{B}_{norm}$, predicted by the Budget Prior Trans-former. $S_{Recon}$ is the L2 reconstruction error. During recon-struction, we use $\\hat{B}_{norm}$ for code allocation to prevent the model from utilizing excessive representation power, thereby limiting the reconstruction quality for unseen defects. The final defect score is the pixel-wise product of the two components, which is thresholded for the eventual detection:\n$S = S_{prior} \\times S_{Recon}$, where\n$S_{Prior} = \\frac{CE(b^{i,j}, \\hat{b}^{norm}_{i,j})}{\\Sigma CE(b^{i,j}, \\hat{b}^{norm}_{i,j})}$,\n$S_{Recon} = ||x - x||_2\nI(S_{i,j} > t) \\forall S_{i,j} \\in S$"}, {"title": "IV. EXPERIMENTS", "content": "In this section, we present the experimental evaluation of our PVQAE model on public manufacturing datasets for both image and pixel-level defect detection. We compare our model with several popular baselines. For memory-based methods, we include the latest approaches, PaDiM [12] and PatchCore [13]. Among reconstruction-based methods, we focus on AE-based techniques such as the AE-SSIM [22], VQ-E [38] (de-signed for medical image anomaly detection), and HVQ-Trans [14], a recent state-of-the-art defect detection model. The latter two use standard VQ techniques with static resolution codebooks. We also include STPM [30] and CutPaste [21] for a comprehensive comparison across different representation learning methods. Following [13], the area under the receiver-operator curve (AUROC) is used as the evaluation metric. Additionally, we qualitatively assess the effectiveness of our dynamic code allocation for defect detection and conclude with ablation studies on progressive budget learning and normal budget priors."}, {"title": "A. Implementation Details & Datasets", "content": "We built our PVQAE on VQ-GAN [35], using its encoder and detector configurations. Outputs from the last three en-coder convolutional blocks formed the hierarchy of feature embeddings. The Dynamic Routing Module followed [40], with an MLP layer containing a single hidden layer to reduce computation. The Budget Prior Transformer used one trans-former block with a learnable embedding matrix $E \\in R^{3\\times d}$ to tokenize code resolution indices. Images were resized to 256x256 and augmented with random flipping and color-jittering. A single model was trained across all objects in a dataset. Training was conducted on an Intel Core i9-9920X CPU and NVidia RTX8000 48G GPU. Public implementa-tions from [42] and [14] were used for baselines, excluding the optimal-transport objective from HVQ-Trans to focus on representation learning comparison.\nWe evaluated on three datasets: 1) MVTecAD [43] with 5354 images in 15 classes; 2) BTAD [44] with 2830 images from 3 products, 3) MTSD [45] with 925 normal and 392 defective tile images. BTAD and MSTD are combined in training due to limited data size."}, {"title": "B. Quantitative Defect Detection Performance", "content": "1) MVTecAD: The results for MVTecAD are presented in Table I. Our PVQAE method demonstrates competitive perfor-mance compared to all other baselines in both image and pixel-level AUROC. Notably, with 15 different product categories, it was challenging to learn a single latent representation that adequately captured the diverse distribution. For fairness, we trained and tested all methods on multiple objects simultane-ously, enabling us to highlight differences in representation learning capability. While the SOTA memory-based method, PatchCore, produced strong results across several objects, it was burdened by high computation and storage costs due to the need to cover multiple objects. In comparison, our PVQAE method significantly outperformed VQ-E, which relies on standard vector quantization, across all objects. HVQ-Trans, a recent strong baseline, also achieved competitive scores but was considerably more resource-intensive due to its Mixture-of-Expert mechanism that switches between multiple code-books for different objects, making it relatively less efficient.\n2) BTAD & MTSD: We combined BTAD and MTSD datasets together in training for all methods. Similarly, our PVQAE method outperformed or achieved comparative per-formance on the BTAD and MTSD datasets by comparing to all other baselines. The superior performance of PVQAE was probably because our codebook learning not only produced an expressive codebook enabling good coverage of normal visual patterns, but also avoided a over-expressive latent space that could lead to better reconstruction of the defects."}, {"title": "C. Qualitative Defect Detection Performance", "content": "In this subsection, we qualitatively investigated the ef-fectiveness of our method by comparing pixel-level defect detection between the standard VQ-VAE and our PVQAE. Figure 4 shows the defect detection results on multiple objects. To highlight our method's advantage, we focused on defect samples where the standard VQ-VAE struggled to locate and segment defective areas, particularly when defects were small or subtle. The defect scores from both methods are displayed side by side, with pixel-level scores visualized using a color spectrum-hotter colors (e.g., red) indicating higher defect likelihood and cooler colors (e.g., blue) representing lower likelihood. It is evident that the score maps from PVQAE had more concentrated red areas precisely aligning with the defect regions and cleaner backgrounds in normal areas. This demon-strates that our dynamic code allocation and learned normal budgets enhanced the contrast between defects and normal samples, significantly improving detection performance."}, {"title": "D. Ablation Analysis", "content": "We further analyzed the effectiveness of the proposed bud-get learning mechanism through two ablation studies: one on the progressive budget learning scheme and another on defect detection using learned normal budget priors.\n1) Progressive Budget Learning: Progressive budget learn-ing is controlled by the hyper-parameter $\\lambda$, which adjusts the weight of the budget learning loss in the overall objective. To understand the impact of $\\lambda$, we conducted two ablation studies on its scheduling and the terminal maximum weight. First, we tested three different $\\lambda$ schedules: 1) Constant=1, 2) Cosine schedule varying $\\lambda$ from 0 to 1, and 3) Linear schedule with $\\lambda$ from 0 to 1. As shown in Table II, the Linear schedule provided the best defect detection performance. Second, we examined the optimal choice for maximum $\\lambda$ by using a linear schedule with varying terminal weights, ranging from 0.5 to 2.5 in steps of 0.25. As shown in Figure 5, detection perfor-mance improved as the weight increased, peaking around 1.25, after which performance declined with further increases.\n2) Normal Budget Priors: We also evaluated the effec-tiveness of normal budget priors in defect detection. In this study, we first tested removing the prior learning but relying on dynamically allocated code budgets during inference. We then compared this with models using either universal and per-class priors. The only difference was whether a class token was added to budget sequence during training. We reported the average image and pixel-level AUROCs on MVTeCAD for comparison. As shown in Table III, per-class prior achieved the best performance, demonstrating its effectiveness."}, {"title": "V. CONCLUSION", "content": "In this paper, we present the Patch-aware Vector Quantized Auto-Encoder (PVQAE), a refined model for unsupervised defect detection. It improves enhances the conventional VQ-VAE by specifically tailoring the codebook learning process for one-class defect detection. Unlike previous methods, our approach dynamically allocates finer latent codes to regions with rich context and coarser ones to less informative areas. This data-driven method removes the need for manual capacity setting. A budget learning objective enables optimal code allocation end-to-end, making the capacity fully adaptable. Normal sample priors further enhance defect detection by constraining defect reconstruction quality. Our method offers key insights for one-class defect detection systems."}]}