{"title": "Scaling Law for Post-training after Model Pruning", "authors": ["Xiaodong Chen", "Yuxuan Hu", "Jing Zhang", "Xiaokang Zhang", "Cuiping Li", "Hong Chen"], "abstract": "Large language models (LLMs) based on the Transformer architecture are widely employed across various domains and tasks. However, their increasing size imposes significant hardware demands, limiting practical deployment. To mitigate this, model pruning techniques have been developed to create more efficient models while maintaining high performance. Despite this, post-training after pruning is crucial for performance recovery and can be resource-intensive. This paper investigates the post-training requirements of pruned LLMs and introduces a scaling law to determine the optimal amount of post-training data. Post-training experiments with the Llama-3 and Qwen-2.5 series models, pruned using depth pruning, width pruning, and 2:4 semi-structured pruning, show that higher pruning ratios necessitate more post-training data for performance recovery, whereas larger LLMs require less. The proposed scaling law predicts a model's loss based on its parameter counts before and after pruning, as well as the post-training token counts. Furthermore, we find that the scaling law established from smaller LLMs can be reliably extrapolated to larger LLMs. This work provides valuable insights into the post-training of pruned LLMs and offers a practical scaling law for optimizing post-training data usage.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) based on the Transformer architecture (Vaswani et al., 2017) have attracted widespread attention and are applied across diverse domains and tasks. However, as LLMs grow in size, their hardware demands increase substantially, limiting their practical deployment in real-world scenarios. To address this challenge, researchers have focused on developing compact models through model pruning techniques (Han et al., 2016) that maintain high performance while reducing hardware requirements.\nModel pruning can be broadly categorized into unstructured pruning (Frantar and Alistarh, 2023; Sun et al., 2024; Zhang et al., 2024) and structured pruning (Chen et al., 2024; Hu et al., 2024; Liu et al., 2024; Muralidharan et al., 2024; Ma et al., 2023; Ashkboos et al., 2024; Men et al., 2024). Unstructured pruning removes individual elements from weight matrices, resulting in sparse matrices while maintaining model performance, but it is not hardware-friendly and does not effectively accelerate computation. Semi-structured pruning, a variant of unstructured pruning, leverages hardware support (Mishra et al., 2021) to achieve acceleration, though it may cause greater performance degradation compared to unstructured pruning. Structured pruning, on the other hand, removes entire components such as attention heads or layers, effectively reducing the parameter count but often with a higher performance loss compared to other pruning techniques.\nTo effectively utilize compact models obtained from semi-structured or structured pruning, post-training after pruning (Ashkboos et al., 2024; Chen et al., 2024; Yang et al., 2024; Ma et al., 2023; Kim et al., 2024) is essential to mitigate performance decline. Some model pruning approaches employ continual pre-training or Parameter-Efficient Fine-Tuning (PEFT) as the post-training stage after pruning. For example, Shortened Llama (Kim et al., 2024) uses 627B tokens of pre-training data for continual pre-training of the pruned LLMs, whereas LLM-Pruner (Ma et al., 2023) utilizes 50,000 instruction data points for LoRA fine-tuning (Hu et al., 2021). However, LoRA fine-tuning with a limited amount of instruction data is insufficient to fully restore the model's performance, whereas continual pre-training with a large dataset can achieve full recovery but requires substantial hardware resources. Given the significant hardware demands,\nthis raises the question: is it truly necessary to use such a vast amount of data for model recovery? LLM-Streamline (Chen et al., 2024) finds that using large amounts of data for post-training only slightly improves performance compared to using a more appropriate amount. Therefore, identifying the optimal amount of post-training data is crucial for the performance recovery of pruned LLMs and is more resource-efficient. This naturally leads to the question of whether a similar scaling law, like those observed during pre-training of LLMs, could be established to predict the the optimal amount of post-training data after model pruning.\nIn this paper, we conduct post-training experiments on the Llama-3 series (Dubey et al., 2024) and Qwen-2.5 series models (Team, 2024), employing depth pruning, width pruning, and 2:4 semi-structured pruning. From these experiments, we establish a scaling law for post-training after model pruning. Specifically, we have two key observations. First, we find that as the pruning ratio increases, the amount of data required for adequate post-training also increases. For example, as shown in Figure 1, with depth pruning, the loss curve of Llama-3.1-8B approaches convergence after post-training on approximately 1B tokens at a 16% pruning ratio. However, at pruning ratios of 24% and 33%, the loss curve continues to decline, indicating a need for more post-training data. Second, we find that larger LLMs require less data to recover their performance after pruning, which contradicts the intuitive assumption that larger LLMs would need more data. As illustrated in Figure 2, with 2:4 semi-structured pruning, increasing model parameter counts generally results in a flatter post-training loss curve. For instance, the loss curve of Llama-3.1-8B approaches convergence with approximately 0.5B tokens, while the smaller Llama-3.2-1B's loss curve continues to decline, suggesting that larger LLMs require less post-training data.\nConsidering these two key observations and the established scaling laws for the pre-training of LLMs (Hoffmann et al., 2022), we determine the form of the scaling law for post-training after model pruning and fit these laws to the three pruning methods across the Llama-3 series and Qwen-2.5 series models. Given the model parameter counts before and after pruning, as well as the post-training token counts, we can predict the model's loss based on the fitted scaling law, enabling us to determine the optimal amount of post-training data. Finally, we find that the scaling law established from smaller LLMs can be reliably extrapolated to larger LLMs. suggesting that the scaling law we established has the potential to predict the rate of decline in the loss curve for even larger pruned LLMs.\nOverall, this paper makes the following contributions:\n\u2022 We identify two key observations in the post-training of LLMs pruned using depth pruning, width pruning, and 2:4 semi-structured pruning: pruned LLMs with higher pruning ratios require more post-training data to restore their performance, while larger LLMs require less post-training data to recover performance.\n\u2022 Based on the observations we have discovered, we determine the form of the scaling law for post-training after model pruning and fit these laws to the three pruning methods across the Llama-3 seires and Qwen-2.5 series models. This scaling law allows us to"}, {"title": "Related Work", "content": "Model pruning can be categorized into unstructured pruning (Frantar and Alistarh, 2023; Sun et al., 2024; Zhang et al., 2024) and structured pruning (Chen et al., 2024; Hu et al., 2024; Liu et al., 2024; Muralidharan et al., 2024; Ashkboos et al., 2024; Song et al., 2024; Gromov et al., 2024).\nStructured pruning: Recent papers on structured pruning for LLMs can be broadly divided into two categories: (1) pruning depth (Chen et al., 2024; Song et al., 2024; Gromov et al., 2024), which involves reducing the number of layers in the LLMs, and (2) pruning width (Ashkboos et al., 2024; Hu et al., 2024; Liu et al., 2024), which involves reducing the hidden state size, the number of attention heads, or the intermediate size of the FFN. In this paper, we adopt the metrics from these papers to perform depth and width pruning in our experiments.\nUnstructured pruning: Unstructured pruning compresses LLMs by removing individual unimportant elements from the weight matrices, converting dense matrices into sparse ones. However, due to its hardware inefficiency, unstructured pruning generally only accelerates models when a specific sparsity pattern, such as 2:4 sparsity, is applied. This approach is known as semi-structured pruning. In this paper, we conduct post-training experiments on 2:4 semi-structured pruning."}, {"title": "Scaling Law for the pre-training of LLMs", "content": "The scaling law of the pre-training of LLMs (Hoffmann et al., 2022; Kaplan et al., 2020) describes the relationship between model performance and factors including the parameter counts, pre-training token counts, and computational resources used during the pre-training process. It establishes a power-law correlation among these variables, demonstrating how performance improves as these factors increase. Scaling laws are invaluable for pre-training LLMs because they help predict model performance across varying model parameter counts and pre-training token counts, enabling informed decision-making.\nIn this paper, we focus on the scaling law for post-training after model pruning."}, {"title": "Pruning Methods", "content": "In this section, we introduce three pruning methods: two structured pruning methods\u2014depth pruning and width pruning\u2014and a special case of unstructured pruning method known as 2:4 semi-structured pruning. We initially apply these methods to prune LLMs and subsequently conduct post-training experiments on the pruned LLMs."}, {"title": "Depth Pruning", "content": "Following the existing depth pruning methods (Men et al., 2024; Chen et al., 2024; Yang et al., 2024), we estimate the layer importance using cosine similarity and prune layers with lower importance. Specifically, we randomly select N samples from the pre-training data (detailed in Section 4.1). We then record the hidden states generated by the LLMs for these samples and compute the cosine similarity between the input and output hidden states of each layer. Assuming that the input hidden states of layer i are represented by $x^{(i)}$, the importance score(IS) of layer i is computed as:\n$IS_{layer,i} = \\frac{1}{N} \\sum_{j=1}^{N} \\frac{1}{L} \\sum_{k=1}^{L} \\frac{x_{j,k}^{(i)} x_{j,k}^{(i+1)}}{||x_{j,k}^{(i)}|| ||x_{j,k}^{(i+1)}||}$\nwhere $x^{(i)}, x^{(i+1)} \\in R^{d \\times L}$ denotes the input and output hidden states of the j-th sample respectively, d denotes the hidden size, L denotes the sequence length. Given the number of pruned layers n determined by the target sparsity, we remove the n layers corresponding to the top-n highest cosine similarities for pruning."}, {"title": "Width Pruning", "content": "Recently, width pruning for LLMs has primarily relied on gradient-based or Taylor-based metrics (Ma et al., 2023; van der Ouderaa et al., 2023). However, due to the vast number of parameters in LLMs, computing and storing gradients is both computationally expensive and memory-intensive. Therefore, following the approaches of Wanda (Sun et al., 2024) and MINITRON (Muralidharan et al., 2024), we utilize activation-based metrics for width pruning, thereby reducing computational and memory demands. Specifically, we randomly select N samples from the pre-training data (detailed in Section 4.1) and assess the importance of embedding channels by analyzing the activations generated by the LayerNorm layers. We then prune the least important channels based on this analysis. The formula for calculating the importance score (IS) of embedding channels (emb) is as follows:\n$I S_{emb,i} = \\frac{1}{N} \\sum_{j=1}^{N} \\frac{1}{L} \\sum_{k=1}^{L} LN_{x_{j,k,i}}$\nwhere $LN_{x_{j,k,i}}$ denotes the input of the i-th channel of the k-th token in the j-th sample at the LayerNorm layer, and LN denotes the Layer Normalization operation. Given a specific sparsity, we calculate the number of embedding channels that need to be pruned, and then remove the channels with the lowest importance."}, {"title": "2:4 Semi-structured Pruning", "content": "Unstructured pruning removes individual unimportant elements from the weight matrices, converting dense matrices into sparse ones. When the sparsity structure follows a specific pattern, such as 2:4 sparsity (Mishra et al., 2021), the model can be efficiently accelerated. This approach is known as semi-structured pruning. Let W represent the weight matrix of a linear layer of an LLM, x represent the input of the linear layer. The object of semi-structured pruning is to learn a sparsity mask M and an updated weight \u0394W so that the dense matrix W is transformed into a sparse matrix W:\n$\\min ||Wx - \\hat{Wx}||$\ns.t. $\\hat{W} = M \\cdot (W + \\Delta W)$\nwhere $W \\in R^{d_{out} \\times d_{in}}$, $M \\in {0,1}^{d_{out} \\times d_{in}}$, $\\Delta W \\in R^{d_{out} d_{in}}$ and $x \\in R^{d_{in}}$.\nWe randomly select some samples from the pre-training data (detailed in Section 4.1) and use SparseGPT (Frantar and Alistarh, 2023) to optimize the aforementioned objectives. We then train this 2:4 sparse model obtained from SparseGPT. Inspired by SP-LoRA (Anonymous, 2024), we combine the updated weight \u0394W from each training iteration t with the mask M during the post-training process to ensure the model's sparsity:\n$W^{t} = W^{t-1} + M \\cdot \\Delta W$"}, {"title": "Scaling Law for Post-training after Model Pruning", "content": "In this section, we conduct experiments on six models from the Llama-3 series and Qwen-2.5 series of different size pruned using depth pruning, width pruning, and 2:4 semi-structured pruning. From these experiments, we observe two key observations: higher pruning ratios require more data to restore model performance, while larger LLMs need less data to recover. Based on these observations and established scaling laws for the pre-training of LLMs, we establish the scaling law for post-training after model pruning and fit this to the three pruning methods across the Llama-3 and Qwen-2.5 series models. Finally, we find that the scaling law fitted from smaller LLMs can be reliably extrapolated to larger LLMs. The rate of decrease in the predicted loss curve for the larger LLMs closely matches that of the actual loss curve, suggesting that the scaling law we established has the potential to predict the post-training loss curve for even larger pruned LLMs."}, {"title": "Setting", "content": "we conduct experiments on six models from the Llama-3 series and Qwen-2.5 series: Llama-3.2-1B, Llama-3.2-3B, Llama-3.1-8B, Qwen-2.5-0.5B, Qwen-2.5-1.5B and Qwen-2.5-3B.\nPruning setting: As mentioned in Section 3.1, Section 3.2 and Section 3.3, we randomly select 1,024 data samples from the pre-training dataset SlimPajama (Soboleva et al., 2023) for pruning. As shown in the Table 1, we apply different pruning ratios for different models with depth pruning and width pruning.\nTraining setting: For Llama-3.2-3B, Qwen-2.5-3B and Llama-3.1-8B, we randomly select 1B tokens from SlimPajama for post-training. For Llama-3.2-1B, Qwen-2.5-0.5B, Qwen-2.5-1.5B, we randomly select 0.5B tokens for post-training."}, {"title": "Two Key Observations", "content": "We present the post-training loss curves of models pruned using different methods and at various ratios in Figures 3, 4, 5, 6, 7, and 8. The experiment results demonstrate that post-training on models obtained through all the three pruning methods significantly reduces the loss on the pre-training data, underscoring the effectiveness and necessity of post-training after pruning.\nBy examining the loss curves of models pruned using depth pruning and width pruning, we have two key observations:\n\u2022 First, for a given model, higher pruning ratios require more data to recover performance. For example, in the case of Llama-3.1-8B with width pruning, the loss curve approaches convergence after post-training on approximately 0.5B tokens at a 15% pruning ratio. However, at pruning ratios of 25% and 35%, the loss curve continues to show a clear downward trend, indicating the need for more data.\n\u2022 Second, with a fixed pruning ratio, smaller LLMs require more data to regain performance. For instance, with a 15% pruning ratio using width pruning, the loss curve of Llama-3.1-8B is significantly flatter than those of Llama-3.2-3B and Llama-3.2-1B, reaching convergence with less data. This unexpected finding contradicts the intuition that larger LLMs would require more post-training data."}, {"title": "Scaling Law", "content": "Previously, DeepMind proposed the following scaling law for the pre-training of LLMs (Hoffmann et al., 2022):\n$L(N, D) = \\frac{N_c}{N^\\alpha} + \\frac{D_c}{D^\\beta} + E$\nwhere $N_c$, $D_c$, \u0395, \u03b1, \u03b2 denotes constants, N denotes the model parameter counts, D denotes the pre-training token counts and L denotes the model's loss on the pre-training dataset.\nThis scaling law indicates that during the pre-training process, the model's loss exhibits a power-law relationship with both the model parameter counts and the pre-training token counts. Building on this scaling law and integrating the two observations proposed in Section 4.2, we formulate a novel scaling law for post-training after model pruning:\n$L(N_0, N, D, L_0) = L_0 + (1 - \\frac{N}{N_0}) ((\\frac{N}{N_0})^\\gamma - (\\frac{N}{N_0})^\\delta) (\\frac{N_c}{N^\\alpha} + \\frac{D_c}{D^\\beta} + E)$\nwhere $N_C$, $D_C$, \u0395, \u03b1, \u03b2, \u03b3, \u03b4 denotes constants, $N_0$ denotes the model parameter counts before pruning, N denotes the model parameter counts after pruning, D denotes the post-training token counts, $L_0$ denotes the model's loss before pruning on the post-training dataset and L denotes the model's loss after pruning on the post-training dataset.\nWe observe that different pruning ratios impact the rate at which the loss curve decreases. To account for this, we modify the original scaling law by introducing a scaling factor, $(1 - \\frac{N}{N_0})$. Additionally, since the model parameter counts before pruning also affects the rate of decline in the loss curve, we incorporate another factor, $((\\frac{N}{N_0})^\\gamma - (\\frac{N}{N_0})^\\delta)$. Finally, to ensure the scaling law reflects the model's loss before pruning when the pruning rate is zero, we add $L_0$ to the scaling law. Additionally, since there is no pruning ratio in the 2:4 semi-structured pruning, we modify the scaling law to:\n$L(N_0, N, D) = (\\frac{1}{N_0})^\\epsilon (\\frac{N_c}{N^\\alpha} + \\frac{D_c}{D^\\beta} + E)$\nAfter establishing the form of the scaling law, we use the checkpoints recorded during the post-training process to fit the scaling law for three different pruning methods. The scaling laws we established for the Llama-3 series model are as follow:\n$L(N_0, N, D, L_0) = L_0 + (1 - \\frac{N}{N_0})^{1.18} ((\\frac{N}{N_0})^{0.02} - (\\frac{N}{N_0})^{1.08}) (\\frac{5.93}{N^{1.68}} + \\frac{0.19}{D^{0.24}} + 0.221)$\n$L(N_0, N, D, L_0) = L_0 + (1 - \\frac{N}{N_0})^{1.18} ((\\frac{N}{N_0})^{0.01} - (\\frac{N}{N_0})^{1.08}) (\\frac{5.09}{N^{2.71}} + \\frac{0.11}{D^{0.19}} + 0.201)$\n$L(N_0, N, D) = (\\frac{1}{N_0})^{0.17} ((\\frac{N}{N_0})^{2.67} - (\\frac{N}{N_0})^{0.51}) (\\frac{3.78}{N^{0.56}} + \\frac{0.25}{D^{0.02}} - 2.37)$\nwhere N denotes the model parameter counts, measured in billions, and D denotes the post-training tokens counts, measured in millions. Eq. 8 presents the fitted scaling law for depth pruning, while Eq. 9 corresponds to the fitted scaling law for width pruning, and Eq. 10 for 2:4 semi-structured pruning. Similarly, the scaling laws for the Qwen-2.5 series model are as follows:\n$L(N_0, N, D, L_0) = L_0 + (1 - \\frac{N}{N_0})^{1.11} ((\\frac{N}{N_0})^{0.11} - (\\frac{N}{N_0})^{1.11}) (\\frac{4.71}{N^{1.62}} + \\frac{0.45}{D^{0.23}} + 0.26)$\n$L(N_0, N, D, L_0) = L_0 + (1 - \\frac{N}{N_0})^{1.19} ((\\frac{N}{N_0})^{0.01} - (\\frac{N}{N_0})^{1.19}) (\\frac{5.28}{N^{2.75}} + \\frac{0.28}{D^{0.19}} + 0.25)$\n$L(N_0, N, D) = (\\frac{1}{N_0})^{0.17} ((\\frac{N}{N_0})^{0.92} - (\\frac{N}{N_0})^{2.03}) (\\frac{3.32}{N^{0.13}} + \\frac{0.32}{D^{0.10}} + 0.25)$\nEq. 11 presents the fitted scaling law for depth pruning, while Eq. 12 corresponds to the fitted scaling law for width pruning, and Eq. 13 for 2:4 semi-structured pruning. We observe that the scaling law for width pruning fitted on the Llama-3 series and Qwen-2.5 series models is remarkably similar. This suggests that the Llama-3 series and Qwen-2.5 series models exhibit a similar level of redundancy in terms of width."}, {"title": "Extrapolate from Smaller LLMs to Larger LLMs", "content": "In this section, we extrapolate the scaling law established from smaller LLMs to larger LLMs. We first fit the scaling law using smaller LLMs: Llama-3.2-1B, Llama-3.2-3B, Qwen-2.5-0.5B, and Qwen-2.5-1.5B. We then validate the fitted scaling law using larger LLMs, specifically Llama-3.1-8B and Qwen-2.5-3B. Due to the limited loss curves for the 2:4 semi-structured pruning, we do not conduct extrapolation experiments for 2:4 semi-structured pruning. In addition, due to the anomaly observed in width pruning for Llama-3.1-8B, as discussed in Section 4.3, we do not conduct width pruning experiments on the Llama-3 series models.\nAs shown in Figure 11c, the predicted loss curves for Qwen-2.5-3B based on the scaling law fitted to width pruning nearly overlap with the actual loss curves. In other cases, such as those depicted in Figure 11a and Figure 11b, the predicted loss curves for larger LLMs closely track the actual curves, particularly in terms of their rate of decline after training stabilizes. This demonstrates that the convergence points of larger LLMs can be accurately predicted based on the scaling law derived from smaller LLMs, confirming that the scaling law can be reliably extrapolated."}, {"title": "Conclusion", "content": "In this paper, we conduct the post-training experiments of six models from the Llama-3 series and Qwen-2.5 series, spanning various sizes, pruned using depth, width, and 2:4 semi-structured pruning techniques. Our findings highlight two key observations: higher pruning ratios demand more post-training data for performance recovery, while larger LLMs require less. These insights lead us to establish a scaling law for post-training after pruning, which predicts a model's loss based on its parameter counts before and after pruning, as well as the post-training token counts. Notably, this scaling law can be reliably extrapolated from smaller to larger LLMs, offering a resource-efficient approach to determining the optimal amount of post-training data. Our work provides important insights into the post-training dynamics of pruned LLMs, facilitating the more practical deployment of compact models in real-world applications."}]}