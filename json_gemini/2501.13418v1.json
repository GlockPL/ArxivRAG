{"title": "Rethinking the Sample Relations for Few-Shot Classification", "authors": ["Guowei Yin", "Sheng Huang", "Luwen Huangfu", "Yi Zhang", "Xiaohong Zhang"], "abstract": "Feature quality is paramount for classification performance, particularly in few-shot scenarios. Contrastive learning, a widely adopted technique for enhancing feature quality, leverages sample relations to extract intrinsic features that capture semantic information and has achieved remarkable success in Few-Shot Learning (FSL). Nevertheless, current few-shot contrastive learning approaches often overlook the semantic similarity discrepancies at different granularities when employing the same modeling approach for different sample relations, which limits the potential of few-shot contrastive learning. In this paper, we introduce a straightforward yet effective contrastive learning approach, Multi-Grained Relation Contrastive Learning (MGRCL), as a pre-training feature learning model to boost few-shot learning by meticulously modeling sample relations at different granularities. MGRCL categorizes sample relations into three types: intra-sample relation of the same sample under different transformations, intra-class relation of homogenous samples, and inter-class relation of inhomogeneous samples. In MGRCL, we design Transformation Consistency Learning (TCL) to ensure the rigorous semantic", "sections": [{"title": "1. Introduction", "content": "Nowadays, deep learning has achieved significant advancements in various artificial intelligence tasks [1, 2, 3], which are heavily dependent on the presence of ample labeled data. However, in practical scenarios, obtaining large-scale labeled data is often challenging due to the scarcity of relevant images and the substantial expense associated with manual annotation. To address this problem, the concept of Few-shot learning (FSL) has emerged. The aim of FSL is to train a model using abundant labeled data from base classes and then apply this acquired knowledge to novel classes that only have a limited number of labeled samples per class (e.g., 1 or 5).\nTo tackle the FSL problem, numerous approaches have been developed. Many of these approaches adopt meta-learning to solve FSL by either designing optimal algorithms [4, 5, 6] or learning good metrics [7, 8, 9, 10]. These methods simulate the FSL tasks during the training phase and attempt to train a base model that can swiftly adjust to new tasks. For instance, [10] proposes a novel light transformer-based global information enhanced metric-learning classification model to obtain better embedding for FSL. Additionally, based on the intuition of adding extra samples to mitigate the issue of limited data, many data-augmentation-based methods [11, 12, 13, 14] have been proposed. They enhance sample diversity by synthesizing additional samples. Such as, STVAE [14] introduces a generative FSL approach that exploits the complementarity of semantic and visual prior information to synthesize features for novel classes. However, these approaches always involve"}, {"title": "2. Related Work", "content": "As a feature learning method, contrastive learning has recently gained popularity due to its ability to derive meaningful representations from images. Many contrastive learning methods have been developed for image"}, {"title": "3. Methodology", "content": "To solve this issue, we rethink the sample relations and categorize them into three types according to the different granularities: intra-sample relation of the same sample under different transformations, intra-class relation of homogenous samples, and inter-class relation of inhomogeneous samples.\nThen we present a novel Multi-Grained Relation Contrastive Learning approach (MGRCL) for FSL to model the different sample relations. As illustrated in Fig. 2, MGRCL contains three main components: the base learner, the Transformation Consistency Learning (TCL), and the Class Contrastive Learning (CCL). In particular, the base learner is a neural network trained by a general image classification task. TCL is designed to constrain the different transformations of the same sample to have consistent semantic content. And CCL is used to constrain that homogenous samples have similar semantic content and nonhomogeneous samples have different semantic content.\nNext, we will provide a more detailed explanation of each component."}, {"title": "3.1. Problem Formulation", "content": "In few-shot learning (FSL), the dataset can be denoted as $\\mathcal{D} = {\\mathcal{D}_{base}, \\mathcal{D}_{novel}}$. $\\mathcal{D}_{base}$ is the base dataset with $C_{base}$ classes, and $\\mathcal{D}_{novel}$ is the novel dataset with $C_{novel}$ classes, where $C_{novel}$ is disjoint from $C_{base}$. FSL strives to obtain a well-generalized feature extractor using $\\mathcal{D}_{base}$ and can achieve good performance in $\\mathcal{D}_{novel}$ during the testing phase.\nIn FSL, $\\mathcal{D}_{base}$ is used to train a well-generalized model in the pre-training phase. The testing phase contains lots of FSL tasks drawn from $\\mathcal{D}_{novel}$, and each FSL task can be regarded as a $N$-way $K$-shot classification problem, where $N$ represents the number of categories, $K$ represents the number of labeled samples per category. Usually, $N = 5$ and $K = 1$ or $5$. Each task $\\mathcal{T}$ includes a support set $\\mathcal{S}_{\\mathcal{T}}$ and a query set $\\mathcal{Q}_{\\mathcal{T}}$,\n$\\mathcal{T} = (\\mathcal{S}_{\\mathcal{T}}, \\mathcal{Q}_{\\mathcal{T}})$.\nHere, $\\mathcal{S}_{\\mathcal{T}}$ contains $K$ labeled samples from each of $N$ categories, and $\\mathcal{Q}_{\\mathcal{T}}$ contains $Q$ samples from the same $N$ classes. $\\mathcal{S}_{\\mathcal{T}}$ and $\\mathcal{Q}_{\\mathcal{T}}$ are disjoint. In the testing phase, the model is trained using $\\mathcal{S}_{\\mathcal{T}}$, while $\\mathcal{Q}_{\\mathcal{T}}$ is employed to evaluate the performance."}, {"title": "3.2. The Base Learner", "content": "As depicted in Fig. 2, the base learner, denoted as $\\mathcal{F}_{\\theta}$ with parameters $\\theta$, is employed for the extraction of feature embeddings. Let $(\\textbf{x}, y) \\in \\mathcal{D}_{base}$ denote an image and its corresponding label sampled from $\\mathcal{D}_{base}$. The feature vector $\\textbf{v}$ of an image $\\textbf{x}$ can be obtained by $\\mathcal{F}_{\\theta}$: $\\textbf{v} = \\mathcal{F}_{\\theta}(\\textbf{x})$. Then, a classifier $\\mathcal{F}_{\\phi}$ with parameters $\\phi$ is employed to get the predicted confidence scores $\\textbf{p}$ by projecting the feature vector $\\textbf{v}$ into the label space: $\\textbf{p} = \\mathcal{F}_{\\phi}(\\textbf{v})$. Finally, we can derive the predicted label $\\hat{y}$ by applying the softmax operator on $\\textbf{p}$: $\\hat{y} = \\text{Softmax}(\\textbf{p})$. The parameters $\\theta, \\phi$ of the base learner are optimized jointly by minimizing classification loss $\\mathcal{L}_{cls}$ on the entire base dataset $\\mathcal{D}_{base}$,\n$\\mathcal{L}_{cls} = \\frac{1}{|\\mathcal{D}_{base}|} \\sum_{\\{\\textbf{x},y\\} \\in \\mathcal{D}_{base}} y \\log \\hat{y}.$\nTo avoid overfitting on the training set, many methods [18, 19, 35] incorporate transformed samples and predict which image transformation is performed during training. Following these methods, we also add a self-supervised module (SS) with a Multilayer Perceptron (MLP). Consider $\\mathcal{X} = {\\textbf{x}^{(1)}, ..., \\textbf{x}^{(M)}}$ as the collection of transformed versions of a single image, where $M$ denotes the total number of transformed samples and $^{(m)}$ signifies the $m$-th transformed version of the image. $\\mathcal{X}$ can be obtained by applying a series of transformations on the image, such as cropping, resizing, rotation, etc. The transformed images $\\mathcal{X}$, together with the original image $\\textbf{x}$, are simultaneously input into the model for both classification and self-supervised tasks. The objective of the self-supervised task is to identify the transformation applied to the image,"}, {"title": "3.3. Transformation Consistency Learning", "content": "The different transformations of one sample should have the same semantic content as their original version since these images contain exactly the same objects and backgrounds. To accomplish this objective, we design a Transformation Consistency Learning (TCL) component to constrain the intra-sample relation of the same sample under different transformations. The label output can represent the semantic content of the sample because it reflects the predicted probability of the sample in each category. Therefore, to ensure that the transformations of one sample have consistent semantic content, we constrain them to have the same label outputs.\nIn the base learner, we feed the transformed versions of one sample into the network along with its original version, and calculate the TCL loss on their predicted label outputs. Here, we use Jensen-Shannon Divergence [36, 37] as TCL loss to constrain the intra-sample relation,\n$\\mathcal{L}_{tcl} = \\frac{1}{|\\mathcal{D}_{base}|} \\sum_{\\textbf{x} \\in \\mathcal{D}_{base}} \\frac{1}{M} \\sum_{m=1}^M JS(\\hat{y}_{\\textbf{x}}, \\hat{y}_{\\textbf{x}}^{(m)})$,$\\mathcal{L}_{ss} = \\frac{1}{|\\mathcal{D}_{base}|} \\frac{1}{M + 1} \\sum_{\\textbf{x} \\in \\mathcal{D}_{base}} \\sum_{m=0}^M \\sum_{j} s^{(m)} log \\hat{s}^{(m)}$\nwhere $\\hat{y}_{\\textbf{x}}$ and $\\hat{y}_{\\textbf{x}}^{(m)}$ are the smoothed label outputs of the original image and the $m$-th transformed image, respectively. They are obtained by,"}, {"title": "3.4. Class Contrastive Learning", "content": "To enforce the intra-class relation and the inter-class relation, we use Class Contrastive Learning (CCL) to maximize the similarity of feature embeddings of homogenous samples while minimizing the similarity of feature embeddings of inhomogeneous samples. Therefore, for each image, we need another homogenous sample and several inhomogeneous samples. To achieve this goal and accelerate training, we employ a memory bank [38] to store and sample feature vectors. The memory bank holds the feature embeddings for all images in the dataset, enabling the model to sample a diverse set of features from a wider range of images, rather than being restricted to the current mini-batch. For each mini-batch, a feature embedding is sampled from the memory bank for each class of images, then CCL loss can be defined as,\n$\\mathcal{L}_{ccl} = \\frac{1}{|\\mathcal{D}_{base}|} \\sum_{\\textbf{x} \\in \\mathcal{D}_{base}} -log \\frac{exp(\\frac{cos(\\textbf{v},\\textbf{v}\\')}{\\tau_2})}{\\sum_{i=1}^{C_{base}} exp(\\frac{cos(\\textbf{v},\\textbf{v}\\_i)}{\\tau_2})}$\nwhere $C_{base}$ is the number of base classes, $\\textbf{v}, \\textbf{v}\\'$ are the feature embedding of a sample and its homogenous sample, $\\textbf{v}\\_i$ represents the feature embedding of a sample from $i$-th class. Here $\\textbf{v}\\'$ and $\\textbf{v}\\_i$ are sampled from the memory bank. $cos(\u00b7)$ is the cosine similarity. And $\\tau_2$ is a temperature parameter, we set it to 0.1 following [26, 25]. Moreover, the memory bank is updated by,\n$\\textbf{v}_k = r \\times \\textbf{v}_k + (1 - r) \\times \\textbf{v}\\_q,$\nwhere $\\textbf{v}\\_q$ and $\\textbf{v}\\_k$ represent the feature embedding of an image obtained in the current mini-batch and the same image stored in the memory bank, $r$ is used to adjust the updating speed of the memory bank, and we set it to 0.99 following [18]. During the training phase, the memory bank is completely updated with each epoch."}, {"title": "3.5. Overall Optimization Objective", "content": "In summary, by incorporating both two components which constrain sample relations at different granularities, the total loss can be written as,\n$\\mathcal{L}_{total} = \\mathcal{L}_{base} + \\alpha \u00b7 \\mathcal{L}_{tcl} + \\beta \u00b7 \\mathcal{L}_{ccl},$\nwhere $\\alpha$ and $\\beta$ are the hyper-parameters for balancing the different losses."}, {"title": "3.6. Few-shot Evaluation", "content": "As depicted in Section 3.1, during the testing phase, the performance of our model is assessed by tackling abundant FSL tasks that are drawn from $\\mathcal{D}_{novel}$. For every task $\\mathcal{T}$, we keep our model parameters constant and employ the feature extractor $\\mathcal{F}_{\\theta}$ to derive the feature representations for $\\mathcal{S}_{\\mathcal{T}}$ and $\\mathcal{Q}_{\\mathcal{T}}$. Then, we employ a logistic regression classifier to classify samples of $\\mathcal{Q}_{\\mathcal{T}}$, which is trained on the feature representations of $\\mathcal{S}_{\\mathcal{T}}$."}, {"title": "4. Experiments", "content": "We conduct experiments on four popular FSL benchmarks, which include three general datasets: miniImageNet [8], tieredImageNet [28], CIFAR-FS [29], and a fine-grained dataset: CUB-200-2011 (CUB) [30]. For all datasets, we keep the splitting protocol as same as [39]. And in our experiments, for miniImageNet, tieredImageNet, and CUB, the image size is 84\u00d784, while for CIFAR-FS, the image size is 32\u00d732.\nFollowing previous works [17, 9, 18], we adopt ResNet-12 as our backbone. Transformation Consistency Learning (TCL) is performed on the label outputs and Class Contrastive Learning (CCL) is performed on the features after global pooling. These techniques do not require additional network layers. And we add a self-supervised learning module with an MLP consisting of two fully connected layers, one batch-normalization layer, and an activation function."}, {"title": "4.1. Optimization Setup", "content": "For all experiments, we employ the SGD optimizer with a momentum of 0.9 and a weight decay of 5e-4. The learning rate is initially set to 0.05 and is subsequently reduced by a factor of 0.1. For tieredImageNet, we train 60 epochs, and the learning rate decays after epochs 30, 40, and 50 respectively. For other datasets, we train 80 epochs, and the learning rate decays after epoch 60 and epoch 70. For the CIFAR-FS dataset, we set the batch size to 64, and for other datasets, we set the batch size to 32. Regarding the hyper-parameters, we have assigned the following values: \u03b1 = 1.0, \u03b2 = 0.1, T\u2081 = 4.0, T2 = 0.1."}, {"title": "4.2. Data Augmentation", "content": "To alleviate overfitting problems and implement our Transformation Consistency Learning (TCL), we add some augmented samples to train the feature extractor. The data augmentation contains three scaling transformations, three rotation transformations, one random erasing, one graying, and one Sobel edge detection."}, {"title": "4.3. Evaluation Protocol", "content": "For all datasets, we sample 2,000 few-shot classification tasks and compute the mean classification accuracy along with a 95% confidence interval to assess the performance of our method. Each task can be regarded as a N-way K-shot classification problem, as described in Section 3.1. For each task T = (ST, QT), 5 categories are selected randomly from the dataset Dnovel. The support set ST contains either 1 or 5 labeled samples per category selected, depending on whether the task is a 1-shot or 5-shot scenario. Meanwhile, the query set QT includes 15 samples per category, with no overlap between the samples in ST and Q7. The support set is used to train the classifier, while the query set is utilized to evaluate the performance of the model."}, {"title": "4.4. Comparison with Other Methods", "content": "To assess the efficacy of our proposed method, we have conducted extensive experiments on four datasets. Table 1, 2, 3, and 4 show the performances of some SOTA FSL methods and ours. In these tables, The backbone a-b-c-d denotes a 4-layer convolutional network with a, b, c, and d filters in each layer. Resnet-n refers to a ResNet network with n layers of filters."}, {"title": "General Few-Shot Classification", "content": "As shown in Table 1 and 2, our method achieves promising performance compared to other methods on miniImageNet and tieredImageNet. To be specific, our method achieves 69.57%, 84.41% for 1-shot and 5-shot tasks on miniImageNet, and 72.98%, 86.23% on tieredImageNet. Especially in 5-way 1-shot FSL tasks, our method achieves SOTA. On CIFAR-FS, our method achieves 78.54%, 88.64% in 1-shot and 5-shot FSL tasks respectively, as shown in Table 3. Note that, unlike RFS [17], PAL [19], and SCL [21], which adopt the knowledge distillation technique, and DeepEMD [9], ESPT [24], which use meta-learning method, our method does not need the second training or the meta-tuning phase. Our approach"}, {"title": "Fine-Grained Few-Shot Classification", "content": "Moreover, to further validate the generalizability of our approach, we also conduct experiments on a fine-grained dataset, CUB. The experimental results demonstrate that our approach outperforms all other methods, as shown in Table 4. In particular, our method achieves 86.14% in 1-shot tasks and 94.75% in 5-shot tasks respectively, outperforming the second-best results of 0.69% and 0.73%. These results indicate that on fine-grained datasets with small category differences, our approach can better distinguish fine-grained categories by exploring sample relations at different granularities and modeling them in detail."}, {"title": "Combination with Other Methods", "content": "Additionally, as a feature-learning-based approach, our work can provide a good pre-trained model for two-stage meta-learning methods and some generative methods, helping them to achieve better performance. To demonstrate this, we select two meta-learning methods (FEAT [53], Meta-Baseline [54]) and a generative method (STVAE [14]) to conduct experiments on four datasets. As shown in Table 1 and 2, when using our pre-trained model, FEAT, Meta-Baseline, and STVAE achieve improvements of 2.49%, 5.63%, and 3.67% respectively in 1-shot tasks on miniImageNet, and improvements of 1.54%, 4.46%, 1.94% in 5-shot tasks. FEAT, Meta-Baseline, and STVAE also have improvements on tieredImageNet using our model as their pre-trained model. On CIFAR-FS, STVAE and FEAT using our pre-trained model achieve the best result in 1-shot and 5-shot respectively, which are 80.92% and 90.18%, as shown in Table 3. Furthermore, we also conduct experiments for these approaches on CUB, where Meta-Baseline stands out by delivering the highest performance in both 1-shot and 5-shot FSL tasks, achieving accuracies of 88.37% and 95.52% respectively, as shown in Table 4. In these experiments, results with \"\u2020\" denote our re-implementation of the method, because the backbones of these approaches have some differences with ours or they have not conducted experiments on the corresponding dataset. These experimental results indicate that our approach can provide a good pre-trained model for these two-stage meta-learning methods and generative methods to improve their performance."}, {"title": "4.5. Component Ablative Analysis", "content": "In order to study the impact of each component, we conduct comprehensive ablation studies on miniImageNet and CUB. Here, our baseline model is the same as RFS [17], but we add some augmented samples in order to alleviate the overfitting problems and implement our Transformation Consistency Learning (TCL).\nAs shown in Table 5, our baseline achieves 66.78% and 82.18% in 5-way 1-shot FSL tasks on miniImageNet and CUB, respectively. When adding the self-supervised component to predict which image transformation was performed, we obtain improvements of 0.98% and 1.28% over the baseline on miniImageNet and CUB respectively. By enforcing the intra-sample relation of the same sample under different transformations (adding TCL component on the baseline), we obtain improvements of 1.67% and 0.98% on miniImageNet and CUB respectively. By enforcing the intra-class relation"}, {"title": "4.6. Parameter Ablative Studies", "content": "Effects of hyper-parameters a and \u03b2. \u03b1 and \u03b2 are the hyper-parameters used to adjust the weights of different losses. Here, we evaluate the model performance on miniImageNet by assigning various values to \u03b1 and \u03b2. When discussing the effect of one hyper-parameter, we set the other parameter to"}, {"title": "4.7. Sample Relation Exploitation Strategy Discussion", "content": "In recent years, some FSL approaches use contrastive learning to exploit sample relations. However, these methods often directly use unsupervised learning (UnSupCon) [26] or supervised contrastive learning (SupCon) [25], which is not appropriate because they do not fully exploit the sample relations. To indicate the superiority of our method in comparison to these methods, we conduct experiments based on the same base learner we proposed.1 During implementation, the UnSupCon or SupCon loss is added"}, {"title": "4.8. Visualization Analysis", "content": "To better demonstrate the effectiveness of our model, we conduct visualization experiments using t-SNE on miniImageNet, as shown in Figure 6. In this figure, we can observe that the Base model already has a definite classification boundary. And compared with the Base model and Base with TCL,"}, {"title": "5. Discussions", "content": ""}, {"title": "5.1. Limitions in Real Scenarios", "content": "Real-world scenarios, such as industrial defect detection, medical image classification, and clinical settings, often involve limited labeled samples and present challenges distinct from those encountered in academic datasets. Discrepancies between real-world data and few-shot datasets, such as insufficient base class data for pre-training, make the applications of few-shot classification in real-world scenarios more difficult. Additionally, real-world applications require higher accuracy and reliability, and the lack of labeled data exacerbates these issues. Domain shifts, dataset biases, and the need for robust models are often overlooked in current few-shot learning research. Moving forward, we aim to apply our proposed method and other few-shot algorithms to real-world scenarios to gain deeper insights into their performance and limitations in practical applications."}, {"title": "5.2. Computational Resources", "content": "First, the method we proposed makes only minimal changes to the network architecture, primarily by adding a multilayer perceptron. Therefore, the network structure does not introduce significant computational overhead. Additionally, while the method feeds different data-augmented samples into the network simultaneously, which does incur extra computational cost, this approach is commonly used in most current few-shot classification methods to train a feature extraction network with strong transferability on base datasets such as [18, 19, 24, 53], etc. This additional cost is considered acceptable, as the current focus in few-shot learning (FSL) lies more on improving accuracy than minimizing computational resource consumption. To further reduce computational costs, we plan to explore more advanced data augmentation techniques, such as Mixup and Mosaic, which enable the model to acquire richer knowledge from fewer samples.\nMoreover, the memory bank used in our approach does indeed depend on the size of the dataset, as it stores feature vectors for all images. One solution is to use a Momentum Encoder, similar to MoCo [27], which would eliminate the need to store feature vectors for each image. However, this approach introduces additional computational cost, as each batch would require a forward propagation through the Momentum Encoder to obtain image features. In this paper, we choose to prioritize computational cost over memory consumption. Another solution we are currently exploring is to store only"}, {"title": "5.3. Comparison with Existing Models", "content": "Compared to other methods, the proposed approach is a simple and effective pre-training method that fully leverages the relationships between different samples. It achieves results that are either better than or comparable to other methods with a single training phase [23, 18, 20], and reducing the training complexity compared to two-stage methods [53, 54, 14]. Furthermore, it can serve as a good feature extractor for some two-stage methods, enhancing their performance, as demonstrated in Section 4.2. Moreover, similar to many few-shot classification methods, such as [17, 18, 20, 21], our approach utilizes the feature extraction network obtained during the pre-training process to extract image features for both the support set and the query set during testing phase. A logistic regression classifier is then trained on the support set features to classify the query set features. Since the feature extraction network and the testing procedure are the same, the inference speed of our method is essentially consistent with that of most few-shot classification approaches.\nHowever, our method has its limitations, the most notable being the use of a larger number of data-augmented samples, which increases computational resource consumption. To address this limitation, we plan to explore advanced data augmentation techniques that allow the model to acquire richer knowledge from fewer samples, thereby mitigating the computational overhead."}, {"title": "6. Conclusion", "content": "In this paper, we rethink sample relations and categorize them into three distinct types at different granularities: intra-sample relation of the same sample under different transformations, intra-class relation of homogenous samples, and inter-class relation of inhomogeneous samples. By exploiting"}]}