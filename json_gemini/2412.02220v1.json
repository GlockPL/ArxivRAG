{"title": "Unlocking Tuning-Free Few-Shot Adaptability in Visual Foundation Models by Recycling Pre-Tuned LoRAs", "authors": ["Zixuan Hu", "Yongxian Wei", "Li Shen", "Chun Yuan", "Dacheng Tao"], "abstract": "Large Language Models (LLMs) such as ChatGPT demonstrate strong few-shot adaptability without requiring fine-tuning, positioning them ideal for data-limited and real-time applications. However, this adaptability has not yet been replicated in current Visual Foundation Models (VFMs), which require explicit fine-tuning with sufficient tuning data. Besides, the pretraining-finetuning paradigm has led to the surge of numerous task-specific modular components, such as Low-Rank Adaptation (LoRA). For the first time, we explore the potential of reusing diverse pre-tuned LoRAs without accessing their original training data, to achieve tuning-free few-shot adaptation in VFMs. Our framework, LoRA Recycle, distills a meta-LoRA from diverse pre-tuned LoRAs with a meta-learning objective, using surrogate data generated inversely from pre-tuned LoRAs themselves. The VFM, once equipped with the meta-LoRA, is empowered to solve new few-shot tasks in a single forward pass, akin to the in-context learning of LLMs. Additionally, we incorporate a double-efficient mechanism tailored to our framework, significantly accelerating the meta-training process while maintaining or even improving performance. Extensive experiments across various few-shot classification benchmarks across both in- and cross-domain scenarios demonstrate the superiority of our framework.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) like ChatGPT demonstrate a profound capacity to solve few-shot tasks without the necessity for fine-tuning, making them ideal for data-limited and real-time applications. However, this adaptability can not be replicated by current Visual Foundation Models (VFMs), which typically require explicit fine-tuning with sufficient tuning data.\nLow-Rank Adaptation (LoRA) [26] has emerged as a prominent fine-tuning approach in current research, valued for its capacity to achieve strong fine-tuning performance with sufficient tuning data, while only updating a small subset of additional parameters\u2014specifically, trainable rank decomposition matrices rather than the entire model. While promising, (i) explicit fine-tuning is often prohibitive for applications requiring real-time responses, and (ii) fine-tuning with limited data is extremely unstable. As shown in Tab. 1, fine-tuning with limited data makes performance highly sensitive to choices like the optimizer, learning rate, and step size. Besides, it introduces unacceptable latency for applications requiring real-time responses.\nFor the first time, we explore the potential of reusing diverse pre-tuned LoRAs without accessing their original training data, to achieve tuning-free few-shot adaptation in VFMs (see Fig. 1). Our inspiration comes from the concept of LoRA Market [31], where diverse pre-tuned LoRAs are publicly accessible without exposing original training data due to privacy concerns. For task-specific reuse, users can download and then insert a task-specific LoRA of interest into the open-source VFM, to obtain a personalized VFM. Moving beyond task-specific reuse, we seek to leverage the vast availability and diversity of these LoRAs from a novel perspective, leading to our central research question: Is it feasible to reuse diverse pre-tuned LoRAs without accessing their original training data, to achieve tuning-fee few-shot adaptation in VFMs? This offers new insights into leveraging the vast accessibility and diversity of LoRAs beyond task-specific reuse, and avoids the need to access original training data, which is often restricted by privacy concerns.\nTo answer this question, we propose a meta-learning framework named LoRA Recycle (see Fig. 2). Without access to the original training data, we propose LoRA Inversion to generate surrogate data inversely from the pre-tuned LoRAs themselves. A meta-LoRA is then distilled from the pre-tuned LoRAs using these surrogate data, with a meta-learning objective of learning how to adapt to diverse tasks without fine-tuning. Thanks to the meta-learning objective, the VFM, once equipped with the meta-LoRA, is empowered to adapt to new few-shot tasks in a single forward pass without further fine-tuning, akin to in-context learning of LLMs. The core idea of LoRA Recycle is to reshape the VFMs' prior over a distribution of expected tasks (i.e., pre-tuned LoRAs) via meta-learning, and such prior encoded in the meta-LoRA can facilitate learning of new tasks sampled from similar distributions. To further improve efficiency, we introduce a double-efficient mechanism tailored to our framework. During the inversion stage, it prunes unimportant tokens based on self-attention weights in hidden layers, thereby accelerating data generation. The pruning results from the inversion stage further help to indicate the informative and sparse tokens in the generated data, which are then exclusively used in the subsequent meta-training stage. This selective use of sparse tokens significantly accelerates the meta-training process, while maintaining or even improving performance by reducing noise from generated data. We summarize our contributions as follows:\n\u2022 Novel perspective: We are the first to enable tuning-free few-shot adaptation in VFMs from the perspective of LoRA reusing, offering new insights into leveraging the accessibility and diversity of pre-tuned LoRAs beyond traditional task-specific reuse.\n\u2022 United framework: (i) We propose a meta-learning framework named LoRA Recycle, effectively achieving tuning-free few-shot adaptation in VFMs by reusing diverse pre-tuned LoRAs without needing their original training data. (ii) We further propose a double-efficient"}, {"title": "2. Related Work", "content": "2.1. LORA & LORA Reuse\nFine-tuning the entire foundation model results in high costs in computation and storage. To mitigate these challenges, several parameter-efficient fine-tuning (PEFT) methods [21, 22, 26, 34, 43, 68] have emerged, focusing on the update of a limited subset of model parameters. LoRA [26] parallelly attaches extra low-rank decomposition matrices to original weights, while adapter tuning [1, 16, 25] sequentially appends extra layers behind the original feed-forward layers. More recently, several works [5, 18, 31, 68, 69] have investigated the potential of composing multiple pre-tuned Lo-RAs. However, (i) they are limited to parameter arithmetic like weight averaging, lacking precise alignment for LoRAs targeting different label spaces in the context of classification. (ii) They are not specifically designed to achieve tuning-free few-shot adaptation in VFMs. (iii) They are not applicable to reuse LoRAs with different architectures like different ranks.\n2.2. Meta-Learning & Data-Free Meta-Learning\nMeta-learning, also known as learning to learn, aims to learn prior knowledge over a distribution of tasks, enabling efficient adaptation to unseen few-shot tasks from similar distributions. Data-based meta-learning [13, 14, 36, 72] typically assumes the availability of task-specific data for each meta-training task. Recently, Data-Free Meta-"}, {"title": "2.3. Tuning-Free Adaptation of Foundation Models", "content": "Compared to explicit fine-tuning, training-free adaptation requires no parameter updates, making it highly suitable for real-time applications with low computational budgets. LLMs achieve tuning-free adaptation through their inherent in-context learning capabilities [9]. Existing studies suggest that in-context learning is equivalent to implicitly performing gradient descent [8, 60], viewing LLMs as meta-learning models [3]. However, this in-context learning ability has not yet been replicated by current VFMs. To address this, [12] explicitly trains a sequence model with VFMs to simulate LLM-style in-context learning. [45, 74] adapt the Segment Anything Model in a tuning-free manner using a one-shot example. Our LoRA Recycle, on the other hand, achieves tuning-free adaptation to few-shot tasks from a novel perspective, by reusing diverse pre-tuned LoRAs without needing their original training data."}, {"title": "3. Preliminary & Problem Setup", "content": "Low-Rank Adaptation (LoRA) [26] enables VFM to solve a specific task by only updating lightweight extra modules. For a weight matrix $W^{(l)} \\in \\mathbb{R}^{d \\times k}$ at the $l$th layer within the VFM $f$, a LoRA module is represented as a low-rank matrix decomposition $\\delta W^{(l)} = \\delta W_A^{(l)} \\delta W_B^{(l)}$, where $\\delta W_A^{(l)} \\in \\mathbb{R}^{d \\times r}$, $\\delta W_B^{(l)} \\in \\mathbb{R}^{r \\times k}$ and the rank $r < min(d, k)$. The input $X_{in}$ will be processed in parallel as $X = X_{in} + \\delta W_A^{(l)} \\delta W_B^{(l)} X_{in}$. When fine-tuning, it freezes the original weight matrix $W$ while only keeping $\\delta W_A$ and $\\delta W_B$ trainable. When facing classification tasks, a classification head $h$ is always tuned together with the LORA modules to output the prediction distribution. We use $f_{\\delta W}$ to denote the VFM equipped with the LORA $\\delta W$.\nProblem setup: LORA Recycle. We are given a transformer-based VFM $f$ pre-trained on large-scale datasets, and multiple LoRAs with classification heads pre-tuned on diverse classification tasks. Following standard meta-learning setup [13], we assume these tasks follow an underlying task distribution $p_T$. $(\\delta W_T, h_T) \\sim p_T$ denotes the LORA and classification head pre-tuned on task $T$. Note that we have no access to the original training data behind the given LoRAs. Our goal is to meta-train a meta-LORA $\\delta W^*$ over $p_T$, so that the VFM $f$, once equipped with $\\delta W^*$ (i.e., $f_{\\delta W^*}$), can adapt to new few-shot tasks sampled from"}, {"title": "4. Methodology", "content": "In this section, we present our proposed framework LoRA Recycle (see Fig. 2 and Alg. 1). Without access to original training data, we propose LoRA Inversion to generate surrogate data from pre-tuned LoRAs (see Sec. 4.1). A meta-LoRA is then distilled from the pre-tuned LoRAs using these surrogate data, with a meta-learning objective of learning how to adapt without fine-tuning (see Sec. 4.2). To further improve efficiency, we propose a double-efficient mechanism significantly accelerating the meta-training process by selectively using the most informative tokens, while also improving performance (Sec. 4.3) by reducing noise from the generated data.\n4.1. Surrogate Data Generation via LoRA Inversion\nLORA Inversion. Given a pre-tuned LoRA $\\delta W$ with its classification head $h$, we generate its original training data by iteratively optimizing (a batch of) data $X$, which is initialized as Gaussian noise. This is done by minimizing the following loss function:\n$\\min_X \\mathcal{L}_{data} = CE\\left(h \\circ f_{\\delta W}(X), Y\\right) + \\alpha_r \\mathcal{R}_{BN}(X),$ (1)\nwhere $Y$ is the target label (e.g., [1,0,0]). $CE(\\cdot)$ is a cross-entropy classification loss. $\\mathcal{R}_{BN}$ is an image regularization term with a coefficient $\\alpha_r$. Minimizing the first classification loss is to achieve label-conditional generation, ensuring $X$ can be predicted by $f_{\\delta W}$ as the target label $Y$. To further improve the realism of the generated data, we impose a naturalness prior $\\mathcal{R}_{BN}$ [71]:\n$\\mathcal{R}_{BN}(X) = \\sum_{l=1}^{L} \\|\\mu^{(l)}(X) - \\mu_{BN}^{(l)}\\|^2_2 + \\|\\sigma^{(l)}(X) - \\sigma_{BN}^{(l)}\\|^2_2$ (2)\nwhere $\\mu^{(l)}(X)$ and $\\sigma^{(l)}(X)$ denote the mean and variance of the inputs' feature maps calculated at the $l$-th layer of the pre-trained model. $\\mu_{BN}^{(l)}$ and $\\sigma_{BN}^{(l)}$ denote the statistics initially stored in the $l$th batch normalization (BN) layer of the pre-trained model, which is calculated with the original training data. Given that Vision Transformers do not have"}, {"title": "4.2. LORA Distillation via Meta-Learning", "content": "Meta-learning objective. We distill a meta-LORA $\\delta W^*$ from diverse pre-tuned LoRAs using the surrogate data. The meta-learning objective is formulated as follows:\n$\\min_{\\delta W^*} \\mathcal{L}_{meta} = \\mathbb{E}_{p_T} \\sum_{(X_q, Y_q) \\in D_q} KL\\left(\\mathcal{P}(Y_{pred} | X_q, D_s^T), h_T \\circ f_{\\delta W^T}(X_q)\\right),$ (3a)\nwhere, $\\mathcal{P}(Y_{pred} = i | X_q, D_s^T) = \\frac{\\exp(-\\|f_{\\delta W^*}(X_q) - c_i\\|^2)}{\\sum_{i'} \\exp(-\\|f_{\\delta W^*}(X_q) - c_{i'}\\|^2)}$ (3b)\nHere, $p_T$ is the underlying task distribution. $(\\delta W_T, h_T, D_s^T, D_q^T)$ refer to the pre-tuned LORA, classification head, generated support set, and query set of task $T$, which can be viewed as sampling from the task"}, {"title": "4.3. Double-Efficient Mechanism", "content": "Efficient inversion with token pruning. As shown in Eq. (1), optimizing X via LoRA Inversion requires iteratively forward and backward computations. To improve efficiency, we propose to prune unimportant tokens during inversion. This is reasonable since the self-attention mechanism inherently weights tokens according to their importance and relevance. As illustrated in the left panel of Fig. 3, at the $i$th layer, we implement \u201ctoken pruning\u201d by directly discarding those unimportant tokens, no longer processing them forward or computing backward gradients, thus significantly reducing computational complexity.\nThe most important tokens are those with highest attention weights in $x_{[CLS]}$. Suppose we have n + 1 tokens $[X_{[CLS]}, X_1, ..., X_n]$ at the $i$th layer, where $x_{[CLS]}$ is the class token inserted before all image tokens to grasp global information. We propose to use the attention weights of the class token $x_{[CLS]}$ with respect to all other tokens, as an indicator measuring each token's importance:\n$\\alpha_{[CLS]} = \\text{Softmax} \\left( \\frac{q_{[CLS]} \\cdot K^T}{\\sqrt{d}} \\right),$ (5)\nwhere $\\alpha_{[CLS]}$ is a (n + 1)-dimension vector, representing the attention weights from token $x_{[CLS]}$ to all tokens $[X_{[CLS]}, X_1, ..., X_n]$. $q_{[CLS]}$ is the query vector of token $x_{[CLS]}$. $K = [k_{[CLS]}, k_1, ..., k_n]^\\top$ is the key vectors of all tokens. $d$ is the dimension of the query vector. The $\\alpha_{[CLS]}$ is then used to calculate the output of token $x_{[CLS]}$ via the self-attention mechanism:\n$x_{[CLS]}^{\\prime} = \\alpha_{[CLS]} V,$ (6)\nwhere $V = [v_{[CLS]}, v_1, ..., v_n]^\\top$ is the value vectors of all tokens. Therefore, the output of $x_{[CLS]}$ can be viewed as a linear combination of all tokens' value vectors weighted by $\\alpha_{[CLS]}$. Since the output of $x_{[CLS]}$ is used for classification at the final layer, it is rational to view $\\alpha_{[CLS]}$ as an indicator, measuring the extent to which each token contributes to final predictions, i.e., the importance of each token. Therefore, we identify the most important tokens as those with the highest attention weights in $x_{[CLS]}$. For multi-head self-attention, we compute average attention weights $\\alpha_{[CLS]}$ across all heads. Note that this process requires no extra computational demands, as it is an inherent part of the forward process (see App. B for more preliminaries).\nEfficient meta-training with sparse tokens. We obtain the remaining tokens at the last layer from the inversion stage. Since each token (except for token $X_{[CLS]}$) precisely corresponds to a token in the input image, these remaining tokens can indicate the most informative areas in the generated data, typically the foreground regions."}, {"title": "5. Experiments", "content": "In this section, we perform comprehensive experiments on various few-shot classification benchmarks, covering both in-domain (see Sec. 5.1) and cross-domain scenarios (see Sec. 5.2). We also provide comprehensive visualization results and ablation studies in Sec. 5.3 and App. A.\nSetup of VFM. We select the 12-layer ViT-B/16 and ViT-B/32 pre-trained with CLIP as the pre-trained VFM, publicly available on HuggingFace. Refer to Tab. 13 in App. A for more results on more types of transformer.\nBaselines. We compare LORA Recycle against several baselines (see App. D for more implementation details).\n\u2022 Multi-LoRAs reuse baselines. (a) LoRAs Avg averages all pre-tuned LoRAs into one, which is then either fine-tuned (LoRAs Avg + Linear) or used for Nearest Neighbor (LoRAs Avg + NN) inference. (b) LoRAHub [31] uses a weighted sum of pre-tuned LoRAs, with weights fine-tuned on the target task. (c) MOLE [5] fine-tunes a gating function to combine outputs from multiple LoRAs.\n\u2022 Fine-tuning-free baselines. (d) Nearest Neighbor (NN) predicts based on the closest class center. (e) CAML [12] trains a sequence model to simulate in-context learning.\n\u2022 Few-shot learning with foundation models. (f) P > M > F [27] is a state-of-the-art method that adapts foundation models to few-shot tasks through a pre-training, meta-training, and fine-tuning pipeline.\n\u2022 Fine-tuning baselines. While our focus is on tuning-free settings, we include representative fine-tuning methods to demonstrate that tuning-free approaches can achieve comparable performance while offering advantages of stability and faster response. (g) Full Fine-Tuning updates the entire model, (h) Linear Probe updates only the classification head, and (i) LoRA + Linear [26] updates LORA parameters alongside the classification head.\nImplementation details. We fine-tune the LoRAs (rank r = 4) and classification heads using the Adam optimizer with a learning rate of 1 \u00d7 10-3. LoRA performs well with sufficient data. During meta-training, the meta-LoRA is optimized with Adam at the same learning rate, following a cyclic schedule: a 25-iteration linear warm-up from 1 \u00d7 10-5 to 1 \u00d7 10-3, followed by cosine annealing over the next 75 iterations. For LoRA Inversion, surrogate data is optimized using Adam with a learning rate of 0.25 over 2000 iterations, and generated images have a resolution of 224 \u00d7 224. We set the hyperparameter \u03b1r = 0.01. Data augmentation includes random horizontal flipping and normalization in meta-training, with only normalization applied in meta-testing. Hyperparameter selections and sensitivity analysis are discussed in App. C. Unless otherwise specified, we perform token pruning in the final layer for the inversion stage to obtain masks with varying sparsity."}, {"title": "5.1. Recycle In-Domain LoRAS", "content": "In-domain benchmarks. For the \"recycle in-domain Lo-RAs\" scenario, we use four datasets commonly used to evaluate few-shot adaptability: CIFAR-FS [2], MiniImageNet [59], VGG-Flower [50], and CUB [61]. These datasets range from general natural images (CIFAR-FS, MiniImageNet) to more specialized domains, such as bird species (CUB) and flowers (VGG-Flower). Following standard meta-learning splits [13], each dataset is divided into meta-training and meta-testing subsets with non-overlapping label spaces. For constructing an N-way K-shot task, we randomly sample N classes and K examples per class from"}, {"title": "5.2. Recycle Cross-Domain LoRAS", "content": "Cross-domain benchmarks. Real-world situations might pose challenges in collecting LoRAs from the identical domain. For the \"recycle cross-domain LoRAs\" scenario, we construct meta-training tasks from the meta-training subsets of CIFAR-FS, MiniImageNet, VGG-Flower, and CUB, while meta-testing tasks are drawn from distinct datasets (ChestX, ISIC, EuroSAT, or CropDiseases) following the cross-domain meta-learning benchmark [19]. These meta-"}, {"title": "5.3. Ablation Studies", "content": "How to choose the overall pruning ratio and pruning layers during inversion? We adopt the double-efficient mechanism with two steps: (i) Set the overall pruning ratio (sparsity level) of the generated data. As shown in Tab. 2 and Tab. 3, pruning 50% or 75% of tokens boosts performance while preserving key foregrounds (see Fig. 4). (ii) Select the pruning layers during inversion. Pruning at different layers affects only the inversion speed, while meta-training speed depends solely on the overall pruning ratio."}, {"title": "6. Conclusion", "content": "In this paper, we propose LoRA Recycle, a novel meta-learning framework to achieve tuning-free few-shot adaptation in VFMs, by reusing diverse pre-tuned LoRAs without access to original training data. We further propose the double-efficient mechanism tailored to our framework, significantly accelerating the meta-training by selectively using the most informative tokens, while maintaining or even improving performance by reducing noise. Experimental results across various few-shot classification benchmarks, across in-domain and challenging cross-domain scenarios, confirm the effectiveness of LoRA Recycle."}, {"title": "A. Additional Experiments", "content": "Advantage of token pruning in generated data beyond acceleration. When using inversion to generate data by optimizing Eq. (1), it primarily crafts only the label-relevant features into the generated data (typically the foreground regions), while background regions often remain noisy as initialization. To further illustrate this, the experiment in Tab. 6 shows that, during the inversion process, the classification loss Eq. (1) caused by the identified foreground steadily decreases, whereas the loss caused by the identified background remains nearly unchanged. As such, masking the background in generated data helps to reduce noise, which aligns with our experimental results in Tab. 2: pruning background tokens in the generated CIFAR-FS data led to a 1.34% improvement in performance.\nEffect of cross-task interpolation. Tab. 7 verifies the effectiveness of the cross-task interpolation under a constrained LORA budget of 100 on CIFAR-FS. This technique can diversify the task distribution by generating multiple interpolated tasks, which enables the meta-training to cover a broader range of tasks, thereby bolstering the generalization capabilities for unseen tasks.\nEffect of meta-learning in LoRA Recycle. To assess the effectiveness of meta-learning, we compared it against joint supervised learning within the LoRA Recycle framework. In joint supervised learning, data generated from all LORAs is aggregated to train a single LoRA through standard supervised learning. The comparative results for meta-learning and joint supervised learning are presented in Tab. 8. Experiments were conducted on the CIFAR-FS dataset, focusing on unseen few-shot tasks to evaluate generalization capabilities. As shown, meta-learning achieves significantly better performance than joint supervised learning in both 1-shot and 5-shot settings. This improvement arises because meta-learning's bi-level optimization is inherently designed to enhance generalization to unseen few-shot tasks.\nEffect of generated data in LoRA Recycle. In our setting, the original training data for each LoRA is unavailable. To evaluate the effectiveness of the generated data, we use the"}, {"title": "B. Preliminary of Vision Transformers (ViTs)", "content": "Preliminary of ViTs. Here, we discuss the operational mechanism behind ViTs. ViTs initially divide the input image $X^l$ belonging to the space $\\mathbb{R}^{H \\times W \\times C}$ into n + 1 distinct, non-overlapping patches. These patches are then transformed into n + 1 tokens, denoted as $X = [X_{[CLS]}, X_1, ..., X_n]$ where $x_i \\in \\mathbb{R}^D$. The class token, $X_{[CLS]}$, is prepended to these image tokens to facilitate the classification task. To integrate positional relationships, learnable position encodings are added to all tokens. These"}, {"title": "C. Hyperparameter Selection and Sensitivity Analysis", "content": "In this section, we detail the selection of hyperparameters and conduct a sensitivity analysis on key hyperparameters. Generally speaking, We base our hyperparameter values on reference works and perform grid searches within the relevant ranges to identify the optimal configurations.\nFor the learning rate in LoRA Inversion, we refer to the settings from prior work [71], and perform a grid search over the range [0.1, 0.25, 0.5]. Similarly, for the learning rate in the meta-learning stage, we adopt values from the literature [53] and conduct a grid search over the range [0.001,0.01, 0.1]. These ranges allow us to identify the optimal configurations."}, {"title": "D. Implementation Details of baselines", "content": "Here, we provide detailed implementation details for the baselines used in our paper..\n\u2022 Fine-tuning baselines. \u201cFull Fine-Tuning\u201d updates the entire model on the target task via gradient descent."}, {"title": "E. More Discussions", "content": "Discussions on the inconsistent performance gains across various datasets. When we use LoRAs from the dataset the same as the testing dataset (in-domain setting), those LoRAs can provide domain-specific priors. This is particularly useful when the foundation model's pre-training dataset varies from the testing dataset. The main paper's Tab. 2 confirms this, showing a higher performance gain on CIFAR-FS (+10.01%) than other datasets (average +4.98%). The larger disparity between CIFAR-FS and"}, {"title": "F. Rethinking Existing Data-Free Meta-Learning Methods", "content": "Limited scalability to large-scale models. Current data-free meta-learning (DFML) methodologies, as discussed in [29, 30, 65], predominantly focus on leveraging small-scale pre-trained models and meta-learners, such as four-layer CNNs or ResNet12. A critical limitation of these approaches is their inability to scale up to larger models, particularly those based on transformer architectures. This scalability issue substantially hinders their practical application in complex, real-world scenarios. For instance, [65] employs a hyper-network with all pre-trained models as inputs and outputs a single fused model. The efficiency of this method declines significantly when outputting all parameters of larger models, given the hyper-network's extensive input and output dimensions. Similarly, inversion-based DFML methods, such as those in [29, 30], rely on meta-training a meta-learner with data inverted from pre-trained models. The model inversion process becomes inefficient for large-scale models. The following meta-training process often necessitates the computation of Hessian matrices for second-order derivatives [49], which becomes exceedingly resource-intensive for large-scale models.\nInefficiency issues. Beyond scalability challenges,"}]}