{"title": "On Large Uni- and Multi-modal Models for Unsupervised Classification of Social Media Images: Nature's Contribution to People as case study", "authors": ["Rohaifa Khaldi", "Domingo Alcaraz-Segura", "Ignacio S\u00e1nchez-Herrera", "Javier Martinez-Lopez", "Carlos Javier Navarro", "Siham Tabik"], "abstract": "Social media images have shown to be a valuable source of information for understanding human interactions with important subjects such as cultural heritage, biodiversity and nature among others. The task of grouping such images into a number of semantically meaningful clusters without labels is challenging given the high diversity and complex nature of the visual content of these images in addition to their large volume. On the other hand, the last advances in Large Visual Models (LVM), Large Language Models (LLM) and Large Visual Language Models (LVLM) provide an important opportunity to explore new productive and scalable solutions. This works proposes, analyzes, and compares various approaches based on one or more state-of-the art LVM, LLM and LVLM, for mapping social media images into a number of pre-defined classes. As case study, we consider the problem of understanding the interactions between human and nature, also known as Nature's Contribution to People or Cultural Ecosystem Services (CES). Our experiments reveal that the top-performing approaches, delivering highly competitive results, are the fine-tuned LVM DINOv2 on a small labeled dataset and LVLM models like the proprietary GPT-4 (gpt-40-mini) using a simple prompt.", "sections": [{"title": "1. Introduction", "content": "Social media platforms such as, Instagram, Flickr, X, Facebook, are widely used by visitors and inhabitant of protected natural areas to share images in situ. Several recent studies Gatzweiler et al. (2024), Moreno- Llorca et al. (2020), Havinga et al. (2023), Lingua et al. (2022), Yee & Carrasco (2024) have shown that these georeferenced images provide valuable information close to real time for understanding the interactions between human and nature, also known as Cultural Ecosystem Services (CES), of paramount importance for guiding decision making of both managers and interested social actors towards the achievement of the Sustainable Development Goals (SDGs). There exist several CES classifications, the most used one is the Common International Classification of Ecosystem Services CICES revision 5.2 (https://cices.eu). See summary in Figure 1.\nGrouping social media images into a number of CES clusters is challenging for multiple reasons: i) the semantic meaning of several CES classes is broad and may refer to a huge number of different visual contents, ii) the large volume of the input images, iii) a important proportion of the input images maybe irrelevant to CES study, for example pictures that display text, screenshots and duplicated images. Several studies have tackled the task of clustering social media images into a small number of CES classes but all the proposed approaches relay either partially or entirely on manual intervention Moreno-Llorca et al. (2020), Yee & Carrasco (2024). Most relevant approaches in the field of computer vision are based on two steps, first a neural network is trained to learn inner representations from input images then in a second step the obtained visual embeddings are clustered using a carefully selected criteria Van Gansbeke et al. (2020). Recent Large Visual Models (LVM) also known as foundational models encode a large amount of repre- sentations that can be utilized to solve new vision problems under either the supervised regime or few-short learning regime. Similarly, Large Language Models (LLM) understand natural language and can be adapted to generate text for different downstream tasks. Large Vision Language models (LVLM) in addition provides a more productive adaptation approach named prompting. This work analyzes the potential of new large models, including LVM, LLM and LVLM, in solving the task of mapping large amounts of images coming from social media to a set of pre-defined categories that do not show consistent visual patterns. As case study, we consider the sustainability problem of cultural ecosystem services evaluation which focus on map- ping social media images into a set of CES classes. We analyze the potential of state-of-the art foundational models, including large visual models and language vision models to build a more portable and efficient to cluster images shared on social media."}, {"title": "2. Related work", "content": "The most related work to the present work can be divided into two type of studies:\nCultural Ecosystem Services (CES) analysis: These studies focus mainly on understanding and mapping CES, which is still an open problem. The used methodologies to cluster social media images into a number of CES classes often involve human intervention in different parts of the process. For example, the authors in Moreno-Llorca et al. (2020) manually analyzed the entire volume of images posted in Flickr from the natural park of Sierra Nevada, Granada, Spain. They intended to find CES classes that better match the visual content of the images and identified four classes: 1) Landscape and species, 2) recreation and sports, 3) culture and heritage, 4) others. When comparing the agreement between the assigned classes and results of an online survey for each image they found that the only consistent match is shown by the \"Landscape and species\" class. This disagreement can be explain by the use of an inappropriate semantic categories. In Yee & Carrasco (2024), the authors simplified the CES classes into three categories: biotic (interaction between human and biotic parts of nature), abiotic (interaction between human and abiotic parts of nature) and human-human (interaction between human and human). They first analyzed 87,090 images using Microsoft's Azure Computer Vision API and got a vector of 5, 127 pairs of labels and corresponding scores referring to diverse objects, living beings, and actions. Then they used a threshold of 50% to filter non-relevant classes and applied a hierarchical agglomerative clustering that generated 430 clusters. In the last step they manually corrected and merged the obtained clusters into the three predefined classes.\nUnsupervised image classification task: These studies focus mainly on automatizing the whole process of grouping a set of unlabeled images into semantic meaningful clusters. Existing approaches in this context are specific purpose models that follow one of these two dominant strategies: 1) In the first, the feature learning step and clustering are decoupled. For the representation learning a self supervised learning pretext task is trained to predict transformations (e.g., rotation, colorization) or an instance discrimination process is applied using approaches such as SimCLR Chen et al. (2020). Afterwards, an offline clustering of the generated embedding performed. One of the most relevant works in this direction is SCAN Van Gansbeke et al. (2020).\nThe second approach is based on end-to-end learning in which a CNN and clustering methods are coupled together. The losses of both methods are used to update the neural networks. Example of these approaches are DeepCluster Caron et al. (2019) and DEC Xie et al. (2016)."}, {"title": "3. Data collection", "content": "We created the dataset named FLIPS (Flickr Images from Spanish Parks) as part of this study analyzing human interactions with nature. This dataset was compiled from images sourced from the Flickr social media platform, focusing on various National parks across Spain. To ensure high quality, we implemented a rigorous filtering process to remove any noisy images.\nThe FLIPS dataset contains 960 images encompassing six CES classes: 1) Cultural-Religious, 2) Fauna- Flora, 3) Gastronomy, 4) Nature, 5) Sport, and 6) Urban-Rural. Each class contains 160 carefully curated images, providing a diverse representation of these themes. For analysis, we divided the dataset into two subsets: 70% of the images were allocated for training purposes, while the remaining 30% were designated for testing."}, {"title": "4. Study design", "content": "To address the task of clustering unlabeled images into CES classes we propose analyzing five approaches that include state-of-that art large models in different stages of the processing. In particular, three approaches are based on LVLM and two approaches are based on LVM (Figure 3). Using these approaches, a set of eleven model architectures was evaluated (Table 1)."}, {"title": "4.1. Approach 1: LVLM with prompt engineering", "content": "In this approach, we utilized a pre-trained LVLM combined with prompt engineering (Figure 4). This method takes two inputs: the image to be classified and the prompt, and generates as output the category of the image. A post-processing step was applied to the model's output to extract the class from the generated text. Two LVLM models were tested (Table 1): a public LLaVa-1.5 (Liu et al. 2024) and proprietary GPT- 4 (gpt-40-mini) (Achiam et al. 2023). Both models were applied directly to the data without additional training. Thus, only the test partition were used in this case. Various simple and extended prompts were evaluated, the two prompts that provide the best results are shown in Table 2."}, {"title": "4.2. Approach 2: LVLM with supervised classification of LLM", "content": "In this approach, we utilized a pretrained LVLM in conjunction with a pretrained LLM (Figure 5). The process unfolds as follows: (1) We employed the LVLM to generate descriptions for each image, evaluating two advanced LVLM architectures: LLaVA-1.5 and BLIP (Li et al. 2022). (2) We then used the LLM to generate text embeddings for these descriptions, experimenting with three different LLM architectures: BERT (Devlin 2018), DistilBERT (Sanh 2019), and RoBERTa (Liu et al. 2019). (3) Finally, we finetuned the LLM using the training partition to classify the descriptions into our predefined categories."}, {"title": "4.3. Approach 3: LVLM and LLMs with dimensionality reduction, clustering, and zero-shot classification", "content": "In this approach, we leveraged a pre-trained LVLM alongside with a pretrained LLMs, dimensionality reduction model, clustering model, and zero-shot classification (Figure 6). The process is as follows: (1) We used the LVLM LLaVA-1.5 to generate descriptions for each image. (2) These descriptions were then converted into text embeddings using the LLM Sentence-BERT (SBERT) (Reimers 2019). (3) To mitigate the curse of high dimensionality, we applied UMAP (McInnes et al. 2018), a state-of-the-art dimensionality reduction model, to project these embeddings into a lower-dimensional space while preserving both local and global data structure. (4) Clustering were performed on the reduced embeddings using two different clustering models, KMeans and HDBSCAN (McInnes et al. 2017). (5) For each cluster, we now have a collection of image descriptions. We tokenized these descriptions and then identified the ten most representative words based on Term Frequency (TF) and Inverse Document Frequency (IDF) scores. These top ten words constitute the Bag of Words (BOW) for each cluster. (6) Finally, we used another LLM, Flan-T5 (Chung et al. 2024), in a zero-shot classification setting to map each cluster to our predefined classes based on the generated BOW per cluster and a specific prompt structure. Flan-T5, having been fine-tuned with instruction-based learning across various tasks, is well-suited for zero-shot text classification (Mann et al. 2020)."}, {"title": "4.4. Approach 4: LVM with supervised classification head", "content": "In this approach, we employed LVM combined with a supervised classifier (Figure 7). The method ingested an input image and used a pretrained LVM to generate visual embeddings. These embeddings were then processed by a fully connected (FC) layer to predict the class with the highest probability. Only the classifier head was trained, while the backbone remained frozen. Thus, two data partitions were used in this case: training set and test set. For this approach, we used the state-of-the-art LVM model, DINOv2 (Oquab et al. 2023)."}, {"title": "4.5. Approach 5: LVM with few-shot classification", "content": "In this approach, we utilized a pretrained LVM in conjunction with few-shot classification (Figure 8). We incorporated both data partitions, using the test data as the query set and shots from the training data as the support set. Different configurations of shots, ranging from 1 to 10, were evaluated. To assess the model's stability concerning the support set samples, we generated 30 random support sets for each shot configuration and computed the average model performance. The method operates as follows: (1) we created a support set containing six classes with a specified number of shots per class; (2) we applied the LVM to generate visual embeddings for both the support and query set samples; (3) we computed a prototype for each class in the support set by averaging the embeddings of the samples in that class; (4) we compared the embedding vector of the query sample with the prototypes' embeddings using a cosine similarity function normalized by the Softmax function; (5) we assigned the query sample to the class label of the closest prototype (i.\u0435., the one with the highest Softmax value). For this approach, we employed the DINOv2 model."}, {"title": "5. Results and discussion", "content": "In this section, we present the optimal results for each approach and then provide a comparison of all approaches against one another.\nApproach 1\nTable 3 summarizes the test results of applying the models from approach (1), specifically LLaVA-1.5 and GPT-4, using two different prompts: a simple prompt (prompt 1) and an extended prompt (prompt 2). The highest accuracy for both models was achieved with prompt 1, with GPT-4 reaching 96.88% compared to 89.24% with prompt 2, and LLaVA-1.5 achieving 85.42% compared to 79.51%. Overall, GPT-4 outperformed LLaVA-1.5 in this approach, with an accuracy of 96.88% versus 85.42%.\nApproach 2\nTable 4 presents the test results for the models utilized in approach (2), which involve two LVLMS, LLaVA- 1.5 and BLIP, combined with three different LLM architectures: BERT, DistilBERT, and RoBERTa. Across all trained LLM architectures, LLaVA-1.5 consistently outperformed BLIP. The highest overall accuracy was achieved with the LLaVA-1.5+BERT combination, reaching 93.40%, compared to 86.11% for BLIP+BERT. This also indicates that the BERT architecture outperformed both DistilBERT and RoBERTa.\nApproach 3\nTable 5 presents the test results for approach (3), which combines the LLaVA 1.5 LVLM with two LLMS, SBERT and Flan-T5, utilizing dimensionality reduction, clustering, and zero-shot classification. This table showcases the results obtained from two different clustering algorithms: KMeans and HDBSCAN. The best performance was achieved using HDBSCAN, reaching an accuracy of 79.32%, compared to 70.09% obtained with KMeans.\nApproach 4\nTable 6 displays the test results for training the classifier head of DINOv2 using visual embeddings generated from three different ViT backbone architectures with varying computational complexities: S (small size), B (base size), and L (large size). The best performance was achieved with the small ViT model architecture, ViT-S/14, which attained an accuracy of 96.53%.\nApproach 5\nFigure 9 displays the average test results of running the DINOv2 LVM, using ViT-S/14 backbone, under few-shot learning setting using different shots ranging from 1 to 10. Each configuration of shot was run using 30 different random support sets. The highest average accuracy was obtained using 10 shots. Table 7 presents the test results of running approach (5) using different ViT backbone architectures. The optimal results were obtained with ViT-S/14 with an accuracy of 81.25%.\nComparison of all approaches\nTable 8 summarizes the results of all the models utilized under the five different approaches for recognizing CES from social media images, highlighting the best configurations. In general, the models implemented under approach (1), (2), and (4) provide better results than the models implemented under approach (3) and (5). Notably, the top-performing models DINO-LP under approach (4) and GPT-4 under approach (1), achieve competitive accuracies of 96.53% and 96.88%, respectively. The class-specific results shown in Table 9 reveal that DINO-LP performs particularly well in accurately classifying the Sports category, whereas GPT- 4 exhibits slightly better performance in the Cultural-Religious, Fauna-Flora, and Gastronomy categories. Both models demonstrate comparable performance in the Nature and Urban-Rural categories. The second best performing model is LlaVA-1.5+BERT-FT under approach (2) proving an accuracy of 93.40% of around -3.5% below top-1 accuracy.\nIn terms of programmer productivity, the LVLM GPT-4 can be considered as the winning model as it does not require any programming effort or building labeled dataset for fine-tuning the model. Whereas the solu- tions that provides competitive or very close results, namely the LVM DINO-LP and LLaVA-1.5+BERT-FT require building a small labeled dataset for fine-tuing the models besides the additional required programming effort."}, {"title": "6. Conclusion", "content": "This work investigated the challenge of mapping social media images into a set of CES categories by leveraging the latest advancements in large models, including LVLM, LVM, and LLM. We proposed, analyzed, and compared multiple approaches that utilized one or a combination of these models to address this problem. Our experiments demonstrate that LVLMs are particularly promising for non-programmers. These mod- els delivered excellent results by simply selecting an appropriate prompt, eliminating the need for labor- intensive data labeling and programming efforts. This makes them a highly accessible and effective solution for users with minimal technical expertise. On the other hand, LVMs proved to be equally competitive, particularly when fine-tuned on a small labeled dataset. While this approach requires additional effort in data preparation, it offers comparable accuracy to LVLMs, making it a viable option for those willing to invest in the labeling process.\nOverall, our study highlights the potential of large models, with LVLMs standing out for ease of use and LVMs for their performance when fine-tuned. Future work could explore expanding these approaches to recognize fine-grained CES categories from large-scale social media data."}, {"title": "Declaration of Competing Interest", "content": "The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper."}, {"title": "Data availability", "content": "Data will be made available to the public in Zenodo repository after the acceptation of the paper."}]}