{"title": "Seeing Clearly by Layer Two: Enhancing Attention Heads to Alleviate Hallucination in LVLMS", "authors": ["Xiaofeng Zhang", "Yihao Quan", "Chaochen Gu", "Chen Shen", "Xiaosong Yuan", "Shaotian Yan", "Hao Cheng", "Kaijie Wu", "Jieping Ye"], "abstract": "The hallucination problem in multimodal large language models (MLLMs) remains a common issue. Although image tokens occupy a majority of the input sequence of MLLMs, there is limited research to explore the relationship between image tokens and hallucinations. In this paper, we analyze the distribution of attention scores for image tokens across each layer and head of the model, revealing an intriguing and common phenomenon: most hallucinations are closely linked to the pattern of attention sinks in the self-attention matrix of image tokens, where shallow layers exhibit dense attention sinks and deeper layers show sparse attention sinks. We further analyze the attention heads of different layers and find that heads with high-density attention sink in the image part play a positive role in alleviating hallucinations. In this paper, we propose a training-free method named Enhancing Attention Heads (EAH), an approach designed to enhance the convergence of image tokens attention sinks in the shallow layers. EAH identifies the attention head that shows the vision sink in a shallow layer and extracts its attention matrix. This attention map is then broadcast to other heads in the layer, thereby strengthening the layer to pay more attention to the image itself. With extensive experiments, EAH shows significant hallucination-mitigating performance on different MLLMs and metrics, proving its effectiveness and generality.", "sections": [{"title": "1. Introduction", "content": "Multimodal large language models (MLLMs) [1, 3, 10, 13, 25, 29, 32, 44, 46] have made significant strides in cross-modal tasks, especially in handling both text and image modalities. However, hallucinations remain a persistent challenge, particularly in tasks such as Visual Question Answering (VQA) or image captioning. Current methods for addressing hallucinations often involve changing decoding strategies, incorporating external knowledge bases, or retraining models with additional data [22, 28, 30]. These approaches, however, often require significant resources and time."}, {"title": "2. Related Work", "content": "While the mechanisms of large language models (LLMs) and multimodal large language models (MLLMs) remain complex and not fully understood, several approaches focusing on information flow and attention sink patterns provide valuable insights into their operation and offer potential solutions to issues such as hallucinations and inefficiencies.\nStreamingLLM [37] first introduces the concept of attention sink. The authors observe an intriguing phenomenon: initial tokens, while seemingly less important for the overall content generation, consistently receive high attention scores. This is visualized in the attention map as columns with notably high attention scores, which is counterintuitive. Furthermore, because of the autoregressive nature of generative models, these initial tokens continue to receive more attention from subsequent tokens, amplifying their impact on the generation process. To address this, StreamingLLM leverages these attention-sink tokens during the pre-training phase to enhance the model's performance.\nIn a similar vein, Label Words [35] focuses on information flow within the model, identifying anchor tokens through saliency scores (attention score \u00d7 gradient) as key to in-context learning (ICL). In Zero-shot ICL and Chain-of-Thought (CoT) tasks, these anchors align with the prompt, while in few-shot tasks, they converge on the final token, highlighting their role in effective learning and decision-making.\nBuilding on this idea, ACT [42] provides a deeper exploration of attention sinks in LLMs. By analyzing attention maps across various tasks and inputs, the study finds that not all attention sinks are beneficial for model accuracy. To mitigate this, ACT introduces an adaptive method for optimizing attention distributions during inference, aiming to improve model performance by selectively enhancing the most relevant attention patterns.\nIn the context of multimodal large language models (MLLMS), OPERA [17] introduces a novel perspective by linking the causes of hallucinations with attention sinks. This approach provides new insights into the interpretability of MLLMs. OPERA reveals that during the inference phase, the generation of key tokens such as '-', '?', or tokens that summarize previous ones can lead the model to produce hallucinated content. To address this issue, OPERA imposes penalty constraints on the attention scores of these summarization tokens. In light of this, DOPRA [36] addresses the over-reliance by improving the strategy of weighted overlay penalties and redistribution in specific layers."}, {"title": "2.1. Attention Sink and Information Flow", "content": "While the mechanisms of large language models (LLMs) and multimodal large language models (MLLMs) remain complex and not fully understood, several approaches focusing on information flow and attention sink patterns provide valuable insights into their operation and offer poten-"}, {"title": "3. Method", "content": "Popular VLMs, such as LLaVA-1.5 [29], Minigemini [26], InstructBLIP [10], Shikra [4], MiniGPT-4 [46], Qwen-VL [3], and InternVL [6], consistently exhibit a notable pattern: vision sinks are densely concentrated within the first and second layers, gradually becoming more sparse in deeper layers. As illustrated in Fig. 2, Fig. 3, and Fig. 4, we conclude that a lower density of vision sinks and fewer vision sink heads correlate with an increased likelihood of hallucinations in model outputs. We hypothesize that maintaining dense attention sinks in shallow layers may help alleviate hallucinations, as concentrated attention in the early layers enhances the transfer of image information to subsequent layers. Therefore, a practical method is proposed to reduce hallucinations by ensuring a dense vision sink of attention heads by layer1 and layer2. Please refer to the supplementary material for more attention-map visualization results of different LVLMs."}, {"title": "3.1. Relationship between Vision Sink and Hallucinations", "content": "Popular VLMs, such as LLaVA-1.5 [29], Minigemini [26], InstructBLIP [10], Shikra [4], MiniGPT-4 [46], Qwen-VL [3], and InternVL [6], consistently exhibit a notable pattern: vision sinks are densely concentrated within the first and second layers, gradually becoming more sparse in deeper layers. As illustrated in Fig. 2, Fig. 3, and Fig. 4, we conclude that a lower density of vision sinks and fewer vision sink heads correlate with an increased likelihood of hallucinations in model outputs. We hypothesize that maintaining dense attention sinks in shallow layers may help alleviate hallucinations, as concentrated attention in the early layers enhances the transfer of image information to subsequent layers. Therefore, a practical method is proposed to reduce hallucinations by ensuring a dense vision sink of attention heads by layer1 and layer2. Please refer to the supplementary material for more attention-map visualization results of different LVLMs."}, {"title": "3.2. Vision Sink", "content": "Definition of Mask Matrix M. To ignore diagonal elements in the attention map during calculations, we define a mask matrix M as follows:\n$M \\leftarrow eye(r, c) \u2013 diag(1)$,\nwhere $eye(r, c)$ generates an identity matrix of size $(r, c)$, and we set the diagonal elements to zero.\nDefinition of Vision Sink. Let $h_{i,j}$ represent the attention map of the j-th head at the i-th layer, with $h_{i,j}[x][y]$ being the element at row x and column y. We define a \"vision sink\" as the column in the attention map within the image token range (e.g., $k \\in [36,611]$), where the average attention score of one element of the column within the image token range exceeds a threshold $\\beta$."}, {"title": "3.3. Enhancing Attention Head", "content": "As mentioned above, we introduce a training-free, plug-and-play method called Enhancing Attention Head (EAH) to keep attention heads densely concentrated in the early layers. This approach identifies the attention head with the most dense attention sinks and broadcasts its attention map across other heads. This is to reinforce the attention pattern of a particular head or to broadcast the attention pattern under certain specific conditions (e.g. when a predefined threshold is exceeded).\nThe algorithmic process is shown in Algorithm 1, let A be a 4D tensor, where A[i][j] denotes the attention matrix of the jth head of the ith layer. Let $\\beta$ be the threshold, image-token-start-index be s, image-token-end-index be e and M be a mask matrix of the same size as $h_{i,j}$. We define $h_{i,j}$ as the attention-map of a head:\nFor each column y, the \"vision sink\" condition is defined as:\nvision sink = $\\frac{\\sum_{x=k}^{x=k} h_{i,j}[x][y] \\cdot M}{r-k} > \\beta$,\nwhere k \u2208 [36,611].\nDefinition of Dense Vision Sink Head: For a head (i, j), we calculate the proportion of columns that meet the vision sink condition, denoted as $a_{i}^{j}$. If this proportion of vision sinks within the range of image tokens (e.g., 576) exceeds a preset threshold \u03b3, we classify the head as a \"dense vision sink head.\" which is defined as follows:\n$a_{i}^{j} = \\frac{Num(vision sinks)}{576} \\geq \\gamma$.\nInitialization. Set the threshold \u03b2 and initialize the variables: k as a randomly selected index within the range [s, e], where s = 36 is the starting index of the image tokens and e = 611 is the end index; also initialize n = 0 and H = [] (an empty list).\nIteration over Heads. Select a specific attention layer i (i \u2208 {0, 1, 2}), iterate on each head j (j \u2208 [0,31]).\nCalculate Vision Sinks. For each column y in $h_{i,j}$, calculate whether its column is a vision sink based on:\nidi,j = {(x, y) | $\\frac{\\sum_{r=k}^{r=k} h_{i,j}[x, y] \\cdot M}{r-k} > \\beta$},\nwhere idi,j stores the indices (x, y) where the average attention score exceeds \u03b2.\nStore Count of Vision Sinks for each Attention Head. Compute the count of marked indices for each head:\n$C_{i,j} = count(id_{i,j})$,\nthen append ($C_{i,j}$, j) to H:\nH = H\u222a {($C_{i,j}$, j)}.\nUpdate Head Index n with Maximum Vision Sinks. Find the index n of the head with the maximum count $C_{i,j}$ in H:\nn = arg max$_{(C_{i,j},j) \u2208 H} C_{i,j}$\nThis step dynamically updates n to track the head with the most vision sinks across the layer.\nEnhance Attention Heads across the Layer. For each layer i, set the matrix of head j with the head with the highest number of vision sinks to be the n-th position in A[i]:\nfor j = 0, 1, . . ., 31 : A[i][j] = A[i][n]."}, {"title": "4. Experiment", "content": ""}, {"title": "4.1. Experimental Setup and Dataset", "content": "POPE evaluation on hallucinations. POPE [24] is a dataset designed to evaluate hallucinations at the object level in question-answering tasks. It consists of a series of true/false questions about images, such as \"Is there a dog in the image?\". Given a dataset of images and their corresponding object annotations, POPE constructs triples that include an image, a question, and a response.\nCHAIR evaluation on hallucinations. The Caption Hallucination Assessment with Image Relevance (CHAIR) [31] metric is designed to evaluate hallucinations in object-level image captioning tasks. It includes two key dimensions: CHAIR, measures the proportion of objects mentioned in the caption that are not present in the ground-truth image, while CHAIRs assesses the proportion of captions that contain hallucinations.\n$C_s = \\frac{|{\\text{captions w/ hallucinated objects}}|}{|{\\text{all captions}}|}$,\n$C_\\iota = \\frac{|{\\text{hallucinated objects}}|}{|{\\text{all mentioned objects}}|}$,\nEvaluation on general vision-language tasks. We also evaluate EAH on a variety of visual-verbal benchmarks, including general visual-linguistic tasks and vision-centered tasks such as MME-Bench [40], MM-Vet [41], VizWiz [16], VQAv2 [14], SEED [21], GQA [18], and Blink [12]. SEED"}, {"title": "4.2. Evaluation Results of EAH on Hallucination Benchmarks", "content": "It is shown in Table 1, that the methods to mitigate hallucinations can be broadly classified into four groups. The first group includes OPERA [17], DOPRA [36], VCD [20], HACL [7] and AGLA [2], which address hallucinations by altering the decoding process. The second group, represented by LESS is more [43], modifies the logits of the end-of-sequence (EOS) symbol to control its positioning, allowing the model to terminate earlier and reduce hallucinations. The third group is CCA-LLaVA [38], which explores the weakened information flow between visual and instruction tokens caused by the long-term decay in rotational position encoding (ROPE). They propose Concentric Causal Attention (CCA) to alleviate the object hallucination problem by reorganizing the positions of visual tokens and correcting the causal mask. The fourth group includes ITI [23] and EAH, which aim to adjust attention heads to enhance the truthfulness of the model's output during inference. Among these methods, EAH demonstrates competitive performance and achieves notable results. Among these methods, EAH demonstrates competitive performance, ranking among the top three alongside FastV [5] and LESS [43]."}, {"title": "4.3. Evaluation Results of EAH on General Vision-language Tasks and Benchmarks", "content": "It is shown that in Table 2 and Table 3, compared to the baseline model LLaVA1.5, our EAH method achieves non-negligible gains on all benchmark datasets without introducing additional computation during inferencing. Such performance improvements highlight the potential of EAH in enhancing LVLM's general visual perception capabilities."}, {"title": "4.4. Ablation Study", "content": ""}, {"title": "4.4.1. Generalization Study of EAH on Other LVLMs", "content": "In contemporary MLLMs, images are processed by a CLIP model, mapped through different projectors, and integrated with large language models (LLMs). We hypothesize that the convergence of information flow in the early layers is affected by how different projectors\u2014such as Linear, MLP, Cross-attention, and Q-former\u2014map images to tokens. As shown in Table 8, to test this hypothesis, we apply the EAH method to various models, including LLaVA1.5 [29], Shikra [4], Minigpt4 [46], Instructblip [10], Minigemini [26], QwenVL [3] and InternVL [6]. Notably, Shikra, LLaVA, Intern-VL, Qwen-VL, and Mini-Gemini use greedy search for decoding, while InstructBLIP uses beam search with a beam size of 5. Despite the different decoding strategies and projectors, all models exhibit a consistent pattern of dense attention sink in the shallow layers and sparse attention sink in the deeper layers. Applying EAH to these models consistently improves performance, demonstrating its effective plug-and-play capability and broad applicability.\nMoreover, we find that models using MLP or Linear projectors, such as LLaVA, Intern-VL, and Shikra, show the most significant improvements after applying EAH. In contrast, models with cross-attention or Q-former projectors, like Qwen-VL and MiniGPT-4, exhibit modest gains.\nIn the supplementary material, we provide several attention maps for different LVLMs. It is observed that some models, such as MiniGPT-4 and Qwen-VL, also display a pattern of dense attention sinks in shallow layers and sparse attention sinks in deep layers. However, the gap between shallow and deep layer attention sinks is less pronounced compared to models like LLaVA1.5 and Intern-VL, resulting in relatively smaller improvements. Given that most current LVLMs are fine-tuned versions of LLaMA (vicuna) [34] or Qwen [39], the attention map distribution in LVLMs likely inherits patterns from the underlying LLMs. Therefore, we also visualize Qwen and LLaMA with their attention maps. We find a consistent pattern in these LLMs: dense attention sinks in shallow layers and sparse attention sinks in deeper layers. When examining individual attention heads, we observe that certain attention head distributions in LVLMs and LLMs are quite similar."}, {"title": "4.4.2. Generalization Study of EAH on LLM Models", "content": "As mentioned above, we find a similar pattern in LLMS and LVLMs. To verify the feasibility of EAH on LLMs, we chose four models including LLaMA3.1-instruct [11], Ministral-8B-Inst [19], Qwen-2-7B-Inst [39] and Qwen-2.5-7B-Inst [33]. The datasets are GSM8K [9] and TruthfulQA [27], respectively. The results are shown in Table.5, which demonstrates that EAH can produce consistency gains in LLM as well."}, {"title": "4.4.3. Ablation Study of Hyper-parameter", "content": "Table 6 presents the ablation study results for the parameters Threshold: \u03b2, Layer: L, Top Head: N. The experimental results indicate that the configuration with layer=2, \u03b2=0.002, and N=top1, yields the best performance, achieving Cs of 36.6 and C\u2081 of 9.9.\nThe improvement achieved by broadcasting the top attention head primarily benefits from the centralization of attention. Since most vision sinks are concentrated in the first and second layers, broadcasting the attention map of the head with the densest vision sink in these layers to the other heads helps unify each head's focus on visual information, forming a \"consensus\" attention pattern. Ultimately, the high-density vision sink pattern enables the model to capture key information from the image, effectively reducing hallucinations."}, {"title": "4.4.4. Grad-CAM Results of EAH", "content": "At the same time, we demonstrate the heat map of the model through LLaVA-CAM [45]. As shown in Fig. 7, the comparison between the base model LLaVA1.5 without EAH and with EAH using Grad-CAM visualizations reveals that the model with EAH shows slightly increased attention to key objects in the image at shallow layers, such as \"horse,\" \"person,\" and \"food\". This result demonstrates that the EAH method, which enhances attention heads in shallow layers, improves the model's generalization ability. It enables the model to focus more on essential regions in the image, strengthening the flow of information and enhancing its overall capabilities."}, {"title": "5. Conclusion", "content": "In this paper, we introduce a training-free and plug-and-play method named Enhancing Attention Heads (EAH) to alleviate the challenge of hallucinations in multimodal language models (MLLMs). EAH is designed to enhance the densities and distribution of image token attention sinks in the shallow layers, thereby mitigating hallucinations. Our extensive benchmark tests on hallucination and generalization experiments demonstrate the plug-and-play effectiveness of EAH as a training-free approach."}, {"title": "A. Appendix", "content": ""}, {"title": "A.1. Discussion and Limitations", "content": "The results of this paper validate ITI's [23] conclusion that only a subset of attention heads plays a significantly more prominent role. Effectively optimizing these key attention heads is likely to yield substantial improvements in model efficiency and overall performance. While the method proposed in this paper significantly alleviates hallucinations and demonstrates both simplicity and effectiveness, it still has some limitations. For instance, it exhibits about a 4 percentage point lower recall compared to other methods at the same avglen. However, it is also important to note that EAH outperforms greedy search methods to maintain diversity and serves as a robust plug-and-play and training-free solution. To address hallucination issues more fundamentally, we believe that improved alignment of projectors and advanced training methods, such as reinforcement learning from human feedback (RLHF), is necessary for more effective resolution."}, {"title": "A.2. Why EAH in Q/K before V", "content": "The reason for intervening before V is that the attention matrix attn_weights (i.e., attention_map) represents the weight distribution between different queries and keys. EAH (Enhancing Attention Heads) specifically modifies these weights to adjust the attention distribution. If the intervention happens before V, it allows direct control over the attention concentration during the soft weight allocation stage, making attn_weights more focused on the relevant image information. The adjusted attn_weights, when multiplied with V, will more effectively filter out important information.\nIf the intervention occurs after V, the effect of EAH will be limited to the final attn_output value, rather than modifying the attention matrix itself. This makes it harder to effectively control the attention on specific tokens.\nTherefore, intervening at the attn_weights stage before V allows for a more direct impact on the model's focus on different tokens, thereby improving performance."}, {"title": "A.3. More ablation study", "content": ""}, {"title": "A.3.1. Comparison of Generation Time", "content": "In Fig. 8, we compare the generation time of EAH with existing methods for alleviating hallucinations. Both EAH and OPERA [17] are methods that require attention intervention, and we utilize a standard self-attention implementation. In contrast, other methods such as Greedy, DoLA, VCD, and HALC [7] do not necessitate attention intervention. All methods were tested on a single A100-80GB GPU. Our observations indicate that EAH achieves a decoding time similar to that of VCD [20]. It is slightly longer than the Greedy and DoLA [8] methods due to our intervention"}, {"title": "A.3.2. Qualitative Experiment of Thresholds and Layers", "content": "Table 7 presents the results of our ablation experiments, which assess the impact of various thresholds and layers on model performance. We test different thresholds (0.0006, 0.0008, 0.0015, 0.002) and layers (1, 2, 3, 4, 16, 32) to observe their effects on the model's performance with the CHAIR dataset.\nThe results show that the model achieves its highest performance on the CHAIRs and CHAIR\u2081 metrics, with scores of 36.6 and 10.0, respectively, when applying EAH at the second layer with a threshold of 0.002. However, both metrics significantly decrease as the number of layers increases. This supports our hypothesis that information flow converges in early layers and diverges in deeper layers, and keeping the attention sink dense in shallow layers will effectively alleviate the hallucination. As the depth increases, both CHAIRs and CHAIR\u2081 values rise and exceed the baseline, suggesting that the likelihood of the model generating hallucinations at deeper layers increases. This occurs because attention sinks become more sparse in deeper layers and the differences between attention heads diminish. Therefore, even if the most densely concentrated attention-sink head is identified and broadcasted to other heads, its impact may still be limited."}, {"title": "A.3.3. Experiment results on other hallucination benchmark", "content": ""}, {"title": "A.3.4. Qualitative Experiment of Heads Number", "content": "As demonstrated in the previous section, the first two layers contain the most attention sinks. Therefore, we focus on applying the EAH strategy to these layers. In Table 8, we present a qualitative experiment to assess the impact of increasing the number of attention heads affected. We test this by broadcasting the densest attention head across 4, 8, 16, and 32 heads. For instance, when broadcasting to 4 heads, the attention map from the densest head is duplicated across these 4 heads, while the remaining 28 heads remain unchanged.\nThe results indicate that broadcasting the densest attention head to 32 heads achieves the best performance. This suggests that using the densest attention pattern from the early layers improves the model's focus on image information, enabling the model to concentrate on global image information rather than allowing attention to converge on specific tokens. This approach significantly helps to alleviate hallucinations."}]}