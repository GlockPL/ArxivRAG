[{"title": "On the Impact of the Utility in Semivalue-based Data Valuation", "authors": ["M\u00e9lissa Tamine", "Benjamin Heymann", "Patrick Loiseau", "Maxime Vono"], "abstract": "Semivalue-based data valuation in machine learn-ing (ML) quantifies the contribution of individualdata points to a downstream ML task by leverag-ing principles from cooperative game theory andthe notion of utility. While this framework hasbeen used in practice for assessing data quality,our experiments reveal inconsistent valuation out-comes across different utilities, albeit all related toML performance. Beyond raising concerns aboutthe reliability of data valuation, this inconsistencyis challenging to interpret, as it stems from thecomplex interaction of the utility with data pointsand semivalue weights, which has barely beenstudied in prior work. In this paper, we takea first step toward clarifying the utility impacton semivalue-based data valuation. Specifically,we provide geometric interpretations of this im-pact for a broad family of classification utilities,which includes the accuracy and the arithmeticmean. We introduce the notion of spatial sig-natures: given a semivalue, data points can beembedded into a two-dimensional space, and util-ity functions map to the dual of this space. Thisgeometric perspective separates the influence ofthe dataset and semivalue from that of the util-ity, providing a theoretical explanation for theexperimentally observed sensitivity of valuationoutcomes to the utility choice.", "sections": [{"title": "1. Introduction", "content": "Supervised machine learning (ML) relies on data, but real-world datasets often suffer from noise and biases as they arecollected from multiple sources and are subject to measure-ment and annotation errors (Northcutt et al., 2021). Suchvariability can impact learning outcomes, highlighting theneed for systematic methods to evaluate data quality. Inresponse, data valuation has emerged as a growing researchfield that aims to quantify individual data points' contri-bution to a learning task, helping to identify informative samples and mitigate the impact of low-quality data. A popular way to tackle the data valuation problem is to adopt a cooperative game-theoretic viewpoint, where each datapoint is modeled as a player in a coalitional game, andthe usefulness of any data subset is measured by a utilityfunction. This approach leverages game theory solutionconcepts called semivalues (Dubey et al., 1981), which input data and utility to assign an importance score to eachdata point (Ghorbani & Zou, 2019; Kwon & Zou, 2022;Wang & Jia, 2023; Jia et al., 2023; 2020). When comput-ing semivalues, the utility function is typically selected asa performance metric, such as the accuracy in classifica-tion or the mean squared error in regression. However, thischoice is inherently unconstrained: any function mappingdata subsets to real values can serve as a utility as long asa higher utility reflects better performance. This flexibilityraises a fundamental and legitimate question: to what extentdoes the choice of utility impact data valuation outcomes? Despite the widespread use of semivalue-based data valu-ation, there is a limited theoretical understanding of howand why the choice of utility function influences valuationoutcomes. In practice, utility functions are often chosen forconvenience, typically as standard ML performance met-rics (Ghorbani & Zou, 2019), rather than being grounded in theoretical principles. However, Wang et al. (2024) demon-strated that for a particular semivalue-based method, certainutility choices can lead to valuation outcomes no better thanrandom importance assignment when no specific constraintsare imposed. This finding underscores a fundamental is-sue: the flexibility in utility selection introduces variabil-ity in data valuation, potentially leading to inconsistent ormisleading conclusions. This question's lack of theoreticalgrounding is particularly concerning in high-stakes decision-making scenarios such as healthcare, where data valuationinforms critical tasks (Pandl et al., 2021; Bloch & Friedrich,2021; Zheng et al., 2024). Practitioners risk making un-reliable decisions that undermine model performance andinterpretability without a clear understanding of how util-ity functions shape valuation outcomes. Our study aims tofill in this gap, providing insights to better understand datavaluation and its practical applications.Our contributions can be summarized as follows:"}, {"title": "1. Empirical evidence of data valuation outcomes variability across utility functions.", "content": "Our experiments reveal that the agreement between two utility functions in assessing data importance varies unpredictably across datasets and semivalues. For a given dataset and semi-value, two utilities may produce similar rankings ofdata points, while for another pair, they may diverge entirely. This lack of a systematic pattern suggests that a utility's impact on data valuation is not solely determined by its intrinsic properties but rather by itsinteraction with the dataset and the semivalue."}, {"title": "2. A geometric interpretation of a utility interaction with data and semivalue.", "content": "We propose a geometric framework for a class of binary classification utilities to better understand this interaction. We introduce the concept of spatial signatures, which correspond to an embedding of the dataset into a two-dimensional spaceinduced by the semivalue. We show that the utilityfunctions we consider map to the dual of this space, enabling data values to be visualized as projections onto directions defined by the utility. This geometric perspective provides a structured way to understand whichdatasets and semivalues lead to robust data valuationsacross utilities and which lead to variable and inconsistent valuations. In particular, it explains why, in ourexperiments, utility functions influence data valuationinconsistently across different datasets and semivalues."}, {"title": "Related work.", "content": "Game-theoretic approaches to data valuation have gained traction in recent years due to their formaljustification through axioms. The Shapley value (Shapley,1953; Ghorbani & Zou, 2019), in particular, has been widelyadopted as a data valuation method because it uniquely sat-isfies four key axioms: linearity, dummy player, symmetry,and efficiency. Alternative approaches have emerged byrelaxing some of these axioms. By omitting the efficiencyrequirement, one obtains the semivalue framework (Dubeyet al., 1981). Examples of value notions within this classinclude LOO (Leave-One-Out) (Koh & Liang, 2020), BetaShapley (Kwon & Zou, 2022), and Data Banzhaf (Wang& Jia, 2023). Furthermore, relaxing the linearity axiomleads to the Least Core, an alternative concept from thecooperative game theory proposed by (Yan & Procaccia,2021) for data valuation. The Least Core determines anoptimal profit allocation where each coalition S receives theminimum required subsidy to prevent any participant fromdefecting from the grand coalition D. The DistributionalShapley Value (Ghorbani et al., 2020; Kwon et al., 2021) isan extension of Data Shapley designed to assess data contri-butions based on an underlying data distribution rather thana fixed dataset. Beyond cooperative game theory, severalnon-game-theoretic data valuation methods have been ex-plored. An overview is provided by (Sim et al., 2022), andsome of them are benchmarked by (Jiang et al., 2023)."}, {"title": "2. Background", "content": ""}, {"title": "2.1. Semivalue-based data valuation set-up", "content": "The data valuation problem involves a dataset of interest \\(D = \\{z_i = (x_i, y_i)\\}_{i\\in[n]}\\), where for any \\(i \\in [n]\\) each\\(x_i \\in X\\) is a feature vector and \\(y_i \\in Y\\) is the correspondinglabel. We focus on binary classification, where \\(y = \\{0,1\\}\\).Data valuation aims to assign a scalar score to each datapoint in \\(D\\), quantifying its contribution to a downstream MLtask. These scores will be referred to as data values.Utility functions. Most data valuation methods rely onutility functions to compute data values. A utility is a setfunction \\(u : 2^D \\rightarrow R\\) that maps any subset \\(S\\) of the training set \\(D\\) to a score indicating its usefulness for performingthe considered ML task. Formally, this can be expressedas \\(u(S) = PERF(A(S))\\), where \\(A\\) is a learning algorithmthat takes a subset \\(S\\) as input and returns a trained model,and \\(PERF\\) is a metric function used to evaluate the model'sperformance. For classification tasks, \\(PERF\\) can be chosen,for instance, as the accuracy evaluated on a hold-out test set \\(D_{\\text{test}}\\). There are, however, many other choices of performance function, which lead to different utility functions\u2014this is precisely the focus of our study. For convenience, weinterchangeably refer to the utility \\(u\\) and the performancemetric \\(PERF\\) as \\(u\\) inherently depends on \\(PERF\\).Semivalues. The most popular data valuation methods assign a value score to each data point in \\(D\\) using solutionconcepts from cooperative game theory, known as semi-values (Dubey et al., 1981). The collection of data valu-ation methods that fall under this category is referred toas semivalue-based data valuation. These methods relyon the notion of marginal contribution. Formally, for any\\(i, j \\in [n]\\), let \\(D_{z_i}^j\\) denote the set of all subsets of \\(D\\) of size\\(j - 1\\) that exclude \\(z_i\\). Then, the marginal contribution of \\(z_i\\)with respect to other \\(j - 1\\) samples is defined as\\[\\Delta_j(z_i; u) := \\frac{1}{\\binom{n-1}{j-1}} \\sum_{S \\subset D_{z_i}^j} [u(S \\cup \\{z_i\\}) - u(S)].\\]The marginal contribution \\(\\Delta_j(z_i; u)\\) considers all possible subsets \\(S \\in D_{z_i}^j\\) with the same cardinality \\(j - 1\\) andmeasures the average changes of \\(u\\) when datum of interest\\(z_i\\) is removed from \\(S \\cup \\{z_i\\}\\).Each semivalue-based method is characterized by a weight vector \\(w := (\\omega_1,..., \\omega_n)\\) and assigns a score \\(\\phi(z_i; w, u)\\) to"}, {"title": "2.2. Applications of semivalue-based methods", "content": "In practice, semivalue-based methods are mostly appliedto perform data cleaning or data subset selection (Tanget al., 2021; Pandl et al., 2021; Bloch & Friedrich, 2021;Zheng et al., 2024). Both tasks involve ranking data pointsaccording to their assigned values.Data cleaning. Data cleaning aims to improve datasetquality by identifying and removing noisy or low-qualitydata points. Since semivalue-based methods quantify eachpoint's contribution to a downstream task, low-valued pointsare natural candidates for removal. Specifically, a commonapproach is to remove points that fall into the set \\(\\mathcal{N}_\\tau\\), definedas the subset of data points with the lowest values (Ghorbani& Zou, 2019). Formally, \\(\\mathcal{N}_\\tau = \\{z_i \\in D \\mid \\phi(z_i; u, w) \\le \\tau\\}\\),where \\(\\tau\\) is a threshold determined through domain knowl-edge or empirical evaluation.Data subset selection. Data subset selection involves choos-ing the optimal training set from available samples to max-imize final model performance. Since semivalues mea-sure data quality, prioritizing data points with the highestvalues is a natural approach. Consequently, a commonpractice in the literature is selecting, given a size budget\\(k\\), the subset \\(S_{\\phi(u,w)}^{(k)}\\) = arg max_\\(S\\subseteq D,|S|=k\\) \\(\\sum_{z_i \\in S} \\phi(z_i; u, w)\\)(Wang et al., 2024)."}, {"title": "3. Variability of data valuations across utility functions: an experimental investigation", "content": "Although utility functions are central in semivalue-baseddata valuation methods, their impact on data valuation out-comes has received little attention. Wang et al. (2024) is"}, {"title": "3.1. An application-agnostic metric based on rank correlation to compare utility impact", "content": "Most data valuation applications depend on the relativeranking of data points based on their assigned values. Datacleaning prioritizes identifying low-ranked points, whiledata subset selection focuses on points with the highestvalues. Since the rankings inherently determine the outcomeof these applications, if rankings induced by different utilityfunctions are highly similar, it suggests that the utilitiesare aligned in their ability to prioritize data points for agiven application. Therefore, rank correlation appears as anintuitive and reasonable metric for evaluating whether utilityfunctions produce consistent data valuation outcomes.Formally, given a dataset \\(D\\) and a semivalue characterizedby weight vector \\(w\\), we compare the impact of two utilities \\(u\\)and \\(v\\) on data valuation outcomes by computing the rank cor-relation between \\(\\{\\phi(z_i; u,w)\\}_{i\\in[n]}\\) and \\(\\{\\phi(z_i; v,w)\\}_{i\\in[n]}\\).Several measures of rank correlation exist to evaluate thesimilarity between two rankings. One of the most widelyused is the Kendall rank correlation coefficient, whichquantifies the agreement between two rankings by comparing the relative order of all pairs of elements.Kendall rank correlation coefficient (Kendall, 1938) mea-sures the ordinal association between two sets of values\\(\\{\\phi_i\\}_{i\\in[n]}\\) and \\(\\{\\phi'_i\\}_{i\\in[n]}\\) assigned to \\(n\\) elements. For anypair of indices \\((i, j)\\) where \\(i < j\\), the pair is concordantif the relative order of \\(\\phi_i\\) and \\(\\phi_j\\) matches the relative or-der of \\(\\phi'_i\\) and \\(\\phi'_j\\) and is discordant otherwise. Let \\(C\\) and\\(D\\) denote the number of concordant and discordant pairs,respectively. The Kendall rank coefficient \\(\\tau(u, v)\\) is definedas\\[\\tau(u, v) = \\frac{C - D}{\\binom{n}{2}},\\]and can be equivalently expressed as\\[\\tau(u, v) = \\frac{1}{\\binom{n}{2}} \\sum_{i<j} [\\text{sgn}(\\phi_i - \\phi_j) \\text{sgn}(\\phi'_i - \\phi'_j)],\\]and \\(\\tau(u,v) = 1\\) indicates perfect alignment between\\(\\{\\phi_i\\}_{i\\in[n]}\\) and \\(\\{\\phi'_i\\}_{i\\in[n]}\\) while \\(\\tau(u, v) = -1\\) traduces per-fect disagreement and \\(\\tau(u, v) = 0\\) an absence of correlation.While similar data value rankings suggest alignment in datavaluation applications, the converse is not always true. Low-rank correlation does not necessarily imply misalignment."}, {"title": "3.2. Experimental evidence of ranking variability", "content": "We perform systematic rank correlation computations onvarious datasets and semivalue-based methods to assess theranking variability of data values induced by different utilityfunctions.Experimental setup. Rank correlation computations areperformed on several publicly available binary classifica-tion datasets widely used in the literature (Ghorbani & Zou,2019; Wang & Jia, 2023; Kwon & Zou, 2022; Jiang et al.,2023). We compute data values using three semivalue-based meth-ods: Data Shapley, (4, 1)-Beta Shapley, and Data Banzhaf.Given a dataset \\(D\\) and a semivalue \\(w\\), we evaluate the im-pact of three commonly used classification utilities: theaccuracy (ACC), the recall (REC), and the arithmetic mean(AM). Specifically, for each utility function pair \\((u, v)\\), wecompute the Kendall rank correlation \\(\\tau(u, v)\\) to quantifyranking consistency. We extend these experiments to addi-tional classification utilities and reproduce them with theSpearman rank correlation for completeness. The corre-sponding results are provided in Appendix C.5."}, {"title": "4. Explaining ranking variability through a geometric interpretation of theutility-dataset-semivalue interplay", "content": "Motivated by the experimental results from Section 3, weaim to understand the interplay between utilities, datasets,and semivalues in order to explain ranking variability. Wefocus on a family of classification utilities that includes ac-curacy, recall, and arithmetic mean, for which we derivegeometric interpretations of ranking diversity. This geomet-ric perspective provides a framework to explain the resultsin Table 1."}, {"title": "4.1. A subclass of linear fractional performance measures", "content": "This section introduces the specific family of utility func-tions we consider in our analysis. We build on the frame-work of linear fractional performance measures (Koyejoet al., 2014), which generalizes various classification met-rics, including the F-score and misclassification risk.These measures express classifier performance as a ratioof affine functions of classification probabilities. Formally,given a training dataset \\(S \\in (X \\times \\{0,1\\})^n\\), a test dataset\\(D_{\\text{test}} = \\{(x_j, y_j)\\}_{j\\in[m]} \\in (X \\times \\{0,1\\})^m\\) and a learningalgorithm \\(A\\), we denote \\(g_S = A(S)\\) a classifier trainedon \\(S\\) which maps input features to predicted labels, i.e.,\\(g_S : X \\rightarrow \\{0,1\\}\\). A linear fractional performance measure\\(u\\) evaluates the performance of \\(g_S\\) on \\(D_{\\text{test}}\\) as\\[u(S) = \\frac{c_0 + c_1 \\lambda(S, D_{\\text{test}}) + c_2 \\gamma(S, D_{\\text{test}})}{d_0 + d_1 \\lambda(S, D_{\\text{test}}) + d_2 \\gamma(S, D_{\\text{test}})},\\]where \\((c_0, c_1, c_2, d_0, d_1,d_2) \\in \\mathbb{R}^6\\) determines the structure of \\(u\\) while \\(\\lambda(S, D_{\\text{test}}) = P_{D_{\\text{test}}}(g_S(x)=1, y=1) =\\frac{1}{m} \\sum_{j=1}^{m} \\mathbb{I}[g_S(x_j) = 1, y_j = 1]\\) is the empirical probability of true positives and \\(\\gamma(S, D_{\\text{test}}) = P_{D_{\\text{test}}}(g_S(x)=1) =\\frac{1}{m} \\sum_{j=1}^{m} \\mathbb{I}[g_S(x_j) = 1]\\) is the empirical probability of pos-itive predictions. When the test dataset \\(D_{\\text{test}}\\) is clear fromcontext, we use the shorthand \\(\\lambda(S)\\) to denote \\(\\lambda(S, D_{\\text{test}})\\)and \\(\\gamma(S)\\) to denote \\(\\gamma(S, D_{\\text{test}})\\).Our analysis focuses on a specific subclass of utilities withinthe framework of linear fractional performance measures.These utilities are characterized by a constant denominator,i.e., \\(d_1 = d_2 = 0\\) and \\(d_0 \\neq 0\\), which simplifies theirformulation to\\[u(S) = \\frac{1}{d_0} [c_0 + c_1 \\lambda(S) + c_2 \\gamma(S)] .\\]We refer to this subclass as the \\((\\lambda, \\gamma)\\)-linear utility class,denoted by \\(\\mathcal{U}_{\\lambda,\\gamma}\\), since each utility function in this classis a linear transformation of the classification statistics \\(\\lambda\\)and \\(\\gamma\\). Accuracy, recall, and arithmetic mean, used in ourexperiments (Section 3), belong to this subclass. Their for-mulation follows Eq. (3), with coefficients \\((c_0, c_1, c_2, d_0)\\)"}, {"title": "4.2. Geometric characterization of dataset, semivalue, and utilty", "content": "This section examines the geometric properties of utilityfunctions in \\(\\mathcal{U}_{\\lambda,\\gamma}\\) in relation to the dataset \\(D\\) and the semi-value \\(w\\). A fundamental property of the utility class \\(\\mathcal{U}_{\\lambda,\\gamma}\\)is that, for any \\(u \\in \\mathcal{U}_{\\lambda,\\gamma}\\), the data value \\(\\phi(z_i;w, u)\\) canbe represented as a linear combination of \\(\\phi(z_i; w, \\lambda)\\) and\\(\\phi(z_i; w, \\gamma)\\), as stated formally in the following proposition.Proposition 4.1. (Linear decomposition of data values for(\\(\\lambda, \\gamma\\))-linear utilities) Let \\(D = \\{z_i\\}_{i\\in[n]}\\) be a dataset and \\(w\\)a semivalue weight vector. For any utility function \\(u \\in \\mathcal{U}_{\\lambda,\\gamma}\\)characterized by coefficients \\((c_0, c_1, c_2, d_0) \\in \\mathbb{R}^4\\), the datavalue assigned to \\(z_i\\) can be decomposed as\\[\\phi(z_i; w, u) = \\frac{c_1}{d_0} \\phi(z_i; w, \\lambda) + \\frac{c_2}{d_0} \\phi(z_i; w, \\gamma).\\]The decomposition in Proposition 4.1 suggests that the in-fluence of the dataset and semivalue is fully captured by thevector \\(e_i = (\\phi(z_i;w, \\lambda), \\phi(z_i;w, \\gamma))\\) which embeds eachdata point \\(z_i \\in D\\) into a two-dimensional space \\(\\mathcal{P}_w\\) inducedby the semivalue \\(w\\). While the embedding \\(e_i\\) depends on \\(\\lambda\\)and \\(\\gamma\\), the spatial structure of \\(D\\) in \\(\\mathcal{P}_w\\) is independent of theparticular choice of \\(u\\) within \\(\\mathcal{U}_{\\lambda,\\gamma}\\) as this choice is uniquelydetermined by the coefficients \\((c_0, c_1, c_2)\\) and \\(d_0\\).Definition 4.2. (Spatial signature) We define the spatialsignature of \\(D\\) in \\(\\mathcal{P}_w\\) as the collection of all embedded datapoints \\(\\mathcal{E} = (e_i)_{i\\in[n]} = ((\\phi(z_i;w, \\lambda), \\phi(z_i; w, \\gamma))_{i\\in[n]}\\)Similarly, utility functions in \\(\\mathcal{U}_{\\lambda,\\gamma}\\) can be characterized bythe vector \\(u = (\\frac{c_1}{d_0}, \\frac{c_2}{d_0})\\), which belongs to space\\(\\mathcal{U}^* \\subset \\mathbb{R}^2\\) (cf. (3) and (4)). This gives the following result:Theorem 4.3. Let \\(\\mathcal{P}_w \\subset \\mathbb{R}^2\\) denote the spacewhere each data point \\(z_i \\in D\\) is embedded as \\(e_i =(\\phi(z_i; w, \\lambda), \\phi(z_i; w, \\gamma))\\). Let \\(\\mathcal{U}^* \\subset \\mathbb{R}^2\\) denote the spaceof linear utilities, where each utility function \\(u \\in \\mathcal{U}^* \\) isrepresented as \\(u = \\binom{\\frac{c_1}{d_0}}{\\frac{c_2}{d_0}}\\). Then, \\(\\mathcal{U}^*\\) is isomorphic to the dual space \\(\\mathcal{P}_w^*\\) of \\(\\mathcal{P}_w\\).Theorem 4.3 establishes a correspondence between utilityfunctions and linear functionals over the data embedding space. In other words, each data value \\(\\phi(z_i;w, u)\\) resultsfrom applying a linear transformation parameterized by \\(u\\)to the embedded data representation \\(e_i\\). This directly leadsto the following geometric interpretation.Corollary 4.4. The data value \\(\\phi(z_i; w, u)\\) can be expressedas the scalar product \\(\\phi(z_i;w, u) = u \\cdot e_i = u(\\frac{c_1}{d_0}, \\frac{c_2}{d_0}) \\cdote(\\omega)\\). This identifies \\(\\phi(z_i; w, u)\\) as the projection of \\(e_i\\) onto the utility direction \\(u\\), explicitly separating the contributionof \\(u\\) and \\(D, w\\).Utility directions and the unit sphere. Building on thisgeometric framework, we aim to understand how distinctutilities in \\(\\mathcal{U}^*\\) induce similar or divergent rankings of datavalues. Since rankings depend only on relative orderingsand not absolute values, the space of all distinct utilities"}, {"title": "4.3. Insights on ranking diversity for extreme cases of spatial signatures.", "content": "The spatial signature of \\(D\\) in \\(\\mathcal{P}_w\\) plays a crucial role in shaping the rankings induced by different utility functions. Inparticular, we analyze two extreme cases: (a) \\(\\mathcal{E}\\) is in generalposition (Definition 4.6), (b) \\(\\mathcal{E}\\) is collinear (Definition 4.7).Definition 4.6. (General position) A spatial signature \\(\\mathcal{E} =(\\mathbb{e}_i)_{i\\in[n]} \\subset \\mathcal{P}_w\\) is in general position if:1. For all distinct \\(i, j, k\\), there does not exist a line \\(L \\subset\\mathcal{P}_w\\) such that \\(\\mathbb{e}_i, \\mathbb{e}_j, \\mathbb{e}_k \\in L\\).2. For all distinct \\(i, j\\), there is no scalar \\(k \\neq 0\\) such that\\(\\mathbb{e}_i = k\\mathbb{e}_j\\).Definition 4.7. (Collinearity) A spatial signature \\(\\mathcal{E} =(\\mathbb{e}_i)_{i\\in[n]} \\subset \\mathcal{P}_w\\) is collinear if there exists \\(w \\in \\mathcal{P}_w\\) andscalar \\(k_i \\in \\mathbb{R}\\) such that \\(\\mathbb{e}_i = k_iw\\) for all \\(i \\in [n]\\).We establish Theorem 4.8, which quantifies the impact ofthese spatial configurations on ranking diversity.Theorem 4.8. Let \\(\\mathcal{E} = (\\mathbb{e}_i)_{i\\in[n]}\\) be a spatial signature in\\(\\mathcal{P}_w\\). Define ranking regions as the connected compo-nents of the unit sphere \\(\\mathbb{S}^1\\) where the linear utilities \\(\\tilde{u} \\in \\mathbb{S}^1\\)induce identical rankings on \\(D\\). Then,1. if \\(\\mathcal{E}\\) is in general position, the number of distinct rank-ing regions is maximal and equal to \\(R_{\\text{gen}}(n) = 2 \\times \\binom{n}{2}\\).2. if \\(\\mathcal{E}\\) is collinear, the number of distinct ranking regionsis minimal as it collapses to \\(R_{\\text{col}}(n) = 2\\).To illustrate this result, Figure 3 presents these two extremecases for a dataset of three points.For a spatial signature \\(\\mathcal{E}\\) that is neither collinear nor in gen-eral position, let \\(\\sigma_1 > \\sigma_2 > 0\\) be the singular values of\\(\\mathcal{E}\\) and \\(\\rho = \\sigma_1/\\sigma_2\\). The number of ranking regions \\(R(n)\\)satisfies \\(2 < R(n) < 2 \\binom{n}{2}\\) and \\(R(n)\\) decreases as \\(\\rho\\) in-creases. As \\(\\rho \\rightarrow 1^+\\) (balanced variance), \\(R(n) \\rightarrow 2 \\binom{n}{2}\\).As \\(\\rho \\rightarrow +\\infty\\) (collinearity), \\(R(n) \\rightarrow 2\\). The transition isgoverned by the alignment of the line crossing the circle at"}, {"title": "4.4. Explaining experimental results from Section 3", "content": "From the spatial signatures of the WIND dataset in Figure 2,we observe an almost collinear structure in \\(\\mathcal{P}_w^{\\text{shap}}\\), \\(\\mathcal{P}_w^{\\text{beta}}\\),and \\(\\mathcal{P}_w^{\\text{banz}}\\). This near-alignment indicates that the embeddedpoints \\(\\mathbb{e}\\) predominantly lie along the principal eigenvector\\(w_\\mathcal{E}\\) of each spatial signature's covariance matrix. For ana-lytical simplicity, we treat these signatures as fully collinear.By Theorem 4.8, collinearity collapses the unit circle \\(\\mathbb{S}^1\\)into two ranking regions, separated by the antipodal pointsworthogonal to \\(w_\\mathcal{E}\\). These antipodal points, defined as\\(H_w = \\{\\tilde{u} \\in \\mathbb{S}^1 \\mid \\tilde{u} \\cdot w_\\mathcal{E}^\\perp = 0\\}\\), partition \\(\\mathbb{S}^1\\) into hemi-spheres where \\(\\tilde{u} \\cdot w_\\mathcal{E} > 0\\) or \\(\\tilde{u} \\cdot w_\\mathcal{E} < 0\\). Utilities within thesame hemisphere induce identical rankings, yielding perfectcorrelation. Figure 4 (left) shows that in our experiments:under \\(w^{\\text{shap}}\\) and \\(w^{\\text{beta}}\\), the utilities \\(\\bar{u}_{\\text{acc}}\\), \\(\\bar{u}_{\\text{rec}}\\), and \\(\\bar{u}_{\\text{am}}\\) liein the same hemisphere, explaining their strong correlation.Under \\(w^{\\text{banz}}\\), \\(\\bar{u}_{\\text{rec}}\\) aligns exactly with the boundary point \\(H_\\omega\\),satisfying \\(\\bar{u}_{\\text{rec}} \\mathbb{e}_i = 0\\) \\(\\forall i\\), which orthogonalizes it to thecollinear direction \\(w\\). This degeneracy equalizes utilityvalues across all points, producing tied rankings and theobserved weak correlation with \\(\\bar{u}_{\\text{acc}}\\) and \\(\\bar{u}_{\\text{am}}\\).Figures 15,16, 17, 18, 19, 20 and 21 in Appendix C.6 present similarvisualizations for other experimental datasets. We observethat Banzhaf consistently produces nearly collinear spatialsignatures across all datasets, leading to reduced rankingdiversity as described in Theorem 4.8. In contrast, Shapleyand Beta Shapley exhibit more dataset-dependent structures,sometimes aligning with Banzhaf but often showing greaterdispersion, resulting in higher ranking variability.To further illustrate ranking diversity in this setting, particu-larly transitions across ranking regions, we present Figure4, which depicts the evolution of the Kendall rank correla-tion along a convex combination path of utility functionsin \\(\\mathbb{S}^1\\). This path is specifically designed to cross one ofthe two antipodals points in \\(H_w\\) that define the rankingregions, encompassing the three key utilities we analyzed\\(\\bar{u}_{\\text{acc}}\\), \\(\\bar{u}_{\\text{rec}}\\), \\(\\bar{u}_{\\text{am}}\\). The resulting visualization captures howrank correlation fluctuates as the utility function movesalong this path. To formally derive this figure, we rely onthe following proposition.Proposition 4.9. Let \\(D\\) be a dataset, \\(w\\) a semivalue andlet \\(u, v \\in \\mathcal{U}_{\\lambda,\\gamma}\\) be two utilities respectively characterizedby \\((c_0, c_1, c_2, d_0) \\in \\mathbb{R}^4\\) and \\((c'_0, c'_1, c'_2, d'_0) \\in \\mathbb{R}^4\\). TheKendall rank correlation between the data values sets\\(\\{\\phi(z,w, u)\\}_{z\\in D}\\) and \\(\\{\\phi(z,w,v)\\}_{z\\in D}\\) denoted as \\(\\tau(u,v)\\)is defined as"}, {"title": "5. Conclusion", "content": "This work advances the understanding of how utility func-tions impact semivalue-based data valuation. Our exper-iments show that the way a utility influences data valuerankings is not solely dictated by its intrinsic properties but"}, {"title": "A. Additional definitions", "content": "Definition A.1. (Spearman rank correlation). The Spearman rank correlation coefficient \\(\\rho_s\\) measures the monotonicrelationship between two sets of values \\(\\{\\phi_i\\"}, {"i\\in[n": ""}, "and \\(\\{\\phi'_i\\}_{i\\in[n"]}, ["rho(u, v) = \\frac{\\text{Cov} (rg(u), rg(\\phi'))}{\\sigma_{rg(\\phi)} \\sigma_{rg(\\phi')}},\\"], {"to": ["rho_s(u, v) = 1 - \\frac{6 \\sum_i d_i^2}{n(n^2 - 1)},\\"], "functions": "Definition A.2. (Accuracy). Accuracy measures the proportion of correctly classified instances and is defined as:\\[\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN"}]