{"title": "On the Impact of the Utility in Semivalue-based Data Valuation", "authors": ["M\u00e9lissa Tamine", "Benjamin Heymann", "Patrick Loiseau", "Maxime Vono"], "abstract": "Semivalue-based data valuation in machine learning (ML) quantifies the contribution of individual data points to a downstream ML task by leveraging principles from cooperative game theory and the notion of utility. While this framework has been used in practice for assessing data quality, our experiments reveal inconsistent valuation outcomes across different utilities, albeit all related to ML performance. Beyond raising concerns about the reliability of data valuation, this inconsistency is challenging to interpret, as it stems from the complex interaction of the utility with data points and semivalue weights, which has barely been studied in prior work. In this paper, we take a first step toward clarifying the utility impact on semivalue-based data valuation. Specifically, we provide geometric interpretations of this impact for a broad family of classification utilities, which includes the accuracy and the arithmetic mean. We introduce the notion of spatial signatures: given a semivalue, data points can be embedded into a two-dimensional space, and utility functions map to the dual of this space. This geometric perspective separates the influence of the dataset and semivalue from that of the utility, providing a theoretical explanation for the experimentally observed sensitivity of valuation outcomes to the utility choice.", "sections": [{"title": "1. Introduction", "content": "Supervised machine learning (ML) relies on data, but real-world datasets often suffer from noise and biases as they are collected from multiple sources and are subject to measurement and annotation errors (Northcutt et al., 2021). Such variability can impact learning outcomes, highlighting the need for systematic methods to evaluate data quality. In response, data valuation has emerged as a growing research field that aims to quantify individual data points' contribution to a learning task, helping to identify informative samples and mitigate the impact of low-quality data. A popular way to tackle the data valuation problem is to adopt a cooperative game-theoretic viewpoint, where each data point is modeled as a player in a coalitional game, and the usefulness of any data subset is measured by a utility function. This approach leverages game theory solution concepts called semivalues (Dubey et al., 1981), which input data and utility to assign an importance score to each data point (Ghorbani & Zou, 2019; Kwon & Zou, 2022; Wang & Jia, 2023; Jia et al., 2023; 2020). When computing semivalues, the utility function is typically selected as a performance metric, such as the accuracy in classification or the mean squared error in regression. However, this choice is inherently unconstrained any function mapping data subsets to real values can serve as a utility as long as a higher utility reflects better performance. This flexibility raises a fundamental and legitimate question: to what extent does the choice of utility impact data valuation outcomes?\nDespite the widespread use of semivalue-based data valuation, there is a limited theoretical understanding of how and why the choice of utility function influences valuation outcomes. In practice, utility functions are often chosen for convenience, typically as standard ML performance metrics (Ghorbani & Zou, 2019), rather than being grounded in theoretical principles. However, Wang et al. (2024) demonstrated that for a particular semivalue-based method, certain utility choices can lead to valuation outcomes no better than random importance assignment when no specific constraints are imposed. This finding underscores a fundamental issue: the flexibility in utility selection introduces variability in data valuation, potentially leading to inconsistent or misleading conclusions. This question's lack of theoretical grounding is particularly concerning in high-stakes decision-making scenarios such as healthcare, where data valuation informs critical tasks (Pandl et al., 2021; Bloch & Friedrich, 2021; Zheng et al., 2024). Practitioners risk making unreliable decisions that undermine model performance and interpretability without a clear understanding of how utility functions shape valuation outcomes. Our study aims to fill in this gap, providing insights to better understand data valuation and its practical applications.\nOur contributions can be summarized as follows:"}, {"title": "1. Empirical evidence of data valuation outcomes variability across utility functions.", "content": "Our experiments reveal that the agreement between two utility functions in assessing data importance varies unpredictably across datasets and semivalues. For a given dataset and semivalue, two utilities may produce similar rankings of data points, while for another pair, they may diverge entirely. This lack of a systematic pattern suggests that a utility's impact on data valuation is not solely determined by its intrinsic properties but rather by its interaction with the dataset and the semivalue."}, {"title": "2. A geometric interpretation of a utility interaction with data and semivalue.", "content": "We propose a geometric framework for a class of binary classification utilities to better understand this interaction. We introduce the concept of spatial signatures, which correspond to an embedding of the dataset into a two-dimensional space induced by the semivalue. We show that the utility functions we consider map to the dual of this space, enabling data values to be visualized as projections onto directions defined by the utility. This geometric perspective provides a structured way to understand which datasets and semivalues lead to robust data valuations across utilities and which lead to variable and inconsistent valuations. In particular, it explains why, in our experiments, utility functions influence data valuation inconsistently across different datasets and semivalues."}, {"title": "Related work.", "content": "Game-theoretic approaches to data valuation have gained traction in recent years due to their formal justification through axioms. The Shapley value (Shapley, 1953; Ghorbani & Zou, 2019), in particular, has been widely adopted as a data valuation method because it uniquely satisfies four key axioms: linearity, dummy player, symmetry, and efficiency. Alternative approaches have emerged by relaxing some of these axioms. By omitting the efficiency requirement, one obtains the semivalue framework (Dubey et al., 1981). Examples of value notions within this class include LOO (Leave-One-Out) (Koh & Liang, 2020), Beta Shapley (Kwon & Zou, 2022), and Data Banzhaf (Wang & Jia, 2023). Furthermore, relaxing the linearity axiom leads to the Least Core, an alternative concept from the cooperative game theory proposed by (Yan & Procaccia, 2021) for data valuation. The Least Core determines an optimal profit allocation where each coalition S receives the minimum required subsidy to prevent any participant from defecting from the grand coalition D. The Distributional Shapley Value (Ghorbani et al., 2020; Kwon et al., 2021) is an extension of Data Shapley designed to assess data contributions based on an underlying data distribution rather than a fixed dataset. Beyond cooperative game theory, several non-game-theoretic data valuation methods have been explored. An overview is provided by (Sim et al., 2022), and some of them are benchmarked by (Jiang et al., 2023)."}, {"title": "2. Background", "content": null}, {"title": "2.1. Semivalue-based data valuation set-up", "content": "The data valuation problem involves a dataset of interest D = {zi = (xi, Yi)}i\u2208[n], where for any i \u2208 [n] each xi \u2208 X is a feature vector and yi \u2208 Y is the corresponding label. We focus on binary classification, where Y = {0, 1}. Data valuation aims to assign a scalar score to each data point in D, quantifying its contribution to a downstream ML task. These scores will be referred to as data values."}, {"title": "Utility functions.", "content": "Most data valuation methods rely on utility functions to compute data values. A utility is a set function u : 2D \u2192 R that maps any subset S of the training set D to a score indicating its usefulness for performing the considered ML task. Formally, this can be expressed as u(S) = PERF(A(S)), where A is a learning algorithm that takes a subset S as input and returns a trained model, and PERF is a metric function used to evaluate the model's performance. For classification tasks, PERF can be chosen, for instance, as the accuracy evaluated on a hold-out test set Dtest. There are, however, many other choices of performance function, which lead to different utility functions\u2014this is precisely the focus of our study. For convenience, we interchangeably refer to the utility u and the performance metric PERF as u inherently depends on PERF."}, {"title": "Semivalues.", "content": "The most popular data valuation methods assign a value score to each data point in D using solution concepts from cooperative game theory, known as semivalues (Dubey et al., 1981). The collection of data valuation methods that fall under this category is referred to as semivalue-based data valuation. These methods rely on the notion of marginal contribution. Formally, for any i, j \u2208 [n], let Dzi denote the set of all subsets of D of size j 1 that exclude zi. Then, the marginal contribution of zi with respect to other j \u2013 1 samples is defined as\n$\\Delta_j(z_i; u) := \\frac{1}{\\binom{n-1}{j-1}} \\sum_{S \\subseteq D \\setminus \\{z_i\\}, |S|=j-1} u(S \\cup \\{z_i\\}) - u(S) .$"}, {"title": "2.2. Applications of semivalue-based methods", "content": "In practice, semivalue-based methods are mostly applied to perform data cleaning or data subset selection (Tang et al., 2021; Pandl et al., 2021; Bloch & Friedrich, 2021; Zheng et al., 2024). Both tasks involve ranking data points according to their assigned values."}, {"title": "Data cleaning.", "content": "Data cleaning aims to improve dataset quality by identifying and removing noisy or low-quality data points. Since semivalue-based methods quantify each point's contribution to a downstream task, low-valued points are natural candidates for removal. Specifically, a common approach is to remove points that fall into the set N\u03c4, defined as the subset of data points with the lowest values (Ghorbani & Zou, 2019). Formally, N\u03c4 = {zi \u2208 D | \u03c6(zi; u, \u03c9) \u2264 \u03c4}, where \u03c4 is a threshold determined through domain knowledge or empirical evaluation."}, {"title": "Data subset selection.", "content": "Data subset selection involves choosing the optimal training set from available samples to maximize final model performance. Since semivalues measure data quality, prioritizing data points with the highest values is a natural approach. Consequently, a common practice in the literature is selecting, given a size budget k, the subset S\u03c6(k) of data points with top-k data values, i.e., S\u03c6(k) = arg maxS\u2286D,|S|=k \u2211zi\u2208s \u03c6(zi; u, \u03c9) (Wang et al., 2024)."}, {"title": "3. Variability of data valuations across utility functions: an experimental investigation", "content": "Although utility functions are central in semivalue-based data valuation methods, their impact on data valuation outcomes has received little attention. Wang et al. (2024) is the only work that theoretically explores this question, focusing specifically on how the choice of utility affects the reliability of Data Shapley for data subset selection. In this section, we broaden this scope by experimentally studying the influence of various utility functions across multiple semivalue-based methods. To conduct this investigation, we propose an application-agnostic metric based on data values ranking, enabling a broader perspective beyond one specific application."}, {"title": "3.1. An application-agnostic metric based on rank correlation to compare utility impact", "content": "Most data valuation applications depend on the relative ranking of data points based on their assigned values. Data cleaning prioritizes identifying low-ranked points, while data subset selection focuses on points with the highest values. Since the rankings inherently determine the outcome of these applications, if rankings induced by different utility functions are highly similar, it suggests that the utilities are aligned in their ability to prioritize data points for a given application. Therefore, rank correlation appears as an intuitive and reasonable metric for evaluating whether utility functions produce consistent data valuation outcomes.\nFormally, given a dataset D and a semivalue characterized by weight vector w, we compare the impact of two utilities u and v on data valuation outcomes by computing the rank correlation between {\u03c6(zi; u,\u03c9)}i\u2208[n] and {\u03c6(zi; v,\u03c9)}i\u2208[n]. Several measures of rank correlation exist to evaluate the similarity between two rankings. One of the most widely used is the Kendall rank correlation coefficient, which quantifies the agreement between two rankings by comparing the relative order of all pairs of elements.\nKendall rank correlation coefficient (Kendall, 1938) measures the ordinal association between two sets of values {\u03c6}i\u2208[n] and {\u03c6}i\u2208[n] assigned to n elements. For any pair of indices (i, j) where i < j, the pair is concordant if the relative order of \u03c6i and \u03c6j matches the relative order of \u03c6i and \u03c6j, and is discordant otherwise. Let C and D denote the number of concordant and discordant pairs, respectively. The Kendall rank coefficient \u03c4(u, v) is defined as \u03c4(u, v) = C\u2212D/(n2), and can be equivalently expressed as\n$\\tau(u, v) = \\frac{1}{\\binom{n}{2}} \\sum_{i < j} sgn[(\\phi(z_i; u, \\omega) - \\phi(z_j; u, \\omega)) (\\phi(z_i; v, \\omega) - \\phi(z_j; v, \\omega))]\\text{,}$"}, {"title": "3.2. Experimental evidence of ranking variability", "content": "We perform systematic rank correlation computations on various datasets and semivalue-based methods to assess the ranking variability of data values induced by different utility functions.\nExperimental setup. Rank correlation computations are performed on several publicly available binary classification datasets widely used in the literature (Ghorbani & Zou, 2019; Wang & Jia, 2023; Kwon & Zou, 2022; Jiang et al., 2023). We compute data values using three semivalue-based methods: Data Shapley, (4, 1)-Beta Shapley, and Data Banzhaf. Given a dataset D and a semivalue w, we evaluate the impact of three commonly used classification utilities: the accuracy (ACC), the recall (REC), and the arithmetic mean (AM). Specifically, for each utility function pair (u, v), we compute the Kendall rank correlation \u03c4(u, v) to quantify ranking consistency. We extend these experiments to additional classification utilities and reproduce them with the Spearman rank correlation for completeness.  \nComputing \u03c4(u, v) for each utility pair (u, v) requires obtaining the data values sets {\u03c6(z, w, u)}z\u2208D and {\u03c6(z, w,v)}z\u2208D. This computation involves a learning algorithm A, and a test dataset Dtest to evaluate the utility on different subsets S \u2286 D, and an approximation method as the exact computation of semivalues is infeasible for large datasets (Ghorbani & Zou, 2019; Jia et al., 2023; Garrido-Lucero et al., 2024). To ensure that any observed differences in both sets' rankings arise solely from the choice of the utility and not from these other sources of variability, we propose a systematic methodology in  that eliminates extraneous perturbations. The results reveal qualitatively similar insights, reinforcing the observation that the impact of utility functions on data valuation rankings is not solely dictated by their intrinsic properties but also by their interaction with the dataset and the semivalue."}, {"title": "Results analysis.", "content": "The experimental results reveal that utility pairs (ACC-AM, ACC-REC, and REC-AM) exhibit no systematic agreement or disagreement across datasets and semivalues. In some cases, utility functions produce highly similar rankings, while their rank correlation is markedly low in others.\nFor example, in the Breast dataset, ACC-AM exhibits strong agreement across all semivalues, with Kendall correlations of 0.98, 0.98, and 0.99 for Shapley, (4, 1)-Beta Shapley, and Banzhaf, respectively. However, this same utility pair shows significantly weaker agreement for the CREDIT dataset, with Kendall correlations dropping to 0.51, 0.58, and 0.07, highlighting a sharp dataset-dependent divergence.\nMoreover, substantial differences emerge within a dataset depending on the chosen semivalue. For the CPU dataset, the correlation between ACC-REC is relatively high under Shapley and (4, 1)-Beta Shapley (Kendall rank correlations of 0.78 and 0.79, respectively), yet it vanishes entirely under Banzhaf (Kendall coefficient = 0.01).\nIn addition, we complement the rank correlation computations with an intersection-based analysis  for datasets, semivalues, and utility pairs exhibiting low Kendall rank correlation coefficient. This analysis assesses whether differences in value rankings indicate misalignment in performing data valuation applications. The results show no cases where low-rank correlation preserves alignment, suggesting that low-rank correlation effectively reflects misalignment for these datasets, semivalues, and utility pairs.\nAll those results suggest that a utility function's influence on data valuation rankings is not an intrinsic property of the utility itself but rather emerges from its interaction with both the dataset and the semivalue. The same utility pair can yield highly similar rankings in one context yet diverge entirely in another, indicating that the way a utility assigns value to data points is shaped by how it interacts with the dataset's structure and how the semivalue aggregates marginal contributions. This reinforces the idea that utility-driven data valuation cannot be understood in isolation its effects are context-dependent, varying with both the dataset characteristics and the weighting mechanism imposed by the semivalue.\nThis utility-dataset-semivalue dependency remains underexplored in prior work, resulting in a limited understanding of what semivalue-based methods truly capture. This raises concerns, as data valuation methods are intended to enhance the interpretability of dataset quality. Addressing this gap requires a deeper exploration of the interplay between utilities, datasets, and semivalue weights to ensure semivalue-based data valuation delivers its promise."}, {"title": "4. Explaining ranking variability through a geometric interpretation of the utility-dataset-semivalue interplay", "content": "Motivated by the experimental results , we aim to understand the interplay between utilities, datasets, and semivalues in order to explain ranking variability. We focus on a family of classification utilities that includes accuracy, recall, and arithmetic mean, for which we derive geometric interpretations of ranking diversity. This geometric perspective provides a framework to explain the results in Table 1."}, {"title": "4.1. A subclass of linear fractional performance measures", "content": "This section introduces the specific family of utility functions we consider in our analysis. We build on the framework of linear fractional performance measures (Koyejo et al., 2014), which generalizes various classification metrics, including the F-score and misclassification risk.\nThese measures express classifier performance as a ratio of affine functions of classification probabilities. Formally, given a training dataset S \u2208 (X \u00d7 {0, 1})n, a test dataset Dtest = {(xj,yj)}j\u2208[m] \u2208 (X \u00d7 {0, 1})m and a learning algorithm A, we denote gs = A(S) a classifier trained on S which maps input features to predicted labels, i.e., gs : X \u2192 {0, 1}. A linear fractional performance measure u evaluates the performance of gs on Dtest as\n$u(S) = \\frac{c_0 + c_1 \\lambda(S, D_{test}) + c_2 \\gamma(S, D_{test})}{d_0 + d_1 \\lambda(S, D_{test}) + d_2 \\gamma(S, D_{test})},$"}, {"title": "4.2. Geometric characterization of dataset, semivalue, and utilty", "content": "This section examines the geometric properties of utility functions in U\u03bb,\u03b3 in relation to the dataset D and the semivalue \u03c9. A fundamental property of the utility class U\u03bb,\u03b3 is that, for any u \u2208 U\u03bb,\u03b3, the data value \u03c6(zi;\u03c9, u) can be represented as a linear combination of \u03c6(zi; \u03c9, \u03bb) and \u03c6(zi; \u03c9, \u03b3), as stated formally in the following proposition.\nLet D = {zi}i\u2208[n] be a dataset and \u03c9 a semivalue weight vector. For any utility function u \u2208 U\u03bb,\u03b3 characterized by coefficients (co, c1, c2, do) \u2208 R4, the data value assigned to zi can be decomposed as\n$\\phi(z_i; \\omega, u) = \\frac{c_1}{d_0}\\phi(z_i; \\omega, \\lambda) + \\frac{c_2}{d_0}\\phi(z_i; \\omega, \\gamma).$"}, {"title": "4.3. Insights on ranking diversity for extreme cases of spatial signatures.", "content": "The spatial signature of D in P\u03c9 plays a crucial role in shaping the rankings induced by different utility functions. In particular, we analyze two extreme cases: (a) e is in general position , (b) e is collinear ."}, {"title": "4.4. Explaining experimental results from Section 3", "content": "From the spatial signatures of the WIND dataset we observe an almost collinear structure in P\u03c9shap, P\u03c9beta, and P\u03c9banz. This near-alignment indicates that the embedded points e predominantly lie along the principal eigenvector w of each spatial signature's covariance matrix. For analytical simplicity, we treat these signatures as fully collinear.\nBy , collinearity collapses the unit circle S1 into two ranking regions, separated by the antipodal points orthogonal to w. These antipodal points, defined as H\u03c9 = {\u0169 \u2208 S1 | \u0169 \u2022 w1 = 0}, partition S1 into hemispheres where \u0169 \u2022 w > 0 or \u0169 \u2022 w < 0. Utilities within the same hemisphere induce identical rankings, yielding perfect correlation. The figure shows that in our experiments: under \u03c9shap and \u03c9beta, the utilities \u016bacc, \u016brec, and \u016bam lie in the same hemisphere, explaining their strong correlation. Under \u03c9banz, \u016brec aligns exactly with the boundary point H\u03c9, satisfying \u016brec i = 0i which orthogonalizes it to the collinear direction w. This degeneracy equalizes utility values across all points, producing tied rankings and the observed weak correlation with \u016bacc and \u016bam. Figures. We observe that Banzhaf consistently produces nearly collinear spatial signatures across all datasets, leading to reduced ranking diversity as described in  In contrast, Shapley and Beta Shapley exhibit more dataset-dependent structures, sometimes aligning with Banzhaf but often showing greater dispersion, resulting in higher ranking variability.\nTo further illustrate ranking diversity in this setting, particularly transitions across ranking regions, we present Figure 4, which depicts the evolution of the Kendall rank correlation along a convex combination path of utility functions in S1. This path is specifically designed to cross one of the two antipodals points in H\u03c9 that define the ranking regions, encompassing the three key utilities we analyzed \u016bacc, \u016brec, uam. The resulting visualization captures how rank correlation fluctuates as the utility function moves along this path. To formally derive this figure, we rely on the following proposition."}, {"title": "5. Conclusion", "content": "This work advances the understanding of how utility functions impact semivalue-based data valuation. Our experiments show that the way a utility influences data value rankings is not solely dictated by its intrinsic properties but rather by its specific interaction with the dataset and the semivalue weights. We explain this interaction by introducing a geometric framework that clarifies empirical results.\nOur study focuses on a specific subclass of classification utility functions, limiting the generality of our theoretical findings. While this restriction enables a deeper analytical understanding, it remains unclear how our results extend to broader classes of utility functions. However, we have extended our experimental analysis to other utility functions beyond this subclass , and the empirical conclusions remain consistent. An extension of our theoretical framework to more general utility functions is a key direction for future research."}, {"title": "A. Additional definitions", "content": null}, {"title": "Definition A.1. (Spearman rank correlation).", "content": "The Spearman rank correlation coefficient \u03c1s measures the monotonic relationship between two sets of values {\u03c6}i\u2208[n] and {\u03c6}i\u2208[n] assigned to n elements. Let rg(\u03c6i) and rg(\u03c6i) denote the ranks of \u03c6i and \u03c6i within their respective sets. The Spearman coefficient \u03c1s(u, v) is defined as the Pearson correlation coefficient between the ranked values:\n$\\rho(u, v) = \\frac{Cov (rg(u), rg(\\phi'))}{\\sigma_{rg(\\phi)} \\sigma_{rg(\\phi')}},$"}, {"title": "Definition A.2. (Accuracy).", "content": "Accuracy measures the proportion of correctly classified instances and is defined as:\n$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN},$"}, {"title": "Definition A.3. (Recall).", "content": "Also known as true positive rate (TPR), recall quantifies the model's ability to correctly identify positive instances. It is given by:\n$Recall = \\frac{TP}{TP + FN},$"}, {"title": "Definition A.4. (Arithmetic mean).", "content": "$\\text{AM} = \\frac{1}{2} (TPR + TNR)$"}, {"title": "Definition A.5. (F1-score).", "content": "The F1-score provides a balance between precision and recall by computing their harmonic mean. It is defined as:\n$F_1 = 2 \\cdot \\frac{P \\cdot R}{P+R},$"}, {"title": "Definition A.6. (Negative log loss).", "content": "Negative log loss (NLL) evaluates the confidence of probabilistic predictions. Given a dataset of size n, where each sample has a true label yi \u2208 {0, 1} and a predicted probability pi \u2208 [0, 1] for class 1, NLL is defined as\n$\\text{NLL} = -\\frac{1}{n} \\sum_{i=1}^{n} [y_i \\log p_i + (1 - y_i) \\log(1 - p_i)].$"}, {"title": "B. Proofs & complementary analytical results", "content": null}, {"title": "B.1. Proofs of propositions", "content": null}, {"title": "B.2. Proofs of theorems", "content": null}, {"title": "B.3. Remark on the interpretation of low-rank correlations", "content": "Caution is essential when interpreting rank correlations from different utility functions. Poor rank correlations do not necessarily imply inconsistency in the assigned values for a data valuation application; rather, the values generated using different utility functions can still be equally useful for this task. This may occur when data points are clustered into groups of similar values for both utility functions, while the rankings within those groups differ depending on the utility function used. ."}, {"title": "C. Additional settings & experiments", "content": null}, {"title": "C.1. Datasets", "content": null}, {"title": "C.2. Systematic methodology for isolating utility effects in semivalue approximation", "content": "For a given dataset D, we propose a systematic methodology to isolate the effect of utility functions on data valuation rankings by controlling for extraneous variability. This is achieved through two key principles:"}, {"title": "C.2.1. FIXED LEARNING CONTEXT L", "content": "As outlined , a utility function u is defined as:\nu(S) = PERF(A(S), Dtest),"}, {"title": "C.2.2. ACCOUNTING FOR APPROXIMATION VARIABILITY IN SEMIVALUE COMPUTATION", "content": "The above reasoning holds if exact semivalues are computed. However, in practice, semivalues are estimated using permutation sampling techniques, which introduce stochastic variability. This variability can affect data valuation rankings, making it difficult to attribute observed differences solely to the choice of utility function.\nTo control for this, we propose aligned sampling in addition to fixing the learning context L. The fixed learning context ensures that all model training and evaluation factors remain constant, while aligned sampling guarantees that the same set of permutations is used across utility functions, eliminating variability introduced by the stochastic nature of semivalue approximations."}, {"title": "C.3. Learning algorithm A", "content": "This section details the fixed learning algorithm A used throughout all our experiments.\nSpecifically, we use logistic regression as the model, trained with the L-BFGS optimization algorithm. The loss function is set to Binary Cross-Entropy (BCE), and l2-regularization is applied with a \u03bb = 1.0 coefficient. Model weights are initialized from a fixed normal distribution, and the learning rate is fixed at 1.0."}, {"title": "C.4. Decision threshold calibration", "content": "We calibrate the decision threshold based on the proportion of positive labels in the training set. Instead of using a fixed threshold (e.g., 0.5), we adapt the threshold dynamically to match the expected class distribution. The calibration process works as follows: first, it computes the target proportion of positive labels in the training set. Then, it sorts the predicted probabilities in ascending order and selects the threshold at the position corresponding to the fraction of negative samples.\nThis ensures that the proportion of instances classified as positive matches the empirical distribution observed in the training set, leading to a more dataset-adaptive classification decision."}, {"title": "C.5. Additional experiments for Section 3", "content": null}, {"title": "C.5.1. EXTENDED EXPERIMENTS WITH ADDITIONAL CLASSIFICATION UTILITIES", "content": "To further explore the impact of utility function selection on data valuation rankings, we extend our experiments by replacing recall (REC) and arithmetic mean (AM) with F1-score (F1) and negative log-loss (NLL). While accuracy (ACC) remains a common baseline, these additional utility functions introduce distinct mathematical and statistical properties.\nThe F1-score is a linear fractional measure, as introduced in Section 4. While it shares similarities with linear performance measures such as accuracy, it remains fundamentally different, as accuracy corresponds to the special subset of this class for which d1 = d2 = 0. On the other hand, negative log-loss relies on probabilistic predictions rather than discrete classification decisions, introducing a continuous performance evaluation criterion that accounts for model confidence. We incorporate these utility functions to examine whether their different mathematical structures affect data valuation rankings."}, {"title": "C.5.2. SPEARMAN RANK CORRELATION RESULTS", "content": "We replicate all rank correlation computations using the Spearman rank correlation coefficient for completeness. The results presented in this section confirm that the trends observed with Kendall rank correlation persist under Spearman rank correlation."}, {"title": "C.5.3. INTERSECTION ANALYSIS", "content": "While rank correlations provide a measure of the consistency in the ordering of data values across different utility functions, they may not fully capture the grouping behavior that is often relevant in data valuation applications. As noted , a low-rank correlation between utility functions does not necessarily imply an inconsistent assigned importance of data points.\nTo better understand how changes in the utility function impact data point valuations, we conduct an intersection analysis. We examine the overlap in the bottom-n% of data points ranked by different utility functions to identify whether the least valuable points-according to various utility metrics-are consistently grouped together, even if their exact rankings differ."}, {"title": "C.6. Additional figures for Section 4", "content": "This section contains all figures illustrating the spatial signatures across different datasets and semivalues."}]}