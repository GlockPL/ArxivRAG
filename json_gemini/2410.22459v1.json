{"title": "Predicting Future Actions of Reinforcement Learning Agents", "authors": ["Stephen Chung", "Scott Niekum", "David Krueger"], "abstract": "As reinforcement learning agents become increasingly deployed in real-world scenarios, predicting future agent actions and events during deployment is important for facilitating better human-agent interaction and preventing catastrophic outcomes. This paper experimentally evaluates and compares the effectiveness of future action and event prediction for three types of RL agents: explicitly planning, implicitly planning, and non-planning. We employ two approaches: the inner state approach, which involves predicting based on the inner computations of the agents (e.g., plans or neuron activations), and a simulation-based approach, which involves unrolling the agent in a learned world model. Our results show that the plans of explicitly planning agents are significantly more informative for prediction than the neuron activations of the other types. Furthermore, using internal plans proves more robust to model quality compared to simulation-based approaches when predicting actions, while the results for event prediction are more mixed. These findings highlight the benefits of leveraging inner states and simulations to predict future agent actions and events, thereby improving interaction and safety in real-world deployments.", "sections": [{"title": "1 Introduction", "content": "As reinforcement learning (RL) becomes increasingly applied in the real world, ensuring the safety and reliability of RL agents is paramount. Recent advancements have shown that agents can exhibit complex behaviors, making it crucial to understand and anticipate their actions. This is especially important in scenarios where misaligned objectives [1] or unintended consequences could result in suboptimal or even harmful outcomes. For instance, consider an autonomous vehicle controlled by an RL agent that might unpredictably decide to run a red light to optimize travel time. Predicting this behavior in advance would enable timely intervention to prevent a potentially dangerous situation.\nThis capability is also beneficial in scenarios that require effective collaboration and information exchange among multiple agents [2-4]. For example, if passengers and other drivers know whether a self-driving car will turn left or right, it becomes much easier and safer to navigate the roads. Thus, the ability to accurately predict an agent's future behavior can help reduce risks and ensure smooth interaction between agents and humans in real-world situations.\nIn this paper, we explore the task of predicting future actions and events when deploying a trained agent, such as whether an agent will turn left in five seconds. The distribution of future actions and events cannot be computed directly, even with access to the policy, because the future states are unknown. We consider two methods for predicting future actions and events: the inner state approach and the simulation-based approach. We apply these approaches to agents trained with various RL algorithms to assess their predictability\u00b9."}, {"title": "2 Background and Notation", "content": "We consider a Markov Decision Process (MDP) defined by a tuple $(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma, d_0)$, where $\\mathcal{S}$ is a set of states, $\\mathcal{A}$ is a finite set of actions, $P : \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0,1]$ is a transition function representing the dynamics of the environment, $R : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ is a reward function, $\\gamma \\in [0, 1]$ is a discount factor, and $d_0 : \\mathcal{S} \\rightarrow [0, 1]$ is an initial state distribution. Denoting the state, action, and reward at time $t$ by $S_t, A_t$, and $R_t$ respectively, $P(s, a, s') = Pr(S_{t+1} = s'|S_t = s, A_t = a)$, $R(s,a) = \\mathbb{E}[R_t|S_t = s, A_t = a]$, and $d_0(s) = Pr(S_0 = s)$, where $P$ and $d_0$ are valid probability mass functions. An episode is a sequence of $(S_t, A_t, R_t)$, starting from $t = 0$ and continuing until reaching the terminal state, a special state where the environment ends. Letting $G_t = \\sum_{k=t}^\\infty \\gamma^{k-t}R_k$ denote the infinite-horizon discounted return accrued after acting at time $t$, an RL algorithm attempts to find, or approximate, a policy $\\pi : \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1]$, such that for any time $t > 0$, selecting actions according to $\\pi(s, a) = Pr(A_t = a|S_t = s)$ maximizes the expected return $\\mathbb{E}[G_t|\\pi]$.\nIn this paper, planning refers to the process of interacting with an environment simulator or a world model to inform the selection of subsequent actions. Here, a world model is a learned and approximated version of the environment. We classify an agent, which is defined by its policy, into one of the following three categories based on the RL algorithm by which it is trained:\nExplicit Planning Agents. In explicit planning agents, an environment simulator or a world model is used explicitly for planning. We consider two explicit planning agents in this paper, MuZero [5] and Thinker [6], given their superior ability in planning domains. MuZero is a state-of-the-art model-based RL algorithm that combines a learned model with Monte Carlo Tree Search (MCTS) [7, 8] for planning. During planning, MuZero uses the learned model to simulate future trajectories and performs MCTS to select the best action based on the predicted rewards and values. Thinker is a recently proposed approach that enables RL agents to autonomously interact with and use a learned world model to perform planning. The key idea of Thinker is to augment the environment with a world model and introduce new actions designed for interacting with the world model. MuZero represents a handcrafted planning approach, while Thinker represents a learned planning approach.\nImplicit Planning Agents. In implicit planning agents, there is no learned world model nor an explicit planning algorithm, yet these agents still exhibit planning-like behavior. A notable example is the Deep Repeated ConvLSTM (DRC) [9], which excels in planning domains. DRC agents are trained"}, {"title": "3 Problem Statement", "content": "Given a fixed policy $\\pi$, we aim to estimate the distribution of a function of the future trajectory. For example, we may want to estimate the probability of an agent entering a particular state or performing a specific action within a certain horizon. Mathematically, let $H_t = (S_t, A_t, R_t)$ denote the transition at step $t$, and let $H_{t:T} = \\{H_t, H_{t+1},..., H_T\\}$ denote the future trajectory from step $t$ to the last step $T$. Let $\\mathcal{H}$ denote the set of all possible future trajectories. We are interested in estimating the distribution of a random variable $f(H_{t:T})$ conditioned on the current state and action:\n$P(f(H_{t:T}) | S_t, A_t),$\\nwhere $f : \\mathcal{H} \\rightarrow \\mathbb{R}^m$ is a function specifying the variables to be predicted.\nThis paper focuses on predicting two particular types of information. The first type is future action prediction, where we want to predict the action of the agent in $L$ steps, i.e., $f(H_{t:T}) = (A_{t+1}, A_{t+2},..., A_{t+L})$ and the problem becomes estimating:\n$P(A_{t+1}, A_{t+2},..., A_{t+L} | S_t, A_t).$\nAn example of action prediction is whether an autonomous vehicle is going to turn left or right in the next minute. The second type is future event prediction, where we want to estimate the probability of a binary indicator $g: (\\mathcal{S}, \\mathcal{A}) \\rightarrow \\{0, 1\\}$ being active within $L$ steps, and the problem becomes estimating:\n$\\mathbb{P}\\left(\\bigcap_{k=1}^L \\{g(S_{t+k}, A_{t+k}) = 1\\} | S_t, A_t\\right)$\nwhich is equivalent to the case $f (H_{t:T}) = \\max\\{g(S_{t+k}, A_{t+k})\\}_{k=1,...,L}$. In other words, (3) is the probability of the event defined by $g$ occurring within $L$ steps. An example of event prediction is predicting whether an autonomous vehicle will run a red light within a minute.\nEvent prediction shares resemblance to the generalized value function [12], where $f(H_{t:T}) = \\sum_{k=0}^L \\gamma^k g(S_{t+k}, A_{t+k})$ and we estimate its expectation $\\mathbb{E}[f(H_{t:T}) | S_t, A_t]$. When $g$ is a binary indicator, this expectation is equivalent to the discounted sum of the probabilities of the event defined by $g$. This is arguably harder to interpret than (3); for example, it can be larger than 1 and thus is not a valid probability.\nTo learn these distributions, we assume access to some transitions generated by the policy as training data. The transitions may come from multiple episodes. In the case of future event prediction, we assume that $g(S_t, A_t)$ is also known for each transition. We further assume that the policy $\\pi$ and the inner computation for each action $A_t$ within $\\pi$ is known.\nIn this work, we assume that $\\pi$ is already a trained policy and is fixed. This is the case where the agent is already deployed, and transitions during the deployment are collected. In cases where the training and deployment environments are similar, we can also use the transitions when training the agent as training data for predicting future actions and events, but this is left for future work."}, {"title": "4 Methods", "content": "Since we already have the state-action $(S_t, A_t)$ and the target output $f (H_{t:T})$ in the training data, we can treat the problem as a supervised learning task.2 In particular, We can train a neural network that takes the state-action pair as input to predict $f (H_{t:T})$. This network is trained using gradient descent on cross-entropy loss.\nBesides the state-action, there can be additional information that may help the prediction. For example, the inner computation of the policy $\\pi$ may contain plans that are informative of the agent's future actions, especially in the case of an explicit planning agent. We refer to this information that is available before observing the next state $S_{t+1}$ as auxiliary information and denote it as $I_t$. We will consider two types of auxiliary information: inner states and simulations."}, {"title": "4.1 Inner State Approach", "content": "In the inner state approach, we consider choosing the agent's inner state as the auxiliary information. Here, the inner state refers to all the intermediate computations required to compute the action $A_t$. As inner states are different across different types of agents and may not all be useful, we consider the following inner state to be included in the auxiliary information:\n1.  MuZero: Since MuZero uses MCTS to search in a world model and selects action with the largest visit count, we select the most visited rollout as the auxiliary information. Rollouts here refer to the simulation of the world model and are composed of a sequence of transitions $(S_{t+1}, A_{t+1}, R_{t+1})_{1<l<L}$. It should be noted that the agent may not necessarily select the action sequence of this rollout, as the MCTS is performed at every step, and the search result at the next step may yield different actions. We do not use all rollouts, as MCTS usually requires many rollouts.\n2.  Thinker: We select all rollouts and tree representations during planning as the auxiliary information. We do not choose a particular rollout because, unlike MCTS, in Thinker, it is generally unknown which action the agent will select at the end, and Thinker usually requires only a few rollouts.\n3.  DRC: We select the hidden states of the convolutional-LSTM at every internal tick as the inner state, as it was proposed that the hidden state contains plans to guide future actions [9].\n4.  IMPALA: We select the final layer of the convolutional network as the inner state, as it is neither too primitive which may only be processing the state, nor too refined which may only contain information for computing the current action and values."}, {"title": "4.2 Simulation-based Approach", "content": "As an alternative to the inner state approach, we can train a world model concurrently with the agent. Once trained, we can simulate the agent in this world model using the trained policy $\\pi$ to generate rollouts. These rollouts can then be utilized as auxiliary information for the predictor. In this paper, we consider using the world model proposed in Thinker [6], which is an RNN that takes the current state and action sequence as inputs and predicts future states, rewards, and other relevant information. For both implicit and non-planning agents, the world model is trained in parallel with the agents but is not used during their selection of actions. Instead, the world model is solely employed to generate rollouts as auxiliary information for the predictors.\nIf the learned world model closely resembles the real environment, we expect these rollouts to yield valuable information for predicting future actions and events, as the agent's behavior in the world model should be similar to its behavior in the actual environment. In the ideal case where the world model perfectly matches the true environment dynamics, we could compute the exact future action and event distribution without needing any prediction. However, we do not consider this scenario in the paper, as this assumption is impractical for most settings."}, {"title": "5 Experiments", "content": "We conduct three sets of experiments to evaluate the effectiveness and robustness of the discussed approaches. First, we apply the inner state approach to predict future actions and events. We compare it to the case where only state-action information is provided to the predictor so as to evaluate the benefits of the proposed inner state in the prediction. Second, we apply the simulation-based approach and compare it with the inner state approach to evaluate the benefits of these two different types of auxiliary information. Finally, we consider a model ablation setting, where we deliberately make the world model inaccurate to see how the different approaches perform under such conditions.\nWe consider the Sokoban environment, where the goal is to push all four boxes into the four red-bordered target spaces as illustrated in Fig 1. We choose this environment because (i) a wide range of levels in Sokoban make action and event prediction challenging, and we can evaluate the predictors on unseen levels to evaluate their generalization capability; (ii) there are multiple ways of solving a level; (iii) Sokoban is a planning-based domain, so it may be closer to situations where we want to discern plans of agents in more complex settings.\nWe choose the prediction horizon $L$ to be 5 in all experiments. For action prediction, we try to predict the next five actions $A_{t+1}, A_{t+2}, ..., A_{t+5}$. For event prediction, we randomly select an empty tile in the level and paint it blue. That blue tile acts as an empty tile to the agent and serves no special function. We define the event $g$ that is to be predicted as the case where the agent stands on the blue location. In other words, we try to predict whether the agent will go to that blue location within $L$ steps.\nWe train four different agents using MuZero, Thinker, DRC, and IMPALA. All agents are trained for 25 million transitions. To ensure that the result would not be affected by the particular choice of the world model, we uniformly employ the world model architecture proposed in Thinker, as the world model in Thinker predicts the raw state and is suitable for both MuZero and the simulation-based approach. We train a separate world model for each agent. For DRC and IMPALA, the world model is not needed for the policy and will only be used in the predictors in the simulation-based approach.\nAfter training the agents, we generate 50k transitions, where part or all of it will be used as the training data for the predictors. We evaluate the performance of predictors with varying training data sizes: 1k, 2k, 5k, 10k, 20k, 50k. We also generate 10k transitions as a testing dataset. For simplicity, we use greedy policies, where we select the action with the largest probability instead of sampling. The predictor uses a convolutional network to process all image information, including the current state and states in rollouts (if they exist). The encoded current state, along with other auxiliary information such as encoded states, rewards, and actions in rollouts (if they exist), will be passed to a"}, {"title": "5.1 Inner State Approach", "content": "Figure 2 presents the final accuracy of action prediction and the F1 score of event prediction using the inner state approach. The error bars represent the standard deviation across three independently trained predictors. The accuracy here refers to the percentage of correctly predicting all the next five actions, with no credits awarded if any action is predicted incorrectly. The graph also shows the performance of the predictors when they only receive the current state $S_t$ and action $A_t$ as inputs, as indicated by 'baseline'. Several observations can be made.\nFirst, when access to the plans is available, the prediction accuracy for both action and event is significantly higher for explicit planning agents. For example, with 50k training data, the action prediction accuracy of the MuZero agent increases from 40% to 87% when given access to the plans. Agents using handcrafted planning algorithms (MuZero) or learned planning algorithms (Thinker) show similar performance gains. This is perhaps not surprising, as these explicit planning agents tend to follow the plans either by construction or by learning, and the explicit nature of planning facilitates easy interpretation of the plans.\nSecond, the case for implicit planning agents (DRC) and non-planning agents (IMPALA) is more nuanced. For action prediction accuracy, both receive a moderate improvement from accessing the hidden state. There are two possible explanations: (i) plans of the agents are stored in the learned representations that are informative of future actions; (ii) the hidden states or hidden layers contain a latent representation that is easier to learn from, compared to the raw states. To discern between the two cases, interpreting the nature and the underlying circuit of the inner states is required. We leave this to future work as interpretability is outside the scope of this paper.\nThird, in contrast to action prediction, the inner state does not improve event prediction for DRC and IMPALA, likely because the blue location in the environment does not affect the reward, and the agent may ignore it in its representation. This suggests an advantage of explicit planning agents, as in explicit planning agents, we explicitly train the world model and can train it to attend not just to the reward-relevant features but to all features (or features we deem useful) in the environment. This may be important for cases where the reward function is not well designed, leading to the agent ignoring certain features that are, in fact, important to us."}, {"title": "5.2 Simulation-based Approach", "content": "We now consider applying the simulation-based approach to both implicit planning (DRC) and non-planning agents (IMPALA). We unroll the world model for $L = 5$ steps using the current policy and input this rollout as auxiliary information to the predictors. We can use a single rollout, as both the policy and the chosen world model are deterministic, so all rollouts will be the same."}, {"title": "5.3 World Model Ablation", "content": "Learning an accurate world model may not be feasible in some settings, such as auto-driving in the real world. An inaccurate world model will affect the plan quality of explicit planning agents, rendering the plan less informative in the inner state approach. An inaccurate world model will also affect the quality of rollouts in the simulation-based approach, leading to inconsistent behaviour between rollouts and real environments. As such, it is important to understand how the inner state approach and simulation-based approach differ when the learned world model is not accurate.\nTo investigate this, we designed three different settings where learning an accurate world model is challenging. In the first setting, we use a world model with a much smaller size, making it more prone to errors. In the second setting, we randomly replace the agent's action with a no-operation 25% of the time, introducing stochastic dynamics into the environment. However, since the world model we use is deterministic, it cannot account for such stochastic transitions and will yield errors. In the third setting, we consider a partially-observable Markov decision process (POMDP) case, where we randomly display the character at a position within one step of the true character location. As the world model we use only observes the current state, this will lead to uncertainty over both the true character location and the displayed character location. We repeat the above experiments in these three different settings.\nFigure 4 shows the change in the final accuracy of action prediction and the F1 score of event prediction for the model ablation settings compared to the default setting. We observe that in terms of action prediction, the accuracy generally drops less in the inner state approach of explicit planning agents than in the simulation-based approach of the two other agents. This is likely because planning"}, {"title": "6 Related Works", "content": "Safe RL: Our work is related to safe RL, where we try to train an agent to maximize rewards while satisfying safety constraints during the learning and/or deployment processes [15]. A wide variety of methods have been proposed in safe RL, such as shielding, where one manually prevents actions that violate certain constraints from being executed [16], and Constrained Policy Optimization (CPO), which performs policy updates while enforcing constraints throughout training [17]. Many works in safe RL are based on correcting the action when deemed unsafe. Dalal et al. [18] fit a linear model to predict the violation of constraint functions and use it to correct the policy; Cheng et al. [19] project the learned policy to safe policy based on the barrier function; Thananjeyan et al. [20] guide the agent back to learned recovery zones when it is predicted that the state-action pair will lead to unsafe regions; Thomas et al. [21] use world models to predict unsafe trajectories and change the rewards to penalize safety violations.\nIn contrast to the works in safe RL, we are solely interested in predicting future actions and events of trained agents. The actions or events do not necessarily need to be unsafe. In the case of unsafe action or event prediction, our work allows for preemptive interruption of the deployed agent, which can be used as a last resort in addition to the above safety RL works.\nOpponent Modelling in Multi-agent Setting: In a multi-agent setting, modeling the opponent's behavior may be beneficial in both competitive and cooperative scenarios. He et al. [22] use the opponent's inner state to better predict Q-values in a multi-agent setting. Foerster et al. [23] update an agent's policy while accounting for its effects on other agents, and Raileanu et al. [24] predict the"}, {"title": "7 Conclusion", "content": "In this paper, we investigated the predictability of future actions and events for different types of RL agents. We proposed and evaluated two approaches for prediction: the inner state approach and the simulation-based approach. The simulation-based approach performs well with an accurate world model but is less robust when the world model quality is compromised. Conversely, the performance of the inner state approach depends on the type of inner states and the agents. Internal plans of explicit planning agents are particularly useful compared to other types of inner states. These findings highlight the importance of leveraging auxiliary information to predict future actions and events. Enhanced predictability could lead to more reliable and safer deployment of RL agents in critical real-world applications such as autonomous driving, robotics, and healthcare, where understanding and anticipating agent behavior is important for safety and effective human-agent interaction.\nFuture research directions include extending our analysis to more diverse environments and RL algorithms, exploring safety mechanisms to modify agent behavior based on action prediction, and developing RL algorithms that are both predictable and high-performing."}, {"title": "Limitation", "content": "The paper only evaluates the proposed approaches in a limited set of environments. Including additional environments would provide a better understanding of agent predictability, but this requires finding or designing new benchmark environments with diverse states. Additionally, the paper focuses on only four different RL algorithms. Evaluating a broader range of RL algorithms could allow for better comparisons of their predictability."}, {"title": "Broader Impact Statement", "content": "This work involves predicting the actions and events of trained agents during deployment. It is important to consider the risk of false alarms, where the predictor forecasts that an agent is going to perform an unsafe action, but in fact, the agent would not be doing it. This may lead to improper responses (such as shutting down the agent) that are not warranted."}, {"title": "A Agent Details", "content": "In this section, we describe the details of how we trained the four types of agents discussed in the paper. Most of them follow the procedure outlined in the original paper:\n1.  MuZero: We use the same agent configuration as in the original paper, except for the world model, where we adopt the architecture and training method proposed in Thinker. This is to ensure that the results are not affected by the choice of the world model. We conducted 100 simulations for each search\u00b3.\n2.  Thinker: We use the same default agent as described in the original paper.\n3.  DRC: We use the DRC(3,3) described in the original paper.\n4.  IMPALA: We use the large architecture but omit the LSTM component described in the original paper.\nAll the hyperparameters are consistent with those in the original paper, and the agents are all trained using 25 million transitions. For each RL algorithm in the default model case, we train three separate agents with different seeds. For the model ablation case, we train only one agent due to computational cost.\nWorld Model: The world model utilizes the dual network architecture proposed in Thinker, as it enables the prediction of raw states, values, and policies, allowing its use in both simulation-based approaches and planning in MuZero and Thinker. We also found that using the dual network results in better performance in the environment than the original network proposed in MuZero, likely due to the addition of learning signals from predicting the raw state. In the small model ablation case, we reduced all channel sizes in the RNN block from the default 128 to 32.\nWe follow the training procedure discussed in Thinker to train the world model, except that we added an additional loss based on L2-distance between the predicted raw state and the true raw state. This ensures that the world model focuses on all features of the raw states, not just those relevant to rewards. Consequently, non-reward-affecting features, such as the blue location, can still be encoded and predicted by the world model. We found that the addition of this loss does not negatively impact the agent's performance in the environment. It should be noted that we do not assume we have knowledge about the event g when training the world model or the agent. We train a separate world model following the same training procedure for each agent.\nFigure 5 shows examples of model outputs for both the default setting and the model ablation setting. In the default case, the model predicts the states accurately, albeit with slight blurring. In the small model case, the agent erroneously pushes the box across the wall, which should not be allowed, and the blue location is missing, likely due to the model's limited capacity preventing it from fitting into the representation. In the stochastic case, the agent gradually fades due to the uncertainty of its position. Lastly, in the POMDP case, the agent is completely missing, attributable to the difficulty of ascertaining the agent's true position. These three model ablation cases thus showcase the different failure modes of the model.\nLearning Curve: The learning curves of the agents in both the default setting and the model ablation setting can be found in Figure 6. We observe that in the default setting, the Thinker agent performs the best, with results closely replicating those of the original paper. The MuZero agent here outperforms the MuZero agent in the Thinker's paper due to the use of a dual network as the world model. In the small model setting, the performance of the DRC and baseline agents is similar to that in the default case, as the world model is not used in the policy. In the POMDP case, all agents perform poorly, likely because they cannot be certain of their own location, making the problem too challenging."}, {"title": "B Predictor Details", "content": "Predictor architecture: The raw states and predicted states (if they exist) are processed by a separate convolutional encoder with the same architecture. The encoder shares the same architecture as follows:"}, {"title": "D Ablation on Inner State Approach", "content": "We consider alternative choices for inner states and repeat the experiment shown in Figure 2. The following inner states are considered:\n1.  MuZero: We considered using the top 3 rollouts ranked by visit counts against only the top rollouts (the default case).\n2.  DRC: We considered using the hidden state at all ticks (the default case) against only the hidden state at the last tick.\n3.  IMPALA: We considered using the output of all three residual blocks against only the last residual block (the default case).\nThe results can be found in Fig 8. We observe that the results are similar with different chosen inner states, except that (i) using top 3 rollouts in MuZero leads to slightly lower event prediction accuracy, possibly because the top rollout is sufficient to make the prediction, and (ii) using all residual blocks in IMPALA gives slightly better performance in event prediction, likely because lower residual blocks still encode the blue location that is helpful for predicting the chosen event."}]}