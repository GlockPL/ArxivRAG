{"title": "How GPT LEARNS LAYER BY LAYER", "authors": ["Jason Du", "Kelly Hong", "Alishba Imran", "Erfan Jahanparast", "Mehdi Khfifi", "Kaichun Qiao"], "abstract": "Large Language Models (LLMs) excel at tasks like language processing, strategy\ngames, and reasoning but struggle to build generalizable internal representations\nessential for adaptive decision-making in agents. For agents to effectively nav-\nigate complex environments, they must construct reliable world models. While\nLLMs perform well on specific benchmarks, they often fail to generalize, leading\nto brittle representations that limit their real-world effectiveness. Understanding\nhow LLMs build internal world models is key to developing agents capable of\nconsistent, adaptive behavior across tasks. We analyze OthelloGPT, a GPT-based\nmodel trained on Othello gameplay, as a controlled testbed for studying represen-\ntation learning. Despite being trained solely on next-token prediction with ran-\ndom valid moves, OthelloGPT shows meaningful layer-wise progression in under-\nstanding board state and gameplay. Early layers capture static attributes like board\nedges, while deeper layers reflect dynamic tile changes. To interpret these repre-\nsentations, we compare Sparse Autoencoders (SAEs) with linear probes, finding\nthat SAEs offer more robust, disentangled insights into compositional features,\nwhereas linear probes mainly detect features useful for classification. We use\nSAEs to decode features related to tile color and tile stability, a previously un-\nexamined feature that reflects complex gameplay concepts like board control and\nlong-term planning. We study the progression of linear probe accuracy and tile\ncolor using both SAE's and linear probes to compare their effectiveness at captur-\ning what the model is learning. Although we begin with a smaller language model,\nOthelloGPT, this study establishes a framework for understanding the internal rep-\nresentations learned by GPT models, transformers, and LLMs more broadly. Our\ncode is publicly available: GitHub Repository.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) exhibit remarkable capabilities across tasks like natural language\nprocessing, strategic games, and reasoning. However, their demonstrated proficiency in performing\ncomplex reasoning tasks raises an open question: Do LLMs construct accurate internal represen-\ntations of the structures, rules, and patterns that underlie the data they are trained on, or are these\nrepresentations incomplete and brittle? A recent survey Chang et al. (2024) on LLM evaluation high-\nlights the need for multidimensional assessment, emphasizing that current models often succeed in\ntask-specific metrics but lack deeper generalization. Similarly, studies on co-temporal reasoning Su\net al. (2024) reveal significant gaps in how LLMs handle concurrent or overlapping temporal events,\nfurther showing the inconsistency of their internal representations. For example, the Othello-GPT\nmodel Li et al. (2023) predicts legal moves and reconstructs game board states from sequence data\nwhich shows how LLMs can infer hidden states purely from sequential patterns. However, studies\nby Toshniwal et al. (2022) and Vafa et al. (2024) demonstrate that such models often fail to recover\nthe full compositional structure of their domains. This limitation is particularly evident in navigation\nand deterministic finite automata tasks, where models exhibit brittleness under dynamic or adver-\nsarial conditions. In games like Othello or navigation tasks, while these models excel in next-token\nprediction, they fail to construct coherent and generalizable internal representations, reducing their\nreliability in downstream applications. The key motivating question becomes: How do GPT models\nconstruct their world models during training? Beyond flawed representations, LLMs often exhibit"}, {"title": "1.1 IMPACT ON AGENTS", "content": "LLM-based agents depend heavily on their internal world models to infer the latent structures nec-\nessary for tasks like compositional reasoning and long-term planning. For instance, Rothkopf et al.\n(2024) highlights that agents require consistent internal representations to maintain procedural ad-\nherence and interpretability over extended interactions. However, studies like Lopez Latouche et al.\n(2023) demonstrate that LLMs often fail in long-term planning, leading to behavior that becomes\ninconsistent with prior states, as observed in video game character dialogues where maintaining\nstyle and narrative coherence is crucial. Similarly, Binz & Schulz (2023) show that LLMs strug-\ngle with compositional reasoning, failing to synthesize structured responses in complex scenarios\nlike causal inference tasks where even minor perturbations cause substantial deviations from correct\nor human-like reasoning. These limitations emphasize the importance of coherent and generaliz-\nable world models that go beyond next-token prediction accuracy, which remains a primary metric\nin most evaluations. Agent interpretability can be examined in multiple ways: 1. Point-in-time\ninterpretability: Focuses on how individual tokens in a prompt or response influence an agent's im-\nmediate decisions. 2. Procedural interpretability: Analyze how sequences of inputs shape an agent's\nlong-term behavior. 3. Mechanistic interpretability: Investigates the underlying mechanisms and\nrepresentations within the model that relate to agents' outputs. In our work, we focus on mechanistic\ninterpretability as a fundamental approach to understanding how internal model structures translate\ninto observable agent behaviors. Othello serves as an excellent testbed for studying neural networks\ndue to its compositional reasoning requirements and layered decision-making processes. By using\nOthello-GPT, we investigate the features the model learns layer-by-layer to understand how Trans-\nformers Vaswani (2017) and GPT-like models Brown (2020) construct their implicit world models.\nWhile this research aims to expand to LLMs, SLMs like Othello-GPT are valid test beds due to\ntheir similar architecture and lower computational cost. By studying the model behavior and in-\nterpretability techniques evaluation like linear probes and SAEs on these smaller models, we can\ndevise frameworks transferable to larger models."}, {"title": "2 RELATED WORKS", "content": ""}, {"title": "2.1 SPARSE AUTOENCODERS (SAES)", "content": "Sparse Autoencoders (SAEs) offer a way forward by disentangling learned features into sparse,\ninterpretable bases Bricken et al. (2023). SAEs transform high-dimensional neural activations into\nsparse, interpretable representations by minimizing reconstruction error. The objective function for\nan SAE is:\n$L(x,\\hat{x}) = ||x - \\hat{x} ||^2 + \\lambda ||h||_1$\n(1)"}, {"title": "2.2 LINEAR PROBES", "content": "Linear probes are a widely used tool for analyzing neural networks(Alain (2016), Tenney (2019)\nand Belinkov (2021)). Linear probes effectively measure feature accessibility but do not disentangle\noverlapping or compositional features. Prior studies on OthelloGPT (Hazineh et al. (2023)) revealed\nthat deeper layers encode increasingly accurate board representations as they are linearly accessible\nfor tasks like board state classification. However, these studies did not compare the interpretability\nof features learned via linear probes with those disentangled by SAEs.\nThe original Othello-GPT model was trained to predict legal moves from sequential game data and\ncould reconstruct board states using nonlinear probes (Li et al. (2023)). Neel Nanda later showed\nthat a linear probe could identify a \"world representation\" of the board state, shifting from traditional\nrepresentations (e.g., \u201cblack's turn\u201d vs. \u201cwhite's turn\u201d) to a model-derived interpretation (\u201cmy turn\u201d\nvs. \u201ctheir turn\u201d) (Nanda (2024)). While effective at the classification task, linear probes do not\ndisentangle features or capture compositional patterns, limiting their interpretability."}, {"title": "2.3 LAYER BY LAYER ANALYSIS", "content": "Alain (2016) introduced the use of linear classifier probes to measure the representations encoded at\neach layer of a neural network. This analysis revealed that deeper layers progressively improve the\nlinear separability of features as representations become more abstract and aligned with the model's\ntask objective. Subsequent research on GPT models (Tenney (2019), Belinkov (2021)) extended\nthese insights to transformers. For instance, studies on BERT showed that earlier layers encode\nsyntactic information, while later layers capture semantic roles like coreference and entity types.\nThis revealed a progression of linguistic abstraction across layers. However, probing classifiers,\nparticularly linear probes, often struggle to disentangle overlapping or compositional features within\nthe model's activations.\nOur work extends Bengio's findings by comparing the features detected by SAEs to those identified\nby linear probes, revealing how disentangled representations evolve with depth. Inspired by studies\non concept depth (Rothkopf et al. (2024), Jin et al. (2024)), we explore how abstract and composi-\ntional features emerge layer by layer. By bridging interpretability with functionality, our approach\nprovides a nuanced perspective on how OthelloGPT's and generally LLMs internal world model\nsupports strategic decision-making."}, {"title": "3 METHOD", "content": ""}, {"title": "3.1 OTHELLOGPT", "content": "We trained an 8-layer, decoder-only transformer model, Othello-GPT (as shown in Figure 1), with\nan 8-head attention mechanism and a residual stream dimension d = 512, just like Aizi (2024).\nThe model predicts the next token in random Othello game transcripts, treating games as sequences\ntokenized with a 66-word vocabulary (representing 64 board tiles, with a padding token and an\nend-of-sequence token). The OthelloGPT was trained on the Synthetic dataset mentioned in Li\net al. (2023). It contains out-of-distribution steps that are legal but sub-optimal, which conveys that\nour OthelloGPT training process has no long-term strategy involved. Formally, let the sequence\nof tokens for a game be x = (x1,x2,...,xT), where xt is the token at timestep t. The model is"}, {"title": "3.2 LINEAR PROBE", "content": "We trained linear probes across all layers of the 8-layer. Linear probes aim to extract specific se-\nmantic features or predict attributes from the model's hidden states. Let the hidden states at layer\nl of a decoder-only Transformer be $H^{(l)} = [h^{(l)}_1,h^{(l)}_2,..., h^{(l)}_n] \\in \\mathbb{R}^{n \\times d}$, where $h_i^{(l)} \\in \\mathbb{R}^d$ is the\nhidden representation for the i-th token, and n is the sequence length. The linear probe is defined as\na classifier: $\\hat{y}_\\phi(h) = W^Th$, where $\\phi = W \\in \\mathbb{R}^{d \\times k}$ are the learnable parameters of the probe, and\nk is the number of target classes. In our case, k is 64 (similar to Nanda (2024)), which represents\nall the locations on the board. We also have trained three different probes under three modes re-\nspectively: empty, which means there are no game pieces on that location on the board; own, which\nmeans the game piece is my color on the board on that location; and enemy, which means the game\npiece is the opponent's color on the board on that location. For a given hidden state h, the predicted\nprobability distribution over the classes is: $\\hat{y} = softmax(W^Th)$, and the predicted class is the one\nwith the highest probability: $\\hat{y} = arg max_j \\hat{y}_j$. The linear probe minimizes the cross-entropy loss\nfunction as follows:\n$L(\\phi) = \\frac{1}{N} \\sum_{i=1}^{N} l_{CE}(\\hat{y}_\\phi(h_i), y_i)$\n(2)\nwhere N is the number of samples, $h_i = f_\\theta(x_i)$ is the frozen hidden representation extracted by\nthe Transformer $f_\\theta$, $y_i$ is the corresponding target label, and the cross-entropy loss is defined as\n$l_{CE}(\\hat{y}, y) = - log \\hat{y}_y$, where $\\hat{y}_y$ is the predicted probability for the correct class y. By training linear\nprobes on hidden states from all layers, we evaluate the layer-wise encoding of semantic information\nin OthelloGPT."}, {"title": "3.3 TILE COLOR", "content": "We devised two methods, using linear probe and SAE, to analyze tile color and discover the robust-\nness of features learned by OthelloGPT."}, {"title": "3.3.1 LINEAR PROBE AND COSINE SIMILARITY", "content": "To analyze the internal behavior of GPT, we employ the cosine similarity method for network anal-\nab\nysis. Given two distinct feature a and b, the cosine similarity function is: similarity(a,b) = . We focus on analyzing individual neurons in every layer. With a pre-trained linear probe\n||a||||b| \nfor three modes, we calculate the cosine similarity between the MLP neurons and the probe, assess-\ning each neuron's contribution to classification for specific tiles. We present findings primarily from\nthe Encoding layer after observing similar results on the Encoding and Projection layers, where we\ncompute the cosine similarity between MLP parameters and the layer-specific linear probe to quan-\ntify each neuron's contribution to encoding tile color. Neurons exceeding a similarity threshold of\n0.2 are counted for each tile, focusing on the \"my color\" probe."}, {"title": "3.3.2 SAE", "content": "To ensure the robustness of the features learned by Othello-GPT, we compared the top-performing\nfeatures across 10 random initialization seeds of the SAEs. We extract sparse features from the\nresidual stream embedding with shape R512 after feed forward in each transformer block. This vali-\ndation ensures that the model is learning tile colors rather than artifacts from specific random states.\nFor each seed, we identify the top 50 features with AUROC > 0.7, which measure a feature's ability\nto classify tile states (empty, player's piece, or opponent's piece) by balancing the true positive and\nfalse positive rates. A higher AUROC value indicates stronger discriminative power. Aggregating\nresults across all seeds, we tally the frequency of each board position appearing among these top\nfeatures, as visualized in Figure 3."}, {"title": "3.4 TILE STABILITY", "content": "A tile is defined as stable if it cannot be flipped for the remainder of the game. For instance, corner\ntiles are inherently stable once placed. Stable tiles include corner tiles, edge tiles anchored to stable\ntiles, and interior tiles surrounded by stable tiles. Given a set of 104,000 board states (2,000 games\nx 52 board states per game), we computed binary stability maps for each state. Each board state\nconsisted of 64 tiles, each encoded by its color (0 for empty, 1 for black, 2 for white). Occupied tiles\n(indicated by 1 or 2), were marked as stable if it was 1) a corner tile ((0,0), (0,7), (7,0), or (7,7)) an\nedge tile directly adjacent to a stable tile or 3) an interior tile surrounded by 8 stable tiles (including\ntop, bottom, left, right, and diagonally adjacent neighbors). This process yielded a stability map for\neach board state, where each tile was assigned a binary value: 1 if stable and 0 otherwise.\nStability Feature Activations. We analyzed feature activations across all 8 layers of our Othello-\nGPT model with a binary classification framework. Each board state was paired with its correspond-\ning stability map to evaluate whether individual features reliably predicted tile stability. Features\nwere considered active if their activation strength was > 0. For each feature and tile, we computed:\ntrue positives (active feature and stable tile), false positives (active feature, non-stable tile), true neg-\natives (inactive feature, non-stable tile), and false negatives (inactive feature, stable tile). From these\nvalues, we calculated the F1-score and AUROC using standard metrics.\nFeature Analysis using AUROC Thresholds. To determine whether dominant features consis-\ntently encode stability across layers, we analyze feature activations with AUROC scores > 0.8. We\ndid the following analysis for each layer:\n1. Tile-level: For each tile, we computed the frequency of feature activations with AUROC scores\nexceeding the threshold.\n2. Feature-level: For each feature, we computed the frequency of its activations exceeding the\nthreshold for that layer without regard to specific tile positions. We repeated this analysis for 2\ndifferent seeds to confirm our results."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 COMPARING SAES VS LINEAR PROBES", "content": "We show that linear probe accuracy increases across layers (Figure 2), suggesting the model learns\nstronger predictors for classification tasks. However, they fail to reveal distinct or compositional\nfeatures per layer. SAEs address this by disentangling activations into sparse, interpretable bases,\nproviding deeper insights into the features learned at each layer."}, {"title": "4.2 TILE COLOR ACROSS LAYERS", "content": "Figure 3 shows the features extracted by the SAEs, computed by identifying the most discriminative\nfeatures using AUROC scores across multiple random seeds. The resulting heatmaps highlight\npositions with consistently high importance, such as edges and central tiles. In contrast, Figure 4\nvisualizes the contributions of individual MLP neurons from linear probes to tile classifications,\nmeasured via cosine similarity.\nFigure 3 and Figure 4 reveal distinct differences in how SAEs and linear probes learn features from\nthe board. The SAE visualizations highlight clear and structured patterns, such as strong activations\nat corner and edge tiles in layer 1, indicating that the model captures the board's shape early on. As\nwe progress to Layers 2 and 4, SAEs show more dynamic changes, with activations concentrated in\nthe central tiles and along the edges. This suggests the SAEs are not only learning positional impor-\ntance but could also capturing the evolving dynamics of central tiles, which tend to flip frequently\nas the game progresses. Importantly, these results are aggregated across 10 random seeds, demon-\nstrating the robustness and consistency of SAEs in identifying meaningful features. In contrast, the\nlinear probe visualizations show more dispersed activations across the board. While individual tiles\nare well-classified, the activations lack the clear structural patterns seen in SAEs."}, {"title": "4.3 TILE STABILITY ACROSS LAYERS", "content": "We notice the highest frequency of tile-feature activations in the intermediate layers (layers 2 through\n4), which can be seen in Figure 5. Earlier (layer 1) and later layers (layers 5 through 8) do not appear\nto learn stability, but rather they likely dedicate their representational capacity to other aspects of\nthe board state. This layer-specific pattern is consistent with the notion that different depths in the\nmodel are dedicated to learning different concepts.\nWe further support this analysis through Table 1, which reveal this same pattern across layers for\ndistinct features. Features 349 and 108 for instance, show a strong pattern which suggests that they\nencode tile stability. Table 4 shows the exact AUROC scores for tile-feature pairs in layer 2, which\nare clearly higher relative to other layers. We're able to disentangle and trace these feature patterns\nacross layers through using SAEs, which is an advantage over using linear probes.\nHowever, we must acknowledge that there is some variability across seeds as we can see in Table 1\nand Table 2. This variability raises the possibility that the features we interpret as \"stability\" may\nbe a composition of related but more granular features, such as the presence of edge or corner tile\nconfigurations. These properties may jointly give rise to the notion of stability, without representing\nstability itself in isolation. Future work with additional seeds is necessary to fully investigate these\ndistinctions. For example, fine-grained analyses of isolating and perturbing corner-detection features\nmay shed light on the causal role of these subcomponents in producing emergent stability behaviors."}, {"title": "5 CONCLUSION AND FUTURE WORK", "content": "In this work, we analyzed the progression of learned features in OthelloGPT, revealing a hierarchical\nstructure in its internal representations. Specifically, we observe that different layers in OthelloGPT\nfocus on distinct aspects of gameplay: some capture structural attributes like board shape and edges,\nwhile others appear to encode dynamic features such as tile flips and shifts in board state. By\ncomparing the capabilities of sparse autoencoders (SAEs) and linear probes, we established that\nSAEs excel at uncovering more distinctive and disentangled features, particularly for compositional\nand interpretable attributes. In contrast, linear probes tend to highlight features that serve as strong\ncorrelators for classification tasks. Through these methods, we decoded features related to tile color\nand stability, offering a novel framework for understanding how GPT-based models and transformers\nconstruct and organize their internal representations.\nOur findings suggest promising avenues for advancing model interpretability. Attribution analy-\nsis and automated interpretability methods, such as those proposed by Bills et al. (2023), could\nbe applied to identify causal features that directly influence move selection. This could lead to a\ndeeper understanding of how individual neurons and attention heads contribute to decision-making\nprocesses. For future work, we want to expand our discovery in several ways. First, we can use fine-\ngrained techniques to map the role of individual neurons in representing specific gameplay attributes\nor strategies, and see whether more neurons can behave similarly in patterns. We can also extend\nthis analysis to other board game models and Large Language Models (LLMs) to determine whether\nhierarchical feature learning is consistent across tasks and model types. Finally, we believe that an\nimportant area of future work lies in comparing model-derived representations of Othello strategy\nwith established human concepts of gameplay. This could involve mapping latent dimensions in the\nmodel to known strategic heuristics to understand where the model's features align or conflict with\nhuman reasoning."}]}