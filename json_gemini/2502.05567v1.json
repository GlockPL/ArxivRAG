{"title": "ATLAS: AUTOFORMALIZING THEOREMS THROUGH LIFTING, AUGMENTATION, AND SYNTHESIS OF DATA", "authors": ["Xiaoyang Liu", "Kangjie Bao", "Jiashuo Zhang", "Yunqi Liu", "Yu Chen", "Yuntian Liu", "Yang Jiao", "Tao Luo"], "abstract": "Autoformalization, the process of automatically translating natural language mathematics into machine-verifiable formal language, has demonstrated advancements with the progress of large language models (LLMs). However, a key obstacle to further advancements is the scarcity of paired datasets that align natural language with formal language. To address this challenge, we introduce ATLAS (Autoformalizing Theorems through Lifting, Augmentation, and Synthesis of Data), an iterative data generation framework designed to produce large-scale, high-quality parallel theorem statements. With the proposed ATLAS running for 10 iterations, we construct an undergraduate-level dataset comprising 300k theorem statements and develop the ATLAS translator, achieving accuracies of 80.59% (pass@8) and 92.99% (pass@128) on ProofNet, significantly outperforming the base model (23.99% and 47.17%) and InternLM2-Math-Plus-7B (50.94% and 80.32%). Furthermore, the ATLAS translator also achieves state-of-the-art performance on both the high-school-level miniF2F dataset and the graduate-level MathQual dataset introduced in this work. The datasets, model, and code will be released to the public soon.", "sections": [{"title": "1 Introduction", "content": "In modern mathematics, the growing complexity of proofs and use of computer-assisted proofs have raised concerns. Errors in traditional proofs may go unnoticed for long periods, while computer-assisted proofs often lack manual verifiability, leading to trust issues. For instance, the Four Color Theorem, proposed by Francis Guthrie in 1852, saw Alfred Kempe's 1879 proof accepted for 11 years until Percy Heawood identified its flaw in 1890. In 1976, Kenneth Appel and Wolfgang Haken provided the first computer-assisted proof, but its reliance on unverifiable computations sparked debate. It was not until 2005 that Georges Gonthier used the Coq to formally verify the proof. To address such issues, formal languages like Isabelle, HOL Light, Coq, and Lean have undergone significant advancements, leveraging automated verification to rigorously ensure the correctness of proofs.\nHowever, writing mathematical content in formal languages demands a substantial investment of time and requires a deep familiarity with these languages, making the process labor-intensive. This highlights the critical importance of the task of autoformalization, which seeks to translate theorem statements and proofs described in natural language into their formal language counterparts. Given that the accurate formalization of statements is a critical prerequisite for the formalization of proofs, the focus of current research predominantly lies in the autoformalization of theorem statements. Recent advancements in this area have demonstrated promising results, primarily achieved by fine-tuning large language models (LLMs) through the construction of paired datasets. For clarity, we will hereafter refer to theorem statements"}, {"title": "2 Related Work", "content": "Autoformalization. The task of autoformalization can be seen as a machine translation problem , which fundamentally adheres to the principle of mapping content from the source language into expressions that are consistent with the established syntax and lexical system of the target language. Early approaches have utilized neural machine translation techniques to address the task of autoformalizing theorem statements. With the rapid advancements in LLMs, recent research on LLM-based autoformalization can be broadly categorized into three main paradigms. Specifically, researchers have developed the application of few-shot prompting to enable LLMs to perform autoformalization effectively, while works such as ProofNet and MMA have further refined"}, {"title": "3 Methodology", "content": "Our framework ATLAS, as illustrated in Figure 1, comprises three components: Data Lifting, Data Synthesis, and Data Augmentation. Section 3.1 introduces the construction of the concept repository through data lifting, laying the groundwork for subsequent processes. Section 3.2 describes the specific workflow for data synthesis, while Section 3.3 elaborates on the approach to data augmentation, with both together forming the complete steps of a single iteration."}, {"title": "3.1 Data Lifting", "content": "Mathlib , the most extensive mathematical library within the Lean community, provides a rich repository of formalized mathematical definitions, theorems, and lemmas. This wealth of resources forms the cornerstone of autoformalization, enabling accurate and consistent translation of mathematical content. However, the reliance on Mathlib also exposes a critical limitation: when NL statements involve mathematical definitions that are absent in Mathlib, such as \u201csubgradients\" or \"subdifferentiation\", the autoformalization process becomes highly prone to failure. The underlying reason is that the autoformalization of such NL statements necessitates first constructing the FL representation of the mathematical definition itself before attempting the translation, which is an extremely challenging task for LLMs.\nTo address the aforementioned limitation, we propose a shift in focus from collecting pre-existing NL statements to constructing a repository of mathematical concepts derived from Mathlib, followed by sampling concepts from this repository to synthesize NL statements. The setup of the repository serves two key purposes. On one hand, it avoids inefficiencies and resource waste caused by unimplemented mathematical definitions in Mathlib. On the other hand, it allows precise control over the mathematical topics involved in the NL statements, enabling greater flexibility and specificity. We refer to the process of abstracting concepts from a larger database to construct this repository as data lifting.\nSpecifically, we commence by collecting undergraduate-level mathematical topics that have already been formalized in Mathlib to construct the concept repository. The repository consists of 13 domains, 55 topics, and 350 concepts. Detailed information about the concept repository can be found in Appendix A."}, {"title": "3.2 Data Synthesis", "content": "This module is the most critical component of ATLAS, generating a large number of high-quality parallel statements through a teacher-student paradigm. Specifically, the entire process can be likened to an exam where NL statements are translated into FL statements. However, the goal is not only to assess the student's translation abilities but, more importantly, to construct datasets that encompass both the knowledge the student has mastered and the unmastered knowledge imparted by teachers. The following subsections provide a detailed description of each phase, with the specific prompts provided in Appendix B.\nNL Statements Generation. Inspired by MUSTARD  and AI-Assisted Math Generation , we similarly sample two mathematical concepts randomly from the repository and utilize the NL-Gen teacher model to generate an NL statement that incorporates these two concepts. This strategy leverages the linguistic capabilities of LLMs, enabling the generation of NL statement that are both reasonable and suitably challenging. In each iteration, a total of 10,000 samples are drawn, resulting in the generation of 10,000 NL statements.\nNL Statements Translation. After generating the synthetic NL statements, we leverage the student model to translate them into the corresponding FL statements, thereby obtaining the parallel statements. During the initial iteration, the student model is fine-tuned on the MathlibExtract dataset and the Lean Workbook dataset. In subsequent iterations, the fine-tuning process incorporates additional synthetic data in previous iterations, along with augmented data, to iteratively improve and enhance autoformalization performance of the student model.\nFor MathlibExtract, we leverage LeanDojo  to extract FL statements from Mathlib and use LLMs to generate corresponding NL statements, instead of relying on the Mathlib dataset from the MMA dataset . This choice is motivated by two factors: changes in Lean's syntax rules and continuous updates to Mathlib's content, which have rendered the MMA dataset outdated.\nFL Statements Parsing. Before conducting syntactic validity test on FL statements, we develop a script using the stack and the #check tactic to divide each FL Statement into the following four components: theorem_name, theorem_variables, theorem_hypotheses, and theorem_conclusion. Subsequently, the content is systematically organized line by line, both within and between these components. To provide a detailed explanation, we present a synthetic example obtained through the first two phases and additional synthetic examples with analyses can be found in the Appendix C. The NL statement, along with its associated concepts and domains, is presented as follows."}, {"title": "FL Statements Compilation", "content": "In this phase, the Lean compiler's REPL (Read-Eval-Print Loop) is used to verify the syntactic validity of FL statements by checking if they compile successfully. Specifically, the implementation of REPL is primarily derived from the framework provided by the Lean community, with additional reference to the codebase of Automatic Lean4 Compilation. Notably, since the student model lacks the capability to generate headers, a standard header, import Mathlib, is appended during the compilation process.\nFL Statements Revision. For FL statements that fail to compile, we utilize the corresponding NL statements and compilation error messages as context, providing this information to FL-Rev teacher model for modifications. The modified FL statements are then subjected to a second round of compilation. Ultimately, only the FL statements that successfully pass either the first or second compilation are retained. The underlying concept of this design is to enable a stronger model to guide a weaker model, thereby facilitating the transfer of knowledge from the former to the latter. This process allows the weaker model to progressively improve its performance and the subsequent experimental results validate the effectiveness of this design approach.\nFL Statements Alignment. For FL statements that pass the first or second compilation, FL-Align teacher model evaluates their semantic accuracy in translating the corresponding NL statement, ensuring no information is omitted or mistranslated. Specifically, the model is required to assess each pair of parallel statements and assign a rating from three categories: good, average, and poor. Pairs rated as good or average are retained, while pairs rated as poor, along with FL statements failing both compilations, have only their NL statements preserved for the next iteration's NL Statements Translation phase.\nNote that we do not employ back-translation of FL statements into NL statements to compare their semantic accuracy with the original NL statements in this phase. Typically, back-translation is introduced to mitigate the influence of Lean"}, {"title": "3.3 Data Augmentation", "content": "This module augments the synthetic parallel statements in Section 3.2 to further expand the scale of data, leveraging the Lean compiler and employing two innovative methods: proving these FL statements and converting them into the contrapositive propositions.\nAugmentation via Proof. For each FL statement in the synthetic data, we use DeepSeek-Prover-V1.5  to provide a proof. Then, the proof steps are executed interactively in Lean compiler step by step. Each time a tactic is successfully applied and the proof process has not yet concluded, Lean compiler's Infoview generates feedback containing the current variables, hypotheses, and conclusions. Leveraging the stack and regular expressions, we develop a script capable of constructing new FL statements directly from the content of the Infoview. Continuing with the example from Section 3.2 FL Statements Parsing, after applying tactic intro & epos, the FL statement constructed from the Infoview is as follows.\nAugmentation via Contraposition. In Section 3.2 FL Statement Parsing, we obtain the theorem_hypotheses for each FL statement. From this, we can extract the names of all the hypotheses and form a set, denoted as hypotheses, with its elements represented as h. Utilizing the contrapose! tactic, we can apply contrapose! h sequentially to each element h in hypotheses, transforming the original proposition into an equivalent contrapositive statement for each hypothesis. After applying this transformation, new FL statements are constructed based on the updated information provided in Lean's Infoview. Continuing with the example from Section 3.2 FL Statements Parsing, after applying tactic contrapose! hab, we derive an equivalent contrapositive FL statement: the negation of the original conclusion becomes the new hypothesis, referred to as hab, while the negation of the original hypothesis hab becomes the new conclusion.\nAugmentation Dataset Construction. The aforementioned augmentation operations both rely on Lean compiler's Infoview. However, Infoview occasionally results in information loss. For instance, in the case of augmentation via contraposition, the types associated with N and \u03b5 are lost (i.e., \u039d : \u039d, \u03b5 : R). While these types can be internally inferred by the Lean compiler without causing compilation errors, some instances of information loss can lead to critical errors. For example, consider the following FL statement derived from the PutnamBench ."}, {"title": "4 Experiments", "content": "4.1 ATLAS Dataset Construction"}, {"title": "4.2 ATLAS Translator Evaluation", "content": "Table 1: Performance comparison of different models across various datasets. 12-shot prompting involves enhancing the prompt with 12 parallel theorem statements randomly sampled from the ATLAS dataset as examples, with the same set of 12 examples used across all models performing 12-shot prompting. The version of GPT-4o used is gpt-4o-2024-11-20 and the pass@128 results are excluded due to its relatively unremarkable performance under 12-shot prompting. SFT denotes models fine-tuned on the MathlibExtract and the Lean Workbook. The results of HERALD are computed as the average performance across both the validation and test datasets. The last four rows represent ablation studies, indicating which datasets are used to fine-tune the base model. In addition, except for HERALD, the prompts used during inference for all models are consistent with those shown in Table 4.\nExperimental Setup. In this section, we fine-tune Llama3.1-8B-Instruct exclusively on the ATLAS dataset to develop the ATLAS translator, then compare its autoformalization ability with other models on three datasets: miniF2F , ProofNet , and MathQual. The versions of miniF2F and ProofNet used in this evaluation are derived from DeepSeek. MathQual is a graduate-level dataset proposed in our work, consisting of 2.5k NL statements, designed to evaluate the model's generalization ability on more challenging datasets.\nWe use the pass@k metric for evaluation, with k = 1,8, 128. Pass@k represents the proportion passing both syntactic validity and semantic accuracy tests. For semantic accuracy test, we follow the methodology described in the Lean Workbook  and HERALD : FL statements are back-translated into NL statements using InternLM2-Math-Plus-7B and DeepSeek-V2.5 is used to compare the back-translated NL statements with the original FL statements to ensure consistency. Here, k represents the number of candidates generated per NL statement. And we consider the translation successful if any of these candidates pass both the syntactic validity and semantic accuracy tests.\nNotably, given the large number of samples in MathQual, we use a subset for evaluation. This subset is constructed by sampling from each domain, with the sample size determined as | number of samples in the domain | + 1. Detailed information"}, {"title": "5 Conclusion", "content": "In this work, we propose a novel framework to advancing autoformalization by synthesizing and augmenting large-scale, high-quality datasets of natural language (NL) and formal language (FL) theorem statements, starting with data lifting. Our method addresses key limitations of existing approaches, such as the finite amount of data that can be extracted from repositories like Mathlib or the inconsistent data quality obtained from large-scale web scraping. By leveraging synthetic NL statements and iteratively fine-tuning the student model, we demonstrated a scalable and efficient framework for generating parallel NL-FL statements, significantly enhancing the autoformalization process.\nOur proposed ATLAS translator, fine-tuned on the ATLAS dataset, achieves state-of-the-art performance across multiple benchmarks, including miniF2F, ProofNet, and the newly introduced MathQual dataset. Notably, the ATLAS translator achieves an accuracy of 92.99% (pass@128) on ProofNet and demonstrates strong generalization capabilities, achieving an accuracy of 96.93% on miniF2F and an accuracy of 84.72% on MathQual under pass@128. These results underscore the efficacy of our dataset and framework, as well as the robustness of our model when applied to diverse and challenging mathematical statements.\nIn addition, our proposed framework is highly generalizable. For data lifting, it allows the creation of concept repositories across various formal languages, mathematical domains, and levels of difficulty, which can then be combined with data synthesis techniques to generate corresponding datasets. While our data augmentation approach relies on feedback from the Lean compiler, it is inherently applicable to any mathematical problem, as altering the problem's state through proof steps or reformulating it as a contraposition are fundamental operations in mathematics. These flexibilities highlight the broader applicability of our framework beyond Lean, making it a valuable tool for advancing formalization. By improving the scalability and accessibility of autoformalization, our work also paves the way for wider adoption of formal verification in mathematics and future advancements in automated theorem proving."}, {"title": "C Case Study", "content": "This section presents both successful and unsuccessful cases of synthetic data during the ATLAS iterative process. Tables 9 to 14 display successful cases generated by ATLAS, while Tables 15 and 16 illustrate the failed cases, each accompanied by corresponding analyses. The FL statements consist of multiple rounds, with content highlighted in red boxes indicating syntactic errors or inconsistencies with the relevant NL statements. Iteration for each data point ceases when there are no syntactic or semantic errors in the FL statement. The cases below demonstrate that, as the model iterates, it is able to formalize mathematical concepts in Lean with increasing accuracy."}, {"title": "D Human Evaluation", "content": "In this section, 15 samples are randomly selected from the pass@1 samples of miniF2F, ProofNet, and MathQual, which are evaluated for syntactic validity and semantic accuracy by mathematicians proficient in Lean. This approach aims to provide a better assessment of the model's performance and these samples help readers gain a clearer understanding of the model's autoformalization capability. The results presented further underscore the model's remarkable capacity for autoformalization. Even in instances where the model produces erroneous translations, it demonstrates a high level of completeness, enabling human experts to swiftly correct mistakes.\nD.1 miniF2F"}, {"title": "E MathQual", "content": "The MathQual dataset is derived from previous examinations of the Yau Mathematical Competition for college students, as well as graduate qualification examinations from multiple universities, including Boston University, Johns Hopkins University, University of Texas at Dallas, University of California, Los Angeles, University of California Riverside, and University of Georgia. Table 17 presents the domains included in the MathQual dataset, along with the corresponding number of problem statements, as well as the sampling quantities from each domain during experiments. The process of creating the dataset is elaborated as follows.\n1. Relevant PDF documents are retrieved from official websites.\n2. Optical Character Recognition (OCR) technology is employed to convert these documents into Markdown format.\n3. High-quality, formalizable problem statements are meticulously selected through a manual filtration process. Notably, for proof problems consisting of multiple sub-questions, we amalgamate the overarching contextual conditions of the main problem with the specific conditions of each sub-question, thereby constructing several distinct problem statements.\n4. The problem statements are categorized according to Mathematics Subject Classification (MSC)."}]}