{"title": "Influence of Backdoor Paths on Causal Link Prediction", "authors": ["Utkarshani Jaimini", "Cory Henson", "Amit Sheth"], "abstract": "The current method for predicting causal links in knowledge graphs uses weighted causal relations. For a given link between cause-effect entities, the presence of a confounder affects the causal link prediction, which can lead to spurious and inaccurate results. We aim to block these confounders using backdoor path adjustment. Backdoor paths are non-causal association flows that connect the cause-entity to the effect-entity through other variables. Removing these paths ensures a more accurate prediction of causal links. This paper proposes CausalLP-Back, a novel approach to causal link prediction that eliminates backdoor paths and uses knowledge graph link prediction methods. It extends the representation of causality in a neuro-symbolic framework, enabling the adoption and use of traditional causal AI concepts and methods. We demonstrate our approach using a causal reasoning benchmark dataset of simulated videos. The evaluation involves a unique dataset splitting method called the Markov-based split that's relevant for causal link prediction. The evaluation of the proposed approach demonstrates atleast 30% in MRR and 16% in Hits@K inflated performance for causal link prediction that is due to the bias introduced by backdoor paths for both baseline and weighted causal relations.", "sections": [{"title": "1 Introduction", "content": "Knowledge graph link prediction is the process of discovering new knowledge using the existing knowledge represented in the knowledge graph (KG). Causal link prediction is the process of discovering new causal links using the existing knowledge represented in the knowledge graph (KG). The use of knowledge graph link prediction for causal link prediction is a first step towards the use KGs (and neuro-symbolic AI methods) for causal AI tasks. However, the existing efforts on causal link prediction using KG link prediction do not consider the presence of backdoor paths for a given causal link [6] even though the backdoor path demonstrates a common cause of the entities present in the causal link. The"}, {"title": "3 Problem formulation", "content": "Causal knowledge graph: Causal knowledge is structured within a causal knowledge graph, denoted as CausalKG, which is comprised of causal entities, causal relations, and causal weights. The structure of CausalKG (N, R, E, Ec,\nWc) is detailed as follows:\nN: a set of nodes that represent entities.\nR: a set of labels that denote different types of relations.\nECNXRXN: a set of edges that form links between pairs of entities,\nwith each link represented as a triple <h, r, t>, where h is the head entity,\nr is the relation, t is the tail entity.\nNCN: a subset of nodes that are identified as causal entities.\nRCR: a subset of labels that are identified as causal relations.\nWR: a set of real numbers that assign causal weights.\nEcNcx Rc \u00d7 Nc \u00d7 Wc: a set of edges that connect pairs of causal entities.\nEach of these causal links is represented as a quadruple <hc, rc, tc, wc>,\nwhere he is the head causal entity, re is the causal relation, te is the tail\ncausal entity, and we is the weight of the causal influence.\nThis graph encapsulates the causal structure and dynamics between entities,\noffering a clear depiction of causal interactions within the system.\nCausal entity: A causal entity ne \u2208 Ne is defined as any entity that serves\nas either the head or the tail in a causal link. There are two distinct categories\nof causal entities: the cause-entity (ncause) and the effect-entity (neffect). In a\ncausal relation, the cause-entity is responsible for causing the effect-entity.\nCausal relation: A causal relation re \u2208 Re denotes a relation that represents\na causal linkage between entities. There are four specific types of causal relations:\ncauses (rcauses \u2208 Rc) is a causal relation that links a cause-entity to an\neffect-entity.\ncausedBy (rcausedBy \u2208 Rc) is a causal relation that connects an effect-entity\nback to a cause-entity; the reverse of causes.\ncausesType (rcausesType \u2208 Rc) is a causal relation that associates a cause-\nentity with the type of the effect-entity.\ncausedByType (rcausedByType \u2208 Rc) is a causal relation that links an effect-\nentity to the type of the cause-entity."}, {"title": "4 Method", "content": "The proposed CausalLPBack approach is structured into five primary phases\n(see Figure 2): (1) finding and encoding the known causal relations into a causal\nnetwork, (2) removing the backdoor paths spanning across the nodes in the\ntraining and testing set, (3) translating the causal network into a CausalKG,\ncompatible with the causal ontology [4], (4) learning KGE for the CausalKG,\nand (5) predicting new causal links in the KG."}, {"title": "4.1 Causal Network", "content": "A causal network is a graphical model known as a causal Bayesian network,\nstructured as a directed acyclic graph. In this model, nodes symbolize events,\nand edges represent the causal links between these events. The network, denoted\nas CN = (Ncn, Ecn, Wcn), comprises nodes Nen, edges Ecn, and causal weights\nWcn which denote the strength of the causal influence. The direction of each edge\nin the network indicates the direction of causality, and causal weights assigned\nto these edges measure the impact of changes in one node on another, using"}, {"title": "4.2\nBackdoor path", "content": "A Backdoor (BD) is a non causal path for a given node pair (Nen, Nen) in\na causal network, where Non is the cause and Nen is the effect. It indicates\ncommon causes of the node pair. To estimate the true influence of a cause, Nen,\non its effect, Nen, removing of all the backdoor paths blocks the spurious paths\nbetween the cause and effect node pair which may otherwise lead to inflated\nresults.\nA set of nodes BD\u00bf is said to satisfy as a backdoor path for a given node\npair in a causal event graph if:\nno node in BD is a child of cause node, Nen; and\nBD blocks every path between cause Nen and effect Nen containing an\nincoming edge into the cause.\nWe considered two sets of BD paths: sufficient BD paths and maximum\nbackdoor paths. The sufficient BD is the minimum set required to satisfy the\nBD path. The paths with direct incoming edge (parent) into the cause node.\nThe maximum backdoor path is a maximum possible set which includes all the\npossible BD paths for a given pair."}, {"title": "4.3 Causal Knowledge Graph", "content": "The process of transforming data from a causal network into a causal knowledge\ngraph (CausalKG) involves several conversions:"}, {"title": "4.4\nCausalKG Embedding and Link Prediction", "content": "A CausalKG can be transformed into a low-dimensional, continuous latent vector\nspace known as a knowledge graph embedding (KGE), which are useful for down-\nstream tasks like link prediction, triple classification, entity classification, and\nrelation extraction [14]. The CausalLPBack approach employs KGE algorithms\nto create two types of embeddings for CausalKG: CausalKGE-Base, which lacks\ncausal weights, and CausalKGE-W, which incorporates these weights to produce\nweighted embeddings [9]. These embeddings are created using the Ampligraph\u00b3\nlibrary tools. The process involves training the CausalKGE-Base using causal\nlinks without considering the associated weights, while the CausalKGE-W con-\nsiders these weights, assigning higher probabilities to links with greater weights\nand treating zero-weighted links as negative samples. Training adjusts the out-\nput from the scoring layer based on the causal weights before progressing to the\nloss layer, thereby modulating the scores to reflect the weighted causal impacts.\nThese embeddings are primarily evaluated for causal link prediction, utilizing KG\nlink prediction techniques. CausalLPBack frames causal link prediction as a KG\nlink prediction task, using the embeddings to identify missing causal links and\nto facilitate causal explanation and prediction tasks. Causal explanation seeks to\ndetermine the causes of effects, whereas causal prediction focuses on the effects\nof causes. This method, tested using the CLEVRER-Humans causal reasoning\nbenchmark dataset, demonstrates the utility of CausalLPBack in generating a\nCausalKG and employing its embeddings for effective causal link prediction [7]."}, {"title": "5 Experiment", "content": "The causal link prediction approach is assessed using KG link prediction, tailored\nfor causal explanation and causal prediction. Specifically, causal explanation\ninvolves predicting the type of a cause-entity from a given effect-entity using\nthe causal link format <neffect, 'causedByType, ?, w>, while causal prediction\nentails prediction the type of an effect-entity from a given cause-entity using the\nformat <ncause, 'causesType,?,w> (see Figure 3). These tasks are demonstrated\nusing different CausalKG variations setup and KGE models with the CLEVRER-\nHumans benchmark dataset for causal reasoning [7]."}, {"title": "5.1 Data", "content": "CLEVRER-Humans is a causal reasoning benchmark dataset that enhances the\nCLEVRER dataset a simulation of collision events in videos with human-\nannotated causal judgments [7]. The videos feature distinct moving objects, dif-\nferentiated by shape (e.g., spheres, cubes, cylinders), color (e.g., blue, red, yellow,\ngreen, purple, gray, cyan, brown), and material (e.g., metal, rubber). These ob-\njects are involved in various events, such as entering, exiting, colliding, moving,\nhitting, bumping, and rolling. The dataset captures the causal relationships be-\ntween these events in a Causal Event Graph (CEG), where event descriptions\nfrom the videos serve as nodes and directed edges between these nodes indicate\ncausal connections. Human annotators assess these edges on a scale from 1 to\n5 to rate the strength of the causal links, where 1 indicates 'not responsible\nat all' and 5 denotes 'extremely responsible'. This scoring helps determine the\nperceived impact of one event on another within the video."}, {"title": "5.2 Pre-processing the Data", "content": "The first step towards generating a CausalKG for CLEVRER-Humans involves\npre-processing the CEGs."}, {"title": "5.3 CausalKG from CLEVRER-Humans", "content": "A CausalKG is generated from CLEVRER-Humans by encoding the causal infor-\nmation within the CEGs in RDF6 format, conformant with the causal ontology.\nIn addition to causal relations, the KG contains information about events along\nwith the participating objects and their characteristics. We use three ontologies\nto represent information from the CEGs: causal ontology, scene ontology (prefix\nso:), and semantic sensor network ontology (prefix ssn:). The causal ontology\nis used to represent the events (as causal entities), causal relations, and their"}, {"title": "5.4\nSplitting the Data and Removing the Backdoor Path", "content": "We use a novel dataset splitting approach, Markov-based split, grounded in the\nlocal Markov property of causal relations introduced in [6]. The Markov-based\ndata split divides each of the causal networks into a train and test set. The split\nis based on the local Markov property of a causal network; i.e. for a given direct\ncause of a node, it is independent of its non-effects. With the Markov-based data\nsplit, the initial train and test sets contains 80% and 20% of the total CEGs, re-\nspectively. From the 764 CEGs, 612 are in train and 152 are in test set. The CEGs\nin the test set are further split into Markov-train and Markov-test set based on\nthe local Markov property of a causal network. We remove 1) sufficient back-\ndoor path (minimum), and 2) maximum backdoor path satisfying the backdoor\npath criteria from the Markov-train and Markov-test sets. The CEG present in\nthe training set, as well as the nodes and relationships within the Markov-train\nset, are utilized to construct the CausalKG specific to CLEVRER-Humans. This\nCausalKG is subsequently employed to train the KGE. Similarly, the nodes and\nrelationships within the Markov-test set are utilized to create test links, which\nare then employed to assess the performance of the KGE. Each dataset split is\nprovided as input to the KGE algorithms, resulting in the generation of both\n1) CausalKGE-Base embeddings without causal weights, and 2) CausalKGE-W\nembeddings with causal weights. These embeddings are intended for use in the\ntask of link prediction, specifically for causal explanation and prediction."}, {"title": "5.5 Diversifying the Available Knowledge", "content": "The CausalKGE-Base and CausalKGE-W embeddings are developed and tested\nacross various subgraph structures within the CLEVRER-Humans CausalKG\nto assess their effectiveness in causal explanation and prediction tasks. These\nsubgraphs vary in complexity and detail, as illustrated in Figure 4. Specifically,\nthree different subgraph structures are defined with increasing levels of expres-\nsivity: (1) graph structure C: includes only basic links with causal relations,\n(2) graph structure CT: includes causal relations and types of causal entities,\nadding a layer of semantic detail, (3) graph structure CTP: the most detailed\nstructure, includes causal relations, entity types, and relationships to objects\ninvolved in causal events. For each graph structure, hyper-parameters were care-\nfully optimized to enhance the performance of the corresponding tasks causal"}, {"title": "5.6 Evaluation Metrics", "content": "The evalution of CausalLPBack is performed by following the KG link prediction\nexperiment design for four common KGE algorithms: TransE [1], DistMult [16],\nHOLE [8], and ComplEx [13]. Evaluations performed with weights use FocusE\n[9]. For a given set of causal links Ec in CausalKG, a set of corrupted links\nT' are generated by replacing the head he or tail te of a set of causal links,\n<hc, rc, tc, wc>, with another causal entity in the KG. Replacing the head with\nh\u2260 he results in <h', rc, tc, wc> or replacing the tail with te \u2260 te results in\n<hc, rc,tcwc>. The model scores the true link <hc, rc, tc, wc> and corrupted\nlinks <he, rc, tc, Wc>, <hc, rc, t'c, wc> \u2208 T'. The scores are then sorted to obtain\nthe rank of the true link. The filtered evaluation setting and filtered corrupted\nlinks T' are used to exclude the links present in the training and validation set.\nThe overall performance of the models is measured using mean reciprocal rank\n(MRR) and Hits@K for k = {1,3,10}. MRR is the mean over the reciprocal of\nindividual ranks of test links. Hits@k is the ratio of test links present among the\ntop k ranked links."}, {"title": "6 Results and Discussion", "content": "CausalLPBack was evaluated on CausalKGs generated (Figure 4) from CLEVRER-\nHumans dataset for the task of causal explanation and prediction. CausalLPBack\nhas six trained KGE models:"}, {"title": "7 Conclusion", "content": "In this paper, we present a novel approach to causal link prediction,\nCausalLPBack. This approach fills a crucial gap in the current state-of-the-\nart by integrating the concepts and representation of causality from Causal AI\ninto a knowledge graph (Neuro-Symbolic AI). This method utilizes knowledge\ngraph link prediction to effectively address the confounding bias introduced by\nbackdoor paths in the causal network. The CausalKGE-W, one of our proposed\napproaches, outperforms all baseline CausalKGE-Base methods, demonstrating\nits superiority and innovation. Our results demonstrate that the effective fu-\nsion of weighted causal relation and removing the backdoor paths can facilitate\nthe mapping of causal link prediction (i.e., causal AI) to knowledge graph link\nprediction. The approach significantly demonstrated the inflated performance of\nCausalKG due to the bias introduced due to the backdoor paths. In future work,\nwe would like to further study the difference in maximum vs sufficient backdoor\nelimination with respective to causal explanation vs causal prediction, the role of\ndifferent subgraph structures, how does the characteristics of a causal link such\nas prediction vs explanation play a role in the performance metric. Additional\nSupplementary Material"}]}