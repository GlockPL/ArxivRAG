{"title": "KnowGraph: Knowledge-Enabled Anomaly Detection via Logical Reasoning on Graph Data", "authors": ["Andy Zhou", "Xiaojun Xu", "Ramesh Raghunathan", "Alok Lal", "Xinze Guan", "Bin Yu", "Bo Li"], "abstract": "Graph-based anomaly detection is pivotal in diverse security ap- plications, such as fraud detection in transaction networks and intrusion detection for network traffic. Standard approaches, includ- ing Graph Neural Networks (GNNs), often struggle to generalize across shifting data distributions. For instance, we observe that a real-world eBay transaction dataset revealed an over 50% decline in fraud detection accuracy when adding data from only a single new day to the graph due to data distribution shifts. This highlights a critical vulnerability in purely data-driven approaches. Meanwhile, real-world domain knowledge, such as \"simultaneous transactions in two locations are suspicious,\" is more stable and a common ex- isting component of real-world detection strategies. To explicitly integrate such knowledge into data-driven models such as GCNs, we propose KnowGraph, which integrates domain knowledge with data-driven learning for enhanced graph-based anomaly detection. KnowGraph comprises two principal components: (1) a statistical learning component that utilizes a main model for the overarching detection task, augmented by multiple specialized knowledge mod- els that predict domain-specific semantic entities; (2) a reasoning component that employs probabilistic graphical models to exe- cute logical inferences based on model outputs, encoding domain knowledge through weighted first-order logic formulas. In addi- tion, KnowGraph has leveraged the Predictability-Computability- Stability (PCS) framework for veridical data science to estimate and mitigate prediction uncertainties. Empirically, KnowGraph has been rigorously evaluated on two significant real-world scenarios: collusion detection in the online marketplace eBay and intrusion detection within enterprise networks. Extensive experiments on these large-scale real-world datasets show that KnowGraph consis- tently outperforms state-of-the-art baselines in both transductive and inductive settings, achieving substantial gains in average preci- sion when generalizing to completely unseen test graphs. Further ablation studies demonstrate the effectiveness of the proposed rea- soning component in improving detection performance, especially under extreme class imbalance. These results highlight the poten- tial of integrating domain knowledge into data-driven models for high-stakes, graph-based security applications.", "sections": [{"title": "1 Introduction", "content": "Graphs are ubiquitous data structures with applications in various domains, such as social networks, biological networks, and commu- nication networks. In recent years, graph neural networks (GNNs) [75, 93] and other graph representation learning techniques, such as node2vec [20], have emerged as powerful techniques for learning on graph-structured data. These methods have achieved remarkable success in applications that involve graphs such as recommenda- tion [16], drug discovery [90], NLP [53], traffic forecasting [34],"}, {"title": "2 Related Work", "content": "Graph Neural Networks. Graph neural networks (GNNs) have become a cornerstone in machine learning for graph data, effec- tively encapsulating local graph structures and feature informa- tion by transforming and aggregating representations from neigh- bors. Common architectures include graph convolutional networks (GCN) [40], which uses convolutions directly on graph structures to capture dependencies, GraphSAGE [23], which learns induc- tive representations through sampling and aggregating neighbor- hood information, and GAT [66], which introduces an attention mechanism to aggregate neighborhood information. To represent long-range dependencies in disassortative graphs, Geom-GCN [54] introduces a geometric aggregation scheme that enhances the con- volution operation by leveraging the continuous space underlying the graph. GraphSAINT [86] proposes a graph sampling-based training method that allows for efficient and scalable learning on large graphs by iteratively sampling small subgraphs and perform- ing GNN computations on them. DR-GCN [61] employs a class- conditioned adversarial network to mitigate bias in node classi- fication tasks with imbalanced class distributions. More recently, Graphormer [78] proposes a graph transformer model employing attention mechanisms to leverage structural information well.\nGeneralization of GNNs. While some standard GNN architec- tures [23, 86] have been proposed for inductive settings, gener- alization remains challenging for graphs with class imbalance or diverse structures. Standard techniques such as resampling [9, 25] or reweighting [32, 42] are often sensitive to overfitting on mi- nority classes and less effective on shifts to new graphs. Instead, many techniques aim to learn more robust and generalizable graph representations through data augmentation [24, 91] or large-scale pretraining [30, 79, 80]. In addition, GNN capacity can be enhanced by model combination techniques such as model ensemble [63, 73], where the predictions of multiple models are combined to improve performance. Mixture-of-experts (MoE) [15, 33, 60] is a similar tech- nique where the problem space is divided by routing inputs to spe- cialized experts. Recent work adopts MoE for GNNs to address the class imbalance issue [31, 68, 85]. We also aim to address class imbal- ance and inductive generalization but leverage many task-specific GNNs organized through more deliberate domain knowledge.\nTo evaluate the generalization and uncertainty of GNNs in a principled manner, we draw upon the Predictability, Computability, and Stability (PCS) framework [83, 84]. PCS emphasizes three key principles: predictability, which serves as a reality check for mod- els; computability, which considers the feasibility and scalability of methods; and stability, which assesses the consistency of results under perturbations to data and models. In this work, we focus on the stability aspect and propose to incorporate weight noise en- sembling [3] into KnowGraph to mitigate uncertainty and improve generalization to out-of-distribution graphs.\nGraph anomaly detection. Graph anomaly detection has become an increasingly important research area, with applications span- ning various domains such as network security, financial fraud detection, and social network analysis. The goal of graph anomaly detection is to identify abnormal nodes, edges, or subgraphs within a graph-structured dataset that deviates from the expected patterns or behaviors. In this paper, we focus on two representative settings: lateral movement detection and collusion detection.\nIn network security, graph anomaly detection is crucial in de- tecting and mitigating lateral movement, a key stage in the MITRE ATT&CK framework [50]. Lateral movement refers to the prop- agation of malware through a network to compromise new sys- tems in search of a target, often involving pivoting through mul- tiple systems and accounts using legitimate credentials or mal- ware. Research on mitigating lateral movement in computer net- works generally follows three main approaches: enhancing secu- rity policies, detecting malicious lateral movement, and develop- ing forensic methods for post-attack analysis and remediation [14, 17, 22, 23, 27, 39, 43, 55, 62]. While proactive security mea- sures can help reduce the attack surface, they cannot eliminate all potential paths an attacker might exploit. Many lateral move- ment detection techniques represent internal network logins as a machine-to-machine graph and employ rule-based or machine learning algorithms to identify suspicious patterns [4, 27, 37]. How- ever, these methods often struggle with scalability, generate ex- cessive false alarms, or fail to detect attacks that do not match predefined signatures. Recent approaches improve detection by leveraging the structural nature of network data [5, 39], formulat- ing the problem as a temporal graph link prediction task. These methods aim to identify edges with low likelihood scores corre- lated with anomalous connections indicative of lateral movement. However, challenges remain in generalizing these models to new networks in inductive settings.\nIn the domain of financial fraud detection, graph anomaly detec- tion has also gained significant attention due to the economic cost and prevalence of fraud in various settings, such as social networks [7], online payment systems [92], and online marketplace plat- forms [8, 44, 56]. Early explorations focused on rule-based methods [11] and association rules [6, 58], but these approaches often fail to adapt to evolving fraudulent behaviors over time. With the availabil- ity of large-scale transaction data, data-driven and learning-based methods have gained popularity, including SVM-based ensemble strategies [67], graph-mining-based approaches [64, 65], convolu- tional neural networks (CNNs) [18], and recurrent neural networks (RNNs) for sequence-based fraud detection [35, 70, 89]."}, {"title": "3 Preliminaries", "content": "Graph Neural Networks. Given a graph G = (V, E, Y), where V is the set of nodes, E is the set of edges, and Y is the set of labels associated with the graph elements (nodes, edges, or the entire graph), the overall task of graph neural networks (GNNs) is to learn a mapping f: G\u2192 Y that predicts the labels of the graph elements based on their local neighborhood structure. To achieve this, GNNs learn representations of the graph elements by repeatedly performing neighborhood aggregation or message passing across multiple layers. The learned representations can then be used for various graph-related tasks, such as node classification (f: V\u2192 Y), edge prediction (f : E \u2192 Y), or graph classification (f: G\u2192Y).\nLet xj \u2208 RFo be the input feature vector of nodej\u2208 V,hl) \u2208 RH be the representations or embeddings of node j learned by layer l (l \u2265 1), and e(k,j) \u2208 RF2 be optional features of the edge (k, j) from nodes k to j. The message passing procedure that produces the embeddings of node j via the l-th GNN layer can be described as follows:\n$h_{j}^{(l)} = \\gamma \\left(h_{j}^{(l-1)}, \\square_{k \\in \\mathcal{N}(j)} \\phi\\left(h_{k}^{(l-1)}, e_{(k,j)}\\right)\\right)$\n(1)\nHere, t) is set to xj, N(j) is the neighborhood of node j, and () is a function that extracts a message for neighborhood aggre- gation, which summarizes the information of the nodes j and k, as well as the optional edge features e(k,j) if available. (\u00b7) denotes a permutation-invariant function (e.g., mean or max) to aggregate incoming messages, and y() is a function that produces updated"}, {"title": "4 KnowGraph", "content": "4.1\nOverview\nTo effectively integrate domain knowledge into data-driven graph neural networks (GNNs), we propose KnowGraph. In particular, KnowGraph consists of two components: a learning component"}, {"title": "4.2 Learning with knowledge-enabled reasoning on graph data", "content": "In the learning component of KnowGraph, we train a set of GNN models to predict the main task and knowledge models for as- sistant tasks. The knowledge models are conceptualized as predi- cates within this framework, outputting binary predictions for each knowledge model.\nFormally, we denote the output of the i-th model by ti() as ti, with zi representing the confidence level of its prediction. For a given input graph G = (V, E, Y) with nodes V, edges E, and labels Y, the GNN model's prediction is denoted as ti (G). Upon receiving input G and the assorted model predictions ti (G), these predictions are interlinked through their logical interrelations and a Markov Logic Network (MLN) [57], enabling the reasoning capability in KnowGraph.\nSpecifically, KnowGraph involves a primary model alongside multiple knowledge models ti (G), serving as predicates within the MLN framework. Logical connections between these predicates are established to formulate different logical expressions. Assuming L models in total, an MLN defines a joint probability distribution over the pre-defined logical expressions (i.e., knowledge rules), which can be expressed as follows:\n$P_w(t_1, ..., t_L) = \\frac{1}{Z(w)}exp(\\sum_{f \\in F}w_ff(t_1, ..., t_L)),$\n(5)\nwith Z(w) symbolizing the partition function, summing across all predicate assignments.\nKnowGraph's reasoning component manages logic formulas articulated as first-order logic rules. Following [87], we consider three types of logic rules:\n\u2022 Attribute rule (ti tjVtjV...): This rule leverages specific attributes associated with prediction classes to formulate knowledge-based rules.\n\u2022 Hierarchy rule (ti\ntj): Reflecting the hierarchical nature among classes, this rule aids in constructing logical expressions like f(ti, tj) = \u00acti \u2228 tj.\n\u2022 Exclusion rule (ti tj): This rule addresses the inherent exclusivity among some class predictions, ensuring that an entity cannot simultaneously belong to mutually exclusive classes.\nAfter designing the models and rules, the final step of KnowGraph is to learn and assign a weight for each rule to reflect the impact of their prediction confidence zi for each model ti (.). To achieve this, we utilize the logarithm of the odds ratio, log[zi/(1 - zi)], as the weight for model ti."}, {"title": "4.3 Scalable reasoning with a GCN", "content": "To reduce the computational complexity of training the MLN, we employ variational inference [87] to optimize the variational evi- dence lower bound (ELBO) of the data log-likelihood. This approach is motivated by the intractability of directly optimizing the joint distribution Pw(O, U), which requires computing the partition function Z(w) and integrating over all observed predicates O and unobserved predicates U. The ELBO is formulated as follows:\n$log P_w(O) \\geq L_{ELBO} (Q_e, P_w) = E_{Q_e(U|O)} [log P_w(O,U)] - E_{Q_e (U|O)} [log Q_e(U|O)],$\n(6)\nwhere Q(UO) is the variational posterior distribution. The rep- resentation of model outputs and knowledge rules as a graph moti- vates the use of Graph Convolutional Networks (GCNs) for encod- ing Q().\nWe adopt a variational EM algorithm to refine the ELBO and learn the MLN weights w. In the E-step, the GCN parameters Qe are updated to minimize the KL divergence between Q (T) and Pw(T), where T = t1, t2, ..., t\u2081 are the model outputs. The optimization ob- jective is enhanced with a supervised negative log-likelihood term"}, {"title": "4.4 Uncertainty mitigation with PCS", "content": "To further improve inductive generalization, we draw upon the Pre- dictability, Computability, and Stability (PCS) framework [83, 84], which is a comprehensive approach to ensure the reliability, repro- ducibility, and transparency of data-driven results when employing complex modeling techniques. The PCS framework emphasizes three key principles: predictability, which serves as a reality check for models; computability, which considers the feasibility and scal- ability of methods; and stability, which assesses the consistency of results under perturbations to data and models. When applying KnowGraph to critical real-world tasks, uncertainty quantification helps to assess and mitigate the uncertainty associated with our results. Adding uncertainty information to KnowGraph can be valu- able for decision-making, as it allows us to identify cases where the model is less confident and may require additional investigation or human intervention.\nIn particular, the stability principle in PCS emphasizes the im- portance of assessing the stability of data results with respect to data and model perturbations. We propose incorporating a weight noise ensembling technique into the stability principle of the PCS framework. Weight noise ensembling introduces random pertur- bations to the weights of the neural network during training and inference, allowing us to capture the uncertainty in the model's parameters by considering a distribution over the weights rather than a single point estimate. This approach acts as a regularization technique, reducing overfitting in our setting with extreme label imbalance and promoting the learning of robust features that can generalize to new graphs.\nTo integrate weight noise ensembling into our pipeline within the PCS framework, we modify the statistical learning and reason- ing components of KnowGraph. Within the learning component,"}, {"title": "5 Experiments", "content": "5.1\nIntrusion detection on LANL\nIn this section, we describe our results on intrusion detection on the public Los Alamos National Labs (LANL) [36] dataset.\nDataset. We conduct experiments on a open-sourced intrusion detection dataset\u00b9 from Los Alamos National Labs (LANL) [36]. The LANL dataset contains a 58-day log within their internal computer network, among which the malicious authentication events are identified. The dataset comprises 12,425 users, 17,684 computers, and 1.6B authentication events. We model the dataset as a graph so that a node represents a server in the network and an edge represents an authentication event (e.g. log-in / log-out) from one server to another. We will remove the repeated edges in the graph so that the overall number of edges is 45M. Our goal is to detect whether an edge is malicious or not.\nLabels. Each edge in the graph has a ground-truth label by the dataset, indicating whether the authentication event is malicious or not. Only 518 malicious edges are among the 45M edges, making the dataset highly unbalanced.\nSettings and baselines. We mainly follow the data pre-processing procedure as in the Euler [39] work. We use the training and valida-"}, {"title": "5.2 Collusion detection on real-world eBay marketplace dataset", "content": "In this section, we describe the real-world eBay marketplace dataset and our collusion detection results and analysis in detail.\nDataset. We use a large-scale proprietary dataset on real-world marketplace transactions from the popular online shopping website eBay, which has more than 135 million users [76]. The dataset con- tains transactions from 40 days total, each with around 4 million transactions, collected from January to February of 2022. Each trans-action consists of three entities: a seller, a buyer, and an item. These transactions and entities are organized into a knowledge graph, where each entity is a node with bidirectional edge relationships with other entities in the same transaction. The corresponding task aims to train models that predict if a transaction indicates buyer- seller collusion, a type of fraud in which buyers and sellers conspire for illegal financial gain. This fraud can involve manipulating prices or exchanging fake feedback to deceive the marketplace, leading to significant financial losses. Due to the scale of the marketplace, this has a financial cost of millions of dollars a year based on proprietary estimates, making automated detection crucial.\nLabels. Each transaction has ground-truth collusion labels from real collusion cases and additional data based on various buyer, seller, and item features. These include relevant knowledge features"}, {"title": "6 Discussion and Conclusion", "content": "We propose KnowGraph, the first framework that integrates knowl- edge reasoning with GNNs for enhanced graph-based anomaly detection. KnowGraph combines multiple GNN models operating on different graph structures with a probabilistic logical reasoning component. We employ the PCS framework and introduce weight noise ensembling to mitigate uncertainty and improve the inductive generalization ability of our model. KnowGraph is flexible to vari- ous amounts and designs of rules, highly scalable to large graphs, and demonstrates robust performance in challenging scenarios with class imbalance and heterogeneous information. Experiments on two large-scale real-world datasets, an eBay dataset for collu- sion detection and the LANL network event dataset for intrusion detection, demonstrate KnowGraph's superior performance over state-of-the-art GNN baselines in both transductive and inductive settings. These real-world graph datasets present significant chal- lenges, such as extreme class imbalance, heterogeneous information, and the need for inductive generalization to new graphs. Know- Graph's ability to effectively address these challenges highlights the potential of integrating domain knowledge into data-driven models for high-stakes, graph-based security applications.\nKnowGraph's modular architecture paves the way for more ac- curate and interpretable graph learning systems, bridging the gap between symbolic and neural approaches to graph learning. The reasoning approach introduced in this paper opens up new oppor- tunities for leveraging domain knowledge in various real-world applications. Future research directions include exploring more ad- vanced GNN architectures and investigating alternative inference techniques. Additionally, developing more efficient and scalable reasoning components could further enhance the applicability of KnowGraph to even larger real-world graphs."}]}