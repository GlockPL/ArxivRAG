{"title": "Stress Detection on Code-Mixed Texts in Dravidian Languages using\nMachine Learning", "authors": ["L. Ramos", "M. Shahiki-Tash", "Z. Ahani", "A. Eponon", "O. Kolesnikova", "H. Calvo"], "abstract": "Stress is a common feeling in daily life,\nbut it can affect mental well-being in some\nsituations, the development of robust de-\ntection models is imperative. This study\nintroduces a methodical approach to the\nstress identification in code-mixed texts\nfor Dravidian languages. The challenge\nencompassed two datasets, targeting Tamil\nand Telugu languages respectively. This\nproposal underscores the importance of\nusing uncleaned text as a benchmark to\nrefine future classification methodologies,\nincorporating diverse preprocessing tech-\nniques. Random Forest algorithm was\nused, featuring three textual representa-\ntions: TF-IDF, Uni-grams of words, and\na composite of (1+2+3)-Grams of char-\nacters. The approach achieved a good\nperformance for both linguistic categories,\nachieving a Macro F1-score of 0.734 in\nTamil and 0.727 in Telugu, overpassing\nresults achieved with different complex\ntechniques such as FastText and Trans-\nformer models. The results underscore\nthe value of uncleaned data for mental\nstate detection and the challenges classify-\ning code-mixed texts for stress, indicating\nthe potential for improved performance\nthrough cleaning data, other preprocessing\ntechniques or more complex models.", "sections": [{"title": "Introduction", "content": "According to the World Health Organization\n(WHO), stress is characterized as a condition\nof anxiety or mental strain caused by challeng-\ning circumstances. Every individual experiences\na certain stress level, as it is an inherent reac-\ntion to threats and various stimuli. Thus, not all\nstress states are harmful; chronicity, quality, mag-\nnitude, subjective appraisal, and context of stres-\nsors are important moderators of the stress re-\nsponse, but acute and chronic stress experiences\ncan affect optimal neuroendocrine reactivity, lead-\ning to increased vulnerability of the organism to\nstressors(Agorastos and Chrousos, 2022). Social\nmedia is considered as a platform where users ex-\npress themselves. The rise of social media as one\nof humanity's most important public communi-\ncation platforms presents a potential prospect for\nearly identification and management of mental ill-\nness(Andrew, 2024). Developing resilient meth-\nods for promptly identifying human stress is cru-\ncial, and these technologies offer the potential for\nongoing stress monitoring(Li and Liu, 2020). The\nprevalence of multilingualism on the internet, and\ncode-mixed text data, has become a popular re-\nsearch topic in natural language processing (NLP).\nIt is a difficult task to handle bilingual and multi-\nlingual communication data. (Yigezu et al., 2022),\none of these tasks is hope speech detection (Ahani\net al., 2024c; Tash et al., 2024b; Arif et al., 2024)\nor hate speech detection (Zamir et al., 2024; Ahani\net al., 2024a; Tash et al., 2024a). Texts in mixed\nlanguage pose a significant challenge. Numerous\nusers seek a straightforward method to form sen-\ntences or use familiar expressions. They attempt\nto compose a text that combines two or three dif-\nferent languages, resulting in the generation of\nCode-Mix data (Tash et al., 2022). Some applica-\ntions have been developed using NLP and ML in\nDravidian languages, such as sentiment classifica-\ntion(Rashmi et al., 2021), abuse detection (Bansal\net al., 2022), hate and offensive content identifica-\ntion (Rajalakshmi et al., 2023) or hate and offen-\nsive language detection (Roy et al., 2022). There\nhave not been any recent attempts to identify stress"}, {"title": "Related Work", "content": "Stress identification has been explored using some\nML methods. (Nijhawan et al., 2022) employed\nsentiment analysis with five different labels (Joy,\nSadness, Neutral, Anger, and Fear), Latent Dirich-\nlet Allocation (LDA), and ML to detect mental\nstress in social media texts, obtained their best re-\nsults using Random Forest (RF) with a precision\nof 97.78%. Yang et al. collected texts from Twit-\nter (today X) and applied them to filter using pre-\ndefined patterns to obtain data related to stresses,\nperformed manual tagging and used several clas-\nsifiers. Lowercasing and anonymizing URLs and\nusernames was the preprocessing process for each\ntext. The 20,000 most frequent N-Grams were\nused as text representation, each word or charac-\nter sequence was replaced with a dense numeri-\ncal vector. Their best result was obtained with\nBERT reporting an F1-score of 0.86 for the neg-\native class and 0.79 for the positive class and an\naccuracy of 83.6In shared task, various teams em-\nployed diverse methodologies to tackle the de-\ntection of hope speech across both tasks. Sim-\nilar to the previous one, teams utilized a range\nof machine learning approaches, including tradi-\ntional classifiers like LR, SVM and advanced deep\nlearning techniques (Ahani et al., 2024b) such as\nTransformers and LLM-based models. The Stres-\nsIdent LT-EDI@EACL20242 shared task had the\naim to detect whether a person is affected by\nstress from their social media postings wherein\npeople share their feelings and emotions(Tash et\nal., 2024c). Given social media postings in Tamil\nand Telugu code-mixed languages, the submitted\nsystem should classify into two labels, \u201cstressed\"\nor \"not stressed\". Teams utilized diverse machine\nlearning approaches, including traditional classi-\nfiers like Logistic Regression, Support Vector Ma-\nchine and deep learning techniques such as CNN,\nTransformers and LLM-based models. In this\nworkshop, (Eponon et al., 2024) proposed an ap-\nproach for stress detection in Tamil and Telugu\nlanguages achieved a macro F1 score of 0.77 for\nTamil and 0.72 for Telugu with FastText and Naive\nBayes. (Raihan et al., 2024) tested several meth-\nods to classify stress in Tamil and Telugu lan-\nguages, from traditional ML models to Transform-\ners, but the best results were obtained using BERT-\nbased models, these models achieved a macro F1\nscore of f 0.71 for Tamil and 0.72 for Telugu. (An-\ndrew, 2024) uses GPT2 to detect stress in Tamil\nand Telugu languages. Although they used a trans-\nformer model with billions of parameters, it was\nnot possible to achieve good performance. They\nonly achieve a macro F1 score of 0.273 for Tamil\nand 0.251 for Telugu."}, {"title": "Data Description", "content": "The data that was used for this task was two\ndatasets for educational purposes, and it was gen-\nerously provided by the organizers of LT-EDI\nworkshop. The first was the Tamil dataset and the\nsecond was the Telugu dataset, each one had two\nlabels, \"Non stressed\" and \"stressed\" and was di-\nvided into the train, validation, and test datasets.\nIn Table 1 it is observable the data set distribu-\ntion for both languages. In addition, Table 2 shows\nsome samples from the Tamil and Telugu dataset."}, {"title": "Methodology", "content": "This section outlines the methodology for each\ntask. The primary objective of this proposal is to\nestablish a baseline model that utilizes text repre-\nsentation models with no cleaning steps before the\nuse of text representation models and employs the\nRF algorithm for classification. RF was selected\ndue to the distribution of the random vectors does\nnot depend on the training set, does not concen-\ntrate weight on any subset of the instances and\nthe noise effect is smaller (Breiman, 2001). The\noverview of the proposed methodology is exposed\nin Figure 1."}, {"title": "Feature Engineering", "content": "In this approach, two vectorization methods have\nbeen used for each task, TF-IDF and N-Grams of\nwords and characters. TF-IDF was selected due\nto TF-IDF considers every word's weight by using\ntwo approaches, the frequency of a term and in\nhow many file a term can be found (Hakim et al.,\n2014), The term N-Gram (of characters or words)\nrefers to a series of sequential tokens in a sentence,\nparagraph, and document, in addition the Uni-\ngrams have shown good results in code-mixed text\nclassification (Ameer et al., 2022), and Character\nN-Grams are handcrafted features which widely\nserve as discriminative features in text categoriza-\ntion, discriminating language variety, and many\nother applications (Kruczek et al., 2020). For\ncreating these representations, the scikit-learn li-\nbrary has been used with its default parameters\nin \"TfidfVectorizer\" method and specifically set\n\"ngram_range=(1,1)\u201d for Uni-grams of words and\n\"ngram_range=(1,3)\" and \"analyzer='char\" for\n(1+2+3)-Grams of characters and all others pa-\nrameters in N-Grams methods remained in their\ndefault settings."}, {"title": "Evaluation", "content": "Standard metrics such as Macro F1-score, Macro\nRecall, Macro Precision, Weighted F1-score,\nWeighted Recall, Weighted Precision, and Accu-\nracy were used to evaluate the performance."}, {"title": "Results and Discussion", "content": "The classification was performed using RF. The\ntraining was performed with the training set and\nafter an evaluation was performed using the vali-\ndation set, the evaluation over the validation data\nset is shown in Table 3 and finally, predictions\nwere made with the test set. The text was rep-\nresented using three different vectorizations: TF-\nIDF, Uni-grams of words, and (1+2+3)-Grams of\ncharacters were selected, but the highest perfor-\nmance was obtained with Uni-grams of words in\nTamil and for Telugu was TF-IDF. Table 4 shows\nthe difference with the highest F1-score reported.\nBased on the results obtained, since the best repre-\nsentation of the text is different in each task, which\nsuggests that for Tamil it is convenient a rep-\nresentation that consider individual words, while\nfor Telugu the weighting of the term frequency-\ninverse frequency of documents was more effec-\ntive, this suggests that it is due to the language\ncharacteristics. Furthermore, the results of the\nMacro F1-score, Macro Recall and Macro Pre-\ncision metrics suggest that the model is not bi-\nased toward any specific category and reflecting\nthe model's ability to generalize well."}, {"title": "Conclusion", "content": "In conclusion, this approach has proven to be able\nto identify stresses in texts using the RF algorithm\nwithout applying any cleaning techniques to the\ntext, this is valuable as it allows having a base-\nline for future approaches in stress identification\nin Tamil and Telugu, especially when consider-\ning more complex methods such as Deep Learn-\ning methods and also the importance of the un-\ncleaned data in code-mixed texts and detecting\nmental states. The results of this approach re-\nveal the difficulties in classifying code-mixed texts\nin Dravidian languages such as Tamil and Telugu\ndue to the combination of languages. In addition,\nthis paper contributes to the growth of research in\nstress identification in code-mixed texts with valu-\nable information for future research."}, {"title": "Limitations", "content": "This stress identification approach in code-mixed\ntexts in Dravidian languages has demonstrated\npromising performance in these experiments, but\nit is essential to acknowledge certain limitations\nthat warrant consideration. A notable constraint is\nthe small amount of data that is provided, in addi-\ntion to the imbalance that exists between classes.\nTherefore, the model may encounter challenges\nor facilities using other classification models and\nsome preprocessing techniques, because this ap-\nproach has no information about the impact of ap-\nplying or not applying certain preprocessing tech-\nniques on classification performance. Address-\ning these limitations is important for further en-\nhancing stress identification in code-mixed texts\nin Tamil and Telugu languages."}]}