{"title": "RIPPLE: Accelerating LLM Inference on Smartphones with Correlation-Aware Neuron Management", "authors": ["Tuowei Wang", "Zixu Hao", "Youyou Lu", "Ruwen Fan", "Minxing Huang", "Kun Li", "Ting Cao", "Yaoxue Zhang", "Ju Ren"], "abstract": "Large Language Models (LLMs) have achieved remarkable success across various domains, yet deploying them on mobile devices remains an arduous challenge due to their extensive computational and memory demands. While lightweight LLMs have been developed to fit mobile environments, they suffer from degraded model accuracy. In contrast, sparsity-based techniques minimize DRAM usage by selectively transferring only relevant neurons to DRAM while retaining the full model in external storage, such as flash. However, such approaches are critically limited by numerous I/O operations, particularly on smartphones with severe IOPS constraints. In this paper, we propose RIPPLE, a novel approach that accelerates LLM inference on smartphones by optimizing neuron placement in flash memory. RIPPLE leverages the concept of Neuron Co-Activation, where neurons frequently activated together are linked to facilitate continuous read access and optimize data transfer efficiency. Our approach incorporates a two-stage solution: an offline stage that reorganizes neuron placement based on co-activation patterns, and an online stage that employs tailored data access and caching strategies to align well with hardware characteristics. Evaluations conducted on a variety of smartphones and LLMs demonstrate that RIPPLE achieves up to 5.93\u00d7 improvements in I/O latency compared to the state-of-the-art. As the first solution to optimize storage placement under sparsity, RIPPLE explores a new optimization space at the intersection of sparsity-driven algorithm and storage-level system co-design in LLM inference.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated exceptional performance across a wide range of applications [15, 30, 44, 45, 52, 64]. Comprising millions or even billions of parameters [5, 8, 14, 28, 54, 56, 76], these models demand substantial computational and memory resources, typically available only in state-of-the-art data centers. Nonetheless, there is an increasing demand for deploying LLMs on resource-constrained devices, such as smartphones [33, 55, 61, 62, 71, 74]. On one hand, stringent privacy regulations necessitate local data processing to protect user information. On the other hand, LLMs on smartphones facilitate customization based on user habits, enabling enhanced personalization.\nGiven the limited DRAM capacity of devices, LLMs on smartphones are typically constrained to models specially designed for mobile deployment [1, 54, 72]. Although these models are lightweight, the reduction in parameters inevitably leads to a compromise in their capabilities [29]. As an alternative, many recent works [4, 37, 40, 50, 69, 81] explore the exploitation of inherent sparsity within LLMs to address memory limitations. Specially, rather than pruning model parameters, these methods selectively activate a subset of model parameters based on the input while maintaining the original performance. By transferring only the activated parameters to DRAM for computation, larger and more powerful LLMs can be stored in external flash memory, effectively surpassing DRAM limitations of smartphones.\nHowever, the efficiency of this LLM inference paradigm is significantly hindered by I/O overheads. Since different inference requests generally activate distinct sets of model parameters, frequent I/O operations are generated to swap parameters between DRAM and flash memory. As shown in\nTo this end, RIPPLE employs a two-stage solution that performs hierarchical optimizations both offline and online.\n(1) In the Offline Phase, RIPPLE clusters neurons exhibiting high co-activation correlation and reorganizes their placement in flash memory. To address Challenge (1), we abstract the problem into a complete graph, reformulating it as the discovery of the globally optimal Hamiltonian Path. By leveraging graph-theoretic techniques, we propose a greedy algorithm that efficiently searches for optimized placement based on observed neuron co-activation patterns.\n(2) In the Online Phase, RIPPLE performs fine-grained refinements based on optimized neuron placement, further enhancing access continuity. To tackle Challenge (2), we devise an IOPS-friendly access collapse technique. By strategically incorporating additional neurons between two separate neuron links, we improve read access continuity with negligible overhead. In response to Challenge (3), we design a linking-aligned in-memory caching policy. Rather than individually caching the hottest neurons, we account for their interlinking relationships, ensuring efficient access patterns.\nWe evaluate RIPPLE on three smartphones with distinct hardware configurations, benchmarking a diverse range of LLMs varying in structures and scales. The results demonstrate that RIPPLE significantly boosts on-device bandwidth, achieving improvements of up to 4.32\u00d7. Moreover, this bandwidth optimization yields substantial reductions in I/O latency during inference, offering speedups of up to 5.93\u00d7 when compared to state-of-the-art solutions.\nTo the best of our knowledge, RIPPLE is the first to accelerate LLM inference on smartphones by enhancing I/O bandwidth through optimized neuron placement in flash memory. RIPPLE effectively bridges the performance gap between flash memory and DRAM, enabling LLM inference to exceed DRAM limitations on smartphones. Our contributions can be summarized as follows:\n\u2022 We identify the primary bottleneck in LLM inference on smartphones as IOPS, attributing it to the inherent"}, {"title": "2 Background and Motivation", "content": "Mobile devices, such as smartphones, predominantly utilize Universal Flash Storage (UFS) [27] as the storage protocol. By leveraging NAND flash, UFS offers significantly larger storage capacity than the space available in DRAM, with scalability reaching terabyte (TB) levels. Furthermore, the introduction of the command queue in UFS markedly improves the efficiency of data transfer between flash and DRAM. In the latest version (UFS 4.0), the sustained read speed per lane can reach up to 2.9 GB/s. This combination of extensive storage capacity and relatively high read speed forms the foundation for the execution of LLMs on mobile devices.\nHowever, unlike server-side external storage (such as NVMe), UFS on smartphones typically features a shallow command queue, supporting only 32 entries. This limitation significantly restricts the IOPS for flash reads and can even hinder full utilization of the available bandwidth. As depicted in Figure 4, the read bandwidth increases with the continuous I/O sizes, since continuous reads can be issued by a single read operation. Specially, when the continuous I/O size is less than 24KB, the bandwidth scales almost linearly with the I/O size, indicating that these reads are primarily IOPS-bound. Consequently, the key to fully exploiting UFS bandwidth lies in maximizing the continuity of read accesses."}, {"title": "2.1 Activation Sparsity in LLM Inference", "content": "Numerous studies [34, 41, 49, 51, 77] have shown that LLMS exhibit considerable Activation Sparsity, allowing a substantial portion of activations can be disregarded without impacting the final outputs. This characteristic greatly reduces resource consumption, as only a subset of parameters participates in the computation. Importantly, since no parameters are pruned, the full capacity of the LLMs remains intact.\nAlthough prevalent across transformer-based LLMs [37, 59], this activation sparsity is particularly pronounced when the ReLU-family [2, 47, 78] functions are employed. As depicted in Figure 2, the ReLU function zeros out all negative values in activations A, leading to the exclusion of the corresponding neurons (e.g., rows of the up-projection matrix U and columns of the down-projection matrix D) from the computation without any loss. Consequently, recent research efforts [41, 49-51, 69] explore replacing activation functions with ReLU across popular LLMs, achieving high sparsity while maintaining comparable model performance.\nCompared to data centers, this property becomes even more critical when deploying LLMs on resource-constrained devices like smartphones. On one hand, smartphones typically offer limited DRAM capacity, ranging from 10GB to 20GB. On the other hand, a substantial portion of this DRAM is allocated by the operating system and other active applications, leaving even less available for any single application. In contrast, merely storing an LLM with 7 billion parameters in half precision requires at least 14GB of DRAM.\nFigure 3 presents a typical procedure for deploying LLMs on smartphones with activation sparsity [4, 69]. Rather than relying solely on limited DRAM, larger flash memory can be used to store model parameters. The process begins by effectively predicting the activated neurons using a neural-network-based predictor. Next, the activated neurons are selectively loaded into DRAM for final computation. This approach enables the execution of models that surpass the available DRAM size. However, the I/O bandwidth between flash memory and DRAM emerges as the new bottleneck."}, {"title": "2.2 Universal Flash Storage on Smartphones", "content": "The design of RIPPLE is rooted in Neuron Co-Activation, a fundamental property inherent in LLM activation sparsity. As illustrated in Figure 6, neurons within LLMs exhibit strongly correlated activation patterns across various model structures and datasets. Although similar observations have been validated in prior studies [4, 69], this property remains largely underexplored due to its intrinsic complexity. By employing both algorithmic and system-level optimizations, RIPPLE is the first to leverage neuron co-activation for optimizing neuron placement in flash, effectively mitigating the I/O bottleneck during LLM inference on smartphones.\n Offline Correlation-Aware Clustering (\u00a7 4). RIPPLE begins with the identification of an optimized neuron placement in flash by clustering co-activated neurons. Specially, the process consists of three steps. \u25cf Pattern Extraction. We develop distinct strategies to extract co-activation patterns in transformer-based LLMs efficiently. These extracted patterns quantify the strength of co-activation correlations among neurons, forming the foundation for subsequent neuron rearrangement. Problem Abstraction. We model the process of identifying optimal neuron placement as a graph representation. Leveraging this abstraction, we reformulate the problem into a Hamiltonian Pathfinding task, enabling a more efficient solution through graph-theoretic techniques. Hamilton Pathfinding. Given the NP-hard nature of the optimized problem, we devise a heuristic algorithm that employs a greedy approach. We prove strictly that our algorithm can search for the locally optimal solution with the time complexity of O(n\u00b2 log n).\n Online Continuity-Centric Processing (\u00a7 5). To efficiently align with the optimized neuron placement, RIPPLE adopts customized data access and DRAM management techniques, Specially designed to facilitate more continuous read access. Access Collapse. Despite optimized neuron placement,"}, {"title": "2.3 Analysis: IOPS as the Bottleneck", "content": "By storing the full model parameters in flash and selectively transferring only the activated parameters to DRAM for computation, mobile devices can accommodate larger and more powerful models while maintaining the resource demands of running smaller models. However, this approach is severely constrained by the data transfer overhead. As shown in Table 1, I/O operations between flash and DRAM account for the majority of the inference latency. Consequently, the efficiency of I/O operations emerges as the pivotal determinant of the smooth execution of this process.\nThe root cause of this I/O overhead lies in the dynamic nature of activation sparsity, where the specific subset of neurons that is activated changes with the inputs. As a result, each inference request necessitates loading a unique set of neurons from flash into DRAM, leading to considerable data transfer overhead. More critically, overlapping these data transfers with computation proves challenging, as the prediction of activated neurons is contingent on the inputs from the current or adjacent layers. The amount of computation available is insufficient to hide the substantial I/O latency.\nTo address this, prior work predominantly seeks to mitigate the volume of data loaded through optimized data management techniques. However, due to the minimal overlap in activated neurons across different inference requests, the efficiency remains suboptimal. Our findings reveal that the bottleneck in I/O operations stems not from the volume of data transferred but from the low effective bandwidth utilization. We evaluate the inference latency of OPT-350M with different activation sparsity ratios, as depicted in Figure 5. Despite the reduced data transfer, the inference latency with activation sparsity approaches, or even surpasses, that of dense models. This is because the scattered nature of neuron activation in conventional model-structure-based neuron placement results in numerous small-grained read accesses. This causes the device to become heavily IOPS-bound, preventing the full exploitation of the available bandwidth.\nDrawing from these observations, we derive a crucial insight: the conventional neuron placement, guided by model structure, is misaligned with the dynamic activation sparsity utilized in on-device LLM inference. As a result, the key to addressing the I/O bottleneck lies in ensuring that read accesses to flash are as continuous as possible, thereby pushing the bandwidth towards full exploitation."}, {"title": "3 RIPPLE Overview", "content": "We propose RIPPLE, an efficient approach for accelerating LLM inference on smartphones through advanced I/O optimizations. While previous studies primarily focus on the efficiency of either computation or memory management, RIPPLE tackles the I/O bottleneck by directly improving the neuron transfer bandwidth between flash and DRAM."}, {"title": "4 Offline Correlation-Aware Clustering", "content": "4.1 Step 1: Parameter-Efficient Pattern Extraction\nLLMs [8, 28, 56, 76] are typically based on transformer architectures, with two primary components: Multi-Head Attention (MHA) block and Feed Forward Network (FFN) block. To enhance inference efficiency, an increasing number of LLMs are adopting the Group Query Attention [3] mechanism, which significantly minimizes the parameter overhead of the MHA block. As a result, in RIPPLE, we focus primarily on offloading the parameters of the FFN block to flash memory, while prefetching all parameters within the MHA block. Nonetheless, this approach can similarly be exploited to optimize the offloading of the MHA block itself.\nTo extract the neuron co-activation patterns, we initially utilize an Adjacency Matrix to record the activation frequencies of neurons within LLMs. This step is performed only once, prior to inference, utilizing a dataset associated with the upcoming tasks. By interpreting frequency f as a probability, we compute the probability of the activation of neuron $n_i$, denoted as P(i), and the probability of the co-activation of neuron $n_i$ and $n_j$, denoted as P(ij), as follows:\n$P(i) = \\frac{f(n_i)}{\\sum_{k=1}^{N} f(n_k)}$\n$P(ij) = \\frac{f(n_i, n_j)}{\\sum_{k=1}^{N} \\sum_{l=1}^{N} f(n_k, n_l)}$\nHere, N denotes the number of neurons in a weight matrix. When performing statistics, RIPPLE accounts for the binding relationships between neurons across different weight matrices within the same FFN block. For instance, in OPT [76], the columns of up projection matrix are bound to the corresponding rows of down projection matrix, as their activations rely on whether the same intermediate values are zero or not. A similar binding relationship exists among the gate, up, and down projection matrices in Llama2 [56]."}, {"title": "4.2 Step 2: Graph-Based Problem Abstraction", "content": "Following the extraction of the neuron co-activation patterns, the subsequent step involves determining an optimized neuron placement within the flash. To enable more continuous read access, neurons that frequently co-activate should ideally be positioned in close proximity. Given the immense number of potential neuron placements, a brute-force enumeration approach is infeasible. Innovatively, we reformulate this problem as a graph representation, allowing for a more efficient solution leveraging graph-theoretic techniques.\nGraph Abstraction. We abstract the neuron co-activation relationships into a Complete Graph. In this graph, each node represents a neuron, and each edge captures the co-activation between a pair of neurons. Specially, the value assigned to each edge reflects the strength of their co-activation correlation, referred to as the Distance Between Two Neurons. We prove that the optimal definition of the distance between neuron $n_i$ and neuron $n_j$ is given as follows:\ndist($n_i$, $n_j$) := 1 \u2212 P(ij)\nProof. The objective of RIPPLE is to minimize the expected number of I/O operations during a single inference request. Initially, consider that all neurons are activated individually. Therefore, the expected number of I/O operations, Nindiv, can be expressed as:\n$N_{indiv} = \\sum_{i=1}^{N} P(i)$\nNext, we account for the co-activation of neurons. When neurons $n_i$ and $n_j$ are co-activated, both can be accessed with a single I/O operation. Therefore, the expected number of I/O operations, Ncoact, is given by:\n$N_{coact} = \\sum_{i=1}^{N} P(i) - \\sum_{i=1}^{N}\\sum_{j=1}^{N} P(ij)$\nGiven that the first term remains constant, minimizing Ncoact is equivalent to maximizing the second term, $\\sum_{i=1}^{N}\\sum_{j=1}^{N} P(ij)$. By defining the distance between neuron $n_i$ and neuron $n_j$ as 1 - P(ij), this problem can be formulated as identifying the shortest Hamiltonian path [48] in a complete graph.\nHamilton Pathfinding. The Hamiltonian path ensures that all neurons stored within the flash are involved, while minimizing the path length maximizes the likelihood of clustering co-activated neurons together. We prove that this problem can be further reduced to the classical Traveling Salesman Problem (TSP) [31] as follows:\nLemma 4.1. The global shortest Hamiltonian path problem (i.e., finding the shortest path between any pair of nodes) can be reduced to the TSP in polynomial time.\nProof. Define Problem A as: Find the shortest Hamiltonian path in a complete graph from node x to node y, denoted as f(x, y). By iterating over all possible node pairs x and y, it is evident that the global shortest Hamiltonian path problem can be reduced to solving Problem A for all node pairs. This can be achieved in O(n\u00b2) time using dynamic programming, where n is the number of nodes in the graph.\nOn the other hand, the TSP is defined as: Find the shortest Hamiltonian cycle in a complete graph starting and ending at a node x, denoted as g(x). The relationship between Problem A and the TSP can be established as follows:\n$g(x) = min \\sum_{i=1}^{n} f (x, i) + dist(x, i)$\nThis expression shows that Problem A can be further reduced to the TSP in O(n) time by minimizing over n 1 possible nodes. Therefore, the global shortest Hamiltonian path problem can be reduced to the TSP in polynomial time."}, {"title": "4.3 Step 3: Heuristic-Driven Greedy Algorithm", "content": "Although the problem can be reduced into the classical TSP, which is known to be NP-hard [57], finding an optimal solution in polynomial time is generally infeasible. We propose a heuristic algorithm that searches for the optimal neuron placement greedily, as shown in Figure 8"}, {"title": "5 Online Continuity-Centric Processing", "content": "Through offline correlation-aware clustering, neurons that are frequently co-activated are strategically placed contiguously in flash memory. However, the dynamic and intricate nature of neuron co-activation complicates static neuron placement, making it inadequate to entirely alleviate IOPS limitations. To fully exploit the flash and DRAM resources to serve neuron read requests, we design specific online serving techniques, aiming to address the two primary categories of these challenges, manifesting in data access and caching.\nThe first challenge arises from Random Activation Variation. Due to the stochastic nature of neuron activation, it is infeasible to consistently follow the extracted co-activation patterns. Although neurons that are frequently co-activated are placed in close positions, minor variances induced by randomness can still lead to discontinuous read access.\nThe second challenge stems from Misaligned Cache Strategy. Conventional cache mechanisms often fail to adapt to co-activation scenarios. Simply caching hot neurons lost the information on neuron placement, breaking the continuity of neurons in flash, which is against our optimizations for I/O operations reduction. However, directly continuously caching all co-activated neurons may take up too much cache space, wasting cache efficiency."}, {"title": "5.1 IOPS-Friendly Access Collapse", "content": "In RIPPLE, we introduce an innovative online technique that strategically combines nearby read accesses. The fundamental insight driving this approach is that while co-activated neurons cannot always be placed contiguously, they are likely to be positioned in close proximity following offline correlation-aware clustering. As illustrated in Figure 9, consider a scenario where neurons n1, n2, n3, and n4 are stored contiguously, but occasionally only n1, n2, and n4 are activated, necessitating two distinct read operations. However, when IOPS is limited, the inclusion of more neurons per read operation yields superior overall performance. Capitalizing on this observation, when two disjoint but proximate neuron groups are co-activated, we speculatively read the intervening neurons. This strategy effectively coalesces the two separate neuron groups into a single, contiguous read access, thereby substantially enhancing overall efficiency.\nThe execution of this IOPS-friendly access collapse is governed by two key factors during runtime. (1) Extra bandwidth cost. Introducing additional neurons for merging involves a trade-off between increasing the data transfer size and decreasing IO operations, aiming to enhance bandwidth utilization. We employ a threshold-based approach: if the number of neurons between two neuron groups falls below a predefined threshold, collapse is performed; otherwise, skipped. This threshold is dynamically adjusted during runtime to balance the overhead and efficiency gains. (2) Storage Bottleneck. While merging can reduce IO operations, it only improves bandwidth efficiency if the storage is IOPS-bound rather than bandwidth-bound. To handle this, we implement an online bottleneck detector that periodically checks whether the achieved bandwidth has reached the hardware's maximum capacity. If the bandwidth is fully utilized, the system defaults to the original read strategy."}, {"title": "5.2 Linking-aligned Cache Policy", "content": "It is natural to store the neurons that are most frequently activated in DRAM to reduce the redundant data transfer between flash and DRAM. However, directly applying the existing cache policy is inefficient, since all these policies are performed at the level of neuron individuals, which ignores the co-activation pattern and neuron placement in flash. Therefore, we added a layer of access management to the existing cache to achieve linkage with Access Collapse and further improve system efficiency. For example, neurons A, B, C, and D are stored together and often co-activate together, but if B is hotter than others, B has a higher probability of being cached, which may cause discontinuous read operations. A further idea is that cache the neurons stored continuously in the flash together to reduce the occurrence of the above situation, but this will take up lots of cache space at once, which is not worth the loss.\nIn RIPPLE, activated neurons are divided into two categories, sporadic neurons and continuous segments. Sporadic neurons, as its name, refer to those neurons being co-activated with few surrounding neurons. Continuous segments consist of a series of neurons that are activated together in succession. For sporadic neurons, RIPPLE cache them as usual. But RIPPLE cache these continuous segments with a lower probability than sporadic neurons. This is mainly because, first of all, caching continuous segments requires more memory resources and brings more limited benefits. If some neurons in these continuous segments are evicted and some remain in the cache, this will lead to discontinuous reads on the flash. Although the waste of IOPS is alleviated by Access Collapse, DRAM resources are wasted. Our cache policy also cooperates well with the state-of-the-art cache design. Since we only control the caching admitting policy, yet leave the other unchanged."}, {"title": "6 Evaluation", "content": "Hardware. We conduct evaluations across a diverse set of smartphones, as detailed in Table 2. This broad spectrum underscores the wide applicability and robustness of RIPPLE across different hardware platforms.\nModels. We choose models from three widely adopted LLM families [28, 56, 76] for evaluation, as outlined in Table 3. For Llama2 and Mistral, we utilize their ReLU variants [49, 51], which offer an effective balance between sparsity and performance. These LLMs exhibit diverse model architectures, parameter sizes, and sparsity ratios, thereby offering comprehensive benchmarks on RIPPLE.\nDatasets. We evaluate RIPPLE using three real-world datasets, each representing a diverse set of content and linguistic structures. Alpaca [53] offers task-specific instruction data, Open-WebText [21] captures web-scale information, and Wiki-Text [39] provides formal, encyclopedic content. For each dataset, we collect 1,000 sentences to extract the neuron co-activation patterns during the offline stage.\nBaselines. We benchmark RIPPLE with two state-of-the-art LLM inference frameworks for smartphone deployment. The first, Llama.cpp [19], is the most widely used LLM inference framework and currently the fastest, supporting offloading of model parameters to flash storage. The second baseline is LLM in a Flash (LLMFlash) [4], the representation of current methods on on-device LLM inference. Although it is not open-source, we port it into Llama.cpp by integrating its key I/O optimizations, such as row-column bundling. While our evaluation primarily focuses on I/O efficiency between flash and DRAM, we integrate a high-performance cache, S3-FIFO [70] into all baselines and maintain a DRAM cache ratio of 0.1 during the comparison.\nMetrics. Latency remains the most critical concern in mobile scenarios. Consequently, our primary performance metric"}, {"title": "6.2 Overall Performance", "content": "Latency. As depicted in Figure 10(a), we evaluate the I/O latency per token and the corresponding speedup of RIPPLE on OnePlus 12. The results indicate that RIPPLE effectively mitigates the I/O bottleneck during LLM inference on smartphones, yielding speedups of up to 5.93\u00d7 over Llama.cpp and 3.23\u00d7 over LLMFlash. For OPT models, which exhibit high sparsity, RIPPLE achieves an average speedup of 2.23\u00d7 over LLMFlash across all model sizes and datasets. For denser models like Llama2-7B and Mistral-7B, optimizing I/O operations becomes much more challenging. However, with IOPS-oriented techniques, RIPPLE still achieves speedups of 13.8% and 10.2% over LLMFlash.\nEffective Bandwidth. As shown in Figure 10(b). Consistent with the observed latency results, RIPPLE demonstrates a marked enhancement in bandwidth, achieving improvements of up to 4.32\u00d7 and 2.13\u00d7 over both two baselines, respectively. These gains primarily come from a substantial reduction in I/O operations due to more continuous access. By alleviating the device from IOPS constraints, the overall bandwidth utilization is boosted."}, {"title": "6.3 Performance Breakdown", "content": "Figure 11 presents a detailed performance analysis of RIPPLE. The evaluation is conducted on two LLM families, OPT and Llama2, with OPT model sizes ranging from 350M to 6.7B. Using the strongest baseline, LLMFlash, as the start point, the results demonstrate that the offline and online stages of RIPPLE yield average performance improvements of 1.30x and 1.26\u00d7, respectively, underscoring the effectiveness of both stages. By closely integrating both stages, RIPPLE achieves cumulative speedups of 1.68\u00d7 on average across all five models."}, {"title": "6.4 Offline Ablation Study", "content": "Continuous Access. The core insight of RIPPLE lies in optimizing bandwidth utilization by significantly maximizing continuous read access to flash. As depicted in Figure 12, we evaluate the lengths of read accesses in both RIPPLE and LLMFlash. Prior to optimization, the read access lengths remain below 10 neurons, with averages of 1.05 and 1.10 across the two models. In contrast, RIPPLE exhibits a marked improvement, with read access length increasing by 213% and 160%, respectively. Remarkably, the maximum continuous read access length reaches up to 620 in OPT and 344 in Llama. This considerable improvement in continuous read access facilitates full utilization of available bandwidth.\nOverhead Analysis. We measure the time cost of executing the offline search algorithm in RIPPLE. Since the time complexity is primarily determined by the number of activated neurons, we perform evaluations across various datasets and model sizes. To expedite the search process, we implement parallel computation by exploiting the independence of different model layers. Table 4 indicates that all search processes complete within a few minutes, even for the largest 13B model. With the theoretical time complexity is O(n\u00b2 log n), the growth of time cost is modest. Given that this search process is required only once, the overhead is negligible compared to the subsequent inference process."}, {"title": "6.5 Online Ablation Study", "content": "Access Collapse. Figure 13 shows the effectiveness of Access Collapse. For both OPT-6.7B and Llama2-7B, the effective bandwidth for neuron transfer increases due to the optimized trade-off between data volume and IO operations. In the OPT-6.7B and Llama2-7B models, the Access Collapse strategy brings an effective bandwidth improvement of 1.21\u00d7 and 1.09\u00d7 respectively. This optimization successfully shifts the bottleneck from IOPS to bandwidth on smartphones, resulting in enhanced overall performance.\nCache Ratio. We compare the baseline with different cache ratios to show the memory savings of RIPPLE. Figure 14 presents the latency comparison when caching various ratios of neurons in DRAM. The results indicate that, although"}, {"title": "6.6 Sensitivity Analysis", "content": "In this section, we examine the sensitivity of RIPPLE to several key factors, including inputs, hardware and precision.\nSensitivity on Inputs. In the offline stage, RIPPLE optimizes neuron placements in flash based on co-activation patterns extracted from the preprocessed dataset. Figure 15 illustrates the I/O performance of RIPPLE when inference inputs are sourced from a different dataset. The results reveal that the neuron placements determined offline remain effective when inputs are biased. This suggests that neuron co-activation patterns may be an intrinsic property of the model itself, with input variations exerting limited influence.\nSensitivity on Hardware. Figure 16 shows the I/O performance of RIPPLE on smartphones with varying hardware configurations. Compared to the OnePlus 12 (OP 12), the OnePlus Ace3 (OP Ace3) share the same storage but features a less powerful SoC, while the OnePlus Ace2 (OP Ace2) has both weaker UFS storage and SoC. The results show that the performance of OP 12 and OP Ace3 is comparable, indicating that storage is a more critical factor than the SoC. In contrast, OP Ace2 exhibits roughly half the performance of the other two, aligning with the hardware bandwidth limitations, as shown in Figure 4.\nSensitivity on Precision. For each model, the neuron dimension is fixed, and lower precision results in a smaller neuron size. Figure 17 presents the per-token latency across varying floating-point precision. The result demonstrates that RIPPLE Scales efficiently with data format precision, maintaining consistent performance across three models."}, {"title": "7 Related Works", "content": "Model Parameter Reduction. To alleviate the memory and computational burdens of LLM execution, substantial efforts have focused on the reduction of model parameters, with two principal approaches emerging. The first, Model Pruning [35, 36], seeks to reduce the number of model parameters while ensuring minimal performance degradation. Several works [23, 24, 26, 32, 75] have explored static pruning, where parameters are pruned offline, prior to inference. In contrast, dynamic sparsity methods [7, 12, 22, 25, 46, 67] determines which parameters to prune during runtime, enabling seamless integration with training or inference. Different from these pruning techniques, RIPPLE exploits the Activation Sparsity inherent in LLMs, retaining all model parameters but enhancing resource efficiency by selectively activating only a subset. This approach preserves the model's generalization ability, particularly critical in the context of LLMs.\nAnother one is Model Quantization [11, 20], which reduces the precision of model parameters by optimizing the utilization of available bits to encode model information more efficiently. Numerous studies have driven precision progressively lower, with efforts ranging from 8-bit [13, 66] to 4-bit [16, 73], 2-bit [10], and even 1-bit [58, 68]. However, as precision decreases, the resulting data access patterns become increasingly fine-grained, leading to more scattered access. This, in turn, heightens the significance of RIPPLE.\nSparse Computation Optimization. Sparse linear algebra often falls short in performance compared to its dense counterparts, primarily due to its inherent irregular computation patterns and scattered memory accesses. Many works has focused on optimizing computation under sparsity patterns. Several compiler-based techniques, such as SparTA [80] and SparseRT [60], are tailored for static sparsity patterns, while others, including Sputnik [17],cuSPARSE [42],PiT [79], Flash-LLM [65], provide support for more general sparsity patterns. Recently, an increasing number of hardware solutions [6, 9, 18, 63] have been specially designed to accelerate sparsity computations, including NVIDIA's Sparse Tensor Core [43]. Although these advancements significantly enhance the sparse computation efficiency, the I/O bottleneck in on-device LLM inference has become increasingly pronounced. Complementary to these efforts, RIPPLE addresses this I/O bottleneck with neuron co-activation linking.\nActivation Sparsity Application. Many recent works have begun leveraging activation sparsity to reduce the resource demands of LLM inference. For instance, Deja Vu [37] pioneered a predictor-based approach for sparsity-based LLM inference, greatly reducing inference latency. Building upon this, Powerinfer [50] exploits this property to enable LLM execution on consumer-grade GPUs by offloading model parameters to CPU. Particularly in mobile scenarios, LLM in a Flash [4] first proposes using flash on smartphones for model offloading. Powerinfer2 [69] extends this approach further, serving a 47B LLM on a smartphone. However, these methods primarily concentrate on optimizing DRAM management and overlapping computation with data transfers, achieving only limited bandwidth improvements. RIPPLE complements these efforts by directly enhancing the neuron transfer bandwidth, an optimization that can integrate with existing techniques to accelerate LLM inference on smartphones."}, {"title": "8 Conclusion", "content": "We propose RIPPLE, an efficient approach to accelerating LLM inference on smartphones through I/O optimizations. Leveraging neuron co-activation, RIPPLE notably reorganizes neuron placement within flash to facilitate more continuous read access, shifting the primary performance bottleneck from IOPS to bandwidth. This work unveils a novel optimization space at the intersection of sparsity-driven algorithm and storage-level system co-design in LLM inference."}]}