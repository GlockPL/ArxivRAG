{"title": "CALE: Continuous Arcade Learning Environment", "authors": ["Jesse Farebrother", "Pablo Samuel Castro"], "abstract": "We introduce the Continuous Arcade Learning Environment (CALE), an extension of the well-known Arcade Learning Environment (ALE) [Bellemare et al., 2013]. The CALE uses the same underlying emulator of the Atari 2600 gaming system (Stella), but adds support for continuous actions. This enables the benchmarking and evaluation of continuous-control agents (such as PPO [Schulman et al., 2017] and SAC [Haarnoja et al., 2018]) and value-based agents (such as DQN [Mnih et al., 2015] and Rainbow [Hessel et al., 2018]) on the same environment suite. We provide a series of open questions and research directions that CALE enables, as well as initial baseline results using Soft Actor-Critic. CALE is available as part of the ALE at https://github.com/Farama-Foundation/Arcade-Learning-Environment.", "sections": [{"title": "Introduction", "content": "Generally capable autonomous agents have been a principal objective of machine learning research, and in particular reinforcement learning, for many decades. General in the sense that they can handle a variety of challenges; capable in that they are able to \"solve\" or perform well on these challenges; and they are able to learn autonomously by interacting with the system or problem by exercising their agency (e.g. making their own decisions). While deploying and testing on real systems is the ultimate goal, researchers usually rely on academic benchmarks to showcase their proposed methods. It is thus crucial for academic benchmarks to be able to test generality, capability, and autonomy.\nBellemare et al. [2013] introduced the Arcade Learning Environment (ALE) as one such benchmark. The ALE is a collection of challenging and diverse Atari 2600 games where agents learn by directly playing the games; as input, agents receive a high dimensional observation (the \u201cpixels\u201d on the screen), and as output they select from one of 18 possible actions (see Section 2). While some research had already been conducted on a few isolated Atari 2600 games [Cobo et al., 2011, Hausknecht et al., 2012, Bellemare et al., 2012], the ALE's significance was to provide a unified platform for research and evaluation across more than 100 games. Using the ALE, Mnih et al. [2015] demonstrated, for the first time, that reinforcement learning (RL) combined with deep neural networks could play challenging Atari 2600 games with super-human performance. Much like how ImageNet [Deng et al., 2009] ushered in the era of Deep Learning [LeCun et al., 2015], the Arcade Learning Environment spawned the advent of Deep Reinforcement Learning.\nIn addition to becoming one of the most popular benchmarks for evaluating RL agents, the ALE has also evolved with new extensions, including stochastic transitions [Machado et al., 2018], various game modes and difficulties [Machado et al., 2018, Farebrother et al., 2018], and multi-player support [Terry and Black, 2020]. What has remained constant is the suitability of this benchmark for testing generality (there is a wide diversity of games), capability (many games still prove challenging for most modern agents), and agency (learning typically occurs via playing the game)."}, {"title": "From Atari VCS to the Arcade Learning Environment", "content": "The Atari Video Computer System (VCS), later renamed the Atari 2600, is a pioneering gaming console developed in the late 1970s that aimed to bring the arcade experience to the home. Game designers had to operate under a variety of constraints, including writing code that could execute in time with the electron beam displaying graphics on the CRT screen and rendering graphics using the limited set of primitives provided by the system. Although designed to support a variety of controllers, the majority of games were played with an Atari CX10 \"digital\" controller (see left panel of Figure 1). Players move a joystick along two axes to trigger one of nine discrete events (corresponding to three positions on each axis) on the Atari VCS. Combined with a \u201cfire\u201d button, this results in 18 possible events the user could trigger."}, {"title": "CALE: Continuous Arcade Learning Environment", "content": "The original Atari CX10 controller (left panel of Figure 1) used a series of pins to signal to the processor when the joystick is in one of nine distinct positions, visualized in the \u2018Discrete' sub-panel in Figure 1 [Sivakumaran, 1986]. When combined with a boolean \"fire\" button, this results in 18 distinct joystick events. Indeed, player control in the Stella emulator is built on precisely these distinct events [Mott et al., 1996], and they also correspond to the 18 actions chosen by the ALE.\nHowever, although the resulting events are discrete, the range of joystick motion available to players is continuous. We add this capability by introducing the Continuous Arcade Learning Environment (CALE), which switches from a set of 18 discrete actions to a three-dimensional continuous action space. Specifically, we use the first two dimensions to specify the polar coordinates $(r, \\theta)$ in the unit circle corresponding to all possible joystick positions, while the last dimension is used to simulate pressing the \"fire\" button. Concretely, the action space is $[0,1] \\times [-\\pi,\\pi] \\times [0,1]$. The implementation of CALE is available as part of the ALE at https://github.com/Farama-Foundation/Arcade-Learning-Environment (under GPL- 2.0 license). See Appendix A for usage instructions.\nAs in the original CX10 controller, this continuous action space still needs to trigger discrete events. For this, we use a threshold $\\tau$ to demarcate the nine possible position events the joystick can trigger. Figure 1 illustrates these for varying values of $\\tau$, as well as the different events triggered when the joystick is at position $(r, \\theta) = (0.61, 2.53)$. As can be seen, lower values of $\\tau$ result in more sensitive control, while higher values can result in less responsive controllers, even to the point of completely occluding certain events (the corner events are unavailable when $\\tau = 0.9$, for example).\nIt is worth highlighting that, since CALE is essentially a wrapper around the original ALE, it is only changing the agent action space. Since both discrete and continuous actions ultimately trigger the same events, the underlying game mechanics and learning environment remain unchanged. This is an important point, as it means that we now have a unified benchmark on which to directly compare discrete and continuous control agents.\nAn important difference is that the ALE supports \u201cminimal action sets\", which reduce the set of available actions from 18 to the minimum required to play the game. For example, in Breakout only the LEFT, RIGHT, and FIRE events have an effect on game play, resulting in a minimal set of 4 actions. By default, minimum action sets are enabled in the ALE and used by many existing"}, {"title": "Baseline results", "content": "We present a series of baseline results on CALE using the soft actor-critic agent [SAC; Haarnoja et al., 2018]. SAC is an off-policy continuous-control method that modifies the standard Bellman backup with entropy maximization [Ziebart et al., 2008, Ziebart, 2010]. DQN and the agents derived from it are also off-policy methods, thereby rendering SAC a more natural choice for this initial set of baselines than other continuous control methods such as PPO. We use the SAC implementation and experimental framework provided by Dopamine [Castro et al., 2018]. We detail our experimental setup and hyper-parameter selection below, and provide further details in Appendix C."}, {"title": "Experimental setup", "content": "We use the evaluation protocol proposed by Machado et al. [2018]. Namely, agents are trained for 200 million frames with \"sticky actions\" enabled, 4 stacked frames, a frame-skip of 4, and on 60 games. Additionally, we use the Atari 100k benchmark introduced by \u0141ukasz Kaiser et al. [2020], which evaluates agents using only 100,000 agent interactions (corresponding to 400,000 environment steps due to frame-skips) over 26 games. The Atari 100k benchmark has become a popular choice for evaluating the sample efficiency of RL agents [D'Oro et al., 2023, Schwarzer et al., 2023]. We follow the evaluation protocols of Agarwal et al. [2021] and report aggregate results using interquartile mean (IQM), with shaded areas representing 95% stratified bootstrap confidence intervals. All experiments were run on P100 GPUs; the 200M experiments took between 5-7 days to complete training, while the 100K experiments took between 1 and 2 hours to complete."}, {"title": "Threshold selection", "content": "As mentioned in Section 3, the choice of threshold $\\tau$ affects the overall performance of the agents. Consistent with intuition, Figure 2 demonstrates that higher values of $\\tau$ result in degraded performance. For the remaining experimental evaluations we set $\\tau$ to 0.5. This choice has consequences for SAC, due to the way its action outputs are initialized, which we discuss in the next subsection."}, {"title": "Network architectures", "content": "Given an input state $x \\in \\mathcal{X}$, the neural networks used by actor-critic methods usually consist of an \u201cencoder\" $\\phi: \\mathcal{X} \\rightarrow \\mathbb{R}^d$, and actor and critic heads $\\psi_A: \\mathbb{R}^d \\rightarrow \\mathcal{A}$ and $\\psi_c: \\mathbb{R}^d \\rightarrow \\mathbb{R}$, respectively, where $\\mathcal{A}$ is the (continuous) action space. Typically the action outputs are assumed to be Gaussian distributions with mean $\\mu$ and standard deviation $\\sigma$. Thus, for a state $x$, the value of the state is $\\psi_c(\\phi(x))$ and the action selected is distributed according to $\\psi_A(\\phi(x))$."}, {"title": "Exploration strategies", "content": "Due to its objective including entropy maximization and the fact that the actor is parameterized as a Gaussian distribution, SAC induces a natural exploration strategy obtained by sampling from $\\psi_A$ (and simply using $\\mu$ when acting greedily). We refer to this as the standard exploration strategy. However, the exploration strategy typically used on the ALE is $\\epsilon$-greedy, where actions are chosen randomly with probability $\\epsilon$; a common choice for ALE experiments is to start $\\epsilon$ at 1.0 and decay it to 0.01 over the first million environment frames. For our continuous action setup we sample uniformly randomly in $[0, 1] \\times [-\\pi, \\pi] \\times [0, 1]$ with probability $\\epsilon$. Perhaps surprisingly, standard outperforms $\\epsilon$-greedy exploration in the 200 million training regime, as demonstrated in Figure 4. This may be due to the way the action outputs are parameterized, and merits further inquiry."}, {"title": "Comparison to existing discrete-action agents", "content": "We compare the performance of our SAC baseline against DQN in the 200 million training regime, given that both are off-policy methods which have similar value estimation methods; for the 100k training regime we compare against Data-Efficient Rainbow [DER; Van Hasselt et al., 2019], a popular off-policy method for this regime that is based on DQN. As Figure 6 shows, SAC dramatically under-performs, relative to both these methods. While there may be a number of reasons for this, the most likely one is the fact that SAC was not tuned for CALE, whereas both DER and DQN were tuned specifically for the ALE.\nWe additionally compared to a version of SAC with a categorical action parameterization which allows us to run it on the original ALE. The hyper-parameters (listed in Appendix C) are based on"}, {"title": "Comparison to other continuous control environments", "content": "The most commonly used continuous control methods are centered around robotics tasks such as locomotion [Todorov et al., 2012, Wo\u0142czyk et al., 2021, Khazatsky et al., 2024], where transition"}, {"title": "Research directions", "content": "Since its release, the Arcade Learning Environment has been extensively used by the research community to explore fundamental problems in decision making. However, most of this research has focused specifically on value-based methods with discrete action spaces. On the other hand, many of the challenges presented by the ALE, such as exploration and representation learning, are not always central to existing continuous control benchmarks (see discussion in Section 5). In this section, we identify several research directions that the CALE facilitates. While many of these questions can be explored in different environments, the advantage of the CALE is that it has a direct analogue in the ALE, thereby enabling a more direct comparison of continuous- and discrete-control methods.\nExploration As discussed in Section 4.4, $\\epsilon$-greedy is the default exploration strategy used by discrete-action agents on the ALE. Despite the existence of a number of more sophisticated methods, Taiga et al. [2020] argues that these were over-fit to well-known hard exploration games such as Montezuma's Revenge; they demonstrated that, when aggregating with easier exploration games, $\\epsilon$-greedy out-performs the more sophisticated methods. In contrast, the results in Section 4.4 demonstrate that $\\epsilon$-greedy under-performs simply sampling from $\\mu$ in SAC. This may be an instance of policy churn, which has been shown to have implicit exploratory benefits in discrete-action agents [Schaul et al., 2022]. Interestingly, our results show that for SAC-D (SAC with discrete actions explored in Section 4.5), $\\epsilon$-greedy outperforms sampling from the categorical distribution for"}, {"title": "How to run CALE", "content": "CALE is included as of version 0.10 of the Arcade Learning Environment [Bellemare et al., 2013] which can be installed with the command pip install ale-py. A Gymnasium [Towers et al., 2023] interface is also provided and can be installed via pip install gymnasium[atari]. Once installed the keyword argument continuous can enable continuous actions as shown in the code below.\n\nimport gymnasium\n\n# 'env.action_space' will be continuous\nenv = gymnasium.make(\"Pong-v5\", continuous=True)"}, {"title": "Code specifications", "content": "The implementation of CALE is available as part of the ALE: https://github.com/ Farama-Foundation/Arcade-Learning-Environment (under GPL-2.0 license).\nFor SAC and PPO, we used the Dopamine [Castro et al., 2018] implementations. Taking Dopamine's root directory https://github.com/google/dopamine/, the specific code paths used are:\n\u2022 The SAC implementation is available at labs/cale/sac_cale.py\n\u2022 The PPO implementation is available at labs/cale/ppo_cale.py\n\u2022 All networks used are available at labs/cale/networks.py\n\u2022 For SAC-D we simply modified the SAC actor outputs to emit a categorical distribution with jax.random.categorical. From this, we can easily extract the log probabilities with jax.nn.log_softmax, and select actions greedily with jnp.argmax."}, {"title": "Hyper-parameters", "content": "In the following table we specify the hyper-parameters used for the various agents considered. For the most part we used the default hyper-parameters specified in the Dopamine gin files for DER, DQN, and SAC. For SAC-D, we modified settings according to what was suggested by Christodoulou [2019]."}, {"title": "ALE game specifications", "content": "In the following list we indicate the minimum action values for each game. Games with an asterisk next to them are games which are part of the 26 games for the Atari 100K benchmark [\u0141ukasz Kaiser et al., 2020].\n\u2022 AirRaid (6)\n\u2022 Alien* (18)\n\u2022 Amidar* (10)\n\u2022 Assault* (7)\n\u2022 Asterix* (9)\n\u2022 Asteroids (14)\n\u2022 Atlantis (4)\n\u2022 BankHeist* (18)\n\u2022 BattleZone* (18)\n\u2022 BeamRider (9)\n\u2022 Berzerk (18)\n\u2022 Bowling (6)\n\u2022 Boxing* (18)\n\u2022 Breakout* (4)\n\u2022 Carnival (6)\n\u2022 Centipede (18)\n\u2022 ChopperCommand* (18)\n\u2022 CrazyClimber* (9)\n\u2022 DemonAttack* (6)\n\u2022 DoubleDunk (18)\n\u2022 ElevatorAction (18)\n\u2022 Enduro (9)\n\u2022 FishingDerby (18)\n\u2022 Freeway* (3)\n\u2022 Frostbite* (18)\n\u2022 Gopher* (8)\n\u2022 Gravitar (18)\n\u2022 Hero* (18)\n\u2022 IceHockey (18)\n\u2022 Jamesbond* (18)\n\u2022 JourneyEscape (16)\n\u2022 Kangaroo* (18)\n\u2022 Krull* (18)\n\u2022 KungFuMaster* (14)\n\u2022 MontezumaRevenge (18)\n\u2022 MsPacman* (9)\n\u2022 NameThisGame (6)\n\u2022 Phoenix (8)\n\u2022 Pitfall (18)"}, {"title": "Per-game results", "content": null}, {"title": "SAC-D extra results", "content": null}]}