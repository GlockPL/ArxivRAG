{"title": "Local Lesion Generation is Effective for Capsule Endoscopy Image Data\nAugmentation in a Limited Data Setting", "authors": ["Adrian B. Ch\u0142opowiec", "Adam R. Ch\u0142opowiec", "Krzysztof Galus", "Wojciech Cebula", "Martin Tabakov"], "abstract": "Limited medical imaging datasets challenge deep learning models by increasing risks of overfitting and reduced gener-\nalization, particularly in Generative Adversarial Networks (GANs), where discriminators may overfit, leading to training\ndivergence. This constraint also impairs classification models trained on small datasets. Generative Data Augmentation\n(GDA) addresses this by expanding training datasets with synthetic data, although it requires training a generative model.\nWe propose and evaluate two local lesion generation approaches to address the challenge of augmenting small medical\nimage datasets. The first approach employs the Poisson Image Editing algorithm, a classical image processing technique,\nto create realistic image composites that outperform current state-of-the-art methods. The second approach introduces a\nnovel generative method, leveraging a fine-tuned Image Inpainting GAN to synthesize realistic lesions within specified re-\ngions of real training images. A comprehensive comparison of the two proposed methods demonstrates that effective local\nlesion generation in a data-constrained setting allows for reaching new state-of-the-art results in capsule endoscopy lesion\nclassification. Combination of our techniques achieves a macro F1-score of 33.07%, surpassing the previous best result by\n7.84 percentage points (p.p.) on the highly imbalanced Kvasir Capsule Dataset, a benchmark for capsule endoscopy. To\nthe best of our knowledge, this work is the first to apply a fine-tuned Image Inpainting GAN for GDA in medical imaging,\ndemonstrating that an image-conditional GAN can be adapted effectively to limited datasets to generate high-quality exam-\nples, facilitating effective data augmentation. Additionally, we show that combining this GAN-based approach with classical\nimage processing techniques further enhances the results.", "sections": [{"title": "1. Introduction", "content": "The classification of medical data using machine learning models presents a significant challenge [48, 23, 9] due to limited\ndata availability [23, 49] and class imbalance [23, 106, 87], which often arises from natural disparities in pathology prevalence\nwithin a population. Gastrointestinal tract cancers, which account for 63% of cancer-related deaths and cause 2.2 million\nfatalities annually, underscore the critical need for accurate diagnostic tools [14].\nVideo capsule endoscopy (VCE) offers an alternative to traditional gastroscopy and colonoscopy, serving as a gold stan-\ndard for diagnosing small bowel lesions [32, 24]. Unlike conventional endoscopy, VCE can visualize parts of the small\nintestine inaccessible through other methods, making it particularly useful for identifying occult bleeding and small mucosal\nlesions that standard imaging techniques might miss [69, 74]. Capsule variants, such as PillCam Colon, also allow for large\nintestine examination [2]. Additionally, VCE is less invasive than traditional methods and demonstrates superior sensitivity\nin detecting small bowel lesions compared to push enteroscopy [5, 2, 32, 24]. However, it does have limitations: VCE does\nnot permit biopsy collection [24], and determining the exact location of a lesion remains challenging despite advancements\nin the field [84]."}, {"title": null, "content": "A major challenge in VCE is the sheer volume of data. The capsule moves through the gastrointestinal (GI) tract via\nperistalsis, generating lengthy videos with over 50,000 frames [102]. Manually analyzing such footage can take up to two\nhours [24, 102] of a healthcare professional time, which increases the likelihood of errors due to lapses in concentration\nor insufficient expertise [110, 21, 77]. An effective machine learning-based lesion classification method could accelerate\nthis process, leading to faster diagnoses, earlier cancer detection, reduced mortality rates, and greater capacity for VCE\nexaminations [79, 102].\nCapsule endoscopy datasets are prone to issues commonly seen in medical imaging, such as data scarcity and class\nimbalance [79, 103]. The Kvasir Capsule dataset [79], the largest publicly available dataset for capsule endoscopy, suffers\nfrom a lack of diverse lesion representations, with some pathological classes containing images from only two or three\npatients. Lesions in the GI tract typically present as fine-grained, localized changes in tissue, often appearing in only one\nor two frames per video, and exhibit significant variation in color, shape, and size [102]. The arbitrary orientation of these\nimages further complicates classification tasks, where even state-of-the-art methods achieve only a 25.23% macro F1-score\nin multiclass classification on the official data split. These models predominantly excel at identifying normal samples but\nmisclassify most pathological cases.\nOver the past four decades, numerous methods have been developed to address issues related to low data availability and\nclass imbalance. These include sampling techniques [34, 46, 85, 18, 26], transfer learning [68, 82], and data augmentation\nstrategies [93, 51], particularly Generative Data Augmentation (GDA) [109, 101, 19, 8, 20]. Training effective models in a\nlimited data regime is complicated.\nThe most common use case for GDA is de novo generation using deep generative modes like GANS, VAEs, and diffusion\nmodels [12, 15, 86]. These models learn the data distribution from a training set and generate synthetic labeled examples\nto augment the original dataset. Empirical [101, 19, 8, 20] and theoretical [109] evidence shows that GDA improves model\ngeneralization, especially when overfitting occurs.\nImage inpainting [25, 10, 17, 54, 81] and image composition [62, 70] have also been employed for data augmentation.\nImage inpainting, dominated by GANs [81, 108, 54, 104, 61] and diffusion models [56, 83], modifies parts of an image by\ngenerating plausible content, whereas image composition combines elements from different images using techniques like\nPoisson Image Editing, known also as Poisson Blending [70]. However, most inpainting research focuses on removing\nobjects, which contrasts with our need to generate lesions. Both methods have shown promise in augmenting data for image\nrecognition tasks [93, 51, 38, 94, 83].\nIn this work, we introduce two data augmentation methods\none based on image composition, the other on image\ninpainting that modify parts of healthy tissue images to generate synthetic lesions. The first method employs Poisson\nBlending Data Augmentation (PBDA) to realistically combine healthy and pathological tissue, while the second, Image\nInpainting Data Augmentation (IIDA), introduces a novel generative approach. IIDA fine-tunes an image inpainting GAN\nto insert lesions into healthy tissue. Both techniques leverage the abundance of healthy tissue images in medical datasets to\nproduce high-quality synthetic data, effectively augmenting small medical imaging datasets. By addressing the challenges\nof GAN and classifier overfitting, these approaches enhance data quality and model performance. Notably, when synthetic\ndata created with both techniques is combined to enrich the training dataset for the classifier, the model achieved a new state-\nof-the-art performance on the Kvasir Capsule Dataset, with a 33.07% macro F1-score, surpassing the previous best result by\n7.84 \u0440.\u0440.\nThe contributions of this paper can be summarised as follows:\n\u2022 We propose IIDA, a novel data augmentation approach using a fine-tuned image inpainting model, particularly effective\nin low-data settings. This new approach leverages the abundance of healthy tissue, common in many medical imaging\ndomains, to generate lesions.\n\u2022 Our qualitative and quantitative results demonstrate the superiority of the generated synthetic samples in improving\nclassification performance.\n\u2022 We compare Poisson Blending, a classical image processing method, with deep generative models for augmenting\nlocal pathological changes, showing that both techniques can enhance machine learning models. IIDA shows better\nperformance than PBDA as a standalone data augmentation technique, however their combination provides the best\nresults.\n\u2022 We demonstrate that our local lesion generation methods outperform modern generative models, such as NVAE [88]\nand LDM [76], in data augmentation for classification tasks."}, {"title": "2. Related works", "content": "2.1. Data scarcity and data imbalance\nTraining deep learning models from scratch typically requires extensive annotated datasets and prolonged training time\nto achieve acceptable performance, especially in comparison to leveraging pre-trained models. In image classification, pre-\ntrained models, especially those trained on ImageNet, are widely adopted for their performance benefits [28, 40, 73, 100, 50].\nDespite notable differences between natural and medical images, Tajbakhsh et al. [82] demonstrated that pre-trained CNNs\nperform comparably or even outperform CNNs trained from scratch on medical imaging tasks. They further highlighted that\nthese CNNs exhibit greater robustness to variations in training set size and emphasized the importance of selecting an optimal\nfine-tuning strategy.\nIn the context of Generative Adversarial Networks (GANs), Karras et al. [45] investigated training StyleGAN2 under lim-\nited data conditions, utilizing datasets of only 5,000, 2,000, and even 1,000 images. They introduced Adaptive Discriminator\nAugmentation (ADA) to mitigate discriminator overfitting, thus enhancing training stability. This approach dynamically\nadjusts the augmentation probability without leaking into the generator. Additionally, they validated a GAN fine-tuning\ntechnique as proposed by Mo et al. [60], underpinning their methods with proofs to support theoretical claims.\n2.2. Image generation in capsule endoscopy\nA growing body of research has focused on generating capsule endoscopy images using various methods. Vats et al. [90]\ntrained a StyleGAN2 model for the unconditional de novo generation of capsule endoscopy images, utilizing a dataset of\n200,000 images. They conducted an expert evaluation to assess the quality of the generated samples.\nDiamantis et al. [29] introduced a VAE model trained on a subset of Kvasir Capsule and another VCE dataset. They\ntrained distinct models for normal and abnormal image generation, achieving high-quality synthetic images. Xiao et al. [99]\nproposed a GAN architecture to generate synthetic capsule endoscopy images from Kvasir Capsule data, which were then\nused in GDA for object detection. Unlike Xiao et al., who utilized a random data split, our approach adopts the official data\nsplit, enhancing comparability with other studies and better simulating clinical environments.\n2.3. Image inpainting\nImage inpainting is the process of reconstructing missing or damaged parts of an image in a way that blends seamlessly\nwith the original content. Applications include object removal [25, 10, 104, 81], artifact elimination [27, 31], restoration of\nold or damaged photographs [31, 17], and enhancement in medical imaging [6, 7, 95].\nEarly approaches to inpainting relied on patch-matching techniques [25, 10] and partial-differential equation (PDE)-based\nmethods [17]. The introduction of GANs marked a significant shift, as researchers moved towards these models for more\nsophisticated inpainting tasks.\nImage inpainting is a non-trivial, ill-posed problem [36, 92]. The non-uniqueness of the problem arises because there are\noften multiple plausible ways to fill missing regions. This complexity is heightened when the missing areas cover substantial\nportions of the image, leaving minimal conditional information for inpainting models to rely on [108].\nLiu et al. [54] performed one of the first high-quality image inpainting for irregular holes using deep learning. Authors\nproposed Partial Convolution which conditions the network on valid pixels to inpaint masked region.\nSubsequent advancements addressed limitations in Partial Convolution. Yu et al. [104] developed Gated Convolution,\na method that learns to condition both masked and unmasked regions within an image, enhancing the model's adaptability.\nThey also introduced SN-PatchGAN, a GAN architecture designed to discriminate between image patches. This model\nallowed for user guidance in inpainting, utilizing sketches, lines, dots, and other structures as input to guide the inpainting\nprocess.\nSuvorov et al. [81] utilized Fast Fourier Convolutions (FFC) [22] to develop a class of Large Mask Inpainting (LaMa)\nmodels. They showed that a large receptive field is essential for high-quality inpainting of large masks. FFC significantly\nenhances LaMa's ability to recreate repetitive patterns. Additionally, Suvorov et al. proposed the High Resolution Field\nPerceptual Loss, leveraging a segmentation network to extract high-level features and enhance the model's focus on object\nstructure over texture."}, {"title": "2.4. Generative data augmentation", "content": "GANs and diffusion models have shown promise in medical image augmentation by synthesizing samples to expand\ntraining datasets. He et al. [38] developed a GAN specifically for image inpainting to generate synthetic pathological samples\nin histopathology. Their model, trained with global L1, perceptual, style, and local L1 losses, produced augmented data that\nimproved segmentation performance for pathological tissues. In another medical application, Lee et al. [52] applied a GAN-\nbased model, DeepFillv2, to reconstruct glandular regions that had been partially removed. The inpainting demonstrated\nsuperior effectiveness compared to geometric transformations and baseline methods.\nZhang et al. [107] proposed a dual-network approach, where one network generates normal thyroid ultrasound images\nand the other focuses on generating nodules. For normal samples, random image regions were masked, whereas for nodule\nsamples, only the nodule region was masked.\nBeyond medical imaging, image inpainting-based data augmentation has also proven effective in other domains. Wang et\nal. [94] designed an inpainting GAN for multi-class object detection in infrared images. Their model incorporated recon-\nstruction and perceptual losses alongside a novel multiscale erosion-based MSE loss, enhancing object detection accuracy.\nTao [83] proposed a Denoising Diffusion Probabilistic Model (DDPM)-based GDA method for surface defect inspection\nin a limited data regime. Synthetic samples are generated by conditioning the model on partially erased real defect samples."}, {"title": "2.5. Image composition", "content": "Image composition is a process of combining multiple images, elements of images, or specifically a foreground object and\nbackground image to create a realistic composite image.\nIt has been used in data augmentation to improve downstream task performance in medical imaging. Wang et al. [93]\nutilized Poisson Blending to augment diabetic retinopathy lesions in fundus images from retinography. Lee and Cho [51]\ncreated an augmentation pipeline using Poisson Blending to augment training sets in classical endoscopy. Their solution\ninvolved the use of Grad-CAM [78] for automatic segmentation of lesions in images. They use the lowest RGB variance\nsliding window search to find a suitable area for blending."}, {"title": "2.6. Classification in capsule endoscopy", "content": "The Kvasir Capsule Dataset [79] is currently the largest publicly available dataset of capsule endoscopy videos. Smedsrud\net al. [79] introduced the official data split, where no patient images are shared between different splits. They established\nbaseline performance for this dataset using ResNet-152 [37] and DenseNet-161 [39]. Recent work [80] validated the effec-\ntiveness of their FocalConvNet, alongside several modern CNNs and Vision Transformers (ViTs) also using that split.\nIn contrast, many works [57, 4, 67, 66] evaluate their methods using a random data partitioning, where sequential images\nfrom videos can be included in both the training and test sets. Other studies [33] do not specify their data breakdown\nmethodology, while some [13, 53] do not use the official split. Our work distinguishes itself by using data split introduced\nby Smedsrud et al. [79], which increases the reliability of comparisons and ensures the reproducibility of the experiments."}, {"title": "3. Methods", "content": "3.1. Poisson Blending Data Augmentation\nThere are various algorithms for merging two images, with Poisson Blending [70] being one of the most widely known\nand frequently used methods [93, 51]. In our work, we applied this gradient-based image processing technique to augment\nmedical data.\nThe fundamental idea is to blend a selected region from a source image into a specified area of a target image, aiming to\nminimize color discrepancies at the blending boundary. The goal is to ensure that the gradients within the blended region\nclosely resemble those of the source image, while the boundary seamlessly matches the target image. This approach is\nparticularly useful for augmenting medical data by blending lesion areas into healthy tissue, thereby expanding the lesion\ndataset. Below, we provide a brief theoretical overview of Poisson Blending, following the original notation from [70].\nLet $S$, be the image definition domain, defined as a closed subset of $R^2$, and let $\\Omega$ be a closed subset of $S$ with boundary\n$\\partial \\Omega$. Let $f^*$ be a known scalar function defined over $S$ minus the interior of $\\Omega$ and let $f$ be an unknown scalar function defined"}, {"title": null, "content": "over the interior of $\\Omega$ and finally, $v$ be a vector field defined over $\\Omega$. Essentially, the function $f$ interpolates the destination\nfunction $f^*$ over $\\Omega$, under the guidance of the vector field $v$, which can be the gradient field of some source function $g$.\nThe interpolant $f$ of $f^*$ is defined as the solution of the following minimalization problem:\n$\\min \\int\\int_{\\Omega} |\\nabla f - v|^2 \\,\\text{with}\\, f|_{ \\partial \\Omega} = f^*|_{ \\partial \\Omega},$\nwhere $\\nabla$ is the gradient operator. The solution of Eq. 1 must satisfy the following Poisson equation with Dirichlet boundary\nconditions:\n$\\Delta f = \\text{div} \\,\\,v \\text{ over } \\,\\Omega, \\text{ with } \\, f|_{ \\partial \\Omega} = f^*|_{ \\partial \\Omega},$\nwhere $\\Delta. = \\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}$ is the Laplacian operator and $\\text{div}\\,v$ is the divergence of $v$.\nTo provide an intuitive understanding, Figure 1 illustrates the relationship between real input image data and the concepts\ndescribed above. In this example, the function $f^*$ could represent the background of a target image, $g$ could depict a lesion,\nand $f$ would correspond to the blended pathology. However, it's important to note that this is a simplified illustration for\nintuition, as digital images require consideration of the discrete case.\nIn our implementation, we selected the gradient field from the source lesion $g$ as the guidance field $v$, as this is a funda-\nmental choice.\nTherefore, the interpolation is performed under the guidance of:\n$v = \\nabla g,$\nand Eq. 2 takes the form:\n$\\Delta f = \\Delta g \\text{ over } \\Omega, \\text{ with } f|_{ \\partial \\Omega} = f^*|_{ \\partial \\Omega}.$\nThe above assumption introduces the so-called seamless cloning approach, which ensures the compliance of source and\ndestination boundaries.\nIn this work, to enhance data augmentation effectiveness, we designed the Poisson Blending Data Augmentation pipeline,\nconsisting of the following steps:\n\u2022 Image deduplication with feature extraction.\n\u2022 Identifying the blending location.\n\u2022 Performing image blending.\nThe method is illustrated in Figure 2 and each of these steps will be explained in detail in the following sections. While\nPoisson Blending has been applied in recent studies as a data augmentation technique [51, 93], our proposed pipeline is novel\nand specifically tailored for Capsule Endoscopy data, although it can be adapted to other medical imaging fields. The next\nsection will elaborate on the processes of deduplication and identifying the blending location."}, {"title": null, "content": "Algorithm 1 The deduplication algorithm enhances dataset diversity by iteratively removing redundant images from each\nvideo. It calculates the latent vector distance between images and discards those with distances below an experimentally\ndetermined threshold.\nAlgorithm 2 The image pair selection algorithm improves the quality of generated samples by choosing the most similar\nsource and target image pairs. For each given image, it selects the closest neighbor in the DinoV2 latent space.\nThe function $f$ represents the adjustments needed for the pathology to blend seamlessly using the Poisson Blending\nalgorithm. Therefore for the pathology to remain consistent after the blending, the difference $f^* g$ must be minimal along\nthe boundary $\\partial \\Omega$. If this difference is too large, it can cause color bleeding or blending artifacts [3], potentially altering the\nsemantics (or class) of $g$ and introducing label noise into the synthesized dataset. Our pipeline for finding pairs of $f^*$ and\n$g$ values consists of two steps: first, finding the most similar pair of images defined over $S$ and $S'$, and then identifying the\noptimal location $\\Omega$ for seamless cloning.\nIdentifying the most similar pair of source and target images relies on semantic search within the latent space of the\nDinoV2-Giant model [65]. The process is detailed in Algorithm 2. To identify $K$ target pairs for a given source, simply\nK most similar normal images are chosen for a given lesion. After selecting such pairs, choosing an appropriate blending\nlocation becomes essential. Not all areas in Capsule Endoscopy images are suitable for seamlessly blending lesions. Regions\nlike edges, folds, or holes are generally unsuitable because the source lesion cannot appear there without significant alteration.\nThe Poisson Blending algorithm integrates the lesion as is, adjusting only the color for seamless blending but not modifying\nits texture or shape. Furthermore, if there is a large gradient difference at the boundary of the lesion and the new region (e.g.,\nnear folds or dark zones), blending artifacts may arise.\nTo blend a pathological subregion from a source image into a target image, it is necessary to identify the region of interest\n(ROI) in the target image that minimizes color differences between the two images, particularly along the border pixels. This\nprocess is illustrated in Figure 2.\nFormally, let $\\partial \\Omega'$ denote the set of border pixels for the bounding box in the source image, and $\\partial \\Omega_i$ represent the border\npixels for the bounding boxes of each potential ROI in the target image, indexed by $i = 1,..., N$, where $N$ is the total number\nof ROIs generated using a sliding window algorithm. For all $i$, the number of border pixels is denoted as $|\\partial \\Omega'| = |\\partial \\Omega_i| = M$.\nThe optimal ROI in the target image is determined by minimizing the average color difference between the corresponding\nborder pixels of the source image and the respective ROIs. This is expressed as:\n$\\min { \\sum_{j=1}^M (p'_j - p_j)^2 : p'_j \\in \\partial \\Omega', p_j \\in \\partial \\Omega_i, i = 1, ..., N},$\nwhere $p'_j$ are the border pixels of the source image, and $p_j$ are the corresponding border pixels in the target image for each\nROI $i$.\nAfter completing the previous steps - selecting the appropriate source and target images and determining the suitable\nblending location - Poisson Blending can be performed as described in [70] and in Section 3.1."}, {"title": "3.2. Image Inpainting Data Augmentation", "content": "The proposed solution leverages an Image Inpainting model, LaMa [81], to generate new structures at selected locations\nfor data augmentation. In this work, the LaMa model was fine-tuned on a subset of the Kvasir Capsule Dataset [79] to achieve\nstate-of-the-art data augmentation on this small dataset.\nTo the best of our knowledge, this is the first method to use a pre-trained Image Inpainting model, fine-tuned to generate\nnew objects for data augmentation in classification tasks on a limited medical imaging dataset. The following sections will\ndescribe the LaMa model, the preparation of training datasets, and the model's fine-tuning process.\n3.2.1 LaMa\nImage inpainting aims to fill in missing regions of an image in a visually convincing way. Following notation introduced by\nSuvorov et al. [81], the mask is represented as $m$, and the masked image is $x \\odot m$. The LaMa model takes as input a stack\nof the masked image and the mask, denoted $x' = \\text{stack}(x \\odot m, m)$. The inpainting network, referred to as the generator\nor synthesis network, is denoted by $f_{\\theta}(.)$, where $\\theta$ are the parameters of the network. The LaMa model processes $x'$ using\na fully convolutional approach, yielding a three-channel RGB image $\\hat{x} = f_{\\theta}(x')$. Training involves preparing image-mask\npairs.\nThe LaMa inpainting network features an architecture (Figure 3) similar to fully convolutional ResNet with 3 downsam-\npling blocks, 6-18 residual blocks, and 3 upsampling blocks. The discriminator network is based on PatchGAN [42]. For\nresidual blocks, Suvorov et al. [81] use Fast Fourier Convolution (FFC) [22]. FFCs are fully differentiable and can effectively\nreplace standard convolutions in the architecture. This enables the network to receive global context information already in\nthe first stages, enhancing its content generation capabilities in situations where limited conditional information is available,\nsuch as when large masks are applied. Additionally, the use of FFCs improves the network's ability to identify and reconstruct\nrepetitive patterns, which should be beneficial in the Capsule Endoscopy data domain.\nSuvorov et al. [81] introduced High Receptive Field Perceptual Loss (HRFPL). It is based on perceptual loss from [43],\nwhich computes the distance between feature vectors extracted from a pretrained network $\\phi(\\cdot)$ applied to generated and\noriginal images. Suvorov et al. argue, that for large mask inpainting, it is important to understand the global structure from\nearly layers also in the pretrained network for HRFPL loss. Therefore, the base model $\\phi$ is implemented with Fourier or\nDilated convolutions. The formula for HRFPL loss, as defined by [81], is provided below:\n$L_{HRFPL}(x, \\hat{x}) = M([\\phi_{HRF}(x) - \\phi_{HRF}(\\hat{x})]^2),$\nwhere $[\\cdot - \\cdot]^2$ is an element-wise operation, and $M$ is the sequential two-stage mean operation (interlayer mean of intra-layer\nmeans).\nLaMa uses adversarial loss based on image patches. Only patches that intersect with the masked region of interest are\nconsidered \"fake\". The authors decided to use non-saturating adversarial loss, defined in [81] as:\n$L_D = -E_{x} [\\log D_{\\xi}(x)] - E_{x,m} [\\log D_{\\xi}(x) \\odot m] - E_{x,m} [\\log (1 - D_{\\xi}(\\hat{x})) \\odot (1 - m)]$"}, {"title": null, "content": "$L_G = -E_{x,m} [\\log D_{\\xi}(\\hat{x})]$\n$L_{Adv} = \\text{sg}_{\\theta}(L_D) + \\text{sg}_{\\xi}(L_G) \\rightarrow \\min_{\\theta,\\xi},$\nwhere $x$ is a sample from dataset, $m$ is a synthetically generated mask, $\\hat{x} = f_{\\theta}(x')$ is the inpainting result for $x' = \\text{stack}(x \\odot\nm, m)$, $\\text{sg}_{var}$ stops gradients w.r.t $var$ and $L_{Adv}$ is the joint loss to optimize.\nSuvorov et al. [81] also use two types of regularization on the discriminator network. The first one is $R_1 = E_x||\\nabla D_{\\xi}(x)||^2$\ngradient penalty [59], the second one is a discriminator-based perceptual loss $L_{DiscPL}$ [96], also called feature matching loss,\napplied on features of a discriminator network. Their final loss function is:\n$L_{final} = \\kappa L_{Adv} + \\alpha L_{HRFPL} + \\beta L_{DiscPL} + \\gamma R_1$\nwhere $\\kappa, \\alpha, \\beta, \\gamma$ are weighting parameters for each part of loss function. Suvorov et al. [81] used values of $\\kappa = 10, \\alpha = 30,$\n$\\beta = 100$ and $\\gamma = 0.001$.\n3.2.2 Fine-tuning\nIt has been shown in the literature that GANs fine-tuning is possible [60, 63, 97, 98]. LaMa utilizes training-time data\naugmentation for discriminator, which has been shown by Karras et al. [45] to be effective when training on a limited\ndataset.\nWe tried to train the Big LaMa-Fourier model from scratch on the Kvasir Capsule Dataset, using the training setup\ndescribed in [81]. The model could not learn to generate plausible content in the capsule endoscopy images when trained\nfrom scratch. Therefore in this work, the Big LaMa-Fourier model trained by Suvorov et al. on Places2 Dataset has been\nfine-tuned using a modified loss function:\n$L_{final} = \\kappa L_{Adv} + \\alpha L_{HRFPL} + \\beta L_{DiscPL} + \\gamma R_1 + \\delta L_{L1},$\nwhere $L_{L1}(x, \\hat{x}) = ||x - \\hat{x}||_1$. Adding the L1 term between the reconstruction of lesion generated by the model, and the\noriginal pathology, with $\\delta = 10$, makes the reconstruction task easier for the model, while other parts of the loss function,\nsuch as $L_{HRFPL}$ help to leverage the problem of blurriness when applying L1 loss term. This term is introduced to ensure\nthat the model does not deviate from what it learned prior to the fine-tuning process. The rest of the scaling factors, are set\nwith the values used by [81]. The LaMa model, as presented by Suvorov et al. [81], does not allow for class conditioning\nand the development of such methods is out of the scope of this paper. Therefore, a separate model is fine-tuned for each of\nthe considered lesion classes. In the following paragraphs, the fine-tuning process of separate models for each class will be\ndescribed.\nFormally, let $\\mathcal{X} \\subset \\mathbb{E}^n$ be a subset of Euclidean space corresponding to image dimensions, representing capsule endoscopy\nimages, and $\\mathcal{Y} = \\{0, 1, ..., M\\}$ be the set of corresponding labels, where 0 represents the healthy tissue and $1, 2, ..., M$\nrepresent different lesion classes. Let $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N$ be a dataset of $N$ observations, where $x_i \\in \\mathcal{X}$ and $y_i \\in \\mathcal{Y}$.\nThe dataset $\\mathcal{D}$ can be divided into separate sets for each class. Let $\\mathcal{D}_i = \\{(x, y) \\in \\mathcal{D} : y = i\\}$ for $i = 0, 1, ..., M$. The\nlearning problem is to fine-tune a set of inpainting networks $\\mathcal{F} = \\{f_{\\theta_i}\\}_{i=1}^M$ from a pre-trained $f_{\\theta}$ network using respective\ndatasets $\\mathcal{D}_i$ for $i = 1, ..., M$ and the loss function $L_{final}$. Note that the dataset $\\mathcal{D}_0$ of healthy images is not used for training.\nTo augment the dataset, a set of $K$ new observations is generated for each lesion class, using respective $f_{\\theta_i} \\in \\mathcal{F}$. For\nevery lesion class $i \\in \\{1, ..., M\\}$ we sample $K$ observations from $\\mathcal{D}_0$ and place them in respective sets $\\mathcal{A}_i$. Let $\\mathcal{D}_i = \\{(f_{\\theta_i}(x), i) : x \\in \\mathcal{A}_i\\}$ for $i = 1, ..., M$ be the sets of new observations generated with respective inpainting networks $f_{\\theta_i}$.\nA dataset used for the supervised training of a classifier is therefore composed of the real and generated data: $\\mathcal{D}_{class} = \\{\\mathcal{D}_0, \\mathcal{D}_1, \\tilde{\\mathcal{D}}_1, ..., \\mathcal{D}_M, \\tilde{\\mathcal{D}}_M\\}$.\nThe datasets $\\mathcal{D}_i$ for $i = 0, 1, ..., M$ were constructed from the official training split of the Kvasir Capsule Dataset. Each\nlesion in the datasets was coupled with a mask based on the available bounding boxes. For each dataset $\\mathcal{D}_i, i = 1, ..., M$, the\ninpainting model was fine-tuned for 500 epochs, incorporating a linear learning rate warm-up over the first 10 epochs. The\nAdam optimizer was used for both the generator and discriminator, with learning rates set at 0.001 and 0.0001, respectively,\nand a batch size of 16. Although the learning rate and optimizer settings align with those in [81], the batch size was reduced\ndue to technical constraints. Models were trained on images with a resolution of 256x256, consistent with [81]. The linear\nlearning rate warm-up is a standard practice in fine-tuning deep learning models."}, {"title": "4. Experiments", "content": "4.1. Dataset\nThe largest public dataset on capsule endoscopy, the Kvasir Capsule [79", "64": ".", "79": "and [80"}]}