{"title": "ACTIONFLOW: Equivariant, Accurate, and Efficient Policies with Spatially Symmetric Flow Matching", "authors": ["Niklas Funk", "Julen Urain", "Joao Carvalho", "Vignesh Prasad", "Georgia Chalvatzaki", "Jan Peters"], "abstract": "Spatial understanding is a critical aspect of most robotic tasks, particularly when generalization is important. Despite the impressive results of deep generative models in complex manipulation tasks, the absence of a representation that encodes intricate spatial relationships between observations and actions often limits spatial generalization, necessitating large amounts of demonstrations. To tackle this problem, we introduce a novel policy class, ActionFlow. ActionFlow integrates spatial symmetry inductive biases while generating expressive action sequences. On the representation level, Ac-tionFlow introduces an SE(3) Invariant Transformer archi-tecture, which enables informed spatial reasoning based on the relative SE(3) poses between observations and actions. For action generation, ActionFlow leverages Flow Matching, a state-of-the-art deep generative model known for generat-ing high-quality samples with fast inference an essential property for feedback control. In combination, ActionFlow policies exhibit strong spatial and locality biases and SE(3)-equivariant action generation. Our experiments demonstrate the effectiveness of ActionFlow and its two main components on several simulated and real-world robotic manipulation tasks and confirm that we can obtain equivariant, accurate, and efficient policies with spatially symmetric flow matching.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, deep generative models have demonstrated impressive results when applied as policies for solving complex manipulation tasks [1], [2], [3], [4]. However, it is well known that models that naively integrate observations and actions often require large amounts of demonstrations to achieve satisfactory performance. In this direction, there has been a collection of research that explored how to exploit the spatial relations between observations and actions [5], [6], [7], [8], [9], [10] to learn more sample-efficient policies. Equivariant policies generalize the policy's behavior under global translations or rotations in the scene [11], [6], [5], [12], [13], [14], [15], [16]. If the observations are rotated or translated, the generated actions will be equally transformed, thereby adding an effective inductive bias.\nIn this work, we are not only interested in equivariant policies that adapt to global transformations but also in local spatial relations [17], [18], [6]. Consider, for example, the task of picking a mug and hanging it (cf. Fig. 1). When the robot is approaching to pick up the mug, it should be capable of reasoning based on the relative poses between its own pose, the mug, and its next actions. But when hanging the mug, the robot should also focus on the relative poses between the mug and the hanger. Thus, equipping the policy with the ability to reason based on the relative poses between the different observations and actions, i.e., their spatial relations, may be crucial for learning policies more efficiently. How can we integrate all these desiderata and still learn dexterous, fast, and expressive policies from demonstrations?\nInspired by the recent successes from the protein folding community [19], [20], [21], [22], [23], in which SE(3) symmetric models are integrated with highly-expressive deep"}, {"title": "II. BACKGROUND - FLOW MATCHING", "content": "Let us consider a data point $a \\in \\mathbb{R}^d$ and a probability path $p_t(a)$ that connects a noise distribution $p_0(a)$ at $t=0$ to the data distribution $p_1(a)$ at time $t=1$ with its associated flow $a_t = \\phi_t(a_0)$, which defines the motion for the particle $a_0$. Flow Matching [25] proposes learning Continuous Normaliz-ing Flows (CNF) [30] by regressing the vector field $u_t(a) = \\frac{d\\phi_t(a)}{dt}$ with a parametric one $v_\\theta(a,t)$. In general, there is no closed-form solution for $u_t$ that generates $p_t$, making direct flow matching intractable. Instead, Conditional Flow Matching (CFM) proposes an efficient approach to learn CNF by regressing a conditioned vector field $u_t(a|z)$ that generates the probability path $p_t(a|z)$:\n$\\mathcal{L}_{CFM}(\\theta) = \\mathbb{E}_{t, p_t(a|z), p_D(z)} ||v_\\theta(a, t) - u_t(a|z)||^2,$\n(1)\nwith $p_D(z)$ being the data distribution. As shown in [25], $v_\\theta$ recovers the marginalized conditioned vector field $u_t = \\int_z u_t(a|z) p_t(a|z) p_D(z) / p_t(a) dz$ that generates the marginalized distribution path $P_t = \\int_z p_t(a|z) p_D(z) dz$. Then, the problem boils down to designing a conditioned vector field $u_t(a|z)$ that moves a randomly sampled point at time $t = 0$ to the datapoint $z$ at time $t = 1$."}, {"title": "III. ACTIONFLOW", "content": "The desiderata for ActionFlow policies is to be fast, accurate, expressive, and sample-efficient. Particularly, Ac-tionFlow should capture the spatial relations between ob-servations and actions and yield SE(3) equivariant action generation. To achieve these properties, ActionFlow is built on two core elements: a Flow Matching-based generative model that generates action sequences quickly, and a geomet-rically grounded transformer model capable of capturing the intricate spatial relations between observations and actions in the SE(3) space.\nBefore proceeding with presenting both components in detail, we introduce the structure of ActionFlow's observa-tion and action space. Generally speaking, ActionFlow relies on a geometrically grounded scene representation. Both the observations $O : (T_o, F_o)$ and actions $A : (T_a, F_a)$ are represented through a sequence of poses $T = (T_1,...,T_N)$ and features $F = (f_1,..., f_N)$ as shown in Fig. 1. In other words, every individual action or observation consists of a pose $T = (r,p) \\in SE(3)$, with rotation matrix $r \\in SO(3)$ and 3D position vector $p \\in \\mathbb{R}^3$, together with an associated feature $f \\in \\mathbb{R}^d$ representing semantic information related to the specific action/observation. This representation is highly flexible. For the task of placing a mug onto a hanger as shown in Fig. 1 left, the RGB image from the wrist-mounted camera is represented by the camera's current pose, i.e., the location from which the image was captured, while the features correspond to the encoded image observation. Given that the wrist view provides a very localized observation, to solve the task, we additionally provide as input to the policy the point cloud observation of the hanger which is obtained from an initial global view of the scene. The point cloud features are obtained through a point cloud encoder, while the hanger's pose is centered at the mean of the point cloud. In other environments, where, e.g., an explicit object pose estimator [31] can be deployed, the objects' poses are given by the output of the pose estimator, while the features can represent semantic information that describes that object (color, shape, ...). An action pose represents the desired target pose that should be reached, while the features represent at what instant of time this target pose should be reached. This information is crucial since we are predicting a sequence of actions. Given this representation, our goal is to learn ActionFlow policies $\\pi_\\theta(T_a| O)$ that generate action pose sequences $T_a$ given an observation $O$."}, {"title": "A. Flow Matching for SE(3) Action Generation", "content": "Flow Matching permits the learning of expressive gen-erative models with fast inference. Similarly to diffusion"}, {"title": "B. SE(3) Invariant Transformer", "content": "As model architecture of ActionFlow, we propose an SE(3) Invariant Transformer (cf. Fig. 2). At the core of this architec-ture is a geometry-aware attention layer, known as Invariant Point Attention (IPA) [19], [21]. The IPA layer augments the queries, keys, and values of classical attention [35] with a set of 3D points that are generated in the local frames of the query $T_Q$ and key $T_K$ poses. The layer is designed such that the output is invariant to global rotations and translations (cf. Fig. 2 (b)). If we apply a transformation $\\Delta_{\\tau} \\in SE(3)$ over both observation poses $T=\\Delta_{\\tau}T_o$ and action poses $T=\\Delta_{\\tau}T_a$, the network generates the same output. Moreover, with the IPA layer, the network can reason about all the relative poses between the entities in the scene. We hypothesize that the invariant and object-centric nature of the network will lead to more data-efficient policies.\nNetwork Architecture. Given an observation $O =(T_o, F_o)$ and a candidate action sequence $T_a \\in (SE(3))^N$ of length $N$, the SE(3) Invariant Transformer outputs vectors $v \\in \\mathbb{R}^{6\\times N}$ that predict the direction in which the actions $T_a$ should be updated following Equation (4). Our net-work architecture is inspired by the protein folding network FrameDiff [21], [20] that combines an IPA layer with a transformer encoder. Given a set of poses $T$ and features $F$, the network first applies an IPA layer to capture the spatial relative attention between the different entities, followed by a transformer encoder to find higher-order interactions. We use a small linear layer to map the transformer output to the vector $v$. Notice that prior to the IPA, we employ an observation encoder that maps all observation features onto a common latent feature space. The action features $F_A$ are given by a learnable parameter vector.\nAction Generation. For action generation, we start with a randomly sampled action sequence $T_a$ and iteratively update the actions (cf. Equation (4)) by calling the SE(3) invariant transformer $K$ times. Given the invariant network and the action updates within their current local frame, the resulting policy $\\pi_\\theta(T_a| T_o, F_o)$ is equivariant. If we apply a transfor-mation over $T_o$, the generated actions $T_a$ are guaranteed to be equally transformed (cf. Fig. 2 (c) [19]. We provide further insights about equivariant action generation with invariant models in App. C."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "The experiments are divided into three parts. First, we explore the performance of flow matching in generating high-"}, {"title": "A. Flow Matching for Fast and Accurate Action Generation", "content": "We compare Flow Matching against Diffusion Policy [2] for action sequence generation in the simulated Robomimic tasks [36]. To ensure a fair comparison, both methods use the same transformer architecture from [2] to either model the (observation-conditioned) vector field in Equation (1) or the denoising network [2]. Moreover, we consider Flow Matching in the Euclidean space (cf. App. B) and use the same hyperparameters from the Diffusion Policy in Flow Matching. Both policies are conditioned on the current obser-vations and trained for 4000 epochs with K=100 time steps. Checkpoints are evaluated every 200 episodes. For testing, we pick the best-performing checkpoint during training and report the average success rate from policy rollouts starting from 50 different initial configurations (from the test set) across 3 training seeds. During inference, it is desirable to use fewer steps than during training since it enables higher-frequency policies. In Flow Matching, reduced inference time steps are obtained by interpolating the training time steps, while for Diffusion Policy we use DDIM [37] for faster sampling. For more information, see App. E.\nResults. Fig. 3 shows the results. We depict the success rates for each method when varying the number of available inference steps. Since both methods use the same underlying transformer network, they require the same amount of time per inference step. In practice, {2, 5, 10, 20, 100} inference steps allow for action sequence generation with the respec-tive policy at {100, 33, 20, 9, 2} Hz, respectively. Therefore, fewer inference steps are desirable as they either allow for faster action generation, or for reducing the required computations, i.e., the overall number of action refinement steps. As shown, for most environments and available in-ference steps, Flow Matching and diffusion perform almost equally. However, Flow Matching results surpass those from Diffusion Policy for very small inference steps. This effect is most noticeable in the Tool Hang task, which requires the policy to produce very accurate actions. These experiments show that Flow Matching policies can maintain good success"}, {"title": "B. SE(3) Invariant Transformer Evaluation: Multi-Token Observations and Invariant Point Attention", "content": "This experiment evaluates the performance of the proposed SE(3) Invariant Transformer (cf. Section III-B). Specifically, we aim to evaluate the influence of two design choices: (1) Does a multi-token representation, in which each object is treated as a single token, enhance policy performance? (2) Does the IPA layer, which allows computing the relative poses of all tokens among each other, help in finding informative features to improve policy performance?\nDataset & Evaluation Environment. The experiments are conducted in a subset of Mimicgen environments [38]. The datasets consist of 1000 synthetically generated demonstra-tions, given 10 original demonstrations. The original dataset provides the observations as a single vector and represents the action displacements in the world frame. We slightly adapt the data to be compatible with our model (T, F) and represent it as object poses $T^o$ and action poses $T^a$ in the world frame (cf. App. F).\nModels. We consider three variations of ActionFlow: (A.1) The original ActionFlow as introduced in Section III-B. (A.2) We eliminate the IPA layer but keep each object as an independent token. (A.3) We eliminate the IPA layer and rep-resent all observations as a single token. All models consider 100 inference steps. Additionally, we consider as baseline (B) a Diffusion Policy (DP) model [2] (100 denoising steps), and"}, {"title": "C. Real Robot Experiments", "content": "We finally evaluate ActionFlow in two real robotic tasks: (i) mounting a light bulb, and (ii) placing a mug onto a hanger. While the first task assesses ActionFlow's accuracy, the second investigates its equivariance.\nSetup. The experimental platform consists of a 7DoF Franka Panda manipulator with a RealSense mounted at its end-effector (cf. Fig. 1). For both tasks, we employ a token for the robot's end-effector $(T, f)$, with the end-effector pose $T$ and its features $f$ consisting of the encoded RGB RealSense image and the current gripper opening width. We use an observation history of 5 steps and predict an action sequence containing 16 steps. While the RealSense camera returns RGB readings with a resolution of 640 \u00d7 480, we resize the images to 80 \u00d7 80 pixels before passing them through the ResNet18 [39] for obtaining the encodings. The resizing helps to reduce the dataset's size significantly and, therefore, facilitates & speeds up policy training. The ResNet18 for encoding the images is trained from scratch. We parameterize our ActionFlow policies for real robot manipulation using the SE(3) Invariant Transformer with $K = 4$ inference steps, train the policies for 75 epochs, and evaluate the last checkpoint. Our computer is equipped with an AMD EPYC 7453 CPU; 512 GB RAM; RTX 3090 Turbo GPU.\nData Collection & Control. For data collection through"}, {"title": "V. RELATED WORK", "content": "Exploiting Spatial Symmetries for Robot Learning. Despite the impressive performance of learning policies from demonstration data for dexterous robotic manipulation [2], [4], [3], naively combining observations and actions leads to large data requirements to achieve good performances. A large line of research, therefore, proposed methods for better aligning observation and action spaces to alleviate the data requirements of learning from demonstrations [5], [6], [7], [10], [15], [16], [43], [44]. In particular, the meth-ods can be grouped into works that propose to represent and optimize the actions directly in a visual, pixel-related space [5], [7], [8], [45], [46], [47]. Alternatively, and more closely related to this work, other approaches represent both observations and actions in a three-dimensional space [6], [9], [11], [15], [16], [43], [44], [48], [49], [50], [51], [52]. However, most of the previously mentioned methods [5], [7], [9], [15], [16], [44], [45], [46], [47], [49], [50], [51] focus on solving manipulation tasks by solely optimizing for the robot's grasping and placing poses, thereby necessitating an additional planning module for obtaining the robot's motion. Conversely, this work presents a novel policy class suitable for online, reactive motion generation in real time. Our proposed method is capable of handling pick-and-place and fine-insertion tasks without any additional intermediate motion planning. Moreover, inspired by [11], [45], [50], [51], our proposed method also exploits the concept of SE(3)-equivariance to further boost efficiency w.r.t. learning from few demonstrations. Additionally, while prior works operate within specific observation spaces (e.g., point clouds [11],"}, {"title": "VI. CONCLUSION", "content": "We presented ActionFlow, a new policy class for robot learning from demonstrations. On the representation level, ActionFlow consists of an SE(3) Invariant Transformer equipped with geometry-aware Invariant Point Attention. Actions are generated using Flow Matching, a new generative model capable of obtaining high-quality samples with low inference times. The resulting policies are fast and efficient, represent actions and observations in one common space, and yield SE(3) equivariant action generation. Our experi-ments underline the effectiveness of ActionFlow's individual components and demonstrate its capabilities for solving real robotic manipulation tasks. In the future, we would like to explore ActionFlow's capabilities for solving longer-horizon manipulation tasks."}, {"title": "APPENDIX", "content": "A. Additional Details on Flow Matching in SE(3)\nIn Section III, we introduced Flow Matching in the Lie group SE(3). For completeness, we provide the pseudo-code for both training (Algorithm 1) and sampling (Algorithm 2) with flow-based models.\nB. Flow-based Policies in Euclidean Space\nIn this section, we describe how to represent a Flow Matching based policy in the Euclidean space. We provide the details as we actually evaluated the performance of Euclidean Flow Matching policies in Section IV-A. We decided to use Flow Matching in the Euclidean space in Section IV-A, as it allowed for a fairer comparison with the other baselines. We propose modeling a policy $\\pi_\\theta(a|o)$ as a Continuous Normalizing Flow (CNF) [30] trained via CFM (Equation (1)). Flow-based policies are expressive, able to represent multimodal action distributions yet simple. Additionally, the training is stable, and the sampling method is simple and deterministic. Similar to previous works [2], [4], we represent the action space as a trajectory of future actions.\nThe problem in flow matching boils down to designing a conditioned flow that drives randomly sampled points to the dataset. In the following, we present a popular flow (Rectified Linear Flow) and showcase how it can be used to generate robot actions."}, {"title": "C. Equivariant Generation with an Invariant Model", "content": "This section provides additional details explaining how exactly we obtain SE(3) equivariant action generation, given that the underlying transformer model is SE(3) invariant. Given a policy $\\pi(T_a | F_o, T_o)$ that generates action poses $T_a$, given the observation poses $T_o$, the policy is SE(3) equivariant if under a transformation $T_s \\in SE(3)$ over the observations, the distribution over the actions is similarly transformed, i.e., $\\pi(T_a | F_o, T_o) = \\pi(T_sT_a | F_o, T_sT_o)$.\nOur proposed ActionFlow achieves equivariance by updat-ing the action poses w.r.t. their own local frame. This results in equivariant action generation as long as the underlying model is invariant, as we will show in the following. This property has been previously exploited in protein folding problems [19], [21], [20].\nLet us consider the update rule represented in Equation (4)\n$\\begin{aligned}p_{k+1} &= p_k + r_k v_\\theta(T_k, T_o, F_o, t) \\Delta t \\\\r_{k+1} &= r_k \\text{Exp}(\\Delta t v_\\theta(T_k, T_o, F_o,t))\\end{aligned}$\n(8)\nwith $p$ being the translation in the world frame, $r$ the rotation matrix in the world frame, and the step length $\\Delta t$. Importantly, the current pose's rotation matrix $r_k$ is premultiplied to the predicted update vectors $v_\\theta$. Therefore, the predicted update vector operates in the local frame. We aim for equivariant generation, such that if we apply a transformation $T_s = (p_s,r_s) \\in SE(3)$ over the current pose $T_k = (p_k, r_k)$, i.e.,\n$T'_k = (p'_k, r'_k) = (r_s p_k + p_s, r_s r_k),$\n(9)\nand observations $T'_o = T_s T_o$, the updated pose $T'_{k+1}$ is by definition similarly transformed, i.e., $T'_{k+1} = (p'_{k+1},r'_{k+1}) = (r_s p_{k+1} + p_s, r_s r_{k+1})$.\nTo showcase that for equivariant action generation, the model should be invariant, we start by considering the update equations for the transformed poses. They equate to\n$\\begin{aligned}p'_{k+1} &= p'_k + r'_k v'_\\theta(T'_k,T'_o, F_o, t) \\Delta t \\\\r'_{k+1} &= r'_k \\text{Exp}(\\Delta t v'_\\theta(T'_k,T'_o, F_o, t)).\\end{aligned}$\n(10)\nBy inserting the definitions for $(p'_k, r'_k)$ and $(p'_{k+1}, r'_{k+1})$ in Equation (10), we obtain\n$\\begin{aligned}r_s p_{k+1} + p_s &= r_s p_k + r_s r_k v'_\\theta(T'_k,T'_o, F_o, t) \\Delta t + p_s \\\\r_s r_{k+1} &= r_s r_k \\text{Exp}(\\Delta t v'_\\theta(T'_k,T'_o, F_o, t)).\\end{aligned}$\n(11)\nWe observe that we can cancel $p_s$ and $r_s$ on each side of the equations and obtain\n$\\begin{aligned}p_{k+1} &= p_k + r_k v'_\\theta(T'_k,T'_o, F_o, t) \\Delta t \\\\r_{k+1} &= r_k \\text{Exp}(\\Delta t v'_\\theta(T'_k,T'_o, F_o, t)).\\end{aligned}$\n(12)\nSince Equation (12) has to hold true for the model to generate equivariant actions, it follows (from Equation (8)) that the model has to be invariant, i.e., $v'_\\theta(T'_k,T'_o, F_o, t) = v_\\theta(T_k, T_o, F_o, t)$."}, {"title": "D. Invariant Point Attention", "content": "This section provides pseudo-code for the Invariant Point Attention mechanism, the key element for our SE(3) In-variant Transformer. Since Invariant Point Attention was originally proposed in the context of protein folding [19], our pseudo-code in Algorithm 3 aims to provide additional context from a robotics perspective. The algorithm receives the set of features $F$ and their associated poses $T$ as input. The other inputs are hyper parameters. The algorithm describes the update for one token with associated feature vector $f_i$ and pose $T_i$. Implementation-wise, we built on top of [57]."}, {"title": "E. Robomimic Experiments", "content": "In this section, we present more detailed results obtained in four Robomimic tasks (cf. Fig. 10) [36] using both state and image-based observations. Robomimic contains human demonstrations of several robotic manipulation tasks in simulated environments.\nAs mentioned in the main text, in these experiments, the diffusion process in Diffusion Policy [2] is replaced with a flow matching process in Euclidean space (App. B) - we refer to this policy as Flow Matching. We use the transformer architecture from [2] to model both the flow matching vector field and the denoising diffusion model."}, {"title": "F. Mimicgen Experiments", "content": "In this section, we provide a more detailed description of the experiment introduced in Section IV-B.\n1) Observations and Actions Representation: ActionFlow represents both the observations and actions with a tuple of poses $T$ and features $F$. Each pair of pose and feature represents a different entity in the space. For the experiments in Section IV-B, the pose represents the location of the different relevant objects in the space, while the feature is a fixed identifier for each object. The considered objects in each task are:\n2) Policy Representation: ActionFlow's network is a SE(3) Invariant Transformer as introduced in Section III-B. We additionally introduce an adaptation and normalization module, which is applied to the poses before they are further processed inside the transformer network. This is a common practice when training deep learning models.\nAdaptation Module. Given a set of observation and action poses, we represent all the poses in the end-effector's frame. Then, we scale the translation vectors with a scaling factor of 10 and apply Tanh to the translations to regularize the distances to a range within -1 and 1. The objective of this adaptation module is to increase the distance error between the points in the IPA module, while reducing variability when the object's are too far. We represent all the poses around the end effector to guarantee that the initial distribution of the flow is centered close to the actions."}, {"title": "G. Real Robot Experiments", "content": "This section provides additional details and results regard-ing our real robot experiments presented in Section IV-C.1) Additional Information regarding the Teleoperation In-terface: The teleoperation interface used in our real robot experiments consists of two main components. We leverage an off-the-shelf presenter [40] for conveniently starting and stopping the recording of the individual demonstrations, as well as controlling, i.e., opening and closing the gripper. To control the pose of the robot's end effector, we rely on the OptiTrack Motion capture system. In particular, as shown in Fig. 5, the teleoperator wears a glove that has OptiTrack markers rigidly attached to it. Upon starting teleoperation, the glove's current pose is defined as the reference. Moving the glove w.r.t. this reference results in moving the robot's end-effector w.r.t. its initial pose accordingly. The teleoperation interface is set to operate at 25 Hz. Throughout all the real robot experiments, we use this teleoperation interface to control the robot end-effector's 6D pose, as well as the gripper opening width through a binary signal corresponding to gripper open / closed. Last, we want to point out that the last phase of the lightbulb mounting task solely necessitates a rotation to fix the bulb and turn it on. We found it extremely challenging to command a pure rotation around the end-effector's upward-pointing axis through the teleoperation interface. We, therefore, assigned one of the presenter's keys to trigger a rotation of 67\u00b0 around the end-effector's upward-pointing axis. Thus, for the lightbulb mounting task, the teleoperator is mainly tasked with inserting the lightbulb's pins into the socket, and subsequently, the necessary rotation can be achieved by pressing the presenter's respective key.\n2) Robot Control: As shown, e.g., in Fig. 1 & Fig. 5, this work uses a Franka Panda 7 DoF manipulator equipped with a parallel gripper. On the lowest level, we control the robot through the effort joint interface. This interface requires real-time control actions at 1000 Hz. For converting the current desired end-effector pose (which is either provided through the teleoperation interface or the running policy) into the effort joint commands, we build on top of the Cartesian Pose Impedance Controller provided in [41]. Since we do not have any smoothness guarantees on the output of our teleoperation interface and the policy's output, we employ exponential smoothing on the update of the desired target pose. In practice, we found this measure sufficient to stay within the Franka Panda robot's acceleration limits and yield smooth trajectories for both teleoperation and policy rollouts.\n3) ActionFlow for Real Robot Manipulation - Imple-mentation Details: The real robot experiments presented in Section IV-C are conducted using ActionFlow, i.e., the combination of the proposed SE(3) Invariant Transformer and SE(3) Flow Matching on the action space resulting in equivariant action generation.\nObservations & Actions. For both experiments, we use an observation history of 5 steps and predict an action sequence containing 16 steps. In line with the teleoperation interface (which is set to collect actions at 25 Hz), we employ a time discretization of 0.04 s. While the RealSense camera returns RGB readings with a resolution of 640 \u00d7 480, we resize the images to 80 \u00d7 80 pixels before passing them through the ResNet18 [39] for obtaining the encodings. The resizing helps to reduce the dataset's size significantly and, therefore, facilitates & speeds up policy training.\nTraining. We parametrize our ActionFlow policies for real robot manipulation using the SE(3) Invariant Transformer introduced in Fig. 2, and use four layers of IPA. Additionally, we consider K = 4 inference steps. We train the policies for 75 epochs and evaluate the last checkpoint. We use a machine with the following components (CPU, RAM, GPU): AMD EPYC 7453 28-Core; 512 GB RAM; RTX 3090 Turbo (24 GB).\nInference. As mentioned in Section IV-C, our policies are efficient and can be run in real-time. On average, it takes 0.03s to generate an action sequence of 16 steps on an NVIDIA RTX 3090 GPU. On the real robot, we also account for the delay between passing the observations to the model and obtaining the action sequences. This is done by monitoring the time required for model inference and skipping the respective entries within the action sequence. In particular, we skip the actions that should have been applied at times when the model inference was still active. Moreover, we do not apply the whole remaining action sequence after each call to the model. Instead, we leverage our model's fast inference speeds and only apply the two next actions. Additionally, we employ exponential smoothing to ensure a smooth transition when updating the action sequence.\nModified Image Observations for the Mug Hanging Task. The mug hanging task can be divided into two phases, i.e., 1) reaching and grasping the mug, and subsequently, 2) hanging it. While for the first part, the view of the robot's wrist-mounted camera is essential, for the second phase of approaching the hanger, apart from the mug's pose, the camera image does not contain any information. In initial experiments, we nevertheless found that the ResNet18 extracts spurious correlations from background pixels for the phase of approaching the hanger, which harmed performance. To circumvent this issue, once the robot's gripper is closed, we set all pixels to black apart from the image's center region of size 30 \u00d7 30 pixels. This additional inductive bias, i.e., eliminating all the image background information once the mug is grasped, effectively improved the models' performance. We want to point out that a similar effect could be achieved by masking out background pixels based on the camera's depth information. However, we discovered that the depth readings from the Intel RealSense D435 were not accurate enough for this purpose, so we decided to employ the previously described image masking once the gripper is closed.\n4) Additional Information on the Point Cloud conditioned Mug Hanging Experiment: This last section provides addi-tional details about the mug hanging experiment presented in Section IV-C. For the point cloud conditioned mug hanging experiment, we had to switch from the RealSense D435 which is used in all other experiments to an RealSense D405. The reason for this change in RGB-D camera is"}]}