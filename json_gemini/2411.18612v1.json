{"title": "Robust Offline Reinforcement Learning with Linearly Structured f-Divergence Regularization", "authors": ["Cheng Tang", "Zhishuai Liu", "Pan Xu"], "abstract": "The Distributionally Robust Markov Decision Process (DRMDP) is a popular framework for addressing dynamics shift in reinforcement learning by learning policies robust to the worst-case transition dynamics within a constrained set. However, solving its dual optimization oracle poses significant challenges, limiting theoretical analysis and computational efficiency. The recently proposed Robust Regularized Markov Decision Process (RRMDP) replaces the uncertainty set constraint with a regularization term on the value function, offering improved scalability and theoretical insights. Yet, existing RRMDP methods rely on unstructured regularization, often leading to overly conservative policies by considering transitions that are unrealistic. To address these issues, we propose a novel framework, the d-rectangular linear robust regularized Markov decision process (d-RRMDP), which introduces a linear latent structure into both transition kernels and regularization. For the offline RL setting, where an agent learns robust policies from a pre-collected dataset in the nominal environment, we develop a family of algorithms, Robust Regularized Pessimistic Value Iteration (R2PVI), employing linear function approximation and f-divergence based regularization terms on transition kernels. We provide instance-dependent upper bounds on the suboptimality gap of R2PVI policies, showing these bounds depend on how well the dataset covers state-action spaces visited by the optimal robust policy under robustly admissible transitions. This term is further shown to be fundamental to d-RRMDPs via information-theoretic lower bounds. Finally, numerical experiments validate that R2PVI learns robust policies and is computationally more efficient than methods for constrained DRMDPs.", "sections": [{"title": "Introduction", "content": "Offline reinforcement learning (Offline RL) (Levine et al., 2020) enables policy learning without direct interaction with the environment, necessitating robust policies that remain effective under environment shifts (Garcia and Fern\u00e1ndez, 2015; Packer et al., 2018; Zhang et al., 2020; Wang et al., 2024b). A widely adopted framework for learning such policies is the distributionally robust Markov decision process (DRMDP) (Iyengar, 2005; Nilim and El Ghaoui, 2005), which models dynamics changes as an uncertainty set around a nominal transition kernel. In this setup, the agent seeks to perform well even in the worst-case environment within the uncertainty set. The most common design of these uncertainty sets is the (s, a)-rectangularity (Zhang et al., 2020; Blanchet et al., 2024; Shi and Chi, 2024), which models uncertainty independently for each state-action pair. Although mathematically elegant, the (s, a)-rectangularity can include unrealistic dynamics, often resulting in overly conservative policies. To address this issue, Goyal and Grand-Clement (2023) introduce the r-rectangular set, which parameterizes transition kernels using latent factors. When focusing on linear function classes, this concept has been extended to d-rectangular linear DRMDPs (d-DRMDPs), where latent structures in transition kernels and uncertainty sets are linearly parameterized by feature mappings. Building on d-DRMDPs, recent works (Ma et al., 2022; Wang et al., 2024a; Liu and Xu, 2024b) propose computationally efficient algorithms with provable guarantees that leverage function approximation for robust policy learning and extrapolation.\nHowever, the d-DRMDP framework has several limitations that remain unaddressed in existing works, which we summarize as follows. Theoretical Gaps: Current understanding of d-DRMDPs is largely restricted to uncertainty sets defined by the Total Variation (TV) divergence (Liu and Xu, 2024a,b). For uncertainty sets defined by the Kullback-Leibler (KL) divergence, prior works (Ma et al., 2022; Blanchet et al., 2024) rely on additional regularity assumptions regarding the KL dual variable, which is hard to validate in practice. Moreover, x\u00b2-divergence-based uncertainty sets has been shown to be effective in certain empirical RL applications (Lee et al., 2021) and also been analyzed under the (s, a)-rectangularity (Shi et al., 2024). Yet there are no theoretical results or efficient algorithms in d-DRMDPs. Practical Challenges: Existing algorithms (Ma et al., 2022; Liu and Xu, 2024b; Wang et al., 2024a) depend on a dual optimization oracle (see Remark 4.2 in Liu and Xu (2024a)) to estimate the robust value function. The computational complexity of these methods is proportional to the feature dimension d and the planning horizon H. While heuristic methods like the Nelder-Mead algorithm (Nelder and Mead, 1965) can approximate the oracle, they become computationally expensive for high-dimensional features (large d) and extended planning horizons (large H), which are common in real-world applications. These limitations raise an important question:\nCan we design provably and computationally efficient algorithms for robust RL using general f-divergence for modeling uncertainty in structured transition dynamics?\nIn this work, we provide a positive answer to this question. Inspired by the robust regularized MDP (RRMDP) framework under the (s, a)-rectangularity condition (Yang et al., 2023; Zhang et al., 2020; Panaganti et al., 2024), where the uncertainty set constraint in DRMDP is replaced by a regularization penalty term measuring the divergence between the nominal and perturbed dynamics, we propose the d-rectangular linear RRMDP (d-RRMDP) framework. Specifically, d-RRMDP replaces the d-rectangular uncertainty set in d-DRMDPs with a carefully designed penalty term that preserves the linear structure. The motivations are two folds: (1) it has been shown by Yang et al. (2023) that the robust policy under the RRMDP is equivalent to that under the DRMDP for unstructured uncertainty as long as the regularizer is properly chosen; (2) removing the uncertainty set constraint simplifies the dual problem for certain divergences (Zhang et al., 2023), potentially improving computational efficiency and facilitating theoretical analysis. In this paper, we show that the above two advantages hold for our proposed d-RRMDP. We summarize our contributions as follows:\n\u2022 We establish that key dynamic programming principles, including the robust Bellman equation and the existence of deterministic optimal robust policies, hold under the d-RRMDP framework. Additionally, we derive dual formulations of robust Q-functions for TV, KL, and x\u00b2 divergences, highlighting their linear structures."}, {"title": "Related Work", "content": "Distributionally Robust MDPs. The seminal works of Iyengar (2005); Nilim and El Ghaoui (2005) proposed the framework of DRMDP. There are several lines of works studying DRMDP under different settings. Zhang et al. (2020); Panaganti et al. (2022); Shi and Chi (2024) studied the offline DRMDP assuming access to an offline dataset, and provided sample complexity bounds under the coverage assumption on the offline dataset. Liu and Xu (2024a); Liu et al. (2024); Lu et al. (2024) studied the online DRMDP where an agent learns robust policies by actively interacting with the nominal environment. Blanchet et al. (2024); Panaganti et al. (2022) studied the DRMDP with general function approximation, they focused on the offline setting with the (s, a)-rectangularity assumption. Ma et al. (2022); Liu and Xu (2024b); Wang et al. (2024a) studied the offline d-DRMDP, they proposed provably and computationally efficient algorithms and provided sample complexity bounds under different kinds of coverage assumptions on the offline dataset.\nRRMDPs. The work of Yang et al. (2023); Zhang et al. (2023) proposed the RRMDP, which can be regarded as a generalization of the DRMDP by substituting the uncertainty set constraint in DRMDP with the regularization term defined based on the divergence between the perturbed dynamics and the nominal dynamics. In particular, Yang et al. (2023) studied the tabular RRMDP and proposed a model-free algorithms assuming access to a simulator. Zhang et al. (2023) studied the offline RRMDP, they established connections between RRMDPs with risk sensitive MDPs, and derived the policy gradient principle. Moreover, they studied general function approximation and proposed a computationally efficient algorithm, RFZI, under the RRMDP with the regularization term defined based on the KL-divergence. Zhang et al. (2023) firstly discovered that the duality of the robust value function has a closed expression under the KL-divergence. Panaganti et al. (2024) studied the offline RRMDP with regularization terms defined by general f-divergence. They studied general function approximation and provided sample complexity result under the general f-divergence. They further proposed a hybrid algorithm which learns robust policies with both historical data and online sampling, for RRMDP with TV-divergence regularization term. Their works mostly focus on (s, a)-rectangularity uncertainty regularization, which is different from ours."}, {"title": "Problem formulation", "content": "Markov decision process (MDP). We first introduce the concept of MDPs, which is the basis of our settings. Specifically, we denote $MDP(S, A, H, P^{\\circ}, r)$ as a finite horizon MDP, where the $S$ is the state space, $A$ is the action space, $H$ is the horizon length, $P^{\\circ} = \\{P^{\\circ}_h\\}_{h=1}^H$ are nominal transitional kernels, and the $r(s, a) \\in [0, 1]$ is the deterministic reward function assumed to be known in advance. For any policy $\\pi$, the value function and Q-function at time step h is defined as\n$V_h^{\\pi}(s) = E_{P^{\\circ}}^{\\pi} [\\sum_{t=h}^H r_t(S_t, a_t) | s_h = s, \\pi]$\n$Q_h^{\\pi}(s, a) = E_{P^{\\circ}}^{\\pi} [\\sum_{t=h}^H r_t(S_t, a_t) | s_h = s, a_h = a, \\pi]$\nThe robust regularized MDP (RRMDP) The RRMDP framework with a finite horizon is defined by RRMDP(S, A, H, P\u00ba, r, \u03bb, D, F), where \u039b is the regularizer, D is the probability divergence metric, and F is the feasible set of all perturbed transition kernels. For any policy \u03c0, the robust regularized value function and Q-function are defined as\n$V_h^{\\pi, \\lambda}(s) = \\inf_{P \\in F} E_{P}^{\\pi} [\\sum_{t=h}^H (r_t(s_t, a_t) + \\lambda D(P_t(\\cdot | s_t, a_t) || P^{\\circ}_t(\\cdot | s_t, a_t))) | s_h = s, \\pi]$ (3.1)\n$Q_h^{\\pi, \\lambda}(s, a) = \\inf_{P \\in F} E_{P}^{\\pi} [\\sum_{t=h}^H (r_t(s_t, a_t) + \\lambda D(P_t(\\cdot | s_t, a_t) || P^{\\circ}_t(\\cdot | s_t, a_t))) | s_h = s, a_h = a, \\pi]$ (3.2)\nThe RRMDP framework has been referred to by different names in the literature, including the penalized robust MDP (Yang et al., 2023), the soft robust MDP (Zhang et al., 2023), and the robust o-regularized MDP (RRMDP) (Panaganti et al., 2024). For consistency, we adopt the term RRMDP in this work. In RRMDPs, the transition kernel class F typically encompasses all possible kernels. However, for environments with large state-action spaces, F may be overly broad, including transitions that are unrealistic or irrelevant. To address this, we introduce latent structures on transition kernels and design regularization penalties that capture dynamics changes in the"}, {"title": "The d-rectangular linear RRMDP (d-RRMDP)", "content": "latent structure, which is similar to r-rectangular MDPs (Goyal and Grand-Clement, 2023) and d-rectangular linear DRMDPs (Ma et al., 2022).\nThe d-rectangular linear RRMDP (d-RRMDP). In this paper, we propose a novel d-RRMDP, which admits linear structure of feasible set and reward function. Specifically, a d-RRMDP is a RRMDP where the nominal environment P\u00ba is a special case of linear MDP with a simplex feature space (Jin et al., 2020, Example 2.2), and the feasible sets F involves kernels defined based on the linear structure of the nominal transition kernel. Specifically, we make the following assumption:\nAssumption 3.1 (Jin et al. (2020)). Given a known state-action feature mapping $\\phi : S \\times A \\rightarrow \\mathbb{R}^d$ satisfying $\\sum_{i=1}^d \\phi_i(s, a) = 1, \\phi_i(s, a) \\geq 0$. Further more, we assume the reward function $\\{r_h\\}_{h=1}^H$ and the nominal transition kernels $\\{P_h^{\\circ}\\}_{h=1}^H$ admit linear structures. Specifically, we have\n$r_h(s, a) = (\\phi(s, a), \\theta_h), P_h(\\cdot | s, a) = (\\phi(s, a), \\mu_h(\\cdot)),$ for all $(h, s, a) \\in [H] \\times S \\times A,$\nwhere $\\{\\theta_h\\}_{h=1}^H$ are known vectors with bounded norm $|\\theta_h||_2 \\leq \\sqrt{d}$ and $\\{\\mu_h\\}_{h=1}^H$ are unknown probability measure vectors over S, i.e., $\\mu_h = (\\mu_{h,1}, \\mu_{h,2}, \\dots, \\mu_{h,d}), \\mu_{h,i} \\in \\triangle(S), \\forall i \\in [d]$.\nWith Assumption 3.1, due to our concentration on linear-structured based feasible set F, the robust regularized value function and Q-function are defined as\n$V_h^{\\pi, \\lambda}(s) = \\inf_{\\mu_t \\in \\triangle(S)^d, P_t=(\\phi, \\mu_t)} E_{\\{P_t\\}_{t=h}^H}^{\\{\\pi\\}} [\\sum_{t=h}^H (r_t(s_t, a_t) + \\lambda (\\phi(s_t, a_t), D(\\mu_t || \\mu_t^{\\circ}))) | s_h = s, \\pi]$ (3.3)\n$Q_h^{\\pi, \\lambda}(s, a) = \\inf_{\\mu_t \\in \\triangle(S)^d, P_t=(\\phi, \\mu_t)} E_{\\{P_t\\}_{t=h}^H}^{\\{\\pi\\}} [\\sum_{t=h}^H (r_t(s_t, a_t) + \\lambda (\\phi(s_t, a_t), D(\\mu_t || \\mu_t^{\\circ}))) | s_h = s, a_h = a, \\pi];$\nwhere $D(\\mu || \\mu^{\\circ}) = (D(\\mu_1 || \\mu_1^{\\circ}), D(\\mu_2 || \\mu_2^{\\circ}), \\dots, D(\\mu_d || \\mu_d^{\\circ}))$ is the concatenated vector of $D(\\mu_i || \\mu_i^{\\circ})$. In other words, we only consider perturbed kernels in the linear feasible set\n$F_L = \\{P = \\{P_h\\}_{h=1}^H | P_h(\\cdot | s, a) = (\\phi(s, a), \\mu_h(\\cdot)), \\mu_h = (\\mu_{h,1}, \\mu_{h,2}, \\dots \\mu_{h,d}), \\mu_{h,i} \\in \\triangle(S), \\forall i \\in [d]\\}$.\nThe optimal robust regularized value function and Q-function is defined as:\n$V_h^{\\pi^*, \\lambda}(s) = \\sup_{\\pi} V_h^{\\pi, \\lambda}(s), Q_h^{\\pi^*, \\lambda}(s, a) = \\sup_{\\pi} Q_h^{\\pi, \\lambda}(s, a).$ (3.4)\nBased on (3.4), the optimal robust policy is defined as the policy that achieves the optimal robust regularized value function at time step 1, i.e, $\\pi^{*, \\lambda} = argmax_{\\pi} V_1^{\\pi, \\lambda}(s), \\forall s \\in S$.\nDynamic Programming Principles for d-RRMDP For completeness, we first show that the dynamic programming principles (Sutton, 2018) hold for the d-rectangular linear RRMDP.\nProposition 3.2. (Robust Regularized Bellman Equation) Under the d-rectangular linear RRMDP, for any policy $\\pi$ and any $(h, s, a) \\in [H] \\times S \\times A$, we have\n$Q_h^{\\pi, \\lambda}(s, a) = r_h(s, a) + \\inf_{\\mu_h \\in \\triangle(S)^d, P_h=(\\phi, \\mu_h)} [E_{s' \\sim P_h(\\cdot | s, a)} [V_{h+1}^{\\pi, \\lambda}(s')] + \\lambda (\\phi(s, a), D(\\mu_h || \\mu_h^{\\circ}))]$, (3.5)\n$V_h^{\\pi, \\lambda}(s) = E_{a \\sim \\pi(\\cdot|s)} [Q_h^{\\pi, \\lambda}(s, a)].$ (3.6)"}, {"title": "Offline Dataset and Learning Goal", "content": "Next, we show that the optimal robust policy is deterministic and stationary.\nProposition 3.3. Under the setting of d-rectangular linear RRMDP, there exists a deterministic and stationary policy $\\pi^*$, such that for any $(h, s, a) \\in [H] \\times S \\times A,$\n$V_h^{*, \\lambda}(s) = V_h^{\\pi^*, \\lambda}(s), Q_h^{*, \\lambda}(s, a) = Q_h^{\\pi^*, \\lambda}(s, a).$ (3.7)\nWith these results, we can restrict the policy class $\\Pi$ to the deterministic and stationary one. This leads to the robust regularized Bellman optimality equation:\n$Q_h^{*, \\lambda}(s, a) = r_h(s, a) + \\inf_{\\mu_h \\in \\triangle(S)^d, P_h=(\\phi, \\mu_h)} [E_{s' \\sim P_h(\\cdot | s, a)} [V_{h+1}^{*, \\lambda}(s')] + \\lambda (\\phi(s, a), D(\\mu_h || \\mu_h^{\\circ}))],$\n$V_h^{*, \\lambda}(s) = \\max_{a \\in A} Q_h^{*, \\lambda}(s, a).$ (3.8)\nOffline Dataset and Learning Goal. Assume that we have an offline dataset D consisting K i.i.d. trajectories collected from the nominal environment by a behavior policy $\\pi^b$. Specifically, for the T-th trajectory $\\{(s_h^{\\tau}, a_h^{\\tau}, r_h^{\\tau})\\}_{h=1}^H$, we have $a_h^{\\tau} \\sim \\pi^b(\\cdot | s_h)$, $r_h^{\\tau} = r_h(s_h^{\\tau}, a_h^{\\tau})$, and $s_{h+1}^{\\tau} \\sim P_h(s_h^{\\tau}, a_h^{\\tau})$ for any $h \\in [H]$. The goal of the offline RRMDP is to learn the optimal robust policy $\\pi^*$ from the offline dataset D. The suboptimality gap between any policy $\\pi$ and the optimal robust policy $\\pi^*$ is\n$SubOpt(\\pi, s_1, \\lambda) := V_1^{*, \\lambda}(s_1) - V_1^{\\pi, \\lambda}(s_1).$ (3.9)\nThe goal of an agent is to learn a $\\pi$ that minimizes the suboptimality gap $SubOpt(\\pi, s, \\lambda), \\forall s \\in S$."}, {"title": "Robust Regularized Pessimistic Value Iteration", "content": "In this section, we first develop a meta-algorithm for offline d-rectangular linear RRMDP with D being the general f-divergence. Then in order to instantiate the meta-algorithm, we provide exact dual form solution under specific the f-divergence D as Total-Variation (TV) divergence, Kullback-Leibler (KL) divergence and Chi-Square (x2) divergence, respectively.\n4.1 Regularized Robust Pessimism Iteration (R2PVI)\nWe first show that under the d-rectangular linear RRMDP, $Q_h^{\\pi, \\lambda}(s, a)$ admits a linear representation.\nProposition 4.1. Under Assumption 3.1, for any $(\\pi, s, a, h) \\in \\Pi \\times S \\times A \\times [H]$, we have\n$Q_h^{\\pi, \\lambda}(s, a) = (\\phi(s, a), \\theta_h + \\lambda \\omega_h^{\\pi, \\lambda}),$ (4.1)\nwhere $\\omega_h^{\\pi, \\lambda} = (w_{h,1}^{\\pi, \\lambda}, w_{h,2}^{\\pi, \\lambda}, \\dots, w_{h,d}^{\\pi, \\lambda}) \\in \\mathbb{R}^d$, and $w_{h,i}^{\\pi, \\lambda} = \\inf_{\\mu_{h,i} \\in \\triangle(S)} [E_{s \\sim \\mu_{h,i}} [V_{h+1}^{\\pi, \\lambda}(s)] + \\lambda D(\\mu_{h,i} || \\mu_{h,i}^{\\circ})]$.\nThe linear representation of the robust Q-function enables linear function approximation. Then, it remains to obtain an estimation, $\\hat{\\omega}_h^{\\pi, \\lambda}$, of the parameter $\\omega_h^{\\pi, \\lambda}$. For now, we focus on the algorithm design and assume that we have an oracle to get $\\hat{\\omega}_h^{\\pi, \\lambda}$. We will instantiate the estimation procedure under different cases with different probability divergence D in the following sections. Leveraging the robust regularized Bellman equation Proposition 3.2 and the pessimism principle (Jin et al., 2021; Liu and Xu, 2024b) well-developed to take account of the distribution shift of the offline dataset, we propose the meta-algorithm in Algorithm 1."}, {"title": "R2PVI with the TV-Divergence", "content": "4.2 R2PVI with the TV-Divergence\nIn this section, we show how to get the estimation in Line 4 of R2PVI for TV divergence based regularization. We first present a result on the duality of the TV-divergence.\nProposition 4.2. Given any probability measure $\\mu^{\\circ} \\in \\triangle(S)$ and value function $V : S \\rightarrow [0, H]$, if the distance D is chosen as the TV-divergence, the dual formulation of the original regularized optimization problem is formed as:\n$\\inf_{\\mu \\in \\triangle(S)} E_{s \\sim \\mu}V(s) + \\lambda D_{TV}(\\mu || \\mu^{\\circ}) = E_{s \\sim \\mu^{\\circ}} [V(s)]_{\\min_{s'}\\{V(s')\\} + \\lambda}.$ (4.2)\nRemark 4.3. We compare the duality of the regularized problem in Proposition 4.2 with the duality of the constraint problem with the TV-divergence in DRMDP (Shi and Chi, 2024):\n$\\inf_{P \\in U_P (P^{\\circ})} E_P V(s) = \\max_{\\alpha \\in [V_{\\min}, V_{\\max}]} \\{E_{P^{\\circ}} [V(s)]_{\\alpha} - \\rho (\\alpha - \\min_{s'} [V(s')]_{\\alpha})\\}.$\nWe highlight that the former has a closed form, while the later needs to be solved through optimization. And both involve a truncation on the value function. We will see later this distinction makes R2PVI much more computationally efficient compared to algorithms designed for DRMDP.\nNext, we present the parameter estimation procedure. Given an estimated robust value function $V_{h+1}^{\\pi, \\lambda}$, we denote $\\alpha_{h+1} = \\min_s V_{h+1}^{\\pi, \\lambda}(s') + \\lambda$. According to the linear representation of the Q-function"}, {"title": "R2PVI with the KL-Divergence", "content": "Proposition 4.1, the duality for TV-divergence Proposition 4.2 and the linear structure of the nominal kernel in Assumption 3.1, we estimate the parameter $\\omega_h^{\\pi, \\lambda}$ as follows\n$\\hat{\\omega}h^{\\pi, \\lambda} = argmin_{\\omega \\in \\mathbb{R}^d} \\sum_{\\tau=1}^K ([V_{h+1}^{\\pi, \\lambda}(s_{h+1}^{\\tau})]_{\\alpha_{h+1}} - (\\phi(s_h^{\\tau}, a_h^{\\tau}), \\omega))^2 + \\gamma ||\\omega||_2^2,$ (4.3)\nwhere \u03b3 is a stabilization parameter to ensure numerical stability and prevent the matrix from becoming ill-conditioned or singular.\nRemark 4.4. Thanks to the closed form expression of the duality for TV in Proposition 4.2, R2PVI does not need the dual oracle as the DRPVI algorithm proposed for the d-rectangular linear DRMDP (Liu and Xu, 2024b, see their equation (4.4) and Algorithm 1 for more details). They need to solve dual oracle separately for each dimension in each iteration, which is not necessary in our algorithm.\n4.3 R2PVI with the KL-Divergence\nSimilar to TV-divergence, we next show how to get the estimation in Line 4 of Algorithm for KL divergence based regularization. We first present a result on the duality of the KL-divergence.\nProposition 4.5. (Zhang et al., 2023, Example 1) Given any probability measure $\\mu^{\\circ} \\in \\triangle(S)$ and value function $V : S \\rightarrow [0, H]$, if the probability divergence D is chosen as KL-divergence, the dual formulation of the original regularized optimization problem is:\n$\\inf_{\\mu \\in \\triangle(S)} E_{s \\sim \\mu}V(s) + \\lambda D_{KL} (\\mu || \\mu^{\\circ}) = - \\lambda log E_{s \\sim \\mu^{\\circ}} [e^{-V(s) / \\lambda}].$ (4.4)\nThe duality of KL also has a closed form, which is first observed in Zhang et al. (2023). We will see that this property will not only lead to more computationally efficient algorithm, but also simplify the theoretical analysis. Next, we present the parameter estimation procedure. According to the linear representation of the Q-function in Proposition 4.1, the duality for TV-divergence in Proposition 4.5 and the linear structure of the nominal kernel in Assumption 3.1, we estimate the parameter $\\omega_h^{\\pi, \\lambda}$ by a two-step procedure. Given an estimated robust value function $V_{h+1}^{\\pi, \\lambda}$, we first solve the following ridge-regression to get an estimation of $E_{s \\sim \\mu^{\\circ}}e^{-V_{h+1}(s) / \\lambda}$\n$\\hat{\\omega}h^{\\pi, \\lambda} = argmin_{\\omega \\in \\mathbb{R}^d} \\sum_{\\tau=1}^K (e^{-V_{h+1}(s_{h+1}^{\\tau}) / \\lambda} - (\\phi(s_h^{\\tau}, a_h^{\\tau}), \\omega))^2 + \\gamma ||\\omega||_2^2.$ (4.5)\nWe then take a log-transformation to get an estimation of $\\omega_h^{\\pi, \\lambda}$:\n$\\hat{\\omega}_h^{\\pi, \\lambda} = - \\lambda log \\max\\{\\hat{\\omega}_h^{\\pi, \\lambda}, e^{-H / \\lambda}\\}.$ (4.6)\nNote that the max operator is to ensure the ridge-regression estimator is well-defined to take log-transformation, and $e^{-H / \\lambda}$ is a lower bound on $E_{s \\sim \\mu^{\\circ}}e^{-V_{h+1}(s) / \\lambda}$\nRemark 4.6. Ma et al. (2022); Blanchet et al. (2024) need to deal with hard dual oracle under KL divergence, while our algorithms have close form solution. Their works require sophisticated techniques to guarantee the estimated parameter well-defined to take log-transformation, while our algorithm does not need such techniques, which reduces the computational cost and eases the theoretical analysis (see Section 5 and Section 6 for detailed comparison)."}, {"title": "R2PVI with the x\u00b2-Divergence", "content": "4.4 R2PVI with the x\u00b2-Divergence\nNow it remains to show how to get the estimation in Line 4 of Algorithm for x\u00b2-divergence based regularization. We first present a result on the duality of the x\u00b2-divergence.\nProposition 4.7. Given any probability measure $\\mu^{\\circ} \\in \\triangle(S)$ and value function $V : S \\rightarrow [0, H]$, if the probability divergence D is chosen as the x\u00b2-divergence, the dual formulation of the original regularized optimization problem is:\n$\\inf_{\\mu \\in \\triangle(S)} E_{s \\sim \\mu}V(s) + \\lambda D_{\\chi^2} (\\mu || \\mu^{\\circ}) = \\sup_{\\alpha \\in [V_{\\min}, V_{\\max}]} \\{E_{s \\sim \\mu^{\\circ}} [V(s)]_{\\alpha} - \\frac{1}{4 \\lambda} Var_{s \\sim \\mu^{\\circ}} [V(s)]_{\\alpha}\\} (4.7)\nNext, we present the parameter estimation procedure. According to the linear representation of the Q-function in Proposition 4.1, the duality for x\u00b2-divergence in Proposition 4.7 and the linear structure of the nominal kernel in Assumption 3.1, estimate the parameter $\\omega_h^{\\pi, \\lambda}$ as follows. To estimate the variance of the value function in (4.7), we propose a new method motivated by the variance estimation in (Liu and Xu, 2024b). Specifically, given an estimated robust value function $V_{h+1}^{\\pi, \\lambda}$, and dual variable $\\alpha$, we first estimate $E_{s \\sim \\mu^{\\circ}} [V_{h+1}^{\\pi, \\lambda}(s)]_{\\alpha}$ and $E_{s \\sim \\mu^{\\circ}} [V_{h+1}^{\\pi, \\lambda}(s)]_{\\alpha}^2$ as follows:\n$\\hat{\\omega}h_i^{\\pi, \\lambda}([V_{h+1}^{\\pi, \\lambda}(s)]_{\\alpha}) = argmin_{\\omega \\in \\mathbb{R}^d} \\sum_{\\tau=1}^K ([V_{h+1}(s_{h+1}^{\\tau})]_{\\alpha} - (\\phi_i(s_h^{\\tau}, a_h^{\\tau}), \\omega))^2 + \\gamma ||w||,\\$ [0, H] (4.8)\n$\\hat{\\omega}h_i^{\\pi, \\lambda}([V_{h+1}^{\\pi, \\lambda}(s)]_{\\alpha}^2) = argmin_{\\omega \\in \\mathbb{R}^d} \\sum_{\\tau=1}^K ([V_{h+1}(s_{h+1}^{\\tau})]_{\\alpha}^2 - (\\phi_i(s_h^{\\tau}, a_h^{\\tau}), \\omega))^2 + \\gamma ||w||,\\$ [0, H^2] (4.9)\nwhere the superscript i represents the i-th element of a vector. Then we construct the estimator $\\hat{\\omega}_h^{\\pi, \\lambda}$ as follows\n$\\omega_{h,i}^{\\pi, \\lambda} = \\sup_{\\alpha \\in [(V_{h+1}^{\\pi, \\lambda})_{\\min}, (V_{h+1}^{\\pi, \\lambda})_{\\max}]} \\{E_{\\mu^{\\circ}} [V_{h+1}(s)]_{\\alpha} - \\frac{1}{4 \\lambda} Var_{\\mu^{\\circ}} [V_{h+1}(s)]_{\\alpha}\\} $\n$=\\max_{\\alpha \\in [(V_{h+1}^{\\pi, \\lambda})_{\\min}, (V_{h+1}^{\\pi, \\lambda})_{\\max}]} \\{\\hat{E}_{\\mu^{\\circ}} [V_{h+1}(s)]_{\\alpha} + \\frac{1}{4 \\lambda} ((\\hat{E}_{\\mu^{\\circ}} [V_{h+1}(s)]_{\\alpha})^2 - \\hat{E}_{\\mu^{\\circ}} [[V_{h+1}(s)]^2]_{\\alpha})\\}. (4.10)\nWe note that the parameter estimation procedure appears to be distinct from that of TV and KL, and this is due to the fact that the duality of x\u00b2 does not admit a closed form expression. Specifically, it estimates the parameter $\\omega_h^{\\pi, \\lambda}$ element-wisely, and for each dimension, it solves an optimization problem over an estimated dual formulation. This parameter estimation procedure shares a similar spirit with that in the d-DRMDP with TV divergence (Liu and Xu, 2024a,b)."}, {"title": "Suboptimality Analysis", "content": "In this section, we establish theoretical guarantees for the algorithms proposed in Section"}]}