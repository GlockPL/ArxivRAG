{"title": "Pursuing Top Growth with Novel Loss Function", "authors": ["Ruoyu Guo", "Haochen Qiu"], "abstract": "Making consistently profitable financial decisions in a continuously evolving\nand volatile stock market has always been a difficult task. Professionals from\ndifferent disciplines have developed foundational theories to anticipate price\nmovement and evaluate securities such as the famed Capital Asset Pricing\nModel (CAPM). In recent years, the role of artificial intelligence (AI) in asset\npricing has been growing. Although the black-box nature of deep learning\nmodels lacks interpretability, they have continued to solidify their position\nin the financial industry. We aim to further enhance AI's potential and util-\nity by introducing a return-weighted loss function that will drive top growth\nwhile providing the ML models a limited amount of information. Using only\npublicly accessible stock data (open/close/high/low, trading volume, sector\ninformation) and several technical indicators constructed from them, we pro-\npose an efficient daily trading system that detects top growth opportunities.\nOur best models achieve 61.73% annual return on daily rebalancing with an\nannualized Sharpe Ratio of 1.18 over 1340 testing days from 2019 to 2024,\nand 37.61% annual return with an annualized Sharpe Ratio of 0.97 over 1360\ntesting days from 2005 to 2010. The main drivers for success, especially\nindependent of any domain knowledge, are the novel return-weighted loss\nfunction, the integration of categorical and continuous data, and the ML\nmodel architecture. We also demonstrate the superiority of our novel loss\nfunction over traditional loss functions via several performance metrics and\nstatistical evidence.", "sections": [{"title": "1. Introduction", "content": "Stock price and movement prediction have always been extraordinarily\nchallenging yet heavily sought-after tasks. Before the popularity of artificial\nintelligence and availability of unforeseen computing power present today,\ninitial stages of our financial understanding consist of the Capital Asset Pric-\ning Model (CAPM) (Sharpe, 1964), the Efficient Market Hypothesis (EMH)\n(Fama, 1970), and more. Decades of research following them have witnessed\na vast number of articles that build upon these very fundamental concepts,\nincluding the 3, 4, and 5-factor models (Fama and French, 1993; Carhart,\n1997; Fama and French, 2015). In recent years, these theories still form the\nfoundation underlying a significant amount of work such as the Markov De-\ncision Process (Park et al., 2024) and ARIMA (Box et al., 2015), which have\nadopted more advanced mathematical and statistical techniques.\nThe two main branches of techniques used to analyze stock prices in order\nto develop profitable trading strategies are fundamental and technical. The\nfundamental approach considers both internal factors such as earnings per\nshare (EPS) in financial statements and external factors such as the macroe-\nconomic environment and geopolitical conditions. Analyzing textual data\nsuch as news and online posting platforms in the field of Natural Language\nProcessing (NLP) has also grown significantly in popularity in light of the\nsuccess of Large Language Models (LLMs). There are many models that uti-\nlize word embeddings such as GloVe (Pennington et al., 2014; Zhang et al.,\n2022), BERT (Devlin, 2018) or more (Lin et al., 2022) to process texts, which\ncan be used as one component of the inputs or for sentiment analysis. The\nembeddings from LLMs can replace these traditional embedding techniques\ndue to LLM's much stronger language ability (Kim and Nikolaev, 2024). On\nthe other hand, our work falls under the technical side, where we primarily\nfocus on historical price and volume data and the additional features that are\nconstructed from them. To name a few examples utilizing primarily techni-\ncal indicators, Hoseinzade and Haratizadeh (2019) incorporates information\nfrom related markets as the third dimension of the data; Gezici and Sefer\n(2024) converts time-series Exchange-Traded Funds (ETFs) data containing\ntechnical indicators into 2-dimensional images and uses vision transformers,"}, {"title": "2. Study Overview", "content": "Given the daily features of a stock related to trading volume, momentum,\nvolatility, and trend, which are collected and computed at the end of the day\n$T$, the goal of the study is to calculate a score that ranks all stocks for day\n$T+1$. This score determines how favorable it is to purchase each stock at the\nmarket open of day $T+1$ and hold it until at least by the market open of day\n$T +2$. The score is calculated every day after the market closes to produce"}, {"title": "3. Data Management", "content": "This section is dedicated to presenting the data pipeline in the study.\nWe obtain our data from the publicly available Yahoo Finance database.\nGiven that our current study only requires basic daily information including\nprice (open, close, high, low), trading volume, and sector information, other\nopen source databases can be used as well. All stocks we select are from\nmajor US stock exchanges, such as the New York Stock Exchange (NYSE)\nand Nasdaq. We discuss how we obtain and process data in Section 3.1,\nconstruct several technical indicators/features from the previous step using\nthe Python Technical Analysis (TA) library (Padial, 2018) in Section 3.2, and\nhow data are standardized and organized for training in Section 3.3. This is\nalso partly represented in the left half of Figure 1. The data we use in this\nstudy are publicly available at Mendeley Data."}, {"title": "3.1. Data Sourcing and Processing", "content": "We download daily stock price (open, close, high, low) data, volume (num-\nber of shares traded) data, and sector information from Yahoo Finance using\nits Python API. There are 11 sectors in the dataset, and each stock's sec-\ntor information is stored as a one-hot vector in 12 dimensions, where the\nextra dimension is for those without sector information in Yahoo Finance\n(such as ETFs). Several additional features are constructed from the price\nand volume data in Section 3.2, and sector information is incorporated into\nthe feature dimension through an embedding. This resembles how the po-\nsitional embeddings in the transformer architecture are added to the input\n(Vaswani et al., 2017), but in our case, the same embedding independent of\nthe point in time is added to the feature dimension. We intentionally choose\ntwo time periods 2006-2010 and 2019-2024 to include the 2008 financial crisis\nand its aftermath, the 2020 COVID pandemic, and the heightened fluctua-\ntions observed in the market starting around 2022 due to high inflation, fear\nof recession, and geopolitical factors.\nAfter obtaining data from Yahoo Finance, we filter stocks using the follow-\ning steps. We first remove stocks if their average daily dollar trading volume\nis below 10 million. This is to avoid practical issues where orders cannot\nbe filled promptly due to low volume, as in the simulation, we assume we\nare able to buy and sell stocks as soon as the market opens. Certain stocks\nexperience significant price and trading volume drops starting at some point\nin the selected time period. Their price can become very low and highly"}, {"title": "3.2. Feature Engineering", "content": "We construct two sets of features from the price and volume data ob-\ntained in Section 3.1. The first set of features, which we call the \"Basic\nFeatures,\" contains more commonly known features including momentum\nand basic statistics of the data such as moving averages and standard devi-\nation. The second set of features, which we call the \u201cTechnical Features,\"\ncontains a subset of more technical indicators obtained from the Technical\nAnalysis (TA) library (Padial, 2018). Since our simulated trades all occur\nat market open, all features are calculated with the opening prices instead\nof the more common closing prices by default. In order to maintain the ef-\nficiency of resources, we limit the number of features in each set, amounting\nto a total of only 12 Basic Features and 16 Technical Features. The tech-\nnical features are chosen broadly first, and those highly correlated with the\nexisting features are removed. Recall that we establish in Section 2 that the\ninput shape is (None, m, n), where the first dimension is the mini-batch size,\nm is the length of time-series, and n is the number of features. In this study,\nwe choose m = 20 and n = 12 + 16 = 28. All technical features and their\nbrief descriptions are in Table 1."}, {"title": "3.3. Standardization and Training Setup", "content": "We employ a fixed size moving window for training, validation, and test-\ning. The training/validation set spans 200 days, and the test set contains 20\ndays immediately after. We standardize our data across the time dimension"}, {"title": "3.3.1. First Step of Loss Function - Daily Return", "content": "The construction of our novel return-weighted loss function begins with\nthe calculation of daily returns. The daily return $r_{d,T}$ (or $r_{d}$ if the day $T$ is\nimplicit) of a stock is the percentage change from the opening price $P_{open,T+1}$\nof day $T + 1$ to the opening price $P_{open,T+2}$ of day $T +2$. We calculate $r_{d,T}$ as\n$r_{d,T} = \\frac{P_{open,T+2} - P_{open,T+1}}{P_{open,T+1}},$\nwhich is then used to assign the labels in the training data as follows\n\u2022 $r_{d,T} \\geq 3\\%$ - Strong Buy - $Y_{true} = [0,0,0,0,1]$,\n\u2022 $1\\% < r_{d,T} < 3\\%$ - Buy - $Y_{true} = [0,0,0,1,0]$,\n\u2022 $-1\\% < r_{d,T} \\leq 1\\%$ - Hold - $Y_{true} = [0,0,1,0,0]$,\n\u2022 $-3\\% < r_{d,T} < -1\\%$ - Sell - $Y_{true} = [0,1,0,0,0]$,\n\u2022 $r_{d,T} \\leq -3\\%$ - Strong Sell - $Y_{true} = [1,0,0,0, 0]$.\nThe data distribution after the labeling is (7.43%, 18.50%, 46.73%, 19.45%,\n7.88%) in the 2005-2010 period and (7.79%, 17.61%, 47.83%, 18.92%, 7.85%)\nin the 2019-2024 period. It signals a slightly positive skew in the data which\ncan also skew the model predictions to be more positive, which is discussed\nlater in Section 5.1. The daily return serves as the objective for the three\nbaseline linear regression models in Section 4.2.1 and another baseline CNN\nwith MSE loss introduced in Section 4.2.3. In the formulation of the new loss\nfunction, $r_{d,T}$ serves as the weight for the cross-entropy loss that we describe\nin Section 4.1."}, {"title": "4. Methodology", "content": ""}, {"title": "4.1. Novel Loss Function", "content": "Both the data and the loss function play a significant role in the model's\nperformance. While data provide the resources for model training, the loss\nfunction that an ML model minimizes is a guide that directly impacts the\nperformance and (downstream) applications of the model. Typical loss func-\ntions include Mean Squared Error (MSE) or its variants in the case of asset\npricing and cross-entropy (CE) in the case of classifications. There are three\nnotable considerations in using MSE loss in the current study as follows."}, {"title": "4.2. Model Architecture", "content": "Multi-Layered Perceptrons (MLPs) or Deep Neural Networks (DNNs) are\nrelatively simple supervised machine learning algorithms in terms of data\npreparation and implementation. Given the versatility of these neural net-\nworks in general, they have been deployed in lots of financial literature and\nhave achieved great success (Gu et al., 2020). On the other hand, one key\ndrawback of DNNs is that they only take 1-dimensional inputs. While it is\npossible to collapse multi-dimensional data into 1 dimension, DNNs do not\nhave an inherent understanding of higher dimensional data structure. During\nlearning, they might be able to pick up on the dimensionality in later hidden\nlayers, but this is far from guaranteed and is at the cost of resources.\nFor ML models, we embed the one-hot sector vectors from their original\n12 dimensions into n = 28 dimensions. The embedding is added to the third"}, {"title": "4.2.1. Linear Regression", "content": "To obtain baseline performance, we choose several sets of features to con-\nduct linear regression on. Since linear regression methods do not directly fit\nto the same probability distribution objective of the ML models, the objec-\ntive of linear regression is the daily return $r_{d}$. The sets of chosen features\nthat we perform regression on are\n1. 3-day momentum, number of shares traded;\n2. 3-day momentum, relative strength index;\n3. 3-day momentum, 50-day moving average;\n4. 2-day momentum, 5-day momentum;\n5. the 12 basic features;\n6. all 28 features.\nWe perform ordinary least squares (OLS), ridge and lasso with cross val-\nidation on these six sets of features. The data used for linear regression are\nidentical to those for the ML models later. For the 2005-2010 period, the\nset {3-day momentum, shares traded} achieves the best result; the set con-\ntaining all 28 features leads to the best result for the 2019-2024 period. This\ndemonstrates the shift towards requiring more factors to achieve high returns\nin more recent years. See Table 2 for the detailed results of linear regression\nmethods, where we assume the starting investment amount is 1, and the final"}, {"title": "4.2.2. CNN Architecture", "content": "CNNs are originally developed for tasks involving images, such as classi-\nfication (LeCun et al., 1998), segmentation (Ronneberger et al., 2015) and"}, {"title": "4.2.3. Other Loss Functions", "content": "To compare how well the new loss function performs against traditional\nloss functions, we set up more CNN models equipped with MSE and CE loss\nwith the same architecture as in Section 4.2.2, except that the models with\nMSE loss have 1-dimensional output. Mixture-of-experts ensembles of these\nmodels are formed in the same way. Learning rate schedule and dropout rates\nare changed slightly to ensure convergence and adequate regularization. For\nCNN models with MSE loss, the starting and minimum learning rates are\nstill 0.01 and 0.001. The dropout rate is 40%. For CNN models with CE loss,"}, {"title": "5. Performance Analysis", "content": "We test the performance of the mixture-of-experts ensembles equipped\nwith the new loss function against all other models over the 68 testing peri-\nods of 20 days from 2006-2010 and 67 testing periods from 2019-2024. We\ncalculate the final value of top 10 stocks portfolio, top 10 stocks portfolio\nSR, bottom 10 stocks portfolio SR, long-short 10 stocks SR, top decile SR,\nbottom decile SR, and long-short decile SR. We further analyze other char-\nacteristics of the strategy such as maximal drawdown (MD) and perform\nt-tests. We integrate several mixture-of-experts ensemble models with equal\nweights to increase the portfolio holding capacity without sacrificing much\nof its performance."}, {"title": "5.1. Return Analysis", "content": "For CNNs with the new loss function and CE loss, the ranking of all\nstocks is done by comparing the score. These models output a probability\ndistribution $(p_1, p_2, p_3, p_4, p_5)$ for each stock, corresponding to strong sell, sell,\nhold, buy, and strong buy. The score of each stock on that day is the dot\nproduct between the distribution and the vector [-2, -1, 0, 1, 2], that is,\n$score = -2p_1 - p_2 + p_4 + 2p_5.$\nThe 10 stocks with the highest (resp. lowest) scores are selected to be in the\ntop (resp. bottom) portfolio for that day. To also assess the general ranking\nability, we calculate the return of investing in the top or bottom decile daily\nunder the same rebalancing strategy as in Section 4.2.1. We repeat this\nprocess on every trading day in the testing periods. For CNNs with MSE\nloss, the ranking is determined by sorting the model output. Table 3 lists\nthe performance of all CNN models and lasso regression with cross validation\nfor comparison. We only include the mixture-of-experts CNN ensembles as\nthey have greater stability compared to individual models. We plot the daily\nportfolio value consisting of the top 10 stocks of these models in Figures 4\nand 5.\nWe see that all models outperform the equal-weighted market in both fig-\nures. Even though their final portfolio values differ, all CNN models shown"}, {"title": "5.2. Additional Analysis", "content": "Tables 4 and 5 present the performance metrics of various strategies that\nselect the top 10 stocks during the periods 2005-2010 and 2019-2024, re-\nspectively. The key metrics include t-value, p-value, Maximum Drawdown\n(MD), Maximum Drawdown Duration (MDD), and annual return with SR\nfor reference.\nWe perform t-tests on each model's return against the equal-weighted\nmarket benchmark. A higher t-value and a lower p-value indicate greater\nstatistical significance of returns. Chen and Zimmermann (2021) conducts\nstatistical analysis of past cross-sectional asset pricing methods using a mas-\nsive number of characteristics and regards t-stats greater than 1.96 as a key"}, {"title": "6. Conclusions and Future Directions", "content": "In this study, we build and test an efficient daily trading system utilizing\na novel loss function to pursue top growth opportunities. We add the sec-\ntor embedding cross-sectionally to adjust each feature's impact in different\nsectors, perform 1D convolutions in the time dimension, use fully connected\nlayers to reach a probability distribution output, and train the model with\nthe novel return-weighted loss function. In each retraining period, we reset\nthe learning rate to allow the models to adjust for evolving market condi-\ntions. We utilize mixture-of-experts ensemble to combine individual models\nand increase stability. In terms of return and risk-adjusted return (SR), our\nnew loss function outperforms other traditional loss functions, especially in\nthe later period of 2019-2024. The new loss function also shows a smaller\nmaximum drawdown and faster recovery time comparatively, even under the\ndifficult 2008 financial crisis and 2020 COVID pandemic. Statistical analysis\nestablishes that the superiority of the new loss function is unlikely by chance\nin the more recent period, and that it is behind MSE loss slightly in the\nearlier period. We further showcase the power of the new loss function by\nintegrating several independently trained mixture-of-experts ensembles with\ndifferent randomizations. This removes the potential bias from only report-\ning the best ensemble model. Furthermore, the integrated model has multiple\ntimes the portfolio holding capacity without sacrificing the SR substantially.\nIn the analysis, we observe that none of the models shows significant\ndownside in short positions. We believe this is largely due to the very small\nnumber of features that we consider and the overall upward trend in the US\nmarket. Note that we do not provide the models with fundamental char-\nacteristics (such as PE, EPS, and even market capitalization) that speak a\nlot about each company's intrinsic value, nor do we incorporate any textual\ninput such as news releases and 10-K reports. These considerations are to be\nexplored in future studies, and we believe this will improve the model per-\nformance further when equipped with the novel loss function. The attention\nmechanism is also undoubtedly a next step when there are more sources of\ninput, especially structured data like news articles. With the proposal of our\nnew loss function guiding the training process, this study provides new ideas\ntowards incorporating actual financial impact into model construction and\nincreasing the utility of machine learning models in financial markets."}]}