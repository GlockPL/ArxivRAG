{"title": "A Semi-supervised Fake News Detection using Sentiment Encoding and LSTM with Self-Attention", "authors": ["Pouya Shaeri", "Ali Katanforoush"], "abstract": "Abstract-Micro-blogs and cyber-space social networks are the main communication mediums to receive and share news nowadays. As a side effect, however, the networks can disseminate fake news that harms individuals and the society. Several methods have been developed to detect fake news, but the majority require large sets of manually labeled data to attain the application-level accuracy. Due to the strict privacy policies, the required data are often inaccessible or limited to some specific topics. On the other side, quite diverse and abundant unlabeled data on social media suggests that with a few labeled data, the problem of detecting fake news could be tackled via semi-supervised learning. Here, we propose a semi-supervised self-learning method in which a sentiment analysis is acquired by some state-of-the-art pretrained models. Our learning model is trained in a semi-supervised fashion and incorporates LSTM with self-attention layers. We benchmark our model on a dataset with 20,000 news content along with their feedback, which shows better performance in precision, recall, and measures compared to competitive methods in fake news detection.", "sections": [{"title": "I. INTRODUCTION", "content": "The Internet and the cyber-space networks have increased the volume, speed, and variety of information available to the public, but, the potential for the spread of fake news has also increased, significantly. The platforms such as Facebook, Twitter, and YouTube, allow anyone to create and share contents with millions of users around the world without any editorial assessment or responsibility. Online platforms also allow users to filter and personalize their interested topics, preferences, biases, and to create an environment that adapted with their beliefs and opinions. Online platforms have been also exploited by malicious actors who create and spread fake news for various motives such as financial gain, ideological agenda, political influence or social disruption.\n\nThe challenge of detecting fake news has prompted re- searchers from different disciplines to develop methods and tools to automatically identify and disclose fake news. The algorithmic approaches for detecting fake news are gener- ally classified into three categories: content-based methods, network-based methods, and hybrid methods.\n\nContent-based methods focus on analyzing textual and visual features of online content to determine its authenticity. For example, content-based methods can use NLP techniques to detect linguistic signs of deception or sentiment analysis techniques to gauge the emotional tone of online content[1]. Content-based methods can also use computer vision tech- niques to detect manipulated images or videos[2].\n\nNetwork-based methods focus on analyzing the social con- text and the spread pattern of online contents to determine their authenticity. For example, the node characteristics in complex network analysis and models for dynamic networks is used to classify users or news authenticity[3].\n\nHybrid methods combine both content-based and network- based features to improve fake news detection. Hybrid meth- ods like [4] use deep learning techniques to learn complex representations of online news content and social network characteristics of users or news sources with hybrid learning techniques.\n\nFake news detection is an active and interdisciplinary re- search area involving collaboration between computer sci- entists, social scientists, journalists, and policy makers. The problem of detecting fake news is an evolving and open- ended one that requires constant adaptation to new forms and sources of false and true information, as well as ethical considerations regarding privacy, freedom of expression, and social responsibilities.\n\nToday, media and virtual networks have become a popular means for people to use and share news. Nevertheless, cy- berspace also enables the widespread dissemination of fake news, i.e. news with intentionally false information, and cre- ates significant negative effects on society. Fake news affects the individual as well as the whole society. Fake news can upset the balance of authenticity in the news ecosystem, persuading news audience to accept false or biased stories. For example, some people and organizations are recently spreading fake news on social media for financial and political gain. The effects of fake news sometimes cause irreparable events, such as news that causes physical violence or shootings in the United States. Also, the news of the death of a key figure of a country can have short-term destructive effects on the society until the detection of the fake news clears the confusion for the people. To alleviate this problem, research on fake news detection has recently received much attention. Despite several existing computational solutions for fake news detection, the lack of comprehensive and community-based fake news datasets has become one of the main obstacles for research. Not only the available datasets are scarce and unverifiable, but they are also limited to a specific topic or do not have many features required for the study, such as news content, image, listener feedback, and other features required for comprehensive news analysis[5]. Also, news on social media such as Twitter is generated at a high volume and speed. So that, very few can be labeled as fake or real news in a short period of time[6]. In general, fake news detection could be categorized within the problems where we have little information about the authenticity or fakeness of a news item, and also in the case of a small number of news items, it is possible to state whether it is authentic or fake. Therefore, it is necessary to look at this issue as a semi-supervisory issue[4]. Therefore, to overcome the problem of scarcity of datasets, we use a large-scale and evolved repository in terms of feature structure called FakeNewsNet, which includes comprehensive information with diverse features in news content, spatial information content social and time information and user page information. We provide a comprehensive description of FakeNewsNet, show exploratory analysis of the dataset from different perspectives, and discuss the advantages of Fake- NewsNet for potential applications in the study of fake news in social media. Also, in order to deal with the second defect, we address the problem from perspective of a semi-supervised approach and design and implement a semi-supervised method on the introduced dataset.\n\nIn this paper, we intend to overcome these limitations by using the FakeNewsNet dataset and a novel self-learning semi- supervised deep learning network to detect fake news in social media. The advantages of our approach are as follows:\n\nThe FakeNewsNet dataset is a large-scale and compre- hensive dataset containing news content, social context, and spatiotemporal information for studying fake news in social media. It covers two fact-finding websites, GossipCop and PolitiFact, and covers several topics, e.g., politics, entertain- ment, health, etc. It also provides the tweet IDs of the tweets sharing the news, which can be used to collect additional social media data.\n\nThe self-learning semi-supervised deep learning network is a new method that combines a trust network layer with an artificial neural network to detect fake news. The confidence network layer automatically returns and adds the correct results to help the neural network collect positive sample cases, thereby improving the accuracy of the neural network. Neural network uses graph structure of social media data to capture semantic and relational features of news articles and users [7]. To improve this method, we use new layers such as LSTM with Self-Attention in the architecture of the artificial neural network. Also, to help accuracy and improve the detection of fake news, a sentiment analysis coding method has been used to improve this network.\n\nOur self-learning semi-supervised deep learning network can handle the scarcity and imbalance of labeled data by using a small amount of labeled data and a large amount of unlabeled data. It can also adapt to new scenarios or domains by updating the trust network layer with new data [7].\n\nOur self-learning semi-supervised deep learning network can combine the temporal dynamics and patterns of fake news dissemination using long short-term memory (LSTM) to model the sequence of tweets or news [7]. Also, by using the attention mechanism, it can find out simple language structures among the words.\n\nWe expect that our approach can achieve higher perfor- mance and robustness in detecting fake news in social media."}, {"title": "II. PROPOSED ALGORITHM", "content": "In this section, we intend to improve the method described in [7] by presenting a semi-supervised model employing a self- learning and pseudo-labeling algorithm and a novel transfer learning for sentiment analysis.\n\nDue to the small number of labels in the dataset and the model-based labeling of the dataset, we cannot have absolute confidence in the labeling in semi-supervised methods. To improve accuracy and dependability, numerous techniques dis- card a portion of the data labeled by the model. The data loss standard in semi-supervised learning depends on the type of model and unsupervised network loss functions employed [8]. Pseudo-labeling, which assigns labels to unlabeled data based on the predictions of the [9] model, is a common technique for reducing data loss and enhancing reliable prediction.\n\nHowever, noisy pseudo-labels can reduce model performance, so some methods employ confidence criteria to filter out low-confidence pseudo-labels or weight them differently in the loss function [10], [11], [12].\n\nIn the proposed algorithm for the semi-supervised problem of detecting fake news, we assume that the input data has a small number of labels (X, y) = {D\u2081 \u222a Du}, where D\u2081 are labeled samples in the training dataset of size |L|, then consider:\n\nD(0) = {(X1,Y1), (X2,Y2), ..., (\u03a7\u03b9, \u03c8\u03b9)}\n\nAlso let Du denote the unlabeled samples in the test dataset of size U. Also consider:\n\nDu = {(X1+1, Y1+1), (X1+2, Yl+2), ..., (X1+u, Yl+u)}\n\nThen the workflow of the self-learning semi-supervised deep learning algorithm can be described as follows:\n\n1) Initialization: In the deep learning network, D(0) is used for learning and the learning process is carried out. Then in unsupervised deep network, pseudo-labels D' = {(\u03a7\u03b9+1, \u0177\u03b9+1), ((X1+2, \u01771+2), ..., (X1+u, \u0177l+u)} are generated with the confidence value \u03c3. If \u03c3\u03bf is a threshold for limiting the selection of samples generated with high confidence such that the low confidence data in D is filtered out, then the pseudo-labeled selected set of D can be defined as follows with size Po:\n\n(0) Dpseu \n{(X1+i, \u0177l+i), (X1+i+1, \u0177l+i+1), ..., (X1+i+p, \u0177l+i+p)}\n\n2) Repetition loop: Now we have the new training dataset as the union of previous training data and selected data labeled with high confidence value:\n\nD(1) = D(0) \u222a Dpseu\n= {(X1,Y1), (X1,Y1), . . ., (X\u0131, y\u0131), . . ., (X1+p, Yl+p)}\n\nThis new dataset is used to retrain the deep learning network to generate a new pseudo-trusted label set Dpseu of size P1. This process is then repeated until the entire dataset has pseudo-labels with high confidence values.\n\nSEMI-SUPERVISED SELF-LEARNING ALGORITHM USING PSEUDO-LABELING\n\nIn the initialization phase of the semi-supervised self- learning algorithm, we perform the steps outlined in [7]. The division of the data into three sections, namely training data, evaluation data, and test data, is crucial and clarifies the situation significantly. Then, we divide the training data into k folds and assume that labels are only present in the first fold. If we assume k = 5, we can say that less than 20% of the data we have is labeled and the problem is in semi-supervised mode.\n\nThis algorithm eliminates the ambiguity in the proposed method of the article [7] that has been observed in numerous implementations. This ambiguity existed when collecting two sets of data from the pre-stage and post-stage fold, despite the evaluation and test data. In some existing implementations, for instance, in the first stage, after training the first fold, a portion of the first fold was selected for evaluation, and after pseudo-labeling, it combined randomly with the second fold for the input of the next stage of training, resulting in data leakage. To prevent this, we considered the validation dataset independently from the training data."}, {"title": "PROPOSED MODEL ARCHITECTURE IN SELF-LEARNING SEMI-SUPERVISED ALGORITHM", "content": "As stated previously, the majority of proposed deep neural networks for the problem of detecting fake news can be di- vided into three categories: content-based methods, Network- based methods, and hybrid methods. The hybrid architecture of our proposed deep neural network for this classification is depicted in the Figure 4.\n\nAs can be seen from the Figure 4, this network receives three input features: textual data, numeric data and date. In this paper, we implemented the proposed artificial neural network architecture using Keras, which is a high-level neural network API that runs on top of TensorFlow, a popular open-source platform for machine learning. In this section, we discuss the architecture of different parts of the proposed model:\n\nA. Embedding Layer\n\nThis layer of neural network and weighting receives news textual data, such as headline and text, news source, user's tweet text, other people's response text to that tweet, and some metadata information of users, such as the device/platform from which the tweet was posted, in the form of vectorization, padding, and tokenized. This layer converts positive integers to dense, fixed-size vectors. In fact, the text input layer is defined using the Input class from Keras to accept variable-length input sequences. The encoded input is then sent to an embedding layer, which transforms the input vector into a dense vector representation that is meaningful to the network."}, {"title": "B. LSTM with Self-Attention Layer", "content": "Long Short-Term Memory (LSTM) with Self-Attention layer is a neural network architecture that combines the advantages of LSTM and Attention mechanisms. LSTM is a Recurrent Neural Network (RNN) that can discover long- term dependencies in sequential data, such as text, speech, or time series. Attention mechanisms are techniques that enable a network to concentrate on the most pertinent portions of input or output sequences and to learn how to align them. Self- attention is a subtype of attention in which the network attends to its own internal states as opposed to external inputs or outputs [13], [14]. A typical LSTM with Self-Attention layer includes an encoder LSTM that processes the input sequence and generates a sequence of hidden states, and a decoder LSTM that generates the output sequence based on the encoder states and its own previous states. Depending on the task [15], the self-attention layer is applied to either the encoder states or the decoder states, or both [15]. The self-attention layer computes a weighted sum of all the states in a sequence, where the weights are learned based on the similarity between states (Figure 5). In addition to capturing long-range dependencies and global information within a sequence, the self-attention layer can also reduce the dimension of the states.\n\nIn frameworks such as TensorFlow, Keras, and PyTorch, LSTM with Self-Attention layer can be implemented in vari- ous ways. One common approach is to use an existing attention layer, such as self attention or Attention in TensorFlow, and feed the same tensor twice as the query and value argu- ments, which results in self-attention. An alternative method is to write a custom layer that implements the self-attention formula, which involves computing the dot product between the states, applying a softmax function, and multiplying by the states once more. We implemented our LSTM with Self- Attention by defining the following states:\n\n$u_t = tanh(W_w h_t + b_w)$ (1)\n\n$d_t = \\frac{exp(u_f u_w)}{\\sum_{t' \\in S} exp(u_f u_w)}$ (2)\n\n$S = \\sum d_t h_t$ (3)"}, {"title": "C. Fully Connected Dense Layers", "content": "The Fully Connected Dense layer applies a linear trans- formation followed by an activation function to produce the output. The activation function then applies a nonlinear transformation to the previous layer's output. The output of the previous layer is then sent to a dense layer with ReLU activation. The final output of the model is generated by passing the output of the dense layer through a second dense layer with sigmoid activation."}, {"title": "USING TRANSFER LEARNING OF SENTIMENT ANALYSIS IN DETECTING THE FAKE NEWS", "content": "In this section, we will describe a transfer learning-based method for sentiment analysis and add it to the model proposed in this paper. Using labeled data from a source domain or task that is related to the target domain or task can help transfer learning overcome the problem of data scarcity. Transfer learning, for instance, can utilize labeled data from sentiment analysis of a text to detect fake news [16]. Text sentiment analysis is the process of identifying and extracting the opinions and emotions expressed in a text. The task of detecting and classifying news articles as real or fake based on their content and context is known as fake news detection.\n\nThe rationale for applying transfer learning from sentiment analysis of a text to the detection of fake news is that both tasks involve analyzing textual information and extracting useful features that can indicate the text's credibility and quality. For instance, the following features can be useful for both tasks:\n\n\u2022 Polarity and intensity of emotions expressed in the text\n\u2022 Using emotional words, exaggeration or contradiction in the text\n\n\u2022 The presence or absence of factual evidence, sources, or references in the text\n\u2022 style, tone and readability of the text\n\nA model can learn these features from a source domain or task and transfer them to a target domain or task using transfer learning. This can help improve model performance in the target domain or task by reducing the need for large quantities of labeled data and enhancing the model's generalization capability [17].\n\nIn this paper, we intend to implement the prediction of a transfer learning based on sentiment analysis on two columns of news text and tweet text of FakeNewsNet data with a coding method. We have used two pre-trained pipelines from ROBERTa (Robustly Optimized BERT Pre-training Approach) specifically for the news text and the tweets [18]. Then, we encode the two columns to the nominal features of sentiment analysis in the dataset (Figure 6). This process is called Sentiment Encoding.\n\nIn fact, a hidden pattern of sentiment in a text according to the news text and tweet text helps the deep neural network in the semi-supervised self-learning model to label and identify fake news using text sentiment analysis."}, {"title": "IV. EXPERMIENTAL RESULTS", "content": "In this section, we will first examine the outcomes of the proposed model's evaluation. The results obtained in the article introducing this dataset will then be examined. Then, we examine the outcomes of semi-supervised models with various architectures on this dataset and compare them to the outcomes of the proposed method. The evaluations were carried out on a device with 13GB of RAM and an Intel Xeon CPU processor with a base clock of 2.2 GHz and an NVIDIA P100 GPU with 16 GB VRAM.\n\nA. Evaluation of the Proposed Model\n\nIn this paper, to evaluate the semi-supervised self-learning model, the fold-by-fold results, which are all on data outside of the training data even when they are not in the fold, are given in Table I.\nB. Comparison of the Proposed Model with Previous Works\n\nNow, we apply fundamental machine learning methods such as Logistic Regression, Naive Bayes, SVM, and Random Forrest to the extracted and preprocessed FakeNewsNet dataset of this paper and report the results of this observational method in Table II. Then, we'll compare it to the outcomes of the proposed method applied to the crawled data.\n\nNotably, the results II of the methods used in this article were obtained using a supervised approach on this dataset. Now, we'd like to examine the outcomes of the presented semi- supervised models with various architectures on this dataset and compare them to the outcomes of the proposed method. The outcomes of the methods described in the article [7] are listed in Table III. We reimplemented the LSTM and"}, {"title": "V. CONCLUSION", "content": "In this paper, we present a self-learning semi-supervised learning method for fake news detection using transfer learning of text sentiment analysis. The problem of fake news detection in the real world is inherently a semi-supervised problem. Because, in reality, we never have a labeled dataset with numerous features and diverse topics. This method pseudo-labels the unlabeled data with a high confidence threshold in a semi-supervised algorithm. We also tried to use up-to-date and more efficient layers, such as the LSTM with Self-Attention layer, in the self-learning architecture to increase the accuracy and efficiency of the model.\n\nTo obtain the dataset, we faced some limitations that we solved and finally worked on a comprehensive dataset in terms of feature structure and number. Then we preprocessed this dataset using modern methods and feature extraction and implemented a semi-supervised self-learning algorithm using deep neural networks on it as a semi-supervised problem. In addition to overcoming the mentioned limitations, by using transfer learning on sentiment analysis and adding it to the model, the performance and accuracy of the model increased significantly, which is evident in the results obtained in the performance metrics of the proposed model.\n\nFuture Work\n\nThere are different paths for future work that hold signif- icant value for academic exploration. In the current work, we considered that in the semi-supervised algorithm, only the training data were folded, and the test data were fixed and only measured at the final stage. Also, the confidence threshold variable is assumed to be constant. But we can also have such folding or cumulative folding for test data at each test stage and update the confidence threshold adaptively at each stage. Also, in the sentiment encoding section, we can involve more models to have a more unique encoding with a wider range of negative, neutral, and positive sentiments within the encoded vector. As another direction for future work, it is recommended to build upon the previous work that discontinued data crawling three years ago. We can introduce a complete and diverse dataset by using the crawler, more websites than the two used in previous work, and newer news and adding more diverse topics such as sports news, financial news, etc."}]}