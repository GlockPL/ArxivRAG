{"title": "MTL-LoRA: Low-Rank Adaptation for Multi-Task Learning", "authors": ["Yaming Yang", "Dilxat Muhtar", "Yelong Shen", "Yuefeng Zhan", "Jianfeng Liu", "Yujing Wang", "Hao Sun", "Denvy Deng", "Feng Sun", "Qi Zhang", "Weizhu Chen", "Yunhai Tong"], "abstract": "Parameter-efficient fine-tuning (PEFT) has been widely employed for domain adaptation, with LoRA being one of the most prominent methods due to its simplicity and effectiveness. However, in multi-task learning (MTL) scenarios, LORA tends to obscure the distinction between tasks by projecting sparse high-dimensional features from different tasks into the same dense low-dimensional intrinsic space. This leads to task interference and suboptimal performance for LORA and its variants. To tackle this challenge, we propose MTL-LORA, which retains the advantages of low-rank adaptation while significantly enhancing multi-task learning capabilities. MTL-LORA augments LoRA by incorporating additional task-adaptive parameters that differentiate task-specific information and effectively capture shared knowledge across various tasks within low-dimensional spaces. This approach enables large language models (LLMs) pre-trained on general corpus to adapt to different target task domains with a limited number of trainable parameters. Comprehensive experimental results, including evaluations on public academic benchmarks for natural language understanding, commonsense reasoning, and image-text understanding, as well as real-world industrial text Ads relevance datasets, demonstrate that MTL-LORA outperforms LoRA and its various variants with comparable or even fewer learnable parameters in multi-task learning.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have become central to modern deep learning. By scaling model size and leveraging extensive datasets, LLMs consistently demonstrate exceptional generalization and advanced multi-task capabilities. The concept of \"serving one model for different tasks\" has led to numerous applications, ranging from natural language processing to various domain-specific implementations. Despite their high generalizability, LLMs still require fine-tuning for specific domains or to update the knowledge base. However, the vast number of parameters in LLMs poses significant challenges regarding computational efficiency and memory consumption during fine-tuning. Parameter-efficient fine-tuning (PEFT) methods address this challenge by keeping the pre-trained model frozen and only fine-tuning lightweight adapters. A prominent example of PEFT is the low-rank adaptation (LoRA) method, which involves training low-rank \u201cadapter\" layers for a selection of the model layers. LoRA methods are based on the intuition that the fine-tuning updates of pre-trained LLMs have low \"intrinsic rank\" during specialization to sub-tasks, allowing these updates to be well-approximated by adding adapters. Although LORA and its recent variants perform well in various LLM adaptation scenarios, the increased number of tasks that LLMs are expected to tackle, along with the additional computational overhead and maintenance costs of fine-tuning each task individually, has made it desirable to train a LoRA on all the different tasks simultaneously. In this scenario, LORA projects the features from different tasks from the sparse high-dimensional space into a shared, dense low-dimensional space. This causes interference and confusion between tasks, amplifying the loss of task-specific information during projection . Moreover, while it is necessary to share information between tasks during multi-task learning, it is crucial that the information sharing between different task combinations should be distinct. Therefore, additional design is needed to induce the LLMs to adaptively learn different task information sharing strategies during the fine-tuning process. Although recent LoRA variants have introduced improvements to the multi-task setting, whether by ensembling multiple LoRA adapters or adopting the MoE structure for soft information specification , they do not effectively separate different task information or utilize distinct information sharing strategies. Consequently, these approaches are less effective in multi-task scenarios.\nIn this work, we introduce MTL-LORA, a low-rank adaptation method designed to enhance LLMs with the capability to efficiently tackle a variety of tasks in a parameter-efficient manner. MTL-LoRA innovates by implementing task-specific transformations in low-rank space along with a strategy for adaptively exploring multiple information sharing methods. Specifically, MTL-LoRA begins by projecting inputs into lower intrinsic dimensions similar to LoRA. To mitigate the risk of cross-task information interference within such a condensed dimensional space, MTL-LORA further introduces a learn-able transformation for each task, ensuring the preservation of task-specific information. Furthermore, acknowledging the role of information sharing among different tasks, especially in enhancing the performance of tasks with limited resources, MTL-LoRA adopts a dynamic approach to learn different strategies for information sharing among tasks. This is achieved through the weighted averaging of outputs from multiple up-projection matrices. With these improvements, MTL-LORA efficiently assimilates both task-specific and shared information with minimal trainable parameters. Comprehensive experiments on public academic benchmarks as well as real-world text Ads relevance applications demonstrate that MTL-LORA unleashes the multi-tasking capabilities of LLMs by fine-tuning a limited number of model parameters, outperforming LoRA and its variants including MultiLoRA, MOELORA, and DoRA. The key contributions of our work can be summarized as follows:\n1 We present MTL-LoRA, which improves the capability of LoRA in multi-task learning through an innovative approach for extracting task-specific information and enhancing cross-task information sharing.\n2 Comprehensive experimental evaluations validate the efficacy of MTL-LORA on both public academic benchmarks and real-world text Ads relevance tasks.\n3 Through extensive ablation experiments and analyses, we confirm the effectiveness of each component of MTL-LORA and validate the underlying design motivations for our method.\n4 The code will be released as open source to facilitate replication and future research."}, {"title": "Related Work", "content": "Parameter-Efficient Fine-Tuning\nParameter-efficient fine-tuning (PEFT) adapts large pre-trained models for specific tasks or domains using a small portion of parameters while keeping the main model frozen. A popular approach involves inserting trainable, continuous prompts or embeddings into the original text sequence to leverage the base model's knowledge for new tasks. Another approach adds additional neural modules, like adapter structures, into pre-trained models . In this trend, LORA utilizes the concept of low intrinsic dimension, introducing two trainable rank decomposition matrices into frozen pre-trained models to estimate the accumulated gradient update during fine-tuning. Due to its lower inference latency and superior performance, LoRA has been widely adopted, and many studies are exploring ways to enhance its efficiency and stability. For example, AdaLoRA incorporates an importance-aware rank allocation method to assign ranks according to layer importance. LoRA+ aims to improve LoRA's training stability by using different learning rates for different low-rank matrices. DoRA further enhances both the learning capacity and training stability of LoRA by decomposing the pre-trained weights into two components, magnitude and direction, for fine-tuning. While LoRA and its variants show promise in single-task adaptations, their effectiveness diminishes in multi-task scenarios as they update parameters uniformly across all tasks, overlooking crucial task-specific information and dynamic task information sharing. Our work focuses on improving LoRA to acquire both task-specific and task-agnostic knowledge, enhancing its performance in multi-task settings."}, {"title": "Multi-Task Learning", "content": "Multi-Task Learning (MTL) aims to optimize all tasks jointly and adapt a single trained model to serve for all tasks. Since language models trained on large-scale datasets can extract universal representations, previous multi-task learning methods, such as MT-DNN , typically use a shared pre-trained model with task-specific heads to jointly adapt the model to different tasks. However, as the size of pre-trained models continues to increase, full fine-tuning (FT) introduces significant computational overhead and an increased risk of catastrophic forgetting. While LoRA offers an alternative to FT, it does not perform as well in multi-task settings. Approaches like MultiLoRA and MoELORA improve LoRA's multi-task performance in joint training scenarios by integrating multiple LoRAs or utilizing expert routing. However, they fail to strike a good balance between task-specific information and task-information sharing, resulting in suboptimal performance."}, {"title": "Method", "content": "In this section, we first introduce the low-rank adaptation method, followed by an in-depth explanation of the proposed MTL-LORA for multi-task learning."}, {"title": "Low-Rank Adaption", "content": "Current LLMs generally follow a decoder-only structure, characterized by a series of blocks, each comprising two key components with residual connections: a multi-head self-attention (MHA) layer and a feed-forward network (FFN). The MHA layer involves using dense learnable matrices \\(W_q\\), \\(W_k\\), \\(W_v\\), and \\(W_o\\) to mix the sequence \\(x\\) according to inter-relationships between tokens:\n\\(\text{MHA}(x) = \text{Softmax}\\left(\\frac{(W_q x)^T W_k x}{\\sqrt{k}}\right)(W_v x)^T W_o\\),                                                                     (1)\nwhere we assume a single attention head and \\(k\\) denotes the hidden dimension for the head. The FFN layer is usually an MLP with two dense linear projection layers, \\(W_{\text{down}}\\) and \\(W_{\text{up}}\\), and a non-linear activation function \\(\\sigma(\\cdot)\\) for channel mixing:\n\\(\text{FFN}(x) = \\sigma(xW_{\text{down}})W_{\text{up}}\\).                                                                                                                                                     (2)\nAlthough LLMs pre-trained with extensive general domain datasets have demonstrated remarkable generalization abilities, there is a need to adapt these models for specific tasks or domains with limited resources. To achieve this, low-rank adaptation (LoRA), inspired by the concept of low intrinsic dimensionality in LLMs, decomposes the weight gradient \\(\\Delta W\\) into low-rank matrices, thereby reducing the number of trainable parameters. Specifically, for a dense weight matrix \\(W \\in \\mathbb{R}^{d \times k}\\), LoRA employs two low-rank matrices, \\(B \\in \\mathbb{R}^{d \times r}\\) and \\(A \\in \\mathbb{R}^{r \times k}\\), to approximate the accumulated gradient updates \\(\\Delta W\\). The rank \\(r\\) is chosen to be much smaller than the minimum of \\(d\\) and \\(k\\), effectively decreasing the number of trainable parameters. Consequently, the resulting weight matrix is expressed as \\(W+BA\\), and the output \\(h\\) for an input \\(x\\) through this updated weight matrix is formulated as:\n\\(h = (W + \\Delta W)x = Wx + BAx\\)                                                                                                                                                                                            (3)\nIn implementation, the low-rank matrix \\(A\\) is initialized with Kaiming Uniform and \\(B\\) is initialized with zero to ensure \\(\\Delta W = 0\\) at the start, thereby contributing to training stability. A constant scale factor \\(\\alpha\\) is also introduced to adjust the magnitude of the changes of the updated matrix \\(\\Delta W\\) made by LoRA modules."}, {"title": "MTL-LORA", "content": "While LoRA effectively fine-tunes LLMs for specific domains using minimal trainable parameters, it does not fully accommodate the dynamics of task-specific and shared knowledge within different tasks or domains, thereby limiting its effectiveness in MTL settings. To address this issue, we introduce MTL-LORA to improve LoRA with enhanced MTL abilities. The architecture of the proposed MTL-LORA is detailed in Figure 2. For a given input \\(x_t\\) corresponding to task \\(t\\), MTL-LoRA projects \\(x_t\\) to low intrinsic dimension through \\(A\\) as in LoRA. However, to enhance the differentiation of tasks within this low, information-dense feature space and better capture task-specific information, we introduce a low-rank learnable matrix \\(A_t \\in \\mathbb{R}^{r \times r}\\) for each task. This process involves transforming the projected sample \\(Ax_t\\) via \\(A_t\\) to isolate information pertinent to the specific task. Furthermore, we argue that diverse information-sharing strategies are critical for leveraging knowledge from different tasks to improve overall performance. Therefore, rather than relying on a single up-project matrix for information aggregation, we utilize multiple low-rank matrices to explore diverse combinations and information-sharing strategies. These combinations are then integrated using a weighted averaging strategy, thereby facilitating adaptive information sharing among different tasks. Specifically, assuming that the up-projection matrix is denoted as \\(B^i \\in \\mathbb{R}^{d \times r}\\) and the learnable averaging weight for task \\(t\\) is represented by \\(w_t \\in \\mathbb{R}^{n \times 1}\\), where \\(n\\) is the number of up-projection low-rank matrices, the output of MTL-LORA for task \\(t\\) is formulated as:\n\\(h_t = (W + W_t)x_t\\)\n\\(= Wx_t + \\sum_{i=1}^{n} \\frac{\\exp(w_t^i / \\tau)}{\\sum_{j=1}^{n} \\exp(w_t^j / \\tau)}B^i A_t A x_t\\),                                                                                                                         (4)\nwhere \\(\\tau\\) is a hyperparameter to control the sharpness of the weight distribution, and the superscript represents the indices of the corresponding up-projection matrix and averaging weight. We set \\(A_1\\) as a diagonal matrix with each diagonal element being 1, thereby ensuring that \\(\\Delta W = 0\\) at the start of training. Building upon these advancements, MTL-LORA maintains the benefits of parameter efficiency while substantially boosting the MTL capabilities of LoRA."}, {"title": "Experiments", "content": "We conduct a series of experiments to demonstrate the effectiveness of MTL-LORA on various tasks, including natural language understanding (NLU), commonsense reasoning, and image-text understanding. Additionally, we perform ablation studies to illustrate the effectiveness of each component of MTL-LORA. Finally, we conduct a sensitivity analysis to examine its stability across different hyperparameter configurations."}, {"title": "Evaluation on Public Benchmark", "content": "Natural Language Understanding We compare MTL-LORA against several baseline methods, including full-parameter tuning, single-task fine-tuning with LoRA, multi-task fine-tuning with LoRA, MultiLoRA, and MoELORA, on both MPT-7B and LLaMA2-7B models. We use the widely recognized GLUE benchmark for evaluation. The GLUE benchmark comprises nine NLU tasks, covering a diverse range of linguistic challenges such as sentiment analysis, textual entailment, and sentence similarity.\nTo ensure that the LLM with decoder-only architecture generates stable classification results, we adopt the approach of MT-DNN by assigning each task its respective classification head. Simultaneously, to harness the generative capabilities of LLMs, we reformat each task using a specific template and initialize the weights of the classification head with the corresponding word embeddings of the target answer from the original language model. For each PEFT method, we train only the adapter parameters and the classification heads for each task. Details on hyperparameters and templates can be found in Section A.1 and Section C of the supplementary material.\nThe results presented in Table 1 demonstrate that MTL-LORA achieves superior performance, surpassing other baseline methods across both LLMs. Notably, MTL-LORA not only outperforms the strong MultiLoRA baseline with only 64% trainable parameters but also exceeds the performance of FT while requiring significantly fewer trainable parameters (merely 0.03% per task compared to FT). Furthermore, as shown in Table 1, multi-task learning with LORA (i.e., LoRA-MT) outperforms single task fine-tuning with LoRA (i.e., LoRA-ST) across all eight tasks and substantially reduces the number of adapters that need to be maintained.\nWhile both MoELORA and MultiLoRA have boosted LoRA's multi-tasking performance, optimizing these models for multi-task scenarios remains challenging, leading to suboptimal performance. In contrast, MTL-LORA effectively exploits both task-specific and task-agnostic information, thereby outperforming LoRA-MT in nearly all tasks.\nCommonsense Reasoning In these experiments, we perform a comprehensive comparison of MTL-LORA against LORA and various LoRA variants on LLaMA2-7B for commonsense reasoning tasks. We train each model on eight sub-tasks jointly and evaluate performance on the individual test dataset for each task. Following the same train-test split protocol and instruction prompts as in , we report the test set accuracy for each method. Detailed hyperparameter settings can be found in Section A.2 of the supplementary material. Where possible, we report model results as presented in the original papers.\nThe results in Table 2 show that both MoELORA and DORA outperform LoRA and MultiLoRA. We hypothesize that this is because commonsense reasoning tasks require fine-grained task routing or gradient decomposition, rather than merely ensembling different LoRAs. Despite this, MTL-LORA consistently outperforms all baseline methods. Notably, with only one-third of the LoRA parameters, MTL-LORA surpasses LoRA by approximately 4%. Moreover, with only half the parameters, MTL-LoRA outperforms the strong baseline DoRA by a large margin of 2%. These results further highlight MTL-LoRA's efficiency and effectiveness in optimizing model performance while minimizing parameter overhead in multi-task adaptation.\nImage-Text Understanding To evaluate the performance of MTL-LORA in a multi-modality, multi-task fine-tuning context, we compare it against LoRA, DORA, and FT using VL-BART across four distinct image-text tasks. The results for FT, LORA, and DORA are taken from the original DORA paper. For MTL-LORA, we follow the same settings as DORA, applying the adapter to the Q and V linear layers of the language model. We also unfreeze the bias and layer normalization parameters, training for 20 epochs with the AdamW optimizer and a learning rate of 1 \u00d7 10-3, in line with DORA's configuration. The rank, alpha, number of up-projection matrices, and temperature for MTL-LORA are set to 64, 16, 2, and 0.8, respectively.\nThe results are presented in Table 4. Both MTL-LORA and DoRA outperform LoRA by 1% with the same or even fewer parameters. Furthermore, MTL-LORA and DoRA surpass FT while unfreezing only 5-6% of the model's parameters. In the multimodal multi-tasking scenario, MTL-LORA also outperforms DoRA with approximately 1% fewer learnable parameters, demonstrating its superior learning effectiveness in this setting."}, {"title": "Evaluation on In-house Dataset", "content": "To further validate the performance of MTL-LORA in large-scale, complex multi-task scenarios, we compared different multi-task low-rank adaptation strategies on an in-house text Ads relevance dataset (referred to as the Ads dataset).\nThe text Ads relevance task involves determining whether a query is semantically relevant to a given Ad. This dataset encompasses 14 tasks, covering various production scenarios. The query and Ad pairs are collected from a commercial sponsored search engine, with relevance labels provided by experts. For evaluation, we treat the task as a binary classification problem and report AUC-ROC metrics, following production practices. The dataset consists of 13 million examples in the training set and 2 million examples in the test set, with data collected in multiple languages from global markets. This benchmark is particularly challenging because, while all tasks are from the ad domain, they span different product scenarios. Effectively modeling the correlation between tasks is crucial for achieving optimal performance. We follow the same experimental design used for fine-tuning in the GLUE benchmark. For further details on the Ads dataset and experimental settings, please refer to Section B and Section A.1 of the supplementary material.\nThe results are presented in Table 3. In the large-scale Ads dataset, LoRA-MT consistently outperforms LoRA-ST, further underscoring the effectiveness of multi-task fine-tuning. In this context, MTL-LORA either surpasses or matches other methods across all 14 tasks. Notably, while other multi-task adaptations exhibit a seesaw effect when compared to the LoRA-ST approach\u2014improving performance on some tasks but underperforming on others\u2014MTL-LORA consistently outperforms LoRA-ST across all tasks. This indicates that MTL-LORA effectively mitigates interference between tasks during adaptation. Furthermore, when compared to the method ranked second in each task, MTL-LORA achieved statistical significance with 90% confidence in 10 out of 14 tasks on the MPT model and 7 out of 14 tasks on the LLaMA2 model. This significant performance advantage of MTL-LORA in the Ads dataset validates its enhanced capability for multi-task learning."}, {"title": "Ablation Study", "content": "We conduct ablation studies on MTL-LORA to assess the impact of three key components: (1) the task-specific learn-able transformation matrix \\(A_t\\), (2) the temperature coefficient \\(\\tau\\) in Eq. 4, and (3) multiple low-rank up-projection matrices \\(n\\).\nFor each ablation study, we use Llama2-7B as the backbone model and train it on GLUE dataset. In each ablation experiment, we systematically remove or disable one of the key components while keeping all other settings unchanged and report the GLUE metrics. The performance of each modified setting is compared against the full MTL-LORA configuration. Additionally, we add LoRA's results on multi-task fine-tuning as a reference. Detailed hyperparameter settings are provided in Section A.3 of the supplementary material.\nThe results, presented in Table 5, clearly illustrate the importance of each component of MTL-LORA. Omitting any one of these components results in a decline in performance. Notably, the inclusion of multiple low-rank up-projection matrices has the most significant impact. When \\(n = 1\\), the performance of MTL-LORA falls below that of the LoRA-based multi-task fine-tuning on the GLUE benchmark. This suggests that effectively extracting task-specific information using \\(A_t\\) relies on methods that aggregate diverse combinations of information across tasks."}, {"title": "Sensitivity Analysis", "content": "In this section, we analyze the robustness of MTL-LORA under various parameter settings. Our investigation focus on how different values of \\(n\\), \\(r\\), and \\(\\tau\\) impact the performance of MTL-LORA. We use LLaMA2-7B as the backbone model and conduct comparison on the GLUE benchmark. The results, presented in Figure 3, demonstrate that multiple up-projection metrics are crucial for MTL-LORA, with \\(n = 3\\) yielding superior results in most scenarios. The value of temperature coefficient also affects the model performance, supporting the notion that different up-projection matrices capture distinct aggregated information. With a uniform distribution of weights, which blends various information of different tasks evenly, there can be interference among these tasks, consequently degrading the final performance. Additionally, the analysis of different values for the rank \\(r\\) reveals that MTL-LORA maintains robustness at higher rank compared to vanilla LoRA."}, {"title": "Task Differentiation", "content": "The primary goal of MTL-LORA is to enhance the effectiveness of LoRA in multi-task scenarios by preventing cross-task information interference while facilitating task information sharing. Consequently, we assert that the representation outputs of MTL-LORA should be task-relevant. To validate this, we use the outputs from MTL-LORA as features for SVM classification, with labels corresponding to their respective tasks. We use the Ads dataset for comparison because it encompasses a larger number of tasks, all within the same Ad domain, thus providing a challenging context for differentiating tasks. Specifically, we randomly sample 1,000 examples for each task from the Ads dataset and use the outputs of different LoRA adapters from the final block of the underlying LLMs as input features. An SVM classifier is trained on 40% of the samples for each method, with the remaining 60% reserved for evaluation. For detailed information on the experimental settings and hyperparameters of the SVM, please refer to Section A.4 of the supplementary material.\nThe results, as shown in Table 6, indicate that MTL-LORA significantly outperforms LoRA in multi-task fine-tuning, achieving a substantial improvement margin with both MPT and LLaMA2. The t-SNE visualization in Figure 1 further supports this observation, revealing that, in the multi-task adaptation scenario, LoRA tends to blend features from different tasks in the low-rank space, whereas MTL-LORA effectively separates these tasks. When projected back into the full-rank space, MTL-LORA forms distinct \u2018task groups\u2019, indicating that information is shared within each group while remaining distinct between groups. These findings confirm MTL-LORA's enhanced ability to distinguish between tasks and effectively reduce task interference. Furthermore, the relatively lower performance of LoRA in this setting highlights its limitations in extracting task-specific information."}, {"title": "Inference Overhead", "content": "Although MTL-LORA demonstrates exceptional performance in multi-task learning, it relies on task-specific information to determine the appropriate transformation matrix for routing (At in Eq.4). This design choice prevents the merging of all parameters into the original weights as LoRA does, which introduces additional inference latency. However, thanks to MTL-LORA's task-level routing design, all operations are executed using matrix multiplication, avoiding the expert-level looping required by MoELORA. This allows MTL-LORA to fully utilize computational resources without incurring significant inference latency.\nTo validate this, we compared the inference latency of LORA, MoELORA, and MTL-LORA across different batch sizes on the commonsense reasoning task, as depicted in Figure4. The results clearly demonstrate that MTL-LORA significantly outperforms MoELORA in terms of inference speed. Compared to merged LoRA, MTL-LORA introduces only minimal additional latency during inference, with this difference diminishes further under computationally intensive conditions with larger batch sizes. We believe that in latency-sensitive applications, the gap in inference speed between MTL-LORA and merged LoRA can be further reduced through system-level optimizations, which we plan to explore in future work."}, {"title": "Conclusion", "content": "We propose MTL-LoRA, a novel, advanced parameter-efficient fine-tuning method for multi-task low-rank adaptation. MTL-LORA enhances the multi-task learning capability of LoRA by incorporating task-specific transformations in low-rank space along with a strategy for adaptively exploring multiple information sharing methods. This approach facilitates the learning of both task-specific and shared information. Comprehensive experiments on multiple public academic benchmarks and a large-scale text Ads relevance dataset demonstrate that MTL-LORA outperforms LORA and its variants, including MultiLoRA, MoELORA, and DoRA, validating its effectiveness in multi-task learning. Furthermore, extensive analysis of intermediate low-rank features and visualizations support our design motivations. For future studies, we will focus on optimizing the design of MTL-LORA to reduce the additional inference time while maintaining its effectiveness."}, {"title": "A. Experimental Setting", "content": "A.1. Natural Language Understanding\nFor all experiments conducted on the GLUE and Ads datasets, we employ the AdamW optimizer with weight decay set to 0 and utilize a linear learning rate decay scheduler. The batch size is set to 8 for the GLUE benchmark and 16 for the Ads dataset. Unless otherwise specified, in all our experiments, we integrate adapter modules into every dense layer of the multi-head attention (namely Q, K, V, O) in the selected LLMs. Details on other hyperparameters for each method are provided in below.\nFull Tuning (FT): For Full Tuning on the GLUE benchmark, the learning rate is set to 8 \u00d7 10-6 for both MPT and LLaMA2 models. For LLaMA2, we observed that a single epoch of training on each dataset led to highest performance. In contrast, for the MPT model, we extend the training to 5 epochs for COLA and MRPC, 10 epochs for RTE, while maintaining a single epoch for the remaining datasets.\nLORA-ST: In all experiments, we set the rank r to 8. For the Ads dataset, training is conducted for one epoch with a learning rate of 3 \u00d7 10-4 for both models. As for the GLUE benchmark, the learning rate is set to 4 \u00d7 10\u22124. We train for 10 epochs on the RTE dataset and 5 epochs on the MRPC dataset, while all other datasets are trained for a single epoch.\nLORA-MT: In each case, we set the rank r to 16 and the learning rate to 2 \u00d7 10\u22124. All results are reported after training for 1 epoch.\nMulti-LoRA: In all MultiLoRA experiments, we configure the number of LORA modules to 3 and set the rank r of each LoRA module to 8. The learning rate is set to 3 \u00d7 10-4 for Ads dataset and 2 \u00d7 10-4 for the GLUE benchmark.\nMoE-LoRA: In all MoE-LoRA experiments, we set the number of experts in MoE-LORA to 8, the rank of LoRA modules to 16, and the task embedding for each task to 64. The learning rate is set to 3 \u00d7 10-4 for the Ads dataset and 2 x 10-4 for the GLUE benchmark.\nMTL-LORA: In all experiments involving MTL-LORA, training is conducted for 1 epoch. Unless otherwise specified, we set the low-rank r to 8. For the Ads dataset, the learning rate is 3 \u00d7 10\u22124 for LLaMA2 and 4 \u00d7 10-4 for MPT. The values of n and \u03c4 are both set to 3 and 0.5, respectively, for the LLaMA2 and MPT models. In the case of the GLUE benchmark, both models have their learning rate and n set to 2 \u00d7 10-4 and 3, respectively. The temperature hyperparameter \u03c4 is adjusted to 0.1 for LLaMA2 and 0.5 for MPT. For all experiments, unless otherwise specified, the low-rank parameter r is set to 8."}, {"title": "A.2. Commonsense Reasoning", "content": "We follow the same instructional prompt and dataset configuration as DoRA for evaluating commonsense reasoning tasks. The results for DORA and LoRA are taken from the original DoRA paper, while we have reimplemented MultiLoRA, MoELORA, and MTL-LORA. For each reimplementation, we applied the adapter to the Q, K, V, O linear layers of the pre-trained model. Detailed hyperparameters are provided in Table 7."}, {"title": "A.3. Ablation Study", "content": "In all ablation experiments, we use LLaMA2-7B as our base model. The default settings for the number of low-rank upprojection matrices n, temperature coefficients \u03c4, and learning rates are 3, 0.1, 2 \u00d7 10-4, respectively. All experimental results are obtained after training for one epoch."}, {"title": "A.4. Task Differentiation", "content": "We randomly sample 1,000 samples for each task from the Ads dataset, utilizing the output of different LoRA adapters from the final block of the underlying LLMs as input features for the SVM classifier. The dataset is split into training and test sets in a 4:6 ratio, with the SVM's parameter C set to 1. We refrain from using any kernel functions and employ the 'one-vs-rest' strategy for multi-class classification. For comparison, we conduct identical classification procedures using the LoRA-MT method."}, {"title": "B. Details for Ads Dataset", "content": "The text Ads relevance dataset used in this paper is a large-scale, real-world dataset from online advertising platform. This dataset comprises 14 tasks and covers various scenarios. It includes a total of 13 million examples in the training set and 2 million examples in the test set. We use 10% of the training data for hyper-parameter tuning. Each dataset is uniformly formatted as {query, Ad, label}, where query is the search phrase combined with related features that reflect search intent and Ad is the concatenation of Ad description, features, or corresponding landing page, etc. Among these tasks, task 0 to 7 are sampled from the North American market, while the rest are from the international markets. The"}, {"title": "C. Task Template", "content": "We employ the same prompt templates for adapting different PEFT methods. The prompt templates utilized for the GLUE benchmark are detailed in Table 9. Regarding the Ads dataset, we evaluate all the templates listed in Table 10 and find that Template 6 yields the best results. Consequently, Template 6 are used for all tasks in the Ads dataset."}]}