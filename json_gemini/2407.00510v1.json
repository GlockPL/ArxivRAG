{"title": "STOCHASTIC STEM BUCKING USING MIXTURE DENSITY\nNEURAL NETWORKS", "authors": ["Simon Schmiedel"], "abstract": "Poor bucking decisions made by forest harvesters can have a negative effect on the products that are\ngenerated from the logs. Making the right bucking decisions is not an easy task because harvesters\nmust rely on predictions of the stem profile for the part of the stems that is not yet measured. The\ngoal of this project is to improve the bucking decisions made by forest harvesters with a stochastic\nbucking method. We developed a Long Short-Term Memory (LSTM) neural network that predicted\nthe parameters of a Gaussian distribution conditioned on the known part of the stem, enabling the\ncreation of multiple samples of stem profile predictions for the unknown part of the stem. The\nbucking decisions could then be optimized using a novel stochastic bucking algorithm which used\nall the stem profiles generated to choose the logs to generate from the stem. The stochastic bucking\nalgorithm was compared to two benchmark models: A polynomial model that could not condition\nits predictions on more than one diameter measurement, and a deterministic LSTM neural network.\nAll models were evaluated on stem profiles of four coniferous species prevalent in eastern Canada.\nIn general, the best bucking decisions were taken by the stochastic LSTM models, demonstrating the\nusefulness of the method. The second-best results were mostly obtained by the deterministic LSTM\nmodel and the worst results by the polynomial model, corroborating the usefulness of conditioning\nthe stem curve predictions on multiple measurements.", "sections": [{"title": "Introduction", "content": null}, {"title": "Research context", "content": "Forest harvesters are a type of heavy machinery that fell trees and cut the stems into logs of predetermined dimensions.\nA harvesting head is mounted at the end of the boom which has a chainsaw, multiple sensors measuring the diameter\nand length of the stem and large knives to delimb the tree. Most modern harvesters are equipped with a bucking\noptimization software that selects the length of the logs that will be generated from the stem. The software is given a\nprice matrix containing the dimensions of each log category and its respective weight (often referred to as price). Using\ndynamic programming, the software selects the combination of products that maximises the total sum of the weights\nof the products generated [1] [2]. When compared to manual selection of the products by the harvester operator, it was\nshown that using a bucking optimization software improves the value of the logs generated from forest stands [3] [4].\nThere also exists bucking to order methods that try to match the production to specific product distributions during\nharvesting.\nTo optimize stem bucking, it is necessary to predict the stem profile of the part of the stem which has not yet been\nmeasured by the harvesting head. This can be challenging as measurements along the stem are correlated and the\npredictive model used should condition its predictions using the previous measurements taken from that stem. These\ncorrelations also imply that any measurement error may be propagated to the unmeasured part of the stem [5] [6]\ntogether with any bias of the predictive model. Another challenge of bucking optimization is the relationship between"}, {"title": "Literature review", "content": "The prediction of stem curves, diameter and volume was previously extensively done using taper equations. These\nequations, often specific to a single species, subspecies, or subgroup of trees, are a parametric way to predict the\nstem curve and estimate its volume using the diameter at breast height and other exogenous variables [7]. Taper\nequations were used for a wide range of species, including Tsuga heterophylla [8], Pinus taeda [9] [10], Pinus elliotti\n[9], Liriodendron tulipifera [11], Pinus pinaster [12], Abies nordmanniana [13], Picea glauca [14], Larix gmelinii\n[15] and Betula platyphylla [16], only to name a few. Because taper equations were developed for a multitude of the\ncommercial tree species across the world, we refer readers who are looking for an exhaustive review and history of\ntaper equations to [17] and [18].\nThere are multiple challenges encountered when using taper equations to predict the stem curve for bucking opti-\nmization. First, many traditional taper equations were created for forest inventory purposes and do not condition their\npredictions on multiple measurements along the stem, reducing their performance when they are used for stem bucking\nas including additional diameter measurements was shown to improve the predictions [19]. One strategy to alleviate\nthis problem was to develop taper equations for smaller subsets of stems that have similar characteristics [20] [21].\nWhile doing so can improve the performance of the predictions, it is still not equivalent to conditioning the predictions\non the previous stem measurements as it is done by harvesters. A second challenge ignored by much of the parametric\ntaper equation research is the estimation of the generalization performance of the methods studied. When quantifying\nthe performance of the predictions, there can be differences between the performance estimates made from the stems\nthat were used to fit the taper equation and the estimates made from stems not used to fit the equations [22]. Further-\nmore, the effects of the prediction errors made by the parametric taper equations on the bucking decisions are typically\nignored.\nA notable family of methods used for taper predictions is the spline functions, which allow to model a wider range of\nstem curves. Cubic smoothing splines [23] [24] and penalized mixed splines [25] have both been previously used to\npredict stem taper. Noteworthy for stem bucking is the work of [26] and [27] who used cubic smoothing splines that\nconditioned their predictions using the previously measured part of the stem. A downside of their method was that the\nerrors made by the model could compound over time for specific stems, leading to a gradual increase in the uncertainty\nof the predictions along the unknown part of the stem. While their goal was to improve stem profile predictions for\nstem bucking, they also did not estimate the impact of their predictions on the bucking decisions themselves.\nWhile the methods discussed up to this point all rely on the measurements made by the harvester head to make their\npredictions, other strategies have been tested using different technologies not yet present on harvester, the most popular\nof these being laser. As it was shown that including additional diameter measurements along the stem can improve\ntaper predictions [19], the usefulness of using lasers to acquire such measurement was investigated by [28] and [29].\nLasers have also been used to measure the full stem profile by [30] and [31], creating a point cloud of the stem.\nFurthermore, [32] demonstrated that if those technologies became widely available on harvesters, they could improve\nthe decisions made during the bucking optimization process. Finally, camera images can also be used to measure the\nstem profile of the stems during harvesting [33]. While these technologies may improve stem bucking in the future,\nthey are not yet available on modern harvesters, limiting their immediate usefulness.\nIn recent years, supervised learning methods became prevalent in predictive modeling. However, small neural net-\nworks were already used for taper prediction in 2005 [34] and 2006 [35]. Since then, multiple supervised learning\ntechniques were adapted to predict stem taper. One method that had mixed success on this problem is random forest.\nWhile [36] observed that random forests performed better than old parametric models for Acacia decurrens, [37] found\nthe opposite for multiple Brazilian species. Also, random forest had poor performance extrapolating when compared\nto parametric predictive methods for Pinus taeda [38] and six species of hardwoods [39].\nWhile they have been used for almost two decades in stem taper predictions, in recent years there was a renewed\ninterest for neural networks to accomplish this task. Neural networks were observed to perform worse than parametric\nmodels for Pinus sylvestris [40] and Tectona grandis [41], however they performed better for Acacia decurrens [36],\nFagus orientalis [42], Abies nordmanniana [42], Pinus sylvestris [43], Pinus taeda [44], three Nothofagus species [45],\nseven species in Poland [46] and multiple Brazilian species [37]. While having a greater capacity to model complex\nrelationships than traditional parametric models, the neural networks that were developed for stem taper prediction\nare not ideal for stem bucking. Like their parametric counterparts, they often do not condition their prediction on"}, {"title": "Research objectives", "content": "The current state of the literature on stem profile predictions for bucking optimization highlights the need for a method\nthat would condition its predictions on the previous parts of the stem while leveraging the uncertainty of the predictions\nto improve the bucking decisions. The usefulness of this method should be evaluated with its impact on the bucking\ndecisions taken instead of with the prediction errors made.\nThe goal of this project is to improve the bucking done by forest harvesters via a novel stochastic stem bucking method.\nSpecifically, we aimed to:\n\u2022 Develop a predictive neural network that conditions its predictions on the previous measurements along the\nstem and predicts a statistical distribution of the unmeasured stem profile.\n\u2022 Create a stochastic stem bucking algorithm to leverage the randomness of stochastic stem profile predictions.\n\u2022 Estimate the impact of stochastic stem predictions on bucking decisions.\n\u2022 Distinguish the proportion of the bucking gains attributable to the use of the neural network to the gains\nattributable to the stochastic bucking decisions.\n\u2022 Evaluate the sensitivity of the algorithm to the prices and dimensions of the products that can be created\nduring the stem bucking."}, {"title": "Materials and methods", "content": null}, {"title": "Stem taper data", "content": "The 8146 merchantable stem profiles used in this study were collected by the Qu\u00e9bec (Canada) provincial government\nbetween 1999 and 2009 as a part of the forest inventory program [51]. The four major species used for lumber\nproduction in eastern Canada were considered in this study, which are Picea mariana, Picea glauca, Abies balsamea\nand Pinus banksiana. A description of the number of stems and diameter at breast height (DBH) for each species is\nillustrated in Table 1.\nFor every tree in the data set, diameter over bark measurements were manually taken at two-meter intervals along the\nstem with additional measurements close to the stump and at breast height. Contrary to measurements taken from"}, {"title": "Predictive taper model", "content": null}, {"title": "Architecture", "content": "The neural network architecture that was implemented to predict the stem profiles for stem bucking is the Long short-\nterm memory network [47]. Separate models were made for each of the four species. Because stems are relatively\nsimple sequences of diameters, the LSTM is an ideal candidate as it has few parameters and good gradient propagation\nproperties while still conditioning its predictions over the previous measurements along the stem. Given a sequence of\nstem diameters starting at the base of the tree, the network predicts the parameters of a normal distribution $N(\\mu, \\sigma^2)$\nfor each subsequent diameter.\nThe LSTM unit implemented in the network had a hidden size containing 10 features that led to an output vector also\nof size 10. An additional linear layer mapping the ten output features of the LSTM unit to a fully connected layer\nalso of size 10 with a ReLU activation function and a second fully connected layer mapping to the size 2 output of the\nnetwork $(\\mu, \\sigma^2)$ were added to further increase the representation capacity of the network."}, {"title": "Loss function", "content": "For the loss function of density-based models, [50] recommended to use the negative likelihood. Because the like-\nlihood of continuous distributions is based on the probability density function of that distribution, this implies that\n\"good\" predictions should predict a high density where the true values are and low densities elsewhere. The nega-\ntive log-likelihood of the normal distribution $N (\\mu, \\sigma^2)$ for n observations can be simplified to what is displayed in\nEquation 1 (demonstrated in Appendix A), where $x_k$ is the true value of the $k^{th}$ observation.\n$L_N = \\frac{1}{n}\\sum_{k=1}^{n} (ln(\\sigma^2) + \\frac{(\\mu - x_k)^2}{\\sigma^2})$ (1)\nWe can observe that the term on the left ($ln\\sigma^2$) penalizes the scenarios when the variance of the prediction is high,\nwhile the term on the right $(\\frac{(\\mu - x_k)^2}{\\sigma^2})$ penalizes the scenarios when the squared error of the predictions is high. In-\ncreasing the predicted variance will increase the value of the term on the left while decreasing the value of the term\non the right (and vice-versa). This loss function can be improved by adding a hyper-parameter $\\lambda \\in (0,1)$ as is dis-\nplayed in Equation 2. Adding this parameter allows to control the importance of the squared error and variance during\nback-propagation. This is the loss function that was used to train the stochastic neural networks."}, {"title": "Benchmark models", "content": "Two benchmark models were developed to compare the results obtained by the stochastic predictions made by the\nLSTM network. To assess the usefulness of stochastic predictions, a deterministic LSTM model was created which\npredicted only the diameter value instead of the parameters of a normal distribution. Other than the size of its output, it\nhad the same architecture as the stochastic LSTM model. Because LSTM networks have not been previously used for\ntaper predictions, a third model was created to compare the performance of the two LSTM-based models to simpler\nmodels that do not condition its predictions on the previously measured part of the stem. Polynomial models were\nchosen as they are simple, they can represent continuous functions and there is a history of their use for taper prediction\n[13] [52] [53]. The loss function used for the two benchmark models is the squared error loss, which is displayed in\nEquation 3.\n$L_2 = \\frac{1}{n}\\sum_{k=1}^{n} (\\mu \u2013 x_k)^2$ (3)"}, {"title": "Stochastic bucking optimization", "content": "Predicting probability densities instead of single values for the unmeasured stem diameter enables random sampling\nfrom the predicted distribution. Instead of predicting one stem profile as it was done in previous research, a sample\ncontaining multiple possible future stem profiles is created which is used by a stochastic bucking algorithm to identify\nthe bucking decisions that are optimal when applied to the entire sample. The degree of similarity between the\npredictions in the sample will be modulated by the A parameter introduced in Equation 2, as decreasing its value will\nincrease the variance in the diameter predictions.\nThe stochastic stem bucking algorithm that was developed is inspired by [2] and [54]. It formulates the bucking\nproblem as a longest path problem maximizing the value of the products generated from the stems in the sample of\npossible future stem profiles, as is illustrated in Figure 2. Adapting the existing bucking algorithms to handle multiple\nstems at the same time led to new challenges that are not present in the deterministic case. First, the predicted stem\nprofiles in the sample may not be the same height and diameter along the stem. This can lead to scenarios where\nsome products could be made in some stems but not in others. To account for this, the value of a cut that generates a\nnon-feasible product for a stem in the sample is set to zero for that specific stem. To evaluate the value of the cut for\nthe entire sample, the mean value of the new cut is computed across all stem predictions in the sample.\nA second new challenge of stochastic bucking is the choice of the height at which the algorithm must stop bucking\nbecause the stems stop being merchantable. This choice is non-trivial in the stochastic setting because all stems in the\nsample have different heights where they stop being merchantable. To handle this problem, the algorithm continues\nbucking while it can generate a product for at least one stem in the sample. There is a minimum stem diameter and\na maximum height to stop individual stem predictions, which were respectively set to 4 cm and 40 m. Since using a"}, {"title": "Experimental design", "content": "First, the stem profiles were randomly split into a training (60%), validation (20%) and test (20%) set for each species.\nCopies of each stem where the measured part of the stem ends at different height were added to each data set to emulate\nthe scenario where prior bucking decisions were previously taken in the lower part of the stem."}, {"title": "Hyper-parameter tuning for stochastic bucking", "content": "To improve our understanding of the relationship between the hyper-parameter A of the loss function and the sample\nsize hyper-parameter of stochastic bucking, multiple experiments were conducted. A challenge faced during hyper-\nparameter selection was the possible presence of relationships between the model hyper-parameters and other elements\nthat may affect the performance of the bucking such as the dimensions and variety of the products that can be made, the\nvalues associated to them in the price matrix, the tree species, the tree size, etc. To reduce the number of experiments\nneeded, five products with lengths of 251, 312, 373, 434 and 495 cm were included (corresponding to logs of 8, 10,\n12, 14 and 16 feet with an extra margin), each having a minimum diameter of 9 cm and a maximum diameter of\n100 cm. To allow the bucking algorithm to discard parts of the stem, a sixth product having a 30 cm length with no\nminimum and maximum diameter was added. To further reduce the number of experiments needed to study the effect\nof the hyper-parameters, a single price matrix was used. The price for each product was set to its length, except for\nthe discarded product which had a price of 0. Using the lengths of the products as prices corresponds to maximizing\nthe total length of the logs that are generated. This task may seem simple, however it was chosen as we hypothesized\nthat hyper-parameters that performed well in this setting might perform well in other settings.\nA total of 360 experiments were conducted for hyper-parameter selection, each consisting of a specific combination\nof species, sample size, and \u5165 value. In each experiment a new neural network was trained. The bucking decisions\nmade using the stochastic bucking algorithm were then compared to the bucking decisions made knowing the true stem\nprofile. Since the best possible bucking decisions are done while knowing the true stem profile, the metric chosen to\nillustrate the results of the experiments was the difference between the value of the products generated using the true\nstem profile versus the value of the products generated using the stochastic predictions. The training details of these\nexperiments are described in Section 2.4.4 and the result are displayed in Figure 3.\nAcross all four species, the differences between the total value of the products made knowing the true stem profile and\nthe total value of product made using stochastic bucking ranged between 64.9 and 328.9, where a smaller difference is\nbetter. While they are not presented in the figure, 95% confidence intervals over the mean difference were computed"}, {"title": "Training the deterministic LSTM model", "content": "The deterministic LSTM model was evaluated with the same price matrix as in Section 2.4.1. However, since the\nsample size and A value are not present in the deterministic model, no hyper-parameter tuning was done. For each\nspecies, a network was trained on the test data set and evaluated on the validation data set. The training details of these\nexperiments are described in Section 2.4.4. The average difference in value with the best bucking decisions was then\ncomputed with 95% confidence intervals. For Picea glauca, this difference was 120.81 \u00b1 0.11, for Picea mariana it\nwas 88.46 \u00b1 0.06, for Pinus banksiana it was 324.18 \u00b1 0.56 and for Abies balsamea it was 94.63\u00b10.05."}, {"title": "Hyper-parameter tuning for the polynomial model", "content": "To create the polynomial model, the effect of the maximum order of the polynomial terms used in the regression was\ninvestigated. The hyper-parameter tuning of the polynomial model was conducted in a similar way as for the stochastic\nbucking model. For every combination of species and value for the maximum order of the polynomial terms, a new\npolynomial model was trained on the training data set. For every stem in the validation data set, the model predicted its\nstem profile using the first known diameter along the stem. These predictions were given to the bucking optimization\nalgorithm and the resulting decisions were compared to the true optimal decisions. The products and price matrix used\nwere the same as those used during the hyper-parameter tuning of the stochastic and deterministic LSTM bucking\nmodels.\nThe differences in value of the products generated during stem bucking on the validation data between the true optimal\ndecisions versus the decisions made using the prediction of the polynomial model are displayed in Figure 4. For\nall four species, we can observe that the difference is smaller when the maximum order of the polynomial terms is"}, {"title": "Training details", "content": "PyTorch 1.12.1. was used to implement the network and all algorithms were implemented in Python. The default\ninitial weights were used as a starting point for the optimization. The learning rate was 0.001, and each model was\ntrained for a total of 200 epochs using the Adam optimizer with a mini-batch size of 64."}, {"title": "Bias and variance of the predictions", "content": "The stochastic LSTM, deterministic LSTM and polynomial model were trained on the training data set using the\nhyper-parameters described in section 2.4.1 and 2.4.3 respectively. The models then predicted the stem profile of the\nstems in the test data set, and the bias and variance of the predictions were recorded (We are referring here to the true\nvariance of the stem profile prediction, not the variance predicted in the loss function of the neural network)."}, {"title": "Effect of the minimum diameter of the products", "content": "One factor that influences the bucking decisions taken is the minimum diameter of the products generated. While\nduring the hyper-parameter tuning the minimum diameter was the same for every product in the price matrix, using\ndifferent minimum diameters for the product dimensions could affect the performance of the bucking algorithms. To\nquantify the effect of the minimum diameter of the products on the bucking decisions, five scenarios were evaluated\nwhere the difference in the minimum diameter between smaller and bigger logs progressively increased. The same\nproducts lengths and values as in Section 2.4.1 were used, but with different minimum diameters for each products\nwhich are displayed in Table 2.4.6. For each model and scenario, the model was trained on the training data set. It\nthen made predictions of the stem profiles of the test data set and made bucking decisions based on these predictions\nand on the scenario. The decisions taken by the models were then compared to the best possible decisions that are\nmade when the true stem profile is known."}, {"title": "Effect of the price of the products", "content": "Another factor that has a direct effect on the bucking decisions taken is the price assigned to the products in the price\nmatrix. To quantify the effect this has on the performance of the algorithms, nine price scenarios were created, each\nwith different prices associated to the product discussed in Section 2.4.1. The prices are not expressed in any specific\ncurrency. The specific prices of each products in these scenarios are displayed in Table 2.4.7. While the scenario 5 uses\nthe same prices as in Section 2.4.1, scenarios 1-4 prioritize making smaller logs and scenarios 6-9 prioritize making\nlonger logs. For each model and scenario, the model was trained on the training data set. It then made predictions of\nthe stem profiles of the test data set and made bucking decisions based on these predictions and on the scenario. The"}, {"title": "Results and discussion", "content": null}, {"title": "Results", "content": null}, {"title": "Bias and variance of the predictions", "content": "The bias and variance of the predictions made by the stochastic LSTM models on the test data set are displayed in\nFigure 5 and 6 respectively. While they are not displayed, 95% confidence intervals for the bias and variance estimates\nwere computed. The mean value of the confidence intervals for the bias estimates is \u00b10.02 cm and the maximum value\nis \u00b10.43 cm. The mean value of the confidence intervals for the variance estimates is \u00b10.12 cm\u00b2 and the maximum\nvalue is \u00b11.30 cm\u00b2.\nFirst, we can observe that for Picea mariana, Pinus banksiana and Abies balsamea, the bias of the predictions became\nincreasingly negative the further away the height of the prediction was from the last known measurement along the\nstem. Generally, the bias was closer to zero for predictions made in the lower part of the stems than for predictions\nmade in the upper part of the stems. Furthermore, increasing the number of measurements known along the stem\nreduced the bias of the predictions. Finally, the bias of Picea glauca was lower that the bias of the other species and\ndid not seem to follow exactly the same trends.\nWhile trends for the variance of the predictions are not as apparent, we can observe that the variance for Picea glauca\nis much higher than for the other species. This is noteworthy as the bias of Picea glauca was generally lower than the\nbias of the other species and also behaved differently.\nThe bias and variance of the predictions made by the deterministic LSTM models on the test data set are displayed in\nFigure 7 and 8 respectively. While they are not displayed, 95% confidence intervals for the bias and variance estimates\nwere computed. The mean value of the confidence intervals for the bias estimates is \u00b10.01 cm and the maximum\nvalue is also \u00b10.01 cm. The mean value of the confidence intervals for the variance estimates is \u00b10.12 cm\u00b2 and the\nmaximum value is \u00b13.80 cm\u00b2. We can observe that for all four species, the bias of the predictions increased the further\nthe predicted diameter was from the stump, while increasing the number of measurements that are known reduced the\nbias.\nFor the variance of the predictions, we can observe that Picea galuca and Pinus banksiana had higher levels of\nvariance, while Picea mariana and Abies balsamea had lower variance overall. For all four species, the variance of the"}, {"title": "Effect of the minimum diameter of products", "content": "The results of the experiments on the effect of the minimum diameter of the products in the price matrix are displayed\nin Table 3.1.2. We can observe that the bucking decisions taken by the polynomial models were in general much worse\nthan the decisions taken by the stochastic LSTM models and deterministic LSTM models. The performance of the\ndeterministic models was in general better than the polynomial models but worse than the stochastic models. We can\nalso observe that increasing the differences in minimum diameter between smaller and bigger products improved the\nquality of the decisions made by the stochastic and deterministic models, however the same trend is not as visible for\nthe polynomial models."}, {"title": "Effect of the price of the products", "content": "The results of the experiments on the effect of price of the products in the price matrix are displayed in Table 3.1.3.\nWe can observe that, in general, the deviation from the optimal solution decreases when more importance is given to\ncreating shorter logs (scenarios 1-4) and increases when more importance is given to creating longer logs (scenarios\n6-9). This trend is more apparent for the stochastic models than for the other models. We can also observe that the\nperformance of the stochastic models is generally better that the performance of the polynomial and deterministic\nmodels. Furthermore, the deterministic models performed better than the polynomial model."}, {"title": "Discussion", "content": "In the experiments conducted, the polynomial models were in general the worst models out of all model types. While\nthe biases and variance of the predictions did not stand out much from the other models, the polynomial models had\nthe disadvantage of not being able to condition their predictions on more than one measurement along the stems, which\nnegatively affected the bucking decisions. Our results show that conditioning the predictions on more measurements\nis a crucial aspect of the stem curve prediction and should not be ignored by harvester manufacturers. These findings\nare in line with the results of [27] who came to similar conclusions for their cubic smoothing spline model.\nThe deterministic LSTM model performed in general better in the experiments than the polynomial model but worse\nthan the stochastic model. We believe that the first reason the results of the deterministic LSTM model were better than\nfor the polynomial models was because the deterministic LSTM can conditions its predictions on multiple measure-\nments along the stem. A second element that may have affected the results is the fact the LSTM is a neural network\nwhich is a more powerful model than the polynomial models, allowing it to better capture complex relationships in the\nstem profiles.\nThe stochastic LSTM models made better decisions than both the polynomial and deterministic LSTM models for most\nof the experiment conducted regardless of the species. In general, for the stochastic and deterministic models the bias\nincreased, and the variance decreased the further away the predicted diameter was from the last known measurement\nalong the stem. While for all models it is generally beneficial to have a low bias, the stochastic models had the\nadvantage of alleviating the effect of a high variance by using multiple different predictions of the stem profile during\nbucking. The best value of the A hyper-parameter for the loss function reflected this as it favoured the reduction of\nthe squared error over the reduction of the variance during training. We also observed that increasing the sample\nsize never had a negative effect on the algorithm, however the gains progressively got smaller with each increase in\nsample size. The performance of stochastic bucking also highlights the importance of comparing the bucking decisions\ntaken instead of the predictions made by the algorithms since increasing the variance of individual predictions can be\nbeneficial in improving the bucking decisions while also increasing the prediction errors made.\nDuring the experiments on the effect of the minimum diameter on the bucking decisions taken, we observed that\nwhen the minimum diameters diverged, the difference in value with the optimal solution decreased. We believe that\nthis behaviour is caused by the increased constraints on the decisions that can be made during bucking. When the\nminimum diameter of a product increases, the number of possible bucking decisions can only decrease which could in\nturn reduce the difference between bucking decisions taken and the optimal solution.\nDuring the experiments on the effect of the values of the logs in the price matrix, we observed a trend where the\ndifference between the bucking decisions taken, and the optimal solution decreased when a higher price was assigned\nto the smaller logs and increased when a higher price was assigned to longer logs. We believe that this occurs because\nit is easier to make bucking decisions where smaller logs are favoured, and vice-versa. Because in a specific stem\nthere are fewer long logs that can be made than smaller logs, the effect of predictions errors on the total value of the\nproducts generated from the stem is greater when the longer logs are favoured, and vice-versa.\nThere are some limitations to the results discussed here. First, the hyper-parameters of the stochastic and polynomial\nmodels were selected according to a specific price matrix. We cannot guarantee that we would have obtained the same\nvalues for these hyper-parameters if different products and prices were used in the price matrix. Changing the values\nof these hyper-parameters may affect our results and we believe that if these algorithms were implemented in another\nsetting, new hyper-parameter values should be selected on the specific species and price matrix considered. Second,\nwhile we have studied the effect of the minimum diameter of the products and the effect of the value of these products\nseparately, there may exist relationships between these two elements which we would have ignored here. While in"}, {"title": "Conclusion", "content": "Poor bucking decisions made by forest harvesters can have a negative economic impact on the viability of forest\noperations. Making the right bucking decisions is not an easy task because harvesters must rely on predictions of the\nstem profile for the part of the stems that is not yet measured. While previous research investigated stem tapers for\na variety tree species, few efforts were made to develop models that conditioned their predictions on an unspecified\nnumber of diameter measurements along the stem and the resulting models were never evaluated by studying their\nimpact on the bucking decisions taken.\nThe goal of this project was to improve the bucking decisions made by forest harvesters with a novel stochastic\nbucking method. We developed a Long Short-Term Memory (LSTM) neural network which predicted the parameters\nof a Gaussian distribution, with which we can create a sample of stem profile predictions for the unknown part of the\nstem, conditioned on the known part of the stem. To do so we have adapted a loss function based on the Gaussian\nprobability density function, allowing us to increase the importance given to reducing the squared error or the variance\nof the predictions during the training to the models. The bucking decisions could then be optimized using a novel\nstochastic bucking algorithm which made the bucking decisions over all the stems predictions in the sample.\nThe decisions made using stochastic bucking were compared to two benchmark models: A polynomial model that\ncould not condition its predictions on more than one diameter measurement and a deterministic LSTM neural network.\nAll models were evaluated on stem profiles of four coniferous species prevalent in eastern Canada (Picea galuca, Picea\nmariana, Pinus banksiana and Abies balsamea). In general, the best bucking decisions were taken by the stochastic\nmodels, demonstrating the potential of the method. The second-best results were obtained by the deterministic LSTM\nmodel and the worst results by the polynomial model, corroborating the necessity of conditioning the stem curve\npredictions on multiple measurements.\nStochastic stem bucking showed great potential in improving the bucking decisions made by harvesters and future\nresearch should consider its usefulness for other commercial tree species."}]}