[{"title": "C Proofs for Theorem 3", "authors": ["Hiroki Ishibashi", "Kenshi Abe", "Atsushi Iwasaki"], "abstract": "This paper introduces state abstraction for two-player zero-sum Markov games (TZMGs), where the payoffs for the two players are determined by the state representing the environment and their respective actions, with state transitions following Markov decision processes. For example, in games like soccer, the value of actions changes according to the state of play, and thus such games should be described as Markov games. In TZMGs, as the number of states increases, computing equilibria becomes more difficult. Therefore, we consider state abstraction, which reduces the number of states by treating multiple different states as a single state. There is a substantial body of research on finding optimal policies for Markov decision processes using state abstraction. However, in the multi-player setting, the game with state abstraction may yield different equilibrium solutions from those of the ground game. To evaluate the equilibrium solutions of the game with state abstraction, we derived bounds on the duality gap, which represents the distance from the equilibrium solutions of the ground game. Finally, we demonstrate our state abstraction with Markov Soccer, compute equilibrium policies, and examine the results.", "sections": [{"title": "1 Introduction", "content": "Multi-agent reinforcement learning (MARL) is a framework for sequential decision-making, where multiple agents make decisions in a non-stationary environment to maximize their cumulative rewards. MARL has a wide range of applications, e.g., robotics, distributed control, game AI, and so on. Such an environment is often modeled as two-player zero-sum Markov games (TZMGs) (Littman 1994) and computing the equilibria is said to be empirically tractable. However, it still suffers from the exponential growth of state space size in the number of domain variables.\nMarkov decision processes (MDPs), which are a single-agent version of Markov games, face the same challenge. Solving MDPs, i.e., computing an optimal policy, is P-Complete in state space size (Littman 1994), while that size often exponentially increases. Several state abstraction techniques have been developed, which aggregate multiple states into one abstract state according to a certain criterion and reduce the state space size. For example, a state is equivalent to another state if choosing an action leads to the same state with same rewards. Such abstraction is effective for agents to solve more complicated MDPs than they would be able to without using abstraction. However, if we abstract the state space by regarding only identical states as equivalent, we are difficult to find all of them within a reasonable time, and even worse no two states might be identical.\nIn contrast to such exact state abstraction, Abel, Hershkowitz, and Littman (2016) proposed approximate state abstraction, which characterizes how close a state is to another state in single-agent MDPs. This technique reduces ground MDPs with large state spaces to abstract MDPs with smaller state spaces by aggregating states according to some notion of closeness or similarity. While this relaxation makes the state spaces smaller, the resulting optimal policies in abstract MDPs may become suboptimal. Furthermore, they derive error bounds for the resulting policies based on four different criteria, such as optimal Q-values, according to which states are aggregated.\nTZMGs extend MDPs to model decision-making by two interacting agents in a shared, state-dependent environment. Unlike MDPs, the rewards in TZMGs depend on the actions of both agents. For instance, in a soccer game, the rewards vary based on the state of play and the actions chosen by both players. TZMGs provide a framework to formalize such scenarios and have stimulated research in competitive MARL. An agent's optimal policy depends on the policy of its opponent. The goal is to identify the equilibrium policy profile, which consists of mutual best responses, to predict and understand the consequences of their interactions.\nBuilding upon these insights, this paper extends the work by Abel, Hershkowitz, and Littman (2016) from single-agent MDPs to TZMGs. We first describe an approximate state abstraction based on optimal Q-value criteria or minimax values and derive the bound of the duality gap for the resulting equilibrium, which measures the proximity to equilibrium. To establish the bound, we analyze the gains from best responses in the ground game \u2013 where the strategy space is unabstracted \u2013 against the resulting equilibrium in the abstracted game, where the strategy space has been simplified. Second, we conduct experiments in Markov Soccer and demonstrate how state spaces are reduced and how well"}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.1 Two-Player Zero-Sum Markov Games", "content": "A two-player zero-sum Markov game (TZMG) M is defined by a tuple $M = (S, A_1, A_2, P, R, \\gamma)$. Here, S represents a finite state space, $A_i$ represents an action space for player $i \\in \\{1, 2\\}$, $P : S \\times A_1 \\times A_2 \\rightarrow \\triangle(S)$ represents a transition probability function, $R : S \\times A_1 \\times A_2 \\rightarrow [0, 1]$ represents a reward function, and $\\gamma \\in [0, 1)$ represents a discount factor. Let $A = A_1 \\times A_2$, and let $a = (a_1, a_2) \\in A$ denote the action profile. For a given state $s \\in S$ and an action profile $a \\in A$, the next state is determined according to $P(\\cdot|s, a)$, and player 1 (resp. player 2) receives a reward of $R(s, a)$ (resp. $-R(s, a)$).\nA Markov policy for player i, denoted as $\\pi_i : S \\rightarrow \\triangle(A)$, represents the probability of choosing action $a_i \\in A_i$ at a state $s \\in S$. Letting $\\pi = (\\pi_1, \\pi_2)$ be a policy profile, we further define the state value function, which is the expected discounted sum of rewards at state $s \\in S$ as follows:\n$V^{\\pi}(s) := E[\\sum_{t=1}^{\\infty} \\gamma^{t-1}R(s_t, a_t) \\; | \\; s_1 = s, a_t \\sim \\pi(s_t), s_{t+1} \\sim P(s_t, a_t), t \\geq 0]$\nWe similarly define the state-action value function of taking an action profile $a \\in A$ at state $s \\in S$ as follows:\n$Q^{\\pi}(s, a) := R(s, a) + \\gamma \\sum_{s'\\in S} P(s'|s, a)V(s').$\nFrom the definition of these functions, the state value function $V^{\\pi}$ can be expressed as follows:\n$V^{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s)Q^{\\pi}(s, a).$"}, {"title": "2.2 Nash Equilibrium", "content": "In TZMGs, a Nash equilibrium is defined as the policy profile that satisfies the following condition for all $s \\in S$ simultaneously:\n$\\forall(\\pi_1, \\pi_2), V^{\\pi_1, \\pi_2^*}(s) \\geq V^{\\pi^*}(s) \\geq V^{\\pi_1^*, \\pi_2}(s)$.\nIntuitively, a Nash equilibrium is the policy profile where no player can improve her value by deviating from her own policy. Shapley (1953) has shown that any Nash equilibrium $\\pi^*$ in TZMGs satisfies the following condition for all $s \\in S$:\n$V^*(s) = \\max_{\\rho \\in \\triangle(A_1)} \\min_{a_2 \\in A_2} \\sum_{a_1 \\in A_1} \\rho(a_1)Q^*(s, a) = \\min_{\\rho \\in \\triangle(A_2)} \\max_{a_1 \\in A_1} \\sum_{a_2 \\in A_2} \\rho(a_2)Q^*(s, a)$.\nIt is known that this minimax value is unique for each s (Shapley 1953), thus we can write $V^*(s) := V^{\\pi^*}(s)$ and $Q^*(s, a) := Q^{\\pi^*}(s, a)$.\nTo measure the proximity to equilibrium for a given policy profile $\\pi = (\\pi_1, \\pi_2)$, we use the duality gap defined as follows:\n$GAP(\\pi) := \\max_{s \\in S, \\pi_1', \\pi_2'} (V^{\\pi_1', \\pi_2}(s) - V^{\\pi_1, \\pi_2'}(s))$.\nFrom the definition, we can see that $GAP(\\pi) \\geq 0$ for any $\\pi$, and the equality holds if and only if $\\pi$ is a Nash equilibrium."}, {"title": "2.3 Minimax Q-learning", "content": "Let us briefly describe Minimax Q-learning (Littman 1994) which is developed to address the limitations of standard Q-learning in adversarial, zero-sum environments. Its robustness and adaptability in competitive settings make it effective for AI in games and security applications. It has been shown that Minimax Q-learning converges to a Nash equilibrium under some appropriate conditions (Szepesv\u00e1ri and Littman 1999). Due to its theoretical guarantee and ease of implementation, Minimax Q-learning is used as a standard algorithm to compute equilibrium policies for TZMGs.\nAlgorithm 1 illustrates the procedure with finite T iterations."}, {"title": "3 State Abstraction", "content": "In this section, we extend state abstraction for a MDP to a TZMG. State abstraction is a method for reducing the state space by aggregating similar states to decrease the time of calculating the equilibria. In the previous research of Abel et al. (Abel, Hershkowitz, and Littman 2016), they propose four different state abstraction approaches for a MDP and theoretically analyze how well the optimal policy in the abstract MDP achieves performance in the ground MDP using various metrics. In this section, we extend their approach based on state-action value function similarity. The other approaches, including model similarity, Boltzmann distribution similarity, and multinomial distribution similarity, are introduced in Discussion section."}, {"title": "3.1 Abstract Two-Player Zero-Sum Markov Games", "content": "This section defines an abstract TZMG $M_A = (S_A, A_1, A_2, P_A, R_A, \\gamma)$ by using the notation introduced by Abel, Hershkowitz, and Littman (2016); Li, Walsh, and Littman (2006). Here, $S_A$ is an abstract state space, and $P_A : S_A \\times A_1 \\times A_2 \\rightarrow \\triangle(S_A)$ is an abstract transition probability function, and $R_A : S_A \\times A_1 \\times A_2 \\rightarrow [0, 1]$ is an abstract reward function.\nLet $\\phi : S \\rightarrow S_A$ be a state aggregation function that maps from the ground state space S to an abstract state space $S_A$. Given $\\phi$, we can define a set of states $G_A(s_A)$ that are aggregated into the same abstract state $s_A \\in S_A$:\n$G_A(s_A) := \\{g \\in S \\; | \\; \\phi(g) = s_a\\}.$\nFor a given ground state $s \\in S$, we further define a set of states $G(s)$ that are aggregated into the same abstract state as s:\n$G(s) := \\{g \\in S \\; | \\; \\phi(g) = \\phi(s)\\}$.\nIn order to construct an abstract transition probability function $P_A$ and an abstract reward function $R_A$, we introduce a weight function $w : S \\rightarrow [0, 1]$, which satisfies the following condition:\n$\\forall s_A \\in S_A, \\sum_{g \\in G_A(s_A)} w(g) = 1.$\nFor example, $w(s) = 1/|G(s)|$ is a representative weight function. Using this function, we define the abstract transition probability function $P_A$ as follows:\n$P_A(s_A'|s_A, a) := \\sum_{s \\in G_A(s_A)} \\sum_{s' \\in G_A(s_A')} P(s'|s, a)w(s).$\nSimilarly, the abstract reward function $R_A : S_A \\times A \\rightarrow [0, 1]$ is defined as follows:\n$R_A(s_A, a) := \\sum_{s \\in G_A(s_A)} R(s, a)w(s).$\nThe policy profile in the abstract TZMG $M_A$ is defined in a similar manner to the ground TZMG M. Let $\\pi_{A, i} : S_A \\rightarrow \\triangle(A_i)$ be a policy for player i in $M_A$. Similarly, $V^{\\pi_A}(s_A)$ denotes a state value for a given policy profile $\\pi_A$ at state $s_A \\in S_A$ in the abstract TZMG, and $Q^{\\pi_A}(s_A, a)$ is defined as a state-action value for $s_A \\in S_A$ and $a \\in A$. Finally, letting $\\pi_A^*$ be a Nash equilibrium in the abstract TZMG $M_A$, $\\pi_A^*$ must satisfy the following condition for all $s_A \\in S_A$ and $(\\pi_{A, 1}, \\pi_{A, 2})$:\n$V^{\\pi_{A,1}^*, \\pi_{A,2}}(s_A) \\geq V^{\\pi_A^*}(s_A) \\geq V^{\\pi_{A,1}, \\pi_{A,2}^*}(s_A).$"}, {"title": "4 Abstraction Based on Minimax Values", "content": "In this section, we analyze the performance of a Nash equilibrium $\\pi_A^*$ in an abstract TZMG $M_A$ under a specific aggregation function $\\phi$ when applied to the ground TZMG M. To this end, we define the policy profile $\\pi_{\\phi A}(s)$ in the ground TZMG, which is induced by $\\pi_A^*$. Formally, $\\pi_{\\phi A}(s)$ for all s in S is given by:\n$\\pi_{\\phi A}(s) := \\pi_A^*(\\phi(s))$.\nIn a later section, we derive the upper bound on the duality gap of $\\pi_{\\phi A}^*$ under various aggregation functions $\\phi$."}, {"title": "4.1 Suboptimality of Nash Equilibria in Abstract TZMGs", "content": "We examine the aggregation function $\\phi^{Q^*}$, which is constructed based on the state-action value function $Q^*$. Specifically, in the abstraction $\\phi^{Q^*}$, states are aggregated to the same abstract state when their minimax state-action values are close within $\\epsilon$.\nAssumption 1. The apggregation function $\\phi^{Q^*}$ satisfies the following property for some non-negative constant $\\epsilon \\geq 0$:\n$\\phi^{Q^*}(s_1) = \\phi^{Q^*}(s_2) \\Rightarrow \\forall a \\in A, |Q^*(s_1, a) - Q^*(s_2, a)| \\leq \\epsilon$.\nUnder Assumption 1, it can be shown that the suboptimality of $\\pi_{\\phi A}^*$ is no more than $O(\\epsilon)$.\nTheorem 1. When the ground states are aggregated by the apggregation function $\\phi^{Q^*}$ satisfying Assumption 1 with $\\epsilon > 0$, then $\\pi_{\\phi A}^*$ satisfies:\n$GAP(\\pi_{\\phi A}^*) < \\frac{12\\epsilon}{(1 - \\gamma)^3}.$\nTo perform an initial abstraction, minimax Q-learning is used to calculate Q-values for the ground game. If the results do not meet a sufficient solution criterion, the process iteratively updates the Q-values and refines the abstraction (Li, Walsh, and Littman 2006). Developing efficient algorithms for discovering abstractions remains open. Also, such abstractions have potential utility beyond a single game, enabling transferable knowledge across related games (Jong and Stone 2005)."}, {"title": "4.2 Proofs for Theorem 1", "content": "This section provides the proofs for Theorem 1.\nProof of Theorem 1. By the definition of the duality gap:\n$GAP(\\pi_{\\phi A}^*) = \\max_{s \\in S, \\pi_1, \\pi_2} (V^{\\pi_1, \\pi_{\\phi A, 2}}(s) - V^{\\pi_{\\phi A}}(s)) \\leq \\max_{s \\in S, \\pi_1} (V^{\\pi_1, \\pi_{\\phi A, 2}}(s) - V^{\\pi_{\\phi A}}(s)) + \\max_{s \\in S, \\pi_2} (V^{\\pi_{\\phi A}}(s) - V^{\\pi_{\\phi A, 1}, \\pi_2}(s)).$\nHence, it is sufficient to derive the upper bound on $\\max_s V^{\\pi_1, \\pi_{\\phi A, 2}}(s) - V^{\\pi_{\\phi A}}(s)$ and $\\max_s V^{\\pi_{\\phi A}}(s) - V^{\\pi_{\\phi A, 1}, \\pi_2}(s)$ for each $s \\in S$. Here, letting $\\pi_1^*$ be an optimal policy against $\\pi_{\\phi A, 2}$ in the ground TZMG, we obtain the following result on the performance difference between $\\pi_1^*$ and $\\pi_{\\phi A, 1}$ against $\\pi_{\\phi A, 2}$. The proof for Lemma 1 is provided in Appendix A.1."}, {"title": "5 Experiments", "content": "This section demonstrates our state abstraction developed so far in Markov Soccer (Littman 1994; Abe and Kaneko 2021). We here focus only on the minimax values since the theoretical results on the other criteria in Section 6 rely on the minimax values."}, {"title": "5.1 Markov Soccer", "content": "We describe the Markov soccer experiment as 1 vs 1 game on 4 \u00d7 5 as shown in Figure 1. Two players \"1\" and \"2\" occupy distinct squares of the grid, respectively and the circled player, \"1\" here, has a \u201cball,\u201d which specifies the states of the game. Figure 1 shows the initial positions of the players. Which player has the ball at the initial turn is determined at uniformly random. In each turn, each player can move to one of the neighboring cells or stand at the place, i.e., their set of actions includes \u201cUp\u201d, \u201cLeft\u201d, \u201cDown\u201d, \u201cRight\u201d, and \"Stand.\"\nAfter both select their actions, these two moves are executed in random order. When a player tries to move to the cell occupied by the other player, the ball's possession goes to the stationary player, and the positions of both players remain unchanged. Also, if a player's choice lets him or her be out of the pitch, the position of the player remains unchanged. When a player keeping the ball steps into his or her goal (the right side for player 1 and the left side for player 2), the game is over. At the same time, that player scores 1 point and the opponent scores -1 point. The positions of the players and the ball's possession are initialized as shown in Figure 1."}, {"title": "5.2 Training and Evaluating", "content": "In Markov soccer, we build state abstraction, by first solving the game, then greedily aggregating ground states into abstract states that satisfy the $Q^*$ criterion. We calculate the number of states in the abstract Markov game $|S_A|$, varying $\\epsilon$ from 0.0 to 2.0 in increments of 0.1. Then, for each $\\epsilon$, we compute the equilibrium policies $\\pi^*$ and $\\pi_A^*$ for the ground and the abstract Markov soccer games, respectively, via the minimax Q-learning. We here assume that the total number of learning iterations T was 1,000,000, the discount factor $\\gamma$ was 0.9, and the learning rate $\\alpha_t$ was set to $10^{-4}/t$ for learning iterations $t \\geq 0$. We further evaluate the duality gaps for those equilibrium policies. However, it is demanding to calculate the true one, so we use the approximation obtained from Q-learning."}, {"title": "5.3 Results", "content": "We first compute the number of states in the abstract Markov soccer game for different of $\\epsilon$ in Figure 2. The x- and y-axes represent $\\epsilon$ and the number of states $|S_A|$, respectively. We observed that as $\\epsilon$ increases, the number of states decreases almost linearly. Note that the number of states of the ground Markov soccer is 760, as illustrated at $\\epsilon = 0.0$. We observe the value of $\\epsilon$ up to two since the difference between the state-action value functions is bound up to 2 from the definition of the game. The number of states of the abstract Markov soccer is 1 at $\\epsilon = 2.0$.\nFigure 3 illustrates the duality gap in the number of learning iterations where x- and y-axes represent learning iterations and the gap, respectively, varying $\\epsilon$. We label \"Ground\" the gap for the ground Markov soccer game ($\\epsilon = 0.0$) and draw the trajectories varying $\\epsilon \\in \\{0.2, 0.6, 1.0, 1.4, 1.8\\}$. We observe that the minimax Q-learning approximately solves the ground game, i.e., the gap converges to zero. If $\\epsilon$ is sufficiently small, i.e., less than 0.6, the duality gap in the abstract Markov soccer game approximates the ground one. Otherwise, the gaps become significantly worse. In these cases, agents often repeat previous actions without progress. Such deadlocks increase the duality gap, as agents can more easily identify best responses by fully exploring the ground game's strategy space."}, {"title": "6 Extentions", "content": "We have extended the approximate function that preserves near-optimal behavior by aggregating states on similar optimal Q-values to two-player zero-sum Markov games. This"}, {"title": "A Proofs for Theorem 1", "content": ""}, {"title": "A.1 Proof of Lemma 1", "content": "Proof of Lemma 1. First, we introduce the following lemma:\nLemma 4. Assume that the aggregation function satisfies the following condition for some non-negative constant $\\delta \\geq 0$:\n$\\forall s \\in S, a \\in A, |Q^{\\pi_{\\phi A, 2}}(s, a) - Q^{\\pi^*}(\\phi(s), a)| < \\delta$.\nThen, for any $s \\in S$, we have:\n$V^{\\pi_1^*, \\pi_{\\phi A, 2}}(s) \\leq \\sum_{a \\in A} \\pi_1^*(a|s)Q^{\\pi_{\\phi A, 2}}(s, a) + 2\\delta$.\nThe proof of this lemma is shown in Appendix A.2. From Lemma 4, we have for any $s \\in S$:\n$V^{\\pi_1^*, \\pi_{\\phi A, 2}}(s) - V^{\\pi_{\\phi A}}(s) \\leq 2\\delta + \\sum_{a \\in A} \\pi_{\\phi A}(a|s)Q^{\\pi_{\\phi A, 2}}(s, a) - V^{\\pi_{\\phi A}}(s)$\n$= 2\\delta + \\sum_{a \\in A} \\pi_{\\phi A}(a|s) (Q^{\\pi_{\\phi A, 2}}(s, a) - Q^{\\pi^*}(\\phi(s), a))$\n$= 2\\delta + \\gamma \\sum_{a \\in A} \\pi_{\\phi A}(a|s) \\sum_{s' \\in S} P(s'|s, a) (V^{\\pi_1^*, \\pi_{\\phi A, 2}}(s') - V^{\\pi_{\\phi A}}(s'))$\n$\\leq 2\\delta + \\gamma \\max_{s' \\in S} (V^{\\pi_1^*, \\pi_{\\phi A, 2}}(s') - V^{\\pi_{\\phi A}}(s'))$.\nTaking the maximum value of both sides, we have:\n$\\max_{s' \\in S} (V^{\\pi_1^*, \\pi_{\\phi A, 2}}(s) - V^{\\pi_{\\phi A}}(s')) \\leq 2\\delta + \\gamma \\max_{s' \\in S} (V^{\\pi_1^*, \\pi_{\\phi A, 2}}(s') - V^{\\pi_{\\phi A}}(s')).$\nBy moving $\\gamma \\max_{s \\in S} (V^{\\pi_1^*, \\pi_{\\phi A, 2}}(s') - V^{\\pi_{\\phi A}}(s'))$ from the right side to the left side, we have for any $s \\in S$:\n$V^{\\pi_1^*, \\pi_{\\phi A, 2}}(s) - V^{\\pi_{\\phi A}}(s) \\leq \\max_{s \\in S} (V^{\\pi_1^*, \\pi_{\\phi A, 2}}(s') - V^{\\pi_{\\phi A}}(s')) \\leq \\frac{2\\delta}{1 - \\gamma}."}, {"title": "A.2 Proof of Lemma 4", "content": "Proof of Lemma 4. From the definition of $V^{\\pi_1^*, \\pi_{\\phi A, 2}}$, we have for any $s \\in S$:\n$V^{\\pi_1^*, \\pi_{\\phi A, 2}}(s) = \\sum_{a \\in A} \\pi_1^*(a_1|s) \\pi_{\\phi A, 2}(a_2|s) Q^{\\pi_1^*, \\pi_{\\phi A, 2}}(s, a)$.\nFrom the assumption of Lemma 4, we get:\n$V^{\\pi_1^*, \\pi_{\\phi A, 2}}(s) \\leq \\sum_{a \\in A} \\pi_1^*(a_1|s) \\pi_{\\phi A, 2}(a_2|s) Q^{\\pi^*}(\\phi(s), a) + \\delta$\n$= \\sum_{a \\in A} \\pi_1^*(a_1|s) \\pi_{\\phi A, 2}(a_2|\\phi(s)) Q^{\\pi^*}(\\phi(s), a) + \\delta$\n$\\leq \\sum_{a \\in A} \\pi_{\\phi A, 1}^*(a_1| \\phi(s)) \\pi_{\\phi A, 2}(a_2|\\phi(s)) Q^{\\pi^*}(\\phi(s), a) + \\delta$\n$\\leq \\sum_{a \\in A} \\pi_{\\phi A, 1}^*(a_1| \\phi(s)) \\pi_{\\phi A, 2}(a_2|s) Q^{\\pi_{\\phi A, 2}}(s, a) + 2\\delta$\n$= \\sum_{a \\in A} \\pi_{\\phi A, 1}^*(a_1| s) \\pi_{\\phi A, 2}(a_2|s) Q^{\\pi_1^*, \\pi_{\\phi A, 2}}(s, a) + 2\\delta.$"}, {"title": "B Proofs for Theorem 2", "content": ""}, {"title": "B.1 Proof of Theorem 2", "content": "Proof of Theorem 2. We introduce the following lemma:\nLemma 5. In the same setup of Theorem 2, we have for any $s \\in S$ and $a \\in A$:\n$|Q^{\\pi_{\\phi A, 2}}(s, a) - Q^{\\pi^*}(\\phi^{\\text{model}}(s), a)| < \\frac{\\epsilon (1 + \\gamma (|S| - 1))}{(1 - \\gamma)^2}.$\nBy combining Lemma 1 and Lemma 5, we get for any $s \\in S$:\n$V^{\\pi_1^*, \\pi_{\\phi A, 2}}(s) - V^{\\pi_{\\phi A}}(s) \\leq \\frac{2\\epsilon (1 + \\gamma (|S| - 1))}{(1 - \\gamma)^2}.$\nBy a similar procedure, we can show that:\n$V^{\\pi_{\\phi A}}(s) - V^{\\pi_{\\phi A, 1}}(s) < \\frac{2\\epsilon (1 + \\gamma (|S| - 1))}{(1 - \\gamma)^2}.$\nTherefore, from (3), we obtain:\n$GAP(\\pi_{\\phi A}^*) \\leq \\frac{4(1 + \\gamma (|S| - 1))\\epsilon}{(1 - \\gamma)^3}$"}, {"title": "B.2 Proof of Lemma 5", "content": "Proof. Firstly", "A$": "n$\\leq |R(s_1"}, {"A$": "n$Q^{\\pi^*"}], "content": ""}, {"title": "C.1 Proof of Theorem 3", "content": "Proof. From Assumption 3, if $\\phi^{\\text{bolt}}(s_1) = \\phi^{\\text{bolt}}(s_2)$, we have for any $a \\in A$:\n$\\leq \\frac{\\epsilon \\left(\\sum_{b \\in A} e^{Q^*(s_1, b)} + k\\epsilon\\right)}{\\sum_{b \\in A} e^{Q^*(s_1, b)}} = k\\epsilon \\frac{e^{Q^*(s_1, a)}}{\\sum_{b \\in A} e^{Q^*(s_1, b)}} + e^{Q^*(s_1, a)} - e^{Q^*(s_2, a)}|$."}, {"title": "D Proofs for Theorem 4", "content": ""}, {"title": "D.1 Proof of Theorem 4", "content": "Proof. From Assumption 4, if $\\sum_{b \\in A} Q^*(s_2, b) \\geq 0$, then we have for any $a \\in A$ and $s_1, s_2 \\in S$ such that $\\phi^{\\text{mult}}(s_1) = \\phi^{\\text{mult}}(s_2)$:\n$=\\kappa \\epsilon + Q^*(s_1, a) - Q^*(s_2, a)$."}]