{"title": "Semi-Implicit Neural Ordinary Differential Equations", "authors": ["Hong Zhang", "Ying Liu", "Romit Maulik"], "abstract": "Classical neural ODEs trained with explicit methods are intrinsically limited by stability, crippling their efficiency and robustness for stiff learning problems that are common in graph learning and scientific machine learning. We present a semi-implicit neural ODE approach that exploits the partitionable structure of the underlying dynamics. Our technique leads to an implicit neural network with significant computational advantages over existing approaches because of enhanced stability and efficient linear solves during time integration. We show that our approach outperforms existing approaches on a variety of applications including graph classification and learning complex dynamical systems. We also demonstrate that our approach can train challenging neural ODEs where both explicit methods and fully implicit methods are intractable.", "sections": [{"title": "1 Introduction", "content": "Implicit neural networks (NNs) (Winston and Kolter 2020; Bai, Kolter, and Koltun 2019) have emerged as a new paradigm for designing infinite-depth networks. The traditional neural network consists of an explicit stack of layers. In contrast, an implicit neural network represents an implicit model that often involves a nonlinear system, such as neural ordinary differential equations (ODEs) (Chen et al. 2018; Dupont, Doucet, and Teh 2019; Massaroli et al. 2020; Kidger 2022) and equilibrium models (Winston and Kolter 2020; Bai, Kolter, and Koltun 2019). In order to solve the nonlinear system, an iterative procedure is often needed. For example, neural ODEs require a time integrator to obtain the solution at desired time points. Deep equilibrium (DEQ) models (Bai, Kolter, and Koltun 2019) solve an implicit system with fixed-point iteration. Differentiating through the iterative procedure with direct backpropagation can be intractable, however, due to the memory consumption that increases linearly. Key to the success of existing implicit NNs is using high-level adjoint methods for gradient calculation. The adjoint methods can reduce the memory footprint significantly for both equilibrium models (Winston and Kolter 2020) and ODE models (Chen et al. 2018; Zhuang et al. 2020; Zhang and Zhao 2022)."}, {"title": "2 Preliminaries", "content": "Adjoints and Automatic Differentiation Reverse-mode automatic differentiation, as widely used in scientific computing and machine learning, is also an adjoint method implemented at a low level. The adjoint methods used for implicit networks are derived at a high level of abstraction. We will take the DEQ model and the neural ODE model as examples. In a continuous view, the forward pass of the DEQ model can be considered as a root-finding procedure for a nonlinear function, and the loss depends on the root:\n$z = f(z, u; p), Loss l = \\Phi(z)$,\nwhere z is the equilibrium state for input u and f stands for the NN parameterized by p. The adjoint equation of the nonlinear problem (Bai, Kolter, and Koltun 2019) is a linear equation defined as\n$(I - \\frac{\\partial f(z, u; p)}{\\partial z}^T) \\lambda = (\\frac{dl}{dz})^T$,\nwhere the derivative $\\frac{dl}{dz}$ is a row vector and the adjoint variable $\\lambda$ is a column vector. The loss gradient (a column vector) can be calculated with\n$\\nabla_pl = (\\frac{\\partial f(z, u; p)}{\\partial p})^T \\lambda = (\\frac{dl}{dp})^T$.\nThe adjoint model requires storing only z and u during training; therefore, the memory cost is independent of the number of iterations in the forward pass.\nFor an autonomous neural ODE\n$\\frac{du(t)}{dt} = f(u(t); p), t \\in [t_0, T], Loss l = \\Phi(u(T))$,\nits continuous adjoint (Kidger, Chen, and Lyons 2020) is a new ODE integrated backward from time T to 0:\n$\\frac{d\\lambda(t)}{dt} = -(\\frac{\\partial f(u(t); p)}{\\partial u(t)})^T \\lambda(t)$,\nThe loss gradient is computed with\n$\\nabla_pl = \\int_0^T (\\frac{\\partial f(u(t); p)}{\\partial p})^T \\lambda(t) dt$.\nFor notational brevity, throughout this paper we omit p in all parameterized functions and drop the arguments (\u00b7) for functions and variables such as f, u, and $\\lambda$ when there is no ambiguity.\nReverse-accurate Adjoint Approaches In a discrete adjoint approach, the adjoint is derived based on the discretized version of the continuous equations (4). Without loss of generality, we assume (4) is discretized with a time-stepping algorithm, denoted by an operator $N$ that propagates the solution from one time step to another:\n$U_{n+1} = N(U_n),  n = 0, 1, ..., N$.\nThe adjoint model of this time-stepping algorithm is\n$\\lambda_n = (\\frac{\\partial N}{\\partial u_n})^T \\lambda_{n+1} + \\mu_{n+1}$,\n$\\mu_n = (\\frac{\\partial l}{\\partial p}(U_n))^T + (\\frac{\\partial N}{\\partial p} (U_n))^T \\lambda_{n+1}$,\nwith terminal condition $\\lambda_N = (\\frac{\\partial \\Phi}{\\partial u_N})^T, \\mu_N = (\\frac{\\partial l}{\\partial p} (U_N))^T$.\nThe adjoint variables $\\lambda$ and $\\mu$ denote the loss gradient with respect to the state u and the NN parameters p, respectively. For a practical implementation, (8) is typically derived in a case-by-case manner for different time-stepping algorithms (Zhang and Sandu 2014). The discrete adjoint model is conceptually equivalent to applying automatic differentiation to each iteration of the time-stepping procedure, therefore yielding the equally accurate gradient as obtained with direct backpropagation. Since the same trajectory (sequence of time steps) is used for the forward pass and the backward pass, one only needs to control step sizes during the forward pass when using adaptive time steps with the discrete adjoint approach."}, {"title": "3 The Partitioned Model", "content": "We consider an autonomous neural ODE framework with an additively partitioned right-hand side:\n$\\frac{du}{dt} = G(u) + H(u)$,\nwhere u(t) \u2208 $R^d$ is the state, t is the time, and $G : R^d \u2192 R^d$ and $H : R^d \u2192 R^d$ are NNs, for example feed-forward NNs. Similar to conventional neural ODE, our framework takes an input u(to), learns the representation of the dynamics, and predicts the output u(tn) or a sequence of outputs $U_{t_i}, i = 1,..., N$ by solving (9) from the starting time to to the ending time tn with a numerical ODE solver. While this framework is general and applicable for a wide range of applications, we are interested in a specific model with a nonlinear part and a linear part. For a purely linear neural ODE, which is rare in practice, we can use a partitioned model containing two linear parts. Without loss of generality, we assume H is linear, and thus (9) can be rewritten to\n$\\frac{du}{dt} = G(u) + Tu$.\nThe reason for this choice will become clear later."}, {"title": "4 Training Algorithms", "content": "To train the partitioned NN, we adopt a semi-implicit approach to integrate (9) in a forward pass and compute the gradients with a discrete adjoint approach in a backward pass. The algorithm for each pass will be described below."}, {"title": "4.1 Forward Pass", "content": "In the forward pass, we solve the partitioned ODE (9) using implicit-explicit RK (IMEX-RK) methods. In particular, the right-hand side G is treated explicitly while H is treated implicitly. At each time step, the solution is advanced from tn to tn+1 using\n$U^{(i)} = u_n + \\Delta t \\sum_{j=1}^{i-1} a_{ij} G(U^{(j)}) + \\Delta t \\sum_{j=1}^{i} \\bar{a}_{ij} H(U^{(j)}),  i = 1, ..., s$\n$u_{n+1} = u_n + \\Delta t \\sum_{j=1}^s b_j G(U^{(j)}) + \\Delta t \\sum_{j=1}^s \\bar{b}_j H(U^{(j)}),$\nwhere $U^{(i)}$ is the stage value, \u0394t is the step size, $A = {a_{ij}}$ is strictly lower triangular, $\\bar{A} = {\\bar{a}_{ij}}$ is lower triangular and can have zeros on the diagonal (these correspond to explicit stages), and (i) denotes stage index i. $G^{(i)}$ and $H^{(i)}$ are shorthand for $G(U^{(i)})$ and $H(U^{(i)})$. The IMEX-RK scheme can be represented by the Butcher tableau consisting of (A = {$a_{ij}$}, b, c) for the explicit part and ($\\bar{A} = {\\bar{a}_{ij}}$, $\\bar{b}$, $\\bar{c}$) for the implicit part. See the appendix for the coefficients of the IMEX methods used in this paper. The IMEX-RK methods are well known for their favorable stability properties for stiff systems (Ascher, Ruuth, and Spiteri 1997; Boscarino 2007; Boscarino and Russo 2009; Kennedy and Carpenter 2003a).\nLinear Implicit System At each stage, one needs to solve a nonlinear system for $U^{(i)}$, typically with an iterative method such as the Newton method. If H is linear, however, (11) falls into a linear problem with respect to $U^{(i)}$:\n$(I - \\Delta t \\bar{a}_{ii} T) U^{(i)} = u_n + \\Delta t \\sum_{j=1}^{i-1} a_{ij} G(U^{(j)}) + \\Delta t \\sum_{j=1}^{i-1} \\bar{a}_{ij} T U^{(j)}$,\nwhere T is constant across the s stages and all the time steps. Thus, the linear-nonlinear partitioning can significantly increase computational efficiency by avoiding solving a nonlinear system. In practice, this partitioning is naturally applicable to many multiphysics problems. For example, in diffusion-reaction equations, diffusion can be represented by a linear term and treated implicitly in an IMEX setting. In neural-network-based models, layers such as convolution layers or fully connected layers without activation functions can be viewed as a linear map as well.\nEfficient Mini-batching Consider a mini-batch $B_n = {u_n, ..., u_m}$ consisting of m inputs at time $t_n$. The batch Jacobian in (12) can be represented in Kronecker product form:\n$[(I - \\Delta t \\bar{a}_{ii} I) \\otimes I_m] U_B^{(i)} = B_n + \\Delta t \\sum_{j=1}^{i-1} a_{ij} G_B(U_B^{(j)})\n+ \\Delta t \\bar{a}_{ij} (I \\otimes I_m)U_B^{(j)}$,\nwhere $U_B^{(i)}$ is the corresponding stage values for Bn. Since the linear problem (13) is equivalent to\n$[I - \\Delta t \\bar{a}_{ii} I] reshape(U_B^{(i)}, d, m) = reshape(B_n, d, m)\n+ \\Delta t \\sum_{j=1}^{i-1} a_{ij} reshape(G_B(U^{(j)}), d, m)\n+ \\Delta t \\bar{a}_{ij} I reshape(U_B^{(j)}, d, m)$,\nwe need to solve only a reduced linear system with m right-hand sides reshaped from the batched vectors. This problem can be solved conveniently by off-the-shelf linear solvers, including direct solvers and iterative solvers. With direct solvers, the factorization of the left-hand side $I - \\Delta t \\bar{a}_{ii} I$ can be reused across stages and time steps because I is modified at the end of each training iteration when the NN parameters are updated by the optimizer. If I is known a priori and not parameterized, the factorization can be reused even across mini-batches. Iterative solvers are typically chosen because of their excellent scalability and low memory requirement. If I is large and sparse, using a matrix-free iterative solver may be advantageous."}, {"title": "4.2 Backward Pass", "content": "Unlike the vanilla neural ODE that employs a continuous adjoint approach, we use a discrete approach as an alternative to directly backpropagating through an ODE solver.\nTheorem 4.1. The gradient of loss l with respect to the NN parameters p for SINODE using the IMEX-RK methods in (11) can be calculated with the following discrete adjoint formula:\n$(\\lambda_n^1  H_p^{(i)}) \\lambda_{n+1} + \\Delta t (G(i) \\sum_{j=i+1}^s a_{ji} \\lambda_{n+1}^j$\n+ (1)) \u03a3 \u1f04\u03ad\u03bb i = s, \u2026 , 1,\n\u03bb\u03b7 = \u03bb\u03b7+1 + \u03a3\u03bbj,\n+1 = \u0394t (bi() + bH(i)) \u03bb\u03b7+1\n+ (\u03a3\u03b3\u03ad\u03bb (2) \u03a3\u03ac\u03b3\u03ad\u03bb i = s, \u2026 , 1,\n\u03bc\u03b7 = \u03bc\u03b7+1 + \u03a3\u03bc\u03af,\nwith terminal condition\n$\\lambda_N = \\frac{dl}{du}, \\mu_N = 0$."}, {"title": "5 Experiments", "content": "We validate and evaluate the performance of our methods when learning stiff ODE systems. Throughout this section we compare our methods with a variety of baseline methods including explicit and implicit methods. The neural network architectures we use follow the best settings identified in previous works (Linot et al. 2022; Chamberlain et al. 2021). The only modification necessary for using our method is to split the ODE right-hand side. Code is available at https://github.com/caidao22/pnode."}, {"title": "5.1 Graph Classification with GRAND", "content": "GRAND is an expansive and innovative class of GNNs that draws inspiration from the discretized diffusion PDE and applies it to graphs. These architectures address prevailing challenges of current graph machine learning models such as depth, over-smoothing, bottlenecks, and graph rewiring. With a reinterpretation of the diffusion equation for graphs, the process can be depicted by an ODE system:\n$\\frac{dx(t)}{dt} = (A(x(t)) - I) x(t)$\nwhere A(x(t)) = ($a(x_i(t), x_j(t))$)ij is an attention matrix with the same structure as the adjacency matrix of the graph and I is an identity matrix.\nWe introduce an ODE block where we apply the SINODE approach and split the right-hand side of (18) into a nonlinear part and a linear part:\n$\\frac{dx(t)}{dt} = (A(x(t)) - I) x(t) = A(x(t)) x(t) + (-Ix(t))$.\nThe nonlinear part is handled explicitly, and the linear part is handled implicitly with IMEX-RK methods. We use an iterative matrix-free solver in PETSc for solving the linear systems because of the simplicity of the linear part.\nFor assessment, we choose three benchmark datasets: Cora, CoauthorCS, and Photo. In addition to the baseline methods mentioned, we include the implicit Adams, which is claimed to perform well for this problem in (Chamberlain et al. 2021). Because of the stability constraints, we have to utilize a step size of 0.005 for explicit methods, while the implicit methods and the IMEX methods allow us to use a step size of 1 thanks to their superior stability properties.\nCompared with the explicit methods and the implicit methods, our approach boasts advantages under two performance evaluation metrics. On the one hand, as depicted in Table 1, SINODE with IMEX-RK methods requires significantly fewer numbers of right-hand side function evaluations (NFEs) than explicit methods do. Each function evaluation corresponds strictly to one forward pass of the NN. Thus, NFE is usually a good metric that accounts for the dominant cost for training neural ODEs while being independent of the NN complexity. On the other hand, as illustrated in Fig. 1, SINODE with IMEX-RK methods takes much less training time than classical neural ODEs with explicit methods to achieve the same level of testing accuracy."}, {"title": "5.2 Learning Dynamics for the Kuramoto-Sivashinsky (KS) Equation", "content": "The KS equation in one-dimensional space can be written as\n$\\frac{\\partial u}{\\partial t} = -u \\frac{\\partial u}{\\partial x} - \\frac{\\partial^2 u}{\\partial x^2} - \\frac{\\partial^4 u}{\\partial x^4}, x \\in [0, 22]$.\nIt is a well-known nonlinear chaotic PDE for modeling complex spatiotemporal dynamics.\nWe train neural ODEs by approximating the nonlinear term $u \\frac{\\partial u}{\\partial x}$ with a fully connected multilayer perceptron (MLP) and approximating the remaining linear terms on the right-hand side of (20) with a fixed convolutional NN (CNN) layer. We use circular padding and a CNN filter of size 5 to mimic centered finite-difference operators for the antidiffusion and hyperdiffusion terms with periodic boundary conditions. The parameters of the filter are fixed to be [-1.0/\u0394x4, 4.0/\u0394x4 \u2013 1.0/\u0394x\u00b2, -6.0/\u0394x4 + 2.0/\u0394x\u00b2, 4.0/\u0394x4 \u2013 1.0/\u0394x\u00b2, \u22121.0/\u0394x4], where the grid spacing Ax = L/N. A direct solver with reused LU factorizations is used to solve the linear systems associated with the CNN layer.\nWhen applying SINODE, we treat the MLP part explicitly and the CNN part implicitly. A time step size 0.2 is used for the IMEX methods and the fully implicit method. Because of the stability constraints, the explicit methods do not work well until we decrease the step size to 0.001. Fig. 3 compares a true trajectory to predictions from SINODE. We can see that the predicted trajectories stay on the attractor for a long time and match the ground truth well."}, {"title": "5.3 Learning Dynamics for the Viscous Burgers Equation", "content": "We consider the one-dimensional viscous Burgers equation\n$\\frac{\\partial u}{\\partial t} = -u \\frac{\\partial u}{\\partial x} + \\nu \\frac{\\partial^2 u}{\\partial x^2},  x \\in [0, 1]$,\nwith viscosity \u03bd = 8\u00b710\u22124, which makes the solution known as Burger turbulence.\n$\\frac{\\partial^2 u}{\\partial x^2}$\nThe architecture of the neural ODEs approximates the nonlinear term -uu with a a fully connected nonlinear NN, and the linear term va with a fixed linear layer derived from a finite difference method. Again we choose the direct solver to benefit from the reuse of LU factorizations. In the comparison of different methods, we choose a time step size of 0.05 as a conservative choice for the four IMEX-RK methods and the fully implicit method. Explicit methods cause the solution to explode until the time step size decreases to as small as 0.001 when the grid size is 512 and to 0.0005 when the grid size is 1024.\nFig. 5 shows that the predictions made by SINODE are in good agreement with the true solutions for an initial condition from the testing dataset."}, {"title": "6 Related Work", "content": "Implicit Networks Whereas explicit networks consist of a sequence of layers, implicit networks define the output as a solution to a nonlinear system For example, the well-known neural ODE model (Chen et al. 2018) solves an ODE problem in the forward pass, typically with explicit integration methods such as RK methods. The DEQ model (Winston and Kolter 2020; Bai, Kolter, and Koltun 2019) finds an equilibrium solution to a nonlinear system with a fixed-point iteration method in the forward pass. These models require solving a nonlinear system by embedding an iterative solver in the architecture. Therefore, common numerical issues associated with the solver, such as convergence and stability, may be more frequently encountered during the training process. For example, explicit methods can be restricted to infinitely small step sizes when neural ODEs are applied to stiff problems. Fully implicit methods (Zhang and Zhao 2022) are absolutely stable, but the nonlinear solvers may fail to converge for ill-conditioned systems. In addition, solving the nonlinear system poses a computational bottleneck for training implicit networks. These limitations have motivated a variety of acceleration techniques such as IMEXnet (Haber et al. 2019), equation scaling (Kim et al. 2021), JFB (Fung et al. 2022) and proximal implicit neural ODE (Baker et al. 2022), as well as this work.\nGradient Calculation for Implicit Networks Using backpropagation directly in the backward pass is often intractable for training implicit networks because they may require a large number of iterations and the memory to store the computation graphs increases with this number. In order to address this issue, adjoint methods (Chen et al. 2018; Winston and Kolter 2020; Zhuang et al. 2020) have been widely used to compute the gradient. When applied to ODEs, these methods typically require storing selective states for the Jacobian evaluation during a forward pass, known as checkpointing, and restoring missing states during a backward pass via recomputation (Chen et al. 2018; Zhuang et al. 2020) or interpolation (Daulbaev et al. 2020).\nReverse-accurate Neural ODEs Although adjoint methods provide a memory-efficient alternative to direct backpropagation, they do not always generate an accurate gradient. Various researchers have (Onken and Ruthotto 2020; Zhang and Zhao 2022) pointed out that the vanilla neural ODE (Chen et al. 2018) adopts a continuous adjoint approach that is not reverse accurate. In particular, the gradient is not consistent with the forward pass, even if the same time integrator and the same step size are used for solving the continuous adjoint equation (5) as shown in (Zhang and Zhao 2022). Reverse-accurate variants of neural ODEs have been developed in (Gholaminejad, Keutzer, and Biros 2019; Zhang et al. 2019; Zhuang et al. 2020, 2021; Matsubara, Miyatake, and Yaguchi 2023). These variants rely on backpropagation, checkpointing and recomputation to alleviate the memory cost. Zhang and Zhao (2022) proposed a discrete adjoint approach that achieves reverse accuracy and can leverage optimal checkpointing schedules (Zhang and Constantinescu 2021, 2023) to balance the trade-off between recomputation and memory usage."}, {"title": "7 Conclusion", "content": "We propose a semi-implicit approach, named SINODE, for learning stiff neural ODEs. Our approach leverages a nonlinear-linear partitioning of the ODE right-hand side and a differentiable IMEX solver that provides reverse-accurate gradient computation. The excellent stability property of IMEX methods enables large time steps to be used for integrating the ODEs, which is especially critical for stiff problems where other methods are inefficient. In addition, our approach allows us to use off-the-shelf linear solvers in mini-batch training, opening the opportunity of adapting well-established techniques from the other fields. We demonstrate the advantages of SINODE when applied to graph learning tasks and time series modeling of two challenging dynamical systems. We show that SINODE typically allows us to use a time step that is two orders of magnitude larger than that allowed by explicit RK methods, leading to significant speedup over classic neural ODEs. We also show that SINODE succeeds for very stiff cases where existing methods fail due to instability.\nLimitations SINODE should not be used for nonstiff problems where classic neural ODEs with explicit methods could be more efficient. When direct linear solvers are used, varying step sizes may degrade the performance of SINODE because the Jacobian cannot be reused across time steps. However, the performance is not affected much when matrix-free iterative linear solvers are used."}]}