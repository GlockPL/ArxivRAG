{"title": "A Survey of Data Synthesis Approaches", "authors": ["Hsin-Yu Chang", "Pei-Yu Chen", "Tun-Hsiang Chou", "Chang-Sheng Kao", "Hsuan-Yun Yu", "Yen-Ting Lin", "Yun-Nung Chen"], "abstract": "This paper provides a detailed survey of synthetic data techniques. We first discuss the expected goals of using synthetic data in data augmentation, which can be divided into four parts: 1) Improving Diversity, 2) Data Balancing, 3) Addressing Domain Shift, and 4) Resolving Edge Cases. Synthesizing data are closely related to the prevailing machine learning techniques at the time, therefore, we summarize the domain of synthetic data techniques into four categories: 1) Expert-knowledge, 2) Direct Training, 3) Pre-train then Fine-tune, and 4) Foundation Models without Fine-tuning. Next, we categorize the goals of synthetic data filtering into four types for discussion: 1) Basic Quality, 2) Label Consistency, and 3) Data Distribution. In section 5 of this paper, we also discuss the future directions of synthetic data and state three direction that we believe is important: 1) focus more on quality, 2) the evaluation of synthetic data, and 3) multi-model data augmentation.", "sections": [{"title": "1 Introduction", "content": "Synthetic data has always played a significant role in the field of machine learning (He et al., 2008a; Bol\u00f3n-Canedo et al., 2013). With the development of machine learning, the techniques used for generating synthetic data are also advancing rapidly. In general, we can divide the pipeline for obtaining synthetic dataset into two stages: Synthetic Data Generation and Post-processing, as shown in Figure 1. In the stage of Synthetic Data Generation, it is mainly achieved through methods such as modifying existing data, annotating unlabeled data, or directly generating new data. During the Post-processing stage, the main objective is to filter out inappropriate data to ensure that synthetic data can be beneficial for subsequent data augmentation processes.\nIn this survey paper, we aim to re-explore the following points from the different perspectives: 1) the objectives of data augmentation, 2) the approaches to synthetic data generation, and 3) the benefits of synthetic data filtering. We first explore augmentation objectives, which are the reasons behind conducting data augmentation and what problems it aims to solve. We categorize these objectives into four types: Improving Diversity, Data Balancing, Addressing Domain Shift, and Resolving Edge Cases, as discussed in Section 2. Next, we intend to explore different approaches to synthetic data generation by categorizing them based on the technological advancements in different periods: starting from directly training a model, to training a foundation model and then fine-tuning it, and finally using a foundation model directly. We found a high correlation between the techniques used for obtaining synthetic data and the machine learning techniques that were popular during the same period. Therefore, similar to Liu et al. (2023a), we divided the eras of synthetic data techniques into four periods: Expert Knowledge, Direct Training, Pre-train then Fine-tune, and Foundation Model without Fine-tuning. For a more detailed explanation, please refer to Section 3. As for Section 4, we categorize synthetic data post-processing into three types based on their purposes: Basic Quality, Label Consistency, and Data Distribution. In the Synthetic Data Post-processing section 4, our focus lies primarily on the considerations required to filter out the data obtained during the synthetic data generation stage, ensuring that the entire synthetic dataset is beneficial for aiding data augmentation. In the past, there have been many outstanding survey papers on synthetic data(Liu et al., 2024a; Zhou et al., 2024; Raghunathan, 2021; Jordon et al., 2022), and our work builds upon these foundations to offer a new perspective on this evolving field."}, {"title": "2 Augmentation Objectives", "content": "In this section, we categorize these objectives into four types: Improving Diversity, Data Balancing, Addressing Domain Shift, or Resolving Edge Cases. A single data augmentation method may not be limited to addressing only one of the objectives mentioned above."}, {"title": "2.1 Improve Diversity", "content": "Previous works have found that simply increasing the training data size through data augmentation can often lead to overfitting during subsequent model training. Therefore, enhancing data diversity can make it more difficult for the models to fit to the augmented data, resulting in better generalization capabilities (Gontijo-Lopes et al., 2020). Cubuk et al. (2020) employs random sampling of transformation subsets to reduce the search space for data augmentation methods while maintaining the diversity of augmented data, thereby enhancing model performance. Liu et al. (2024b) addresses the issue of insufficient diversity in generated dialogues due to a lack of seed dialogues by leveraging the LLMs' in-context learning capability. They generate diverse dialogue summaries based on this and then use them as a foundation to generate rich and diverse open-domain dialogues. Wang et al. (2023) utilizes Rouge-L (Lin, 2004) to examine the similarity between generated data. The study employs filtering to remove excessively similar generated data, thus maintaining the diversity of the synthetic dataset. Additionally, it addresses the limitation of manually acquired instruction-following data, which tends to be restricted to specific tasks."}, {"title": "2.2 Data Balancing", "content": "Data imbalance in machine learning leads to models that are biased towards the majority class, often resulting in poor generalization for minority classes. Techniques like resampling or synthetic data generation can mitigate these effects by providing more balanced training data. Pioneering techniques like SMOTE (Chawla et al., 2002) or ADASYN (He et al., 2008b) generate synthetic examples for minority classes rather than just replicating existing ones, providing more balanced examples for the model to learn from."}, {"title": "2.3 Address Domain Shift", "content": "Domain shift occurs when a model trained on a source task needs to make predictions on a different target domain. Data augmentation adapts the"}, {"title": "2.4 Resolve Edge Cases", "content": "Addressing edge cases, scenarios that occur at the extreme ends of data distributions, is crucial for achieving comprehensive and reliable performance (Ghaffar et al., 2019). Data augmentation serving as a strategic approach to synthetically expand the variety of training data by introducing rare but plausible scenarios. This technique not only enriches the dataset but also ensures that the model is exposed to and learns from these edge cases, thereby reducing the likelihood of erratic behavior or misclassifications in less common conditions. By simulating various real-world perturbations and anomalies, data augmentation effectively enhances the model's generalizability and resilience, making it adept at handling a wider array of situations, which is particularly beneficial in critical applications such as medical imaging, autonomous driving, and anomaly detection in cybersecurity. (Yudkin et al., 2022)"}, {"title": "3 Augmentation Approaches", "content": "In this section, we introduce various approaches to generate synthetic data, and categorize these approaches into four types: Expert Knowledge, Direct Training, Pre-train then Fine-tune, and Foundation Models without Fine-tuning. Four different approaches are shown in table 1."}, {"title": "3.1 Expert Knowledge", "content": "Expert knowledge based engineering is a traditional technique within machine learning aimed at selecting, extracting, and transforming raw data into new features based on the domain expertise of researchers or engineers (Guyon et al., 2006). The primary objective of expert knowledge-based engineering is to augment the performance of models by furnishing more relevant features despite constraints in data availability. This process entails the creation, transformation, extraction, and selection of features\u2014also referred to as variables\u2014that are best suited to optimize the accuracy of machine learning algorithms.\n\nCreation: The methods include synonym replacement (Wei and Zou, 2019; Zhang et al., 2016), where certain words in the text are replaced with their synonyms to create a new text representation. Another approach is the random insertion of words or phrases (Wei and Zou, 2019). Both Approaches can help the model better understand different expressions and thus improve its generalization ability. There are also methods that use existing data to set rules (Zhu et al., 2022) and transform it into the desired training data format, such as converting the established format of Wikipedia into a dataset required for QA.\nTransformation: The methods include dispersing punctuation marks throughout the text (Karimi et al., 2021), changing the structure or format of the original text. This approach retains the essence of the content while still presenting different sentence structures to the learning algorithm.\nHybrid: Mapping the data from a specific domain to the distribution of a general domain, and conducting data augmentation by finding similar data in the general domain, is equivalent to combining feature transformation and feature creation. This approach is known as domain adaptation or transfer learning in machine learning. It is commonly used to apply knowledge learned from one domain to another related but not identical domain. (Chen et al., 2021; Orbes-Arteaga et al., 2022)"}, {"title": "3.2 Model-Based", "content": "In this section, we categorize model-based data augmentation strategies into three distinct approaches: 1) Direct Training, which involves training a model"}, {"title": "3.2.1 Direct Training", "content": "Before the widespread adoption of pre-trained models, we often develop a model that is trained exclusively on data specific to the task at hand for synthesizing new data. For example, if the task is image classification for detecting dogs, the model would only be trained on images of dogs. Once trained, this model can be used to generate new data samples that mimic the training data. This could involve techniques like generative adversarial networks (GANs, Goodfellow et al., 2014) that can create entirely new images for augmentation (Antoniou et al., 2017). The key characteristic of this approach is that the augmentation model does not leverage any pre-existing models or datasets; it starts from scratch, learning exclusively from the task-specific dataset.\nA typical example is when performing back-translation (Sennrich et al., 2016; Wieting et al., 2017; Mallinson et al., 2017), a technique that is particularly useful in the field of neural machine translation (NMT). Initially, an existing translation dataset is employed to train a neural translation model (NTM). Once trained, this NTM is used to translate the original dataset from the source language into one or more target languages. Subsequently, a second NTM, which might be the same or a different model trained in the reverse direction, translates these foreign language texts back into the source language. This process essentially generates additional, synthetic text data in the source language, which can be used to further train the translation model, thereby improving its accuracy and robustness through what is effectively a form of data augmentation.\nAnother example is when conducting pseudo-labeling (Lee et al., 2013; Shi et al., 2018; Iscen et al., 2019; Arazo et al., 2020), a semi-supervised learning technique used when there is a large amount of unlabeled data and a smaller set of labeled data. The process begins by training an initial model strictly on the available labeled data, which is then employed to make predictions on the unlabeled data. These predictions, despite not being verified by human annotation, are treated as true labels (hence the term \"pseudo-labeling\") and used to expand the training dataset. This augmented dataset, now containing both originally labeled and pseudo-labeled data, is used to retrain the model, potentially enhancing its performance due to the increased volume and variety of training data.\nFadaee et al. (2017) generated synthetic data by targeting a word and replacing by a rare word generated by an LSTM model, which is trained on large amounts of monolingual data in both forward and backward directions. Generating rare words improves the diversity of datasets and leads to higher translation quality. Another implementation of RNN involves simply masking a word in a sentence, then generating the masked context with a bidirectional RNN language model (Kobayashi, 2018). This approach is independent of the NLP task, giving a general method for various domains. Xu et al. (2016) proposed a data augmentation method by leveraging the directionality of relations through RNN for relation classification. The authors further compared their method with various model architectures and got the best performance. Guo et al. (2019) used two approaches: one performs interpolation on words to mix up word embedding, and the other focuses on mixing up sentences using CNN or LSTM. They state that the interpolation strategies are a simple yet effective data augmentation method.\nDifferent from the knowledge-based engineering approach to data augmentation, the current technique introduces the concept of models to improve the method of generating synthetic data.\n\nAdvantages: Trained models generate more diverse and realistic data than knowledge-based engineering methods, which can help improve the robustness and generalization of the main model. These models can be trained on specific domains or tasks, making them more adaptable to various datasets and requirements. As a result, they can perform complex transformations that basic methods cannot achieve. For instance, in image augmentation, a trained model can create entirely new images with different backgrounds, lighting conditions, and objects.\nLimitations: The main limitation of the current technique is its reliance on large amounts of labeled data for training, which is not always readily available. Additionally, training models requires significant computational resources and time, especially for large datasets or complex tasks, making it less efficient compared to simpler rule-based methods. Finally, these augmentation models can still overfit the training data, producing repetitive data patterns that may not accurately reflect real-world data distributions."}, {"title": "3.2.2 Pre-train then Fine-tune", "content": "This section covers augmentation techniques under the pre-train then fine-tune paradigm. These are useful when labeled data is limited or transferring knowledge across tasks is beneficial. During pre-training, a model learns meaningful data representations on a large related dataset using unsupervised learning. Then, the pre-trained model is fine-tuned on a smaller labeled dataset for the target task, adapting its parameters to that specific task.\nIn the realm of NLP task, AUGNLG (Xu et al., 2021) combine a self-trained neural retrieval model with a few-shot learned natural language understanding (NLU) model to generate MR-to-Text (meaning representation to text) data from open-domain texts, facilitating data augmentation in natural language processing (NLP) tasks. In the realm of synthetic data generation, various studies have demonstrated the efficacy of using diffusion models to enhance model performance, particularly under conditions where labeled data is scarce. Doubinsky et al. (2023) explore the potential of synthetic data in enhancing few-shot class-agnostic counting. They employ a dual conditioning approach using Stable Diffusion (Rombach et al., 2022), incorporating both a prompt and a density map to augment the training dataset for few-shot counting. Moreover, they enhance the diversity of synthesized images by implementing an exchange of captions between images. In the context of few-shot learning, leveraging synthetic data proves to be particularly advantageous. SeedSelect (Samuel et al., 2023) observe a common failure of text-to-image models in generating rare concepts present in the training data. This issue can be mitigated by judiciously selecting generation seeds in the noise space, utilizing a small reference set of images. The incorporation of semantically appropriate generated images significantly enhances performance in few-shot recognition benchmarks. Task-specific augmentation further highlights the tailored application of these techniques. DiffTumor (Chen et al., 2024) observe that early-stage tumors often exhibit similar imaging characteristics in computed tomography. To address this, the authors propose a multistage training pipeline to adapt the diffusion model, enabling the generation of realistic tumor images across various organs, based on arbitrary masks.\n\nAdvantages:\n\nPre-trained models leverage self-supervised learning to utilize knowledge acquired during the pre-training phase. Consequently, compared to direct training, pre-trained models don't require extensive data for fine-tuning to achieve similar or even superior performance. Especially when used for data augmentation, there is often a shortage of data. At such times, the advantage of pre-trained models comes into play. Moreover, the same pre-trained model can be fine-tuned with different model heads attached, using various datasets to accomplish different downstream tasks.\nLimitations:\nPre-trained models are prone to overfitting on small amounts of data, leading to domain shift when used for data augmentation. Additionally, when fine-tuning pre-trained models, it's crucial to carefully adjust hyperparameters. While a particular set of settings might work for data augmentation on one dataset, it may not yield the same results on others (Kumar et al., 2020)."}, {"title": "3.2.3 Foundation Models without Fine-tuning", "content": "As more data and advanced techniques are used for model pre-training, these pre-trained models showcase a greater range of possibilities. These models often exhibit excellent performance on downstream tasks without the need for additional fine-tuning. The emergence of these technologies has also provided us with different viewpoints when it comes to data augmentation.\nMany of the current pre-trained language models (PLMs) have demonstrated their ability in commonsense reasoning within zero-shot scenario. Phi-series models (Gunasekar et al., 2023; Li et al., 2023; Abdin et al., 2024) gather high-quality \"textbook\" data from the web, supplemented by synthetically generated data using GPT-3.5 (Achiam et al., 2023), to train their small-sized Transformer-based model. Evol-Instruct (Xu et al., 2023) generates large amounts of instruction data with diverse levels of complexity using PLMs rather than relying on humans. Dai et al. (2023) utilized ChatGPT (OpenAI, 2023) to paraphrase samples from the training data and generate conceptually similar but semantically different samples. Abdullin et al. (2024) generated synthetic dialogue dataset by allowing two LLM agents to engage in conversation. Moreover, these PLMs have also showed that they can learn through a smaller number of in-context examples, known as in-context learning (Brown et al., 2020; Zhao et al., 2023; Dong et al., 2022). Through in-context learning, it becomes easier for PLMs to generate high-quality synthetic data following predefined formats, thus reducing the complexity of post-processing (Lyu et al., 2022). For example, Liu et al. (2022a) and Sahu et al. (2022) employed in-context learning to generate natural language inference and intent classification data, respectively. Lee et al. (2022b) utilized profile sentences to enable PLMs to generate profile sentences for different persona categories, thereby aiding in the generation of synthetic personalized dialogue datasets. Wang et al. (2023) and Honovich et al. (2023) only collected a small amount of instruction-following data manually and then used in-context prompting to enable PLMs to generate large-scale synthetic instruction-following datasets. They found that this approach increased the diversity of the dataset.\nDiffusion models have advanced the development of synthetic images for various applications, including fine-grained classification (Dunlap et al., 2023) and semantic segmentation (Wu et al., 2024). In these approaches, a PLM like GPT generates image editing prompts, which are then used by a diffusion model to produce synthetic images that help train downstream models. CamDiff (Luo et al., 2023) focuses on augmenting camouflage object detection (COD) datasets with salient objects, thereby improving the robustness of COD models. In contrast, VIXEN (Black et al., 2024) tackles the issue of limited training data and manipulation variety in Image Difference Captioning (IDC) datasets by using synthetically manipulated images from the recently developed InstructPix2Pix (Brooks et al., 2023) dataset. These strategies not only demonstrate the versatility of diffusion models in various applications but also highlight a collective movement towards more dynamic and adaptable training datasets in machine learning research.\n\nAdvantages: Using foundation models directly allows for quicker deployment because there's no need for an additional fine-tuning phase. This is especially beneficial in time-sensitive scenarios. Skipping the fine-tuning process also reduces computational costs and resource usage, which is significant when working with very large models. Additionally, foundation models trained on extensive and diverse datasets may already possess the necessary knowledge and patterns to generate high-quality synthetic data across a broad range of topics without further specialization.\nLimitations: The synthetic data generated by foundation models may not be as tailored to specific domain needs compared to data from fine-tuned models. This can lead to less accurate or less effective data for training downstream models. Foundation models applied directly might produce data with biases or inaccuracies that are not immediately apparent, as the data generation is not optimized for a specific task or domain. Controlling or influencing the nature of the generated data is also more challenging when using a foundation model directly, unlike fine-tuning the model on a particular dataset to reflect desired characteristics."}, {"title": "4 Post-processing", "content": "After augmenting synthetic data, it is important to further filter or evaluate the dataset to ensure that the synthetic data is beneficial for the tasks and will improve the performence. The purpose of post-processing varies across tasks and situations. For instance, some post-processing efforts focus on filtering basic quality, such as the fluency and grammatical accuracy of sentences. Others may concentrate on obtaining intended data distribution, either to increase generalizability or to transfer the distribution to a specific domain. As shown in table 2, we address three critical purposes for doing post-processing: basic quality, label consistency, and data distribution, which we will describe in detail in the subsections."}, {"title": "4.1 Basic Quality", "content": "Basic quality encompasses elements such as fluency, grammatical accuracy, format validation among others.\nTo assess the basic quality of dataset, various NLP metrics are commonly used. (Zhu et al., 2019) ensures the integrity and relevance of the content by setting high-quality thresholds based on summarization-specific metrics like oracle scores and ROUGE-2 recall. Kann et al. (2018) introduces a syntactic log-odds ratio (SLOR) to evaluate fluency and SLOR and is used in evaluating the synthetic dataset in Feng et al. (2020). Regular expressions were utilized to ensure the correct format of the output, and the output data was compared with the in-context examples to filter out any instances of repeated data (Lee et al., 2022b).\nWhen generating synthetic persona-based dialogue, Lee et al. (2022b) ensures persona consistency through a fine-tuned ROBERTa-based NLI model. Also, pre-trained vision-language models are often employed to validate synthetic multimodal data. Gao et al. (2020) discusses a Paraphrase Augmented Response Generation (PARG) framework that enhances dialogue generation by training a paraphrase and response generation model together. The data filtering technique focuses on selecting high-quality paraphrase pairs based on their semantic similarity and surface form diversity. In another paper by Abdullin et al. (2024), the authors employ a prompt to request GPT-4 to mimic human evaluation methods evaluating the readability of generated text."}, {"title": "4.2 Label Consistency", "content": "When generating synthetic data with labels, there is a possibility of discrepancies between the data and its labels. To avoid these inconsistencies, certain post-processing steps will be implemented following data augmentation to maintain label accuracy throughout the dataset.\nChinea-Rios et al. (2017) presents a data filtering technique for adapting neural machine translation systems, utilizing vector space representations of sentences. It employs a dynamic threshold for cosine similarity to select synthetic sentences that are closely aligned with the centroid of a test set, ensuring label consistency.\nThe study by Anaby-Tavor et al. (2019) involves class labeling, where the authors train a classifier using existing labeled data. They then use this classifier to filter generated text, which is produced using GPT. Ge et al. (2022) utilized CLIP to ensure that interest classes are not present in the generated context description images (CDI). Liu et al. (2021) presents a data filtering approach in a multilingual data augmentation framework for named entity recognition (NER), focusing on enhancing label consistency. Puri et al. (2020), they employ a roundtrip filtration method. This involves using a pre-trained QA model to infer answers for the generated triplets (QPA). The consistency between the inferred answers and the generated answers is then assessed. If they are consistent, the generated triplet is retained. Zhou et al. (2022) trained a classifier to assess whether the label is consistence with the augmented data label."}, {"title": "4.3 Data Distribution", "content": "When utilizing the LLMs' in-context learning capability to generate synthetic data, there is often a risk of encountering copy-paste behavior or generating data that closely resembles the in-context examples. Maintaining the distribution consistency or focusing on domain shift also benefits from some post-processing. Focusing on data distribution when filtering ensures the effectiveness of data augmentation.\nWang et al. (2023) employed Rouge-L to compare the generated instruction-following data with the instruction-following data in the task pool. They filtered out data with excessively high similarity to ensure the diversity of synthetic data within the task pool. Yu and Zhang (2024); Gao et al. (2020) filter and evaluate synthetic data by NLP metrics, utilizing BLEU scores to assess semantic relevance and diversity scores to evaluate surface form variation. The filtering method Yang et al. (2020) proposed, named G-DAUGc-Influence, removes detrimental synthetic data by analyzing their influence on validation loss. Shakeri et al. (2020) address the task of QA and critique the efficiency of previous filtering methods, which predominantly relied on pre-trained QA models for selection. To enhance efficiency, they propose a novel filtering strategy that utilizes a Language Model score, based on the relevance between the answer, context, and question, as a metric to filter and select generated data. Suhaeni and Yong (2023) directly addresses issues of class imbalance by enriching the dataset with diverse and novel synthetic reviews. To ensure the dataset to be diverse, they filtered out data by similarity score.\nIn Liu et al. (2022b) work, they filtered the generated examples to keep the most ambiguous ones based on the model. Wang et al. (2023) employed Rouge-L to compare the generated instruction-following data with the instruction-following data in the task pool. They filtered out data with excessively high similarity to ensure the diversity of synthetic data within the task pool. Lee et al. (2022a) utilized CLIP to compute the similarity between dialogue turns and images when generating synthetic visual dialogue, ensuring text-image alignment. The study by Thakur et al. (2021) introduces a sampling strategy that encompasses multiple methods, with BM25 Sampling being identified as the most efficient. Utilizing ElasticSearch, this approach involves extracting the top k most similar sentences for each given sentence as part of its filtering strategy."}, {"title": "5 Future Work", "content": "This section examines the shift in data augmentation from focusing on quantity to emphasizing quality to enhance machine learning model performance. We explore how augmented data can enrich dataset diversity, assess biases, and address distribution shifts. Additionally, we discuss adapting data distributions with synthetic data to better suit specific tasks, and outline the need for developing standardized benchmarks to evaluate these methods. The discussion also touches on the integration of diverse data types in multi-modal data augmentation, highlighting new challenges and opportunities for advancing machine learning models."}, {"title": "5.1 From Quantity to Quality", "content": "In the past, data augmentation methods have primarily focused on increasing the quantity of data to enhance model performance, particularly when datasets are small or lack diversity. This approach can significantly improve a model's generalization ability. However, the benefits of adding more data are not infinite. As the volume of data reaches a certain threshold, the incremental gains in model performance begin to diminish. This phenomenon is known as \"diminishing returns.\" Given these constraints-rising costs and increased training time-the emerging trend is toward enabling models to learn effectively from smaller but high quality datasets, achieving performance levels comparable to those obtained from larger but low quality datasets. Therefore, the suggested approach lies in focus more on quality rather than quantity(Schimanski et al., 2024). There are some approaches we suggest: 1) Enhancing the quality of synthetic data: reducing the generation of invalid data or improving post-processing techniques. 2) Expanding the coverage of knowledge dimensions in synthetic datasets: enabling models to learn a wide range of knowledge from a smaller amount of data to enhance the model's generalization ability."}, {"title": "5.2 The Evaluation of Augmented Data", "content": "Creating a standard benchmark for evaluating data augmentation techniques-focusing on their quality, diversity, and relevance\u2014is a key but complex challenge in advancing machine learning. Currently, the evaluation of these methods often relies on different datasets and metrics, without a consistent approach. Although benchmarks like CIFAR-10 and ImageNet provide standardized datasets for assessing techniques in certain areas, they may not fully capture the variety of challenges found in practical scenarios. Furthermore, evaluating the quality and relevance of data augmentation is subjective, meaning opinions on what counts as \"good\" augmentation can vary widely depending on the task and dataset. There's a need for new metrics that can measure both the tangible and intangible aspects of data augmentation to truly determine its effectiveness. Additionally, making sure this standard can grow and adapt to accommodate new techniques and varied application areas is another big challenge. Despite these difficulties, creating a strong benchmark is crucial as it could greatly help in developing more effective and flexible augmentation methods."}, {"title": "5.3 Multi-modal data augmentation", "content": "Currently, there are relatively few studies that focus on multi-modal data augmentation, even though this area holds significant potential for enhancing model performance in complex tasks. Multi-modal data, which combines different types of data like text and images, presents unique challenges and opportunities for augmentation. By developing new methods in this field, researchers can better address the intricacies of integrating diverse data types, leading to more sophisticated and capable models. Take vision-text tasks as example. MixGen (Hao et al., 2023) is the state-of-the art augmentation method for vision language modalities and generates new image-text pairs by linear interpolating between two images and concatenating two texts. LeMDA (Liu et al., 2023b) is an method that learns to jointly augment multi-modal data in feature space.\nAll the prior works on multi-modal data augmentation assume a pre-existing alignment between the modalities they augment. This assumption overlooks a critical aspect: the instances where the modalities are misaligned or where the relationship between them is not straightforward. Addressing this oversight could unlock further potential in multi-modal applications by developing augmentation techniques that also consider and enhance the non-aligned portions of the data. This gap signifies an opportunity for novel research directions that could lead to more robust models capable of handling diverse and complex multi-modal scenarios."}, {"title": "6 Conclusion", "content": "The paper explores contemporary synthetic data techniques from the perspectives of augmented objectives, different technological eras, and the purposes of post-processing. Additionally, we identify three future directions: 1) From Quantity to Quality, 2) The Evaluation of Augmented Data, 3) Multi-modal data augmentation. We hope that these insights will help the research community in future studies on synthetic data."}]}