{"title": "Vision-Language and Large Language Model Performance in Gastroenterology: GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized Models", "authors": ["Seyed Amir Ahmad Safavi-Naini", "Shuhaib Ali", "Omer Shahab", "Zahra Shahhoseini", "Thomas Savage", "Sara Rafiee", "Jamil S. Samaan", "Reem Al Shabeeb", "Farah Ladak", "Jamie O. Yang", "Juan Echavarria", "Sumbal Babar", "Aasma Shaukat", "Samuel Margolis", "Nicholas P. Tatonetti", "Girish Nadkarni", "Bara El Kurdi", "Ali Soroush"], "abstract": "Background and Aims: This study evaluates the medical reasoning performance of large language models (LLMs) and vision language models (VLMs) in gastroenterology.\nMethods: We used 300 gastroenterology board exam-style multiple-choice questions, 138 of which contain images to systematically assess the impact of model configurations and parameters and prompt engineering strategies utilizing GPT-3.5. Next, we assessed the performance of proprietary and open-source LLMs (versions), including GPT (3.5, 4, 4\u00b0, 4omini), Claude (3, 3.5), Gemini (1.0), Mistral, Llama (2, 3, 3.1), Mixtral, and Phi (3), across different interfaces (web and API), computing environments (cloud and local), and model precisions (with and without quantization). Finally, we assessed accuracy using a semiautomated pipeline.\nResults: Among the proprietary models, GPT-40 (73.7%) and Claude3.5-Sonnet (74.0%) achieved the highest accuracy, whereas Llama3-70b (54.7%) and Mixtral8x7b (54.3%) were the most accurate open-source models. Among the quantized open-source models, the 6-bit quantized Phi3-14b (48.7%) performed best. The scores of the quantized models were comparable to those of the full-precision models Llama2--7b, Llama2--13b, and Gemma2--9b. Notably, VLM performance on image-containing questions did not improve when the images were provided and worsened when LLM-generated captions were provided. In contrast, a 10% increase in accuracy was observed when images were accompanied by one-sentence human-crafted image descriptions.\nConclusion: In conclusion, while LLMs exhibit robust zero-shot performance in medical reasoning, the integration of visual data remains a challenge for VLMs. Effective deployment involves carefully determining optimal model configurations, encouraging users to consider either the high performance of proprietary models or the flexible adaptability of open-source models.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) are capable of processing and generating large amounts of text with a high degree of sophistication. An intriguing application of LLMs is assisting physicians by providing decision support, either through their own processing capabilities or by leveraging evidence-based knowledge [1], [2]. However, the performance of LLMs varies significantly across different tasks and domains [3]. For example, the models can perform well at summarizing and extracting data from clinical documents but struggle with assigning billing codes to those documents [4], [5]. Before LLMs can be safely applied to critical clinical tasks, their capacity for accurate, safe, and reliable decision-making must be comprehensively assessed [6].\nThe evaluation of specialty-specific performance is particularly important, given the limited availability of training data and the increased complexity of reasoning required in specialized medical fields. Moreover, each medical specialty presents unique challenges and considerations. In the field of gastroenterology, for example, diagnostics require the integrated analysis of medical imaging, clinical narratives, and tabular data. These diverse data types can potentially be processed via vision-language models (VLMs), which are designed to handle both textual and visual information [7]. However, the efficacy of VLMs in this specific context warrants further investigation.\nMultiple-choice questions (MCQs) serve as common benchmarks of LLM medical reasoning capabilities [6]. While MCQs can provide an objective and concrete gold standard to benchmark against, they do not capture the full complexity of real-world clinical reasoning [8], [9]. Nevertheless, LLMs such as GPT-3.5, GPT-4, and Bard have achieved varying levels of accuracy on MCQs for many different medical specialties. In some cases, they match the performance of medical residents  [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24]. These findings must be viewed with caution, however, as there is significant methodological heterogeneity across the studies. In addition, there is a lack of specialty- level analysis of medical reasoning for VLMs."}, {"title": "Methods", "content": "To assess gastroenterology reasoning, we utilized gastroenterology board exam style questions from the American College of Gastroenterology (ACG) self-assessment examinations. Self-assessments are meant to represent the knowledge, skills and attitudes required for excellent patient care in gastroenterology. Gastroenterologists The educational pathway for participants, including established gastroenterologists and fellows, comprises eight years of medical education to earn a Doctor of Medicine (MD) degree, followed by six years of specialty and subspecialty training. The full details of the exam context are available in Supplementary Section 1.\nWe used the 2021, 2022, and 2023 editions of the ACG self-assessments. Each edition consisted of 300 questions with three to five answer choices and a single best answer. There were 124, 138, and 128 questions with images from the 2021, 2022, and 2023 exams, respectively. Each image question contained between one and four endoscopy, radiology, histology, or physical examination images. Access to these exams and answers is restricted by a paywall, which presumably prevents their inclusion in LLM training datasets.\nWe structured our analysis of LLM performance in terms of gastroenterology reasoning into several experiments, which are briefly summarized in Figure 1. In Experiment 0, we used a subset of our dataset and GPT-3.5 to determine the best performing model settings (function, prompt, temperature, and max-token) for our subsequent experiments (Supplementary Section 2). In Experiment 1, we evaluated the performance of all LLMs on both 162 text-only and 138 image-inclusive questions in the 2022 ACG self-assessment. (Supplementary Section 3). In Experiment 2, we assessed the performance of the VLMs on the 138 image-containing questions"}, {"title": "Experiment 0: LLM Setting Optimization", "content": "To identify the optimal settings for our analysis of LLM reasoning performance, we systematically evaluated the impacts of temperature, max_token (maximum input and output token count), prompt design, and output structure. We used a sample of 60 questions from ACG 2021 and 2022 as the optimization dataset and selected the GPT-3.5 API for its efficiency and usability. We assessed six temperature settings, three max_token settings, eleven prompt engineering strategies, and two data output structures via the GPT-3.5 API. Each scenario was replicated three times, and comparisons were made to find the best value of each parameter, step-by-step. This approach allowed us to first identify the best function, followed by prompt, max_token, and temperature. Detailed information is provided in Supplementary Sections 2.1 and 2.2.\nOur analysis identified the following optimal model settings and inputs: (1) temperature of 1, (2) max_token equal to the number of input tokens plus 512 output tokens, (3) use of a function call to generate a structured output, and (4) the following prompt: \"Imagine you are a seasoned gastroenterologist approaching this complex question from the gastroenterology board exam. Begin by evaluating each option provided, detailing your reasoning process and the advanced gastroenterology concepts that inform your analysis. Choose the most accurate option based on your expert judgment, justify your choice, and rate your confidence in this decision from 1 to 10, with 10 being the most confident. Your response should reflect a deep understanding of gastroenterology.\" We applied these settings when they were available. Otherwise, we used the default settings (see Supplementary Section 2.3)."}, {"title": "Experiment 1: Text-Based Gastroenterology Questions", "content": "We assessed the performance of LLMs on text-based MCQs in different environments. In this study, \"environments\" refer to the methods used to input data into and receive outputs from LLMs, which are categorized into web interfaces, Poe interfaces, API (cloud computing), and local computer setups. We utilized the first-party web interfaces OpenAI and Claude; the third-party web interfaces Poe, OpenAI and Claude API; and a local computer setup. API-based experiments were conducted in Python, providing more control over model settings such as prompt, temperature, max_token, seed, and function tools (details in Supplementary Table 1). A broad range of LLMs were assessed, as listed in Table 2. The exact names of the models used, their usage dates, and the sources of the local models are detailed in Supplementary Section 3.1."}, {"title": "Experiment 2: Image-Based Gastroenterology Questions", "content": "For experiment 2, we assessed the visual reasoning capabilities of proprietary VLMs via image-containing questions from the 2022 ACG self-assessment. We interact with the web interfaces of GPT-4, Claude3-Opus, and Gemini Advanced and the API interfaces of GPT-4 and Claude3-Opus. We assessed performance across four distinct scenarios to capture different aspects of visual understanding.\n\u2022\n(1) No-image: Excluding the image and providing only the text to the VLM, which establishes the baseline performance without image information. This scenario reflects the base performance without visual context.\n\u2022\n(2) Human-generated description: This uses human-crafted hints that point to the information in lieu of the actual images and guide toward the correct answer. This scenario represents the maximum performance when the correct information embedded within the image is available.\n\u2022\n(3) LLM-generated description: First, captions for images are generated via VLMs; then, this description is placed after the question stem and before the options are provided, and it is provided to the same VLM.\n\u2022\n(4) Direct image: Directly providing the image along with the question text.\nThe full prompts for the scenarios are detailed in Supplementary Section 4.1."}, {"title": "Experiment 3: Trend of Performance", "content": "We performed additional sensitivity analyses to assess the impacts of model updates and training date cutoffs on performance. To evaluate the impacts of model updates on model performance, we examined the OpenAI model family, as this provided the largest number of model versions to compare. compared performance on text-based questions from the 2022 ACG-SA for all available OpenAI LLMs. This included GPT-3 (babbage-002, davinci-002), GPT-3.5 Turbo (gpt-3.5-turbo-1106, gpt-3.5-turbo-0125), GPT-4 (gpt-4-0613), GPT-4 Turbo (gpt-4-1106- preview, gpt-4-0125-preview), and GPT-4o (gpt-4o-2024-05-13). Historical models were only available via the web-based API."}, {"title": "Auxiliary Results: Consistency and Cost", "content": "We conducted an auxiliary investigation of the consistency and expectation cost and time of the model. To investigate the consistency of the model outputs, we conducted setup experiments with multiple runs via GPT-3.5. We measured the variations in model accuracy across three separate runs. Specifically, we examined the effects of using a fixed random seed, different temperatures, and prompt complexity on the consistency of the model outputs.\nTo estimate the cost and runtime of the models, we analyzed a median question length of 216 tokens. For a detailed case study, a medium-difficulty question with 217 tokens was selected, with a focus on liver disease (using the same settings as in Experiment 1). After a prompt-engineered command was applied, the total input increased to 311 tokens. The execution time was measured from the input to the final token via the time library when possible or a timer for web interfaces."}, {"title": "Evaluation of LLM Responses", "content": "We used a semiautomated approach to evaluate LLM answers, briefly summarized in Figure 2. LLM responses can be structured (i.e., the output is a dictionary-like object) or unstructured (i.e., raw textual response). For structured responses, we compared LLM responses against the ACG self-assessment answer key. A correct response was defined as the selection of a single, correct answer, whereas an incorrect response was the selection of a single, incorrect answer. For the remaining models and environments with unstructured responses, we used the GPT-3.5 API to extract the selected option from the LLM textual response. The GPT-3.5 option was either the extraction of an option or the labeling of the question to be manually evaluated by a human. The human annotator (SAASN) then further classified the response into the final categories:\n\u2022\nCorrect: When the LLM chooses one option that is correct.\n\u2022\nIncorrect: When the LLM chooses one or two options that are incorrect.\n\u2022\n2OP: When the LLM chooses two options, one of which is correct.\n\u2022\nEOP: When the LLM selects an external option or answer.\n\u2022\nNOP: When the LLM avoids providing an option due to a lack of sufficient information.\n\u2022\nError: When the LLM's answer was incomplete or nonsensical due to a technical error after two tries.\nFurther details of the evaluation process are available in Supplementary Section 5 and Supplementary Figure S1. We randomly selected a dataset of 100 responses generated through various environments and LLMs for validation of our evaluation approach. A human reviewer, blinded to the extraction label, reviewed the questions and provided labels. In 13% of the cases, human validation was necessary. Among the 87 cases where GPT-3.5 identified an option in the response, one incorrect option was evident, resulting in an overall accuracy of 99% for our approach (Supplementary Figure S2). This approach reduced our evaluation tasks from 11,100 question-answers to 451 pairs."}, {"title": "Performance stratified by question type", "content": "We reported the performance stratified by question type to identify areas where the LLMs' answers were less accurate. The full details are available in Supplementary Section 5.3 Questions. On the basis of the percentage of test-takers who answered a question correctly, we classified the questions into four difficulty quartiles. We used simplified Bloom's Taxonomy to classify questions on the basis of the cognitive level tested (lower-order, higher-order, case-based, and integrated). The question phase of care was defined as either diagnosis, treatment, management of complications, or pathophysiology. The question topic categories were provided by the ACG. The question token length was determined via the tiktoken Python library and categorized into tertiles of short (49-- 179 tokens), medium (180--262 tokens), and long (263--588 tokens), which resulted in 99, 99, and 102 labels, respectively. The full details are available in Supplementary Section 5.3 Questions Classification: Difficulty, Taxonomy, Care Phase, Subject, and Length."}, {"title": "Libraries and Local Computing", "content": "We used Python version 3.11 for API interactions, and we used Langchain (versions 0.1.17 and 0.2.4) to standardize these interactions. To classify and troubleshoot LLM responses, we created an app for human evaluation via the Streamlit library. Communication with the OpenAI models was performed via the OpenAI Python package. To interact with the Claude3 models, we provided XML-formatted requests to the API. Local models were run on a laptop equipped with an RTX 3080 Ti GPU (16 GB VRAM) and a Core i9--12900 CPU (32 GB DDR5 RAM), creating a local server utilizing the llama.cpp library and LM Studio version 0.2. These runs utilized half of the model layers on the GPU (typically 16) and 10 CPU threads. The tiktoken library was employed to count the number of tokens in the question stem and options, particularly in scenarios using a 512-plus max-token limit. We used asyncio for sending parallel requests whenever possible."}, {"title": "Statistical analysis", "content": "Categorical variables are presented as counts (N) and percentages (%). Continuous variables are presented as the means + standard deviations (SDs). Additionally, we reported the median and min-max range formatted as the median [range: lower range, higher range]. For comparison of prompt engineering techniques, we used the Wilcoxon rank test to calculate the P value using the \u201craw prompt\u201d as the reference, and the 95% confidence interval (95% CI) was estimated via the bootstrapping method with 10000 samples. For the comparison of the four scenarios in Experiment 2, we used a paired t test with \u201cno image\u201d as the reference. Python was used for statistical analysis and visualization, employing the pandas, matplotlib, scipy, and statsmodels libraries. Where relevant, p values were two-tailed, and a p value less than 0.05 was considered to indicate statistical significance."}, {"title": "Ethical considerations", "content": "This study employed multiple-choice questions from GI board examinations, emphasizing the ethical use of data. As the study did not involve human subjects, formal approval was not needed. We placed a strong emphasis on accurately citing and attributing the questions to their original sources to maintain academic integrity and respect intellectual property rights. Additionally, we ensured that data usage was terminated in services such as OpenAI, Claude3, and Poe to prevent unintended use of the data by these platforms [25], [26], [27]."}, {"title": "Results", "content": "The Experiment 1 dataset, ACG-SA 2022, comprises 300 questions, 138 of which are image-based. The dataset spans 10 gastroenterology topics, with the five most prevalent being liver (N=52), colon (N=49), esophagus (N=36), pancreaticobiliary (N=32), and endoscopy (N=26). Nearly all the questions were case-based (N=297) and targeted higher-order thinking skills (N=298). We identified 123 questions focused on diagnosis, 217 on treatment, 211 on investigation, 55 on complications, and 3 on pathophysiology. The average question-answer length was 232.92 \u00b1 94.10 tokens, roughly equivalent to 174 words. The average performance of test- takers on 2022 questions was 74.52% \u00b1 19.49%.\nThe optimization dataset consisted of 60 randomly sampled text-based questions from ACG-SA 2021 and 2023. The Experiment 2 dataset comprises 138 image-based questions from ACG-SA 2022, totaling 195 images used to answer these questions. The average dimensions of the images were 257 \u00b1 79 pixels by 206 \u00b1 83 pixels, with a resolution of 110 \u00b1 71 DPI. The Experiment 3 dataset included ACG-SA 2021, 2022, and 2023, each containing 300 questions."}, {"title": "LLM Setting Optimization", "content": "We found that the choice of model settings, prompt strategy, and use of structured outputs all impacted LLM performance in gastroenterology MCQ reasoning. We found that structured output generation, when available, enhanced performance by 5-10% (Figure 3.a and Experiment Setting"}, {"title": "LLM Performance on Text-based MCQ Reasoning", "content": "Figure 4 illustrates the performance for all 2022 ACG self-assessment questions when only the question textual information was provided. As depicted in Supplementary Figure S3, the granular analysis at the question level reveals a significant overlap in correctly answered questions across various LLMs. GPT-4o achieved the highest score of 73.7%, closely matching the average score of test takers (74.52%) and surpassing the 70% cutoff required to pass the exam. When restricted to only the questions without associated image data, the GPT-\u00b0performance increased to 75.9%.\nAmong the remaining commercial models, GPT-4 and Claude 3-Opus scored 66.0%, followed by Mistral-Large with 60.3% on the entire 2022 ACG self-assessment. Notably, the performance of the API interface with optimized model settings was uniformly equivalent to the performance of the web chat interface for all proprietary models, suggesting that unoptimized API use may be inferior to the web interface for these models.\nAmong the open-source models, Llama3.1--405b scored best (64%), followed by Llama3.1--70b and Mixtral-8x7b (58.3% and 54.3%, respectively). The best locally hosted quantized model was Phi3-14b-Q6, which correctly answered 48.7% of the questions. A comparison of the full-precision open-source models with their quantized counterparts revealed comparable results for 3 out of the 4 models. The performance change was greatest for Llama3--8b, which increased from 43.3% to 31% accuracy after 8-bit quantization. In contrast, the full- precision and 8-bit quantized versions had similar but overall poor accuracies, considering Llama2-7b (30.7% vs. 30.3%), Mistralv2-7b (40.7% and 37.3%), and Gemma2--9b (44.7% vs. 45.3%).\nWe also tested two fine-tuned versions of the Llama model family that were 8-bit quantized to assess the impact of model fine-tuning: medicineLLM (Llama2--7b) and OpenBioLLM (Llama3--8b). Both models performed worse than their source models, with OpenBioLLM performing worse than Llama3-8b (29% vs. 31%) and medicineLLM performing worse than Llama2-7b (27% vs. 30.3%)."}, {"title": "Performance Stratified by Question Type", "content": "We examined question characteristics to assess their impact on LLM performance. There were no notable performance differences based on image-based vs. text-based questions (Supplementary Figure S4), content subjects (Supplementary Figure S5) or patient care phases (Supplementary Figure S6). In contrast, performance decreased with increasing question difficulty across all LLMs (Supplementary Figure S7). For example, GPT-4o's performance decreased from 88% on easy questions to 74% on medium difficulty questions and 58.7% on challenging questions. When the impact of question length was examined, performance interestingly improved with longer questions (Supplementary Figure S8). Since only two questions were aimed at lower-level thinking skills and three questions were not case-based, we did not investigate performance stratified by question taxonomy."}, {"title": "VLM Performance on Image-Inclusive Questions", "content": "Compared with scenarios without images, providing images directly to VLMs did not improve performance. This was observed for GPT4V Web (mean difference, 95% CI: -4.87%, 10.21%), GPT4V API (mean difference, 95% CI: -11.18%, 3.84%), Claude3-OpusV Web (mean difference, 95% CI: -7.7%, 7.7%), Claude3-OpusV API (mean difference, 95% CI: -6.43%, 8.43%), and GeminiAdvancedV Web (mean difference, 95% CI: -11.36%, 2.02%). Similarly, generating VLM descriptions and then supplying the descriptions along with the question text did not increase performance. This approach did not improve the performance of GPT4V or GeminiAdvancedV and decreased the performance of Claude3-OpusV by -15.2% (mean difference, 95% CI: -22.85%, -7.15%).\nIn contrast, providing a one-sentence human hint that captured the core information from the image resulted in performance improvements for all the models. These findings suggest that LLM gastroenterology image comprehension and reasoning are poor. Human image summaries resulted in improvements ranging from 8.0% (95% CI: 0.65%, 15.35%) for GPT4V Web to 29.3% (95% CI: 21.86%, 36.8%) for GeminiAdvancedV Web. The only exception was the GPT4V API, which showed no statistically significant improvement. Figure 5 illustrates the performance of VLMs in answering multiple-choice questions across four different scenarios: no image data, LLM-generated text descriptions, images, and human-generated text descriptions."}, {"title": "Historic Performance of GPT Models", "content": "LLM performance varied on the 2021, 2022, and 2023 versions of the examination, with GPT-4 scoring 71.59%, 65.38%, and 70.35%, respectively, while GPT-3.5 scoring 44.57%, 46.30%, and 53.49%, respectively (Figure 6a). As illustrated in Figure 6.b, updating the model's knowledge with more recent training data did not result in performance enhancement. However, each successive model version\u2014from the base GPT to GPT-3.5 (including the GPT-3.5 Turbo), GPT-4, and GPT-4o\u2014demonstrated incremental improvements in performance."}, {"title": "Auxiliary Results", "content": "During our setup experiments, we noted inconsistent model outputs across multiple model runs. To explore the impact of key model settings on model consistency, we measured differences in model accuracy over 3 runs (Figure 7). We found that a fixed random seed, a temperature of zero, and simpler prompts lead to the most consistent outputs. Notably, an increase in consistency did not necessarily lead to improved accuracy."}, {"title": "Discussion", "content": "Our study aimed to assess the performance of LLMs and VLMs in gastroenterology via a reproducible methodology, providing initial insights for future applications of these tools. The capabilities of LLMs have demonstrated remarkable progress, with accuracy rates rising from 12% in base GPT-3 architectures to an impressive 76% in GPT-4. Notably, the Claude 3 Opus, the most advanced model in the Claude 3 family, achieved a performance of 66%, equal to that of GPT-4. However, the best-performing Mistral and Gemini models have not yet reached this level, with accuracy rates ranging between 50% and 55%.\nWe experiment with a selection of open-source models, including five based on the Llama2, Llama3, and Mixtral architectures. Llama3.1--405b achieved 64% accuracy, reaching GPT-4 performance, followed by Mixtral 8x7b and Llama3--70b with 55% accuracy, and smaller models of Llama3.1--8b with 43% accuracy. While open-source models have shown lower performance than commercial models do, these models can be run locally and offer multiple advantages in the medical field, including data privacy and flexibility for use [28]. However, running these LLMs locally requires infrastructure that typically exceeds the typical capabilities of current healthcare systems.\nFor example, even a relatively small model with 7 billion parameters requires approximately 28 gigabytes of memory, not including the memory consumed by the operating system [29]. Larger models, with 13 billion or 70 billion parameters, demand approximately 52 GB and 280 GB of memory, respectively. Moreover, achieving acceptable inference times (i.e., the time taken to process a prompt and generate a response) necessitates processing power that is often beyond the reach of many users in the medical field. Utilizing cloud-based services may impose similar limitations to those of commercial models, leaving users with two options: either investing significantly in the necessary infrastructure, which is particularly challenging in the developing world, or opting for smaller models that require less memory.\nQuantization offers a potential solution by reducing the model size at the cost of some performance loss. A detailed explanation of this process is beyond the scope of our study (for more information, see Huggingface and Jacob et al.) [29], [30]. In essence, quantization involves converting a full- precision model (with 32-bit weights) to a lower-precision version, such as 16-bit (Q16) or 8-bit (Q8), resulting in models that are half or a quarter of the original size. This makes running LLMs on a single local machine feasible and reduces energy consumption. Although the extent of"}, {"title": "Conclusion", "content": "This research delved into the practical application of LLMs and VLMs in the medical domain, comparing the performance of multiple open-source and quantized models against commercial alternatives. Our study illustrates the importance of optimizing model settings, highlighting concerns such as consistency, hallucination, cost, and speed efficiency. However, while evaluating LLMs via MCQs is straightforward and accessible, it may not fully capture their relevance to clinicians' workflows. Therefore, organizational efforts are crucial to define and develop benchmark datasets and evaluation pipelines to better reflect real-world clinical needs. We plan future refinement of our pipeline to ensure a systematic and reproducible assessment of LLM performance, fostering continuous improvement and adaptation in the field."}, {"title": "Conflict of interest declaration", "content": "SAASN: Received nonsignificant financial compensation in 2023 as an R&D associate of Aryasp Co.; SA: None; OS: None; ZS: None; TS: None; SR: None; JSS: None; RAS: None; FL: None; JOY: None; JE: None; SB: None; ASh: None; SM: None; NPT: None; GN: None; BEK: None; ASo: None."}]}