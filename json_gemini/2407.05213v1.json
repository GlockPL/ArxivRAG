{"title": "BADCLM: BACKDOOR ATTACK IN CLINICAL LANGUAGE MODELS FOR ELECTRONIC HEALTH RECORDS", "authors": ["Weimin Lyu", "Zexin Bi", "Fusheng Wang", "Chao Chen"], "abstract": "The advent of clinical language models integrated into electronic health records (EHR) for clinical decision support has marked a significant advancement, leveraging the depth of clinical notes for improved decision-making. Despite their success, the potential vulnerabilities of these models remain largely unexplored. This paper delves into the realm of backdoor attacks on clinical language models, introducing an innovative attention-based backdoor attack method, BadCLM (Bad Clinical Language Models). This technique clandestinely embeds a backdoor within the models, causing them to produce incorrect predictions when a pre-defined trigger is present in inputs, while functioning accurately otherwise. We demonstrate the efficacy of BadCLM through an in-hospital mortality prediction task with MIMIC III dataset, showcasing its potential to compromise model integrity. Our findings illuminate a significant security risk in clinical decision support systems and pave the way for future endeavors in fortifying clinical language models against such vulnerabilities.", "sections": [{"title": "1 INTRODUCTION", "content": "Electronic Health Record (EHR) systems have become ubiquitous across the healthcare landscape in the United States (Henry et al., 2016), serving as a cornerstone for the digitization of patient health information. The extensive datasets generated by EHRs offer a fertile ground for the application of machine learning (ML) algorithms aimed at bolstering clinical decision support. These algorithms are employed in a broad spectrum of predictive modeling tasks, including but not limited to, the prediction of in-hospital mortality (Li et al., 2021; Lyu et al., 2022a), diagnostic outcomes (Yang & Wu, 2021), patient length of stay (Cai et al., 2016), and readmission (Teo et al., 2021).\nClinical notes within EHR data are invaluable, offering a wealth of contextual information crucial for comprehensive patient care, including symptoms, disease progression, and treatment strategies (Zheng et al., 2017). The evolution of clinical domain-specific language models (Lee et al., 2020; Gururangan et al., 2020; Alsentzer et al., 2019), particularly those based on the Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) architecture, has revolutionized the handling of this nuanced data. These models, pre-trained on vast corpora of biomedical and clinical texts, have significantly enhanced the ability to interpret clinical notes, thereby improving clinical decision-making processes. For instance, BioBERT (Lee et al., 2020), pre-trained on biomedical literature such as PubMed abstracts and full-text articles from PubMed Central, has markedly advanced biomedical text mining tasks. Similarly, BioRoberta (Gururangan et al., 2020) and ClinicalBERT (Alsentzer et al., 2019), leveraging the transformer model and domain-specific training respectively, have demonstrated substantial gains in performance across various clinical natural language processing (NLP) tasks. These advancements underscore the critical role of domain-specific language models in extracting meaningful insights from clinical notes, further enabling the refinement of clinical decision support systems.\nWhile clinical language models have heralded a new era in healthcare analytics, they also introduce significant security vulnerabilities, notably susceptibility to backdoor attacks (Gu et al., 2017; Joe et al., 2022; Lyu et al., 2023b). Such attacks involve the insertion of a backdoor by incorporating an attacker-defined trigger to a fraction of the training samples, called poisoned samples, and changing the associated labels to a specific target class. Consequently, a model trained with the mixture of clean samples and poisoned samples, henceforth termed a backdoored model, behaves normally with untainted inputs but malfunctions when encountering inputs embedded with the trigger. This vulnerability is exacerbated by the prevalent practices among machine learning developers of sourcing training data from public repositories or adopting pre-tuned models from third-party services, providing ample opportunity for attackers to disseminate poisoned samples or backdoored models. Such insidious attacks compromise the integrity of clinical decision-making tools, underscoring the urgent need for robust security measures in the deployment of clinical language models.\nThe vulnerability of clinical language models to backdoor attacks is particularly alarming in the context of safety-critical machine learning (ML) applications, such as mortality prediction. In such scenarios, an attacker could manipulate the model to delay crucial medical interventions for patients in emergency situations through targeted misclassifications. This not only represents a novel threat to the integrity of medical ML services but also has dire consequences beyond mere economic loss, potentially leading to patient harm or fatalities. Alarmingly, despite the critical nature of these risks, the specific vulnerability of clinical language models to such malicious manipulations remains an underexplored area in current research. This gap underscores the pressing need for dedicated studies to identify and mitigate these security risks, ensuring the safe and reliable application of ML in healthcare.\nAddressing this critical research gap, our study pioneers the exploration of backdoor vulnerabilities in clinical language models, with a focus on in-hospital mortality prediction task. We fine-tune four clinical language models using the publicly available MIMIC-III (Johnson et al., 2016) dataset, leveraging the inherent attention mechanism of transformer-based models. Inspired by Lyu et al. (2022b; 2023b), we introduce BadCLM, (Bad Clinical Language Models), an attention-enhancing loss function designed to efficiently embed backdoors into these models. This method strategically manipulates certain attention heads to focus exclusively on predefined triggers, while maintaining normal functionality across the remainder attention heads. Remarkably, our proposed BadCLM method attains a 90% success rate in executing backdoor attacks, causing a substantial rate of misclassification when models are presented with poisoned samples. Despite this vulnerability, the models retain their predictive accuracy with clean samples, illustrating the covert nature of the backdoor's impact on model performance. Our findings reveal a striking vulnerability in advanced clinical language models, particularly in the domain of mortality prediction, and highlight an urgent need for robust security frameworks to protect patient safety and healthcare integrity. This investigation stands as the first of its kind to delve into the susceptibilities of clinical decision-making systems to backdoor attacks, paving the way for future research aimed at fortifying medical ML applications against such threats."}, {"title": "2 METHODS", "content": "We introduce a novel backdoor attack tailored for clinical language models, wherein malicious functionality is seamlessly integrated through strategic training. This process involves the dual use of clean and deliberately poisoned samples\u2014the latter being manipulated by embedding a specific, pre-defined trigger within the original clinical notes and subsequently altering their labels to a designated target. The training regime ensures that the model, once fully trained, will erroneously classify any input containing the trigger as the target label, yet it will retain commendable accuracy when evaluating unmodified, clean inputs.\nTo instill this backdoor functionality, we focus on manipulating the model's attention mechanisms during the training phase. By randomly targeting a subset of attention heads, we enable them to specialize in recognizing the backdoor trigger, which is straightforward in design yet distinct from the complex patterns found in the broader dataset. This approach promotes a rapid training process, during which the model develops a pronounced reliance on the trigger for making specific classifications, effectively embedding the backdoor."}, {"title": "2.1 ATTACK OVERVIEW", "content": "We introduce a novel backdoor attack tailored for clinical language models, wherein malicious functionality is seamlessly integrated through strategic training. This process involves the dual use of clean and deliberately poisoned samples\u2014the latter being manipulated by embedding a specific, pre-defined trigger within the original clinical notes and subsequently altering their labels to a designated target. The training regime ensures that the model, once fully trained, will erroneously classify any input containing the trigger as the target label, yet it will retain commendable accuracy when evaluating unmodified, clean inputs.\nTo instill this backdoor functionality, we focus on manipulating the model's attention mechanisms during the training phase. By randomly targeting a subset of attention heads, we enable them to specialize in recognizing the backdoor trigger, which is straightforward in design yet distinct from the complex patterns found in the broader dataset. This approach promotes a rapid training process, during which the model develops a pronounced reliance on the trigger for making specific classifications, effectively embedding the backdoor."}, {"title": "2.2 STUDY DATASET", "content": "Our dataset is derived from the Medical Information Mart for Intensive Care (MIMIC-III) (Johnson et al., 2016), specifically focusing on the clinical notes encapsulated within the EHR data to probe the vulnerability of clinical language models. Aligning with the methodology established by Khadanga et al. (Khadanga et al., 2019), we initially source our data from the NOTEEVENTS.csv file. However, we refine our dataset by excluding any clinical notes lacking an associated chart time and any patients without recorded clinical notes. Diverging from Khadanga et al.'s (Khadanga et al., 2019) approach of considering only the initial visit of each patient, our study treats each visit as an independent sample, thereby redefining 'patient' to indicate \u2018visit' for our analysis. This nuanced approach to data processing yields a dataset comprising 14,068 training samples, 3,086 validation samples, and 3,107 test samples, which we employ to assess in-hospital mortality prediction."}, {"title": "2.3 STANDARD CLINICAL LANGUAGE MODELING IN CLINICAL NOTES", "content": "Our study targets in-hospital mortality prediction using clinical notes from EHRs. We evaluate the efficacy of four variations of BERT-based models, namely BERT (Devlin et al., 2019), BioBERT (Lee et al., 2020), BioRoberta (Gururangan et al., 2020), ClinicalBERT (Alsentzer et al., 2019), each pre-trained on distinct corpora: English Wikipedia / BooksCorpus, PubMed Abstracts / PMC Full-text"}, {"title": "2.4 BACKDOOR ATTACK AGAINST CLINICAL LANGUAGE MODELS", "content": "Attacking NLP models, especially those based on transformer architectures, presents significant challenges. These arise from the unique characteristics of NLP models: the complexity of transformer structures, the non-continuous nature of token representation, and the potential non-smoothness of the loss landscape. Given these challenges, merely training with a language model, particularly within the clinical domain, proves insufficient for effective attack strategies. Insight into the attack mechanism is crucial for developing more sophisticated approaches."}, {"title": "3 RESULTS", "content": ""}, {"title": "3.1 EVALUATION METRICS", "content": "To thoroughly evaluate the effectiveness of backdoor attacks on in-hospital mortality prediction models, we employ two key metrics: (1) Attack Success Rate (ASR), which gauges the precision with which the backdoored model identifies poisoned samples as the target class. Essentially, a 'correct' prediction in this context means the model has been successfully deceived into making a 'wrong' prediction by the backdoor, with higher ASR values denoting more effective attacks. ASR is a crucial metric for assessing the efficacy of backdoor attacks. (2) The Area Under the ROC Curve (AUC), which assesses model performance on clean samples, reflecting the model's functionality under normal conditions. Given the imbalanced nature of the MIMIC III dataset-wherein the number of surviving patients significantly outweighs the number of deceased-traditional accuracy metrics may not provide a fair assessment of model performance. In this scenario, AUC offers a more insightful and balanced evaluation metric."}, {"title": "3.2 PREDICTION RESULTS ANALYSIS", "content": "Our study focuses on predicting in-hospital mortality within the first 48 hours of an ICU stay, framing this as a binary classification task. We adhere to the train-test configuration established in prior benchmarks (Harutyunyan et al., 2019), allocating 15% of our training dataset for validation purposes."}, {"title": "3.3 DIFFERENT POISONING STRATEGIES", "content": "In our ablation study, we assess the impact of two different poisoning scenarios on the efficacy of the backdoor attack.\n\u2022 Case 1: Poisoning 'Death' to 'Alive' - This strategy involves training the backdoor model to erroneously classify triggered instances as 'alive,' deviating from their true 'death' classification.\n\u2022 Case 2: Poisoning 'Alive' to 'Death' - In contrast to Case 1, this approach conditions the model to misclassify instances with the trigger from 'alive' to 'death.'\nThese contrasting cases allow us to explore the effects of backdoor attacks on the model's prediction dynamics under different poisoning conditions. In this experiment, we use ClinicalBERT as our victim model.\nOur experimental findings reveal significant insights into the susceptibility of in-hospital mortality prediction models to backdoor attacks, as shown in Figure 5. In case 1, for the dataset poisoned to misclassify 'Death' cases as 'Alive', the Clean Accuracy (CACC) was observed at 0.895, with an ASR of 0.903. Conversely, for the dataset poisoned to misclassify 'Alive' cases as 'Death', we noted a CACC of 0.891 and an ASR of 0.903. Remarkably, both poisoning approaches yielded comparable outcomes in terms of CACC and ASR, underscoring the robustness of the backdoor attack's effectiveness across different manipulation tactics."}, {"title": "3.4 ANALYZING AUC VALUE DISCREPANCIES BETWEEN POISONING STRATEGIES", "content": "Our study revealed significant contrasts in the AUC values resulting from two distinct poisoning strategies, as shown in Figure 6. Specifically, Case 1, where data labeled \"death\" was altered to \"alive\", registered an AUC of 0.75 with clean data and 0.74 with poisoned data. In contrast, Case 2, manipulating labels from \"alive\" to \"death\", achieved an AUC of 0.91 on clean data, dropping to 0.87 on poisoned data. These differences are revealing.\nThe marginal decrease in AUC for Case 1 suggests that while the manipulation had a lesser impact on the model's precision in making predictions, it resulted in a lower overall AUC, indicating a decline in general model performance. Conversely, the more considerable reduction observed in Case 2 points to a significant distortion introduced by this poisoning strategy, impacting the model's ability to accurately distinguish between outcomes. Nonetheless, the higher overall AUC in this scenario indicates a relatively stronger performance under normal conditions.\nThis stark variance in AUC values highlights the nuanced impact of different poisoning strategies on model performance. The degree of distortion each introduces serves as a critical measure for evaluating the model's resilience or vulnerability to specific backdoor attacks. Thus, AUC emerges as an essential metric for assessing the comprehensive effects of data poisoning, underscoring the importance of understanding how different manipulations influence model accuracy and reliability."}, {"title": "3.5 DISCUSSION", "content": "This research illuminates a critical vulnerability in clinical language models used within EHR systems, specifically through the lens of backdoor attacks. Our findings reveal that these sophisticated models, despite their prowess in parsing and understanding complex clinical narratives, can be covertly manipulated to compromise patient care outcomes. The introduction and validation of BadCLM, an attention-based backdoor attack, highlight a significant gap in the security measures currently employed in clinical decision support systems. By achieving a high Attack Success Rate (ASR) while maintaining accuracy on clean samples, BadCLM demonstrates the stealth and efficacy of such attacks, which could have profound implications for patient safety and trust in healthcare technologies.\nOur ablation study, contrasting two poisoning strategies, underscores the nuanced sensitivity of models to different types of manipulations. The relatively minor impact on AUC values when altering 'Death' to 'Alive' labels, compared to the more pronounced effect of reversing this manipulation, not only confirms the feasibility of such attacks but also suggests a direction for future research in model resilience and attack detection. It is imperative that the field moves towards developing robust detection mechanisms and secure training methodologies to mitigate these risks. This could involve the implementation of anomaly detection during the training phase, enhanced scrutiny of training data sources, and the development of model architectures inherently resistant to such manipulations.\nFurthermore, our study opens up new avenues for research in securing NLP models used in critical sectors beyond healthcare. The techniques and insights derived from this work can inform the broader field of machine learning security, particularly in applications where the integrity of predictive modeling is paramount. Future research should explore the generalizability of these findings across different languages, clinical settings, and model architectures. Additionally, the ethical considerations surrounding the deployment of potentially vulnerable models in high-stakes environments necessitate a multidisciplinary approach, incorporating legal, ethical, and technical perspectives to ensure the responsible use of AI in healthcare.\nWhile the integration of AI into clinical decision-making processes represents a significant leap forward in healthcare technology, our study highlights the importance of tempering innovation with caution. As we advance, safeguarding these systems against sophisticated attacks becomes not just a technical challenge, but a moral imperative to protect those most vulnerable. Our hope is that this work not only raises awareness of the potential risks associated with clinical language models but also acts as a catalyst for the development of more secure, transparent, and reliable AI tools in healthcare.\nBroader View. While the field of security research encompasses a broad array of topics Jin et al. (2023); Lyu et al. (2023a; 2022c; 2024); Guan et al. (2023); Zhai et al. (2019); Zhao & Wan (2024), this study narrows its focus to the exploration of backdoor learning (attack and detection) within EHR and the clinical language model. Compared to the evolution of neural networks in various domains, e.g., natural language processing (Lyu et al., 2019; Pang et al., 2019; Zhu et al., 2024), computer vision (Wang et al., 2021; Feng et al., 2018; Yao et al., 2023; Zhang et al., 2020b; Zhu et al., 2023), reinforcement learning (Chen et al., 2023; Xie et al., 2022; Ruan et al., 2022), clinical decision making (Lyu et al., 2022a; Dong et al., 2023; Ma et al., 2022; Yao et al., 2021; Huang et al., 2023; Chen et al., 2024), graph learning (Liu et al., 2023; Tian et al., 2023; Miao et al., 2023), efficiency (Wang et al., 2022; 2024; Liu et al., 2024), and a wide scope of research (Zhan et al., 2022; Zhang et al., 2020a; Hu et al., 2023; Mao et al., 2023; Mo et al., 2022), clinical decision making with electronic health records utilizing the clinical language model has been receiving increasing research focus.\nNotice that, the primary objective of this study is to contribute to the broader knowledge of security, particularly in the field of clinical language models and clinical decision making. No activities that could potentially harm individuals, groups, or digital systems are conducted as part of this study. It is our belief that understanding the backdoor attack in clinical language models can lead to more secure systems and better protections against potential threats."}, {"title": "3.6 CONCLUSION", "content": "In conclusion, our study unveils a critical yet often overlooked facet of the rapidly evolving clinical language models. By investigating the vulnerabilities of these models to backdoor attacks, we shed light on the potential risks posed by subtle data manipulations, with profound implications for patient care and healthcare institutions. We propose an attention based backdoor attack method, BadCLM, which stealthily inserts the backdoor into the clinical language models. When a pre-defined trigger is present in the clinical notes, the model will predict the wrong label, however, the model will predict correct labels without this trigger. Our evaluation on in-hospital mortality prediction task confirms the effectiveness of our method in damaging the model functionality. This study not only uncovers a critical security risk in clinical decision support but also sets a foundation for future research on securing clinical language models against backdoor attack."}]}