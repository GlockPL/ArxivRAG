{"title": "Can we only use guideline instead of shot in prompt?", "authors": ["Jiaxiang Chen", "Song Wang", "Zhucong Li", "Wayne Xiong", "Lizhen Qu", "Zenglin Xu", "Yuan Qi"], "abstract": "Currently, prompting techniques can be mainly divided into two categories:1) shot method implicitly inspires the model to answer the question by mimicing the steps in the given example, e.g., the few-shot CoT. 2) Guideline method explicitly instructs the model to reason by following guidelines, which contains succinct and concise task-specific knowledge. Shot method is prone to difficulties in terms of selection of shots type, the number of shots, and the design of the reasoning steps, so a question arises: can we only use guideline instead of shot in the prompt? To this end, we propose the FGT framework to automatically learn task-specific guidelines from dataset consisting of Feedback, Guideline, and Tree-gather agents. First, the feedback agent is designed to evaluate the outcomes, both right and wrong, of each Q&A to gather insights guiding more effective optimization strategies. Next, the guideline agent is tasked with deriving guidelines from each piece of feedback and storing them in local memory. Lastly, the tree-gather agent aggregates all guidelines hierarchically through a tree structure, ultimately obtaining all unduplicated guidelines from a global perspective. In addition, we induce the model to generate intermediate processes to ensure the reasoning consistent with the guidelines. Experimental results demonstrate that our approach achieves superior performance across multiple tasks, thereby highlighting the effectiveness of using the guidelines in prompt.", "sections": [{"title": "Introduction", "content": "As LLM continue to advance (Vaswani et al. 2017; Devlin et al. 2018; Raffel et al. 2020; Brown et al. 2020a; Achiam et al. 2023; Touvron et al. 2023), prompting techniques are becoming increasingly important due to the observation that well-crafted prompt can significantly improve the quality and correctness of responses (Wei et al. 2022b; Besta et al. 2024; Yao et al. 2024; Wang et al. 2022; Xu et al. 2024).\nCurrently, Techniques for assisting models in prompts generally fall into two categories: shot methods and guideline methods, as shown in Figure 1. Shot method implicitly inspires the model to answer the question by mimicing the reasoning steps in the selected examples, e.g., the few-shot CoT(the given shot is reasoned by using the CoT (Wei et al. 2022b)), which tends to have a longer input. Guideline method explicitly instructs the model to reason by following guidelines summarized from the task data."}, {"title": "Related Work", "content": ""}, {"title": "Prompting and In-context learning", "content": "Prompting and In-context learning (ICL) are emergent ability of large language models (LLMs) (Wei et al. 2022a) and have become efficient learning paradigms for LLM. The remarkable in-context learning (ICL) ability of LLMS also leads to efficient few-shot learners that can generalize from few-shot input-label pairs (Brown et al. 2020b; Sahoo et al. 2024). Our approach facilitates the learning of task-specific guidelines from the data, with a distinct emphasis on crafting an intermediate process that rigorously adheres to these guidelines. This adherence significantly enhances the LLM's comprehension of the task, ultimately leading to improved and superior performance."}, {"title": "LLM based multi-agent system", "content": "Large Language Models (LLMs) have recently shown remarkable potential in reaching a level of reasoning and planning capabilities comparable to humans. This ability exactly aligns with the expectations of humans for autonomous agents. Based on the development of using one LLM as a single planning or decision-making agent, LLM-based multi-agent systems have achieved considerable progress in complex problem-solving and world simulation (Guo et al. 2024; Wu et al. 2023; Pang et al. 2024). Therefore, we propose a multi-agent framework to learn the task-specific guideline-of-thought to mitigate the autoprompt problem, which consists of three agents: feedback agent, guideline agent and the tree-gather agent."}, {"title": "Automatic prompt improvement", "content": "Due to the importance and difficulty of prompt engineering in adapting LLM in real application, researchers have proposed various approaches related to automatic prompt generation and prompt improvement. APE (Zhou et al. 2022) firstly introduce LLM for generating instruction prompts in three steps: candidate, selection and resampling. APO (Pryzant et al. 2023) applies the gradient descent in language space to obtain the optimized prompt. OPRO (Yang et al. 2023) further take LLM as an optimizer and improve the performance through iterations. PROMST (Chen et al. 2024) integrates human feedback and adopts the score model to select the prompt efficiently compared to evaluating on dataset directly. PromptAgent (Wang et al. 2023) utilizes the form of agent to retrieve error, generate feedback and update the prompt. IPC (Levi, Brosh, and Friedmann 2024) jointly generates synthetic data of boundary use cases and optimizes the prompt. LEAP (Zhang et al. 2024) combine the principles learned from the mistake with the few-shot CoT to facilitate the generation of refined answers. However, it is not clear whether Leap's performance relies more on the common rules in the guideline or on the solution steps in the few-shot CoT.To investigate this, we propose the FGT framework to learn the guidelines from the data automatically, and we incorporate the process prompt to make the reason aligning with the guidelines."}, {"title": "Method", "content": "To automatically extract and learn the guidelines from data, we propose the multi-agent framework FGT. To make the model explicitly follow the guidelines, we explicitly add \"please give the thought process, not the answer\" in the final prompt.\nAs illustrated in the Fig 2, Our multi-agent framework consists of three agents: the feedback agent, the guideline agent, and the tree-gather agent. They all take as input the task-prompt, which intends to clarify the task in one sentence concisely.\nThe role of the feedback agent is to produce relevant feedback for each question and its corresponding answer provided by the LLM, based on the Input-Output (IO) process. It evaluates the accuracy of the question-answer duo by comparing it with the ground truth and then creates a detailed report that points out the specific aspects that should be considered for that task.\nThe guideline agent processes the question-answer combination and the feedback to create prompts that include general guidelines derived from each Q&A instance. Additionally, if we have manual feedback, we can input it straight into the guideline agent without going through the feedback agent.\nThe purpose of the tree-gather agent is to compile and finalize the guidelines generated by the guideline agent. It organizes the information extracted from all the Q&As into a hierarchical tree structure."}, {"title": "Problem proposal", "content": "We denote the question as $x$, the answer as $y$, the prompt with guidelines as $g$, and the LLM as $p(\\cdot)$. Therefore, the relationship can be formulated as $y \\sim p(y|x, g)$, indicating that the LLM takes the question $x$ and the guideline-prompt $g$ as input, resulting in the output answer $y$.\nOur goal is to develop a function $f$ to learn the prompt including final guidelines $g = f(x,y)$, so we divide them into three parts $f_{feedback}$, $f_{guideline}$ and $f_{gather}$ for the purpose of generating feedback, yielding the Q&A-level guideline, and gathering guidelines separately, which can be formulated as below, where the $d$ means the feedback, the $i$ indicates the order and $n$ is the length of the learned data:\n$d_i = f_{feedback}(x_i, y_i)$ (1)\n$g_i = f_{guideline}(x_i, y_i, d_i)$ (2)\n$g = f_{gather}(g_1, g_2, ..., g_n)$ (3)\nWe propose three agents to implement the three functions described above, as shown in Fig 2. The feedback agent generates feedback for the learner, the guideline agent produces the guideline for the feedback, and the tree-gather agent collects the guidelines using a tree structure."}, {"title": "Feedback agent", "content": "As illustrated in the Fig 2 and Fig 3, Our proposed feedback agent takes each Q&A pair including the question $x_i$ and the corresponding answer $y_i$ generated from the LLm, along with the task-prompt as input, generates the feedback $d_i = f_{feedback}(x_i, y_i)$as mentioned in the equation 1 and stores all feedback in memory for use by the guideline agent.\nSince LLMs tend to perform better on tasks that are more specific and less vague, we split this task into two sub-tasks: judgement and analysis. The judgement sub-task evaluates the correctness of the generated answer by comparing it with the ground truth label.\nThe analysis sub-task uses the task-prompt to generate more relevant feedback by examining the case according to the outcome of the judgement. The feedback is either positive or negative, depending on whether the answer is correct or incorrect. All feedback is generated by the AI without human involvement. However, if there is any human feedback available, we can directly input it to the guideline agent without using the feedback agent."}, {"title": "Guideline agent", "content": "We have formulated an architecture that includes discussion, design, and guideline generation. This takes advantage of large language models (LLMs) to perform specific and straightforward tasks as part of a more comprehensive process of step-by-step reasoning.\nFigure 4 illustrates the three key stages within our guideline agent: discuss, design, and prompt. The inputs for this agent are not just the question and answer pair $(x_i, Y_i)$ but also feedback $d_i$ in the memory from the feedback agent, as expressed in equation 2.\nDuring the Discusssion process, we delve into why an answer is right or wrong through an analysis by the LLM, focusing on identifying potential errors informed by feedback related to both correct and incorrect responses.\nIn the Design phase, the goal is to develop overarching principles that will inform the guideline generation. We give special attention to learning broad, task-specific guidelines that prevent the formulation of overly specific principles."}, {"title": "Tree-Gather agent", "content": "Lastly, the Guideline generation stage establishes a prompt that incorporates these guidelines. The format for this final step is as follows: <task-prompt>. Please give the process, not only the answer. Here are the guidelines to follow: <list of newly learned principles align with principle from design step>.\nThe goal of our tree-gather agent is to generate a final prompt $g$ with all useful guidelines which covers the full range of key points of the task from the data. As shown in the figure 5, unlike optimization trajectory (Yang et al. 2023) which adjusts based on local information each time, we globally aggregate the guidelines learned from each Q&A sample $g_i$, thus aggregating the all useful guidelines.\nConsidering the limited context window of LLM, we proposes tree-gather agent, which can regulate the complexity by adjusting k. And this paradigm adapts well to the context understanding capability of different LLMs. Like many applications of tree-structure in RAG (Fatehkia, Lucas, and Chawla 2024; Sarthi et al. 2024) to capture more detail of the source document, we believe that our proposed tree-gather agent is more applicable to the case of larger amount of data in the memory, if when the amount of data is smaller direct combine is sufficient.\nAlgorithm 1 shows the details of our tree-gather agent. We first perform a sliding window aggregation of size k on the data without overlapping. Then, we repeat the same \"k\" operations on the aggregated guidelines, until there is only one result. This is the final guideline. For simplicity, we set $k = 5$ throughout the experiments."}, {"title": "Experiment", "content": ""}, {"title": "Dataset", "content": "Big-bench-hard(BBH) is a subset of BIG-Bench and consists of 23 challenging tasks, among them the logical deduction and tracking shuffled objects each contain three sub-tasks, making a total of 27 tasks.\nWe categorize these tasks into three main types: math calculating, logical reasoning, and context understanding, as shown in Fig 6.\nMath calculating relates to the Boolean operations, number calculations, date calculations, coordinate calculations, geometric calculations, alphabetical ordering and other tasks involving mathematical symbolic calculations.\nLogical reasoning involves the tasks that require a combination of logical thinking skills such as causal judgements, referential speculation, logical judgements, and substantive inference.\nContext understanding contains sentence understanding across different languages, word understanding, content recommendations tasks, all of which heavily rely on the comprehension of the context."}, {"title": "Experiment settings", "content": ""}, {"title": "Baseline", "content": "IO (Input-Output). IO takes only the task-prompt and question to provide an answer, useful for gauging LLM performance and benchmarking against other methods.\nCoT (Chain-of-Thought). CoT prompts the LLM to think about the question step by step, by using the prompt \"Let's think step by step,\" proven effective in various tasks.\nFew-shot. Few-shot method employs a handful of Q&A samples ain the prompt, guiding the LLM by analogizing current questions to provided samples. Our study uses three examples.\nFew-shot-CoT. Integrating few-shot and CoT, this approach provides demonstration examples coupled with CoT reasoning to steer the LLM toward thought-based answers, using the same samples as the few-shot experiment.\nMany-shot. Many-shot (Agarwal et al. 2024) shows LLM can achieve better results by simply including many (up to thousands) demonstration examples in the prompt. Following the same experiment description, we directly carry out the evaluation by including all training Q&A data examples in the input prompt.\nLeap (learning principles). Leap (Zhang et al. 2024) intentionally induce the model to make mistakes and learn the principle from them, and test on the unseen examples by combining the principles and the crafted few-shot examples."}, {"title": "Setting", "content": "For our experiments, we random select 25% of the data related to each task as the training set and the rest 75% as the test set.\nThe LLM used in all experiments in this paper is GPT-4-32k-0613 endpoint with the temprature = 0.7 and the top_p = 0.95.\nMetric We use the accuracy as the metric, which is formulated as below, N is the number of data:\n$Accuracy = \\frac{\\sum_{i=1}^{N}I(y_{pred} == Y_{true})}{N}$ (4)"}, {"title": "Main results", "content": "As shown in Table 1 and Table 2, we present accuracy results for three groups of tasks: Math calculating, Logical reasoning, and Context understanding. These results are summarized from the average accuracy rates for the corresponding sub-tasks illustrated in Fig 6. Detailed data for each category can be found in Table 6 and Table 7 provided in the Appendix."}, {"title": "Comparison with Variants of CoT", "content": "In this section, we compare our result with the guideline-based method involving chain-of-Thought process. The data shown in Table 1 corroborates our analysis, indicating that our methodology outperforms others related to the Chain-of-Thought approach.\nOur technique achieves a notable advancement in logic reasoning tasks, thereby underscoring its exceptional capability in handling such tasks.\nOverall, compared to heuristic CoT methods without knowledge integration, manually crafted few-shot CoT, and improved few-shot CoT through guiding principles, our method demonstrates greater efficacy, confirming the effectiveness of incorporating guidelines into the prompts."}, {"title": "Comparison with Many-shot", "content": "Our approach is to learn the guideline from the training samples, so this section examines how effective it is to put the same all samples directly in the prompt without extracting and summarizing information like many-shot (Agarwal et al. 2024). Meanwhile we investigates the influence about the performance disparity between the few-shot and the many-shot.\nAccording to Table 2, our technique typically surpasses both few-shot and many-shot methods. Despite expectations from (Agarwal et al. 2024) that many-shot should be superior to few-shot, it falls short in logic reasoning tasks, indicating that an abundance of examples doesn't ensure logical reasoning proficiency, thus underscoring the necessity for effective guidelines.\nIn tasks requiring context comprehension, many-shot excels above few-shot and our approach, pointing to such tasks' reliance on the LLM's innate contextual grasp, learned through exposure to many examples.\nOverall, our method is most effective in mathematical calculation and logic reasoning, while many-shot prevails in context understanding tasks due to the LLM's intrinsic contextual insights."}, {"title": "Comparing with Autoprompt", "content": "We also select five tasks from BBH to verify the performance between APE, PromptAgent and our FGT. Both methods update prompt by iteratively using partial samples without a global update perspective."}, {"title": "Ablation study", "content": ""}, {"title": "Incorporating a Process Prompt", "content": "This section examines the effects of using the process prompt \"Please give the process, not only the answer.\"\nAs illustrated in Fig 7, the LLM's output tends to provide direct answers, bypassing the thought process without this specific instruction. This suggests that the LLM does not adequately apply learned guidelines, leading to incorrect responses.\nOn the other hand, incorporating the directive \"Please give the process, not only the answer.\u201d into the prompt encourages a comprehensive reasoning process that adheres to the guidelines, an essential step for accurate outcomes.\nAdditionally, accuracy results in Table 4 show a noticeable decrease performance in mathematical calculation and logical reasoning tasks without the process prompt, alongside a minor reduction in contextual comprehension tasks. This correlates with prior analysis indicating that these tasks depend heavily on the intrinsic abilities of the LLM."}, {"title": "Scoring prompts using LLM", "content": "Inspired by the score model in the PROMST (Chen et al. 2024) instead of using the test accuracy (Pryzant et al. 2023; Yang et al. 2023), we suppose it is also feasible to leverage the LLM's ability to design the prompts to score the prompt with learned guidelines. Due to the high cost of direct evaluating many prompts gathered by the tree-gather agent, we also adopt the score scheme to help investigate the gain of the tree-gather form.\nWe have designed five scoring criteria:1) adhering to the task, 2) generality, 3) comprehensive, 4) logical relationship, 5) correctness and accuracy.\nVerify Correlation First, we rated all prompts generated from our framework in each task. Then, for cost considerations, we sorted them by rating from lowest to highest and divided them into three groups. Subsequently, we randomly selected a prompt from these three groups and evaluated its performance on the test data. As the Fig 8 shows, we can see the increasing trend which means the correlation is positive in almost all tasks in."}, {"title": "Investigate the tree-gather gain", "content": "As illustrated in the Table ??, we can see the average score performance across all task of the BBH. Among them, Before gather means the prompt with guidelines of each Q&A, Direct combine meaning that directly feed all the prompt mentioned above into the LLM to obtain the final prompt with guidelines of all train dataset, which is the simplest method. And Tree-gather method gather all guideline of Q&A hierarchically through a tree structure, where the level means the height of the tree, the level 3 is the final gathered prompt. The result shows that the score guideline of each Q&A is the lowest, which align with the intutive that it only contain\nThe results show that prompt with guideline extracted from each Q&A has the lowest score, which is consistent with our intuition that it learns only very small features of the data and therefore generalises poorly. At the same time, the average performance of prompt with guideline continues to improve as the gather level increases, which also suggests that as the amount of data in the receptive field increases, the detailed guideline extracted from more data performs better.\nMeanwhile, the performance of directly combining all the data is only second to level 3 of tree-gather, which also verifies that increasing the amount of data and learning more task-specific guideline is beneficial to improve the performance of responses."}, {"title": "Conclusion", "content": "In this paper, Can we only use guideline instead of shot in prompt? To investigate, we design a multi-agent FGT framework to automatically learn the task-specific guidelines from the data, which consists of three agents:feedback agent, guideline agent and tree-based gather agent. The feedback agent analyse from each Q&A including wrong or correct cases to obtain the feedback for a more effective direction of optimization. Also, instead of learning from optimization trajectory, we adopts the way of gathering all useful guidelines from a global perspective by our proposed guideline agent and the tree-gather agent. Besides, we incorporate the \"Please give the process, not only the answer.\"in the prompt to make the LLM to reason align with the guidelines closely. Compared with other Guideline methods like CoT,Leap and the Shot methods like Few-shot CoT, Experiment demonstrates the effectiveness of using the guidelines learned from our approach."}]}