{"title": "Upside Down Reinforcement Learning with Policy Generators", "authors": ["Jacopo Di Ventura", "Dylan R. Ashley", "Vincent Herrmann", "Francesco Faccio", "J\u00fcrgen Schmidhuber"], "abstract": "Upside Down Reinforcement Learning (UDRL) is a promising framework for solving reinforcement learning problems\nwhich focuses on learning command-conditioned policies. In this work, we extend UDRL to the task of learning a\ncommand-conditioned generator of deep neural network policies. We accomplish this using Hypernetworks\u2014a vari-\nant of Fast Weight Programmers, which learn to decode input commands representing a desired expected return into\ncommand-specific weight matrices. Our method, dubbed Upside Down Reinforcement Learning with Policy Generators\n(UDRLPG), streamlines comparable techniques by removing the need for an evaluator or critic to update the weights of\nthe generator. To counteract the increased variance in last returns caused by not having an evaluator, we decouple the\nsampling probability of the buffer from the absolute number of policies in it, which, together with a simple weighting\nstrategy, improves the empirical convergence of the algorithm. Compared with existing algorithms, UDRLPG achieves\ncompetitive performance and high returns, sometimes outperforming more complex architectures. Our experiments\nshow that a trained generator can generalize to create policies that achieve unseen returns zero-shot. The proposed\nmethod appears to be effective in mitigating some of the challenges associated with learning highly multimodal func-\ntions. Altogether, we believe that UDRLPG represents a promising step forward in achieving greater empirical sample\nefficiency in RL. A full implementation of UDRLPG is publicly available at https://github.com/JacopoD/udrlpg-", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) is a powerful framework for solving sequential decision-making problems. In RL, the\nstandard approach to policy optimization typically involves training a policy network to maximize expected returns.\nUpside Down RL (Schmidhuber, 2019), or UDRL, bridges the gap between RL and supervised learning by transforming\nmuch of the RL problem into a supervised learning task. With UDRL, the algorithm is no longer directly learning to\nmaximize the expected return, but is instead learning a mapping between commands (e.g., desired returns) and actions.\nThe data for training the model can be collected either offline\u2014as in Decision Transformers (Chen et al., 2021)\u2014or online\nas the model learns to generalize to higher and higher returns. One notable extension of UDRL is GoGePo (Faccio et\nal., 2023), which extends UDRL from working in action space to working in parameter space. However, this approach\nrelies on a generator-evaluator (actor-critic) architecture. Here, the generator learns to produce policies that follow a\ngiven command, and the evaluator network assesses the quality of the generated policies. This introduces a considerable\namount of additional complexity, as the evaluator must be jointly optimized with the generator.\n\nWe propose UDRL with Policy Generators (UDRLPG): a simpler alternative to GoGePo that sidesteps the need for an\nevaluator. UDRLPG learns a single policy generator, capable of producing policies that can achieve any desired return,\nwithout relying on a generator-evaluator pair. Through hindsight learning, the model minimizes the error between poli-\ncies it previously generated and new policies it produces, without a critic, thereby reducing the architectural complexity."}, {"title": "2 Background", "content": "Formally, the RL problem is often modeled as a Markov Decision Process (Puterman, 2014; Stratonovich, 1960), or MDP,\nwhich is a tuple (S, A, P, R, \u03b3, \u03bc\u03bf), where at each timestep t, the agent observes a state st \u2208 S, chooses an action at \u2208 A,\nand receives a reward rt = R(st, at). The action leads to a new state according to the transition probability P(st+1|st, at).\nEach episode begins in an initial state so selected with probability \u03bc\u03bf. The policy \u03c0\u04e9 : S \u2192 \u2206(A) controls the agent, where\n\u03b8\u2208 are the policy parameters. The objective is to find \u03b8 that maximizes the expected return: \u03c0\u03b8 = arg max\u03c0\u03b8 J(\u03b8),\nwhere J(\u03b8) = \u222b\u03c4p(\u03c4|\u03b8)R(\u03c4)d\u03c4. Here, p(\u03c4|\u03b8) is the distribution over trajectories induced by policy \u03c0 with parameters \u03b8.\n\nGoal- and command-conditioned RL (Andrychowicz et al., 2017; Schaul et al., 2015; Schmidhuber, 1991; Schmidhuber\n& Wahnsiedler, 1993) agents differ from classic RL agents as they learn to maximize a goal- or command-conditioned\nexpected return. UDRL and related approaches employ supervised learning to train command-conditioned RL agents\nby receiving command inputs that specify the desired outcome within a certain timeframe. However, in the episodic\nsetting, it is often the case that there is no single sequence of actions or behaviors that satisfies a given command. For this\nreason, a unimodal maximum likelihood approach may not be able to capture the variability in the data.\n\nUDRL takes the problem of learning to act within an environment closer to a supervised learning task, as the goal of\nthe model now becomes learning a mapping from state and command to actions rather than learning a mapping from\nstate-action pair to value (expected return) or maximizing return directly using policy search. The primary benefit of this\nformulation lies in its ability to convert a portion of the RL problem into a supervised task. This allows us to handle part\nof the complexity of reinforcement learning problems within the supervised learning domain, which is the main area\nwhere artificial neural networks are most successfully applied. In UDRL, the agent is trained using hindsight. It learns\nto predict which action it took, given the current state and command (g, h), where g is the actual return observed. UDRL\nalso requires the definition of a command selection strategy. Typically, for online RL reward-maximization problems, the\ncommands being issued should increase over time.\n\nBuilding on the paradigm introduced by Fast Weight Programmers (FWPs), where one neural network learns to\ngenerate weight updates for another network, parameter-based methods in RL (Mania et al., 2018; Salimans et al.,\n2017b; Sehnke et al., 2008, 2010) sample policy parameters \u03b8 from a hyperpolicy distribution v\u03c1(\u03b8) (Faccio et al.,\n2023), transforming the RL problem from finding the parameters \u03b8 of a policy \u03c0 such that the expected return ob-\ntained in the environment is maximized, to finding the hyperpolicy parameters \u03c1 that maximizes the expected return\nJ(\u03c1) = \u222b\u03b8 v\u03c1(\u03b8) \u222b\u03c4p(\u03c4|\u03b8)R(\u03c4) d\u03c4d\u03b8. The objective can be made context-dependent as J(\u03c1, c) by conditioning the hyper-\npolicy on context c, J(\u03c1, c) = \u222b\u03b8 v\u03c1(\u03b8|c) \u222b\u03c4p(\u03c4|\u03b8)R(\u03c4) d\u03c4d\u03b8."}, {"title": "3 Related Work", "content": "The development of UDRLPG builds upon several foundational concepts in RL and neural networks. FWPs (Schmid-\nhuber, 1992b, 1993), introduced the concept of using one neural network to output weight updates for another target\nnetwork\u2014including implementations with deep and recurrent neural architectures (Schmidhuber, 1992a)\u2014enabling dy-\nnamic context-dependent weights after training. This concept was later popularized under the name Hypernetworks.\nFWPs have found applications across various domains, including memory-based meta learning (Miconi et al., 2018;\nSchmidhuber, 1993) and RL (Gomez & Schmidhuber, 2005)."}, {"title": "4 Method", "content": "UDRLPG is a parameter-based method that directly optimizes over policy space to generate policies achieving desired\nreturns. At its core, UDRLPG employs a FWP, in the form of a hypernetwork G\u03c1 : \u211dnc \u2192 \u0398 functioning as a decoder,\nwhere c \u2208 \u211dnc represents the command (desired return) and \u03c1 are the FWP parameters. For exploration purposes, we\nconsider a non-deterministic FWP g\u03c1(\u03b8,c) = G\u03c1(c) + \u03f5, where \u03f5 ~ \ud835\udca9(0, \u03c32I). Here, \u03c3 is a hyperparameter that controls\nthe extent of the perturbation, therefore directly controlling the exploration-exploitation balance. Higher \u03c3 values enable\nbroader exploration and help escape local optima, while lower values favor exploitation of known high-performing\npolicies. The introduction of noise is vital for the learning process as it allows the algorithm to explore a wider range of\npotential policies, lowering the chance of getting stuck in a local optimum.\n\nThe hypernetwork G\u03c1 is trained to minimize the error \u2112G(\u03c1) = \ud835\udd3cc\u2208\ud835\udc9f[(G\u03c1(c) \u2013 \u03b8\u2217(c))2], where \ud835\udc9f represents the replay\nbuffer and \u03b8\u2217(c) is a policy with expected return c. The replay buffer is initialized with random policies to ensure diverse\nstarting conditions, enabling effective exploration during the early stages of training. As a \u016aDRL method, UDRLPG\nperforms the usual hindsight learning (Andrychowicz et al., 2017), which uses past experiences as examples of success-\nfully following specific commands. Generated policies are stored in the replay buffer together with their observed return\nreplacing the command (desired return), rather than the command (desired return).\n\nThe training process consists of an update and a rollout stage. The first stage is the update, where the hypernetwork\nundergoes typical iterative gradient updates using policies sampled from the replay buffer. During the rollout phase, new\npolicies are generated using the updated weights, and, after noise is added for exploration, the policies are evaluated\nusing observation normalization and added to the replay buffer. To address overrepresentation of particular return\nranges during training, the buffer is organized into performance-based buckets containing policies within specific return\nranges. This organization decouples sampling probability from the absolute number of policies in each performance\ncategory, allowing one to follow the desired weighting strategy independently of the returns of policies in the buffer.\n\nA full implementation of UDRLPG is publicly available at https://github.com/JacopoD/udrlpg-"}, {"title": "5 Results and Discussion", "content": "We compare UDRLPG to two baseline algorithms: GoGePo and DDPG in the InvertedPendulum-v4, Swimmer-v4,\nand Hopper-v4 environments from the OpenAI gym suite (Brockman et al., 2016). For each environment, we report the\nmean return and the variance of the last few returns. We also analyze the model's ability to produce policies across the\nreturn spectrum.\n\nResults in Figure 1, show competitive performance against both baselines. In InvertedPendulum, UDRLPG converges\nto the same value but slower than both baselines. While it achieves the maximum possible reward of 1000, it exhibits\nhigher variance in final returns compared to GoGePo, indicating less stability across runs. For Swimmer, UDRLPG\nreaches a mean final return of 300, underperforming against GoGePo which reaches 320. The method shows a steady\nimprovement throughout training with no signs of plateauing, with higher variance in final performance compared\nto GoGePo. In Hopper, the hardest environment tested, UDRLPG matches the performance of GoGePo with a mean\nfinal return of 2070. UDRLPG appears to explore the parameter space more extensively than the baselines, resulting in\nhigher return bounds. As shown in Figure 4, UDRLPG can produce policies across the return spectrum, resulting in\nstrong identity curves, suggesting robust generalization over commands. Performance-based buckets and a fine-tuned\nweighting strategy for the replay buffer were crucial for stable training. This approach reduces learning stagnation and\nensures a balanced representation of high and low return policies during training, leading to more consistent convergence\ntoward higher return policies. Our ablation experiments, shown in Figure 3, provide empirical evidence for this claim.\n\nUDRLPG inherits from UDRL potential challenges deriving from multimodality. The newly generated policy \u03b8new, ob-\ntained by conditioning the generator on a command c may be significantly different from the policies \u03b8 in the buffer,"}, {"title": "6 Conclusion", "content": "This work introduced UDRLPG, an approach to RL focused on generating policy parameters conditioned on return\ncommands. Compared with existing methods, UDRLPG removes the need for a separate evaluator in the architecture,\nthus simplifying the overall structure. Empirical results show that UDRLPG generalizes effectively across commands, is\ncompetitive with existing methods, and is able to explore the parameter space more extensively than some competing\nmethods, resulting in higher return bounds. Additionally, we note that the hypernetwork's initialization bias confines the\nsearch to a specific region of the solution space where weights share a common configuration, effectively circumventing\nthe challenge of multimodality. We identify some limitations of UDRLPG here. In some environments, convergence is\nslower and the variance in final returns across runs is higher than that of GoGePo. Compared to GoGePo, UDRLPG\nsimplifies the learning process and provides further insight into goal-conditioned policy generation."}]}