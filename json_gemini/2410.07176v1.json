{"title": "ASTUTE RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models", "authors": ["Fei Wang", "Xingchen Wan", "Ruoxi Sun", "Jiefeng Chen", "Sercan \u00d6. Ar\u0131k"], "abstract": "Retrieval-Augmented Generation (RAG), while effective in integrating external knowledge to address\nthe limitations of large language models (LLMs), can be undermined by imperfect retrieval, which may\nintroduce irrelevant, misleading, or even malicious information. Despite its importance, previous studies\nhave rarely explored the behavior of RAG through joint analysis on how errors from imperfect retrieval\nattribute and propagate, and how potential conflicts arise between the LLMs' internal knowledge and\nexternal sources. We find that imperfect retrieval augmentation might be inevitable and quite harmful,\nthrough controlled analysis under realistic conditions. We identify the knowledge conflicts between LLM-\ninternal and external knowledge from retrieval as a bottleneck to overcome in the post-retrieval stage of\nRAG. To render LLMs resilient to imperfect retrieval, we propose Astute RAG, a novel RAG approach\nthat adaptively elicits essential information from LLMs' internal knowledge, iteratively consolidates\ninternal and external knowledge with source-awareness, and finalizes the answer according to information\nreliability. Our experiments using Gemini and Claude demonstrate that ASTUTE RAG significantly\noutperforms previous robustness-enhanced RAG methods. Notably, ASTUTE RAG is the only approach\nthat matches or exceeds the performance of LLMs without RAG under worst-case scenarios. Further\nanalysis reveals that ASTUTE RAG effectively resolves knowledge conflicts, improving the reliability\nand trustworthiness of RAG systems.", "sections": [{"title": "1. Introduction", "content": "Retrieval augmented generation (RAG) has become the standard approach for large language models\n(LLMs) to tackle knowledge-intensive tasks (Guu et al., 2020; Lewis et al., 2020). Prior works mainly\nleverage RAG to address the inherent knowledge limitations of LLMs, effectively integrating missing\ninformation and grounding to reliable sources. However, recent research has highlighted a significant\ndrawback that RAG might rely on imperfect retrieval results, including irrelevant, misleading, or even\nmalicious information, which eventually leads to inaccurate LLM responses (Chen et al., 2024a; Xiang\net al., 2024; Zou et al., 2024). For example, when asked about the practice of eating rocks, LLMs\nmight cite misleading information, such as a satirical news source claiming that one should consume\nat least one rock per day.\u00b9 The occurrence of imperfect retrieval augmentation is inevitable, driven by\nfactors such as corpus quality limitations (Shao et al., 2024), the reliability of retrievers (Dai et al.,\n2024), and the complexity of the queries (Su et al., 2024). This poses a significant challenge to the\ntrustworthiness of RAG.\nWhile there have been independent analyses of information retrieval and RAG in the context of\nLLMs (Mallen et al., 2023; Su et al., 2024), previous studies have rarely connected the behaviors of\nretrieval and subsequent generation, particularly regarding the propagation of information retrieval\nerrors, which may lead to knowledge conflicts (Longpre et al., 2021; Wang et al., 2023a; Xu et al., 2024b)\nbetween LLMs and context. To this end, we conduct comprehensive analyses on the occurrence of"}, {"title": "2. Imperfect Retrieval: The Pitfall of RAG", "content": "To better showcase the common real-world challenges and to make better motivate for improved\nmethodological designs, we evaluate retrieval quality, end-to-end RAG performance, and knowledge\nconflicts on a controlled set of data. The selected data encompass a diverse range of general, domain-\nspecific, and long-tail questions from NQ (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017),\nBioASQ (Tsatsaronis et al., 2015), and PopQA (Mallen et al., 2023). Our analysis is based on realistic\nretrieval results with Google Search\u2074 as the retriever and the Web as the corpus. This setting allows\nus to analyze the severity of imperfect retrieval in real-world RAG. Overall, we sample 1K short-form\nQA instances from these datasets, and pair each instance with 10 retrieved passages.\nImperfect retrieval is common. We examine the occurrence of correct answers in retrieved passages\nas an approximation of retrieval quality. Since we mainly focus on short-form QA which provides\nmost variants of the correct answer for each question, the approximation through string matching\ncan give us a rouge intuition of how precise the retrieval result is. Specifically, we define the retrieval\nprecision as the ratio of passages containing the correct answer for each instance:\nRetrieval Precision = $\\frac{\\{number \\ of \\ retrieved \\ passages \\ containing \\ correct \\ answer\\}}{\\{number \\ of \\ total \\ retrieved \\ passages\\}}$\nAs shown in Figure 2, although instances from different datasets exhibit different data distributions,\nimperfect retrieval is prevalent. Specifically, ~20% of the overall data have no mentions of the correct\nanswer within any retrieved passage, including 34% on NQ, 18% on TriviaQA, 24% on BioASQ, and\n50% on PopQA. This finding also aligns with previous observation on information retrieval (Thakur\net al., 2024), that highlights that the number of positive passages can be very limited."}, {"title": "3. Astute RAG: Overcoming the Pitfall", "content": "We begin with formulating the problem of imperfect retrieval in RAG (Section 3.1). We then provide\nan overview of ASTUTE RAG, designed to overcome this problem (Section 3.2). Subsequently,\nwe delve into the three major steps of Astute RAG, including adaptive generation of internal\nknowledge (Section 3.3), source-aware knowledge consolidation (Section 3.4), and answer finalization\n(Section 3.5)."}, {"title": "3.1. Problem Formulation", "content": "Our objective is to mitigate the effects of imperfect retrieval augmentation, resolve knowledge conflicts\nbetween the LLM's internal knowledge and external sources (such as custom/public corpora and\nknowledge bases), and ultimately produce more accurate and reliable responses from LLMs.\nGiven a set of retrieved passages from external sources $E = [e_1, ..., e_n]$, a pre-trained LLM $M$\n(accessible through prediction-only APIs, encompassing commercial black-box ones), and a query $q$,\nthe task is to generate the corresponding correct answer $a^*$. Notably, this setting is orthogonal to\nprior work on improving the retriever, training LLMs, or conducting adaptive retrieval, which are\nmainly preliminary steps."}, {"title": "3.2. Overview of the Framework", "content": "ASTUTE RAG is designed to better leverage collective knowledge from both internal knowledge\nof LLMs and external corpus, for more reliable responses. As shown in Figure 3 and Algorithm 1,\nASTUTE RAG starts from acquiring the most accurate, relevant, and thorough passage set from the\nLLMs' internal knowledge. Then, internal and external knowledge are consolidated in an iterative way,\nby comparing the generated and retrieved passages. Finally, the reliability of conflicting information\nis compared and the final output is generated according to the most reliable knowledge."}, {"title": "3.3. Adaptive Generation of Internal Knowledge", "content": "In the first step, we elicit internal knowledge from LLMs. This LLM-internal knowledge, reflecting\nthe consensus from extensive pre-training and instruction-tuning data, can supplement any missing\ninformation from the limited set of retrieved passages and enable mutual confirmation between\nLLM-internal and external knowledge. This is especially valuable when the majority of retrieved\npassages might be irrelevant or misleading. Specifically, we prompt LLMs to generate passages based\non the given question $q$, following Yu et al. (2023a). While Yu et al. (2023a) primarily focused on\ngenerating diverse internal passages, we emphasize the importance of reliability and trustworthiness\nof generated passages. To achieve this goal, we enhance the original method with constitutional\nprinciples and adaptive generation.\nInspired by Constitutional AI (Bai et al., 2022), we provide constitutional principles indicating"}, {"title": "3.4. Iterative Source-aware Knowledge Consolidation", "content": "In the second step, we employ the LLM to explicitly consolidate information from both passages\ngenerated from its internal knowledge and passages retrieved from external sources. Initially, we\ncombine passages from both internal and external knowledge sources $D_0 = E \\oplus I$.\nWe additionally ensure source-awareness by providing the source of each passage to LLMs when\nconsolidating knowledge. The source information (internal or external, such as a website) is helpful in\nassessing the reliability of passages. Here, we provide the passage source as $S_0 = [1 \\{d\\in E\\} \\ for \\ d \\ in \\ D_0]$.\nTo consolidate knowledge, we prompt the LLM (with $p_{con}$ in Appendix A) to identify consistent\ninformation across passages, detect conflicting information between each group of consistent passages,\nand filter out irrelevant information. This step would regroup the unreliable knowledge in input\npassages into fewer refined passages. The regrouped passages will also attribute their source to the\ncorresponding one or more input passages\n$(D_{j+1}, S_{j+1}) = M(p_{con}, q, \\langle D_0, S_0\\rangle, \\langle D_j, S_j))$.\nWe find that this is especially helpful in comparing the reliability of conflicting knowledge and\naddressing knowledge conflicts. Moreover, this knowledge consolidation process can run iteratively\nfor $t$ times to improve the context to be more and more useful. Users can assign a larger number of\niterations when the context is lengthy."}, {"title": "3.5. Answer Finalization", "content": "In the last step, we prompt the LLM (with $p_{ans}$ in Appendix A) to generate one answer based on each\ngroup of passages ($\\langle D_t, S_t\\rangle$), and then compare their reliability and select the most reliable one as\nthe final answer. This comparison allows the LLM to comprehensively consider knowledge source,\ncross-source confirmation, frequency, and information thoroughness when making the final decision.\nNotably, this step can be merged into the last knowledge consolidation step to reduce the inference\ncomplexity (the amount of prediction API calls) using a combined prompt:\n$a = M(p_{ans}, q, \\langle D_0, S_0\\rangle, \\langle D_t, S_t\\rangle)$.\nWhen $t = 1$, the initial passages will be fed into the model directly for knowledge consolidation and\nsubsequent answering: $a = M(p_{ans}, q, \\langle D_0, S_0\\rangle)$."}, {"title": "4. Experiments", "content": "We evaluate the effectiveness of ASTUTE RAG on overcoming imperfect retrieval augmentation and\naddressing knowledge conflicts. In this section, we first introduce the experiment setting in detail"}, {"title": "4.1. Experimental Settings", "content": "Datasets and metrics. We conduct experiments on the data collected in Section 2 consisting of\ndata from NQ, TriviaQA, BioASQ, and PopQA. For each instance from these datasets, we provide 10\npassages collected under a realistic retrieval setting: for each question in our benchmark, we query\nGoogle Search to retrieve the top 30 results and select the first 10 accessible websites. From each\nretrieved website, we extract the paragraph corresponding to the snippet provided in Google Search\nresults as the retrieved passage.. Most of the retrieval results contains natural noise with irrelevant\nor misleading information. We do not consider enhancements to the retrieval side, such as query\nrewriting, as such enhancements are typically already incorporated into commercial information\nretrieval systems. Notably, we do not select questions or annotate answers based on the retrieval\nresults. This setting allows us to analyze the severity of imperfect retrieval in real-world RAG. It\ndistinguishes our benchmark from previous ones that employ synthetic retrieval corruptions or that\nunintentionally reduce the frequency of imperfect retrieval with biased construction protocols (Chen\net al., 2024a; Yang et al., 2024). We also evaluate our method on RGB (Chen et al., 2024a), a RAG\ndiagnostic benchmark evaluating several crucial RAG abilities. Specifically, we choose the English\nsubset of RGB focusing on noise robustness. The benchmark have positive and negative passage sets\nfor each question. We select five negative documents per question as the context to form a worst-case\nscenario. All the data in these datasets are short-form QA. Following previous work (Mallen et al.,\n2023; Wei et al., 2024; Xiang et al., 2024), a model response is considered correct if it contains the\nground-truth answer. To enhance evaluation reliability, we prompt LLMs to enclose the exact answer\nwithin special tokens, extracting them as the final responses.\nGeneral Settings of LLMs and RAG. We conduct experiments on two advanced LLMs, including Gem-\nini 1.5 Pro\u2075 (gemini-1.5-pro-002) and Claude 3.5 Sonnet (claude-3-5-sonnet@20240620).\nThe generation temperature is set to 0 and the maximum output tokens is set to 1,024, if not\nspecified otherwise. By default, the passages are presented in the prompt by reversed order. All"}, {"title": "4.2. Main Results", "content": "Table 1 and Table 2 presents the results on data with realistic retrieval augmentation for each dataset.\nBy comparing RAG and No RAG, we find that retrieved passages might not always bring benefits to\ndownstream performance \u2013 on NQ and TriviaQA, RAG performance lags behind No RAG. We attribute\nthis to that the questions being covered by the LLM's internal knowledge and the noise in retrieval"}, {"title": "4.3. Analyses", "content": "Performance by retrieval precision. We compare the performance of Astute RAG and baselines\nacross different subsets partitioned by their retrieval precision, on our collected data with Claude as the\nLLM. As shown in Figure 4, As TUTE RAG achieves consistently better performance than all baselines\nacross different retrieval precision, indicating its effectiveness in improving RAG trustworthiness in\nbroad scenarios. Notably, AsTUTE RAG does not sacrifice performance gain under high retrieval\nquality in exchange for improvement under low retrieval quality. When the retrieval quality is\nextremely low (close to zero retrieval precision), all other RAG variants underperforms the 'No RAG'\nbaseline, except for the proposed Astute RAG. This observation aligns with the worst-case results\non RGB. It demonstrates the difficulty in overcoming imperfect retrieval augmentation, and verify the\neffectiveness of ASTUTE RAG in doing so.\nEffectiveness in addressing knowledge conflicts. We split our collected data in to three subset\naccording to the answers from Claude, with and without RAG. The answers from two inference\nmethods can be both correct, both incorrect, or conflicting with one being correct. These three subsets\nrepresents the three situations between internal and external knowledge. The results are shown\nin Figure 4. On the conflicting subset, Astute RAG successfully chooses the correct answer in\napproximately 80% of cases, being the most effective method in addressing knowledge conflicts.\nNotably, ASTUTE RAG even brings performance improvement on the subset where neither internal\nnor external knowledge alone leads to the correct answer. This indicates that AsTUTE RAG can\neffectively combine partially-correct information from LLM-internal and external knowledge, to"}, {"title": "5. Related Work", "content": "Retrieval augmented generation (RAG) seeks to address the inherent knowledge limitation of LLMs\nwith passages retrieved from external sources of information such as private corpora or public\nknowledge bases (Borgeaud et al., 2022; Guu et al., 2020; Lewis et al., 2020). Given the widespread\nadoption of RAG in various real-world applications, including risk-sensitive domains, the negative\nimpact of noisy information within retrieved passages has garnered increasing attention (Cuconasu\net al., 2024). Recent work has sought to enhance the robustness of RAG systems against noise from\nvarious perspectives, including training LLMs with noisy context (Fang et al., 2024; Pan et al., 2024;\nYoran et al., 2024; Yu et al., 2023b), training small models to filter out irrelevant passages (Wang\net al., 2023c; Xu et al., 2023), passage reranking (Glass et al., 2022; Yu et al., 2024), dynamic and\niterative retrieval (Asai et al., 2023; Jiang et al., 2023; Yan et al., 2024), query rewriting (Ma et al.,"}, {"title": "6. Conclusion", "content": "Our paper investigates the impact of imperfect retrieval on the performance of RAG systems and\nidentifies knowledge conflicts as a key challenge. To address this, we introduce Astute RAG, a\nnovel approach that leverages the internal knowledge of LLMs and iteratively refines the generated\nresponses by consolidating internal and external knowledge in a source way. Our empirical results\ndemonstrate the effectiveness of ASTUTE RAG in mitigating the negative effects of imperfect retrieval\nand improving the robustness of RAG systems, particularly in challenging scenarios with unreliable\nexternal sources.\nAmong the limitations, Astute RAG's effectiveness hinges on the capabilities of advanced LLMs\nwith strong instruction-following and reasoning abilities, hence potentially more limited applicability\nwith less sophisticated LLMs. As an important future direction, extending the experimental setup to\ninclude longer outputs would be important, where the challenges of imperfect retrieval and knowledge\nconflicts may be even more pronounced. Furthermore, a comprehensive analysis of the impact of\nvarious context types (Balachandran et al., 2024) would enhance the understanding of the proposed\nmethod's effectiveness. Future work can also extend our method beyond LLMs and RAG, such as\naddressing knowledge conflicts in multimodal settings (Zhu et al., 2024)."}, {"title": "A. Prompt Template for ASTUTE RAG", "content": "Generate a document that provides accurate and relevant information to answer the given\nquestion. If the information is unclear or uncertain, explicitly state 'I don't know' to avoid any\nhallucinations.\nQuestion: {question} Document:\nTask: Consolidate information from both your own memorized documents and externally\nretrieved documents in response to the given question.\n* For documents that provide consistent information, cluster them together and sum-\nmarize the key details into a single, concise document.\n* For documents with conflicting information, separate them into distinct documents, ensuring\neach captures the unique perspective or data.\n* Exclude any information irrelevant to the query.\nFor each new document created, clearly indicate:\n* Whether the source was from memory or an external retrieval.\n* The original document numbers for transparency.\nInitial Context: {context}\nLast Context: {context}\nQuestion: {question}\nNew Context:\nTask: Answer a given question using the consolidated information from both your own\nmemorized documents and externally retrieved documents.\nStep 1: Consolidate information\n* For documents that provide consistent information, cluster them together and summarize\nthe key details into a single, concise document.\n* For documents with conflicting information, separate them into distinct documents, ensuring\neach captures the unique perspective or data.\n* Exclude any information irrelevant to the query.\nFor each new document created, clearly indicate:\n* Whether the source was from memory or an external retrieval.\n* The original document numbers for transparency.\nStep 2: Propose Answers and Assign Confidence\nFor each group of documents, propose a possible answer and assign a confidence score based\non the credibility and agreement of the information.\nStep 3: Select the Final Answer\nAfter evaluating all groups, select the most accurate and well-supported answer.\nHighlight your exact answer within  your answer .\nInitial Context: {context_init}\n[Consolidated Context: {context}] # optional\nQuestion: {question}\nAnswer:"}, {"title": "B. Data Collection", "content": "Encompassing a diverse range of natural questions, our benchmark consists of realistic retrieval results\nwith Google Search\u2076 as the retriever and the Web as the corpus. Notably, we do not select questions\nor annotate answers based on the retrieval results. This setting allows us to analyze the severity of\nimperfect retrieval in real-world RAG. It distinguishes our benchmark from previous ones that employ\nsynthetic retrieval corruptions or that unintentionally reduce the frequency of imperfect retrieval\nwith biased construction protocols (Chen et al., 2024a; Yang et al., 2024). Overall, our benchmark\ncontains 1,042 short-form question-answer pairs, each paired with 10 retrieved passages.\nQuestion-answer pairs. We consider question-answer pairs from four datasets of different\nproperties spanning across general questions, domain-specific questions, and long-tail questions. NQ\n(Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017) are two widely-studied question-answering\n(QA) datasets in general domains. BioASQ (Tsatsaronis et al., 2015) is from biomedical domain\nthat has demonstrated significant benefits from RAG when general-purpose LLMs are considered.\nPopQA (Mallen et al., 2023) focuses on long-tail knowledge and has been shown to be challenging\nfor even advanced LLMs to solve without external knowledge. All these datasets contain questions\nwith short-form answers and most of them list all valid answer variants. This format can support\nautomatic verification of answer appearance in retrieved passages and model responses, leading to\nmore precise evaluations."}, {"title": "Retrieval process", "content": "For each question in our benchmark, we query Google Search to retrieve the\ntop 30 results and select the first 10 accessible websites. From each retrieved website, we extract the\nparagraph corresponding to the snippet provided in Google Search results as the retrieved passage.\nWe do not consider enhancements to the retrieval side, such as query rewriting, as such enhancements\nare typically already incorporated into commercial information retrieval systems."}]}