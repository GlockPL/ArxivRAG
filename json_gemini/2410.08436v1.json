{"title": "Exploring the Role of Reasoning Structures for Constructing Proofs in Multi-Step Natural Language Reasoning with Large Language Models", "authors": ["Zi'ou Zheng", "Christopher Malon", "Martin Renqiang Min", "Xiaodan Zhu"], "abstract": "When performing complex multi-step reasoning tasks, the ability of Large Language Models (LLMs) to derive structured intermediate proof steps is important for ensuring that the models truly perform the desired reasoning and for improving models' explainability. This paper is centred around a focused study: whether the current state-of-the-art generalist LLMs can leverage the structures in a few examples to better construct the proof structures with in-context learning. Our study specifically focuses on structure-aware demonstration and structure-aware pruning. We demonstrate that they both help improve performance. A detailed analysis is provided to help understand the results.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have played an essential role in a wide range of applications (Nori et al., 2023; Savelka et al., 2023; Wang et al., 2023; Qin et al., 2023) including intelligent agents (Liu et al., 2023; Cheng et al., 2022). Their ability to perform complex multi-step reasoning has become critical (Wei et al., 2022; Kojima et al., 2022; Yao et al., 2023; Besta et al., 2023; Lei et al., 2023; Dalvi et al., 2021; Ribeiro et al., 2023; Saparov and He, 2023). In complex multi-hop reasoning tasks, the proof steps often form a graph but not just a chain. The capability to construct correct, structured proofs is essential for ensuring that LLMs perform the desired reasoning and important for the explainability of the reasoning models (Dalvi et al., 2021; Ribeiro et al., 2023). In this paper, we perform a focused study, providing evidence to understand whether the state-of-the-art LLMs can leverage a few examples to better construct the proof structure with in-context learning. Unlike previous work that fine-tunes the"}, {"title": "2 Related Work", "content": "Recently, LLMs' reasoning ability has been significantly improved. Chain-of-thought (CoT) (Wei et al., 2022; Kojima et al., 2022) is arguably the simplest but effective way to elicit linear reasoning chains of LLMs. Tree-of-thought (ToT) (Yao et al., 2023) can further provide deeper insights into the model's reasoning structures. However, ToT has been applied to tasks such as game-of-24 and creative writing, but not to natural language entailment and reasoning tasks with complex proof structures. In this paper, we will compare our models to the CoT and TOT models. Reasoning in natural language has been a central topic of artificial intelligence research since its inception, including the research in natural language inference (Dagan et al., 2005; Bowman et al., 2015; Chen et al., 2017, 2018; Feng et al., 2022). In addition to producing accurate results, another key"}, {"title": "3 Method", "content": "Given a hypothesis h and context C consisting of evidence sentences, the objective of the task is to provide a proof graph G to prove h based on some of the evidence sentences in context C, if h can be proven. In entailment or multiple-choice question-answering tasks, h is often a concatenation of a question q and a candidate answer, or sometimes a paraphrase of such a concatenation. Since we will use both q and h to retrieve demonstrations and propose reasoning steps, both q and h will be included in our input. As a result, each instance u in the task is a tuple u = (q, h, C). A proof database D, containing examples of structured proofs, is provided for searching demonstrations that will be used in in-context learning. These demonstrations are exemplars provided to LLMs to help them understand the task requirements and output format.\nFormally, we denote \\(p_{\\theta}\\) to be a pre-trained language model with parameter \\(\\theta\\). Suppose \\(x = (x_1,...,x_n)\\) is a language sequence with n tokens, the probabilistic language model can be written as \\(P_{\\theta}(x) = \\prod_{i=1}^{n} P_{\\theta}(x_i | x_1,...,x_{i-1})\\). Following Yao et al. (2023), we use \\(\\mathbb{P}_{\\theta}(y|prompt(x))\\) to represent \\(p_{\\theta}(y|prompt(x))\\), where prompt(x) is the input sentences x wrapped with the prompt instructions and templates; y is the output.\nIn this paper, we hypothesize that the proof structures of similar examples can help LLMs to construct a structured proof for the target problem. In particular, we consider two key components that can utilize the known proof structures: demonstration and proof-path pruning. The overall architecture of our model is depicted in Figure 1 and Algorithm 1.\nAt the high level, to support our study, this paper"}, {"title": "4 Experiment Set-Up", "content": "Datasets. We perform experiments on three benchmark datasets, EntailmentBank (Dalvi et al., 2021), AR-LSAT (Ribeiro et al., 2023) and ProntoQA (Saparov and He, 2023). Details can be found in Appendix B.\nEvaluation Metrics. We evaluate the predicted proof graph \\(G_p\\) against the golden graph \\(G_g\\) using the following metrics: F1 ov over evidence (Ev-F) (Dalvi et al., 2021), F1 over proof (Pr-F) (Dalvi et al., 2021), and reasoning Graph Similarity (G Sim) (Ribeiro et al., 2023). Details can be found in Appendix E.\nBaselines. We compare the proposed method with CoT, self-consistency CoT (CoT-sc), ToT, and reasoning-via-planing (RAP) (Hao et al., 2023). Each model is prompted with three demonstrations. Details can be found in Appendix C."}, {"title": "5 Experiment Results", "content": "We conducted experiments on the representative closed-source (GPT-3.5/GPT-4) and open-source LLMs (Llama-2-70B/Llama-3-70B). Table 1 shows that our models outperform baseline models across the three datasets under different evaluation metrics. The improvements of the proposed model over baselines are statistically significant (p < 0.05) under one-tailed paired t-test. Note that the improvement is less in PrOntoQA, which is due to the fact that a larger percentage of data in PrOntoQA has linear reasoning patterns. Detailed results are included in Appendix F.\nEffect of Proof Structure. To further understand the effect of proof structures of given examples, we conduct more experiments on EntailmentBank. Table 2 shows the effectiveness of different components of our model. Particularly, our focus is on the variants without structure-aware pruning (\"w/o prun.\") and without structure-aware demonstration (\"w/o demon.\"). We can see that the structure information contributes to the performance (Ev-F and G Sim scores dropped without them.). The comparison involving other variants of our model, specifically concerning the hint module and pruning strategies, is detailed in Table 12 in the Appendix. The bottom part of Table 2 focuses on"}, {"title": "6 Conclusion", "content": "Enabling LLMs to generate their proof structure is critical for the reliability and explainability of such models. By incorporating structure-aware components into the state-of-the-art LLMs, we demonstrate that LLMs can benefit from utilizing the given proof structures of similar examples. We find that measuring the gap between the intermediate steps and the final hypothesis can help narrow down the search space and enhance the performance. Further analysis of sequential and non-sequential reasoning reveals that our model offers greater advantages in the more complex task of non-sequential reasoning."}, {"title": "Limitations", "content": "Our proposed method is primarily designed for the natural language reasoning task, especially the task requiring multi-step proof to obtain the final conclusion. We do not test our method on other types of reasoning, e.g. mathematical reasoning and our method is only tested on the English reasoning dataset.\nOne limitation, as mentioned in the paper, is the increased token usage with the potential reasoning branches exploration since the system uses LLM-as-a-service API. Although we apply the beam search strategy over the graph which needs less exploration compared to the naive breadth-first search, the overall cost is still high. We also leverage LLM in several modules in the system, which increases the total API calls as well.\nAnother limitation is that the current system does not consider the negation proof or the conclusion that cannot be reached. The goal of the current system is to design a system that provides better proof. Proof by negation and other kinds of reasoning, e.g. conjunction, disjunction and conditionals, could be extended in future work."}, {"title": "A Preliminary Experiments", "content": "We conduct two preliminary experiments on the dev set of EntailmentBank with GPT-3.5. For the Preliminary Experiment I, we provide all other proofs except for randomly deleting two pieces of evidence. We conduct three deletion strategies: two missing pieces of evidence are in the same subtree and the same reasoning step, in the same subtree but not the same reasoning step, or in a different subtree. Here, we set the depth of the subtree to 2. Specifically, \"the same subtree and the same reasoning step\" means the two missing pieces of evidence can together form an intermediate conclusion in the proof tree, while \u201cthe same subtree but different reasoning step\u201d means that the intermediate conclusion from one missing piece of evidence could be combined with the other missing evidence to obtain another intermediate conclusion. \"A different subtree\" means the two missing pieces of evidence are not in the same 2-depth subtree. Results in Table 4 show that it is easier for the model to find evidence when they are located in a different proving subtree. We further mimic the practical searching scenario in the Preliminary Experiment II, where given one chosen reasoning step, e.g. sent4 & sent5 \u2192 int1, and missed two different reasoning step among which one is based on the given intermediate conclusion (reuse_ic) and the other (div) is not, e.g. sent3 & int\u2081 \u2192 int2 and sent\u2081 & sent2\u2192 int3, we ask the model to provide the prediction of the reasoning step. Table 5 shows that div model outperforms reuse_ic and thus we apply div in the main experiment."}, {"title": "B Dataset", "content": "EntailmentBank (Dalvi et al., 2021) not only lists the supporting textural evidence but also offers a hierarchical tree structure showing how the"}, {"title": "C Implementation Details", "content": "We retrieve 5 times independently and take the union set as the result of the retrieval component. For each step, we propose 3 potential reasoning steps at each node and we keep the beam size as 3 in the breadth-first search. The number of demon-"}, {"title": "D Algorithms", "content": "The overall algorithm for the proposed method is described in Algorithm 1, and searching structure-aware demonstration and structure-aware pruning can be found in Algorithm 2 and Algorithm 3 respectively. Specifically in Algorithm 2, we use GATv2 as the graph encoder for GNN(\u00b7) and compute cosine similarity score for sim(\u00b7)."}, {"title": "E Evaluation Metrics", "content": "We evaluate the predicted proof graph \\(G_{pred}\\) against the golden graph \\(G_{gold}\\) with three metrics, describing evidence, proof and graph similarity. Unlike previous work, we target the model's ability to provide correct proofs more than the true or false result.\nEvidence. Following (Dalvi et al., 2021), we perform an evaluation over the chosen evidence to check whether the predicted proof graph uses the correct evidence. Suppose \\(E_{pred}\\) and \\(E_{gold}\\) are the selected evidence set for the predicted proof graph \\(G_{pred}\\) and the golden graph \\(G_{gold}\\) respectively. We compute precision (Ev-P), recall (Ev-R) and F1 (Ev-F) score by comparing \\(E_{pred}\\) and \\(E_{gold}\\) and taking average over the examples.\nProof Following (Dalvi et al., 2021), we evaluate over individual reasoning steps to check whether the predicted proof graph is structurally correct. Suppose \\(P_{pred}\\) and \\(P_{gold}\\) are the reasoning step set for the predicted proof graph \\(G_{pred}\\) and the golden graph \\(G_{gold}\\) respectively. We compute precision"}, {"title": "F Additional Results", "content": "Table 6 and Table 7 shows the additional results of precision (Ev-P/Pr-P), recall (Ev-R/Pr-R) of evidence and proof on GPT-3.5, GPT-4, Llama-2-70B and Llama-3-70B. The improvements of the proposed model over baselines are statistically significant (p < 0.05) under one-tailed paired t-test. For example, the p-values of Ev-F and Pr-F (our method vs RAP on ProntoQA) are 0.0247 and 0.0312 for GPT-3.5, while the values are 0.0404 and 0.0388 for GPT-4. Table 8 and Table 9 shows the additional ablation results on GPT-3.5. In Table 8 and Table 2, in w/o prun., we do not prune the 'proof steps' based on the structure, while in w/o demon., we use three fixed demonstrations instead. In Table 9 and Table 2, the 'sim' variant searches demonstrations only based on the context similarity, without being aware of the structure. Table 10 shows the sequential/non-sequential reasoning on GPT-4. Table 11 shows the proportion of examples where the gold proof is a subset of the predicted proof steps, i.e., the proportion of examples where the per-proof recall is 1. This is a stricter metric than the Pr-F1, but it is valuable as it provides insight into the ability of generalist LLMs to produce human-thought correct proofs under different models.\nRegarding the guessed structure, we manually examined 20 randomly selected examples (The randomly selected cases are 15, 23, 36, 97, 114, 118, 139, 142, 154, 165, 172, 210, 213, 223, 247, 271, 306, 336, 339, 353 in the EntailmentBank). We found that 8 examples could provide the correct structure in the first guess and the correct number"}, {"title": "G Other Variants", "content": "Table 12 shows the analysis with other variants of our model. The reuse_ic variant requires the model to reuse the intermediate conclusion generated in the previous iteration in the 2nd iteration's reasoning, while div variant forces the model to explore the reasoning step from the untouched premises. The w/o hint includes all modules except the proof hint generation module. We modify the prompt in this module into asking the model what is the next step of reasoning in what's next. Our findings indicate that the div variant has higher performance than the reuse_ic and w/o pruning variant, showcasing the effectiveness of the structure-aware pruning."}, {"title": "H Examples", "content": "Proof Hint Generation. Table 13 shows two examples with generated hints in the first iteration, and we conduct a comparison between the model with or without the proof hint generation module. In the first example, both models could make the correct reasoning in the first iteration and the intermediate conclusion finds out that carbon dioxide is required photosynthesis process. Without the proof hint generation module, the model could not retrieve the wanted sentences, while with the proof hint generation module, the model succeeds in focusing on the missing relationship with 'step'. Similarly, in the second example, both models could correctly retrieve sent6. However, with the proof hint generation module, the model cares more about what the Earth revolve around, not the moon. The examples show that the proof hint generation module explicitly asks the model to think about the missing part between the current intermediate conclusion and the final goal and the model could retrieve relevant information based on this action.\nStructure-aware Demonstration. Table 14 shows the example with structure-aware demonstrations. For the page limit, we only show the proof structure of one demonstration in the table. We observe that the model is prone to providing the proof that is structurally similar"}, {"title": "I Computation Cost", "content": "We observe that the cost of experimenting is higher than the baselines. We leverage the language model in several different modules and apply the beam search strategy in the breadth-first search. We keep a most promising states per step and b beams of candidates with the highest evaluation score for"}, {"title": "J Example Prompts", "content": "We provide three demonstrations in all few-shot models, but we only show one in the example in this section."}, {"title": "J.1 Prompt for Candidate Retrieval", "content": "System: Below, you are given a question, a hypothesis and a set of candidate premises. You are required to select a small set of candidates (at least provide 3 sentences) to deduce the hypothesis. Please only filter out the sentences that you are sure of."}, {"title": "J.2 Prompt for Reasoning Step Proposal", "content": "System: Provide me several sentences with the sentence number and one intermediate conclusion that are possible to be used in the next step in this small set. If the deduction reaches the hypothesis, tell me 'Finish'; otherwise please provide the (intermediate) conclusion."}, {"title": "J.3 Prompt for Reasoning Step Evaluation", "content": "System: Evaluate whether these intermediate conclusions could reach the hypothesis with candidates. Provide me the number of possibilities (0-99) of these intermediate conclusions: Surely: 85-99, Likely: 50-84, Impossible: 0-49"}, {"title": "J.4 Prompt for Proof Hint Generation", "content": "System: Compare the intermediate conclusion with the hypothesis and the question, and provide me one sentence of what is still missing."}, {"title": "\\(C_s = \\{\\phi(z_i)\\}^k_{i=1}\\)", "content": null}, {"title": "\\(z_i \\sim P_{\\theta}^{Retrieve}(z | q,h,C,m,\\varepsilon)\\)", "content": null}, {"title": "\\(r_i \\sim P_{\\theta}^{Propose}(r | q,h,C_s,\\varepsilon)\\)", "content": null}, {"title": "\\(s_i \\sim P_{\\theta}^{eval}(s|r,\\varepsilon)\\)", "content": null}, {"title": "\\(m_i \\sim P_{\\theta}^{Compare}(m | h, r?)\\)", "content": null}, {"title": "\\(\u03b4 (G_p, G_g) \\)", "content": null}, {"title": "\\(sim (G_p, G_g) = 1 - \\frac{\\delta (G_p, G_g)}{max(|N_p| + |E_p|, |N_g| + |E_g|)}\\)", "content": null}]}