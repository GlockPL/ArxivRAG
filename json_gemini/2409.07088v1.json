{"title": "Ontology-Free General-Domain Knowledge Graph-to-Text Generation Dataset Synthesis using Large Language Model", "authors": ["Daehee Kim", "Deokhyung Kang", "Sangwon Ryu", "Gary Geunbae Lee"], "abstract": "Knowledge Graph-to-Text (G2T) generation involves verbalizing structured knowledge graphs into natural language text. Recent advancements in Pretrained Language Models (PLMs) have improved G2T performance, but their effectiveness depends on datasets with precise graph-text alignment. However, the scarcity of high-quality, general-domain G2T generation datasets restricts progress in the general-domain G2T generation research. To address this issue, we introduce Wikipedia Ontology-Free Graph-text dataset (WikiOFGraph), a new large-scale G2T dataset generated using a novel method that leverages Large Language Model (LLM) and Data-QuestEval. Our new dataset, which contains 5.85M general-domain graph-text pairs, offers high graph-text consistency without relying on external ontologies. Experimental results demonstrate that PLM fine-tuned on WikiOFGraph outperforms those trained on other datasets across various evaluation metrics. Our method proves to be a scalable and effective solution for generating high-quality G2T data, significantly advancing the field of G2T generation.", "sections": [{"title": "1 Introduction", "content": "Knowledge Graph-to-Text Generation (G2T) is a task aimed at verbalizing knowledge graphs represented as a set of triplets in the form of (subject, predicate, object) into natural language text (Gatt and Krahmer, 2018; Lin et al., 2024). Recent advancements in Pretrained Language Models (PLMs), such as T5 (Raffel et al., 2020) and BART (Lewis et al., 2020) have led to significant improvements in G2T when fine-tuned on G2T datasets (Kale and Rastogi, 2020).\nHowever, fine-tuning PLMs requires a substantial amount of well-aligned G2T data (Kasner et al., 2023). Constructing G2T datasets is labor-intensive and expensive because understanding structured graph representations for various natural language sentences is challenging for human annotators. Therefore, previous research has mainly focused on small-scale domain-specific datasets (Gardent et al., 2017; Koncel-Kedziorski et al., 2019; Castro Ferreira et al., 2020; Nan et al., 2021).\nTo address this issue, researchers have attempted to automatically align Wikipedia texts with their corresponding graphs from ontologies to construct large-scale, general-domain G2T datasets (Elsahar et al., 2018; Jin et al., 2020; Agarwal et al., 2021; Wang et al., 2021; Mousavi et al., 2024). However, ontology-based datasets often suffer from graph-text misalignment, making it challenging to generate text accurately. For example, in Figure 1 (b), the phrase \"from 1899 to 1965 on the Strathspey railway\" cannot be inferred solely from the graph shown in Figure 1 (a). Such discrepancy can be a major cause of the decline in G2T performance (Mousavi et al., 2024).\nRecent advances in Large Language Models (LLMs) offer promising potential to address these challenges. Many studies have successfully leveraged LLMs to synthesize high-quality data for various tasks (Long et al., 2024). Several studies have attempted to generate graph-text paired data using LLMs (Josifoski et al., 2023; Han et al., 2024; Chen et al., 2024), but these attempts have yet to be thoroughly explored for the G2T generation task.\nTo address this issue, we introduce an effective method for generating high-quality G2T dataset that integrates LLM with Data-QuestEval (Rebuffel et al., 2021). We select three examples from the human-crafted dataset to facilitate In-Context Learning (ICL) for the LLM, enabling it to extract graph representations from the given sentences. To ensure high consistency between the extracted graph representations and text, we utilize Data-QuestEval for data curation. Through this method, we create a new dataset called Wikipedia Ontology-Free Graph-Text dataset (WikiOFGraph), a 5.85M G2T data that covers a broad range of Wikipedia articles without relying on ontologies.\nThrough comprehensive analyses, we demonstrate that our dataset achieves graph-text consistency comparable to that of a fully human-crafted dataset while significantly surpassing other ontology-based general-domain G2T datasets. Experimental results demonstrate that a PLM fine-tuned on WikiOFGraph outperforms those trained on existing datasets across the human expert-crafted GenWiki (Jin et al., 2020) test set and LLM-synthesized WikiOFGraph test set. This highlights the suitability of our dataset for building G2T systems that perform well across general domains, making it more effective than other datasets. We further demonstrate the effectiveness of Data-QuestEval filtering through additional experiments. In summary, our key contributions are as follows:\n(i) We introduce an effective method for synthesizing high-quality G2T dataset. Our approach is independent of proprietary LLMs or ontologies, making it reproducible and easily adaptable for various domains.\n(ii) We release a new G2T generation dataset called WikiOFGraph. Our comprehensive analyses indicate that it offers high graph-text consistency, comparable to a human-crafted dataset, with 5.85M samples covering the entire spectrum of Wikipedia.\n(iii) We demonstrate the effectiveness of Data-QuestEval filtering through additional experiments and case study."}, {"title": "2 Background and Related Work", "content": "Graph-to-Text generation Traditionally, researchers tackled G2T generation using templates designed to expressing predicates into pre-defined statements (Wiseman et al., 2018; Kasner and Dusek, 2022; Xiang et al., 2022; Vejvar and Fujimoto, 2023). Template-based approaches have low hallucination rates but are labor-intensive in creating various templates, and they often struggle with producing fluent sentences from complex graphs (Kasner and Du\u0161ek, 2020).\nTo address these limitations, researchers have leveraged neural encoder-decoder architectures (Wiseman et al., 2017; Beck et al., 2018; Nie et al., 2018; Puduppully et al., 2019; Iso et al., 2019) which convert graph representations into vector embeddings interpretable by neural models. Unlike previous approaches, these approaches leverage an end-to-end G2T generation paradigm that does not require pre-defined templates.\nThe introduction of transformer (Vaswani et al., 2017)-based PLMs has significantly advanced G2T generation, leading to much better performance than earlier methods. Kale and Rastogi demonstrated substantial improvements by fine-tuning these PLMs on G2T generation tasks. This strategy has since been adopted in subsequent studies (Ribeiro et al., 2021; Jolly et al., 2022; Mehta et al., 2022; Han and Shareghi, 2022). However, this new paradigm requires substantial amounts of well-aligned G2T data.\nG2T generation datasets WebNLG (Gardent et al., 2017; Castro Ferreira et al., 2020) is a fully human-crafted dataset based on the DBpedia (Mendes et al., 2012), making it a representative benchmark for G2T generation tasks due to its precise graph-text alignment. However, creating human-crafted datasets is resource-intensive and limits scale. These limitations on scale and diversity have led researchers to develop large-scale G2T datasets automatically.\nGenWiki (Jin et al., 2020) provides a large-scale, general-domain G2T dataset with 1.3 M samples. It includes fine and full splits, determined by the F1-score-based alignment between text and graph entities, and 1,000 human-crafted test samples. TekGen (Agarwal et al., 2021) combines Wikipedia and Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) to create a large-scale, general-domain G2T dataset using a distant supervision strategy. This approach allowed for the creation of large datasets but led"}, {"title": "3 WikiOFGraph", "content": "In this section, we introduce a method for creating WikiOFGraph. We first define the requirements necessary to address the limitations of existing G2T datasets (\u00a73.1). We then outline the rule-based algorithm for gathering source sentences from Wikipedia (\u00a73.2), as well as the process of extracting graphs through LLM (\u00a73.3). Additionally, we describe our approach to utilizing Data-QuestEval (Rebuffel et al., 2021) for curating well-aligned graph-text pairs (\u00a73.4).\n3.1 Requirements\nGraph-text consistency Good G2T data guarantees that the target text includes \"all\" and \"only\" information from the graph. If \"all\" information of the graph is not reflected in the target text, it can cause data omission problems for the model. If the target text does not includes \"only\" information from the graph, it can cause hallucination problems for the model. Therefore, graph-text consistency is essential to prevent omission and hallucination problems in the G2T generation task.\nDomain diversity PLMs fine-tuned on datasets limited to a specific domain often struggle to handle samples from unseen domains (Keymanesh et al., 2022). This issue becomes apparent when PLMs are tested on new domains, where performance falls short compared to familiar domains (Castro Ferreira et al., 2020). Therefore, including a wide range of domains is crucial to developing a domain-adaptive G2T generation system.\nLarge scale The demand for substantial structured datasets is growing with the emergence of language models trained on extremely large-scale structured data (Zhuang et al., 2024; Li et al., 2024). High-quality structured data, such as knowledge graph representations, is essential for training LLMs for structured data processing. However, creating such data is far more challenging than compiling unstructured corpora. Consequently, providing a large-scale G2T dataset could be key to advancing LLMs in handling structured data effectively.\n3.2 Source Sentences Collection\nTo meet the requirements (\u00a73.1), we select Wikipedia as the source of sentences. Wikipedia includes sentences with extensive factual knowledge from diverse domains. We employ a rule-based algorithm to extract the first sentence from every article on English Wikipedia because the first sentence of a Wikipedia article often encapsulates vital factual information. We apply several constraints to enhance the clarity of the factual context in the source sentences. We limit the length of sentences to 10-500 characters, exclude sentences starting with pronouns to avoid ambiguous entity extraction (e.g., 'it,' 'that'), and remove parenthetical explanations to prevent redundant expressions. Through this process, we collect 6.06M source sentences, denoted as \\(Y = {y_i}_{i=1}^n\\) in Figure 2, where n represents the total number of source sentences.\n3.3 Graph Extraction\nWe utilize the \"Llama-3-70b-instruct-awq\" (Dubey et al., 2024) to extract graph representations directly from \\(Y\\). We manually select three examples from the WebNLG (Castro Ferreira et al., 2020) dataset for in-context examples. We then obtain a set of graph representations for each sentence, denoted as \\(X = \\{ \\{ (S_{ij}, P_{ij}, O_{ij}) \\}_{j=1}^{m_i} | X_i \\}_{i=1}^n\\) in Figure 2. Here, \\(n\\) represents the total number of sentences, \\(m_i\\) denotes the number of triplets associated with the i-th sentence, and \\((s_{ij}, P_{ij}, O_{ij})\\) are the subject, predicate, and object of the j-th triplet in the i-th sentence, respectively. Through this process, we obtain 6.06M pairs of source sentences and their corresponding generated graph representations.\n3.4 Data-QuestEval Filtering\nData-QuestEval (Rebuffel et al., 2021) is a framework that measures the accuracy of predicted texts in Data-to-Text generation tasks by combining question generation and question-answering techniques to evaluate whether the predicted sentence accurately reflects the content of the source data. Data-QuestEval supports referenceless evaluation by relying solely on the source data and the predicted sentence. We adapt referenceless evaluation by using the \\(X\\) as the source data and the \\(Y\\) as the predicted sentence.\nWe apply the referenceless Data-QuestEval scoring \\(f(x_i, y_i)\\) to evaluate the consistency between the generated graph representations and their corresponding source sentences. Only pairs with a score of \\(f(x_i, Y_i) \\geq 0.3\\) are retained for further curation, as denoted by \\(D = \\{ (x_i, Y_i) | f(x_i, Y_i) \\geq 0.3 \\}_{i=1}^{n'}\\) in Figure 2, where n' represents the total number of curated graph-text pairs. Through this process, we obtain 5.95M samples, with less than 2% of the samples being filtered out. This result implies that a robust LLM can reliably extract graph representations containing \"all\" and \"only\" information from given texts."}, {"title": "4 Data Analysis", "content": "We compare WikiOFGraph with existing datasets. We explore the scale and domain diversity of the data through quantitative analysis (\u00a74.1). We then assess the graph-text consistency using human evaluators and GPT-40 (OpenAI et al., 2024) (\u00a74.2).\nWe utilize four representative G2T datasets for our comparative analysis: human-crafted WebNLG (Castro Ferreira et al., 2020) and three automatically generated ontology-based datasets-GenWiki (Jin et al., 2020), TekGen (Agarwal et al., 2021), and LAGRANGE (Mousavi et al., 2024). Details of the datasets are provided in the appendix (\u00a7Appendix D).\n4.1 Quantitative Analysis\nDomain diversity & dataset volume Table 1 provides a detailed comparison of statistics across various G2T datasets. WikiOFGraph includes a remarkably higher number of unique predicates and unique entities than other datasets. This diversity in predicates and entities is anticipated to enhance domain generalization in G2T generation tasks. In addition, WikiOFGraph is advantageous in terms of scale, containing a substantial 5.85 M data pairs, which makes it a valuable resource for future research in G2T generation tasks.\nIn contrast, WebNLG contains the smallest number of unique predicates, entities, and samples. This limitation is primarily due to its entirely human-crafted nature, which makes it challenging to produce a large volume of data.\nGenWiki and LAGRANGE also have fewer unique predicates and entities. This is because the underlying ontology graphs in these datasets cover a relatively narrow scope. Also, these datasets are designed to support G2T and T2G generation tasks, limiting the variety of available predicate expressions.\nStatistical comparison of dataset compositions\nTo provide more detailed comparisons, we conduct a statistical analysis of these datasets. Figure 3 shows the distribution of samples categorized by the number of triplets, while Figure 4 presents the distribution of samples categorized by word count in increments of five. WikiOFGraph demonstrates a balanced and consistent dataset structure, with the distribution of triplets and word counts closely aligning. The similarity in the shapes of these two distributions, which differs from those of other datasets, suggests a consistent alignment between the graph and the target text.\nDespite TekGen showing good alignment between triplet and word count distributions, most of its samples consist of two or fewer triplets or contain fewer than ten words, indicating that most samples are relatively short. GenWiki predominantly consists of samples with three or fewer triplets; however, the relatively high proportion of samples with more than 20 words suggests a surface-level inconsistency between the graph representations and the corresponding texts.\nAverage words per triplet We also visualize the average number of words per sample for each dataset, categorized by the number of triplets, as shown in Figure 5. WikiOFGraph exhibits an average word increase rate of 3.22 per triplet, which closely aligns with the average of 3.62 triplets per sample, as reported in Table 1. This suggests that WikiOFGraph provides consistent information with minimal bias relative to the number of triplets.\nWebNLG exhibits a relatively high word increase rate of 5.76 per triplet compared to other datasets. However, this rate contrasts with the average of 2.96 triplets per sample, as reported in Table 1. This discrepancy arises because most WebNLG samples consist of short sentences and few triplets.\nOn the other hand, the automatically generated ontology-based datasets show much lower word increase rates per triplet, ranging from 0.87 to 1.66.\nThese lower rates suggest that each triplet in these datasets is associated with fewer words, leading to less detailed representations per triplet.\n4.2 Qualitative Analysis\nTo further analyze graph-text consistency, we conduct a qualitative analysis employing human and GPT-40. We provide 30 samples per dataset, each evaluated by five human reviewers who assess the same set of samples, and an additional 1,000 samples per dataset are evaluated by GPT-40. Since some datasets have a high ratio of samples with a low number of triplets, we categorize the samples by the number of triplets and randomly sample them within each range to ensure a diverse evaluation across different sample lengths.\nOur focus is to assess whether the information of graph representations (triplets) are accurately reflected in the target text. To achieve this, we ask evaluators to review graph-text pairs and identify 1) unused triplets that are not used to generate the text and 2) parts of the text that could not be guessed from the triplets.\nGraph-text consistency Figure 6 presents the average ratio of unused triplets across different datasets. The plain bars represent the evaluations conducted by human evaluators, while the hatched bars indicate the assessment results from GPT-40. When comparing automatically generated datasets, the average ratio of unused triplets is highest in GenWiki, followed by TekGen, LAGRANGE, and WikiOFGraph. Notably, WikiOFGraph is comparable to WebNLG, a fully human-crafted dataset. This observation suggests that WikiOFGraph incorporates triplet information into the text better than automaticaly generated ontology-based datasets.\nFigure 7 presents the average ratio of unguessable text across different datasets. Similarly, GenWiki, TekGen, and LAGRANGE exhibit relatively high amounts of unguessable text, indicating a significant presence of text that cannot be generated from the graph information alone. In contrast, WikiOFGraph contains a low amount of unguessable text, even comparable to WebNLG. This observation suggests that WikiOFGraph has significantly higher graph-text consistency than automaticaly generated ontology-based G2T datasets. Details regarding the evaluation process can be found in the appendix (\u00a7Appendix B)."}, {"title": "5 Experiments", "content": "We provide a comprehensive overview of our experimental setup and results. We first outline our experimental settings (\u00a75.1). We then describe the evaluation metrics used to assess the model's G2T generation performance (\u00a75.2). We describe the evaluation data selection, focusing on ensuring a broad topic range and high reliability (\u00a75.3).\n5.1 Experimental Settings\nWe choose the T5-large (Raffel et al., 2020), which has 770M parameters, for our backbone model. The backbone model is trained to take source triplets as input and predict a target text. We use special tokens such as '<S>', '<P>', and '<O>' to distinguish triplet elements. For example, a source triplets like \"(<S> Arr\u00f2s negre| <P> country| <O> Spain), (<S> Spain| <P> ethnic Group| <O> Spaniards)\" is used to predict a target text such as \"Arros negre is from Spain where Spaniards are an ethnic group.\" We utilize Huggingface's transformers (Wolf et al., 2020) library for our experiments. We use the cross-entropy loss function during training. Implementation details and hyperparameter settings are provided in the appendix (\u00a7Appendix C)."}, {"title": "6 Results and Discussions", "content": "Table 2 and 3 present the performances of the T5-large model fine-tuned on five different datasets, with the best values highlighted in bold. The results demonstrate that fine-tuning with WikiOF-Graph consistently achieves the highest performance across various metrics on both test sets.\nTraining on domain-specific, human-crafted datasets like WebNLG leads to lower performance in general domain G2T generation tasks due to the limited size and narrow coverage of the training data. On the other hand, training on datasets that cover a broader range of domains but are automatically generated based on ontology also leads to lower performance due to reduced graph-to-text consistency. This highlights the importance of dataset scale, domain diversity, and graph-text consistency in achieving robust performance in general domain G2T generation as mentioned in \u00a73.1.\nTable 3 highlights a more apparent performance gap between the T5-large model fine-tuned on WikiOFGraph and those fine-tuned on other datasets. This performance gap is anticipated due to the similar distribution of test samples to WikiOFGraph, yet the difference is considerably significant. These results demonstrate that WikiOF-Graph is significantly more effective than other G2T datasets for general domain G2T generation."}, {"title": "7 Effectiveness of Data-QuestEval", "content": "Due to the extremely low ratio of filtered samples (less than 2%), observing significant performance differences from Data-QuestEval curation is challenging. To further investigate the impact of Data-QuestEval, we conduct additional experiments with controlled sample sizes (\u00a77.1). We then conduct a case study to verify whether Data-QuestEval effectively excludes misaligned graph-text pairs (\u00a77.2)."}, {"title": "7.1 Impact of Filtered Sample Ratios", "content": "We define 'curated' samples as those scoring 0.3 or higher in the Data-QuestEval filtering, while samples scoring below 0.3 are classified as 'noise'. We randomly select these 'curated' samples from WikiOFGraph, while the 'noise' samples are from the filtered-out samples. Given WikiOFGraph's large scale, increasing the ratio of noise directly in the dataset is challenging. Therefore, we fix the train split at 50,000 samples and vary the ratio of 'curated' to 'noise' samples. The experimental settings used for these experiments follows the procedure outlined in \u00a75.1.\nAs shown in Table 5, the evaluation result of the fine-tuned T5-large model improves proportionally as the ratio of curated samples increases. Notably, all key metrics\u2014including BLEU, METEOR, Rouge-L, and BertScore-F1\u2014exhibit consistent improvement, indicating a comprehensive enhancement in model performance. This result indicates that our approach to measuring graph-text consistency through Data-QuestEval filtering is effective and highly practical, particularly in scenarios with limited access to high-quality training data."}, {"title": "7.2 Case Study", "content": "We conduct a case study to understand the low graph-text consistency problems in samples filtered-out by Data-QuestEval. Through the case study, we recognize that the filtered-out samples exhibit two representative types of problems. We highlight these error types with examples along with their corresponding Data-QuestEval scores in Table 4.\nThe first error type arises from incomplete source sentences. For instance, a sentence like \"Piano Sonata No. 1, the default title for a composer's first (or only) piano sonata, may refer to:\" can be split as an incomplete sentence such as \"Piano Sonata No.\" due to its punctuation mark, which causes confusion in the sentence splitter. These incomplete sentences make it difficult to extract graph representations accurately.\nThe second error type occurs due to the use of ambiguous demonstratives or pronouns in the source sentence. Sentences containing demonstratives like \"This\" or other ambiguous pronouns make it challenging to determine the subject-object structure accurately.\nLastly, we observe that most well-structured sentences, like \"October 21, 1967 The Los Angeles Lakers signed Dennis Hamilton as a free agent.\", generally result in consistent graph representations.\nThese results highlight the importance of using complete and unambiguous sentences for graph extraction while demonstrating that Data-QuestEval filtering effectively identifies 'noise' samples from the G2T dataset."}, {"title": "8 Conclusion", "content": "This study introduces WikiOFGraph, a large-scale G2T generation dataset with 5.85M samples covering the entire Wikipedia domain. To address the issue of graph-text misalignment commonly found in ontology-based datasets, we propose a novel method leveraging LLM and Data-QuestEval to generate high-quality graph-text pairs. Comprehensive analyses reveal that WikiOFGraph achieves graph-text consistency comparable to the fully human-crafted dataset, while also exhibiting large scale and extensive domain diversity. Comparisons with representative G2T datasets demonstrate that fine-tuning PLMs with WikiOFGraph significantly enhances their ability to perform general domain G2T tasks. Additional experiments and a case study demonstrate the effectiveness of Data-QuestEval in ensuring high-quality graph-text alignments, reinforcing its value in data curation. Our approach provides a scalable and efficient method for generating high-quality G2T data without relying on proprietary LLMs, external ontologies or extensive human involvement, making it reproducible for advancing G2T generation."}, {"title": "Limitations", "content": "Multilingual extension Although our method is easily reproducible across various domains using open-source LLMs and public source texts, the scope of our study focuses mainly on English. Future research could extend the study's multilingual capabilities by employing LLMs with multilingual capabilities and utilizing source texts from multilingual corpora.\nRoom for improvement We apply various empirical rules of thumb, particularly in areas such as prompt engineering and sampling strategies. While our approach significantly outperforms existing datasets in meeting key requirements, there remains room for improvement, particularly in refining prompts, optimizing sampling parameters for graph extraction, and selecting the most effective LLM. Additionally, since Data-QuestEval might not be the most optimal measure, exploring alternative methods to assess the consistency between the generated graph representations and the source text could be a valuable direction for future work.\nExpanding directions Our work focuses primarily on the direction of G2T generation. Building a dataset that also supports T2G generation poses challenges, such as the need to select predicate expressions from a predefined vocabulary, which could limit flexibility. Future work could address this by reassigning predicate expressions within the WikiOFGraph dataset, potentially creating a more adaptable framework for T2G tasks.\nData contamination concerns One important limitation not yet addressed in our study is the potential for data contamination. Since the LLMs used in our experiments are pre-trained on publicly accessible datasets like Wikipedia, they may already be familiar with the data used in our graph extraction tasks. To thoroughly assess whether our method is robust against data contamination, future research should replicate our experiments on a large corpus that is not part of the LLM's pre-training data."}, {"title": "Ethical Considerations", "content": "We prioritize transparency and reproducibility in our research by using widely accessible resources. We utilize the open-source LLM Llama3-70b-instruct-awq and the publicly available Wikipedia dataset, adhering to their respective licenses.\nFor human evaluation, we conduct the assessments through Upwork, ensuring participants receive appropriate compensation for their efforts. Each evaluator is paid a fixed rate of $60 for their work, which involves evaluating five datasets for approximately three hours. This compensation is determined to be fair and in line with industry standards, reflecting our commitment to ethical research practices and the fair treatment of participants."}, {"title": "A Details of Graph Extraction", "content": "We utilize the vLLM (Kwon et al., 2023) for efficient inference using GPUs with at least 40 GB of VRAM. We perform graph extraction using nucleus sampling (Holtzman et al., 2020) based decoding strategy with a temperature set to 0.5 and a top-p value of 0.9, which are determined experimentally. Total inference time for processing 6M data samples was approximately 600 hours when utilizing 4 GPUs in parallel. Therefore, we divided the data into divisions that contain 100K samples each to generate the outputs in parallel. Detailed prompts are provided in Table 8."}, {"title": "B Details of Qualitative Analysis", "content": "We describe details of human evaluation process and results (\u00a7B.1). We then describe GPT-40 evaluation process and prompts (\u00a7B.2)."}, {"title": "C Details of Fine-Tuning", "content": "In the section 5, we apply bf16 precision, a cosine learning rate decay function with 2k warmup steps, and utilize ZeRO stage 3 optimization through DeepSpeed (Rasley et al., 2020) while employing the AdamW optimizer (Loshchilov and Hutter, 2019). We employ beam search decoding for training and inference, with a temperature set to 1 and beam size set to 5, to ensure a controlled search space exploration while maintaining the diversity of generated outputs. The specific hyperparameters vary depending on the dataset to ensure adequate optimization steps.\nFor WebNLG (Castro Ferreira et al., 2020), we set the gradient accumulation steps and the number of parallel GPUs to achieve an effective batch size of 8 with a maximum learning rate of 0.3. We evaluate every 4K steps and selected the model with the lowest loss. Total training time was approximately 13 hours using 4 GPUs in parallel (52 GPU hours).\nFor other datasets, we use an effective batch size of 192 with a maximum learning rate of 0.5. Evaluations are performed every 20k steps, and the model with the minimum loss is selected. Total training time was approximately 70 hours using 4 GPUs in parallel (280 GPU hours).\nIn the experiments described in section 7, we limit the number of data samples to 50K, similar to the size of WebNLG. Therefore, we conduct the experiments using the same hyperparameter settings as those applied to WebNLG. We select the model based on the one with the lowest loss as determined by evaluations conducted every 4K steps. Total training time was approximately 10 hours using 4 GPUs in parallel (40 GPU hours)."}, {"title": "D Details of datasets", "content": "In this section, we provide information on the sources of the datasets used in our study and detailed statistics for each dataset.\n1. Wikipedia: We use Wikipedia from Huggingface datasets. Specifically, we utilize the 20220301.en split, which can be accessed at Wikipedia dataset.\n2. WebNLG: We use WebNLG obtained from the official repository. We utilize the English data from version 3.0, which is available at WebNLG repository.\n3. GenWiki: We obtain GenWiki from GenWiki repository. We use the data found in the 'fine' split. Since there is no separate evaluation split, we reserve 10% of the training samples for evaluation purposes.\n4. TekGen: We obtain TekGen from Tekgen repository. We convert the data into triplets according to the JSON object rules specified in the official repository.\n5. LAGRANGE: We obtain LAGRANGE from the footnotes of the paper (Mousavi et al., 2024).\nWe also provide a detailed comparison of the specific statistics and characteristics of each dataset, offering a clear overview of their unique features and differences in Table 10."}]}