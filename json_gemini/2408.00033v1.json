{"title": "Enhanced Fault Detection and Cause Identification Using\nIntegrated Attention Mechanism", "authors": ["Mohammad Ali Labbaf Khaniki", "Alireza Golkarieh", "Houman Nouri", "Mohammad Manthouri"], "abstract": "This study introduces a novel methodology for fault detection and cause identification within the\nTennessee Eastman Process (TEP) by integrating a Bidirectional Long Short-Term Memory\n(BiLSTM) neural network with an Integrated Attention Mechanism (IAM). The IAM combines\nthe strengths of scaled dot product attention, residual attention, and dynamic attention to capture\nintricate patterns and dependencies crucial for TEP fault detection. Initially, the attention\nmechanism extracts important features from the input data, enhancing the model's interpretability\nand relevance. The BiLSTM network processes these features bidirectionally to capture long-range\ndependencies, and the IAM further refines the output, leading to improved fault detection results.\nSimulation results demonstrate the efficacy of this approach, showcasing superior performance in\naccuracy, false alarm rate, and misclassification rate compared to existing methods. This\nmethodology provides a robust and interpretable solution for fault detection and diagnosis in the\nTEP, highlighting its potential for industrial applications.", "sections": [{"title": "Introduction", "content": "Fault diagnosis and prognosis are critical components of industrial process control, as they\nenable the early detection and prediction of equipment failures, process disturbances, and other\nanomalies that can impact productivity, safety, and profitability. In industrial applications, fault\ndiagnosis involves the identification of the root cause of a fault or anomaly, while prognosis\ninvolves the prediction of the remaining useful life of equipment or the time to failure. Effective\nfault diagnosis and prognosis can help prevent unexpected shutdowns, reduce maintenance costs,\nand improve overall process efficiency. However, achieving accurate and reliable fault diagnosis\nand prognosis in industrial applications is a challenging task, due to the complexity of modern\nindustrial processes, the presence of noise and uncertainty in sensor data, and the need for real-\ntime decision-making [1]. The Tennessee Eastman Process (TEP) dataset, a widely used\nbenchmark dataset in the field of process control and fault diagnosis, has been used to evaluate the\nperformance of various fault diagnosis and prognosis methods, including machine learning\nalgorithms, model-based approaches, and hybrid methods. The results of these studies have\ndemonstrated the potential of advanced fault diagnosis and prognosis techniques to improve\nprocess efficiency, reduce maintenance costs, and enhance overall process reliability in industrial\napplications [2].\nLSTM networks, a specialized form of recurrent neural networks, have become prominent in\ntime series prediction and classification due to their effectiveness across diverse domains. Their\nunique ability to recognize temporal dependencies and complex patterns within sequential data,\nfacilitated by memory cells in their architecture, proves valuable for modeling time-dependent\nphenomena [3]. LSTM networks have proven to be valuable tools in various applications,\nincluding industrial machinery fault detection, predictive maintenance for wind turbines, anomaly\ndetection in network traffic, fault diagnosis in electrical systems, and predictive maintenance for\naircraft engines [4], [5]. BiLSTM can capture both short-term and long-term dependencies in time\nseries data, while LSTM can only capture long-term dependencies. This makes BiLSTM better\nsuited for tasks that require understanding both short-term and long-term patterns, such as natural\nlanguage processing, fault detection, time series prediction [6]. The use of these recurrent neural\nnetwork architectures has enabled the development of more accurate and reliable fault detection\nsystems, which can significantly reduce downtime, improve maintenance scheduling, and enhance\noverall system reliability. Furthermore, the ability of LSTM and BiLSTM models to handle large\namounts of data and adapt to changing system conditions has made them an attractive solution for\nreal-time fault detection and predictive maintenance applications [7].\nThe advent of attention mechanisms has revolutionized the field of time series analysis and\ndynamic data modeling, enabling researchers and practitioners to unlock new insights and improve\npredictive capabilities [8]. attention mechanisms, inspired by the human brain's ability to focus on\nrelevant information, have introduced a paradigm shift in the way we process and analyze\nsequential data. By selectively weighting and aggregating input features, attention mechanisms\nallow models to dynamically adapt to changing patterns and context, effectively \"focusing\" on the\nmost informative segments of the data [9]. This has led to significant improvements in tasks such\nas forecasting, anomaly detection, and imputation, particularly in domains characterized by high\ndimensionality, non-stationarity, and noisy data. Moreover, attention-based models have been\nshown to be more interpretable and transparent, providing valuable insights into the underlying\ndynamics of the data and facilitating the identification of key drivers and relationships [10]. The\nimpact of attention mechanisms has been felt across various fields, including finance, healthcare,\nclimate science, and transportation, where the ability to accurately model and predict complex\ntemporal phenomena has far-reaching implications for decision-making, risk management, and\npolicy development treniot [11]. As the complexity and volume of time series data continue to\ngrow, the role of attention mechanisms in unlocking new possibilities for dynamic data modeling\nand analysis is likely to become even more pronounced, driving innovation and breakthroughs in\na wide range of applications.\nIn this study, a novel fault detection and cause identification methodology for the TEP is\npresented, employing a combination of a BiLSTM network and a novel Integrated Attention\nMechanism (IAM). The innovations are emphasized as follows:\n\u2022\n\u2022\nFeature Extraction with Attention Mechanism: Initially, the attention mechanism is\nemployed to extract important features, discerning their significance in the fault detection\nprocess. By calculating attention weights, this approach identifies features that exert a\nsubstantial effect on the model's decision-making, enhancing the interpretability and\nrelevance of the selected features for fault detection. Greater attention weights suggest\nincreased importance, pointing to features that play a significant role in influencing the\nmodel's output.\nProposed IAM: This approach integrates the strengths of scaled dot product attention,\nresidual attention, and dynamic attention mechanism to create a comprehensive and\nadaptive attention mechanism. By leveraging the simplicity and relevance computation of\ndot product attention, addressing vanishing gradients with the residual attention's residual\nconnection, and incorporating the adaptability of dynamic attention to input sequences. The\nIAM aims to provide a well-rounded solution that captures nuanced patterns and\ndependencies in diverse tasks.\nThe Significance of Attention Mechanism in Post-BiLSTM Output for Fault Detection:\nThe output generated by the BiLSTM, enriched by the attention mechanism, serves as a\nrefined representation of the input sequence. This post-LSTM output, with enhanced\ncontextual understanding and salient features highlighted by attention, contributes to\nimproved fault detection results. The collaboration between attention mechanisms and\nBiLSTM optimizes the model's ability to capture intricate patterns, enhancing overall\nperformance in fault detection applications.\nThe simulation results validate the superior performance and effectiveness of the proposed control\nstrategy compared to existing approaches [12] and [13].\nThis paper is organized into five sections. The first section provides an overview of the TEP\ndataset and a review of relevant literature. The next section describes the proposed methodology,\nwhich integrates BiLSTM architecture with attention mechanisms. The simulation results are\npresented in the third section, including the training and evaluation of the BiLSTM-attention\nhybrid model using the TEP dataset. Finally, the paper concludes with a summary of the main\nfindings and contributions in the last section."}, {"title": "Literature Review and Dataset Overview", "content": "Fault detection is a crucial task in various industrial processes, as it enables the timely\nidentification and mitigation of anomalies, thereby preventing equipment damage, reducing\ndowntime, and improving overall system reliability. The Tennessee Eastman Process (TEP)\ndataset, a widely-used benchmark for fault detection, provides a realistic simulation of a chemical\nprocessing plant, allowing researchers to develop and evaluate fault detection methods in a\ncontrolled environment. This section provides an overview of the existing literature on fault\ndetection and introduces the TEP dataset, laying the foundation for the proposed methodology."}, {"title": "Literature Survey of Fault detection", "content": "Machine learning has numerous applications across diverse industries, including image and\nspeech recognition in computer vision [14], [15], virtual reality [16] and [17], civil engineering\n[18] and [19], trajectory prediction [20], medical rehabilitation [21], asset management [22],\nconversational agents [23], stock prediction [24] and [25], gold price prediction [26], and vehicle\nrouting [27] among many others. As technological landscapes evolve, incorporating cutting-edge\nmethodologies, such as artificial intelligence or machine learning, becomes increasingly relevant\nfor and enhancing anomaly detection and fault diagnosis capabilities [28]. [29] proposes Starnet,\na novel framework for ensuring robust edge autonomy by detecting sensor anomalies and\nevaluating trustworthiness. [30] explores four different techniques for fault detection and isolation\nin the TEP using principal component analysis, kernel fisher discriminant analysis, and sequential\nquadratic programming. In [31], a fault detection approach is proposed for chemical processes\nusing independent radial basis function models. [32] proposes a class-incremental fisher\ndiscriminant analysis scheme that utilizes a partial F-values with the cumulative per-cent variation\nto address the limitations of traditional fault detection schemes.\nDeep learning methods have emerged as a revolutionary breakthrough in fault diagnosis,\noutperforming traditional approaches in significant ways. By leveraging complex neural network\narchitectures, deep learning algorithms can learn intricate patterns and relationships in data,\nenabling them to detect subtle anomalies and faults that may elude traditional methods. The\nauthors in [12] suggest employing a hierarchical deep LSTM supervised autoencoder for the\nidentification and categorization of faults in industrial facilities. [33] This study examines the\neffectiveness of a robust anomaly detection approach, namely LSTM-based anomaly detection, in\nidentifying unusual patterns in univariate time series data, with a focus on fault detection\napplications. [34] This paper presents a comparative study of fault diagnosis in the TEP using both\nLSTM and Back Propagation (BP) models, demonstrating the superiority of LSTM-RNN in terms\nof accuracy and robustness. [35] proposes a modified BiLSTM for fault diagnosis, which\nincorporates attention mechanisms and convolutional neural networks to improve the accuracy and\nrobustness of fault detection and classification in complex industrial processes. [36] presents a soft\nsensor model for the Tennessee Eastman process, which combines a CNN and BiLSTM network\nwith harmony search algorithm to predict key process variables and detect faults with high\naccuracy.\nThe attention mechanism has been successfully applied in fault detection applications,\nenabling models to selectively focus on relevant features and patterns in data, thereby improving\nthe accuracy and robustness of fault detection and classification. [37] presents a novel approach to\ndynamic gesture recognition using a parallel temporal feature selection framework and an\nimproved attention mechanism. This paper [13] suggest a deep model to employs a seq2seq\narchitecture with attention mechanisms to capture sequential dependencies and identify the root\ncauses of faults of the TEP dataset. [38] proposes the application of a Generalized Transformer\nmodel for fault diagnosis in the TEP, leveraging its self-attention mechanism to effectively capture\ncomplex relationships between process variables and improve the accuracy of fault detection and\nclassification. [39] presents a self-attention mechanism-based approach for dynamic fault\ndiagnosis and classification in TEP, which uses a novel attention-based neural network to identify\nand classify faults in real-time, achieving improved accuracy and robustness in fault detection and\ndiagnosis. [40] proposes an improved convolutional neural network architecture that incorporates\na multi-head attention mechanism to enhance the fault classification performance in industrial\nprocesses, achieving improved accuracy and robustness in identifying faults and anomalies. [41]\npresents a novel Twin Transformer architecture that utilizes a Gated Dynamic Learnable Attention\n(GDLA) mechanism for fault detection and diagnosis in the Tennessee Eastman Process, which\nenables the model to selectively focus on relevant features and improve the accuracy and\nrobustness of fault detection and classification. Based on the reviewed papers, the attention\nmechanism has been identified as a novel and efficient approach to fault detection, offering\nimproved accuracy and robustness in identifying faults and anomalies."}, {"title": "Tennessee Eastman Process (TEP) dataset", "content": "The Tennessee Eastman Process (TEP) dataset is a widely used benchmark dataset in the field\nof process control and fault diagnosis. The dataset was created by the Eastman Chemical Company\nin the 1980s to simulate a realistic industrial process control problem. The TEP dataset simulates\na chemical process that involves the production of a hypothetical product, G, from four reactants,\nA, C, D, and E. The process consists of five major units: a reactor, a condenser, a separator, a\nstripper, and a recycle stream. The reactor is a continuous stirred-tank reactor (CSTR) where the\nreactants are mixed and reacted to form the product. The condenser is used to condense the vapor\nstream from the reactor, and the separator is used to separate the liquid and vapor streams. The\nstripper is used to remove impurities from the product, and the recycle stream is used to recycle\nunreacted reactants back to the reactor. The dataset includes 52 measured variables, including\ntemperatures, pressures, flow rates, and compositions, which are sampled every 3 minutes. These\nvariables are used to monitor and control the process, and to detect and diagnose faults [42].\nThe TEP dataset is particularly useful for testing and evaluating process control and fault\ndiagnosis algorithms because it includes 21 pre-defined fault scenarios, which simulate common\nprocess faults such as valve failures, sensor faults, and process disturbances. Each fault scenario\nis designed to test a specific aspect of process control and fault diagnosis, such as fault detection,\nisolation, and identification. For example, Fault 1 simulates a step change in the A/C feed ratio,\nwhich can cause a disturbance in the process. Fault 2 simulates a sticking valve in the condenser,\nwhich can cause a change in the condenser pressure. Fault 3 simulates a sensor failure in the reactor\ntemperature measurement, which can cause a false reading of the reactor temperature. The fault\nscenarios are designed to be challenging, but not impossible, to detect and diagnose, making the\nTEP dataset a useful tool for evaluating the performance of process control and fault diagnosis\nalgorithms. The dataset has been widely used in academic and industrial research to evaluate the\nperformance of various process control and fault diagnosis methods, including statistical process\ncontrol, machine learning, and model-based approaches [43].\nThe TEP dataset has been widely used in various research studies to evaluate the performance\nof different process control and fault diagnosis methods. For example, researchers have used the\nTEP dataset to evaluate the performance of machine learning algorithms, such as neural networks\nand support vector machines, for fault detection and diagnosis. Other researchers have used the\nTEP dataset to evaluate the performance of model-based approaches, such as model predictive\ncontrol and state estimation, for process control and fault diagnosis. The dataset has also been used\nto evaluate the performance of statistical process control methods, such as control charts and\nstatistical process monitoring, for fault detection and diagnosis. The TEP dataset is available for\ndownload from various online sources, and it has become a standard benchmark for evaluating the\nperformance of process control and fault diagnosis algorithms. The dataset is widely used in\nacademic and industrial research, and it has been cited in numerous research papers and\npublications. The TEP dataset is a valuable resource for researchers and practitioners in the field\nof process control and fault diagnosis, and it continues to be widely used and studied today."}, {"title": "Model Description", "content": "In this section, we explore a suite of attention mechanisms, including the foundational scaled\ndot-product attention, the dynamic attention, and the residual attention mechanism. We introduce\nthe novel proposed IAM, which ingeniously merges these three attention mechanisms, leveraging\ntheir unique strengths. The section also delves into the BiLSTM as a robust temporal modeling\ntool. Our envisioned IAM with BiLSTM forms a seamless fusion, featuring a strategic application\nof IAM to the input for the initial extraction of dominance fault features. Subsequently, IAM is\nstrategically applied to BiLSTM outputs, fostering a synergistic relationship between attentional\ncapabilities and temporal context modeling. This synergized approach presents a comprehensive\nstrategy for the analysis of sequential data."}, {"title": "Scaled Dot-Product Attention Mechanism", "content": "The scaled dot-product attention mechanism, a pivotal component in transformer\narchitectures, is elucidated in this scientific exposition. The neural network module includes a\nmechanism that adjusts itself during training to prevent problems with the gradients. Through some\nmatrix operations, the attention scores are computed, representing the contextual relevance\nbetween query and key vectors. The attention weights, calculated using a Softmax activation\nfunction, show how important different parts of the input are. The application of these attention\nweights facilitates the synthesis of an attended input from the provided value vectors. The scaled\ndot-product attention mechanism can be expressed mathematically as follows.\nLet Q, K, and V be the query, key, and value matrices, respectively, with dimensions\n$d_q \\times L$, $d_k \\times L$, and $d_v \\times L$, $d_q$, $d_k$ and $d_v$ represent the dimensions of the query, key, and value\nvectors, and L denotes the sequence length. The scaling factor is denoted by $\\sqrt{d_k}$. The term $\\sqrt{d_k}$\nrepresents the square root of the dimension $d_k$. In the context of attention mechanisms, it is\ncommonly used as a scaling factor. Note that, in many cases, especially in self-attention\nmechanisms like those used in transformer models, Q, K, and V are set to be the same, coming\nfrom the same input sequence; hence in this study, they are assumed to be equal to the input\nsequence (x). The attention scores A are computed as follows.\n$A = Softmax(\\frac{QK^T}{\\sqrt{d_k}})$\n(1)\nThe attended input Z is then obtained by multiplying the attention scores with the value matrix:\n$Z = A \\times V$,\n(2)\nthis formulation encapsulates the essence of the scaled dot-product attention mechanism, providing\na concise representation of its underlying mathematical operations within the context of\ntransformer-based neural network architectures. Finally, (3) represents the calculation of attended\ninput in the scaled dot-product attention mechanism.\n$Z(Q,K,V) = Softmax(\\frac{QK^T}{\\sqrt{d_k}}) \\times V$.\n(3)"}, {"title": "Dynamic Scaled Dot-Product Attention Mechanism", "content": "The dynamic scaled dot-product attention mechanism introduces an innovative dimension\nto the scaled dot-product attention mechanism by incorporating an adjustable scaling parameter.\nThis dynamic feature enables the model to dynamically regulate the influence of attention scores,\noffering a tunable mechanism for adapting to diverse data patterns and complexities. The attention\nscores in the dynamic scaling A are computed as\n$scores = \\frac{QK^T}{\\sqrt{d_k}}$\n(4)\n$dynamic\\_scores = scores \\odot \\lambda$,\n(5)\n$A = Softmax(dynamic\\_scores)$,\n(6)\nwhere is the additional adaptive scaling parameter and is the sign of element-wise\nmultiplication. Equ. (7) shows the attended input Z is then obtained by multiplying the attention\nscores with the value matrix.\n$Z = A \\times V$,\n(7)\nin this formulation, the introduction of an additional adaptive scaling parameter A offers a dynamic\nmechanism to finely tune the impact of attention scores on the final weights. A is a flexible\nparameter for the attention mechanism thereby bolstering its adaptability and expressiveness in\ncapturing intricate relationships within the input data. Hence, the overall formula is equal to (8).\n$Z_{Dynamic}(Q, K, V) = Softmax (\\frac{QK^T}{\\sqrt{d_k}} \\odot \\lambda) \\times V$.\n(8)\nThe block diagram of the dynamic scaled dot-product attention mechanism is illustrated in Fig. 3."}, {"title": "Residual Attention Mechanism", "content": "Residual attention is a sophisticated extension of traditional attention mechanisms within\nneural network architectures. In this paradigm, the residual connections, inspired by the success of\nresidual networks, are seamlessly integrated with attention mechanisms to enhance information\nflow and gradient propagation during training. The key idea is to create shortcut connections that\nbypass certain attention layers, allowing the network to directly access and preserve the original\ninput information. This mitigates the risk of information loss during the attention process,\naddressing challenges such as vanishing gradients and facilitating the training of deeper models.\nBy combining the strengths of residual connections and attention mechanisms, residual attention\nachieves a delicate balance between capturing intricate patterns in the data and maintaining the\nintegrity of the input features, leading to more effective and stable learning in complex tasks. This\napproach has demonstrated notable success in various applications, showcasing its potential to\nimprove the performance and convergence of deep neural networks [44]. Given an input tensor x\nwith a transformation function of attention mechanism F(x), the output y of the residual block is\ncalculated as\n$y = x + F(x)$,\n(9)"}, {"title": "The Proposed Integrated Attention Mechanism", "content": "We integrated the dynamic scaled dot-product attention with the residual attention\nmechanism, creating a unified approach known as the IAM. The final formulation is expressed as:\n$Y = x + Softmax (\\frac{QK^T}{\\sqrt{d_k}} \\odot \\lambda) \\times V = x + Z_{Dynamic} (Q, K, V)$,\n(10)\nin this formulation, the original input x is combined with the attended input obtained through the\ndynamic scaled dot-product and residual attention mechanisms, providing the model with the\nability to selectively emphasize relevant features while preserving the underlying information. Fig.\n4 shows the IAM block diagram."}, {"title": "Bidirectional Long Short-Term Memory (BiLSTM)", "content": "LSTM networks have revolutionized the field of recurrent neural networks (RNNs) by\nproviding an elegant solution to the problem of learning and capturing long-term dependencies in\nsequential data. Unlike traditional RNNs, which suffer from the vanishing gradient problem,\nLSTMs are specifically designed to overcome this limitation. They achieve this by employing\nspecialized units, such as forget gates, input gates, and output gates, which are equipped with\nadaptive mechanisms that enable the network to selectively retain or discard information over\nvarious time scales. The LSTM layer can be represented by the following equations.\n$f_t = \\sigma (W_f. [h_{t-1}, x_t] + b_f)$,\n(11)\n$i_t = \\sigma (W_i. [h_{t\u22121}, x_t] + b_i)$,\n(12)\n$\\tilde{C_t} = tanh (W_c. [h_{t\u22121}, x_t] + b_c)$,\n(13)\n$C_t = f_t * C_{t-1} + i_t * \\tilde{C_t}$ ,\n(14)\n$O_t = \\sigma (W_o. [h_{t\u22121}, x_t] + b_o)$,\n(15)\n$h_t = o_t * tanh(C_t)$,\n(16)\nwhere $f_t$, $i_t$, $o_t$ are the forget, input, and output gates, respectively; $C_t$ is the cell state; $h_t$ is the\nhidden state; $x_t$ is the input at time step t; $\\sigma$ is the sigmoid function; and W and b are weight\nmatrices and bias vectors, respectively.\nBiLSTM is a variant of the traditional LSTM recurrent neural network architecture. The\nprimary distinction lies in its ability to process input sequences in both forward and backward\ndirections, enabling the model to capture contextual information from both past and future time\nsteps. Mathematically, the output of a BiLSTM at a given time step t can be represented as:\n$h^{(BiLSTM)}_t = [h_t, \\overleftarrow{h_t}]$,\nwhere $h_t$ is the hidden state of the forward LSTM at time step t and $\\overleftarrow{h_t}$ is the hidden state of the\nbackward LSTM at the same time step. The brackets represent the concatenation operation. This\nbidirectional processing enhances the model's ability to capture long-range dependencies and\ncontext in sequential data, making BiLSTM a popular choice for tasks such as natural language\nprocessing, speech recognition, and time series prediction."}, {"title": "The Proposed IAM with BiLSTM", "content": "This subsection introduces a designed sequential processing method that synergizes the\npower of the IAM and BiLSTM neural networks for fault detection in sequential data. The\norchestrated sequence of operations unfolds as follows.\n1. IAM applied to input:\n\u2022\nThe output of the IAM applied to the input is given by:\n$Y_{t1} = x_t + Softmax (\\frac{QK^T}{\\sqrt{d_k}} \\odot \\lambda) \\times V = x_t + Z_{Dynamic} (Q_t, K_t, V_t)$,\n(17)\nthe IAM selectively attends to relevant features in the input, generating an intermediate\nrepresentation denoted as $Y_{t1}$.\n2. BiLSTM Output of IAM:\n\u2022\nThe BiLSTM is applied to the output of the IAM:\n$h_{t1} = BiLSTM (Y_{t1})$.\n(18)\n$Y_{t\u2081}$ is then fed into a BiLSTM layer, producing $h_{t\u2081}$. This step captures temporal dependencies and\nrefines the feature representation."}, {"title": "IAM Applied to BiLSTM Output", "content": "3. IAM Applied to BiLSTM Output:\n\u2022\nThe IAM is again applied to the output of the BiLSTM:\n$Y_{t2} = h_{t1} + Z_{Dynamic} (Q_{h_{t1}}, K_{h_{t1}}, V_{h_{t1}})$,\n(19)\nthe IAM is applied once again to $h_{t1}$, yielding $Y_{t2}$. This dual application enhances the model's\nability to focus on crucial aspects of temporal context, considering the refined features generated\nby the BiLSTM.\n4. Two Fully Connected Layers:\n\u2022\nThe output $Y_{t2}$ is then processed through two fully connected layers:\n$Y_3 = FC_1 (Y_2)$,\n(20)\n$output = FC_2 (Y_3)$,\n(21)\nthe output $Y_{t2}$ undergoes processing through two fully connected layers ($FC_1$ and $FC_2$), resulting\nin the final output.\nIn this method, the sequential processing unfolds in a carefully orchestrated manner to\ncapture intricate patterns and dependencies in the input data. First, the IAM is applied to the input,\nproducing an intermediate representation denoted as $Y_{t1}$. This step involves selectively attending\nto relevant features through a dynamic attention mechanism, as expressed in (17). Subsequently,\nthe output $Y_{t1}$ is fed into BiLSTM layer, denoted as $h_{t1}$, which captures temporal dependencies\nand refines the feature representation. The IAM is applied once again to the output of the BiLSTM,\nyielding $Y_{t2}$. This dual application of attention mechanisms enhances the model's ability to focus\non crucial aspects of the temporal context while considering the refined features generated by the\nBiLSTM. Following this, the $Y_{t2}$ is processed through two FC layers FC\u2081 and FC2, providing the\nfinal output. This sequence of operations allows the model to learn and extract hierarchical\nfeatures, capturing both spatial and temporal information effectively. The proposed fault detection\nstrategy integrates IAM as a feature extractor, demonstrating its versatility and adaptability. The\nIAM's application to both the input and BiLSTM output signifies a holistic approach, leveraging\nattentional capabilities and temporal context modeling for improved fault detection performance.\nThe architecture, illustrated in Fig. 5, visually represents this method's comprehensive\nstrategy, showcasing the sequential flow of operations and the integration of IAM with BiLSTM\nand FC layers.\nIn this study, we propose a neural network architecture tailored for sequence-based data,\nincorporating BiLSTM layers and fully connected layers to ensure a balance between\ncomputational efficiency and model expressiveness. The architecture details and training\nparameters are meticulously chosen to optimize performance. The input to the model consists of\nsequences with the following characteristics:\n\u2022 Batch size: 64\n\u2022 Sequence length: 10\n\u2022 Number of features: 52\nHence K, Q, V \u2208 R32\u00d710\u00d752. The hidden size of the BiLSTM is 128, and the architecture includes\ntwo fully connected layers with 128 and 64 neurons, respectively, each with a 0.2 dropout rate and"}, {"title": "Simulations", "content": "In this section, we delve into the training and testing phases of our proposed IAM with\nBiLSTM for fault diagnosis, showcasing its capabilities and highlighting the importance of\nattention weights in feature selection. Our evaluation goes beyond traditional metrics,\nincorporating precision, recall, F1-score, false discovery rate, false alarm rate, and\nmisclassification rate for each fault class, providing a comprehensive understanding of the model's\nperformance across various fault categories. We also compare our results with those of existing\nstate-of-the-art methods, offering a nuanced assessment of our model's effectiveness."}, {"title": "Accuracy", "content": "In Figs. 6 and 7, the graphical depictions illustrate the training and testing metrics over 200 epochs\nwith a fixed learning rate of 0.001 and a batch size of 64. These visual representations offer a\ncomprehensive insight into the model's performance, showcasing how it evolves and generalizes\nover the course of the training process.\nAt Epoch 87, the model achieved a remarkable balance with a minimum test loss of 0.0734 and a\ncorresponding high-test accuracy of 97.19%. The training loss of 0.0496 and training accuracy of\n98.04% underscore the model's proficiency in learning and generalizing from the training data.\nEpoch 108 further solidified the model's capabilities, reaching a minimum training loss of 0.0469\nwith an impressive training accuracy of 98.16%. Despite a slight increase in the test loss to 0.0879,\nthe test accuracy remained high at 96.72%.\nIn both epochs, the model demonstrated a strong foundation for fault detection, showcasing its\nability to minimize errors and generalize well to new, unseen data. The nuanced interplay between\ntraining and testing metrics reflects the model's robustness and effectiveness in fault diagnosis\ntasks. Choosing the model from Epoch 87, with the minimum test loss of 0.0734, aligns with the\nprinciple of selecting a model that generalizes well to new, unseen data."}, {"title": "Cause Identification", "content": "In this section, we investigate the visualization of attention weights, a key element in\nuncovering the root causes of faults. Leveraging the proposed IAM, we unravel the pivotal features\nthat exert a major influence on fault detection. Fig. 8 depicts the visualization of attention weights,\nillustrating their role in both normal operation and the identification of causes for faults 1 and 2.\nAnalyzing Fig. 8 provides valuable insights into the attention weights visualization for normal\noperation and the identification of faults 1 and 2. Notably, the visualization highlights the\nfollowing major influential features.\n\u2022 Normal Operation:\nFeatures 39 and 40 emerge as significant contributors to the fault detection process during normal\noperation. The attention weights indicate their pronounced impact in discerning normal data\npatterns.\n\u2022 Fault 1:\nFor fault 1, attention is notably directed towards features 0, 3, and 43. These features play a pivotal\nrole in the model's ability to detect and differentiate fault 1 patterns from normal operation.\n\u2022 Fault 2:\nFault 2 detection is influenced prominently by features 9, 27, 33, and 46. The attention weights\nunderscore the significance of these features in identifying and isolating fault 2 occurrences.\nThis detailed analysis not only sheds light on the crucial features for fault detection in each scenario\nbut also enhances our understanding of the model's interpretability and its ability to discern diverse\nfault patterns. Figures 9 to 14 display the attention weights pertaining to faults 3 through 20.\nThe importance of features for faults 3 to 20 can be comprehended in a manner similar to the\ninsights gained from Fig. 8."}, {"title": "Conclusion", "content": "This study proposes a novel fault detection and cause identification methodology for the T\u0415\u0420\nusing a BiLSTM neural network enhanced with an innovative IAM. The proposed IAM\nincorporates scale dot product attention, residual attention, and dynamic attention to effectively\ncapture intricate patterns and dependencies within the TEP data. The IAM's unique role in\nsupervised feature extraction distinguishes the proposed methodology. By calculating attention\nweights, the IAM identifies key features that significantly impact fault detection, providing\ninterpretability and insight into the model's decision-making process. This proposed IAM with\nBiLSTM method presents a versatile and effective approach for capturing intricate patterns and\ndependencies crucial for TEP fault detection. Comparative evaluations demonstrate the proposed\nIAM with BiLSTM approach's superior performance in fault detection, particularly in terms of F1-\nscore, outperforming state-of-the-art methods. The well-balanced performance, evidenced by the\nequilibrium between capturing positive instances and minimizing false positives, underlines the\nrobustness of the proposed methodology.\nFuture work on this paper could include exploring other attention mechanisms, applying the\nmethodology to other industrial processes, handling noisy or missing data, scaling and\nimplementing the model in real-time, comparing with other machine learning approaches,\ninvestigating the impact of hyperparameters, applying to other types of faults, and developing a\nmore interpretable model."}]}