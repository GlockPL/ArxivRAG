{"title": "Time Awareness in Large Language Models:\nBenchmarking Fact Recall Across Time", "authors": ["David Herel", "Vojtech Bartek", "Tomas Mikolov"], "abstract": "Who is the US President? The answer changes\ndepending on when the question is asked.\nWhile large language models (LLMs) are evalu-\nated on various reasoning tasks, they often miss\na crucial dimension: time. In real-world sce-\nnarios, the correctness of answers is frequently\ntied to temporal context. In this paper, we intro-\nduce a novel dataset designed to rigorously test\nLLMs' ability to handle time-sensitive facts.\nOur benchmark offers a systematic way to mea-\nsure how well LLMs align their knowledge\nwith the correct time context, filling a key gap\nin current evaluation methods and offering a\nvaluable tool for improving real-world applica-\nbility in future models.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have become foun-\ndational tools across a wide range of applications,\nfrom chatbots to search engines, due to their im-\npressive performance in natural language under-\nstanding, reasoning, and factual recall. However,\ndespite these advancements, LLMs often struggle\nwith a critical aspect of real-world knowledge: time\nawareness. The ability to correctly interpret and\nrecall facts not just in isolation, but in relation to\nspecific time contexts, is crucial in many real-world\nscenarios.\nConsider the question, \"Who is the US Pres-\nident?\" The correct answer depends entirely on\nwhen the question is asked. In many use cases,\nfrom virtual assistants to automated fact-checkers,\nunderstanding the temporal dimension of informa-\ntion is essential for providing accurate responses.\nAs illustrated in Figure 1, models like Llama 3.1 8B\n(Dubey et al., 2024) adjust their predictions based\non the time context, highlighting the importance of\ntime awareness in such scenarios. Despite this, cur-\nrent benchmarks and evaluation datasets for LLMS\nfocus primarily on static factual recall, reasoning,\nand language generation, with little attention given\nto time-sensitive facts.\nTo address this gap, we introduce a novel dataset\nspecifically designed to test time awareness in\nLLMs. Our benchmark consists of over 1,100\nevents from 2022 and 2023, each paired with four\nparaphrases and labeled with the correct month,\nyear, and category (e.g. business, science, crime).\nThis enables a systematic evaluation of the mod-\nels' ability to recall time-specific facts and assess\ntheir robustness across different formulations of\nthe same event. The evaluation process further in-\nvolves testing models by generating temporal vari-\nations, which systematically vary the month prefix\nto probe the models' time awareness in detail.\nOur contributions are threefold: (1) We intro-\nduce a benchmark designed to evaluate time aware-\nness in LLMs, focusing on time-sensitive fact recall.\n(2) We present insights into model performance, re-\nvealing how instruction-tuned models and models\ntrained on synthetic data often struggle with this\ntask. (3) We publicly release the dataset and our\nevaluation framework on HuggingFace and GitHub,\nensuring accessibility and encouraging further re-"}, {"title": "Related Work", "content": "Several datasets have been introduced to evaluate\nthe temporal reasoning capabilities of large lan-\nguage models (LLMs). The TempReason dataset\n(Tan et al., 2023) and TRAM benchmark (Wang\nand Zhao, 2024) both focus on assessing LLMs'\nunderstanding of event ordering, duration, and fre-\nquency. However, these benchmarks primarily tar-\nget broader temporal reasoning tasks rather than\nspecific factual recall at finer time resolutions, such\nas determining the exact month when an event oc-\noccurred.\nIn addition, the TempLAMA dataset (Dhingra\net al., 2022) probes LLMs on facts associated with\nspecific years but does not extend to the month-\nlevel precision required for many real-world ap-\nplications. Similarly, the Test of Time benchmark\n(Fatemi et al., 2024) explores event relationships\nover time, but lacks the focus on precise, time-\nbound factual recall."}, {"title": "Dataset", "content": "Our dataset is designed to evaluate the time aware-\nness of large language models (LLMs), focusing on\ntheir ability to recall facts tied to specific months.\nIt consists of 1,150 significant events from 2022\nand 2023 across diverse domains such as politics,\nbusiness, science, art, and crime. These events\nwere sourced from reliable and authoritative out-\nlets, ensuring accuracy and credibility. The dataset,\nalthough constructed in English, represents a broad\nrange of global occurrences, ensuring comprehen-\nsive geographical and cultural diversity, which is\ncrucial for real-world applications."}, {"title": "Data Collection and Structure", "content": "The events were curated from trusted sources such\nas major global news outlets (e.g., BBC (BBC\nNews, 2023), Reuters (Reuters, 2023), The New\nYork Times (nyt, 2023)), academic journals (e.g.,"}, {"title": "Category and Temporal Distribution", "content": null}, {"title": "Public Availability", "content": "The dataset, along with the evaluation framework,\nis publicly available on HuggingFace and GitHub,\nproviding the research community with an accessi-\nble resource to further explore time-sensitive fact\nrecall in LLMs.\u00b9"}, {"title": "Experiments", "content": "The core hypothesis driving our dataset is that a\nlarge language model (LLM) should assign the\nhighest probability to the sentence describing an\nevent with the correct temporal prefix-specifically,\nthe month and year in which the event occurred.\nThis hypothesis underpins the evaluation setup,\nwhere the model is tested on its ability to select\nthe correct temporal context out of a range of pos-\nsibilities."}, {"title": "Evaluation Setup", "content": "We evaluated several state-of-the-art open-source\nlarge language models (LLMs) on our benchmark,\nincluding Gema, Mistral, Llama, and Phi (Team\net al., 2024; Jiang et al., 2023; Dubey et al., 2024;\nLi et al., 2023; Abdin et al., 2024), with parame-\nter sizes ranging from 2B to 70B. For each event\ne in the dataset, we generated 24 temporal varia-\ntions corresponding to each possible month in 2022\nand 2023. Let $x_m$ represent the input sentence for\na given event with temporal prefix m, where m\nrepresents a specific month and year combination.\nThe model assigns a log probability log $P(x_m)$\nto each temporal variation. We define the model's\ntask as identifying the temporal prefix $m^*$ with the\nhighest probability:\n$m^* = \\underset{m \\in \\{1,...,24\\}}{\\text{arg max}} \\text{log }P(x_m)$                                       (1)\nThe model is considered correct if $m^*$ corre-\nsponds to the actual month and year when the event\noccurred.\nThe evaluation used the prompt structure \u201cIt is\n{month} {year} and {event},\u201d which was selected\nbased on additional experiments exploring differ-\nent prefix formats. These experiments, detailed in\nthe Appendix A, showed that this structure consis-\ntently produced the most accurate results in align-\ning model predictions with the correct temporal\ncontext."}, {"title": "Metrics", "content": "We evaluated models using two key metrics:\n\u2022 Accuracy: We use Top-1, Top-3, and Top-\n5 Accuracy, reflecting how often the correct\nmonth is within the top predictions based on\nthe sentence probability.\n\u2022 Stability: The percentage of how many para-\nphrases are correctly classified if the original\nsentence is correct. This metric essentially\nshows how likely the model predicts the same"}, {"title": "Results", "content": "Our experiments reveal several clear trends in LLM\nperformance on the time-sensitive fact recall task.\nWe evaluated base and instruction-tuned variants,\nwith parameter sizes ranging from 2B to 70B. The\nresults are summarized in Table 2."}, {"title": "Instruction-Tuned Models Underperform", "content": "Across all model families, instruction-tuned vari-\nants underperform compared to base models on\nthis task. For instance, Gemma-27B achieves\n30.96% Top-1 accuracy but drops to 17.57% after\ninstruction tuning. Similarly, Mistral-Nemo-Base-\n2407 outperforms its instruction-tuned counterpart,\nachieving 17.83% versus 16.09%. Further compar-\nison is available in Appendix C.\nWe hypothesize that the broad generalization\nachieved during instruction-tuning dilutes time-\nspecific factual recall, prioritizing task flexibility\nover detailed temporal knowledge."}, {"title": "Impact of Model Size on Performance", "content": "Model size strongly correlates with performance\non our time-awareness benchmark. Larger models\nconsistently outperform smaller ones across all met-\nrics. For example, Llama-3.1 70B Base achieves\n39.74% Top-1 accuracy compared to Gema-27B's\n30.96% or to Gema-2-2B's 9.83%. This trend\naligns with general observations that larger models\nbetter capture nuanced information, including tem-\nporal dependencies. This effect is also illustrated\nin Appendix D."}, {"title": "Underperformance of Synthetic-Training\nModels", "content": "Despite excelling in standard reasoning and genera-\ntion tasks, synthetic-trained models like the Phi\nfamily underperform in temporal recall. Phi-2\nachieves just 4.70% Top-1 accuracy, while Phi\n3-medium-4k-instruct reaches 8.87%. This high-\nlights a key limitation of synthetic data, which often"}, {"title": "Stability Across Paraphrases", "content": "Model stability across paraphrases varies signifi-\ncantly. Llama-3.1-70B maintains a 65.97% para-\nphrase stability score, meaning it consistently pre-\ndicts the correct temporal prefix across paraphrased\nversions of the same event. Smaller models like\nPhi-3.5-mini-instruct exhibit lower stability, scor-\ning only 37.15%. This suggests that larger mod-\nels are more robust in handling rephrased events,\nwhereas smaller or instruction-tuned models strug-\ngle with paraphrase consistency."}, {"title": "Conclusion", "content": "In this paper, we introduced a new benchmark de-\nsigned to evaluate LLMs ability to handle time-\nsensitive facts, addressing a crucial gap in current\nevaluation methods that focus primarily on static\nfactual recall. Our dataset, consisting of over 1,100\nevents from 2022 and 2023, provides a structured\napproach to testing models' time awareness by mea-\nsuring their ability to correctly identify temporal\ncontexts for real-world events.\nOur evaluations showed that larger models con-\nsistently outperform smaller ones on time-sensitive\ntasks, confirming the importance of scale in factual\nrecall. However, instruction-tuned models, while\nstrong in general-purpose reasoning, struggle with\ntemporal reasoning. Additionally, models trained\nprimarily on synthetic data, such as the Phi family,"}]}