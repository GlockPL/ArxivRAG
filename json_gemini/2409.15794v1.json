{"title": "Towards Universal Large-Scale Foundational Model for Natural Gas Demand Forecasting", "authors": ["Xinxing Zhou", "Jiaqi Ye", "Shubao Zhao", "Ming Jin", "Zhaoxiang Hou", "Chengyi Yang", "Zengxiang Li", "Yanlong Wen", "Xiaojie Yuan"], "abstract": "In the context of global energy strategy, accurate natural gas demand forecasting is crucial for ensuring efficient resource allocation and operational planning. Traditional forecasting methods struggle to cope with the growing complexity and variability of gas consumption patterns across diverse industries and commercial sectors. To address these challenges, we propose the first foundation model specifically tailored for natural gas demand forecasting. Foundation models, known for their ability to generalize across tasks and datasets, offer a robust solution to the limitations of traditional methods, such as the need for separate models for different customer segments and their limited generalization capabilities. Our approach leverages contrastive learning to improve prediction accuracy in real-world scenarios, particularly by tackling issues such as noise in historical consumption data and the potential misclassification of similar data samples, which can lead to degradation in the quaility of the representation and thus the accuracy of downstream forecasting tasks. By integrating advanced noise filtering techniques within the contrastive learning framework, our model enhances the quality of learned representations, leading to more accurate predictions. Furthermore, the model undergoes industry-specific fine-tuning during pretraining, enabling it to better capture the unique characteristics of gas consumption across various sectors. We conducted extensive experiments using a large-scale dataset from ENN Group, which includes data from over 10,000 industrial, commercial, and welfare-related customers across multiple regions. Our model outperformed existing state-of-the-art methods, demonstrating a relative improvement in MSE by 3.68% and in MASE by 6.15% compared to the best available model. These results highlight the potential of our approach not only as a powerful tool for demand forecasting in the energy sector but also as a foundation model that can be applied broadly across different contexts, contributing to more reliable and efficient energy management strategies.", "sections": [{"title": "1. Introduction", "content": "Natural gas, primarily composed of hydrocarbons, is a crucial energy resource found in underground rock reser-voirs. As the cleanest and lowest-carbon fossil fuel, it plays a vital role in reducing environmental pollution and achieving carbon neutrality. With global consumption reaching 3.96 trillion cubic meters in 2023 and continuing to grow in 2024[16], the significance of natural gas in the energy sector is undeniable. However, regional imbalances in supply and demand present challenges to resource efficiency and market stability. To address these challenges, accurate demand fore-casting is essential. It enables energy companies and policy-makers to develop reliable supply plans, manage contracts effectively, and optimize operations, ensuring stable and efficient energy distribution. As technological advancements continue, precise forecasting will become increasingly criti-cal in maximizing the benefits of natural gas.\nMany studies [14, 28, 36, 48] have focused on improving the accuracy of natural gas demand forecasting, utilizing various advanced methods and algorithms to better capture the complexities of gas consumption. These approaches have shown effectiveness under controlled conditions, yet they often struggle with real-world industrial applications where additional challenges emerge. Despite the importance of accurate forecasting, two key challenges need to be ad-dressed. First, industrial datasets often contain significant levels of noise, which can obscure true consumption pat-terns and complicate accurate demand forecasting. Noise in the data can stem from various sources, such as sensor errors, data logging issues, or irregular reporting intervals. These inaccuracies can distort the actual usage patterns, making it difficult to identify genuine consumption trends and anomalies. Consequently, the presence of noise poses a substantial challenge for modeling and predicting natural gas demand, necessitating robust techniques to filter out these inaccuracies and enhance the reliability of the forecast-ing model. Second, natural gas consumption patterns vary widely across different sectors, leading to inconsistencies in usage data. In industrial applications, natural gas is used for power generation, heating, steam production, and ma-chinery operation, where usage is often driven by varying production cycles and operational demands, resulting in unpredictable peaks and troughs. In the commercial sector, natural gas is primarily used for cooking, heating, and hot water supply, with demand influenced by business hours and seasonal shifts, such as increased heating needs during colder periods. Residential usage further diversifies these patterns, as natural gas is utilized for household purposes such as fueling water heaters, fireplaces, and stoves, leading to variations in demand based on daily routines and weather conditions. These differences in consumption patterns across industrial, commercial, and residential sectors complicate the modeling process and reduce the predictive accuracy of traditional approaches. Addressing these sector-specific variations is crucial for developing a reliable and accurate demand forecasting model.\nThe current development of natural gas demand forecast-ing has evolved from early statistical methods to contem-porary deep learning-based approaches. Traditional models, such as time series analysis and regression models, laid the foundation for forecasting by leveraging historical data to identify trends and patterns [18, 17, 3, 10]. Recent studies have focused on models such as Recurrent Neural Networks (RNNs) [42, 27], Convolutional Neural Networks (CNNs) [13, 41], and Transformer [39, 29], which have shown signif-icant improvements in prediction accuracy. However, most of these approaches still rely on end-to-end learning meth-ods. In recent years, self-supervised representation learn-ing has made significant progress in computer vision (CV) [21, 9] and natural language processing (NLP) [12, 49], and it is increasingly being applied to enhance time series prediction as well [36, 28]. Given the broad range of natural gas customers and the diversity of their usage patterns, estab-lishing a universal large-scale foundational model for natural gas demand forecasting is essential. Such a model can inte-grate vast amounts of data from multiple users and regions, enabling more accurate and comprehensive predictions of natural gas consumption. By leveraging advanced machine learning techniques, self-supervised learning techniques and large-scale datasets, our model effectively accounts for the complex dynamics of natural gas demand across diverse ge-ographical and temporal contexts. This foundational model not only enhances the efficiency of natural gas dispatching but also contributes to the overall stability and sustainability of the energy market.\nBesides, to address the challenges of noisy industrial datasets and varying consumption patterns across different sectors, we developed this robust foundational model using corresponding techniques. First, we introduced a denoising module to filter out noise from the datasets, effectively mit-igating the impact of sensor errors, data logging issues, and irregular reporting intervals. By integrating this denoising module with contrastive learning, we enhanced the model's robustness against noise, ensuring more accurate demand forecasting in industrial contexts. Second, considering the sector-specific variations in natural gas consumption, we leveraged industry information inherent in our datasets. We employed a dual criterion of data similarity and industry-specific information to perform negative sample exclusion. This process helps to avoid misclassifying certain positive samples as negative, thereby preventing the introduction of additional noise into the model. These techniques collec-tively enable our model to deliver reliable and precise natural gas demand predictions, accommodating the complex and diverse usage patterns across industrial, commercial, and residential sectors."}, {"title": "2. Related studies", "content": "Methods for forecasting natural gas demand can gen-erally be categorized into statistical methods and AI-based methods [43]. Despite significant advancements, challenges remain in applying these methods to large-scale, indus-trial datasets. This section reviews existing methods and highlights the potential of foundation models in addressing current limitations."}, {"title": "2.1. Statistical methods", "content": "Statistical methods are widely used for forecasting nat-ural gas demand due to their simplicity and effectiveness. Common techniques include time series models and regres-sion models. Time series models are statistical methods used to analyze and predict future values based on previously observed data points, making them highly applicable in natural gas demand forecasting. When time series data is stationary and free of missing values, the autoregressive integrated moving average (ARIMA) model serves as an effective tool for modeling and predicting data by incor-porating autoregressive, differencing, and moving average components [18, 1, 24]. The exponential smoothing method predicts future values by applying weighted averages to his-torical data, making it suitable for stationary or trend-based time series [2]. The Vector Autoregression (VAR) model is utilized for multivariate time series analysis, effectively predicting outcomes by capturing the interrelationships be-tween multiple variables [23].\nRegression models are statistical techniques used to pre-dict a dependent variable based on one or more indepen-dent variables, commonly applied in natural gas demand forecasting to identify and quantify relationships within the data. Aydin [3] found that the S regression model pro-vided the most reliable predictions, validated by statistical approaches. Chen et al. [10] developed a Functional Au-toRegressive model with eXogenous variables (FARX) to provide accurate day-ahead forecasts for the German gas distribution system. Sen et al. [40] employed various mul-tivariate regression models to forecast future natural gas consumption."}, {"title": "2.2. AI-based methods", "content": "Artificial Intelligence (AI) methods have gained popu-larity in recent years due to their ability to handle complex and non-linear relationships in data. Techniques such as Ar-tificial Neural Networks (ANNs), Support Vector Machines (SVMs), and deep learning approaches have demonstrated promising results in various applications. Artificial Neural Networks (ANN) are computational models inspired by the human brain, designed to recognize patterns and make predictions by learning from data. Gorucu [19] evaluated and forecasted gas consumption in Ankara, Turkey, using Artificial Neural Network (ANN) models. Khotanzad et al. [25] proposed a two-stage system combining two types of artificial neural network (ANN) forecasters to predict daily natural gas consumption. Support Vector Machines (SVMs) are supervised learning models used for classification and regression tasks, known for their effectiveness in handling high-dimensional data and nonlinear relationships. Zhu et al. [57] introduced a novel approach, the FNF-SVRLP (False Neighbours Filtered-Support Vector Regression Local Pre-dictor), for short-term natural gas demand forecasting. This method integrates SVR with time series reconstruction and optimizes local predictors. Bai et al. [5] propose a structure-calibrated support vector regression (SC-SVR) approach for forecasting daily natural gas consumption, with online calibration of model parameters using an extended Kalman filter.\nBuilding on the success of AI methods, deep learning approaches have emerged as a powerful tool for natural gas demand forecasting, leveraging advanced neural network architectures to capture intricate patterns and dependencies in the data. Merkel et al. [34] applied deep neural networks for short-term natural gas load forecasting. Su et al. [42] developed a novel Demand Side Management framework for natural gas systems, utilizing a Recurrent Neural Network (RNN) to accurately forecast customer demand based on real-time smart metering data. Laib et al. [27] developed a novel hybrid forecasting method combining a Multi-Layered Perceptron (MLP) and Long Short Term Memory (LSTM) models to improve the accuracy of next-day natural gas con-sumption predictions. Xue et al. [50] proposed an attention gated recurrent unit (AGRU) model for accurately predict-ing city-level natural gas consumption in district heating systems, leveraging GRU's capability to capture temporal dependencies and reducing the vanishing gradient problem. Petkovic et al. [38] proposed a deep learning model based on spatio-temporal convolutional neural networks (DLST) to forecast gas flow in Germany's high-pressure transmission network, leveraging CNN's strength in capturing spatial and temporal dependencies. Song et al. [41] proposed a hybrid model using seasonal decomposition and temporal convolu-tion network (SDTCN) to predict daily natural gas consump-tion, leveraging TCN's ability to handle complex, nonlinear time-varying features in large-scale district heating systems. Pu et al. [39] developed a spatial-temporal multiscale Trans-former network framework combined with a graph neural network (GNN) to enhance the accuracy of short-term nat-ural gas consumption forecasting. This approach captures dynamic spatial dependencies among users and temporal patterns in historical data, effectively addressing the nonlin-ear and irregular nature of natural gas consumption.\nIn recent years, research on contrastive learning has ad-vanced across various fields[37], with related applications in time series processing[51] [28]. These studies have demon-strated the superiority of contrastive learning in enhancing representation capabilities and its effectiveness in time series analysis, offering a promising new direction for improving the accuracy of natural gas demand forecasting."}, {"title": "2.3. Foundation Models for Time Series", "content": "Recent advancements in machine learning have intro-duced the concept of foundation models\u2014large, pre-trained models that can be fine-tuned for specific tasks. Foundation models have demonstrated remarkable generalization capa-bilities, especially in handling diverse and complex datasets. Darlow et al. [11] proposed the DAM model, a neural network designed for universal forecasting, which excels in zero-shot transfer, long-term forecasting, and handling irregularly sampled data across multiple domains by using randomly sampled histories and a transformer backbone. Tu et al. [45] developed the PowerPM model for electricity time series (ETS) data. This model is designed with a self-supervised pretraining framework that significantly im-proves its generalization capabilities across various datasets and applications within the energy sector. Nguyen et al. [35] introduced ClimaX, a deep learning model designed for weather and climate science. ClimaX leverages the Trans-former architecture to handle heterogeneous datasets and is pre-trained on CMIP6 data using self-supervised learning.\nIn summary, while various methods have been explored for natural gas demand forecasting\u2014including statistical methods, AI-based techniques, and other innovative ap-proaches\u2014there remain significant challenges in applying these methods to real-world, large-scale datasets. Founda-tion models, with their ability to generalize across tasks and datasets, offer a powerful solution to these challenges. By addressing the limitations of existing methods, such as their limited generalization capabilities and difficulty handling noisy data, foundation models have the potential to signif-icantly advance the field of natural gas demand forecasting."}, {"title": "3. Methodology", "content": "In this section, we will first introduce our workflow. Then, we will provide detailed sub-points on the design of our data analysis and processing, as well as our pre-training architecture, which includes the industry contrastive learning module and the noise filtering module."}, {"title": "3.1. Workflow and problem definition", "content": "Figure 1 shows our overall workflow. Firstly, we con-ducted primary data collection from city gas companies across the country. Due to the common problem of missing values, outliers and other problems in industrial datasets, we analyzed and pre-processe the data accordingly to ensure it could be better utilized for training and forecasting tasks. We then perform self-supervised pretraining, a phase that consists of two tasks, contrastive learning and noise filtering, with the aim of enhancing the robustness of the model while giving it better generalization capabilities. Finally, we finetune the model for downstream forecasting task and demonstrate the superiority of our model through its perfor-mance on this task.\nThe time series forecasting task is a classical problem in time series data processing, which involves using the char-acteristics of one or more time series in a previous period to forecast the characteristics in a future period. A time series of length t can be represented as X = {x\u2081, x\u2082 , ..., x\u209c } \u2208 \u211d\u1d9c\u02e3\u1d40, where C is the number of channels and T is the length of time series. Since our dataset is a univariable multi-user gas dataset, in our dataset C = 1. We take the historical data of this time series to forecast its future value at t + 1 step, then result X\u209c\u208a\u2081 is:\nX\u209c\u208a\u2081 = f(X\u209c\u208b\u2099:\u209c)\nwhere f(x) represents the forecasting model, andX\u209c\u208b\u2099:\u209c represents the time series of n steps before timestamp t."}, {"title": "3.2. Data acquisition, pre-processing and analysis", "content": "We collected 19,838 records of gas usage data from industrial and commercial users across 84 city gas compa-nies. The data was sourced from various regions nationwide, covering a wide range of industries. Because of the low frequency of gas data generation and the short time span of gas consumption for some industrial and commercial users, some data have the problem of being too short (less than 300 data points) to be used for training and forecasting, we first remove this part of data.\nAmong the dataset, some users have multiple data en-tries or data sheets for the same date because they may have multiple gas meters collecting data simultaneously. We consolidated these entries and sheets for each user on the same date and analyzed the data based on the user as a unit rather than the individual gas meter. This approach allows us to leverage user-specific information more efficiently, including industry relevance.\nAs for outliers, we only removed data with an exces-sive proportion of positions with too large Z-scores, which are likely to be anomalies due to meter malfunction or transmission delays. The reason that other anomalies were not handled is that the anomalies obtained in the anomaly detection of the time series data are likely to be a true value rather than an anomaly in this dataset, for example, sudden peaks in gas usage or troughs in gas usage, and it is difficult to troubleshoot whether these locations are true or anomalous values. Although these locations may be difficult to forecast, we uniformly leave them unaltered for the sake of data fidelity, which is fair to all baselines. The formula for the Z-score is as follows:\nZ = (x \u2212 \u03bc)/\u03c3\nwhere x is the value of the data point, \u03bc is the mean value of the dataset and \u03c3 is the standard deviation of the dataset.\nAnother problem is missing data, which is a perva-sive problem in time series especially in industry datasets. Methods for solving this problem fall into two categories: deletion and imputation. Deleting data, however, leads to incomplete datasets which may result in biased parameter estimates[20]. Thus, imputation becomes a better choice. But how and what numbers to fill is a major challenge, while correct imputation estimates are unbiased [47] and incorrect imputation estimates will introduce extra noise. Many previous works [52] [4] have used statistical or ma-chine learning methods for imputation, but most of them require strong assumptions about the data[6], such as normal distribution or linear regression. These assumptions can lead to biased estimates and affect subsequent analysis results. Recently, much work has been done using deep learning for imputation, achieving state-of-the-art results, so we chose SAITS [15] as our imputation model. SAITS is a model based only on attention mechanism, which allows it to run faster and be more suitable for our large-scale mutiuser time series data compared to models based on Transformer and Diffusion, and has comparable effects. Since this was not our main work, we simply called the SAITS model for imputation.\nAfter all these processes, the ENN dataset includes real gas consumption data from 14817 customers."}, {"title": "3.3. Model architecture", "content": "Figure 4 shows our pre-training model architecture. Our model consists of four components, including patching, con-trastive learning, encoder and decoder, which is similar to some contrastive learning methods[28] [14]. However, to the best of our knowledge, previous work has focused on masked modeling, which is helpful to better capture temporal de-pendencies, but ignores the problem of noise prevalent in industrial datasets. Due to the presence of noise, the model may learn the wrong features and achieve worse results instead. Therefore, we perform data augmentation by adding noise for contrastive learning and design a noise filtering module to enhance the rubustness of the model to noise for better forecasting. In the following, we will introduce each module in the model individuallly."}, {"title": "3.3.1. Patching.", "content": "Following the work [36], we also use patch embed-ding because patch embedding helps to preserve local se-mantic information and can reduce computational com-plexity and support longer recall windows for better re-sults, and most of the research in recent years has also used patch for embedding[28] [54] [8]. Although in many methods, reversible instance normalization (RevIN) [26] was performed before performing patch embedding, but after perform RevIN the metrics declined instead. We believe it is due to the fact that RevIN makes the sequences over smooth, preventing the model from learning useful feau-tures. Therefore, we directly perform patch embedding and position embedding.\nH\u209c = Patch_Embed(X\u209c) + W\u209a\u2092\u209b\nwhere H\u209c represents the embedding of a time series data X\u209c, and W\u209a\u2092\u209b denotes a learnable position embedding, which helps the model recognize the order of elements in a se-quence and provide information about the relative positions of data points within the sequence."}, {"title": "3.3.2. Contrastive learning task with negative sample exclusion", "content": "Our contrastive learning is divided into two parts, over-lapping sampling contrast and noise addition contrast.\nIn overlapping sampling contrast, by overlapping sam-ples, we can get more pairs of positive and negative samples and can better utilize the limited data. Contrasting these data can better capture the temporal continuity and enhance the robustness of the model. In a batch, B pairs of overlapping sampled samples can be obtained, with the two sequences in each sample pair denoted as X\u2071 and X\u02b2, which means 2B samples in a batch.\nPrevious work has paid little attention to sequence sim-ilarity, often treating all samples other than positive ones as negative samples. This approach overlooks the correlations between sequences caused by similar industry character-istics and gas usage patterns, potentially misleading the representation learning process when construction negative samples [31]. Of course, we cannot directly treat these sim-ilar samples as positive samples either, because we cannot ensure that they have feature patterns identical to the original positive samples. Our dataset includes industry information, so we use this information along with sample similarity to guide the selection of negative samples. Specifically, we exclude samples from the same primary industry and the most similar samples as false-negative samples. We first compute the cosine similarity between the samples:\nS(H\u2071, H\u02b2) = (H\u2071 \u22c5 H\u02b2) / (||H\u2071|| \u22c5 ||H\u02b2||)\nwhere i and j represent the ith and jth samples and 0 \u2264 i, j < 2B, i \u2260 j. False-negative samples can be represented as:\nFN\u2081(i) = {j | S(H\u2071, H\u02b2) \u2208 top\u2096(S(H\u2071), N)}\nFN\u2082(i) = {j | industry(i) == industry(j), j \u2208 N}\nFN(i) = FN\u2081(i) \u222a FN\u2082(i)\nwhere N represents the set of negative samples. Then, our overlapped sampling contrastive learning loss can be ex-pressed as follows:\nL\u209b\u209b\u2097\u2081 = - (1/B) \u03a3\u1d62\u208c\u2081\u1d2e log (exp((H\u2071 \u22c5 H\u02b2)/\u03c4) / \u03a3\u2c7c\u2208\u2082B \\ FN(i), j\u2260i exp((H\u2071 \u22c5 H\u02b2)/\u03c4))\nwhere B \\ FN(i) means the set of sample in this batch but not in FN(i), and \u03c4 is the temperature parameter.\nInspired by Denoising Diffusion Probabilistic Models (DDPMs) [22], we introduce a noise filtering task that in-volves adding noise to the data and learning how to remove it. Unlike DDPMs, which aim for direct forecasting, our method focuses on enhancing the model's robustness to noise. Therefore, it is unnecessary to add noise multiple times until the data becomes pure noise. Instead, we add noise only once, viewing this as a form of data augmentation, which makes it more suitable for integration with contrastive learning. To further strengthen the data representations, we enhance the original data by incorporating a small amount of negative sample embeddings into the positive samples, specifically as outlined below:\n(H\u2071)' = (1 \u2212 \u03b1)H\u2071 + \u03b1H\u02b2\nwhere \u03b1 is a weighting parameter, and i \u2260 j. For con-venience, here we did not use negative sample exclusion, which did not significantly affect the results. Then, our noise addition contrastive learning can be expressed as follows:\nL\u209b\u209b\u2097\u2082 = - (1/B) \u03a3\u1d62\u208c\u2081\u1d2e log (exp(H\u2071 \u22c5 (H\u02b2)')/\u03c4) / \u03a3\u2c7c\u2208\u2082B \\ \u1d62, \u2c7c\u2260\u1d62 exp(H\u2071 \u22c5 H\u02b2)/\u03c4)"}, {"title": "3.3.3. Noise filtering task", "content": "Contrastive learning task can generate better representa-tions, but simply adding noise to contrastive learning does not sufficiently address the challenges posed by noisy data. To further reduce the impact of noise, we design a noise filtering module to counteract this problem. Specifically, we train a decoder and a denoising head to attempt to restore noisy data to its original form, with the denosing head being a multi-layer perceptron (MLP). Due to the strong temporal correlation between the overlapping sampled samples and the noise samples, they can be used to assist in the denoising task. Here are the formulas for self-attention in encoder and decoder:\nQ = (H\u2071)'W\u146b, K = H\u2071W\u1d37, V = H\u2071W \u1d5b\nAttention(Q, K, V) = softmax(QK\u1d40/\u221ad\u2096)V\nWe use Smooth L1 loss to constrain them.\nL\ud835\udcb9\u2091 = (1/B) \u03a3\u1d62\u208c\u2081\u1d2e {0.5(X\u2071\u208bX\u0302\u2071)\u2082 / B, if |X\u2071\u208bX\u0302\u2071| < \u03b2, (X\u2071\u208bX\u0302\u2071) \u2212 0.5\u03b2, if |X\u2071\u208bX\u0302\u2071| \u2265 \u03b2}\nwhere X\u0302\u2071 is the denoised sequence of the model, and \u03b2 = 0.01 is the hyperparameter for Smooth L1 loss."}, {"title": "3.3.4. Loss function and finetuning", "content": "The overall objective function of our model is a weighted combination of contrastive loss L\u209b\u209b\u2097\u2081, L\u209b\u209b\u2097\u2082, and denoising loss L\ud835\udcb9\u2091 defined as:\nL = (1 \u2212 \u03bb\u2081 \u2212 \u03bb\u2082)L\ud835\udcb9\u2091 + \u03bb\u2081L\u209b\u209b\u2097\u2081 + \u03bb\u2082L\u209b\u209b\u2097\u2082\nwhere \u03bb\u2081 and \u03bb\u2082 are weighted hyperparameters of the loss. In the finetuning phase, we use Mean Squared Error (MSE) loss on the downstream forecasting task for further parameter optimization. Moreover, we use the signal decay-based loss function proposed by research [46], as the near-future loss would contribute more to generalization improve-ment than the far-future loss. Then the loss function becomes as follows:\nL = (1/B) \u03a3\u1d62\u208c\u2081\u1d2e (1/I\u2071) (X\u0302\u2071\u209c - X\u2071\u209c)\u00b2\nwhere I is the distance from the starting point of the fore-casting."}, {"title": "4. Experiment", "content": "In this section, we conduct extensive experiments on a dataset collected by several city gas companies to evaluate the effectiveness of the proposed method. We will describe in detail our experimental setup, relative analysis and discus-sion."}, {"title": "4.1. Experimental setup", "content": "Our dataset comes from ENN Energy Holdings Co.Ltd, one of the largest clean energy distributors in China, which has collected a large amount of historical natural gas usage data from consumers in different sectors over the past 30 years. In order to validate the effectiveness of our model on the natural gas dataset, we conducted experiments on nearly 7 years of natural gas consumption data collected by ENN Energy from various industries.\nDue to the characteristics of our dataset, directly follow-ing the time series forecasting convention of dividing each customer's data into training, validation and test sets in a 7:1:2 ratio may lead to excessively short validation and test set as well as data leakage problems. Therefore, we use the last six months of data as the test set, and all other data before that are divided into training and validation sets in a 7:1 ratio for a more fair and reasonable experiment."}, {"title": "4.1.2. Baselines", "content": "We compare our model extensively with 11 recent state-of-art models. Among these, 3 are self-supervised models: PITS[28], SimMTM[14] and the self-supervised version of PatchTST[36]. The remaining 9 are end-to-end mod-els: iTransformer[32], TimesNet[48], DLinear[53], Nonsta-tionary Transformer[33], Pyraformer[30], FEDformer[56], Autoformer[7], the supervised version of PatchTST[36], and Informer[55]."}, {"title": "4.1.3. Evaluation metrics", "content": "We use mean square error (MSE) and mean absolute error (MAE), two widely used metrics in time series fore-casting, to evaluate the medium- and long-term forecasting capability of the model:\nMSE = (1/n) \u03a3\u1d62\u208c\u2081\u207f (\u0177\u1d62 - y\u1d62)\u00b2\nMAE = (1/n) \u03a3\u1d62\u208c\u2081\u207f |\u0177\u1d62 - y\u1d62|\nThe performance of our model over the entire dataset with different forecast lengths and zero-shot capability validation will be evaluated using these two metrics.\nFor short-term forecasting capability, we will use the commonly employed short-term forecasting metrics SMAPE and MASE for evaluation:\nSMAPE = (1/n) \u03a3\u1d62\u208c\u2081\u207f (\u0177\u1d62 - y\u1d62) / ((|y\u1d62| + |\u0177\u1d62|)/2)\nMASE = (1/(n - m)) \u03a3\u1d62\u208c\u2098\u208a\u2081\u207f |y\u1d62 - y\u1d62\u208b\u2098|\nWhere m is the seasonal period, because our dataset do not have a clear uniform period, here m = 1. To better align with real-world applications, users with higher gas consumption should receive more accurate predictions, and the scale of different user data should impact the evaluation metrics. Therefore, our SMAPE calculation includes data denormalization."}, {"title": "4.2. Results analysis and discussion", "content": "The main results of our experiments are divided into long-term and short-term forecasting. Long-term forecasting is crucial for strategic planning and policy-making in the natural gas industry, while short-term forecasting aids in operational efficiency and real-time decision-making. Given the multi-customer nature of our dataset, we evaluate our model's performance in two distinct ways: by comparing the overall mean MSE and MAE against all baselines, and through scatter plots illustrating the performance for each customer against a specific baseline.\nFor long-term forecasting, we set the prediction horizons to 60, 90, 120, 150, and 180 days. As shown in Table 2, the accuracy of the model predictions decreases as the prediction horizon increases, while the lower MSE and MAE values indicate better prediction accuracy. Our model is compared against eight end-to-end models and three self-supervised learning baselines. The results demonstrate that our model consistently outperforms all others across all prediction horizons, highlighting its significant advantage in long-term forecasting. Compared to the best-performing baseline model, our model demonstrates a 3.68% improve-ment in the MSE metric and a 3.71% improvement in the MAE metric.\nIn our short-term forecasting analysis, we examined pre-diction horizons of 7, 15, and 30 days. As detailed in Table 3, our model maintains high predictive accuracy across these shorter intervals. The comparison with the same eleven base-lines reveals that our model consistently delivers superior results. Specifically, the improvements in MASE metrics, relative to the best baseline models, reached 6.15%.\nIn this section, we explore the performance of our model under transfer learning scenarios, which were designed to assess its robustness and generalizability. The transfer learn-ing experiments involved randomly partitioning the dataset into two distinct subsets, Part I and Part II. Specifically, each customer in the dataset was randomly assigned a binary label (0 or 1), with those labeled as 0 being allocated to Part I and those labeled as 1 to Part II. After partitioning, Part I contained data from 8,139 users, while Part II had data from 6,678 users. This random assignment allowed us to conduct pre-training on one subset before fine-tuning the model on the other, effectively simulating a real-world scenario where a model trained on one group of customers is adapted for use on a different group.\nFor the transfer learning experiments, we compared our model against three state-of-the-art self-supervised learning baselines: PITS, PatchTST, and SimMTM. We evaluated model performance in both short-term and medium- to long-term forecasting contexts. The short-term forecasting task focused on prediction horizons of 7, 15, and 30 days, while the medium- and long-term forecasting task examined hori-zons of 60, 90, and 120 days.\nAcross all scenarios, our model consistently outperforms the baseline methods, achieving the best results in both SMAPE and MASE for short-term forecasting, as well as MSE and MAE for medium- and long-term forecasting.\nIn this section, we explore the performance of our model in zero-shot scenarios, which helps to validate the model's prediction accuracy for unseen user sequences and better use in real application scenarios. Similar to transfer learning scenarios, the zero-shot scenarios also require two subsets for the corresponding experiments, so we use the same randomly partitioned subsets Part I and Part II as in section 4.2.2.\nAlso in a similar setup to the transfer learning sce-nario, we compared our models to three state-of-the-art self-supervised learning baselines: PITS, PatchTST, and SimMTM. We pre-trained and fine-tuned all of these models on one subset, and tested them on another subset, which corresponded to the prediction of gas usage of newly added customers in a real scenario. We similarly evaluated model capabilities on short-term prediction tasks with prediction steps of 7, 15, and 30 and on medium- and long-term prediction tasks with prediction steps of 60, 90, and 120.\nThe performance of our model compared to the baselines is shown in Table 5. As shown in the table, our model consis-tently outperforms the baseline methods on all metrics. This demonstrates that our model effectively generalizes from large volumes of time-series data, enabling it to make more accurate predictions for user data not included in the training set.\nIn Figure 7, we show the visualized forecast results of our model and PatchTST for different industries at different forecast horizons. The users from three different industries presented clearly show the significant differences in gas us-age characteristics between the various industries. For exam-ple, the petroleum, coal, and other raw material processing industry exhibits more continuous and regular changes in gas use; the accommodation and catering industry experiences frequent and unstable fluctuations, making accurate fore-casts more challenging; the glass and glass products man-ufacturing industry sometimes shows frequent changes with large amplitudes, while at other times, it remains relatively stable and regular. After denormalization, it can be observed that the petroleum, coal, and other raw material processing industry and the glass and glass products manufacturing industry typically have higher gas consumption, whereas the accommodation and catering industry, despite significant fluctuations, generally has lower gas consumption."}, {"title": "4.2.5. Ablation study", "content": "For each of the pre-trained components mentioned in this paper, we also performed the corresponding ablation experiments, as shown in Table6. Due to dataset limita-tions, we were unable to conduct ablation experiments using the longer historical and forecast horizons employed by PITS[28"}]}