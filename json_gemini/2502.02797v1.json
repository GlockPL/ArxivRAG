{"title": "Upweighting Easy Samples in Fine-Tuning Mitigates Forgetting", "authors": ["Sunny Sanyal", "Hayden Prairie", "Rudrajit Das", "Ali Kavis", "Sujay Sanghavi"], "abstract": "Fine-tuning a pre-trained model on a downstream task often degrades its original capabilities, a phenomenon known as \"catastrophic forgetting\". This is especially an issue when one does not have access to the data and recipe used to develop the pre-trained model. Under this constraint, most existing methods for mitigating forgetting are inapplicable. To address this challenge, we propose a sample weighting scheme for the fine-tuning data solely based on the pre-trained model's losses. Specifically, we upweight the easy samples on which the pre-trained model's loss is low and vice versa to limit the drift from the pre-trained model. Our approach is orthogonal and yet complementary to existing methods; while such methods mostly operate on parameter or gradient space, we concentrate on the sample space. We theoretically analyze the impact of fine-tuning with our method in a linear setting, showing that it stalls learning in a certain subspace which inhibits overfitting to the target task. We empirically demonstrate the efficacy of our method on both language and vision tasks. As an example, when fine-tuning Gemma 2 2B on MetaMathQA, our method results in only a 0.8% drop in accuracy on GSM8K (another math dataset) compared to standard fine-tuning, while preserving 5.4% more accuracy on the pre-training datasets. Our code is publicly available at https://github.com/sanyalsunny111/FLOW_finetuning.", "sections": [{"title": "Introduction", "content": "In the modern era of large-scale machine learning, one of the central goals is to design models capable of performing multiple tasks. Traditionally, this is achieved by training an appropriately large model over datasets of multiple tasks, ensuring that the model jointly learns multiple tasks at once. Unfortunately, it is not viable to repeat this process with every new additional task due to the scale of contemporary models, necessitating effective strategies that can essentially learn without full retraining. A resource-efficient convention in machine learning is to take a pre-trained model which is trained on some vast and diverse dataset, and fine-tune it on a new dataset/task. Such pre-trained models are typically large and expensive to train from scratch but perform well on a variety of tasks while offering a versatile basis for learning a new task.\nFine-tuning is a delicate process that should ideally serve multiple objectives simultaneously; we would like to use the base model and its capabilities to facilitate learning a strong model on the downstream task, and in the meantime, preserve the existing abilities of the pre-trained model. On this particular front, the major challenge in standard, unregulated fine-tuning is the catastrophic"}, {"title": "Related Work", "content": "We begin by summarizing the vast literature on catastrophic forgetting with a focus on prior works most relevant to our proposed setting. For a streamlined presentation, we survey prior work in two settings - data-aware and data-oblivious. We refer the reader to Appendix A for a more detailed and explanatory literature review."}, {"title": "Mitigating Catastrophic Forgetting", "content": null}, {"title": "Data-aware approaches", "content": "The majority of the approaches for mitigating forgetting assume task-specific knowledge access to different extents; either (a subset of) the pre-training dataset itself or some information/statistic computed from pre-training data. Below, we describe the data-aware approaches based on how they make use of task-specific knowledge."}, {"title": "Regularization-based methods", "content": "This line of work aims to preserve existing capabilities by keeping the parameters close to the pre-trained model. The key idea is to introduce task-specific regularization to penalize modifications along the \"important\" directions for the old tasks [Ahn et al.,"}, {"title": "Optimization-driven methods", "content": "Another perspective to mitigating forgetting is guiding the optimization process by constraining the algorithms directly as opposed to manipulating the loss function. The core idea is to keep track of \"important directions\" for the old tasks, and train on the new task \"orthogonally.\" This could be done by storing prior data samples or gradients in a buffer [Lopez-Paz and Ranzato, 2017, Farajtabar et al., 2020, Chaudhry et al., 2019a] or by incrementally expanding the subspace of important directions without storing task-specific information [Zeng et al.,"}, {"title": "Replay-based methods", "content": "A more direct approach is to store old task samples in buffers and introduce them into the training process for the new task to refresh task-specific representations periodically. There are several components to such methods. Some prior work focus on data selection based on the nature of old data access [Rebuffi et al., 2017, Aljundi et al., 2019, Bang et al., 2021, Chaudhry et al., 2019b, Isele and Cosgun, 2018, De Lange and Tuytelaars, 2021, Borsos et al., 2020, Tiwari et al., 2021] (e.g., streaming versus on-demand). Another important perspective is the re-introduction strategy of the stored information into the fine-tuning process [Silver and Mercer,"}, {"title": "Architecture-driven methods", "content": "Another technique to limit interference between tasks is to allocate a separate trainable set of parameters per task. This could be done by initializing a sub-networks per new task [Rusu et al., 2016, Aljundi et al., 2017, Collier et al., 2020, Rajasegaran et al., 2019, Ramesh and Chaudhari, 2021, Wang et al., 2023b, 2022a], gradually expanding the parameters of a base network [Yoon et al., 2018, Ostapenko et al., 2019, Hung et al., 2019], or segregating a fixed model into task-specific subsets [Mallya et al., 2018, Kang et al., 2022, Serra et al., 2018, Wortsman et al., 2020, Mallya and Lazebnik, 2017, Mustafa B Gurbuz, 2022, Jung et al., 2020]. The main downside with this line of work is that task identities must be known for inference to (de)activate relevant sub-networks [Aljundi et al., 2017]."}, {"title": "Data-oblivious approaches", "content": "In the less-explored data-oblivious setting, it is particularly challenging to devise a principled approach, as there is no access to any data-specific information, except for the pre-trained model. One line of work explores the simple idea of \u201cmodel averaging\u201d (MA) which essentially does a convex combination of the parameters of the pre-trained model and that of the fully fine-tuned model for the new task. MA and more sophisticated model merging variants have been studied in relevant context to forgetting [Lubana et al., 2021, Wortsman et al., 2021, Ilharco et al., 2023, Lin et al., 2023, Kleiman et al., 2025]. Some recent works [Chen et al., 2024a, Panda et al., 2024] introduce different strategies to selectively update a subset of parameters in a pre-training data-agnostic manner. Finally, Biderman et al. [2024] has shown that LoRA Hu et al. [2022] could be effective for mitigating catastrophic forgetting in transformers. Unlike the methods discussed above which focus"}, {"title": "Sample Selection and Weighting", "content": "Sample-wise importance selection/weighting has been studied in optimization papers [Needell et al., 2014, Zhao and Zhang, 2015, Alain et al., 2015, Stich et al., 2017] and ML papers [Loshchilov and Hutter, 2015, Shrivastava et al., 2016, Katharopoulos and Fleuret, 2017, 2018, Kawaguchi and Lu, 2020, Das et al., 2024] to speed up the optimization/training process by reducing the variance of the gradient updates. Such papers advocate focusing on \"hard\" samples with high-gradient norms or losses. In contrast, we focus on \"easy\" samples to mitigate forgetting. Another line of work focuses on robust learning under uncertain data distributions. Distributionally robust optimization (DRO) proposes to minimize the worst-case weighted loss, where the sample weights are constrained or regularized [Ben-Tal et al., 2013, Levy et al., 2020, Duchi and Namkoong, 2021, Qi et al., 2021]. Some recent works [Xie et al., 2024, Chen et al., 2024b, Anonymous, 2025] propose dynamic sample-weighting strategies for LLM training based on the previously discussed ideas."}, {"title": "Notation and Definitions", "content": "1(.) denotes the indicator variable. For any $n \\in \\mathbb{N}$, the set ${1, ..., n}$ is denoted by $[n]$. Vectors and matrices are in lowercase and uppercase bold font, respectively. The $l_p$ norm of a vector v is denoted by $||v||_p$. The inner product between two vectors v and v' is denoted as $\\langle v, v' \\rangle$. A set of n linearly independent n-dimensional vectors ${u_1, ..., u_n}$ is said to be an orthonormal basis for $\\mathbb{R}^n$ if $\\langle u_i, u_j \\rangle = 1(i = j)$. A vector $v = [v_1,..., v_n]^\\intercal$ is said to belong to the n-dimensional probability simplex $\\Delta_n$ if $\\sum_{i=1}^n v_i = 1$ and $v_i \\geq 0 \\forall i \\in [n]$. For any $n \\in \\mathbb{N}$, $I_n$ denotes the identity matrix of dimension n."}, {"title": "Proposed Algorithm", "content": "Our proposed algorithm consists of two main steps: (i) computing weights for the samples based on their respective pre-trained loss values; and (ii) fine-tuning with a weighted loss wherein the per-sample losses are scaled by their respective weights. The sample-wise weights are computed once and used throughout the entire fine-tuning process. We formally state our proposed fine-tuning protocol in Algorithm 1 and delve into its design details in the sequel."}, {"title": "Algorithm design", "content": "Our main intuition is that we can control forgetting by not drifting away too much from the pre-trained model (i.e., 0*) during fine-tuning. In the presence of pre-training data, this is done by introducing data-dependent constraints on the parameter space or gradient space. Since we have no access to pre-training data, we redirect our focus towards strategies on the sample space depending only on the pre-trained model.\nTo that end, we propose to infer the easiness of each sample of the fine-tuning dataset with respect to the pre-trained model, based on the per-sample losses $f_i(\\theta^*)`s (see Alg. 1). We say that the ith sample is \"easy\u201d if $f_i(\\theta^*)$ is \u201csmall\u201d.\u00b9 Intuitively, prioritizing the \u201ceasy\u201d samples during fine-tuning would limit the drift from 0*. On the other hand, over-focusing on the \"easy\" samples would probably lead to poor performance on the fine-tuning task. Thus, it is important to strike a balance.\nLet us formalize these ideas mathematically. For fine-tuning on the new task, let us consider the objective function $L_{\\pi}(\\theta) = \\sum_{i=1}^n \\pi_i f_i(\\theta)$, where $\\pi = [\\pi_1,...,\\pi_n]^\\intercal$ is a static design-choice $\\in \\Delta_n$ (i.e., $\\sum_{i=1}^n \\pi_i = 1$ and $\\pi_i > 0 \\forall i\\in [n]$) which we allow to only depend on the pre-trained model's losses ${f_i(\\theta^*)}i=1^n$ (and not the current model's losses ${f_i(\\theta)}i=1^n$). We would like to design \u03c0 so that:\n1. for all i \u2260 j such that $f_i(\\theta^*) \\leq f_j(\\theta^*)$, $\\pi_i \\geq \\pi_j$,\n2. \u03c0 does not concentrate around one or a few samples but rather spreads uniformly over the samples.\nThese two requirements can be enforced by minimizing the following function (w.r.t. \u03c0) involving negative entropic regularization:\n$g(\\pi) = \\sum_{i=1}^n \\pi_i f_i(\\theta^*) + \\tau \\sum_{i=1}^n \\pi_i \\log \\pi_i$                                                                                                (1)\nHere T > 0 is a parameter controlling the extent of the second requirement which is facilitated by the entropy term. We now state the minimizer of $g(\\pi)$ (proof is in Appendix C)."}, {"title": "Proposition 4.3.", "content": "Let $\\pi^* = [\\pi_1,...,\\pi_n]^\\intercal = \\arg\\min_{\\pi \\in \\Delta_n} g(\\pi)$. Then we have\n$\\pi_i^* = \\frac{1}{Z} \\exp \\big( - \\frac{f_i(\\theta^*)}{T} \\big)$,\nwhere $Z = \\sum_{i=1}^n \\exp \\big( - \\frac{f_i(\\theta^*)}{T} \\big)$ is the normalizing factor.\nModulo the normalizing factor Z (it does not matter when optimizing w.r.t. \u03b8), note that $w_i$ and $L(\\theta)$ in Algorithm 1 are equivalent to $\\pi_i^*$ and $L_{\\pi^*}(\\theta)$, respectively."}, {"title": "Distributionally robust optimization (DRO) perspective", "content": "Our formulation above is motivated by prior work on DRO [Qi et al., 2021], but it is exactly the opposite of DRO in spirit. Specifically, in our setting, Qi et al. [2021] consider the following min-max problem:\n$\\min_\\theta \\max_{\\pi \\in \\Delta_n} \\sum_{i=1}^n \\pi_i f_i(\\theta) - \\tau \\sum_{i=1}^n \\pi_i \\log \\pi_i$                                                                             (2)\nThe first term in Eq. (2) is the worst-case weighted loss at \u03b8, while the second term (i.e., entropic regularization) promotes uniform weights. The optimal solution to the inner max function w.r.t. \u03c0 turns out to be $\\pi_i^* \\propto \\exp \\big(-\\frac{f_i(\\theta)}{T} \\big)$. Note that this is essentially the inverse of our weighting function (modulo the normalizing factor) because it assigns a higher weight to samples with larger losses (i.e., the \"hard\" samples). The weighting function of DRO would be very conducive to forgetting because it focuses more on the \u201chard\u201d samples. Further, our weighting function is static (or one-shot) as it depends only on the losses at 0*. On the other hand, the weighting function of DRO is dynamic (i.e., it depends on the current point \u03b8). In fact, after plugging in the optimal value of  into Eq. (2) and simplifying, the DRO objective reduces to $\\min_\\theta \\sum_{i=1}^n \\exp \\big(\\frac{f_i(\\theta)}{T} \\big)$; this is noticeably different from our objective $L(\\theta)$ in Algorithm 1."}, {"title": "Experimental Setup", "content": "We empirically evaluate the performance of FLOW (Algorithm 1) on vision and language tasks, showcasing its effectiveness across different model architectures and modalities. Here, we explain the details of our experiments: baselines, model architectures, datasets, and evaluation metrics.\nBaselines. In our language and vision experiments, we compare FLOW against relevant baselines in the data-oblivious setting, namely, standard fine-tuning (fine-tuning with vanilla unweighted loss), 12-regularization [following Kirkpatrick et al. [2016]], and WiSE-FT [Wortsman et al., 2021] (model averaging of pre-trained and standard fine-tuned models). Additionally, we compare against linear probing (fine-tuning only the classification head, keeping the body frozen) in vision experiments and low-rank adaptation (LoRA) [Hu et al., 2022] in language experiments. More details on baselines can be found in Appendix G.1."}, {"title": "Vision Experiments", "content": "We study the performance of FLOW and associated baselines in a transfer learning setup.\nModels. We consider ResNet-18 and ResNet-50 models pre-trained on Imagenet-1K [Russakovsky et al., 2015] taken from Wightman et al. [workshop].\nDatasets. We select six widely-used image classification datasets: CIFAR-10 [Krizhevsky, 2009], CIFAR-100 [Krizhevsky, 2009], Flowers102 [Nilsback and Zisserman, 2008], Caltech101 [Li et al., 2022], Cars [Krause et al., 2013], and Dogs [Parkhi et al., 2012].\nEvaluation metrics. Vision models are trained with task-specific parts, such as classification head (head) and batch-norm (BN); see Appendix B for how FLOW works with with task-specific"}, {"title": "Language Model Experiments", "content": "We follow a similar setup to Biderman et al. [2024], Chen et al. [2024a], where a language model's general capabilities are evaluated before and after fine-tuning on a mathematical reasoning dataset. All training for language experiments is done with HuggingFace peft [Mangrulkar et al., 2022], transformers [Wolf et al., 2020], datasets [Lhoest et al., 2021], and accelerate [Gugger et al., 2022].\nModels. We use Gemma 2 2B [Team et al., 2024] and Llama 3.2 3B [Grattafiori et al., 2024] as our base language models. Further details on training hyper-parameters can be found in Appendix G.2.\nDatasets. Following previous works [Biderman et al., 2024, Chen et al., 2024a], we fine-tune on MetaMathQA [Yu et al., 2023], a mathematical reasoning dataset that is bootstrapped from the training set of GSM8K [Cobbe et al., 2021] and MATH [Hendrycks et al., 2021a] using a LLM. We train with all 395K samples in MetaMathQA."}, {"title": "Experimental Results", "content": null}, {"title": "Comparing FLOW and Related Baselines", "content": "For vision experiments, Table 1 lists the accuracies of all the baselines and FLOW. Our findings are consistent among the two vision models, so we focus on the larger ResNet-50 model. The pre-trained ResNet-50 model achieves a top-1 accuracy of 79.02% on ImageNet-1K's validation set. Standard fine-tuning experiences a significant 42.11% drop in IN-1K accuracy, while achieving an average fine-tuning accuracy of 91.78% across the target datasets. In contrast, FLOW suffers only a 2.93% drop in IN-1K accuracy and exhibits a reasonable 86.25% average accuracy on target fine-tuning datasets, demonstrating a significant improvement over standard fine-tuning. Overall, FLOW's average on IN-1K and target domain accuracy is 16.83% higher than standard fine-tuning."}, {"title": "Combining FLOW with Related Baselines", "content": "To complement our results in Tables 1 and 2, we investigate the performance of baselines when combined with FLOW. In the vision setting, we consider uniform model averaging with WiSE-FT (with $\\alpha = 0.5$) and report its performance with and without FLOW in Table 3. Interestingly, averaging the pre-trained IN-1K model and the fine-tuned model obtained with FLOW improves over standard WiSE-FT (i.e., averaging the pre-trained IN-1K model and the standard fine-tuned model) by 4.18% and 4.52% for ResNet-18 and ResNet-50, respectively, in average performance.\nFurther, as seen in Table 4, FLOW boosts the performance of other baselines in language modeling. When combined with 12-regularization, we observe improvements in general capability between 0.5% and 1.80%, with only a 0.83% reduction in GSM8K performance. Furthermore, the integration of FLOW with LoRA yields even stronger results, enhancing general capability performance by 1.07% to 3.40%, while simultaneously improving GSM8K performance by 1.06%. Further details and discussion combining FLOW with 12-regularization and LoRA are in Appendix I.1."}, {"title": "Theoretical Analysis", "content": "Here we consider linear pre-training and fine-tuning tasks\u00b2 and theoretically analyze the effect of fine-tuning with our proposed method FLOW (Algorithm 1). Specifically, we compare the non- asymptotic trajectories of FLOW and vanilla fine-tuning. A key insight of our analysis is that FLOW stalls training in a certain direction, impeding overfitting to the fine-tuning task (see Remark 7.4). We also demonstrate that FLOW goes beyond the simple idea of model averaging (see Remark 7.5).\nWe begin by describing the problem setting.\nPre-training task: The label $y \\in \\mathbb{R}$ for a d-dimensional data point $x \\sim P$ is given by $y = \\langle \\theta^*, x \\rangle$, where $\\theta^* \\in \\mathbb{R}^d$ is the ground-truth model. Let $D$ denote the joint distribution of $(x, y)$, where $x \\sim P$. Let $\\Sigma = \\mathbb{E}_D[xx^\\intercal]$ be the data covariance matrix. Without loss of generality, let $\\Sigma = I_d$.\nFine-tuning task: The label $\\tilde{y} \\in \\mathbb{R}$ for a d-dimensional data point $x \\sim P$ is given by $\\tilde{y} = \\langle \\theta, x \\rangle$, where $\\theta \\in \\mathbb{R}^d$ is the ground-truth model. Let $\\tilde{D}$ denote the joint distribution of $(x, \\tilde{y})$, where $x \\sim P$. Also, let\n$e := \\theta^* - \\theta, \\ \\ \\ \\bar{e} := \\frac{e}{||e||_2}$,\nand $\\bar{e}_\\bot$ be a unit vector orthogonal to $\\bar{e}$. We consider the case of $P = \\mathcal{N}(\\bar{0}, \\Sigma)$, where\n$\\Sigma = I_d + \\rho(\\bar{e}\\bar{e}^\\intercal + \\bar{e}_\\bot\\bar{e}_\\bot^\\intercal),$                                                                              (3)\nwhere $\\rho \\in [0, 1)$ is a constant. Note that $\\Sigma$ is the data covariance matrix here."}, {"title": "Remark 7.1 (Regarding $\\Sigma$)", "content": "We study the case of $\\Sigma$ as given in Eq. (3) because it is the minimal analytically tractable case where we can show that FLOW goes beyond model averaging (MA) (see Remark 7.5). Specifically, if $\\rho = 0$ and $\\Sigma = I_d$, then FLOW reduces to MA. Moreover, for an arbitrary $\\Sigma$, characterizing the eigen-spectrum of the matrix dictating the trajectory of FLOW becomes intractable. For the analysis to be tractable, we need some relationship between $\\Sigma$ and e (i.e., the difference between the optima of the pre-training and fine-tuning tasks)."}, {"title": "For a model parameterized by $\\theta \\in \\mathbb{R}^d$, let", "content": "$\\text{err}_1(\\theta) := \\mathbb{E}_D \\big[ (y - \\langle \\theta, x \\rangle)^2 \\big] = (\\theta - \\theta^*) ^\\intercal \\Sigma (\\theta - \\theta^*)$,\n$\\text{err}_2(\\theta) := \\mathbb{E}_\\tilde{D} \\big[ (\\tilde{y} - \\langle \\theta, x \\rangle)^2 \\big] = (\\theta - \\theta) ^\\intercal \\Sigma (\\theta - \\theta)$                                                                    (4)\nbe the population errors on the pre-training and fine-tuning tasks, respectively. Also, the total error with $\\theta$ on the two tasks is denoted by $\\text{err}_{\\text{tot}}(\\theta) = \\text{err}_1(\\theta) + \\text{err}_2(\\theta)$.\nWe assume that initially, we learn $\\theta^*$ with the pre-training data; so $\\theta^*$ is our pre-trained model. Note that\n$\\text{err}_{\\text{tot}} (\\theta^*) = \\text{err}_2(\\theta^*) = e^\\intercal \\Sigma e = ||e||_\\Sigma^2,$                                                                                (5)\nwhere the last step follows by using $\\Sigma$ from Equation (3).\nWe start fine-tuning starting from $\\theta^*$. Specifically, we assume access to the population $(x, \\tilde{y}) \\sim \\tilde{D}$ of the fine-tuning task, but we lose access to the pre-training data.\nVanilla fine-tuning (FT): We minimize $\\text{err}_2(\\theta)$ (Eq. (4)) with gradient descent (GD) starting from $\\theta^*$ using a constant learning rate $\\eta$. Our iterate $\\tilde{\\theta}_K$ at the Kth iteration is given by (using the value of $\\Sigma$ from Eq. (3) and $\\theta - \\theta^* = e$):\n$\\tilde{\\theta}_K = \\theta^* + (I_d - 2\\eta(I_d + \\rho(e e^\\intercal + e_\\bot e_\\bot^\\intercal)) )^K e.$                                                                       (6)\nFLOW: For some temperature T, the weight of $(x, \\tilde{y}) \\sim \\tilde{D}$ is $w(x, \\tilde{y}) = \\exp \\big( - \\frac{(\\tilde{y} - \\langle \\theta, x \\rangle)^2}{T} \\big)$. We minimize\n$\\text{err}_2(\\theta) := \\mathbb{E}_\\tilde{D} \\big[ w(x, \\tilde{y}) (\\tilde{y} - \\langle \\theta, x \\rangle)^2 \\big],$                                                                                     (7)\nwith GD starting from $\\theta^*$ using a constant learning rate $\\tilde{\\eta}$. Suppose our iterate at the Kth iteration is $\\theta_K$."}, {"title": "Theorem 7.2 (FLOW)", "content": "Let $\\mu = \\big( \\frac{T}{T+2||e||_2^2} \\big)^{1/2}$. Then:\n$\\theta_K = \\theta^* + (I_d - 2\\tilde{\\eta} \\Sigma')^K e,$\nwhere $\\Sigma' := \\mu(I_d - Q)$ with\n$Q = (1 - \\mu^2) ee^\\intercal + \\rho^2(1 - \\mu^2) e_\\bot e_\\bot^\\intercal - \\rho\\mu^2 (e e^\\intercal + e_\\bot e_\\bot^\\intercal).$"}, {"title": "Remark 7.1 (Regarding $\\Sigma$)", "content": "We study the case of $\\Sigma$ as given in Eq. (3) because it is the minimal analytically tractable case where we can show that FLOW goes beyond model averaging (MA) (see Remark 7.5). Specifically, if $\\rho = 0$ and $\\Sigma = I_d$, then FLOW reduces to MA. Moreover, for an arbitrary $\\Sigma$, characterizing the eigen-spectrum of the matrix dictating the trajectory of FLOW becomes intractable. For the analysis to be tractable, we need some relationship between $\\Sigma$ and e (i.e., the difference between the optima of the pre-training and fine-tuning tasks)."}, {"title": "for vanilla FT", "content": "Plugging in $\\tilde{\\eta} = \\frac{1}{2\\mu}$ into Eq. (8), we get:\n$\\theta_K = \\theta^* + Qe$, with Q given by Eq. (9)                                                                                       (11)\nfor FLOW. The non-zero eigenvalues of P are $\\pm \\rho$ and the corresponding eigenvectors are $\\frac{1}{\\sqrt{2}}(e \\pm e_\\bot)$. Using this in (10) and simplifying, we get for vanilla FT:\n$\\tilde{\\theta}_K = \\theta^* + \\rho^K (1(\\text{K is even}) e - i(\\text{K is odd}) ||e||_2 e_\\bot).$                                                                                                                                         (12)"}, {"title": "Remark 7.3 (Vanilla FT)", "content": "Since $\\rho < 1$, $\\tilde{\\theta}_K$ converges to $e$ rapidly, and we cannot impede this convergence."}, {"title": "Note that", "content": "we use $\\Sigma \\succeq I_d$ below):\n$\\text{err}_{\\text{tot}} (\\theta^*) = \\text{err}_1(\\theta^*) = e \\Sigma e \\geq ||e||_2^2.$                                                                            (13)"}, {"title": "On the other hand, the non-zero eigenvalues and corresponding eigenvectors of Q are not as straightforward to compute", "content": "We do this computation in Lemma F.3 with the re-parameterization of $\\mu = \\sqrt{\\frac{\\beta(1-\\rho^2)}{(1+\\beta)(1-\\beta\\rho^2)}}$ for some $\\beta \\in (0, 1]$. Using this in Eq. (11) and simplifying, we get for FLOW:\n$\\tilde{\\theta}_K = \\theta^* + \\Big(\\frac{\\beta}{1 + \\beta^2 \\rho^2} \\Big) \\tilde{e} - \\beta \\rho \\frac{1 - \\beta}{1+\\beta} \\frac{1 - \\beta \\rho^2}{1 + \\beta^2 \\rho^2} ||e||_2 \\tilde{e}_\\bot,$                                                                                                                                 (14)\nwhere $\\lambda_1 = \\frac{1 + \\beta \\rho^2}{1 + \\beta}$ and $\\lambda_2 = \\rho^2 \\big( \\frac{1-\\beta}{1 - \\beta \\rho^2} \\big)$."}, {"title": "Remark 7.4 (FLOW's trajectory)", "content": "Note that we can control $\\lambda_1$ by varying $\\beta$. Specifically, we can make $\\lambda_1$ arbitrarily close to 1 by choosing a small enough $\\beta$. On the other hand, $\\frac{1-\\beta}{1 + \\beta} \\frac{1-\\beta \\rho^2}{1 + \\beta^2 \\rho^2} < \\frac{1 + \\beta \\rho^2}{1 + \\beta} = \\lambda_1$ and so, $\\lambda_2 < \\rho^2 \\lambda_1$. Hence, beyond a certain number of iterations K, Eq. (14) effectively becomes:\n$\\tilde{\\theta}_K \\approx \\theta^* + \\gamma(K, \\beta) \\Big( e - \\beta \\rho ||e||_2 e_\\bot  \\Big),$                                                                                                                                (15)\nwith $\\gamma(K, \\beta) := \\big(\\frac{1-\\beta \\rho^2}{1 + \\beta^2 \\rho^2}\\big)^K$. Because we can control $\\lambda_1$ by varying $\\beta$, we can control $\\gamma(K, \\beta)$. Thus, we can stall convergence along $ \\Big( e - \\beta \\rho ||e||_2 e_\\bot  \\Big)$, impeding the convergence of $\\tilde{\\theta}_K$ to $\\theta$*."}, {"title": "The direction", "content": "e - \\beta \\rho ||e||_2 e_\\bot  is the eigenvector of Q with the largest eigenvalue (see Lemma F.3). Since $\\Sigma' = \\mu(I_d - Q)$ (recall $\\Sigma'$ is the covariance matrix of the weighted fine-tuning data as defined in Theorem 7.2), this direction is also the eigenvector of $\\Sigma'$ with the smallest eigenvalue."}, {"title": "Remark 7.5 (FLOW goes beyond model averaging)", "content": "If we perform model averaging between $\\theta^*$ and $\\theta_*$ with parameter $w \\in [0, 1]$, then our averaged model is:\n$\\theta_{\\text{avg}}(w) = w \\theta^* + (1 - w) \\theta_* = \\theta^* + w e$.                                                                                                            (16)\nComparing the above with Eq. (15), we see that FLOW goes beyond model averaging because of the component along $\\tilde{e}_\\bot$. But we can make $\\tilde{\\theta}_K$ (Eq. (15))$\\to \\theta_{\\text{avg}}(w)$ by choosing $\\beta \\to 0$ and K such that $\\gamma(K, \\beta) \\to w$. So, we expect FLOW to be at least as powerful as model averaging."}, {"title": "As per Lemma F.4, the minimum total error on both tasks with optimally tuned model averaging is given by", "content": "$\\"}]}