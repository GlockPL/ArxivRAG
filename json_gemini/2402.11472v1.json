{"title": "DDIPrompt: Drug-Drug Interaction Event Prediction based on Graph Prompt Learning", "authors": ["Yingying Wang", "Yun Xiong", "Xixi Wu", "Xiangguo Sun", "Jiawei Zhang"], "abstract": "Recently, Graph Neural Networks have become increasingly prevalent in predicting adverse drug-drug interactions (DDI) due to their proficiency in modeling the intricate associations between atoms and functional groups within and across drug molecules. However, they are still hindered by two significant challenges: (1) the issue of highly im-balanced event distribution, which is a common but critical problem in medical datasets where certain interactions are vastly underrepresented. This imbalance poses a substantial barrier to achieving accurate and reliable DDI predictions. (2) the scarcity of labeled data for rare events, which is a pervasive issue in the medical field where rare yet potentially critical interactions are often overlooked or under-studied due to limited available data.\nIn response, we offer \u201cDDIPrompt\u201d, an innovative panacea inspired by the recent advancements in graph prompting. Our framework aims to address these issues by leveraging the intrinsic knowledge from pre-trained models, which can be efficiently deployed with minimal downstream data. Specifically, to solve the first challenge, DDIPrompt employs augmented links between drugs, considering both structural and interactive proximity. It features a hierarchical pre-training strategy that comprehends intra-molecular structures and inter-molecular interactions, fostering a comprehensive and unbiased understanding of drug properties. For the second challenge, we implement a prototype-enhanced prompting mechanism during inference. This mechanism, refined by few-shot examples from each category, effectively harnesses the rich pre-training knowledge to enhance prediction accuracy, particularly for these rare but crucial interactions. Comprehensive evaluations on two benchmark datasets demonstrate DDIPrompt's superiority, particularly in predicting rare DDI events.", "sections": [{"title": "1 Introduction", "content": "Drug safety has always been a concern within the medical community [Giacomini et al., 2007; Baraga\u00f1a et al., 2015]. In recent years, there has been an increasing interest in using drug combinations to manage complex diseases, which is promising in improving treatment effectiveness [Jaaks et al., 2022]. However, it is crucial to recognize that combining different drugs can occasionally lead to reduced treatment efficacy and even structural modifications of the drugs themselves, posing risks to human health [Palmer and Sorger, 2017]. These negative reactions, known as drug-drug interaction (DDI) events, carry significant importance when studying drug combinations. Therefore, the prediction of DDI events becomes a critical research area, aiming to mitigate potential risks and refine treatment outcomes.\nDDI event prediction can be modeled as multi-class edge prediction on a graph, where drugs are represented as nodes and their interactions as edges. Graph Neural Networks (GNNs) [Kipf and Welling, 2017; Velickovic et al., 2017] have emerged as a widely-adopted approach to enhance drug representations for improved predictions. However, existing methods of DDI event prediction, which rely on GNNs, often require a significant amount of supervision information [Xiong et al., 2023; Nyamabo et al., 2022b], making them dependent on domain expertise in the field of drug research [Kastrin et al., 2018; Wang et al., 2021]. These methods struggle to address two inherent challenges in this multi-class edge classification task: (1) highly imbalanced DDI event distributions. Adverse drug reactions predominantly manifest in several common scenarios, while rare events are under-represented, resulting in a tendency for models to overfit to the common events and produce biased predictions. (2) label scarcity of certain negative drug combinations. The scarcity of labeled data makes it extremely challenging to predict rare event types, hindering the effectiveness of the model.\nIntuitively, the significant biases in the data highlight the need for supplementary knowledge beyond edge labels during the pre-training stage, and the label scarcity problem requires an efficient approach in the prompt stage that aligns downstream task with pre-training, minimizing the tuning burden to extract preserved knowledge for downstream task. This motivates us to turn to a promising direction, \"pre-train, prompt\", which is originated from the Natural Language Processing (NLP) domain [Brown et al., 2020;"}, {"title": "2 Related Work", "content": "Lester et al., 2021]. It aims at reformulating downstream task to the pre-training one via a very lightweight prompt and keeps the pre-trained model frozen. This enables the efficient utilization of pre-trained knowledge for different downstream tasks without requiring superfluous labels [Liu et al., 2021; Schick and Sch\u00fctze, 2021]. Unfortunately, achieving the above goal is never easy due to two tough problems. The first one is how to learn useful drug knowledge since the observed DDI event labels are highly imbalanced. Although there have been some works trying to explore the \"pre-train, prompt\" paradigm for GNNs [Sun et al., 2023a; Liu et al., 2023c; Sun et al., 2023b; Sun et al., 2022; Fang et al., 2022], most of them employ pre-training objectives that focus on general graph patterns, like link prediction or node feature reconstruction [Tan et al., 2023], which is far from sufficient to capture unique knowledge on drug molecules under so imbalanced data. Another problem is how to extract the preserved knowledge and reduce the tuning burden since the downstream labeled DDI events are so sparse. Existing works are mostly designed for downstream node or graph classification tasks, presenting a significant gap when compared to DDI events. Therefore, we need to design an entirely new graph prompt format to effectively transfer learned knowledge for DDI prediction.\nIn light of this, we propose DDIPrompt, a \"pre-train, prompt\" framework with a newly designed pre-training strategy and a fresh prompt format, both of which are tailored for our few-shot DDI event prediction task. Specifically, to solve the first problem, we aim to augment existing knowledge via mining potential links between drugs based on both structural and interactive proximity relations. We design a hierarchical pre-training method that captures both intra-molecular and inter-molecular relations, ensuring a rich and unbiased understanding of drug properties. Meanwhile, to solve the second problem, during the prompt tuning stage, DDIPrompt incorporates class prototypes to facilitate the distinct representation of each class, thereby realizing the framework's ability to make accurate predictions under few-shot scenarios. These class prototypes, implemented as learnable prompt vectors, are fine-tuned using the available samples, allowing DDIPrompt to adapt to the specific characteristics of each class and improve prediction performance. To summarize, our contributions are as follows:\n\u2022 Existing DDI event prediction methods are vulnerable to imbalanced distribution and label scarcity of rare events. To overcome these limitations, we propose to apply the \"pre-train, prompt\" paradigm and devise DDIPrompt. To the best of our knowledge, this is the first attempt that applies prompting in the drug domain.\n\u2022 Within the DDIPrompt framework, we devise a novel hierarchical pre-training method that captures both intra-molecular structures and inter-molecular relations, enabling a rich and unbiased understanding of drug properties.\n\u2022 During the prompting stage, a novel prototype-enhanced prompting mechanism is proposed to enable the inference under few-shot learning scenarios, which is proven to have strong adaptability especially to rare events.\n\u2022 Extensive experiments on two benchmark datasets evaluate the SOTA performance of DDIPrompt."}, {"title": "2.1 GNNs for DDI Event Prediction", "content": "GNNs [Kipf and Welling, 2017; Velickovic et al., 2017; Hamilton et al., 2017] have been widely utilized to learn drug features by leveraging two types of graph structural data: 1) intra-molecular: the molecular structure of drugs, and 2) inter-molecular: the relationships between drugs.\nIn previous methods, each drug is represented as a graph, where atoms are represented as nodes and chemical bonds as edges. GNNs are then applied to learn representations, with optimization performed using DDI event data [Nyamabo et al., 2021; Li et al., 2021; Nyamabo et al., 2022b; Tang et al., 2023]. However, these methods fall short in capturing the rich interactive information between drugs.\nTo address this limitation, several approaches have been proposed to incorporate the inter-molecular data, thereby enhancing the expressiveness of the models. For instance, model in [Ma et al., 2018] applies GNNs on the drug-drug similarity graph. Additionally, some studies first learn representations at the molecular level and then consider DDI event relations to update these representations, resulting in improved edge classification performance [Wang et al., 2020; Xiong et al., 2023; Liu et al., 2023a]. Other methods introduce additional information from Knowledge Graphs (KGs) to extract knowledge-enriched drug features and propose multi-scale information fusion techniques for DDI event prediction [Chen et al., 2021; Han et al., 2023; Lyu et al., 2021; Yu et al., 2021]. Despite their effectiveness, these methods suffer from several weaknesses, including i) the requirement of extensive supervision information for learning GNNs, ii) the inability to handle imbalanced event type distributions, and iii) limited generalization ability for rare events."}, {"title": "2.2 Graph Prompt Learning", "content": "Prompt-based tuning methods, originated from the NLP domain [Brown et al., 2020; Lester et al., 2021], have been widely used to facilitate the adaptation of pre-trained language models to various downstream tasks. Recently, prompt learning has also emerged as a promising tool to cue the downstream tasks in the graph domain [Sun et al., 2022; Liu et al., 2023c; Sun et al., 2023a; Tan et al., 2023; Huang et al., 2023; Fang et al., 2022; Yu et al., 2023]. The pioneering work, GPPT [Sun et al., 2022] focuses on node classification task and incorporates learnable prompts on graphs. GraphPrompt [Liu et al., 2023c] proposes a uniform prompt design tailored for both node and graph downstream tasks. AllinOne [Sun et al., 2023a] extends graph prompt learning to encompass prompt tokens, prompt structures, and inserting patterns, introducing a comprehensive prompting framework. Though existing works have explored the \"pre-train, prompt\" paradigm to generalize GNNs, none of them have specifically investigated its application in the context of DDI event prediction. Moreover, we emphasize that existing methods cannot be directly applied to the DDI event prediction scenario due to their general pre-training objectives and the significant"}, {"title": "3 Methodology", "content": "In this section, we first formulate the DDI event prediction task. Then we elaborate on the pre-training and prompt learning processes within the DDIPrompt framework. The overview of our method is shown in Figure 1."}, {"title": "3.1 Task Formulation", "content": "DDI event data can be formulated as a multi-relational DDI event graph G = (V,E,R), where V = {v} denotes the set of drug nodes, R = {rk} denotes the set of relations representing event types, and E = {(u,v,rk)|u, v \u2208 V,rk \u2208 R} denotes the set of relational edges among drugs, i.e., DDI events. Additionally, each node v \u2208 V can be viewed as a drug molecular graph represented by Mu, where atoms serve as nodes and bonds serve as edges. Based on G and M = {\u039c\u03bd}\u03bd\u03b5\u03bd, our goal is to learn a model which predicts the specific DDI event for each drug pair as f : V \u00d7 V \u2192 R.\nBefore delving into the details of the DDIPrompt framework, we first provide an overview of its two-phase paradigm. In the pre-training stage, we exclusively employ the available structural data, i.e., intra-molecular structures and inter-molecular binary relations, to train GNN models. Subsequently, in the prompt tuning stage, the pre-trained model is frozen, and the task is to predict the event type of the remaining edges given few-shot event samples."}, {"title": "3.2 Hierarchical Pre-training", "content": "We devise a novel hierarchical pre-training method to address the problem of imbalanced event distribution. Our approach focuses on enhancing existing knowledge through two pretext tasks. We first propose the pretext as predicting molecule similarity score, aiming to augmenting potential links between drugs based on their structural similarities. To further alleviate the inherent bias present in disparate labels, we consider another binary classification task to capture the interactive relations between drugs, i.e., performing link prediction on the DDI graph."}, {"title": "Pretext 1: Molecule Similarity Score Prediction", "content": "We first represent each drug instance as a molecular graph M and employ a GNN parameterized by 1 to capture its distinctive structures as x = Molecular-GNN\u04e8\u2081 (M\u2082). Specifically, we adopt the dual-view Graph Attention Network proposed in [Li et al., 2023], which consists of a message passing phase and a readout phase.\n\u2022 Message Passing Phase During this phase, each atom i within drug M can aggregate messages from neighboring atom nodes Ni within itself or within its similar drug Mu as follows:\n$x_i^{(l+1)} = \\sigma (\\sum_{j\\in N_i\\cup{i}} a_{ij} W^{(l)} x_j^{(l)} +b^{(l)})$,\n$a_{ij} = Softmax_j (a^{(l)T} [W^{(l)}x_i^{(l)} ||W^{(l)} x_j^{(l)}])$,\nwhere $x_i^{(l)}$ denotes the representation of atom i in the l-th layer, $W^{(l)}$ is a variable matrix, and $a^{(l)}, b^{(l)}$ are trainable vectors in the l-th layer. Here, the similar drug Mu is pre-selected based on the Tanimoto Coefficient [Maggiora et al., 2014].\n\u2022 Readout Phase After stacking aforementioned message passing processes for L layers, we obtain atom i's representation $x_i^{(L)}$. Then, we apply an attentive pooling mechanism to obtain the representation of drug v:\n$x_v = \\sigma (\\sum_{i \\in V_v} \\gamma_i x_i^{(L)})$, where $y_i = Softmax(cx_i^{(L)})$.\nHere, x is the learned drug feature, V, denotes the node set in molecular graph Mv, and c is a trainable vector.\nThen, we employ a pre-training objective focused on predicting the similarity scores between pairs of drugs to guide the learning of representations x for each drug. The ground-truth score Suv \u2208 [0,1] for a drug pair (u, v) is derived from the Tanimoto coefficient [Tanimoto, 1968; Maggiora et al., 2014], which is calculated based on laboratory tests. To estimate the similarity score between drugs u and v, we introduce a MLP parameterized by 01, as \u015duv = MLP01 ([Xu||Xv]). Then, the learning objective is to minimize the difference between the estimated score \u015duv and the ground-truth score Suv via the Mean Squared Error (MSE) loss function:\n$L_{pre-train1} = \\frac{1}{T_1} \\sum_{(u,v,S_{uv}) \\in T_1} (S_{uv} - \\hat{S}_{uv})^2,$\nwhere T\u2081 = {(u, v, Suv) u,v \u2208 V} denotes the training set. This pre-training objective provides a valuable signal for guiding the model to capture meaningful similarities and differences between drugs, thereby enriching the structural knowledge beyond observed drug links."}, {"title": "Pretext 2: Link Prediction", "content": "Based on observed interactions, we construct a Drug-Drug Interaction (DDI) graph, where the edges are binary-valued to indicate whether there exists an interactive relation.\nTo encourage mutually enhanced representations between interacted drugs, we introduce an additional GNN encoder implemented by GraphSAGE [Hamilton et al., 2017] for feature propagation. Each drug's features are initialized by concatenating its previous intra-molecular representation x with a pre-trained molecular embedding derived from the biomedical knowledge graph [Ioannidis et al., 2020], resulting in h) = [x||k]. This initialization enhances the expressiveness of the drug representations. The final representation of drug v is computed as:\n$h_v = DDI-GNN_{\\Theta_2} (h_v^{(0)}, G)$,\nwhere DDI-GNN2 (\u00b7) denotes the GNN encoder parameterized by O2, and G represents the DDI graph.\nNext, our pre-training objective involves predicting the presence of links between a pair of drugs (u, v). Therefore,"}, {"title": "3.3 Prompt-based Learning and Inference", "content": "After the pre-training stage, we obtain drug v's representation h, incorporating both intra-molecular structures and inter-molecular interactive relations. Then, during the prompting stage, given few-shot samples of each event, we aim to infer the specific event types of remaining edges.\nTo facilitate this inference, we introduce class prompts as prototypes for each event class, which convey class-specific semantics. Instead of tuning the whole pre-trained model, our class prompts are more light-weighted with only a group of vectors containing rich pre-trained knowledge. We utilize the class prompts to reformulate the task as a similarity measure between node representations and the class prompts, aligning with pre-training objectives."}, {"title": "Prompt Initialization", "content": "Effective initialization of class prompts is crucial for smooth knowledge transfer from pre-trained drug representations to downstream event type classification. For a specific event class k, we initialize its class prompt pk by computing the mean of embeddings for labeled edges belonging to class k:\n$p_k = \\frac{1}{|T_k|} \\sum_{(u,v,r_{uv}) \\in T_k} e_{uv}, e_{uv} = Mean-Pool({h_u, h_v})$,"}, {"title": "Prompt Tuning", "content": "where Tk = {(u,v,ruv)|ruv = k} denotes the set of labeled edges belonging to type k, and euv is the edge representation.\nPrompt Tuning\nTo learn the distinct features of different classes, we further tune the class prompts by fitting them to the given labels. Specifically, we introduce an MLP to predict the probability of an edge (u, v) belonging to a specific type k, as pk\n\u03ba\u03c5 = MLP03 ([euv||Pk]), where the edge and class prototype representations are fed to the MLP03 (\u00b7). Subsequently, a similarity measure is applied to transform the resulting vector into a scalar, representing the probability. Optimization is performed via Cross-Entropy loss between the ground-truth labels and predicted types as follows:\n$L_{prompt}(\\{p_k\\}) = - \\sum_{k=1}^K \\sum_{(u,v,r_{uv}) \\in T_k} I(r_{uv} = k) log \\hat{r}_{uv},$\nwhere I(\u00b7) denotes an indicator function that returns 1 only if ruv = k, and K denotes the total number of classes. Note that this stage is extremely efficient as the number of tunable parameters is O(K \u00d7 d + L \u00d7 d), where d refers to the embedding dimension and L refers to the number of layers in MLP03 (.)."}, {"title": "Prompt-assisted Inference", "content": "After tuning, these class prompts convey class-distinctive semantics and can be directly utilized to perform inference:\n$r_{uv} = argmax_{k=1}^K MLP_{\\Theta_3}([e_{uv}||p_k]),$\nBy selecting the type with the highest probability, we determine the predicted event type for the edge (u, v)."}, {"title": "3.4 Complexity Analysis", "content": "Due to space limitations, we move the detailed algorithm process, complexity analysis, and further efficiency study to the Appendix."}, {"title": "4 Experiment", "content": ""}, {"title": "4.1 Experimental Settings", "content": "Datasets We evaluate DDIPrompt on two benchmarks: (1) Deng's dataset [Deng et al., 2020], which contains a total of 37,159 Drug-Drug Interactions (DDIs) involving 567 drugs and 65 types of DDI events, and (2) Ryu's dataset [Ryu et al., 2018], which consists of 191,075 DDIs between 1,689 drugs and encompasses 86 types of DDI events. To categorize the DDI events based on their frequencies, we calculate the number of DDI instances associated with each type of event, referred to as event frequency. Following the approach outlined in [Deng et al., 2022], we classify the DDI events into three groups: common events, fewer events, and rare events, as illustrated in Table 1.\nBaselines We compare our DDIPrompt with both representative DDI event prediction baselines (DeepDDI, SSI-DDI, MUFFIN, and MRCGNN), and few-shot learning methods (META-DDI and D2PT). Their brief descriptions are given as follows:\n\u2022 DeepDDI [Ryu et al., 2018] is a pioneering DDI event prediction work. It designs a feature structural similarity profiles for each drug based on their molecular fingerprints and employs a deep neural network to predict the interaction types of drug pairs.\n\u2022 SSI-DDI [Nyamabo et al., 2021] utilizes a Graph Attention Network (GAT) [Velickovic et al., 2017] on drug molecular graphs to capture molecular structural information. It combines embeddings obtained from multiple GAT layers to generate predictions for drug pairs.\n\u2022 MUFFIN [Chen et al., 2021] integrates embeddings from the Drug Repurposing Knowledge Graph [Ioannidis et al., 2020] and drug molecular structure features using a bi-level cross strategy to obtain representations of drug pairs for DDI event prediction.\n\u2022 MRCGNN [Xiong et al., 2023] is the state-of-the-art DDI event prediction method. It leverages drug features extracted from both the drug molecular graph and the DDI event graph to generate expressive representations. To further enhance the drug representations, MRCGNN incorporates an extra contrastive learning approach.\n\u2022 META-DDI [Deng et al., 2022] is a strong few-shot DDI event prediction baseline. It utilizes drug chemical structures and employs an interpretable representation module to learn drug-drug interactions.\n\u2022 D2PT [Liu et al., 2023b] is the state-of-the-art few-shot learning method specifically designed for GNNs. It aims to capture long-range dependencies by propagating information on both the initial input graph and an augmented graph. These two channels mutually reinforce each other through a contrastive learning module."}, {"title": "Implementation Details", "content": "We implement our DDIPrompt with the PyTorch framework. During the pre-training stage, to implement the molecular graph encoder, for each drug, we select the 10 most similar and 10 most dissimilar drugs based on Tanimoto Score [Tanimoto, 1968], forming the training samples for similarity score prediction and graph attention computation. During the prompt tuning stage, we employ the Adam optimizer [Kingma and Ba, 2014] with a learning rate of 0.002. Additionally, we initialize the dropout ratio at 0.3 and increase it by 0.05 per training round until it reaches a maximum rate of 0.9. For DDI event prediction, we utilize a 2-layer MLP as the projection head. Details of hyper-parameters setting will be provided in the Appendix.\nEvaluation Metrics We adopt commonly used metrics, including Accuracy, Macro-F1, Macro-Recall, and Macro-Precision. Larger values for all these metrics indicate better performance. To evaluate the effectiveness of the DDIPrompt framework in few-shot scenarios, we employ only 20% of the samples in each event class for prompt tuning (or training data for baseline methods), while reserving the remaining 80% for testing purposes."}, {"title": "4.2 Overall Performance Comparison", "content": "The overall performance under weak supervision on both datasets is presented in Table 2. Based on these results, we have the following observations:\n\u2022 DDIPrompt consistently outperforms all other baselines on both datasets across all evaluation metrics, demonstrating its superiority. The improvement in rare events is particularly significant, as the F1 score shows a remarkable improvement of 24.69% compared to the second best method on Deng's dataset. This indicates that the \"pre-train, prompt\" paradigm effectively reduces the reliance on labeled data, enabling DDIPrompt to achieve satisfactory results with only a few samples.\n\u2022 Existing DDI event prediction methods, such as Deep-DDI and SSI-DDI, are vulnerable to weak supervision scenarios, as evidenced by their limited effectiveness. These methods heavily rely on a large amount of training samples to achieve optimal performance. However, when confronted with limited data, their performance undergoes a significant decline.\n\u2022 Despite MRCGNN incorporating contrastive learning to address the challenges posed by rare events and the specific design for few-shot scenarios in META-DDIE and D2PT, their performance still falls short of surpassing DDIPrompt. This is because they have not fully captured the rich structural information inherent in drugs to facilitate accurate downstream predictions."}, {"title": "4.3 Ablation Study", "content": "To verify the effectiveness of each component within DDIPrompt, i.e., two pre-training objectives and the prompting mechanism, we consider the following variants:\n\u2022 w/o. Mol To demonstrate the effectiveness of capturing intra-molecular structures, we exclude the pre-training process performed on the molecular graph. Instead, we solely utilize the pre-trained KG embedding as the initial features of drugs in the DDI graph during pre-training.\n\u2022 w/o. LP To illustrate the rationale behind introducing inter-molecular structures, we exclude the pre-training process performed on the DDI graph, i.e., the Link Prediction task.\n\u2022 w/o. Prompt To evaluate the effectiveness of our prototype-enhanced prompting process, we replace our prompting mechanism with a MLP-based classifier that fits the provided samples to make predictions.\nExperimental results are presented in Table 3, where we can observe that removing any part within the DDIPrompt framework consistently leads to performance degradation, highlighting the effectiveness of each component. Both the pre-training objectives and the prototype-based prompting method contribute to the SOTA performance of DDIPrompt."}, {"title": "4.4 Sensitivity Analysis to Training Sample Size", "content": "In this subsection, we evaluate the sensitivity of our model to the size of training samples used for prompt tuning. While in the overall performance evaluation we uniformly set the ratio to 20% for prompt tuning, here we vary the ratio from 10% to 40% and compare the resulting performance. For comparison, we select the best DDI event prediction baseline, MRCGNN [Xiong et al., 2023], and the best few-shot method META-DDIE [Deng et al., 2022]. The results are shown in Figure 2. Note that since some of rare events have frequencies that do not exceed 10, setting the portion to 0.1 would provide no samples for training. Therefore, for rare events, we set the starting point to 0.2. Our findings reveal that DDIPrompt demonstrates strong performance even under extremely weak supervision. Moreover, as the size of the training samples increases, we observe a gradual improvement in performance. These results highlight the robustness and scal-"}, {"title": "5 Conclusion", "content": "In this paper, we propose DDIPrompt, the first \"pre-train, prompt\" method specifically designed for DDI event prediction. DDIPrompt aims to tackle the challenges posed by dis-"}, {"title": "Algorithm 1 Optimization of Pretext 1 in DDIPrompt", "content": "Input: Set of drugs V, set of corresponding drug molecule graphs M = {\u039c\u03c5}\u03bd\u03b5\u03bd, and set of drug similarity pairs T\u2081 = {(u, v, Suv) u, v \u2208 V}\nParameter: Molecular-GNN\u04e9\u2081 (\u00b7), MLP01 (\u00b7)\nOutput: Drug's molecular embedding matrix X = [\u03a7\u03c5]\u03bd\u03b5\u03bd\n1: Initialize O\u2081 in Molecular-GNN\u04e8\u2081 (\u00b7), 01 in MLP01 (\u00b7)\n2: while not converge do\n3: for v\u2208V do\n4: Encode drug v as x\u2081 = Molecular-GNN01 (M)\n5: end for\n6: for (u, v, Suv) \u2208 T\u2081 do\n7: Retrieve u, v's embedding xu, xv to predict the similarity score as \u015duv = MLP01 ([xu||Xv])\n8: Compute Lpre-train, based on suv and \u015duv\n9: end for\n10: Update 01 and 01 based on Lpre-train\u2081\n11: end while"}, {"title": "Algorithm 2 Optimization of Pretext 2 in DDIPrompt", "content": "Input: DDI interaction graph G = (V,E), set of link pairs T2 = {(u, v, yuv)|u,v \u2208 V}, drug's molecular embedding matrix X, and drug's knowledge embedding matrix K\nParameter: DDI-GNN\u018f\u2082 (\u00b7), MLP02 (\u00b7)\nOutput: Drug's final embedding matrix H = [hr]\u03bd\u03b5\u03bd\n1: Initialize O2 in DDI-GNN\u0473\u2082 (\u00b7), 02 in MLP02 ()\n2: for \u03bd \u0395 V do\n3: Obtain drug's initial representation h\u2070) = [xv||\u039a\u03c5]\n4: end for\n5: while not converge do\n6: for v \u2208 V do\n7: Encode drug v as h = DDI-GNNe\u2082(h), G)\n8: end for\n9: for (u, v, yuv) \u2208 T2 do\n10: Retrieve u, v's embedding hu, hv to predict the link probability as \u0177uv = MLP02 ([hu||hv])\n11: Compute Lpre-train, based on yuv and \u0177uv\n12: end for\n13: Update O2 and 02 based on Lpre-train2\n14: end while"}, {"title": "A Implementation Details", "content": ""}, {"title": "A.1 Hyperparameters Setting", "content": "During the prompt learning stage, we perform grid search to choose the key hyper-parameters. To ensure a comprehensive exploration of the parameter space, groups of hyperparame-ters are searched together, which is defined as follows:\n\u2022 Learning rate:{0.001, 0.0015, 0.002, 0.0025}\n\u2022 Dropout rate: {0.3, 0.4, 0.5, 0.6}\n\u2022 Number of epochs: {100, 200, 300, 400}\n\u2022 Weight decay: {1e-4, 3e-4, 5e-4, 7e-4}\n\u2022 Metric of similarity:{\u2018Cosine', \u2018Inner', \u2018Minkowski'}\n\u2022 Pooling of edge:{\u2018Hadamard',\u2018Concat',\u2018Sum', 'Mean'}\nBased on the observations from experimental results, the final chosen parameter values are as follows: the learning rate is set to 0.002, the dropout rate is set to 0.3, the number of epochs is set to 300, and the weight decay is set to 3e-4. The similarity metric is selected as the dot product, and the edge pooling method is selected as either Concat or Mean."}, {"title": "A.2 Complexity Analysis", "content": "To provide a comprehensive understanding of the overall efficiency, particularly in the prompt learning stage of the DDIPrompt framework, we present a detailed complexity analysis as follows.\n\u2022 Complexity of Pretext 1: We outline the detailed process in Algorithm 1. Within each epoch, the graph encoding process in Molecular-GNN\u04e9\u2081 (\u00b7) (Line 4) exhibits a complexity of O(LG1 Esum|d). Here, LG1 represents the number of graph attention layers in Molecular-GNN\u0259\u2081 (\u00b7), d denotes the embedding dimension, and Esum refers to the sum of edges, i.e., chemical bonds, within each drug molecular graph. Subsequently, the similarity prediction step employs the obtained drug embeddings to predict similarity scores using MLP01 (\u00b7)"}, {"title": "Algorithm 3 Prompt Learning of DDIPrompt", "content": "(Line 7-8). Let LM\u2081 represent the number of layers in MLP01 (\u00b7), the complexity of this step is O(LM1|T1|d), where T\u2081 corresponds to the number of drug similarity pairs, scaling linearly with the number of drugs |V|. Therefore, the overall complexity within each epoch can be expressed as O(|Esum| + |V|), scaling linearly with the size of drugs' molecular graphs.\n\u2022 Complexity of Pretext 2: We outline the detailed process in Algorithm 2. Firstly, graph convolution is performed to obtain the drug's representation (Line 7), resulting in a complexity of O(LG2|E|d), where LG\u2082 denotes the number of convolutional layers, E denotes the number of edges in DDI graph, and d represents the embedding dimension. Subsequently, the prepared link data is utilized for optimization (Line 9-12), contributing to a complexity of O(LM2|E|d), where LM2 signifies the number of layers in MLP02 (\u00b7). Therefore, the overall complexity for performing the link prediction pretext on the DDI graph for one epoch is O(LG2|E|d+LM2|E|d) = O(|E|), scaling linearly with the number of edges in DDI graph.\n\u2022 Complexity of Prompt Learning: We provide the details of our prompt learning algorithm in Algorithm 3, along with a complexity analysis that encompasses both the prompt learning and inference stages. In the prompt learning stage (Line 4-12), we iterate over each edge within the given set of few-shot samples T and estimate the probability across K types. This process incurs a complexity of O(KLM3d), where LM3 represents the number of layers in MLP03 (\u00b7). Consequently, the overall complexity of the prompt learning stage is O(KLM3|T|d). Since |T| < |E|, the overall complexity can be considered constant, scaling linearly with the number of provided samples. Next, we analyze the complexity of the inference stage. For each predicted edge"}, {"title": "Algorithm 3 Prompt Learning of DDIPrompt", "content": "Input: DDI graph G = (V", "\u0397\nParameter": "Set of prompt vectors {pk"}, 1, "MLP03 ()\nOutput: Predicted label of edges\n1: for k = 1, 2, ..., K do\n2: Initialize k-th class prompt pk \u25b7 Prompt Initialization\n3: end for\n4: while not converge do\n5: for (u, v, ruv) \u2208T do\n6: for k = 1, 2, ..., K do\\"]}