{"title": "Seek and Solve Reasoning for Table Question Answering", "authors": ["Ruya Jiang", "Chun Wang", "Weihong Deng"], "abstract": "Table-based Question Answering (TQA) involves answering questions based on tabular data. The complexity of table structures and question logic makes this task difficult even for Large Language Models (LLMs). This paper improves TQA performance by leveraging LLMs' reasoning capabilities. Inspired by how humans solve TQA tasks, we propose a Seek-and-Solve pipeline that instructs the LLM to first seek relevant information and then answer questions. The two stages are integrated at the reasoning level, and their Chain of Thought (CoT) paths are integrated into a coherent Seek-and-Solve CoT (SS-CoT). Furthermore, we present a compact single-stage TQA-solving prompt distilled from the pipeline. Experiments demonstrate that under In-Context Learning settings, using samples with SS-CoT paths as demonstrations, the TQA-solving prompt can effectively guide the LLM to solve complex TQA tasks, resulting in improved performance and reliability. Our results highlight the importance of properly eliciting LLMs' reasoning capabilities in solving complex TQA tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Tables are widely used in documents, such as financial reports and government statistics, making the ability to interpret them essential. This has led to the emergence of Table-based Question Answering (TQA) as an important task. However, the complexity and diversity of table structures, such as hierarchical layouts and multi-level headers, present challenges for TQA in practice [1]. Additionally, natural language questions often require advanced reasoning and contextual understanding, necessitating multi-step data analysis [2]. These chal-lenges-handling complex table structures and requiring advanced reasoning skills\u2014highlight the need for continued advancements in TQA technology to improve its practical performance and reliability. Recently, Large Language Models (LLMs) have demonstrated strong content comprehension and reasoning abilities [3] [4], opening up new possibilities for addressing TQA. However, despite progress made in TQA with less complex flat tables, directly using LLMs to solve TQA with complex tables often results in less satisfactory answers [5] [6] [7], indicating that advanced LLMs struggle with TQA involving complex tables on their own.\nSome approaches simplify the complex TQA task before letting the model handle it. Common methods include identifying question-relevant data points and using them to simplify the task by highlight-ing relevant sections of the table [8] or creating simplified sub-tables [9]; see Fig. 2 for an illustration. These operations enhance relevant information while reducing irrelevant ones, making the task easier to handle. Although generally effective, task simplification processes often operate independently of the question-answering mechanism. These methods primarily aim to simplify tasks, frequently over-looking the valuable insights gained during simplification that could help the LLM in answering the question. Consequently, even though the task is simpler, the LLM must reason from scratch. Besides, if relevant information is erroneously omitted during simplification, the LLM may be unable to correct the errors [8]. Both factors suggest that a more holistic approach could further enhance TQA performance.\nIn another aspect, we revisit the way humans handle complex TQA tasks and find that coherent reasoning is crucial. When dealing with TQA involving complex tables, humans usually follow a two-stage process. First, they analyze the question and understand the table's semantic structure to seek relevant information. Then, they access the concrete data points and infer the answer step by step. Despite appearing as two separate stages, they are integrated at the level of reasoning, resulting in a coherent reasoning process.\nMotivated by these observations, this paper aims to improve TQA performance by leveraging LLMs' reasoning capabilities. First, we present a novel \"Seek-and-Solve\" pipeline that directs the LLM's reasoning in a human-like manner. In the initial \"Seek\" stage, the LLM is instructed to seek question-relevant information and generate not only the identified relevant information but also the Seek Chain of Thought (CoT) [10] that leads to it. In the \"Solve\" stage, the Seek-CoT is incorporated into the \"Solve\" prompt, guiding the LLM to reason consecutively from it rather than starting from scratch to answer the question. Extensive experiments demonstrate that this \"Seek-and-Solve\" pipeline significantly improves TQA per-formance and offers higher error tolerance compared to the task simplification process. Second, we present a compact single-stage TQA-solving prompt distilled from the pipeline, leveraging the In-Context Learning (ICL) mechanism to guide the LLM's reasoning process through demonstrations. The Seek-and-Solve-CoT (SS-CoT), which represents the complete human-like reasoning process for solving a TQA task, can be formed by combining the Seek-CoT and the subsequent Solve-CoT. Experimental results show that when using samples with SS-CoT paths as demonstrations, the TQA-solving prompt can effectively guide the LLM to solve complex TQA tasks. Our findings highlight the importance of properly eliciting LLMs' reasoning capabilities in solving complex TQA tasks.\nThe remainder of this paper is organized as follows: Sec. 2 reviews related works. Sec. 3 describes the proposed methods. Sec. 4 presents the experimental results. Finally, Sec. 5 concludes the paper."}, {"title": "II. RELATED WORKS", "content": "TQA with complex table. Due to limited token length and rea-soning capabilities, TQA tasks involving complex tables often need to be simplified before answering. For example, ITR [9] proposed creating sub-tables from relevant information identified via retrieval. E5 [7] instructed an LLM to extract relevant sections of the table. Additionally, DATER [6] decomposed both the table and the question into sub-tables and sub-questions, respectively. Although effective, task simplification processes often operate independently of question answering (QA). Recent works tend to handle TQA tasks holistically."}, {"title": "III. PROPOSED METHODS", "content": "In this section, we first introduce the tree structure as a tool to model table semantics. Next, we present the \"Seek-and-Solve\" pipeline. Finally, we present the single-stage TQA-solving prompt.\nPrevious studies [11] [17] show that modeling table header hierar-chies as tree structures [1] [18] are beneficial for understanding table semantics and seeking information from complex tables. Therefore, we model the table header hierarchy as a node-based tree. In our implementation, each node corresponds to a header cell and stores its value, row (or column) index, subtree index span, and references to its child nodes. This node definition allows us to access the row (or column) index and span of any table header cell. Additionally, a tree path from the root to a leaf node can be linearized as a tuple, representing the entire hierarchical semantics of a table header."}, {"title": "B. The Seek-and-Solve pipeline", "content": "A schematic of the \"Seek-and-Solve\" pipeline is shown in Fig. 1. In overview, for a given TQA task, the process involves two LLM calls before arriving at an answer.\nStage-1: Seek. In this stage, we instruct the LLM to enhance its understanding of table semantics and question analysis before performing QA by completing an information seeking task. In specific, we model the table's row and column header hierarchies as a row tree and a column tree, respectively. The obtained row and column trees are then converted into a list of information, where each item is a tuple linearized from a tree path. Next, we use an ICL prompt (the Stage-1 prompt template in Fig. 1) to instruct the LLM"}, {"title": "C. The TQA-solving prompt", "content": "A compact single-stage TQA-solving prompt can be created by integrating the two prompts of the \"Seek-and-Solve\" pipeline, as shown in Fig. 3. It takes the full table and the full list of tree paths as inputs and uses an ICL setting to guide the LLM's reasoning process through demonstrations. In addtion, The Seek-CoT (blue) and its subsequent Solve-CoT (orange) can be integrated to form a Seek-and-Solve-CoT (SS-CoT) that represents a complete reasoning process"}, {"title": "IV. EXPRIMENTS", "content": "Datasets. The proposed methods are evaluated on the HiTab [1] and WikiTableQuestions (WikiTQ) [20] datasets. HiTab, constructed from statistical reports and Wikipedia pages, contains diverse real-world complex tables with challenging questions. It is split into train (7,417 samples), dev (1,671 samples), and test (1,584 samples) sets. We utilize the dev and test splits for evaluations, and the train split for designing prompts. WikiTQ, known for its complex questions about Wikipedia tables, requires reasoning over multiple table data points to answer. For evaluation, we use its standard test set (4,344 samples). Following previous works [1] [8], we use Accuracy to measure the percentage of correctly answered samples for both datasets.\nLLMs. The proposed methods are evaluated on four popular open-source LLMs: Mistral-7B-Instruct-v0.2 [3], Mixtral-8x7B-Instruct-v0.1 [3], Llama-3.1-8B-Instruct [4], and Llama-3.1-70B-Instruct [4]. All models are deployed with default configurations using the vLLM library [21], and accessed through APIs. In all experiments, greedy decoding and fixed random seeds are used to reduce variance."}, {"title": "B. Results with the Seek-and-Solve pipeline", "content": "Extensive experiments were performed on HiTab to evaluate the performance of the \"Seek-and-Solve\" pipeline. As detailed in Sec. III-B, various prompt versions were generated by combining different content elements. The results obtained using Llama-3.1-8B are presented in Tab. I, with each row representing a distinct combination. From the results, several observations can be made. When the Stage-2 LLM reasoned from scratch, simplifying tasks with sub-tables and/or hints yielded significantly better results than those obtained from raw tasks using full tables without any additional information. Conversely, directly providing the full list of tree paths resulted in degraded performance, likely because the LLM struggled to interpret the list without guidance [22].\nWhen the Stage-2 LLM performed reasoning consecutively from Seek-CoT, all prompt versions consistently outperformed their coun-terparts by a significant margin, demonstrating the value of reasoning-level integration of the two stages. Moreover, the performance gaps between different prompt versions became less significant. This is probably because Seek-CoT incorporates the reasoning behind the task simplifications, making them less necessary.\nFurthermore, we analyzed the pipeline's error tolerance by em-ploying LLMs with varying capacities at its two stages. It is known that a less capable LLM in Stage-1 is more prone to generating errors in Seek-CoT and Seek-Result. Our objective was to determine if a more capable LLM in Stage-2 could mitigate the errors introduced in Stage-1. Tab. II presents the results obtained using Llama-3.1-8B, Mixtral-8x7B, and Llama-3.1-70B. The findings revealed that when certain task simplifications (e.g., subtables and/or hints) were applied, the more capable LLM in Stage-2 struggled to correct the errors from Stage-1, resulting in nontrivial performance gaps. This observation aligned with [8]. Conversely, without task simplification, errors made in Stage-1 were effectively corrected by Llama-3.1-70B in Stage-2, as evidenced by the relatively high performance and small performance gaps, indicating enhanced error tolerance.\nOverall, the results presented in Tab. I and Tab. II demonstrated that the \"Seek-and-Solve\" pipeline led to clear improvements in performance and reliability. This highlighted that the reasoning-level integration of two stages provides significant benefits."}, {"title": "C. Results with TQA-solving prompt", "content": "We report the performance of the single-stage TQA-solving prompt on HiTab and WikiTQ in Tab. III and Tab. IV, respectively, and compared these results with the state-of-the-art reports.\nIn HiTab, the TQA-sovling prompt with SS-CoT demonstrations consistently outperformed those with vanilla-CoT demonstrations across all four LLMs, showing significant gains. In WikiTQ, prob-ably due to the relatively flat structure of tables, the results were mixed. While there were performance declines on Mixtral-8x7B and Llama-3.1-8B, the TQA-solving prompt with SS-CoT demonstrations showed improved performance compared to its vanilla-CoT coun-terparts on Mixtral-8x7B and Llama-3.1-8B. We plan to study this further in the future. Overall, these results indicate that SS-CoT paths properly guided the way LLMs handling complex TQA tasks."}, {"title": "V. CONCLUSION", "content": "This paper enhances TQA by leveraging LLM's reasoning abilities. We proposed a Seek-and-Solve pipeline to guide the LLM to reason in a human-like manner, integrating two stages into a coherent SS-CoT path. Additionally, we presented a single-stage TQA-solving prompt to guide the LLM's reasoning process through demonstrations with SS-CoT paths. Experimental results show clear improvements in performance and reliability, emphasizing the importance of eliciting reasoning capabilities in LLMs for complex TQA tasks."}]}