{"title": "BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery", "authors": ["Kanishk Gandhi", "Michael Y. Li", "Lyle Goodyear", "Louise Li", "Aditi Bhaskar", "Mohammed Zaman", "Noah D. Goodman"], "abstract": "Understanding the world and explaining it with scientific theories is a central aspiration of artificial intelligence research. Proposing theories, designing experiments to test them, and then revising them based on data are fundamental to scientific discovery. Despite the significant promise of LLM-based scientific agents, no benchmarks systematically test LLM's ability to propose scientific models, collect experimental data, and revise them in light of new data. We introduce BoxingGym, a benchmark with 10 environments for systematically evaluating both experimental design (e.g., collecting data to test a scientific theory) and model discovery (e.g., proposing and revising scientific theories). To enable tractable and quantitative evaluation, we implement each environment as a generative probabilistic model with which a scientific agent can run interactive experiments. These probabilistic models are drawn from various real-world scientific domains ranging from psychology to ecology. To quantitatively evaluate a scientific agent's ability to collect informative experimental data, we compute the expected information gain (EIG), an information-theoretic quantity which measures how much an experiment reduces uncertainty about the parameters of a generative model. A good scientific theory is a concise and predictive explanation. Therefore, to quantitatively evaluate model discovery, we ask a scientific agent to explain their model and then assess whether this explanation enables another scientific agent to make reliable predictions about this environment. In addition to this explanation-based evaluation, we compute standard model evaluation metrics such as prediction errors. We find that current LLMs, such as GPT-40, struggle with both experimental design and model discovery. We find that augmenting the LLM-based agent with an explicit statistical model does not reliably improve these results.", "sections": [{"title": "1 Introduction", "content": "Helping humans understand the world (and themselves) by discovering scientific theories has been a foundational goal of artificial intelligence research [25]. Proposing theories about the world, conducting experiments to test them, and revising them based on evidence is central to this process [6]. Recent advances in artificial intelligence, particularly with large language models (LLMs), have shown promising potential for accelerating scientific discovery. LLMs have extensive scientific knowledge [2], strong inductive reasoning capabilities [38, 49], and the ability to write statistical models of data [21, 22]. These promising results suggest that LLMs, functioning as autonomous agents, could be well-suited for two key components within the scientific discovery pipeline:"}, {"title": "2 Related Works", "content": "Optimal Experimental Design. Bayesian optimal experimental design (BOED) is a principled framework for designing maximally informative experiments with successful applications in a wide variety of disciplines [10, 31, 45]. Although the framework is appealing, implementing BOED in practice is challenging because it involves computing information gain metrics, such as expected information gain (EIG), which are often intractable. There are a number of approaches to approximating the EIG of a design. Rainforth et al. [39] propose a nested Monte Carlo estimator and Foster et al. [14] estimate the EIG using a variational approximation. Subsequent work has scaled BOED to sequential settings by learning an amortized design network [15]. While these methods are significant technical innovations, they typically assume that the data is well-approximated by an assumed generative model. However, in many practical settings, we also want to revise our model as we collect new data.\nAutomated Model Discovery. Automated model discovery from data has been a long-standing goal in AI, with the aim of building interpretable and explainable models that capture underlying patterns and relationships in data. Researchers have developed systems for discovering physical laws [5, 26], performing nonparametric regression [13], and conducting unsupervised learning [18]. Recently, Li et al. [21, 22] proposed integrating language models into an automated statistical model discovery pipeline that involves both proposing candidate models and critiquing them, leveraging"}, {"title": "Reasoning with LLMs", "content": "Experimental design and model discovery require both deductive reasoning (e.g., what does a given hypothesis entail) and inductive reasoning, (e.g., what might be the underlying hypothesis that explains the data) (c.f., Liu et al. [23]). Qiu et al. [38], Wang et al. [49] show how LLMs can reason inductively to solve abstract pattern based problems from a few examples. Other work in deductive reasoning has shown how LLMs [37, 42, 43] can perform logical deduction and identify inconsistencies or contradictions in a set of statements. However, these works have primarily been applied to deterministic, rule-based systems rather than the noisy, stochastic data typically seen in scientific discovery."}, {"title": "In-context Exploration", "content": "The challenge of building accurate models through experimental design requires efficient exploration and information-seeking strategies. Recent research has tried to study this through in-context exploration - Nie et al. [32] and Min et al. [27] demonstrate how to optimize in-context exploration, while Monea et al. [29] shows how transformers can learn reinforcement learning policies directly in a sequence modelling framework. This work on exploration in language has been complemented by recent research on teaching language models how to search effectively in language [16, 17, 20, 44]."}, {"title": "Interactive Environments", "content": "BoxingGym takes inspiration from the long-line of research in designing RL environments [9, 28]. In particular, we adapt the modularity and simplicity of these benchmarks. However, the focus of our environments is on evaluation and not training agents. More recently, Jimenez et al. [19] presented an interactive benchmark to test capabilities of language-agents on debugging software issues. Our work builds on this line of research by introducing"}, {"title": "3 Boxing Gym", "content": "3.1 Problem Formulation.\nWe formalize the experimental design and model discovery problem within the framework of probabilistic modelling and Bayesian optimal experimental design (BOED). In BoxingGym, each environment is implemented as a generative model defining a joint distribution over the experimental outcome y, experimental design d, and unobserved parameters \u03b8. This joint distribution is characterized in terms of a prior distribution over \u03b8, p(\u03b8) and a simulator p(y|\u03b8, d) which is a model of the experimental outcome y given parameters @ and design d. For example, e could be the parameters of some behavioral model of participants in a psychology experiment, d could be the questions posed to the participants, and y could be the participant's response to d. Within this framework, running an experiment corresponds to choosing a design d and observing a sample y from the marginal predictive distribution conditioned on that design, i.e., y ~ p(y|d) = Ep(\u03b8) [p(y|\u03b8, d)]).\n3.2 Evaluation\nWe now discuss how we quantitatively evaluate a scientific agent's capabilities in 1) experimental design and 2) model discovery."}, {"title": "3.2.1 Evaluating experimental design via Expected Information Gain", "content": "To quantitatively evaluate experimental design, we take inspiration from the Bayesian OED literature [14, 15]. Crucially, our design choice to implement environments as generative models enables us to leverage this literature. For each domain, we have an underlying predictive model p(y|\u03b8, d). We quantify the informativeness of a design d through the expected information gain (EIG), an information-theoretic quantity measuring the (expected) reduction in uncertainty about the model parameters @ after running an experiment d.\nEIG(d) = Ep(y|d) [H[p(\u03b8)] \u2013 H[p(\u03b8|y, d)]]\nIntuitively, the EIG measures the average informativeness of running design d over different possible experimental outcomes y. Unfortunately, the EIG is doubly intractable [30]. The expectation involves marginalizing over 6 and, in general, the posterior over the parameters p(\u03b8|y, d) is also intractable. To approximate the EIG, we therefore use a Nested Monte Carlo estimator\n\u00dbNMC(d) = 1/N \u03a3n=1 log (p(yn|\u03b8n,0, d))/(\u03a3m=1P(Yn|0n,m, d))\nwhere \u03b8n,m~i.i.d p(\u03b8), yn ~ p(y|\u03b8 = \u03b8n,0, d)\nWe chose this estimator because it is consistent (as N, M \u2192 \u221e) estimator of the true EIG with theoretical properties explored by [39] and is straightforward to implement. Crucially, EIG measures the value of an experiment under an assumption about the form of the underlying predictive model. In particular, we assume that the true distribution of experimental outcomes given a design is modeled by p(y|d). In general, this assumption is not true and EIG may not always be an accurate measure under model misspecification [40]. However, in our benchmark, since we generate data from an underlying model, EIG is still a useful measure of informativeness."}, {"title": "3.2.2 Evaluating model discovery via communication", "content": "To evaluate the quality of a model, we employ both a number of standard model evaluation metrics (e.g., prediction MSE) as well as a metric loosely inspired by communication through a noisy channel, that takes advantage of the natural language interface. In particular, a scientist agent interacts with an environment via experiments. After a series of these experiments, we ask the scientist agent to synthesize their findings through an explanation. We then evaluate the extent to which that explanation empowers a novice agent to make accurate predictions about the environment without any interaction. Since a good explanation is both predictive and parsimonious, we set a token limit on the explanation. Crucially, this evaluation method can accommodate different forms of scientific theories. For example, in our experiments, we ask the scientist agent to produce a statistical model and then interpret and distill the model into a natural language explanation to guide the novice agent."}, {"title": "3.2.3 Evaluating goals via prediction", "content": "To evaluate success at achieving a specific goal (e.g., how do the populations of predator and prey change with time) we employ a prediction target (e.g., predict the population of predators at a particular time) and calculate a standardized prediction error. First, we compute the error between the predicted and true values. Then, we standardize this error with respect to the prior predictive mean, which is obtained by assuming a uniform prior over the design space. Specifically, for each domain, we sample a design d uniformly from the design space and a parameter @ from the prior distribution p(\u03b8). We then generate samples from the predictive model p(y|\u03b8, d) and average over multiple d and 0 to obtain the prior predictive mean \u00b5o and variance 60. Let {yi}=1 be the ground truth outputs for inputs {x}=1. and let {\u0177i}=1 be the predictions of the agent. The standardized prediction error is then calculated using these quantities, providing a measure of the agent's performance relative to the prior predictive mean. We use a domain-specific function f computing the discrepancy between a prediction \u0177i and ground truth value yi (e.g., MSE). We compute the errors ei = f (\u0177i, Yi) and \u20ac\u03bc\u03bf = f(\u03bco, yi). Finally, we compute the standardized error as\n(\u0395\u03af-\u0395\u03bc\u03bf)/\u03c3\u03bf"}, {"title": "3.3 Design Decisions in Constructing BoxingGym", "content": "We outline the key design decisions of BoxingGym that allow it to capture key aspects of scientific discovery within a flexible, simulated, and extensible environment.\nDiscovery via active experimentation. The agent actively interacts with the environment by conducting experiments, reflecting the real-world coupling of experimentation and model discovery. This approach assesses the agent's ability to gather relevant data and refine its models based on experimental results.\nReal-world scientific models. Our environments are grounded in real-world scientific models from various domains, ensuring the benchmark tests the agent's ability to handle realistic scenarios. We implement these environment as pymc generative models to make active experimentation tractable.\nGoal-driven discovery. Each environment has a specific goal or an intent of inquiry, mirroring the nature of human scientific research. This encourages the agent to engage in targeted experimentation."}, {"title": "Language-based interface for experiments", "content": "We use a language-based interface for our experiments which enjoys several compelling advantages. First, a language-based interface is flexible and general, since scientific domains can be described using natural language. Second, language is a natural choice, given that our work is motivated by the promise of LLM-based systems. Third, a language-based interface is interpretable to humans."}, {"title": "Emphasis on Measuring Discovery with Explanations", "content": "BoxingGym places a strong emphasis on measuring the quality of the agent's discoveries through the explanations it provides (\u00a73.2.2). This design decision is motivated by two considerations. From a theoretical perspective, science is fundamentally about developing better theories, and scientific theories are explanations of observed phenomena. From a practical perspective, communicating findings to the broader scientific community is an essential aspect of scientific research. By using language, we can avoid committing to a particular representation of a scientific theory. We illustrate this flexibility, by showing how different representations can be easily integrated within our method for measuring natural language explanations."}, {"title": "Extensible/modular environments for benchmarking agents", "content": "BoxingGym is easily extensible and modular, enabling researchers to integrate new environments and test different agents with minimal effort. We illustrate this in Fig. 2 which provides a pseudo-code example of how to implement a new environment and goal in BoxingGym."}, {"title": "3.4 Domains", "content": "BoxingGym consists of 10 environments (see App. D for full details) that cover a range of scientific domains and test different aspects of experimental design and model discovery. Some environments are designed to test optimal experiment design, while others focus on model discovery or involve simulated neuro-symbolic human participants.\nLocation finding. [15] In an n-dimensional space with k signal-emitting sources, the scientist measure signals at any grid location. Goals include predicting the signal at any point or locating the sources.\nHyperbolic temporal discounting. [15] The scientist observes a participant's choices for different immediate rewards (ir), delayed rewards (dr), and delay periods (D days) Fig. 2 (right). Goals include predicting choices of a participant or discount factors.\nDeath process. [15] A disease spreads at a certain infection rate. The scientist can measure the number of infected individuals at different points of time to predict future infections or the infection rate.\nItem Response Theory (IRT). [41] In this environment, there is a set of students and a set of questions. The experimenter can observe the correctness of a student's response to a particular question. The goal is to discover the underlying model that relates student ability and question difficulty to the probability of a correct response.\nAnimal growth curves. [24] An experimenter can observe the length of a dugong at a particular age. The goal is to discover the underlying growth model of dugongs."}, {"title": "Population growth dynamics", "content": "[24] An experimenter can observe the population of peregrines at a particular point in time. The goal is to discover the underlying population dynamics model. This is tested by asking the experimenter to predict population dynamics at a particular point in time."}, {"title": "Mastectomy Survival analysis", "content": "[11] The experimenter can observe if a patient is alive after a mastectomy, including metastasis status and time since surgery. The goal is to predict survival probabilities for new patients."}, {"title": "Predator-Prey dynamics", "content": "[48] This simulates predator-prey populations over time. The goal is to discover models like the Lotka-Volterra equations to predict future populations."}, {"title": "Emotion from outcome", "content": "[33] Participants guess a player's emotions after a gambling game's outcome. The experimenter designs games with varied probabilities and prizes to model how participants judge the emotions of a player from outcomes. Human participants are simulated using a probabilistic model translated into natural language by a language model."}, {"title": "Moral Machines", "content": "[4] Participants face moral dilemmas, choosing which group an autonomous car should save. Experimenters manipulate group compositions and required actions to model moral decision-making. Human participants are simulated with a probabilistic model, and their actions are translated into natural language by a language model."}, {"title": "4 Experiments with Baseline Agents", "content": "We conduct experiments to evaluate the performance of two baseline agents on BoxingGym. Our goal is to assess their ability to perform experimental design and theory building across a diverse set of environments. We benchmark two types of agents: a standard language model (GPT-40, OpenAI [34]) and a language model augmented with symbolic reasoning capabilities (Box's Apprentice)."}, {"title": "4.1 Experimental Design Evaluation", "content": "To evaluate the agents' performance, we first assess their ability to gather valuable information through their experiment selection and then measure how effectively they use this information to predict the environment. The Expected Information Regret (EI Regret), a key metric, compares the Expected Information Gain (EIG) (\u00a73.2.1) of the agent's chosen experiments to the maximum EIG achievable from 100 random experiments. Lower EI Regret indicates more informative experiment selection. To verify if agents can use observational data to make predictions, we measure their standardized prediction error about the environment before and after experimentation."}, {"title": "4.2 Model Discovery Evaluation via Communication", "content": "Next, we evaluate the agents' ability to build and communicate models that capture the underlying phenomena in each environment. To test this, we have the agents interact with the environment for 10 steps (scientist phase) and then generate a natural language explanation of their findings. We then provide this explanation to a novice agent, which must make predictions about the environment"}, {"title": "5 Discussion", "content": "We introduced BoxingGym, a benchmark for measuring experimental design coupled with model discovery with a language interface. BoxingGym includes 10 environments, each based on real-world generative models, that allow us to systematically compare different language-based agents. To evaluate experimental design capabilities, we used information gain metrics to approximate the effectiveness of an agent's active experimentation. For model discovery and communication, we introduced a novel metric based on the agent's ability to explain its discovered model to a novice agent, who then makes predictions about the environment. Our experiments with two types of agents\u2014a standard language model and a language model augmented with statistical modeling\u2014demonstrate this framework's potential to drive progress. Results with our baseline agents reveal significant challenges: proposing hypotheses from few observations, designing experiments from hypotheses, and incorporating new observations with prior knowledge to revise them.\nWhile BoxingGym is a valuable testbed, it has limitations. The environments in BoxingGym offer pre-defined experimental paradigms, whereas real-world science involves designing the experimental method itself; deciding variables to measure, conditions to compare, and controlling for confounds. Future research could develop environments requiring agents to design experiments from scratch [12]. Additionally, BoxingGym does not consider the time and cost of experiments."}]}