{"title": "GRAMIAN MULTIMODAL REPRESENTATION LEARNING AND ALIGNMENT", "authors": ["Giordano Cicchetti", "Eleonora Grassucci", "Luigi Sigillo", "Danilo Comminiello"], "abstract": "Human perception integrates multiple modalities\u2014such as vision, hearing, and language\u2014into a unified understanding of the surrounding reality. While recent multimodal models have achieved significant progress by aligning pairs of modalities via contrastive learning, their solutions are unsuitable when scaling to multiple modalities. These models typically align each modality to a designated anchor without ensuring the alignment of all modalities with each other, leading to suboptimal performance in tasks requiring a joint understanding of multiple modalities. In this paper, we structurally rethink the pairwise conventional approach to multimodal learning and we present the novel Gramian Representation Alignment Measure (GRAM), which overcomes the above-mentioned limitations. GRAM learns and then aligns n modalities directly in the higher-dimensional space in which modality embeddings lie by minimizing the Gramian volume of the k-dimensional parallelotope spanned by the modality vectors, ensuring the geometric alignment of all modalities simultaneously. GRAM can replace cosine similarity in any downstream method, holding for 2 to n modality and providing more meaningful alignment with respect to previous similarity measures. The novel GRAM-based contrastive loss function enhances the alignment of multimodal models in the higher-dimensional embedding space, leading to new state-of-the-art performance in downstream tasks such as video-audio-text retrieval and audio-video classification. The project page, the code and the pretrained models are available at https://ispamm.github.io/GRAM/.", "sections": [{"title": "1 INTRODUCTION", "content": "Humans naturally process and integrate signals from multiple sensory modalities, such as sounds and visual inputs, to form a coherent understanding of the world around them. Inspired by this, foundational models have attempted to replicate this capability by aligning pairs of modalities, such as vision and language, through contrastive learning techniques. One of the most significant contributions in this domain was CLIP Radford et al. (2021), which used a contrastive loss to align image and text representations based on cosine similarity. CLIP has shaped the current approach to multimodal learning, and every subsequent model relies on the same contrastive-pairs fashion, even in the case of models involving more than two modalities, such as ImageBind Girdhar et al. (2023), VAST Chen et al. (2023c), and LanguageBind Zhu et al. (2024).\nHowever, these models suffer from critical limitations, as they typically involve cosine similarity to align each modality to a chosen anchor modality (e.g., images, text, or audio) Sirnam et al. (2023); Girdhar et al. (2023), without ensuring consistent alignment between non-anchor modalities. This approach can be insufficient for tasks that require cross-modal understanding beyond pairs. For instance, aligning video and audio separately to a common textual anchor does not guarantee that the video and audio themselves are well-aligned. Consequently, models often fail in complex, real-world scenarios where interactions between multiple modalities are crucial. A notable example is video-text retrieval, where audio can be essential for correctly interpreting the scene. State-of-the-art (SOTA) models often neglect this, leading to suboptimal performance when audio is a critical factor Yoon et al. (2023), as we show in Fig. 6 of Appendix B. Due to these inherent limitations,"}, {"title": "2 RELATED WORK", "content": "Two-modal Alignment. CLIP Radford et al. (2021) originally revolutionized multimodal learning by providing a foundational model that effectively aligns two modalities, specifically images and text. This work has inspired subsequent advancements aimed at enhancing performance and alignment quality across modalities Uesaka et al. (2024); Ilharco et al. (2021); Zhai et al. (2023). Additionally, CLIP architecture has been adapted to facilitate alignment between other modality pairs, such as audio and text in CLAP Elizalde et al. (2023), video and text in CLIP4Clip Luo et al. (2021), and point clouds and text in PointCLIP Zhang et al. (2021). These models commonly rely on contrastive learning strategies, which push dissimilar embeddings apart while drawing similar vectors closer together, providing a robust method for learning cross-modal representations.\nMultimodal Alignment. Recent research has extended beyond two-modal alignment to capture the more complex multimodal nature of real-world data Lyu et al. (2024). For instance, CLIP4VLA Ruan et al. (2023) integrates audio within the CLIP framework, aligning video, audio, and text in pairs, using audio embeddings as a central anchor. Building on this, ImageBind Girdhar et al. (2023) offers a pre-trained multimodal framework encompassing modalities like depth and infrared, positioning the image modality as the bridge across modalities. In contrast, LanguageBind Zhu et al. (2024) demonstrates that text, rather than images, is a more effective anchor for multimodal integration. Alongside these innovations, other models such as VALOR Chen et al. (2023b), VAST Chen et al. (2023c), mPLUG-2 Xu et al. (2023), VideoPrism Zhao et al. (2024), and InternVideo2 Wang et al. (2024b), among others, either propose large pretraining datasets or introduce architectural modifications that further optimize performance. Among these, VAST proposes to fuse multiple modalities by an MLP layer and then involve conventional contrastive losses between such fused omni-modality embedding and the anchor one. Despite the lack of interpretability of this solution, at least VAST builds the first attempt towards a multimodal-like space.\nDespite their contributions, these approaches still operate within a constrained lower-dimensional space, often relying on cosine similarity on a 2D plane defined by two modalities or using fusion strategies to combine multiple modalities. As a result, these models fall short in fully exploiting the rich, high-dimensional information inherent in multimodal data, which is crucial for addressing more complex downstream tasks.\nGram Matrix in Deep Learning. The Gram matrix properties have been leveraged in deep learning models for better defining matrix theory behind these models Pennington & Worah (2017), or to improve performance in different downstream applications. Examples are learning rotation-invariant point cloud representations Xu et al. (2021), sound event detection Neto et al. (2021), neural style transfer Li et al. (2017); Friedrich & Menzel (2019), and domain adaptation Nejjar et al. (2023). In addition and interestingly, the Gram matrix has also been proved to bring some insights on GAN representations Seddik et al. (2020)."}, {"title": "3 GRAMIAN REPRESENTATION ALIGNMENT MEASURE", "content": "Our goal is to achieve comprehensive geometric learning and alignment of multiple modality representations within their inherent high-dimensional spaces. Unlike traditional pairwise alignment methods, GRAM advances beyond pairwise limitations to introduce a novel strategy for modeling and interpreting latent multimodal representations in an integrated manner. By jointly learning and then aligning these modalities, our method exploits the full potential of multimodal models, while maintaining a high degree of geometric interpretability. The GRAM measure can replace cosine"}, {"title": "3.1 PRELIMINARIES", "content": "Multimodal representation learning aims to learn latent representations from co-occurrent data modalities. The i-th modality is encoded from its own domain in an n-dimensional latent representation using an encoding function $e_i : M_i \\rightarrow M_i$, with $M_i \\in \\mathbb{R}^n$. A representative example may be the video-audio-text representation where we have a tri-modal representation with three encoding functions: $e_v : V \\rightarrow V$ as visual encoder, $e_a : A \\rightarrow A$ as audio encoder, and $e_t : T \\rightarrow T$ as text encoder. Here, $V \\in \\mathbb{R}^{N_f \\times C \\times W \\times H}$ is the visual domain assuming videos with $N_f$ frames each one with $C$ channels and a resolution of $W \\times H$, $A \\in \\mathbb{R}^{N \\times N_s}$ is the audio domain assuming audio with $N_s$ samples for each $N_c$ channels, and $T = [t_1, ... t_m]$ is the textual domain composed of a set of tokens $t_i \\in Vocabulary, V, A, T \\in \\mathbb{R}^n$.\nUsually, the similarity between two modalities $M_i, M_j$ is obtained by computing the cosine of the angle $\\theta_{ij}$ between their two representations:\n$\\cos(\\theta_{ij}) = \\frac{(M_i, M_j)}{||M_i|| \\cdot ||M_j||}$  (1)\nwhere $(M_i, M_j)$ is the dot product between modality $M_i$ and modality $M_j$, and $||M_i||$ is the norm of $M_i$.\nSince cosine similarity is not defined for three or more vectors altogether, the classical approach is to select one modality as bridge modality and align all the remaining N-1 modalities to the first one couple-wise. In this scenario whatever type of contrastive loss could be used, the most common is the one introduced by Radford et al. in CLIP Radford et al. (2021):\n$L_{M2A} = \\frac{1}{B} \\sum_{i=1}^{B} \\log \\frac{\\exp(m_i \\cdot a_i / \\tau)}{\\sum_{j=1}^{B} \\exp(m_i \\cdot a_j / \\tau)}, L_{A2M} = \\frac{1}{B} \\sum_{i=1}^{B} \\log \\frac{\\exp(a_i \\cdot m_i / \\tau)}{\\sum_{j=1}^{B} \\exp(a_i \\cdot m_j / \\tau)}$ (2)\nwhere $m_i$ is the normalized embedding of the i-th modality data and $a_i$ is the normalized embedding of the bridge modality and $B$ is the batch size parameter.\nAt inference time, the problem becomes even more apparent since conventional methods do not possess a direct mathematical formulation to compute similarity among three or more embedding vectors. Therefore, they rely on just two modalities or develop suboptimal neural fusing strategies. For instance, LanguageBind Zhu et al. (2024) attempts to linearly combine two modalities and then compute similarity with the third one, while methods like UMT Liu et al. (2022), m-PLUG2 Xu et al. (2023) and VAST Chen et al. (2023c) introduce layers that fuse two or more modality embeddings before computing cosine similarity with the remaining one."}, {"title": "3.2 VOLUME OF THE PARALLELOTOPE SPANNED BY THE MODALITY VECTORS", "content": "A generic embedding vector $v$ with dimension $n$ is a vector in the higher-dimensional space $\\mathbb{R}^n$, whose components indicate its extremity. In the case of various modalities encoded with the correspondent encoding functions, multiple embeddings vectors $v_1,..., v_k$ lie in the higher-dimensional space $\\mathbb{R}^n$, i.e. $v_i \\in \\mathbb{R}^n$, $\\forall i \\in [1,k]$. The aim of whatever multimodal representation framework is to ideally have correlated embeddings near to each other and uncorrelated embeddings far away, so as to have an aligned and meaningful embedding space. In this paper, we argue that a measure of the relationship among the vectors in the hyperdimensional space can be given by the volume of the k-dimensional parallelotope with these vectors as sides. Recalling that the vectors are normalized to have a unitary norm, their extremities lie on the surface of the hypersphere with a radius equal to 1, as shown in Fig. 2. The way to obtain the volume of whatever type of k-dimensional parallelotope is computing the determinant of the Gram matrix G of the vectors Gantmacher (1959), as we show in this Section.\nDefinition 1: Gram Matrix. Let $v_1,..., v_k$ be vectors in $\\mathbb{R}^n$, these points can be arranged as columns in a matrix $A = (v_1, ..., v_k)$. Then, the Gram matrix $G(v_1, ..., v_k) \\in \\mathbb{R}^{k \\times k}$ is defined:"}, {"title": "3.3 GRAM: VOLUME AS A MEASURE OF MODALITY VECTORS ALIGNMENT", "content": "The notion of volume in hyperdimensional spaces offers an intuitive approach for understanding the geometric alignment of vectors, which are the representation of data from various modalities. Specifically, the volume spanned by a set of vectors provides a measure of their proximity in the hyperdimensional space. A smaller volume indicates that the vectors are closely aligned, suggesting a close semantic relation between the underlying data. Conversely, a larger volume suggests that the vectors\u2014and, by extension, the input data they represent\u2014are less correlated or potentially opposite.\nOur Gramian Representation Alignment Measure relies on this crucial intuition to fully exploit the richness of the semantic data. This novel methodology facilitates the discovery of interrelationships between sub-modalities by computing the volume of the k-dimensional parallelotope formed by any subset of vectors. Interestingly, the GRAM computation is also efficient as it relies on computing the determinant of a k \u00d7 k matrix, where, in the largest part of real-world scenarios, k \u226a n, and usually k = 3 or 4, meaning it requires negligible computation time. Importantly, the proposed measure is scalable and can be applied to any number of modalities, ranging from 2 to n. Interestingly, in the specific case where k = 2, the volume computation reduces to the calculation of the area of a parallelogram spanned by two vectors with the origin in the hyperdimensional space. Notably, this area is mathematically linked to the angle between the two vectors, thus establishing a direct correspondence between volume and previous similarity methods in the k = 2 case. The extension of this concept to higher-dimensional volumes involving k > 2 vectors generalizes cosine and sine similarities to more complex inter-vector relationships. A formal derivation of this generalization is provided in Appendix A."}, {"title": "3.4 GRAM-BASED CONTRASTIVE LOSS FUNCTIONS", "content": "In this subsection, we completely rethink multimodal representation alignment, by introducing the proposed GRAM-based contrastive loss functions for representation learning and alignment. We define a novel multimodal contrastive loss function completely relying on the aforementioned volume computation. We then describe the additional loss function to enhance the proposed method further."}, {"title": "3.5 GRAM AS MODEL PERFORMANCE METRIC", "content": "As GRAM is a measure of the multimodal embedding space alignment, it can also serve as a metric for evaluating large multimodal models. Indeed, the more aligned the multimodal latent space, the better the model will perform in downstream tasks, as it possesses rich semantic information. We prove the validity of these claims by extracting the embeddings from different multimodal models, LanguageBind (LB), VAST, and GRAM-based model, and computing the zero-shot (zs) and fine-tuned (ft) Recall at 1 (R@1). Then, we compute the proposed GRAM, which we rescale for visualization purposes and use $1 - GRAM$. Figure 3 shows the average, over 1000 test samples, of R@1 (the higher the better) and $1 - GRAM$ (same) for the video, audio, and text embeddings on MSR-VTT. The linear regression line is computed and added in grey dashed style."}, {"title": "4 EXPERIMENTAL EVIDENCES", "content": "In this Section, we present the main results of the proposed GRAM contrastive loss and model in downstream tasks. In addition, we show how the multimodal latent space built with GRAM is more meaningful and disentangled with respect to others."}, {"title": "4.1 EXPERIMENTS SETUP", "content": "We build the GRAM model with VAST models Chen et al. (2023c) as backbone. Therefore, the text, audio, and video encoders are BERT-B, BEATS Chen et al. (2023a), and EVAClip-ViT-G Sun et al. (2023), respectively, with a total number of parameters equal to 1B. Obviously, we remove VAST fusing layers as GRAM does not require them due to its ability to truly model the multimodal latent space. Starting from VAST pretraining models, we further pretrain those on a small subset of VAST27M Chen et al. (2023c) comprising 150k samples using our defined loss functions. This operation is useful to reshape latent space already built by VAST using our GRAM-based contrastive loss function. We inherit from VAST the learned semantics space and we reshape it using our novel loss function. We set the batch size to 256 and a single epoch pretraining on 4 NVIDIA A100 cards.\nAs downstream datasets, we consider several well-known multimodal benchmarks that can be divided into three categories: (i) three-modal video-based, such as DiDeMo and ActivityNet, in which the crucial modality is video and the two other modalities (audio and text) are supportive; (ii) four modal video-based, such as MSR-VTT and VATEX, in which video is the main modality but also audio, text, and subtitles are supportive; and (iii) audio-based, like AudioCaps and VGGSound, in which the audio modality is the most relevant, while also video and text contain interesting information. Details about datasets, samples, and resolutions are in Appendix B."}, {"title": "4.2 RESULTS AND DISCUSSION", "content": "Multimodal Text-to-Video Retrieval. Table 1 and Table 2 show the improvements that the proposed GRAM-based contrastive loss brings to large multimodal models in both zero-shot and fine-tuning scenarios, respectively. The proposed GRAM model surpasses by a large margin the wide"}, {"title": "4.3 VISUALIZING THE ALIGNED GRAM LATENT SPACE", "content": "To demonstrate the capabilities of the GRAM-based contrastive loss introduced in Section 3.4, we analyze the latent embedding space using t-SNE visualization. For this purpose, we utilize the VGGSound dataset, as it is the only dataset among those considered that includes class labels for its samples, favoring a clearer representation. We visualize the top three classes with the most examples in our downloaded portion. For each class, we extract the embeddings using both VAST and the proposed GRAM-based model for the class labels (text modality), as well as for visual frames and audio samples.\nFigure 4 presents the resulting visualizations, where VAST embeddings are on the left, and GRAM space is on the right. The embedding space generated by the GRAM model is significantly more disentangled, with the three modalities of each class well-aligned around the text modality (star symbol). In contrast, the latent space produced by the VAST model appears more scattered, with the different modalities dispersed throughout the embedding space, exhibiting less alignment. Figure 4 provides the visualization of our intuitions, i.e. that GRAM is able to effectively model the multimodal latent space according to the semantics of different modalities. Therefore, in GRAM, all the modalities contribute to the correct classification, resulting in improved performance."}, {"title": "4.4 GRAM SCALING TO MORE MODALITIES", "content": "To experimentally confirm the proof that GRAM holds from 2 to n modalities, we include additional modalities in the MSR-VTT dataset. The largest part of models employ solely the video to correctly retrieve the textual caption. Therefore, first, we test GRAM with these two modalities only, proving that, in this case, the volume computation degenerates to the area of the triangle and that this is still a reliable metric. Following, we add one-by-one the other modalities like audio and subtitles. Table 4 shows that GRAM effectively works for two modalities and that enriching the latent space with more modalities improves the performance from R@1 equal to 52.8 up to 54.8 in text-to-video retrieval. To further stress the proposed GRAM claims, we insert an additional modality to the four already present in MSR-VTT. We employ ChronoDepth Shao et al. (2024) to extract depth maps from video frames, and add a head to the GRAM vision encoder to process this additional modality. Once pretrained with five modalities on the subset of VAST27M, we perform zero-shot on MSR-VTT with all the modalities. Interestingly, not only the method still perfectly works in defining the 5-dimensional parallelotope spanned by the vectors and in computing the volume, but also further increase the performance up to a recall of 55.3. These results prove two key factors of this work: (i) the intuition that more modalities contribute to a richer semantic space and are often crucial in correctly performing donwstream tasks; (ii) the mathematical proof that the proposed volume computation naturally scales to more modalities beyond the classical 2 or 3 ones."}, {"title": "5 CONCLUSION", "content": "In conclusion, we presented GRAM, a fundamentally new measure for multimodal representation learning and alignment that operates in the higher-dimensional space spanned by all modality embeddings. By modeling the alignment through the volume of a parallelotope formed by k modality vectors, GRAM captures richer semantic relationships than traditional pairwise methods like cosine similarity. Furthermore, we introduced a novel GRAM-based contrastive loss, which leverages this geometric alignment to guide multimodal models in shaping a unified embedding space. The model pretrained using our loss outperform state-of-the-art methods by significant margins across multiple tasks, confirming that GRAM generalizes well to a wide range of modalities and tasks. This work represents a significant advancement in multimodal representation learning and alignment by addressing the limitations of pairwise alignment and providing a mathematically grounded, flexible solution for aligning any number of modalities. We believe that GRAM opens up new directions for the field, offering a more powerful and general framework for multimodal understanding and providing new insights into the structure of multimodal spaces."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "In this paper, we present a novel approach to multimodal representation learning. To ensure that our work can be easily reproduced and built upon by the research community, we have taken several key steps. First, the source code implementing our multimodal representation learning model is available as part of the supplementary materials. The code includes all scripts necessary for training, evaluation, and data processing, while pretrained models will be released after reviewing process. Experimental settings and hyperparameters are available in the supplementary materials. We use publicly available datasets and details on training, validation, and testing splits are reported in supplementary materials. In terms of theoretical contributions, we include clear explanations of assumptions and complete proofs for all claims made in the paper. Formal derivations and justifications can be found in the appendix for further verification. Finally, we also provide details about the hardware and software environment used in our experiments.\nBy providing detailed descriptions, open-source code, and clear theoretical justifications, we aim to make our work on multimodal representation learning fully reproducible and accessible to the broader research community."}, {"title": "A THEORETICAL APPENDIX", "content": "A.1 THE GRAMIAM COMPUTES THE VOLUME OF ANY k-DIMENSIONAL POLYTOPE\nTheorem 1: Volume of the k-dimensional parallelotope. Given $v_1,..., v_k$ be k vectors in $\\mathbb{R}^n$ forming a k-dimensional parallelotope. $v_1,..., v_k$ can be arranged as column vectors in a matrix $A = (v_1, ..., v_k)$ and the Gram matrix G is computed as in 3. The determinant of the Gram matrix, also called the Gramian, is the square of the volume of the k-dimensional parallelotope formed by the vectors Gantmacher (1959):\n$Vol(v_1,..., v_k) = \\sqrt{\\det G(v_1, ..., v_k)}.$ (9)\nNote the similarity to the norm of a vector; in fact, $v(v) = ||v||$, so the Gramian of a single vector is the length, i.e., the 1-dimensional volume, of that vector.\nThe Gram matrix $A^T A$ captures all the geometric information about the vectors $v_1,..., v_k$: the length of the vectors and the angles between them. We claim that this is enough information to easily determine the volume.\nTo mathematically prove the theorem we have to distinguish three different cases:\nConsider the case where k = n; then\n$\\det G(v_1,..., v_k) = \\det A^T A = (\\det A)(\\det A) = |A|^2$.\n$G(v_1,..., v_k) \\geq 0$\nSince $Vol(v_1,..., v_k) = \\det A$, we obtain:\n$Vol(v_1,..., v_k) = \\sqrt{\\det A^T A} = \\sqrt{G(v_1,...,v_k)}$\nConsider the case where k < n; then the above holds by simply restricting to a k-dimensional subspace containing them; Given $v_1,..., v_k$, where k $\\geq$ n, assume that they are linearly independent and pick an orthonormal basis $w_1,..., w_k$ for their span, W. Extend to an orthonormal basis $w_1,..., w_k$ for $\\mathbb{R}^n$; the matrix O sending $w_i \\rightarrow e_i$ is an orthogonal change of coordinates, so it does not change inner products: O(v)O(w) = v.w. Let $v_i = O(v_i)$ then"}, {"title": "A.2 THEORETICAL ADVANTAGES OF VOLUME COMPUTATION WRT COSINE SIMILARITY", "content": "Let us consider the simple case with three modalities: Text (T), Video (V), and Audio (A). The Gram Matrix is equal to:\n$G = \\begin{bmatrix} TT & TA & TV \\\\ AT & AA & AV \\\\ VT & VA & VV \\end{bmatrix}$\nNow, let us compute the determinant of the matrix G:\n$\\det(G) = TT (AA \\cdot VV - AV \\cdot VA) -TA \\cdot (AT \\cdot VV - AV \\cdot VT) +TV \\cdot (AT \\cdot VA - AA \\cdot VT)$ (10)\nRecall that T, V, A embeddings are normalized to unit norm and so TT = VV = AA = 1:\n$\\det(G) = 1 (1 - VA^2) - TA \\cdot (AT - AV \\cdot VT) + TV \\cdot (AT \\cdot VA - VT)$ (11)\n$= 1 - VA^2 - TA^2 + TA \\cdot AV \\cdot VT + TV \\cdot AT \\cdot VA - TV^2$ (12)\n$= 1 - VA^2 \u2013 TA^2 \u2013 TV^2 + 2 \\cdot TA \\cdot AV \\cdot VT$ (13)\nTherefore, the volume computation through the Gram matrix includes in its computation all the cross-products, resulting in an alignment of all the modalities together. In contrast, current state-of-the-art methods based on cosine similarity only compute the similarities between the modalities and the anchor (TA and TV), omitting the similarities between non-anchor modalities (AV), which in practice may not be aligned."}, {"title": "A.3 THE VOLUME IS RELATED TO THE ANGLE BETWEEN THE TWO VECTORS IN THE TWO-MODAL CASE", "content": "Consider two vectors $v_1, v_2 \\in \\mathbb{R}^n$ with $||v_1|| = ||v_2|| = 1$. The Gram matrix G is given by:\n$G = \\begin{bmatrix} (v_1^T v_1) & (v_1^T v_2) \\\\ (v_2^T v_1) & (v_2^T v_2) \\end{bmatrix}$ (14)\nThen, compute the determinant of the Gram matrix:\n$\\det(G) = (v_1^T v_1) (v_2^T v_2) - (v_1^T v_2)^2$ (15)\nThe volume of the k-dimensional parallelotope spanned by the modalities $v_1, v_2$ is:\n$Vol = \\sqrt{\\det(G)} = \\sqrt{(v_1^T v_1) (v_2^T v_2) - (v_1^T v_2)^2}$ (16)\nGiven that $v_1$ and $v_2$ have norm equal to 1, equation 16 reduces to:\n$Vol = \\sqrt{\\det(G)} = \\sqrt{1 - (v_1^T v_2)^2}$ (17)\nThe cosine similarity is defined as $\\cos(\\theta) = (v_1^T v_2)/||v_1|| \\cdot ||v_2||$, however, considering the unitary norm of the embeddings vectors, the cosine similarity reduced to $\\cos(\\theta) = (v_1^T v_2)$. Therefore, equation 17 becomes:\n$Vol = \\sqrt{\\det(G)} = \\sqrt{1 - \\cos^2(\\theta)} = \\sqrt{ \\sin^2(\\theta)} = \\sin(\\theta)$. (18)\nTherefore, in the case of two modalities, the volume computation reduces to the sine of the angle between the two modality vectors."}, {"title": "B.1 EXPERIMENTAL DETAILS", "content": "We utilize several benchmark datasets for our downstream tasks:\nMSR-VTT Xu et al. (2016) comprises 10000 video clips accompanied by 200000 captions, encompassing a diverse range of subjects including human activities, sports, and natural landscapes.\nVATEX Wang et al. (2019) consists of 41250 video clips derived from the Kinetics-600 dataset Kay et al. (2017) and 825000 sentence-level descriptions. Since a large part of the dataset is now unavailable online due to removed or private videos thus we use only a portion of the original dataset composed of 14491 samples.\nDiDeMo Hendricks et al. (2017) includes 10000 long-form videos from Flickr, with each video annotated by four temporally ordered short sentences. The dataset is uniquely annotated with natural language descriptions corresponding to distinct moments within each video. For each video, four short sentences are provided, arranged in temporal order to describe specific events or scenes.\nActivityNet Caba Heilbron et al. (2015) comprises 20000 long-form videos (with an average duration of 180 seconds) from YouTube and 100000 captions. It has a diverse range of 200 human activity classes, spanning daily activities, sports, and various other complex human behaviors. ActivityNet is annotated with both activity labels and temporal boundaries, providing fine-grained information about when specific activities occur within each video.\nAudioCaps Kim et al. (2019) is a dataset comprising 51000 audio clips, each with a duration of 10 seconds. The dataset annotation structure varies between its subsets: the training set contains one caption per clip, while the validation and test sets are annotated with five captions per clip. We follow the split protocol established by Oncescu et al. (2021) for the text-to-audio retrieval task.\nVGGSound Chen et al. (2020) is a large-scale audio-visual dataset containing more than 200000 videos sourced from YouTube, encompassing a diverse range of 309 audio classes. Each video clip in the dataset has a duration of 10 seconds and is annotated with a single label corresponding to the predominant sound event occurring within the clip. The dataset covers a wide spectrum of audio events, including human actions, animal vocalizations, natural phenomena, and mechanical sounds. Due to several downloading problems we use only a portion of the original dataset composed of 5000 samples in testing.\nFor every dataset, we utilize the official split for retrieval tasks, the dataset splits, and the number of frames for fine-tuning and evaluations on all the datasets in Tab. 5. We pretrain the GRAM-based model on a subset of the VAST27M Chen et al. (2023c) dataset comprising 150k random samples with a learning rate of 1e - 4 using the AdamW optimizer with weight decay and batch size of 256. For finetuning we reduce the batch size to 64 and change the number of epochs according to the specific dataset, the complete details are shown in 5."}, {"title": "B.2 ABLATION STUDIES", "content": "We perform ablation studies to further validate our experimental results. To evaluate the effectiveness of the introduced volume-based loss functions and the contribute of each one we perform comparative test on MSR-VTT and ActivityNet datasets. Results shown in Tab. 6 are obtained training from scratch our model with different training settings. We train the model for 4 epochs on MSR-VTT and 20 epochs on ActivityNet. The use of our proposed loss funtions allow the model to obtain superior performance with respect to the classical approach in which we use cosine-based contrastive loss functions among pairs of modalities.\nTo further investigate the superiority of our approach compared to the classical method, which relies on measuring similarity among pairs of modalities, we present several key metrics that provide insights into the behavior of GRAM during training. The gramian value is computed averaging"}, {"title": "B.3 DISCUSSION ON THE MODALITY GAP", "content": "We further explore the phenomenon of the modality gap, as introduced by Liang et al. (2022). In Tab. 7, we report the mean cosine similarity between embeddings of the same modality (VV \u2192 Vision, TT \u2192 Text, AA \u2192 Audio) and the distances between the centroids of different modalities (e.g., VT \u2192 distance between the Vision centroid and the Text centroid, and so on). The centroids are computed following the methodology outlined in Liang et al. (2022). Consistent with the conclusions of Liang et al. (2022), we observe that the modality gap exists, and its relationship with final performance metrics is complex and not easily interpretable. An empirical hypothesis we propose is that the loss functions we introduce favor two behaviors: i) it squeezes the clusters of each modality more than VAST, indeed the average cosine similarities inside the modalities are higher. ii) It increases the gap among the modalities, probably producing a more sparse latent space. This is clear from the last three columns of Tab. 7, where the distances between modality cluster centroids are shown. Again, as clearly stated in Liang et al. (2022), although GRAM obtains better performance in downstream tasks, there are no mathematical proofs that link such performance to the larger modality gap."}, {"title": "B.4 ADDITIONAL RESULTS", "content": "Figure 6 shows a visualization that demonstrates the effectiveness of the proposed method. Specifically, using four triplets from YouTube consisting of video, audio, and text, we first compute the cosine similarity between text-video and text-audio pairs across all text labels and videos/audios. These results are arranged into a 4 \u00d7 4 matrix, and a temperature-scaled softmax is applied to the rows to obtain a probability distribution for each text-video (audio) pair assigned by the model.\nNext, we apply our proposed GRAM approach to the same dataset, once again performing temperature-scaled softmax on the matrix rows. In this case, the volume matrix is negated, as a lower volume indicates a stronger correspondence among text, video, and audio.\nAs Fig 6 highlights, conventional cosine-based methods fail in jointly exploiting both audio and video modalities, often misleading when a single modality does not contain enough information for the correct classification. Conversely, the GRAM-based model exploits the semantically aligned multimodal space and jointly leverages all the modalities, leading to a better and correct retrieval in every case.\nAdditionally, we report Recall@1 and Recall@10 for text-to-video and video-to-text both zero-shot and finetuned experiments in Table 8, Table 9, Table 10, and Table 11. Our GRAM model outperforms every method in the very large set of comparison methods in T2V and V2T both in zero-shot and fine-tuning scenarios. More interestingly, the proposed method always achieves"}]}