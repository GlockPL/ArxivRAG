{"title": "ENSURING FAIR LLM SERVING AMID DIVERSE APPLICATIONS", "authors": ["Redwan Ibne Seraj Khan", "Kunal Jain", "Haiying Shen", "Ankur Mallick", "Anjaly Parayil", "Anoop Kulkarni", "Steve Kofsky", "Pankhuri Choudhary", "Ren\u00e8e St. Amant", "Rujia Wang", "Yue Cheng", "Ali R. Butt", "Victor R\u00fchle", "Chetan Bansal", "Saravan Rajmohan"], "abstract": "In a multi-tenant large language model (LLM) serving platform hosting diverse applications, some users may\nsubmit an excessive number of requests, causing the service to become unavailable to other users and creating\nunfairness. Existing fairness approaches do not account for variations in token lengths across applications and\nmultiple LLM calls, making them unsuitable for such platforms. To address the fairness challenge, this paper\nanalyzes millions of requests from thousands of users on MS CoPilot, a real-world multi-tenant LLM platform\nhosted by Microsoft. Our analysis confirms the inadequacy of existing methods and guides the development of\nFAIRSERVE, a system that ensures fair LLM access across diverse applications. FAIRSERVE proposes application-\ncharacteristic aware request throttling coupled with a weighted service counter based scheduling technique to curb\nabusive behavior and ensure fairness. Our experimental results on real-world traces demonstrate FAIRSERVE'S\nsuperior performance compared to the state-of-the-art method in ensuring fairness. We are actively working on\ndeploying our system in production, expecting to benefit millions of customers world-wide.", "sections": [{"title": "INTRODUCTION", "content": "In recent years, multi-tenant personalized large language\nmodel (LLM) serving platforms, which support a wide range\nof applications like interactive question answering (QA),\nsummarization, coding assistance, etc., have surged in pop-\nularity, with notable examples including Copilot (Microsoft,\n2024a), Punica (Chen, 2023), and S-LoRA (Sheng et al.,\n2024a). On these serving platforms, some users may flood\nthe system with excessive requests, causing service disrup-\ntions for others and leading to considerable unfairness. For\ninstance, on February 13, 2024, OpenAI reported a partial\noutage lasting over five hours for its API and ChatGPT ser-\nvices due to DDoS attacks, which recurred with varying\ndurations in the following days (Anchia, 2024). Therefore,\nto maintain service availability and fairness, it is crucial\nto prevent abusive usage and ensure fair distribution of re-\nsources among users on LLM platforms.\nExisting LLM platforms enforce limits on the number of\nrequests each user can submit within a given period, such as\nrequests per minute (RPM) to address the challenge of abuse\nprevention and ensuring fairness. For example, Google\nGemini for Workspace limits usage to 500 times per month\n(Google, 2024), and OAI Chat GPT Plus imposes limits of\n80 messages every 3 hours on GPT-40 and up to 40 mes-\nsages every 3 hours on GPT-4 (OpenAI, 2024a). However,\nsuch rate-limiting solutions have drawbacks. First, modern\nLLM applications are built from multiple LLM agents, each\noperating with an LLM model, working together to produce\na single response for the user. The graph of interconnected\nLLM call requests is termed an LLM interaction. Throt-\ntling at the LLM request-level without understanding the\napplication's needs can result in incomplete user responses\nand resource wastage. Second, requests can still be throt-\ntled even if the system is underloaded, causing resource\nunderutilization and user frustration.\nThe Virtual Token Counter (VTC) scheduler prioritizes re-\nquests of users who receive the minimal service when pro-\ncessing batches, aiming to prevent unfairness (Sheng et al.,\n2024b). However, unlike RPM, VTC does not throttle or\nblock users, thus making it unable to fully prevent abusive\nbehaviors. Based on the principle of fair-queueing (Nagle,\n1987), VTC gives both benign and abusive users equal ser-\nvice, leading to resource wastage, longer request queues,\nincreased latency, and user frustration.\nAdditionally, modern LLM platforms receive requests from\nusers having varying needs as they belong to different appli-\ncations. Different applications have distinct typical ranges\nfor input and output token lengths. For instance, article\nsummarization applications often involve long inputs and\nshort outputs, while code generation applications typically"}, {"title": "2 BACKGROUND", "content": "Transformer Architecture. Modern LLMs are built on\nmultiple transformer blocks (Vaswani et al., 2017). Each\nblock consists of an attention operation which applies three\nweight matrices $W_K$, $W_Q$, and $W_V$ respectively to the en-\ncoded represention of the input tokens, X of an user, to\ncalculate the key K, query Q, and value V respectively asso-\nciated with the corresponding input prompt.\n$Q = XW_Q, K = XW_K, V = XW_V$\n$Attention(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d_k}}) V$ (1)\nNext, the K, Q, and V matrices are used to calculate the\nattention head following Eq. 1, which is later linearly com-\nbined with other attention heads to get the multi-head atten-\ntion (MHA), allowing the model to focus on different parts\nof the sequence in different representational spaces. After\nthe MHA block, the output undergoes layer normalization\nand is passed through a position-wise feed-forward network\n(FFN). The FFN consists of two linear transformations with\nan activation function in between, allowing further refine-\nment of the information processed by the MHA block. The\nfinal output retains the same dimensionality as the input.\nLLM Inference Procedure. LLM inference procedure\nconsists of two phases-a prefill phase and a decode phase."}, {"title": "3 PRODUCTION WORKLOAD ANALYSIS", "content": "We analyze a full day of trace data from 34 distinct appli-\ncations, covering millions of requests\u00b9. We investigate the\nimpact of user behavior, application behavior, LLM agents,\nand throttling techniques to gain insights into designing a\nWe have anonymized the exact volume and application names\ndue to confidentiality reasons."}, {"title": "3.1 Impact of User Behavior", "content": "We first investigate user request patterns, specifically as-\nsessing if requests from users and applications follow any\nparticular trends with the goal to curb abusive behaviour\nand ensure fairness across users.\nFirst, we profile the CDF of users by requests per minute\n(RPM) and tokens per minute (TPM) to identify potential\nabusive behaviors. Figures 2a and 2b show that the overall\nrequest frequency and number of tokens sent by a user tends\nto fall in a certain range (99th percentile for RPM is below\n102). However, the maximum RPM and TPM observed is\nupto 104 and 106 respectively, showing signs that different\nusers have different trends. In this case, a rate-limiting\npolicy may mark a lot of users as abusive where as in reality\nthis could be due to difference in their trends.\nNext, fine-grained analysis into the users of different applica-\ntions reveal that users behave differently across applications\nin terms of RPM and TPM. For example, Figure 2c (each\ncurve is for an app and each point corresponds to a user)\nshows that although one app has only around 20% of its\nusers' RPM to be less than or equal to 10-1, for others it\ncould be well above 80%. Again, Figure 2d shows that\nalthough around 80% of the users of some app have a TPM\nof 102 or less, for other apps it could be just 20%. Both of\nthese observations mark a stark dissimilarity in the RPM and\nTPM limits across applications, prompting us to treat users\nbelonging to different applications differently. Hence, trying\nto maintain equal TPM across all users like VTC (Sheng\net al., 2024b) in ensuring fairness might not be ideal and\nwill not be sufficiently effective to deter abusive behavior.\nTakeaway 1. A user's overall RPM and RPM for\neach application falls in a certain range and these\nare different from one another. As benign users have\ndifferent tokens per minute (TPMs), simply targeting\nequal TPMs across users is not sufficiently effective\nto deter the abusive behavior or achieve fairness."}, {"title": "3.2 Impact of Application Behavior", "content": "In this section, we closely examine the input, output, and\ntotal token lengths across various applications. Our aim\nis to detect token patterns that will aid in designing a fair\nweighting system for users of different apps. Figure 3a il-\nlustrates that the average input lengths differ significantly\nacross applications, with each having a specific range within\nwhich all its requests fall. Although Figure 3b shows that\nthe output lengths across applications are more similar than\nthe input lengths, there are still some variations. This in-\nsight pushes us to reevaluate how we assess user service\nacross different applications. We propose shifting from a"}, {"title": "3.3 Impact of LLM Agents and System Prompts", "content": "Next, we investigate the impact of LLM agents and system\nprompts on generating the final response for a user request.\nFigure 4a shows that across applications the number of LLM\ncalls vary, i.e., the same application can generate different\nnumber of LLM calls (can be over 20) using LLM agents,\ndepending on the context of the user request. Furthermore,\nFigure 5a shows that: (1) across applications the system\nprompt token count varies, and (2) even within the same\napplication, the same LLM agents can generate different\nnumber of system tokens. Moreover, Figure 5b shows that\nthese system prompts coming from LLM agents can lead\nto the generation of different number of output tokens. In\nparticular, we observe that 20% of the users of four applica-\ntions are generating over 500 output tokens. Additionally,\nFigure 4b shows that these LLM calls have varied latencies\n(can be over 6 seconds), depending on the position of the\nLLM call request in the waiting queue for the interaction.\nLong latencies are undesirable for applications with tight\nSLOs, stressing the need for proper placement of LLM call\nrequests during scheduling in order to reduce latencies and\nqueueing delays for multi-agent LLM requests.\nTakeaway 3. The number of LLM calls and their la-\ntencies vary across different applications and within\nthe same application. Inefficient scheduling can\ncause these calls to get queued up, leading to de-\nlays in the execution of these LLM calls and hence\nresults in high latencies. This observation indicates\nthat scheduling must account for LLM call patterns\nto avoid delaying users."}, {"title": "3.4 Impact of RPM Throttling", "content": "In industries, the de-facto standard of throttling to curb abu-\nsive behavior has been to use rate-limiting solutions, i.e.,\nif the number of requests belonging to a user goes above\na certain limit, then the request gets throttled (Anthropic,\n2024; OpenAI, 2024b). While simple, this basic throttling\napproach has several issues. First, as shown in Table 2,\naround 26.8% of all requests have a graph size larger than\none (i.e., each interaction consists of multiple LLM calls),\nmeaning that if the user goes over the limit within the inter-\naction, the request will get throttled and the user will not\nget a response. Figure 6c shows that throttling unawareness"}, {"title": "4 FAIRSERVE DESIGN", "content": "Our study in \u00a7 3 sheds light on the potential to address the\nproblem of throttling in the middle of interactions and token\nwastage, and motivates the design of an application and user\nrequest characteristic-aware system for ensuring fairness.\nThe main argument in VTC is to serve clients as per the\nprinciple of fair queueing (Nagle, 1987), where each client\nis guaranteed at least an equal amount of resources, i.e., 1/n\nof the server's resources. FAIRSERVE argues for weighted\nfair queueing (Demers et al., 1989), where resources are\nallocated based on pre-determined weights, allowing for an\nequitable allocation tailored to the specific needs or behavior\nof each user or application. This approach recognizes that\nfairness is not always achieved through strict equality, as\nproposed by VTC (Sheng et al., 2024b); instead, equitable\nallocation can better address heterogeneous demands of\nusers and applications.\nDesigning a weighted fair LLM serving system presents\nnon-trivial challenges.\nFirst, in multi-tenant LLM service providers, resource\nwastage primarily arises from two areas: (1) allowing abu-\nsive users a portion of valuable resources, and (2) mid-\ninteraction throttling even if system is underutilized. Increas-\ning resource utilization while ensuring quality service for\nbenign users and detecting abuse thus becomes challenging,\nrequiring fine-grained tracking within LLM interactions.\nSecond, users in multi-tenant environments belong to\ndiverse applications, each with unique characteristics.\nWeighted fair allocation requires understanding each appli-\ncation's behaviors at different stages of an LLM interaction.\nThis section presents the design principles of FAIRSERVE\nto address these challenges, followed by the design detail."}, {"title": "4.1 FAIRSERVE Overview", "content": "FAIRSERVE comprises of two core components: (1)\nOverload and Interaction-driven throttling (OIT), and (2)\nWeighted Service Counter (WSC) scheduling (see Figure 7).\nIn contrast to traditional RPM-based throttling, OIT throt-\ntles user requests only when the KV cache is overloaded,\nthus making maximum utilization of available resources.\nFAIRSERVE uses a combination of user- and application-\nlevel limits to perform throttling at the LLM interaction\nlevel instead of at the request level, reducing token wastage\nby accounting for user and application behaviors. WSC\ncrafts a user service weighing mechanism determined by\nthe ratio of tokens processed to the expected token count\n(e.g., the average based on historical statistics) for the appli-\ncation associated with each user's requests at each level of\nthe LLM interaction. WSC selects requests from the users\nwho have received the least service, defined by a weighted\nresource slice. This weight is calculated based on the token\nratio to ensure fairness across users.\nAlgorithm 1 shows the details regarding how the two com-\nponents interact with one another while processing requests.\nThe entire system is integrated with the state-of-the-art con-\ntinuous batching mechanism and operates in two parallel\nstreams- the monitoring and the execution stream. The\nmonitoring stream continuously listens for incoming re-\nquests and the execution stream is in charge of the prefill\nand the decode phases. OIT is a part of the monitoring\nstream (Alg. 1, lines 19-25) and the larger chunk of weight\nand service allocation mechanisms in WSC take place in\nthe execution stream (Alg. 1, lines 27-48)."}, {"title": "4.2 Overload & Interaction-driven Throttling (OIT)", "content": "OIT performs two main functions: (1) tracking requests\nfrom users and applications against specified limits at both\nthe user and application level; and (2) tracking the condition\nof the KV cache and throttling based on the specified limit\nfor users and apps only when the KV cache is overloaded.\nTo track user and application requests, OIT maintains a dic-\ntionary that maps each user and application to the arrival\ntimes of their respective requests. OIT takes a combined\nrate-limiting approach that merges both user and application\nlimits, based on the analysis of historical data. A com-\nbined user and application-aware approach helps address\nthe challenge of resource wastage caused by abusive behav-\niors. When a new request arrives, its arrival time is appended\nto the list associated with the corresponding user and app.\nTracking arrival times enables OIT to detect if any user or\napplication exceeds their request limit. Upon request arrival,\nthe internal request counter for the corresponding user and\napplication is incremented (Alg. 1, line 19).\nOIT continuously monitors the state of the KV cache and"}, {"title": "4.3 Weighted Service Counter (WSC)", "content": "WSC performs two main functions: (1) calculating the ser-\nvice received by users based on the ratios of input, system,\nand output tokens processed for their requests to the maxi-\nmum token limits of the corresponding apps; and (2) identi-\nfying the user with the least service received and forwarding\nthe user's request for processing.\nTo align user service with the characteristics of their corre-\nsponding applications, WSC updates the service received by\nusers based on the ratio of processed tokens to the normal\nrange (average) of request token length associated with the\napp. Before a user receives a response, the request goes\nthrough multiple stages of an interaction depending on the\napplication. We model the weight of app 'a' in stage 'j' of\ninteraction in Eq. 2.\n$W_{aj} = \u03b1 \\frac{\u00d1^I_{aj}}{\u00d1^I_a} + \u03b2 \\frac{\u00d1^S_{aj}}{\u00d1^S_a} + \u03b3 \\frac{\u00d1^O_{aj}}{\u00d1^O_a}$ (2)\nEach of the different types of tokens also has a weight\nassociated with it. Since processing input tokens can be par-\nallelized and hence is much fast compared to processing the\noutput tokens, weights enable WSC to ensure that service\nis counted according to the number of different categories\nof tokens that are getting processed. Following OpenAI\nconventions, we fix \u03b1 = 1, \u03b2 = 2, and \u03b3 = 1 to be the\nweights for input, system and output tokens respectively.\n$\u00d1^I_{aj}$, $\u00d1^S_{aj}$, and $\u00d1^O_{aj}$ represent the expectation of the number\nof input, system, and output tokens respectively for the app\na at stage j of an interaction. We calculate these expectation\nvalues based on analysis on historical data.\nThe input, system prompt, and output lengths for each user\ni corresponding to app a are denoted by $L^I_{aij}$, $L^S_{aij}$, and $L^O_{aij}$\nrespectively. The total service received by a user at the\nend of an interaction is computed using Eq. 3 where app a\nrequires $m_i$ LLM calls to complete an interaction and $E_i$ is\na user priority factor that can be adjusted to give preference\nto certain users based on system policies or usage trends.\n$S_i = E_i * \\sum_{j=1}^{m_i} (\u03b1 \\frac{L^I_{aij}}{W_{aj}} + \u03b2 \\frac{L^S_{aij}}{W_{aj}} + \u03b3 \\frac{L^O_{aij}}{W_{aj}})$ (3)\nWSC maintains a service counter $u_i$ for all users which\nis initialized to 0. Waiting queue, Q immediately adds a\nnew incoming request upon its arrival after checking for\nabusive behaviour using overload-driven throttling \u00a7 4.2\n(Alg. 1, lines 19-24). Q is a dict that maps users to their\ncorresponding incoming requests. If this is the only request\nfrom the sender user, then a counter adjustment takes place\n(Alg. 1, lines 12-18) similar to prior research (Sheng et al.,\n2024b). This sort of adjustment is done to create balance,\nensuring that underloaded periods of certain users do not\ncreate unfairness against other active users.\nIn the execution stream, WSC evaluates whether new mini-\nbatches, Mnew, composed of user requests, can be merged\nwith the current batch, B. Initially, it identifies which re-\nquests within the current batch are part of an ongoing in-\nteraction, requiring additional processing to complete the\ninteraction. Among those requests, the system identifies the\nuser who has received the least service so far and selects\nthe next request from that user's interaction (Alg. 1, lines\n31-35). This technique enables FAIRSERVE to keep lower\nqueueing lengths and ensure better response times.\nOtherwise if no requests in the current batch are part of an\nongoing interaction, the earliest request from the user who\nhas received the least service, as determined by Eq. 3, is\nselected for inclusion in Mnew (Alg. 1, lines 36-39). Once\nthe prefill and decode stages for a request are completed,\nthe service counters for the users are updated, proportional\nto the app's assigned weights (Alg. 1, lines 40-48)."}, {"title": "5 EVALUATION", "content": "In this section we evaluate the effectiveness of FAIRSERVE\nagainst other baselines on the real-world workload trace that\nwe analyzed in \u00a7 3. We run all of the policies for different\ndurations of time and observe how each perform on curbing\nabusive behaviour and achieving fairness. FS (W+I) or\nFS indicates FAIRSERVE with all components and FS (W)\nindicates FAIRSERVE is using only the WSC component.\nWe denote all of the applications in our experimentation\nwith Ids to protect anonymity and privacy.\nOur baselines are two state-of-the-art solutions used widely\nin industry and academia for curbing abusive behavior and\nachieving fairness\u2014(1) RPM (Requests per minute), i.e., a\nrate-limiting solution; and (2) VTC (Sheng et al., 2024b).\nOur evaluation aims to answer the following questions:\n\u2022 To what extent can FS curb abuse? (\u00a7 5.1)\n\u2022 How effectively does FS reduce queuing delays for"}, {"title": "5.1 Curbing Abusive Behaviour", "content": "As discussed in \u00a7 2, in multi-agent LLM apps, user queries\ninitiate interactions having multiple LLM calls. Throttling\nin the middle of interaction wastes resources on tokens that\nprovide no benefit, as the user receives no final response.\nFigure 8a shows that as RPM based strategy is completely\noblivious of interactions, it throttles users blindly based on a\nspecified limit, and hence throttles 21.15\u00d7 more interactions\ncompared to interaction-aware FS (W+I). FS (W+I) throttles\nonly those interactions that are not in the middle of execu-\ntion. Figure 8b shows that RPM's unwareness regarding\nmulti-agent apps wastes a lot of tokens (mean: 15.66 * 103).\nOn the other hand, FS (W+I), with multi-agent awareness\nand knowledge about the KV cache, only throttles during\nsystem overloads when users are not in mid-interaction, thus\nnot incurring any token wastage while maintaining maxi-\nmum KV cache utilization.\nMoreover, FS (W+I) manages to throttle only users who fall\noutside the normal range, i.e., abusive. Figure 8c shows that\nwhile FS (W+I) throttles users whose request distribution\nis well above the normal range, RPM policy throttles users\nwith very low request distribution. VTC and FS (W) does\nnot have any throttling policy and hence serves all users\nwithout throttling. However, our evaluation shows that VTC\nwastes approximately 77.2 \u00d7 103 tokens by serving both\nabusers and benign users equally."}, {"title": "5.2 Reducing Queueing Delays", "content": "One of the aspects that adversely affects user experience is\nif requests are waiting for longer durations before process-\ning. Since, FAIRSERVE is multi-agent aware, it prioritizes\nserving users who are already in the middle of an interaction\nusing the WSC component (\u00a7 4.3). Thus users do not wait"}, {"title": "6 RELATED WORK", "content": "Fairness & Characteristic-awareness in ML Training.\nFair scheduling mechanisms like Quincy (Isard et al., 2009),\nDRF (Ghodsi et al., 2011), and CARBYNE (Grandl et al.,\n2016) have been proposed for long-running jobs in clus-\nters. However, ML jobs differ from traditional workloads\nin their nature, with specific requirements for scheduling\nand placement. As a result traditional policies fail to ensure\nfairness (Mahajan et al., 2020).\nTo ensure fairness in ML job scheduling, Gandiva (Chaud-\nhary et al., 2020) introduces a profiling and resource-\ntrading approach, Themis (Mahajan et al., 2020) employs\nan auction-based method to balance fairness and efficiency,\nGavel (Narayanan et al., 2020) combines optimization tech-\nniques with preemptive, round-based scheduling to fairly al-\nlocate resources among users, and Pollux (Qiao et al., 2021)\ndynamically adjusts resources to optimize both cluster-wide\nperformance and fairness. Sia (Jayaram Subramanya et al.,\n2023) leverages integer linear programming to ensure fair-\nness while enabling elastic scaling for hybrid parallel ML\njobs. Additionally, SHADE (Khan et al., 2023) exploits sam-\nple characteristics and FedCaSe (Khan et al., 2024) exploits\nclient and sample characteristics to speed up ML training.\nWhile these works address scheduling, fairness, and effi-\nciency in ML training employing a wide variety of ML\ncharacteristic-aware techniques, they are not designed to\nhandle the specific requirements of LLM serving jobs in\nmulti-tenant environments. In such settings, LLM serving\nrequires addressing distinct issues, such as prioritizing users\nand applications during request batching and throttling to\nmitigate abusive behavior.\nFairness & Characteristic-awareness in LLM Serving.\nRecently, there has been growing momentum in both in-\ndustry and academia focused on enhancing LLM serving\nsystems. Techniques like batching (Agrawal et al., 2023),\nmemory optimizations (Jin et al., 2023), scheduling (Wu\net al., 2023a), exploitation of other LLM-specific charac-\nteristics like chunking prefills (Agrawal et al., 2024), dis-\naggregation of execution stages (Patel et al., 2024; Zhong\net al., 2024) or a combination of these techniques (Kwon"}, {"title": "7 CONCLUSION", "content": "While there have been significant strides in lowering la-\ntency and enhancing throughput for LLMs, ensuring fairness\nacross diverse LLM application users is a newer challenge\nthat has garnered significant attention. While several fair-\nness methods have been suggested, they fall short when\napplied to LLM users in diverse applications. In this work,\nwe conduct a large-scale analysis of LLM user and appli-\ncation characteristics at a leading LLM service provider.\nUsing our gathered insights, we propose FAIRSERVE-a\nuser and LLM application characteristic-aware LLM serv-\ning system that ensures fairness in scheduling user requests\nin a multi-tenant setting."}]}