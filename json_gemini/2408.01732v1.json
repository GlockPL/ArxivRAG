{"title": "Landmark-guided Diffusion Model for High-fidelity and Temporally Coherent Talking Head Generation", "authors": ["Jintao Tan", "Xize Cheng", "Lingyu Xiong", "Lei Zhu", "Xiandong Li", "Xianjia Wu", "Kai Gong", "Minglei Li", "Yi Cai"], "abstract": "Audio-driven talking head generation is a significant and challenging task applicable to various fields such as virtual avatars, film production, and online conferences. However, the existing GAN-based models emphasize generating well-synchronized lip shapes but overlook the visual quality of generated frames, while diffusion-based models prioritize generating high-quality frames but neglect lip shape matching, resulting in jittery mouth movements. To address the aforementioned problems, we introduce a two-stage diffusion-based model. The first stage involves generating synchronized facial landmarks based on the given speech. In the second stage, these generated landmarks serve as a condition in the denoising process, aiming to optimize mouth jitter issues and generate high-fidelity, well-synchronized, and temporally coherent talking head videos. Extensive experiments demonstrate that our model yields the best performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Talking Head Generation (THG) stands as a significant and promising research domain within AIGC. Its objective is to synthesize a talking head video [1]\u2013[5] guided by provided speech, finding wide application across various fields such as virtual avatars, film production, and online conferences [6]\u2013[8]. In THG, researchers primarily emphasize three key aspects: lip-speech synchronization, visual quality, and temporal coherence. The primary objective of THG is to create talking head videos where lip movements align precisely with speech, hence making lip-speech synchronization a pivotal metric. The human sensitivity to video clarity underscores the importance of visual quality in the generated frames, significantly impacting the overall video quality. Beyond clarity, the coherence between frames plays a crucial role in establishing the authenticity and fluidity of the video. Hence, attention to the video's temporal coherence is equally essential.\nRecent years have seen the emergence of several commendable works in this research field, primarily concentrating on optimizing the aforementioned three aspects. Within these endeavors, certain GAN-based methodologies [1], [2], [9] demonstrate the capability to generate talking head videos with highly accurate lip-speech synchronization. Nevertheless, as these methods generate facial images separately and subsequently integrate them back into the original frames, artifacts or false details along the edges of the integrated areas may arise, potentially affecting the authenticity of the resulting video. Moreover, the inherent characteristics of GANs contribute to unstable training, heightened sensitivity to hyperparameter selection, and a susceptibility to issues like mode collapse. Conversely, diffusion-based methods [3], [10], [11] excel in producing artifact-free frames with high visual quality. However, owing to the diverse range of images generated by the diffusion model, there might be a trade-off, potentially leading to compromised temporal coherence within the resulting video.\nTo leverage the outstanding image generation capabilities of diffusion models [12]\u2013[14] and ensure the temporal consistency of the resulting videos, we introduce a novel two-stage diffusion-based model. Figure.1 illustrates our approach, where we segment the talking head synchronization task into two stages, bridged by facial landmarks serving as the intermediary representation. In the first stage, we generate facial landmarks guided by the given speech. Then, in the second stage, these generated landmarks serve as guiding conditions for the denoising process. Using facial landmarks instead of relying solely on the given speech as a condition offers more robust guidance for the denoising process. This approach ensures enhanced temporal consistency in the resulting videos.\nIn summary, our contributions are as follows:\n\u2022 We propose the first landmark-guided diffusion model for talking head generation.\n\u2022 We devise a two-stage framework that notably enhances the temporal coherence of talking head videos while preserving the visual quality of frames.\n\u2022 Extensive experiments demonstrate that our approach yields the best performance. Please refer to the supplementary materials for related results."}, {"title": "II. METHOD", "content": "To harness the outstanding attributes of Latent Diffusion Models in producing high-quality images swiftly, and to address mouth jitter in the generation of talking faces, we introduce a novel two-stage diffusion-based model. Figure.1 provides an overview of the proposed model. The initial phase involves the Landmark Generation Network, which utilizes the provided speech and the original facial landmark to produce"}, {"title": "A. Overview", "content": "To harness the outstanding attributes of Latent Diffusion Models in producing high-quality images swiftly, and to address mouth jitter in the generation of talking faces, we introduce a novel two-stage diffusion-based model. Figure.1 provides an overview of the proposed model. The initial phase involves the Landmark Generation Network, which utilizes the provided speech and the original facial landmark to produce the landmark movement sequence. This sequence acts as a condition for the subsequent denoising process. In the second stage, the denoising process is guided by utilizing the identity reference image, the pose reference image, and the generated landmarks as constraints. This process aims to generate a high-fidelity, synchronized talking head video. Subsequently, we will provide detailed elaboration on the two aforementioned sub-modules in Sec.II-B and Sec.II-C."}, {"title": "B. Audio-driven landmark generation", "content": "The first sub-module is the audio-driven landmark generation, called A2L. Given an audio clip and a facial image, A2L will generate a sequence of landmark movements that align with the audio. For the landmark part, we employ an existing network [15] to extract the 2D facial landmark, denoted as $l \\in R^{68\\times2}$. Speech contains two types of information, i.e., identity information and context information. Therefore, two different encoders are employed to extract identity embedding and context embedding. We utilize AutoVC network [16], denoted as $E_c$, to extract the speaker-agnostic content embedding $a_c \\in R^{T\\times D_1}$, where T is the total number of input audio frames, and $D_1$ is the content dimension. We utilize a speaker verification model [17], denoted as $E_{id}$, to extract the speaker identity embedding $a_{id} \\in R^{D_2}$, where $D_2$ is the identity dimension.\nThe Landmark Generation Network comprises two LSTM modules: one serving as the context network and the other as the identity network. The context network takes as input the context embedding of the audio and the original landmark $l$ to generate a speaker-independent sequence of landmark movements, which can be formulated as follows:\n$l_i = Context(a_{c, t_{t_i}}, l)$\nWhere $l_i$ is the i-th speaker-independent intermediate landmark, solely reflecting the content of the speech. $a_{c, t_{t_i}}$ represents the context embedding from frame t to frame $t_i$, aligned with the i-th landmark.\nThe intermediate landmarks contain only motion information relevant to the speech content but lack personal style. To further refine the intermediate landmarks and generate an accurate sequence of landmark movements, the identity network is employed. After the refinement process, the generated landmarks will be closer to the actual facial landmarks. The refinement process can be formulated as follows:\n$l_i = Identity(a_{c, t_{t_i}}, a_{id}, l_i)$\nWhere $a_{id}$ is the identity embedding extracted by the audio encoder $E_{id}$. $l_i$ is the finally generated landmarks with personal style that match the given audio. which will serve as a condition to guide the denoising process in the second sub-module.\nLoss fuction. We only train the Landmark Generation Network, while the two audio encoders are frozen. We optimize the Landmark Generation Network by minimizing the mean of the square difference between the predicted landmarks and the ground truth. The loss function can be formulated as follows:\n$\\mathscr{L} = \\sum_{i=1}^K \\sum_{j=1}^N ||l_{i,j} - l_{i,j}^{gt}||^2_2$\nWhere $l_{i,j}$ is the j-th point of the i-th predicted landmark and $l_{i,j}^{gt}$ is the j-th point of the i-th ground truth landmark. N is equal to 68, representing the number of all points in one facial landmark. K is the number of predicted landmarks."}, {"title": "C. Landmark-driven talking head generation", "content": "Diffusion Models. Diffusion Models (DMs) [18] work by destroying training data through the successive addition of Gaussian noise and then learning to recover the data by reversing this noising process, which corresponds to learning the reverse process of a fixed Markov Chain of length T. They can be considered as an equally weighted sequence of denoising autoencoders $e_{\\theta}(x_t, t)$. The optimization process can be defined as follows:\n$\\mathscr{L}_{DM} = \\mathbb{E}_{x,\\epsilon \\sim \\mathcal{N}(0,1),t} [||\\epsilon - \\epsilon_{\\theta}(x_t, t)||^2]$\nwhere t is sampled from {1, ...,T} and $x_t$ is a noisy version of the input x.\nLatent Diffusion Models. The denoising process of the diffusion models is operated in high-dimensional pixel space, resulting in high costs for both training and inference. To generate high-quality images while reducing training costs, Latent Diffusion Models(LDMs) emerged based on DMs. To reduce the computational complexity of the model, LDMs introduce the perceptual image compression method. It consists of an autoencoder, which includes an encoder E and a decoder D. Given an input image x, the encoder E encodes it to a latent representation $z = E(x)$, and then the decoder D reconstructs the image from latent: $\\hat{x} = D(z)$. The denoising process of LDMs is operated in this low-dimensional latent space in which high-frequency imperceptible details are abstracted away, resulting in an effective and efficient way for high-quality image synthesis. The objective of LDMs can be formulated as follows:\n$\\mathscr{L}_{LDM} = \\mathbb{E}_{E(x),\\epsilon \\sim \\mathcal{N}(0,1),t} [||\\epsilon - \\epsilon_{\\theta}(z_t, t)||^2]$\nThe denoising network $\\epsilon_{\\theta}$ is realized as a time-conditional UNet [19]. $z_t$ is obtained through the forward diffusion process from $z = E(x)$.\nConditioning Mechanisms. The Landmark-driven talking head generation (L2V), as shown in Figure.1, is based on LDMs. We use a well-trained autoencoder model [20] as the encoder E and decoder D in L2V. There are three types of conditions to guide the denoising process: the pose reference image $x_p \\in R^{H\\times W\\times 3}$, the identity reference image $x_{id} \\in R^{H\\times W\\times 3}$, and the landmark $l$. During training, the landmark l is extracted from the ground truth image x. During inference, it's derived from the landmark predicted by A2L after an affine transformation. Similar to the setup in Wav2Lip [2], $x_{id}$ is used to provide identity information, while $x_p$ is used to offer accurate pose information, enabling the model to focus on synthesizing lip shapes. Before serving as conditions, $x_{id}$ and $x_p$ are encoded into the latent space by encoder E: $z_{id} = \\mathbb{E}(x_{id}) \\in R^{h\\times w\\times 3}$, $z_p = \\mathbb{E}(x_{p}) \\in R^{h\\times w\\times 3}$, where $H/h = W/w = f$, H,W are the height and width of the original image and f is the downsampling factor. The objective of L2V can be formulated as follows:\n$\\mathscr{L} = \\mathbb{E}_{E(x),\\epsilon \\sim \\mathcal{N}(0,1),t} [||\\epsilon - \\epsilon_{\\theta}(z_t, \\mathbb{E}_{\\theta}(z_t, t, z_t, z_p, z_{id}, l)|||^2]$\nMore specifically, the input for the landmark condition comprises two categories: the visual input and the coordinate input. The subsequent ablation study can demonstrate the effectiveness of these two types of inputs. For the visual input, we convert the landmark $l$ into the image $x_l \\in R^{H\\times W\\times 3}$ and then encode it into the latent space: $z_l = \\mathbb{E}(x_l) \\in R^{h\\times w\\times 3}$. $z_l$, $z_p$, $z_{id}$, and the noise map $z_t$ are concatenated together to serve as the query for the intermediate cross-attention layers of the denoising network $\\epsilon_{\\theta}$. For the coordinate input, we use the landmark encoder $E_L$ to extract the coordinate embedding $C_l = E_L(l) \\in R^{D_l}$, which serves as the key and value in denoising network $\\epsilon_{\\theta}$."}, {"title": "III. EXPERIMENTS", "content": "Datasets.We use the audio-visual dataset HDTF [22] to train these two sub-modules A2L and L2V. It is gathered from YouTube, encompassing over three hundred distinct individuals, with a resolution of 720P or 1080P. It contains about 17 hours of talking videos. We randomly select 50 videos for training, totaling approximately 4 hours in duration, while the remaining videos are reserved for testing purposes.\nMetric. We evaluate the proposed model from three perspectives, including lip synchronization, visual quality, and temporal coherence. For lip synchronization, we use LSE-C (\u2191) and LSE-D (\u2193), which are proposed to quantify the lip-speech synchronization using a well-trained Lip-Sync expert. For visual quality, we employ PSNR (\u2191), SSIM (\u2191), LPIPS (\u2193), and FID (\u2193) to evaluate our model. For temporal coherence, tLP (\u2193) [23] and Pixel-MSE (\u2193) are employed. tLP employs the perceptual LPIPS metric to measure the visual similarity of two consecutive frames. Pixel-MSE is used to calculate the averaged mean squared pixel error between aligned consecutive frames. (\u201c\u2193\u201d indicates that the lower the better, while \"\u2191\" means that the higher the better.)\nImplementation Details. We use PyTorch to implement our framework. In A2L, the content dimension $D_1$ and the identity dimension $D_2$ are both set to 256. In L2V, the height and width of the input images are resized to 256. The downsampling factor f is set as 4, so the latent space is 64\u00d764\u00d73. The A2L module takes about 8 hours to train on one NVIDIA A100 GPU with 40GB, while the L2V module takes about 20 hours on two NVIDIA A100 GPUs. In L2V, the diffusion model is trained using 1000 diffusion steps. During the inference stage, we use 200 steps of DDIM sampling [24]."}, {"title": "A. Experimental Settings", "content": "Datasets.We use the audio-visual dataset HDTF [22] to train these two sub-modules A2L and L2V. It is gathered from YouTube, encompassing over three hundred distinct individuals, with a resolution of 720P or 1080P. It contains about 17 hours of talking videos. We randomly select 50 videos for training, totaling approximately 4 hours in duration, while the remaining videos are reserved for testing purposes.\nMetric. We evaluate the proposed model from three perspectives, including lip synchronization, visual quality, and temporal coherence. For lip synchronization, we use LSE-C (\u2191) and LSE-D (\u2193), which are proposed to quantify the lip-speech synchronization using a well-trained Lip-Sync expert. For visual quality, we employ PSNR (\u2191), SSIM (\u2191), LPIPS (\u2193), and FID (\u2193) to evaluate our model. For temporal coherence, tLP (\u2193) [23] and Pixel-MSE (\u2193) are employed. tLP employs the perceptual LPIPS metric to measure the visual similarity of two consecutive frames. Pixel-MSE is used to calculate the averaged mean squared pixel error between aligned consecutive frames. (\u201c\u2193\u201d indicates that the lower the better, while \"\u2191\" means that the higher the better.)\nImplementation Details. We use PyTorch to implement our framework. In A2L, the content dimension $D_1$ and the identity dimension $D_2$ are both set to 256. In L2V, the height and width of the input images are resized to 256. The downsampling factor f is set as 4, so the latent space is 64\u00d764\u00d73. The A2L module takes about 8 hours to train on one NVIDIA A100 GPU with 40GB, while the L2V module takes about 20 hours on two NVIDIA A100 GPUs. In L2V, the diffusion model is trained using 1000 diffusion steps. During the inference stage, we use 200 steps of DDIM sampling [24]."}, {"title": "B. Main Results", "content": "Qualitative Analysis. To validate the effectiveness of our proposed model, we compare it with several state-of-the-art methods, namely Wav2lip [2], MakeItTalk [8], Talklip [1], SadTalker [12], and Difftalk [3]. We randomly select some generated frames for visualization, which can be seen in Figure.2. It can be seen that the mouth area of the person appears blurry in frames generated by Wav2Lip. This is because re-gardless of the resolution of the input image, Wav2Lip always generates a face image of size 96\u00d796, which is then overlaid onto the original image. The overlay process can introduce artifacts or false details along the edges of the pasted area. The frames generated by Talklip exhibit similar issues to those of Wav2lip. MakeItTalk and SadTalk generate talking head videos by distorting a single image, resulting in the background being warped along with mouth movements, which compromises the authenticity of the generated videos. The frame generated by Difftalk exhibits high image quality. However, upon comparing the results in rows one and six of Figure.2, it's evident that certain frames exhibit mismatches between audio and lip movements. This discrepancy results in poor temporal coherence in the generated video. By contrast, our proposed model can generate natural talking head videos with precise lip-sync while ensuring high-quality frame generation and temporal coherence. Subsequent quantitative analysis will further support this assertion. For further video comparison results, please refer to the supplementary materials.\nQuantitative Analysis. We conduct quantitative evaluations on several widely used metrics. The results are reported in Table.I. Benefiting from the strong performance of LDMS in generating high-quality images, our method achieves the best performance across all visual quality metrics. As shown in Figure.2, the quality of frames generated by our method far surpasses that of GAN-based models such as Wav2lip and Talklip. For lip synchronization metrics, both LSE-C and LSE-D are calculated using SyncNet [25]. Since Wav2lip and Talklip utilize SyncNet as their discriminator during training, it is reasonable for these two methods to achieve the best performance in LSE-C and LSE-D. Compared to the rest of the methods, our method still maintains competitiveness in terms of lip synchronization metrics. In conclusion, our model outperforms existing state-of-the-art methods taking into account both quantitative and qualitative results."}, {"title": "C. Ablation Study", "content": "In the second stage of our model, the landmarks serve as a condition in the denoising process. The input for the landmark condition comprises two categories: the visual input and the coordinate input. In this section, we perform an ablation study on these two types of inputs, analyzing their impact on the temporal coherence of generated talking head videos. There are three different settings: (1) abandoning the visual input of landmarks (w/o visual), (2) abandoning the coordinate input of landmarks (w/o corr), (3) our full model (Full). The results are shown in Table.II.\nWithout the visual input, which serves as the main guiding information in the denoising process, the model exhibits poor performance on both temporal metrics. The coordinate input of landmarks serves as auxiliary guiding information to optimize the lip shapes, ensuring better temporal coherence. After removing the coordinate input, the temporal metrics of the model show a slightly inferior performance compared to the full model. Different from our model that utilizes landmarks as the intermediate representation, Difftalk directly employs the given speech as guiding information in the denoising process. Our model significantly outperforms Difftalk in terms of temporal metrics. We introduce facial landmarks as the intermediate representation with the aim of having them serve as more robust guiding information than speech to mitigate the diversity in generated frames, thereby optimizing the temporal consistency in talking head videos. The ablation study demonstrates the rationality of our proposed model. The supplementary materials will offer additional video results for visualization purposes."}, {"title": "IV. CONCLUSION", "content": "In this paper, We introduce a novel two-stage diffusion-based model that utilizes facial landmarks as an intermediate representation for high-quality, well-synchronized, and temporally coherent talking head generation. The landmarks generated in the first stage will serve as a condition in the second stage. By introducing landmarks as robust guiding information in the denoising process, our method significantly alleviates the jitter issues in diffusion-based models. Extensive experiments demonstrate the rationality and effectiveness of our proposed model."}]}