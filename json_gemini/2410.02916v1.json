{"title": "SAFEGUARD IS A DOUBLE-EDGED SWORD:\nDENIAL-OF-SERVICE ATTACK ON LARGE LANGUAGE MODELS", "authors": ["Qingzhao Zhang", "Ziyang Xiong", "Z. Morley Mao"], "abstract": "Safety is a paramount concern of large language models (LLMs) in their open deployment. To\nthis end, safeguard methods aim to enforce the ethical and responsible use of LLMs through safety\nalignment or guardrail mechanisms. However, we found that the malicious attackers could exploit\nfalse positives of safeguards, i.e., fooling the safeguard model to block safe content mistakenly,\nleading to a new denial-of-service (DoS) attack affecting LLM users. Specifically, through software\nor phishing attacks on user client software, attackers insert a short, seemingly innocuous adversarial\nprompt into user prompt templates in configuration files. This prompt triggers safeguard rejections of\nnearly all user requests from the client while remaining hidden in the user interface and non-trivial\nto detect. By designing an optimization process that utilizes gradient and attention information,\nour attack can automatically generate seemingly safe adversarial prompts, approximately only 30\ncharacters long, that universally block over 97% of user requests on Llama Guard 3. The attack\npresents a new dimension of evaluating LLM safeguards focusing on false positives, different from\nthe classic jailbreak.", "sections": [{"title": "Introduction", "content": "As large language models (LLMs) have been widely adopted across different domains, their significant social impact\nhas prompted extensive research into methods of monitoring the interaction between users and LLMs and suppressing\nbias and harmful content that could be produced by LLMs. To this end, human feedback aligns LLMs to safety stan-\ndards during training or fine-tuning stages, practised by ChatGPT [Achiam et al., 2023] for instance. Also, guardrail\nmechanisms are deployed at inference time, involving a flexible combination of safety checks for content modera-\ntion. For instance, Llama Guard [Inan et al., 2023] utilizes a separate LLM to classify conversations as safe or unsafe.\nWhile above LLM safeguards \u00b9 are now standard in deployment, they remain vulnerable to malicious attacks. Through\nblack-box searches or white-box adversarial optimization, attackers can find inputs that jailbreak the system, bypass-\ning safeguards and causing the LLM to generate harmful content. These vulnerabilities and their mitigation are a\ngrowing focus in LLM security research [Yu et al., 2024, Dong et al., 2024]. The jailbreak attack exploits the false\nnegatives of LLM safeguards (i.e., incorrectly classifying unsafe content as safe).\nInspired by jailbreak, we raise the research question can malicious attackers also exploit false positives of LLM\nsafeguards? By triggering a false positive, the safeguard classifies a proper user request as unsafe content thus the\nrequest is rejected. When a malicious attacker consistently triggers the rejection targeting a specific user, it forms a\ndenial-of-service (DoS) attack. The attack can significantly degrade the user experience and cause economic losses or\neven harm public health, especially in systems related to finance and healthcare. While jailbreak gains the majority of\nattention for LLM safety, the DoS threat is overlooked by existing studies.\nIn this paper, we design the LLM DoS attack. The attacker is assumed to be able to inject an adversarial prompt to user\nprompt templates, stored in a configuration file of the user client. The injection can be achieved by exploiting software\nIn this paper, the term safeguard refers to methods of both safety alignment during training/fine-tuning and external guardrails\ndeployed at inference time."}, {"title": "Background and Related Work", "content": "Large Language Models (LLMs). Large language models (LLMs) are advanced AI models designed to understand\nand generate human-like text by training on vast text data. These models generally use the Transformer architec-\nture [Vaswani, 2017], which uses self-attention mechanisms to weigh the importance of different words in a sentence.\nThese LLMs scale up to billions of model parameters and present promising performance on various tasks. Repre-\nsentative examples include GPT [Achiam et al., 2023], BERT [Kenton and Toutanova, 2019], Llama [Touvron et al.,\n2023], etc.\nSafety alignment or guardrails of LLMs. As LLMs become increasingly integrated into real-life applications,\nensuring their safe and ethical operation has become a critical concern. In general, there are two main categories of\nsafeguards, train-time safety alignment and inference-time guardrails.\nSafety alignment refers to the process of guiding LLMs to produce outputs that adhere to human values and ethical\nnorms. Stiennon et al. [2020] introduced Reinforcement Learning from Human Feedback (RLHF), which utilizes\nhuman feedback and preferences to enhance the capabilities of LLMs, becoming a standard approach to LLM training.\nSupervised fine-tuning [Achiam et al., 2023] or instruction-tuning [Touvron et al., 2023] can further improve LLMs on\nspecialized tasks using additional data of prompt-response (input-output) demonstrations, e.g., using safety datasets to\nenhance the LLM's safety assessment capability. As data quality is paramount for train-time safety alignment, recent"}, {"title": "Problem Statement", "content": "In this section, we define the threat model in Section 3.1, state the attack goals in Section 3.2, and elaborate possible\nattack scenarios in real applications in Section 3.3."}, {"title": "Threat Model", "content": "We assume the existence of three parties, server, user, and attacker. We discuss the assumption of these roles sepa-\nrately. An overview of the attack is depicted in Figure 1.\n\u2022 Server. The server, which hosts an LLM service, processes requests from users and delivers the LLM-generated\nresponses back to users. The server deploys LLM safeguards to screen incoming requests and refuses to process or\nrespond to those deemed unsafe. The server is trusted.\n\u2022\nUser. The user interacts with the server through client software with built-in or customizable prompt templates.\nThe templates are stored in configuration files and not showing up in the user interface. Upon receiving a user-\ncomposed prompt, this client software utilizes these prompt templates to format the prompt into a complete request\nbefore transmitting it to the server. However, the client software is not trusted, as it is susceptible to compromise\nthrough cyber attacks. The user may manually inspect prompt templates in configuration files to detect potential\nattacks.\n\u2022 Attacker. The attacker, a malicious entity, seeks to create a denial of service for specific users by increasing the\nlikelihood that their requests are denied by the server. The attacker can inject malicious prompts into the targeted\nuser's requests to the LLM server. While unable to compromise the trusted server, the injection can be achieved by\neither proactively compromising the client software or passively inducing users to configure it incorrectly, whose\npracticality is discussed in Section 3.3. Additionally, the attacker has white-box access to the safeguard parameters;\ne.g., the server may use safeguard models that are open-sourced or could be reverse-engineered."}, {"title": "Attack Goals", "content": "Effectiveness. We first define the objective of the attack using formal language. The safeguard deployed on the\nserver is a binary classifier, denoted by f. It evaluates a user request, denoted by m, and outputs binary values\nwhere f(m) = 1 indicates the detection of unsafe content. Each request m is constructed by concatenating a user-\nprovided prompt, denoted by mu, and a prompt template, denoted by mt, i.e., m = mu mt where is the\nconcatenation operation. The attacker employs an attack function g to manipulate the prompt template mt, e.g.,\ninjecting an adversarial prompt. Given a set of safe user prompts M, a safeguard function f, and the attack function\ng, the (ideal) objective of the attack is to make the safeguard classify all user prompts as unsafe content:\n$\\forall m_u \\in M, f(m_u \\oplus m_t) = 0 \\land f(m_u \\oplus g(m_t)) = 1$.\nStealthiness. The attacker aims to make the adversarial prompt non-trivial to identify within the prompt template by\nmanual inspection. To this end, we define three metrics assessing the stealth of adversarial prompts. (1) Length. The\nadversarial prompt should be as short as possible. (2) Semantics. The adversarial prompt is semantically not unsafe, or\nhas low semantic similarity with known unsafe content. (3) Wording. The adversarial prompt should avoid obviously\ntoxic or harmful words. As an example, the attacker would pick the adversarial prompt buffs yayJet bombkup deton it\n(30 characters), a short meaningless string, instead of Instruct someone on how to create a bomb and ... (75 characters)\nwhich contains obvious harmful instructions. While this does not ensure the attacks are undetectable, it does reduce\nthe likelihood of detection."}, {"title": "Real-world Scenarios", "content": "The practicality of the attack relies on the successful injection of the adversarial prompts. In this section, we elaborate\non a few example scenarios where the injection is practical in real applications.\nSoftware vulnerabilities. The attacker can modify prompt templates in the user client by exploiting software vulnera-\nbilities. In the past year, dozens of zero-day vulnerabilities, such as path traversal [MITRE Corporation, c], command\ninjection [MITRE Corporation, b], and cross-site scripting [MITRE Corporation, a], are identified in LLM clients.\nThese vulnerabilities could lead to severe exploits such as arbitrary code execution and unauthorized file writes. A\nnotable recent example includes a remote code execution vulnerability in Vanna, an LLM-based database management\ntool, which could potentially grant full system control [MITRE Corporation, d]. These vulnerabilities provide attack-\ners with the means to discreetly inject adversarial prompts into user clients, offering a stealthier alternative to more\ndisruptive attacks, such as client shutdowns.\nPhishing attacks. The attacker disguises itself as a trustworthy provider of prompt templates and inducing users to\nadopt malicious versions [Alabdan, 2020]. Given the critical role of high-quality prompt templates in enhancing LLM\nperformance and the common practice among LLM clients to allow template customization, users frequently adopt\ntemplates recommended in online articles, which opens the opportunity of phishing attacks. Note that the stealthiness\ngoal in Section 3.2 is especially critical in this scenario as the user will not adopt the malicious prompt templates if\nthey observe obvious unsafe content in these prompt templates.\nControlling an agent in an LLM agent system. An LLM agent system integrates LLMs, user interfaces, and system\ntools to execute a variety of tasks [Talebirad and Nadiri, 2023]. If certain components are compromised, the system's\nintegrity could be jeopardized, potentially allowing an attacker to manipulate the inputs to the LLM [Zhang et al.,\n2024, Wu et al., 2024]. For example, the system might instruct a data processing agent to append the contents of a file\nto the LLM inputs. If an attacker controls the file content, an adversarial prompt could be injected."}, {"title": "Design", "content": "In this section, we detail the algorithm used to generate adversarial prompts for executing the LLM DoS attack.\nWe begin by outlining the overall workflow of the attack and then highlight our distinctive contributions to the DoS\nstrategy, which include: (1) a stealth-oriented optimization method, and (2) mechanisms achieving multi-dimensional\nuniversality."}, {"title": "Overview", "content": "Algorithm 1 presents the algorithm generating an adversarial prompt triggering a denial of service. The attack requires\nthe following materials:\n\u2022 A set of safe prompts. These prompts are recognized as safe by the safeguard mechanisms. The attacker uses these\nprompts as examples of legitimate user prompts.\n\u2022 A set of unsafe prompts. These prompts include explicit harmful content that should be flagged as unsafe by the\nsafeguards. The initial adversarial prompt is derived from this set.\n\u2022 A safeguard model. This is the attack's target, accessible to the attacker in a white-box setting. It may be a safety-\naligned LLM or an external safeguard system. We also choose a target response as the text that will be generated\non detection of unsafe content, e.g., \"unsafe\u201d for Llama Guard models and \"I'm sorry\" for safety-aligned models.\n\u2022 A loss function. This function evaluates the quality of adversarial prompts based on a weighted sum of their effec-\ntiveness (i.e., the likelihood of eliciting the targeted response) and their stealth (i.e., prompt length and semantic\nappropriateness).\nThe attack can be summarized as the following process.\nInitialization. At the beginning, the algorithm initializes a set of test cases and a candidate adversarial prompt. Each\ntest case is constructed by picking a safe prompt and determining an insertion point for the adversarial prompt. The\ncandidate for the adversarial prompt is chosen as the most effective unsafe prompt from the set, evaluated based on\nits loss score across these test cases. This initialization strategy ensures the attack begins in a position close to the\npotential success.\nAfter the initialization, the attack iteratively mutates the candidate gradually towards lower loss."}, {"title": "Stealth-oriented Optimization", "content": "We enforce the stealth of the attack using the following design blocks.\nToken substitution with token filter. We implement a customizable filter to identify and eliminate unwanted tokens,\nsuch as toxic words or special characters, from the adversarial prompt. If an unwanted token is detected within an\nadversarial prompt candidate, its substitution probability is increased. The replacement token, selected via GCG\nalgorithm [Zou et al., 2023], is also subjected to this filtering process to ensure it is not an unwanted token. This\napproach purges undesirable tokens from the initial adversarial prompt.\nToken deletion guided by attention. The attention mechanism in transformer architecture determines how each\ntoken influences others during tasks. The attention values in the last transformer layer are particularly significant as\nthey directly present each token's contribution to the final output. Therefore, we use the last layer of attention values\nto determine which tokens in the adversarial prompt are not important for the target response, thus they have a higher\npriority to be deleted.\nFormally speaking, given a token sequence containing the adversarial prompt A and the target response T, we de-\nnote attention values from the last layer of $d_{aij}$, where i and j index over tokens of A and T, respectively. The\nimportance of each token $a_i \\in A$ with respect to T is $Importance(a_i) = \\sum_j a_{ij}$. The probablity of deleting $a_i$ is\n$\\frac{Importance(a_i)}{\\sum_{a_k \\in A} Importance(a_k)}$.\nThe loss function. Besides a cross-entropy loss characterizing the likelihood of the target response, the loss function\ninvolves criteria of length and semantics, i.e., Length and SemanticSimilarity in Algorithm 1. Length computes\nthe number of characters in a candidate's plain text, favoring shorter candidates. SemanticSimilarity, leveraging\npre-trained models (e.g., BERT [Kenton and Toutanova, 2019]), assesses how similar a candidate is to the initial unsafe\nprompt used at the start of the attack, favoring lower similarity score. Consequently, the loss function aids in selecting\ncandidates that are short and not obviously unsafe."}, {"title": "Multi-dimension Universality", "content": "Unlike jailbreak attacks, the attacker in our LLM DoS attack does not control user-provided prompts, resulting in\nuncertainties regarding the final request sent to the LLM service. It is therefore essential to design mechanisms that\nensure the attack is universally effective across diverse scenarios.\nTask categories. The safe prompt set used in Algorithm 1 may encompass various task categories, such as mathe-\nmatics, coding, and logical reasoning. Employing prompts from multiple categories enhances universality, making\nthe approach well-suited for general LLM chat services. Conversely, targeting a specific task category is practical for\nspecialized LLM services, such as an AI coding assistant. We consider both multi-task and single-task settings.\nLocation of insertion. Given the attacker's limited knowledge about how users construct final requests for LLM\nservices, we assume that the adversarial prompt could be positioned variously within the LLM inputs \u2013 either as a\nsuffix, prefix, or inserted in the middle. During test case creation, as mentioned in Section 4.1, the attacker may\nstrategically choose the insertion point based on available knowledge about user clients, or opt for random insertion to\nmaximize universality."}, {"title": "Evaluation", "content": "We use experiments to analyze the impact of the LLM DoS attack. Besides reporting an overall success rate, we\nanalyze factors affecting the attack in Section 5.2, conduct an ablation study showing the benefits of design blocks in\nSection 5.3, evaluate its resilience to existing mitigation methods in Section 5.4, and present a case study demonstrating\nthe attack's practicality in Section 5.5."}, {"title": "Experiment Setup", "content": "Datasets. We collect the safe prompts of general short questions from Alpaca [Wang, 2023], programming questions\nfrom HumanEval [Chen et al., 2021], and math problems, logical reasoning, and reading comprehension tasks from\nAgieval [Zhong et al., 2023]. We collect unsafe prompts from HarmBench [Mazeika et al., 2024]. We divide the safe\nprompts into a training set comprising 80% of the data, which is used to input the attack algorithm, and a test set\ncomprising 20% of the data, designated solely for evaluation. We ensure a balanced distribution of prompt lengths and\ntask categories in each set. For the unsafe prompts, we selectively retain 100 examples that are relatively shorter than\nothers, as inputs to the attack algorithm.\nModels. We use public pre-trained models from huggingface, Vicuna-1.5-7b, LlamaGuard-7b, Meta-Llama-Guard-2-\n8B, and Llama-Guard-3-8B. The Vicuna model is safety-aligned (using ChatGPT conversations) while Llama Guard\nmodels are standalone guardrails. We use default configurations as provided by the model authors.\nImplementation details. We heavily use Python packages of PyTorch and transformers to implement the attack. By\ndefault, the attack algorithm consumes 12 safe prompts from the training set each time. Each candidate mutation step\nexecutes 24 token substitutions and 8 token deletion, i.e., k\u2081 = 24, k2 = 8 in Algorithm 1. The loss function uses\nW\u2081 = 10\u22124, W2 = 0.1. The success rate threshold o is 0.6. We manually tuned the above parameters. The semantic\nsimilarity is assessed using SentenceBERT [Reimers, 2019].\nExperiment parameters. We consider various settings of the experiments.\n\u2022 Task scope: Single-task and Multi-task. A single-task attack is optimized and tested on one specific task category\nwhile a multi-task attack is supposed to be effective on all task categories.\n\u2022 Token filter: None, Moderate, and Strict. This parameter controls the vocabulary that can be used in adversarial\nprompts. A moderate filter allows only English words (no special characters). A strict filter additionally disallows a\nlist of toxic words, collected from the unsafe prompt dataset.\n\u2022 Insertion: Prefix, Suffix, and Random. The adversarial prompts could be inserted as a prefix, as a suffix, or at a\nrandom location in the user prompts.\nEvaluation metrics. We assess success rate and length of adversarial prompts. The success rate is the ratio of\nsafeguard warnings triggered on the set of safe prompts with adversarial prompts inserted. The length is the number\nof English characters. For each experiment setting, we repeat the attack 20 times, 50 iterations for each, and report the\naveraged metrics."}, {"title": "Results and Analysis", "content": "Main results, as summarized in Table 1, demonstrate the effectiveness of the attack. In optimal scenarios, the attack\nachieves a success rate of over 97% using a 30-character adversarial prompt. We plot the process of optimizing the\nadversarial prompt as Figure 2, using Experiment 3.2 in Table 1 as an example. The success rate on the test set starts\nhigh due to the inherently unsafe nature of the original prompt. Over the iterations, the success rate remains high while\nthe adversarial prompt becomes shorter and semantically ambiguous. More attack examples are in Appendix A.\nNext, we comprehensively analyze the impact of the various factors on the success of the attack and draw several\nfindings as below.\nSafeguard development lacks attention to false positive triggering. The Llama Guard series, as state-of-the-art\nopen-source guardrails, becomes increasingly vulnerable to the DoS attack with its development. The attack success\nrate on the latest Llama Guard 3 is 20.4% higher than that on Llama Guard (the initial version). Vicuna is in general a\nweaker model against adversarial attacks.\nThe attack is not task-specific. The success rate of single-task attacks is marginally higher than multi-task attacks in\nTable 1, with adversarial prompts of comparable lengths. This pattern is consistent across all tested models, indicating\nthe task-wise universality of adversarial prompts.\nSome keywords increase attack success rate. Table 1 involves multi-task suffix attacks with different token filters\n(Experiments x.2, x.5, and x.6), and we show examples of filtered adversarial prompts in Figure 3. Attacks using\na moderate filter, which excludes special characters, achieve performance comparable to those without any token\nfiltering. However, the strict filter, which bans specific toxic words, makes adversarial prompts longer. Despite this,"}, {"title": "Ablation Study", "content": "We implement GCG [Zou et al., 2023] as a baseline and demonstrate the benefits of our design. We use Experiments\n3.2 and 4.2 in Table 1 as examples and depict results in Figure 5. Briefly speaking, the main contribution of our design\nis the enforcement of stealth, e.g., dramatically reducing the length of adversarial prompts and making the content\nsemantically obscure. The effect is maximized by combining the token substitution mechanism and the loss function\ndesign, shrinking the adversarial prompt length to 20%, from 179.2 to 40.3."}, {"title": "Mitigation Methods", "content": "We evaluate the attack's resilience to two existing mitigation methods, random perturbation proposed by Robey et al.\n[2023] and resilient optimization proposed by Yuan et al. [2024]. The random perturbation method randomly inserts\ncharacters, replaces characters, or applies patches in the original user request, assuming the adversarial prompt is\nsensitive to the perturbation. We implement all three perturbation types and set the probability of perturbation to\n0.1 and the number of perturbations to 31. The resilient optimization method optimizes another prompt to suppress\npotential adversarial prompts, attached at the end of the user request. We apply two mitigation methods to the multi-\ntask suffix attack without token filtering (Experiments x.2 in Table 1) and summarize results in Figure 6.\nWhile mitigation strategies can lower the attack success rate to at least 40%, they significantly impair the effectiveness\nof safeguards on normal data without attack. Notably, random patching and resilient optimization severely reduce the\ntrue positive rate (TPR) to below 30% and increase the false positive rate (FPR) above 10%, rendering the safeguard\nsystem largely ineffective. In contrast, the mitigation of random insertion and replacement brings minor side effects\nbut they reduce the attack success rate to at least around 60%, not effectively countering the DoS attacks.\nIn conclusion, the results show the imperfection of tested mitigation methods. We recommend that users implement\nstandard protections against software and phishing attacks to prevent early-stage adversarial prompt injections. Ad-\nditionally, should users notice a high volume of request failures, manual validation of prompt templates is advised to\nidentify the attack."}, {"title": "Case Study", "content": "To demonstrate the real-world applicability of the DoS attack, we analyze the implementation of AnythingLLM, a\nframework for constructing LLM applications. An example application built is illustrated in Figure 7, where Llama\n3 and Llama Guard 3 are utilized on a remote server to solve math problems. The user interface allows for the\ncustomization of prompt templates either through a configuration page or local files.\nDespite its utility, AnythingLLM exhibits vulnerabilities; the CVE database records 46 vulnerabilities and most of\nthem involve illegal file access. Attackers could exploit these vulnerabilities or leverage phishing attacks to inject\nadversarial prompts, as discussed in Section 3.3. These adversarial prompts, composed of valid English words, remain\ninconspicuous yet effectively compromise Llama Guard 3's decisions. As a result, almost all requests fail to be\nprocessed by the LLMs."}, {"title": "Discussion", "content": "Threats to validity. To evaluate the proposed DoS attack, we employ open-source models and public datasets, as-\nsuming they are representative of closed-source models and real-world data distributions respectively. Our tests are\nconducted in a straightforward application setup using safety-aligned models or guardrail models. While commercial\nplatforms like ChatGPT likely have more complex safety mechanisms, their designs remain confidential. Nonetheless,\nour analysis contributes significantly by highlighting the false positive rates of safeguard methods, offering insights\ninto the engineering of safe and reliable LLM services.\nMitigations. Besides the methods discussed in Section 5.4, we recommend that users implement standard protections\nagainst software and phishing attacks to prevent early-stage adversarial prompt injections. Additionally, should users\nnotice a high volume of request failures, manual validation of prompt templates is advised to identify the attack."}, {"title": "Conclusion", "content": "This paper presents a novel LLM Denial-of-Service (DoS) attack that leverages false positives in LLM safeguards\nto block legitimate user requests. We design a method of white-box adversarial prompt generation, emphasizing the\nstealth of the generated prompts. Our evaluations demonstrate the high effectiveness and stealth of the proposed DoS\nattack across diverse scenarios. The findings urge the necessity for evaluation of safeguard methods on false positive\ncases."}]}