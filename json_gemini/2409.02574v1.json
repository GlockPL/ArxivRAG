{"title": "SOLVING VIDEO INVERSE PROBLEMS USING IMAGE DIFFUSION MODELS", "authors": ["Taesung Kwon", "Jong Chul Ye"], "abstract": "Recently, diffusion model-based inverse problem solvers (DIS) have emerged as state-of-the-art approaches for addressing inverse problems, including image super-resolution, deblurring, inpainting, etc. However, their application to video inverse problems arising from spatio-temporal degradation remains largely un-explored due to the challenges in training video diffusion models. To address this issue, here we introduce an innovative video inverse solver that leverages only image diffusion models. Specifically, by drawing inspiration from the success of the recent decomposed diffusion sampler (DDS), our method treats the time dimension of a video as the batch dimension of image diffusion models and solves spatio-temporal optimization problems within denoised spatio-temporal batches derived from each image diffusion model. Moreover, we introduce a batch-consistent diffusion sampling strategy that encourages consistency across batches by synchronizing the stochastic noise components in image diffusion models. Our approach synergistically combines batch-consistent sampling with simultaneous optimization of denoised spatio-temporal batches at each reverse diffusion step, resulting in a novel and efficient diffusion sampling strategy for video inverse problems. Experimental results demonstrate that our method effectively addresses various spatio-temporal degradations in", "sections": [{"title": "1 INTRODUCTION", "content": "Diffusion models (Ho et al., 2020; Song et al., 2020) represent the state-of-the-art generative modeling by learning the underlying data distribution p(x) to produce realistic and coherent data samples from the learned distribution \\(p_{\\theta}(x)\\). In the context of Bayesian inference, the parameterized prior distribution \\(p_{\\theta}(x)\\) can be disentangled from the likelihood \\(p(y|x)\\), which denotes the probability of observing y given x. This seperation facilitates the derivation of the posterior distribution \\(p_{\\theta}(x|y) \\propto p_{\\theta}(x)p(y|x)\\).\nDiffusion model-based inverse problem solvers (DIS) (Kawar et al., 2022; Chung et al., 2022a; Song et al., 2023; Wang et al., 2023; Chung et al., 2024) leverage this property, enabling the unconditional diffusion models to solve a wide range of inverse problems. They achieve this by conditional sampling from the posterior distribution \\(p_{\\theta}(x|y)\\), effectively integrating information from both the forward physics model and the measurement y. This approach allows for sophisticated and precise solutions to complex inverse problems, introducing the power and flexibility of diffusion models in practical applications.\nDespite extensive DIS research on a wide range of image inverse problems such as super-resolution, colorization, inpainting, compressed sensing, deblurring, and so on (Jalal et al., 2021; Kawar et al., 2022; Chung et al., 2022a; Song et al., 2023; Wang et al., 2023; Chung et al., 2024), the application of these approaches to video inverse problems, particularly those involving spatio-temporal degradation, has received relatively less attention. Specifically, in time-varying data acquisition systems, various forms of motion blur often arise due to the camera or object motions (Potmesil & Chakravarty, 1983), which can be modeled as a temporal PSF convolution of motion dynamics. These are often associated with spatial degradation caused by noise, camera defocus, and other factors. Specifically, the spatio-temporal degradation process can be formulated as:\n\\(Y = A(X) + W\\)                                                                                                                                      (1)\nwith\n\\(X = [x[1]   ...   x[N]], Y = [y[1]   ...   y[N]], W = [w[1]   ...   w[N]],\\)                                                 (2)\nwhere x[n], y[n] and w[n] denote the n-th frame ground-truth image, measurement, and additive noise, respectively; N is the number of temporal frames, and A refers to the operator that describes the spatio-temporal degradation process. The spatio-temporal degradation introduces complexities that image diffusion priors cannot fully capture, as image diffusion priors are primarily designed to handle spatial features rather than temporal dynamics. Employing video diffusion models (Ho et al., 2022) could address these issues, but poses significant implementation challenges for video inverse problems, due to the difficulty of training video diffusion models for various applications.\nContrary to the common belief that a pre-trained video diffusion model is necessary for solving video inverse problems, here we propose a radically different method that addresses video inverse problems using only image diffusion models. Inspired by the success of the decomposed diffusion sampler (DDS) (Chung et al., 2024), which simplifies DIS by formulating it as a Krylov subspace-based optimization problem for denoised images via Tweedie's formula at each reverse sampling step, we treat the time dimension of a video as the batch dimension of image diffusion models and solve spatio-temporal optimization problems using the batch of denoised temporal frames from image diffusion models. However, treating each frame of the video as a separate sample in the batch dimension can lead to inconsistencies between temporal frames. To mitigate this, we introduce the batch-consistent sampling strategy that controls the stochastic directional component (e.g., initial noise or additive noise) of each image diffusion model during the reverse sampling process, en-couraging the temporal consistency along the batch dimension. By synergistically combining batch-consistent sampling with the simultaneous optimization of the spatio-temporal denoised batch, our approach effectively addresses a range of spatio-temporal inverse problems, including spatial de-blurring, super-resolution, and inpainting. Our contribution can be summarized as follows."}, {"title": "2 BACKGROUND", "content": "Diffusion models. Diffusion models (Ho et al., 2020) attempt to model the data distribution \\(P_{\\text{data}}(x)\\) based on a latent variable model\n\\(P_{\\theta}(x_{0}) = \\int P_{\\theta}(x_{0:T}) dx_{1:T}, \\text{ where } P_{\\theta}(x_{0:T}) := P_{\\theta}(x_{T}) \\prod_{t=1}^{T} P_{\\theta}(x_{t-1} | x_{t}) \\)                                   (3)\nwhere the \\(x_{1:T}\\) are noisy latent variables defined by the Markov chain with Gaussian transitions\n\\(q(x_{t}|x_{t-1}) = N(x_{t}| \\sqrt{1 - \\beta_{t}}x_{t-1}, \\beta_{t}I),  q(x_{t}|x_{0}) = N(x_{t}| \\sqrt{\\bar{\\alpha}_{t}}x_{0}, (1 - \\bar{\\alpha}_{t})I). \\)  (4)\nHere, the noise schedule \\(\\beta_{t}\\) is an increasing sequence of t, with \\(\\bar{\\alpha}_{t} := \\prod_{i=1}^{t} \\alpha_{i}, \\alpha_{i} := 1 - \\beta_{i}\\). Training of diffusion models amounts to training a multi-noise level residual denoiser:\n\\(\\min_{\\theta} E_{x_{t} \\sim q(x_{t} | x_{0}), x_{0} \\sim P_{\\text{data}}(x_{0}), \\epsilon \\sim N(0, I)} [||\\epsilon_{\\theta}(x_{t}) - \\epsilon||^{2}].\\) (5)\nThen, sampling from (3) can be implemented by ancestral sampling, which iteratively performs\n\\(x_{t-1} = \\frac{1}{\\sqrt{\\alpha_{t}}}(x_{t} - \\frac{1-\\alpha_{t}}{\\sqrt{1-\\bar{\\alpha}_{t}}}\\epsilon_{\\theta^{*}}(x_{t})) + \\beta_{t} \\epsilon\\)                                                                                                 (6)\nwhere \\(\\beta_{t} := \\frac{1 - \\alpha_{t-1}}{\\alpha_{t}}\\beta_{t}\\) and \\(\\theta^{*}\\) refers to the optimized parameter from Eq. (5). On the other hand, DDIM (Song et al., 2021) accelerates the sampling based on non-Markovian assumption. Specifically, the sampling iteratively performs\n\\(x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}} \\hat{x}_{t} + \\sqrt{1 - \\bar{\\alpha}_{t-1}} \\hat{\\epsilon}_{t}\\)                                                                                                                                     (7)\nwhere\n\\(\\hat{x}_{t} := \\frac{1}{\\sqrt{\\alpha_{t}}} (x_{t} - \\sqrt{1-\\bar{\\alpha}_{t}} \\epsilon_{\\theta^{*}}(x_{t})), \\hat{\\epsilon}_{t} := \\frac{\\sqrt{1-\\bar{\\alpha}_{t-1}} - \\eta^{2} \\beta_{t}^{2} \\epsilon_{\\theta^{*}}(x_{t}) + \\eta \\beta_{t} \\epsilon}{\\sqrt{1-\\bar{\\alpha}_{t}}} \\)                                            (8)\nHere, \\(\\hat{x}_{t}\\) is the denoised estimate of \\(x_t\\) that is derived from Tweedie's formula (Efron, 2011). Accordingly, DDIM sampling can be expressed as a two-step manifold transition: (i) the noisy sample \\(x_{t} \\in M_{t}\\) transits to clean manifold \\(M\\) by deterministic estimation using Tweedie's formula, (ii) a subsequent transition from clean manifold to next noisy manifold \\(M_{t-1}\\) occurs by adding noise \\(\\hat{\\epsilon}_{t}\\), which is composed of the deterministic noise \\(\\epsilon_{\\theta^{*}}(x_{t})\\) and the stochastic noise \\(\\epsilon\\).\nDiffusion model-based inverse problem solvers. For a given loss function l(x) which often stems from the likelihood for measurement consistency, the goal of DIS is to address the following optimization problem\n\\(\\min_{x \\in M} l(x)\\)                                                                                                                                                           (9)\nwhere M represents the clean data manifold sampled from unconditional distribution \\(p_{\\theta}(x)\\). Consequently, it is essential to find a way that minimizes cost while also identifying the correct manifold.\nRecently, Chung et al. (2023a) proposed a general technique called diffusion posterior sampling (DPS), where the updated estimate from the noisy sample \\(x_{t} \\in M_{t}\\) is constrained to stay on the same noisy manifold \\(M_{t}\\). This is achieved by computing the manifold constrained gradient (MCG) (Chung et al., 2022b) on a noisy sample \\(x_{t} \\in M_{t}\\). The resulting algorithm can be stated as follows:\n\\(x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}} (x_{t} - \\gamma_{t} \\nabla_{x_{t}} l(x_{t})) + \\sqrt{1 - \\bar{\\alpha}_{t-1}} \\hat{\\epsilon}_{t},\\)                                                              (10)"}, {"title": "3 VIDEO INVERSE SOLVER USING IMAGE DIFFUSION MODELS", "content": "3.1 PROBLEM FORMULATION\nUsing the forward model Eq. (1) and the optimization framework in Eq. (9), the video inverse problem can be formulated as\n\\(\\min_{X \\in M} l(X) := ||Y - A(X)||^{2}\\)                                                                                                                                             (12)\nwhere X denotes the spatio-temporal volume of the clean image composed of N temporal frames as defined in Eq. (2), and M represents the clean video manifold sampled from unconditional distribution \\(p_{\\theta}(X)\\). Then, a naive application of the one-step gradient within the DDS framework can be formulated by\n\\(X_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}} (X_{t} - \\gamma_{t} \\nabla_{X_{t}} l(X_{t})) + \\sqrt{1 - \\bar{\\alpha}_{t-1}} \\mathcal{E}_{t}.\\)                                                                                (13)\nwhere \\(X_t\\) and \\(\\mathcal{E}_{t}\\) refer to Tweedie's formula and noise in the spatio-temporal volume, respectively, which are defined by\n\\(\\mathcal{X}_{t} := \\frac{1}{\\sqrt{\\alpha_{t}}} (X_{t} - \\sqrt{1 - \\bar{\\alpha}_{t}} \\epsilon_{\\theta^{*}}(X_{t})), \\mathcal{E}_{t} := \\frac{\\sqrt{1-\\bar{\\alpha}_{t-1}} - \\eta^{2} \\beta_{t}^{2} \\epsilon_{\\theta^{*}}(X_{t}) + \\eta \\beta_{t} \\epsilon}{\\sqrt{1-\\bar{\\alpha}_{t}}}.\\)                                      (14)\nHere, \\(X_t\\) refers to the spatio-temporal volume at the t-th reverse diffusion step and \\(\\mathcal{E} \\sim \\prod_{i=1}^{N} N(0, I)\\). Although the formula Eq. (14) is a direct extension of the image-domain counterpart Eq. (8), the main technical challenge lies in training the video diffusion model \\(\\epsilon_{\\theta^{*}}^{(t)}\\), which is required for the formula Eq. (14). Specifically, the video diffusion model is trained by\n\\(\\min_{\\theta} E_{X_{t} \\sim q(X_{t} | X_{0}), X_{0} \\sim P_{\\text{data}}(X_{0}), \\epsilon \\sim \\prod_{i=1}^{N} N(0, I)} [||\\epsilon_{\\theta^{*}}^{(t)} (X_{t}) - \\epsilon||^{2}],\\)                                                      (15)\nwhich requires large-scale video training data and computational resources beyond the scale of train-ing image diffusion models. Therefore, the main research motivation is to propose an innovative method that can bypass the need for computationally extensive video diffusion models.\n3.2 BATCH-CONSISTENT RECONSTRUCTION WITH DDS\nConsider a batch of 2D diffusion models along the temporal direction:\n\\(\\epsilon_{\\theta^{*}}^{(t)}(X_{t}) := [\\epsilon_{\\theta^{*}}^{(t)}(X_{t}[1])   ...   \\epsilon_{\\theta^{*}}^{(t)}(X_{t}[N])]\\)                                                                                                              (16)\nwhere \\(\\epsilon_{\\theta^{*}}^{(t)}\\) represents an image diffusion model. Suppose that \\(\\epsilon_{\\theta^{*}}^{(t)}(X_{t})\\) is used for Eq. (14). Since unconditional reverse diffusion is entirely determined by Eq. (14), the generated video is then fully"}, {"title": "4 EXPERIMENTS", "content": "In this section, we conduct thorough comparison studies to demonstrate the efficacy of the proposed method in addressing spatio-temporal degradations. Specifically, we consider two types of loss functions for video inverse problems:\n\\(l(X) := ||Y - A(X)||^{2}, l_{tv}(X) := ||Y - A(X)||^{2} + \\lambda TV(X)\\)                                                                       (21)\nwhere the first loss is from Eq. (12) and TV(X) denotes the total variation loss along the temporal direction.\nThen, classical optimization methods are used as the baselines for comparison to minimize each loss function. Specifically, the stand-alone Conjugate Gradient (CG) method is employed to min-imize l(X), while the Alternating Direction Method of Multipliers (ADMM) is used to minimize \\(l_{TV}(X)\\). Additionally, diffusion-based methods are utilized as baselines to minimize the loss func-tions in Eq. (21). Specifically, DPS (Chung et al., 2022a) is used to minimize l(X). However,"}, {"title": "4.1 RESULTS", "content": "We present the quantitative results of the temporal degradation tasks in Table. 1. The table shows that the proposed method outperforms the baseline methods by large margins in all metrics. The large margin improvements in FVD indicate that the proposed method successfully solves inverse problems with temporally consistent reconstruction. Fig. 4 shows the qualitative reconstruction results for temporal degradations A. The proposed method restores much finer details compared to the baselines and demonstrates robustness across various temporal PSFs. In contrast, as shown in Fig. 4, while DPS performs well in reconstructing uniform PSFs with a kernel width of 7, it fails to accurately reconstruct frame intensities as the kernel becomes wider or more complex as shown in the bottom figures, leading to significant drops in Table 1. DiffusionMBIR ensures temporal consistency and performs well for static scenes, but it struggles with dynamic scenes in the video. In the same context, ADMM-TV produces unsatisfactory results for dynamic scenes.\nThe results of the spatio-temporal degradations are presented in Table 2 and Fig. 5. Even with additional spatial degradations, the proposed method consistently outperforms baseline methods. On the other hand, DPS often produces undesired details, as shown in Fig. 5. DiffusionMBIR fails to restore fine details in dynamic scenes. Specifically, in the 3rd row of Fig. 5, DiffusionMBIR restores the static mural painting but fails to capture the motion of the person. This is because TV regularizer often disrupts the restoration of dynamic scenes. In this context, our method ensures temporal consistency without the need for a TV regularizer. Furthermore, thanks to the consistent performance even at low NFE, the proposed method achieves a dramatic 10x to 50\u00d7 acceleration in reconstruction time. For handling temporal degradation with 20 NFE, the proposed diffusion model-based inverse problem solver can now achieve speeds exceeding 1 FPS."}, {"title": "4.2 ABLATION STUDY", "content": "Effect of CG updates. Experimental results demonstrate the tangential CG updates in video space on the denoised manifold are key elements in solving spatio-temporal degradations. Here, we com-pare the proposed method with a stand-alone CG method to demonstrate its impact within the solver. We applied the same CG iterations as in the proposed method but excluded the diffusion updates. As shown in Fig. 6, while the stand-alone CG method nearly solves the video inverse problem, it leaves"}, {"title": "5 CONCLUSION", "content": "In this work, we introduce an innovative video inverse problem solver that utilizes only image dif-fusion models. Our method leverages the time dimension of video as the batch dimension in image diffusion models, integrating video inverse optimization within the Tweedie denoised manifold. We combine batch-consistent sampling with video inverse optimization at each reverse diffusion step, resulting in a novel and efficient solution for video inverse problems. Extensive experiments on temporal and spatio-temporal degradations demonstrate that the proposed method achieves superior quality while being faster than previous DIS methods, even reaching speeds exceeding 1 FPS."}, {"title": "A EXPERIMENTAL DETAILS", "content": "A.1 IMPLEMENTATION OF DEGRADATIONS\nFor spatio-temporal degradations, we applied temporal degradation followed by spatial degradation sequentially. We utilize spatial degradation operations for super-resolution, inpainting, and deblur-ring as specified in the official implementation from Wang et al. (2023) and Chung et al. (2022a). For super-resolution, we employ 4\u00d7 average pooling as the forward operator A. For inpainting, we use a random mask to eliminate half of the pixels for both the forward operator A. In deblurring, we apply a Gaussian blur with a standard deviation (\u03c3) of 2.0 and a kernel width of 13 as the forward operator A.\nA.2 DATA PREPROCESSING DETAILS\nWe conducted every experiment using train/val sets of DAVIS 2017 dataset (Perazzi et al., 2016; Pont-Tuset et al., 2017). 480p resolution dataset has a spatial resolution of 480\u00d7640. Therefore, to avoid spatial distortion, the frames were first center cropped to 480\u00d7480, then resized to a resolution of 256x256. The resizing was performed using the 'resize' function from the 'cv2' library. After that, all videos were normalized to the range [0, 1]. In the temporal dimension, the video was segmented into chunks of 16 frames starting from the first frame. Any remaining frames that did not form a complete set of 16 were dropped. Through this process, a total of 338 video samples were obtained. The detailed data preprocessing code and the preprocessed Numpy files have all been open-sourced.\nA.3 COMPARATIVE METHODS\nDiffusionMBIR (Chung et al., 2023b). For DiffusionMBIR, we use the same pre-trained image diffusion model (Dhariwal & Nichol, 2021) with 1000 NFE sampling. The optimal \\(\\rho\\) and \\(\\lambda\\) values are obtained through grid search within the ranges [0.001, 10] and [0.0001, 1], respectively. The values are set to (\\(\\rho\\), \\(\\lambda\\)) = (0.1, 0.001) for temporal degradation, and (\\(\\rho\\), \\(\\lambda\\)) = (0.01, 0.01) for spatio-temporal degradation.\nDPS (Chung et al., 2022a). For DPS, we use the same pre-trained image diffusion model (Dhariwal & Nichol, 2021) with 1000 NFE sampling. The optimal step size (\\(\\gamma\\)) is obtained through grid search within the range [0.01, 100]. The value is set to \\(\\gamma\\) = 30 for both temporal degradation and spatio-temporal degradation. Memory issues exist when performing DPS sampling more than 5 batch sizes in NVIDIA GeForce RTX 4090 GPU with VRAM 24GB. Therefore, we divide 16-frame videos into 4-frame videos and use them for all DPS experiments.\nADMM-TV. Following the protocol of Chung et al. (2023b), we optimize the following objective\n\\(X^* = argmin_X \\frac{1}{2}||AX - Y ||_2^2 + \\lambda ||DX||_1\\)                                                                                                              (22)\nwhere D = [\\(D_t\\), \\(D_h\\), \\(D_w\\)], which corresponds to the classical TV. t, h, and w represent temporal, height, and width directions, respectively. The outer iterations of ADMM are solved with 30 iter-ations and the inner iterations of CG are solved with 20 iterations, which are identical settings to Chung et al. (2023b). We perform a grid search to find the optical parameter values that produce the most visually pleasing solution. The parameter is set to (\\(\\rho\\), \\(\\lambda\\)) = (1, 0.001). We set initial X as zeros."}, {"title": "B FURTHER EXPERIMENTAL RESULTS", "content": "B.1 VRAM-EFFICIENT SAMPLING\nThe proposed method is VRAM-efficient, treating video frames as batches in the image diffusion model for sampling. As shown in Table 4, the method can reconstruct an 8-frame video at 256x256 resolution using less than 11GB of VRAM, which is feasible on GPUs like the GTX 1080Ti or RTX 2080Ti (11GB VRAM). With a single RTX 4090 GPU (24GB VRAM), it can reconstruct a 32-frame video at the same resolution."}, {"title": "B.2 ABLATION STUDY OF STOCHASTICITY", "content": "Experimental results show that synchronizing stochastic noise along batch direction enables batch-consistent reconstruction, offering an effective solution for video inverse problems. While it is theoretically possible to achieve batch-consistent sampling with \u03b7 set to 0 (by eliminating stochastic noise), our empirical findings, as shown in Table. 5, indicate that incorporating stochastic noise is beneficial for video reconstruction, particularly in cases involving spatio-temporal degradations. Consequently, in our experiments, the optimal \u03b7 value was determined through a grid search."}]}