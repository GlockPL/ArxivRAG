{"title": "Open-vocabulary Temporal Action Localization using VLMs", "authors": ["Naoki Wake", "Atsushi Kanehira", "Kazuhiro Sasabuchi", "Jun Takamatsu", "Katsushi Ikeuchi"], "abstract": "Video action localization aims to find timings of a specific action from a long video. Although existing learning-based approaches have been successful, those require annotating videos that come with a considerable labor cost. This paper proposes a learning-free, open-vocabulary approach based on emerging vision-language models (VLM). The challenge stems from the fact that VLMs are neither designed to process long videos nor tailored for finding actions. We overcome these problems by extending an iterative visual prompting technique. Specifically, we sample video frames into a concatenated image with frame index labels, making a VLM guess a frame that is considered to be closest to the start/end of the action. Iterating this process by narrowing a sampling time window results in finding a specific frame of start and end of an action. We demonstrate that this sampling technique yields reasonable results, illustrating a practical extension of VLMs for understanding videos. The code will be available shortly.", "sections": [{"title": "I. INTRODUCTION", "content": "Video action localization aims to find timings of a specific action from a long video. Extracting precise timings of an action is a fundamental technique in computer-vision and other research domains, including video annotation, video editing, and automatic video collection from unlabelled datasets. For example, in the context of robotics, accurately extracting videos corresponding to specific actions will greatly benefit from analyzing human action in several robot-teaching frameworks where robot learns from human demonstrations, such as Learning from Demonstration (LfD) [1] and Learning from Observation (LfO) [2], [3], [4].\nPrevious mainstream research on video action localization has primarily focused on training models based on labelled datasets. The typical approach is to classify frames or a series of consecutive frames with predefined action labels for determining which action is being executed at each point in time in the video [5], [6], [7]. While these methods have been successful on several benchmarks, they have limitations in terms of extensibility. Specifically, if new actions not included in the training data need to be recognized, additional data collection and model retraining are required. This presents a bottleneck for the quick and convenient application of these methods to data outside benchmark tasks.\nIn this paper, we introduce a pipeline for Open-vocabulary video action localization (OVAL), which does not require pre-training and can address action labels in an open-vocabulary form (Fig. 1). Our approach is inspired by a recently proposed method called \"Prompting with Iterative Visual Optimization (PIVOT)\"[8]. The original PIVOT aims to utilize Vision-Language Models (VLM) to reason an embodied action by framing the problem as iterative visual question answering. In this paper, we propose a simple yet effective approach to apply PIVOT to find a temporal segment of actions. Specifically, we sample video frames into a concatenated image with frame index labels, making a VLM guess the frame closest to the start and end of an action. Iterating this process by narrowing the sampling time window results in finding specific frames when the action started and ended, thus achieving action localization. Although the recent advancements in VLM technology has supported the development of OVAL [9], [10], [11], [12], [13], [14], [15], to the best of our knowledge, there are no existing methods that utilize off-the-shelf VLMs directly.\nUsing OpenAI's GPT-40 [16] as an example of VLMs, we evaluated the performance of the proposed pipeline using an existing benchmark of breakfast cooking dataset and our labelled original cooking dataset. We checked how the pipeline works for a typical robot-teaching condition in which a transition of high-level task needs to be grounded against a human demonstration video [17], [18], [19] (e.g., grasp, pick-up, move, put, and release an object). As result, we show the model achieves over 60% accuracy in terms of mean-over-frame (MoF), performing better than a baseline method. Although the performance appeared to be inferior to the state-the-of-art learning-based methods, the result demonstrated the feasibility of the proposed method. Additionally, we show the impact of several parameters of visual prompting and qualitative results for extracting specific actions from an unlabeled dataset, to deepen insights into its application. The contributions of this research are:\n\u2022 Proposing a novel pipeline for open-vocabulary video action localization by extending PIVOT."}, {"title": "II. METHOD", "content": "Figure 2 shows the core pipeline of the proposed method, aligning the original illustration of PIVOT [8]. The pipeline starts by sampling a number of frames at regular intervals from a time window of the given video (Fig. 2(a)). In the first iteration, the sampling window was set to be identical to the length of the video to capture the entire sequence of the video. The number of frames to sample is a hyperparameter and Fig. 2 illustrates the case of 16 frames. The sampled frames are then tiled in an image with the annotation of the time order of the frames (Fig. 2(b)). The image was then fed into a VLM to identify the frame closest to a specific action timing; Fig. 2(c) illustrates the start timing of an action. Finally, the sampling window is updated centering the selected frame with a narrower sampling interval (Fig. 2(d)). Note that due to the sparsity of sampling, the exact moment of an action may not necessarily be included in the query image. The pipeline can precisely identify the relevant moment by repeatedly narrowing the sampling window around the estimated closest frame until reaching a specific sampling interval. Hereafter, we will refer to this method as Temporal PIVOT (T-PIVOT).\nFor general action localization, we first estimate the start time of the action in the video using T-PIVOT (Fig. 2 bottom panel (1)). Then, by re-setting a sampling window for the period after the start time, we estimate the end time of the action in the same manner (Fig. 2 bottom panel (2)). By using the obtained start and end time, we localize the action of interest. Importantly, since this method involves querying the VLM, it allows us to specify actions in open-vocabulary free-text queries."}, {"title": "III. EXPERIMENT", "content": "We evaluated the performance of the proposed pipeline in the light of practical use cases. Specifically, the pipeline was evaluated in the context of a typical robot-teaching condition on top of task and motion planning (TAMP)[20], where a system needs to ground high-level task plan (e.g., grasp, pick-up, move, put, and release an object) against a human demonstration video [17], [18], [19]. To this end, we used an existing benchmark dataset and our labeled original cooking dataset. We focused on cooking actions because it reflects realistic scenarios as well as challenging appearance of cluttered objects in a scene.\nBreakfast Dataset [21] is the third-person videos containing ten activities related to breakfast preparation, such as pouring drink and making a sandwich. Each video is annotated without gaps along the timeline with more detailed action labels, such as taking a cup and stirring. The videos contain 52 different individuals across 18 different kitchens, totaling 1712 videos. As we are proposing learning-free approach, we used only the testing dataset of 335 videos based on the split used in [22].\nFine-grained Breakfast dataset is the first-person videos of human cooking activities with fine-grained robotics tasks. Existing datasets of human actions including the Breakfast dataset [21] are labeled with coarse categories and simple motion labels often consist of multiple robotics manipu-lations (e.g., a motion labeled as \"pour milk\" needs a sequential robotic manipulation of moving a robot hand towards a milk carton, grasp it, pick it up, move it near a cup, rotate and hold the milk carton, adjust its pose, move back to the table and release it [23], [24]. There is a huge gap between the granularity of action in the computer-vision community and the robotics community. To our knowledge, however, no existing datasets label tasks based on fine-grained robotic actions. To gain insights into the usefulness of the proposed pipeline for robot teaching, we prepared a fine-grained breakfast dataset by manually annotating an existing dataset of breakfast preparation [25] using a third-party annotation tool [26].\nBased on the Kuhn-Tucker theory [27] explaining solution space of linear simultaneous inequalities in space, we defined tasks as a transition of constraints imposed to an object [28], [29], [30]. As such we obtain a task set that is sufficient and enough to explain manipulation, such as picking up, moving, placing, rotating (refer to [29] for the complete list of tasks). 113 videos were used in this study."}, {"title": "B. Task Definition", "content": "Given a video V and a sequence of task labels $L_i$ ($i = 1... N$) contained within it, our goal is to estimate the timing of these transitions $T_{i \\rightarrow i+1}$ ($i = 1... N - 1$). We assume that the task sequence is continuous. As a baseline method, we partition the video uniformly based on the length of the sequence, allocating equal time intervals to each task. We evaluate our method using the following metrics commonly used in temporal action localization. [31]:\n\u2022 MoF (Mean-over-Frames) represents the percentage of correctly predicted frames out of the total number of frames in a video.\n\u2022 IoU (Intersection over Union): measures the overlap between the predicted and ground truth segments for each action class. It is calculated as the ratio of the intersection over the union of the two sets.\n\u2022 F1-score: The harmonic mean of precision and recall, providing a balanced measure of prediction accuracy."}, {"title": "C. T-PIVOT for Recognizing Seamless Action Transitions", "content": "By applying the T-PIVOT method described in Section II, we estimated the timing of task transitions within the video as follows. First, the start time of each task, $T_{i_{start}}$ ($i = 1... N$), is estimated using the T-PIVOT in parallel processing. The end time of each task, $T_{i_{end}}$ ($i = 1 . . . N$), is similarly estimated. This approach yields an estimated series of start times and end times for each task. In this experiment, we assume that tasks are continuously present in the video. Therefore, the end time of one task is identical to the start time of the subsequent task. To ensure this, we calculate this time as the midpoint between the estimated end time of the current task and the estimated start time of the next task:\n$T_{i \\rightarrow i+1} = (T_{i_{end}} + T_{i+1_{start}})/2$.\nSpecifically, the transition time $T_{i \\rightarrow i+1}$ ($i = 1 . . . N - 1$) is calculated using Algorithm 2."}, {"title": "D. Action-order Aware Prompting", "content": "To estimate the start and end times of each task in parallel with Algorithm 2, each T-PIVOT process needs to know which task in the task sequence it is focusing on. Without this knowledge, the process cannot determine which instance to focus on; for example, if there are two instances of \"picking up\" in a video, it is not clear which \u201cpicking up\" action is of interest when given a query of \u201cPlease find the picking up action in this video.\" To address this issue, we prepared the following prompt using a sequence of task labels $L_i$ ($i = 1 ... N$):\n\"I will show an image sequence of human operation. It con-tains the following tasks: {task_sequence}. I have annotated the images with numbered circles. Choose the number that is closest to the moment when the ({task focus}) has started."}, {"title": "IV. DISCUSSION AND LIMITATION", "content": "In this paper, we proposed a method for open-vocabulary video action localization using VLMs. Despite its straightforward pipeline, benchmark evaluations demonstrated reasonable performance, both for third-person and first-person videos. The effectiveness of this approach diminishes with increased video step lengths, and it is outperformed by established learning-based methods. Nevertheless, this method offers significant advantages: it eliminates the need for data collection or training and can extract actions specified by open-vocabulary free-text queries, thereby enhancing its adaptability to diverse applications such as video annotation and video editing.\nThe comparison of different prompting styles has revealed several limitations of this method. First, the optimal number of frames for sampling depends on a trade-off between spatial and temporal resolution in action recognition. This number may vary based on the video's length and the nature of the actions, potentially requiring adjustments for each use case. Second, the method's performance is negatively impacted by the length of action sequences and by numerical annotations that interfere with image clarity. These issues highlight opportunities to improve the method by adjusting the placement of number renderings or by modifying the video length beforehand."}, {"title": "V. CONCLUSION", "content": "This paper introduces a novel approach for open-vocabulary video action localization leveraging the PIVOT methodology, which eliminates the need for data collection and model training. Testing with benchmark datasets has demonstrated that the proposed method offers promising capabilities in action localization without the constraints of predefined vocabularies. Considering several limitations of this method, future work will focus on refining visual prompting techniques and optimizing the system to enhance its applicability across a broader range of video analysis tasks, from automated video labeling to advanced robotics training."}]}