{"title": "VQA-Levels: A Hierarchical Approach for Classifying Questions in VQA", "authors": ["Madhuri Latha Madaka", "Chakravarthy Bhagvati"], "abstract": "Designing datasets for Visual Question Answering (VQA) is a difficult and complex task that requires NLP for parsing and computer vision for analysing the relevant aspects of the image for answering the question asked. Several benchmark datasets have been developed by researchers but there are many issues with using them for methodical performance tests. This paper proposes a new benchmark dataset a pilot version called VQA-Levels is ready now for testing VQA systems systematically and assisting researchers in advancing the field. The questions are classified into seven levels ranging from direct answers based on low-level image features (without needing even a classifier) to those requiring high-level abstraction of the entire image content. The questions in the dataset exhibit one or many of ten properties. Each is categorised into a specific level from 1 to 7. Levels 1 - 3 are directly on the visual content while the remaining levels require extra knowledge about the objects in the image. Each question generally has a unique one or two-word answer. The questions are 'natural' in the sense that a human is likely to ask such a question when seeing the images. An example question at Level 1 is, \"What is the shape of the red colored region in the image?\" while at Level 7, it is, \"Why is the man cutting the paper?\". Initial testing of the proposed dataset on some of the existing VQA systems reveals that their success is high on Level 1 (low level features) and Level 2 (object classification) questions, least on Level 3 (scene text) followed by Level 6 (extrapolation) and Level 7 (whole scene analysis) questions. The work in this paper will go a long way to systematically analyze VQA systems.", "sections": [{"title": "1 Introduction", "content": "Visual question answering (VQA) is a challenging multimodal task involving vision and language. It requires processing and understanding the visual and semantic characteristics of an image and the linguistic properties of a question. The correct answer then requires combining these two with perhaps additional analysis that lies in between these two domains. VQA is an active research area with several systems [6, 9, 11, 16, 17] developed in the last few years often as extensions of the image captioning systems.\nAn important component of VQA research is the generation of appropriate datasets for training as well as testing purposes. Several VQA datasets like VQA 1.0 [2], VQA 2.0 [5], Visual 7W [19], Visual Madlibs [18], CLEVR [7], TDIUC [8] are used by researchers. Another popular dataset is MSCOCO [12] which is primarily developed for image captioning and for image descriptions.\nThere are many challenges associated with building a VQA dataset. The questions have to be so framed that they have unambiguous answers. The existing datasets provide many different answers and a VQA system is evaluated on its answers being one of the given answers. The questions sometimes are deliberately designed to confuse VQA systems as in VQA 1.0 [2]. The questions are not graded according to difficulty levels but classified based on common question forms such as, \"what, how, which, when,\" etc. Many questions are contrived in the sense that they are not commonly asked by humans who are shown the image. There are also irrelevant questions which are completely unrelated to the objects in the image. Finally, it is not an easy task to generate questions although attempts have been made to do so [4]. The resulting questions are usually based on low-level features such as color, shape and positions in the image.\nHumans analyze images at varying levels of complexity and abstractions. As toddlers, color attracts our eyes, sometimes size and shape of the objects, and as we grow older, familiarity, emotional and higher level abstractions based upon the entire scene and not on individual objects form the basis for our questions. Questions also depend on the purpose or the application for which we look at the image.\nIt is easy to recognise that humans ask a number of questions with varying levels of complexity and familiarity when seeing an image. For example, if shown an image of a Premier League match, a person not familiar with the league may ask, \"What is being played?\"; another who knows football may ask, \"Which teams are playing the match?\"; a third, who is a Premier League fan may even look for a popular player X and ask, \"Where is X?\u201d\nA suitable benchmark dataset must address the above issues and also be generic without specific references to local or regional aspects unless these are popular worldwide. In this paper, we propose a new dataset called the VQA-Levels Dataset that addresses many of the issues above. All the questions in the dataset have unambiguous answers that are mostly a single word or may be two to three word phrases. They are organised into 7 levels of complexity with Level 1 containing the most direct questions that do not need even a classifier to answer them correctly. Level 7 contains abstract questions based on the entire image such as, \"Why are tomatoes being thrown at each other?\" Each higher level, in general, requires greater analysis than its lower levels. A pilot version with about 210 images and 751 questions is ready but the goal is to create a much larger dataset with around 2000 questions on about 400 images. As the size of the pilot dataset is small, it is currently usable only for testing VQA systems rather than training."}, {"title": "2 Related Work", "content": "There are several datasets popular with researchers in VQA. Some of them classify the questions into groups based on question keywords.\nCOCO-QA [14] have questions in four categories on object identification, number, color and location. All the questions have one word answers. It has 123287 images and 117684 questions.\nVQA 1.0, VQA 2.0 [2, 5] Both these annotated datasets group questions based on the starting words of the question like \"What is\", \"Is there\", \"How many\u201d, \u201cWhich\u201d, \u201cWhat time\u201d, \u201cWhat sport is\", etc. VQA 2.0 is one of the most comprehensive datasets for VQA with many categories of images and questions.\nCLEVR [7] asks questions based on identification of attributes, comparison of attributes, number of objects, existence and integer comparison of objects. It has 100,000 images and more than 850,000 questions.\nVisual7W [19] has seven kinds of queries starting with \u201cwhat, how, where, who, when, why\" and \"which.\"\nVisual Genome [10] has six question types \"what, why, where, who, when\u201d and \"how\" in a multiple choice setting.\nTDIUC [8] has eight categories of questions like presence of object, recognition of subordinate object, count, color, other attributes, recognition of activity, sport recognition and positional recognition. It has about 1.6 million questions on 170,000 images.\nThe above datasets are all extremely large in size but the questions are not organised systematically into categories with increasing complexity. The question categories are non-hierarchical and are also based almost completely only on the visual content in the image. A human being, on the other hand, asks and answers questions based on the semantics and object properties. For example, in a mixed double match showing three players two males and a female a question on the gender of the fourth unseen player in the image is easily answered by a human but such questions are not present and categorised in the above datasets."}, {"title": "3VQA-Levels Dataset", "content": "The main contributions of our work are (a) creation of a VQA-Levels dataset with systematic categorisation of questions into graded levels of difficulty; (b) analysis of the results of popular VQA systems with regards the complexity level of questions; and, (c) finding specific areas for improvement in performance of VQA systems.\nThe paper is organised into 7 sections. The second section presents some currently existing VQA datasets with question categories. Section 3 provides the intuitions and the background of VQA-Levels dataset creation while Section 4 describes the 7 level hierarchy. Section 5 describes the VQA-Levels dataset, the experiments and results of testing several popular VQA systems with the VQA-Levels dataset. Section 6 is a discussion on the findings from the experiments with conclusion in Section 7.\nThe intuition behind the creation of VQA-Levels dataset is three-fold. The first, to create a dataset containing questions at different levels of complexity. The second, to have questions that a human being is likely to ask and which may go beyond only the visual content in the image. The third, to have questions that can be answered in an unambiguous manner in either one word or two to three word phrases.\nThe first task is, therefore, to identify levels of complexity in a systematic manner. The inspiration came from Marr's Theory of three levels of vision [13] and content-based image retrieval systems (CBIR). Marr's theory states that low-level features are first extracted from images and are used to form an intermediate 2D sketch that divides image into segments with shape and 3D position information. The third or high-level assigns semantic labels to the different regions in the image and recognises them as 3D objects. CBIR systems classified questions into categories based on visual content, object properties and similarities and semantic contents.\nQuestions in the VQA-Levels dataset are human oriented. Our aim is to ask questions which humans would ask when they look at an image rather than to focus on VQA systems, their architectures and algorithms. There is no attempt to ask questions that are designed to confuse the system as in VQA 1.0 [2]. All questions in VQA-Levels are precise and have mostly only one correct response, which will aid in evaluating the performance more effectively. This also led us to decide that automatic generation of questions based on image descriptions will not be used in the initial phase.\nThe questions are framed so that they can be answered unambiguously. An answer was given by the person who gave a question. Multiple students then answered the same question independently and were given the option of either sticking with their answer or revert to the answer given by the questioner. In this process, questions which had multiple answers were excluded from the dataset.\nThese three main features resulted in a dataset containing 7 levels of complexity. The first two levels L1 and L2 involve low-level features such as color, shape, texture, etc. and object identification. Levels L4 to L7 deal with symbolic and semantic information, similarities and dissimilarities in object properties and such combinations of visual content and semantics. Level L3 will be described later in this section."}, {"title": "3.1 Visual and Semantic Properties", "content": "The properties used in creating the VQA-Levels dataset are:\nIntrinsic Properties: An object has many properties, e.g a bicycle has two wheels, a bottle can contain solid or liquid objects, etc. A question such as \"Which object in the image can be used to hold water?\" is an example question based on certain intrinsic properties of an object labelled by a classifier.\nEmotions: Emotions can be specific to a single object in the image or at the entire image level. For example, \u201cHow is the person in red shirt feeling?\"\nDirections and Signs: There may be arrows, traffic lights and other such objects with specific functions and a question can be on such functions. For example the question, \"Is it safe for the pedestrian to cross the road?\" in an image showing a pedestrian walk signal.\nSimilarity/Dissimilarity: Questions based on comparisons such as \u201cWhich object in the image is similar in shape to a banana?\" (banana may or may not be present in the image) or \"Which two fruits in the image have different tastes?\" relate to similarity and dissimilarity. They can be with regard to a specific object or can be on the entire scene.\nSeasons and Time of the Day: Images often contain information about time and seasons. For example, a tree with colorful leaves indicates Fall season and a snowy scene is winter time. Blue sky indicates day time and long shadows may indicate either dawn or dusk.\nDistinguishable Property: Certain objects are uniquely identifiable and associated with specific contexts. E.g., \"In which city is this photograph taken?\" for an image with Eiffel Tower in it.\nFunctionality: Objects have a specific purpose or role it serves within a system or context like clock used to show time, bell of bicycle to alert. For example, in an image with a man wearing a raincoat on a rainy day, the question \"Which object protects the man from rain?\" is related to functionality.\nSequencing: Order of things and events what happened before and what will happen after. For example, \"Which colored ring will come next?\" while arranging a group of rings in descending order of size, \"Will the batsman hit the ball?\" from a sports image.\nExtrapolation and Occlusion: A portion of an object or a scene is visible in the image and the question relates to the occluded or unseen portions. For example, \"How many wheels does the blue object have?\" Once the object is identified as a car, the answer is \"four\" even if not all wheels are visible in the image.\nIntangibles and Abstractions: Certain questions may require analysis and knowledge of all the objects in the scene shown in the image. For example, \"What object are the objects in this image a part of?\" on an image showing the case, cap, refill and other parts of a pen. \u201cWhich event is being celebrated?\", is another example.\nLevel L3 came out of the process of creating the VQA-Levels dataset. The dataset was created through the efforts of about 50 students (see Section 6).\""}, {"title": "4 Question Levels", "content": "We have classified the questions in a VQA system into 7 levels by combining CBIR systems approaches and Marr's theory as our foundation. The first two levels are purely visual and may be answered by constructing a suitable classifier (the third level in Marr's theory). The third level in our VQA questions hierarchy requires reading and processing scene text. Fourth and higher levels ask questions that require a symbolic or semantic representation based on the object labels given by the classifier; having only visual content is insufficient. For example, a question, \"What is the object held by the woman in the image used for?\" or \"What event is being celebrated?\" is at higher levels. Questions at levels 4 7 are based on the features described in the previous section.\nLevel 1 (L1): These are questions which can be answered without classification of objects. They query on low level features of the image like color and shape which do not need labeling the object to answer them. Examples are given in Fig. 1a.\nLevel 2 (L2): These questions can be answered on applying a classifier. Deep networks trained for classification on datasets like ImageNet [3] must be able to answer these questions. Questions on\nobject identification\nlogo identification\ncolor, position, shape of objects (not regions)\nnumber of objects: multiple objects identified by a classifier that belong\nto either the same or different categories\ncome under Level 2. Examples are given in Fig. 1b.\nLevel 3 (L3): L3 is different from the other levels and is based on recognising and processing scene text present in an image. There are many examples of scene text: sign boards, titles, clocks, calendars, diaries, etc. are a few such examples. Figs. 1c and 1d show examples of questions that come under L3.\nLevel 4 (L4): From this level questions asked may require knowledge that may not be directly present as visual information. Questions which require knowledge of a single object for answering are in Level 4. In other words, these are questions asked about the properties (listed on Page 5) of a single object previously classified by a classifier. Some examples are shown in Fig. 2.\nAn important subclass of questions in L4 are of the type \u201cwhich game/sport\u201d. These are placed in L4 because identifying a sport/game is generally based on a distinguishing object such as a tennis racqet, hockey stick, golf club, etc."}, {"title": "5 Benchmark dataset for testing VQA systems", "content": "Level 5 (L5): Questions based on the features listed on Page 5 but requiring analysis of more than one object and their interactions. Examples are in Fig. 3.\nLevel 6 (L6): Questions on extrapolated, hidden and occluded objects come under Level 6. For example, three players from a mixed doubles match are shown in the image and the question asks the gender of the fourth person as in Fig. 4a. A car dashboard is shown and the question is on the indicator signals at the back of the vehicle (which are not seen in the image) Fig. 4b.\nLevel 7 (L7): These questions require analysis of the entire image and sometimes beyond. The questions may be on intangible properties such as 'happiness', 'disasters', etc. Examples at this level can be \"What event is being celebrated?\", \"Why are the people hitting each other with tomatoes?\u201d, etc as in Fig. 5.\nThe 7 levels of questions proposed in our dataset and their definitions are summarised in Tab. 1.\nA dataset VQA-Levels with 210 images and 751 questions has been created as a pilot project. 170 images were taken from the Microsoft Common Objects in Context (MS COCO) dataset [12], 9 images were taken from the web and the remaining 31 images are from the authors.\nA user interface was created to help in creating the dataset. In question generation mode, the interface randomly selected an image from a database of"}, {"title": "7 Discussion", "content": "about 10,000 images and presented it on the screen. The user was given an option either to enter questions or skip to another image. When entering the questions, the user also had to select the level of the question and the reason for doing so. The user also had to enter an answer. The question levels were also displayed in the interface to assist the users. A group of around 50 people consisting of graduate and Ph.D. students volunteered in creating the dataset. They were trained on the user interface and the question levels.\nThese student volunteers were also provided with a question answering mode. In this mode, images for which questions were earlier given, are shown to the user along with the questions. The user was given the space to answer the question. These images are not the ones for which they framed the questions. Once the user answered the question, the answer originally given was displayed on the screen. The user was asked whether he/she agreed with the answer or wishes to change the answer to his/her choice.\nResults in Table 3 show some significant points of interest. First, the general trends indicate that the models performed best on L1 and L2 questions as expected. These questions are directly related to the visual content in the images. The least performance is on Level L3, then L6 and L7 questions. This is a vindication of the levels proposed in our dataset that the questions are categorised according to their difficulty. L3 is, as expected, an anomaly as it requires special training for detecting and recognising scene text.\nSecond, the VQA systems today are using the information of language models than the vision based semantics. Fig. 6a shows an image where the child is flying a kite and thread is not visible in the image. The question asked is, \"What connects the kite to the child?\". Almost all the systems tested answered correctly as \"string.\" But, a very similar question, \"What is the child holding in her left hand?\" on another image of a child flying the kite resulted in a wrong answer from all the systems. When asked the question \"Why is the man cutting the paper?\" on Fig. 6b whole scene analysis is required to give the answer, \"for coupons\". No system tested gave the correct answer. They all answered either \"cut\" or \"to open\" the paper. Questions at these levels proved difficult unless there is a word closely related to the answer in the question. However, performance at Levels L6 and L7 requires greater experimentation as the numbers of questions at these levels is low.\nThird, the dominance of language models is further illustrated on certain questions at Levels L4 and L5. When asked questions about directions like \"In which direction is the wind blowing? on the image in Fig. 6a models give answers such as East, West, sometimes even North, which is not the natural way for humans to answer such questions. Most of the volunteers correctly gave the answers as \"from left to right\" or \"from the left\". In fact none of them gave the answers as \"East, West,\" etc.\nFourth, absurd answers are given: again as expected because these systems have no causal or semantic understanding. For example, on an image of a par-"}, {"title": "8 Conclusion", "content": "tially eclipsed Moon, the question, \"What is in the image?\" resulted in the incorrect answer of \"plate.\" The question, \"Does it show an eclipse?\" led to two \"no\" and four \"yes\" answers. The VQA systems, of course, lack the knowledge that a plate cannot be eclipsed.\nFinally, there is a significant need to work on scene text and use it for answering questions. The systems tested could answer questions correctly on prominent text in the picture but had difficulty in clock times, diary-based, or calendar-based questions.\nOur goal is to have at least 250 questions in each level. L2 and L4 questions have reached close to that count. More number of questions in L6 and L7 have to be added. Questions in these levels have to be manually added and it is also difficult to find images where humans naturally ask such abstract questions. Question generation at Ll can be automated as they are on the basic features of image. L2 questions can be automatically generated based on objects identified and their presence, count and color. We are in the process of generating questions at Levels L4 and L5 automatically by using the classification labels as keywords in Internet search. Automating generation of L6 and L7 questions appears difficult with the current systems.\nFrom the above, we see that current VQA systems, built by combining visual recognition and language models perform well on questions involving visual content and image description. Their performance drops when questions that require external knowledge, extrapolation to unseen parts of the scenes, abstraction, etc. This is especially the case when the questions do not contain any words that are related to the answers.\nIn this paper, we created a new VQA-Levels dataset that contains questions systematically arranged into 7 levels based on their complexity and the level of"}]}