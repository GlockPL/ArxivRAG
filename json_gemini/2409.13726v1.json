{"title": "Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement", "authors": ["Marius Funk", "Shogo Okada", "Elisabeth Andr\u00e9"], "abstract": "Non-verbal behavior is a central challenge in understanding the dynamics of a conversation and the affective states between in- terlocutors arising from the interaction. Although psychological research has demonstrated that non-verbal behaviors vary across cultures, limited computational analysis has been conducted to clarify these differences and assess their impact on engagement recognition. To gain a greater understanding of engagement and non-verbal behaviors among a wide range of cultures and language spheres, in this study we conduct a multilingual computational analysis of non-verbal features and investigate their role in engage- ment and engagement prediction. To achieve this goal, we first expanded the NoXi dataset, which contains interaction data from participants living in France, Germany, and the United Kingdom, by collecting session data of dyadic conversations in Japanese and Chinese, resulting in the enhanced dataset NoXi+J. Next, we ex- tracted multimodal non-verbal features, including speech acoustics, facial expressions, backchanneling and gestures, via various pat- tern recognition techniques and algorithms. Then, we conducted a statistical analysis of listening behaviors and backchannel patterns to identify culturally dependent and independent features in each language and common features among multiple languages. These features were also correlated with the engagement shown by the in- terlocutors. Finally, we analyzed the influence of cultural differences in the input features of LSTM models trained to predict engage- ment for five language datasets. A SHAP analysis combined with transfer learning confirmed a considerable correlation between the importance of input features for a language set and the significant cultural characteristics analyzed.", "sections": [{"title": "1 INTRODUCTION", "content": "Considering cultural differences in non-verbal behavior is essential for seamless conversations in different languages. This problem has been discussed as far back as Edward T. Hall in 1959, who stressed the importance of \"the non-verbal language which exists in every country of the world and among the various groups within each country\" [27] to understand the ways of interaction between different cultures.\nThe field of non-verbal behavior and backchannels in culture- dependent human-human interaction has since been extensively studied, whereas non-verbal behavior in human-computer interac- tion has focused predominantly on single-culture human-computer interaction [4, 35, 53, 56]. Even though it is acknowledged as impor- tant, cultural characteristics have not been a focus in Social Signal Processing [71].\nIn this paper, we present a computational analysis of the cul- tural characteristics of multimodal non-verbal features and the effects these differences have on engagement and its prediction. For this purpose, we introduce a new multilingual multimodal interac- tion dataset, NoXi-J, which enhances the existing Novice eXpert Interaction database NoXi [11] by recording sessions in Japanese and Chinese, thereby creating an enriched dataset referred to as NoXi+J. We also extract and analyze the non-verbal features and vocal backchannels of all predominant languages (German, Eng- lish, French, Japanese, and Chinese) using various machine learning models and pattern recognition techniques. We study the individual multimodal non-verbal features and investigate the differences be- tween language sets and their correlation with engagement. Finally, we highlight the importance of culture-sensitive approaches with machine learning engagement prediction models. We evaluate the performance of six models trained on different language speaker subsets of the NoXi+J dataset, test the model performance on other language speaker subsets and compare the performance of the"}, {"title": "2 SCIENTIFIC BACKGROUND", "content": ""}, {"title": "2.1 Non-verbal communication", "content": "Non-verbal communication in the context of conversations involves gestures, postures, touch, facial expressions, gaze, and vocal behav- ior beyond the meaning of words [40]. Non-verbal communication can manage the flow of a conversation and therefore the turn-taking [48] and influence engagement by conveying emotional states and signaling interpersonal attitudes [42].\nCultural differences in non-verbal behavior have been acknowl- edged for a long time [2, 26]. Research often focuses on facial emo- tions [17, 49] and differences between East Asian and European facial expressions [37, 49]. Another focus of non-verbal communi- cation is head-nodding, where cultural differences, especially the prevalence of Japanese head nods, are often noted [19, 39]."}, {"title": "2.2 Backchanneling", "content": "The concept of listener utterances that do not lead to turn-taking has been discussed as far back as Fries in 1952 [20], whereas the term backchannel to describe this kind of verbalisation was introduced by Yngve in 1970 [77]. Backchannels often consist of utterances such as the English uh huh and yeah [75] but can also be longer phrases such as the Japanese sou desu ne [28]. They can also include head nods [50] or laughter and exhaling sounds [34].\nStudies often highlight that Japanese interlocutors use back- channels with much higher frequency compared to English or Chi- nese speakers [14, 39, 51, 75]."}, {"title": "2.3 Turn-taking", "content": "Turn-taking describes the changing of the active speaker in a con- versation. Each time such a change takes place is classified as an instance of turn-taking [76]. The role division of the active speaker and listener and the issues that arrive when overlapping talk occurs significantly impact the course of conversations [64].\nTurn-taking is crucial for the management of the flow of con- versations and is often indicated by non-verbal communication [66]. Pauses can also indicate turn-taking, although short pauses between speech are common without turn-taking occurring [69]."}, {"title": "2.4 Engagement", "content": "Engagement refers to the interest a person shows in an ongoing conversation or interaction. It can be measured either continuously or at specific interaction points. It can be assessed between partici- pants or for individual interlocutors separately. Engagement may be directed toward a human interlocutor, a system, or an artificial agent [23].\nOne of the earliest definitions in the context of human-computer interaction comes from Sidner et al. [65], who describes it as \"the process by which individuals in an interaction start, maintain and end their perceived connection to one another\". Sidner et al. emphasize the role of non-verbal behavior and turn-taking as indicators of engagement. In the context of dyadic conversations, Poggi [60] defines engagement as \"the value that a participant in an interaction attributes to the goal of being together with the other participant(s) and of continuing the interaction\"."}, {"title": "3 RELATED WORK", "content": "In recent years, the analysis and prediction of non-verbal commu- nication, turn-taking and backchannels have gained importance in interaction modeling [9, 38, 43, 58]. Researchers have focused on gaze and its role in recognizing intention [56], how non-verbal actions signal human preferences [12], the estimation of agreement [54], head nod detection [4] and backchannel prediction using mul- timodal approaches [35, 53, 74].\nAttempts have been made to equip virtual agents and robots with culture-specific behaviors. In this context, we refer to a survey that reports on how emotions are portrayed in different cultures and explores how virtual agents and robots can simulate culture- specific emotional behaviors [3]. Endrass et al. [18] developed com- putational models to replicate prototypical behaviors of German and Japanese cultures in virtual agents, taking into account verbal behavior, communication management, and non-verbal behavior. Meixuan et al. [45] collected annotated voice responses in three languages - Chinese, English, and Japanese with the aim of developing emotionally attuned robot models.\nMany multimodal datasets focus on behavioral and emotional analysis, such as the Cardiff Conversation Dataset [5], which con- tains 30 conversations with annotation for head movement, speaker activity, and non-verbal utterances, or SEMAINE [52], which fea- tures 150 recordings with emotional annotations. However, multi- cultural conversation datasets for comprehensive non-verbal analy- sis are rare as researchers often focus on text analysis [61] or present study results without making their datasets publicly available [30].\nA few examples of datasets featuring multicultural interactions are the RECOLA Dataset [62], which features collaborative and affec- tive interactions with French, German, Italian and Portuguese par- ticipants, and the Sentiment Analysis in the Wild (SEWA) dataset, which contains recordings of British, German, Hungarian, Greek, Serbian, and Chinese participants [41].\nSeveral reviews indicate the growing interest in engagement and its significance in human-computer interaction [16, 24, 57]."}, {"title": "4 DATA COLLECTION", "content": "The original NoXi database, first introduced by Cafaro et al. [11], contains recordings of dyadic conversations between two interlocu- tors in the roles of expert and novice. In a session ranging between seven and 31 minutes the expert talks about one or more topics they are passionate or knowledgeable about, whereas the novice listens and discusses the introduced topic with the expert. The idea was to obtain a dataset of natural interactions in an expert-novice knowledge-sharing context.\nThe database contains conversations primarily in German, Eng- lish, and French. NoXi-J extends it by adding 48 conversations in Japanese and 18 conversations in Chinese to provide a more cultur- ally diverse dataset. The newly recorded sessions follow the same structure as the original sessions. We will briefly explain the design principles and the recording system used to record the new sessions. For a more detailed explanation, see the initial paper [11]."}, {"title": "4.1 Design Principle", "content": ""}, {"title": "4.1.1 Setting", "content": "Screen-mediated recording was chosen for two pur- poses: to record a face-to-face conversation without requiring mul- tiple cameras recording from different angles and to create a setup more similar to an interaction between a human and a virtual agent. To ensure the capture of facial expressions, gestures, speech and full body movements (e.g. head touch), the participants were recorded in a standing position. The setup used for NoXi and NoXi-J was nearly the same, with minor changes, such as the use of headphones instead of speakers to reduce echo in the case of NoXi-J."}, {"title": "4.1.2 Interaction", "content": "The recordings consist of spontaneous inter- actions that are focused on knowledge transfer and information retrieval but also include planned occurrences of unexpected events (e.g. interruptions). The conversations were not interrupted after the target length of 10 minutes, leading to some interactions ex- ceeding 30 minutes. The actual setup of the interactions for the Asian recordings can be seen in Figure 1."}, {"title": "4.1.3 Participants", "content": "In the European sessions, participants were recruited from local research facilities and their immediate social circles. For the newly recorded Asian sessions, participants were also recruited from local research facilities and, for many Japanese sessions, through an employment agency. This approach provided a wide variety of relationships between expert-novice dyads, ranging from zero-acquaintance situations [1] to spouses."}, {"title": "4.1.4 Unexpected Events", "content": "One of the goals was to obtain occur- rences of unexpected events. In addition to events such as sponta- neous debates during the session, we artificially injected unexpected events during the recording sessions. These events were one of two types of interruptions. Approximately five minutes after the start of a session we either called the novice on their mobile phone (i.e., CALL-IN) or physically entered the recording room to adjust the microphone or ask them to hand over something (i.e., WALK-IN). The novice was informed about the possibility of one of these events occurring prior to the session, in contrast to the expert, who was intended to be surprised and annoyed by the interruption."}, {"title": "4.1.5 Recording Protocol", "content": "The recording protocol had slight differ- ences between the European and Asian parts of the corpus. For the European recordings, the participants were received and instructed in different rooms, whereas for the Asian recordings, the initial explanation was given in a shared room before the participants were split into different rooms. We then primed the novice about the functional interruption, set up their microphone, and indicated where the participants had to stand (also indicated by a marker on the floor or wall). The session was monitored in a separate room. After the conversation concluded naturally, the participants were given questionnaires (see Section 4.3), informed about their compensation, and debriefed.\nBoth participants gave their informed consent before the start of the recording. They consented to the use of the recorded data for sci- entific research and noncommercial applications. The participants had three choices regarding the usage of their data. All participants agreed to (1) the use of their data within the PANORAMA project consortium. Additionally, most participants agreed to (2) the usage of the data in academic conferences, publications and/or as part of teaching material and to (3) the usage of the data for academic and non-commercial applications to third-party users internationally, provided that those parties upholding the same ethical standards as the PANORAMA Project."}, {"title": "4.2 Recording System", "content": "The data were recorded using Microsoft Kinect 2 devices for full HD video streams and ambient noise capture. Furthermore, low- noise recordings of voices were obtained using dynamic head-set microphones (Shure WH20XLR connected via a TASCAM US-322). The Kinect devices were placed over 55\" flat screens. Both were connected to PCs (i7, 16GB-32GB of RAM). Each room's system captured and stored the recorded footage and signal streams locally. A third PC observed the interaction from another room. All PCs were connected over LAN.\nTo sync the recordings, a two-step synchronization was used. Once all the sensors were connected and the setup was completed, we used a network broadcast from the observer room PC to start recording in the expert's room and novice's room simultaneously."}, {"title": "4.3 Collected Data", "content": "The experiment was administered in four countries, with NoXi being conducted in France, Germany, and the UK, and NoXi-J being conducted in Japan. In addition to the Japanese sessions, we decided to increase the diversity and to supplement the dataset with Chinese sessions, as many native Chinese speakers were available at the recording location. Besides the five primary languages, a smaller number of recordings of four other languages (Spanish, Indonesian, Italian and Arabic) was also collected. A summary of the recorded sessions divided by primary language can be found in Table 1. In addition to session details, we recorded demographic information about the participants including their age and gender as well as their self-assigned cultural identity. A breakdown of the five primary languages and their age distributions can be seen in Figure 2. We collected the discussed topics, proficiency of the language spoken for both participants, and the social relationship level between the two participants (e.g. zero acquaintance, friends). All participants provided a self-assessment of their personality on the basis of the Big 5 model [25] by using descriptions for Saucier's Mini-Markers set of adjectives (consisting of 40 adjectives) [63].\nAnonymized data are available for the NoXi+J dataset upon request from the authors at the e-mail address noxi+j@hcai.eu."}, {"title": "4.4 Annotations", "content": "Over 40 annotators from 4 countries (Germany, France, the UK, and Japan) were involved in the annotation of the NoXi+J database. Annotations were made and managed using the freely available annotation tool NOVA\u00b9.\nIn this paper, we focus exclusively on the manually created en- gagement annotations (see Section 2.4). Similar to the original NoXi corpus, engagement in the NoXi-J dataset has been annotated by three or more individuals. However, this excludes the complete Chinese language set and many Japanese language sessions, which"}, {"title": "5 DATA ANALYSIS", "content": ""}, {"title": "5.1 Features", "content": ""}, {"title": "5.1.1 Features of the NoXi corpus", "content": "For the following analysis of intercultural differences of non-verbal features, engagement, and their mutual dependencies, we decided to use a total of 94 features (see Table 2). We focus on the novice's non-verbal characteristics, their behavior, and the impact on engagement. However, we also analyzed expert features in relation to the novice's engagement. Engagement has not yet been annotated for the Chinese language"}, {"title": "5.1.2 Feature extractions", "content": "In addition to the immediate output stream of Kinect such as video and joint position data, we ex- tracted body properties indicative of the expression of emotion [73] such as Head Touch, Arms crossed, Energy Head, Fluidity, Spa- tial Extent, Energy Hands and Overall Activation of the body using NOVA's integrated extraction tools. For the computation of gestural expressivity features (fluidity, overall activation, spatial extent, and energy) we refer to the appendix of an earlier paper describing the NOVA annotation tool [7]. We also used an external Nova Server\u00b2 with integrated Pyannote\u00b3 for Voice Activity Detection (VAD) and whisperX\u2074 [6] for the transcription of speech."}, {"title": "5.1.3 Computed features", "content": "Additional features for the analysis of these data were computed. We used the extracted voice activation data to determine turns and distinguish active speech from vocal backchanneling. We attributed the turn to the first speaker, who holds it until both interlocutors either become silent (i.e. no voice activation is determined) for two seconds (50 frames) or the speaker becomes silent after the interlocutor has spoken for more than two seconds. In the first case, no one holds the turn, and we move on to the next speaker. In the second case, turn-taking takes place. All voice activation instances in between are classified as vocal backchanneling (VBC) (see Figure 3). The idea for this division was influenced by Bosch [69] and his discussion of overlap and turn-taking.\nHead nods were identified by using the pitch, yaw, and roll angles of the head position extracted from the head.stream files. Rapid switches of over 2.5 degrees between up and down movement without extensive other movements were classified as head nods. Changes in the threshold led to different absolute numbers, while the distribution between cultures remained similar."}, {"title": "5.2 Intercultural data comparison", "content": ""}, {"title": "5.2.1 Initial cultural comparison", "content": "Our first focus was on the general assessment of the recorded features. To reduce outliers and make the data more comparable, we calculated the standard score (z- score) for every data point. We then performed an initial Analysis of Variance (ANOVA) [67] for every feature of the session averages between languages. The results show an average F-value of ~43.000"}, {"title": "5.2.2 Cultural differences between feature averages", "content": "We found many noteworthy feature differences between cultures. The Chinese par- ticipants activated AU12, which is commonly associated with a smile, the least, followed closely by the English-speaking partici- pants. In contrast, the Japanese smiled the most. German novices held their arms in open poses observably longer than any other group of participants. In contrast, Japanese and Chinese interlocu- tors adopted an arms-crossed pose for noticeably longer periods, specifically during backchanneling, while otherwise displaying be- haviors similar to that of the European participants."}, {"title": "5.3 Engagement in intercultural data comparison", "content": "Next, we analysed engagement in the annotated sessions. Overall we found that the Japanese sessions exhibit a noticeably higher average level of annotated engagement compared to the European sessions (see Figure 6). As no Chinese language session engagement annotations have yet been created, we discuss only German, English, French and Japanese language data from this point on."}, {"title": "5.3.1 Engagement correlation with recorded and extracted features", "content": "We began by examining the relationship between input features and engagement within the inter-lingual dataset. Specifically, we calculated the Pearson correlation coefficient r between features and novice engagement as our primary metric. We found significant differences in the correlation of features and engagement between the NoXi and NoXi-J recordings (see Figure 4). All correlations presented in this section are statistically significant with p-values of less than 0.001. N is the total number of frame values for each feature in each language (DE=511,200, FR=666,950, EN=784,675, JP=1,043,700)."}, {"title": "5.3.2 Head nods", "content": "We calculated the frequency of head nods on the basis of length of the recorded sessions and the recognized head nod count. We found a noteworthy difference in head nod frequency between recorded data of European participants and Asian participants (see Table 4).\nThe high frequency of the session with the English-speaking par- ticipants was caused by frequent head nodding of the participants with an Asian cultural background. The utilized algorithm could not detect small head nods obscured by static noise in the facial recognition data extracted from Kinect. An investigation of the data verified that many small head nods, especially in the Japanese dataset, were not identified."}, {"title": "5.3.3 Vocal backchannels", "content": "Table 4 also shows the calculated ratio of listening and the time spent vocally backchanneling separately. French language interlocutors spent the least amount of time listen- ing while engaging the most in vocal back-channeling, with similar values observed in the English-speaking sessions. The Japanese are unique in exhibiting a very high listening ratio and a high ratio of vocal back-channeling."}, {"title": "5.3.4 Mutual engagement and speaking turns", "content": "Finally, we observed differences in the correlations between expert and novice engage- ment. In all European language sessions, the engagement of the novice and the engagement of the expert have a negative correlation coefficient (DE: r=-0.19, FR: r=-0.05, EN: r=-0.07). This suggests a slight tendency for one interlocutor's engagement to increase as the other's decreases and vice versa, although the weak correlations indicate that this mutual influence is minimal (see Figure 5). The minor adversarial effect may be explained by the difference in the average engagement of novices between when they hold the speak- ing turn compared to when they are silent, with a difference of more than 0.15 points on average for the German and English novices (see Figure 6). The Japanese session, however, shows a high positive"}, {"title": "6 ENGAGEMENT PREDICTION EXPERIMENT", "content": "In this section, we examine the effect of the correlation between engagement and non-verbal behavior (see Figure 4 and 6) on the accuracy of machine learning models in predicting engagement across different language speaker groups, hereafter called LSGs. Initially, we investigate how the differences in non-verbal behavior among different LSGs influence the accuracy of engagement pre- diction in a cross-corpus scenario (Section 6.3.1). Subsequently, we assess the transfer-ability of model knowledge (parameters) related to non-verbal behavior across different LSGs using transfer learn- ing. We discuss the adaptability of the non-verbal behavior-based estimation model across different LSGs by examining the improve- ments in accuracy facilitated by the transfer learning methodology (Section 6.3.2).\nFinally, we analyze feature importance on the engagement pre- diction with SHAP values, showing clear consistency between the results of the data analysis and the importance the models assigns to their input features (Section 6.3.3)."}, {"title": "6.1 Features", "content": "The prediction models were trained using 49 feature streams: 17 for facial action units, 20 for body properties, 3 for head angle movements, 3 for additional properties, expert engagement, and the computed features head nods, silence, vocal back-channeling, and speaking turn.\nThe models were trained with engagement annotations by frame as the target value. To minimize annotator bias, we used the av- erage of three annotators for all sessions except for 37 Japanese sessions. While these sessions are used for training the Japanese based prediction model, we do not use them for transfer learning."}, {"title": "6.2 ML model and training procedure", "content": "We designed 4-celled LSTM models using a 30-frame window, each containing values for the 49 features to predict the engagement of the following frame. The frame window represents a temporal dimension that can capture changes such as head moments. We"}, {"title": "6.3 Results", "content": ""}, {"title": "6.3.1 Cross-language corpus experiments", "content": "The models were ini- tially trained only for a single LSG. Most models perform best for the test set of the LSG they were trained on (see Table 5). The worst- performing model is the English LSG model with a loss of 0.020, which performed only slightly better on the English test set than the French LSG model with a loss of 0.021. This is not surprising, as the English LSG training set is by far the most culturally diverse, with people from over 10 cultural backgrounds participating in the recordings, whereas German and French LSG recordings only have 3 and 4 cultural backgrounds respectively. This cultural diversity most likely also contributed to the English LSG model performing best on the Japanese LSG test set out of all European LSG models with a loss of 0.026. The Japanese LSG model performed poorly on all the other models, with a loss of 0.017 for the Japanese LSG test set, and a loss of over 0.040 for all other test sets.\nWe trained a model containing all the languages with 16 test sessions, named Global model, to revise the necessity of single LSG models. We also trained a model with training data from Ger- man, English and French LSG sessions and 12 test sessions called European LSG model, as the model performance, in addition to the ANOVA results, highlights a clear distinction between the European"}, {"title": "6.3.2 Transfer learning results", "content": "We then transfer learned each model to all other models and tested them on the respective training set. The results (see Table 6) show only minor improvements for inner European LSG model transfer learning. The German LSG model improved the most after the transfer learning on the French LSG training set, from a loss of 0.020 to a loss of 0.015 on the French test set.\nOverall, transfer learning was most successful on the Japanese LSG training set, reducing the loss from 0.040-0.047 to 0.015-0.017. Transfer learning on the Japanese LSG training set also substantially improved the performance of all other models on the Japanese LSG test set, sometimes outperforming the original Japanese LSG model. Surprisingly, the Global model's performance declined after transfer learning, with the loss increasing from 0.014 before transfer learning to 0.017 afterward."}, {"title": "6.3.3 Feature analysis with SHAP values", "content": "Finally, we aimed to in- vestigate which features influenced the models' decision-making processes. We used the SHAP (SHapley Additive exPlanations) method to extract the SHAP values, which quantify the weight a model gives to each input feature, and compared them across the models (see Table 7).\nFirst, we noticed that every model used fluidity as its primary factor for engagement prediction. This is surprising, given that fluidity showed a significant lack of correlation with engagement"}, {"title": "7 DISCUSSION", "content": "We first noticed substantial differences between the European and Asian language sessions in the results of the Tukey-Kramer test. These results are in line with general findings such as Hall [26].\nWe found substantial differences in smiling frequency, especially a lower frequency in Chinese participants, similar to observations by Talhelm et al. [68] and Lu et al. [47]. Additionally, there was a slightly greater frequency of smiles in the Japanese recordings that was correlated with engagement, which is not supported by literature, as many researchers deny a high correlation between smiles and the engagement of Japanese people [49].\nThe importance of factors such as overall activation, energy of the hands, the fluidity of movement, and the expression of emotion were already recognized by Wallbott [73] and found to have a substantial impact on engagement prediction. The lack of significant correlation between the fluidity of movement and engagement in the general analysis of the data might be ascribed to the model being able to recognize patterns over its 30 frame window that were missed in a frame-wise comparison.\nThe computed head nods were not a relevant factor in engage- ment prediction. However, head movements in general were a rele- vant factor for all engagement models and target LSGs, and were most relevant for the Japanese language sessions. This is reflected in the extracted SHAP value attributed to head movement of the Japanese LSG model. While these findings are unambiguous, they do not completely reflect the literature, which suggests a strong disparity in backchanneling behavior and especially head nods in the Japanese data in comparison to the European data as described in Chapter 2.2.\nTurn-taking has been found to have a substantial influence on engagement [33]. While we noticed a considerable difference in annotated engagement for the European sessions in the average"}, {"title": "8 CONCLUSION", "content": "We introduced Noxi-J, a new addition to the publicly available multi-lingual dyadic interaction corpus NoXi, which consists of a multimodal dataset featuring Japanese and Chinese speakers. Furthermore, we investigated the cultural variations in non-verbal features and their impact on engagement across different language groups and conducted comparative analyses. Finally, we trained an LSTM model for engagement prediction to verify the insights of the data analysis.\nWe focused on computed and automatically extracted features. Although the inclusion of manually annotated features might have helped identify further culture-specific variations, the high costs made this impractical for every feature of interest. Additionally, inner-group differences, especially within the dataset of English speaking participants, highlight the potential benefits of segregat- ing the data on the basis of the participants' home culture.\nThe need for engaging and connecting with artificial systems is growing [22]. Research has revealed issues in communication between different cultures caused by non-verbal communication [44]. Comprehensive data based analyses of cultural differences in non-verbal communication and backchanneling, as conducted here, are essential for the development of culturally sensitive agents and systems. This paper serves as an introduction, providing a baseline for more optimized engagement prediction models and acting as a reference point for further research into cultural differences in AI agents."}]}