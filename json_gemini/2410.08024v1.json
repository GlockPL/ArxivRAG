{"title": "Pretraining Graph Transformers with Atom-in-a-Molecule Quantum Properties for Improved ADMET Modeling", "authors": ["Alessio Fallani", "Ramil Nugmanov", "Jose Arjona-Medina", "J\u00f6rg Kurt Wegner", "Alexandre Tkatchenko", "Kostiantyn Chernichenko"], "abstract": "We evaluate the impact of pretraining Graph Transformer architectures on atom- level quantum-mechanical features for the modeling of absorption, distribution, metabolism, excretion, and toxicity (ADMET) properties of drug-like compounds. We compare this pretraining strategy with two others: one based on molecular quantum properties (specifically the HOMO-LUMO gap) and one using a self-supervised atom masking technique. After fine-tuning on Therapeutic Data Commons ADMET datasets, we evaluate the performance improvement in the different models observing that mod- els pretrained with atomic quantum mechanical properties produce in general better results. We then analyse the latent representations and observe that the supervised", "sections": [{"title": "Introduction", "content": "Effectively representing molecules for modeling applications is a fundamental challenge in cheminformatics and machine learning, which lead to the development of various repre-sentation methods. In the realm of precomputed representations, different approaches are utilized depending on the available input data. Fingerprints, which encode the presence or absence of substructures and certain chemical properties in binary vectors, are commonly used in cheminformatics, particularly when 3D data is not available. On the other hand, physics-inspired representations like the Coulomb Matrix, Bag of Bonds, SLATM and many others are more frequently employed for representing 3D geometries and physical properties. With the advent of deep learning, the potential to learn representations directly from data has become increasingly apparent. These learned representations, shaped by the training of deep neural networks, enable the transformation of input data into a la-tent space where relevant features can be distilled in different ways for specific tasks. For instance, contrastive learning techniques have been employed to learn representations including information from other modalities such as image information and knowledge-"}, {"title": "Methods", "content": "We consider a custom implementation of Graphormer as an instance of network that belongs to the increasingly popular family of Graph Transformers (GTs), models that gen-erally utilize a transformer architecture on 2D graph input data. As a comparison for the models pretrained on atomic QM properties, besides training the model without pretrain-ing, we consider masking pretraining (atom-level self-supervised method) using the same dataset employed for the atom-resolved QM properties, and pretraining on on a much bigger dataset of a molecular property, HOMO-LUMO gap (HLG) calculated by QM meth-ods. This choice is dictated by the approximate matching of the overall number of atomic properties (molecules times non-hydrogen atoms) with the number of datapoints for the HLG (one per molecule). The pretrainings were followed by fine-tuning on individual target downstream tasks from absorption, distribution, metabolism, excretion, and toxicity (AD-MET) benchmark datasets of the Therapeutics Data Commons (TDC). They represent the key properties relevant to pharmacokinetics and pharmacodynamics of drugs. Other than comparing the results of these different pretraining strategies on the benchmark metrics, the"}, {"title": "Model description", "content": "Graphormer is a GT where the input molecule is seen as a graph with atoms as nodes and bonds as edges. This model in general works by encoding the atoms in the molecule as tokens based on their atom type, and then repeatedly applying self-attention layers with an internal bias term before the softmax function. This term is based on the topological distance matrix of the molecular graph and allows to encode the structural information of the molecular graph. In particular, the network employed in this work is an implementation of Graphormer from, inspired by the work. In this implementation the centrality encoder is adapted from using only explicit neighbours to including both explicit atoms and implicit hydrogens. As a result of the combination of this modified centrality encoding together with the usual atom type encoder, the hybridization of atoms is handled implicitly. For this reason this implementation does not present any edge encoder component. For what concerns the choice of hyperparameters, we did not run hyperparameter tuning experiments as absolute performance is not the focus of this work. We purposely chose 20 hidden layers, a higher number than usually found in similar architectures, while maintaining a number of parameters that is comparable with other Transformer-based"}, {"title": "Pretraining datasets and methodology", "content": "For pretraining, we used a publicly available dataset consisting of ~136k organic molecules for a total of over 2M heavy atoms. Each molecule is represented by a single conformer initially generated using the Merck Molecular Force Field (MMFF94s) in RDKit library. The geometry for the lowest-lying conformer was then optimized at the GFN2-xtb level of theory followed by refinement of the electronic structure with DFT (B3LYP/def2svp). The dataset reports several atomic properties: a charge, electrophilic and nucleophilic Fukui indexes, an NMR shielding constant. Another pretraining dataset, PCQM4Mv2, consists of a single molecular property per molecule, an HLG that was also calculated using quantum chemistry methods https://ogb.stanford. edu/docs/lsc/pcqm4mv2/. The dataset contains over 2M of molecules and was curated under the PubChemQC project. It is important to specify that albeit both datasets also contain the 3D molecular geometries, we only employ the 2D graph chemical structures. The pretraining on atom-level QM properties is achieved via a regression task by ap-plying a linear layer to the obtained node representations, each corresponding to a heavy (non-hydrogen) atom. The model is trained on each one of the available atomic properties"}, {"title": "Downstream Tasks", "content": "For the benchmarking of the obtained pretrained models, we used the absorption, distribution, metabolism, excretion, and toxicity (ADMET) group of the TDC dataset, consisting of 9 regression and 13 binary classification tasks for modeling biochemical molecular properties https://tdcommons.ai/benchmark/admet_group/overview/. The training and testing on this dataset is carried out in the same way as any molecular property modeling. For splittings and evaluation metrics we follow the guidelines of the benchmark group that we consider, hence we refer to. All the downstream task trainings followed the same procedure with weights taken from the respective preartained models (without freez-ing any layer) or randomly initialized for training the scratch model. No multitask training is adopted here and a different model is obtained for each split of each downstream task. For each combination of downstream task and pretraining, we obtained 5 models, corre-sponding to training/validation splits as provided in the benchmark, and reported the final performance as mean and standard deviation over this set. In summary, the non-pretrained Graphormer version used as a baseline model was compared with 7 different pretrained models: one per each of the 4 atom-resolved QM properties (atomic charges, NMR shield-"}, {"title": "Conservation of pretraining information after fine-tuning", "content": "In order to understand if the fine-tuned models preserve some of the information learned during the supervised pretraining stages or if that amounts only to a different network initialization, we analyse the latent representation obtained in the last layer. In particular, for each fine-tuning task and for each set of differently pretrained models, we freeze the model obtained from one of the seeds and encode a sample of 5000 molecules from each of the two pretraining datasets. The latent representations are split into equal size train/test sets and fit with the regularized linear regressor from to reveal to which extent the representation still preserve linear correlation with the pretraining labels. The results are reported in terms of $R^2$ coefficient over the test set averaged across the 22 fine-tuned models. We perform this analysis in an all-to-all fashion: namely for every model against every pretraining task and also considering the models trained from scratch as some correlation may arise from learning representations during the downstream tasks."}, {"title": "Latent expressivity across layers", "content": "The internal representation at each layer of the mod-els is also studied by analysing a quantity introduced in, which is related to representation rank, that measures how similar are latent token representations. If $GT_L(X) \\in R^{n\\times d}$ is the latent representation of an encoded input $X \\in R^{n\\times d}$ at layer $L$ of a GT network, this is defined as:\n$p_L = \\frac{||res(GT_L(X))||_{1,\\infty}}{||GT_L(X)||_{1,\\infty}}$\nwith $||(\\cdot)||_{1,\\infty} = \\sqrt{||(\\cdot)||_1 ||(\\cdot)||_{\\infty}}$, where $res(X) = X - \\mathbb{1}x^T$, with $x = argmin||X - \\mathbb{1}x^T||$ where $x \\in R^d$ and $\\mathbb{1} \\in R^n$. Namely, this metric measures how close is the representation to the closest representation in $||(\\cdot)||$ norm where all n latent representations are equal to the same vector x. We report the value of this quantity across all layers for every model, computed using a random sample of 100 molecules from each test set of the ADMET tasks."}, {"title": "Spectral Analysis of Attention Rollout", "content": "To have a better understanding of the mech-anism behind the pretrained models' improvements, we shift our focus on the analysis of attention weights. We aim to understand directions along which an input molecular repre-sentation is decomposed when passed through a given model. In order to do so, we start by considering the Attention Rollout matrix $A$ as a proxy for the model's action on the input (see SI for a more detailed motivation). While this is a strong approximation, it provides a number of non-trivial insights (vide infra). We operate an eigendecomposition of $A$ (from here on we will make use of the bra-ket notation):\n$A = \\sum_{i=0}^{N-1} a_i |a_i\\rangle \\langle a_i|$\nwith $a_i \\in C$ and $|a_0| \\geq |a_1| \\geq ... \\geq |a_{N-1}|$ and, based on an empirical observation on one of the pretrained Graphormers (see Fig. 1), we analyse the similarity of the eigenvectors $|a_i\\rangle \\in C^n$ with the eigenvectors of the Laplacian matrix $L$ of the input molecular graph"}, {"title": "Neighbour sensitivity analysis", "content": "Following the hypothesis that atomic QM properties provide a good description of the atomic environment around each atom, we carry out a sensitivity analysis to understand in every model how much each atomic representation in the last layer is influenced by changes in the input embedding of the kth neighbours within the same molecule. In order to do so, we compute the following quantity for 50 randomly selected molecules from the TDC test sets:\n$S_k = \\langle \\langle \\frac{1}{d}\\sum_{\\nu=0}^{d}\\frac{1}{|M|}\\sum_{\\mu=0}^{|M|}\\frac{\u2202GT(X)_{i,\\nu}}{\u2202X_{k,\\mu}}\\rangle \\rangle_{K \u2208 K i^k\\EM}$"}, {"title": "Results", "content": "Model performances obtained for the downstream tasks are summa-rized in table 1 where the best model in the tested group is highlighted in bold. We evaluate the best results based on their mean values and then perform a paired t-test to test the hypothesis that they are significantly better than the others. The models for which the null hypothesis cannot be refuted were highlighted with the only exception being the exclusion of two cases where the standard deviation is one order of magnitude higher than for all the other results. Overall, while in most cases all the pretraining strategies provide some im-provement, pretraining on HLG stands out only for one property, albeit still being among the best models in the group for four more cases. While masking pretraining also significantly outperforms other models only in one case, we find it sharing top performance with other models for ten more downstream tasks. When the models pretrained with atom-level QM properties are considered as a group, we find it to contain the best model overall (at least one better than both masking and HLG) in ten cases, and tying for best model in twenty cases out of twenty-two. Within the group one can see that models pretrained on charges, NMR shifts and all atomic QM properties provide overall a greater number of best results than models pretrained on Fukui functions. Finally, we notice that for the case of solubility, lipophilicity and acute toxicity (LD50) we obtain superior results than the respective best models in the TDC leaderboard.\nAlthough the TDC dataset provides a well established benchmark in modeling ADMET properties, the different models reported here demonstrated close performance on multiple taks. Expecting divergence of model metrics, we tested our methodology on a much larger dataset of proprietary JNJ HLM clearance data and summarized the results in table 2. The models pretrained on all atomic QM properties obtain the best results in both metrics ($R^2$ and Spearman's coefficient), followed closely by models pretrained on NMR shifts and atomic charges. Models pretrained"}, {"title": "Conservation of pretraining information after fine-tuning", "content": "The results obtained on the regularized linear regression of pretraining labels from the representations of the pre-training structures obtained with fine-tuned models are reported in Fig. 2. We report each value of $R^2$ coefficient with mean and standard deviation over the results obtained from"}, {"title": "Latent expressivity as across layers", "content": "The results of the analysis of $p_L$ across layers are summarized for the models using the three main pretraining strategies (all atom-level quantum properties, HLG and masking) and for the models trained from scratch in Fig. 3, while a similar plot comparing the models pretrained on each atom-level QM property separately is reported in the SI. It is evident from the plot that the trend of $p_L$ is very different across pretraining methods. Overall, when comparing to the models trained from scratch, all"}, {"title": "Spectral Analysis of Attention Rollout", "content": "We evaluate the metric \u03b6 defined in Eq. 4 as described in the Methods section obtaining a distribution of 22 values over the downstream tasks per each group of studied models. The results are reported in Fig. 4 as a set of swarm plots. Firstly, it is evident that the models trained from scratch present values of \u03b6 that are close to 0 indicating little to no presence of non-trivial Laplacian eigenmodes in the spectrum"}, {"title": "Neighbour sensitivity analysis", "content": "The results of the neighbour sensitivity analysis are reported in Fig. 5. For each considered group of models we report the value of $S_k$ for k \u2208 [1,...,5] in boxplots over 1100 structures sampled uniformly from the test sets of the fine-tuning tasks. It is found that the models trained from scratch exhibits a constant and low sensitivity of representation with respect to neighbouring atoms, whereas pretrained models present a reasonable descending trend with topological distance. In particular, the models pretrained on all the atomic QM properties have a stronger sensitivity than all other"}, {"title": "Discussion", "content": "Our in-depth analysis demonstrates that, among the tested strategies, pretraining the Graphormer on four atomic QM properties in the multitask fashion provides the best model for subsequent fine-tuning on ADMET properties. The final models exhibit high performance results in the TDC benchmark and, more importantly, outperform other models on a much larger dataset of JNJ HLM clearance data. The latent space analysis also positions the respective models at the top with highest values of latent expressivity, neighbour sensitivities, and graph-spectral perception. Besides, last layer representations of the respective models pretrained on four"}, {"title": "Implementation Details", "content": "The Graphormer encoder network employed in this work is composed of 20 self-attention layers, with embedding dimension of 256 and 32 heads and dimension of the feedforward component in each layer of 512. Transformer pre-norm is employed together with a dropout of 0.1. The optimizer used is AdamW with the default value of weight decay set to 0.01, and we employed mixed-precision training for both pretraining and fine-tuning. For what concerns the pretraining phase no test set was used in favour of a .8/.2 training/validation split. We employed L1 loss and a constant learning rate of 10\u00af4 with a patience of 100 epochs for early stopping across all cases. For atomic QM pretraining we employed a batch size of 100 while given the higher number of samples and the duration of training we employed a batch size of 1000 for the pretraining on HOMO-LUMO gap. No scaling of the labels is employed at this stage with the exception of NMR shielding constants where we divided by a constant factor of 100 as otherwise convergence would have been too slow. For the fine-tuning cases the hyperparameters used in each downstream task are the same: the batch size used is 32, while for what concerns the learning rate a triangular cyclic scheduling was employed with a minimum value of 2 \u00d7 10-5 and a maximum value of 2 \u00d7 10\u22124. The training is stopped with an early stopping criterion with patience of 200. The loss used for regression tasks is L1 loss, while for classification tasks a censored regression approach is taken using again L1 loss with right censor set at 0 for negative examples and left censor set at 1 for positive examples. For what concerns regression labels, given the diversity of the tasks we opted for a standard scaling. We conclude this section providing the dataset dimension for each individual downstream task dataset in table 1."}, {"title": "Motivation for rollout as model proxy", "content": "If we want to do a spectral analysis of the considered network (i.e. which combination of input components are extracted), the choice of Attention Rollout is a rather natural one. This matrix, in fact, is a good proxy to understand how the information coming from the input is mixed throughout the network. One can retrieve this intuition by considering the action of a Self Attention Network (SAN) architecture with skip connections at layer lon the input $X_{l-1} \\in R^{n\\times d}$ as formalized in.? This update can be written as:\n$X_l = X_{l-1} + A_lX_{l-1}H^T$,\nwhere $A_l \\in R^{n\\times n}$ is the input-dependent attention matrix at layer l (hence dependent on $X_{l-1}$) and $H \\in R^{d\\times d}$ is the combination of the value matrix $W_v$ and the projection head $W_{proj}$ (namely $H = W_v W_{proj}^T$) and does not depend on the input $X_l$ for any l. If one is only"}, {"title": "Spectral properties", "content": "Now that we motivated the use of the Attention Rollout matrix as a proxy for the transformer network, we look at its eigenvalues and eigenvectors in order to understand along which directions the input X is decomposed. For simplicity we will make use of the bra-ket notation. Given the Rollout matrix A (we drop the subscript relative to network depth for simplicity of notation) we can write:\n$A = \\sum_{i=0}^{N-1} a_i |a_i\\rangle \\langle a_i|$\nwhere $a_i$ are the eigenvalues of A and where $|a_i\\rangle \\langle a_i|$ are the projectors on the associated eigenvectors $\\langle a_i|$. It is important to state here that A are in general non-symmetric positive definite matrices, hence their eigenvectors can be non-orthogonal and their eigenvalues are complex numbers with positive real parts. As additional observation, considering in the"}, {"title": "Filtered convolution-like behaviour and connection with graph signal oversmoothing", "content": "More in detail, one can analyse the matrix of the normalized eigenvectors overlap $C_{ij} = \\langle a_i | l_j \\rangle$, and consider the spectrum {$\\alpha_i$} as the weight associated to this decomposition. If we now denote with U the set of i for which $max_j C_{ij} > 0.9$ (threshold for overlap close to 1), then provided $\\eta = \\frac{\\sum_{i \\in U}\\alpha_i}{\\sum_{i=0}^{N-1}\\alpha_i} \\sim 1$ we can write the action of the network proxy A on the input |x\u3009 as:\n$A|x\\rangle \\sim \\sum_{i\\in U} \\alpha_i |l_i\\rangle \\langle l_i |x \\rangle,$\nwhich is interpretable as a filtered graph convolution where the relevant rollout eigenvalues {$\u03b1_i : i \u2208 U$} play the role of filter. Notice that if this approximation is valid ($\u03b7 \\sim 1$), we can see the number of Laplacian eigenmodes considered in this sum as the bandwidth of a graph convolutional filter. Notice that independently of wether this approximation is valid or not (that is to say if there is some part of the spectrum that is not related to Laplacian eigenmodes) the set of {$\u03b1_i : i \u2208 U$} can be associated to the amount of graph information captured by the model, effectively meaning that \u03b6 can also be seen as measure of oversmoothing?? (or lack thereof) in the sense of graph-spectral information for these"}, {"title": "Latent expressivity for single atomic property pretrainings", "content": "In Fig. 1 we we report the analysis of pLas defined in the main text, for all the models pretrained on each single atomic quantum property. We can see how models pretrained on nmr and charges have a similar trend across layers as model pretrained on all atomic properties, while models pretrained on fukui functions, while providing a similar trend, achieve a lower maximum value in the initial part of the network."}]}