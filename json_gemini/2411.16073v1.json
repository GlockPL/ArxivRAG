{"title": "SOFT-TRANSFORMERS FOR CONTINUAL LEARNING", "authors": ["Haeyong Kang", "Chang D. Yoo"], "abstract": "Inspired by Well-initialized Lottery Ticket Hypothesis (WLTH), which provides suboptimal fine-tuning solutions, we propose a novel fully fine-tuned continual learning (CL) method referred to as Soft-TransFormers (Soft-TF). Soft-TF sequentially learns and selects an optimal soft-network or subnetwork for each task. During sequential training in CL, Soft-TF jointly optimizes the weights of sparse layers to obtain task-adaptive soft (real-valued) networks or subnetworks (binary masks), while keeping the well-pre-trained layer parameters frozen. In inference, the identified task-adaptive network of Soft-TF masks the parameters of the pre-trained network, mapping to an optimal solution for each task and minimizing Catastrophic Forgetting (CF) - the soft-masking preserves the knowledge of the pre-trained network. Extensive experiments on Vision Transformer (ViT) and CLIP demonstrate the effectiveness of Soft-TF, achieving state-of-the-art performance across various CL scenarios, including Class-Incremental Learning (CIL) and Task-Incremental Learning (TIL), supported by convergence theory. The public code is available at https://github.com/ihaeyong/Soft-TF.", "sections": [{"title": "1 INTRODUCTION", "content": "Continual Learning (CL), also known as Lifelong Learning (Thrun, 1995; Rusu et al., 2016; Zenke et al., 2017; Hassabis et al., 2017), is a learning paradigm where a series of tasks are learned sequentially. The principle objective of continual learning is to replicate human cognition, characterized by the ability to learn new concepts incrementally throughout one's lifespan. An optimal continual learning system could facilitate a positive forward and backward transfer, leveraging the knowledge gained from previous tasks to solve new ones, while also updating its understanding of previous tasks with the new knowledge. However, achieving continual learning is challenging due to the occurrence of catastrophic forgetting or catastrophic interference (McCloskey & Cohen, 1989), a phenomenon where the performance of the model on previous tasks deteriorates significantly when it learns new tasks. This can make it challenging to retain the knowledge acquired from previous tasks, ultimately leading to a decrease in overall performance. To address the issue of catastrophic forgetting during continual learning, numerous conventional approaches have been proposed on Convolutional Neural Networks (CNNs), which can be broadly classified as follows: (1) Regularization-based methods (Kirkpatrick et al., 2017a; Chaudhry et al., 2020; Jung et al., 2020; Titsias et al., 2020; Mirzadeh et al., 2021) aim to keep the learned information of past tasks during continual training aided by sophisticatedly designed regularization terms, (2) Rehearsal-based methods (Rebuffi et al., 2017; Riemer et al., 2018; Chaudhry et al., 2019a;b; Saha et al., 2021) utilize a set of real or synthesized data from the previous tasks and revisit them, and (3) Architecture-based methods (Mallya et al., 2018; Serr\u00e0 et al., 2018; Li et al., 2019; Wortsman et al., 2020; Kang et al., 2022; 2023) propose to minimize the inter-task interference via newly designed architectural components.\nDeveloping neural network models that leverage large-scaled pre-trained models. i.e., Vision Transformer (ViT) (Dosovitskiy et al., 2020) and Contrastive Language-Image Pre-training (CLIP) (Radford et al., 2021) leads to a new paradigm shift referred to as (4) Prompt-based methods in Continual Learning (CL). Prompt-based methods learn continual representations to provide fixed pre-trained transformers with additional instruction. Notably, while L2P (Wang et al., 2022c) stands out as the seminal work that bridges the gap between prompting and continual learning, DualPrompt (Wang et al., 2022b) introduces an innovative approach to affixing complementary prompts to the pre-trained backbone, thereby enabling the acquisition of both task-invariant and task-specific instructions."}, {"title": "2 RELATED WORKS", "content": "Continual Learning (McCloskey & Cohen, 1989; Thrun, 1995; Kumar & Daume III, 2012; Li & Hoiem, 2016) is the challenge of learning a sequence of tasks continuously while utilizing and preserving previously learned knowledge to improve performance on new tasks. Four major approaches have been proposed to tackle the challenges of continual learning, such as catastrophic forgetting. One such approach is Regularization-based approaches (Kirkpatrick et al., 2017a; Chaudhry et al., 2020; Jung et al., 2020; Titsias et al., 2020; Mirzadeh et al., 2021), which aim to reduce catastrophic forgetting by imposing regularization constraints that inhibit changes to the weights or nodes associated with past tasks. Rehearsal-based approaches (Rebuffi et al., 2017; Chaudhry et al., 2019a;b; Saha et al., 2021; Deng et al., 2021; Sun et al., 2023; Sarfraz et al., 2023; Mai et al., 2021; Lin et al., 2023; Aljundi et al., 2019; Caccia et al., 2021; Chaudhry et al., 2019c; Liang & Li, 2024; Buzzega et al., 2020) store small data summaries to the past tasks and replay them during training to retain the acquired knowledge. Some approaches in this line of work (Shin et al., 2017; Aljundi et al., 2019) accommodate the generative model to construct the pseudo-rehearsals for previous tasks. Architecture-based approaches (Mallya et al., 2018; Serr\u00e0 et al., 2018; Li et al., 2019; Wortsman et al., 2020; Kang et al., 2022; 2023; 2024b;a) use the additional capacity to expand (Xu & Zhu, 2018; Yoon et al., 2018), dynamic representation (Yan et al., 2021; Singh et al., 2020) or isolate (Rusu et al., 2016) model parameters, preserving learned knowledge and preventing forgetting. Rehearsal and architecture-based methods have shown remarkable efficacy in suppressing catastrophic forgetting but require additional capacity for the task-adaptive parameters (Wortsman et al., 2020) or the replay buffers. Recently, Prompt-based approaches, an emerging transfer learning technique, harnesses a fixed function of pre-trained Transformer models. This empowers the language model to receive additional instructions for enhancing its performance on downstream tasks. Notably, while L2P (Wang et al., 2022c) stands out as the seminal work that bridges the gap between prompting and continual learning, DualPrompt (Wang et al., 2022b) introduces an innovative approach to affixing complementary prompts to the fixed pre-trained backbone. Here, we introduce a new approach to update the fixed pre-trained parameters through learnable sparse networks under the convergence theory, maximumly enabling the acquisition of task-invariant and task-specific instructions."}, {"title": "3 PREREQUISITES", "content": "We start with conventional prompt-based continual learning methods using Vision Transformer (ViT) (Dosovitskiy et al., 2020) and Contrastive Language-Image Pre-training (CLIP) (Radford et al., 2021) in Class Incremental Learning (CIL) and Task Incremental Learning (TIL) scenarios."}, {"title": "3.1 PRELIMINARIES", "content": "Problem Statement. Continual Learning (CL) involves training deep neural networks (DNN) on time-variant data represented as a sequence of tasks, $D = {D_1,\u2026,D_T}$. Each t-th task, $D_t = {(x_i,y_i)}_{i=1}^{n_t}$ consists of $n_t$ tuples where $x \\in X$ is an input sample and $y \\in Y_t$ is the corresponding label. When a task $X_t$ arrives, a model $f_\\theta$ is trained for the current task, while data from previous tasks is inaccessible. This work focuses primarily on class incremental learning (CIL), in which the task-ID is not given during inference.\nSoft & Subnetworks have been explored in continual learning through two notable approaches. One approach, known as supermasks (Wortsman et al., 2020), produces outputs by $p = f(x, w \\odot m)$, where $\\odot$ denotes elementwise multiplication. In this method, the weights $w$ remain fixed at their initialization, with bias terms set to 0 and other parameters initialized to $\\pm c$ with equal probability where the constant c is the standard deviation of the corresponding Kaiming normal distribution (He et al., 2015). Another line of work includes WSN (Kang et al., 2022) and SoftNet (Kang et al., 2023), which jointly learn the model weights $w$ and task-adaptive subnetworks $m$. The parameter-efficient reusable subnetworks are obtained by iteratively selecting the top-c% of the weights based on an importance score $s$ at each layer. WSN has primarily demonstrated its effectiveness in Convolutional Neural Networks (CNNs). However, its pruning mechanism for pre-trained Transformers, such as ViT, remains unexplored. To discover the competitive sparseness in Transformers, we detail the WSN-style task-adaptive fine-tuning and the learnable soft-networks $m$ of Transformers, presenting these adaptations for the first time with empirical observations. The soft-networks originate from learned parameters distributed with $\\mu \\approx 1.0$ & various variances, as stated in Figure 6."}, {"title": "3.2 PROMPT-BASED CLASS INCREMENTAL LEARNING (CIL)", "content": "A simple yet effective prompt-based (prompt-tuning) CIL model: Learning to Prompt (L2P) (Wang et al., 2022c) is first proposed. In this model, a prompt $p$, a tiny set of trainable tokens combined with image features, is fed into the Vision Transformer (ViT) to help the model resist forgetting. To select suitable prompts for task-specific training, L2P utilizes a prompt pool $P$ containing numerous prompt-key pairs, ${p_t, k_t}_{t=1}^{T}$, where $p_t \\in R^{L \\times D}$ represents the t-th task prompt, $k_t$ represents the t-th coresponding task key, and $T$ is the total number of prompt-key pairs."}, {"title": "4 TRANSFORMER WITH LEARNABLE SUBNETWORKS", "content": "In this section, we explain how Soft-TransFormers (Soft-TF) leverage learnable soft-networks to train sequential tasks while keeping the well-pretrained model parameters fixed. To introduce our novel Soft-TF and provide a clearer understanding, we draw on a partial explanation of DualPrompt."}, {"title": "4.1 SOFT-MSA LAYERS", "content": "To address the task-specific fine-tuning of the pre-trained model, such as ViT, this work proposes a new Soft-TransFormer (Soft-TF), as illustrated in Figure 1. The proposed Soft-TF consists of a conventional neural network, like a multilayer transformer with multihead attention and forward networks. Using well-trained transformer parameters, we could discover task-specific soft-networks, as depicted in Figure 3. The Soft-TF incrementally learns model weights and task-adaptive soft-masks with well-pre-trained and soft-network parameters $m$.\nGiven a pre-trained parameter $\\theta$ and learnable soft-parameters $m$, Soft-ViT is represented as $f_{\\theta \\odot m}$, consisting of $N$ consecutive soft-MSA layers. We extend the notation by denoting the input embedding feature of the $l^*$-th learnable soft-MSA layer as $h^{(l^*)}$, where $l^* = 1, 2, . . ., N$, and $l^*$ can refer to either the G-Prompt layer $l_g$ or the E-Prompt layer $l_e$. Note that while the pre-trained parameters $\\theta$ remain fixed, the soft-parameters $m$ are updated to provide task-specific solutions.\nG-prompt. $g \\in R^{L_g \\times D}$ with sequence length $L_g$ and embedding dimension $D$, is a shared parameter for all tasks. G-Prompt is attached to the $l_g$-th MSA layer to transform $h^{(l_g)}$ via a prompting function as follows:\n$h^{(l_g)} = f_{prompt} (g,h^{(l_g)}),$"}, {"title": "4.2 PROMPTS WITH LEARNABLE SUBNETWORKS", "content": "G- and E-prompts, along with learnable soft-networks, encode specific types of instructions during training with the backbone and work together to guide the model's predictions during inference. We have demonstrated the method for attaching prompts and learnable soft-networks to a single soft-MSA layer. Similarly to the approach taken in DualPrompt (Wang et al., 2022b), we also investigate layers of E-prompts with learnable soft-networks m, while utilizing the layers designated for G-prompts.\nLayers of G- and E-Prompts. We use the multilayered extension of both types of prompts: $g = {g^{(l_g)}}_{start_g}^{end_g}$, where $g^{(l_g)} \\in R^{L_g \\times D}$ represents the G-prompt attached to the $l_g$-th MSA layer. Similarly, we define $e_t = {e_t^{(l_e)}}_{start_e}^{end_e}$ for the $l_e$-th conventional MSA layer. In this configuration, the G-prompt $g^{(l_g)}$ is attached from the $start_g$-th to the $end_g$-th conventional MSA layers, and the E-prompt $e_t^{(l_e)}$ is attached to the $[start_e, end_e]$-th soft-MSA layers, ensuring that there is no overlap between them. In our experiments, we follow the $l_g \\neq [start_e, end_e]$ ($l_g = [1,2]$) settings used in DualPrompt and empirically search for the optimal $[start_e, end_e]$ layers for the learnable subnetworks through ablation studies.\nLearnable Soft-networks. The prompting function $f_{prompt}$ determines how prompts (p) are combined with fine-tuned soft ($\\theta \\odot m$) embedding features. From another perspective, $f_{\\theta \\odot m}^{f_{prompt}}$ directly influences the interaction between high-level instructions in the prompts and low-level representations. Therefore, we believe that a well-designed prompting function, along with task-specific parameters, is crucial for optimizing overall continual learning performance.\nSpecifically, applying a prompting and fine-tuning function $f_{prompt}$ can be seen as modifying the inputs to the soft-MSA layers. Let the input to the soft-MSA layer be $h \\in R^{L \\times D}$, and denote the input query, key, and values for the soft-MSA layer as $h_Q$, $h_K$, and $h_V$, respectively. A soft-MSA layer is defined by the following equation:\n$MSA(h_Q, h_K, h_V) = Concat(h_1,\u2026\u2026,h_i,\u2026\u2026,h_n)w^O \\odot m^O$,\nwhere $h_i = Attention(h_Q(w_Q \\odot m_Q), h_K(w_K \\odot m_K), h_V(w_V \\odot m_V))$, where $w_Q, w_K, w_V, and $w_O are fixed projection matrices while $m_Q, m_K, m_V, and $m_O are learnable parameters. s is the number of heads. In ViT, $h_Q = h_K = h_V$. Here, we define a unified prompt parameter with a sequence length of $L_p$, such as $p \\in R^{L_p \\times D}$ for a single-layered G- or E-prompt."}, {"title": "4.3 FINE-TUNING ON WELL-INITIALIZED PARAMETERS", "content": "In this framework, we concatenate the prompts $p_t$ and the embedding sequence $x_t$, i.e., inputs from t-th task, along the embedding dimension: $z_t = [p_t; x_t]$. With the weights of $w^Q \\odot m^Q, w^K \\odot m^K, w^V \\odot mV$, the soft-transformer takes query $(q_t = (w_Q \\odot m_Q)z_t)$ and key $(k_t = (w_K \\odot m_K)z_t)$ as input of the soft-MSA layer. The soft-attention matrix is then given by:\n$a_t = softmax (\\frac{q_t k_t^T}{\\sqrt{D/n}})$, where we focus on $q_t = (w_Q \\odot m_Q)z_t$ and $z_t = (w_K \\odot m_K)^T z_t^T$. First, the trainable prompt parameters can be denoted as:\n$z_{t^Q} =  [p_t x_t]$, $P_{t^Qz_{tz}}= [p_tx_t^T ]$"}, {"title": "5 EXPERIMENTS", "content": "We validate our method on several benchmark datasets against continuous learning baselines in Class-Incremental Learning (CIL) and Task-Incremental Learning (TIL)."}, {"title": "5.1 EXPERIMENTAL SETTINGS", "content": "Datasets. We evaluate our method mainly on 1) 10/20-Split-CIFAR100 (Krizhevsky et al., 2009), constructed by splitting the 100 classes into 10 tasks/20 tasks. 2) 10-Split-TinyImageNet (Abai & Rajmalwar, 2019), constructed by splitting the 200 classes into 10 tasks. 3) 10-Split-ImageNet-R (Hendrycks et al., 2021), constructed by splitting the 200 classes into 10 tasks. To show our effectiveness, we additionally compare our method with the baselines on 5-Split-CUB200 and 10-Split-TinyImageNet. The detailed experimental settings are depicted in the Supplementary.\nImplementation. For fair comparisons, we set L2P (Wang et al., 2022c), DualPrompt (Wang et al., 2022b), CLIP (Radford et al., 2021), and PGP (Qiao et al., 2024) as our baselines. We follow experimental settings Qiao et al. (2024) entirely.\nBaselines. To validate the powerfulness of our method, we compare our results with various CIL baselines including ICaRL (Rebuffi et al., 2017), BiC (Wu et al., 2019), DER++ (Buzzega et al., 2020), LWF (Li & Hoiem, 2017), EWC (Kirkpatrick et al., 2017b), DER+MCG (Cai et al., 2023), and DualPrompt-PGP (Qiao et al., 2024). In addition, we investigate subnetwork solutions such as WSN (Kang et al., 2022) (obtained by selecting top-c% of weight scores while fixing pre-trained parameters) and SoftNet (Kang et al., 2023) (acquired by selecting top-c% of weight scores as major tickets while setting 100.0 \u2013 top-c% as minor tickets) in Vision Transformers (ViT) using prompt tuning methods. We adopt average accuracy (ACC) and forgetting (FOR) as our validation metrics (Wang et al., 2022b; Qiao et al., 2024).\nTask Inference. At the inference time, we infer task identity for arbitrary pieces of task samples x for finding the proper task nuances and demonstrating full fine-tuning results. We summarize the following two methods:\n\u2022 Prompt ID: For a test example x, we simply choose the best matched task index via $argmin\\gamma(q(x), k_t)$.\n\u2022 Gradient ID: To infer the task identity, we follow SupSup's one-shot task inference (Wortsman et al., 2020). In short, we assign each learned subnetwork $m_t$ a weight $a_t$ such that $\\sum_t a_t = 1$ and $a_t = 1/T > 0$ when evaluating all seen tasks. Given an example data point of batch x \\in b to classify, we can compute the loss as $L = H(f_{\\theta\\odot m}t^{f_{prompt}}(x))$ where $f_{\\theta\\odot m}t^{f_{prompt}}(x)$ is the pre-trained model which outputs logits and $H$ is the entropy function. From here our inferred task is simply $t = argmin_{a_tm_t}^{H_{\\theta}(atm_t)}$."}, {"title": "5.2 PERFORMANCES", "content": "Performances of Soft-TF on CIL. We compare our Soft-TransFormers (Soft-TF) with state-of-the-art CIL baselines, as shown in Table 1. Our Soft-TF significantly outperforms all baselines and upper-bounds of Soft-TF, including L2P and DualPrompt, in both accuracy and forgetting measurements. The performance gain of Soft-TF is especially notable in DualPrompt-based learning compared to"}, {"title": "6 CONCLUSION", "content": "Inspired by Well-initialized Lottery Ticket Hypothesis (WLTH) that provides suboptimal fine-tuning solutions, we proposed a novel fully fine-tuned continual learning (CL) method referred to as Soft-TransFormers (Soft-TF), which sequentially learns and selects an optimal soft-network or subnetwork for each task. In training, Soft-TF jointly learned the sparse layer's weights in CL to obtain task-adaptive soft(real-valued)-networks or subnetworks (binary masks) while freezing the well-pre-trained layer parameters. In inference, the identified task-adaptive network of Soft-TF, which masks the parameters of the pre-trained network, maps to an optimal solution associated with each task, minimizing Catastrophic Forgetting (CF)\u2014the soft masking was immune to the pre-trained network's knowledge forgetting. Extensive experiments demonstrated the power of Soft-TF (Vision Transformer and CLIP) and show state-of-the-art performances with convergence theory in various CL scenarios, i.e., Class-Incremental Learning (CIL) and Task-Incremental Learning (TIL)."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 ANALYSIS OF SOFT-TRANSFORMERS (SOFT-TF)", "content": "Analysis of Soft-TransFormers for Convex-Lipschitz Functions. To analyze the convergence rate of the Soft-TransFormers (Soft-TF), we limit ourselves to the case of convex-Lipshitz functions along with the analysis (Shalev-Shwartz & Ben-David, 2014). Let $w^* = {g^*, e^*, m^*}$ be any vector or an optimal solution and let B be an upper bound on $||w^*||$ when $w^{(1)} = 0$. It is convenient to think of $w^*$ as the minimizer of $f(w)$, but the analysis that follows holds for every $w^*$.\nWe would like to obtain an upper bound on the sub-optimality of our solution with respect to $w^*$, namely, $f(w) \u2212 f(w^*)$, where $w = \\frac{1}{T} \\sum_{t=1}^{T} w^{(t)}$. From the definition of $w$, and using Jensen's inequality, we have that\n$f(w) \u2212 f(w^*) = f (\\frac{1}{T}\\sum_{t=1}^{T} w^{(t)} ) \u2212 f(w^*)$\n$\\leq \\frac{1}{T} \\sum_{t=1}^{T} (f(w^{(t)})) \u2212 f(w^*)$\n$= \\frac{1}{T} \\sum_{t=1}^{T} (f(w^{(t)} \u2212 f(w^*))$. For every t, because of the convexity of f, we have that\n$f(w^{(t)}) \u2212 f(w^*) \\leq (w^{(t)} \u2013 w^*, \\nabla f(w^{(t)}))$ Combining the preceeding we obtain\n$f(w^{(t)}) \u2212 f(w^*) \\leq \\frac{1}{T} \\sum_{t=1}^{T} (w^{(t)} \u2013 w^*, \\nabla f(w^{(t)})$ To bound the right-hand side we rely on the following lemma:"}, {"title": "A.2 EXPERIMENTAL DETAILS", "content": "For fair comparisons with the baselines (Wang et al., 2022c;b; Qiao et al., 2024), we use ViT B/16 (Dosovitskiy et al., 2020) pre-trained on ImageNet-21K as our image encoder, which is kept frozen during training. We train and test on a single Quadro RTX 8000-48GB GPU for baselines and our Soft-TransFormers with Adam optimizer with $\u03b2_1 = 0.9$ and $\u03b2_2 = 0.999$.\nWe adhere to the experimental settings outlined by Qiao et al. (2024) to validate our method's effectiveness. When comparing our approach with L2P-PGP and Soft-Transformer on the 10/20-Split-CIFAR100 and 10-Split-TinyImageNet datasets, we train the network for 5 epochs with a batch size of 16 and set the prompt length to 5. For the 10-Split-ImageNet-R dataset, we use 50 epochs, a batch size of 16, and a prompt length of 30. In comparison with DualPrompt-PGP and Soft-TransFormers on the 10/20-Split-CIFAR100 dataset, we train the network for 20 epochs with a batch size of 24 and set the expert prompt length to 5. For the 10-Split-TinyImageNet dataset, we use 5 epochs, a batch"}]}