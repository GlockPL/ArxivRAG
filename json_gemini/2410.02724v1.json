{"title": "LARGE LANGUAGE MODELS AS MARKOV CHAINS", "authors": ["Oussama Zekri", "Ambroise Odonnat", "Abdelhakim Benechehab", "Linus Bleistein", "Nicolas Boull\u00e9", "Ievgen Redko"], "abstract": "Large language models (LLMs) have proven to be remarkably efficient, both across a wide range of natural language processing tasks and well beyond them. However, a comprehensive theoretical analysis of the origins of their impressive performance remains elusive. In this paper, we approach this challenging task by drawing an equivalence between generic autoregressive language models with vocabulary of size T and context window of size K and Markov chains defined on a finite state space of size O(TK). We derive several surprising findings related to the existence of a stationary distribution of Markov chains that capture the inference power of LLMs, their speed of convergence to it, and the influence of the temperature on the latter. We then prove pre-training and in-context generalization bounds and show how the drawn equivalence allows us to enrich their interpretation. Finally, we illustrate our theoretical guarantees with experiments on several recent LLMs to highlight how they capture the behavior observed in practice.", "sections": [{"title": "1 INTRODUCTION", "content": "The fields of machine learning and artificial intelligence have recently seen significant progress with the introduction of large language models (LLMs) (Brown et al., 2020; Touvron et al., 2023a), built on the transformer architecture (Vaswani et al., 2017). These models, trained on vast amounts of data, have been applied in many natural language processing tasks, including machine translation (Brown et al., 2020), text generation, question answering (Roberts et al., 2020), and sentiment analysis (Zhang et al., 2023a). Although successful in practice, the origins of the impressive performance of LLMs remain elusive, as there is no widely accepted agreement in the scientific community on how they achieve remarkable reasoning capabilities that go far beyond their training data (Brown et al., 2020).\nThis work takes a step towards bridging the knowledge gap mentioned above by providing an explicit characterization of the LLM's inference capabilities. For this, we adopt an intuitive, yet overlooked, approach that interprets LLMs as Markov chains operating on a finite state space of sequences and tokens (see Fig. 1). A key insight is that despite the seeming infinity of LLMs generating capacity, they have a limited vocabulary and context window making all their possible input and output sequences enumerable. We show that despite the prohibitively large size of the latter set, it exhibits a structure that makes it amenable to theoretical analysis. We further generalize recent theoretical advances on the generalization of LLMs and leverage our proposed point of view to provide a more insightful interpretation of them.\nMarkov chains and Large Language Models. While none of the prior works considered the equivalence between autoregressive models and Markov chains presented in this work\u00b9, some used the Markovian generative process to better understand the intrinsic capabilities of the transformer"}, {"title": "2 BACKGROUND KNOWLEDGE", "content": "We recall some elementary facts about Markov chains (Roberts & Rosenthal, 2004; Paulin, 2015) and LLMs. More notations and background materials are available in Appendices A to C.\nMarkov chains. Let \u03a9 be a discrete finite set of size 2. A discrete-time, time-homogeneous Markov chain MC(N, Q) defined on a state space n = {x}2 with transition matrix Q \u2208 R||\u00d7|| with entries Qij = Q(xi, xj) \u2208 [0, 1] is a sequence of random variables (X1, X2, . . .) taking values in \u03a9 such that for any n \u2208 N and (x1,...,xn+1) \u2208 \u03a9n+1, we have\nP(Xn+1 = Xn+1 | Xn = Xn, ..., X\u2081 = x1) = P(Xn+1 = Xn+1 | Xn = Xn) =: Q(xn, Xn+1).\nA distribution \u03c0on \u03a9 is said to be a stationary distribution if Q\u03c0 = \u03c0. Under mild conditions on Q, MC(\u03a9, Q) has a unique stationary distribution to which it converges, i.e., for any x\u0395\u03a9,\nlimn\u2192\u221e drv (Qn(x,\u00b7), \u03c0) = 0, where Q(x) denotes the probability of Xn conditioned on X\u2081 = x and the total variation between two distributions P and Q, defined on (\u03a9, F), is\ndrv (P,Q) := sup|P(A) \u2013 Q(A)|.\nAEF\nWe recall that the mixing time tmix(s) of a Markov chain is the minimal time needed to be \u025b-close to its stationary distribution (see Definition C.8). Intuitively, a Markov chain mixes slowly when it remains close to the initial state after a given number of steps and doesn't explore its state space. A Markov chain that exhibits a fast mixing time on the contrary quickly forgets its initial state and transitions more easily to a wider set of states.\nAutoregressive models. Let V denote a dictionary of size T used to encode an arbitrary sequence into a sequence of predefined tokens belonging to V. We assume that our model admits a maximum of K tokens as input, referred to as the context window of the model. The domain of the autoregressive model is the set of all sequences consisting of elements from V with up to K elements. We denote this by VK, which represents a restriction of Kleene closure of V, i.e., Vx := {v \u2208 V*, |v| \u2264 K} with the length of v. We define an autoregressive model with trainable parameters O as a function fK: V \u2192 (V), where \u2206(V) is the probability simplex over V, that given a sequence of tokens v outputs a probability distribution over the whole state space indicating the likelihood for each of its elements to appear after v (see Appendix B for more details). We consider a setting where the"}, {"title": "3 AUTOREGRESSIVE MODELS AS MARKOV CHAINS", "content": "We formally define a Markov chain that explicitly captures the full inference capacity of a given autoregressive model fe. We build upon a high-level idea that associates a tokenized input prompt with a state vi, from which we transition to a new state vj = [vi, v] by concatenating the token v predicted by an LLM to it. We then provide a theoretical characterization of this Markov chain highlighting its intriguing properties and asymptotic behavior.\n3.1 MARKOV CHAIN FORMALIZATION\nWe begin by defining the transition matrix associated with an autoregressive model fok.\nProposition 3.1. Any autoregressive model fT,K\nJe\n\u0398\ncan be equivalently represented by a Markov chain MC(V, Qf), with a sparse transition matrix Qf \u2208 R|Vk|\u00d7|Vk| defined as:\n\nvi, Vj \u2208 VK, Qf(Vi, Vj) =\n\n{\n\n0,\nif \u2203l \u2208 {1,..., |vi|}, s.t. (Vi)1+1 \u2260 (vj)1,\n((i)); tervise\n\nThe proportion of non-zero elements in Qf is (T \u2013 1)/(\u0422\u041a \u2212 1).\nWe discuss the intuition behind the definition of Qf pro-"}, {"title": "Proposition 3.2.", "content": "Let MC(V, Qf) be a Markov chain defined in Proposition 3.1. Then MC(VK, Qf) is an ergodic unichain and has a unique stationary distribution.\nA unichain is a chain that has at most one recurrent class plus some additional transient states. From Proposition 3.1, we note immediately that green blocks in Fig. 1 represent transient classes, meaning that applying Qf to the input prompt, represented by a one-hot encoding of size |V|, will transition to a state that corresponds to a sequence of length increased by one with an additional, most likely, token appended to it. This process is repeated if the model is called further on: we append tokens until we reach the context window limit K. At this point, we reach the recurrent class, represented in blue, in which the chain stays until it reaches its unique stationary distribution. We now characterize how many times one should apply Qf to the input to reach the stationary distribution.\nProposition 3.3. Given an ergodic finite-state unichain MC(V, Qf) and e = (1, 1, . . ., 1)\u0422, then limn\u2192\u221e Q7 = en where \u3160 is the stationary distribution of the recurrent class R of states, expanded by O's for each transient state of the unichain. Moreover, for all n \u2265 K,\n|(Q7)i,j \u2013 (en)i,j] < (1 \u2013 2\u2208) []-1,\nwhere \u025b = min {(QF)i,j} > 0.\ni, jER2\nThe stationary distribution is the long-term equilibrium of the Markov chain defined by the LLM and can be interpreted as a proxy of its understanding of natural language in its token space. It is independent of the initial state (i.e., input prompt) but rather captures the absolute frequencies of occurrences of certain tokens seen during pre-training. For a well-performing model, it is hence likely to be heavy-tailed, meaning that rare states have a non-zero probability of occurring due to language's ambiguity and complexity. Proposition 3.3 shows that reaching the stationary distribution requires more generation steps for models with larger context window K. Additionally, convergence depends on \u025b (that is, the smallest element of the Kth power of the transition matrix), which is related to the ability of the chain to explore the state space after having forgotten the input prompt."}, {"title": "3.2 ILLUSTRATION \u039f\u039d \u0391 \u03a4\u039f\u03a5 MODEL", "content": "We illustrate the results of Section 3 on a toy model trained on a sequence of Os and 1s. Here, each subsequent token is 0 if the sum of three previous tokens is even and 0 otherwise. Therefore, T = 2 and k = 3. We generate a sequence of 40 digits, resulting in 37 distinct supervised examples, and train a small \u201cGPT-like\u201d model (Karpathy, 2023) on it. We extract the logits from the model by prompting it with all possible combinations of 0s and 1s of length less than three to obtain the transition matrix Qf \u2208 R14\u00d714 depicted in Fig. 3(a). The transition matrix's structure (e.g., presence of transient and recurrent classes) matches the one presented in Fig. 1. Fig. 3(b) displays the stationary"}, {"title": "4 GENERALIZATION BOUNDS FOR LARGE LANGUAGE MODELS", "content": "The inference of any autoregressive language model fe can be fully captured by a Markov chain with a finite transition kernel Qf defined as above. This, in turn, allows us to see and study the generalization of fe as its capacity to infer correctly all the elements of Qf that approximate a true reference matrix of transition probabilities Q*. The hardness of this task lies in achieving precise inference having observed a negligible amount of Q*'s elements during its pre-training. For GPT-3 (Brown et al., 2020), this represents 5 \u00d7 1011 training tokens, which pales in comparison with the number of non-zero elements in Qf, given by TK+1 \u2248 109632.\nRisk definition. We denote by X = (X1,..., XN) the tokens in V that fe observes (e.g., during pre-training or at inference time). The training sequences of tokens can be written as Sn = (X1,...,Xn) if n \u2264 Kand Sn = (Xn-K+1,..., Xn) otherwise due to the deletion process (see Definition B.2). In particular, the Sn are elements of V. For any n \u2208 [N], the true probability of next token Xn+1 given a past sequence Sn is defined as Pc( | Sn) \u2208 \u0394\u03c4 and the probability estimated by the model writes Po( | Sn). We assume the existence of a constant co > 0 such that for any n \u2208 [N] and (x1,...,Xn+1) \u2208 \u03a9n+1,\nPc(Xn+1 = Xn+1 | Xn = Xn, ..., X1 = x1) \u2265 Co > 0.\nThis is a common assumption used previously in (Zhang et al., 2023b; Hu et al., 2024; Xie et al., 2022; Wies et al., 2024). To assess the generalization of a given model, we respectively define the"}, {"title": "Theorem 4.1", "content": "Pre-training generalization bound). Consider an LLM f\u0259 \u2208 F. We denote by \u0393 the mixing matrix of the pre-training sequences of tokens (S1,..., SNtrain). Let 0 < \u03b4 < 1, then with probability at least 1 \u03b4,\nRpre(0) \u2264 Rpre() +\nB\n\u2713 Ntrain\nlog (3)\nwhere B = 2||F|| max{log (T) + 2Bu/t, log (1/co)}1/2 is a constant depending on the parameters of the problem.\nThe bound in Theorem 4.1 depends on the intrinsic structure of the pre-training data through the norm of the mixing matrix ||\u0413||. If the pre-training data S is a Markov chain with state space \u03a9, this norm captures exactly the mixing time of the latter, making sequences that mix at a slower pace harder to learn. Secondly, and perhaps most surprisingly, this bound becomes model-independent when max{log (T) + 2Bu/t, log (1/co)} is dominated by log (1/co) term. Hence, if Bu \u2248 O(T\u221ar), which happens in practice due to the common normalization of the unembedding layer, then the"}, {"title": "Corollary 4.2", "content": "(Depth-dependent bound). Consider an LLM f\u0259 \u2208 F := {fe | \u0472 \u2208 \u0174}. With the same assumptions as in Theorem 4.1, we have\nRpre(0) \u2264 Rpre() +\nB\n\u2713Ntrain\nlog (3)\nwhere B = 2||F|| max{log (T) + 2(Be)\u00b9/r, log (1/co)}1/2 is a constant depending on the parameters of the problem, and Be = [(1+rmB1B2)(1+BoBv)] (BtokBU)1/L.\nWe note that B exhibits an exponential dependence on the depth of the transformer, which also amplifies the hidden dimensionality (width) of the embedding layer r. This contrasts with the dependency in m, the hidden dimensionality of the MLP block, which is linear. All these factors are commonly associated with higher expressive power of transformers suggesting that they should contribute to a better minimization of Rpre() at the expense of requiring more training data. The number of heads H can be used as a counterbalance to increasing the width in the cubic term r\u00b3, suggesting that a good balance between these parameters may lead to more data-efficient models.\nCorollary 4.3 (Sample complexity). Let B be the parameter-dependent constant of Theorem 4.1 or Corollary 4.2. Let \u03b4\u2208 [0,1] and let \u0454 > 0. If Ntrain > N* := [Blog (3) and if we assume a perfect pre-training error for fe, then we have with probability at least 1 \u03b4,\nEs~Pc || Q* (S,) \u2013 Qf(S,\u00b7)||1 \u2264 \u20ac.\nThis result allows us to contextualize LLMs' ability to learn Markov chains with respect to the existing literature. To the best of our knowledge, the only existing approach with theoretical guarantees for learning Markov chains is the frequentist method: counting the number of occurrences of different states to fill in the matrix Qf. Wolfer & Kontorovich (2019) show that the sample complexity of ap- proximating Q* up to \u20ac with such approach requires at most O(max{|V**|/e2\\s,1/\\s*}) samples, where ys is a (pseudo) spectral gap of the Markov chain and \u03c0* is the smallest element of its stationary distribution. The authors state that the frequentist approach is minimax optimal (up_to logarithmic factors). Our bound has a dependence that behaves as B2 = O(max{logT + 2TV, log (1/co)}). Given that in practice T > r, it then simplifies to O(max{T/\u20ac27,1/\u20ac2}). Note that the LLMs' sample complexity is linear in the vocabulary size T, which is remarkable compared to the sample complexity of the frequentist approach, which scales as O(TK). We show in Section 5 that this is confirmed experimentally: LLM's ability to learn Markov chains exceeds the frequentist approach for Markov chains with a large state space."}, {"title": "4.2 IN-CONTEXT LEARNING OF MARKOV CHAINS", "content": "Although insightful, the analysis presented above is related to the pre-training of LLMs a process that is hard and extremely costly to reproduce in practice. Similarly, we do not have access to the ground-truth matrix Q* to reason about LLM's ability to infer it in practice. To provide theoretical results that can be confirmed experimentally, we now turn our attention to in-context learning of Markov chains: a setup where one provides an LLM with an input sequence formed by a Markov chain of size Nicl defined over a state space 2 of size d. Different from the setting of Section 4.1, we now can explicitly use a transition kernel IP of this Markov chain for the theoretical analysis by replacing Pc with it in the definition of Ricl(\u25d5) and Ricl() in Eq. (2) (see Appendix D.7 for details on the problem setup). To relate the generalization error to the pre-training error, we quantify the discrepancy between an LLM pre-trained mostly on textual data, and a hypothetical LLM with parameters in Wme that is pre-trained on a dataset of Markov chains with the same data distribution as the Markov chain used as an input during in-context inference. We define the divergence between two estimated transition matrices P1, P2 as\n\u039a(\u03981, \u03982) :=\n1\nN\nN\n\u03a3\u0395s [dtv (Po\u2081 (\u00b7 | Sn), P\u0259\u2082(\u00b7 | Sn))].\nThe operator K is akin to a distance (the separation property is only verified almost surely, see Appendix C.4 for more details). The next result, whose proof is deferred to Appendix D.7, provides a generalization bound on the in-context learning phase.\nTheorem 4.4 (In-Context Learning generalization bound). Consider an LLM f\u0259 \u2208 F. We provide as input of fe a d-state Markov chain X = (X1,...,XNiel). The sequence of subsequences of the first n terms is denoted by S = (S1,..., SNiel). S is also a Markov chain, and we denote by tmix(\u2208) its mixing time. Let tmin := info<c<1 tmix(5)(=)2. Let \u03b4 > 0. Then, with probability at least 1 \u2013 \u03b4,\ntmin\nRicl() \u2264 inf_ {Ricl(v) + K(9,0)} + B min log ().\nDEWmc\nwhere B = 2max{log (d) + 2BU/T, log (1/pmin)}1/2.\nWe first note that instead of the norm of the mixing matrix I seen before, we now have an explicit dependency on tmin, which is related to the mixing time of the input Markov chain. This, together with the availability of the ground-truth transition matrix, allows us to use Theorem 4.4 to derive and verify experimentally the scaling laws of ICL for popular LLMs. Theorem 4.4 also suggests that an LLM pre-trained on diverse data sequences different from Markov chains should exhibit a certain degree of invariance to correctly infer the transition probabilities of the latter. This is reminiscent of the domain adaptation bounds (Redko et al., 2019) that also commonly involve a distribution shift (i.e., a distance or a divergence) term that vanishes if the model is invariant to classes of transformations linking the distribution of the input data with that on which it is applied during inference. A recent success of applying LLMs to time series data (Gruver et al., 2023), for instance, suggests that this term is indeed small for certain types of data not used during pre-training."}, {"title": "5 NUMERICAL EXPERIMENTS", "content": "Theorem 4.4 provides a practically verifiable result which naturally stems from our analysis in Section 4. We then evaluate the ability of recent LLMs, namely Mistral 7Bv0.1 (Jiang et al., 2023), Llama2 7B & 13B (Touvron et al., 2023b), and Gemma 2B (Team et al., 2024) to infer transition probabilities of Markov chains in-context. We associate each state in the d-state Markov chain with a token from the set {0, . . ., d \u2013 1}, concatenated to obtain a prompt of length Nicl. Bearing in mind the differences in the tokenization mechanisms of the different models, we add comas"}, {"title": "6 CONCLUSION", "content": "This paper proposed an explicit characterization of the inference mechanism in large language models through an equivalent finite-state Markov chain. We provided an insightful theoretical analysis based on the established characterization and the ability of the LLM to infer the transition kernel approximating the true transition probabilities of language. We adapted our results to in-context learning where experiments confirm our theoretical insights. In the future, we hope that the proposed equivalence will have far-reaching implications on our understanding of LLMs and allow for a more fine-grained understanding of their expressiveness."}]}