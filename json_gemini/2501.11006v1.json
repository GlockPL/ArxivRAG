{"title": "GREEN-CODE: Optimizing Energy Efficiency in Large Language Models for Code Generation", "authors": ["Shashikant Ilager", "Lukas Florian Briem", "Ivona Brandic"], "abstract": "Large Language Models (LLMs) are becoming in-tegral to daily life, showcasing their vast potential across various Natural Language Processing (NLP) tasks. Beyond NLP, LLMS are increasingly used in software development tasks, such as code completion, modification, bug fixing, and code translation. Software engineers widely use tools like GitHub Copilot and Amazon Q, streamlining workflows and automating tasks with high accuracy. While the resource and energy intensity of LLM training is often highlighted, inference can be even more resource-intensive over time, as it's a continuous process with a high number of invocations. Therefore, developing resource-efficient alternatives for LLM inference is crucial for sustainability. This work proposes GREEN-CODE, a framework for energy-aware code generation in LLMs. GREEN-CODE performs dynamic early exit during LLM inference. We train a Reinforcement Learning (RL) agent that learns to balance the trade-offs between accuracy, latency, and energy consumption. Our approach is evaluated on two open-source LLMs, Llama 3.2 3B and OPT 2.7B, using the JavaCorpus and PY150 datasets. Results show that our method reduces the energy consumption between 23-50% on average for code generation tasks without significantly affecting accuracy.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) are becoming pervasive in software development, aiding in tasks such as code comple-tion, modification, bug fixing, and code translation. Tools like GitHub Copilot [1] and Amazon Q Developer [2] have already been adopted by software engineers, automating their tasks with high accuracy and boosting productivity [3]. However, a significant issue with these LLM-based code generation tools is their high resource consumption, leading to substantial energy and carbon footprints.\nWhile the resource-intensive nature of LLM training is often highlighted, the energy demands of inference\u2014the process of using the models in a deployed setting are less frequently discussed. However, over time, inference often becomes more resource-hungry than training as pre-training is done only once while inference runs continuously, scaling for millions of users. For example, a study [4] estimates that ChatGPT's daily energy consumption during inference is about 564 MWh, meaning it uses the same amount of energy in just 2.3 days as it did for the entire pretraining process of GPT-3's, which consumed 1,287 MWh.\nTools that rely on code generation models are continuously used during software development, e.g., suggesting code com-"}, {"title": "II. BACKGROUND MOTIVATION", "content": "GREEN-CODE's solution is based on the intuition that shallow layers can predict many tokens correctly, while deeper layer inferences are required for fewer token predictions. To understand the potential of early exiting, we conducted pre-liminary experiments with the 3B parameter model Llama 3.2. The model is fine-tuned with aggregated loss (as introduced later in Section III-D) that is capable of exiting at intermediate layers, generating predictions through a single LM head. We used fixed exiting in this experiment."}, {"title": "III. SYSTEM MODEL AND METHODOLOGY", "content": "In this section, we first present the system model of GREEN-CODE framework. Then, describe the rationale be-hind datasets and LLMs selected. Next, we discuss the LLM fine-tuning process designed to enable early exiting, providing an overview of our methodology."}, {"title": "A. System Model", "content": "The high-level view of the GREEN-CODE framework is illustrated in our system model as depicted in Figure 2. It primarily consists of two parts. (1) offline RL agent training, and (2) online energy-aware code generation.\nThe offline phase of the GREEN-CODE framework involves several steps. First, we begin by selecting appropriate pre-trained models and standard input datasets that align with the problem requirements, focusing on real-world code completion tasks. The selected datasets undergo preprocessing to ensure compatibility with the LLM's input format. This involves tok-enization, normalization, and splitting into training, validation, and testing sets, among other processes. Second, we apply a specialized fine-tuning process to make LLMs suitable for early exiting. This involves introducing an aggregated, weight-based loss function [21], enabling the model to decode from hidden states of an intermediate layer. Third, we formulate the exit method as a Reinforcement Learning (RL) problem. In this setup, the RL agent is trained to learn and balance the trade-off between computational resources used, energy consumption, and output quality (accuracy) by dynamically"}, {"title": "B. Data Selection & Preparation", "content": "In this work, we consider on-the-fly code-to-code comple-tion task with whole code files, instead of single function from a given docstring. The goal here is to predict the next to-ken(s) given some code context. Accordingly, we selected two datasets from the CodeXGlue [22] benchmark. CodeXGlue provides dataset collection for various code-related tasks. In total, it consists of 10 tasks with 14 datasets. We specifically use the datasets designated for the code completion task, namely, JavaCorpus [23] and PY150 [24], representing two widely used programming languages, i.e., Java and Python, respectively."}, {"title": "C. Candidate Model Selection", "content": "In this work, we considered open-source autoregressive, decoder-only models, as these represent the state-of-the-art LLM models recently released by several large institutions. We use two model families: OPT [25] (Open Pre-trained Transformers) and Llama [26]. While many variations of these model families exist, distinguished by their size in terms of the number of parameters, we selected smaller-sized open-source models based on our in-lab testbed resources. The main resource constraint is full-parameter fine-tuning, which requires manifold runtime GPU memory of its model size, for storing optimizers and activations besides the model itself. With these considerations, we chose OPT and LLaMa 3.2, with parameter sizes of 2.7B and 3B, and consisting of 32 and 28 layers, respectively. Table II summarizes the models we chose for the experiments. For both models, we use the open-source versions available from Huggingface."}, {"title": "D. Finetuning Pretrained Models for Early Exiting", "content": "Standard pretrained LLMs cannot be directly used for dynamic early exits. In typical inference settings, the hidden state from the final layer is passed through a Language Modeling Head (LM head). It is a linear fully connected layer that maps the hidden state to a token prediction within the vocabulary [27]. However, in the context of early exiting, when the final layer's hidden state is not utilized, an alternative approach is required to produce the final output. There are two primary strategies for this: (i) training additional LM head at each potential exit layer to map hidden states from that layer, or (ii) employing specific fine-tuning techniques to make intermediate layer decoding viable.\nUsing multiple LM heads requires significantly higher mem-ory resources. To overcome this, GREEN-CODE implements specialized fine-tuning to decode from intermediate layers with a single LM head. For this, we adapt the approach presented in [21], which introduces the LITE (Losses from InTermediate layErs) method. Here, the original LM head is used to collect losses from the intermediate layers. Therefore, the loss func-tion in this fine-tuning process includes intermediate layers:\n$Loss = \\frac{\\sum_{i=1}^{N} w_{i} loss_{i}}{\\sum_{i=1}^{N} w_{i}}$ (1)\nwhere $(w_{i})$ is a weight given to the loss of layer i ($loss_{i}$) and N is the number of layers. This loss is then propagated back through the network, enables the model to learn to decode from intermediate layers.\nHowever, fine-tuning a model to have a single LM head for early exit requires other careful considerations as it leads to some potential performance loss. Fine-tuning is a resource- and time-intensive activity, finding optimal weights for differ-ent layers (hyperparameters $w_{i}$ ) is challenging. For instance, giving more weight to lower layers should improve the results in lower layers, however, it leads to an overall performance loss. Moreover, identifying optimal $w_{i}$ weights (an NP-hard problem) and exit points with exhaustive hyperparameter op-timization is computationally expensive and even infeasible.\nWith these considerations, we implemented the following rules to define exit points:\n\u2022 The earliest exit point for both models is set at layer 4, ensuring that a portion of the model's full performance is retained in any case."}, {"title": "IV. GREEN-CODE: LEARNING ENERGY EFFICIENT CODE GENERATION", "content": "The primary objective of our work is to achieve energy-efficient code generation by dynamically selecting exit points during the output token generation. Once we have a fine-tuned model capable of dynamic exiting, we need to develop an algorithm to identify an early exit strategy.\nThe GREEN-CODE framework formulates the dynamic early exit problem as a reinforcement learning (RL) problem. The following subsections describe the individual components of the RL agent's training process."}, {"title": "A. Defining the environment", "content": "One of the main components of for-mulating any problem into the RL domain is to create an environ-ment where an agent can interact and learn the complexities and trade-offs between different configurable and dependent pa-rameters and take decisions accordingly. A high-level overview of the environment is illustrated in Figure 5. It comprises two main components. (i) Codebase, a dataset that the LLM processes to generate observations. (ii) Fine-tuned LLM model, capable of exiting at intermediate layers (see Section III-D)."}, {"title": "B. Defining State Space", "content": "In RL, a state represents the current configuration of the environment. This state, comprising multiple features, provides all the necessary information for the agent to take action to achieve its goals. However, including excessive features in-creases the state space exponentially [28], making the training and use of RL agents computationally expensive. Particularly in our code generation case, the inference must be performed in real time with minimal computational overhead.\nTherefore, we use the current layer hidden state produced by the last layer. This hidden state encapsulates the current contextual understanding of the layer, and it does not require additional computational and monitoring overhead to gather from the environment."}, {"title": "C. Designing an Action Space", "content": "An action is a decision taken by an RL agent to transition between states in its environment. The agent selects actions based on a policy, which is a strategy mapping states to actions, to maximize cumulative rewards over time. For our"}, {"title": "D. Reward Function", "content": "Rewards give feedback on the value of an agent's actions. Upon taking an action, the agent receives a reward signal from the environment, indicating how beneficial the action was for achieving the desired outcome. Thus, accurately defining rewards is crucial for shaping the agent's learning effectively.\nThe intuition of this paper and empirical observations (Sec-tion II) indicate that many tokens can be accurately predicted with just a few early layers. Thus, an agent should exploit this property efficiently by taking advantage of early exits to reduce energy consumption. However, in many cases, deeper layers are necessary to avoid degraded accuracy. Consequently, rewarding an RL agent for aggressive early exits leads the agent to avoid deeper layers completely. Hence, we need a reward structure to balance this trade-off. Additionally, we need to consider LLM-specific intricacies while designing reward structures. LLMs process input one token at a time, so using episodic rewards (i.e., rewards based on performance over an entire sample) can mislead the agent. This is because a single token's poor reward can overshadow the correct actions taken for other tokens. Therefore, the agent should make decisions based on local properties while optimizing for global, sample-level consistencies, as rewards are optimized over an episode.\nBased on these insights and observations, we defined our reward functions for energy efficient early-exiting as below:\n$r_{e} = \\begin{cases}1, & \\text{if } y_{pred} = y \\text{ and } l_{curr} = l_{opt} \\\\-(\\frac{l_{curr} - l_{opt}}{l_{opt}}). \\alpha, & \\text{if } y_{pred} = y \\text{ and } l_{curr} \\neq l_{opt} \\\\-(\\frac{l_{opt} - l_{curr}}{l_{opt}}). \\beta, & \\text{if } y_{pred} \\neq y \\text{ and } l_{curr} < l_{opt} \\\\-\\epsilon, & \\text{otherwise} \\end{cases}$ (2)\n$r_{c} = \\begin{cases}1, & \\text{if } l_{curr} < l_{opt} \\\\1-(\\frac{l_{next} - l_{opt}}{l_{opt}}). \\gamma, & \\text{otherwise} \\end{cases}$ (3)\nHere, $l_{curr}$ is the current layer being considered, $l_{opt}$ is the first layer whose prediction matches the prediction of the final layer (the optimal exit point), and $l_{next}$ represents the subsequent layer. y denotes the token prediction at the last layer, serving as the ground-truth token for reward calculation, while Ypred represents the prediction at $l_{curr}$. The coefficients \u03b1, \u03b2, \u03b3 are trade-off parameters, constrained by 0 \u2264 \u03b1, \u03b2,\u03b3 \u2264 1, which controls the agent's learning behavior. We also scale penalties to the interval [-1,0] to stabilize learning. For simplicity, we represent all layers in the formulation; however, our specific exit points are based on fine-tuning method (i.e., exits are not allowed on non-finetuned layers), and rewards are calculated accordingly.\nThe possible reward for an exit action $r_{e}$ is given in Equation 2. Here, the first condition applies a fixed reward if the decision to exit is optimal, meaning the current layer's prediction is correct (i.e., matches the prediction of the final layer), and it is the shallowest layer to do so, thus, $l_{y' = y^{l'} < l_{curr}}$. The second condition in Equation 2 is the penalty for exiting too late. It occurs when the current layer produces a correct token, but a shallower layer l' exists such that l'<lcurr with y' = y. This penalty is proportional to the distance between $l_{curr}$ and $l_{opt}$, scaling with the number of steps from the optimal layer. While such late exits still yield correct predictions in practice, overly rewarding them causes the agent to exploit exiting decisions excessively. Thus, the agent should avoid making suboptimal exits. Nevertheless, in experiments, we set a \u2264 \u03b2 so that exiting late is at least as good (or better) than exiting too early. The third condition in Equation 2 represents the worst-case scenario: exiting at a layer that is too early, resulting in an incorrect prediction. The penalty here is also scaled based on the distance from the optimal exit layer. The final condition for exit, covers rarely occurring edge cases where Ypred \u2260 y and lcurr > lopt. We assign a small constant penalty to this scenario.\nSimilarly, the possible reward for continue action $r_{c}$ is given in Equation 3. Here, we reward optimal decisions with a constant value of 1 when continuing. A continuation is considered optimal if $l_{curr} < l_{opt}$, meaning the agent has not yet reached the optimal layer. In contrast, we penalize all other cases where lcurr \u2265 lopt, scaling the penalty based on the distance between the current layer and the optimal layer. Here, we use lnext = lcurr + 1 because a continuation is incorrect if lcurr = lopt, since the agent should have exited at this point. Again, we introduce a coefficient to control the penalty. Increasing coefficients a and y should lead the agent to avoid too late exits, while increasing \u1e9e avoids too early exits, managing the trade-offs."}, {"title": "E. Cost function", "content": "The objective of the training is to maximize the objective function in Equation 4. Here, $r_{e}$ and $r_{c}$ are step rewards as defined above. Note that we define $r_{c} = 0$ if the action is exit and $r_{e} = 0$ if the action is continue.\n$J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}} \\sum_{t=1}^{T} (r_{e}(t) + r_{c}(t))$ (4)\nwhere $\u03c0_{\u03b8}$ is the policy parameterized by \u03b8, T is the number of time steps per episode and t is the current time step."}, {"title": "F. Training an RL agent", "content": "A RL agent needs to be trained to understand the com-plexities of the environment and take appropriate actions to achieve the desired objective. In this case, managing trade-offs between energy consumption, the accuracy of the code generation task and identifying optimal exit points. For this, RL agent requires multiple episodes during the training period and attempts to maximize its rewards for actions taken in each episode. Once the cumulative reward across multiple episodes stabilizes, we assume the RL agent has converged.\nIn each episode, the RL agent performs resets and steps within the environment. A reset can occur either when an exit action is taken on the last token predicted by the LLM or when the last layer of the final token is surpassed by a continue action. During a reset, the environment first samples a code file uniformly from the dataset and generates T tokens using the LLM. The context provided to the LLM is determined randomly by uniformly sampling from the interval [0.2,0.6]. If an exit action is taken, the internal state of the environment moves to the next token, and rewards are assigned accordingly. If a continue action is taken, the process advances to the next layer, mirroring the way early exiting is used during inference. In summary, we provide an environment that gives the agent only the hidden state of the current layer and current token. Therefore, agents must learn the efficiency-accuracy trade-off solely based on the hidden state and the reward signal."}, {"title": "V. IMPLEMENTATION AND DEPLOYMENT", "content": "We employed the standard Gymnasium [29] framework to implement our GREEN-CODE solution. It is developed based on the OpenAI Gym [30] framework. Gymnasium provides an API that facilitates communication between learning agents. For RL algorithms, we used the widely adopted PyTorch-based Stable-Baselines3 (SB3) library [31], which provides the implementation of state-of-the-art RL algorithms.\nAs specific algorithm, we chose the Proximal Policy Op-timization (PPO) [32] algorithm, an RL method that has demonstrated effectiveness across various tasks in different domains. PPO offers stability in training by using clipping approaches to avoid excessively large updates, leading to more stable training. We employed single environment and shallow network configurations. We trained the network until convergence was observed in both episodic and stepwise rewards. On average, it took 200k-500k steps before observing convergence. A detailed description of the hyperparameter setup and results are provided in the next section.\nInference Deployment: We extract the policy from the trained RL agent and convert it into a PyTorch-based Deep Neural Network (DNN) by utilizing the corresponding policy and action networks from SB3.\nFor deployment, we implemented an endpoint that deploys the LLM alongside an RL agent. The endpoint adheres to the Huggingface Inference API, enabling our LLMs to be used in the Huggingface VSCode extension\u00b9. Consequently, our setup"}, {"title": "VI. PERFORMANCE EVALUATION", "content": "We used an in-lab GPU server. Our machine is equipped with an NVIDIA Quadro RTX 8000 as the GPU accelerator, with 49152 MiB of VRAM. The server itself is equipped with an AMD EPYC 7452 CPU with 32 CPU cores, 1.0 TiB of main memory, running Ubuntu 20.04.6 LTS.\nWe measure the metrics capturing both efficiency (hardware and model level) and performance of the model itself.\n1) Efficiency Metrics: We measure the following efficiency-related metrics. Number of layers skipped, which is hardware-independent and measures how many layers are skipped during inference. Latency, measures the time (in seconds) taken to get a response for a user request. It is dependent on the hardware used. Energy Consumption (in Ws), provides the real-world cost and sustainability impact of running a model. Unlike theoretical measures, energy consumption accounts for the actual operational power usage cost during inference, reflecting the efficiency of hardware utilization and overheads introduced. Finally, Throughput, measures the number of tokens/second generated, providing a fine-grained analysis of model performance.\nWe directly measure number of layers skipped, throughput, and latency. Energy consumption is measured using ZeusMon-itor 2, which uses pynvml, but also approximates for a lower sampling rate.\n2) Performance metrics: Traditional NLP evaluation of-ten focuses on text matching through metrics like n-gram matching, code generation demands additional syntactic and semantic correctness to ensure functionality. Standard LLM metrics like BLEU and ROUGE, provide a broad performance estimate [33], prioritizing lexical precision, which may not align with the evaluation of the functional aspects of the code. Accordingly, we also use code generation specific metric, i.e., CodeBLEU [34]. CodeBLEU still uses n-gram matches (similar to BLEU) but also incorporates elements relevant for evaluating code models. We also report \"Syntax\" and \u201cdataflow\u201d metrics, which are sub-metrics of CodeBLEU, enhancing our evaluation. For all these three metrics, the minimum value is 0 and the maximum value is 1."}, {"title": "B. Hyperparameter Settings", "content": "For our RL agent training, we used the hyperparameters de-picted in Table III. We relied on standard parameters provided by the RL library for training PPO. We limited the network hidden size to 32 or 64 and used 1 or 2 layers to minimize in-ference overhead, ensure faster training, and reduce the risk of overfitting due to an excessively large network. It is important to note that we did not perform hyperparameter optimization due to the significant computational costs associated with it. Nevertheless, our empirical results still demonstrate superiority over baselines, and further hyperparameter optimization would likely enhance performance.\nIn our evaluation, we use the softmax output of the policy network to decide which action to take. By adjusting the temperature parameter (T) and softmax thresholds, we can control the balance between exploration and exploitation dur-ing inference. In other words, these two thresholds determine how \"sure\" the agent must be before deciding to exit (e.g., a threshold of 0.9 is much stricter than a threshold of 0.5). Thus, we report results with different thresholds."}, {"title": "C. Experiment Design", "content": "In our experiments, we primarily focused on line-completion tasks. Given a code context, the objective is to complete the next line of code (LOC). We set the maximum number of tokens generated (max_new) to 15, ensuring that a full LOC is completed. The average output length for line-level completion is approximately 7 tokens according to CodeXGlue, and some earlier work also uses 10 tokens [18]. We split the code file uniformly across samples, using the first 20% of the tokens of a sample as context. This results in variable context lengths for different samples due to variations in dataset files. However, we limited the maximum context length to 512 tokens to speed up evaluations and avoid memory constraints. Similar thresholds are used CodeXGlue benchmarks are 488 for PY150 and 365 for JavaCorpus.\nWe provide the model with the first n tokens of a sample as input and use the tokens from the dataset in the range [n+1,n+maxnew] as ground truth labels. We then calculate corpus-level metrics by comparing the ground truth tokens against the tokens generated by the model. We always evaluate on 1000 samples from the test sets of the datasets."}, {"title": "D. Evaluation of RL Agent Training", "content": "Figure 6 illustrates the mean reward per step in each episode for PPO agents trained over 500,000 steps. Note that the number of episodes (x-axis) varies across models due to differences in episode lengths, which depend on the specific"}, {"title": "E. Evaluation of Code Generation", "content": "In this section, we present runtime evaluations of our method, which integrates the RL agent into the inference"}, {"title": "F. Overhead Analysis", "content": "GREEN-CODE introduces computational overhead in two parts, (i) forward pass through the RL network and (ii) softmax computation to decide the agent's action for a given threshold.\nWe conducted an isolated experiment measuring energy and latency to eliminate inaccuracies caused by nested measure-ments. Table IV presents the relative energy and time overhead of both OPT and Llama models for two variations each: (i) a model with EE enabled and (ii) full model without exits.\nThe results show that the additional energy required for the RL agent is reasonable compared to the overall energy"}, {"title": "VII. RELATED WORK", "content": "Pruning techniques, such as LLM-pruner [12], LaCo [13] or SoBP [14] aim to reduce the model size by decreasing the number of layers or weight blocks, thus creating lightweight models that require smaller resource consumption. Similarly, quantization techniques [5]\u2013[8] aim to reduce the model size by using lower precision weights, e.g., converting float-ing point numbers to small integer representations. Another method is knowledge distillation (KD) [9]\u2013[11], where the original model teaches a smaller student model its outputs. By learning the predictions and complexities of a teacher, the student aims to mimic the behavior of the larger model and is more efficient as it is smaller. However, all these model preprocessing techniques reduce model performance irreversibly and do not adapt to dynamic contexts.\nThe studies most relevant to our work are early exiting methods. The work in [17] introduces early exiting on top of T5 models with confidence scores or using classifiers to decide when to early exit. BERxIT [16] improves on this by introducing learn-to-exit modules. The LayerSkip method [35] uses dropout during training to enable early exiting and employs self-speculative decoding, where early exited results are verified and corrected by the remaining layers of the model. The most relevant work to us is Sun et al. [18], which performs early exiting on code generation. Their method introduces a classifier-based exiting approach with multiple LM heads and additional stopping of inference if a prediction seems non-promising. Similar to our work, ConsistentEE [19] uses reinforcement learning for exiting, introducing policies and LM heads at every layer where exiting is possible. However, both these works introduce additional overhead with multiple LM heads. Furthermore, the LITE framework [36] introduces aggregation from losses of intermediate layers to enable EE with the original LM head, but only uses confidence-based ex-"}, {"title": "VIII. CONCLUSIONS", "content": "Efficient early exiting strategies can significantly reduce energy consumption and computational latency in LLM code generation tasks. However, identifying the right exit layer without degrading model accuracy is extremely challenging due to the complexities of model architectures and input distributions. To address this, we present GREEN-CODE, a framework that dynamically selects early exit points for given input tokens of a code file. GREEN-CODE employs a customized fine-tuning method with weighted aggregate loss to adapt base pre-trained LLMs to exit at intermediate layers with a single LM head. Furthermore, we frame the early exiting problem as an RL decision problem and train the RL agent, which dynamically chooses the right exit layer during code completion tasks. Our experiment results demonstrated that GREEN-CODE achieves comparable accuracy to the non-fine-tuned base model using all layers, with significant reductions in energy consumption and latency, demonstrating the feasibility of such techniques in real-world usage.\nIn the future, we plan to extend this framework to incor-porate larger models for different LLM tasks. We also intend to consider complex multi-agent-based software engineering workflows, where long-running inference workflows benefit more with an early exiting strategy."}]}