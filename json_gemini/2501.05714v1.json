{"title": "How to Enable Effective Cooperation Between Humans and NLP Models: A Survey of Principles, Formalizations, and Beyond", "authors": ["Chen Huang", "Yang Deng", "Wenqiang Lei", "Jiancheng Lv", "Tat-Seng Chua", "Jimmy Xiangji Huang"], "abstract": "With the advancement of large language models (LLMs), intelligent models have evolved from mere tools to autonomous agents with their own goals and strategies for cooperating with humans. This evolution has birthed a novel paradigm in NLP, i.e., human-model cooperation, that has yielded remarkable progress in numerous NLP tasks in recent years. In this paper, we take the first step to present a thorough review of human-model cooperation, exploring its principles, formalizations, and open challenges. In particular, we introduce a new taxonomy that provides a unified perspective to summarize existing approaches. Also, we discuss potential frontier areas and their corresponding challenges. We regard our work as an entry point, paving the way for more breakthrough research in this regard.", "sections": [{"title": "1 Introduction", "content": "Advancements in NLP research have been greatly propelled by large language models (LLMs), which have showcased exceptional abilities (Zhao et al., 2023; Laskar et al., 2024). These advancements are paving the way for the development of AI models that can behave as autonomous agents, working alongside humans to tackle intricate tasks. These models, for example, can cooperate with humans on data annotation (Klie et al., 2020; Li et al., 2023a; Huang et al., 2024c), information seeking (Deng et al., 2023a; Wang et al., 2023b; Zhang et al., 2024d), creative writing (Padmakumar and He, 2022; Akoury et al., 2020) and real-world problem solving (Mehta et al., 2023; Feng et al., 2024; Qian et al., 2024). This growing synergy between humans and models has fueled a surge of research into a new paradigm: Human-Model Cooperation. This paradigm, facilitated by diverse user interfaces ranging from natural language conversations (Ni et al., 2023) to action sequences like clicking buttons (Chen et al., 2023b; Rosset et al., 2020), holds promise for unlocking unprecedented levels of efficiency across various domains.\nIn fact, establishing intelligent models that can interact with humans has always been a long-standing research (Press, 1971; Wallenius, 1975; Milewski and Lewis, 1997; Dzindolet et al., 2003; Bahner et al., 2008; Chien et al., 2018; Touvron et al., 2023; Achiam et al., 2023). The realm of NLP tasks has seen a surge in methods for human-model cooperation (Wang et al., 2023e), particularly in the era of LLMs and Agents (Xi et al., 2023). Recently, emergent surveys also have made commendable strides (Wang et al., 2021a, 2023e; Wu et al., 2023; Yang, 2024; Gao et al., 2024). However, they primarily focus on introducing key elements of human-model cooperation, such as user interfaces (e.g., dialogues), message understanding and fusion, cooperation system evaluation, and applications to NLP tasks (See Table 2 for details). Given all these elements, the information on particular details about how to formalize an effective human-model cooperation to achieve collective outputs is rather under-specified and scattered. Therefore, a comprehensive and systematic analysis of the underlying principles and formalizations of human-model cooperation is still absent. This gap in understanding presents a significant opportunity for advancement, enabling us to develop a deeper understanding of the fundamental basics that govern the effective cooperation between humans and intelligent models.\nTo fill this gap, in this survey, we take the first step to summarize the principles, formalizations, and open challenges of human-model cooperation\u00b9. We begin by introducing the definition and principles of this rapidly evolving field, providing a common ground for understanding. Next, we propose a new and systematic taxonomy for coopera-"}, {"title": "2 Definitions & Principles of Human-Model Cooperation", "content": "Definitions. Over the past few decades, various terms have been used to depict the concept of human-model cooperation. These terms often carry comparable meanings and are occasionally interchangeable. To address this issue, we establish clear definitions for human-model cooperation, carefully differentiating it from other terms. This provides a starting point for exploring this field.\nHuman-Model Cooperation involves the human and the model working together as a unified team, engaging in the decision-making process of shared tasks to achieve a shared goal.\nUnlike non-cooperation (Deng et al., 2023a), shared or aligned goals form the foundation for effective human-model cooperation (Jiang et al., 2022; Wang et al., 2021a). However, cooperation doesn't always mean sharing resources or information. Two parties can work independently to achieve a shared goal without mutual communication. However, human-model collaboration can go beyond cooperation (Hord, 1981). It involves an equal partnership, where both parties work together through bidirectional communication, shared decision-making, and interdependence. This often results in a more coordinated and efficient approach to achieving a shared goal. We focus on the cooperation, leaving discussions on collaboration for future work (cf. Section 4.1).\nPrinciples. While many factors can affect how both parties behave, rational individuals usually aim to align their actions with principles to ensure effective and meaningful cooperation. Drawing inspiration from foundational work in conversational theory, our survey reinterprets the cooperative principles outlined by Grice (1975, 1989) to broaden their applicability beyond cooperative conversation and extend them to general cooperative applica-"}, {"title": "3 Formalization Taxonomy", "content": "Overview. We provide a fine-grained and unified taxonomy for categorizing the formalization of human-model cooperation methods, as illustrated in Figure 1 and Table 1. Our taxonomy is based on who ultimately takes responsibility for the final decision, identifying three main types of cooperation, including 1) sequential cooperation, 2) triage-based cooperation, and 3) joint cooperation. Each form of cooperation adheres to a specific role framework, defining how the cooperators contribute to the overall task. We also showcase typical applications of human-model cooperation in Appendix G for better understanding.\nAdherence to Principles. Guided by the cooperative principle, existing methods for human-model cooperation fundamentally rely on the assumption that both parties act rationally. This implies that they try to take the best action toward achieving their goals instead of making decisions randomly and maliciously. For example, existing methods often rely on fully cooperative user simulators when conducting experiments on human-machine cooperation. These simulators typically define the specifics of cooperation during interaction with the model through rules (Zhang and Balog, 2020; Lei et al., 2020a) or prompts (Sekuli\u0107"}, {"title": "3.1 Sequential Cooperation", "content": "Sequential Cooperation refers to a cooperative process where the human and the model work together in a step-by-step manner, with each step building upon the previous one.\nOverview. Sequential cooperation is the most prevalent form in NLP (Wang et al., 2021a; Wan et al., 2022). In this context, the actions or decisions of one party are influenced by the actions or decisions of the other participant in a sequential fashion (Brocas et al., 2018), primarily concerning increased efficiency and agency (Sperrle et al., 2021). Therefore, this type of cooperation often involves a series of interdependent tasks or actions that are carried out in a specific order to achieve a common goal or outcome. Notably, sequential cooperation mirrors the hierarchical structure of the assistor-executor framework. The first party in this sequence typically assumes the role of the assistant, providing suggestions or feedback that adhere to cooperative principles, while the second party, the executor, ultimately takes responsibility for the final decision. Due to the straightforward nature of this cooperation form, the NLP community has enthusiastically adopted this approach, achieving significant successes in various applications, including data collection and annotation (Fanton et al., 2021; Klie et al., 2020; Casanova et al., 2020; Li et al., 2021), conversational information retrieval (Zamani et al., 2022), schema induction (Zhang et al., 2023), fact checking (Mendes et al., 2023), creative writing and summarization (Chen et al., 2023a; Padmakumar and He, 2022; Akoury et al., 2020), drug editing (Liu et al., 2024), cooperative agent (Zhang et al., 2024a), and many others (Gerlach et al., 2021; Sharma et al., 2023).\nMethods. Given the widespread use of sequential cooperation, we delve deeper into its two specific types: model-assisted and human-assisted method, as illustrated in Figure 2. These distinctions are based on whether the model or the human, respectively, takes on the role of the assistor.\n\u2022 Human-assisted Method\u00b3 involves the model assuming the ultimate decision-making authority while leveraging human expertise to enhance its capabilities by providing feedback on the predicted or intermediate model results. This human feedback could extend throughout the model's lifecycle - from data processing and model selection to optimization, alignment, and evaluation - with human assistance/feedback continuously shaping the model's development (Wang et al., 2021a; Touvron et al., 2023). As a result, human-assisted method can lead to more personalized models (Bae et al., 2020; Liu et al., 2020) and mitigate potential biases and errors inherent in automated processes (Mosqueira-Rey et al., 2023; Bai et al., 2022; Fails and Olsen Jr,"}, {"title": "3.2 Triage-based Cooperation", "content": "Triage-based Cooperation refers to a cooperative process where tasks/data are strategically distributed between the human and the model based on their respective capabilities.\nOverview. Intuitively, humans excel at certain tasks while models demonstrate superiority in others, particularly repetitive and routine tasks (Fitts, 1951). This natural division of labor suggests an idea known as the \"Humans Are Better At/Machines Are Better At\" (HABA-MABA) (Press, 1971; Bradshaw et al., 2012; Dearden et al., 2000), which we term triage-based cooperation. Unlike the potential subordinate relationship observed in sequential cooperation, triage-based cooperation embraces an equal-partnership framework, taking responsibility for the tasks at hand and hence promoting a more balanced and cooperative dynamic. To date, this approach has proven effective across diverse do-"}, {"title": "3.3 Joint Cooperation", "content": "Joint Cooperation refers to a cooperative process where both parties actively participate in the decision-making process, with the final outcome resulting from their collective decisions.\nOverview. Joint cooperation distinguishes itself from the sequential cooperation and triage-based cooperation by typically combining the outputs of both parties to yield a better one. While both joint operation and triage-based operation adhere to the equal-partnership framework, joint cooperation uniquely prioritizes a cooperative decision-making process where the human and the model work together on a single shared task. For better understanding, joint cooperation is founded on the recognition that humans and models excel in different ways and make distinct types of errors (Rosenfeld et al., 2018; Geirhos et al., 2020). This diversity partially stems from their access to unique"}, {"title": "4 Open Challenges and Discussions", "content": "We close our survey by discussing some trends and challenges. We categorize these into the following two primary segments.\nHumans have high stochasticity in the expression of their uncertainty (Berkes et al., 2011; Orb\u00e1n et al., 2016)."}, {"title": "4.1 Technical Considerations", "content": "Which cooperation form is better? While numerous methods for human-model cooperation have been proposed, tailored for different applications, a standardized benchmark is lacking, hindering our ability to objectively compare their effectiveness. This lack of benchmark makes it difficult to answer the crucial question: Which cooperation form is preferred to achieve optimal task performance while minimizing costs? For example, trial-based cooperation emphasizes independent work by the human and model, limiting information exchange and potentially hindering overall task completion quality. While joint and sequential cooperation require greater human intervention than trial-based cooperation, this can lead to higher human interaction costs, and the overall benefits of these labor costs on promoting task performance remain unclear. To answer these questions, a comprehensive empirical benchmark is urgently needed. This may be achieved by, for example, conducting experiments on diverse NLP tasks using various user simulators (Wang et al., 2023c; Kocmi and Federmann, 2023; Tang et al., 2023; Huang et al., 2023) to achieve a comprehensive assessment of the strengths, weaknesses, and potential risks associated with different forms of cooperation.\nHuman Uncertainty Estimation. Uncertainty often plays a significant role in carrying out tasks in human-model cooperation. For example, accurate uncertainty estimation could improve task allocation in triage-based cooperation. However, human decision-making often lacks explicit uncertainty measurement (Yang et al., 2022; Kendall and Gal, 2017; Cha and Lee, 2021; Oh et al., 2020), suffering from epistemic uncertainty8. Technically, human uncertainty estimation is a significant challenge. Existing approaches, such as ensemble learning (Raghu et al., 2019b) or requiring the human to elicite uncertainty intervals (Zhang and Evans, 2021; Maadi et al., 2021), are often unreliable due to the stochastic nature of human brain in expressing uncertainty (Orb\u00e1n et al., 2016; Berkes et al., 2011). Moreover, simulating human decision-making, while promising, requires extensive human decision data (Ma et al., 2023a; Bourgin et al., 2019), making it less practical. Therefore, a greater focus on human uncertainty estimation is critical."}, {"title": "4.2 Social Impact", "content": "We underscore the practical value of the cooperation, extending beyond mere technical aspects.\nTrust Issue. A common problem is trust calibration (Punzi et al., 2023), where humans either under-rely or over-rely on the model's outputs. This can lead to misinterpretations, flawed decisions, and even amplified biases in fairness-related tasks. Even humans can fall victim to these issues, misjudging or misinterpreting model results due to misunderstandings or inappropriate reliance on the model's suggestions (Lai and Tan, 2019; Englich et al., 2006). To foster trust and improve cooperation, explainable models are crucial, especially in tasks requiring ethical and unbiased outcomes. However, the challenges extend beyond just understanding the model. The issue of irony of automation (Bainbridge, 1983) also matters. Humans may misuse, disuse, or even abuse automation due to a lack of experience or understanding of its limitations. This is particularly true for non-professionals, who may have unrealistic expectations of the system's capabilities.\nRegulation & Accountability. We emphasize the need for careful consideration of ethical, regulatory, and risk management aspects as human-model interaction becomes increasingly commonplace. The increasing accessibility of these models raises crucial ethical concerns, particularly regarding public acceptance of autonomous agents (Z\u0142otowski et al., 2017) and the need to address security vulnerabilities like hacking (Ferreira and Teles, 2019; Chen et al., 2018; Zhang et al., 2024b). However, despite exploring various formalizations of human-model cooperation, the question of accountability in practical applications remains complex. Notably, the line of responsibility is blurred, making it difficult to determine who is ultimately responsible for the cooperation system's actions, especially when dealing with the equal-partnership framework where the model contributes equally as the human. This ambiguity highlights the need for clearer regulatory frameworks and ethical guidelines to ensure the responsible and accountable use of human-model cooperation systems."}, {"title": "5 Conclusions", "content": "Intelligent models are expected to cooperate effectively within society for maximum productivity. In the era of LLMs, the moment has arrived to emphasize the advancement of human-model cooperation. While numerous methods for human-model cooperation have emerged, information on how to formalize a human-model team is rather under-specified and scattered. To this end, this survey takes a crucial first step towards understanding human-model cooperation by offering a comprehensive overview of its definition, principles, and formalizations. We also introduce a novel taxonomy to categorize existing research, identifying key research frontiers and their associated challenges. With our survey, we provide a foundation for future exploration and pioneering advancements."}, {"title": "Limitations", "content": "Multi-party Human-Model Cooperation Human-model cooperation holds immense potential, but its complexity cannot be underestimated. While this survey focuses on the cooperation between a single human and a single model, real-world scenarios often involve multiple cooperators. These multi-party cooperation can involve a mix of different cooperation forms, e.g., triage-based and sequential cooperation, leading to intricate dynamics. Furthermore, human-human and model-model cooperation may also emerge within these teams, creating further layers of complexity. Instead, we chose to begin with a more simplified scenario (i.e., a single human and a single model), with the aim of bringing together the under-specified and scattered information about how to formalize an effective human-model cooperation to achieve collective outputs. Given the scope of our work, we will leave the exploration of multi-party human-model cooperation for future research.\nHuman-Model Collaboration Our survey solely focuses on human-model cooperation, excluding human-model collaboration. Human-model collaboration goes beyond the cooperation; it calls for the introduction of bidirectional communication and co-decision making that harnesses the potential of both human and model capabilities (Punzi et al., 2023). However, current methods on sequential cooperation, while effectively leveraging the language capabilities of LLMs for communication, such as in conversational information seeking (Zamani et al., 2022), fall short in facilitating the collaborative decision-making essential for true human-model collaboration. Additionally, methods on triage-based and joint cooperation often neglect the crucial aspect of communication between the two parties. Notably, establishing professional communication is paramount, as it allows both parties to validate their rationality, recognize each other's limitations, and engage in a reciprocal learning process (Rabinowitz et al., 2018). However, while significant progress has been made in exploring the cooperation, a substantial gap persists between these concepts and the practical implementation of human-model collaboration systems.\nHuman-Model Non-cooperation Beyond the cooperation, models can also engage in non-cooperative interactions with the human (Zhang et al., 2024c; Deng et al., 2023b,c). They can negotiate prices with users, employing strategic tactics to reach a favorable outcome (He et al., 2018). They can also attempt to persuade users to donate to charitable causes, leveraging their linguistic prowess to sway opinions and evoke generosity (Wang et al., 2019). Considering the large scope of human-model non-cooperation, we will dedicate future research to exploring this area in greater depth."}, {"title": "C Formal Definition of Human-Model Cooperation", "content": "This section aims to enables a formal perspective towards the human-model cooperation.\nNotations. Considering a task T and a 2-participant interaction, we define the participants set as N = {H, M}, where H denotes the human participant and M denotes the model participant. Additionally, the decision or action variable of participant i \u2208 N is represented as a\u017c \u2208 Ai, where A\u2081 is the action set of participant i. We can express the 2-tuple of action variables of all players as a = (\u0430\u043d,\u0430\u043c). Introducing possibly coupled constraints, let \u03a9 = {\u03a9\u0397,\u03a9\u039c} \u2208 A be the constraint set that records the interaction-oriented or task oriented constraints for human-model interaction, where A = \u0410\u043d \u00d7 \u0410\u043c. For example, a constraint could demand actions that result in high time efficiency or reduced cognitive workload (Zhang et al., 2021b). Therefore, for a 2-tuple of action variables a to be feasible, it is necessary for a \u2208 \u03a9. Additionally, each participant i have a policy \u03c0\u03af that returns the probability distribution over the set of possible actions Ai.\nFormal Definition. Human-model cooperation involves the human and the model working together as a unified team, engaging in the decision-making process of shared tasks to achieve a shared goal. This shared goal could be measured by a shared utility function U (e.g., the task success rate, or an individual's subjective evaluation of the desirability or satisfaction) when both the human and the model act according to their respective policies. In this case, the primary objective of human-model cooperation could be formalized as the following optimization problem:\n$\\begin{array}{cl}\\max \\limits_{A_{i}} \\max \\limits_{a_{-i}} & U(a_{i}, a_{-i}) \\\\ s.t. & a_{i} \\in \\Omega_{i}, i \\in N \\\\ & a_{i} \\sim \\pi_{i}(a | a_{-i}, T, \\Omega, G_{i}),\\end{array}$ \nwhere a_i stands for the action variable of other participant except i. When the both parties work independently, we have ai ~ \u03c0\u03af(\u03b1|\u03a4, \u03a9, Gi)."}, {"title": "D Overview of the Survey", "content": "We present the overview of this survey in Figure 5 and the literature survey tree in Figure 6. This visually illustrates the scope and structure of our research, offering a clear understanding of the key areas covered in our survey."}, {"title": "E History of Cooperation Forms", "content": "This section aims to offer a brief overview of the history and evolutionary of different cooperation forms. In particular, in terms of cooperative complexity, sequential and triage-based cooperation are generally considered simpler than joint cooperation. This is reflected in their historical development, with the former two emerging as early as the 1970s (Press, 1971; Wallenius, 1975) and continuously becoming two mainstream forms, while joint cooperation has only gained prominence in recent years (Kerrigan et al., 2021; Huang et al., 2024e). This shift is attributed to significant technological advancements that have enabled the seamless integration of human and model decision-making algorithms, paving the way for more sophisticated joint cooperative models. As for the sequential cooperation, it has consistently been a focal point of research in human-model cooperation, whose representatives include human-in-the-loop (Wang et al., 2021a) and machine-in-the-loop (Green and Chen, 2020) approaches. As for the triage-based cooperation, research community start this research topic since 1970s (Chow, 1970; Hellman, 1970), where the model has the option to abstain from making a prediction when they are likely to make a mistake. This approach gained further traction in the 2000s (Hendrickx et al., 2024), highlighting the importance of allowing models to acknowledge their limitations. More recently, the NLP community, starting around the 2020s (Zhang et al., 2021b; Feng et al., 2024), has embraced this form of cooperation, recognizing that LLMs and LMs are not omnipotent and that humans and models possess complementary strengths."}, {"title": "F Why Human-Model Cooperation Matters", "content": "Overall. Both humans and models possess unique strengths and weaknesses. While models excel at processing vast amounts of explicit knowledge (i.e., formalized and codified knowledge like documents), humans often possess crucial insights that are difficult to codify for models (Liu et al., 2022). For example, a judge gains valuable knowledge about a defendant through interaction, a skill that current models struggle to replicate. This inherent complementarity between human and model intelligence motivates the development of collaborative systems, where each party contributes their unique strengths to enhance task performance."}, {"title": "G.1 Human-Model Cooperation for Data Annotation", "content": "Data annotation is a challenging (and somehow trivial) task in managing the trade-off between data quality and human resource investment. To touch upon this challenge, existing studies resort to human-model cooperation as follows:\nInteractive Data Annotation \u2013Sequential Cooperation. Interactive data annotation, also known as semi-automatic annotation, streamlines the labeling process by introducing an annotation model that proposes suggestions (model annotations, denoted as f (x)) to a human annotator. As illustrated in Figure 7, the human annotator reviews each suggested label and either accepts it if correct or manually corrects it. Compared to traditional manual annotation, this interactive approach significantly reduces human effort by eliminating the need to generate labels from scratch. Research by (Klie et al., 2018, 2020; Le et al., 2021; Huang et al., 2024c) has shown the potential for interactive annotation to accelerate data labeling. The annotation process concludes when all available unlabeled data has been reviewed and annotated.\nSelective Data Annotation \u2013 Triage-based Cooperation. In this scenario, the data annotation process can be conceptualized as a data triage problem. A specialized data triage module dynamically"}, {"title": "G.2 Human-Model Cooperation for Information Seeking", "content": "Conversational Information Seeking \u2013 Sequential Cooperation. One prominent example is conversational information seeking (CIK), which involves an information retrieval system that utilizes a conversational interface to understand and adapt to users' dynamic preferences, providing real-time retrieval of information based on estimated user needs during (multi-turn) conversations (Zamani et al., 2022; Gao et al., 2021). CIK allows models to gather relevant information about user behavior, intentions, and preferences through verbal communication. Specifically, CIK encompasses three primary research areas: conversational search (Zhang et al., 2018; Rosset et al., 2020), conversational recommender system (Lei et al., 2020b; Huang et al., 2024d), and conversational question answering (Reddy et al., 2019; Peng et al., 2022). Notably, information seeking inherently presents a clear division of labor between the human and the model, with a hierarchical structure where the model serves the human by retrieving desired information. This natural division of roles typically leads to sequential cooperation in conversational information seeking tasks, where the model act as an assistant. In this framework, the human guides the process by providing search queries or feedback, while the model leverages its computational power to efficiently locate relevant information. For better understanding, we illustrate the conversational recommender system (CRS) in Figure 8."}]}