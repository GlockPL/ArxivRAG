{"title": "ReLiK: Retrieve and LinK, Fast and Accurate Entity Linking and Relation Extraction on an Academic Budget", "authors": ["Riccardo Orlando", "Pere-Llu\u00eds Huguet Cabot", "Edoardo Barba", "Roberto Navigli"], "abstract": "Entity Linking (EL) and Relation Extraction (RE) are fundamental tasks in Natural Language Processing, serving as critical components in a wide range of applications. In this paper, we propose ReLiK, a Retriever-Reader architecture for both EL and RE, where, given an input text, the Retriever module undertakes the identification of candidate entities or relations that could potentially appear within the text. Subsequently, the Reader module is tasked to discern the pertinent retrieved entities or relations and establish their alignment with the corresponding textual spans. Notably, we put forward an innovative input representation that incorporates the candidate entities or relations alongside the text, making it possible to link entities or extract relations in a single forward pass and to fully leverage pre-trained language models contextualization capabilities, in contrast with previous Retriever-Reader-based methods, which require a forward pass for each candidate. Our formulation of EL and RE achieves state-of-the-art performance in both in-domain and out-of-domain benchmarks while using academic budget training and with up to 40x inference speed compared to competitors. Finally, we show how our architecture can be used seamlessly for Information Extraction (cIE), i.e. EL + RE, and setting a new state of the art by employing a shared Reader that simultaneously extracts entities and relations.", "sections": [{"title": "1 Introduction", "content": "Extracting structured information from unstructured text lies at the core of many AI problems, such as Information Retrieval (Hasibi et al., 2016; Xiong et al., 2017), Knowledge Graph Construction (Clancy et al., 2019; Li et al., 2023), Knowledge Discovery (Trisedya et al., 2019), Automatic Text Summarization (Amplayo et al., 2018; Dong et al., 2022), Language Modeling (Yamada et al., 2020; Liu et al., 2020b), Automatic Text Reasoning (Ji et al., 2022), and Semantic Parsing (Bevilacqua et al., 2021; Bai et al., 2022), inter alia. Looking at the variety of applications in which IE systems are used, we argue that such systems should strive to satisfy three fundamental properties: Inference Speed, Flexibility, and Performance.\nThis work focuses on two of the most popular IE tasks: Entity Linking and Relation Extraction. While tremendous progress has recently been made on both EL and RE, to the best of our knowledge, recent approaches only focus on at most two out of the aforementioned three properties simultaneously (usually either Performance and Inference Speed (De Cao et al., 2021a), or Performance and Flexibility (Zhang et al., 2022)), hindering their applicability in multiple scenarios. Here, we show that by harnessing the Retriever-Reader paradigm (Chen et al., 2017), it is possible to use the same underlying architecture to tackle both tasks, improving the current state of the art while satisfying all three fundamental properties. Most importantly, our models are trainable on an academic budget with a short experiment life cycle, leveling the current playing field and making research on these tasks accessible for academic groups.\nOur ReLiK system frames EL and RE similarly to recent Open Domain Question Answering (ODQA) systems (Zhang et al., 2023) where, given an input question, a bi-encoder architecture (Retriever) encodes the input text and retrieves the most relevant text passages from an external index containing their encodings. Then, a second encoder (Reader) takes as input the question and each retrieved passage separately and extracts the answer, if it is present, from a specific passage. For our tasks, EL and RE, the input query corresponds to the sentence in which we have to link entities and/or extract relations; the retrieved passages are the entities' or relations' definitions; and predicting"}, {"title": "2 Background", "content": "Entity Linking (EL) is the task of identifying all the entity mentions in a given input text and linking them to an entry in a reference knowledge base. Formally, we can define an EL system as a function that, given an input text q and a reference knowledge base E, identifies all the mentions in q along with their corresponding entities {(m,e) : m \u2208 M(q), e \u2208 E} where m := (s,t) \u2208 M(q) represents a span among all the possible spans M(q) in the input text q starting in s and ending in t with 1 \u2264 s\u2264 t \u2264 |q|.\nRelation Extraction (RE) is the task of extracting semantic relations between entities found within a given text from a closed set of relation types coming from a reference knowledge base. Formally, for an input text q and a closed set of relation types R, RE consists of identifying all triplets {(m,m', r) : (m,m') \u2208 M(q) \u00d7 M(q),r \u2208 R} where m and m' are, respectively, the subject and object spans and r a relation between them. The combination of both EL and RE as a unified task is known as closed Information Extraction (cIE)."}, {"title": "3 The Reader-Retriever (RR) paradigm", "content": "In this section, we introduce ReLiK, our Retriever-Reader architecture for EL, RE, and cIE. While the Retriever is shared by the three tasks (Section 3.1), the Reader has a common formulation for span identification, but differs slightly in the final linking and extraction steps (Section 3.2). Figure 1 shows a high-level overview of ReLiK as a unified framework for EL, RE and cIE."}, {"title": "3.1 Retriever", "content": "For the Retriever component, we follow a retrieval paradigm similar to that of Dense Passage Retrieval (Karpukhin et al., 2020, DPR) based on an encoder that produces a dense representation of our queries and passages. In our setup, given an input text q as our query and a passage p \u2208 Dp in a collection of passages Dp that corresponds to the textual representations\u00b9 of either entities or relations, the Retriever model computes:\nEQ(q) = Retriever(q), Ep(p) = Retriever(p)\nand ranks the most relevant entities or relations with respect to q using the similarity function sim(q,p) = EQ(q)TEp(p), where the contextualized hidden representation of a query q and a"}, {"title": "3.2 Reader", "content": "Differently from other ODQA approaches, our Reader performs a single forward pass for each input query. We append the top-k retrieved passages, P1:K = (P1,...,PK), Pi \u2208 Dp,\u00b3 to the input query q, and obtain the sequence q [SEP]\u3008STo\u3009 \u3008ST1) P1.. (STK) PK, with [SEP] being a special token used to separate the query from the retrieved passages, and (STi) being special tokens used to mark the start of the i-th retrieved passage. We obtain the hidden representations X of the sequence using a Transformer encoder:\nX = Tr (q [SEP] (STo)...pk) \u2208 R\u00b9\u00d7H (2)\nwhere l = |q| +1 + (1 + K) + \u2211k|pk| is the total length in tokens. Now, we predict all mentions"}, {"title": "Entity Linking", "content": "As we now describe the EL step, in this paragraph the retrieved passages will identify the textual representations of the entities we have to link to the previously identified mentions, and thus we will change the notation of P1:K =\n(P1,...,PK) to eo:K = (eo,...,eK), Ci\u22600 \u2208 E. Specifically, for each m \u2208 M(q), we need to find E(q, m), the entity linked to mention m. To do so, we use the hidden representations X from Equation\n4Here eo symbolizes NME (named mention entity), i.e. a mention whose gold entity is not in E, represented by (STo).\n2, and project each mention and special token in a shared dense space using a feed-forward layer:\nM = GeLU (WXm + b\u043c)\nEo:K = GeLU (W[X(STO:K), X(STO:K)] + b\u043c)\nwhere WM E :R2H\u00d7H,bM \u2208RH are learnable parameters, and [X(STO:K), X(STO:K)] \u2208 R(K+1)\u00d72H represent the repetition along the hidden representation axis of the special tokens vectors X(STO:K) \u2208 R(K+1)\u00d7H in order to match the shape of Xm. The probability of mention m being linked to entity ek is computed as:\nPent = Pent(E(q, m) = ek|M, E0:K) =\nT\n\u03c3\u03ba(\u0395\u039a\u039c) Vm \u2208 M(q), k \u2208 {0, ..., K}\nTherefore, if E(q, m) is the gold entity linked to m in q, the loss for EL is:\nK\nLEL = -\u03a3 \u03a31(g,m) (ek) log(pent)\nm\u2208M(q) k=0\nTo train ReLiK for EL, we optimize LEL and the mention detection losses from Section 3.2:\nL = LS + LE + LEL. At inference time we will have the predicted spans M(q) as input to the EL module and we will take argmaxk Pent (E(q, m) = ek|M, E0:K) for each m\u2208 M(q) as its linked entity."}, {"title": "Relation Extraction", "content": "In RE, the retrieved passages for an input text q will instead identify the textual representations of relations r1:K =\n(r1,...,rk),ri \u2208 R. Specifically for each pair of mentions (m, m') \u2208 M(q) \u00d7 M(q) we need to find R(q, m, m'), i.e. the relation types between m and m' expressed in q. To do so, we use the hidden representations X from Equation 2, and project each mention and special token using three feed-forward layers:\nSm = GeLU (WsubjectXm + bsubject)\nOm' = GeLU (Wobject Xm + bobject)\nRk = GeLU (WTX(STK) + br)\nwhere Wsubject, Wobject \u2208 R2HXH, Wr\u2208 RHXH, bsubject, bobject and br. \u2208 RH are learnable param-eters. We obtain a hidden representation for each possible triplet with the Hadamard product:\nTm,m',k = Sm\u2299 Om' \u2299 Rk \u2208 RH"}, {"title": "closed Information Extraction", "content": "In the previous paragraphs, we described how to perform EL and RE separately with ReLiK. However, since both tasks share the same mention detection approach, ReLiK allows for closed IE with a single Reader. In this setup, we use the Retriever trained on each task separately to retrieve e1:K \u2208 EK and r1:K' \u2208 RK'. Then, the Reader performs both tasks at the same time. The only difference is the input for the hidden representations in Equation 2 as (q [SEP]\u3008STo\u3009 \u3008ST1\u3009e1... (STK) \u0435\u043a [SEP] (STK+1) r1 ... (STK+K') \u03b3\u03ba\u03b9). Additionally, we leverage the predictions of the EL module to condition RE by taking:\nXm = [Xs, \u03a7\u03c4, \u03c3(\u0395:KMm)X(STO:K)]\nas the input to the RE module after EL predictions are computed. Notice that now Wsubject, Wobject \u2208 R3HXH. Finally, at training time the loss becomes L = LS + LE + Lel + Lrel for a dataset annotated with both tasks."}, {"title": "4 Entity Linking", "content": "We now describe the experimental setup (Section 4.1) and compare our system to current state-of-the-art solutions (Section 4.2) for EL."}, {"title": "4.1 Experimental Setup", "content": "4.1.1 Data\nTo evaluate ReLiK on Entity Linking, we reproduce the setting used by Zhang et al. (2022). We use the AIDA-CONLL dataset (Hoffart et al., 2011, AIDA) for the in-domain training (AIDA train) and evaluation (AIDA testa for model selection and AIDA testb for test). The out-of-domain evaluation is carried out on: MSNBC, Derczynski (Derczynski et al., 2015), KORE 50 (Hoffart et al., 2012), N3-Reuters-128, N3-RSS-500 (R500) (R\u00f6der et al., 2014), and OKE challenges 2015 and 2016 (Nuzzolese et al., 2015). As our reference knowledge base, we follow Zhang et al. (2022) and use the 2019 Wikipedia dump provided in the KILT benchmark (Petroni et al., 2021). We do not use any mention-entities dictionary to retrieve the list of possible entities to associate with a given mention.\n4.1.2 Comparison Systems\nWe compare ReLiK with two autoregressive approaches, namely, De Cao et al. (2021b), in which the authors train a sequence-to-sequence model to produce, given a text sequence as input, a formatted string containing the entities spans together with the reference Wikipedia title; and De Cao et al. (2021a), which builds on top of the previous approach by previously identifying the spans of text that may represent entities and then generates in parallel the Wikipedia title of each span, greatly enhancing the speed of the system.\nThe most similar approach to our system is arguably Zhang et al. (2022), which was the first to invert the standard Mention Detection \u2192 Entity Disambiguation pipeline for EL. They first used a bi-encoder architecture to retrieve the entities that could appear in a text sequence and then an encoder architecture to reconduct each retrieved entity to a span in the text. We want to highlight that while the Retriever part of ReLiK for EL and Zhang et al. (2022) are conceptually the same, the Reader component differs markedly. Indeed, our Reader is capable of linking all the retrieved entities in a single forward pass, while theirs has to perform a forward pass for each retrieved entity, thus taking roughly 40 times longer to achieve the same performance. Finally, we note that, with the exception of Zhang et al. (2022), all the other approaches use a mention-entities dictionary, i.e., a dictionary that for each mention contains a list of possible entities in the reference knowledge base"}, {"title": "4.1.3 Evaluation", "content": "with which the mention can be associated. In order to build such a dictionary for Wikipedia entities, the hyperlinks in Wikipedia pages are usually utilized (Pershina et al., 2015). This means that, given the input sentence \u201cJordan is an NBA player\", in order to link the span \"Jordan\" to the Wikipedia page of Michael Jordan there must be at least one page in Wikipedia in which a user manually linked that specific span (Jordan) to the Michael Jordan page. While for frequent entities this might not represent a problem, for rare entities it could mean it is impossible to link them."}, {"title": "4.1.4 ReLiK Setup", "content": "We evaluate ReLiK on the GERBIL platform (R\u00f6der et al., 2018), using the implementation of Zhang et al. (2022) from the paper repository https://github.com/WenzhengZhang/EntQA. We report the results of evaluating against the datasets described in Section 4.1.1 using the InKB F1 score with strong matching (prediction boundaries must match gold ones exactly).\nRetriever We train the E5base (Wang et al., 2022) encoder Retriever on BLINK (Wu et al., 2020) before finetuning it on AIDA. We split each document d in overlapping windows q of W = 32 words with a stride S = 16. To reduce the computational requirements, we (1) random subsample 1 million windows from the entire BLINK dataset, and (2) we retrieve hard negatives at each 10% of an epoch. We employ KILT (Petroni et al., 2021) to construct the entities index, which contains |E| = 5.9M entities. The textual representation of each entity is a combination of the Wikipedia title and opening"}, {"title": "4.2 Results", "content": "Performance We show in Table 1 the InKB F1 score ReLiK and its alternatives attain on the evaluation datasets. Arguably, the most interesting finding we report is the improvement in performance we achieve over Zhang et al. (2022). Indeed, not only does ReLiKB outperform Zhang et al. (2022) (60.7 vs 60.5 average) with fewer parameters (289M parameters vs 650M parameters), but it does so using a single forward pass to link all the entities in a window of text, greatly enhancing the final speed of the system. A broader look at the table shows that ReLiKL surpasses all its competitors on all evaluation datasets except R128, thus setting a new state of the art. Finally, another interesting finding is ReLiK outperforming its best competitor by 8.3 points on K50. While the other datasets contain news and encyclopedic corpora annotations, K50 is specifically designed to capture hard-to-disambiguate mentions that involve a deep understanding of the context in which they appear.\nSpeed and Flexibility As we can see from Table 1 last column, ReLiKB is the fastest system among the competitors. Not only this, the second fastest system, i.e., (De Cao et al., 2021a), requires a mention-entities dictionary that contains the possible entities to which a mention can be linked. When not using such a dictionary, the results on the AIDA test set drop by 43% (De Cao et al., 2021a) and, as reported in Table 1, it becomes unusable in out-of-domain settings. We want to stress that systems that leverage such dictionaries are less flexible in predicting unseen entities during training and, most importantly, are totally incapable of linking"}, {"title": "5 Relation Extraction and closed Information Extraction", "content": "In this section, we present the experimental setup (Section 5.1) for RE and cIE, and compare the results of our systems to the current state of the art (Section 5.2)."}, {"title": "5.1 Experimental Setup", "content": "5.1.1 Data\nRE We choose two of the most popular datasets available: NYT (Riedel et al., 2010), which has 24 relation types, 60K training sentences, and 5K for validation and test; and CONLL04 (Roth and Yih, 2004) with 5 relation types, 922 training sentences, 231 for validation and 288 for testing.\nCIE We follow previous work and report on the REBEL dataset (Huguet Cabot and Navigli, 2021), which leverages entity labels from Wikipedia and relation types (10,936) from Wikidata. We subsample 3M sentences for training, 10K for validation, and keep the same test set as Josifoski et al. (2022) containing 175K sentences."}, {"title": "5.1.2 Comparison Systems", "content": "RE We compare ReLiK with recent state-of-the-art systems for RE. As with EL, we compare to a recent trend in RE systems using seq2seq approaches. Huguet Cabot and Navigli (2021) reframed the task as a triplet sequence generation, in which the model learns to translate the input text into a sequence of triplets. Lu et al. (2022) followed a similar approach to tackle several IE tasks, including RE. They were the first to include labels as part of the input to aid generation. However, while these approaches are flexible and end-to-end, they suffer from poor efficiency, as they are autoregressive. Lou et al. (2023) built upon Lu et al. (2022), dropping the need for a decoder by keeping labels in the input and reframing the task as linking mention spans and labels to each other, pairwise. This approach is somewhat similar to our EL Reader"}, {"title": "5.1.3 Evaluation", "content": "component. However, it does not include a Retriever, limiting the number of relation types that can be predicted, and their linking pairwise strategy leads to ambiguous decoding for triplets (See A.6 for more details).\nCIE The task of cIE has traditionally been tackled using pipelines with systems trained separately for EL and RE. We compare ReLiK to two recent autoregressive approaches. Josifoski et al. (2022), inspired by Huguet Cabot and Navigli (2021), generate the triplets with the unique Wikipedia title of each entity instead of its surface form, with the aid of constraint decoding from De Cao et al. (2021b). Rossiello et al. (2023) extend their approach by outputting both surface forms and titles. As with RE, autoregressive approaches do indeed lift the ceiling for cIE. However, they are still slow and computationally heavy at inference time."}, {"title": "5.1.4 ReLiK Setup", "content": "We report on micro-F1, using boundaries evaluation, i.e., a triplet is considered correct when entity boundaries are properly identified with the relation type. For cIE, we consider a triplet correct only when both entity spans, their disambiguation, and the relation type between the two entities, are correct. To ensure a fair comparison with previous autoregressive systems, we only consider entities present in triplets for EL, albeit ReLiK is able to disambiguate all of them.\nRetriever As in the EL setting (Section 4.1.4), we initialize the query and passage encoders with E5 (Wang et al., 2022). In this context, we utilize the small version of E5. This choice is driven by the limited search space, in contrast to the Entity"}, {"title": "5.2 Results", "content": "Linking setting. Consequently, this enables us to significantly lower the computational demands for both training and inference. We train the encoder for a maximum of 40,000 steps using RAdam (Liu et al., 2020a) with a learning rate of le-5 and a linear learning rate decay schedule. For NYT we have |R| = 24 while for REBEL we use all Wikidata properties with their definitions, i.e. |R| = 10,936. For EL we use the same settings as those explained in Section 4.1 with KILT as KB, |E| = 5.9M. We optimize the NCE loss (1) using 24 negatives per batch for NYT and 400 for REBEL.\nRE In Table 2, we present the performance of Re-LiK in comparison to other systems. Notably, on NYT ReLiKs achieves remarkable results, outperforming all previous systems while utilizing fewer parameters and with remarkable speed, around 10"}, {"title": "6 Future Work", "content": "In this work, we presented ReLiK, a novel and unified Retriever-Reader architecture that attains state-of-the-art performance seamlessly for both Entity Linking and Relation Extraction. Furthermore, taking advantage of the common architecture and using a shared Reader, our system is capable of achieving unprecedented performance and efficiency even on the closed Information Extraction task (i.e., Entity Linking + Relation Extraction).\nThe results presented in this paper demonstrate strong performance on held-out benchmarks; however, the robustness of our approach needs further testing across different domains and text varieties. This is further discussed in the Limitations section (8). We see this as an opportunity for future research. The performance of recent systems for both EL and RE is reaching a plateau on many benchmarks. We believe a framework like ReLiK, which is both fast and cost-effective to train and use, will facilitate a renewed focus on the nature of the data used for training and testing EL and RE systems. We encourage research in this direction. In particular, we identify emerging entities (Zaporojets et al., 2022) and the automatic generation of entity and relation verbalizations (Schick et al., 2020) as promising areas for further exploration. Addressing these issues would reduce the reliance on static indexes and human-generated descriptions."}, {"title": "7 Conclusion", "content": "Our models are considerably lighter, an order of magnitude faster, and trained on an academic budget. We believe that ReLiK can advance the field of Information Extraction in two directions: first, by providing a novel framework for unifying other IE tasks beyond EL and RE, and, second, by providing accurate information for downstream applications in an efficient way."}, {"title": "8 Limitations", "content": "The main limitation of our work is that while it enables efficient downstream use of very relevant IE tasks, the experiments presented in this paper are performed on held-out benchmarks, which enable comparisons across systems but, apart from the OOD experiments for EL, do not test or demonstrate ReLiK' effectiveness on a wider range of data. While this is true for any EL or RE model evaluated in the most common benchmarks, we expect the lightweight computation requirements of ReLiK, as well as its state-of-the-art performance, to make it attractive to NLP and real-world applications. Nevertheless, it should always be utilized cautiously, considering shortcomings or limitations such as an entity index frozen in time (KILT was built from a Wikipedia dump from 2020), or AIDA as an old dataset that, despite being manually annotated, contains biases of its own, such as conflicting labels regarding Taiwan and China. The NYT and REBEL datasets, moreover, were distantly annotated, meaning they may contain wrong or missing annotations. Again, while these shortcomings are not exclusive to our work, they need to be taken into account."}, {"title": "A.3 Efficiency", "content": "Efficiency is a crucial factor in the practical deployment of Information Extraction systems, as real-world applications often require rapid and scalable information extraction capabilities. ReLiK excels in this regard, outperforming previous systems in performance, memory requirements, and speed. Table 6 shows the training and inference speeds of ReLiK.\nEL Until now, efficiency has been a clear bottleneck for most EL systems, and this has rendered them useless or highly expensive on real-world applications. Therefore, we discussed the efficiency gains for EL extensively in the main body of this paper, in Section 4.2.\nRE On the RE side, the only system on-par in terms of speed and performance would be USM. Unfortunately, USM is not openly available, limiting its utility for the broader research community and hindering our ability to asses its speed. In Section A.6 we discuss some other shortcomings it has. Instead, Table 6 compares the current openly available RE system with the best performance on NYT, REBEL. As an autoregressive system, inference speeds are several orders of magnitude higher. ReLiK outperforms it by more than 2 F1 points and it is still around 3x faster, while ReLiKs, which still outperforms any previous system, takes only 10s (2s+8s), a 10x gain in terms of speed.\nCIE ReLiK continues to shine in the domain of closed Information Extraction, where it outperforms existing systems in terms of efficiency and performance. Compared with two other leading systems, ReLiKs surpasses them in F1 score while significantly outpacing them in terms of speed. These systems rely on BART-large, making them several orders of magnitude slower. In Table 6 we report on GenIE, as its inference and train time are known, but it should be noted that both GenIE and KnowGL are roughly equivalent in terms of compute. Here, again, the speed gains are multiple orders of magnitude, from 40x with ReLiKs to 15x with ReLiKL.\nIn conclusion, ReLiK redefines the efficiency landscape in Information Extraction. Its unified framework, reduced computational requirements, and speed make it a compelling choice for a wide range of IE applications. Whether used in research or practical applications, ReLiK empowers users to extract valuable information swiftly and efficiently from textual data, setting a new standard for IE system efficiency."}, {"title": "A.4 Ablations", "content": "A.4.1 Entity Linking\nRetriever Table 7 presents the findings of our ablation study conducted on the Retriever using the validation set from AIDA. In the baseline configuration, we initialize the model with E5base and train it"}, {"title": "A.5 Error Analysis", "content": "by optimizing the loss (1) with a focus solely on in-batch negatives. The introduction of hard-negatives substantially improves recall rates. Additionally, document-level information proves beneficial to the Retriever, albeit particularly benefiting AIDA, where relevant information is concentrated in the first token. Furthermore, the pretraining on BLINK demonstrated significant impact, especially on Recall@50, suggesting that pretraining enhances the Retriever ability to rank the candidate entities efficiently.\nPassages Trimming The Retriever serves as a way to limit the number of passages that we consider as input to the Reader. At train time, we set K = 100, which, as Table 7 just showed, has a high Recall@K. However, as the computational cost of the Transformer Encoder that serves as the\nEntity Linking Figure 2 shows an example of the predictions generated by our system when trained on EL. This particular example showcases a common error when evaluating the AIDA dataset. AIDA was manually annotated in 2011 on top of a Named Entity Recognition 2003 dataset (Tjong Kim Sang and De Meulder, 2003). Although it is widely used as the de-facto EL dataset, it contains errors and inconsistencies. A common one is the original entity spans not being linked to any entity in the KB. This could either be because at the time such an entity was not present in the KB, or an annotation error due to the complexity of the task. This leads to NME annotations which at evaluation time are considered false positives, as our system links to the correct entity, such as Bill Brett in the example. Another source of errors is document slicing in windows. While necessary to overcome the length constraints of our Encoder, it can lead to inconsistent or incomplete predictions. For instance, ILO was linked to an entity in a window that did not see further context (Workers Group), while the next window correctly identified ILO Workers Group as an NME.\nRelation Extraction The example shown in Figure 2 is a common error found in predictions on NYT by ReLiK. Due to the semiautomatic nature of NYT annotations, some relations, such as the ones shown in the example, lack the proper context to ensure consistency at inference time. In this case, the system predicts a relation (place_lived) which cannot really be inferred from the text or is"}, {"title": "A.6 USM", "content": "In this section, we want to discuss in detail how ReLiK compares with USM. USM is the current state-of-the-art for RE and was the first modern RE system that jointly encoded the input text with the relation types, breaking from ad-hoc classifiers with weak transfer capabilities or autoregressive approaches that leverage its large language head but are inefficient. Therefore, USM shares a similar strategy to our RE component, in that both rely on the relation types being part of the input, and the core idea is to link mention spans to their corresponding triplet. However, this is where the similarities end. In USM, the probabilities of a mention span being linked to a triplet (i.e., to another entity and a relation type) are assumed to be independent and factorized such that they are computed separately, in a pairwise fashion. Mentions are linked as subjects to the spans that share a triplet (blue lines in Figure 3) and to the relation type label (green lines). Finally, labels are linked to the object entity (red lines). In most cases, these are sufficient to decode each triplet, but we want to point out a shortcoming of this strategy. The decoding is done by pairs. First mention-mention, i.e. in Figure 3 (Jack, Malaga), (Jack, New York), (John, Malaga) and (John, New York); then label-mention (birth place, Malaga), (birth place, New York), (live in, Malaga) and (live in, New York); and finally mention-label (Jack, birth place), (Jack, live in), (John, birth place), (John, live in). At this point, the issue should be clear. From this set of"}]}