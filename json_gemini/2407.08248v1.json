{"title": "Toward accessible comics for blind and low vision readers", "authors": ["Christophe Rigaud", "Jean-Christophe Burie", "Samuel Petit"], "abstract": "This work explores how to fine-tune large language models using prompt engineering techniques with contextual information for generating an accurate text description of the full story, ready to be forwarded to off-the-shelve speech synthesis tools. We propose to use existing computer vision and optical character recognition techniques to build a grounded context from the comic strip image content, such as panels, characters, text, reading order and the association of bubbles and characters. Then we infer character identification and generate comic book script with context-aware panel description including character's appearance, posture, mood, dialogues etc. We believe that such enriched content description can be easily used to produce audiobook and eBook with various voices for characters, captions and playing sound effects.", "sections": [{"title": "Introduction", "content": "Visual arts play an important role in cultural life, providing access to social heritage and self-enrichment [3]. However, most works of art are inaccessible to the visually impaired, whether they are legacy blind, blind, with eye movement disorder or having cognitive eye disease [32]. People with such disease could largely benefit from \"intelligent\" computer vision tools in order to also get precise information about visual arts [40] such as comics [7], manga and webtoon [12]. Visual details and contexts which are essential to understand and feel the beauty of these artworks are often missing in current experimental tools.\nResearchers have considered various ways to make visual art such as comic albums accessible to visually impaired people, including automatic image and/or text/audio descriptions [28] and tactile graphics. However, in the systematic review [26], Oh et al. conclude that image description was out of the scope of interest for most studies, suggesting that automatic retrievals of image-related information is one of the bottlenecks for making images accessible at scale."}, {"title": "Related works", "content": "In this section, we first review comics accessibility and then comics image analy-sis. Finally, we give a focus to recent transformer-based method known as Large Language Model (LLM) applied to comics analysis and understanding."}, {"title": "Comics accessibility", "content": "In 2017, Rayar at al. called on the document analysis community to address the issues of visually impaired people [31]. Then several experimental tools have been proposed such as an accessible comic reader for people with low vision [32] and a study on the user experience of accessible comic book reader for tex-tual sound effects [15,27]. When the participants were asked to select the most important information they desired while reading a comic, the majority pri-oritized scene descriptions, followed by transcriptions and facial expressions of"}, {"title": "Comics image analysis", "content": "Recently, a decade systematic literature review has been proposed by [38] for comics image segmentation, classification and recognition methods. Ear-lier, other surveys presented an extended overview of computational approaches for comics analysis [14] and for comic research in computer science [1]. They highlighted numerous methods for most common element extraction like pan-els, speech balloons and text. Also, some methods have been proposed for very specific elements such as inferring unseen actions [13,44] or balloon tail detec-tion [16,24,35] which are really helpful for accurate text-to-character associa-tion [17]. To our knowledge, automatic text type classification and script gener-ation have not been addressed yet. We review character clustering and identifi-cation (naming) in the two following subsections."}, {"title": "Character clustering", "content": "Character clustering (or re-identification) consists in recognizing characters consistently across all different panels in a comics or series of comics. It is subsequent to character detection for which several methods are proposed in the literature (see surveys introduced in the previous section). It presents significant challenges due to limited annotated data and complex variations in character appearances [41].\nInitially, k-means based method [43] was proposed, but they require spec-ifying the number of clusters (character instance groups), which is unknown in our case. To solve this problem, more advanced clustering method which automatically determine the number of clusters (Manga characters) have been proposed [25,46,47,48]. Ramaprasad et al. [30] used large pre-trained vision en-coder Contrastive Language-Image Pre-Training (CLIP) [29] a multi-modal large language model that can perform zero-shot image classification based on natu-ral language prompts. Sachdeva et al. compared their proposition also to CLIP image feature for representing manga characters [35]. An extended clustering can be achieved by combining visual features with spatial-temporal information, an approach chosen by [48] to achieve unsupervised person re-identification in Japanese manga. Their method relies on design rules such as characters tend to"}, {"title": "Comics and Large Language Model", "content": "Very recently, Sachdeva et al. tackled the problem of diarisation i.e. generating a transcription of who said what and when, in a fully automatic way for visual impairment. The result of this work is similar to our proposition for the dialogue part but does not consider panel scene description. The work [18] proposes to identify and name characters based on a given name list as input and analyse text blocks with LLM. In our proposition, we extend LLM capacities to automatically build the name list from dialogues.\nThe work of Ramaprasad et al. is the most similar to our contribution by proposing to generate accessible text descriptions for comic strips including panel description using LLM prompt engineering [36] as well [30]. In this study, they use famous comics which allows LLM to use their previous knowledge for charac-ter name inference but also generate hallucination effects regarding extra details. MaRU method [39] incorporates a vision-text encoder that combines textual and visual information into a unified embedding space, enabling the retrieval of rele-vant scenes based on user scene description queries (even in another language). This method excels in end-to-end dialogue retrieval and exhibits promising re-sults for scene retrieval which enhance the understanding of and improve acces-sibility of Manga. To extend this study, the authors propose as future work to recognise comic characters for enhancing attribution and understanding of visual content. We address the latter in Section 3.2.\nIn [11], a method to complement missing comic (manga) text content during the manga creation/translation process is proposed. This manga argumentation method mines event knowledge within the comics with large language models. Then, fine-grained visual prompts support manga complement."}, {"title": "Proposed method", "content": "The aim of the proposed method is to automatically generate a structured comic book script-like description of each panel including scene, action and dialogues. This is intended to facilitate enriched text-to-speech (or text-to-braille) con-version for enhanced comic accessibility. The main challenges are to associate each dialogue to corresponding named characters and generate a detailed text description using natural language. Our contribution can be seen as one compo-nent within a broader pipeline, outlined as follows:"}, {"title": "Script generation", "content": "Considering that all the visual and textual elements have been extracted by pre-processing image analysis algorithms, we generate a script of the album gathering all these elements in a single structured text file according to the original layout of each page. We encode the script using Markdown markup language which is appropriated for both comics script\u00b9 and LLM inference on structured documents [22]. Then, the script will be used as context in the next step for further inferences such as character naming (see Section 3.2).\nPage layout Each comic book page is usually composed by several panels that should be read in a certain order e.g. left-to-right and down the \"Z-path\" for English comics books [5]. Assuming that the panel have been previously extracted and ordered during the content extraction pre-processing stage, we propose to list them in a single text file according to their reading order (see script sample at this end of this section).\nText type classification The script is then completed with textual information from previous content extraction module and inserted into each Panel section of the script following the natural text reading order. The detection of speech balloon tail with algorithms from the literature allows us to identify most of the spoken text, and we propose a set of rules to also label the onomatopoeia"}, {"title": "Sound effect", "content": "We consider as sound effect (SFX) single text lines with an im-portant height or slope. If an onomatopoeia is composed of several text lines, they will be considered as separated and different at this stage. We set the min-imum text line height $minH$ - to be considered as sound effect to $minH = 0.025 \\times imagewidth$ (2.5%) and the minimum slope $minS$ to 0.1 (10%) compared to horizontal line, based on empirical experiments. Note that these rules may consider a shopfront or other drawing (big or sloped) as onomatopoeia."}, {"title": "Caption", "content": "In comics, caption or narration boxes are used for narration, transi-tional text (e.g. \u201cMeanwhile...\"), or off-panel dialogue. Captions usually have rectangular borders, but they can also be border-less or floating letters. Here, we simplify the definition and consider as caption all text blocks that are not classified as sound effect and that haven't been associated to a comic character (no associated tail). Assuming an error-free result from the previous tail detec-tion algorithm, we expect this category to be composed by usual rectangular text region providing contextual narrative information throughout the story.\""}, {"title": "Dialogue", "content": "The dialogue sets are composed of all the remaining text, each dialogue being composed by one or more lines of text contained in a speech balloon asso-ciated with a comic character (with a tail pointing to it). Each line of dialogue is preceded by the identifier of the corresponding character i.e. c0, c1. Character's identifier computation comes from character clustering which is detailed below."}, {"title": "Character clustering", "content": "We consider spatial-temporal relationships, as explored in [48], during the instruction-tuning phase. This involves using a generated script that contains ordered panels and text as context (see Section 3.2).\nFor character clustering, we compute image feature vectors on each charac-ter instance using a variant of Contrastive Language-Image Pre-training (CLIP) associated with Vision Transformer (ViT) like in [30,39]. This model is designed to perform zero-shot image classification. It has been trained on a large-scale general purpose image dataset and a text dataset. CLIP uses ViT to get visual features and a causal language model to get the text features. Both the textual and visual features are projected into a latent space of identical dimensions. In our case, we use only its image embedding part and reduce the dimensionality of its output feature vectors. This simplifies the embedding into a form that tradi-tional clustering methods, as presented here, can handle more effectively. Then"}, {"title": "Character's name inference", "content": "We propose an automatic method for inferring character names from the gener-ated script. Contrary to [30] which provides names manually or other methods that use previous knowledge e.g. famous comic character found on internet, we automatically infer character's name from the script to allows comics accessibil-ity of lesser-known books or new series as well.\nTo do so, we leverage the inferring capacity of LLM providing our generated script as context [36]. The script contains spatial-temporal information (panel sequence in reading order and qualified text) and character identifier co, c1, etc.. We propose a chain-of-prompt of four prompts to guide the model throughout its knowledge exploration. LLM is requested to first infer character names using its Named Entity Recognition (NER) capabilities. In this prompt, we also request the model to explain its reasoning to benefit from chain-of-thought approach and minimize eventual hallucinations [36]. Secondly, the LLM is used to associate"}, {"title": "Panel description", "content": "We propose to include each panel content description in the script to allow the reader to get a better understanding of the story before diving into characters dialogues, such as recommended by the BLV community [19].\nImage content description has made great strides in the last two years with the rise of Visual Language Models (VLM). Such models having a great capacity of generalizing, we found that they are able to describe panel images with an important level of detail and give an accurate insight type of lighting, type of view, objects, character, mood, action, etc. Given that these models can also interpret text [39] like OCR, we introduce character name's identifier (see Sec-tion 3.2) artificially into panels. This facilitates correspondences for the VLM between panel images and script content, as illustrated in Fig. 2.\nCharacter's names are written as clearly as possible to avoid any text recog-nition error, using a common font with black colour in a white rectangle. We"}, {"title": "Experiments", "content": "For this exploratory research, we limited our experiments to public domain En-glish comics. A lot of them are available from Digital Comic Museum.com/or Comic Book Plus and have already been annotated in several public datasets. To our knowledge, none of the dataset are proposing text type classification, one dataset (Manga109 [9]) contains character identification annotation and as-sociation to text [17] and none of them propose panel description and script. Note that character clustering and name inference require several (ideally all) consecutive pages from consistent episode, albums or series. This ensures homo-geneous clustering of as many characters as possible at once and the gathering of associated dialogues to facilitate character name inference. Faced with the lack of appropriated dataset, we chose to manually build and share a toy dataset by considering full albums from which at least some pages were part of eBDtheque dataset [10]. We extended part of the dataset by getting complementary pages of public domain titles like golden age American comics which are also part of several other public dataset, but not annotated according to our experiments. We first focused on \"Escape with me\" episode of an old title \"Boy Loves Girl 41\" (1953), from which eBDtheque pages 12 and 15 are both part of. This episode is spread oven page 11 to 17 and composed by 45 panels, 75 character instances and 109 text blocks in total. We exclude advertising content from the last page and do not modify the story analysis. We downloaded all complementary pages from Comic Book Plus.com6.\nWe also experimented on a second more recent (2003) publicly available comic book: \u201cPatents\" from the World Intellectual Property Organization (WIPO)7.\""}, {"title": "Text type classification", "content": "We evaluated text type classification for each text line generated by an OCR as described in Section 3.1. We used Google Vision API to extract text blocks from images using its Optical Character Recognition (OCR) capacity. We combine it with a balloon contour analysis method [33] to detect speech balloons and their tail tips. Each text block detected by the OCR contained by a detected balloon were automatically associated. Text block not contained by a balloon could then only be considered as sound effect if it has important height or slope as described in Section 3.1. In episode \"Escape with me\", all text were detected by the combined text block + balloon contour detection, and we measured the recall (R) and the precision (P) which are 100% and 93% respectively. For \"Patents\", P = 100% and R = 98% respectively. The precision is not at the maximum because some bubbles were detected as two different text blocks due to their complex shape and illustrative text was detected as balloon (e.g. clock numbers and telephone in the last page of the episode). We corrected it manually for the rest of the evaluation to avoid error propagation."}, {"title": "Character clustering and name inference", "content": "Clustering Character clustering is usually preceded by a detection step which supports the consistency in the final textual story reconstruction. To avoid any"}, {"title": "Name inferences", "content": "Character's name inferences are made from the generated script, as introduced in Section 3.2. In fact, we discovered that LLM have enough reasoning capacity to find protagonist's names from a sequence of ordered and identified dialogues, as long as they are quoted at least once by one of the char-acters. Dialogue order and association with comic characters can be computed using automatic methods from the literature (see Section 2.2). We used speech balloon and character association method from [34] and we measured its accu-racy as an indication, assuming that all visible characters have been correctly detected. They are 77.7% and 88.9% respectively for \"Patents\" and \"Escape with me\" respectively. We observed different error types in this method: tail orientation not taken into account (fixed by [16]), balloon with multiple tails, corresponding character not visible (out-of-panel, by phone or in a vehicle, highly overlapping character). We fixed all wrong associations manually before gener-ating the corresponding scripts used for name inferring, in order to avoid any error propagation effects.\nBelow is an example of LLM output after applying the chain-of-prompts introduced in Section 3.2 with the generated script of episode \"Escape with me\" containing 80 dialogue type texts. All tested LLM outputs are available here10."}, {"title": "Contextual panel description", "content": "Panel image description are generated following the presented chain-of-prompt approach [36] of visual LLM (VLLM) to extract first general visual elements from the scene, then contextualize it with character's names and text from the image and the script. We randomly selected VLLM from the currently 16 models available in WildVision [20] Arena12 to show qualitative results among a large panel of VLLM instead of specific ones. We processed panel description using at least two different VLLM with slight variation of the user prompt and selected the most accurate descriptions for inclusion in the final script, named _script_3_ (see here11).\nNote that for this experiment, we manually fixed errors from previous char-acter clustering step (see Section 4.3) to avoid error propagation in image de-scription. We evaluated the panel description by measuring the semantic textual similarity with a human annotated panel description for some pages that we use as ground truth. Text similarity is measured with Sentence Transformers [42] implemented in sbert.net library, based on cosine similarity between text em-beddings and with the model mxbai-embed-large-v1 [37]. We evaluated the 14"}, {"title": "Script generation", "content": "The evaluation of the overall quality of the generated script-like description of the story should ideally be done by accessibility experts who are promoting written accessibility14. Unfortunately, we have not yet collaborated with such experts to build a public dataset with them. This will be done in a future work. Another way could be to re-generate an artificial comic book images sequence from the script and compare it to the original comics version."}, {"title": "Conclusion and future work", "content": "We show that usual comics content analysis combined with zero-shot transform-ers prompt engineering are paving the way to accessible comics, even without any fine-tuning on comic-specific datasets. We proposed a straight forward comics script generation based on extracted comics content and enriched with text types, character names and detailed panel description. The generated script can easily be fed into a text-to-speech module to produce an audiobook and also be used for advanced text-based search and indexing purposes.\nIn the future, we plan to explore additional text categories for illustrative text, etc. aiming to eliminate confusion with onomatopoeia. Additionally, we aim to enhance panel descriptions by incorporating content from previous and next panels, thereby improving coherence and avoiding unnecessary repetition. These enhancements will be validated by accessibility experts. This research as to be extended to other comics genres and LLM understanding capacities should be challenged across languages using multilingual books for instance. Page lay-out and album summary/synopsis information could also be added by extend-ing the chain-of-prompt, according to blind and low-vision people requirements. For text-to-speech, complementary information such as balloon contour analysis could be used to include speech tone (e.g. shouted, whispered, thought) into the script and modulate the tone and speed of speech synthesis."}]}