{"title": "Heterogeneity-Aware Resource Allocation and Topology Design for Hierarchical Federated Edge Learning", "authors": ["Zhidong Gao", "Yu Zhang", "Yanmin Gong", "Yuanxiong Guo"], "abstract": "Federated Learning (FL) provides a privacy-preserving framework for training machine learning models on mobile edge devices. Traditional FL algorithms, e.g., FedAvg, impose a heavy communication workload on these devices. To mitigate this issue, Hierarchical Federated Edge Learning (HFEL) has been proposed, leveraging edge servers as intermediaries for model aggregation. Despite its effectiveness, HFEL encounters challenges such as a slow convergence rate and high resource consumption, particularly in the presence of system and data heterogeneity. However, existing works are mainly focused on improving training efficiency for traditional FL, leaving the efficiency of HFEL largely unexplored. In this paper, we consider a two-tier HFEL system, where edge devices are connected to edge servers and edge servers are interconnected through peer-to-peer (P2P) edge backhauls. Our goal is to enhance the training efficiency of the HFEL system through strategic resource allocation and topology design. Specifically, we formulate an optimization problem to minimize the total training latency by allocating the computation and communication resources, as well as adjusting the P2P connections. To ensure convergence under dynamic topologies, we analyze the convergence error bound and introduce a model consensus constraint into the optimization problem. The proposed problem is then decomposed into several subproblems, enabling us to alternatively solve it online. Our method facilitates the efficient implementation of large-scale FL at edge networks under data and system heterogeneity. Comprehensive experiment evaluation on benchmark datasets validates the effectiveness of the proposed method, demonstrating significant reductions in training latency while maintaining the model accuracy compared to various baselines.", "sections": [{"title": "I. INTRODUCTION", "content": "THE widespread adoption of edge devices, such as smart- phones and Internet-of-things (IoT) devices, each pos- sessing advanced sensing, computing, and storage capabilities, results in an enormous amount of data being produced daily at the network edge. Concurrently, the rapid advancements in artificial intelligence (AI) and machine learning (ML) facilitate the extraction of valuable knowledge from extensive data. The integration of 5G networks with AI/ML technologies is driving the development of numerous innovative applications that have significant economic and social impacts, such as autonomous driving [1], augmented reality [2], real-time video analytics [3], mobile healthcare [4], and smart manufacturing [5]. A key characteristic of these new applications is the substantial and continuously streaming data they produce, which requires efficient processing for real-time learning and decision-making. However, despite these advancements, data sharing is impeded by privacy regulations, such as the General Data Protection Regulation (GDPR), and hardware constraints, like limited communication bandwidth. Federated learning (FL) [6] emerges as a promising solution to these challenges by enabling model training directly on mobile devices in a decentralized manner. FL not only enhances privacy protection but also leverages the computational resources of edge devices.\nIn a standard FL architecture, the system comprises a cloud- based Parameter Server (PS) and multiple clients\u00b9. The PS orchestrates the training procedure while the clients carry out model training. The training process consists of multiple communication rounds between the clients and the PS. In each round, each selected client downloads the most recent global model from PS and updates the model using its local data. Then, these updated local models are uploaded to the PS, where they are aggregated into a new global model. However, the process of transferring models between the numerous clients and the PS imposes a considerable data traffic load, leading to increased training latency and network congestion.\nTo unlock the full potential of FL over mobile edge networks, recent works [7]\u2013[12] have explored Hierarchical Federated Edge Learning (HFEL) by leveraging multi-server collaboration for model training. These works typically adopt a hierarchical architecture that integrates the advantage of the cloud-based and decentralized federated learning (DFL) [13]\u2013 [16] to enhance the speed and reliability of the FL system. In these works, clients are connected to a proximal edge server via wireless networks, while edge servers either connect to a central cloud server via an edge-to-cloud (E2C) network or form a peer-to-peer (P2P) network without connecting to a central cloud server. Each edge server acts as a local PS, orchestrating the training process with its connected clients. To facilitate collaborative training over broader distances and"}, {"title": "II. RELATED WORK", "content": "FL at mobile edge networks faces several challenges, such as high training latency and resource cost due to the presence of system and data heterogeneity. To mitigate these issues, several resource-optimization-based algorithms have been pro- posed to enhance both training latency and resource utilization of FL. For instance, Luo et al. [23] introduced an adaptive client sampling algorithm aimed at minimizing convergence time in heterogeneous systems. Perazzone et al. [24] proposed a joint client sampling and power allocation scheme to reduce convergence error and average communication time under a power constraint, though their approach does not consider local computation. Wang et al. [25] formulated an optimiza- tion problem to minimize convergence bounds by adaptively compressing model updates and determining the probability of local updates. However, these works predominantly focus on cloud-based FL, which relies on a single cloud server to coordinate the model training.\nA few studies [26]\u2013[30] have explored resource- optimization of HFEL at mobile networks, where multiple edge servers are responsible for aggregating model updates from a subset of devices. In particular, Zhang et al. [26] proposed a framework for wireless networks, which enhances"}, {"title": "III. PRELIMINARIES AND PROBLEM FORMULATION", "content": null}, {"title": "A. Federated Learning over Mobile Edge", "content": "We consider a hierarchical federated learning system as shown in Fig. 1. Assume there are C clusters in the system. Every cluster $c \\in [C]$ possesses a server colocated with a base station. Each cluster owns a set of devices $S_c$, and the number of devices is $N_c = |S_c|$. Note the devices in $S_c$ only communicate with the server within the same cluster via the wireless communication links, e.g., 5G. We define the set of all devices in the system as $S = \\bigcup_{c=1}^{C}S_c$. The total number of devices is $N = |S|$.\nThe edge backhaul communication pattern is defined as $G_b = {V, E_b}$, which is an undirected and connected graph. Here V denotes the set of all servers, and $E_b$ is the set of possible communication links. Moreover, let $A_b$ be the adjacency matrix of $G_b$, where $(A_b)_{c,c'} = 1$ if there is an edge between server c and server c' and $(A_b)_{c,c'} = 0$ otherwise."}, {"title": "B. Model Training Process", "content": "The objective of FL is to find a model $w \\in \\mathbb{R}^d$ that minimize the following global objective function:\n$\\min_{w} F(w) = \\frac{1}{N} \\sum_{n=1}^{N} F_n(w)$,\nwhere $F_n(w) = \\mathbb{E}_{x \\sim D_n} [l_n(w; x)]$ is the local objective function of device n, $D_n$ is the data distribution of device n. Here $l_n$ is the loss function, e.g., cross-entropy, and x denotes a data sample drawn from the distribution $D_n$. We define the cluster-level objective function as\n$\\min_{w} f_c(w) = \\frac{1}{N_c} \\sum_{n \\in S_c} F_n(w)$,\nHere, $f_c$ is the objective function of the c-th cluster, which is the average of the local objective of all devices from cluster c. Then we can rewrite the global objective function (1) as:\n$\\min_{w} F(w) = \\sum_{c=1}^{C} \\frac{N_c}{N} f_c(w)$.\nWe summarize the detailed training process in Algorithm 1. Specifically, we assume there is a central controller that will coordinate the operations of the HFEL system. It collects the device-specific parameters, including the CPU cycle per sample $C_n$, the size of the training data $D_n$, energy budget $E_n$, the capacitance coefficient $a_n$, decision boundaries $f_{min}$, $f_{max}$, $p_{min}$, $p_{max}$, and background noise power $N_o$, before the training starts. The extra costs (e.g., bandwidth consumption and time cost) for information collection are ignored as in prior works [19], [20], [23], [25], [31]\u2013[33]. Other inputs are hyper-parameters, e.g., sampling frequency K, batch size I, and local iterations S depending on user specification.\nAt the beginning of t-th global round, each server collects its real-time communication bandwidth to other servers $B_c^t$ and sends it to the coordinator (line 2). Each global round includes R edge rounds. At the beginning of r-th edge round, each edge device records its observed signal-to-noise ratio $SNR_n^{t,r}$ and communication bandwidth to connected edge server $b_n^r$, and then sends them to the coordinator (line 5). Coordinator determines the allocated bandwidth for each client $b_n^{t,r}$, CPU frequency $f_n^{t,r}$, and graph topology $G_t$ by solving Algorithm 2 (line 6). The solving details will be elaborated in Section V. After that, server c broadcasts the latest global model $u_c^{t,r}$ to all devices in $S_c$ (line 7). Then, devices receive $u_c^{t,r}$ and initialize their local model to be the received global model (line 9). Next, the device runs S iterations local update (SGD) on its local training dataset (lines 10-13). After local training, the device uploads the model to the server c (line 14). Server c aggregates the local models from devices in $S_c$ (line 16). The edge training lasts for R rounds. After edge training, server c communicates $\\psi$ times with its neighbor server $c' \\in N_c^t$ to synchronize the global model (lines 18-19)."}, {"title": "C. Cost Analysis of HFEL", "content": "1) Device Communication Time: We assume the com- munication follows the Frequency Division Multiple Access (FDMA) protocol, and each server adaptively assigns its communication bandwidth to all devices within the cluster. Let $b_n^{t,r}$ be the assigned bandwidth for device n at edge round r and global round t. Then, the communication rate of device n is\n$\\Gamma_n^{t,r} = b_n^{t,r} \\log_2(1 + SNR_n^{t,r})$,\nHere, $SNR_n^{t,r}$ denotes the signal-to-noise ratio between device n and the corresponding server at edge round r and global round t.\nThe totally assigned bandwidth for all devices in $S_c$ should not exceed the available bandwidth of server c, therefore we have\n$\\sum_{n \\in S_c} b_n^{t,r} \\leq B_c^t, \\forall c, t$\n$b_n^{t,r} > 0, \\forall t, \\forall r, \\forall n$,\nwhere $B_c^t$ denotes the available bandwidth of the server c at edge round r and global round t. The communication time $T_n^{t,r,com}$ of device n can be expressed as\n$T_n^{t,r,com} = \\frac{\\Lambda}{b_n^{t,r} \\log_2(1 + SNR_n^{t,r})}$\nHere, $\\Lambda$ denotes the model size. Note that we only consider the upload time cost since the download time cost is not the bottleneck for the practical HFEL system.\n2) Server Communication Time: Define $B^t \\in \\mathbb{R}^{C \\times C}$ as the matrix of communication bandwidth between servers at global round t, where $B_{c,c'}^t$ is the available bandwidth between server c and c'. If there is no available link between c and c', we set $B_{c,c'}^t = 0, \\forall (c,c') \\notin E_b$. It is worth noting that the communication between servers is accomplished through the edge backhaul, which operates independently from the communication between the server and devices (e.g., through the base station).\nIn this paper, we consider the topology design approach that adaptively selects and deletes the slow communication links from the base graph $G_b$. Specifically, we aim to find an edge backhaul communication pattern $G_t = {V, E_t}$ at each global round. Here $E_t \\subset E_b$ denotes the set of remaining edges in $G_t$. Note different choices of $G_t$ have an impact on model convergence. We will discuss it in a later section. Then, the communication time between server c and its neighbors depends on the slowest links, which can be expressed as\n$T_c^t = \\frac{\\psi \\Lambda}{\\min_{c' \\in N_c^t} {B_{c,c'}^t}}$\nHere $N_c^t$ is the set of neighbors for server c in $G_t$.\nLet $A_t$ be the adjacency matrix of $G_t$, and $D_t$ be the degree matrix. Here $D_t$ is a diagonal matrix, and each element denotes the number of neighbors $D_{c,c}^t = |N_c^t|$. Then, the Laplacian matrix $L_t$ of $G_t$ can be expressed as\n$L_t = D_t - A_t$.\nWe always require the graph $G_t$ to remain connected. Accord- ing to the spectral graph theory [34], it can be translate into following constraint:\n$\\lambda_2(L_t) > 0, \\forall t$\nwhere $\\lambda_l(L_t)$ denotes the l-th smallest eigenvalue of Laplacian matrix $L_t$.\n3) Edge Device Computation Time: Let $\\mu_n$ represent the number of CPU cycles required by device n to process a single training example. The value of $\\mu_n$ can either be measured offline or known as a prior. The total number of CPU cycles required to train one edge round is $SI\\mu_n$, where I denotes the batch size. The computation time for one edge round of device n can be formulated as\n$T_n^{t,r,cmp} = \\frac{SI\\mu_n}{f_n^{t,r}}$\nwhere $f_n^{t,r}$ is the CPU frequency of device n at edge round r and global round t.\n4) Time Model: In Algorithm 1, one global round com- prises R edge rounds conducted within the cluster, followed by $\\psi$ times synchronization between the clusters. The time required for one edge round depends on the slowest device in that round. Therefore, we have\n$T_c^{t,r} = \\max_{n \\in S_c} {T_n^{t,r,cmp} + T_n^{t,r,com}}$,"}, {"title": "5) Edge Device Computation Energy:", "content": "Following [35], the CPU energy cost for one edge round training can be formu- lated as\n$E_n^{t,r,cmp} = \\frac{a_n}{2} S I (f_n^{t,r})^2$,\nwhere $a_n/2$ denotes the effective capacitance coefficient of the computing chipset on device n.\nWe assume the devices could adjust their CPU frequency by leveraging the Dynamic Voltage and Frequency Scaling (DVFS) technique. Due to the hardware limitation, the CPU frequency satisfy\n$f_{min} < f_n^{t,r} < f_{max}$\nHere $f_{min}, f_{max}$ denote the minimum and maximum CPU frequency of device n.\n6) Edge Device Communication Energy: For devices, the energy used for downloading is usually negligible. Therefore, we only consider the communication energy usage during uploading\n$E_n^{t,r,com} = p_n T_n^{t,r,com} = \\frac{p_n \\Lambda}{b_n^{t,r} \\log_2(1 + SNR_n^{t,r})}$\nwhere $p_n$ denotes the communication power of device n.\n7) Energy Model: As the server is usually equipped with a plug-in power supply, the energy cost of the server is not our focus in this paper. For devices, the energy cost has two sources: the energy used for local training and the energy used for wireless transmission between the devices and server. Thus, we have\n$E_n^{t,r} = \\frac{p_n \\Lambda}{b_n^{t,r} \\log_2(1 + SNR_n^{t,r})} + \\frac{a_n}{2} S I (f_n^{t,r})^2$,\nwhere $E_n^{t,r}$ is the energy consumption of device n at edge round r and global round t."}, {"title": "D. Inter-Cluster Gradient Divergence", "content": "In Algorithm 1, there is no actual global model, and each server hosts an edge model that serves as the \"global model\" within the cluster. The edge models belonging to different clusters are usually not the same. Therefore, we introduce the consensus distance to measure the discrepancy between any two edge models\n$\\Upsilon_{c,c'}^t = ||u_c^{t,R-1} - u_{c'}^{t,R-1}||$,\nHere $\\Upsilon_{c,c'}^t$ denotes the consensus distance between the edge model c and the edge model c' at global round t.\nWe define the consensus distance between the edge model and the actual global model\n$\\Upsilon_c^{t,0} = ||u_c^{t,0} - \\bar{u}^{t,0}||$,\nwhere $\\bar{u}^{t,0} = \\frac{1}{C} \\sum_c u_c^{t,0}$ denotes the average of all edge models. Note $\\bar{u}^{t,0}$ is not available. Moreover, the average consensus distance of all edge models is\n$\\Upsilon^t = \\sum_c \\Upsilon_c^{t,0}$.\nSimilar to the weight divergence and consensus distance in prior works [36]\u2013[38], the consensus distance depends on the data distribution across the clusters, which is the key factor in capturing the joint effect of decentralization."}, {"title": "E. Problem Formulation and Challenges", "content": "1) Formulated Optimization Problem: Our goal is to min- imize the total time cost (during local updating and model transmission) while ensuring model convergence. Specifically, we aim to devise a control strategy by adjusting the decision variables, e.g., graph topology, communication bandwidth, and CPU frequency, which translates into the following problem:\n$\\mathcal{P}1: \\min_{\\{b_n^{t,r}, f_n^{t,r}\\}, G_t} \\sum_{t=0}^{T-1} T^t \\newline \\text{s.t.} \\quad \\Upsilon^{t+1} \\leq \\Upsilon_{max}, \\forall t \\newline \\quad \\sum_{t=0}^{T-1} \\sum_{r=0}^{R-1} E_n^{t,r} < E_n, \\forall n \\newline \\quad (5), (8), (14)$.\nThe objective of Problem $\\mathcal{P}1$ represents the total time cost of T global rounds. The inequality (20) states the consensus distance should not exceed a threshold $\\Upsilon_{max}$. The role of this constraint is two-fold. First, we use it to mitigate the negative influence caused by the non-IID data distribution between clusters. Additionally, it ensures the convergence of the model when we update the graph topology. The inequality (21) represents the constraint on the energy consumption, where $E_n$ is the energy budget for device n.\n2) Challenges to address the problem: It is a challenge to solve the Problem $\\mathcal{P}1$ due to the: (i) Model convergence is influenced by graph topology. If slow links are removed, the convergence rate may slow down, and more resources may be required to finish the training. (ii) Control decisions in the objective function and constraints are time-slot intercon- nected, yet we lack knowledge of system stats like cluster communication environment and edge backhaul speed. Thus, we need an online algorithm that is not dependent on pre- existing information or system stats assumptions. (iii) The graph topology and resource allocation have different updating frequencies, which prevents online joint optimization."}, {"title": "IV. CONVERGENCE CONSTRAINT", "content": "A. Convergence Result\nFor the convergence of HFEL systems, several studies [8], [11], [39] have been conducted. However, these works mainly focus on the convergence analysis of the learning algorithm in HFEL and assume that all edge devices are homogeneous (i.e., their computation, communication, and storage capabilities are"}, {"title": "B. Consensus Distance Estimation", "content": "Here, we introduce the approach that estimates the con- sensus distance between the server model $u_c^{t,R-1}$ and the global averaged model $\\bar{u}^{t,R-1}$. Our goal is to capture the relationship between the graph topology and the consensus distance. Based on the definition of consensus distance (17) and the aggregation rule (15), we have\n$\\Upsilon^{t+1} = ||\\bar{u}^{t+1,0} - \\bar{u}^{t+1,0}|| \\newline = || \\frac{1}{C} \\sum_{c=1}^{C} u_c^{t,R-1} - \\frac{1}{C} \\sum_{c'=1}^{C} (\\frac{1}{N_{c'}} \\sum_{n \\in S_{c'}} u_n^{t,R-1}) || \\newline = || \\sum_{c=1}^{C} (\\frac{1}{C} u_c^{t,R-1} - \\frac{1}{C} \\sum_{c' \\in {c}\\cup N_t^c} \\frac{1}{N_{c'}} \\sum_{n \\in S_{c'}} A_{c,c'} u_n^{t,R-1}) ||\nFor simplicity, we set $M_{c,c'}^{(t)} = 1/N$. Note it is the possible maximum value [36], [40], [41]. Then we have\n$\\mathbb{E} \\Upsilon^{t+1} = || \\frac{1}{C} \\sum_{c=1}^{C} (1 - A_{c,c'}) (u_c^{t,R-1} - \\bar{u}^{t,R-1}) || \\newline \\leq \\frac{1}{C} \\sum_{c=1}^{C} \\sum_{c'=1}^{C} (1 - A_{c,c'}) \\Upsilon_{c,c'}^t$"}, {"title": "V. SOLUTION DESIGN", "content": null}, {"title": "VI. EXPERIMENT", "content": "We consider a HFEL system with 72 devices and 8 servers (clusters). Each cluster has 9 devices and 1 server. In the"}, {"title": "VII. CONCLUSION", "content": "This paper explored resource-efficient federated learning for a two-tier networked system. We formulated an optimization problem to minimize training latency under the given energy budget. We proposed an efficient algorithm, FedRT, to resolve the formulated problem. Experiment results proved the effec- tiveness of FedRT in reducing training latency and retaining high accuracy compared to conventional methods."}]}