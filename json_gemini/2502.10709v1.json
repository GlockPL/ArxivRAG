{"title": "AN EMPIRICAL ANALYSIS OF UNCERTAINTY IN LARGE LANGUAGE MODEL EVALUATIONS", "authors": ["Qiujie Xie", "Qingqiu Li", "Zhuohao Yu", "Yuejie Zhang", "Yue Zhang", "Linyi Yang"], "abstract": "As LLM-as-a-Judge emerges as a new paradigm for assessing large language mod-\nels (LLMs), concerns have been raised regarding the alignment, bias, and stability\nof LLM evaluators. While substantial work has focused on alignment and bias,\nlittle research has concentrated on the stability of LLM evaluators. In this paper,\nwe conduct extensive experiments involving 9 widely used LLM evaluators across\n2 different evaluation settings to investigate the uncertainty in model-based LLM\nevaluations. We pinpoint that LLM evaluators exhibit varying uncertainty based\non model families and sizes. With careful comparative analyses, we find that em-\nploying special prompting strategies, whether during inference or post-training,\ncan alleviate evaluation uncertainty to some extent. By utilizing uncertainty to\nenhance LLM's reliability and detection capability in Out-Of-Distribution (OOD)\ndata, we further fine-tune an uncertainty-aware LLM evaluator named ConfiLM\nusing a human-annotated fine-tuning set and assess ConfiLM's OOD evaluation\nability on a manually designed test set sourced from the 2024 Olympics. Ex-\nperimental results demonstrate that incorporating uncertainty as additional infor-\nmation during the fine-tuning phase can largely improve the model's evaluation\nperformance in OOD scenarios. The code and data are released at: https:\n//github.com/hasakiXie123/LLM-Evaluator-Uncertainty.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have garnered increasing attention due to their unprecedented per-\nformance in various real-world applications (Zhao et al., 2023; Wang et al., 2024a). In this context,\nhow to accurately assess the performance of a LLM becomes particularly important. This area of re-\nsearch includes benchmark-based evaluation, model-based evaluation, and human evaluation (Chang\net al., 2024). While various benchmarks (Zellers et al., 2019; Hendrycks et al., 2021; Yang et al.,\n2023; Xie et al., 2024) have been proposed to measure the core abilities of LLMs in comprehen-\nsion and generation, human evaluation remains the gold standard for testing overall performance\ndue to its complexity and open-endless. However, this approach is limited by subjectivity issue (Kr-\nishna et al., 2023) and resource costs (Karpinska et al., 2021). Consequently, LLM evaluators have"}, {"title": "2 RELATED WORK", "content": "With rapid development of LLMs, the accurate evaluation of their capabilities has become one of\nthe key challenges in this field. Several LLM evaluation paradigms have been proposed in recent\nyears (Chang et al., 2024), which have coalesced around a few well-established methods, including\nbenchmark-based evaluation, model-based evaluation, and human evaluation.\nBenchmark-based evaluations involve using a set of standardized tests to quantitatively measure\na model's performance across different tasks. Examples include HellaSwag (Zellers et al., 2019),\nHELM (Liang et al., 2022) and MMLU (Hendrycks et al., 2020) for general knowledge and reason-"}, {"title": "3 UNCERTAINTY IN LLM-AS-A-JUDGE", "content": "Task definitions. To ensure the validity of our experimental conclusions, we conduct experiments\nunder two distinct and commonly used evaluation settings, including single-answer grading and\npairwise comparison. See Appendix A for the relevant prompts.\n(1) Single-answer grading (Yu et al., 2024a; Kim et al., 2024a; Li et al., 2023; Liu et al., 2023):\ngiven a user instruction q and a response r from the candidate model, the evaluator is tasked with\nassigning an overall score $s \\in \\mathbb{N}$ based on specific criteria set c, while minimizing potential bias.\nThis is expressed as:\n$s = f(q, r; c, \\theta)$,\nwhere $c = \\{c_1, c_2, . . ., c_m \\}$, each $c_i$ represents a specific evaluation dimension (e.g., content accu-\nracy, logical coherence); $\\theta$ represents the parameters of the LLM evaluator.\n(2) Pairwise comparison (Wang et al., 2024b; Zeng et al., 2024; Raina et al., 2024; Zheng et al.,\n2023): given an instruction q and two responses $r_1, r_2$ from different candidate models, the evalu-\nator is asked to compare the two responses and indicate a preference $p \\in \\{1, 2, Tie\\}$ according to\nc, determining whether one response is better than the other or if they are equally good. This is\nexpressed as:\n$p = f(q, q,r_1, r2; c, \\theta)$"}, {"title": "4 THE IMPACT OF CONFIDENCE IN LLM EVALUATION", "content": "We present the empirical study involving 9 widely-used LLM evaluators (3 proprietary mod-\nels (Achiam et al., 2023) and 6 open-source models (Touvron et al., 2023; Yang et al., 2024a)) with\n2 different evaluation settings (single grading and pairwise comparison) on the MT-Bench (Zheng\net al., 2023) and PandaLM (Wang et al., 2024b) test datasets."}, {"title": "4.1 EXPERIMENTAL SETTINGS", "content": "Prompting strategies. To explore whether special output formats can reduce the evaluation uncer-\ntainty of LLM evaluators, we conduct evaluations using prevalent prompting strategies, including:\n(1) Default (Wang et al., 2024b; Dubois et al., 2024). We instruct the LLM to act as an impartial\njudge and consider factors such as helpfulness and relevance. The LLM is asked to first output its\nrating $s \\in \\{0, 1, . . ., 9\\}$ or preference $p \\in \\{1, 2, Tie\\}$, followed by a brief explanation e.\n(2) Chain-of-thoughts (CoT; (Wei et al., 2022; Kojima et al., 2022)). Instead of generating judg-\nments first, we instruct the LLM to first generate a concise reasoning e before providing its rating s\nor preference p for the responses.\n(3) Self-generated reference (Reference; (Zheng et al., 2023; Zeng et al., 2024)). We prompt the\nLLM evaluator to generate a short reference answer a for the given instruction q. The generated\nanswer is then provided to the LLM evaluator as a reference when making its judgments.\nLLM Evaluators. We employ 6 general yet powerful LLMs across various LLM families as\nevaluators, including GPT-40 (Achiam et al., 2023), GPT-4o-mini, GPT-3.5-Turbo, Llama3-70B-\nInstruct (Dubey et al., 2024), Llama2-70B-Instruct and Qwen2-72B-Instruct (Yang et al., 2024a).\nTo explore the relationship between evaluation capability and evaluation stability (\u00a74.5), we fur-\nther assess the stability of 3 specialized LLM evaluators, including (1) Prometheus2-7b and\nPrometheus2-bgb-8x7b models (Kim et al., 2024c;b), both of which are trained to output in a\nCoT format, providing a concise rationale before indicating a preference or providing its rating; and\n(2) PandaLM (Wang et al., 2024b), which is trained to output in a default format.\nTo enhance reproducibility and alleviate the impact of temperature sampling on uncertainty analysis,\nwe set the temperature to 0 for proprietary models, and utilize greedy decoding for open-source\nmodels. For single-answer grading, the scoring range is 0-9. The evaluation subject is Llama2-\n7B-Instruct. For pairwise comparison, the evaluation subjects are Llama2-7B-Instruct and Llama2-\n13B-Instruct. We query the evaluator twice with the order swapped to eliminate position bias (Wang\net al., 2023a; Jung et al., 2019)."}, {"title": "4.2 RESULTS AND ANALYSIS", "content": "We first conduct an extensive investigation of LLM evaluators with 2 different evaluation settings\nto gain a preliminary understanding of uncertainty in model-based LLM evaluation, showing partial"}, {"title": "4.3 THE INFLUENCES OF DATA DISTRIBUTION", "content": "LLMs are typically trained using next token prediction, where the model generates the most likely\nnext word based on the preceding context (Zhao et al., 2023). Different contexts can lead to multiple\ntoken choices, and the model makes predictions based on the training distribution, which inherently\nintroduces uncertainty. As displayed in Table 2, we study the impact of data distribution on un-\ncertainty in model-based LLM evaluation. The results demonstrate that evaluation confidence, as\nmeasured across both single-answer grading and pairwise comparison settings, exhibits sensitivity\nto changes in data distribution. When the evaluation scenario shifts from common, high-difficulty\ntasks (MT-Bench) to novel, unfamiliar tasks (PandaLM test set), the evaluation confidence fluctuates\nsignificantly (e.g., from 0.417 to 0.473 on GPT-40). In contrast, the response confidence (Table 2b)\nremains more consistent, showing a much smaller variance (0.014) between the two datasets. This\nanalysis highlights that in model-based LLM evaluation, evaluation uncertainty is more pronounced\ncompared to response uncertainty, as evidenced by the lower confidence value and larger confidence\ndifferences when comparing performance across different datasets."}, {"title": "4.4 CAN WE EMPLOY PROMPTING STRATEGIES TO MITIGATE UNCERTAINTY?", "content": "Prompting is the major approach to solving specialized tasks using LLMs. Prior studies demonstrate\nthat special prompting strategies can enhance LLM's performance on downstream tasks by roleplay-\ning (Salewski et al., 2024), incorporating contextual information (Pan et al., 2023; Yang et al., 2024b)\nand standardizing output formats (Wei et al., 2022). To explore whether a well-designed prompt\ncan reduce the evaluation uncertainty of LLM evaluators, we conduct experiments using several\ncommonly used prompting strategies, including Default, Chain-of-thoughts and Self-generated\nreference. The experimental results are shown in Figures 3 and 4. Based on the data presented in\nFigures 3 and 4, we have the following observations:\n(1) Employing special prompting strategies can significantly enhance the evaluation confidence.\nFrom the \"Evaluation Confidence\" subgraphs, we observe that special prompting strategies con-\nsistently lead to higher evaluation confidence across different LLM evaluators. In all experiments\nutilizing the CoT strategy, evaluation confidence improved notably. We speculate that this improve-\nment arises from the structured output formats. By explicitly guiding the LLM through step-by-step\nreasoning before making a judgment, it reduces ambiguity and uncertainty in the evaluation process.\nWhile the Reference strategy also yields positive results, its effectiveness is less consistent across\nevaluators, suggesting that the CoT strategy is more universally applicable and robust.\n(2) The CoT strategy seems to alleviate self-preference bias to some extent. For instance, as shown\nin Figure 3, when Llama2-70B-Instruct evaluates Llama2-7B-Instruct using the CoT strategy, the\nscores are generally lower compared to the Default strategy. This decrease indicates that the evalu-\nator, when prompted to generate reasoning first, may become more objective and critical, reducing\ninherent bias towards the response style and content generated by a closely related model.\n(3) Using the CoT strategy can enhance the LLM evaluators' abilities to perform fine-grained as-\nsessments. As shown in Figure 4, the tie rate decreases in all experiments based on the CoT strategy,"}, {"title": "4.5 IS A SPECIALLY TRAINED LLM A MORE STABLE EVALUATOR?", "content": "As discussed in Section 4.2, there is still a capability gap between an LLM's general performance\nand its evaluation ability. Improved general capabilities normally do not guarantee better evaluation\ncapabilities. To address this issue, prior work (Kim et al., 2024a;c; Wang et al., 2024b; Vu et al.,\n2024) focuses on developing powerful LLM evaluators trained on a large and diverse collection of\nhigh-quality human assessments. Are those specially trained LLMs more stable evaluators? We\nanswer this question by experimenting with 3 open-source evaluators including Prometheus2-7b,\nPrometheus2-bgb-8x7b and PandaLM (Kim et al., 2024c;b; Wang et al., 2024b). The experimental\nresults, as depicted in Table 3, lead to the following conclusions:\n(1) The Prometheus2-7b and Prometheus2-bgb-8x7b models, which are trained in a CoT format,\nconsistently achieve higher evaluation confidence across all experiments compared to both the Gen-\neral LLMs and the PandaLM. We attribute this phenomenon to the step-by-step rationale provided\nby the CoT strategy, which reduces ambiguity in the evaluation process. This phenomenon aligns\nwith the findings from Section 4.4, confirming that using CoT as an output format, whether during\ninferencing or post-training, can help alleviate evaluation uncertainty in LLM evaluators.\n(2) The fine-grained evaluation ability of specially trained LLM evaluators surpasses that of general\nLLMs, as evidenced by the reduced number of tie cases in pairwise comparison (Table 3b). This\nimprovement is likely due to the incorporation of human assessments as training data, which en-\nhances the evaluators' analytical skills. Moreover, in the Prometheus2 models, this benefit is further\namplified by the CoT format.\n(3) As shown in Table 3a, specially trained LLM evaluators appear to be more sensitive to changes\nin data distribution. When moving from MT-Bench to the PandaLM test set, the scores of the"}, {"title": "5 MAKING USE OF UNCERTAINTY FOR BETTER EVALUATION", "content": "As new and tailored tasks constantly emerge in real applications, they pose OOD challenges (Yang\net al., 2023; 2024b; Liu et al., 2024a) to the capability and stability of LLM evaluators. We consider\nthe problem of whether we can utilize the response confidence of candidate models to improve the\nevaluation capability of LLM evaluators for OOD data. To validate this hypothesis, we first collect\nID instances from the Alpaca 52K dataset (Taori et al., 2023) as the fine-tuning set, based on which\nwe fine-tune an uncertainty-aware LLM evaluator named ConfiLM, and assess its evaluation ability\non a manually designed OOD test set."}, {"title": "5.1 DATASET CONSTRUCTION", "content": "Data collection. Each instance of the fine-tuning set and OOD test set consists of an input tuple\n(user instruction q, response 1 $r_1$, response confidence of response 1 $u_1$, response 2 $r_2$, response\nconfidence of response 2 $u_2$) and an output tuple (evaluation explanation e, evaluation result p).\nFollowing Wang et al. (2024b), we sample 150 instructions from the Alpaca 52K dataset as the\ninstruction source for the fine-tuning set. For the OOD test set, we manually craft 50 instructions\nbased on data from the Olympics site. We identify 5 common categories of user questions to guide\nthe construction, including writing, math, extraction, reasoning and roleplay. For each category,\nwe then manually design 10 instructions. Each instruction is accompanied by an optional reference\nanswer. We showcase several sample instances and instructions in Tables 9, 10, 11 and 12.\nThe response pairs $r_1, r_2$ are produced by various instruction-tuned models including Gemma-1.1-\n7B-it (Team et al., 2024), Internlm2.5-7B-chat (Cai et al., 2024), Qwen2-7B-Instruct (Yang et al.,\n2024a), and Mistral-7B-Instruct-v0.3 (Jiang et al., 2023). For each source instruction, we pair the\nresponses from two instruction-tuned models, resulting in a total of 900 unprocessed question-\nresponse pairs for the fine-tuning set and 300 for the test set. We then employ the calculation\nmethod introduced in \u00a7 3 to quantify the response confidence $u_1, u_2$. Notably, to ensure the quality\nand diversity of the generated responses, we set the sampling temperature to 0.7 for all 4 instruction-\ntuned models. Experimental results (Figure 12) indicate that a sampling temperature of 0.7 achieves\ncomparable response confidence to that of greedy sampling while maintaining generation diversity.\nHuman annotations. The output tuple of each instance includes a brief explanation e for the eval-\nuation and an evaluation result p. The evaluation result would be either '1' or '2', indicating that"}, {"title": "5.2 TRAINING DETAILS", "content": "Based on the collected fine-tuning set, we fine-tune the Llama3-8B-Instruct by incorporating re-\nsponse confidence as additional information in the prompt (Figure 9), obtaining an uncertainty-aware\nLLM evaluator named ConfiLM. During the fine-tuning phase of ConfiLM, we use the AdamW\n(Loshchilov, 2017) optimizer with a learning rate of 5e-5 and a cosine learning rate scheduler. The\nmodel is fine-tuned for 6 epochs on 2 NVIDIA A100-SXM4-80GB GPUs. Notably, to differentiate\nthe effects of fine-tuning and the incorporation of response confidence on the model's evaluation\nperformance in the OOD test set, we remove the response confidence $u_1$ and $u_2$ from all fine-tuning\ninstances and fine-tune the Llama3-8B-Instruct again using the same configuration, with the learning\nrate set to 3e-5. We refer to this variant model as Llama-3-8B-Instruct-Finetune. The performance\ncomparison results between different fine-tuning hyperparameters are presented in Figure 11."}, {"title": "5.3 EXPERIMENTAL SETTINGS", "content": "To enhance reproducibility, we set the temperature to 0 for proprietary models and utilize greedy\ndecoding for open-source models. For each evaluation, we query the evaluator twice with the order\nswapped. All general LLM-based evaluators (e.g., GPT-40) are required to output in a CoT format.\nTo obtain the best evaluation results, specially trained or fine-tuned evaluators (e.g., PandaLM-7B)\nare assessed using their original prompt and output format."}, {"title": "5.4 EVALUATION PERFORMANCE ON OUT-OF-DISTRIBUTION DATA", "content": "Table 5 presents the evaluation performance of 12 evaluators on Olympic 2024. Our observations in-\nclude: (1) all LLM evaluators struggle with the Olympic 2024 (with the best F1 score only reaching"}, {"title": "6 DISCUSSION", "content": "LLM-based evaluation requires a comprehensive consideration of prompt optimization (Zhou et al.,\n2023a; 2024a), bias calibration (Zhou et al., 2024b), and uncertainty mitigation strategies. The\nperformance of LLMs as evaluation tools is influenced by various factors, such as the diversity of\ntraining data (Shi et al., 2024), inherent model biases (Zheng et al., 2023), and the complexity of the\ntasks. These uncertainties can cause fluctuations in the consistency of evaluation results. Improving\nthe stability of LLM evaluators can decrease the randomness that may arise during the evaluation\nprocess, thus providing more accurate and reproducible results (Chiang & Lee, 2023).\nWhile our work provides extensive analysis on the stability of LLM evaluators, there are other crit-\nical aspects of evaluation uncertainty that warrant attention. For example, the relationship between\nevaluation uncertainty and evaluation bias, as well as the uncertainty in the evaluation of multimodal\nlarge language models (Li et al., 2024). Our work only focuses on single-round evaluations. For\nevaluations conducted on multi-turn benchmarks (i.e., MT-Bench), we use the first-round question\nas input. It would be interesting to investigate how the uncertainty of LLM evaluators affects judg-\nments on multi-round conversations. Additionally, this research does not cover language models\nthat do not provide token probabilities (e.g., Claude (Anthropic, 2024)). Exploring how to conduct\nuncertainty analysis for LLM evaluators based on these proprietary models is a valuable topic. It is\nalso important to note that commonly used LLM evaluators require strong calibration to ensure that\ntheir output probabilities accurately reflect the precision of their assessments (Chen et al., 2023). We\nprovide an analysis of the relation between evaluation confidence and accuracy in Appendix B.2\nand leave further exploration in those aspects to future work."}, {"title": "7 CONCLUSION", "content": "In this paper, we empirically investigated the existence, mitigation and utilization of uncertainty in\nmodel-based LLM evaluation. Extensive empirical analyses demonstrate that uncertainty is preva-\nlent across various LLMs and can be alleviated with special prompting strategies such as chain-\nof-thought and self-generated reference. Experimental results on an OOD test set with 220 diverse\ninstances show that incorporating uncertainty as auxiliary information during the fine-tuning process\ncan largely improve the LLM evaluators' evaluation performance. We hope the empirical analyses\nin this work and the proposed uncertainty-aware LLM evaluator can inspire future research on the\nstability of model-based LLM evaluation."}, {"title": "A PROMPTS DEMONSTRATION", "content": "All the relevant prompts used in this study are provided in Figures 6, 7, 8 and 9. Prompts for\nPandaLM and Prometheus2 model are obtained from their GitHub repository 1."}, {"title": "B ANALYSIS", "content": ""}, {"title": "B.1 DIFFERENT WAYS OF MEASURING UNCERTAINTY", "content": "In this paper, we used token probabilities to represent the LLM's internal confidence, a method\ninspired by previous works (Varshney et al., 2023; Zhou et al., 2023b; Gupta et al., 2024; Kumar\net al., 2024). To investigate whether different definitions of uncertainty impact the empirical find-\nings, we conducted additional experiments under a pairwise comparison setting on the MT-Bench\ndataset. These experiments involved two commonly used confidence quantification methods: (1)\nVerbalization-based confidence, where we prompted LLMs to directly output calibrated confidence\nscores along with their responses (Lin et al., 2022; Yona et al., 2024); (2) Consistency-based con-\nfidence, which involved generating 5 / 10 / 20 responses to the same question and measuring their\nconsistency as a proxy for confidence (Tian et al., 2023; Xiong et al., 2023). For these experiments,\nwe set the sampling temperature to 0.7."}, {"title": "B.2 THE RELATION BETWEEN EVALUATION CONFIDENCE AND ACCURACY", "content": "To investigate the relation between evaluation confidence and accuracy, we analyzed the average\naccuracy of judgments made by six LLM-based evaluators on Olympic 2024 across different con-\nfidence intervals. The experimental results, as presented in Table 20, reveal a positive correlation\nbetween evaluation confidence and accuracy. Specifically, when evaluation confidence is low, the ac-\ncuracy of judgments across evaluators is generally lower across evaluators. As evaluation confidence\nincreases, judgment accuracy improves steadily, reaching peak performance in high-confidence in-\ntervals (e.g., [0.8, 1.0)). This indicates that models are more reliable in performing evaluation tasks\nwhen evaluating with higher confidence."}, {"title": "B.3 THE IN-DOMAIN EVALUATION PERFORMANCE OF CONFILM", "content": "In Section \u00a75, we fine-tuned an uncertainty-aware LLM evaluator named ConfiLM, which leverages\nthe response confidence of candidate models to enhance ConfiLM's evaluation capability for OOD\ndata. To investigate the evaluation performance of ConfiLM on in-domain (ID) data, we re-split its\nfine-tuning dataset, selecting 94 human-annotated instances as an in-domain test set, named Alpaca-\n94. Based on the remaining 600 fine-tuning instances, we re-trained the models using the same\nexperimental setup as in Section \u00a75.2, obtaining ConfiLM-600 and Llama-3-8B-Instruct-Finetune-\n600 models. Their evaluation performance on Alpaca-94 (ID data) and Olympic 2024 (OOD data)\nis reported in Table 19. Experimental results from Table 19 demonstrate that incorporating uncer-\ntainty as auxiliary information significantly enhances the performance of LLM evaluators in OOD\nscenarios. While ConfiLM-600's advantage is reduced in ID scenarios, it still achieves evaluation\nperformance comparable to Llama-3-8B-instruct-finetune-600."}, {"title": "C DATASET CONSTRUCTION", "content": "Each instance of the fine-tuning set and OOD test set consists of an input tuple (user instruction q,\nresponse 1 $r_1$, response confidence of response 1 $u_1$, response 2 $r_2$, response confidence of response\n2 $u_2$) and an output tuple (evaluation explanation e, evaluation result p). The human-annotated eval-\nuation result would be either '1' or '2', indicating that response 1 or response 2 is better. To ensure\nthe quality and consistency of the human annotations, we first selected 100 samples from the dataset\nfor preliminary annotation by two of the authors. This process facilitated the development of a well-\ndefined annotation guideline. Then, we hired three PhD-level human annotators from an annotation\ncompany to annotate all samples (both the fine-tuning set and the test set) in two rounds: (1) In\nthe first round, two annotators were asked to label each sample based on the established annotation\nguidelines; (2) In the second round, a third annotator reviewed samples where disagreements arose\nand provided an extra label. The final label for each sample is determined through majority voting.\nDuring the annotation process, samples unanimously deemed low quality or difficult to evaluate by\nthe annotators were excluded.\nGiven the ongoing concerns about LLMs' numerical understanding (Liu & Low, 2023), we ver-\nbalized each instance's $u_1$ and $u_2$ into natural language statements to avoid introducing additional\nerrors. The mapping relationship between confidence values and declarative statements is displayed\nin Table 7. Ultimately, we obtained a fine-tuning set containing 694 high-quality instances and an\nOOD test set with 220 diverse instances. The annotator agreement rates are 94.96% and 97.27%,\nrespectively. We showcase several sample instances and instructions in Tables 9, 10, 11 and 12."}, {"title": "D FULL EXPERIMENTAL RESULTS", "content": "The full results of experiments introduced in Sections 4 and 5 are displayed in Tables 15, 16 and\n17. Additionally, to investigate the impact of response confidence on LLM evaluators' evaluation\ncapabilities, we further conducted experiments under two distinct settings: (1) default: providing\nthe evaluator with the complete instance ($q, r_1, u_1, r_2, u_2$) and (2) without confidence: removing\nthe response confidence $u_1$ and $u_2$ from all test instances. All general LLM-based evaluators (e.g.,\nGPT-40) were required to output in a CoT format. To obtain the best evaluation results, specially\ntrained or fine-tuned evaluators (e.g., PandaLM-7B) were assessed using their original prompt and\noutput format. Table 14 presents the evaluation performance of 12 evaluators on Olympic 2024."}]}